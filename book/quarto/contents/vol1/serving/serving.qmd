---
quiz: serving_quizzes.json
concepts: serving_concepts.yml
glossary: serving_glossary.json
---

# Model Serving Systems {#sec-model-serving-systems}

::: {layout-narrow}
::: {.column-margin}
*DALL·E 3 Prompt: An illustration of a machine learning serving system, depicted as a high-speed assembly line in a modern factory. On one end, raw data (images, text documents, sensor readings) enters on conveyor belts. In the center, a glowing neural network processes inputs through preprocessing stations, a central inference engine, and postprocessing units. Workers monitor latency gauges and throughput meters. Some requests wait in small batches while others stream through individually. The scene conveys speed, precision, and the transformation from raw input to useful predictions, with visible clocks emphasizing the time-critical nature of serving.*
:::

\noindent
![](images/png/cover_serving.png)

:::

## Purpose {.unnumbered}

_Why does serving invert every optimization priority that made training successful?_

Benchmarks validated your optimizations under controlled conditions. Serving reveals whether those optimizations survive contact with production reality. Training and serving demand opposite physics. Training maximizes throughput—large batches and long epochs where latency spikes are absorbed invisibly. Serving demands immediacy—individual requests answered in milliseconds, where a single slow response is a broken product. Training amortizes hardware costs across billions of examples; serving pays a tax on every single request, where inefficiency compounds into massive operational debt. Training checkpoints and restarts; serving must never fail visibly. This inversion is why models that train beautifully often serve poorly: the batch-heavy architectures designed to saturate GPUs are fundamentally ill-suited for the bursty, latency-critical reality of production.

::: {.callout-tip title="Learning Objectives"}

- Contrast training and serving system priorities by explaining the inversion from throughput optimization to latency minimization
- Decompose request latency into preprocessing, inference, and postprocessing phases to identify optimization bottlenecks
- Apply Little's Law and M/M/1 queuing models to predict serving system latency under varying load conditions
- Perform capacity planning to meet percentile latency SLOs while accounting for traffic variance and fault tolerance requirements
- Identify sources of training-serving skew and select prevention strategies appropriate to deployment contexts
- Select batching strategies (dynamic, continuous, none) based on traffic patterns and latency constraints
- Evaluate runtime and precision tradeoffs to meet deployment cost and performance requirements

:::

## The Serving Paradigm {#sec-model-serving-systems-serving-paradigm-9634}

Serving marks the transition from model development to production deployment, where the **Iron Law of ML Systems** (@sec-ai-training-iron-law-training-performance-a53f) undergoes a fundamental shift. In training, we optimized for high Throughput and Bandwidth to process massive datasets. In serving, the **Latency term** ($\text{Latency}_{\text{fixed}}$)—the irreducible overhead of request scheduling, network round-trips, and system orchestration—suddenly becomes the dominant constraint. This chapter explores how to re-engineer the system to minimize this fixed 'tax' on every prediction.

Serving introduces an inversion that reshapes the priorities established in prior chapters. Training optimizes for samples processed per hour over days of computation. Serving must deliver predictions within milliseconds under unpredictable load. @sec-benchmarking-ai established techniques for measuring throughput and accuracy under controlled conditions; production serving faces traffic patterns that no benchmark could anticipate. @sec-model-compression provided quantization methods that reduced model size; serving must validate that those optimizations preserve accuracy under real traffic distributions. This inversion from throughput to latency, from controlled to unpredictable, from offline to real time defines the serving challenge.

::: {.callout-perspective title="The Serving Inversion"}
Applying the **DAM Taxonomy** reveals how deployment fundamentally flips your engineering priorities:

*   **Data (Information)**: In training, you maximize **Volume** (shuffling billions of samples). In serving, you maximize **Freshness** (processing one request *right now*).
*   **Algorithm (Logic)**: In training, the math is **Mutable** (updating weights via backprop). In serving, the math is **Frozen** (fixed weights, forward pass only).
*   **Machine (Physics)**: In training, you maximize **Utilization** (keeping GPUs at 100% to saturate throughput). In serving, you maximize **Headroom** (keeping GPUs at 50-70% to survive traffic spikes).
:::

This inversion of priorities motivates a precise definition of *model serving* as the discipline this chapter addresses.

::: {.callout-definition title="Model Serving"}

**Model Serving** refers to the process of exposing trained machine learning models for _real-time prediction_, requiring systems that transform raw inputs into useful outputs while meeting _latency constraints_, maintaining _consistency_ with training behavior, and achieving _cost-effective resource utilization_.

:::

Serving systems must execute a complete inference pipeline under latency constraints, not just the neural network computation. @fig-serving-inference-pipeline illustrates this pipeline: raw inputs flow through preprocessing (traditional computing), neural network inference (deep learning), and postprocessing (traditional computing) before producing final outputs. Each stage contributes to total latency, and bottlenecks can occur anywhere in the pipeline.

::: {#fig-serving-inference-pipeline fig-env="figure" fig-pos="htb" fig-cap="**The Inference Pipeline**: ML serving systems transform raw inputs into final outputs through sequential stages—preprocessing, neural network computation, and postprocessing. The neural network represents just one component; preprocessing and postprocessing rely on traditional computing and often dominate total latency in optimized systems." fig-alt="Flow diagram showing six connected boxes: Raw Input, Preprocessing, Neural Network, Raw Output, Postprocessing, Final Output. Preprocessing and postprocessing are labeled Traditional Computing; neural network is labeled Deep Learning."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n},line width=0.75pt]
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=3pt,
    node distance=0.6,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    align=flush center,
    minimum width=15mm,
    minimum height=10mm
  },
}
%
\node[Box](B1){Raw\\ Input};
\node[Box,right=of B1](B2){Pre-processing};
\node[Box,node distance=1, right=of B2,fill=BlueL,draw=BlueLine](B3){Neural\\ Network};
\node[Box,node distance=1, right=of B3,fill=VioletL2,draw=VioletLine2](B4){Raw\\ Output};
\node[Box,right=of B4,fill=VioletL2,draw=VioletLine2](B5){Post-processing};
\node[Box, right=of B5,fill=VioletL2,draw=VioletLine2](B6){Final\\ Output};
%
\draw[Line,-latex](B1)--(B2);
\draw[Line,-latex](B2)--(B3);
\draw[Line,-latex](B3)--(B4);
\draw[Line,-latex](B4)--(B5);
\draw[Line,-latex](B5)--(B6);
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=3mm,inner ysep=5mm,yshift=2mm,
            fill=BackColor,fit=(B1)(B2),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Traditional Computing};
%
\scoped[on background layer]
\node[draw=OrangeLine,inner xsep=4mm,inner ysep=5mm,yshift=2mm,
            fill=OrangeL!70!red!10,fit=(B3),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Deep Learning};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=3mm,inner ysep=5mm,yshift=2mm,
            fill=BackColor,fit=(B4)(B6),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Traditional Computing};
\end{tikzpicture}
```
:::

This chapter explains how to build systems that orchestrate this pipeline efficiently. The chapter proceeds in three parts. **Part 1** establishes system fundamentals: serving architectures, server anatomy, and the protocols that connect clients to models. **Part 2** follows the request lifecycle: where latency accumulates across preprocessing, inference, and postprocessing, then how queuing dynamics govern system behavior under load. **Part 3** addresses optimization: model lifecycle management ensures models are ready to serve, batching strategies maximize throughput, LLM-specific techniques handle generative workloads, runtime selection tunes performance, and economics translate these choices into infrastructure decisions.

### Static vs Dynamic Inference {#sec-model-serving-systems-static-vs-dynamic-inference-e864}

The first architectural decision in any serving system is whether predictions happen before or during user requests [@google2024staticdynamic]. This choice shapes system design, cost structure, and capability boundaries.

**Static inference** (also called offline or batch inference) pre-computes predictions for anticipated inputs and stores them for retrieval. Consider a recommendation system that generates predictions for all user-item pairs nightly. When a user requests recommendations, the system retrieves pre-computed results from a lookup table rather than running inference. This approach eliminates inference latency entirely since results already exist, enables quality verification before deployment, and reduces serving costs. However, static inference cannot handle novel inputs that were not anticipated during the batch computation and introduces hours or days of latency when models update.

**Dynamic inference** (also called online or real-time inference) computes predictions on demand when requests arrive. This handles any input, including rare edge cases and novel combinations, and immediately reflects model updates. The cost is strict latency requirements that may force simpler models and more intensive monitoring infrastructure.

For our ResNet-50 image classifier, consider two deployment scenarios. A **static approach** suits a photo organization app that pre-classifies all images in a user's library overnight—with 10,000 photos and 5ms inference each, batch processing takes ~50 seconds total, and users see instant classification when browsing. A **dynamic approach** suits a content moderation API that must classify user-uploaded images in real-time, with each image requiring the full preprocessing→inference→postprocessing pipeline and a 100ms latency budget. Most production image classification systems use a **hybrid approach**: frequently requested images (popular products, known memes) are pre-classified and cached, while novel uploads trigger dynamic inference.

The choice between static and dynamic serving has direct economic implications. Stricter latency requirements directly translate into higher infrastructure costs, creating a tradeoff that serving system architects must navigate carefully.

::: {.callout-notebook title="The Cost of Latency"}

Latency constraints directly dictate infrastructure costs. Consider a GPU server renting for \$4/hour.

**Scenario A (Low Latency):** Batch size 1.

*   Latency: 5ms.
*   Throughput: 200 req/s.
*   Cost per million queries: **\$5.55**.

**Scenario B (High Throughput):** Batch size 8.

*   Latency: 10ms (doubled due to batching overhead).
*   Throughput: 800 req/s (quadrupled due to parallel efficiency).
*   Cost per million queries: **\$1.38**.

**The Trade-off:** Reducing latency from 10ms to 5ms increases the hardware bill by **400%**. Engineers must quantify whether that 5ms speedup generates enough business value to justify the 4x cost increase.

:::

Most production systems combine both approaches. Common queries hit a cache populated by batch inference while uncommon requests trigger dynamic computation. Understanding this spectrum is essential because it determines which subsequent optimization strategies apply. Static inference optimizes for throughput during batch computation and storage efficiency for serving. Dynamic inference optimizes for per-request latency under concurrent load, which requires understanding where time goes within each request.

The static-versus-dynamic decision is just the first of several architectural choices that shape serving system design. Equally important is *where* the model executes, since deployment context fundamentally constrains every subsequent optimization.

### The Spectrum of Serving Architectures {#sec-model-serving-systems-spectrum-serving-architectures-8966}

While "serving" often implies a networked server processing API requests, the architectural pattern varies fundamentally by deployment environment. Understanding this spectrum is essential for Volume I's focus on mastering the ML node, since the same model may require radically different serving strategies depending on where it executes.

**1. Networked Serving (Cloud/Datacenter)**

The model runs as a standalone service (microservice). The primary interface is the network (HTTP/gRPC). Optimization focuses on **throughput** (batching) and **concurrency**.

*   *Key Constraint:* Network bandwidth and serialization cost.
*   *Typical Hardware:* NVIDIA GPUs (V100, A100, H100), Google TPUs, AWS Inferentia.
*   *Cold Start:* Seconds to minutes (container startup, model loading, warmup).

**2. Application-Embedded Serving (Mobile/Edge)**

The model runs within the user application process (e.g., a smartphone app using CoreML or TensorFlow Lite). There is no "server." The interface is a function call. Optimization focuses on **energy** and **responsiveness** (SingleStream).

*   *Key Advantage:* **Zero-Copy Inference**. When data moves through a system, each copy consumes CPU cycles and memory bandwidth. In cloud serving, a camera frame might be copied four times: from network buffer to application memory, then to a preprocessing buffer, then to GPU-accessible memory, and finally to GPU VRAM. Mobile NPUs can eliminate most of these copies by sharing memory directly with the camera hardware. The camera writes pixels into a buffer that the NPU reads directly, avoiding the CPU entirely. This reduces both latency (no copy operations) and energy (memory copies consume significant power). The mechanism requires hardware support: the camera, CPU, and NPU must share a unified memory architecture, which modern mobile SoCs like Apple's M-series and Qualcomm Snapdragon provide.
*   *Typical Hardware:* Mobile NPUs (Apple Neural Engine, Qualcomm Hexagon), embedded GPUs (Jetson).
*   *Cold Start:* Milliseconds (model already in app memory); first inference may trigger JIT compilation (100-500ms).
*   *Power Budget:* 1-5W sustained, with thermal throttling after prolonged inference.

**3. Bare-Metal Serving (TinyML)**

The model is compiled into the firmware of a microcontroller. There is no operating system or dynamic memory allocator. "Serving" is a tight loop reading sensors and invoking the interpreter. Optimization focuses on **static memory usage** (fitting in SRAM).

*   *Key Difference:* All memory is pre-allocated (Tensor Arena). Dynamic batching is impossible.
*   *Typical Hardware:* ARM Cortex-M series, ESP32, specialized TinyML accelerators.
*   *Cold Start:* Microseconds (model weights in flash, tensor arena pre-allocated).
*   *Power Budget:* Microwatts to milliwatts; battery operation for months or years.

@tbl-serving-spectrum summarizes how these deployment contexts shape serving system design:

+----------------------+----------------------+----------------------+-----------------+
| **Characteristic**   | **Cloud/Datacenter** | **Mobile/Edge**      | **TinyML**      |
+:=====================+:=====================+:=====================+:================+
| **Latency Target**   | 10-100ms             | 20-50ms              | 1-100ms         |
| **Batch Size**       | 1-128 (dynamic)      | 1 (fixed)            | 1 (fixed)       |
| **Memory**           | 16-80GB VRAM         | 2-8GB shared         | 256KB-2MB SRAM  |
| **Power**            | 300-700W             | 1-10W                | 1-100mW         |
| **Update Mechanism** | Container deploy     | App store update     | Firmware OTA    |
| **Failure Mode**     | Retry/failover       | Graceful degradation | Silent or reset |
| **Monitoring**       | Full telemetry       | Limited analytics    | Heartbeat only  |
+----------------------+----------------------+----------------------+-----------------+

: **Serving Architecture Spectrum**: The deployment context fundamentally shapes every aspect of serving system design. Cloud systems optimize for throughput with dynamic batching; mobile systems optimize for energy with fixed batch-1; TinyML systems operate under extreme memory and power constraints with no dynamic allocation. {#tbl-serving-spectrum}

To make these architectural differences concrete, consider how a single model must adapt to each deployment context:

::: {.callout-perspective title="ResNet-50 Across the Serving Spectrum"}

The same ResNet-50 architecture requires dramatically different serving strategies across deployment contexts:

**Cloud (V100 GPU):**

- Model format: TensorRT FP16 engine (49MB)
- Inference: 1.4ms at batch-1, 14ms at batch-16
- Throughput: 1,143 images/second (batched)
- Memory: 2GB VRAM (model + activations for batch-32)

**Mobile (Pixel 6 NPU):**

- Model format: TensorFlow Lite INT8 (25MB)
- Inference: 12ms at batch-1 (NPU), 45ms (CPU fallback)
- Throughput: ~80 images/second (single-stream)
- Memory: 150MB peak (shared with app)
- Energy: 0.8mJ per inference (NPU), 4.2mJ (CPU)

**TinyML (Cortex-M7):**

- Model format: Not feasible—ResNet-50 requires 98MB weights
- Alternative: MobileNetV2-0.35 quantized to INT8 (1.4MB)
- Inference: 120ms at batch-1
- Throughput: ~8 images/second
- Memory: 320KB tensor arena (fits in 512KB SRAM)
- Energy: 12mJ per inference

**Key insight**: The "same model" claim is misleading—each deployment requires not just different optimization but often different architectures entirely. TinyML serving cannot use ResNet-50; it requires architectures designed for the constraints from the start.

:::

### The Load Balancer Layer {#sec-model-serving-systems-load-balancer-layer-9c4d}

The preceding spectrum focused on how deployment context shapes serving constraints. For cloud and datacenter deployments, where multiple replicas serve the same model, an additional infrastructure layer is required: the load balancer. Production serving systems place load balancers between clients and model servers, providing three essential functions for serving infrastructure.

**Request Distribution** routes incoming requests to available model replicas using algorithms like round-robin or least-connections. For latency-sensitive ML serving, algorithms that route away from slow or overloaded replicas improve tail latency.

**Health Monitoring** continuously verifies that replicas are ready to serve, routing traffic away from unhealthy instances. For ML systems, health checks must verify not just process liveness but model readiness, confirming that weights are loaded and warmup is complete.

**Deployment Support** enables safe model updates by gradually shifting traffic between versions. @sec-machine-learning-operations-mlops examines deployment strategies including canary testing, blue-green deployments, and shadow mode validation.

For single-machine serving with multiple model instances, such as running several ONNX Runtime sessions, the framework and operating system handle request queuing. The full complexity of load balancing becomes essential when scaling to distributed inference systems, where multiple machines serve the same model. The implementation details of request distribution algorithms and multi-replica architectures belong to that distributed context.

**Impact on Queuing Analysis**: When capacity planning considers "the server" in this chapter, it means the single machine's model serving capacity. The queuing dynamics analyzed in @sec-model-serving-systems-queuing-theory-tail-latency-29a6 apply to understanding single-machine behavior and determining when scaling to multiple machines becomes necessary.

While load balancers distribute requests across replicas, achieving predictable latency also requires controlling what happens *within* each machine. The operating system environment introduces its own sources of variability.

### Deterministic Latency and Resource Isolation {#sec-model-serving-systems-deterministic-latency-resource-isolation-4d1c}

An inference server does not operate in isolation. On a single machine, the operating system manages multiple competing processes—logging agents, monitoring tools, and system interrupts—which can intermittently steal CPU cycles from the inference pipeline. These "noisy neighbors" are a primary source of **latency jitter**, where the time required to process identical requests varies significantly, causing the 99th percentile (P99) latency to spike even when the hardware is under-utilized.

To achieve deterministic performance on a single node, systems engineers employ three primary isolation techniques:

1.  **CPU Affinity (Pinning)**: Restricting the inference server's threads to specific physical CPU cores. This prevents the operating system from context-switching the server's processes, ensuring that the "preprocessing" stage of the pipeline always has immediate access to computational resources.

2.  **Memory Locking (`mlock`)**: Instructing the OS to lock the model weights and KV caches in physical RAM. This prevents the system from "paging out" model data to slow disk storage during periods of high memory pressure, ensuring consistent microsecond-scale access times.

3.  **Interrupt Shielding**: Configuring the system to route network and storage interrupts to CPU cores not used by the inference runner. This ensures that a burst of incoming network traffic does not interrupt the GPU's command stream, protecting the "heartbeat" of the inference execution.

Understanding these isolation principles transforms a simple "model script" into a **deterministic service**, a transition essential for safety-critical applications like autonomous driving or real-time industrial control.

## Serving System Architecture {#sec-model-serving-systems-serving-system-architecture-4879}

The preceding section established the architectural spectrum from cloud to TinyML and the infrastructure layers that route requests to models. Building a high-performance serving system requires coordinating multiple software components to minimize overhead and maximize hardware utilization. This section examines the internal architecture of inference servers and the protocols that connect them to clients.

### Internal Architecture and Request Flow {#sec-model-serving-systems-anatomy-inference-server-f12e}

While model optimization focuses on the mathematical artifact, model serving requires a specialized software architecture to manage high-frequency request streams and hardware utilization. An inference server[^fn-inference-server] (such as NVIDIA Triton, TensorFlow Serving, or TorchServe) is not a simple wrapper around a model script; it is a high-performance scheduler that manages concurrency, memory, and data movement.

[^fn-inference-server]: **Inference Server**: The concept emerged from Google's TensorFlow Serving [@olston2017tensorflow], open-sourced February 2016, which pioneered the separation of model logic from serving infrastructure. NVIDIA's Triton [@nvidia2024triton], originally TensorRT Inference Server with GA release in March 2019, extended this to multi-framework support. These servers implement dynamic batching that can improve GPU utilization by up to 70% compared to naive single-request serving. The architecture mirrors the separation of concerns in traditional web servers like Apache (1995) and nginx (2004), applying decades of distributed systems knowledge to ML deployment.


Understanding the internal anatomy of these servers reveals how they bridge the gap between irregular user traffic and the highly regular, batch-oriented requirements of accelerators.

**The Request Pipeline.** Every request traverses a multi-stage pipeline designed to maximize hardware throughput while minimizing latency overhead. @fig-server-anatomy visualizes this internal flow.

::: {#fig-server-anatomy fig-env="figure" fig-pos="htb" fig-cap="**Inference Server Anatomy**: Modern inference servers organize request processing into a decoupled pipeline. The Network Ingress handles high-concurrency protocols (HTTP/gRPC), the Queue buffers bursts of traffic, and the Dynamic Batcher aggregates individual requests into optimized tensors. The Inference Runner manages the low-level execution on the hardware accelerator, ensuring the GPU remains utilized through asynchronous execution." fig-alt="Flowchart showing 6-stage inference server pipeline: Client to Network Ingress to Request Queue (cylinder) to Dynamic Batcher, then down to Inference Runner to Accelerator. Arrows connect stages sequentially."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.2cm]
  \tikzset{
    Box/.style={draw=black!70, thick, rounded corners=2pt, align=center, minimum width=2.5cm, minimum height=1cm},
    Hardware/.style={Box, fill=gray!10},
    Software/.style={Box, fill=blue!10},
    Queue/.style={draw=black!70, thick, shape=cylinder, shape border rotate=90, aspect=0.25, minimum width=1.5cm, minimum height=1.2cm, fill=yellow!10}
  }

  \node[Box, fill=white] (client) {Client\\(Request)};
  \node[Software, right=of client] (ingress) {Network Ingress\\(HTTP/gRPC)};
  \node[Queue, right=of ingress] (queue) {Request\\Queue};
  \node[Software, right=of queue] (scheduler) {Dynamic\\Batcher};
  \node[Software, below=1.5cm of scheduler] (runtime) {Inference Runner\\(TensorRT/ONNX)};
  \node[Hardware, below=1.0cm of runtime] (gpu) {Accelerator\\(GPU/TPU)};

  \draw[->, thick] (client) -- (ingress);
  \draw[->, thick] (ingress) -- (queue);
  \draw[->, thick] (queue) -- (scheduler);
  \draw[->, thick] (scheduler) -- (runtime);
  \draw[->, thick] (runtime) -- (gpu);

  % Labels
  \node[right=0.2cm of queue, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Request Buffering};
  \node[right=0.2cm of scheduler, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Throughput Opt.};
  \node[right=0.2cm of runtime, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Execution Opt.};

\end{tikzpicture}
```
:::

The server architecture serves three critical functions:

1.  **Concurrency Management**: Servers use asynchronous event loops or thread pools to handle thousands of concurrent client connections without blocking. This ensures that network I/O wait times do not idle the accelerator.

2.  **Request Transformation**: The server handles the conversion of network payloads (JSON/Protobuf) into the specific tensor formats required by the optimized model runtime. Image tensors, for example, can be stored as NCHW (batch, channels, height, width) or NHWC (batch, height, width, channels). PyTorch and TensorRT prefer NCHW because it places channel data contiguously, enabling efficient convolution on GPUs. TensorFlow defaults to NHWC, which is more efficient on CPUs. A format mismatch between client and server silently corrupts inference: the model interprets pixel rows as color channels, producing garbage outputs without raising errors.

3.  **Model Management**: Servers manage the lifecycle of models, including loading weights into VRAM, managing versioning, and ensuring that "warm-up" inferences are completed before exposing the model to live traffic.

Of these components, the scheduler deserves special attention because it embodies the fundamental serving tradeoff between throughput and latency.

### The Scheduler: Where Throughput Meets Latency {#sec-model-serving-systems-scheduler-throughput-meets-latency-d022}

The **Scheduler** is the "brain" of the inference server. It implements the dynamic batching logic discussed in @sec-model-serving-systems-throughput-optimization-18d1. The scheduler must decide: "Should I run this one request now to minimize its latency, or wait 5 milliseconds for a second request to arrive and process them together to maximize throughput?"

Systems designers use the **Batching Window** parameter to tune this trade-off. A window of 0ms optimizes for pure latency (no batching), while a window of 10-50ms is common for high-throughput cloud services. This decision determines the "duty cycle" of the GPU—the percentage of time the hardware is actually computing vs. waiting for work.

### Interface Protocols and Serialization {#sec-model-serving-systems-interface-protocols-serialization-5510}

The mechanism used to transport data between client and server directly affects the latency budget. While model inference is often highly optimized, the cost of moving data into the model—serialization and network protocol overhead—can become the dominant bottleneck, especially for lightweight models where inference time is small.

**The Serialization Bottleneck.** Text-based formats like JSON are ubiquitous but computationally expensive. Parsing a JSON object requires reading every byte, validating syntax, and converting text representations into machine-native types. For high-throughput systems, this consumes CPU cycles that could otherwise be used for request handling or preprocessing.

Binary formats like Protocol Buffers (Protobuf) or FlatBuffers reduce this overhead by designing the wire format to map directly to in-memory data structures. This enables "zero-copy" deserialization in optimal cases, where the network buffer can be used directly without allocating new memory.

#### REST vs gRPC {#sec-model-serving-systems-rest-vs-grpc-c7b7}

Two dominant paradigms define modern serving interfaces, each with distinct system characteristics:

**REST (Representational State Transfer)** typically uses HTTP/1.1 and JSON. It is universally supported, human-readable, and stateless, making it the default choice for public-facing APIs. However, standard HTTP/1.1 requires a new TCP handshake for each request (unless keep-alive is carefully tuned), and JSON serialization adds significant latency for numerical data like tensors.

**gRPC (gRPC Remote Procedure Call)**[^fn-grpc] uses HTTP/2 and Protobuf. HTTP/2 enables multiplexing multiple requests over a single persistent TCP connection, eliminating handshake latency and allowing efficient binary streaming. Protobuf provides strict type safety and efficient binary serialization, making it the standard for internal service-to-service communication where latency is critical.

[^fn-grpc]: **gRPC**: Open-sourced by Google in February 2015, gRPC evolved from Stubby, Google's internal RPC framework that had been handling tens of billions of calls per second across their datacenters since approximately 2001. The combination of HTTP/2 multiplexing and Protocol Buffers binary serialization achieves roughly 10x lower serialization overhead than REST/JSON, making it the de facto standard for latency-sensitive ML inference APIs.

The following example quantifies the serialization overhead difference:

::: {.callout-notebook title="JSON vs Protobuf Serialization"}

Consider a request payload containing 1,000 floating point numbers (e.g., an embedding vector).

*   **JSON**: Uses ~9 KB on the wire. Requires ~50$\mu$s to parse.
*   **Protobuf**: Uses ~4 KB on the wire. Requires ~5$\mu$s to parse.

For a system processing 10,000 requests per second, switching to Protobuf saves nearly half a core of CPU time just in serialization overhead. This 10$\times$ efficiency gain makes gRPC essential for high-throughput internal microservices.

:::

**System Choice**: Use REST for public APIs to maximize developer accessibility. Use gRPC for high-performance internal communication to minimize the serialization tax.

The architectural components and protocols examined so far describe *how* serving systems are built. Understanding *why* certain configurations perform better requires analyzing what happens to individual requests as they traverse these components.

## The Request Lifecycle {#sec-model-serving-systems-request-lifecycle-d9c6}

With the serving architecture established, we now trace what happens to a single request as it flows through the system. Understanding where time goes within each request is essential for effective optimization: you cannot improve what you do not measure.

### The Latency Budget {#sec-model-serving-systems-latency-budget-ef40}

For dynamic inference systems, the fundamental difference from training lies in optimization objectives. Training optimizes throughput: maximizing samples processed per hour over days of computation. A training job that processes 1000 samples per second is successful regardless of how long any individual sample takes. Serving inverts this priority, optimizing latency (introduced in @sec-ml-system-architecture) per request under strict time constraints. A serving system with 1000ms per-request latency has failed, even if it achieves excellent throughput.

This shift has concrete implications for system design [@gujarati2020serving]. The metrics that matter change from aggregate throughput to latency distributions. Mean latency tells you little about user experience; p50, p95, and p99 latencies reveal how the system performs across the full range of requests. If your mean latency is 50ms but p99 is 2 seconds, one in a hundred users waits 40 times longer than average. For consumer-facing applications, these tail latencies often determine user satisfaction and retention.[^fn-tail-latency]

[^fn-tail-latency]: **Tail Latency Impact**: Research at Google and Amazon in the mid-2000s established that users are more sensitive to latency variance than mean latency. Industry experience suggests that latency increases of 100ms can measurably impact user engagement and conversion rates for e-commerce applications, though the magnitude varies by context. This is why service level objectives (SLOs) typically specify percentile targets rather than averages.


Managing these percentile constraints requires decomposing the total allowed response time into a *latency budget* that allocates time across each processing phase.

::: {.callout-definition title="Latency Budget"}

**Latency Budget** refers to the maximum time allowed for a serving request to complete, decomposed into allocations for _preprocessing_, _inference_, and _postprocessing_ phases. Effective latency budgeting requires understanding where time is consumed and allocating resources accordingly.

:::

Every serving request decomposes into three phases that each consume part of the latency budget. Preprocessing transforms raw input such as image bytes or text strings into model-ready tensors. Inference executes the model computation. Postprocessing transforms model outputs into user-facing responses.

A common misconception is that faster hardware automatically means faster serving. In practice, preprocessing and postprocessing often dominate total latency. Studies of production systems show preprocessing consuming 60 to 70 percent of total request time when inference runs on optimized accelerators [@nvidia2024tritontutorial]. Optimizing only the inference phase yields diminishing returns when the surrounding pipeline remains bottlenecked on CPU operations.

### Latency Distribution Analysis {#sec-model-serving-systems-latency-distribution-analysis-b0f8}

Understanding where time goes requires instrumenting each phase independently. Consider what happens when our ResNet-50 classifier receives a JPEG image:

::: {.callout-notebook title="ResNet-50: Latency Budget Breakdown"}

A typical serving request for our ResNet-50 classifier shows the following latency distribution:

+--------------------+----------------------------+------------+----------------+
| **Phase**          | **Operation**              | **Time**   | **Percentage** |
+:===================+:===========================+===========:+===============:+
| **Preprocessing**  | JPEG decode                | 3.0ms      | 30%            |
| **Preprocessing**  | Resize to 224×224          | 1.0ms      | 10%            |
| **Preprocessing**  | Normalize (mean/std)       | 0.5ms      | 5%             |
| **Data Transfer**  | CPU→GPU copy               | 0.5ms      | 5%             |
| **Inference**      | **ResNet-50 forward pass** | **5.0ms**  | **50%**        |
| **Postprocessing** | Softmax + top-5            | 0.1ms      | ~0%            |
| **Total**          |                            | **10.1ms** | **100%**       |
+--------------------+----------------------------+------------+----------------+

Key insight: **Preprocessing consumes 45% of latency** despite model inference being the computationally intensive phase. With TensorRT optimization reducing inference to 2ms, preprocessing would dominate at 75%.

:::

::: {.content-hidden}

David Patterson famously argued that general-purpose CPUs are inefficient for ML because they dedicate significant silicon area to complex logic (branch prediction, out-of-order execution) that is unnecessary for the regular, data-parallel patterns of neural networks.

For serving, this efficiency gap is magnified. A CPU executing an inference request at batch size 1 might achieve only 1--2% of its theoretical peak performance because the "killer microseconds" of instruction fetch and decode dominate the few arithmetic operations performed per token or pixel.

**Domain-Specific Architectures (DSAs)**, like Google’s TPU or NVIDIA’s Tensor Cores, solve this by replacing complex instruction logic with massive arrays of simple Multiply-Accumulate (MAC) units. This specialization allows a DSA to achieve 10--100$\times$ higher **Arithmetic Intensity**—the ratio of compute to memory access—even at the small batch sizes required for low-latency serving. Understanding this architectural advantage explains why hardware acceleration is not just a speedup, but a requirement for economically viable serving.

:::

This breakdown reveals why straightforward optimization efforts often fail. Engineers focus on model optimization (quantization, pruning) because that is where ML expertise applies, but the actual bottleneck is image decoding running on CPU. Adopting *the quantitative approach to serving* exposes these hidden bottlenecks before engineering effort is misallocated.

::: {.callout-notebook title="The Quantitative Approach to Serving"}

**Amdahl's Law at Work**: Preprocessing (4.5ms) and data transfer (0.5ms) consume 50% of total latency. Optimizing the model 10× faster (5ms → 0.5ms) yields only 1.8× end-to-end speedup—from 10.1ms to 5.6ms. This is why focusing exclusively on model optimization (quantization, pruning) often disappoints: the bottleneck is elsewhere.

**DSA Efficiency**: General-purpose CPUs achieve only 1-2% of peak performance at batch-1 because instruction overhead dominates. DSAs like TPUs and Tensor Cores replace complex logic with dense MAC arrays, achieving 10-100× higher arithmetic intensity. This makes hardware acceleration a requirement for economically viable serving.

**Engineering Implication**: Profile before optimizing. If preprocessing dominates, GPU-accelerated pipelines (NVIDIA DALI) may outperform model quantization.
:::

Moving preprocessing to GPU can reduce total latency by 6x in some pipelines by eliminating CPU-GPU data transfers between stages [@nvidia2024tritontutorial]. Effective optimization targets the largest time consumers first.

#### The Killer Microseconds Problem {#sec-model-serving-systems-killer-microseconds-problem-bc00}

Barroso, Patterson, and colleagues identified a critical gap in how systems handle latency at different time scales [@barroso2017attack]. Modern systems efficiently handle nanosecond-scale events (CPU cache access, DRAM reads) through hardware mechanisms like out-of-order execution, and millisecond-scale events (disk I/O, network calls) through software techniques like threading and asynchronous I/O. But microsecond-scale events fall into an uncomfortable middle ground where neither approach works well.

ML serving lives squarely in this microsecond regime. Individual inference calls complete in 1 to 10ms, but the surrounding operations such as serialization, memory allocation, network stack processing, and encryption each add microseconds that compound into significant overhead. Google's analysis found that a significant fraction, often 20 percent or more, of datacenter CPU cycles are consumed by this "datacenter tax" rather than useful computation. For serving systems, this means a 2μs network fabric can become 100μs end-to-end through software overhead, context switching costs of 5 to 10μs can exceed the inference time for small models, and memory allocation patterns in preprocessing can add unpredictable microsecond delays. These overheads explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization through memory pooling, zero-copy data paths, and kernel bypass matters as much as model optimization.

The latency budget framework provides a systematic approach to optimization. First, measure each phase to identify the true bottleneck. Then allocate engineering effort proportionally to where time is actually spent. Finally, consider architectural changes such as GPU preprocessing or batching strategies that can shift work between phases.

### Resolution and Input Size Tradeoffs {#sec-model-serving-systems-resolution-input-size-tradeoffs-155d}

Input resolution affects both preprocessing and inference latency, but the relationship differs depending on whether the system is compute-bound (limited by arithmetic throughput) or memory-bound (limited by data movement). A compute-bound system slows proportionally to increased computation; a memory-bound system may show minimal slowdown if activation tensors still fit in fast memory. @sec-ai-acceleration covers this distinction in depth through roofline model analysis; understanding it is essential for making informed resolution decisions.

For compute-bound models, @eq-resolution-throughput formalizes how throughput scales inversely with resolution squared:

$$\frac{T(r_2)}{T(r_1)} = \left(\frac{r_1}{r_2}\right)^2$$ {#eq-resolution-throughput}

Doubling resolution from 224 to 448 theoretically yields 4x slowdown (measured: 3.6x due to fixed overhead amortization). However, at high resolutions, models transition from compute-bound to memory-bound as activation tensors exceed cache capacity. @tbl-resolution-bottleneck quantifies this transition for ResNet-50, showing how arithmetic intensity decreases with resolution:

+----------------+---------------------+----------------------+----------------+
| **Resolution** | **Activation Size** | **Arith. Intensity** | **Bottleneck** |
+:===============+====================:+=====================:+:===============+
| **224×224**    | 12.5MB              | 85 FLOPS/byte        | Compute        |
| **384×384**    | 36.8MB              | 49 FLOPS/byte        | Transitional   |
| **512×512**    | 65.5MB              | 28 FLOPS/byte        | Memory BW      |
| **640×640**    | 102.4MB             | 18 FLOPS/byte        | Memory BW      |
+----------------+---------------------+----------------------+----------------+

: **Resolution and Compute Bottleneck**: ResNet-50 arithmetic intensity decreases with resolution as activation sizes grow. For a V100 PCIe (14 TFLOPS FP32, 900 GB/s bandwidth), the ridge point is approximately 16 FLOPS/byte. At 224x224, compute dominates; by 512x512, memory bandwidth becomes the limiting factor. {#tbl-resolution-bottleneck}

#### Resolution Strategies in Production {#sec-model-serving-systems-deploymentspecific-resolution-decisions-1d76}

Different deployment contexts have distinct resolution requirements. Mobile applications often accept lower resolution such as 224×224 for object detection in camera viewfinders, where latency and battery life dominate. Medical imaging requires high resolution of 512×512 or higher for diagnostic accuracy, with relaxed latency requirements. Autonomous vehicles use multiple resolutions for different tasks, with low resolution for detection and high resolution crops for recognition. Cloud APIs typically receive resolution set by client upload and must handle a range gracefully. This variability makes cloud APIs ideal candidates for adaptive resolution strategies, where the system selects resolution dynamically based on content characteristics.

**Adaptive Resolution.** Production systems can select resolution dynamically based on content. One approach runs a lightweight classifier at 128×128 to categorize content type, then selects task-appropriate resolution with documents at 512×512, landscapes at 224×224, and faces at 384×384. This achieves 1.4× throughput improvement with 99.2 percent accuracy retention versus fixed high resolution. This pattern trades preprocessing cost from running the lightweight classifier for inference savings on the main model.

The latency analysis so far has focused on sequential processing: one request completing before the next begins. But the preprocessing, inference, and postprocessing stages use different hardware resources. This separation creates an opportunity to process multiple requests simultaneously.

### Hardware Utilization and Request Pipelining {#sec-model-serving-systems-utilization-request-pipelining-c61c}

The preceding analysis examined where time goes within individual pipeline stages. But optimizing each stage in isolation misses a critical opportunity: the stages use different hardware resources. The latency budget analysis in @sec-model-serving-systems-latency-budget-ef40 reveals that model inference is only one component of the request lifecycle. From a hardware perspective, the primary goal of a serving system is to maximize the **duty cycle** of the accelerator—the percentage of time the GPU is performing useful computation.

In a serialized serving system, the hardware sits idle during network I/O and CPU-based preprocessing. High-performance serving systems use **Request Pipelining** to overlap these stages, ensuring the GPU is fed a continuous stream of tensors.

**Overlapping I/O and Compute.** @fig-serving-pipeline-timing contrasts serial execution with pipelined execution. In the serial case (A), each request must complete its entire lifecycle (Network $\rightarrow$ CPU Preprocessing $\rightarrow$ GPU Inference $\rightarrow$ Postprocessing) before the next request begins. Even with a fast GPU, the system throughput is limited by the slowest stage, and the GPU remains idle for more than 50% of the time.

::: {#fig-serving-pipeline-timing fig-env="figure" fig-pos="htb" fig-cap="**Request Pipelining**: Pipelining hides latency by overlapping independent operations across different hardware resources. In pipelined execution (B), the CPU processes the next request's data while the GPU executes the current request's inference. This increases the GPU duty cycle toward 100%, effectively doubling or tripling throughput on the same hardware without changing the model." fig-alt="Two timing diagrams. A (Serial): alternating CPU preprocessing, GPU inference, and idle blocks in sequence. B (Pipelined): two parallel rows where CPU preprocessing overlaps with GPU inference, eliminating idle time."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  \definecolor{CPUColor}{RGB}{173,216,230}
  \definecolor{GPUColor}{RGB}{144,238,144}
  \definecolor{WaitColor}{RGB}{240,240,240}

  % Serial Execution
  \node[anchor=west] at (0, 3.5) {\textbf{A. Serial Execution} (Low Utilization)};
  \draw[fill=CPUColor] (0, 2.5) rectangle (1.5, 3) node[midway] {Pre};
  \draw[fill=GPUColor] (1.5, 2.5) rectangle (3.0, 3) node[midway] {GPU};
  \draw[fill=WaitColor] (3.0, 2.5) rectangle (4.5, 3) node[midway, text=gray] {Idle};
  \draw[fill=CPUColor] (4.5, 2.5) rectangle (6.0, 3) node[midway] {Pre};
  \draw[fill=GPUColor] (6.0, 2.5) rectangle (7.5, 3) node[midway] {GPU};

  % Overlapped Execution
  \node[anchor=west] at (0, 1.5) {\textbf{B. Pipelined Execution} (High Utilization)};
  % CPU Row
  \draw[fill=CPUColor] (0, 0.5) rectangle (1.5, 1) node[midway] {Pre 1};
  \draw[fill=CPUColor] (1.5, 0.5) rectangle (3.0, 1) node[midway] {Pre 2};
  \draw[fill=CPUColor] (3.0, 0.5) rectangle (4.5, 1) node[midway] {Pre 3};
  \draw[fill=CPUColor] (4.5, 0.5) rectangle (6.0, 1) node[midway] {Pre 4};

  % GPU Row
  \draw[fill=GPUColor] (1.5, 0) rectangle (3.0, 0.5) node[midway] {GPU 1};
  \draw[fill=GPUColor] (3.0, 0) rectangle (4.5, 0.5) node[midway] {GPU 2};
  \draw[fill=GPUColor] (4.5, 0) rectangle (6.0, 0.5) node[midway] {GPU 3};
  \draw[fill=GPUColor] (6.0, 0) rectangle (7.5, 0.5) node[midway] {GPU 4};

\end{tikzpicture}
```
:::

Pipelining is enabled by **Asynchronous I/O** and **Concurrency Models**. Instead of waiting for a GPU kernel to finish, the server's CPU thread submits the work to the GPU's command queue and immediately begins preprocessing the next incoming request.

**The Systems Metric: Hardware Duty Cycle.** In the "Quantitative Approach" to ML systems, we define the efficiency of a serving system by its ability to saturate the bottleneck resource. For most ML systems, this is the GPU's compute cores or memory bandwidth.

$$\text{System Efficiency} = \frac{\sum T_{\text{compute}}}{\text{Wall Clock Time} \times \text{Resource Count}}$$

If a ResNet-50 request takes 10ms total (5ms GPU, 5ms CPU), a serial system achieves only 50% efficiency. By pipelining just two requests, efficiency approaches 100% (assuming the CPU can keep up with the GPU). If the CPU is too slow to feed the GPU, the system becomes **CPU-bound**, and further model optimization provides zero throughput gain, a direct application of Amdahl's Law (introduced in @sec-ml-system-architecture) to serving: if preprocessing consumes 50% of latency, maximum speedup is 2x regardless of how fast the model runs.

### Postprocessing {#sec-model-serving-systems-postprocessing-3b24}

Preprocessing and inference produce raw tensors, but these floating-point arrays carry no inherent meaning to applications or users. The final phase of the request lifecycle, postprocessing, transforms these tensors into actionable predictions: a 0.95 probability becomes a confident "dog" label, a sequence of token IDs becomes readable text, or a bounding box tensor becomes a highlighted region in an image. While often overlooked in system design, postprocessing significantly impacts both latency and the usefulness of predictions.

#### From Logits to Predictions {#sec-model-serving-systems-logits-predictions-09df}

Classification models output logits or probabilities across classes. Converting these to predictions involves several potential steps including argmax selection that chooses the highest-probability class, thresholding that applies confidence thresholds before returning predictions, top-k extraction that returns multiple high-probability classes with scores, and calibration that adjusts raw probabilities to better reflect true likelihoods.

For ResNet-50 image classification, typical postprocessing includes transforming logits to probabilities, extracting top predictions, and formatting responses. @lst-resnet-postprocessing shows a complete postprocessing pipeline with timing annotations. Total postprocessing time is approximately 0.1ms—negligible compared to preprocessing and inference.

::: {#lst-resnet-postprocessing lst-cap="**ResNet-50 Postprocessing**: Transforms raw logits to calibrated probabilities, extracts top-k predictions, and formats the API response."}
```{.python}
# Transform raw logits to calibrated probabilities
# Input: logits tensor of shape (batch_size, 1000) - one score per ImageNet class
probs = torch.softmax(logits, dim=-1)  # Normalize to sum=1; ~0.05ms on GPU

# Extract top-5 predictions for multi-class response
# topk returns (values, indices) sorted by probability
top5_probs, top5_indices = probs.topk(5)  # ~0.02ms; GPU operation

# Map class indices to human-readable labels
# IMAGENET_CLASSES: list of 1000 class names from synset mapping
labels = [IMAGENET_CLASSES[i] for i in top5_indices]  # ~0.01ms; CPU lookup

# Format response with predictions and metadata for API contract
response = {
    "predictions": [
        {"label": label, "confidence": float(prob)}
        for label, prob in zip(labels, top5_probs)
    ],
    "model_version": "resnet50-v2.1",  # Client-side version tracking
    "inference_time_ms": 5.2,          # Observability for latency monitoring
}
```
:::

Each step adds latency but improves response utility. Calibration in particular can add significant computation but is essential when downstream systems make decisions based on confidence scores.

**Output Formatting.** Production systems rarely return raw predictions. Outputs must conform to API contracts, often requiring JSON serialization with specific schema, confidence score formatting and thresholding, error handling for edge cases such as no confident prediction or out-of-distribution input, and metadata attachment including model version, inference time, and feature attributions.

The latency budget analysis reveals where time goes within a single request. But production systems do not process requests in isolation: they must handle hundreds or thousands of concurrent requests competing for finite resources. Understanding this concurrency requires a different analytical framework.

## Queuing Theory and Tail Latency {#sec-model-serving-systems-queuing-theory-tail-latency-29a6}

The request lifecycle analysis explains where time goes within a single request, but production systems must handle many concurrent requests competing for finite resources. Understanding why latency degrades under load requires queuing theory, the mathematical framework that explains how requests wait for service in any system with finite capacity. These principles apply to web servers and ML inference alike, and explain the counterintuitive behavior that causes well-provisioned systems to violate latency SLOs when load increases modestly.

### Queuing Fundamentals {#sec-model-serving-systems-queuing-fundamentals-10d3}

Serving engineers routinely face a concrete question: given a latency SLO and an expected request rate, how many GPUs must be provisioned? Answering this question requires predicting how latency changes as load increases, which is precisely what queuing theory provides. Two mathematical foundations govern serving system behavior: Little's Law, which relates queue depth to throughput, and the M/M/1 model, which predicts how latency degrades under load. Together, they provide the quantitative framework for capacity planning.

### Little's Law {#sec-model-serving-systems-littles-law-9352}

The most fundamental result in queuing theory is Little's Law,[^fn-littles-law] which @eq-littles-law expresses as a simple relationship between three quantities in any stable system:

[^fn-littles-law]: **Little's Law**: Proven by John D.C. Little in 1961 [@little1961proof], this theorem establishes that $L = \lambda W$ holds for any stable queuing system regardless of arrival patterns, service distributions, or scheduling policies. The remarkable generality makes it one of the most useful results in operations research. For serving systems, it enables capacity planning from observable metrics: measuring queue depth and arrival rate directly yields average latency without instrumenting individual requests.

$$L = \lambda \cdot W$$ {#eq-littles-law}

where $L$ is the average number of requests in the system, $\lambda$ is the arrival rate (requests per second), and $W$ is the average time each request spends in the system. This relationship holds regardless of arrival distribution, service time distribution, or scheduling policy.

::: {.callout-notebook title="Napkin Math: Little's Law"}

**The Capacity Physics**: How much memory do you need to serve 1,000 queries per second?

**The Law**: $L = \lambda W$ (Concurrency = Throughput $\times$ Latency).

**Scenario**:

*   **Throughput Target ($\lambda$)**: 1,000 requests/sec.
*   **Latency Target ($W$)**: 50 ms (0.05 s).

**The Calculation**:
$$ L = 1,000 \times 0.05 = \mathbf{50 \text{ concurrent requests}} $$

**The Constraint**: Your server *must* have enough RAM to hold 50 requests simultaneously (batch size + queue).
*   If your GPU runs out of memory at Batch Size 32, you physically **cannot** hit 1,000 QPS at 50ms latency.
*   You must either reduce latency ($W$) or buy more memory ($L$).
:::

Little's Law has immediate practical implications. If your inference service averages 10ms per request ($W = 0.01$s) and you observe 50 concurrent requests in the system on average ($L = 50$), then your arrival rate must be $\lambda = L/W = 5000$ requests per second. Conversely, if you need to limit concurrent requests to 10 (perhaps due to GPU memory constraints), and your service time is 10ms, you can sustain at most 1000 requests per second.

### The Utilization-Latency Relationship {#sec-model-serving-systems-utilizationlatency-relationship-a2f0}

Little's Law tells us what the system looks like on average, but it does not reveal how latency changes as load approaches capacity. To answer the critical question of how much spare capacity a serving system needs, we turn to the M/M/1 queue model. For a system with Poisson arrivals and exponential service times, the average time in system follows:

$$W = \frac{1}{\mu - \lambda} = \frac{\text{service time}}{1 - \rho}$$ {#eq-mm1-wait}

where $\mu$ is the service rate (requests per second the server can handle), and $\rho = \lambda/\mu$ is the utilization (fraction of time the server is busy).

This equation reveals why serving systems exhibit nonlinear behavior. At 50% utilization, average wait time is $2\times$ the service time. At 80% utilization, it is $5\times$. At 90% utilization, it is $10\times$. Small increases in load near capacity cause disproportionate latency increases.

+--------------------------+------------------------+---------------------------+
| **Utilization ($\rho$)** | **Wait Time Multiple** | **Example (5ms service)** |
+:=========================+=======================:+==========================:+
| 50%                      | 2.0×                   | 10ms                      |
| 70%                      | 3.3×                   | 17ms                      |
| 80%                      | 5.0×                   | 25ms                      |
| 90%                      | 10.0×                  | 50ms                      |
| 95%                      | 20.0×                  | 100ms                     |
+--------------------------+------------------------+---------------------------+

: **Utilization-Latency Relationship**: Average wait time as a multiple of service time for an M/M/1 queue. At 50% utilization, wait time is 2x service time; at 90%, it reaches 10x. This nonlinear growth explains why systems that perform well at moderate load suddenly violate SLOs when traffic increases: moving from 80% to 90% utilization doubles wait time. {#tbl-utilization-latency}

The M/M/1 model assumes exponentially distributed service times, but ML inference typically has near-constant service time for fixed batch sizes, making the M/D/1 (deterministic service) model more accurate in practice. We use M/M/1 here because it yields closed-form solutions and produces conservative estimates. @tbl-utilization-latency reveals how average wait time grows rapidly as utilization approaches 100%. For M/D/1 queues, average wait time is approximately half of M/M/1 at the same utilization, which matters for capacity planning: M/M/1 analysis will slightly over-provision, erring on the side of meeting SLOs rather than violating them.[^fn-queuing-models]

[^fn-queuing-models]: **Kendall Notation**: The M/M/1 notation was introduced by British statistician David Kendall in 1953 and follows the pattern A/S/c (Arrivals/Service/servers). "M" stands for "Markovian" (memoryless, meaning exponential distributions), honoring Russian mathematician Andrey Markov (1856-1922). "D" means deterministic. So M/M/1 describes a single server with exponential arrivals and service times, while M/D/1 has deterministic service. ML inference is closer to M/D/1 since inference time is nearly constant, but M/M/1 yields conservative estimates suitable for capacity planning.


### Multi-Server Considerations {#sec-model-serving-systems-multiserver-considerations-00fc}

The preceding analysis focuses on a single ML node—one machine serving inference requests. This scope aligns with Volume I's focus on mastering the fundamental unit of ML systems. Understanding single-node queuing dynamics is prerequisite to effective scaling: you cannot optimize a distributed system without first understanding the behavior of its components.

**When Single-Node Analysis Applies**: M/M/1 analysis remains the foundation for:

- **Right-sizing individual nodes**: Determining whether a single GPU can meet latency SLOs at expected traffic
- **Identifying the scaling trigger**: Calculating when traffic exceeds single-node capacity
- **Cost-effective provisioning**: Avoiding premature scale-out that wastes resources

For traffic exceeding single-node capacity, production systems deploy multiple replicas behind a load balancer. The M/M/c queuing model extends M/M/1 to c parallel servers, showing that multiple replicas dramatically improve tail latency: the probability of all servers being simultaneously slow drops exponentially with server count. At c=4 replicas, p99 latency can be 3× lower than the single-server case at the same total throughput.

**Scope Boundary**: This chapter establishes single-node serving foundations. Distributed inference systems—model sharding across GPUs, tensor parallelism, pipeline parallelism—introduce coordination overhead and consistency challenges that require advanced scaling principles.

### Tail Latency {#sec-model-serving-systems-tail-latency-5376}

Production SLOs typically specify percentile targets (p95, p99) rather than averages because tail latency determines user experience for the slowest requests [@dean2013tail]. For an M/M/1 queue, the p99 latency follows:


$$W_{p99} \approx \text{service time} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right)$$ {#eq-p99-latency}

At 70 percent utilization, p99 latency is approximately fifteen times the service time, while average latency is only 3.3 times. This explains why systems that seem healthy with low average latency can have unacceptable tail latency, since the average hides the experience of the unluckiest requests.

#### The Tail at Scale Problem {#sec-model-serving-systems-tail-scale-problem-958d}

Dean and Barroso's analysis reveals why tail latency becomes critical as systems scale beyond single machines [@dean2013tail]. The key insight is that when requests fan out to multiple servers, the probability of experiencing at least one slow response grows rapidly with server count. This "tail at scale" effect makes individual server tail latency critical for overall system performance.

For single-machine serving, this principle has two implications. First, tail latency on individual machines matters because it will compound when systems eventually scale. Second, the tail-tolerant techniques described below (hedging, graceful degradation) provide value even on single machines and become essential at scale.

**Tail-Tolerant Techniques**: Request hedging sends redundant requests after a timeout, accepting whichever response arrives first. Backup requests and load balancing away from slow servers directly address latency variance. These techniques apply to single-machine serving with multiple GPU streams or model replicas, and become essential when scaling to distributed inference systems.

With the queuing model and tail latency analysis established, we can now apply these tools to a concrete capacity planning exercise.

::: {.callout-notebook title="ResNet-50 Capacity Planning"}

Consider designing a ResNet-50 serving system with these requirements:

- **Target p99 latency**: 50ms
- **Peak expected traffic**: 5,000 requests per second
- **Service time** (TensorRT FP16): 5ms

#### Step 1: Find Safe Utilization {.unnumbered}

Applying @eq-p99-latency to constrain $W_{p99} \leq 50$ms with 5ms service time and solving for $\rho$:

$$5\text{ms} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right) \leq 50\text{ms}$$

This yields $\rho \leq 0.72$ (72% maximum utilization).

#### Step 2: Calculate Required Service Rate {.unnumbered}

$$\mu_{\text{required}} = \frac{\lambda}{\rho_{\text{safe}}} = \frac{5000}{0.72} = 6944 \text{ requests/second}$$

#### Step 3: Determine GPU Count {.unnumbered}

Single V100 throughput at batch=16: 1,143 images/second

$$\text{GPUs needed} = \frac{6944}{1143} = 6.1 \rightarrow 7 \text{ GPUs}$$

#### Step 4: Add Headroom for Variance {.unnumbered}

Production systems add 30% headroom for traffic spikes and variance:

$$\text{Final count} = 7 \times 1.3 = 9.1 \rightarrow 10 \text{ GPUs}$$

#### Step 5: Verify Fault Tolerance {.unnumbered}

The 30% headroom addresses traffic variance, but production systems also need fault tolerance. With 10 GPUs, losing one leaves 9 GPUs handling 5,000 QPS:

$$\text{Utilization after failure} = \frac{5000 / 1143}{9} = 48.6\%$$

This remains well below the 72% safe utilization threshold, confirming N+1 redundancy is satisfied. For stricter fault tolerance requirements, N+2 redundancy (tolerating two simultaneous failures) would require 11-12 GPUs.

**Result**: Provision 10 V100 GPUs to serve 5,000 QPS at 50ms p99 latency with N+1 fault tolerance.

:::

The queuing analysis explains the capacity planning approach detailed in @sec-model-serving-systems-capacity-planning-96a3 and connects directly to the MLPerf Server scenario. @sec-benchmarking-ai explains how MLPerf measures throughput only for requests meeting the latency SLO: a system achieving 10,000 QPS but violating the SLO on 5% of requests reports only 9,500 valid QPS.

### Tail-Tolerant Techniques {#sec-model-serving-systems-tailtolerant-techniques-066e}

Rather than eliminating all sources of latency variability, which is often impractical, production systems employ techniques that tolerate variability while still meeting SLOs [@dean2013tail; @dean2012rapid]. These techniques treat latency variance as a given and design around it.

**Hedged Requests.** When a request has not completed within the expected time, send a duplicate request to another server.[^fn-hedging-etymology] The client uses whichever response arrives first and cancels the other. For ML serving, this means maintaining multiple model replicas and routing slow requests to alternative replicas. The overhead is modest: if you hedge at the 95th percentile, only 5% of requests generate duplicates, increasing load by just 5% while dramatically reducing tail latency.

[^fn-hedging-etymology]: **Hedging**: Borrowed from finance, where "hedging" means reducing risk by making offsetting bets. The term derives from the literal hedge (a boundary of shrubs) that protects a garden. Financial hedging dates to the 1600s Dutch tulip markets. Google's Jeff Dean introduced "hedged requests" in his influential 2013 "Tail at Scale" paper, applying the financial concept to distributed systems: send redundant requests to protect against the risk of slow responses.


**Cancellation Complexity**: A critical implementation detail is that CUDA kernels cannot be interrupted mid-execution. When a hedged request completes, the duplicate must be cancelled, but if inference has already begun on the GPU, cancellation approaches include checking a cancellation flag before launching inference, accepting wasted compute for the in-flight kernel, or using request prioritization to deprioritize the duplicate. Since hedging typically applies only to the slowest 5 percent of requests, the overhead from occasional wasted compute remains acceptable.

**Tied Requests.** Send the request to multiple servers simultaneously, but include a tag allowing servers to cancel execution once another server begins processing. This eliminates the delay of waiting to detect a slow response before hedging. For inference servers with significant startup overhead from model loading and memory allocation, tied requests ensure at least one server begins immediately.

**Canary Requests.** For requests that fan out to many backends, first send the request to a small subset of 1 to 2 servers.[^fn-canary-etymology] If these return within expected time, send to the remainder. If the canary is slow, the system can take corrective action by retrying elsewhere or using cached results before committing to the full fan-out. This prevents a single slow backend from stalling an entire distributed inference request.

[^fn-canary-etymology]: **Canary**: From the practice of using canary birds in coal mines from the early 1900s through the 1980s. Miners brought caged canaries underground because the birds' high metabolic rate made them sensitive to carbon monoxide and methane, dying before gas concentrations became lethal to humans. In software, "canary" describes any small-scale test that detects problems before they affect the full system, whether canary deployments, canary requests, or canary tests.


**Graceful Degradation.** When load exceeds capacity, return approximate results rather than timing out. For classification, return cached predictions for similar inputs. For generative models, return shorter outputs. For ensemble systems, return predictions from a subset of models. This maintains responsiveness at the cost of some accuracy, which users often prefer to outright failures.

**Admission Control.** When traffic exceeds capacity, accepting all requests can trigger widespread SLO violations. Admission control proactively rejects requests when queue depth exceeds a threshold, returning immediate 503 responses rather than accepting requests that are likely to timeout. This sacrifices throughput to protect latency for admitted requests.

**Setting the Threshold**: A practical starting point is 2 to 3 times service time multiplied by the number of workers. For a system with 4 workers and 10ms service time, this yields a queue depth threshold of 80 to 120 requests. Adaptive admission control adjusts thresholds based on observed p99 latency, tightening when latency increases above target and relaxing when latency remains healthy.

**Retry Storm Prevention**: A subtle failure mode occurs when all replicas are overloaded simultaneously. If the load balancer retries rejected requests at other replicas that are also overloaded, retry traffic amplifies the overload. Coordinated load shedding addresses this by sharing load information across replicas, enabling system-wide decisions about which requests to accept. When global load exceeds capacity, replicas collectively reject the same fraction of requests rather than each rejecting independently and triggering retries.

These techniques become essential at scale when fan-out amplification makes individual server tail latency visible to users. Single-machine serving systems can implement hedged and tied requests across GPU streams or model replicas. The queuing analysis here assumes FIFO processing, but production systems often implement priority scheduling such as deadline-aware or shortest-job-first approaches to further reduce tail latency for heterogeneous workloads [@harchol2013performance].

The tail-tolerant techniques examined in this section optimize the flow of requests through a functioning serving system. But the queuing analysis assumes a critical precondition: that models are loaded, initialized, and producing correct predictions. In production, this assumption fails regularly: during deployments, new instances must load models from scratch; during scaling events, cold start latency affects the first requests to new replicas; and when preprocessing pipelines diverge from training, accuracy silently degrades. The next section examines these lifecycle challenges that must be solved before queuing optimization becomes relevant.

## Model Lifecycle Management {#sec-model-serving-systems-model-lifecycle-management-ff2e}

The queuing analysis from previous sections assumes two prerequisites: models are loaded and ready to process requests, and predictions match what was validated during development. Production systems often violate both assumptions. Cold start latency can exceed inference time by orders of magnitude during scaling events. Subtle preprocessing differences between training and serving pipelines cause accuracy degradation that no amount of queuing optimization can address. This section examines the challenges that threaten these foundational assumptions.

### Training-Serving Skew {#sec-model-serving-systems-trainingserving-skew-7b99}

A model that performed well during validation may silently degrade when deployed. This phenomenon, known as *training-serving skew*, represents one of the most subtle failure modes in production ML because it is invisible to latency monitoring and exception tracking.

::: {.callout-definition title="Training-Serving Skew"}

**Training-Serving Skew** occurs when a model's performance in production degrades relative to its _training validation metrics_, typically caused by _discrepancies_ between the training and serving data pipelines or environments.

:::

@sec-machine-learning-operations-mlops provides comprehensive coverage of skew diagnosis, monitoring, and organizational prevention strategies. Here we focus on the *serving-specific* manifestation: **preprocessing divergence**. This occurs when the real-time inference pipeline processes raw data differently than the batch training pipeline, a common failure mode when training uses Python/Pandas while serving uses C++/Java or optimized inference servers. Unlike data drift (which @sec-machine-learning-operations-mlops addresses through monitoring), preprocessing divergence is deterministic and preventable through careful engineering.

::: {.callout-example title="ResNet-50: Image Preprocessing Skew"}

For ResNet-50 serving, common sources of skew include:

**Resize interpolation**: Training uses PIL.BILINEAR while OpenCV defaults to cv2.INTER_LINEAR. These produce pixel-level differences that can shift accuracy by 0.5-1%.

**Color space handling**: JPEG loading in different libraries may produce BGR vs RGB ordering. If the model trained on RGB but serves BGR inputs, predictions are essentially random.

**Normalization constants**: ImageNet normalization uses specific mean/std values. Using `mean=[0.5, 0.5, 0.5]` instead of `mean=[0.485, 0.456, 0.406]` shifts inputs out of the training distribution.

**Prevention**: The safest approach is to export the exact preprocessing code used during training and run it identically in serving, or use a framework like NVIDIA DALI that can help standardize preprocessing across training and serving environments.

:::

### Cold Start and Initialization Dynamics {#sec-model-serving-systems-model-loading-initialization-cc5a}

With preprocessing pipelines designed to avoid training-serving skew, the next challenge is getting models ready to serve. Before processing any request, models must load from storage into memory and prepare for inference [@romero2021infaas]. This initialization latency, known as *cold start*, affects system responsiveness during deployments, scaling events, and recovery from failures.

::: {.callout-definition title="Cold Start"}

**Cold Start** refers to the latency penalty incurred by the first request(s) processed by a new model instance, caused by _initialization overheads_ such as _weight loading_, _runtime compilation_, and _memory allocation_.

:::

Understanding cold start dynamics enables designing systems that meet latency requirements from the moment they begin serving traffic. A *cold start timeline* for a representative model reveals where each phase contributes to total initialization latency.

**Cold Start Anatomy.** Cold start latency compounds from multiple sources, each adding to the time between deployment and serving readiness. Weight loading reads model parameters from disk or network storage. Graph compilation performs just-in-time compilation of operations for the specific hardware. Memory allocation reserves GPU memory for activations and intermediate values. Warmup[^fn-warmup-etymology] execution performs initial inferences that populate caches and trigger lazy initialization.

[^fn-warmup-etymology]: **Warmup**: The computing metaphor derives from physical warming, where engines and machines perform better after reaching operating temperature. In JIT-compiled systems like the JVM (1990s), "warmup" specifically refers to the period when the runtime gathers profiling data and compiles hot paths. For ML serving, warmup serves a dual purpose: triggering lazy memory allocation and populating CPU/GPU caches with frequently-accessed data, ensuring the first real user request does not pay these one-time costs.


::: {.callout-notebook title="ResNet-50: Cold Start Timeline"}

Loading ResNet-50 for production serving involves the following cold start phases:

+---------------------------------+--------------+---------------------------------------------------+
| **Phase**                       | **Duration** | **Notes**                                         |
+:================================+=============:+:==================================================+
| **Weight loading (SSD)**        | 0.5s         | 98MB FP32 weights from local storage              |
| **Weight loading (S3)**         | 3-5s         | Network latency dominates for cloud storage       |
| **CUDA context**                | 0.3-0.5s     | GPU driver initialization and memory setup        |
| **TensorRT compilation**        | 15-30s       | Converts PyTorch model to optimized engine        |
| **Warmup (10 inferences)**      | 0.2s         | Triggers remaining lazy initialization            |
| **Total (local, optimized)**    | **~1.5s**    | With pre-compiled TensorRT engine, warm container |
| **Total (cloud, first deploy)** | **~35s**     | Including compilation from cold state             |
+---------------------------------+--------------+---------------------------------------------------+

**Key insight**: Pre-compiling models and storing the optimized engine eliminates the 30-second compilation phase on subsequent deployments.

**CUDA Context**: Before any GPU operation, the CUDA runtime must establish a *context*: a data structure that tracks memory allocations, loaded kernels, and device state. Creating a context requires communicating with the GPU driver and allocating GPU memory for internal bookkeeping. This one-time cost (0.3-0.5s) affects every new process that uses the GPU. CUDA 11+ introduced lazy initialization that defers some setup until first use, reducing apparent startup time but shifting cost to the first inference.

**CUDA MPS (Multi-Process Service)**: Normally, each process creates its own CUDA context, and the GPU time-slices between contexts. MPS allows multiple processes to share a single context, eliminating redundant initialization and enabling concurrent kernel execution. For serving systems running multiple model replicas, MPS can reduce aggregate cold start time and improve GPU utilization. The trade-off is reduced isolation: a crash in one process can affect others sharing the MPS server.

:::

Without warmup, the first real request triggers compilation and memory allocation mid-inference, often causing timeout failures. A request that normally takes 5ms might require 500ms during cold start, violating SLOs and degrading user experience.

### Loading Strategies {#sec-model-serving-systems-loading-strategies-eb38}

Different loading strategies trade off cold start duration against serving performance and memory efficiency.

**Full loading** reads the entire model into memory before serving begins. This maximizes inference speed since all weights are immediately available, but extends cold start duration and limits model size to available memory. Full loading is appropriate when cold start latency is acceptable and models comfortably fit in memory.

**Memory mapping** maps model files directly into the address space, loading pages on demand as accessed. This reduces cold start time since inference can begin before the full model loads, but causes unpredictable latency as pages fault in during initial requests. Memory mapping works well for infrequently accessed model components but can cause latency spikes if critical weights are not preloaded.

**Lazy initialization** defers compilation and allocation until first use. This minimizes startup time but shifts latency to the first request. Production systems often combine lazy initialization with synthetic warmup requests to trigger initialization before real traffic arrives.

### Model Caching Infrastructure {#sec-model-serving-systems-model-caching-infrastructure-4f1a}

Production systems cache model weights at the infrastructure level to reduce cold start for common deployment scenarios:

**Container Image Embedding**: Bundle model weights directly in the container image. This produces a single deployment artifact and eliminates network fetches at startup, but creates large images (often 10-50GB) that slow container pulls and consume registry storage. Best for models that rarely update.

**Shared Filesystem**: Mount a network filesystem (EFS, GCS FUSE) containing model weights. Multiple replicas share cached weights, and updates propagate immediately without redeployment. Network latency affects cold start, and filesystem availability becomes a critical dependency. Best for organizations with many models and frequent updates.

**Node-Local SSD Cache**: Pre-populate local SSDs on inference nodes with frequently-used models. Provides fast loading (500MB/s+ for NVMe) without network dependency, but requires cache management to handle model updates and capacity limits. Best for high-traffic models where cold start latency is critical.

The choice depends on model update frequency: infrequent updates favor container embedding, frequent updates favor shared filesystem, and performance-critical deployments benefit from local caching with background refresh.

### Multi-Model Serving {#sec-model-serving-systems-multimodel-serving-a9c1}

Production systems often serve multiple models from a single machine, whether different model versions for A/B testing, ensemble components, or entirely different models sharing infrastructure. GPU memory becomes the limiting resource, requiring careful management strategies.

Strategies for multi-model serving include time-multiplexing that loads one model at a time and swaps based on request routing, memory sharing where models share GPU memory to limit concurrent execution but enable more models, and model virtualization where frameworks like Triton manage model lifecycle by loading and unloading based on traffic patterns [@nvidia2024triton]. The choice depends on request patterns. If models receive traffic evenly, concurrent loading works. If traffic is bursty and model-specific, time-multiplexing with intelligent preloading reduces average latency while maximizing GPU utilization.

#### Multi-Stream Execution {#sec-model-serving-systems-multistream-execution-1b1f}

When multiple models or multiple instances of the same model must run concurrently on a single GPU, the hardware must partition resources between them. NVIDIA's Multi-Instance GPU technology enables hardware-level isolation, dividing an A100 into up to 7 independent GPU instances, each with dedicated memory and compute resources. MIG is available on A100, A30 (up to 4 instances), H100, H200, and newer data center GPUs. For older GPUs such as V100 or T4, CUDA stream scheduling provides time-multiplexed sharing without hardware isolation.

The choice depends on whether consistent latency with MIG or maximum utilization with shared streams is the priority.

#### Model Swapping and Host Memory {#sec-model-serving-systems-model-swapping-host-memory-c54f}

When the aggregate size of all models exceeds GPU memory capacity, the serving system must swap models between host memory (DRAM) and device memory (VRAM) on demand. This introduces a new latency component determined by the PCIe bus bandwidth.

For a 10 GB model on PCIe Gen4 x16 (32 GB/s theoretical bandwidth), loading takes at least:
$$ T_{\text{load}} = \frac{10 \text{ GB}}{32 \text{ GB/s}} \approx 312 \text{ ms} $$

To mitigate this, systems use **Pinned Memory** (page-locked host memory). By default, the operating system can move ("page") any memory region to disk when RAM is under pressure. This creates a problem for GPU transfers: if the GPU's DMA (Direct Memory Access) engine begins reading a memory region that gets paged out mid-transfer, the transfer fails or stalls. To avoid this, the CPU must first copy data to a temporary pinned buffer before the GPU can safely read it, adding both latency and CPU overhead.

Pinning memory instructs the OS to keep that region permanently in physical RAM. The GPU's DMA engine can then transfer data directly from the pinned region at full PCIe bandwidth without CPU involvement. The trade-off is that pinned memory reduces the RAM available for other processes and cannot be reclaimed under memory pressure. For model serving, the performance gain (2-3× faster transfers) typically justifies pinning model weights and frequently-used input buffers, while leaving less critical memory pageable.

The lifecycle management strategies examined so far ensure models are ready to serve: loaded into memory, warmed up, and producing predictions consistent with training. With these prerequisites satisfied, the queuing dynamics from @sec-model-serving-systems-queuing-theory-tail-latency-29a6 become relevant. The next optimization opportunity lies in how requests are grouped for processing, which directly affects both the throughput and latency terms in our queuing equations.

## Throughput Optimization {#sec-model-serving-systems-throughput-optimization-18d1}

With models loaded, initialized, and ready to serve, the next optimization opportunity lies in how requests are grouped for processing. Batching[^fn-batch-etymology] differs fundamentally between training and serving [@crankshaw2017clipper]. Training batches maximize throughput, processing hundreds or thousands of samples together with no concern for individual sample latency. Serving batches must balance throughput against individual request latency, typically processing single digits of requests together while ensuring no request waits too long. This adaptive approach is called *dynamic batching* because the system adjusts batch composition in real time based on arriving requests.

[^fn-batch-etymology]: **Batch**: From Old French "bache" (a quantity baked at one time), the term entered computing in the 1950s to describe jobs processed together without human interaction, as contrasted with interactive computing. IBM's batch processing systems of the 1960s would collect punch cards overnight and process them sequentially. The ML usage preserves this core meaning: group samples together for efficient processing, trading individual response time for aggregate throughput.


::: {.callout-definition title="Dynamic Batching"}

**Dynamic Batching** refers to a serving strategy that collects incoming requests within a _time window_ and processes them together, trading individual request latency for improved _throughput_ and _hardware utilization_. The window size and maximum batch size parameters control this tradeoff.

:::

### Why Batching Helps {#sec-model-serving-systems-batching-helps-f1dc}

Modern accelerators achieve peak efficiency only at sufficient batch sizes [@shen2019nexus]. A single inference request leaves most compute units idle because GPUs are designed for parallel execution across thousands of threads. Batching amortizes fixed costs across multiple requests and enables parallel execution across the batch dimension.

Two fixed costs dominate at small batch sizes. **Kernel launch overhead**[^fn-kernel-etymology] is the time for the CPU to prepare and submit work to the GPU. Each layer in a neural network typically requires a separate kernel launch: the CPU must assemble kernel parameters, copy them to GPU-accessible memory, and signal the GPU to begin execution. This overhead is typically 5-20μs per kernel, independent of batch size. ResNet-50 has approximately 50 layers, so kernel launch alone adds 250-1000μs per inference. At batch size 1, this overhead may exceed the actual compute time; at batch size 32, the same overhead is amortized across 32 images. **Weight loading** reads model parameters from GPU memory (VRAM) to the compute units. At batch size 1, the GPU reads all weights to process one image; at batch size 32, the same weight read processes 32 images, achieving 32× better memory efficiency. Measuring *batching efficiency* on a concrete model quantifies how these fixed costs amortize in practice.

[^fn-kernel-etymology]: **Kernel**: From Old English "cyrnel" meaning seed or grain, the essential core of something. In operating systems (1960s), the kernel is the core that manages hardware resources. CUDA borrowed this term around 2007 for GPU functions because they represent the computational "core" of parallel algorithms. Unlike OS kernels that run continuously, GPU kernels are discrete units of parallel work launched by the CPU and executed across thousands of GPU threads simultaneously.


::: {.callout-notebook title="ResNet-50 Batching Efficiency"}

The throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates the power of batching:

+----------------+---------------------+-----------------------+----------------+---------------+
| **Batch Size** | **Inference Time*** | **Per-Image Compute** | **Throughput** | **GPU Util.** |
+:===============+====================:+======================:+===============:+==============:+
| 1              | 5.0ms               | 5.0ms                 | 200 img/s      | 15%           |
| 4              | 7.2ms               | 1.8ms                 | 556 img/s      | 42%           |
| 8              | 9.1ms               | 1.1ms                 | 879 img/s      | 65%           |
| 16             | 14.0ms              | 0.9ms                 | 1,143 img/s    | 85%           |
| 32             | 25.0ms              | 0.8ms                 | 1,280 img/s    | 95%           |
+----------------+---------------------+-----------------------+----------------+---------------+

*Times shown are pure inference time, excluding queue wait. @sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b analyzes how user-perceived latency includes batching window wait.

**Key insight**: Batch size 32 achieves 6.4× higher throughput than batch size 1. However, user-perceived latency includes both queue wait and inference time. With a 10ms batching window and 25ms inference, total latency reaches 35ms versus 5ms at batch size 1.

:::

The table reveals the throughput-latency tradeoff in stark terms: larger batches dramatically improve hardware efficiency but increase per-request latency. In practice, the optimal batch size depends on both the latency Service Level Objective (SLO) and the arrival rate of requests. The question facing every serving engineer is therefore quantitative: given a specific latency budget, what is the largest batch size that still meets the SLO? The following analysis shows how to find this sweet spot.

::: {.callout-notebook title="The Batching Sweet Spot"}

**Problem**: You are serving a ResNet-50 model. At batch=1, the GPU is mostly idle (15% utilization). You want to increase throughput to save money, but you have a **20 ms** latency budget.

**The Math**:

1.  **Baseline (Batch 1)**: Inference = **5 ms**. Throughput = **200 img/s**.
2.  **Optimized (Batch 8)**:
    - **Wait Time**: You set a **5 ms** batching window to collect requests.
    - **Inference Time**: Batch 8 inference takes **9 ms**.
    - **User Latency**: $5 \text{ ms (wait)} + 9 \text{ ms (compute)} = \mathbf{14 \text{ ms}}$.
    - **Throughput**: $8 \text{ img} / 14 \text{ ms} \approx \mathbf{570 \text{ img/s}}$.

**The Systems Conclusion**: By accepting a **3x increase in latency** (5ms $\rightarrow$ 14ms), you have achieved nearly **3x higher throughput** on the same hardware. As long as 14ms is under your 20ms budget, this is "free" capacity. This trade-off is the fundamental lever of serving economics.
:::

The efficiency gains from batching come at a cost: requests must wait for the batch to form. This creates a fundamental tension between throughput optimization (larger batches) and latency minimization (immediate processing). Understanding the different batching strategies and their tradeoffs is essential for tuning this balance.

### Static vs Dynamic Batching {#sec-model-serving-systems-static-vs-dynamic-batching-fd0a}

**Static batching** waits for a fixed batch size before processing. Simple to implement but problematic in practice: during low traffic, requests wait indefinitely for a full batch. During high traffic, large batches increase per-request latency.

**Dynamic batching** collects requests within a time window, processing whatever has arrived when the window closes [@olston2017tensorflow]. This bounds maximum wait time regardless of traffic level. The window size represents a direct tradeoff: shorter windows reduce latency but sacrifice throughput; longer windows improve throughput but increase latency.

Typical configurations use windows of 5-50ms with maximum batch sizes of 8-32 for latency-sensitive applications. The optimal configuration depends on request arrival patterns, model characteristics, and latency requirements.

### Dynamic Batching Latency-Throughput Trade-offs {#sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d}

Dynamic batching introduces a fundamental tension between throughput optimization and latency constraints. Understanding this tradeoff quantitatively enables systematic configuration decisions rather than trial-and-error tuning. But first, it is essential to understand why latency spikes occur at all:

::: {.callout-notebook title="Why Latency Spikes Under Load"}

**Recall** from @sec-model-serving-systems-littles-law-9352: Little's Law ($L = \lambda W$) governs all stable queues. When hardware is saturated (throughput $\lambda$ is maxed out), any increase in traffic increases queue depth ($L$). Since $\lambda$ cannot grow, **latency ($W$) must grow linearly with queue depth**. This is why **admission control** (rejecting requests when $L$ exceeds a threshold) is the only way to preserve latency during overload.
:::

@eq-batching-latency decomposes the total user-perceived latency for a batched request into two components:

$$L_{\text{total}} = L_{\text{wait}} + L_{\text{compute}}(b)$$ {#eq-batching-latency}

where $L_{\text{wait}}$ is the time spent waiting in the batching queue and $L_{\text{compute}}(b)$ is the inference time for batch size $b$. The batching window $T$ bounds wait time ($L_{\text{wait}} \leq T$), while batch size affects compute time through GPU utilization characteristics.

#### Quantitative Analysis of Batching {#sec-model-serving-systems-queue-waiting-time-analysis-8d5c}

For Poisson arrivals with rate $\lambda$ and batching window $T$, requests arrive uniformly within the window. A request arriving at time $t$ within the window waits $T - t$ for the batch to close. @eq-avg-wait shows that the average wait time is simply half the window:

$$E[L_{\text{wait}}] = \frac{T}{2}$$ {#eq-avg-wait}

This simple relationship has direct implications. A 20ms batching window adds 10ms average latency regardless of batch size achieved. If your latency SLO is 50ms and inference takes 5ms, the batching window consumes 20% of your latency budget before any computation begins.

**Batch Size Distribution.** The number of requests collected during window $T$ follows a Poisson distribution with mean $\lambda T$. @eq-batch-distribution formalizes this relationship:

$$P(\text{batch size} = k) = \frac{(\lambda T)^k e^{-\lambda T}}{k!}$$ {#eq-batch-distribution}

@tbl-batch-variability quantifies this variability, showing how batch size fluctuates for different traffic levels with a fixed 10ms window:

+------------------+----------------+-------------+----------------+---------------------+
| **Arrival Rate** | **Mean Batch** | **Std Dev** | **P(batch=0)** | **P(batch≥2×mean)** |
+:=================+===============:+============:+===============:+====================:+
| **50 QPS**       | 0.5            | 0.7         | 61%            | 1%                  |
| **200 QPS**      | 2.0            | 1.4         | 14%            | 9%                  |
| **500 QPS**      | 5.0            | 2.2         | 0.7%           | 12%                 |
| **1000 QPS**     | 10.0           | 3.2         | 0.005%         | 13%                 |
+------------------+----------------+-------------+----------------+---------------------+

: **Batch Size Variability**: At low traffic, batching windows frequently contain zero requests (wasted GPU cycles). At moderate traffic, batch sizes fluctuate significantly around the mean. High traffic provides more stable batching but still sees 13% of batches exceeding twice the mean size. {#tbl-batch-variability}

#### Throughput Maximization Strategy {#sec-model-serving-systems-throughput-maximization-strategy-27f5}

Throughput optimization requires maximizing the number of requests processed per unit time. For a system with service time $S(b)$ for batch size $b$, throughput follows @eq-batch-throughput:

$$\text{Throughput}(b) = \frac{b}{T + S(b)}$$ {#eq-batch-throughput}

The numerator increases linearly with batch size while the denominator increases sub-linearly (due to GPU parallelism). This creates an optimal batch size that balances these competing effects.

For ResNet-50 on a V100 GPU, service time scales as $S(b) = 5\text{ms} + 0.6b$ (5ms fixed overhead plus 0.6ms per additional image in the batch). With $T = 10$ms batching window:

+----------------+------------------+-------------------+----------------+----------------+
| **Batch Size** | **Service Time** | **Total Latency** | **Throughput** | **Efficiency** |
+:===============+=================:+==================:+===============:+:===============+
| 1              | 5.6ms            | 15.6ms            | 64 img/s       | Low            |
| 4              | 7.4ms            | 17.4ms            | 230 img/s      | Moderate       |
| 8              | 9.8ms            | 19.8ms            | 404 img/s      | Good           |
| 16             | 14.6ms           | 24.6ms            | 650 img/s      | High           |
| 32             | 24.2ms           | 34.2ms            | 935 img/s      | Maximum        |
+----------------+------------------+-------------------+----------------+----------------+

: **Batching Throughput Analysis**: ResNet-50 throughput on V100 with 10ms batching window. Throughput increases 14.6x from batch size 1 to 32 (64 to 935 img/s), but total latency more than doubles (15.6ms to 34.2ms). The optimal configuration depends on whether the latency SLO or throughput target is the binding constraint. {#tbl-batching-throughput}

The throughput gains in @tbl-batching-throughput trace directly back to the Iron Law framework established in @sec-ai-training-iron-law-training-performance-a53f, where batching amortizes the fixed overhead term.

::: {.callout-notebook title="The Iron Law of Batching Efficiency"}

**The Iron Law Connection:**
In serving, we maximize throughput by amortizing the **Latency (Overhead)** term.
$$ \text{Time}_{\text{total}} = \frac{\text{Ops}}{\text{Peak} \cdot \text{Util}} + \text{Latency}_{\text{fixed}} $$

**Deriving the Sweet Spot:**

*   **Case 1 (Batch 1):** Overhead (5ms) $\approx$ Compute (0.6ms). Efficiency $\approx 10\%$. The GPU is mostly waiting.
*   **Case 2 (Batch 32):** Overhead (5ms) $\ll$ Compute (19.2ms). Efficiency $\approx 80\%$. The GPU is crunching numbers.

**The Golden Rule:** Increase batch size until the **Latency Term** becomes negligible (< 10% of total time). Beyond this point, you gain minimal throughput but pay a linear latency penalty.
:::

#### Latency-Constrained Optimization {#sec-model-serving-systems-latencyconstrained-optimization-8f66}

When latency SLOs provide the binding constraint, the optimization problem becomes finding the maximum batch size that meets the SLO. For SLO $L_{\text{SLO}}$ and average wait time $T/2$, @eq-latency-constrained-batch defines the maximum allowable batch size:

$$b_{\text{max}} = \max\{b : \frac{T}{2} + S(b) \leq L_{\text{SLO}}\}$$ {#eq-latency-constrained-batch}

Consider a 50ms p95 latency SLO for ResNet-50 serving:

**Scenario 1: Conservative window (T = 5ms)**
- Average wait: 2.5ms
- Latency budget for inference: 47.5ms
- Maximum batch size: 71 (but typically capped at 32 for memory)
- Achieved throughput: ~1,140 img/s (batch=32)

**Scenario 2: Aggressive window (T = 25ms)**
- Average wait: 12.5ms
- Latency budget for inference: 37.5ms
- Maximum batch size: 48
- Achieved throughput: ~1,280 img/s (batch=48)

The aggressive window achieves only 12% higher throughput but increases average latency by 10ms and p99 latency by 25ms. Examine @tbl-batching-throughput: for latency-sensitive applications, the conservative window provides better user experience at modest throughput cost.

**SLO Violation Analysis.** Batch size variability causes SLO violations even when mean latency appears safe. The p99 latency includes both worst-case wait time (full window) and worst-case batch size (governed by Poisson tail). @eq-p99-batch-latency captures this relationship:

$$L_{p99} \approx T + S(b_{p99})$$ {#eq-p99-batch-latency}

where $b_{p99}$ is the 99th percentile batch size. For $\lambda = 500$ QPS and $T = 10$ms:

- Mean batch size: 5
- p99 batch size: 11 (from Poisson distribution)
- Mean latency: $5\text{ms} + 9.0\text{ms} = 14\text{ms}$
- p99 latency: $10\text{ms} + 11.6\text{ms} = 21.6\text{ms}$

The p99 latency is 1.54× the mean, reflecting both wait time variance and batch size variance. Systems that provision based on mean latency will experience SLO violations.

#### Adaptive Batching Windows {#sec-model-serving-systems-adaptive-batching-windows-c404}

Fixed batching windows waste latency budget during high traffic when large batches form quickly. @lst-adaptive-batching demonstrates how adaptive strategies adjust the window based on queue depth.

::: {#lst-adaptive-batching lst-cap="**Adaptive Batching Window**: Dynamically adjusts batch timeout based on queue depth and arrival rate, reducing average latency by 27% compared to fixed windows while maintaining throughput."}
```{.python}
def adaptive_batching_window(queue_depth, arrival_rate, slo_ms):
    """Compute optimal batching window.

    Based on current system state.
    """
    target_batch_size = 16  # Optimal batch for GPU utilization

    # Fast path: batch ready, close immediately to minimize latency
    if queue_depth >= target_batch_size:
        return 0

    # Compute maximum allowable wait from SLO constraint
    # Reserve 30% of latency budget for batching,
    # remainder for inference
    max_wait = slo_ms * 0.3

    # Estimate time to accumulate target batch at current arrival rate
    if arrival_rate > 0:
        requests_needed = target_batch_size - queue_depth
        estimated_wait = requests_needed / arrival_rate
        # Return minimum of estimated wait and SLO-constrained maximum
        return min(estimated_wait, max_wait)

    return (
        max_wait  # Low traffic: use full budget to accumulate batch
    )
```
:::

This approach reduces average wait time during high traffic while maintaining batch sizes. For traffic varying between 200-1000 QPS:

- Fixed window (10ms): Average latency 15ms, throughput 650 img/s
- Adaptive window: Average latency 11ms (27% reduction), throughput 680 img/s (5% improvement)

The interplay between window size and batch limits creates a space of possible configurations, each representing a different balance between throughput and latency.

**Throughput-Latency Pareto Frontier**

The batching configuration space forms a Pareto frontier where improving throughput requires accepting higher latency. @tbl-pareto-batching traces this frontier across five representative configurations:

+-----------------+---------------+-----------------+-----------------+----------------+----------------------+
| **Window (ms)** | **Max Batch** | **Avg Latency** | **p99 Latency** | **Throughput** | **Configuration**    |
+:================+==============:+================:+================:+===============:+:=====================+
| 2               | 16            | 8ms             | 18ms            | 890 img/s      | Ultra-low latency    |
| 5               | 32            | 10ms            | 22ms            | 1,140 img/s    | Balanced             |
| 10              | 32            | 15ms            | 35ms            | 1,240 img/s    | Moderate latency     |
| 20              | 64            | 23ms            | 52ms            | 1,310 img/s    | Throughput-optimized |
| 50              | 128           | 38ms            | 98ms            | 1,350 img/s    | Maximum throughput   |
+-----------------+---------------+-----------------+-----------------+----------------+----------------------+

: **Batching Pareto Frontier**: Each configuration represents a different point on the throughput-latency trade-off curve. Moving from 2ms to 50ms windows improves throughput by only 52% while increasing p99 latency by 5.4×. Diminishing returns make aggressive batching costly for latency-sensitive applications. {#tbl-pareto-batching}

#### Practical Configuration Guidelines {#sec-model-serving-systems-practical-configuration-guidelines-9791}

Based on quantitative analysis, principled batching configuration follows these guidelines. Start with the latency budget by allocating 20 to 30 percent of SLO to batching wait time. Estimate traffic using the p95 arrival rate rather than average to account for traffic spikes. Calculate the maximum window as $T_{\text{max}} = 0.3 \times L_{\text{SLO}}$. Determine the batch size limit from GPU memory and p99 latency constraints. Monitor the actual distribution since batch size variance indicates whether traffic assumptions hold.

For ResNet-50 with 50ms SLO and 500 QPS traffic:

- Latency budget for batching: 15ms
- Maximum window: 15ms
- Expected batch size: 7.5
- Maximum batch size: 32 (memory limit)
- Configuration: $T = 12$ms, $b_{\text{max}} = 32$
- Predicted p99 latency: 43ms (within SLO)
- Predicted throughput: 1,180 img/s

### Continuous Batching {#sec-model-serving-systems-continuous-batching-8bb6}

Autoregressive models like language models generate outputs token by token, creating a batching challenge that differs from single-pass models like ResNet-50. Traditional batching processes all sequences in a batch for all generation steps, wasting compute when sequences complete at different times [@yu2022orca]. If one sequence in a batch of 8 finishes after 10 tokens while others need 100 tokens, 87.5% of the compute for that sequence slot is wasted. This inefficiency matters as language models grow to dominate production inference workloads.

Continuous batching (also called iteration-level batching) addresses this waste by allowing new requests to join a batch between generation steps and completed sequences to exit [@kwon2023vllm]. Rather than forming static batches that persist for the entire generation process, the system manages batch composition dynamically at each decoding iteration.

The mechanism works as follows: when a sequence generates its end-of-sequence token, its slot becomes immediately available. A waiting request can fill that slot for the next iteration rather than waiting for the entire batch to complete. Similarly, the system can add new requests to available slots without interrupting ongoing generation. This dynamic approach maintains high GPU utilization even when sequence lengths vary dramatically.

Systems implementing continuous batching, such as vLLM and TensorRT-LLM, achieve 2-4× higher throughput than traditional static batching [@agrawal2024sarathi]. The improvement comes from two sources: eliminating wasted compute on completed sequences and reducing average wait time for new requests. For production language model serving where response lengths vary from single tokens to thousands, continuous batching has become essential for cost-effective deployment.

Memory management adds complexity to continuous batching. As sequences enter and exit the batch, the key-value cache that stores attention context must be dynamically allocated and freed. Consider what happens when sequences of varying lengths share GPU memory: a 100-token sequence completes and releases its cache, but a new 150-token sequence cannot use that space because it needs a larger contiguous block. Over time, small unusable gaps accumulate between allocated regions, eventually preventing new sequences from starting even when total free memory appears sufficient. This *memory fragmentation* can waste 40 to 50 percent of available memory in naive implementations, severely limiting the concurrent batch size that determines throughput.

**PagedAttention**,[^fn-pagedattention] introduced in vLLM, solves this fragmentation problem by applying operating system virtual memory concepts to GPU memory [@kwon2023vllm]. Instead of allocating one contiguous block per sequence, PagedAttention divides the KV cache into fixed-size *pages* (typically 16 tokens each). A sequence's cache consists of pointers to non-contiguous pages scattered across GPU memory. When a sequence completes, its pages return to a free list and can be reused by any new sequence, regardless of length. This approach achieves near-zero fragmentation: vLLM reports memory utilization above 95% compared to 50-60% for contiguous allocation schemes. The overhead is modest (one pointer lookup per page during attention computation), making PagedAttention the standard for production LLM serving.

[^fn-pagedattention]: **PagedAttention**: Introduced by Kwon et al. at SOSP 2023, this algorithm directly applies operating system virtual memory concepts to GPU memory management for LLMs. Before PagedAttention, researchers found that existing systems wasted 60-80% of KV cache memory due to fragmentation and over-reservation. By borrowing paging and copy-on-write mechanisms from OS design, PagedAttention reduces waste to under 4%, enabling 2-4x higher throughput on the same hardware. This technique has become the de facto standard in production LLM serving systems.

The batching and memory techniques covered here establish the foundation for LLM serving, but several advanced topics warrant additional study:

::: {.callout-note title="LLM Serving: Beyond the Fundamentals"}

Language model serving introduces challenges beyond the batching and memory principles established here. The key-value cache that stores attention context scales with sequence length and batch size, often exceeding the model weights themselves in memory consumption. Techniques like speculative decoding use small draft models to propose multiple tokens that the target model verifies in parallel, achieving 2-3× latency reduction for interactive applications. Weight-only quantization (INT4 weights with FP16 activations) proves more effective than activation quantization for memory-bandwidth-bound LLM inference.

These LLM-specific optimizations build directly on the foundations this chapter establishes: queuing theory governs request scheduling, batching tradeoffs determine throughput-latency curves, and precision selection follows the same accuracy-efficiency principles. The serving fundamentals apply universally; LLM serving adds domain-specific techniques atop this foundation. Advanced treatments provide detailed coverage of KV cache optimization, including advanced techniques for multi-tenant serving and distributed inference.

:::

While continuous batching represents the state of the art for LLM serving, not all deployment scenarios benefit from batching at all. The sophisticated techniques examined so far, from dynamic batching windows to PagedAttention, optimize for high-throughput server workloads. But these techniques introduce complexity and latency overhead that may not be justified for all deployment contexts. A fundamental question remains: when does batching hurt rather than help?

**When Not to Batch.** Some scenarios require single-request processing. Ultra-low latency requirements where p99 latency must stay under 10ms make any batching delay unacceptable. Highly variable request sizes where inputs vary dramatically in size cause batching to create padding overhead that wastes compute. Memory constraints where models already consume most GPU memory mean batch activations may cause out-of-memory errors.

### Session Affinity Constraints {#sec-model-serving-systems-session-affinity-constraints-8b1f}

When requests from the same user or session should route to the same replica, batching becomes constrained. Session affinity, also called sticky sessions, matters for three main reasons.

**KV-Cache Reuse**: For conversational AI, the key-value cache from previous turns dramatically speeds up multi-turn conversations. Routing a follow-up request to a different replica forfeits this cached context, increasing latency by 2 to 5 times for long conversations.

**User-Specific Models**: Some systems serve personalized models or adapters per user. Routing requests to the replica that has already loaded that user's adapter avoids repeated loading overhead.

**Stateful Preprocessing**: When preprocessing maintains state through tokenizer caches or session-specific normalization, routing to a different replica requires rebuilding this state.

The tension with batching is clear since strict affinity constrains which requests can be batched together, potentially reducing batch sizes and GPU utilization. Production systems often implement soft affinity where requests prefer their assigned replica but can overflow to others when that replica is overloaded. This preserves most affinity benefits while maintaining load balance.

### Traffic Patterns and Batching Strategy {#sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b}

The optimal batching strategy depends critically on how requests arrive. Different deployment contexts exhibit fundamentally different arrival patterns, each requiring distinct batching approaches. The MLPerf inference benchmark codifies these patterns into four scenarios that directly map to real-world deployments, as @sec-benchmarking-ai explains in detail.

**Server Traffic (Poisson Arrivals).** Cloud APIs and web services typically receive requests following a Poisson process,[^fn-poisson-process] where arrivals are independent and uniformly distributed over time. @eq-poisson-batch expresses the expected batch size for Poisson arrivals with rate $\lambda$ and batching window $T$:

[^fn-poisson-process]: **Poisson Process**: A stochastic model where events occur continuously and independently at a constant average rate. Named after French mathematician Simeon Denis Poisson (1781-1840), this model accurately describes many real-world arrival patterns including web requests and API calls. The key property for serving systems is that inter-arrival times are exponentially distributed, meaning the probability of long gaps between requests decays exponentially, which is why batching windows can be tuned probabilistically.


$$E[\text{batch size}] = \lambda \cdot T$$ {#eq-poisson-batch}

The variance equals the mean (a property of Poisson distributions), so batch sizes fluctuate significantly at moderate traffic. With $\lambda = 200$ requests/second and $T = 10$ms, expected batch size is 2, but 16% of windows will have zero requests (wasted compute cycles) while others may have 4 or more.

The optimal batching window balances waiting cost against throughput benefit. @eq-optimal-window defines this optimum:

$$T_{\text{optimal}} = \min\left(L - S, \sqrt{\frac{S}{\lambda}}\right)$$ {#eq-optimal-window}

where $L$ is the latency SLO and $S$ is the service time. A perhaps surprising result emerges from this equation: as traffic increases, the optimal window decreases while achieved batch sizes still grow. @tbl-traffic-adaptive demonstrates this phenomenon across four traffic levels.

+------------------+--------------------+--------------------+-----------------+
| **Arrival Rate** | **Optimal Window** | **Avg Batch Size** | **p99 Latency** |
+:=================+===================:+===================:+================:+
| **100 QPS**      | 20ms               | 2.0                | 45ms            |
| **500 QPS**      | 8ms                | 4.0                | 42ms            |
| **1,000 QPS**    | 5ms                | 5.0                | 38ms            |
| **5,000 QPS**    | 2ms                | 10.0               | 35ms            |
+------------------+--------------------+--------------------+-----------------+

: **Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time. {#tbl-traffic-adaptive}

**Streaming Traffic (Correlated Arrivals).** Autonomous vehicles, video analytics, and robotics systems receive inputs from multiple synchronized sensors. Rather than independent arrivals, frames from all cameras for a given timestamp must be processed together as a batch.

::: {.callout-notebook title="Multi-Camera Autonomous Vehicle Serving"}

Consider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial fusion:

**Timeline for processing frame set N:**

+----------+-----------------------------------+
| **Time** | **Event**                         |
+:=========+:==================================+
| T = 0ms  | Cameras begin capturing frame N   |
| T = 8ms  | Camera 1 frame arrives            |
| T = 10ms | Cameras 2-5 frames arrive         |
| T = 15ms | Camera 6 arrives (jitter)         |
| T = 15ms | Batch inference begins (6 images) |
| T = 25ms | Inference complete                |
| T = 32ms | Result ready for planning module  |
+----------+-----------------------------------+

**Key constraints:**

- Hard deadline: 33ms per frame set (real-time requirement)
- Batch size: Fixed at 6 (one per camera)
- Synchronization budget: 12ms of 33ms total (36% for jitter tolerance)
- Timeout policy: If camera frame not received by T+20ms, use previous frame

Unlike Poisson traffic where dynamic batching optimizes throughput, streaming traffic requires synchronization policies that handle sensor jitter while meeting hard deadlines.

:::

**Single-User Traffic (Sequential Arrivals).** Mobile and embedded applications serve one user at a time, with requests arriving only after the previous result is consumed. Batch size is typically 1, eliminating batching optimization entirely but raising different challenges centered on energy efficiency and thermal management.

::: {.callout-notebook title="ResNet-50: Mobile Serving"}

+------------------------+--------------+------------+-------------------+
| **Phase**              | **Duration** | **Energy** | **Notes**         |
+:=======================+=============:+===========:+:==================+
| **Camera buffer read** | 8ms          | 0.08mJ     | System API        |
| **JPEG decode (CPU)**  | 15ms         | 1.5mJ      | Single-threaded   |
| **Resize + Normalize** | 5ms          | 0.4mJ      | CPU preprocessing |
| **NPU inference**      | 12ms         | 0.8mJ      | 82% utilization   |
| **Post-process + UI**  | 5ms          | 0.2mJ      | Result rendering  |
| **Total**              | **45ms**     | **3.0mJ**  | 22 FPS sustained  |
+------------------------+--------------+------------+-------------------+

**Key metrics for ML node serving:**

- **Energy per inference**: 3.0mJ enables ~9,000 inferences per 10Wh battery (typical smartphone)
- **Thermal budget**: At 3.0mJ/45ms = 67mW sustained, indefinite operation without throttling
- **NPU vs CPU tradeoff**: CPU fallback uses 4.2mJ (1.4× energy) at 85ms (1.9× latency)
- **Memory footprint**: 150MB peak (model + activations), competing with app memory

**Critical insight**: Even at batch size 1, the mobile NPU achieves 82% utilization because its compute capacity matches single-image workloads. This differs from datacenter GPUs, which achieve only 15% utilization at batch size 1 because their massive parallelism requires larger batches to saturate.

:::

#### Mobile Serving Constraints {#sec-model-serving-systems-mobile-serving-constraints-eb68}

Unlike cloud serving where cost dominates, mobile serving faces three related constraints that shape optimization strategy:

1. **Energy Budget**: Each inference depletes battery. A photo app running continuous inference at 22 FPS drains 240mW—acceptable for active use but problematic for background processing. The optimization target shifts from throughput to energy-per-inference.

2. **Thermal Throttling**: Sustained high-power operation triggers thermal management. When the SoC reaches thermal limits (typically 45°C junction), the OS reduces NPU frequency by 30-50%, degrading both latency and throughput. Bursty workloads that allow cooling between bursts outperform sustained maximum throughput.

3. **Memory Constraints**: Mobile devices share limited RAM between applications. A model consuming 500MB may be evicted during background operation, requiring reload (cold start) that adds 200-500ms latency. Even a 150MB footprint becomes problematic when the model must coexist with other app components. Memory-efficient quantization directly improves user experience through faster model restoration, and memory-mapped model loading (@sec-model-serving-systems-loading-strategies-eb38) helps further by loading pages on demand rather than requiring the full model in memory.

These constraints make mobile serving optimization fundamentally different from cloud optimization. The goal is not maximum throughput but **sustainable performance**—maintaining acceptable latency without thermal throttling or excessive battery drain.

@tbl-traffic-patterns-summary maps the four MLPerf scenarios to their deployment contexts and optimal batching strategies, providing a decision framework for serving system design.

+------------------+---------------------+------------------+---------------------------+
| **MLPerf**       | **Deployment**      | **Batch**        | **Optimization**          |
| **Scenario**     | **Context**         | **Strategy**     | **Focus**                 |
+:=================+:====================+:=================+:==========================+
| **Server**       | Cloud APIs,         | Dynamic batching | Window tuning,            |
|                  | web services        | with timeout     | utilization-latency curve |
+------------------+---------------------+------------------+---------------------------+
| **MultiStream**  | Autonomous driving, | Synchronized     | Jitter handling,          |
|                  | video analytics     | sensor fusion    | deadline guarantees       |
+------------------+---------------------+------------------+---------------------------+
| **SingleStream** | Mobile apps,        | No batching      | Preprocessing,            |
|                  | embedded devices    | (batch=1)        | power efficiency          |
+------------------+---------------------+------------------+---------------------------+
| **Offline**      | Batch processing,   | Maximum batch    | Throughput,               |
|                  | data pipelines      | size             | hardware utilization      |
+------------------+---------------------+------------------+---------------------------+

: **Traffic Patterns and Batching Strategies**: The four MLPerf inference scenarios map to distinct deployment contexts. Server traffic (cloud APIs) uses dynamic batching with timeout; MultiStream (autonomous driving) uses synchronized sensor fusion; SingleStream (mobile) processes requests individually; Offline (batch processing) maximizes batch size for throughput. {#tbl-traffic-patterns-summary}

## LLM Serving {#sec-model-serving-systems-llm-serving-b8bf}

The traffic patterns and batching strategies examined in the previous section share a common assumption: models produce a single output per request, whether a classification label, a bounding box, or an embedding vector. Large language models break this assumption, generating tokens incrementally over hundreds or thousands of iterations and creating a different latency profile. The p50, p95, and p99 metrics that govern classification serving apply differently when a single request takes 2 to 3 seconds to complete but must feel responsive throughout. While the foundational principles of queuing theory, batching tradeoffs, and latency budgets apply universally, LLMs require additional metrics, different optimization strategies, and unique memory management techniques.

### Performance Metrics: TTFT and TPOT {#sec-model-serving-systems-performance-metrics-ttft-tpot-b009}

Generative models produce a stream of tokens rather than a single output tensor. This streaming nature requires dedicated *LLM performance metrics* that reflect the internal state transition from "prefill" (processing input) to "decode" (generating output). The two key measures are *Time to First Token (TTFT)* and *Time Per Output Token (TPOT)*, which capture responsiveness and fluidity respectively.

::: {.callout-definition title="LLM Performance Metrics"}

**Time to First Token (TTFT)** measures the latency from the moment a request is submitted until the first output token appears. This metric is governed by the compute-bound prefill phase and determines the responsiveness of an interactive application.

**Time Per Output Token (TPOT)** measures the time to generate each subsequent token. This metric is governed by the memory-bandwidth-bound decode phase and determines the perceived fluidity of the generation.

:::

These two metrics capture fundamentally different user experience aspects. A fast TTFT provides immediate responsiveness (the system starts answering quickly), while a fast TPOT provides fluid generation (the answer streams smoothly). Production systems must optimize both, typically with different techniques since they are governed by different hardware constraints. Translating these metrics into concrete *LLM serving latency targets* grounds the discussion in production reality.

::: {.callout-lighthouse title="LLM Serving Latency Targets"}

A production-grade LLM service typically targets the following SLOs:

- **TTFT**: < 500 ms (for a 1000-token prompt)
- **TPOT**: < 50 ms (equivalent to ~20 tokens/second, faster than human reading speed)
- **Throughput**: > 1000 tokens/second aggregate across all users

:::

### Decoding Strategies {#sec-model-serving-systems-decoding-strategies-afe8}

Generative models require decoding strategies that trade off quality, diversity, and latency. The choice of decoding strategy can dramatically affect both output quality and computational cost.

**Greedy decoding** selects the highest-probability token at each step. Fast but often produces repetitive, low-quality outputs because it cannot recover from early mistakes.

**Beam search** maintains multiple candidate sequences, selecting the highest-scoring complete sequence. Produces higher-quality outputs but multiplies computation by the beam width.

**Sampling** with temperature, top-k, and top-p parameters introduces randomness for diversity [@holtzman2020curious]. Temperature scales logits before softmax. Top-k limits sampling to the k highest-probability tokens. Top-p, also called nucleus sampling, limits sampling to tokens comprising probability mass p.

The choice presents latency tradeoffs [@meister2020beam]. Beam search with width 5 takes roughly 5× the compute of greedy decoding. Sampling adds minimal overhead but requires careful parameter tuning to balance quality and coherence.

**Streaming Responses.** Rather than waiting for complete generation, production LLM systems return tokens as they are produced. This improves perceived latency since users see output beginning quickly, but requires infrastructure support for chunked HTTP responses and client-side incremental rendering. Streaming changes the latency profile: TTFT determines when output starts appearing, while TPOT determines the perceived generation speed.

### Memory and KV Cache {#sec-model-serving-systems-memory-kv-cache-d1ea}

Generative inference requires managing the **KV Cache**, a stateful memory structure that grows with sequence length. Unlike traditional models where memory usage is constant per batch, LLM memory usage is dynamic:

*   **State Accumulation**: Each generated token adds to the context window, consuming additional GPU memory.
*   **Fragmentation**: Variable-length sequences can lead to memory fragmentation if not managed explicitly.

The continuous batching and PagedAttention techniques covered in @sec-model-serving-systems-continuous-batching-8bb6 address these challenges. Advanced techniques including prefix caching and speculative decoding are covered in specialized coverage of large-scale systems.

The computational intensity of managing KV caches across concurrent requests raises a broader question: what is the energy cost of each token generated? Translating these hardware demands into energy and carbon metrics makes the environmental impact of LLM serving concrete.

::: {.callout-notebook title="The Carbon Cost of a Chat"}

**Joules per Token: The Green Metric**: 
As LLMs scale, energy efficiency becomes a first-class operational metric alongside latency. For an H100 GPU (700W TDP), we can quantify the energy footprint of serving:

1.  **Throughput**: 114 concurrent requests × 7.5 tokens/sec/req $\approx$ **855 tokens/sec**.
2.  **Power**: 700W (GPU) + 300W (Host/Overhead) = **1000W**.
3.  **Energy per Token**:

    $$\frac{1000 \text{ Joules/sec}}{855 \text{ tokens/sec}} \approx \mathbf{1.17 \text{ Joules/token}}$$

**The Systems Conclusion**:
A typical 500-token response consumes $\approx \mathbf{585 \text{ Joules}}$.

- For comparison, charging a smartphone consumes $\approx 40,000$ Joules.
- Boiling a cup of water consumes $\approx 100,000$ Joules.

**The Engineering Lever**: The primary way to reduce Joules/Token is to **increase hardware utilization**. If the GPU sits at 10% utilization due to poor batching, the "Idle Power" is still ~300W, causing the energy-per-token to skyrocket to **>10 Joules**. MLOps is not just about speed; it's about sustainability through efficiency.
:::


## Inference Runtime Selection {#sec-model-serving-systems-inference-runtime-selection-5eef}

The batching strategies and LLM-specific techniques examined in preceding sections determine *how* requests are grouped and processed. But these strategies assume an underlying execution engine that actually runs the model computations. The execution environment directly affects whether the latency budgets established earlier are achievable. The inference runtime, the software layer that orchestrates tensor operations and manages hardware resources, can vary by an order of magnitude in performance for identical models. Choosing appropriately requires understanding the tradeoffs between framework-native serving, general-purpose optimization, and specialized inference engines.

### Runtime Ecosystem and Configuration {#sec-model-serving-systems-frameworknative-serving-da62}

PyTorch and TensorFlow models can serve directly using their native runtimes. This approach maximizes compatibility (any model that trains will serve) and simplifies the deployment pipeline (no export or conversion step). However, framework runtimes include training functionality that adds overhead, and default execution paths may not exploit hardware-specific optimizations.

TorchScript and TensorFlow SavedModel formats enable ahead-of-time compilation and graph optimization, improving over eager execution while maintaining framework compatibility. These formats represent the first step toward deployment optimization without abandoning the familiar framework ecosystem.

**General-Purpose Optimization.** ONNX Runtime provides a hardware-agnostic optimization layer [@onnxruntime2024]. Models export to ONNX format, then ONNX Runtime applies graph optimizations and selects execution providers for the target hardware. This enables single-format deployment across CPUs, GPUs, and specialized accelerators.

**Specialized Inference Engines.** TensorRT[^fn-tensorrt] (NVIDIA GPUs), OpenVINO (Intel hardware), and similar engines optimize specifically for their target hardware [@nvidia2024tensorrt; @chen2018tvm]. They apply aggressive optimizations that framework-native runtimes cannot safely perform:

[^fn-tensorrt]: **TensorRT**: NVIDIA's inference optimization SDK that applies layer fusion, kernel auto-tuning, and precision calibration to neural networks. Unlike framework-native runtimes that preserve training-time graph structure, TensorRT rebuilds the computation graph for the specific target GPU during a build phase. This GPU-specific compilation means TensorRT engines are not portable across GPU architectures, requiring separate builds for V100, A100, and H100 deployments. The build phase can take minutes but produces engines that often achieve 2-5x speedup over PyTorch.


**Layer fusion** combines multiple sequential operations into a single GPU kernel. Consider a common pattern: convolution → batch normalization → ReLU activation. Without fusion, this requires three kernel launches, three round-trips to GPU memory (write conv output, read for batchnorm, write batchnorm output, read for ReLU), and three sets of intermediate tensors. Fusion combines all three into one kernel that reads inputs once, computes the combined result in registers, and writes final outputs once. This eliminates kernel launch overhead (15-60μs saved per fusion) and reduces memory traffic by 2-3×. TensorRT automatically detects and fuses common patterns; a typical ResNet-50 reduces from ~50 kernels to ~15 after fusion.

**Kernel auto-tuning** selects the fastest algorithm for each operation on the specific GPU. A single convolution can be implemented using dozens of algorithms (direct, FFT-based, Winograd, various tiling strategies), each optimal for different input sizes and GPU architectures. Auto-tuning benchmarks each candidate and caches the winner, trading compilation time for runtime performance.

These optimizations typically achieve 2-5x speedup over framework-native serving but require explicit export and may not support all operations. A *runtime comparison* on a standard model quantifies these gains across the optimization spectrum.

::: {.callout-notebook title="ResNet-50: Runtime Comparison"}

Performance comparison for ResNet-50 inference on V100 GPU (batch size 1):

+-----------------+-------------+-------------+---------------------------+
| **Runtime**     | **Latency** | **Speedup** | **Notes**                 |
+:================+============:+============:+:==========================+
| PyTorch (eager) | 8.5ms       | 1.0×        | Baseline, no optimization |
| TorchScript     | 6.2ms       | 1.4×        | JIT compilation           |
| ONNX Runtime    | 5.1ms       | 1.7×        | Cross-platform            |
| TensorRT FP32   | 2.8ms       | 3.0×        | NVIDIA-specific           |
| TensorRT FP16   | 1.4ms       | 6.1×        | Tensor Core acceleration  |
| TensorRT INT8   | 0.9ms       | 9.4×        | Requires calibration      |
+-----------------+-------------+-------------+---------------------------+

**Key insight**: The 9.4× speedup from TensorRT INT8 comes at the cost of: (1) quantization calibration data, (2) potential accuracy loss (<1% for ResNet-50), and (3) NVIDIA-specific deployment.

:::

The optimization-compatibility tradeoff is inherent. More aggressive optimization yields better performance but increases deployment complexity and may introduce numerical differences from training. The choice depends on latency requirements, deployment constraints, and available engineering resources.

**Runtime Configuration.** Beyond runtime selection, configuration choices impact serving performance including thread pools that control parallelism for CPU inference, memory allocation strategies that choose between pre-allocating buffers versus dynamic allocation, execution providers that select and prioritize hardware backends, and graph optimization level that trades compilation time for runtime performance. Production deployments require experimentation to find optimal configurations for specific models and hardware combinations. A systematic approach tests key parameters and measures their impact on latency distributions.

### Precision Selection for Serving {#sec-model-serving-systems-precision-selection-serving-55ba}

Numerical precision directly trades accuracy for throughput, connecting to the quantization techniques covered in @sec-model-compression. While @sec-model-compression focuses on training-time quantization, serving introduces additional considerations including calibration requirements, layer sensitivity, and dynamic precision selection.

**Precision-Throughput Relationship.** For memory-bandwidth-bound operations, reducing precision proportionally increases throughput by reducing data movement. @eq-precision-throughput quantifies the theoretical maximum speedup from precision reduction:

$$\frac{\text{Throughput}_{\text{INT8}}}{\text{Throughput}_{\text{FP32}}} = \frac{32}{8} = 4\times \text{ (theoretical maximum)}$$ {#eq-precision-throughput}

In practice, GPU compute pipelines and Tensor Core alignment requirements limit achieved speedup to 2.5-3.5x for INT8 versus FP32. Tensor Cores require specific alignment: INT8 operations need tensor dimensions divisible by 16, while FP16 requires divisibility by 8. @sec-ai-acceleration provides the detailed Tensor Core architecture that explains these alignment constraints. The *precision tradeoffs* for a standard vision model illustrate how these theoretical limits manifest in practice.

::: {.callout-notebook title="ResNet-50: Precision Tradeoffs on V100"}

+----------------+-------------+------------+--------------+-----------------------+-----------------+
| **Precision**  | **Latency** | **Memory** | **Accuracy** | **Tensor Core Util.** | **Calibration** |
+:===============+============:+===========:+=============:+======================:+:================+
| **FP32**       | 2.8ms       | 98MB       | 76.13%       | 0%                    | None            |
| **FP16**       | 1.4ms       | 49MB       | 76.13%       | 85%                   | None            |
| **INT8 (PTQ)** | 0.9ms       | 25MB       | 75.80%       | 92%                   | 1,000 samples   |
| **INT8 (QAT)** | 0.9ms       | 25MB       | 76.05%       | 92%                   | Full retraining |
+----------------+-------------+------------+--------------+-----------------------+-----------------+

**Key observations:**

- INT8 achieves 3.1× speedup but loses 0.33% accuracy with post-training quantization (PTQ)
- Quantization-aware training (QAT) recovers most accuracy but requires retraining
- FP16 provides 2× speedup with no accuracy loss for most models

:::

**Layer Sensitivity.** Not all layers tolerate reduced precision equally. @eq-quant-error captures how quantization error for a layer scales with weight magnitude and gradient sensitivity:

$$\epsilon_{\text{quant}} \propto \alpha \cdot \|W\|_2 \cdot 2^{-b}$$ {#eq-quant-error}

where $\alpha$ is a layer-specific sensitivity coefficient, $\|W\|_2$ is the weight L2 norm, and $b$ is the bit width. This explains observed patterns where first convolutional layers with high gradients and large sensitivity coefficients are precision-sensitive and often kept at FP16, middle layers with stable gradients and low sensitivity coefficients tolerate INT8 well, and final classification layers with small weights but high task sensitivity benefit from FP16 or higher precision.

**Calibration Requirements.** Post-training quantization requires a calibration dataset to determine optimal scale factors for INT8 conversion. Production experience shows that calibration data must be representative of actual serving traffic, not just training data. Using ImageNet validation images to calibrate a model serving wildlife camera images resulted in 3.2% accuracy degradation in one production system.

**Dynamic Precision Selection.** Advanced serving systems select precision per request based on runtime conditions. If the system is ahead of latency SLO, it uses higher precision for better accuracy. For low-confidence INT8 results, it recomputes at FP16. Different customer tiers may receive different precision levels. This pattern enables adaptive quality-latency tradeoffs while maximizing throughput during normal operation.

The precision decision has direct infrastructure consequences: INT8 inference achieves roughly 3x higher throughput than FP32, meaning a workload requiring 30 GPUs at FP32 needs only 10 at INT8. This 3x reduction in hardware translates directly to a 3x reduction in operating costs. The connection between model-level optimization and infrastructure economics is why precision selection cannot be treated as purely a model concern.

## Node-Level Optimization & Profiling {#sec-model-serving-systems-node-level-optimization-profiling}

Beyond selecting a runtime and precision, maximizing the efficiency of a single ML node requires diving deeper into how the hardware executes the model. This section explores optimizations that occur at the boundaries of software and silicon: compiling the computation graph, exploiting CPU capabilities when GPUs are absent, minimizing the time to get bytes from disk to memory, and visualizing exactly where every microsecond goes.

### Runtime Graph Compilation {#sec-model-serving-systems-runtime-graph-compilation}

We introduced inference engines like TensorRT in @sec-model-serving-systems-specialized-inference-engines-1924, but *how* do they achieve 2-5x speedups? The answer lies in **Graph Compilation**. Unlike training, where the computation graph is dynamic and mutable, serving graphs are static. This allows compilers to perform aggressive optimizations that would be unsafe or too slow during training.

**Operator Fusion**: The most potent optimization. As discussed in @sec-ai-acceleration, memory bandwidth often limits performance more than compute. Fusion collapses multiple operations (e.g., `Conv2D` -> `BiasAdd` -> `ReLU`) into a single kernel launch. This keeps intermediate data in the GPU's fast L1/L2 cache or registers, avoiding round-trips to global memory (VRAM).

**Constant Folding**: Parts of the graph that depend only on model weights (which are constant during serving) can be pre-computed at compile time. For example, if a model contains `x * (sqrt(2) / 2)`, the compiler replaces the division and square root with a single multiplication by `0.707...`.

**Memory Planning**: Since the graph structure is known, the compiler can pre-calculate the exact memory offsets for every tensor. This allows for **static memory allocation**, where a single large block of memory is allocated at startup and reused for all requests, completely eliminating the overhead of `malloc` and `free` during inference.

::: {.callout-notebook title="JIT vs. AOT Compilation"}
*   **Just-In-Time (JIT)**: Compiles the graph the first time it is run (e.g., `torch.compile`).
    *   *Pros*: Optimizes for the specific input shapes seen at runtime.
    *   *Cons*: First request pays a "compilation penalty" (latency spike).
*   **Ahead-of-Time (AOT)**: Compiles the graph before deployment (e.g., `torch.export`, TensorRT `trtexec`).
    *   *Pros*: Zero compilation latency at startup; guarantees a fixed graph.
    *   *Cons*: Must handle all dynamic shapes explicitly or compile multiple profiles.
:::

### CPU Inference Optimization {#sec-model-serving-systems-cpu-inference-optimization}

While GPUs dominate the narrative, CPUs remain the workhorse for a vast number of inference workloads, particularly for smaller models, latency-insensitive batch jobs, or cost-constrained environments. Optimizing for the CPU requires a different mindset.

**SIMD and Vectorization**: Modern CPUs (Intel Xeon, AMD EPYC) pack powerful vector units (AVX-512, AMX). Standard Python loops cannot use these. Specialized runtimes like **OpenVINO** or **Intel Extension for PyTorch (IPEX)** map neural network operators directly to these vector instructions, achieving order-of-magnitude speedups over vanilla implementations.

**Thread Pinning & NUMA**: On multi-socket servers, accessing memory attached to a different CPU socket (NUMA) adds significant latency. Inference servers must be "NUMA-aware," pinning threads to specific cores and ensuring that memory allocations remain local to those cores.

**The "Small Batch" Advantage**: CPUs often outperform GPUs at batch size 1 for small models. The overhead of launching a GPU kernel (~10$\mu$s) and transferring data (~50$\mu$s) can exceed the compute time for a tiny dense layer. For models under 50MB serving single requests, a well-optimized CPU runtime often delivers lower latency than a GPU.

### Fast Model Loading {#sec-model-serving-systems-fast-model-loading}

In autoscaling systems, the time to spin up a new node is critical. A major component of "Cold Start" (@sec-model-serving-systems-model-loading-initialization-cc5a) is simply reading the model weights from disk into memory.

**The Pickle Problem**: The standard PyTorch `torch.load()` uses Python's `pickle` format. This is inefficient because it requires the CPU to unpickle objects one by one, copy them into memory, and then often copy them *again* to the GPU.

**Zero-Copy with `mmap`**: Memory mapping allows the OS to map a file directly into the process's virtual address space. The data is effectively "loaded" only when accessed, and the OS handles the transfer from disk to RAM efficiently.

**Safetensors**: A modern format designed specifically for fast loading. It stores tensors as raw bytes with a minimal JSON header. This allows for **zero-copy** loading: the raw bytes on disk are mapped directly into the tensor's memory buffer.

::: {.callout-example title="Loading Speed: Safetensors vs. Pickle"}
Loading a 5GB Stable Diffusion model:
*   **Pickle (`torch.load`)**: ~15 seconds. High CPU usage.
*   **Safetensors**: ~0.5 seconds. Near-zero CPU usage.

By using `mmap` and formats like `safetensors`, loading speed becomes limited only by the disk's read speed (e.g., 3GB/s for NVMe), rather than CPU parsing overhead.
:::

### Profiling the Serving Node {#sec-model-serving-systems-profiling-serving-node}

Optimization without measurement is guesswork. To truly master the ML node, you must visualize the execution flow.

**The Timeline View**: Tools like **PyTorch Profiler** or NVIDIA **Nsight Systems (nsys)** generate a timeline trace. This visualization reveals the exact sequence of events on the CPU and GPU.

**What to Look For:**
1.  **Gaps in the GPU Timeline**: If the GPU bar has empty spaces, the GPU is idle. This usually means it's waiting for the CPU (preprocessing bottleneck) or disk (data loading).
2.  **Kernel Launch Overhead**: If you see thousands of tiny slivers on the GPU timeline, your model is launching too many small kernels. This is a prime candidate for **Operator Fusion**.
3.  **Host-to-Device Transfers**: Look for `MemcpyHtoD` (Host to Device) blocks. Are they overlapping with computation, or blocking it?

::: {.callout-tip title="The Profiling Loop"}
1.  **Capture**: Run a warmup, then capture a trace of 10-50 requests.
2.  **Visualize**: Open the trace in a viewer (Chrome Tracing, Nsight).
3.  **Identify**: Find the largest gap or the longest block.
4.  **Optimize**: Apply a specific fix (e.g., fusion, pinning).
5.  **Verify**: Re-capture and confirm the gap is gone.
:::

## Economics and Capacity Planning {#sec-model-serving-systems-economics-capacity-planning-3e7e}

The runtime selection, precision tuning, and node-level optimizations examined in the preceding sections collectively determine the fundamental unit of serving physics: the performance per inference. Production deployment requires translating these technical metrics into infrastructure decisions. Serving costs scale with request volume, unlike training costs that scale with dataset size and model complexity [@zhang2019mark]. Cost structure analysis enables decisions that balance performance requirements against budget constraints.

**Cost Per Inference.** Total serving cost decomposes into several components including compute time for GPU or CPU per inference, memory for accelerator memory required to hold model and activations, data transfer for network bandwidth for request and response payloads, and orchestration overhead for container runtime, load balancing, and monitoring. For GPU inference, compute time dominates when utilization is high. When utilization is low, memory cost dominates because the GPU is reserved but idle. Serving infrastructure should maximize GPU utilization through batching, multi-model serving, or right-sized instance selection.

::: {.callout-notebook title="ResNet-50: Cost Analysis"}

Consider serving ResNet-50 on AWS infrastructure (US-East region, on-demand pricing as of 2024):

+---------------------------+---------------+----------------+------------------------+
| **Instance Type**         | **Cost/Hour** | **Throughput** | **Cost per 1M Images** |
+:==========================+==============:+===============:+=======================:+
| **c5.xlarge (CPU)**       | $0.17         | 50 img/s       | $0.94                  |
| **g4dn.xlarge (T4 GPU)**  | $0.53         | 400 img/s      | $0.37                  |
| **p3.2xlarge (V100 GPU)** | $3.06         | 1,200 img/s    | $0.71                  |
+---------------------------+---------------+----------------+------------------------+

**Key insight**: The T4 GPU instance achieves the lowest cost per inference despite higher hourly cost, because GPU throughput dramatically exceeds CPU throughput. The V100 is only cost-effective at very high sustained traffic where its higher throughput justifies the 6x price increase. Note that cloud pricing varies by region and changes over time; consult current pricing for production planning.

:::

### GPU vs CPU Economics {#sec-model-serving-systems-gpu-vs-cpu-economics-eb06}

GPUs provide significant speedup for parallel operations but cost more per hour [@wu2019machine]. The crossover point depends on model characteristics and latency requirements.

CPU inference makes economic sense when models are small with few parameters and simple operations, latency requirements are relaxed with hundreds of milliseconds acceptable, request volume is low or highly variable, and models use operations that do not parallelize well. GPU inference makes economic sense when models are large with parallel-friendly operations, latency requirements are strict at tens of milliseconds, request volume is high and consistent, and batching can achieve high utilization.

**Scaling Responsiveness.** Beyond steady-state costs, startup time affects scaling economics. CPU instances typically start in 30 to 60 seconds while GPU instances take 2 to 5 minutes including driver initialization, model loading, and warmup. For variable traffic patterns, this startup latency can be more important than cost per inference. If traffic spikes arrive faster than GPU instances can scale, latency SLOs will be violated despite having sufficient eventual capacity.

This asymmetry suggests different scaling strategies where CPU instances enable reactive scaling by responding to current demand while GPU instances often require predictive scaling by provisioning based on anticipated demand. For bursty workloads, a hybrid approach uses always-on GPU capacity for baseline load plus CPU overflow capacity for spikes, trading higher per-inference cost during spikes for better responsiveness.

### Capacity Planning {#sec-model-serving-systems-capacity-planning-96a3}

The GPU versus CPU decision establishes the cost per inference, but determining how much infrastructure to provision requires combining cost analysis with the queuing theory foundations from @sec-model-serving-systems-queuing-theory-tail-latency-29a6. Capacity planning translates latency requirements and traffic projections into infrastructure specifications. Key inputs include traffic patterns such as peak request rate, daily and weekly cycles, and growth projections, latency SLOs including p50, p95, and p99 targets, and model characteristics such as inference time distribution at various batch sizes. From these inputs, queuing theory determines required capacity [@harchol2013performance]. The equations developed in @sec-model-serving-systems-queuing-theory-tail-latency-29a6 provide the mathematical foundation where @eq-mm1-wait shows how latency scales with utilization, while @eq-p99-latency enables calculating p99 latency for capacity planning.

The relationship between utilization and latency is nonlinear as shown in the utilization-latency table. At 70 percent utilization, p99 latency is approximately fifteen times service time. At 90 percent utilization, it reaches approximately 46 times service time. This nonlinearity explains why systems that seem healthy with low average latency can suddenly violate SLOs when traffic increases modestly.

The worked example in @sec-model-serving-systems-queuing-theory-tail-latency-29a6 demonstrates the complete capacity planning process by starting from a 50ms p99 SLO and 5,000 QPS target, deriving the safe utilization threshold of 72 percent, calculating required service rate of 6,944 QPS, and determining GPU count with headroom of 10 V100s. Production systems typically provision for peak load plus 30 percent headroom, using auto-scaling to reduce costs during low-traffic periods while meeting latency objectives during peaks.

### Production Case Study: Serving Llama-3-8B {#sec-model-serving-systems-production-case-study-serving-llama38b-0499}

To apply the principles of latency budgeting, memory management, and hardware efficiency, we analyze the production profile of a modern Large Language Model (LLM) serving workload. This case study demonstrates how physical constraints—memory bandwidth and PCIe capacity—translate directly into service-level metrics and unit economics.

#### Workload Profile {#sec-model-serving-systems-workload-profile-a380}

*   **Model**: Llama-3-8B (quantized to 4-bit AWQ).
*   **Hardware**: 1$\times$ NVIDIA H100 SXM5 GPU (80 GB HBM3, 3.35 TB/s bandwidth).
*   **Request Characteristics**: 1,000-token input prompt (Prefill), 256-token generated response (Decode).
*   **Target SLOs**: TTFT $<$ 200 ms, TPOT $<$ 20 ms.

#### Latency Deconstruction {#sec-model-serving-systems-latency-deconstruction-217e}

The end-to-end request latency is governed by the two-phase execution model of autoregressive transformers.

**1. Prefill Phase (Time to First Token)**

The model processes the 1,000-token prompt in parallel. On an H100, this compute-bound operation achieves approximately 10,000 tokens per second.
*   $T_{\text{prefill}} = \frac{1000 \text{ tokens}}{10000 \text{ tokens/s}} = 100 \text{ ms}$.
*   Accounting for 20 ms of system overhead (network ingress, tokenization), the **TTFT is 120 ms**, comfortably within the 200 ms SLO.

**2. Decode Phase (Time Per Output Token)**

The model generates 256 tokens sequentially. This phase is memory-bandwidth bound; the system must read the entire 3.5 GB weight tensor from VRAM to generate a single token.

::: {.callout-perspective title="The Physics of Token Generation"}

Recall the **Energy-Movement Invariant** from @sec-data-engineering-ml: moving a bit is 100–1,000× more expensive than computing on it. In the **Decode Phase**, this law determines the physical "cost per word."

**The Memory Wall for Generative AI**: Because the decode phase has an arithmetic intensity of $\approx 1$ FLOP/byte (we must read every weight just to generate one token), performance is strictly limited by memory bandwidth ($BW$), not compute.

$$ T_{\text{token}} \approx \frac{\text{Model Size (Bytes)}}{\text{Memory Bandwidth (Bytes/s)}} $$

**The Engineering Implication**:
Every time you generate a token, you are paying a massive "energy tax" to move the model's logic from HBM into compute registers. For Llama-3-8B (3.5 GB int4), an A100 80GB (2 TB/s HBM2e) generates tokens at $\approx 1.7$ ms/token. Adding more *compute cores* yields **zero** latency improvement; only faster memory (Physics) or smaller models (Algorithm) can speed up generation.
:::

*   $T_{\text{token}} \approx \frac{3.5 \text{ GB}}{3.35 \text{ TB/s}} \approx 1 \text{ ms}$ (theoretical limit).
*   Accounting for kernel launch overhead and attention computation, realized $T_{\text{token}}$ is approximately 10 ms.
*   Total decode time = $256 \text{ tokens} \times 10 \text{ ms/token} = 2.56 \text{ seconds}$.
*   **TPOT is 10 ms**, well within the 20 ms "fluidity" SLO.

#### Memory & Throughput {#sec-model-serving-systems-memory-throughput-63dd}

With 4-bit weights occupying 3.5 GB, the remaining ~76 GB of VRAM is available for the **KV Cache**. Using **PagedAttention**, we can allocate this memory with near-zero fragmentation.

*   Each token requires approximately 0.5 MB of KV cache (32 layers $\times$ 4096 dim $\times$ 2 vectors $\times$ 2-byte precision).
*   Total cache capacity $\approx \frac{72 \text{ GB}}{0.5 \text{ MB/token}} \approx 144,000 \text{ tokens}$.
*   At 1,256 tokens per request (input + output), the GPU can handle a **concurrent batch size of ~114 requests**.

#### Unit Economics {#sec-model-serving-systems-unit-economics-b685}

For an H100 SXM5 instance at approximately \$3.00 per hour (specialized cloud providers; hyperscaler rates vary from \$2-13 per hour as of 2024):
*   Total tokens per hour = $114 \text{ batch} \times \frac{3600 \text{ s/hr}}{2.68 \text{ s/req}} \times 1256 \text{ tokens/req} \approx 190 \text{ million tokens/hour}$.
*   **Cost per million tokens**: $\frac{\$3.00}{190} \approx \mathbf{\$0.015}$.

This analysis highlights that for LLMs, **memory capacity** (the size of the KV cache) is the primary determinant of throughput and cost, while **memory bandwidth** is the primary determinant of latency.

This case study applies the core principles developed throughout this chapter: latency budgets decompose into prefill and decode phases, queuing theory governs batch sizing and capacity planning, and hardware constraints in the form of memory bandwidth and capacity determine achievable performance and cost. The quantitative framework established here enables principled engineering decisions, but only when applied correctly. Common misconceptions cause even experienced engineers to misapply these principles in practice.

## Fallacies and Pitfalls {#sec-model-serving-systems-fallacies-pitfalls-336b}

Serving inverts training priorities in unexpected ways. Intuitions from batch processing fail under latency constraints and variable load, causing wasted effort, violated SLOs, and silent accuracy degradation in production.

**Fallacy:** _Faster model inference automatically means faster end-to-end serving._

Engineers assume model inference dominates serving latency. In production, preprocessing and postprocessing often consume 45 to 70 percent of total request time when inference runs on optimized accelerators, as @sec-model-serving-systems-latency-budget-ef40 demonstrates. A team reducing inference from 5ms to 2ms achieves only 23 percent end-to-end improvement if preprocessing remains at 8ms. Amdahl's Law formalizes this: if preprocessing consumes 50 percent of latency, even infinitely fast inference yields at most 2× speedup. Teams that optimize inference without profiling the complete request path waste engineering effort while the actual bottleneck remains unaddressed.

**Pitfall:** _Running serving infrastructure at high utilization to maximize cost efficiency._

Teams target 90 percent utilization to minimize idle capacity. In production, latency degrades nonlinearly as utilization approaches capacity. @eq-mm1-wait shows that at 90 percent utilization, average wait time reaches 10× service time. Moving from 70 percent to 90 percent utilization cuts infrastructure costs by 22 percent but triples average latency. For a 5ms inference service, p99 latency jumps from 25ms to 50ms. Systems provisioned for average load violate SLOs precisely when traffic increases during business-critical periods. Production systems targeting 60 to 70 percent utilization at peak load maintain the latency headroom needed to absorb traffic spikes.

**Fallacy:** _Training accuracy guarantees serving accuracy._

Engineers assume identical model weights preserve validation set performance. In production, preprocessing differences silently shift inputs outside the training distribution. @sec-model-serving-systems-trainingserving-skew-7b99 shows how training-serving skew causes accuracy degradation despite identical weights: PIL versus OpenCV resize interpolation differs subtly, float64 versus float32 normalization produces different values, or feature computation timing changes. A model achieving 95 percent validation accuracy drops to 90 percent in production from these preprocessing mismatches. Standard monitoring checking exceptions and latency violations fails to detect this silent degradation. Production systems require either identical preprocessing code for training and serving, or statistical monitoring comparing input distributions to catch drift before accuracy degrades.

**Pitfall:** _Using average latency to evaluate serving system performance._

Engineers monitor average latency because it trends smoothly and is simple to compute. In production, averages hide the slowest requests that determine user satisfaction. A system with 10ms average latency might have 200ms p99 latency, meaning 1 percent of users experience 20× worse performance. @sec-model-serving-systems-tail-latency-5376 explains how queuing variability causes this divergence: at 70 percent utilization with 5ms service time, average latency is 17ms but p99 reaches 75ms. Production SLOs specify percentile targets (p95, p99) precisely because averages mask tail behavior. Systems reporting only averages pass monitoring checks while violating user experience standards.

**Fallacy:** _Larger batch sizes always improve throughput._

Engineers maximize batch size assuming GPU saturation improves efficiency. In practice, throughput gains diminish while latency grows unbounded beyond the optimal point. For ResNet-50 on V100, increasing batch size from 16 to 32 improves throughput only 12 percent while nearly doubling inference time from 14ms to 25ms. At batch size 64, memory fragmentation reduces effective utilization to 60 percent despite the GPU appearing busy, and variable input sizes create padding overhead. @sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d shows optimal batch size depends on latency SLOs, memory constraints, and traffic patterns: for 50ms p99 targets, batch sizes above 32 routinely violate SLOs.

**Pitfall:** _Calibrating quantized models with training data rather than production traffic._

Teams calibrate with training data because it is readily available and produced validation accuracy. In production, traffic distribution often differs from training data, making calibration scale factors suboptimal. Post-training quantization determines INT8 scale factors by measuring activation ranges on calibration data, but this assumes production inputs match the calibration distribution. One production system experienced 3.2 percent accuracy loss serving wildlife camera images after calibrating with ImageNet validation data. @sec-model-compression shows quantization error scales with activation range: miscalibration amplifies errors precisely on out-of-distribution inputs. Effective quantization requires calibrating with representative samples of actual serving traffic.

**Pitfall:** _Cold start latency only matters for the first request._

Engineers optimize steady-state latency assuming most requests hit warm instances. In production, cold starts affect any request arriving after inactivity, after model updates, or during auto-scaling. Systems with bursty traffic experience cold starts on 10 to 30 percent of requests during scale-up events. ResNet-50 with TensorRT requires 30 seconds for compilation if the optimized engine is not cached; during a traffic spike triggering 10 new instances, 300 seconds of user-facing latency is added across the first requests to each instance. @sec-model-serving-systems-model-loading-initialization-cc5a shows cold start compounds weight loading, CUDA context initialization, and warmup. Systems ignoring cold start meet SLOs during steady state but violate them during scale-up events and deployment windows.

## Summary {#sec-model-serving-systems-summary-9635}

Serving marks the transition from model development to production deployment, where the optimization priorities that governed training must be inverted. The shift from throughput maximization to latency minimization transforms every system design decision. The queuing theory foundations established here reveal why this inversion is not merely a change in metrics but a change in the governing mathematics: the nonlinear relationship between utilization and latency means that systems behaving well at moderate load can suddenly violate SLOs when traffic increases modestly. Little's Law and the M/M/1 wait time equations provide the quantitative foundation for capacity planning, replacing intuition-based provisioning with engineering rigor.

Effective serving optimization requires understanding the complete request path rather than focusing exclusively on model inference. Interface protocols like gRPC and efficient serialization formats minimize the "tax" of data movement, while preprocessing often consumes 45 to 70 percent of total latency when inference runs on optimized accelerators. The microsecond-scale overheads identified by Barroso, Patterson, and colleagues explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization matters as much as model optimization. Training-serving skew represents another dimension of this complexity, silently degrading accuracy when preprocessing logic differs between training and production environments in ways that traditional testing cannot detect.

The traffic pattern analysis reveals how deployment context shapes batching strategy and system design. Server workloads with Poisson arrivals optimize dynamic batching windows, autonomous vehicles with streaming sensor data require synchronized batch formation, and mobile applications with single-user patterns eliminate batching entirely. The MLPerf scenarios codify these patterns for standardized benchmarking, connecting the serving principles established here to the measurement frameworks explored in @sec-benchmarking-ai. Precision selection and runtime optimization extend the quantization techniques from @sec-model-compression and Tensor Core capabilities from @sec-ai-acceleration into the serving domain. Finally, the translation of these technical metrics into unit economics, as shown by the Llama-3 case study, demonstrates how engineering decisions regarding batching, precision, and hardware selection directly determine the financial viability of deployment.

::: {.callout-important title="Key Takeaways"}

* **Serving inverts training priorities**: Training optimizes throughput (samples/hour); serving optimizes latency (ms/request). Different objectives require different system designs.
* **Queuing theory governs capacity planning**: At 80% utilization, wait time is 5× service time; at 90%, it's 10×. Small load increases cause disproportionate latency spikes.
* **Preprocessing dominates optimized systems**: When model inference is fast (5ms), preprocessing (image decode, tokenization) consumes 45–70% of total latency. Optimize the pipeline, not just the model.
* **Batching strategy depends on traffic pattern**: Poisson arrivals (web APIs) use dynamic batching; streaming sensors use synchronized batches; mobile apps eliminate batching entirely.
* **Training-serving skew can degrade accuracy undetected**: Different preprocessing between training and serving (e.g., resize interpolation, normalization order) shifts inputs outside the training distribution, causing accuracy degradation that conventional monitoring cannot detect. Use identical code paths.
* **KV cache optimization is critical for large language models**: Generation is memory-bound. PagedAttention and continuous batching can improve throughput 2–4× over naive serving.

:::

The serving principles established here—queuing theory for capacity planning, preprocessing optimization, batching strategy selection, and training-serving skew prevention—form the foundation for building production ML systems that meet real-world SLAs. Whether deploying a recommendation system serving millions of users or a medical AI where every millisecond affects patient outcomes, these principles translate mathematical understanding into engineering decisions that determine whether systems succeed or fail under load.

::: {.callout-chapter-connection title="From Node to Factory"}

We have optimized the single node for milliseconds, but a single node is fragile. In @sec-machine-learning-operations-mlops, we scale our perspective from the single request to the full system lifecycle, building the automated machinery that keeps production systems running through crashes, model drift, and continuous updates.

:::

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
