---
quiz: serving_quizzes.json
concepts: serving_concepts.yml
glossary: serving_glossary.json
---

# Model Serving {#sec-model-serving-systems}

::: {layout-narrow}
::: {.column-margin}
*DALL·E 3 Prompt: An illustration of a machine learning serving system, depicted as a high-speed assembly line in a modern factory. On one end, raw data (images, text documents, sensor readings) enters on conveyor belts. In the center, a glowing neural network processes inputs through preprocessing stations, a central inference engine, and postprocessing units. Workers monitor latency gauges and throughput meters. Some requests wait in small batches while others stream through individually. The scene conveys speed, precision, and the transformation from raw input to useful predictions, with visible clocks emphasizing the time-critical nature of serving.*
:::

\noindent
![](images/png/cover_serving.png)

:::

## Purpose {.unnumbered}

_Why does serving invert every optimization priority that made training successful?_

Benchmarks validated your optimizations under controlled conditions. Serving reveals whether those optimizations survive contact with production reality. Training and serving demand opposite physics. Training maximizes throughput—large batches and long epochs where latency spikes are absorbed invisibly. Serving demands immediacy—individual requests answered in milliseconds, where a single slow response is a *broken product*. Training amortizes hardware costs across billions of examples; serving pays a tax on every single request, where inefficiency compounds into massive operational debt. Training checkpoints and restarts; serving must never fail visibly. This inversion is why models that train beautifully often serve poorly: the batch-heavy architectures designed to saturate GPUs are fundamentally ill-suited for the bursty, latency-critical reality of production.

::: {.callout-tip title="Learning Objectives"}

- Explain the inversion from throughput optimization to latency minimization that distinguishes serving from training
- Decompose request latency into preprocessing, inference, and postprocessing phases to identify bottlenecks
- Apply queuing theory (Little's Law, M/M/1 models) and capacity planning to meet percentile latency SLOs
- Identify sources of training-serving skew and select appropriate prevention strategies
- Select batching and runtime strategies based on traffic patterns, latency constraints, and cost requirements
- Evaluate deployment tradeoffs across precision, throughput, and infrastructure cost

:::

```{python}
#| label: serving-setup
#| echo: false

from calc.constants import *
from calc.formulas import fmt, sci

# GPU specs
v100_tflops_fp32 = f"{V100_FLOPS_FP32.to(TFLOPs/second).magnitude:.1f}"
v100_bw = f"{V100_MEM_BW.to(GB/second).magnitude:.0f}"
a100_bw_tbs = f"{A100_MEM_BW.to(TB/second).magnitude:.1f}"
h100_bw_tbs = f"{H100_MEM_BW.to(TB/second).magnitude:.2f}"
h100_mem = f"{H100_MEM_CAPACITY.to(GiB).magnitude:.0f}"
h100_tdp = f"{H100_TDP.to(watt).magnitude:.0f}"
```

## The Serving Paradigm {#sec-model-serving-systems-serving-paradigm-9634}

Serving marks the transition from model development to production deployment, where the **Iron Law of ML Systems** (@sec-silicon-contract) undergoes a fundamental shift. Training optimized for throughput, processing massive datasets over days where the goal was samples per hour and latency spikes were absorbed invisibly. Serving must deliver predictions within milliseconds under unpredictable load. The **Latency term** ($L_{lat}$), the irreducible overhead of request scheduling, network round-trips, and system orchestration, becomes the dominant constraint rather than a rounding error. This inversion, from throughput to *latency*, from controlled conditions to *unpredictable traffic*, from offline to *real time*, defines the serving challenge. @sec-benchmarking-ai established techniques for measuring performance under controlled conditions; serving faces traffic patterns that no benchmark could anticipate. @sec-model-compression provided quantization methods that reduced model size; serving must validate that those optimizations preserve accuracy under real traffic distributions. This fundamental shift in constraints creates what we call the *serving inversion*.

::: {.callout-perspective title="The Serving Inversion"}
Applying the **DAM Taxonomy** reveals how deployment fundamentally flips your engineering priorities:

*   **Data (Information)**: In training, you maximize **Volume** (shuffling billions of samples). In serving, you maximize **Freshness** (processing one request *right now*).
*   **Algorithm (Logic)**: In training, the math is **Mutable** (updating weights via backprop). In serving, the math is **Frozen** (fixed weights, forward pass only).
*   **Machine (Physics)**: In training, you maximize **Utilization** (keeping GPUs at 100% to saturate throughput). In serving, you maximize **Headroom** (keeping GPUs at 50-70% to survive traffic spikes).
:::

The consequences of ignoring this inversion become visceral during a *traffic spike* that pushes the system beyond what it was designed to handle.

::: {.callout-example title="The 'Black Friday' Traffic Spike"}
**The Scenario**: An e-commerce recommendation system runs comfortably at 50ms latency with 1,000 queries per second (QPS).

**The Event**: On Black Friday, traffic spikes 10x to 10,000 QPS.

**The Failure**: The system does not just slow down 10x; it **collapses**. Latency hits 10 seconds, then requests start timing out. The servers are 100% utilized, but throughput drops to near zero.

**The Physics**: This is **Little's Law** and **Queueing Theory** in action. As utilization approaches 100%, queue lengths grow exponentially, not linearly. The system spends more time managing the queue (context switching, thrashing) than doing useful work.

**The Fix**:

1.  **Load Shedding**: Reject excess requests immediately to keep the queue short.
2.  **Autoscaling**: Spin up more replicas *before* utilization hits the "knee" of the curve.
3.  **Degradation**: Serve cached/dumber recommendations to reduce compute cost per query.
:::

As shown in @fig-tail-latency-explosion, this physics dictates *why* production systems must run at relatively low utilization (40-60%) to guarantee stable tail latency (p99).

```{python}
#| label: fig-tail-latency-explosion
#| echo: false
#| fig-cap: "**The Tail Latency Explosion**: Request Latency vs. System Utilization ($\\rho$). While median latency (Blue) remains stable, tail latency (Red, p99) explodes exponentially once utilization passes the 'Knee' at ~70%. This physics-driven behavior dictates that production serving clusters must maintain headroom (40-60% utilization) to guarantee stable response times and avoid queue collapse under stochastic load."
#| fig-alt: "Line plot showing latency growing with utilization. Blue line (Median) is flat then rises. Red line (Tail) curves upward sharply at 70% utilization. Shaded regions indicate 'Safe Zone' and 'Danger Zone'."

import sys
import matplotlib.pyplot as plt
# Ensure calc directory is in path
sys.path.append('../../../calc')
import viz

viz.set_book_style()
viz.plot_tail_latency()
plt.show()
```

Beyond the technical limits of latency, the economics of serving have undergone a radical transformation. As models become more efficient and hardware becomes more specialized, the cost of "intelligence" is collapsing. This trend is visualized in @fig-intelligence-deflation, which tracks the plummeting price of token generation across model generations.

```{python}
#| label: fig-intelligence-deflation
#| echo: false
#| fig-cap: "**Intelligence Deflation**: Cost per 1M tokens (USD) over time (Log Scale). The cost of token generation has collapsed by multiple orders of magnitude (2020–2025). Initially driven by OpenAI's GPT series, the market has entered a phase of intense price competition with entrants like Anthropic (Claude), Google (Gemini), and DeepSeek pushing costs below $0.10/million tokens. This hyper-deflation transforms the economics of automated AI workflows."
#| fig-alt: "Line plot showing token pricing collapsing from $20/M tokens in 2020 to <$0.10/M tokens in 2025. Log scale highlights the deflationary trend with models from OpenAI, Anthropic, Google, and DeepSeek."

import sys
import matplotlib.pyplot as plt
# Ensure calc directory is in path
sys.path.append('../../../calc')
import viz

viz.set_book_style()
viz.plot_intelligence_deflation()
plt.show()
```

These priorities motivate a precise definition of the discipline this chapter addresses.

::: {.callout-definition title="Model Serving"}

***Model Serving*** is the operational phase that inverts the **Throughput** priority of training into a **Latency** constraint. It requires a distinct architectural stack designed to minimize the **Tail Latency** of individual inferences under stochastic load, bounded by the **Service Level Objective (SLO)**.

:::

Serving systems must execute a complete inference pipeline under latency constraints, not just the neural network computation. @fig-serving-inference-pipeline illustrates this pipeline: raw inputs flow through preprocessing (traditional computing), neural network inference (deep learning), and postprocessing (traditional computing) before producing final outputs. Each stage contributes to total latency, and bottlenecks can occur anywhere in the pipeline.

::: {#fig-serving-inference-pipeline fig-env="figure" fig-pos="htb" fig-cap="**The Inference Pipeline**: ML serving systems transform raw inputs into final outputs through sequential stages: preprocessing, neural network computation, and postprocessing. The neural network represents just one component; preprocessing and postprocessing rely on traditional computing and often dominate total latency in optimized systems." fig-alt="Flow diagram showing six connected boxes: Raw Input, Preprocessing, Neural Network, Raw Output, Postprocessing, Final Output. Preprocessing and postprocessing are labeled Traditional Computing; neural network is labeled Deep Learning."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n},line width=0.75pt]
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=3pt,
    node distance=0.6,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    align=flush center,
    minimum width=15mm,
    minimum height=10mm
  },
}
%
\node[Box](B1){Raw\\ Input};
\node[Box,right=of B1](B2){Pre-processing};
\node[Box,node distance=1, right=of B2,fill=BlueL,draw=BlueLine](B3){Neural\\ Network};
\node[Box,node distance=1, right=of B3,fill=VioletL2,draw=VioletLine2](B4){Raw\\ Output};
\node[Box,right=of B4,fill=VioletL2,draw=VioletLine2](B5){Post-processing};
\node[Box, right=of B5,fill=VioletL2,draw=VioletLine2](B6){Final\\ Output};
%
\draw[Line,-latex](B1)--(B2);
\draw[Line,-latex](B2)--(B3);
\draw[Line,-latex](B3)--(B4);
\draw[Line,-latex](B4)--(B5);
\draw[Line,-latex](B5)--(B6);
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=3mm,inner ysep=5mm,yshift=2mm,
            fill=BackColor,fit=(B1)(B2),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Traditional Computing};
%
\scoped[on background layer]
\node[draw=OrangeLine,inner xsep=4mm,inner ysep=5mm,yshift=2mm,
            fill=OrangeL!70!red!10,fit=(B3),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Deep Learning};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=3mm,inner ysep=5mm,yshift=2mm,
            fill=BackColor,fit=(B4)(B6),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Traditional Computing};
\end{tikzpicture}
```
:::

This chapter explains how to build systems that orchestrate this pipeline efficiently. The chapter proceeds in three parts. **Part 1** establishes system fundamentals: serving architectures, server anatomy, and the protocols that connect clients to models. **Part 2** follows the request lifecycle: where latency accumulates across preprocessing, inference, and postprocessing, then how queuing dynamics govern system behavior under load. **Part 3** addresses optimization: model lifecycle management ensures models are ready to serve, batching strategies maximize throughput, LLM-specific techniques handle generative workloads, runtime selection tunes performance, and economics translate these choices into infrastructure decisions.

### Static vs Dynamic Inference {#sec-model-serving-systems-static-vs-dynamic-inference-e864}

The first architectural decision in any serving system is whether predictions happen before or during user requests [@google2024staticdynamic]. This choice shapes system design, cost structure, and capability boundaries.

**Static inference** (also called offline or batch inference) pre-computes predictions for anticipated inputs and stores them for retrieval. Consider a recommendation system that generates predictions for all user-item pairs nightly. When a user requests recommendations, the system retrieves pre-computed results from a lookup table rather than running inference. This approach eliminates inference latency entirely since results already exist, enables quality verification before deployment, and reduces serving costs. However, static inference cannot handle novel inputs that were not anticipated during the batch computation and introduces hours or days of latency when models update.

**Dynamic inference** (also called online or real-time inference) computes predictions on demand when requests arrive. This handles any input, including rare edge cases and novel combinations, and immediately reflects model updates. The cost is strict latency requirements that may force simpler models and more intensive monitoring infrastructure.

```{python}
#| label: static-batch-calc
#| echo: false

# Static batch inference timing
n_photos = 10_000
inference_ms = 5
batch_total_s = n_photos * inference_ms / 1000
batch_total_s_str = f"{batch_total_s:.0f}"
```

For our ResNet-50 image classifier, consider two deployment scenarios. A **static approach** suits a photo organization app that pre-classifies all images in a user's library overnight. With `{python} f"{n_photos:,}"` photos and `{python} inference_ms`ms inference each, batch processing takes ~`{python} batch_total_s_str` seconds total, and users see instant classification when browsing. A **dynamic approach** suits a content moderation API that must classify user-uploaded images in real-time, with each image requiring the full preprocessing→inference→postprocessing pipeline and a 100ms latency budget. Most production image classification systems use a **hybrid approach**: frequently requested images (popular products, known memes) are pre-classified and cached, while novel uploads trigger dynamic inference.

The choice between static and dynamic serving has direct economic implications. Stricter latency requirements directly translate into higher infrastructure costs, creating a tradeoff that serving system architects must navigate carefully.

::: {.callout-notebook #notebook-cost-latency title="The Cost of Latency"}

```{python}
#| label: cost-latency-calc
#| echo: false

# Cost of latency tradeoff
gpu_cost_per_hour = 4.0  # $/hour

# Scenario A: batch size 1
latency_a_ms = 5
throughput_a_rps = 200
queries_per_hour_a = throughput_a_rps * 3600
cost_per_million_a = gpu_cost_per_hour / (queries_per_hour_a / 1_000_000)

# Scenario B: batch size 8
latency_b_ms = 10
throughput_b_rps = 800
queries_per_hour_b = throughput_b_rps * 3600
cost_per_million_b = gpu_cost_per_hour / (queries_per_hour_b / 1_000_000)

# Cost increase
cost_increase_pct = (cost_per_million_a / cost_per_million_b - 1) * 100
cost_ratio = cost_per_million_a / cost_per_million_b

cost_a_str = f"{cost_per_million_a:.2f}"
cost_b_str = f"{cost_per_million_b:.2f}"
cost_increase_str = f"{cost_increase_pct:.0f}"
cost_ratio_str = f"{cost_ratio:.0f}"
```

Latency constraints directly dictate infrastructure costs. Consider a GPU server renting for \$`{python} f"{gpu_cost_per_hour:.0f}"`/hour.

**Scenario A (Low Latency):** Batch size 1.

*   Latency: `{python} latency_a_ms`ms.
*   Throughput: `{python} throughput_a_rps` req/s.
*   Cost per million queries: **\$`{python} cost_a_str`**.

**Scenario B (High Throughput):** Batch size 8.

*   Latency: `{python} latency_b_ms`ms (doubled due to batching overhead).
*   Throughput: `{python} throughput_b_rps` req/s (quadrupled due to parallel efficiency).
*   Cost per million queries: **\$`{python} cost_b_str`**.

**The Trade-off:** Reducing latency from `{python} latency_b_ms`ms to `{python} latency_a_ms`ms increases the hardware bill by **`{python} cost_increase_str`%**. Engineers must quantify whether that `{python} latency_a_ms`ms speedup generates enough business value to justify the `{python} cost_ratio_str`x cost increase.

:::

Most production systems combine both approaches. Common queries hit a cache populated by batch inference while uncommon requests trigger dynamic computation. Understanding this spectrum is essential because it determines which subsequent optimization strategies apply. Static inference optimizes for throughput during batch computation and storage efficiency for serving. Dynamic inference optimizes for per-request latency under concurrent load, which requires understanding *where* time goes within each request.

The static-versus-dynamic decision is just the first of several architectural choices that shape serving system design. Equally important is *where* the model executes, since deployment context fundamentally constrains every subsequent optimization.

### The Spectrum of Serving Architectures {#sec-model-serving-systems-spectrum-serving-architectures-8966}

While "serving" often implies a networked server processing API requests, the architectural pattern varies fundamentally by deployment environment. This spectrum matters for Volume I's focus on mastering the ML node, since the same model may require radically different serving strategies depending on *where* it executes.

**1. Networked Serving (Cloud/Datacenter)**

The model runs as a standalone service (microservice). The primary interface is the network (HTTP/gRPC). Optimization focuses on **throughput** (batching) and **concurrency**.

*   *Key Constraint:* Network bandwidth and serialization cost.
*   *Typical Hardware:* NVIDIA GPUs (V100, A100, H100), Google TPUs, AWS Inferentia.
*   *Cold Start:* Seconds to minutes (container startup, model loading, warmup).

**2. Application-Embedded Serving (Mobile/Edge)**

The model runs within the user application process (e.g., a smartphone app using CoreML or TensorFlow Lite). There is no "server." The interface is a function call. Optimization focuses on **energy** and **responsiveness** (SingleStream).

*   *Key Advantage:* **Zero-Copy Inference**. When data moves through a system, each copy consumes CPU cycles and memory bandwidth. In cloud serving, a camera frame might be copied four times: from network buffer to application memory, then to a preprocessing buffer, then to GPU-accessible memory, and finally to GPU VRAM. Mobile NPUs can eliminate most of these copies by sharing memory directly with the camera hardware. The camera writes pixels into a buffer that the NPU reads directly, avoiding the CPU entirely. This reduces both latency (no copy operations) and energy (memory copies consume significant power). The mechanism requires hardware support: the camera, CPU, and NPU must share a unified memory architecture, which modern mobile SoCs like Apple's M-series and Qualcomm Snapdragon provide.
*   *Typical Hardware:* Mobile NPUs (Apple Neural Engine, Qualcomm Hexagon), embedded GPUs (Jetson).
*   *Cold Start:* Milliseconds (model already in app memory); first inference may trigger JIT compilation (100-500ms).
*   *Power Budget:* 1-5W sustained, with thermal throttling after prolonged inference.

**3. Bare-Metal Serving (TinyML)**

The model is compiled into the firmware of a microcontroller. There is no operating system or dynamic memory allocator. "Serving" is a tight loop reading sensors and invoking the interpreter. Optimization focuses on **static memory usage** (fitting in SRAM).

*   *Key Difference:* All memory is pre-allocated (Tensor Arena). Dynamic batching is impossible.
*   *Typical Hardware:* ARM Cortex-M series, ESP32, specialized TinyML accelerators.
*   *Cold Start:* Microseconds (model weights in flash, tensor arena pre-allocated).
*   *Power Budget:* Microwatts to milliwatts; battery operation for months or years.

@tbl-serving-spectrum summarizes *how* these deployment contexts shape serving system design:

| **Characteristic** | **Cloud/Datacenter** | **Mobile/Edge** | **TinyML** |
|:---|:---|:---|:---|
| **Latency Target** | 10-100ms | 20-50ms | 1-100ms |
| **Batch Size** | 1-128 (dynamic) | 1 (fixed) | 1 (fixed) |
| **Memory** | 16-80GB VRAM | 2-8GB shared | 256KB-2MB SRAM |
| **Power** | 300-700W | 1-10W | 1-100mW |
| **Update Mechanism** | Container deploy | App store update | Firmware OTA |
| **Failure Mode** | Retry/failover | Graceful degradation | Silent or reset |
| **Monitoring** | Full telemetry | Limited analytics | Heartbeat only |

: **Serving Architecture Spectrum**: The deployment context fundamentally shapes every aspect of serving system design. Cloud systems optimize for throughput with dynamic batching; mobile systems optimize for energy with fixed batch-1; TinyML systems operate under extreme memory and power constraints with no dynamic allocation. {#tbl-serving-spectrum}

To make these architectural differences concrete, consider *how* a single model must adapt to each deployment context:

::: {.callout-perspective #perspective-resnet-serving title="ResNet-50 Across the Serving Spectrum"}

The same ResNet-50 architecture requires dramatically different serving strategies across deployment contexts:

**Cloud (V100 GPU):**

- Model format: TensorRT FP16 engine (49MB)
- Inference: 1.4ms at batch-1, 14ms at batch-16
- Throughput: 1,143 images/second (batched)
- Memory: 2GB VRAM (model + activations for batch-32)

**Mobile (Pixel 6 NPU):**

- Model format: TensorFlow Lite INT8 (25MB)
- Inference: 12ms at batch-1 (NPU), 45ms (CPU fallback)
- Throughput: ~80 images/second (single-stream)
- Memory: 150MB peak (shared with app)
- Energy: 0.8mJ per inference (NPU), 4.2mJ (CPU)

**TinyML (Cortex-M7):**

- Model format: Not feasible; ResNet-50 requires 98MB weights
- Alternative: MobileNetV2-0.35 quantized to INT8 (1.4MB)
- Inference: 120ms at batch-1
- Throughput: ~8 images/second
- Memory: 320KB tensor arena (fits in 512KB SRAM)
- Energy: 12mJ per inference

**Key insight**: The "same model" claim is misleading: each deployment requires not just different optimization but often different architectures entirely. TinyML serving cannot use ResNet-50; it requires architectures designed for the constraints from the start.

:::

### The Load Balancer Layer {#sec-model-serving-systems-load-balancer-layer-9c4d}

The preceding spectrum focused on *how* deployment context shapes serving constraints. For cloud and datacenter deployments, where multiple replicas serve the same model, an additional infrastructure layer is required: the load balancer. Production serving systems place load balancers between clients and model servers, providing three essential functions for serving infrastructure.

**Request Distribution** routes incoming requests to available model replicas using algorithms like round-robin or least-connections. For latency-sensitive ML serving, algorithms that route away from slow or overloaded replicas improve tail latency.

**Health Monitoring** continuously verifies that replicas are ready to serve, routing traffic away from unhealthy instances. For ML systems, health checks must verify not just process liveness but model readiness, confirming that weights are loaded and warmup is complete.

**Deployment Support** enables safe model updates by gradually shifting traffic between versions. @sec-machine-learning-operations-mlops examines deployment strategies including canary testing, blue-green deployments, and shadow mode validation.

For single-machine serving with multiple model instances, such as running several ONNX Runtime sessions, the framework and operating system handle request queuing. The full complexity of load balancing becomes essential when scaling to distributed inference systems, where multiple machines serve the same model. The implementation details of request distribution algorithms and multi-replica architectures belong to that distributed context.

**Impact on Queuing Analysis**: When capacity planning considers "the server" in this chapter, it means the single machine's model serving capacity. The queuing dynamics analyzed in @sec-model-serving-systems-queuing-theory-tail-latency-29a6 apply to understanding single-machine behavior and determining when scaling to multiple machines becomes necessary.

While load balancers distribute requests across replicas, achieving predictable latency also requires controlling what happens *within* each machine. The operating system environment introduces its own sources of variability.

### Deterministic Latency and Resource Isolation {#sec-model-serving-systems-deterministic-latency-resource-isolation-4d1c}

An inference server does not operate in isolation. On a single machine, the operating system manages multiple competing processes (logging agents, monitoring tools, and system interrupts) that can intermittently steal CPU cycles from the inference pipeline. These "noisy neighbors" are a primary source of **latency jitter**, where the time required to process identical requests varies significantly, causing the 99th percentile (P99) latency to spike even when the hardware is under-utilized.

To achieve deterministic performance on a single node, systems engineers employ three primary isolation techniques:

1.  **CPU Affinity (Pinning)**: Restricting the inference server's threads to specific physical CPU cores. This prevents the operating system from context-switching the server's processes, ensuring that the "preprocessing" stage of the pipeline always has immediate access to computational resources.
2.  **Memory Locking (`mlock`)**: Instructing the OS to lock the model weights and KV caches in physical RAM. This prevents the system from "paging out" model data to slow disk storage during periods of high memory pressure, ensuring consistent microsecond-scale access times.
3.  **Interrupt Shielding**: Configuring the system to route network and storage interrupts to CPU cores not used by the inference runner. This ensures that a burst of incoming network traffic does not interrupt the GPU's command stream, protecting the "heartbeat" of the inference execution.

These isolation principles transform a simple "model script" into a **deterministic service**, a transition essential for safety-critical applications like autonomous driving or real-time industrial control.

## Serving System Architecture {#sec-model-serving-systems-serving-system-architecture-4879}

The preceding section established the architectural spectrum from cloud to TinyML and the infrastructure layers that route requests to models. Building a high-performance serving system requires coordinating multiple software components to minimize overhead and maximize hardware utilization. This section examines the internal architecture of inference servers and the protocols that connect them to clients.

### Internal Architecture and Request Flow {#sec-model-serving-systems-anatomy-inference-server-f12e}

While model optimization focuses on the mathematical artifact, model serving requires a specialized software architecture to manage high-frequency request streams and hardware utilization. An inference server[^fn-inference-server] (such as NVIDIA Triton, TensorFlow Serving, or TorchServe) is not a simple wrapper around a model script; it is a high-performance scheduler that manages concurrency, memory, and data movement.

[^fn-inference-server]: **Inference Server**: The concept emerged from Google's TensorFlow Serving [@olston2017tensorflow], open-sourced February 2016, which pioneered the separation of model logic from serving infrastructure. NVIDIA's Triton [@nvidia2024triton], originally TensorRT Inference Server with GA release in March 2019, extended this to multi-framework support. These servers implement dynamic batching that can improve GPU utilization by up to 70% compared to naive single-request serving. The architecture mirrors the separation of concerns in traditional web servers like Apache (1995) and nginx (2004), applying decades of distributed systems knowledge to ML deployment.

The internal anatomy of these servers reveals *how* they bridge the gap between irregular user traffic and the highly regular, batch-oriented requirements of accelerators.

**The Request Pipeline.** Every request traverses a multi-stage pipeline designed to maximize hardware throughput while minimizing latency overhead. @fig-server-anatomy visualizes this internal flow.

::: {#fig-server-anatomy fig-env="figure" fig-pos="htb" fig-cap="**Inference Server Anatomy**: A modern inference server decouples network handling from accelerator execution through a staged pipeline. Each stage isolates a concern, from absorbing bursty traffic to forming efficient batches, so the hardware accelerator stays highly utilized despite irregular arrival patterns." fig-alt="Flowchart showing 6-stage inference server pipeline: Client to Network Ingress to Request Queue (cylinder) to Dynamic Batcher, then down to Inference Runner to Accelerator. Arrows connect stages sequentially."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.2cm]
  \tikzset{
    Box/.style={draw=black!70, thick, rounded corners=2pt, align=center, minimum width=2.5cm, minimum height=1cm},
    Hardware/.style={Box, fill=gray!10},
    Software/.style={Box, fill=blue!10},
    Queue/.style={draw=black!70, thick, shape=cylinder, shape border rotate=90, aspect=0.25, minimum width=1.5cm, minimum height=1.2cm, fill=yellow!10}
  }

  \node[Box, fill=white] (client) {Client\\(Request)};
  \node[Software, right=of client] (ingress) {Network Ingress\\(HTTP/gRPC)};
  \node[Queue, right=of ingress] (queue) {Request\\Queue};
  \node[Software, right=of queue] (scheduler) {Dynamic\\Batcher};
  \node[Software, below=1.5cm of scheduler] (runtime) {Inference Runner\\(TensorRT/ONNX)};
  \node[Hardware, below=1.0cm of runtime] (gpu) {Accelerator\\(GPU/TPU)};

  \draw[->, thick] (client) -- (ingress);
  \draw[->, thick] (ingress) -- (queue);
  \draw[->, thick] (queue) -- (scheduler);
  \draw[->, thick] (scheduler) -- (runtime);
  \draw[->, thick] (runtime) -- (gpu);

  % Labels
  \node[right=0.2cm of queue, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Request Buffering};
  \node[right=0.2cm of scheduler, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Throughput Opt.};
  \node[right=0.2cm of runtime, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Execution Opt.};

\end{tikzpicture}
```
:::

The server architecture serves three critical functions:

1.  **Concurrency Management**: Servers use asynchronous event loops or thread pools to handle thousands of concurrent client connections without blocking. This ensures that network I/O wait times do not idle the accelerator.
2.  **Request Transformation**: The server handles the conversion of network payloads (JSON/Protobuf) into the specific tensor formats required by the optimized model runtime. Image tensors, for example, can be stored as NCHW (batch, channels, height, width) or NHWC (batch, height, width, channels). PyTorch and TensorRT prefer NCHW because it places channel data contiguously, enabling efficient convolution on GPUs. TensorFlow defaults to NHWC, which is more efficient on CPUs. A format mismatch between client and server silently corrupts inference: the model interprets pixel rows as color channels, producing garbage outputs without raising errors.
3.  **Model Management**: Servers manage the lifecycle of models, including loading weights into VRAM, managing versioning, and ensuring that "warm-up" inferences are completed before exposing the model to live traffic.

Of these components, the scheduler deserves special attention because it embodies the fundamental serving tradeoff between throughput and latency.

### The Scheduler: Where Throughput Meets Latency {#sec-model-serving-systems-scheduler-throughput-meets-latency-d022}

The **Scheduler** is the "brain" of the inference server. It implements the dynamic batching logic discussed in @sec-model-serving-systems-throughput-optimization-18d1. The scheduler must decide: "*Should* I run this one request now to minimize its latency, or wait 5 milliseconds for a second request to arrive and process them together to maximize throughput?"

Systems designers use the **Batching Window** parameter to tune this trade-off. A window of 0ms optimizes for pure latency (no batching), while a window of 10-50ms is common for high-throughput cloud services. This decision determines the "duty cycle" of the GPU, the percentage of time the hardware is actually computing versus waiting for work.

### Interface Protocols and Serialization {#sec-model-serving-systems-interface-protocols-serialization-5510}

The mechanism used to transport data between client and server directly affects the latency budget. While model inference is often highly optimized, the cost of moving data into the model (serialization and network protocol overhead) can become the dominant bottleneck, especially for lightweight models where inference time is small.

**The Serialization Bottleneck.** Text-based formats like JSON are ubiquitous but computationally expensive. Parsing a JSON object requires reading every byte, validating syntax, and converting text representations into machine-native types. For high-throughput systems, this consumes CPU cycles that could otherwise be used for request handling or preprocessing.

Binary formats like Protocol Buffers (Protobuf) or FlatBuffers reduce this overhead by designing the wire format to map directly to in-memory data structures. This enables "zero-copy" deserialization in optimal cases, where the network buffer can be used directly without allocating new memory.

#### REST vs gRPC {#sec-model-serving-systems-rest-vs-grpc-c7b7}

Two dominant paradigms define modern serving interfaces, each with distinct system characteristics:

**REST (Representational State Transfer)** typically uses HTTP/1.1 and JSON. It is universally supported, human-readable, and stateless, making it the default choice for public-facing APIs. However, standard HTTP/1.1 requires a new TCP handshake for each request (unless keep-alive is carefully tuned), and JSON serialization adds significant latency for numerical data like tensors.

**gRPC (gRPC Remote Procedure Call)**[^fn-grpc] uses HTTP/2 and Protobuf. HTTP/2 enables multiplexing multiple requests over a single persistent TCP connection, eliminating handshake latency and allowing efficient binary streaming. Protobuf provides strict type safety and efficient binary serialization, making it the standard for internal service-to-service communication where latency is critical.

[^fn-grpc]: **gRPC**: Open-sourced by Google in February 2015, gRPC evolved from Stubby, Google's internal RPC framework that had been handling tens of billions of calls per second across their datacenters since approximately 2001. The combination of HTTP/2 multiplexing and Protocol Buffers binary serialization achieves roughly 10x lower serialization overhead than REST/JSON, making it the de facto standard for latency-sensitive ML inference APIs.

The following example compares *JSON vs Protobuf serialization*.

::: {.callout-notebook title="JSON vs Protobuf Serialization"}

Consider a request payload containing 1,000 floating point numbers (e.g., an embedding vector).

*   **JSON**: Uses ~9 KB on the wire. Requires ~50$\mu$s to parse.
*   **Protobuf**: Uses ~4 KB on the wire. Requires ~5$\mu$s to parse.

For a system processing 10,000 requests per second, switching to Protobuf saves nearly half a core of CPU time just in serialization overhead. This 10$\times$ efficiency gain makes gRPC essential for high-throughput internal microservices.

:::

**System Choice**: Use REST for public APIs to maximize developer accessibility. Use gRPC for high-performance internal communication to minimize the serialization tax.

The architectural components and protocols examined so far describe *how* serving systems are built. Understanding *why* certain configurations perform better requires analyzing what happens to individual requests as they traverse these components.

## The Request Lifecycle {#sec-model-serving-systems-request-lifecycle-d9c6}

With the serving architecture established, we now trace *what* happens to a single request as it flows through the system. Understanding *where* time goes within each request is essential for effective optimization: you cannot improve what you do not measure.

### The Latency Budget {#sec-model-serving-systems-latency-budget-ef40}

For dynamic inference systems, the fundamental difference from training lies in optimization objectives. Training optimizes throughput: maximizing samples processed per hour over days of computation. A training job that processes 1000 samples per second is successful regardless of how long any individual sample takes. Serving inverts this priority, optimizing latency (introduced in @sec-ml-system-architecture) per request under strict time constraints. A serving system with 1000ms per-request latency has failed, even if it achieves excellent throughput.

This shift has concrete implications for system design [@gujarati2020serving]. The metrics that matter change from aggregate throughput to latency distributions. Mean latency tells you little about user experience; p50, p95, and p99 latencies reveal *how* the system performs across the full range of requests. If your mean latency is 50ms but p99 is 2 seconds, one in a hundred users waits 40 times longer than average. For consumer-facing applications, these tail latencies often determine user satisfaction and retention.[^fn-tail-latency-impact]

[^fn-tail-latency-impact]: **Tail Latency Impact**: Research at Google and Amazon in the mid-2000s established that users are more sensitive to latency variance than mean latency. Industry experience suggests that latency increases of 100ms can measurably impact user engagement and conversion rates for e-commerce applications, though the magnitude varies by context. This is why service level objectives (SLOs) typically specify percentile targets rather than averages.

Managing these percentile constraints requires decomposing the total allowed response time into a *latency budget* that allocates time across each processing phase.

::: {.callout-definition title="Latency Budget"}

***Latency Budget*** is the **Time Capital** allocated to a request, strictly bounded by the **End-to-End SLA**. It acts as a zero-sum constraint system where any milliseconds consumed by **Serialization** or **Network Overhead** directly reduce the computational budget available for Model Inference.

:::

We can visualize this by examining a *ResNet-50 latency budget breakdown*.

::: {.callout-notebook title="ResNet-50: Latency Budget Breakdown"}
Serving is about optimizing the **Tail Latency** under load.

**The Physics of Latency**

- [ ] **Queuing Theory**: Can you explain why latency spikes non-linearly as utilization approaches 100%? (Hint: The M/M/1 queue model).
- [ ] **Batching Trade-offs**: Why does increasing batch size improve throughput (images/sec) but degrade latency (ms/request)?

**Optimization Targets**

- [ ] **The Bottleneck**: In a highly optimized inference server, why does **Preprocessing** often consume more time than the model itself?
:::

Every serving request decomposes into three phases that each consume part of the latency budget. Preprocessing transforms raw input such as image bytes or text strings into model-ready tensors. Inference executes the model computation. Postprocessing transforms model outputs into user-facing responses.

A common misconception is that faster hardware automatically means faster serving. In practice, preprocessing and postprocessing often dominate total latency. Studies of production systems show preprocessing consuming 60 to 70 percent of total request time when inference runs on optimized accelerators [@nvidia2024tritontutorial]. Optimizing only the inference phase yields diminishing returns when the surrounding pipeline remains bottlenecked on CPU operations.

### Latency Distribution Analysis {#sec-model-serving-systems-latency-distribution-analysis-b0f8}

Understanding *where* time goes requires instrumenting each phase independently. Consider *what* happens when our ResNet-50 classifier receives a JPEG image:

::: {.callout-notebook title="ResNet-50: Latency Budget Breakdown"}

A typical serving request for our ResNet-50 classifier shows the following latency distribution:

```{python}
#| label: latency-table-calc
#| echo: false

# Latency components (ms)
l_jpeg = 3.0
l_resize = 1.0
l_norm = 0.5
l_transfer = 0.5
l_inf = 5.0
l_post = 0.1

l_total = l_jpeg + l_resize + l_norm + l_transfer + l_inf + l_post

# Percentages
p_jpeg = l_jpeg / l_total * 100
p_resize = l_resize / l_total * 100
p_norm = l_norm / l_total * 100
p_transfer = l_transfer / l_total * 100
p_inf = l_inf / l_total * 100
p_post = l_post / l_total * 100

# Strings
l_jpeg_str = f"{l_jpeg:.1f}ms"
l_resize_str = f"{l_resize:.1f}ms"
l_norm_str = f"{l_norm:.1f}ms"
l_transfer_str = f"{l_transfer:.1f}ms"
l_inf_str = f"{l_inf:.1f}ms"
l_post_str = f"{l_post:.1f}ms"
l_total_str = f"{l_total:.1f}ms"

p_jpeg_str = f"{p_jpeg:.0f}%"
p_resize_str = f"{p_resize:.0f}%"
p_norm_str = f"{p_norm:.0f}%"
p_transfer_str = f"{p_transfer:.0f}%"
p_inf_str = f"{p_inf:.0f}%"
p_post_str = f"~{p_post:.0f}%"
```

| **Phase** | **Operation** | **Time** | **Percentage** |
|:---|:---|:---|:---|
| **Preprocessing** | JPEG decode | `{python} l_jpeg_str` | `{python} p_jpeg_str` |
| **Preprocessing** | Resize to 224×224 | `{python} l_resize_str` | `{python} p_resize_str` |
| **Preprocessing** | Normalize (mean/std) | `{python} l_norm_str` | `{python} p_norm_str` |
| **Data Transfer** | CPU→GPU copy | `{python} l_transfer_str` | `{python} p_transfer_str` |
| **Inference** | **ResNet-50 forward pass** | **`{python} l_inf_str`** | **`{python} p_inf_str`** |
| **Postprocessing** | Softmax + top-5 | `{python} l_post_str` | `{python} p_post_str` |
| **Total** |  | **`{python} l_total_str`** | **100%** |

```{python}
#| label: latency-budget-calc
#| echo: false

# ResNet-50 latency budget breakdown
jpeg_decode_ms = 3.0
resize_ms = 1.0
normalize_ms = 0.5
cpu_gpu_ms = 0.5
resnet_inference_ms = 5.0
postprocess_ms = 0.1

preprocess_ms = jpeg_decode_ms + resize_ms + normalize_ms
total_latency_ms = preprocess_ms + cpu_gpu_ms + resnet_inference_ms + postprocess_ms
preprocess_pct = preprocess_ms / total_latency_ms * 100

# TensorRT optimization scenario
tensorrt_inference_ms = 2.0
tensorrt_total_ms = preprocess_ms + cpu_gpu_ms + tensorrt_inference_ms + postprocess_ms
tensorrt_preprocess_pct = preprocess_ms / tensorrt_total_ms * 100

total_latency_str = f"{total_latency_ms:.1f}"
preprocess_pct_str = f"{preprocess_pct:.0f}"
tensorrt_preprocess_pct_str = f"{tensorrt_preprocess_pct:.0f}"
```

Key insight: **Preprocessing consumes `{python} preprocess_pct_str`% of latency** despite model inference being the computationally intensive phase. With TensorRT optimization reducing inference to `{python} f"{tensorrt_inference_ms:.0f}"`ms, preprocessing would dominate at `{python} tensorrt_preprocess_pct_str`%.

:::

The ResNet example represents compute-bound inference where math dominates. Recommendation systems reveal a different bottleneck profile entirely.

::: {.callout-lighthouse title="Lighthouse Example: DLRM Serving"}

**The Scenario**: Serving a Recommendation System (DLRM) with a 10ms P99 latency budget.

**The Contrast**: While ResNet-50 serving is limited by **Math** (CNN ops), DLRM serving is strictly limited by **I/O** and **Memory Capacity**.

```{python}
#| label: dlrm-latency-calc
#| echo: false

# DLRM latency breakdown
dlrm_input = 0.5
dlrm_embed = 6.0
dlrm_mlp = 1.5
dlrm_post = 1.0
dlrm_total = dlrm_input + dlrm_embed + dlrm_mlp + dlrm_post

dlrm_input_str = f"{dlrm_input}ms"
dlrm_embed_str = f"{dlrm_embed}ms"
dlrm_mlp_str = f"{dlrm_mlp}ms"
dlrm_post_str = f"{dlrm_post}ms"
dlrm_total_str = f"{dlrm_total}ms"
```

| **Phase** | **Operation** | **Time** | **Bottleneck** |
|:---|:---|:---|:---|
| **Input Parsing** | Request parsing | `{python} dlrm_input_str` | CPU |
| **Embedding Look** | **Fetch 100+ dense vectors** | **`{python} dlrm_embed_str`** | **Memory BW** |
| **Inference** | MLP forward pass | `{python} dlrm_mlp_str` | Compute |
| **Postprocessing** | Ranking & Filtering | `{python} dlrm_post_str` | CPU |
| **Total** |  | **`{python} dlrm_total_str`** |  |

**Key Systems Insight**:
In DLRM, the "Inference" (MLP) is only ~15% of the latency. The majority of time is spent in **Embedding Lookups**—retrieving massive 128-dim vectors from terabyte-scale tables. This is an **IO-bound** workload where adding more GPUs doesn't help unless you also add more **Memory Bandwidth** and **Capacity**.
:::

::: {.content-hidden}

David Patterson famously argued that general-purpose CPUs are inefficient for ML because they dedicate significant silicon area to complex logic (branch prediction, out-of-order execution) that is unnecessary for the regular, data-parallel patterns of neural networks.

For serving, this efficiency gap is magnified. A CPU executing an inference request at batch size 1 might achieve only 1–2% of its theoretical peak performance because the "killer microseconds" of instruction fetch and decode dominate the few arithmetic operations performed per token or pixel.

**Domain-Specific Architectures (DSAs)**, like Google’s TPU or NVIDIA’s Tensor Cores, solve this by replacing complex instruction logic with massive arrays of simple Multiply-Accumulate (MAC) units. This specialization allows a DSA to achieve 10–100$\times$ higher **Arithmetic Intensity** (the ratio of compute to memory access) even at the small batch sizes required for low-latency serving. Understanding this architectural advantage explains why hardware acceleration is not just a speedup, but a requirement for economically viable serving.

:::

This breakdown reveals why straightforward optimization efforts often fail. Engineers focus on model optimization (quantization, pruning) because that is where ML expertise applies, but the actual bottleneck is image decoding running on CPU. Adopting *the quantitative approach to serving* exposes these hidden bottlenecks before engineering effort is misallocated.

::: {.callout-notebook title="The Quantitative Approach to Serving"}

```{python}
#| label: amdahl-serving-calc
#| echo: false

# Amdahl's Law: model optimization impact
non_model_ms = preprocess_ms + cpu_gpu_ms  # preprocessing + transfer
non_model_pct = non_model_ms / total_latency_ms * 100
model_10x_ms = resnet_inference_ms / 10  # 10× faster model
optimized_total_ms = non_model_ms + model_10x_ms + postprocess_ms
amdahl_speedup = total_latency_ms / optimized_total_ms

non_model_pct_str = f"{non_model_pct:.0f}"
optimized_total_str = f"{optimized_total_ms:.1f}"
amdahl_speedup_str = f"{amdahl_speedup:.1f}"
```

**Amdahl's Law at Work**: Preprocessing (`{python} f"{preprocess_ms:.1f}"`ms) and data transfer (`{python} f"{cpu_gpu_ms:.1f}"`ms) consume `{python} non_model_pct_str`% of total latency. Optimizing the model 10× faster (`{python} f"{resnet_inference_ms:.0f}"`ms → `{python} f"{model_10x_ms:.1f}"`ms) yields only `{python} amdahl_speedup_str`× end-to-end speedup (from `{python} total_latency_str`ms to `{python} optimized_total_str`ms). This is why focusing exclusively on model optimization (quantization, pruning) often disappoints: the bottleneck is elsewhere.

**DSA Efficiency**: General-purpose CPUs achieve only 1-2% of peak performance at batch-1 because instruction overhead dominates. DSAs like TPUs and Tensor Cores replace complex logic with dense MAC arrays, achieving 10-100× higher arithmetic intensity. This makes hardware acceleration a requirement for economically viable serving.

**Engineering Implication**: Profile before optimizing. If preprocessing dominates, GPU-accelerated pipelines (NVIDIA DALI) may outperform model quantization.
:::

Moving preprocessing to GPU can reduce total latency by 6x in some pipelines by eliminating CPU-GPU data transfers between stages [@nvidia2024tritontutorial]. Effective optimization targets the largest time consumers first.

**The Serving Tax Bill**

Beyond the model execution itself, every request pays a "tax" to the serving infrastructure. @tbl-serving-tax quantifies these overheads for a typical high-performance inference request (e.g., ResNet-50 classification).

| **Tax Component** | **Typical Cost** | **Scaling Behavior** | **Tax Evasion Strategy** |
|:---|---:|:---|:---|
| **Network I/O** | 1-5 ms | Linear with payload | Compression, Region Colocation |
| **Serialization** | 50-500 $\mu$s | Linear with payload | gRPC/Protobuf (vs JSON) |
| **Queuing** | 0.1-10 ms | Exponential w/ load | Dynamic Batching, Autoscaling |
| **Dispatch** | 10-50 $\mu$s | Constant per batch | Kernel Fusion (reduce launches) |
| **Data Copy** | 100-500 $\mu$s | Linear with tensor | Zero-Copy / Shared Memory |

: **The Serving Tax Bill**: A breakdown of non-inference latency sources. While individual components like serialization seem small ($<1$ ms), they compound. In a 5ms inference service, this "tax" can easily consume 50% of the latency budget. The primary engineering goal is to drive these costs to zero through architectural choices like gRPC and Zero-Copy data paths. {#tbl-serving-tax}

#### The Killer Microseconds Problem {#sec-model-serving-systems-killer-microseconds-problem-bc00}

Barroso, Patterson, and colleagues identified a critical gap in *how* systems handle latency at different time scales [@barroso2017attack]. Operations in the microsecond range are too short for traditional OS scheduling (which operates at millisecond granularity) yet too long to simply spin-wait without wasting CPU cycles. This "killer microseconds" regime dominates modern serving workloads, where individual operations complete quickly but aggregate into significant overhead. The latency budget framework provides a systematic approach to optimization. First, measure each phase to identify the true bottleneck. Then allocate engineering effort proportionally to *where* time is actually spent. Finally, consider architectural changes such as GPU preprocessing or batching strategies that can shift work between phases.

### Resolution and Input Size Tradeoffs {#sec-model-serving-systems-resolution-input-size-tradeoffs-155d}

Input resolution affects both preprocessing and inference latency, but the relationship differs depending on whether the system is compute-bound (limited by arithmetic throughput) or memory-bound (limited by data movement). A compute-bound system slows proportionally to increased computation; a memory-bound system may show minimal slowdown if activation tensors still fit in fast memory. @sec-ai-acceleration covers this distinction in depth through roofline model analysis; understanding it is essential for making informed resolution decisions.

For compute-bound models, @eq-resolution-throughput formalizes how throughput scales inversely with resolution squared:

$$\frac{T(r_2)}{T(r_1)} = \left(\frac{r_1}{r_2}\right)^2$$ {#eq-resolution-throughput}

```{python}
#| label: resolution-scaling-calc
#| echo: false

# Resolution doubling theoretical slowdown
r1 = 224
r2 = 448
theoretical_slowdown = (r2 / r1) ** 2
measured_slowdown = 3.6  # Empirical measurement
theoretical_str = f"{theoretical_slowdown:.0f}"
```

Doubling resolution from `{python} r1` to `{python} r2` theoretically yields `{python} theoretical_str`x slowdown (measured: `{python} measured_slowdown`x due to fixed overhead amortization). However, at high resolutions, models transition from compute-bound to memory-bound as activation tensors exceed cache capacity. @tbl-resolution-bottleneck quantifies this transition for ResNet-50, showing how arithmetic intensity decreases with resolution:

| **Resolution** | **Activation Size** | **Arith. Intensity** | **Bottleneck** |
|:---|---:|---:|:---|
| **224×224** | 12.5MB | 85 FLOPS/byte | Compute |
| **384×384** | 36.8MB | 49 FLOPS/byte | Transitional |
| **512×512** | 65.5MB | 28 FLOPS/byte | Memory BW |
| **640×640** | 102.4MB | 18 FLOPS/byte | Memory BW |

: **Resolution and Compute Bottleneck**: ResNet-50 arithmetic intensity decreases with resolution as activation sizes grow. For a V100 PCIe (`{python} v100_tflops_fp32` TFLOPS FP32, `{python} v100_bw` GB/s bandwidth), the ridge point is approximately 16 FLOPS/byte. At 224x224, compute dominates; by 512x512, memory bandwidth becomes the limiting factor. {#tbl-resolution-bottleneck}

#### Resolution Strategies in Production {#sec-model-serving-systems-deploymentspecific-resolution-decisions-1d76}

Different deployment contexts have distinct resolution requirements. Mobile applications often accept lower resolution such as 224×224 for object detection in camera viewfinders, where latency and battery life dominate. Medical imaging requires high resolution of 512×512 or higher for diagnostic accuracy, with relaxed latency requirements. Autonomous vehicles use multiple resolutions for different tasks, with low resolution for detection and high resolution crops for recognition. Cloud APIs typically receive resolution set by client upload and must handle a range gracefully. This variability makes cloud APIs ideal candidates for adaptive resolution strategies, where the system selects resolution dynamically based on content characteristics.

**Adaptive Resolution.** Production systems can select resolution dynamically based on content. One approach runs a lightweight classifier at 128×128 to categorize content type, then selects task-appropriate resolution with documents at 512×512, landscapes at 224×224, and faces at 384×384. This achieves 1.4× throughput improvement with 99.2 percent accuracy retention versus fixed high resolution. This pattern trades preprocessing cost from running the lightweight classifier for inference savings on the main model.

The latency analysis so far has focused on sequential processing: one request completing before the next begins. The preprocessing, inference, and postprocessing stages use different hardware resources. This separation creates an opportunity to process multiple requests simultaneously.

### Hardware Utilization and Request Pipelining {#sec-model-serving-systems-utilization-request-pipelining-c61c}

The preceding analysis examined where time goes within individual pipeline stages. Optimizing each stage in isolation, however, misses a critical opportunity: the stages use different hardware resources. The latency budget analysis in @sec-model-serving-systems-latency-budget-ef40 reveals that model inference is only one component of the request lifecycle. From a hardware perspective, the primary goal of a serving system is to maximize the **duty cycle** of the accelerator, the percentage of time the GPU is performing useful computation.

In a serialized serving system, the hardware sits idle during network I/O and CPU-based preprocessing. High-performance serving systems use **Request Pipelining** to overlap these stages, ensuring the GPU is fed a continuous stream of tensors.

**Overlapping I/O and Compute.** @fig-serving-pipeline-timing contrasts serial execution with pipelined execution. In the serial case (A), each request must complete its entire lifecycle (Network $\rightarrow$ CPU Preprocessing $\rightarrow$ GPU Inference $\rightarrow$ Postprocessing) before the next request begins. Even with a fast GPU, the system throughput is limited by the slowest stage, and the GPU remains idle for more than 50% of the time.

::: {#fig-serving-pipeline-timing fig-env="figure" fig-pos="htb" fig-cap="**Request Pipelining**: Pipelining hides latency by overlapping independent operations across different hardware resources. In pipelined execution (B), the CPU processes the next request's data while the GPU executes the current request's inference. This increases the GPU duty cycle toward 100%, effectively doubling or tripling throughput on the same hardware without changing the model." fig-alt="Two timing diagrams. A (Serial): alternating CPU preprocessing, GPU inference, and idle blocks in sequence. B (Pipelined): two parallel rows where CPU preprocessing overlaps with GPU inference, eliminating idle time."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  \definecolor{CPUColor}{RGB}{173,216,230}
  \definecolor{GPUColor}{RGB}{144,238,144}
  \definecolor{WaitColor}{RGB}{240,240,240}

  % Serial Execution
  \node[anchor=west] at (0, 3.5) {\textbf{A. Serial Execution} (Low Utilization)};
  \draw[fill=CPUColor] (0, 2.5) rectangle (1.5, 3) node[midway] {Pre};
  \draw[fill=GPUColor] (1.5, 2.5) rectangle (3.0, 3) node[midway] {GPU};
  \draw[fill=WaitColor] (3.0, 2.5) rectangle (4.5, 3) node[midway, text=gray] {Idle};
  \draw[fill=CPUColor] (4.5, 2.5) rectangle (6.0, 3) node[midway] {Pre};
  \draw[fill=GPUColor] (6.0, 2.5) rectangle (7.5, 3) node[midway] {GPU};

  % Overlapped Execution
  \node[anchor=west] at (0, 1.5) {\textbf{B. Pipelined Execution} (High Utilization)};
  % CPU Row
  \draw[fill=CPUColor] (0, 0.5) rectangle (1.5, 1) node[midway] {Pre 1};
  \draw[fill=CPUColor] (1.5, 0.5) rectangle (3.0, 1) node[midway] {Pre 2};
  \draw[fill=CPUColor] (3.0, 0.5) rectangle (4.5, 1) node[midway] {Pre 3};
  \draw[fill=CPUColor] (4.5, 0.5) rectangle (6.0, 1) node[midway] {Pre 4};

  % GPU Row
  \draw[fill=GPUColor] (1.5, 0) rectangle (3.0, 0.5) node[midway] {GPU 1};
  \draw[fill=GPUColor] (3.0, 0) rectangle (4.5, 0.5) node[midway] {GPU 2};
  \draw[fill=GPUColor] (4.5, 0) rectangle (6.0, 0.5) node[midway] {GPU 3};
  \draw[fill=GPUColor] (6.0, 0) rectangle (7.5, 0.5) node[midway] {GPU 4};

\end{tikzpicture}
```
:::

Pipelining is enabled by **Asynchronous I/O** and **Concurrency Models**. Instead of waiting for a GPU kernel to finish, the server's CPU thread submits the work to the GPU's command queue and immediately begins preprocessing the next incoming request.

**The Systems Metric: Hardware Duty Cycle.** In the "Quantitative Approach" to ML systems, we define the efficiency of a serving system by its ability to saturate the bottleneck resource. For most ML systems, this is the GPU's compute cores or memory bandwidth.

$$\text{System Efficiency} = \frac{\sum T_{\text{compute}}}{\text{Wall Clock Time} \times \text{Resource Count}}$$

If a ResNet-50 request takes 10ms total (5ms GPU, 5ms CPU), a serial system achieves only 50% efficiency. By pipelining just two requests, efficiency approaches 100% (assuming the CPU can keep up with the GPU). If the CPU is too slow to feed the GPU, the system becomes **CPU-bound**, and further model optimization provides zero throughput gain, a direct application of Amdahl's Law (introduced in @sec-ml-system-architecture) to serving: if preprocessing consumes 50% of latency, maximum speedup is 2x regardless of how fast the model runs.

### Postprocessing {#sec-model-serving-systems-postprocessing-3b24}

Preprocessing and inference produce raw tensors, but these floating-point arrays carry no inherent meaning to applications or users. The final phase of the request lifecycle, postprocessing, transforms these tensors into actionable predictions: a 0.95 probability becomes a confident "dog" label, a sequence of token IDs becomes readable text, or a bounding box tensor becomes a highlighted region in an image. While often overlooked in system design, postprocessing significantly impacts both latency and the usefulness of predictions.

#### From Logits to Predictions {#sec-model-serving-systems-logits-predictions-09df}

Classification models output logits or probabilities across classes. Converting these to predictions involves several potential steps including argmax selection that chooses the highest-probability class, thresholding that applies confidence thresholds before returning predictions, top-k extraction that returns multiple high-probability classes with scores, and calibration that adjusts raw probabilities to better reflect true likelihoods.

For ResNet-50 image classification, typical postprocessing includes transforming logits to probabilities, extracting top predictions, and formatting responses. @lst-resnet-postprocessing shows a complete postprocessing pipeline with timing annotations. Total postprocessing time is approximately 0.1ms, negligible compared to preprocessing and inference.

::: {#lst-resnet-postprocessing lst-cap="**ResNet-50 Postprocessing**: Transforms raw logits to calibrated probabilities, extracts top-k predictions, and formats the API response."}
```{.python}
# Transform raw logits to calibrated probabilities
# Input: logits tensor of shape (batch_size, 1000) - one score per
# ImageNet class
probs = torch.softmax(
    logits, dim=-1
)  # Normalize to sum=1; ~0.05ms on GPU

# Extract top-5 predictions for multi-class response
# topk returns (values, indices) sorted by probability
top5_probs, top5_indices = probs.topk(5)  # ~0.02ms; GPU operation

# Map class indices to human-readable labels
# IMAGENET_CLASSES: list of 1000 class names from synset mapping
labels = [
    IMAGENET_CLASSES[i] for i in top5_indices
]  # ~0.01ms; CPU lookup

# Format response with predictions and metadata for API contract
response = {
    "predictions": [
        {"label": label, "confidence": float(prob)}
        for label, prob in zip(labels, top5_probs)
    ],
    "model_version": "resnet50-v2.1",  # Client-side version tracking
    "inference_time_ms": 5.2,  # Observability for latency monitoring
}
```
:::

Each step adds latency but improves response utility. Calibration in particular can add significant computation but is essential when downstream systems make decisions based on confidence scores.

**Output Formatting.** Production systems rarely return raw predictions. Outputs must conform to API contracts, often requiring JSON serialization with specific schema, confidence score formatting and thresholding, error handling for edge cases such as no confident prediction or out-of-distribution input, and metadata attachment including model version, inference time, and feature attributions.

The latency budget analysis reveals *where* time goes within a single request. Production systems, however, do not process requests in isolation: they must handle hundreds or thousands of concurrent requests competing for finite resources. Understanding this concurrency requires a different analytical framework.

## Queuing Theory and Tail Latency {#sec-model-serving-systems-queuing-theory-tail-latency-29a6}

The request lifecycle analysis explains *where* time goes within a single request, but production systems must handle many concurrent requests competing for finite resources. Explaining *why* latency degrades under load requires queuing theory, the mathematical framework that explains *how* requests wait for service in any system with finite capacity. These principles apply to web servers and ML inference alike, and explain the counterintuitive behavior that causes well-provisioned systems to violate latency SLOs when load increases modestly.

### Queuing Fundamentals {#sec-model-serving-systems-queuing-fundamentals-10d3}

Serving engineers routinely face a concrete question: given a latency SLO and an expected request rate, *how* many GPUs must be provisioned? Answering this question requires predicting *how* latency changes as load increases, which is precisely what queuing theory provides. Two mathematical foundations govern serving system behavior: Little's Law, which relates queue depth to throughput, and the M/M/1 model, which predicts how latency degrades under load. Together, they provide the quantitative framework for capacity planning.

### Little's Law {#sec-model-serving-systems-littles-law-9352}

The most fundamental result in queuing theory is Little's Law,[^fn-littles-law] [^fn-littles-law-intuition] which @eq-littles-law expresses as a simple relationship between three quantities in any stable system:

[^fn-littles-law]: **Little's Law**: Proven by John D.C. Little in 1961 [@little1961proof], this theorem establishes that $L = \lambda W$ holds for any stable queuing system regardless of arrival patterns, service distributions, or scheduling policies. The remarkable generality makes it one of the most useful results in operations research. For serving systems, it enables capacity planning from observable metrics: measuring queue depth and arrival rate directly yields average latency without instrumenting individual requests.

[^fn-littles-law-intuition]: **Little's Law in the Coffee Shop**: Throughput ($\lambda$) is the rate of arriving customers; Latency ($W$) is the time to make one drink; Queue ($L$) is the number of people waiting. If the barista takes 1 minute per drink ($W=1$) and customers arrive every 30 seconds ($\lambda=2$), the queue ($L$) will grow indefinitely unless more baristas are added.

$$L = \lambda \cdot W$$ {#eq-littles-law}

where $L$ is the average number of requests in the system, $\lambda$ is the arrival rate (requests per second), and $W$ is the average time each request spends in the system. This relationship holds regardless of arrival distribution, service time distribution, or scheduling policy. The following notebook quantifies this capacity relationship using a practical example of Little's Law.

```{python}
#| label: littles-law-calc
#| echo: false

# Little's Law example: L = λ × W
littles_lambda = 1000  # requests/sec
littles_w = 0.05       # 50ms in seconds
littles_l = littles_lambda * littles_w  # concurrent requests
littles_l_str = f"{littles_l:.0f}"
```

::: {.callout-notebook #notebook-littles-law title="Little's Law"}

**The Capacity Physics**: How much memory do you need to serve 1,000 queries per second?

**The Law**: $L = \lambda W$ (Concurrency = Throughput $\times$ Latency) (see @sec-system-foundations-littles-law-9c4c for the derivation).

**Scenario**:

*   **Throughput Target ($\lambda$)**: 1,000 requests/sec.
*   **Latency Target ($W$)**: 50 ms (0.05 s).

**The Calculation**:
$$ L = 1{,}000 \times 0.05 = \mathbf{`{python} littles_l_str` \text{ concurrent requests}} $$

**The Constraint**: Your server *must* have enough RAM to hold 50 requests simultaneously (batch size + queue).

*   If your GPU runs out of memory at Batch Size 32, you physically **cannot** hit 1,000 QPS at 50ms latency.
*   You must either reduce latency ($W$) or buy more memory ($L$).
:::

Little's Law has immediate practical implications. If your inference service averages 10ms per request ($W = 0.01$s) and you observe 50 concurrent requests in the system on average ($L = 50$), then your arrival rate must be $\lambda = L/W = 5000$ requests per second. Conversely, if you need to limit concurrent requests to 10 (perhaps due to GPU memory constraints), and your service time is 10ms, you can sustain at most 1000 requests per second.

### The Utilization-Latency Relationship {#sec-model-serving-systems-utilizationlatency-relationship-a2f0}

Little's Law tells us what the system looks like on average, but it does not reveal *how* latency changes as load approaches capacity. To answer the critical question of *how* much spare capacity a serving system needs, we turn to the M/M/1 queue model. For a system with Poisson arrivals and exponential service times, the average time in system follows:

$$W = \frac{1}{\mu - \lambda} = \frac{\text{service time}}{1 - \rho}$$ {#eq-mm1-wait}

where $\mu$ is the service rate (requests per second the server can handle), and $\rho = \lambda/\mu$ is the utilization (fraction of time the server is busy).

This equation reveals why serving systems exhibit nonlinear behavior. At 50% utilization, average **time in system** is $2\times$ the service time. At 80% utilization, it is $5\times$. At 90% utilization, it is $10\times$. Small increases in load near capacity cause disproportionate latency increases.

The M/M/1 model assumes exponentially distributed service times, but ML inference typically has near-constant service time for fixed batch sizes, making the M/D/1 (deterministic service) model more accurate in practice. We use M/M/1 here because it yields closed-form solutions and produces conservative estimates. @tbl-utilization-latency reveals how average **latency** grows rapidly as utilization approaches 100%. For M/D/1 queues, average wait time is approximately half of M/M/1 at the same utilization, which matters for capacity planning: M/M/1 analysis will slightly over-provision, erring on the side of meeting SLOs rather than violating them.[^fn-queuing-models]

[^fn-queuing-models]: **Kendall Notation**: The M/M/1 notation was introduced by British statistician David Kendall in 1953 and follows the pattern A/S/c (Arrivals/Service/servers). "M" stands for "Markovian" (memoryless, meaning exponential distributions), honoring Russian mathematician Andrey Markov (1856-1922). "D" means deterministic. So M/M/1 describes a single server with exponential arrivals and service times, while M/D/1 has deterministic service. ML inference is closer to M/D/1 since inference time is nearly constant, but M/M/1 yields conservative estimates suitable for capacity planning.

| **Utilization ($\rho$)** | **Latency Multiple** | **Example (5ms service)** |
|:---|---:|---:|
| 50% | 2.0× | 10ms |
| 70% | 3.3× | 17ms |
| 80% | 5.0× | 25ms |
| 90% | 10.0× | 50ms |
| 95% | 20.0× | 100ms |

: **Utilization-Latency Relationship**: Average **latency** as a multiple of service time for an M/M/1 queue. At 50% utilization, latency is 2x service time; at 90%, it reaches 10x. This nonlinear growth explains why systems that perform well at moderate load suddenly violate SLOs when traffic increases: moving from 80% to 90% utilization doubles latency. {#tbl-utilization-latency}

### Multi-Server Considerations {#sec-model-serving-systems-multiserver-considerations-00fc}

The preceding analysis focuses on a single ML node (one machine serving inference requests). This scope aligns with Volume I's focus on mastering the fundamental unit of ML systems. Single-node queuing dynamics are prerequisite to effective scaling: you cannot optimize a distributed system without first understanding the behavior of its components.

**When Single-Node Analysis Applies**: M/M/1 analysis remains the foundation for:

- **Right-sizing individual nodes**: Determining whether a single GPU can meet latency SLOs at expected traffic
- **Identifying the scaling trigger**: Calculating when traffic exceeds single-node capacity
- **Cost-effective provisioning**: Avoiding premature scale-out that wastes resources

For traffic exceeding single-node capacity, production systems deploy multiple replicas behind a load balancer. The M/M/c queuing model extends M/M/1 to c parallel servers, showing that multiple replicas dramatically improve tail latency: the probability of all servers being simultaneously slow drops exponentially with server count. At c=4 replicas, p99 latency can be 3× lower than the single-server case at the same total throughput.

**Scope Boundary**: This chapter establishes single-node serving foundations. Distributed inference systems (model sharding across GPUs, tensor parallelism, pipeline parallelism) introduce coordination overhead and consistency challenges that require advanced scaling principles.

### Tail Latency {#sec-model-serving-systems-tail-latency-5376}

Production SLOs typically specify percentile targets (p95, p99) rather than averages because tail latency determines user experience for the slowest requests [@dean2013tail]. For an M/M/1 queue, the p99 latency follows:

$$W_{p99} \approx \text{service time} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right)$$ {#eq-p99-latency}

At 70 percent utilization, p99 latency is approximately fifteen times the service time, while average latency is only 3.3 times. This explains *why* systems that seem healthy with low average latency can have unacceptable tail latency, since the average hides the experience of the unluckiest requests.

#### The Tail at Scale Problem {#sec-model-serving-systems-tail-scale-problem-958d}

Dean and Barroso's analysis reveals *why* tail latency becomes critical as systems scale beyond single machines [@dean2013tail]. The key insight is that when requests fan out to multiple servers, the probability of experiencing at least one slow response grows rapidly with server count. This "tail at scale" effect makes individual server tail latency critical for overall system performance.

For single-machine serving, this principle has two implications. First, tail latency on individual machines matters because it will compound when systems eventually scale. Second, the tail-tolerant techniques described below (hedging, graceful degradation) provide value even on single machines and become essential at scale.

**Tail-Tolerant Techniques**: Request hedging sends redundant requests after a timeout, accepting whichever response arrives first. Backup requests and load balancing away from slow servers directly address latency variance. These techniques apply to single-machine serving with multiple GPU streams or model replicas, and become essential when scaling to distributed inference systems.

With the queuing model and tail latency analysis established, we can now apply these tools to a concrete capacity planning exercise.

```{python}
#| label: capacity-planning-calc
#| echo: false

import math

# ResNet-50 capacity planning inputs
cp_peak_qps = 5000
cp_service_ms = 5
cp_p99_target_ms = 50
cp_rho_safe = 0.72  # from solving the M/M/1 p99 equation
cp_v100_throughput = 1143  # images/sec at batch=16
cp_headroom = 1.3  # 30% headroom

# Step 2: Required service rate
cp_mu_required = cp_peak_qps / cp_rho_safe
cp_mu_required_str = f"{cp_mu_required:.0f}"

# Step 3: GPU count
cp_gpus_raw = cp_mu_required / cp_v100_throughput
cp_gpus_ceil = math.ceil(cp_gpus_raw)
cp_gpus_raw_str = f"{cp_gpus_raw:.1f}"
cp_gpus_ceil_str = f"{cp_gpus_ceil}"

# Step 4: With headroom
cp_final_raw = cp_gpus_ceil * cp_headroom
cp_final_ceil = math.ceil(cp_final_raw)
cp_final_raw_str = f"{cp_final_raw:.1f}"
cp_final_ceil_str = f"{cp_final_ceil}"

# Step 5: Utilization after 1 failure
cp_gpus_after_fail = cp_final_ceil - 1
cp_gpus_after_fail_str = f"{cp_gpus_after_fail}"
cp_util_after_fail = (cp_peak_qps / cp_v100_throughput) / cp_gpus_after_fail * 100
cp_util_after_fail_str = f"{cp_util_after_fail:.1f}"

# Precision throughput ratio
cp_fp32_bits = 32
cp_int8_bits = 8
cp_precision_ratio = cp_fp32_bits // cp_int8_bits
cp_precision_ratio_str = f"{cp_precision_ratio}"
```

We can formalize this through *ResNet-50 capacity planning*.

::: {.callout-notebook title="ResNet-50 Capacity Planning"}

Consider designing a ResNet-50 serving system with these requirements:

- **Target p99 latency**: 50ms
- **Peak expected traffic**: 5,000 requests per second
- **Service time** (TensorRT FP16): 5ms

#### Step 1: Find Safe Utilization {.unnumbered}

Applying @eq-p99-latency to constrain $W_{p99} \leq 50$ms with 5ms service time and solving for $\rho$:

$$5\text{ms} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right) \leq 50\text{ms}$$

This yields $\rho \leq 0.72$ (72% maximum utilization).

#### Step 2: Calculate Required Service Rate {.unnumbered}

$$\mu_{\text{required}} = \frac{\lambda}{\rho_{\text{safe}}} = \frac{5000}{0.72} = `{python} cp_mu_required_str` \text{ requests/second}$$

#### Step 3: Determine GPU Count {.unnumbered}

Single V100 throughput at batch=16: 1,143 images/second

$$\text{GPUs needed} = \frac{`{python} cp_mu_required_str`}{1143} = `{python} cp_gpus_raw_str` \rightarrow `{python} cp_gpus_ceil_str` \text{ GPUs}$$

#### Step 4: Add Headroom for Variance {.unnumbered}

Production systems add 30% headroom for traffic spikes and variance:

$$\text{Final count} = `{python} cp_gpus_ceil_str` \times 1.3 = `{python} cp_final_raw_str` \rightarrow `{python} cp_final_ceil_str` \text{ GPUs}$$

#### Step 5: Verify Fault Tolerance {.unnumbered}

The 30% headroom addresses traffic variance, but production systems also need fault tolerance. With `{python} cp_final_ceil_str` GPUs, losing one leaves `{python} cp_gpus_after_fail_str` GPUs handling 5,000 QPS:

$$\text{Utilization after failure} = \frac{5000 / 1143}{`{python} cp_gpus_after_fail_str`} = `{python} cp_util_after_fail_str`\%$$

This remains well below the 72% safe utilization threshold, confirming N+1 redundancy is satisfied. For stricter fault tolerance requirements, N+2 redundancy (tolerating two simultaneous failures) would require 11-12 GPUs.

**Result**: Provision `{python} cp_final_ceil_str` V100 GPUs to serve 5,000 QPS at 50ms p99 latency with N+1 fault tolerance.

:::

The queuing analysis explains the capacity planning approach detailed in @sec-model-serving-systems-capacity-planning-96a3 and connects directly to the MLPerf Server scenario. @sec-benchmarking-ai explains how MLPerf measures throughput only for requests meeting the latency SLO: a system achieving 10,000 QPS but violating the SLO on 5% of requests reports only 9,500 valid QPS.

### Tail-Tolerant Techniques {#sec-model-serving-systems-tailtolerant-techniques-066e}

Rather than eliminating all sources of latency variability, which is often impractical, production systems employ techniques that tolerate variability while still meeting SLOs [@dean2013tail; @dean2012rapid]. These techniques treat latency variance as a given and design around it.

**Hedged Requests.** When a request has not completed within the expected time, send a duplicate request to another server.[^fn-hedging-etymology] The client uses whichever response arrives first and cancels the other. For ML serving, this means maintaining multiple model replicas and routing slow requests to alternative replicas. The overhead is modest: if you hedge at the 95th percentile, only 5% of requests generate duplicates, increasing load by just 5% while dramatically reducing tail latency.

[^fn-hedging-etymology]: **Hedging**: Borrowed from finance, where "hedging" means reducing risk by making offsetting bets. The term derives from the literal hedge (a boundary of shrubs) that protects a garden. Financial hedging dates to the 1600s Dutch tulip markets. Google's Jeff Dean introduced "hedged requests" in his influential 2013 "Tail at Scale" paper, applying the financial concept to distributed systems: send redundant requests to protect against the risk of slow responses.

**Cancellation Complexity**: A critical implementation detail is that CUDA kernels cannot be interrupted mid-execution. When a hedged request completes, the duplicate must be cancelled, but if inference has already begun on the GPU, cancellation approaches include checking a cancellation flag before launching inference, accepting wasted compute for the in-flight kernel, or using request prioritization to deprioritize the duplicate. Since hedging typically applies only to the slowest 5 percent of requests, the overhead from occasional wasted compute remains acceptable.

**Tied Requests.** Send the request to multiple servers simultaneously, but include a tag allowing servers to cancel execution once another server begins processing. This eliminates the delay of waiting to detect a slow response before hedging. For inference servers with significant startup overhead from model loading and memory allocation, tied requests ensure at least one server begins immediately.

**Canary Requests.** For requests that fan out to many backends, first send the request to a small subset of 1 to 2 servers.[^fn-canary-etymology] If these return within expected time, send to the remainder. If the canary is slow, the system can take corrective action by retrying elsewhere or using cached results before committing to the full fan-out. This prevents a single slow backend from stalling an entire distributed inference request.

[^fn-canary-etymology]: **Canary**: From the practice of using canary birds in coal mines from the early 1900s through the 1980s. Miners brought caged canaries underground because the birds' high metabolic rate made them sensitive to carbon monoxide and methane, dying before gas concentrations became lethal to humans. In software, "canary" describes any small-scale test that detects problems before they affect the full system, whether canary deployments, canary requests, or canary tests.

**Graceful Degradation.** When load exceeds capacity, return approximate results rather than timing out. For classification, return cached predictions for similar inputs. For generative models, return shorter outputs. For ensemble systems, return predictions from a subset of models. This maintains responsiveness at the cost of some accuracy, which users often prefer to outright failures.

**Admission Control.** When traffic exceeds capacity, accepting all requests can trigger widespread SLO violations. Admission control proactively rejects requests when queue depth exceeds a threshold, returning immediate 503 responses rather than accepting requests that are likely to timeout. This sacrifices throughput to protect latency for admitted requests.

**Setting the Threshold**: A practical starting point is 2 to 3 times service time multiplied by the number of workers. For a system with 4 workers and 10ms service time, this yields a queue depth threshold of 80 to 120 requests. Adaptive admission control adjusts thresholds based on observed p99 latency, tightening when latency increases above target and relaxing when latency remains healthy.

**Retry Storm Prevention**: A subtle failure mode occurs when all replicas are overloaded simultaneously. If the load balancer retries rejected requests at other replicas that are also overloaded, retry traffic amplifies the overload. Coordinated load shedding addresses this by sharing load information across replicas, enabling system-wide decisions about which requests to accept. When global load exceeds capacity, replicas collectively reject the same fraction of requests rather than each rejecting independently and triggering retries.

These techniques become essential at scale when fan-out amplification makes individual server tail latency visible to users. Single-machine serving systems can implement hedged and tied requests across GPU streams or model replicas. The queuing analysis here assumes FIFO processing, but production systems often implement priority scheduling such as deadline-aware or shortest-job-first approaches to further reduce tail latency for heterogeneous workloads [@harchol2013performance].

The tail-tolerant techniques examined in this section optimize the flow of requests through a functioning serving system. The queuing analysis, however, assumes a critical precondition: that models are loaded, initialized, and producing correct predictions. In production, this assumption fails regularly: during deployments, new instances must load models from scratch; during scaling events, cold start latency affects the first requests to new replicas; and when preprocessing pipelines diverge from training, accuracy silently degrades. The next section examines these lifecycle challenges that must be solved before queuing optimization becomes relevant.

## Model Lifecycle Management {#sec-model-serving-systems-model-lifecycle-management-ff2e}

The queuing analysis from previous sections assumes two prerequisites: models are loaded and ready to process requests, and predictions match what was validated during development. Production systems often violate both assumptions. Cold start latency can exceed inference time by orders of magnitude during scaling events. Subtle preprocessing differences between training and serving pipelines cause accuracy degradation that no amount of queuing optimization can address. This section examines the challenges that threaten these foundational assumptions. Retraining frequency spans multiple orders of magnitude: from hourly updates for recommendation systems to annual updates for some embedded devices.

### Training-Serving Skew {#sec-model-serving-systems-trainingserving-skew-7b99}

A model that performed well during validation may silently degrade when deployed. This phenomenon, known as **training-serving skew**, represents one of the most subtle failure modes in production ML because it is invisible to latency monitoring and exception tracking.

::: {.callout-definition title="Training-Serving Skew"}

***Training-Serving Skew*** is the **Distributional Divergence** between the training and inference environments. It arises when the function $f_{train}(x)$ differs from $f_{serve}(x)$ due to inconsistent preprocessing logic or environmental state, violating the **Consistency Imperative** and causing silent accuracy degradation.

:::

@sec-machine-learning-operations-mlops provides comprehensive coverage of skew diagnosis, monitoring, and organizational prevention strategies. Here we focus on the *serving-specific* manifestation: **preprocessing divergence**. This occurs when the real-time inference pipeline processes raw data differently than the batch training pipeline, a common failure mode when training uses Python/Pandas while serving uses C++/Java or optimized inference servers. Unlike data drift (which @sec-machine-learning-operations-mlops addresses through monitoring), preprocessing divergence is deterministic and preventable through careful engineering.

::: {.callout-example title="ResNet-50: Image Preprocessing Skew"}

For ResNet-50 serving, common sources of skew include:

**Resize interpolation**: Training uses PIL.BILINEAR while OpenCV defaults to cv2.INTER_LINEAR. These produce pixel-level differences that can shift accuracy by 0.5-1%.

**Color space handling**: JPEG loading in different libraries may produce BGR vs RGB ordering. If the model trained on RGB but serves BGR inputs, predictions are essentially random.

**Normalization constants**: ImageNet normalization uses specific mean/std values. Using `mean=[0.5, 0.5, 0.5]` instead of `mean=[0.485, 0.456, 0.406]` shifts inputs out of the training distribution.

**Prevention**: The safest approach is to export the exact preprocessing code used during training and run it identically in serving, or use a framework like NVIDIA DALI that can help standardize preprocessing across training and serving environments.

:::

### Cold Start and Initialization Dynamics {#sec-model-serving-systems-model-loading-initialization-cc5a}

With preprocessing pipelines designed to avoid training-serving skew, the next challenge is getting models ready to serve. Before processing any request, models must load from storage into memory and prepare for inference [@romero2021infaas]. This initialization latency, known as **cold start**, affects system responsiveness during deployments, scaling events, and recovery from failures.

::: {.callout-definition title="Cold Start"}

***Cold Start***[^fn-cold-start-etymology] is the **Initialization Latency** incurred when instantiating a new model replica. It represents the fixed cost of **State Hydration** (loading weights, compiling graphs) that effectively blocks the system's ability to scale elastically in response to traffic bursts.

[^fn-cold-start-etymology]: **Cold Start**: A metaphor borrowed from automotive engineering. Internal combustion engines operate inefficiently and suffer high wear until they reach thermal equilibrium. Similarly, ML inference systems suffer high latency during their "warm-up" phase (loading weights, compiling kernels via JIT, and populating caches) before they can serve traffic at peak efficiency.

:::

Cold start dynamics determine whether systems meet latency requirements from the moment they begin serving traffic. A *cold start timeline* for a representative model reveals where each phase contributes to total initialization latency.

**Cold Start Anatomy.** Cold start latency compounds from multiple sources, each adding to the time between deployment and serving readiness. Weight loading reads model parameters from disk or network storage. Graph compilation performs just-in-time compilation of operations for the specific hardware. Memory allocation reserves GPU memory for activations and intermediate values. Warmup[^fn-warmup-etymology] execution performs initial inferences that populate caches and trigger lazy initialization.

[^fn-warmup-etymology]: **Warmup**: The computing metaphor derives from physical warming, where engines and machines perform better after reaching operating temperature. In JIT-compiled systems like the JVM (1990s), "warmup" specifically refers to the period when the runtime gathers profiling data and compiles hot paths. For ML serving, warmup serves a dual purpose: triggering lazy memory allocation and populating CPU/GPU caches with frequently-accessed data, ensuring the first real user request does not pay these one-time costs.

::: {.callout-notebook title="ResNet-50: Cold Start Timeline"}

```{python}
#| label: cold-start-calc
#| echo: false

# Cold start components (seconds)
cs_ssd = 0.5
cs_s3 = 4.0
cs_cuda = 0.4
cs_compile = 30.0
cs_warmup = 0.2

# Total Local (Optimized): SSD + CUDA + Warmup + (Pre-compiled)
cs_local_total = cs_ssd + cs_cuda + cs_warmup + 0.4 # +0.4s runtime overhead
cs_local_str = f"~{cs_local_total:.1f}s"

# Total Cloud (First Deploy): S3 + CUDA + Compile + Warmup
cs_cloud_total = cs_s3 + cs_cuda + cs_compile + cs_warmup
cs_cloud_str = f"~{cs_cloud_total:.0f}s"

cs_ssd_str = f"{cs_ssd}s"
cs_s3_str = f"3-5s"
cs_cuda_str = f"0.3-0.5s"
cs_compile_str = f"15-30s"
cs_warmup_str = f"{cs_warmup}s"
```

| **Phase** | **Duration** | **Notes** |
|:---|:---|:---|
| **Weight loading (SSD)** | `{python} cs_ssd_str` | 98MB FP32 weights from local storage |
| **Weight loading (S3)** | `{python} cs_s3_str` | Network latency dominates for cloud storage |
| **CUDA context** | `{python} cs_cuda_str` | GPU driver initialization and memory setup |
| **TensorRT compilation** | `{python} cs_compile_str` | Converts PyTorch model to optimized engine |
| **Warmup (10 inferences)** | `{python} cs_warmup_str` | Triggers remaining lazy initialization |
| **Total (local, optimized)** | **`{python} cs_local_str`** | With pre-compiled TensorRT engine, warm container |
| **Total (cloud, first deploy)** | **`{python} cs_cloud_str`** | Including compilation from cold state |

**Key insight**: Pre-compiling models and storing the optimized engine eliminates the 30-second compilation phase on subsequent deployments.

**CUDA Context**: Before any GPU operation, the CUDA runtime must establish a *context*: a data structure that tracks memory allocations, loaded kernels, and device state. Creating a context requires communicating with the GPU driver and allocating GPU memory for internal bookkeeping. This one-time cost (0.3-0.5s) affects every new process that uses the GPU. CUDA 11+ introduced lazy initialization that defers some setup until first use, reducing apparent startup time but shifting cost to the first inference.

**CUDA MPS (Multi-Process Service)**: Normally, each process creates its own CUDA context, and the GPU time-slices between contexts. MPS allows multiple processes to share a single context, eliminating redundant initialization and enabling concurrent kernel execution. For serving systems running multiple model replicas, MPS can reduce aggregate cold start time and improve GPU utilization. The trade-off is reduced isolation: a crash in one process can affect others sharing the MPS server.

:::

Without warmup, the first real request triggers compilation and memory allocation mid-inference, often causing timeout failures. A request that normally takes 5ms might require 500ms during cold start, violating SLOs and degrading user experience.

### Loading Strategies {#sec-model-serving-systems-loading-strategies-eb38}

Different loading strategies trade off cold start duration against serving performance and memory efficiency.

**Full loading** reads the entire model into memory before serving begins. This maximizes inference speed since all weights are immediately available, but extends cold start duration and limits model size to available memory. Full loading is appropriate when cold start latency is acceptable and models comfortably fit in memory.

**Memory mapping** maps model files directly into the address space, loading pages on demand as accessed. This reduces cold start time since inference can begin before the full model loads, but causes unpredictable latency as pages fault in during initial requests. Memory mapping works well for infrequently accessed model components but can cause latency spikes if critical weights are not preloaded.

**Lazy initialization** defers compilation and allocation until first use. This minimizes startup time but shifts latency to the first request. Production systems often combine lazy initialization with synthetic warmup requests to trigger initialization before real traffic arrives.

### Model Caching Infrastructure {#sec-model-serving-systems-model-caching-infrastructure-4f1a}

Production systems cache model weights at the infrastructure level to reduce cold start for common deployment scenarios:

**Container Image Embedding**: Bundle model weights directly in the container image. This produces a single deployment artifact and eliminates network fetches at startup, but creates large images (often 10-50GB) that slow container pulls and consume registry storage. Best for models that rarely update.

**Shared Filesystem**: Mount a network filesystem (EFS, GCS FUSE) containing model weights. Multiple replicas share cached weights, and updates propagate immediately without redeployment. Network latency affects cold start, and filesystem availability becomes a critical dependency. Best for organizations with many models and frequent updates.

**Node-Local SSD Cache**: Pre-populate local SSDs on inference nodes with frequently-used models. Provides fast loading (500MB/s+ for NVMe) without network dependency, but requires cache management to handle model updates and capacity limits. Best for high-traffic models where cold start latency is critical.

The choice depends on model update frequency: infrequent updates favor container embedding, frequent updates favor shared filesystem, and performance-critical deployments benefit from local caching with background refresh.

### Multi-Model Serving {#sec-model-serving-systems-multimodel-serving-a9c1}

Production systems often serve multiple models from a single machine, whether different model versions for A/B testing, ensemble components, or entirely different models sharing infrastructure. GPU memory becomes the limiting resource, requiring careful management strategies.

Strategies for multi-model serving include time-multiplexing that loads one model at a time and swaps based on request routing, memory sharing where models share GPU memory to limit concurrent execution but enable more models, and model virtualization where frameworks like Triton manage model lifecycle by loading and unloading based on traffic patterns [@nvidia2024triton]. The choice depends on request patterns. If models receive traffic evenly, concurrent loading works. If traffic is bursty and model-specific, time-multiplexing with intelligent preloading reduces average latency while maximizing GPU utilization.

#### Multi-Stream Execution {#sec-model-serving-systems-multistream-execution-1b1f}

When multiple models or multiple instances of the same model must run concurrently on a single GPU, the hardware must partition resources between them. NVIDIA's Multi-Instance GPU technology enables hardware-level isolation, dividing an A100 into up to 7 independent GPU instances, each with dedicated memory and compute resources. MIG is available on A100, A30 (up to 4 instances), H100, H200, and newer data center GPUs. For older GPUs such as V100 or T4, CUDA stream scheduling provides time-multiplexed sharing without hardware isolation.

The choice depends on whether consistent latency with MIG or maximum utilization with shared streams is the priority.

#### Model Swapping and Host Memory {#sec-model-serving-systems-model-swapping-host-memory-c54f}

When the aggregate size of all models exceeds GPU memory capacity, the serving system must swap models between host memory (DRAM) and device memory (VRAM) on demand. This introduces a new latency component determined by the PCIe bus bandwidth.

```{python}
#| label: model-swap-calc
#| echo: false

model_size_gb = 10
pcie_bw_gbs = PCIE_GEN4_BW.to(GB / second).magnitude
model_swap_ms = f"{model_size_gb / pcie_bw_gbs * 1000:.0f}"
pcie_bw_gbs_str = f"{pcie_bw_gbs:.0f}"
```

For a `{python} model_size_gb` GB model on PCIe Gen4 x16 (`{python} pcie_bw_gbs_str` GB/s theoretical bandwidth), loading takes at least:
$$ T_{\text{load}} = \frac{`{python} model_size_gb` \text{ GB}}{`{python} pcie_bw_gbs_str` \text{ GB/s}} \approx `{python} model_swap_ms` \text{ ms} $$

To mitigate this, systems use **Pinned Memory** (page-locked host memory). By default, the operating system can move ("page") any memory region to disk when RAM is under pressure. This creates a problem for GPU transfers: if the GPU's DMA (Direct Memory Access) engine begins reading a memory region that gets paged out mid-transfer, the transfer fails or stalls. To avoid this, the CPU must first copy data to a temporary pinned buffer before the GPU can safely read it, adding both latency and CPU overhead.

Pinning memory instructs the OS to keep that region permanently in physical RAM. The GPU's DMA engine can then transfer data directly from the pinned region at full PCIe bandwidth without CPU involvement. The trade-off is that pinned memory reduces the RAM available for other processes and cannot be reclaimed under memory pressure. For model serving, the performance gain (2-3× faster transfers) typically justifies pinning model weights and frequently-used input buffers, while leaving less critical memory pageable.

The lifecycle management strategies examined so far ensure models are ready to serve: loaded into memory, warmed up, and producing predictions consistent with training. With these prerequisites satisfied, the queuing dynamics from @sec-model-serving-systems-queuing-theory-tail-latency-29a6 become relevant. The next optimization opportunity lies in how requests are grouped for processing, which directly affects both the throughput and latency terms in our queuing equations.

## Throughput Optimization {#sec-model-serving-systems-throughput-optimization-18d1}

With models loaded, initialized, and ready to serve, the next optimization opportunity lies in how requests are grouped for processing. Batching[^fn-batch-etymology] differs fundamentally between training and serving [@crankshaw2017clipper]. Training batches maximize throughput, processing hundreds or thousands of samples together with no concern for individual sample latency. Serving batches must balance throughput against individual request latency, typically processing single digits of requests together while ensuring no request waits too long. This adaptive approach is called **dynamic batching** because the system adjusts batch composition in real time based on arriving requests.

[^fn-batch-etymology]: **Batch**: From Old French "bache" (a quantity baked at one time), the term entered computing in the 1950s to describe jobs processed together without human interaction, as contrasted with interactive computing. IBM's batch processing systems of the 1960s would collect punch cards overnight and process them sequentially. The ML usage preserves this core meaning: group samples together for efficient processing, trading individual response time for aggregate throughput.

::: {.callout-definition title="Dynamic Batching"}

***Dynamic Batching*** is the runtime optimization of trading **Latency** for **Throughput** under stochastic arrival patterns. By buffering requests into a **Batch Window**, the scheduler amortizes fixed overheads (kernel launch, weight IO) across multiple inputs, pushing the system away from the memory-bound regime.

:::

### Why Batching Helps {#sec-model-serving-systems-batching-helps-f1dc}

Modern accelerators achieve peak efficiency only at sufficient batch sizes [@shen2019nexus]. A single inference request leaves most compute units idle because GPUs are designed for parallel execution across thousands of threads. Batching amortizes fixed costs across multiple requests and enables parallel execution across the batch dimension.

Two fixed costs dominate at small batch sizes. **Kernel launch overhead**[^fn-kernel-etymology-serving] is the time for the CPU to prepare and submit work to the GPU. Each layer in a neural network typically requires a separate kernel launch: the CPU must assemble kernel parameters, copy them to GPU-accessible memory, and signal the GPU to begin execution. This overhead is typically 5-20μs per kernel, independent of batch size. ResNet-50 has approximately 50 layers, so kernel launch alone adds 250-1000μs per inference. At batch size 1, this overhead may exceed the actual compute time; at batch size 32, the same overhead is amortized across 32 images. **Weight loading** reads model parameters from GPU memory (VRAM) to the compute units. At batch size 1, the GPU reads all weights to process one image; at batch size 32, the same weight read processes 32 images, achieving 32× better memory efficiency. Measuring *batching efficiency* on a concrete model quantifies how these fixed costs amortize in practice.

[^fn-kernel-etymology-serving]: **Kernel**: From Old English "cyrnel" meaning seed or grain, the essential core of something. In operating systems (1960s), the kernel is the core that manages hardware resources. CUDA borrowed this term around 2007 for GPU functions because they represent the computational "core" of parallel algorithms. Unlike OS kernels that run continuously, GPU kernels are discrete units of parallel work launched by the CPU and executed across thousands of GPU threads simultaneously.

::: {.callout-notebook title="ResNet-50 Batching Efficiency"}

The throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates the power of batching:

| **Batch Size** | **Inference Time*** | **Per-Image Compute** | **Throughput** | **GPU Util.** |
|:---|---:|---:|---:|---:|
| 1 | 5.0ms | 5.0ms | 200 img/s | 15% |
| 4 | 7.2ms | 1.8ms | 556 img/s | 42% |
| 8 | 9.1ms | 1.1ms | 879 img/s | 65% |
| 16 | 14.0ms | 0.9ms | 1,143 img/s | 85% |
| 32 | 25.0ms | 0.8ms | 1,280 img/s | 95% |

*Times shown are pure inference time, excluding queue wait. @sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b analyzes how user-perceived latency includes batching window wait.

```{python}
#| label: batch-throughput-calc
#| echo: false

# Batching throughput comparison (from table above)
batch1_throughput = 200   # img/s at batch=1
batch32_throughput = 1280  # img/s at batch=32
throughput_ratio = batch32_throughput / batch1_throughput
batch32_inference_ms = 25.0
batch_window_ms = 10.0
batch32_total_ms = batch_window_ms + batch32_inference_ms
batch1_inference_total_ms = 5.0

throughput_ratio_str = f"{throughput_ratio:.1f}"
batch32_total_str = f"{batch32_total_ms:.0f}"
```

**Key insight**: Batch size 32 achieves `{python} throughput_ratio_str`× higher throughput than batch size 1. However, user-perceived latency includes both queue wait and inference time. With a `{python} f"{batch_window_ms:.0f}"`ms batching window and `{python} f"{batch32_inference_ms:.0f}"`ms inference, total latency reaches `{python} batch32_total_str`ms versus `{python} f"{batch1_inference_total_ms:.0f}"`ms at batch size 1.

:::

The table reveals the throughput-latency tradeoff in stark terms: larger batches dramatically improve hardware efficiency but increase per-request latency. In practice, the optimal batch size depends on both the latency Service Level Objective (SLO) and the arrival rate of requests. The question facing every serving engineer is therefore quantitative: given a specific latency budget, what is the largest batch size that still meets the SLO? The following analysis shows how to find *the batching sweet spot*.

::: {.callout-notebook title="The Batching Sweet Spot"}

**Problem**: You are serving a ResNet-50 model. At batch=1, the GPU is mostly idle (15% utilization). You want to increase throughput to save money, but you have a **20 ms** latency budget.

```{python}
#| label: batching-sweetspot-calc
#| echo: false

# Batching sweet spot worked example
batch1_ms = 5.0
batch1_imgs = 200  # img/s
batch8_wait_ms = 5.0
batch8_inference_ms = 9.0
batch8_user_latency_ms = batch8_wait_ms + batch8_inference_ms
batch8_throughput = 8 / (batch8_user_latency_ms / 1000)
latency_increase = batch8_user_latency_ms / batch1_ms

batch8_user_latency_str = f"{batch8_user_latency_ms:.0f}"
batch8_throughput_str = f"{batch8_throughput:.0f}"
latency_increase_str = f"{latency_increase:.0f}"
```

**The Math**:

1.  **Baseline (Batch 1)**: Inference = **`{python} f"{batch1_ms:.0f}"` ms**. Throughput = **`{python} batch1_imgs` img/s**.
2.  **Optimized (Batch 8)**:
    - **Wait Time**: You set a **`{python} f"{batch8_wait_ms:.0f}"` ms** batching window to collect requests.
    - **Inference Time**: Batch 8 inference takes **`{python} f"{batch8_inference_ms:.0f}"` ms**.
    - **User Latency**: $5 \text{ ms (wait)} + 9 \text{ ms (compute)} = \mathbf{`{python} batch8_user_latency_str` \text{ ms}}$.
    - **Throughput**: $8 \text{ img} / `{python} batch8_user_latency_str` \text{ ms} \approx \mathbf{`{python} batch8_throughput_str` \text{ img/s}}$.

**The Systems Conclusion**: By accepting a **`{python} latency_increase_str`× increase in latency** (`{python} f"{batch1_ms:.0f}"`ms → `{python} batch8_user_latency_str`ms), you have achieved nearly **`{python} latency_increase_str`× higher throughput** on the same hardware. As long as `{python} batch8_user_latency_str`ms is under your 20ms budget, this is "free" capacity. This trade-off is the fundamental lever of serving economics.
:::

@fig-throughput-latency-knee visualizes this trade-off, showing the **"Knee"** of the curve. This is the optimal operating point where throughput is maximized before latency spikes due to queuing.

```{python}
#| label: fig-throughput-latency-knee
#| echo: false
#| fig-cap: "**The Throughput-Latency Knee.** Batch Size vs. Throughput (Blue) and Latency (Orange). Throughput increases with batch size as hardware utilization improves, but eventually saturates. Latency remains relatively flat (hidden by parallel resources) until the 'Knee,' after which it spikes linearly due to queuing. The optimal operating point lies just before this spike."
#| fig-alt: "Dual-axis line chart. Blue line (Throughput) rises and plateaus. Orange line (Latency) stays low then spikes upward. A vertical line marks the optimal point where throughput is high before latency explodes."

import sys
import matplotlib.pyplot as plt
# Ensure calc directory is in path
sys.path.append('../../../calc')
import viz

viz.set_book_style()
viz.plot_throughput_latency_knee()
plt.show()
```

The efficiency gains from batching come at a cost: requests must wait for the batch to form. This creates a fundamental tension between throughput optimization (larger batches) and latency minimization (immediate processing). The different batching strategies and their tradeoffs govern how engineers tune this balance.

### Static vs Dynamic Batching {#sec-model-serving-systems-static-vs-dynamic-batching-fd0a}

**Static batching** waits for a fixed batch size before processing. Simple to implement but problematic in practice: during low traffic, requests wait indefinitely for a full batch. During high traffic, large batches increase per-request latency.

**Dynamic batching** collects requests within a time window, processing whatever has arrived when the window closes [@olston2017tensorflow]. This bounds maximum wait time regardless of traffic level. The window size represents a direct tradeoff: shorter windows reduce latency but sacrifice throughput; longer windows improve throughput but increase latency.

Typical configurations use windows of 5-50ms with maximum batch sizes of 8-32 for latency-sensitive applications. The optimal configuration depends on request arrival patterns, model characteristics, and latency requirements.

### Dynamic Batching Latency-Throughput Trade-offs {#sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d}

Dynamic batching introduces a fundamental tension between throughput optimization and latency constraints. Understanding this tradeoff quantitatively enables systematic configuration decisions rather than trial-and-error tuning. To begin, consider *why latency spikes under load*:

::: {.callout-notebook title="Why Latency Spikes Under Load"}

**Recall** from @sec-model-serving-systems-littles-law-9352: Little's Law ($L = \lambda W$) governs all stable queues. When hardware is saturated (throughput $\lambda$ is maxed out), any increase in traffic increases queue depth ($L$). Since $\lambda$ cannot grow, **latency ($W$) must grow linearly with queue depth**. This is why **admission control** (rejecting requests when $L$ exceeds a threshold) is the only way to preserve latency during overload.
:::

@eq-batching-latency decomposes the total user-perceived latency for a batched request into two components:

$$L_{\text{total}} = L_{\text{wait}} + L_{\text{compute}}(b)$$ {#eq-batching-latency}

where $L_{\text{wait}}$ is the time spent waiting in the batching queue and $L_{\text{compute}}(b)$ is the inference time for batch size $b$. The batching window $T$ bounds wait time ($L_{\text{wait}} \leq T$), while batch size affects compute time through GPU utilization characteristics.

#### Quantitative Analysis of Batching {#sec-model-serving-systems-queue-waiting-time-analysis-8d5c}

For Poisson arrivals with rate $\lambda$ and batching window $T$, requests arrive uniformly within the window. A request arriving at time $t$ within the window waits $T - t$ for the batch to close. @eq-avg-wait shows that the average wait time is simply half the window:

$$E[L_{\text{wait}}] = \frac{T}{2}$$ {#eq-avg-wait}

```{python}
#| label: batching-budget-calc
#| echo: false

# Batching window latency budget analysis
batch_window_ms = 20
avg_wait_ms = batch_window_ms / 2
slo_ms = 50
inference_ms = 5
budget_pct = avg_wait_ms / slo_ms * 100

avg_wait_str = f"{avg_wait_ms:.0f}"
budget_pct_str = f"{budget_pct:.0f}"
```

This simple relationship has direct implications. A 20ms batching window adds `{python} avg_wait_str`ms average latency regardless of batch size achieved. If your latency SLO is 50ms and inference takes 5ms, the batching window consumes `{python} budget_pct_str`% of your latency budget before any computation begins.

**Batch Size Distribution.** The number of requests collected during window $T$ follows a Poisson distribution with mean $\lambda T$. @eq-batch-distribution formalizes this relationship:

$$P(\text{batch size} = k) = \frac{(\lambda T)^k e^{-\lambda T}}{k!}$$ {#eq-batch-distribution}

@tbl-batch-variability quantifies this variability, showing how batch size fluctuates for different traffic levels with a fixed 10ms window:

| **Arrival Rate** | **Mean Batch** | **Std Dev** | **P(batch=0)** | **P(batch≥2×mean)** |
|:---|---:|---:|---:|---:|
| **50 QPS** | 0.5 | 0.7 | 61% | 1% |
| **200 QPS** | 2.0 | 1.4 | 14% | 9% |
| **500 QPS** | 5.0 | 2.2 | 0.7% | 12% |
| **1000 QPS** | 10.0 | 3.2 | 0.005% | 13% |

: **Batch Size Variability**: At low traffic, batching windows frequently contain zero requests (wasted GPU cycles). At moderate traffic, batch sizes fluctuate significantly around the mean. High traffic provides more stable batching but still sees 13% of batches exceeding twice the mean size. {#tbl-batch-variability}

#### Throughput Maximization Strategy {#sec-model-serving-systems-throughput-maximization-strategy-27f5}

Throughput optimization requires maximizing the number of requests processed per unit time. For a system with service time $S(b)$ for batch size $b$, throughput follows @eq-batch-throughput:

$$\text{Throughput}(b) = \frac{b}{T + S(b)}$$ {#eq-batch-throughput}

The numerator increases linearly with batch size while the denominator increases sub-linearly (due to GPU parallelism). This creates an optimal batch size that balances these competing effects.

For ResNet-50 on a V100 GPU, service time scales as $S(b) = 5\text{ms} + 0.6b$ (5ms fixed overhead plus 0.6ms per additional image in the batch). With $T = 10$ms batching window:

| **Batch Size** | **Service Time** | **Total Latency** | **Throughput** | **Efficiency** |
|:---|---:|---:|---:|:---|
| 1 | 5.6ms | 15.6ms | 64 img/s | Low |
| 4 | 7.4ms | 17.4ms | 230 img/s | Moderate |
| 8 | 9.8ms | 19.8ms | 404 img/s | Good |
| 16 | 14.6ms | 24.6ms | 650 img/s | High |
| 32 | 24.2ms | 34.2ms | 935 img/s | Maximum |

```{python}
#| label: batching-analysis-calc
#| echo: false

# Batching throughput analysis (from S(b) = 5ms + 0.6b, T = 10ms)
T_window = 10.0  # batching window ms
fixed_overhead_ms = 5.0
per_image_ms = 0.6

def service_time(b):
    return fixed_overhead_ms + per_image_ms * b

def total_latency(b):
    return T_window + service_time(b)

def throughput(b):
    return b / (total_latency(b) / 1000)

# Table values
batch_sizes = [1, 4, 8, 16, 32]
throughputs = {b: throughput(b) for b in batch_sizes}
latencies = {b: total_latency(b) for b in batch_sizes}

throughput_increase = throughputs[32] / throughputs[1]
throughput_increase_str = f"{throughput_increase:.1f}"
b1_throughput_str = f"{throughputs[1]:.0f}"
b32_throughput_str = f"{throughputs[32]:.0f}"
b1_latency_str = f"{latencies[1]:.1f}"
b32_latency_str = f"{latencies[32]:.1f}"
```

The throughput gains in @tbl-batching-throughput trace directly back to the Iron Law framework established in @sec-ai-training-iron-law-training-performance-a53f, where batching amortizes the fixed overhead term.

: **Batching Throughput Analysis**: ResNet-50 throughput on V100 with `{python} f"{T_window:.0f}"`ms batching window. Throughput increases `{python} throughput_increase_str`x from batch size 1 to 32 (`{python} b1_throughput_str` to `{python} b32_throughput_str` img/s), but total latency more than doubles (`{python} b1_latency_str`ms to `{python} b32_latency_str`ms). The optimal configuration depends on whether the latency SLO or throughput target is the binding constraint. {#tbl-batching-throughput}

::: {.callout-notebook title="The Iron Law of Batching Efficiency"}

**The Iron Law Connection:**
In serving, we maximize throughput by amortizing the **Latency Term** ($L_{lat}$).
$$ T = \frac{O}{R_{peak} \cdot \eta} + L_{lat} $$

**Deriving the Sweet Spot:**

*   **Case 1 (Batch 1):** Overhead (5ms) $\approx$ Compute (0.6ms). Efficiency $\approx 10\%$. The GPU is mostly waiting.
*   **Case 2 (Batch 32):** Overhead (5ms) $\ll$ Compute (19.2ms). Efficiency $\approx 80\%$. The GPU is crunching numbers.

**The Golden Rule:** Increase batch size until the **Latency Term** becomes negligible (< 10% of total time). Beyond this point, you gain minimal throughput but pay a linear latency penalty.
:::

#### Latency-Constrained Optimization {#sec-model-serving-systems-latencyconstrained-optimization-8f66}

When latency SLOs provide the binding constraint, the optimization problem becomes finding the maximum batch size that meets the SLO. For SLO $L_{\text{SLO}}$ and average wait time $T/2$, @eq-latency-constrained-batch defines the maximum allowable batch size:

$$b_{\text{max}} = \max\{b : \frac{T}{2} + S(b) \leq L_{\text{SLO}}\}$$ {#eq-latency-constrained-batch}

Consider a 50ms p95 latency SLO for ResNet-50 serving:

**Scenario 1: Conservative window (T = 5ms)**
- Average wait: 2.5ms
- Latency budget for inference: 47.5ms
- Maximum batch size: 71 (but typically capped at 32 for memory)
- Achieved throughput: ~1,140 img/s (batch=32)

**Scenario 2: Aggressive window (T = 25ms)**
- Average wait: 12.5ms
- Latency budget for inference: 37.5ms
- Maximum batch size: 48
- Achieved throughput: ~1,280 img/s (batch=48)

The aggressive window achieves only 12% higher throughput but increases average latency by 10ms and p99 latency by 25ms. Examine @tbl-batching-throughput: for latency-sensitive applications, the conservative window provides better user experience at modest throughput cost.

**SLO Violation Analysis.** Batch size variability causes SLO violations even when mean latency appears safe. The p99 latency includes both worst-case wait time (full window) and worst-case batch size (governed by Poisson tail). @eq-p99-batch-latency captures this relationship:

$$L_{p99} \approx T + S(b_{p99})$$ {#eq-p99-batch-latency}

```{python}
#| label: slo-violation-calc
#| echo: false

import math

# SLO violation analysis
qps = 500
T_slo = 10.0  # ms window
mean_batch = qps * (T_slo / 1000)  # λ × T

# Poisson p99: use inverse CDF approximation
# For λT=5, p99 ≈ 11
p99_batch = 11

# Latency calculations using S(b) = 5ms + 0.6b
mean_wait = T_slo / 2  # average wait = T/2
mean_service = service_time(int(mean_batch))
mean_latency = mean_wait + mean_service

p99_service = service_time(p99_batch)
p99_latency = T_slo + p99_service  # worst-case wait = T

p99_to_mean = p99_latency / mean_latency

mean_batch_str = f"{mean_batch:.0f}"
mean_service_str = f"{mean_service:.1f}"
mean_latency_str = f"{mean_latency:.0f}"
p99_service_str = f"{p99_service:.1f}"
p99_latency_str = f"{p99_latency:.1f}"
p99_ratio_str = f"{p99_to_mean:.2f}"
```

where $b_{p99}$ is the 99th percentile batch size. For $\lambda = `{python} qps`$ QPS and $T = `{python} f"{T_slo:.0f}"`$ms:

- Mean batch size: `{python} mean_batch_str`
- p99 batch size: `{python} p99_batch` (from Poisson distribution)
- Mean latency: $`{python} f"{mean_wait:.0f}"`\text{ms} + `{python} mean_service_str`\text{ms} = `{python} mean_latency_str`\text{ms}$
- p99 latency: $`{python} f"{T_slo:.0f}"`\text{ms} + `{python} p99_service_str`\text{ms} = `{python} p99_latency_str`\text{ms}$

The p99 latency is `{python} p99_ratio_str`× the mean, reflecting both wait time variance and batch size variance. Systems that provision based on mean latency will experience SLO violations.

::: {.callout-perspective title="Practitioner's Perspective: The Latency-Throughput Trade-off" collapse="false"}
In systems engineering interviews and architecture reviews, the most common pitfall is discussing "inference speed" without specifying **Batch Size**.

*   **Batch-1 Regime**: Optimized for **Latency**. Relevant for real-time interaction (e.g., typing helpers, robotics). The bottleneck is usually Python overhead or memory bandwidth.
*   **Batch-32 Regime**: Optimized for **Throughput**. Relevant for offline processing or high-traffic services. The bottleneck is usually Compute (FLOPS).

**The Professional Response**: When asked "how fast is this model?", always clarify: "Are we optimizing for single-stream latency (Batch 1) or maximum throughput (Batch N)?" This distinction demonstrates systems maturity.
:::

#### Adaptive Batching Windows {#sec-model-serving-systems-adaptive-batching-windows-c404}

Fixed batching windows waste latency budget during high traffic when large batches form quickly. @lst-adaptive-batching demonstrates how adaptive strategies adjust the window based on queue depth.

::: {#lst-adaptive-batching lst-cap="**Adaptive Batching Window**: Dynamically adjusts batch timeout based on queue depth and arrival rate, reducing average latency by 27% compared to fixed windows while maintaining throughput."}
```{.python}
def adaptive_batching_window(queue_depth, arrival_rate, slo_ms):
    """Compute optimal batching window.

    Based on current system state.
    """
    target_batch_size = 16  # Optimal batch for GPU utilization

    # Fast path: batch ready, close immediately to minimize latency
    if queue_depth >= target_batch_size:
        return 0

    # Compute maximum allowable wait from SLO constraint
    # Reserve 30% of latency budget for batching,
    # remainder for inference
    max_wait = slo_ms * 0.3

    # Estimate time to accumulate target batch at current arrival rate
    if arrival_rate > 0:
        requests_needed = target_batch_size - queue_depth
        estimated_wait = requests_needed / arrival_rate
        # Return minimum of estimated wait and SLO-constrained maximum
        return min(estimated_wait, max_wait)

    return (
        max_wait  # Low traffic: use full budget to accumulate batch
    )
```
:::

This approach reduces average wait time during high traffic while maintaining batch sizes. For traffic varying between 200-1000 QPS:

- Fixed window (10ms): Average latency 15ms, throughput 650 img/s
- Adaptive window: Average latency 11ms (27% reduction), throughput 680 img/s (5% improvement)

The interplay between window size and batch limits creates a space of possible configurations, each representing a different balance between throughput and latency.

**Throughput-Latency Pareto Frontier**

The batching configuration space forms a Pareto frontier where improving throughput requires accepting higher latency. @tbl-pareto-batching traces this frontier across five representative configurations:

| **Window (ms)** | **Max Batch** | **Avg Latency** | **p99 Latency** | **Throughput** | **Configuration** |
|:---|---:|---:|---:|---:|:---|
| 2 | 16 | 8ms | 18ms | 890 img/s | Ultra-low latency |
| 5 | 32 | 10ms | 22ms | 1,140 img/s | Balanced |
| 10 | 32 | 15ms | 35ms | 1,240 img/s | Moderate latency |
| 20 | 64 | 23ms | 52ms | 1,310 img/s | Throughput-optimized |
| 50 | 128 | 38ms | 98ms | 1,350 img/s | Maximum throughput |

: **Batching Pareto Frontier**: Each configuration represents a different point on the throughput-latency trade-off curve. Moving from 2ms to 50ms windows improves throughput by only 52% while increasing p99 latency by 5.4×. Diminishing returns make aggressive batching costly for latency-sensitive applications. {#tbl-pareto-batching}

#### Practical Configuration Guidelines {#sec-model-serving-systems-practical-configuration-guidelines-9791}

Based on quantitative analysis, principled batching configuration follows these guidelines. Start with the latency budget by allocating 20 to 30 percent of SLO to batching wait time. Estimate traffic using the p95 arrival rate rather than average to account for traffic spikes. Calculate the maximum window as $T_{\text{max}} = 0.3 \times L_{\text{SLO}}$. Determine the batch size limit from GPU memory and p99 latency constraints. Monitor the actual distribution since batch size variance indicates whether traffic assumptions hold.

For ResNet-50 with 50ms SLO and 500 QPS traffic:

- Latency budget for batching: 15ms
- Maximum window: 15ms
- Expected batch size: 7.5
- Maximum batch size: 32 (memory limit)
- Configuration: $T = 12$ms, $b_{\text{max}} = 32$
- Predicted p99 latency: 43ms (within SLO)
- Predicted throughput: 1,180 img/s

### Continuous Batching {#sec-model-serving-systems-continuous-batching-8bb6}

Autoregressive models like language models generate outputs token by token, creating a batching challenge that differs from single-pass models like ResNet-50. Traditional batching processes all sequences in a batch for all generation steps, wasting compute when sequences complete at different times [@yu2022orca]. If one sequence in a batch of 8 finishes after 10 tokens while others need 100 tokens, 87.5% of the compute for that sequence slot is wasted. This inefficiency matters as language models grow to dominate production inference workloads.

Continuous batching (also called iteration-level batching) addresses this waste by allowing new requests to join a batch between generation steps and completed sequences to exit [@kwon2023vllm]. Rather than forming static batches that persist for the entire generation process, the system manages batch composition dynamically at each decoding iteration.

The mechanism works as follows: when a sequence generates its end-of-sequence token, its slot becomes immediately available. A waiting request can fill that slot for the next iteration rather than waiting for the entire batch to complete. Similarly, the system can add new requests to available slots without interrupting ongoing generation. This dynamic approach maintains high GPU utilization even when sequence lengths vary dramatically.

Systems implementing continuous batching, such as vLLM and TensorRT-LLM, achieve 2-4× higher throughput than traditional static batching [@agrawal2024sarathi]. The improvement comes from two sources: eliminating wasted compute on completed sequences and reducing average wait time for new requests. For production language model serving where response lengths vary from single tokens to thousands, continuous batching has become essential for cost-effective deployment.

Memory management adds complexity to continuous batching. As sequences enter and exit the batch, the key-value cache that stores attention context must be dynamically allocated and freed. Consider what happens when sequences of varying lengths share GPU memory: a 100-token sequence completes and releases its cache, but a new 150-token sequence cannot use that space because it needs a larger contiguous block. Over time, small unusable gaps accumulate between allocated regions, eventually preventing new sequences from starting even when total free memory appears sufficient. This *memory fragmentation* can waste 40 to 50 percent of available memory in naive implementations, severely limiting the concurrent batch size that determines throughput.

**PagedAttention**,[^fn-pagedattention] introduced in vLLM, solves this fragmentation problem by applying operating system virtual memory concepts to GPU memory [@kwon2023vllm]. Instead of allocating one contiguous block per sequence, PagedAttention divides the KV cache into fixed-size *pages* (typically 16 tokens each). A sequence's cache consists of pointers to non-contiguous pages scattered across GPU memory. When a sequence completes, its pages return to a free list and can be reused by any new sequence, regardless of length. This approach achieves near-zero fragmentation: vLLM reports memory utilization above 95% compared to 50-60% for contiguous allocation schemes. The overhead is modest (one pointer lookup per page during attention computation), making PagedAttention the standard for production LLM serving.

[^fn-pagedattention]: **PagedAttention**: Introduced by Kwon et al. at SOSP 2023, this algorithm directly applies operating system virtual memory concepts to GPU memory management for LLMs. Before PagedAttention, researchers found that existing systems wasted 60-80% of KV cache memory due to fragmentation and over-reservation. By borrowing paging and copy-on-write mechanisms from OS design, PagedAttention reduces waste to under 4%, enabling 2-4x higher throughput on the same hardware. This technique has become the de facto standard in production LLM serving systems.

The batching and memory techniques covered here establish the foundation for LLM serving, but several advanced topics warrant additional study:

::: {.callout-perspective title="LLM Serving: Beyond the Fundamentals"}

Language model serving introduces challenges beyond the batching and memory principles established here. The key-value cache that stores attention context scales with sequence length and batch size, often exceeding the model weights themselves in memory consumption. Techniques like speculative decoding use small draft models to propose multiple tokens that the target model verifies in parallel, achieving 2-3× latency reduction for interactive applications. Weight-only quantization (INT4 weights with FP16 activations) proves more effective than activation quantization for memory-bandwidth-bound LLM inference.

These LLM-specific optimizations build directly on the foundations this chapter establishes: queuing theory governs request scheduling, batching tradeoffs determine throughput-latency curves, and precision selection follows the same accuracy-efficiency principles. The serving fundamentals apply universally; LLM serving adds domain-specific techniques atop this foundation. Advanced treatments provide detailed coverage of KV cache optimization, including advanced techniques for multi-tenant serving and distributed inference.

:::

While continuous batching represents the state of the art for LLM serving, not all deployment scenarios benefit from batching at all. The sophisticated techniques examined so far, from dynamic batching windows to PagedAttention, optimize for high-throughput server workloads. These techniques, however, introduce complexity and latency overhead that may not be justified for all deployment contexts. A fundamental question remains: *when* does batching hurt rather than help?

**When Not to Batch.** Some scenarios require single-request processing. Ultra-low latency requirements where p99 latency must stay under 10ms make any batching delay unacceptable. Highly variable request sizes where inputs vary dramatically in size cause batching to create padding overhead that wastes compute. Memory constraints where models already consume most GPU memory mean batch activations may cause out-of-memory errors.

### Session Affinity Constraints {#sec-model-serving-systems-session-affinity-constraints-8b1f}

When requests from the same user or session should route to the same replica, batching becomes constrained. Session affinity, also called sticky sessions, matters for three main reasons.

**KV-Cache Reuse**: For conversational AI, the key-value cache from previous turns dramatically speeds up multi-turn conversations. Routing a follow-up request to a different replica forfeits this cached context, increasing latency by 2 to 5 times for long conversations.

**User-Specific Models**: Some systems serve personalized models or adapters per user. Routing requests to the replica that has already loaded that user's adapter avoids repeated loading overhead.

**Stateful Preprocessing**: When preprocessing maintains state through tokenizer caches or session-specific normalization, routing to a different replica requires rebuilding this state.

The tension with batching is clear since strict affinity constrains which requests can be batched together, potentially reducing batch sizes and GPU utilization. Production systems often implement soft affinity where requests prefer their assigned replica but can overflow to others when that replica is overloaded. This preserves most affinity benefits while maintaining load balance.

### Traffic Patterns and Batching Strategy {#sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b}

The optimal batching strategy depends critically on how requests arrive. Different deployment contexts exhibit fundamentally different arrival patterns, each requiring distinct batching approaches. The MLPerf inference benchmark codifies these patterns into four scenarios that directly map to real-world deployments, as @sec-benchmarking-ai explains in detail.

**Server Traffic (Poisson Arrivals).** Cloud APIs and web services typically receive requests following a Poisson process,[^fn-poisson-process] where arrivals are independent and uniformly distributed over time. @eq-poisson-batch expresses the expected batch size for Poisson arrivals with rate $\lambda$ and batching window $T$:

[^fn-poisson-process]: **Poisson Process**: A stochastic model where events occur continuously and independently at a constant average rate. Named after French mathematician Simeon Denis Poisson (1781-1840), this model accurately describes many real-world arrival patterns including web requests and API calls. The key property for serving systems is that inter-arrival times are exponentially distributed, meaning the probability of long gaps between requests decays exponentially, which is why batching windows can be tuned probabilistically.

$$E[\text{batch size}] = \lambda \cdot T$$ {#eq-poisson-batch}

The variance equals the mean (a property of Poisson distributions), so batch sizes fluctuate significantly at moderate traffic. With $\lambda = 200$ requests/second and $T = 10$ms, expected batch size is 2, but 16% of windows will have zero requests (wasted compute cycles) while others may have 4 or more.

The optimal batching window balances waiting cost against throughput benefit. @eq-optimal-window defines this optimum:

$$T_{\text{optimal}} = \min\left(L - S, \sqrt{\frac{S}{\lambda}}\right)$$ {#eq-optimal-window}

where $L$ is the latency SLO and $S$ is the service time. A perhaps surprising result emerges from this equation: as traffic increases, the optimal window decreases while achieved batch sizes still grow. @tbl-traffic-adaptive demonstrates this phenomenon across four traffic levels.

| **Arrival Rate** | **Optimal Window** | **Avg Batch Size** | **p99 Latency** |
|:---|---:|---:|---:|
| **100 QPS** | 20ms | 2.0 | 45ms |
| **500 QPS** | 8ms | 4.0 | 42ms |
| **1,000 QPS** | 5ms | 5.0 | 38ms |
| **5,000 QPS** | 2ms | 10.0 | 35ms |

: **Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time. {#tbl-traffic-adaptive}

**Streaming Traffic (Correlated Arrivals).** Autonomous vehicles, video analytics, and robotics systems receive inputs from multiple synchronized sensors. This scenario illustrates *multi-camera autonomous vehicle serving*.

::: {.callout-notebook title="Multi-Camera Autonomous Vehicle Serving"}

Consider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial fusion:

**Timeline for processing frame set N:**

| **Time** | **Event** |
|:---|:---|
| T = 0ms | Cameras begin capturing frame N |
| T = 8ms | Camera 1 frame arrives |
| T = 10ms | Cameras 2-5 frames arrive |
| T = 15ms | Camera 6 arrives (jitter) |
| T = 15ms | Batch inference begins (6 images) |
| T = 25ms | Inference complete |
| T = 32ms | Result ready for planning module |

**Key constraints:**

- Hard deadline: 33ms per frame set (real-time requirement)
- Batch size: Fixed at 6 (one per camera)
- Synchronization budget: 12ms of 33ms total (36% for jitter tolerance)
- Timeout policy: If camera frame not received by T+20ms, use previous frame

Unlike Poisson traffic where dynamic batching optimizes throughput, streaming traffic requires synchronization policies that handle sensor jitter while meeting hard deadlines.

:::

**Single-User Traffic (Sequential Arrivals).** Mobile and embedded applications serve one user at a time, with requests arriving only after the previous result is consumed. We can analyze these constraints in *ResNet-50 mobile serving*.

::: {.callout-notebook title="ResNet-50: Mobile Serving"}

```{python}
#| label: mobile-serving-calc
#| echo: false

# Mobile serving breakdown
m_cam_ms = 8
m_jpeg_ms = 15
m_resize_ms = 5
m_npu_ms = 12
m_ui_ms = 5
m_total_ms = m_cam_ms + m_jpeg_ms + m_resize_ms + m_npu_ms + m_ui_ms

m_cam_mj = 0.08
m_jpeg_mj = 1.5
m_resize_mj = 0.4
m_npu_mj = 0.8
m_ui_mj = 0.2
m_total_mj = m_cam_mj + m_jpeg_mj + m_resize_mj + m_npu_mj + m_ui_mj

# Strings
m_cam_ms_str = f"{m_cam_ms}ms"
m_jpeg_ms_str = f"{m_jpeg_ms}ms"
m_resize_ms_str = f"{m_resize_ms}ms"
m_npu_ms_str = f"{m_npu_ms}ms"
m_ui_ms_str = f"{m_ui_ms}ms"
m_total_ms_str = f"{m_total_ms}ms"

m_cam_mj_str = f"{m_cam_mj}mJ"
m_jpeg_mj_str = f"{m_jpeg_mj}mJ"
m_resize_mj_str = f"{m_resize_mj}mJ"
m_npu_mj_str = f"{m_npu_mj}mJ"
m_ui_mj_str = f"{m_ui_mj}mJ"
m_total_mj_str = f"{m_total_mj:.1f}mJ"
```

| **Phase** | **Duration** | **Energy** | **Notes** |
|:---|:---|:---|:---|
| **Camera buffer read** | `{python} m_cam_ms_str` | `{python} m_cam_mj_str` | System API |
| **JPEG decode (CPU)** | `{python} m_jpeg_ms_str` | `{python} m_jpeg_mj_str` | Single-threaded |
| **Resize + Normalize** | `{python} m_resize_ms_str` | `{python} m_resize_mj_str` | CPU preprocessing |
| **NPU inference** | `{python} m_npu_ms_str` | `{python} m_npu_mj_str` | 82% utilization |
| **Post-process + UI** | `{python} m_ui_ms_str` | `{python} m_ui_mj_str` | Result rendering |
| **Total** | **`{python} m_total_ms_str`** | **`{python} m_total_mj_str`** | 22 FPS sustained |

**Key metrics for ML node serving:**

- **Energy per inference**: 3.0mJ enables ~9,000 inferences per 10Wh battery (typical smartphone)
- **Thermal budget**: At 3.0mJ/45ms = 67mW sustained, indefinite operation without throttling
- **NPU vs CPU tradeoff**: CPU fallback uses 4.2mJ (1.4× energy) at 85ms (1.9× latency)
- **Memory footprint**: 150MB peak (model + activations), competing with app memory

**Critical insight**: Even at batch size 1, the mobile NPU achieves 82% utilization because its compute capacity matches single-image workloads. This differs from datacenter GPUs, which achieve only 15% utilization at batch size 1 because their massive parallelism requires larger batches to saturate.

:::

#### Mobile Serving Constraints {#sec-model-serving-systems-mobile-serving-constraints-eb68}

Unlike cloud serving where cost dominates, mobile serving faces three related constraints that shape optimization strategy:

1. **Energy Budget**: Each inference depletes battery. A photo app running continuous inference at 22 FPS drains 240mW, acceptable for active use but problematic for background processing. The optimization target shifts from throughput to energy-per-inference.

2. **Thermal Throttling**: Sustained high-power operation triggers thermal management. When the SoC reaches thermal limits (typically 45°C junction), the OS reduces NPU frequency by 30-50%, degrading both latency and throughput. Bursty workloads that allow cooling between bursts outperform sustained maximum throughput.

3. **Memory Constraints**: Mobile devices share limited RAM between applications. A model consuming 500MB may be evicted during background operation, requiring reload (cold start) that adds 200-500ms latency. Even a 150MB footprint becomes problematic when the model must coexist with other app components. Memory-efficient quantization directly improves user experience through faster model restoration, and memory-mapped model loading (@sec-model-serving-systems-loading-strategies-eb38) helps further by loading pages on demand rather than requiring the full model in memory.

These constraints make mobile serving optimization fundamentally different from cloud optimization. The goal is not maximum throughput but **sustainable performance**, maintaining acceptable latency without thermal throttling or excessive battery drain.

@tbl-traffic-patterns-summary maps the four MLPerf scenarios to their deployment contexts and optimal batching strategies, providing a decision framework for serving system design.

| **Scenario** | **Context** | **Strategy** | **Focus** |
|:---|:---|:---|:---|
| **Server** | Cloud APIs, web services | Dynamic batching with timeout | Window tuning, utilization-latency curve |
| **MultiStream** | Autonomous driving, video analytics | Synchronized sensor fusion | Jitter handling, deadline guarantees |
| **SingleStream** | Mobile apps, embedded devices | No batching (batch=1) | Preprocessing, power efficiency |
| **Offline** | Batch processing, data pipelines | Maximum batch size | Throughput, hardware utilization |

: **Traffic Patterns and Batching Strategies**: The four MLPerf inference scenarios map to distinct deployment contexts. Server traffic (cloud APIs) uses dynamic batching with timeout; MultiStream (autonomous driving) uses synchronized sensor fusion; SingleStream (mobile) processes requests individually; Offline (batch processing) maximizes batch size for throughput. {#tbl-traffic-patterns-summary}

## LLM Serving {#sec-model-serving-systems-llm-serving-b8bf}

The traffic patterns and batching strategies examined in the previous section share a common assumption: models produce a single output per request, whether a classification label, a bounding box, or an embedding vector. Large language models break this assumption, generating tokens incrementally over hundreds or thousands of iterations and creating a different latency profile. The p50, p95, and p99 metrics that govern classification serving apply differently when a single request takes 2 to 3 seconds to complete but must feel responsive throughout. While the foundational principles of queuing theory, batching tradeoffs, and latency budgets apply universally, LLMs require additional metrics, different optimization strategies, and unique memory management techniques.

### Performance Metrics: TTFT and TPOT {#sec-model-serving-systems-performance-metrics-ttft-tpot-b009}

Generative models produce a stream of tokens rather than a single output tensor. This streaming nature requires dedicated *LLM performance metrics* that reflect the internal state transition from "prefill" (processing input) to "decode" (generating output). The two key measures are *Time to First Token (TTFT)* and *Time Per Output Token (TPOT)*, which capture responsiveness and fluidity respectively.

::: {.callout-definition title="LLM Performance Metrics"}

***Time to First Token (TTFT)*** measures latency from request to first output token, governed by the compute-bound **Prefill Phase** (processing the full prompt) [@pope2023efficiently]. **Time Per Output Token (TPOT)** measures latency of each subsequent token, governed by the memory-bandwidth-bound **Decode Phase** (autoregressive KV cache lookups). This decomposition isolates the distinct hardware bottlenecks (**Compute** versus **Memory Bandwidth**), enabling targeted optimization of each phase.

:::

These two metrics capture fundamentally different user experience aspects. A fast TTFT provides immediate responsiveness (the system starts answering quickly), while a fast TPOT provides fluid generation (the answer streams smoothly). Production systems must optimize both, typically with different techniques since they are governed by different hardware constraints. Translating these metrics into concrete *LLM serving latency targets* grounds the discussion in production reality.

::: {.callout-lighthouse title="LLM Serving Latency Targets"}

A production-grade LLM service typically targets the following SLOs:

- **TTFT**: < 500 ms (for a 1000-token prompt)
- **TPOT**: < 50 ms (equivalent to ~20 tokens/second, faster than human reading speed)
- **Throughput**: > 1000 tokens/second aggregate across all users

:::

### Decoding Strategies {#sec-model-serving-systems-decoding-strategies-afe8}

Generative models require decoding strategies that trade off quality, diversity, and latency. The choice of decoding strategy can dramatically affect both output quality and computational cost.

**Greedy decoding** selects the highest-probability token at each step. Fast but often produces repetitive, low-quality outputs because it cannot recover from early mistakes.

**Beam search** maintains multiple candidate sequences, selecting the highest-scoring complete sequence. Produces higher-quality outputs but multiplies computation by the beam width.

**Sampling** with temperature, top-k, and top-p parameters introduces randomness for diversity [@holtzman2020curious]. Temperature scales logits before softmax. Top-k limits sampling to the k highest-probability tokens. Top-p, also called nucleus sampling, limits sampling to tokens comprising probability mass p.

The choice presents latency tradeoffs [@meister2020beam]. Beam search with width 5 takes roughly 5× the compute of greedy decoding. Sampling adds minimal overhead but requires careful parameter tuning to balance quality and coherence.

**Streaming Responses.** Rather than waiting for complete generation, production LLM systems return tokens as they are produced. This improves perceived latency since users see output beginning quickly, but requires infrastructure support for chunked HTTP responses and client-side incremental rendering. Streaming changes the latency profile: TTFT determines when output starts appearing, while TPOT determines the perceived generation speed.

### Memory and KV Cache {#sec-model-serving-systems-memory-kv-cache-d1ea}

Generative inference requires managing the **KV Cache**, a stateful memory structure that grows with sequence length. Unlike traditional models where memory usage is constant per batch, LLM memory usage is dynamic:

*   **State Accumulation**: Each generated token adds to the context window, consuming additional GPU memory.
*   **Fragmentation**: Variable-length sequences can lead to memory fragmentation if not managed explicitly.

The continuous batching and PagedAttention techniques covered in @sec-model-serving-systems-continuous-batching-8bb6 address these challenges. Advanced techniques including prefix caching and speculative decoding are covered in specialized coverage of large-scale systems.

The computational intensity of managing KV caches across concurrent requests raises a broader question: *what* is the energy cost of each token generated? Translating these hardware demands into energy and carbon metrics makes the environmental impact of LLM serving concrete.

::: {.callout-notebook #notebook-carbon-chat title="The Carbon Cost of a Chat"}

**Joules per Token: The Green Metric**:
As LLMs scale, energy efficiency becomes a first-class operational metric alongside latency. For an H100 GPU (`{python} h100_tdp`W TDP), we can quantify the energy footprint of serving:

1.  **Throughput**: 114 concurrent requests × 7.5 tokens/sec/req $\approx$ **855 tokens/sec**.
2.  **Power**: `{python} h100_tdp`W (GPU) + 300W (Host/Overhead) = **1000W**.
3.  **Energy per Token**:

    $$\frac{1000 \text{ Joules/sec}}{855 \text{ tokens/sec}} \approx \mathbf{1.17 \text{ Joules/token}}$$

**The Systems Conclusion**: A typical 500-token response consumes $\approx \mathbf{585 \text{ Joules}}$.

- For comparison, charging a smartphone consumes $\approx 40,000$ Joules.
- Boiling a cup of water consumes $\approx 100,000$ Joules.

**The Engineering Lever**: The primary way to reduce Joules/Token is to **increase hardware utilization**. If the GPU sits at 10% utilization due to poor batching, the "Idle Power" is still ~300W, causing the energy-per-token to skyrocket to **>10 Joules**. MLOps is not just about speed; it is about sustainability through efficiency.
:::

## Inference Runtime Selection {#sec-model-serving-systems-inference-runtime-selection-5eef}

The batching strategies and LLM-specific techniques examined in preceding sections determine *how* requests are grouped and processed. These strategies, however, assume an underlying execution engine that actually runs the model computations. The execution environment directly affects whether the latency budgets established earlier are achievable. The inference runtime, the software layer that orchestrates tensor operations and manages hardware resources, can vary by an order of magnitude in performance for identical models. Choosing appropriately requires understanding the tradeoffs between framework-native serving, general-purpose optimization, and specialized inference engines.

### Runtime Ecosystem and Configuration {#sec-model-serving-systems-frameworknative-serving-da62}

PyTorch and TensorFlow models can serve directly using their native runtimes. This approach maximizes compatibility (any model that trains will serve) and simplifies the deployment pipeline (no export or conversion step). However, framework runtimes include training functionality that adds overhead, and default execution paths may not exploit hardware-specific optimizations.

TorchScript and TensorFlow SavedModel formats enable ahead-of-time compilation and graph optimization, improving over eager execution while maintaining framework compatibility. These formats represent the first step toward deployment optimization without abandoning the familiar framework ecosystem.

**General-Purpose Optimization.** ONNX Runtime provides a hardware-agnostic optimization layer [@onnxruntime2024]. Models export to ONNX format, then ONNX Runtime applies graph optimizations and selects execution providers for the target hardware. This enables single-format deployment across CPUs, GPUs, and specialized accelerators.

[](#sec-model-serving-systems-specialized-inference-engines-1924) **Specialized Inference Engines.** TensorRT[^fn-tensorrt-serving] (NVIDIA GPUs), OpenVINO (Intel hardware), and similar engines optimize specifically for their target hardware [@nvidia2024tensorrt; @chen2018tvm]. They apply aggressive optimizations that framework-native runtimes cannot safely perform:

[^fn-tensorrt-serving]: **TensorRT**: NVIDIA's inference optimization SDK that applies layer fusion, kernel auto-tuning, and precision calibration to neural networks. Unlike framework-native runtimes that preserve training-time graph structure, TensorRT rebuilds the computation graph for the specific target GPU during a build phase. This GPU-specific compilation means TensorRT engines are not portable across GPU architectures, requiring separate builds for V100, A100, and H100 deployments. The build phase can take minutes but produces engines that often achieve 2-5x speedup over PyTorch.

**Layer fusion** combines multiple sequential operations into a single GPU kernel. Consider a common pattern: convolution → batch normalization → ReLU activation. Without fusion, this requires three kernel launches, three round-trips to GPU memory (write conv output, read for batchnorm, write batchnorm output, read for ReLU), and three sets of intermediate tensors. Fusion combines all three into one kernel that reads inputs once, computes the combined result in registers, and writes final outputs once. This eliminates kernel launch overhead (15-60μs saved per fusion) and reduces memory traffic by 2-3×. TensorRT automatically detects and fuses common patterns; a typical ResNet-50 reduces from ~50 kernels to ~15 after fusion.

**Kernel auto-tuning** selects the fastest algorithm for each operation on the specific GPU. A single convolution can be implemented using dozens of algorithms (direct, FFT-based, Winograd, various tiling strategies), each optimal for different input sizes and GPU architectures. Auto-tuning benchmarks each candidate and caches the winner, trading compilation time for runtime performance.

These optimizations typically achieve 2-5x speedup over framework-native serving but require explicit export and may not support all operations. A *runtime comparison* on a standard model quantifies these gains across the optimization spectrum.

::: {.callout-notebook title="ResNet-50: Runtime Comparison"}

Performance comparison for ResNet-50 inference on V100 GPU (batch size 1):

| **Runtime** | **Latency** | **Speedup** | **Notes** |
|:---|---:|---:|:---|
| PyTorch (eager) | 8.5ms | 1.0× | Baseline, no optimization |
| TorchScript | 6.2ms | 1.4× | JIT compilation |
| ONNX Runtime | 5.1ms | 1.7× | Cross-platform |
| TensorRT FP32 | 2.8ms | 3.0× | NVIDIA-specific |
| TensorRT FP16 | 1.4ms | 6.1× | Tensor Core acceleration |
| TensorRT INT8 | 0.9ms | 9.4× | Requires calibration |

**Key insight**: The 9.4× speedup from TensorRT INT8 comes at the cost of: (1) quantization calibration data, (2) potential accuracy loss (<1% for ResNet-50), and (3) NVIDIA-specific deployment.

:::

The optimization-compatibility tradeoff is inherent. More aggressive optimization yields better performance but increases deployment complexity and may introduce numerical differences from training. The choice depends on latency requirements, deployment constraints, and available engineering resources.

**Runtime Configuration.** Beyond runtime selection, configuration choices impact serving performance including thread pools that control parallelism for CPU inference, memory allocation strategies that choose between pre-allocating buffers versus dynamic allocation, execution providers that select and prioritize hardware backends, and graph optimization level that trades compilation time for runtime performance. Production deployments require experimentation to find optimal configurations for specific models and hardware combinations. A systematic approach tests key parameters and measures their impact on latency distributions.

### Precision Selection for Serving {#sec-model-serving-systems-precision-selection-serving-55ba}

Numerical precision directly trades accuracy for throughput, connecting to the quantization techniques covered in @sec-model-compression. While @sec-model-compression focuses on training-time quantization, serving introduces additional considerations including calibration requirements, layer sensitivity, and dynamic precision selection.

**Precision-Throughput Relationship.** For memory-bandwidth-bound operations, reducing precision proportionally increases throughput by reducing data movement. @eq-precision-throughput quantifies the theoretical maximum speedup from precision reduction:

$$\frac{\text{Throughput}_{\text{INT8}}}{\text{Throughput}_{\text{FP32}}} = \frac{32}{8} = `{python} cp_precision_ratio_str`\times \text{ (theoretical maximum)}$$ {#eq-precision-throughput}

In practice, GPU compute pipelines and Tensor Core alignment requirements limit achieved speedup to 2.5-3.5x for INT8 versus FP32. Tensor Cores require specific alignment: INT8 operations need tensor dimensions divisible by 16, while FP16 requires divisibility by 8. @sec-ai-acceleration provides the detailed Tensor Core architecture that explains these alignment constraints. The *precision tradeoffs* for a standard vision model illustrate how these theoretical limits manifest in practice.

::: {.callout-notebook title="ResNet-50: Precision Tradeoffs on V100"}

| **Precision** | **Latency** | **Memory** | **Accuracy** | **Tensor Core Util.** | **Calibration** |
|:---|---:|---:|---:|---:|:---|
| **FP32** | 2.8ms | 98MB | 76.13% | 0% | None |
| **FP16** | 1.4ms | 49MB | 76.13% | 85% | None |
| **INT8 (PTQ)** | 0.9ms | 25MB | 75.80% | 92% | 1,000 samples |
| **INT8 (QAT)** | 0.9ms | 25MB | 76.05% | 92% | Full retraining |

**Key observations:**

- INT8 achieves 3.1× speedup but loses 0.33% accuracy with post-training quantization (PTQ)
- Quantization-aware training (QAT) recovers most accuracy but requires retraining
- FP16 provides 2× speedup with no accuracy loss for most models

:::

**Layer Sensitivity.** Not all layers tolerate reduced precision equally. @eq-quant-error captures how quantization error for a layer scales with weight magnitude and gradient sensitivity:

$$\epsilon_{\text{quant}} \propto \alpha \cdot \|W\|_2 \cdot 2^{-b}$$ {#eq-quant-error}

where $\alpha$ is a layer-specific sensitivity coefficient, $\|W\|_2$ is the weight L2 norm, and $b$ is the bit width. This explains observed patterns where first convolutional layers with high gradients and large sensitivity coefficients are precision-sensitive and often kept at FP16, middle layers with stable gradients and low sensitivity coefficients tolerate INT8 well, and final classification layers with small weights but high task sensitivity benefit from FP16 or higher precision.

**Calibration Requirements.** Post-training quantization requires a calibration dataset to determine optimal scale factors for INT8 conversion. Production experience shows that calibration data must be representative of actual serving traffic, not just training data. Using ImageNet validation images to calibrate a model serving wildlife camera images resulted in 3.2% accuracy degradation in one production system.

**Dynamic Precision Selection.** Advanced serving systems select precision per request based on runtime conditions. If the system is ahead of latency SLO, it uses higher precision for better accuracy. For low-confidence INT8 results, it recomputes at FP16. Different customer tiers may receive different precision levels. This pattern enables adaptive quality-latency tradeoffs while maximizing throughput during normal operation.

The precision decision has direct infrastructure consequences: INT8 inference achieves roughly 3x higher throughput than FP32, meaning a workload requiring 30 GPUs at FP32 needs only 10 at INT8. This 3x reduction in hardware translates directly to a 3x reduction in operating costs. The connection between model-level optimization and infrastructure economics is why precision selection cannot be treated as purely a model concern.

## Node-Level Optimization & Profiling {#sec-model-serving-systems-node-level-optimization-profiling}

Beyond selecting a runtime and precision, maximizing the efficiency of a single ML node requires examining in detail *how* the hardware executes the model. This section explores optimizations that occur at the boundaries of software and silicon: compiling the computation graph, exploiting CPU capabilities when GPUs are absent, minimizing the time to get bytes from disk to memory, and visualizing exactly *where* every microsecond goes.

### Runtime Graph Compilation {#sec-model-serving-systems-runtime-graph-compilation}

We introduced inference engines like TensorRT in @sec-model-serving-systems-inference-runtime-selection-5eef, but *how* do they achieve 2-5x speedups? The answer lies in **Graph Compilation**. Unlike training, where the computation graph is dynamic and mutable, serving graphs are static. This allows compilers to perform aggressive optimizations that would be unsafe or too slow during training.

**Operator Fusion**: The most potent optimization. As discussed in @sec-ai-acceleration, memory bandwidth often limits performance more than compute. Fusion collapses multiple operations (e.g., `Conv2D` -> `BiasAdd` -> `ReLU`) into a single kernel launch. This keeps intermediate data in the GPU's fast L1/L2 cache or registers, avoiding round-trips to global memory (VRAM).

**Constant Folding**: Parts of the graph that depend only on model weights (which are constant during serving) can be pre-computed at compile time. For example, if a model contains `x * (sqrt(2) / 2)`, the compiler replaces the division and square root with a single multiplication by `0.707...`.

**Memory Planning**: Since the graph structure is known, the compiler can pre-calculate the exact memory offsets for every tensor. This leads to the fundamental architectural choice of *JIT vs. AOT compilation*.

::: {.callout-notebook title="JIT vs. AOT Compilation"}
*   **Just-In-Time (JIT)**: Compiles the graph the first time it is run (e.g., `torch.compile`).
    *   *Pros*: Optimizes for the specific input shapes seen at runtime.
    *   *Cons*: First request pays a "compilation penalty" (latency spike).
*   **Ahead-of-Time (AOT)**: Compiles the graph before deployment (e.g., `torch.export`, TensorRT `trtexec`).
    *   *Pros*: Zero compilation latency at startup; guarantees a fixed graph.
    *   *Cons*: Must handle all dynamic shapes explicitly or compile multiple profiles.
:::

### CPU Inference Optimization {#sec-model-serving-systems-cpu-inference-optimization}

While GPUs dominate the narrative, CPUs remain the workhorse for a vast number of inference workloads, particularly for smaller models, latency-insensitive batch jobs, or cost-constrained environments. Optimizing for the CPU requires a different mindset.

**SIMD and Vectorization**: Modern CPUs (Intel Xeon, AMD EPYC) pack powerful vector units (AVX-512, AMX). Standard Python loops cannot use these. Specialized runtimes like **OpenVINO** or **Intel Extension for PyTorch (IPEX)** map neural network operators directly to these vector instructions, achieving order-of-magnitude speedups over vanilla implementations.

**Thread Pinning & NUMA**: On multi-socket servers, accessing memory attached to a different CPU socket (NUMA) adds significant latency. Inference servers must be "NUMA-aware," pinning threads to specific cores and ensuring that memory allocations remain local to those cores.

**The "Small Batch" Advantage**: CPUs often outperform GPUs at batch size 1 for small models. The overhead of launching a GPU kernel (~10$\mu$s) and transferring data (~50$\mu$s) can exceed the compute time for a tiny dense layer. For models under 50MB serving single requests, a well-optimized CPU runtime often delivers lower latency than a GPU.

### Fast Model Loading {#sec-model-serving-systems-fast-model-loading}

In autoscaling systems, the time to spin up a new node is critical. A major component of "Cold Start" (@sec-model-serving-systems-model-loading-initialization-cc5a) is simply reading the model weights from disk into memory.

**The Pickle Problem**: The standard PyTorch `torch.load()` uses Python's `pickle` format. This is inefficient because it requires the CPU to unpickle objects one by one, copy them into memory, and then often copy them *again* to the GPU.

**Zero-Copy with `mmap`**: Memory mapping allows the OS to map a file directly into the process's virtual address space. The data is effectively "loaded" only when accessed, and the OS handles the transfer from disk to RAM efficiently.

**Safetensors**: A modern format designed specifically for fast loading. It stores tensors as raw bytes with a minimal JSON header. This allows for **zero-copy** loading: the raw bytes on disk are mapped directly into the tensor's memory buffer.

::: {.callout-example title="Loading Speed: Safetensors vs. Pickle"}
Loading a 5GB Stable Diffusion model:

*   **Pickle (`torch.load`)**: ~15 seconds. High CPU usage.
*   **Safetensors**: ~0.5 seconds. Near-zero CPU usage.

By using `mmap` and formats like `safetensors`, loading speed becomes limited only by the disk's read speed (e.g., 3GB/s for NVMe), rather than CPU parsing overhead.
:::

### Profiling the Serving Node {#sec-model-serving-systems-profiling-serving-node}

Optimization without measurement is guesswork. To truly master the ML node, you must visualize the execution flow.

**The Timeline View**: Tools like **PyTorch Profiler** or NVIDIA **Nsight Systems (nsys)** generate a timeline trace. This visualization reveals the exact sequence of events on the CPU and GPU.

**What to Look For:**
1.  **Gaps in the GPU Timeline**: If the GPU bar has empty spaces, the GPU is idle. This usually means the GPU is waiting for the CPU (preprocessing bottleneck) or disk (data loading).
2.  **Kernel Launch Overhead**: If you see thousands of tiny slivers on the GPU timeline, your model is launching too many small kernels. This is a prime candidate for **Operator Fusion**.
3.  **Host-to-Device Transfers**: Look for `MemcpyHtoD` (Host to Device) blocks. Are they overlapping with computation, or blocking it?

::: {.callout-example title="The Profiling Loop"}
1.  **Capture**: Run a warmup, then capture a trace of 10-50 requests.
2.  **Visualize**: Open the trace in a viewer (Chrome Tracing, Nsight).
3.  **Identify**: Find the largest gap or the longest block.
4.  **Optimize**: Apply a specific fix (e.g., fusion, pinning).
5.  **Verify**: Re-capture and confirm the gap is gone.
:::

**Optimization Technique Impact Matrix**

To guide optimization efforts, @tbl-optimization-impact summarizes the key techniques available at the node level, their primary targets, and expected returns.

| **Technique** | **Target Metric** | **Typical Gain** | **Implement. Cost** | **Best For** |
|:---|:---|---:|:---|:---|
| **Operator Fusion** | Latency & Throughput | 2-5$\times$ | Medium (Compiler) | Memory-bound layers |
| **INT8 Quantization** | Throughput | 3-4$\times$ | High (Calibration) | Inference-heavy nodes |
| **Graph Compilation** | Latency | 1.5-3$\times$ | Low (One-line) | Static graph models |
| **Zero-Copy Loading** | Startup Time | 10-50$\times$ | Low (File format) | Autoscaling / Cold Start |
| **CPU Pinning** | Tail Latency (P99) | 20-50% reduction | Low (Config) | Latency-critical apps |

: **Node-Level Optimization Impact**: A decision matrix for selecting optimization techniques. High-impact techniques like quantization often carry higher implementation costs (calibration data requirements), while architectural changes like zero-copy loading offer dramatic gains for specific metrics (startup time) with low effort. {#tbl-optimization-impact}

This hierarchy of impact guides where to invest engineering effort. Use the following checklist to prioritize your optimization strategy.

::: {.callout-checkpoint title="The Optimization Hierarchy" collapse="false"}
Optimizing inference requires a layered approach.

**The Stack**

- [ ] **System Level**: Have you minimized network round trips and serialization overhead? (gRPC, persistent connections).
- [ ] **Application Level**: Are you batching requests effectively? (Dynamic batching).
- [ ] **Model Level**: Is the model compiled for the target hardware? (TensorRT, ONNX Runtime).
- [ ] **Kernel Level**: Are operations fused to minimize memory bandwidth?
:::

## Economics and Capacity Planning {#sec-model-serving-systems-economics-capacity-planning-3e7e}

The runtime selection, precision tuning, and node-level optimizations examined in the preceding sections collectively determine the fundamental unit of serving physics: the performance per inference. Production deployment requires translating these technical metrics into infrastructure decisions. Serving costs scale with request volume, unlike training costs that scale with dataset size and model complexity [@zhang2019mark]. Cost structure analysis enables decisions that balance performance requirements against budget constraints.

**Cost Per Inference.** Total serving cost decomposes into several components including compute time for GPU or CPU per inference, memory for accelerator memory required to hold model and activations, data transfer for network bandwidth for request and response payloads, and orchestration overhead for container runtime, load balancing, and monitoring. For GPU inference, compute time dominates when utilization is high. When utilization is low, memory cost dominates because the GPU is reserved but idle. We can apply this framework to a *ResNet-50 cost analysis*.

::: {.callout-notebook title="ResNet-50: Cost Analysis"}

Consider serving ResNet-50 on AWS infrastructure (US-East region, on-demand pricing as of 2024):

| **Instance Type** | **Cost/Hour** | **Throughput** | **Cost per 1M Images** |
|:---|---:|---:|---:|
| **c5.xlarge (CPU)** | $0.17 | 50 img/s | $0.94 |
| **g4dn.xlarge (T4 GPU)** | $0.53 | 400 img/s | $0.37 |
| **p3.2xlarge (V100 GPU)** | $3.06 | 1,200 img/s | $0.71 |

**Key insight**: The T4 GPU instance achieves the lowest cost per inference despite higher hourly cost, because GPU throughput dramatically exceeds CPU throughput. The V100 is only cost-effective at very high sustained traffic where its higher throughput justifies the 6x price increase. Note that cloud pricing varies by region and changes over time; consult current pricing for production planning.

:::

### GPU vs CPU Economics {#sec-model-serving-systems-gpu-vs-cpu-economics-eb06}

GPUs provide significant speedup for parallel operations but cost more per hour [@wu2019machine]. The crossover point depends on model characteristics and latency requirements.

CPU inference makes economic sense when models are small with few parameters and simple operations, latency requirements are relaxed with hundreds of milliseconds acceptable, request volume is low or highly variable, and models use operations that do not parallelize well. GPU inference makes economic sense when models are large with parallel-friendly operations, latency requirements are strict at tens of milliseconds, request volume is high and consistent, and batching can achieve high utilization.

**Scaling Responsiveness.** Beyond steady-state costs, startup time affects scaling economics. CPU instances typically start in 30 to 60 seconds while GPU instances take 2 to 5 minutes including driver initialization, model loading, and warmup. For variable traffic patterns, this startup latency can be more important than cost per inference. If traffic spikes arrive faster than GPU instances can scale, latency SLOs will be violated despite having sufficient eventual capacity.

This asymmetry suggests different scaling strategies where CPU instances enable reactive scaling by responding to current demand while GPU instances often require predictive scaling by provisioning based on anticipated demand. For bursty workloads, a hybrid approach uses always-on GPU capacity for baseline load plus CPU overflow capacity for spikes, trading higher per-inference cost during spikes for better responsiveness.

### Capacity Planning {#sec-model-serving-systems-capacity-planning-96a3}

The GPU versus CPU decision establishes the cost per inference, but determining how much infrastructure to provision requires combining cost analysis with the queuing theory foundations from @sec-model-serving-systems-queuing-theory-tail-latency-29a6. Capacity planning translates latency requirements and traffic projections into infrastructure specifications. Key inputs include traffic patterns such as peak request rate, daily and weekly cycles, and growth projections, latency SLOs including p50, p95, and p99 targets, and model characteristics such as inference time distribution at various batch sizes. From these inputs, queuing theory determines required capacity [@harchol2013performance]. The equations developed in @sec-model-serving-systems-queuing-theory-tail-latency-29a6 provide the mathematical foundation where @eq-mm1-wait shows how latency scales with utilization, while @eq-p99-latency enables calculating p99 latency for capacity planning.

The relationship between utilization and latency is nonlinear as shown in the utilization-latency table. At 70 percent utilization, p99 latency is approximately fifteen times service time. At 90 percent utilization, it reaches approximately 46 times service time. This nonlinearity explains why systems that seem healthy with low average latency can suddenly violate SLOs when traffic increases modestly.

The worked example in @sec-model-serving-systems-queuing-theory-tail-latency-29a6 demonstrates the complete capacity planning process by starting from a 50ms p99 SLO and 5,000 QPS target, deriving the safe utilization threshold of 72 percent, calculating required service rate of 6,944 QPS, and determining GPU count with headroom of 10 V100s. Production systems typically provision for peak load plus 30 percent headroom, using auto-scaling to reduce costs during low-traffic periods while meeting latency objectives during peaks.

### Production Case Study: Serving Llama-3-8B {#sec-model-serving-systems-production-case-study-serving-llama38b-0499}

To apply the principles of latency budgeting, memory management, and hardware efficiency, we analyze the production profile of a modern Large Language Model (LLM) serving workload. This case study demonstrates how physical constraints (memory bandwidth and PCIe capacity) translate directly into service-level metrics and unit economics.

@fig-kv-cache-growth visualizes the memory pressure that explains *why* long-context serving is memory-bound even on H100s.

```{python}
#| label: fig-kv-cache-growth
#| echo: false
#| fig-cap: "**The KV-Cache Explosion**: Memory usage vs. Context Length for a 70B parameter model. The linear growth of the Key-Value cache (storing attention history) quickly consumes available GPU memory (red dashed line). For batch size 32 (purple), the system hits the 'OOM Zone' at just 8k context length, forcing a trade-off between batch size (throughput) and context window (capability)."

import sys
import os
sys.path.append(os.path.abspath("../../../calc"))
import viz
import matplotlib.pyplot as plt

viz.set_book_style()
viz.plot_kv_cache_growth()
plt.show()
```

The linear growth of the KV cache with sequence length forces a hard trade-off: to support longer contexts (32k+), we must reduce batch size, which in turn kills throughput efficiency.

#### Workload Profile {#sec-model-serving-systems-workload-profile-a380}

*   **Model**: Llama-3-8B (quantized to 4-bit AWQ).
*   **Hardware**: 1$\times$ NVIDIA H100 SXM5 GPU (`{python} h100_mem` GB HBM3, `{python} h100_bw_tbs` TB/s bandwidth).
*   **Request Characteristics**: 1,000-token input prompt (Prefill), 256-token generated response (Decode).
*   **Target SLOs**: TTFT $<$ 200 ms, TPOT $<$ 20 ms.

#### Latency Deconstruction {#sec-model-serving-systems-latency-deconstruction-217e}

The end-to-end request latency is governed by the two-phase execution model of autoregressive transformers.

**1. Prefill Phase (Time to First Token)**

The model processes the 1,000-token prompt in parallel. On an H100, this compute-bound operation achieves approximately 10,000 tokens per second.
*   $T_{\text{prefill}} = \frac{1000 \text{ tokens}}{10000 \text{ tokens/s}} = 100 \text{ ms}$.
*   Accounting for 20 ms of system overhead (network ingress, tokenization), the **TTFT is 120 ms**, comfortably within the 200 ms SLO.

**2. Decode Phase (Time Per Output Token)**

The model generates 256 tokens sequentially. This phase is memory-bandwidth bound; the system must read the entire 3.5 GB weight tensor from VRAM to generate a single token.

::: {.callout-perspective title="The Physics of Token Generation"}

Recall the **Energy-Movement Invariant** from @sec-data-engineering-ml: moving a bit is 100–1,000× more expensive than computing on it. In the **Decode Phase**, this law determines the physical "cost per word."

**The Memory Wall for Generative AI**: Because the decode phase has an arithmetic intensity of $\approx 1$ FLOP/byte (we must read every weight just to generate one token), performance is strictly limited by memory bandwidth ($BW$), not compute.

$$ T_{\text{token}} \approx \frac{\text{Model Size (Bytes)}}{\text{Memory Bandwidth (Bytes/s)}} $$

**The Engineering Implication**:
Every time you generate a token, you are paying a massive "energy tax" to move the model's logic from HBM into compute registers. For Llama-3-8B (3.5 GB int4), an A100 80GB (`{python} a100_bw_tbs` TB/s HBM2e) generates tokens at $\approx 1.7$ ms/token. Adding more *compute cores* yields **zero** latency improvement; only faster memory (Physics) or smaller models (Algorithm) can speed up generation.
:::

```{python}
#| label: llm-serving-calc
#| echo: false

# Token generation (decode phase)
model_weight_gb = 3.5
h100_bw_tb = H100_MEM_BW.to(TB / second).magnitude
token_time_theoretical_ms = f"{model_weight_gb / (h100_bw_tb * 1000) * 1000:.0f}"  # GB / (TB/s) = ms
realized_tpot_ms = 10
decode_tokens = 256
total_decode_s = f"{decode_tokens * realized_tpot_ms / 1000:.2f}"

# KV cache and batch sizing
kv_cache_gb = 72
kv_per_token_mb = 0.5
kv_capacity_tokens = int(kv_cache_gb * 1000 / kv_per_token_mb)
kv_capacity_tokens_str = f"{kv_capacity_tokens:,}"
tokens_per_req = 1256
concurrent_batch = int(kv_capacity_tokens / tokens_per_req)

# Unit economics
ttft_s = 0.12
req_time_s = ttft_s + decode_tokens * realized_tpot_ms / 1000
req_time_s_str = f"{req_time_s:.2f}"
hourly_cost = 3.00
tokens_per_hour = concurrent_batch * (3600 / req_time_s) * tokens_per_req
tokens_per_hour_m = f"{tokens_per_hour / 1e6:.0f}"
cost_per_m_tokens = f"{hourly_cost / (tokens_per_hour / 1e6):.3f}"
remaining_vram_gb_str = f"{int(80 - model_weight_gb)}"
tokens_per_req_str = f"{tokens_per_req:,}"
hourly_cost_str = f"{hourly_cost:.2f}"
```

*   $T_{\text{token}}$ ≈ `{python} model_weight_gb` GB / `{python} h100_bw_tbs` TB/s ≈ `{python} token_time_theoretical_ms` ms (theoretical limit).
*   Accounting for kernel launch overhead and attention computation, realized $T_{\text{token}}$ is approximately `{python} realized_tpot_ms` ms.
*   Total decode time: `{python} decode_tokens` tokens × `{python} realized_tpot_ms` ms/token = `{python} total_decode_s` seconds.
*   **TPOT is `{python} realized_tpot_ms` ms**, well within the 20 ms "fluidity" SLO.

#### Memory & Throughput {#sec-model-serving-systems-memory-throughput-63dd}

With 4-bit weights occupying `{python} model_weight_gb` GB, the remaining ~`{python} remaining_vram_gb_str` GB of VRAM is available for the **KV Cache**. Using **PagedAttention**, we can allocate this memory with near-zero fragmentation.

*   Each token requires approximately `{python} kv_per_token_mb` MB of KV cache (32 layers × 4096 dim × 2 vectors × 2-byte precision).
*   Total cache capacity ≈ `{python} kv_cache_gb` GB / `{python} kv_per_token_mb` MB/token ≈ `{python} kv_capacity_tokens_str` tokens.
*   At `{python} tokens_per_req_str` tokens per request (input + output), the GPU can handle a **concurrent batch size of ~`{python} concurrent_batch` requests**.

#### Unit Economics {#sec-model-serving-systems-unit-economics-b685}

For an H100 SXM5 instance at approximately \$`{python} hourly_cost_str` per hour (specialized cloud providers; hyperscaler rates vary from \$2-13 per hour as of 2024):

*   Total tokens per hour: `{python} concurrent_batch` batch × (3600 s/hr / `{python} req_time_s_str` s/req) × `{python} tokens_per_req_str` tokens/req ≈ `{python} tokens_per_hour_m` million tokens/hour.
*   **Cost per million tokens**: \$`{python} hourly_cost_str` / `{python} tokens_per_hour_m` ≈ **\$`{python} cost_per_m_tokens`**.

This analysis highlights that for LLMs, **memory capacity** (the size of the KV cache) is the primary determinant of throughput and cost, while **memory bandwidth** is the primary determinant of latency.

This case study applies the core principles developed throughout this chapter: latency budgets decompose into prefill and decode phases, queuing theory governs batch sizing and capacity planning, and hardware constraints in the form of memory bandwidth and capacity determine achievable performance and cost. The quantitative framework established here enables principled engineering decisions, but only when applied correctly. Common misconceptions cause even experienced engineers to misapply these principles in practice.

## Fallacies and Pitfalls {#sec-model-serving-systems-fallacies-pitfalls-336b}

Serving inverts training priorities in unexpected ways. Intuitions from batch processing fail under latency constraints and variable load, causing wasted effort, violated SLOs, and silent accuracy degradation in production.

##### Fallacy: *Reducing model inference latency proportionally reduces user-perceived latency.* {.unnumbered}

Engineers optimize model inference and expect proportional improvement in request latency, but serving systems introduce latency sources absent from offline benchmarks. Under load, queuing delay dominates: @eq-mm1-wait shows that at 80 percent utilization with 5ms service time, average wait time is 20ms before inference even begins. Reducing inference from 5ms to 2ms changes service time but also shifts utilization from 80 percent to 32 percent, reducing queuing wait from 20ms to 2.4ms, a 10× queuing improvement that dwarfs the 3ms inference gain. This nonlinear interaction between inference speed and queuing behavior means the *system-level* speedup (25ms → 4.4ms, or 5.7×) far exceeds the *model-level* speedup (5ms → 2ms, or 2.5×). Conversely, teams that reduce inference by only 20 percent at high utilization see negligible user-facing improvement because queuing still dominates. Serving optimization requires analyzing the complete latency budget, including serialization, queuing, preprocessing, and postprocessing, under realistic load conditions rather than profiling inference latency in isolation.

##### Pitfall: *Running serving infrastructure at high utilization to maximize cost efficiency.* {.unnumbered}

Teams target 90 percent utilization to minimize idle capacity. In production, latency degrades nonlinearly as utilization approaches capacity. @eq-mm1-wait shows that at 90 percent utilization, average wait time reaches 10× service time. Moving from 70 percent to 90 percent utilization cuts infrastructure costs by 22 percent but triples average latency. For a 5ms inference service, p99 latency jumps from 25ms to 50ms. Systems provisioned for average load violate SLOs precisely when traffic increases during business-critical periods. Production systems targeting 60 to 70 percent utilization at peak load maintain the latency headroom needed to absorb traffic spikes.

##### Fallacy: *Training accuracy guarantees serving accuracy.* {.unnumbered}

Engineers assume identical model weights preserve validation set performance. In production, preprocessing differences silently shift inputs outside the training distribution. @sec-model-serving-systems-trainingserving-skew-7b99 shows *how* training-serving skew causes accuracy degradation despite identical weights: PIL versus OpenCV resize interpolation differs subtly, float64 versus float32 normalization produces different values, or feature computation timing changes. A model achieving 95 percent validation accuracy drops to 90 percent in production from these preprocessing mismatches. Standard monitoring checking exceptions and latency violations fails to detect this silent degradation. Production systems require either identical preprocessing code for training and serving, or statistical monitoring comparing input distributions to catch drift before accuracy degrades.

##### Pitfall: *Using average latency to evaluate serving system performance.* {.unnumbered}

```{python}
#| label: tail-latency-calc
#| echo: false

avg_latency_ms = 10
p99_latency_ms = 200
tail_multiplier = p99_latency_ms / avg_latency_ms

tail_mult_str = f"{tail_multiplier:.0f}"
```

Engineers monitor average latency because it trends smoothly and is simple to compute. In production, averages hide the slowest requests that determine user satisfaction. A system with 10ms average latency might have 200ms p99 latency, meaning 1 percent of users experience `{python} tail_mult_str`× worse performance. @sec-model-serving-systems-tail-latency-5376 explains how queuing variability causes this divergence: at 70 percent utilization with 5ms service time, average latency is 17ms but p99 reaches 75ms. Production SLOs specify percentile targets (p95, p99) precisely because averages mask tail behavior. Systems reporting only averages pass monitoring checks while violating user experience standards.

##### Fallacy: *Larger serving batches always improve throughput without affecting latency SLOs.* {.unnumbered}

Engineers maximize batch size assuming GPU saturation improves cost efficiency under production load. In serving systems, however, batching introduces a latency-throughput tradeoff governed by queuing dynamics absent from offline benchmarks. Accumulating requests into larger batches increases wait time for early arrivals: a batch window of 10ms means the first request waits 10ms before inference begins, directly adding to p99 latency. For ResNet-50 on V100, increasing batch size from 16 to 32 improves throughput only 12 percent while nearly doubling per-batch inference time from 14ms to 25ms, and variable input sizes within a batch create padding overhead that wastes 15 to 30 percent of compute on padding tokens. @sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d shows that for 50ms p99 targets, batch sizes above 32 routinely violate SLOs because batch formation delay plus increased per-batch inference time exceeds the latency budget. Serving batch optimization requires jointly tuning batch size, batch timeout, and concurrency against latency SLOs under realistic traffic patterns, not maximizing throughput in isolation.

##### Pitfall: *Calibrating quantized models with training data rather than production traffic.* {.unnumbered}

Teams calibrate with training data because it is readily available and produced validation accuracy. In production, traffic distribution often differs from training data, making calibration scale factors suboptimal. Post-training quantization determines INT8 scale factors by measuring activation ranges on calibration data, but this assumes production inputs match the calibration distribution. One production system experienced 3.2 percent accuracy loss serving wildlife camera images after calibrating with ImageNet validation data. @sec-model-compression shows quantization error scales with activation range: miscalibration amplifies errors precisely on out-of-distribution inputs. Effective quantization requires calibrating with representative samples of actual serving traffic.

##### Pitfall: *Cold start latency only matters for the first request.* {.unnumbered}

Engineers optimize steady-state latency assuming most requests hit warm instances. In production, cold starts affect any request arriving after inactivity, after model updates, or during auto-scaling. Systems with bursty traffic experience cold starts on 10 to 30 percent of requests during scale-up events. ResNet-50 with TensorRT requires 30 seconds for compilation if the optimized engine is not cached; during a traffic spike triggering 10 new instances, 300 seconds of user-facing latency is added across the first requests to each instance. @sec-model-serving-systems-model-loading-initialization-cc5a shows cold start compounds weight loading, CUDA context initialization, and warmup. Systems ignoring cold start meet SLOs during steady state but violate them during scale-up events and deployment windows.

## Summary {#sec-model-serving-systems-summary-9635}

Serving marks the transition from model development to production deployment, where the optimization priorities that governed training must be inverted. The shift from throughput maximization to latency minimization transforms every system design decision. The queuing theory foundations established here reveal *why* this inversion is not merely a change in metrics but a change in the governing mathematics: the nonlinear relationship between utilization and latency means that systems behaving well at moderate load can suddenly violate SLOs when traffic increases modestly. Little's Law and the M/M/1 wait time equations provide the quantitative foundation for capacity planning, replacing intuition-based provisioning with engineering rigor.

Effective serving optimization requires understanding the complete request path rather than focusing exclusively on model inference. Interface protocols like gRPC and efficient serialization formats minimize the "tax" of data movement, while preprocessing often consumes 45 to 70 percent of total latency when inference runs on optimized accelerators. The microsecond-scale overheads identified by Barroso, Patterson, and colleagues explain *why* serving latency often exceeds the sum of its measured parts, and *why* system-level optimization matters as much as model optimization. Training-serving skew represents another dimension of this complexity, silently degrading accuracy when preprocessing logic differs between training and production environments in ways that traditional testing cannot detect.

The traffic pattern analysis reveals *how* deployment context shapes batching strategy and system design. Server workloads with Poisson arrivals optimize dynamic batching windows, autonomous vehicles with streaming sensor data require synchronized batch formation, and mobile applications with single-user patterns eliminate batching entirely. The MLPerf scenarios codify these patterns for standardized benchmarking, connecting the serving principles established here to the measurement frameworks explored in @sec-benchmarking-ai. Precision selection and runtime optimization extend the quantization techniques from @sec-model-compression and Tensor Core capabilities from @sec-ai-acceleration into the serving domain. Finally, the translation of these technical metrics into unit economics, as shown by the Llama-3 case study, demonstrates *how* engineering decisions regarding batching, precision, and hardware selection directly determine the financial viability of deployment.

::: {.callout-takeaways title="Key Takeaways"}

* **Serving inverts training priorities**: Training optimizes throughput (samples/hour); serving optimizes latency (ms/request). Different objectives require different system designs.
* **Queuing theory governs capacity planning**: At 80% utilization, wait time is 5× service time; at 90%, it reaches 10×. Small load increases cause disproportionate latency spikes.
* **Preprocessing dominates optimized systems**: When model inference is fast (5ms), preprocessing (image decode, tokenization) consumes 45–70% of total latency. Optimize the pipeline, not just the model.
* **Batching strategy depends on traffic pattern**: Poisson arrivals (web APIs) use dynamic batching; streaming sensors use synchronized batches; mobile apps eliminate batching entirely.
* **Training-serving skew can degrade accuracy undetected**: Different preprocessing between training and serving (e.g., resize interpolation, normalization order) shifts inputs outside the training distribution, causing accuracy degradation that conventional monitoring cannot detect. Use identical code paths.
* **KV cache optimization is critical for large language models**: Generation is memory-bound. PagedAttention and continuous batching can improve throughput 2–4× over naive serving.

:::

The serving principles established here (queuing theory for capacity planning, preprocessing optimization, batching strategy selection, and training-serving skew prevention) form the foundation for building production ML systems that meet real-world SLAs. Whether deploying a recommendation system serving millions of users or a medical AI where every millisecond affects patient outcomes, these principles translate mathematical understanding into engineering decisions that determine whether systems succeed or fail under load.

::: {.callout-chapter-connection title="From Node to Factory"}

We have optimized the single node for milliseconds, but a single node is fragile. In @sec-machine-learning-operations-mlops, we scale our perspective from the single request to the full system lifecycle, building the automated machinery that keeps production systems running through crashes, model drift, and continuous updates.

:::

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
