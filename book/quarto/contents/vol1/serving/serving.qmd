---
bibliography: serving.bib
quiz: serving_quizzes.json
concepts: serving_concepts.yml
glossary: serving_glossary.json
---

# Serving {#sec-serving}

::: {layout-narrow}
::: {.column-margin}
*DALL·E 3 Prompt: An illustration of a machine learning serving system, depicted as a high-speed assembly line in a modern factory. On one end, raw data (images, text documents, sensor readings) enters on conveyor belts. In the center, a glowing neural network processes inputs through preprocessing stations, a central inference engine, and postprocessing units. Workers monitor latency gauges and throughput meters. Some requests wait in small batches while others stream through individually. The scene conveys speed, precision, and the transformation from raw input to useful predictions, with visible clocks emphasizing the time-critical nature of serving.*
:::

\noindent
![](images/png/cover_serving.png){width=80%}

:::

## Purpose {.unnumbered}

_How do the system requirements for serving trained models differ from training them, and what principles govern the design of responsive prediction systems?_

Training and serving are fundamentally different computational paradigms requiring distinct system designs. Training optimizes throughput over days or weeks of computation while serving inverts this priority, optimizing latency per request under strict time constraints measured in milliseconds. A common misconception is that faster hardware automatically means faster serving, but in practice preprocessing and postprocessing often dominate latency: production systems report preprocessing consuming 60-70% of total request time when inference runs on optimized accelerators. Understanding where latency actually comes from requires mastering queuing theory fundamentals that explain why systems degrade nonlinearly under load, recognizing how traffic patterns (Poisson arrivals for servers, streaming for autonomous vehicles, single-user for mobile) determine batching strategy, and connecting serving decisions to prior chapters on quantization, hardware acceleration, and benchmarking. The principles established here for single-machine serving provide the foundation for understanding when and why scaling to multiple machines becomes necessary.

::: {.callout-tip title="Learning Objectives"}

- Contrast training and serving system priorities by explaining the inversion from throughput optimization to latency minimization

- Decompose request latency into preprocessing, inference, and postprocessing phases to identify optimization bottlenecks

- Apply Little's Law and M/M/1 queuing models to predict serving system latency under varying load conditions

- Perform capacity planning to meet percentile latency SLOs while accounting for traffic variance and fault tolerance requirements

- Identify sources of training-serving skew and select prevention strategies appropriate to deployment contexts

- Select batching strategies (dynamic, continuous, none) based on traffic patterns and latency constraints

- Evaluate runtime and precision tradeoffs to meet deployment cost and performance requirements

:::

## From Training to Production {#sec-serving-training-to-production}

Parts I through III built a complete foundation for creating optimized ML models. You understand where systems deploy (@sec-ml-systems), how data flows through pipelines (@sec-data-engineering), what neural networks compute (@sec-dl-primer, @sec-dnn-architectures), how frameworks enable implementation (@sec-ai-frameworks), and how training produces learned models (@sec-ai-training). Part III then addressed making those models efficient: the conceptual framework for efficiency (@sec-efficient-ai), specific optimization techniques (@sec-model-optimizations), hardware acceleration strategies (@sec-ai-acceleration), and measurement methodologies for validating improvements (@sec-benchmarking-ai). The result is an optimized model ready for deployment. Part IV addresses what happens next: delivering predictions to users in production.

Serving introduces a fundamental inversion that transforms everything established in prior chapters. Training optimizes for samples processed per hour over days of computation. Serving must deliver predictions within milliseconds under unpredictable load. The benchmarking techniques from @sec-benchmarking-ai measured throughput and accuracy under controlled conditions. Production serving faces traffic patterns that no benchmark could anticipate. The quantization methods from @sec-model-optimizations reduced model size. Serving must validate that those optimizations preserve accuracy under real traffic distributions. This inversion from throughput to latency, from controlled to unpredictable, from offline to real time defines the serving challenge.

These consequences manifest concretely in how we measure and optimize. The benchmarking techniques from @sec-benchmarking-ai now target percentile latencies rather than aggregate throughput. The quantization methods from @sec-model-optimizations must be validated not just for accuracy preservation but for calibration with production traffic, since quantization error that averages out over training batches may concentrate in specific request types. The hardware acceleration from @sec-ai-acceleration must be configured for single-request responsiveness rather than batch throughput, often leaving compute capacity idle to ensure low latency. Understanding these connections enables practitioners to apply earlier optimizations correctly in the serving context.

Single-machine deployment serves as the focus here, establishing the foundational principles that govern all inference systems. This scope encompasses systems from a single GPU workstation to a multi-GPU server with 1 to 8 GPUs, where all resources share memory and can be orchestrated by a single process. These foundations prepare practitioners for understanding when and why scaling to distributed multi-machine systems becomes necessary.

::: {.callout-note title="Lighthouse Example: Serving a ResNet-50 Image Classifier"}

Serving a ResNet-50 image classification model serves as a consistent reference point to ground abstract concepts in concrete reality. ResNet-50 represents an ideal teaching example because it spans common deployment scenarios from mobile apps to cloud APIs, has well-documented performance characteristics with 25.6M parameters and approximately 4 GFLOPS per inference, exhibits all key serving challenges including preprocessing overhead from image decoding and resizing along with batching tradeoffs and memory management, and represents production ML systems where image classification remains one of the most widely deployed applications.

**Key ResNet-50 Serving Specifications:**

The model contains 25.6 million parameters, requiring 98MB in FP32, 49MB in FP16, or 25MB in INT8 format. Input consists of 224×224×3 RGB images, typically 150KB as JPEG or 588KB as uncompressed tensors. Inference time measures approximately 5ms on a V100 GPU for a single image and approximately 1ms per image at batch size 32. Preprocessing requires JPEG decoding at approximately 3ms, resizing at approximately 1ms, and normalization at approximately 0.5ms. Memory footprint totals approximately 400MB GPU memory including activations.

**ResNet-50 anchor examples** appear at strategic points where this specific model illuminates the concept under discussion. Each example provides quantitative specifications and concrete implementation decisions encountered when serving this model.

:::

## Serving System Fundamentals {#sec-serving-fundamentals}

Serving systems occupy a unique position in the machine learning lifecycle, operating under constraints that differ fundamentally from both training and batch processing. Before examining specific optimization techniques, we must establish the conceptual framework that distinguishes these systems from other computational workloads.

::: {.callout-definition title="Model Serving"}

**Model Serving** refers to the process of exposing trained machine learning models for _real-time prediction_, requiring systems that transform raw inputs into useful outputs while meeting _latency constraints_, maintaining _consistency_ with training behavior, and achieving _cost-effective resource utilization_.

:::

The defining characteristic of serving systems is their request-driven nature. Unlike training, where the system controls when and how data flows through the model, serving systems must respond to external requests that arrive unpredictably. This fundamental difference shapes every design decision, from memory management to error handling.

### Static vs Dynamic Inference {#sec-serving-static-dynamic}

The first architectural decision in any serving system is whether predictions happen before or during user requests [@google2024staticdynamic]. This choice fundamentally shapes system design, cost structure, and capability boundaries.

**Static inference** (also called offline or batch inference) pre-computes predictions for anticipated inputs and stores them for retrieval. Consider a recommendation system that generates predictions for all user-item pairs nightly. When a user requests recommendations, the system retrieves pre-computed results from a lookup table rather than running inference. This approach eliminates inference latency entirely since results already exist, enables quality verification before deployment, and reduces serving costs. However, static inference cannot handle novel inputs that were not anticipated during the batch computation and introduces hours or days of latency when models update.

**Dynamic inference** (also called online or real-time inference) computes predictions on demand when requests arrive. This handles any input, including rare edge cases and novel combinations, and immediately reflects model updates. The cost is strict latency requirements that may force simpler models and more intensive monitoring infrastructure.

::: {.callout-note title="ResNet-50: Static vs Dynamic Tradeoffs"}

For our ResNet-50 image classifier, consider two deployment scenarios:

**Static approach**: A photo organization app pre-classifies all images in a user's library overnight. With 10,000 photos and 5ms inference each, batch processing takes ~50 seconds total. Users see instant classification when browsing their library.

**Dynamic approach**: A content moderation API must classify user-uploaded images in real-time. Each image requires the full preprocessing→inference→postprocessing pipeline, with a 100ms latency budget to meet user expectations.

Most production image classification systems use a **hybrid approach**: frequently requested images (popular products, known memes) are pre-classified and cached, while novel uploads trigger dynamic inference. This reduces average latency while maintaining flexibility.

:::

Most production systems combine both approaches. Common queries hit a cache populated by batch inference while uncommon requests trigger dynamic computation. Understanding this spectrum is essential because it determines which subsequent optimization strategies apply. Static inference optimizes for throughput during batch computation and storage efficiency for serving. Dynamic inference optimizes for per-request latency under concurrent load, which requires understanding where time goes within each request.

### The Load Balancer Layer {#sec-serving-load-balancer}

Production serving systems place load balancers between clients and model servers. A load balancer provides three essential functions for serving infrastructure.

**Request Distribution** routes incoming requests to available model replicas using algorithms like round-robin or least-connections. For latency-sensitive ML serving, algorithms that route away from slow or overloaded replicas improve tail latency.

**Health Monitoring** continuously verifies that replicas are ready to serve, routing traffic away from unhealthy instances. For ML systems, health checks must verify not just process liveness but model readiness, confirming that weights are loaded and warmup is complete.

**Deployment Support** enables safe model updates by gradually shifting traffic between versions. Deployment strategies including canary testing and blue-green deployments are examined in @sec-ml-operations.

For single-machine serving with multiple model instances, such as running several ONNX Runtime sessions, the framework and operating system handle request queuing. The full complexity of load balancing becomes essential when scaling to distributed inference systems, where multiple machines serve the same model. The implementation details of request distribution algorithms and multi-replica architectures belong to that distributed context.

**Impact on Queuing Analysis**: When capacity planning considers "the server" in this chapter, it means the single machine's model serving capacity. The queuing dynamics analyzed in @sec-serving-queuing apply to understanding single-machine behavior and determining when scaling to multiple machines becomes necessary.

## The Latency Budget {#sec-serving-latency-budget}

For dynamic inference systems, the fundamental difference from training lies in optimization objectives. Training optimizes throughput: maximizing samples processed per hour over days of computation. A training job that processes 1000 samples per second is successful regardless of how long any individual sample takes. Serving inverts this priority, optimizing latency per request under strict time constraints. A serving system with 1000ms per-request latency has failed, even if it achieves excellent throughput.

This mindset shift has concrete implications for system design [@gujarati2020serving]. The metrics that matter change from aggregate throughput to latency distributions. Mean latency tells you little about user experience; p50, p95, and p99 latencies reveal how the system performs across the full range of requests. If your mean latency is 50ms but p99 is 2 seconds, one in a hundred users waits 40 times longer than average. For consumer-facing applications, these tail latencies often determine user satisfaction and retention.[^fn-tail-latency]

[^fn-tail-latency]: **Tail Latency Impact**: Research at Google and Amazon in the mid-2000s established that users are more sensitive to latency variance than mean latency. Industry experience suggests that latency increases of 100ms can measurably impact user engagement and conversion rates for e-commerce applications, though the magnitude varies by context. This is why service level objectives (SLOs) typically specify percentile targets rather than averages.

::: {.callout-definition title="Latency Budget"}

**Latency Budget** refers to the maximum time allowed for a serving request to complete, decomposed into allocations for _preprocessing_, _inference_, and _postprocessing_ phases. Effective latency budgeting requires understanding where time is consumed and allocating resources accordingly.

:::

Every serving request decomposes into three phases that each consume part of the latency budget. Preprocessing transforms raw input such as image bytes or text strings into model-ready tensors. Inference executes the model computation. Postprocessing transforms model outputs into user-facing responses.

A common misconception is that faster hardware automatically means faster serving. In practice, preprocessing and postprocessing often dominate total latency. Studies of production systems show preprocessing consuming 60 to 70 percent of total request time when inference runs on optimized accelerators [@nvidia2024tritontutorial]. Optimizing only the inference phase yields diminishing returns when the surrounding pipeline remains bottlenecked on CPU operations.

### Latency Distribution Analysis {#sec-serving-latency-analysis}

Understanding where time goes requires instrumenting each phase independently. Consider what happens when our ResNet-50 classifier receives a JPEG image:

::: {.callout-note title="ResNet-50: Latency Budget Breakdown"}

A typical serving request for our ResNet-50 classifier shows the following latency distribution:

+--------------------+----------------------------+------------+----------------+
| **Phase**          | **Operation**              | **Time**   | **Percentage** |
+:===================+:===========================+===========:+===============:+
| **Preprocessing**  | JPEG decode                | 3.0ms      | 30%            |
| **Preprocessing**  | Resize to 224×224          | 1.0ms      | 10%            |
| **Preprocessing**  | Normalize (mean/std)       | 0.5ms      | 5%             |
| **Data Transfer**  | CPU→GPU copy               | 0.5ms      | 5%             |
| **Inference**      | **ResNet-50 forward pass** | **5.0ms**  | **50%**        |
| **Postprocessing** | Softmax + top-5            | 0.1ms      | ~0%            |
| **Total**          |                            | **10.1ms** | **100%**       |
+--------------------+----------------------------+------------+----------------+

Key insight: **Preprocessing consumes 45% of latency** despite model inference being the computationally intensive phase. With TensorRT optimization reducing inference to 2ms, preprocessing would dominate at 75%.

:::

This breakdown reveals why naive optimization efforts often fail. Engineers focus on model optimization (quantization, pruning) because that is where ML expertise applies, but the actual bottleneck is image decoding running on CPU.

::: {.callout-note title="Systems Perspective: Optimizing the Whole System"}
**Amdahl's Law at Work**: In the ResNet-50 breakdown above, preprocessing (3ms) and data transfer (0.5ms) consume 35% of the total latency. If we spend months optimizing the model to be 10× faster (reducing it from 5ms to 0.5ms), the end-to-end latency only drops from 10.1ms to 5.6ms—a disappointing **1.8× speedup**. As Dave Patterson often argues, "optimizing the common case" is essential, but Amdahl's Law reminds us that the unoptimized parts of the system will eventually dominate. Holistic serving optimization requires looking past the model to the "killer microseconds" spent in JSON parsing, JPEG decoding, and memory copies.
:::

Moving preprocessing to GPU can reduce total latency by 6x in some pipelines by eliminating CPU-GPU data transfers between stages [@nvidia2024tritontutorial]. Amdahl's Law formalizes this principle: if preprocessing consumes 45% of latency, then even infinitely fast inference can yield at most a \(1/0.45 \approx 2.22\times\) end-to-end speedup. Effective optimization targets the largest time consumers first.

**The Killer Microseconds Problem**

Barroso, Patterson, and colleagues identified a critical gap in how systems handle latency at different time scales [@barroso2017attack]. Modern systems efficiently handle nanosecond-scale events (CPU cache access, DRAM reads) through hardware mechanisms like out-of-order execution, and millisecond-scale events (disk I/O, network calls) through software techniques like threading and asynchronous I/O. But microsecond-scale events fall into an uncomfortable middle ground where neither approach works well.

ML serving lives squarely in this microsecond regime. Individual inference calls complete in 1 to 10ms, but the surrounding operations such as serialization, memory allocation, network stack processing, and encryption each add microseconds that compound into significant overhead. Google's analysis found that a significant fraction, often 20 percent or more, of datacenter CPU cycles are consumed by this "datacenter tax" rather than useful computation. For serving systems, this means a 2μs network fabric can become 100μs end-to-end through software overhead, context switching costs of 5 to 10μs can exceed the inference time for small models, and memory allocation patterns in preprocessing can add unpredictable microsecond delays. These overheads explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization through memory pooling, zero-copy data paths, and kernel bypass matters as much as model optimization.

The latency budget framework provides a systematic approach to optimization. First, measure each phase to identify the true bottleneck. Then allocate engineering effort proportionally to where time is actually spent. Finally, consider architectural changes such as GPU preprocessing or batching strategies that can shift work between phases.

### Resolution and Input Size Tradeoffs {#sec-serving-resolution}

Input resolution dramatically affects both preprocessing and inference latency, but the relationship differs depending on whether the system is compute-bound or memory-bound. Understanding this distinction (covered in depth in @sec-ai-acceleration) is essential for making informed resolution decisions.

For compute-bound models, throughput scales inversely with resolution squared, as shown in @eq-resolution-throughput:

$$\frac{T(r_2)}{T(r_1)} = \left(\frac{r_1}{r_2}\right)^2$$ {#eq-resolution-throughput}

Doubling resolution from 224 to 448 theoretically yields 4× slowdown (measured: 3.6× due to fixed overhead amortization). However, at high resolutions, models transition from compute-bound to memory-bound as activation tensors exceed cache capacity. @tbl-resolution-bottleneck illustrates this transition for ResNet-50:

+----------------+---------------------+----------------------+----------------+
| **Resolution** | **Activation Size** | **Arith. Intensity** | **Bottleneck** |
+===============:+====================:+=====================:+:===============+
| **224×224**    | 12.5MB              | 85 FLOPS/byte        | Compute        |
| **384×384**    | 36.8MB              | 49 FLOPS/byte        | Transitional   |
| **512×512**    | 65.5MB              | 28 FLOPS/byte        | Memory BW      |
| **640×640**    | 102.4MB             | 18 FLOPS/byte        | Memory BW      |
+----------------+---------------------+----------------------+----------------+

: **Resolution and Compute Bottleneck**: ResNet-50 arithmetic intensity decreases with resolution as activation sizes grow. The V100 ridge point for FP32 operations is approximately 17 FLOPS/byte (15 TFLOPS compute divided by 900 GB/s bandwidth). At 224x224, compute dominates; by 512x512, memory bandwidth becomes the limiting factor. {#tbl-resolution-bottleneck}

**Deployment-Specific Resolution Decisions**

Different deployment contexts have fundamentally different resolution requirements. Mobile applications often accept lower resolution such as 224×224 for object detection in camera viewfinders, where latency and battery life dominate. Medical imaging requires high resolution of 512×512 or higher for diagnostic accuracy, with relaxed latency requirements. Autonomous vehicles use multiple resolutions for different tasks, with low resolution for detection and high resolution crops for recognition. Cloud APIs typically receive resolution set by client upload and must handle a range gracefully.

**Adaptive Resolution**

Production systems can select resolution dynamically based on content. One approach runs a lightweight classifier at 128×128 to categorize content type, then selects task-appropriate resolution with documents at 512×512, landscapes at 224×224, and faces at 384×384. This achieves 1.4× throughput improvement with 99.2 percent accuracy retention versus fixed high resolution. This pattern trades preprocessing cost from running the lightweight classifier for inference savings on the main model.

## Queuing Fundamentals {#sec-serving-queuing}

The latency budget framework explains where time goes within a single request, but production systems must handle many concurrent requests competing for finite resources. Understanding why latency degrades under load requires queuing theory, the mathematical framework that explains how requests wait for service in any system with finite capacity. These principles apply universally, from web servers to ML inference, and explain the counterintuitive behavior that causes well-provisioned systems to suddenly violate latency SLOs when load increases modestly.

### Little's Law {#sec-serving-littles-law}

The most fundamental result in queuing theory is Little's Law (@eq-littles-law), which relates three quantities in any stable system:

$$L = \lambda \cdot W$$ {#eq-littles-law}

where $L$ is the average number of requests in the system, $\lambda$ is the arrival rate (requests per second), and $W$ is the average time each request spends in the system. This relationship holds regardless of arrival distribution, service time distribution, or scheduling policy.

Little's Law has immediate practical implications. If your inference service averages 10ms per request ($W = 0.01$s) and you observe 50 concurrent requests in the system on average ($L = 50$), then your arrival rate must be $\lambda = L/W = 5000$ requests per second. Conversely, if you need to limit concurrent requests to 10 (perhaps due to GPU memory constraints), and your service time is 10ms, you can sustain at most 1000 requests per second.

### The Utilization-Latency Relationship {#sec-serving-utilization-latency}

For a system with Poisson arrivals and exponential service times (the M/M/1 queue model), the average time in system follows:

$$W = \frac{1}{\mu - \lambda} = \frac{\text{service time}}{1 - \rho}$$ {#eq-mm1-wait}

where $\mu$ is the service rate (requests per second the server can handle), and $\rho = \lambda/\mu$ is the utilization (fraction of time the server is busy).

This equation reveals why serving systems exhibit nonlinear behavior. At 50% utilization, average wait time is $2\times$ the service time. At 80% utilization, it is $5\times$. At 90% utilization, it is $10\times$. Small increases in load near capacity cause disproportionate latency increases.

+--------------------------+------------------------+---------------------------+
| **Utilization ($\rho$)** | **Wait Time Multiple** | **Example (5ms service)** |
+=========================:+=======================:+==========================:+
| 50%                      | 2.0×                   | 10ms                      |
| 70%                      | 3.3×                   | 17ms                      |
| 80%                      | 5.0×                   | 25ms                      |
| 90%                      | 10.0×                  | 50ms                      |
| 95%                      | 20.0×                  | 100ms                     |
+--------------------------+------------------------+---------------------------+

: **Utilization-Latency Relationship**: Average wait time as a multiple of service time for an M/M/1 queue. The nonlinear relationship explains why systems that perform well at moderate load can suddenly violate SLOs when traffic increases. {#tbl-utilization-latency}

The M/M/1 model assumes exponentially distributed service times, but ML inference typically has near-constant service time for fixed batch sizes, making the M/D/1 (deterministic service) model more accurate in practice. We use M/M/1 here because it yields closed-form solutions and produces conservative estimates. As @tbl-utilization-latency illustrates, average wait time grows rapidly as utilization approaches 100%. For M/D/1 queues, average wait time is approximately half of M/M/1 at the same utilization, which matters for capacity planning: M/M/1 analysis will slightly over-provision, erring on the side of meeting SLOs rather than violating them.[^fn-queuing-models]

[^fn-queuing-models]: **Queuing Model Assumptions**: The M/M/1 model assumes Poisson arrivals and exponential service times. ML inference typically has near-constant service time for fixed batch sizes, making M/D/1 (deterministic service) more accurate. We use M/M/1 because it produces conservative estimates and closed-form solutions, erring on the side of meeting SLOs. For deeper treatment including multi-server models, see Harchol-Balter's *Performance Modeling and Design of Computer Systems* [@harchol2013performance].

### Multi-Server Considerations {#sec-serving-multi-server}

While this chapter focuses on single-machine capacity planning using M/M/1 analysis, production systems at scale deploy multiple replicas. The M/M/c queuing model shows that multiple servers dramatically improve tail latency: the probability of all servers being simultaneously slow drops exponentially with server count.

**When M/M/1 Applies**: M/M/1 analysis remains valuable for single-machine sizing, worst-case analysis, and understanding when scaling to multiple machines becomes necessary. The capacity planning approach in the worked example below uses M/M/1 to determine when a single machine meets SLOs.

Multi-server queuing dynamics become essential when scaling to distributed inference systems, where they inform replica count, load balancing, and fault tolerance decisions.

### Tail Latency {#sec-serving-tail-latency}

Production SLOs typically specify percentile targets (p95, p99) rather than averages because tail latency determines user experience for the slowest requests [@dean2013tail]. For an M/M/1 queue, the p99 latency follows:

$$W_{p99} \approx \text{service time} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right)$$ {#eq-p99-latency}

At 70 percent utilization, p99 latency is approximately fifteen times the service time, while average latency is only 3.3 times. This explains why systems that seem healthy with low average latency can have unacceptable tail latency, since the average hides the experience of the unluckiest requests.

**The Tail at Scale Problem**

Dean and Barroso's foundational analysis reveals why tail latency becomes critical as systems scale beyond single machines [@dean2013tail]. The key insight is that when requests fan out to multiple servers, the probability of experiencing at least one slow response grows rapidly with server count. This "tail at scale" effect makes individual server tail latency critical for overall system performance.

For single-machine serving, this principle has two implications. First, tail latency on individual machines matters because it will compound when systems eventually scale. Second, the tail-tolerant techniques described below (hedging, graceful degradation) provide value even on single machines and become essential at scale.

**Tail-Tolerant Techniques**: Request hedging sends redundant requests after a timeout, accepting whichever response arrives first. Backup requests and load balancing away from slow servers directly address latency variance. These techniques apply to single-machine serving with multiple GPU streams or model replicas, and become essential when scaling to distributed inference systems.

::: {.callout-notebook title="Worked Example: ResNet-50 Capacity Planning" collapse="true"}

Consider designing a ResNet-50 serving system with these requirements:

- **Target p99 latency**: 50ms
- **Peak expected traffic**: 5,000 requests per second
- **Service time** (TensorRT FP16): 5ms

**Step 1: Find safe utilization**

Using @eq-p99-latency, we need $W_{p99} \leq 50$ms with 5ms service time. Solving for $\rho$:

$$5\text{ms} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right) \leq 50\text{ms}$$

This yields $\rho \leq 0.72$ (72% maximum utilization).

**Step 2: Calculate required service rate**

$$\mu_{\text{required}} = \frac{\lambda}{\rho_{\text{safe}}} = \frac{5000}{0.72} = 6944 \text{ requests/second}$$

**Step 3: Determine GPU count**

Single V100 throughput at batch=16: 1,143 images/second

$$\text{GPUs needed} = \frac{6944}{1143} = 6.1 \rightarrow 7 \text{ GPUs}$$

**Step 4: Add headroom for variance**

Production systems add 30% headroom for traffic spikes and variance:

$$\text{Final count} = 7 \times 1.3 = 9.1 \rightarrow 10 \text{ GPUs}$$

**Step 5: Verify fault tolerance (N+1 redundancy)**

The 30% headroom addresses traffic variance, but production systems also need fault tolerance. With 10 GPUs, losing one leaves 9 GPUs handling 5,000 QPS:

$$\text{Utilization after failure} = \frac{5000 / 1143}{9} = 48.6\%$$

This remains well below the 72% safe utilization threshold, confirming N+1 redundancy is satisfied. For stricter fault tolerance requirements, N+2 redundancy (tolerating two simultaneous failures) would require 11-12 GPUs.

**Result**: Provision 10 V100 GPUs to serve 5,000 QPS at 50ms p99 latency with N+1 fault tolerance.

:::

The queuing analysis explains the capacity planning approach mentioned in @sec-serving-capacity-planning and connects to the MLPerf Server scenario from @sec-benchmarking-ai, which measures throughput only for requests meeting the latency SLO. A system achieving 10,000 QPS but violating the SLO on 5% of requests reports only 9,500 valid QPS.

### Tail-Tolerant Techniques {#sec-serving-tail-tolerant}

Rather than eliminating all sources of latency variability (often impractical), production systems employ techniques that tolerate variability while still meeting SLOs [@dean2013tail; @dean2012rapid]. These techniques treat latency variance as a fact of life and design around it.

**Hedged Requests**

When a request has not completed within the expected time, send a duplicate request to another server. The client uses whichever response arrives first and cancels the other. For ML serving, this means maintaining multiple model replicas and routing slow requests to alternative replicas. The overhead is modest: if you hedge at the 95th percentile, only 5% of requests generate duplicates, increasing load by just 5% while dramatically reducing tail latency.

**Cancellation Complexity**: A critical implementation detail is that CUDA kernels cannot be interrupted mid-execution. When a hedged request completes, the duplicate must be cancelled, but if inference has already begun on the GPU, cancellation approaches include checking a cancellation flag before launching inference, accepting wasted compute for the in-flight kernel, or using request prioritization to deprioritize the duplicate. Since hedging typically applies only to the slowest 5 percent of requests, the overhead from occasional wasted compute remains acceptable.

**Tied Requests**

Send the request to multiple servers simultaneously, but include a tag allowing servers to cancel execution once another server begins processing. This eliminates the delay of waiting to detect a slow response before hedging. For inference servers with significant startup overhead from model loading and memory allocation, tied requests ensure at least one server begins immediately.

**Canary Requests**

For requests that fan out to many backends, first send the request to a small subset of 1 to 2 servers. If these return within expected time, send to the remainder. If the canary is slow, the system can take corrective action by retrying elsewhere or using cached results before committing to the full fan-out. This prevents a single slow backend from stalling an entire distributed inference request.

**Graceful Degradation**

When load exceeds capacity, return approximate results rather than timing out. For classification, return cached predictions for similar inputs. For generative models, return shorter outputs. For ensemble systems, return predictions from a subset of models. This maintains responsiveness at the cost of some accuracy, which users often prefer to outright failures.

**Admission Control**

When traffic exceeds capacity, accepting all requests can trigger widespread SLO violations. Admission control proactively rejects requests when queue depth exceeds a threshold, returning immediate 503 responses rather than accepting requests that are likely to timeout. This sacrifices throughput to protect latency for admitted requests.

**Setting the Threshold**: A practical starting point is 2 to 3 times service time multiplied by the number of workers. For a system with 4 workers and 10ms service time, this yields a queue depth threshold of 80 to 120 requests. Adaptive admission control adjusts thresholds based on observed p99 latency, tightening when latency increases above target and relaxing when latency remains healthy.

**Retry Storm Prevention**: A subtle failure mode occurs when all replicas are overloaded simultaneously. If the load balancer retries rejected requests at other replicas that are also overloaded, retry traffic amplifies the overload. Coordinated load shedding addresses this by sharing load information across replicas, enabling system-wide decisions about which requests to accept. When global load exceeds capacity, replicas collectively reject the same fraction of requests rather than each rejecting independently and triggering retries.

These techniques become essential at scale when fan-out amplification makes individual server tail latency visible to users. Single-machine serving systems can implement hedged and tied requests across GPU streams or model replicas. The queuing analysis here assumes FIFO processing, but production systems often implement priority scheduling such as deadline-aware or shortest-job-first approaches to further reduce tail latency for heterogeneous workloads [@harchol2013performance].

## Training-Serving Skew {#sec-serving-skew}

The queuing theory and latency analysis from previous sections assume the model produces correct predictions. However, a critical problem in production ML systems threatens this assumption: training-serving skew, the phenomenon where a model behaves differently in production than during training, despite using identical weights [@sculley2015hidden]. The model has not changed, but something in the preprocessing pipeline produces different inputs in production than during training, causing silent accuracy degradation that may go undetected for weeks or months.

::: {.callout-definition title="Training-Serving Skew"}

***Training-Serving Skew*** occurs when the _preprocessing logic_ or _data characteristics_ differ between training and serving environments, causing models to receive inputs that do not match their training distribution and resulting in degraded prediction quality.

:::

This problem is particularly dangerous because traditional software testing often fails to catch it. The serving system returns predictions, the model runs without errors, and all health checks pass. Only careful monitoring of prediction quality reveals that something is wrong.

### Sources of Skew {#sec-serving-skew-sources}

Understanding the common causes of training-serving skew helps practitioners design systems that avoid these pitfalls.

**Numerical precision differences** arise from different implementations of the same mathematical operations. Training uses float64 mean normalization while serving uses float32, producing different normalized values. A model achieving 0.87 AUC in validation may drop to 0.78 in production purely from these inconsistencies.

**Library version mismatches** introduce subtle behavioral differences. Training uses NumPy 1.24 while serving uses TensorFlow ops with different rounding behavior. Image resizing with different interpolation defaults produces subtly different pixel values that accumulate through the network.

**Missing value handling** diverges when training and serving encounter null values differently. Training fills nulls with column means computed over the training set while serving uses zeros or different defaults. Features present in training become missing in production due to upstream service failures.

**Time and ordering effects** cause misalignment in temporal features. Training uses UTC timestamps while serving uses local time, causing off-by-one-day errors in date features. Tokenizers trained on specific vocabulary versions encounter unknown tokens differently than during training.

**Environment differences** create systemic discrepancies [@polyzotis2017data]. Training runs in analytical environments (notebooks, data lakes) while serving runs in production microservices. Re-implementing feature engineering in different frameworks (Spark for training, pandas for serving) creates two codebases that must produce identical outputs.

::: {.callout-note title="ResNet-50: Image Preprocessing Skew"}

For ResNet-50 serving, common sources of skew include:

**Resize interpolation**: Training uses PIL.BILINEAR while OpenCV defaults to cv2.INTER_LINEAR. These produce pixel-level differences that can shift accuracy by 0.5-1%.

**Color space handling**: JPEG loading in different libraries may produce BGR vs RGB ordering. If the model trained on RGB but serves BGR inputs, predictions are essentially random.

**Normalization constants**: ImageNet normalization uses specific mean/std values. Using `mean=[0.5, 0.5, 0.5]` instead of `mean=[0.485, 0.456, 0.406]` shifts inputs out of the training distribution.

**Prevention**: The safest approach is to export the exact preprocessing code used during training and run it identically in serving, or use a framework like NVIDIA DALI that can help standardize preprocessing across training and serving environments.

:::

### Prevention Strategies {#sec-serving-skew-prevention}

The fundamental solution is using identical preprocessing code for training and serving. This sounds obvious but proves difficult in practice. Training pipelines optimize for batch processing while serving pipelines optimize for single-request latency. Feature stores address this by centralizing feature computation, ensuring both training and serving retrieve features from the same source [@orr2021managing].

When identical code is impractical, rigorous testing catches skew before deployment. Statistical comparison between training feature distributions and serving feature distributions reveals discrepancies. Shadow deployment runs the candidate model on live traffic alongside the baseline, surfacing skew that synthetic tests miss.

Monitoring in production detects skew that emerges over time. Feature drift detection compares serving distributions to training baselines using statistical distance measures [@breck2019data]. When drift exceeds thresholds, alerts trigger investigation before accuracy degrades noticeably.

@tbl-skew-prevention summarizes strategies for preventing and detecting training-serving skew.

+-------------------------------+----------------------------------+---------------------------------+
| **Strategy**                  | **Approach**                     | **Tradeoffs**                   |
+:==============================+:=================================+:================================+
| **Shared preprocessing code** | Use identical code for training  | May require runtime translation |
|                               | and serving                      | (e.g., Python to C++)           |
+-------------------------------+----------------------------------+---------------------------------+
| **Feature stores**            | Centralize feature computation   | Adds infrastructure complexity  |
|                               | for both environments            |                                 |
+-------------------------------+----------------------------------+---------------------------------+
| **Statistical validation**    | Compare feature distributions    | Catches drift but not all skew  |
|                               | between training and serving     | types                           |
+-------------------------------+----------------------------------+---------------------------------+
| **Shadow deployment**         | Run new model alongside baseline | Doubles serving cost during     |
|                               | on live traffic                  | validation                      |
+-------------------------------+----------------------------------+---------------------------------+
| **Continuous monitoring**     | Track prediction distributions   | Reactive rather than preventive |
|                               | and accuracy metrics             |                                 |
+-------------------------------+----------------------------------+---------------------------------+

: **Training-Serving Skew Prevention**: Strategies range from prevention (shared code, feature stores) to detection (statistical validation, monitoring), each with distinct operational tradeoffs. {#tbl-skew-prevention}

## Model Loading and Initialization {#sec-serving-model-loading}

With preprocessing pipelines designed to avoid training-serving skew, the next challenge is getting the model ready to serve. Before processing any request, models must load from storage into memory and prepare for inference [@romero2021infaas]. This initialization phase creates the cold start problem: the first request after deployment or scaling experiences dramatically higher latency. Understanding cold start dynamics is essential for designing systems that meet latency requirements from the moment they begin serving traffic.

### Cold Start Anatomy {#sec-serving-cold-start}

Cold start latency compounds from multiple sources, each adding to the time between deployment and serving readiness. Weight loading reads model parameters from disk or network storage. Graph compilation performs just-in-time compilation of operations for the specific hardware. Memory allocation reserves GPU memory for activations and intermediate values. Warmup execution performs initial inferences that populate caches and trigger lazy initialization.

::: {.callout-note title="ResNet-50: Cold Start Timeline"}

Loading ResNet-50 for production serving involves the following cold start phases:

+---------------------------------+--------------+---------------------------------------------------+
| **Phase**                       | **Duration** | **Notes**                                         |
+:================================+=============:+:==================================================+
| **Weight loading (SSD)**        | 0.5s         | 98MB FP32 weights from local storage              |
| **Weight loading (S3)**         | 3-5s         | Network latency dominates for cloud storage       |
| **CUDA context**                | 0.3-0.5s     | CUDA 11+ lazy loading significantly reduced this  |
| **TensorRT compilation**        | 15-30s       | Converts PyTorch model to optimized engine        |
| **Warmup (10 inferences)**      | 0.2s         | Triggers remaining lazy initialization            |
| **Total (local, optimized)**    | **~1.5s**    | With pre-compiled TensorRT engine, warm container |
| **Total (cloud, first deploy)** | **~35s**     | Including compilation from cold state             |
+---------------------------------+--------------+---------------------------------------------------+

**Key insight**: Pre-compiling models and storing the optimized engine eliminates the 30-second compilation phase on subsequent deployments. Production systems use CUDA MPS (Multi-Process Service) or pre-warmed container pools to amortize CUDA initialization costs across requests.

:::

Without warmup, the first real request triggers compilation and memory allocation mid-inference, often causing timeout failures. A request that normally takes 5ms might require 500ms during cold start, violating SLOs and degrading user experience.

### Loading Strategies {#sec-serving-loading-strategies}

Different loading strategies trade off cold start duration against serving performance and memory efficiency.

**Full loading** reads the entire model into memory before serving begins. This maximizes inference speed since all weights are immediately available, but extends cold start duration and limits model size to available memory. Full loading is appropriate when cold start latency is acceptable and models comfortably fit in memory.

**Memory mapping** maps model files directly into the address space, loading pages on demand as accessed. This reduces cold start time since inference can begin before the full model loads, but causes unpredictable latency as pages fault in during initial requests. Memory mapping works well for infrequently accessed model components but can cause latency spikes if critical weights are not preloaded.

**Lazy initialization** defers compilation and allocation until first use. This minimizes startup time but shifts latency to the first request. Production systems often combine lazy initialization with synthetic warmup requests to trigger initialization before real traffic arrives.

### Model Caching Infrastructure {#sec-serving-model-caching}

Production systems cache model weights at the infrastructure level to reduce cold start for common deployment scenarios:

**Container Image Embedding**: Bundle model weights directly in the container image. This produces a single deployment artifact and eliminates network fetches at startup, but creates large images (often 10-50GB) that slow container pulls and consume registry storage. Best for models that rarely update.

**Shared Filesystem**: Mount a network filesystem (EFS, GCS FUSE) containing model weights. Multiple replicas share cached weights, and updates propagate immediately without redeployment. Network latency affects cold start, and filesystem availability becomes a critical dependency. Best for organizations with many models and frequent updates.

**Node-Local SSD Cache**: Pre-populate local SSDs on inference nodes with frequently-used models. Provides fast loading (500MB/s+ for NVMe) without network dependency, but requires cache management to handle model updates and capacity limits. Best for high-traffic models where cold start latency is critical.

The choice depends on model update frequency: infrequent updates favor container embedding, frequent updates favor shared filesystem, and performance-critical deployments benefit from local caching with background refresh.

### Multi-Model Serving {#sec-serving-multi-model}

Production systems often serve multiple models from a single machine, whether different model versions for A/B testing, ensemble components, or entirely different models sharing infrastructure. GPU memory becomes the limiting resource, requiring careful management strategies.

Strategies for multi-model serving include time-multiplexing that loads one model at a time and swaps based on request routing, memory sharing where models share GPU memory to limit concurrent execution but enable more models, and model virtualization where frameworks like Triton manage model lifecycle by loading and unloading based on traffic patterns [@nvidia2024triton]. The choice depends on request patterns. If models receive traffic evenly, concurrent loading works. If traffic is bursty and model-specific, time-multiplexing with intelligent preloading reduces average latency while maximizing GPU utilization.

**Multi-Stream Execution**

When multiple models or multiple instances of the same model must run concurrently on a single GPU, the hardware must partition resources between them. NVIDIA's Multi-Instance GPU technology enables hardware-level isolation, dividing an A100 into up to 7 independent GPU instances, each with dedicated memory and compute resources. MIG is available on A100, A30, H100, and newer data center GPUs. For older GPUs such as V100 or T4, CUDA stream scheduling provides time-multiplexed sharing without hardware isolation.

MIG provides stronger isolation guarantees but reduces flexibility since once partitioned, GPU slices cannot be dynamically resized. CUDA streams offer flexibility but no performance isolation. The choice depends on whether consistent latency with MIG or maximum utilization with shared streams is the priority.

## Batching for Serving {#sec-serving-batching}

Once models are loaded and ready to serve, the next optimization opportunity lies in how requests are grouped for processing. Batching differs fundamentally between training and serving [@crankshaw2017clipper]. Training batches maximize throughput, processing hundreds or thousands of samples together with no concern for individual sample latency. Serving batches must balance throughput against individual request latency, typically processing single digits of requests together while ensuring no request waits too long.

::: {.callout-definition title="Dynamic Batching"}

**Dynamic Batching** refers to a serving strategy that collects incoming requests within a _time window_ and processes them together, trading individual request latency for improved _throughput_ and _hardware utilization_. The window size and maximum batch size parameters control this tradeoff.

:::

### Why Batching Helps {#sec-serving-batching-why}

Modern accelerators achieve peak efficiency only at sufficient batch sizes [@shen2019nexus]. A single inference request leaves most compute units idle because GPUs are designed for parallel execution across thousands of threads. Batching amortizes fixed costs (kernel launch overhead, weight loading from memory) across multiple requests and enables parallel execution across the batch dimension.

::: {.callout-notebook title="Worked Example: ResNet-50 Batching Efficiency" collapse="true"}

The throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates the power of batching:

+----------------+---------------------+-----------------------+----------------+---------------+
| **Batch Size** | **Inference Time*** | **Per-Image Compute** | **Throughput** | **GPU Util.** |
+===============:+====================:+======================:+===============:+==============:+
| 1              | 5.0ms               | 5.0ms                 | 200 img/s      | 15%           |
| 4              | 7.2ms               | 1.8ms                 | 556 img/s      | 42%           |
| 8              | 9.1ms               | 1.1ms                 | 879 img/s      | 65%           |
| 16             | 14.0ms              | 0.9ms                 | 1,143 img/s    | 85%           |
| 32             | 25.0ms              | 0.8ms                 | 1,280 img/s    | 95%           |
+----------------+---------------------+-----------------------+----------------+---------------+

*Times shown are pure inference time, excluding queue wait. User-perceived latency includes batching window wait (see @sec-serving-traffic-patterns).

**Key insight**: Batch size 32 achieves 6.4× higher throughput than batch size 1. However, user-perceived latency includes both queue wait and inference time. With a 10ms batching window and 25ms inference, total latency reaches 35ms versus 5ms at batch size 1.

:::

However, batching forces requests to wait. A request arriving just after a batch closes must wait for the current batch to complete plus full processing of its own batch. This waiting time directly adds to user-perceived latency, creating the fundamental tradeoff that serving system designers navigate.

### Static vs Dynamic Batching {#sec-serving-batching-types}

**Static batching** waits for a fixed batch size before processing. Simple to implement but problematic in practice: during low traffic, requests wait indefinitely for a full batch. During high traffic, large batches increase per-request latency.

**Dynamic batching** collects requests within a time window, processing whatever has arrived when the window closes [@olston2017tensorflow]. This bounds maximum wait time regardless of traffic level. The window size represents a direct tradeoff: shorter windows reduce latency but sacrifice throughput; longer windows improve throughput but increase latency.

Typical configurations use windows of 5-50ms with maximum batch sizes of 8-32 for latency-sensitive applications. The optimal configuration depends on request arrival patterns, model characteristics, and latency requirements.

### Dynamic Batching Latency-Throughput Trade-offs {#sec-serving-batching-tradeoffs}

Dynamic batching introduces a fundamental tension between throughput optimization and latency constraints. Understanding this tradeoff quantitatively enables principled configuration decisions rather than trial-and-error tuning. The total user-perceived latency for a batched request decomposes into two components as shown in @eq-batching-latency:

$$L_{\text{total}} = L_{\text{wait}} + L_{\text{compute}}(b)$$ {#eq-batching-latency}

where $L_{\text{wait}}$ is the time spent waiting in the batching queue and $L_{\text{compute}}(b)$ is the inference time for batch size $b$. The batching window $T$ bounds wait time ($L_{\text{wait}} \leq T$), while batch size affects compute time through GPU utilization characteristics.

**Queue Waiting Time Analysis**

For Poisson arrivals with rate $\lambda$ and batching window $T$, requests arrive uniformly within the window. A request arriving at time $t$ within the window waits $T - t$ for the batch to close. The average wait time, shown in @eq-avg-wait, is half the window:

$$E[L_{\text{wait}}] = \frac{T}{2}$$ {#eq-avg-wait}

This simple relationship has profound implications. A 20ms batching window adds 10ms average latency regardless of batch size achieved. If your latency SLO is 50ms and inference takes 5ms, the batching window consumes 20% of your latency budget before any computation begins.

**Batch Size Distribution**

The number of requests collected during window $T$ follows a Poisson distribution with mean $\lambda T$, as expressed in @eq-batch-distribution:

$$P(\text{batch size} = k) = \frac{(\lambda T)^k e^{-\lambda T}}{k!}$$ {#eq-batch-distribution}

This distribution reveals batch size variability. @tbl-batch-variability shows how batch size fluctuates for different traffic levels with a fixed 10ms window:

+------------------+----------------+-------------+----------------+---------------------+
| **Arrival Rate** | **Mean Batch** | **Std Dev** | **P(batch=0)** | **P(batch≥2×mean)** |
+=================:+===============:+============:+===============:+====================:+
| **50 QPS**       | 0.5            | 0.7         | 61%            | 1%                  |
| **200 QPS**      | 2.0            | 1.4         | 14%            | 9%                  |
| **500 QPS**      | 5.0            | 2.2         | 0.7%           | 12%                 |
| **1000 QPS**     | 10.0           | 3.2         | 0.005%         | 13%                 |
+------------------+----------------+-------------+----------------+---------------------+

: **Batch Size Variability**: At low traffic, batching windows frequently contain zero requests (wasted GPU cycles). At moderate traffic, batch sizes fluctuate significantly around the mean. High traffic provides more stable batching but still sees 13% of batches exceeding twice the mean size. {#tbl-batch-variability}

**Throughput Maximization Strategy**

Throughput optimization requires maximizing the number of requests processed per unit time. For a system with service time $S(b)$ for batch size $b$, throughput follows @eq-batch-throughput:

$$\text{Throughput}(b) = \frac{b}{T + S(b)}$$ {#eq-batch-throughput}

The numerator increases linearly with batch size while the denominator increases sub-linearly (due to GPU parallelism). This creates an optimal batch size that balances these competing effects.

For ResNet-50 on a V100 GPU, service time scales as $S(b) = 5\text{ms} + 0.6b$ (5ms fixed overhead plus 0.6ms per additional image in the batch). With $T = 10$ms batching window:

+----------------+------------------+-------------------+----------------+----------------+
| **Batch Size** | **Service Time** | **Total Latency** | **Throughput** | **Efficiency** |
+===============:+=================:+==================:+===============:+:===============+
| 1              | 5.6ms            | 15.6ms            | 64 img/s       | Low            |
| 4              | 7.4ms            | 17.4ms            | 230 img/s      | Moderate       |
| 8              | 9.8ms            | 19.8ms            | 404 img/s      | Good           |
| 16             | 14.6ms           | 24.6ms            | 650 img/s      | High           |
| 32             | 24.2ms           | 34.2ms            | 935 img/s      | Maximum        |
+----------------+------------------+-------------------+----------------+----------------+

: **Batching Throughput Analysis**: Throughput increases with batch size but total latency grows. The optimal configuration depends on whether the latency SLO or throughput target is the binding constraint. {#tbl-batching-throughput}

**Latency-Constrained Optimization**

When latency SLOs provide the binding constraint, the optimization problem becomes finding the maximum batch size that meets the SLO. For SLO $L_{\text{SLO}}$ and average wait time $T/2$, @eq-latency-constrained-batch defines the maximum allowable batch size:

$$b_{\text{max}} = \max\{b : \frac{T}{2} + S(b) \leq L_{\text{SLO}}\}$$ {#eq-latency-constrained-batch}

Consider a 50ms p95 latency SLO for ResNet-50 serving:

**Scenario 1: Conservative window (T = 5ms)**
- Average wait: 2.5ms
- Latency budget for inference: 47.5ms
- Maximum batch size: 71 (but typically capped at 32 for memory)
- Achieved throughput: ~1,140 img/s (batch=32)

**Scenario 2: Aggressive window (T = 25ms)**
- Average wait: 12.5ms
- Latency budget for inference: 37.5ms
- Maximum batch size: 48
- Achieved throughput: ~1,280 img/s (batch=48)

The aggressive window achieves only 12% higher throughput but increases average latency by 10ms and p99 latency by 25ms. @tbl-batching-throughput summarizes these trade-offs across different batch sizes. For latency-sensitive applications, the conservative window provides better user experience at modest throughput cost.

**SLO Violation Analysis**

Batch size variability causes SLO violations even when mean latency appears safe. The p99 latency includes both worst-case wait time (full window) and worst-case batch size (governed by Poisson tail). @eq-p99-batch-latency captures this relationship:

$$L_{p99} \approx T + S(b_{p99})$$ {#eq-p99-batch-latency}

where $b_{p99}$ is the 99th percentile batch size. For $\lambda = 500$ QPS and $T = 10$ms:

- Mean batch size: 5
- p99 batch size: 11 (from Poisson distribution)
- Mean latency: $5\text{ms} + 9.0\text{ms} = 14\text{ms}$
- p99 latency: $10\text{ms} + 11.6\text{ms} = 21.6\text{ms}$

The p99 latency is 1.54× the mean, reflecting both wait time variance and batch size variance. Systems that provision based on mean latency will experience SLO violations.

**Adaptive Batching Windows**

Fixed batching windows waste latency budget during high traffic when large batches form quickly. Adaptive strategies adjust the window based on queue depth, as shown in @lst-adaptive-batching.

::: {#lst-adaptive-batching lst-cap="**Adaptive Batching Window**: Dynamically adjusts batch timeout based on queue depth and arrival rate, reducing average latency by 27% compared to fixed windows while maintaining throughput."}
```{.python}
def adaptive_batching_window(queue_depth, arrival_rate, slo_ms):
    """Compute optimal batching window based on current system state."""
    target_batch_size = 16  # Optimal batch for GPU utilization

    # Fast path: batch ready, close immediately to minimize latency
    if queue_depth >= target_batch_size:
        return 0

    # Compute maximum allowable wait from SLO constraint
    # Reserve 30% of latency budget for batching, remainder for inference
    max_wait = slo_ms * 0.3

    # Estimate time to accumulate target batch at current arrival rate
    if arrival_rate > 0:
        requests_needed = target_batch_size - queue_depth
        estimated_wait = requests_needed / arrival_rate
        # Return minimum of estimated wait and SLO-constrained maximum
        return min(estimated_wait, max_wait)

    return (
        max_wait  # Low traffic: use full budget to accumulate batch
    )
```
:::

This approach reduces average wait time during high traffic while maintaining batch sizes. For traffic varying between 200-1000 QPS:

- Fixed window (10ms): Average latency 15ms, throughput 650 img/s
- Adaptive window: Average latency 11ms (27% reduction), throughput 680 img/s (5% improvement)

**Throughput-Latency Pareto Frontier**

The batching configuration space forms a Pareto frontier where improving throughput requires accepting higher latency. @tbl-pareto-batching shows this trade-off for different window configurations:

+-----------------+---------------+-----------------+-----------------+----------------+----------------------+
| **Window (ms)** | **Max Batch** | **Avg Latency** | **p99 Latency** | **Throughput** | **Configuration**    |
+================:+==============:+================:+================:+===============:+:=====================+
| 2               | 16            | 8ms             | 18ms            | 890 img/s      | Ultra-low latency    |
| 5               | 32            | 10ms            | 22ms            | 1,140 img/s    | Balanced             |
| 10              | 32            | 15ms            | 35ms            | 1,240 img/s    | Moderate latency     |
| 20              | 64            | 23ms            | 52ms            | 1,310 img/s    | Throughput-optimized |
| 50              | 128           | 38ms            | 98ms            | 1,350 img/s    | Maximum throughput   |
+-----------------+---------------+-----------------+-----------------+----------------+----------------------+

: **Batching Pareto Frontier**: Each configuration represents a different point on the throughput-latency trade-off curve. Moving from 2ms to 50ms windows improves throughput by only 52% while increasing p99 latency by 5.4×. Diminishing returns make aggressive batching costly for latency-sensitive applications. {#tbl-pareto-batching}

**Practical Configuration Guidelines**

Based on quantitative analysis, principled batching configuration follows these guidelines. Start with the latency budget by allocating 20 to 30 percent of SLO to batching wait time. Estimate traffic using the p95 arrival rate rather than average to account for traffic spikes. Calculate the maximum window as $T_{\text{max}} = 0.3 \times L_{\text{SLO}}$. Determine the batch size limit from GPU memory and p99 latency constraints. Monitor the actual distribution since batch size variance indicates whether traffic assumptions hold.

For ResNet-50 with 50ms SLO and 500 QPS traffic:

- Latency budget for batching: 15ms
- Maximum window: 15ms
- Expected batch size: 7.5
- Maximum batch size: 32 (memory limit)
- Configuration: $T = 12$ms, $b_{\text{max}} = 32$
- Predicted p99 latency: 43ms (within SLO)
- Predicted throughput: 1,180 img/s

### Continuous Batching {#sec-serving-continuous-batching}

Autoregressive models like language models generate outputs token by token, creating a batching challenge that differs fundamentally from single-pass models like ResNet-50. Traditional batching processes all sequences in a batch for all generation steps, wasting compute when sequences complete at different times [@yu2022orca]. If one sequence in a batch of 8 finishes after 10 tokens while others need 100 tokens, 87.5% of the compute for that sequence slot is wasted. This inefficiency becomes critical as language models dominate production inference workloads.

Continuous batching (also called iteration-level batching) addresses this waste by allowing new requests to join a batch between generation steps and completed sequences to exit [@kwon2023vllm]. Rather than forming static batches that persist for the entire generation process, the system manages batch composition dynamically at each decoding iteration.

The mechanism works as follows: when a sequence generates its end-of-sequence token, its slot becomes immediately available. A waiting request can fill that slot for the next iteration rather than waiting for the entire batch to complete. Similarly, the system can add new requests to available slots without interrupting ongoing generation. This dynamic approach maintains high GPU utilization even when sequence lengths vary dramatically.

Systems implementing continuous batching, such as vLLM and TensorRT-LLM, achieve 2-4× higher throughput than traditional static batching [@agrawal2024sarathi]. The improvement comes from two sources: eliminating wasted compute on completed sequences and reducing average wait time for new requests. For production language model serving where response lengths vary from single tokens to thousands, continuous batching has become essential for cost-effective deployment.

Memory management adds complexity to continuous batching. As sequences enter and exit the batch, the key-value cache that stores attention context must be dynamically allocated and freed. PagedAttention, introduced in vLLM, applies operating system paging concepts to manage this memory efficiently, avoiding fragmentation that would otherwise limit batch capacity [@kwon2023vllm]. These techniques represent the intersection of classical systems engineering with modern ML serving challenges.

::: {.callout-note title="LLM Serving: Beyond the Fundamentals"}

Language model serving introduces challenges beyond the batching and memory principles established here. The key-value cache that stores attention context scales with sequence length and batch size, often exceeding the model weights themselves in memory consumption. Techniques like speculative decoding use small draft models to propose multiple tokens that the target model verifies in parallel, achieving 2-3× latency reduction for interactive applications. Weight-only quantization (INT4 weights with FP16 activations) proves more effective than activation quantization for memory-bandwidth-bound LLM inference.

These LLM-specific optimizations build directly on the foundations this chapter establishes: queuing theory governs request scheduling, batching tradeoffs determine throughput-latency curves, and precision selection follows the same accuracy-efficiency principles. The serving fundamentals apply universally; LLM serving adds domain-specific techniques atop this foundation.

:::

While continuous batching represents the frontier for LLM serving, not all deployment scenarios benefit from batching at all. The sophistication of modern batching techniques should not obscure a fundamental question: when does batching hurt rather than help?

### When Not to Batch {#sec-serving-no-batch}

Some scenarios require single-request processing. Ultra-low latency requirements where p99 latency must stay under 10ms make any batching delay unacceptable. Highly variable request sizes where inputs vary dramatically in size cause batching to create padding overhead that wastes compute. Memory constraints where models already consume most GPU memory mean batch activations may cause out-of-memory errors.

### Session Affinity Constraints {#sec-serving-session-affinity}

When requests from the same user or session should route to the same replica, batching becomes constrained. Session affinity, also called sticky sessions, matters for three main reasons.

**KV-Cache Reuse**: For conversational AI, the key-value cache from previous turns dramatically speeds up multi-turn conversations. Routing a follow-up request to a different replica forfeits this cached context, increasing latency by 2 to 5 times for long conversations.

**User-Specific Models**: Some systems serve personalized models or adapters per user. Routing requests to the replica that has already loaded that user's adapter avoids repeated loading overhead.

**Stateful Preprocessing**: When preprocessing maintains state through tokenizer caches or session-specific normalization, routing to a different replica requires rebuilding this state.

The tension with batching is clear since strict affinity constrains which requests can be batched together, potentially reducing batch sizes and GPU utilization. Production systems often implement soft affinity where requests prefer their assigned replica but can overflow to others when that replica is overloaded. This preserves most affinity benefits while maintaining load balance.

### Traffic Patterns and Batching Strategy {#sec-serving-traffic-patterns}

The optimal batching strategy depends critically on how requests arrive. Different deployment contexts exhibit fundamentally different arrival patterns, each requiring distinct batching approaches. The MLPerf inference benchmark codifies these patterns into four scenarios that directly map to real-world deployments (see @sec-benchmarking-ai for MLPerf details).

**Server Traffic (Poisson Arrivals)**

Cloud APIs and web services typically receive requests following a Poisson process, where arrivals are independent and uniformly distributed over time. For Poisson arrivals with rate $\lambda$ and batching window $T$, the expected batch size follows @eq-poisson-batch:

$$E[\text{batch size}] = \lambda \cdot T$$ {#eq-poisson-batch}

The variance equals the mean (a property of Poisson distributions), so batch sizes fluctuate significantly at moderate traffic. With $\lambda = 200$ requests/second and $T = 10$ms, expected batch size is 2, but 16% of windows will have zero requests (wasted compute cycles) while others may have 4 or more.

The optimal batching window balances waiting cost against throughput benefit. As shown in @eq-optimal-window:

$$T_{\text{optimal}} = \min\left(L - S, \sqrt{\frac{S}{\lambda}}\right)$$ {#eq-optimal-window}

where $L$ is the latency SLO and $S$ is the service time. A counterintuitive result emerges from this equation: as traffic increases, the optimal window decreases while achieved batch sizes still grow. @tbl-traffic-adaptive demonstrates this relationship across traffic levels.

+------------------+--------------------+--------------------+-----------------+
| **Arrival Rate** | **Optimal Window** | **Avg Batch Size** | **p99 Latency** |
+=================:+===================:+===================:+================:+
| **100 QPS**      | 20ms               | 2.0                | 45ms            |
| **500 QPS**      | 8ms                | 4.0                | 42ms            |
| **1,000 QPS**    | 5ms                | 5.0                | 38ms            |
| **5,000 QPS**    | 2ms                | 10.0               | 35ms            |
+------------------+--------------------+--------------------+-----------------+

: **Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time. {#tbl-traffic-adaptive}

**Streaming Traffic (Correlated Arrivals)**

Autonomous vehicles, video analytics, and robotics systems receive inputs from multiple synchronized sensors. Rather than independent arrivals, frames from all cameras for a given timestamp must be processed together as a batch.

::: {.callout-note title="Multi-Camera Autonomous Vehicle Serving"}

Consider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial fusion:

**Timeline for processing frame set N:**

+----------+-----------------------------------+
| **Time** | **Event**                         |
+=========:+:==================================+
| T = 0ms  | Cameras begin capturing frame N   |
| T = 8ms  | Camera 1 frame arrives            |
| T = 10ms | Cameras 2-5 frames arrive         |
| T = 15ms | Camera 6 arrives (jitter)         |
| T = 15ms | Batch inference begins (6 images) |
| T = 25ms | Inference complete                |
| T = 32ms | Result ready for planning module  |
+----------+-----------------------------------+

**Key constraints:**

- Hard deadline: 33ms per frame set (real-time requirement)
- Batch size: Fixed at 6 (one per camera)
- Synchronization budget: 12ms of 33ms total (36% for jitter tolerance)
- Timeout policy: If camera frame not received by T+20ms, use previous frame

Unlike Poisson traffic where dynamic batching optimizes throughput, streaming traffic requires synchronization policies that handle sensor jitter while meeting hard deadlines.

:::

**Single-User Traffic (Sequential Arrivals)**

Mobile and embedded applications serve one user at a time, with requests arriving only after the previous result is consumed. Batch size is typically 1, eliminating batching optimization entirely but raising different challenges.

::: {.callout-note title="ResNet-50: Mobile Single-User Serving (Pixel 6 NPU)"}

+------------------------+--------------+------------------------+
| **Phase**              | **Duration** | **Notes**              |
+:=======================+=============:+:=======================+
| **Camera buffer read** | 8ms          | System API overhead    |
| **JPEG decode (CPU)**  | 15ms         | Single-threaded        |
| **Resize + Normalize** | 5ms          | CPU preprocessing      |
| **NPU inference**      | 12ms         | 82% NPU utilization    |
| **Post-process + UI**  | 5ms          | Result rendering       |
| **Total**              | **45ms**     | Perceived as "instant" |
+------------------------+--------------+------------------------+

**Critical insight**: Even at batch size 1, the mobile NPU achieves 82% utilization because its compute capacity matches single-image workloads. This differs fundamentally from datacenter GPUs, which achieve only 15% utilization at batch size 1 because their massive parallelism requires larger batches to saturate.

Mobile serving optimization focuses on preprocessing efficiency and power management rather than batching strategies.

:::

@tbl-traffic-patterns-summary maps MLPerf scenarios to deployment contexts and appropriate batching strategies.

+------------------+---------------------+------------------+---------------------------+
| **MLPerf**       | **Deployment**      | **Batch**        | **Optimization**          |
| **Scenario**     | **Context**         | **Strategy**     | **Focus**                 |
+:=================+:====================+:=================+:==========================+
| **Server**       | Cloud APIs,         | Dynamic batching | Window tuning,            |
|                  | web services        | with timeout     | utilization-latency curve |
+------------------+---------------------+------------------+---------------------------+
| **MultiStream**  | Autonomous driving, | Synchronized     | Jitter handling,          |
|                  | video analytics     | sensor fusion    | deadline guarantees       |
+------------------+---------------------+------------------+---------------------------+
| **SingleStream** | Mobile apps,        | No batching      | Preprocessing,            |
|                  | embedded devices    | (batch=1)        | power efficiency          |
+------------------+---------------------+------------------+---------------------------+
| **Offline**      | Batch processing,   | Maximum batch    | Throughput,               |
|                  | data pipelines      | size             | hardware utilization      |
+------------------+---------------------+------------------+---------------------------+

: **Traffic Patterns and Batching Strategies**: The MLPerf inference scenarios map to distinct deployment contexts, each requiring different batching approaches and optimization priorities. {#tbl-traffic-patterns-summary}

## Postprocessing {#sec-serving-postprocessing}

Batching optimizes how requests traverse the inference phase, but even optimally batched inference produces only raw tensors. These floating-point arrays carry no inherent meaning to applications or users. The final phase of the serving pipeline, postprocessing, transforms these tensors into actionable predictions: a 0.95 probability becomes a confident "dog" label, a sequence of token IDs becomes readable text, or a bounding box tensor becomes a highlighted region in an image. While often overlooked in system design, postprocessing significantly impacts both latency and the usefulness of predictions, determining whether raw model capability translates into practical value.

### From Logits to Predictions {#sec-serving-logits}

Classification models output logits or probabilities across classes. Converting these to predictions involves several potential steps including argmax selection that chooses the highest-probability class, thresholding that applies confidence thresholds before returning predictions, top-k extraction that returns multiple high-probability classes with scores, and calibration that adjusts raw probabilities to better reflect true likelihoods.

::: {.callout-note title="ResNet-50: Postprocessing Pipeline"}

For ResNet-50 image classification, typical postprocessing includes:

```{.python}
# Transform raw logits to calibrated probabilities
# Input: logits tensor of shape (batch_size, 1000) - one score per ImageNet class
probs = torch.softmax(
    logits, dim=-1
)  # Normalize to sum=1; ~0.05ms on GPU

# Extract top-5 predictions for multi-class response
# topk returns (values, indices) sorted by probability
top5_probs, top5_indices = probs.topk(5)  # ~0.02ms; GPU operation

# Map class indices to human-readable labels
# IMAGENET_CLASSES: list of 1000 class names from synset mapping
labels = [
    IMAGENET_CLASSES[i] for i in top5_indices
]  # ~0.01ms; CPU lookup

# Format response with predictions and metadata for API contract
response = {
    "predictions": [
        {"label": label, "confidence": float(prob)}
        for label, prob in zip(labels, top5_probs)
    ],
    "model_version": "resnet50-v2.1",  # Enable client-side version tracking
    "inference_time_ms": 5.2,  # Observability for latency monitoring
}
```

**Total postprocessing time**: ~0.1ms (negligible compared to preprocessing and inference)

:::

Each step adds latency but improves response utility. Calibration in particular can add significant computation but is essential when downstream systems make decisions based on confidence scores.

### Generation and Decoding {#sec-serving-decoding}

Generative models require decoding strategies that trade off quality, diversity, and latency. The choice of decoding strategy can dramatically affect both output quality and computational cost.

**Greedy decoding** selects the highest-probability token at each step. Fast but often produces repetitive, low-quality outputs because it cannot recover from early mistakes.

**Beam search** maintains multiple candidate sequences, selecting the highest-scoring complete sequence. Produces higher-quality outputs but multiplies computation by the beam width.

**Sampling** with temperature, top-k, and top-p parameters introduces randomness for diversity [@holtzman2020curious]. Temperature scales logits before softmax. Top-k limits sampling to the k highest-probability tokens. Top-p, also called nucleus sampling, limits sampling to tokens comprising probability mass p.

The choice involves latency tradeoffs [@meister2020beam]. Beam search with width 5 takes roughly 5× the compute of greedy decoding. Sampling adds minimal overhead but requires careful parameter tuning to balance quality and coherence.

### Output Formatting and Streaming {#sec-serving-output-format}

Production systems rarely return raw predictions. Outputs must conform to API contracts, often requiring JSON serialization with specific schema, confidence score formatting and thresholding, error handling for edge cases such as no confident prediction or out-of-distribution input, and metadata attachment including model version, inference time, and feature attributions.

Streaming responses for generative models add complexity. Rather than waiting for complete generation, systems return tokens as they are produced. This improves perceived latency (users see output beginning quickly) but requires infrastructure support for chunked responses and client-side incremental rendering.

## Inference Runtime Selection {#sec-serving-runtimes}

The complete preprocessing, inference, and postprocessing pipeline we have examined must execute somewhere, and that execution environment significantly impacts whether the latency budgets we established earlier are achievable. The inference runtime, the software layer that orchestrates tensor operations and manages hardware resources, can vary by an order of magnitude in performance for identical models. Choosing appropriately requires understanding the tradeoffs between framework-native serving, general-purpose optimization, and specialized inference engines.

### Framework-Native Serving {#sec-serving-framework-native}

PyTorch and TensorFlow models can serve directly using their native runtimes. This approach maximizes compatibility (any model that trains will serve) and simplifies the deployment pipeline (no export or conversion step). However, framework runtimes include training functionality that adds overhead, and default execution paths may not exploit hardware-specific optimizations.

TorchScript and TensorFlow SavedModel formats enable ahead-of-time compilation and graph optimization, improving over eager execution while maintaining framework compatibility. These formats represent the first step toward deployment optimization without abandoning the familiar framework ecosystem.

### General-Purpose Optimization {#sec-serving-onnx}

ONNX Runtime provides a hardware-agnostic optimization layer [@onnxruntime2024]. Models export to ONNX format, then ONNX Runtime applies graph optimizations and selects execution providers for the target hardware. This enables single-format deployment across CPUs, GPUs, and specialized accelerators.

### Specialized Inference Engines {#sec-serving-specialized}

TensorRT (NVIDIA GPUs), OpenVINO (Intel hardware), and similar engines optimize specifically for their target hardware [@nvidia2024tensorrt; @chen2018tvm]. They apply aggressive optimizations: layer fusion, precision calibration, kernel auto-tuning. These typically achieve 2-5× speedup over framework-native serving but require explicit export and may not support all operations.

::: {.callout-note title="ResNet-50: Runtime Comparison"}

Performance comparison for ResNet-50 inference on V100 GPU (batch size 1):

+-----------------+-------------+-------------+---------------------------+
| **Runtime**     | **Latency** | **Speedup** | **Notes**                 |
+:================+============:+============:+:==========================+
| PyTorch (eager) | 8.5ms       | 1.0×        | Baseline, no optimization |
| TorchScript     | 6.2ms       | 1.4×        | JIT compilation           |
| ONNX Runtime    | 5.1ms       | 1.7×        | Cross-platform            |
| TensorRT FP32   | 2.8ms       | 3.0×        | NVIDIA-specific           |
| TensorRT FP16   | 1.4ms       | 6.1×        | Tensor Core acceleration  |
| TensorRT INT8   | 0.9ms       | 9.4×        | Requires calibration      |
+-----------------+-------------+-------------+---------------------------+

**Key insight**: The 9.4× speedup from TensorRT INT8 comes at the cost of: (1) quantization calibration data, (2) potential accuracy loss (<1% for ResNet-50), and (3) NVIDIA-specific deployment.

:::

The optimization-compatibility tradeoff is fundamental. More aggressive optimization yields better performance but increases deployment complexity and may introduce numerical differences from training. The choice depends on latency requirements, deployment constraints, and available engineering resources.

### Runtime Configuration {#sec-serving-runtime-config}

Beyond runtime selection, configuration choices impact serving performance including thread pools that control parallelism for CPU inference, memory allocation strategies that choose between pre-allocating buffers versus dynamic allocation, execution providers that select and prioritize hardware backends, and graph optimization level that trades compilation time for runtime performance. Production deployments require experimentation to find optimal configurations for specific models and hardware combinations. A systematic approach tests key parameters and measures their impact on latency distributions.

### Precision Selection for Serving {#sec-serving-precision}

Numerical precision directly trades accuracy for throughput, connecting to the quantization techniques covered in @sec-model-optimizations. While @sec-model-optimizations focuses on training-time quantization, serving introduces additional considerations including calibration requirements, layer sensitivity, and dynamic precision selection.

**Precision-Throughput Relationship**

For memory-bandwidth-bound operations, reducing precision proportionally increases throughput by reducing data movement. The theoretical maximum speedup from precision reduction follows @eq-precision-throughput:

$$\frac{\text{Throughput}_{\text{INT8}}}{\text{Throughput}_{\text{FP32}}} = \frac{32}{8} = 4\times \text{ (theoretical maximum)}$$ {#eq-precision-throughput}

In practice, GPU compute pipelines and Tensor Core alignment requirements limit achieved speedup to 2.5-3.5× for INT8 versus FP32. Tensor Cores require specific alignment: INT8 operations need tensor dimensions divisible by 16, while FP16 requires divisibility by 8 (see @sec-ai-acceleration for Tensor Core architecture details).

::: {.callout-note title="ResNet-50: Precision Tradeoffs on V100"}

+----------------+-------------+------------+--------------+-----------------------+-----------------+
| **Precision**  | **Latency** | **Memory** | **Accuracy** | **Tensor Core Util.** | **Calibration** |
+===============:+============:+===========:+=============:+======================:+:================+
| **FP32**       | 2.8ms       | 98MB       | 76.13%       | 0%                    | None            |
| **FP16**       | 1.4ms       | 49MB       | 76.13%       | 85%                   | None            |
| **INT8 (PTQ)** | 0.9ms       | 25MB       | 75.80%       | 92%                   | 1,000 samples   |
| **INT8 (QAT)** | 0.9ms       | 25MB       | 76.05%       | 92%                   | Full retraining |
+----------------+-------------+------------+--------------+-----------------------+-----------------+

**Key observations:**

- INT8 achieves 3.1× speedup but loses 0.33% accuracy with post-training quantization (PTQ)
- Quantization-aware training (QAT) recovers most accuracy but requires retraining
- FP16 provides 2× speedup with no accuracy loss for most models

:::

**Layer Sensitivity**

Not all layers tolerate reduced precision equally. Quantization error for a layer scales with weight magnitude and gradient sensitivity, as captured by @eq-quant-error:

$$\epsilon_{\text{quant}} \propto \alpha \cdot \|W\|_2 \cdot 2^{-b}$$ {#eq-quant-error}

where $\alpha$ is a layer-specific sensitivity coefficient, $\|W\|_2$ is the weight L2 norm, and $b$ is the bit width. This explains observed patterns where first convolutional layers with high gradients and large sensitivity coefficients are precision-sensitive and often kept at FP16, middle layers with stable gradients and low sensitivity coefficients tolerate INT8 well, and final classification layers with small weights but high task sensitivity benefit from FP16 or higher precision.

**Calibration Requirements**

Post-training quantization requires a calibration dataset to determine optimal scale factors for INT8 conversion. Production experience shows that calibration data must be representative of actual serving traffic, not just training data. Using ImageNet validation images to calibrate a model serving wildlife camera images resulted in 3.2% accuracy degradation in one production system.

**Dynamic Precision Selection**

Advanced serving systems select precision per request based on runtime conditions. If the system is ahead of latency SLO, it uses higher precision for better accuracy. For low-confidence INT8 results, it recomputes at FP16. Different customer tiers may receive different precision levels. This pattern enables adaptive quality-latency tradeoffs while maximizing throughput during normal operation.

The precision decision has direct infrastructure consequences: INT8 inference achieves roughly 3x higher throughput than FP32, meaning a workload requiring 30 GPUs at FP32 needs only 10 at INT8. This connection between model-level optimization and infrastructure economics is why precision selection cannot be treated as purely a model concern, and why the next section examines cost and capacity planning in detail.

## Cost and Capacity Planning {#sec-serving-cost}

The runtime and precision choices examined in previous sections determine per-inference performance, but production deployment requires translating these choices into infrastructure decisions. Serving costs scale with request volume, unlike training costs that scale with dataset size and model complexity [@zhang2019mark]. Cost structure analysis enables informed infrastructure decisions that balance performance requirements against budget constraints.

### Cost Per Inference {#sec-serving-cost-per-inference}

Total serving cost decomposes into several components including compute time for GPU or CPU per inference, memory for accelerator memory required to hold model and activations, data transfer for network bandwidth for request and response payloads, and orchestration overhead for container runtime, load balancing, and monitoring. For GPU inference, compute time dominates when utilization is high. When utilization is low, memory cost dominates because the GPU is reserved but idle. Serving infrastructure should maximize GPU utilization through batching, multi-model serving, or right-sized instance selection.

::: {.callout-note title="ResNet-50: Cost Analysis"}

Consider serving ResNet-50 on AWS infrastructure:

+---------------------------+---------------+----------------+------------------------+
| **Instance Type**         | **Cost/Hour** | **Throughput** | **Cost per 1M Images** |
+==========================:+==============:+===============:+=======================:+
| **c5.xlarge (CPU)**       | $0.17         | 50 img/s       | $0.94                  |
| **g4dn.xlarge (T4 GPU)**  | $0.53         | 400 img/s      | $0.37                  |
| **p3.2xlarge (V100 GPU)** | $3.06         | 1,200 img/s    | $0.71                  |
+---------------------------+---------------+----------------+------------------------+

**Key insight**: The T4 GPU instance achieves the lowest cost per inference despite higher hourly cost, because GPU throughput dramatically exceeds CPU throughput. The V100 is only cost-effective at very high sustained traffic where its higher throughput justifies the 6× price increase.

:::

### GPU vs CPU Economics {#sec-serving-gpu-cpu}

GPUs provide orders-of-magnitude speedup for parallel operations but cost significantly more per hour [@wu2019machine]. The crossover point depends on model characteristics and latency requirements.

CPU inference makes economic sense when models are small with few parameters and simple operations, latency requirements are relaxed with hundreds of milliseconds acceptable, request volume is low or highly variable, and models use operations that do not parallelize well. GPU inference makes economic sense when models are large with parallel-friendly operations, latency requirements are strict at tens of milliseconds, request volume is high and consistent, and batching can achieve high utilization.

**Scaling Responsiveness**: Beyond steady-state costs, startup time affects scaling economics. CPU instances typically start in 30 to 60 seconds while GPU instances take 2 to 5 minutes including driver initialization, model loading, and warmup. For variable traffic patterns, this startup latency can be more important than cost per inference. If traffic spikes arrive faster than GPU instances can scale, latency SLOs will be violated despite having sufficient eventual capacity.

This asymmetry suggests different scaling strategies where CPU instances enable reactive scaling by responding to current demand while GPU instances often require predictive scaling by provisioning based on anticipated demand. For bursty workloads, a hybrid approach uses always-on GPU capacity for baseline load plus CPU overflow capacity for spikes, trading higher per-inference cost during spikes for better responsiveness.

### Capacity Planning {#sec-serving-capacity-planning}

The GPU versus CPU decision establishes the cost per inference, but determining how much infrastructure to provision requires combining cost analysis with the queuing theory foundations from @sec-serving-queuing. Capacity planning translates latency requirements and traffic projections into infrastructure specifications. Key inputs include traffic patterns such as peak request rate, daily and weekly cycles, and growth projections, latency SLOs including p50, p95, and p99 targets, and model characteristics such as inference time distribution at various batch sizes. From these inputs, queuing theory determines required capacity [@harchol2013performance]. The equations developed in @sec-serving-queuing provide the mathematical foundation where @eq-mm1-wait shows how latency scales with utilization, while @eq-p99-latency enables calculating p99 latency for capacity planning.

The relationship between utilization and latency is nonlinear as shown in the utilization-latency table. At 70 percent utilization, p99 latency is approximately fifteen times service time. At 90 percent utilization, it reaches approximately 46 times service time. This nonlinearity explains why systems that seem healthy with low average latency can suddenly violate SLOs when traffic increases modestly.

The worked example in @sec-serving-queuing demonstrates the complete capacity planning process by starting from a 50ms p99 SLO and 5,000 QPS target, deriving the safe utilization threshold of 72 percent, calculating required service rate of 6,944 QPS, and determining GPU count with headroom of 10 V100s. Production systems typically provision for peak load plus 30 percent headroom, using auto-scaling to reduce costs during low-traffic periods while meeting latency objectives during peaks.

## Fallacies and Pitfalls {#sec-serving-fallacies-pitfalls}

The principles established throughout this chapter provide a systematic framework for designing serving systems that meet latency requirements while maximizing efficiency. However, practitioners frequently encounter misconceptions that lead to suboptimal designs or production failures. These fallacies and pitfalls emerge from the fundamental differences between training and serving that make intuitions from one domain misleading in the other.

**Fallacy:** _Faster model inference automatically means faster end-to-end serving._

This misconception leads teams to focus optimization efforts exclusively on model inference while ignoring preprocessing and postprocessing overhead. As demonstrated in @sec-serving-latency-budget, preprocessing often consumes 45 to 70 percent of total latency when inference runs on optimized accelerators. A team that reduces inference time from 5ms to 2ms through quantization achieves only 15 percent improvement in total latency if preprocessing remains at 8ms. Effective optimization requires profiling the complete request path and allocating engineering effort proportionally to where time is actually spent.

**Pitfall:** _Running serving infrastructure at high utilization to maximize cost efficiency._

The nonlinear relationship between utilization and latency as shown in @eq-mm1-wait makes high utilization dangerous for latency-sensitive systems. At 90 percent utilization, average wait time reaches ten times service time, and p99 latency becomes unacceptable for most SLOs. Teams that provision for average load rather than peak load find their systems violate latency SLOs precisely when traffic increases, the moment when reliable performance matters most. Production systems typically target 60 to 70 percent utilization at peak load to maintain latency headroom.

**Fallacy:** _Training accuracy guarantees serving accuracy._

Training-serving skew as discussed in @sec-serving-skew causes models to behave differently in production despite using identical weights. Differences in preprocessing libraries, numerical precision, feature computation timing, or input distribution silently degrade accuracy without triggering obvious errors. A model achieving 95 percent accuracy in training evaluation might drop to 90 percent in production due to subtle preprocessing differences that shift inputs outside the training distribution. Prevention requires either identical preprocessing code paths or rigorous statistical monitoring of input distributions.

**Pitfall:** _Using average latency to evaluate serving system performance._

Average latency hides the experience of the slowest requests, which often determine user satisfaction and SLO compliance. A system with 10ms average latency might have 200ms p99 latency, meaning 1 percent of users experience twenty times worse performance. At scale with fan-out amplification as discussed in @sec-serving-tail-latency, even rare slow responses become visible to most users. Production SLOs specify percentile targets such as p95 and p99 precisely because averages mask unacceptable tail behavior.

**Fallacy:** _Larger batch sizes always improve throughput._

While batching amortizes fixed costs and improves GPU utilization, it also increases per-request latency and can cause memory exhaustion. Beyond a certain point, larger batches provide diminishing throughput returns while latency continues to grow linearly. Batch sizes that exceed GPU memory cause out-of-memory failures, and highly variable input sizes create padding overhead that wastes compute. The optimal batch size depends on latency SLOs, memory constraints, and traffic patterns, not just throughput maximization.

**Pitfall:** _Calibrating quantized models with training data rather than production traffic._

Post-training quantization requires calibration data to determine optimal scale factors, but using training data assumes production inputs match the training distribution. When production traffic differs through different image sources, lighting conditions, or user behavior, calibration on training data produces suboptimal scale factors that degrade accuracy. One production system experienced 3.2 percent accuracy loss when calibrating with ImageNet validation images but serving wildlife camera images. Effective calibration requires representative samples of actual serving traffic.

**Fallacy:** _Cold start latency only matters for the first request._

Cold start affects any request that arrives after a period of inactivity, after model updates, or when auto-scaling adds new instances. In systems with bursty traffic or multiple model versions, cold starts can affect a significant fraction of requests. A model requiring 500ms to load impacts not just the first user but every user who triggers a scale-up event or model reload. Production systems mitigate cold start through model preloading, keep-alive mechanisms, and gradual traffic shifting during deployments.

## Summary {#sec-serving-summary}

Serving represents the critical transition from model development to production deployment, where the optimization priorities that governed training must be fundamentally inverted. The shift from throughput maximization to latency minimization transforms every system design decision. The queuing theory foundations established here reveal why this inversion is not merely a change in metrics but a change in the governing mathematics: the nonlinear relationship between utilization and latency means that systems behaving well at moderate load can suddenly violate SLOs when traffic increases modestly. Little's Law and the M/M/1 wait time equations provide the quantitative foundation for capacity planning, replacing intuition-based provisioning with engineering rigor.

Effective serving optimization requires understanding the complete request path rather than focusing exclusively on model inference. Preprocessing often consumes 45 to 70 percent of total latency when inference runs on optimized accelerators, yet engineering effort frequently targets the wrong bottleneck. The microsecond-scale overheads identified by Barroso, Patterson, and colleagues explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization matters as much as model optimization. Training-serving skew represents another dimension of this complexity, silently degrading accuracy when preprocessing logic differs between training and production environments in ways that traditional testing cannot detect.

The traffic pattern analysis reveals how deployment context fundamentally shapes batching strategy and system design. Server workloads with Poisson arrivals optimize dynamic batching windows, autonomous vehicles with streaming sensor data require synchronized batch formation, and mobile applications with single-user patterns eliminate batching entirely. The MLPerf scenarios codify these patterns for standardized benchmarking, connecting the serving principles established here to the measurement frameworks explored in @sec-benchmarking-ai. Precision selection and runtime optimization extend the quantization techniques from @sec-model-optimizations and Tensor Core capabilities from @sec-ai-acceleration into the serving domain, where calibration with representative production traffic becomes essential.

::: {.callout-important title="Key Takeaways"}
* Serving inverts training priorities: latency per request matters more than aggregate throughput, requiring fundamentally different system design
* Queuing theory provides the mathematical foundation for capacity planning, with the utilization-latency relationship explaining why systems degrade nonlinearly under load
* Preprocessing often dominates total latency (45-70%), making pipeline optimization as important as model optimization
* Traffic patterns (Poisson, streaming, single-user) determine optimal batching strategy, directly mapping to MLPerf inference scenarios
* Precision selection and resolution tradeoffs connect serving decisions to optimization techniques from earlier chapters
* Training-serving skew silently degrades accuracy and requires identical preprocessing code or rigorous monitoring to prevent
:::

The serving foundations established here provide the infrastructure for the operational deployment strategies explored in @sec-ml-operations. Production environments introduce additional complexities of monitoring, versioning, and continuous validation that characterize real-world ML system deployment.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
