---
quiz: serving_quizzes.json
concepts: serving_concepts.yml
glossary: serving_glossary.json
---

# Serving {#sec-serving}

::: {layout-narrow}
::: {.column-margin}
*DALL·E 3 Prompt: An illustration of a machine learning serving system, depicted as a high-speed assembly line in a modern factory. On one end, raw data (images, text documents, sensor readings) enters on conveyor belts. In the center, a glowing neural network processes inputs through preprocessing stations, a central inference engine, and postprocessing units. Workers monitor latency gauges and throughput meters. Some requests wait in small batches while others stream through individually. The scene conveys speed, precision, and the transformation from raw input to useful predictions, with visible clocks emphasizing the time-critical nature of serving.*
:::

\noindent
![](images/png/cover_serving.png){width=80%}

:::

## Purpose {.unnumbered}

_How do the system requirements for serving trained models differ from training them, and what principles govern the design of responsive prediction systems?_

Training and serving are fundamentally different computational paradigms requiring distinct system designs. Training optimizes throughput over days or weeks of computation while serving inverts this priority, optimizing latency per request under strict time constraints measured in milliseconds. A common misconception is that faster hardware automatically means faster serving, but in practice preprocessing and postprocessing often dominate latency: production systems report preprocessing consuming 60-70% of total request time when inference runs on optimized accelerators. Understanding where latency actually comes from requires mastering queuing theory fundamentals that explain why systems degrade nonlinearly under load, recognizing how traffic patterns (Poisson arrivals for servers, streaming for autonomous vehicles, single-user for mobile) determine batching strategy, and connecting serving decisions to prior chapters on quantization, hardware acceleration, and benchmarking. The principles established here for single-machine serving provide the foundation for understanding when and why scaling to multiple machines becomes necessary.

::: {.callout-tip title="Learning Objectives"}

- Contrast training and serving system priorities by explaining the inversion from throughput optimization to latency minimization

- Decompose request latency into preprocessing, inference, and postprocessing phases to identify optimization bottlenecks

- Apply Little's Law and M/M/1 queuing models to predict serving system latency under varying load conditions

- Perform capacity planning to meet percentile latency SLOs while accounting for traffic variance and fault tolerance requirements

- Identify sources of training-serving skew and select prevention strategies appropriate to deployment contexts

- Select batching strategies (dynamic, continuous, none) based on traffic patterns and latency constraints

- Evaluate runtime and precision tradeoffs to meet deployment cost and performance requirements

:::

## The Serving Paradigm {#sec-serving-paradigm}

Parts I through III built a complete foundation for creating optimized ML models. You understand where systems deploy (@sec-ml-systems), how data flows through pipelines (@sec-data-engineering), what neural networks compute (@sec-dl-primer, @sec-dnn-architectures), how frameworks enable implementation (@sec-ai-frameworks), and how training produces learned models (@sec-ai-training). Part III then addressed making those models efficient: the conceptual framework for efficiency (@sec-efficient-ai), specific optimization techniques (@sec-model-optimizations), hardware acceleration strategies (@sec-ai-acceleration), and measurement methodologies for validating improvements (@sec-benchmarking-ai). The result is an optimized model ready for deployment. Part IV addresses what happens next: delivering predictions to users in production.

Serving introduces a fundamental inversion that transforms everything established in prior chapters. Training optimizes for samples processed per hour over days of computation. Serving must deliver predictions within milliseconds under unpredictable load. @sec-benchmarking-ai established techniques for measuring throughput and accuracy under controlled conditions; production serving faces traffic patterns that no benchmark could anticipate. @sec-model-optimizations provided quantization methods that reduced model size; serving must validate that those optimizations preserve accuracy under real traffic distributions. This inversion from throughput to latency, from controlled to unpredictable, from offline to real time defines the serving challenge.

::: {.callout-definition title="Model Serving"}

**Model Serving** refers to the process of exposing trained machine learning models for _real-time prediction_, requiring systems that transform raw inputs into useful outputs while meeting _latency constraints_, maintaining _consistency_ with training behavior, and achieving _cost-effective resource utilization_.

:::

### Static vs Dynamic Inference {#sec-serving-static-dynamic}

The first architectural decision in any serving system is whether predictions happen before or during user requests [@google2024staticdynamic]. This choice fundamentally shapes system design, cost structure, and capability boundaries.

**Static inference** (also called offline or batch inference) pre-computes predictions for anticipated inputs and stores them for retrieval. Consider a recommendation system that generates predictions for all user-item pairs nightly. When a user requests recommendations, the system retrieves pre-computed results from a lookup table rather than running inference. This approach eliminates inference latency entirely since results already exist, enables quality verification before deployment, and reduces serving costs. However, static inference cannot handle novel inputs that were not anticipated during the batch computation and introduces hours or days of latency when models update.

**Dynamic inference** (also called online or real-time inference) computes predictions on demand when requests arrive. This handles any input, including rare edge cases and novel combinations, and immediately reflects model updates. The cost is strict latency requirements that may force simpler models and more intensive monitoring infrastructure.

::: {.callout-note title="ResNet-50: Static vs Dynamic Tradeoffs"}

For our ResNet-50 image classifier, consider two deployment scenarios:

**Static approach**: A photo organization app pre-classifies all images in a user's library overnight. With 10,000 photos and 5ms inference each, batch processing takes ~50 seconds total. Users see instant classification when browsing their library.

**Dynamic approach**: A content moderation API must classify user-uploaded images in real-time. Each image requires the full preprocessing→inference→postprocessing pipeline, with a 100ms latency budget to meet user expectations.

Most production image classification systems use a **hybrid approach**: frequently requested images (popular products, known memes) are pre-classified and cached, while novel uploads trigger dynamic inference. This reduces average latency while maintaining flexibility.

:::

::: {.callout-notebook title="Engineering Economics: The Cost of Latency"}

Latency constraints directly dictate infrastructure costs. Consider a GPU server renting for \$4/hour.

**Scenario A (Low Latency):** Batch size 1.
*   Latency: 5ms.
*   Throughput: 200 req/s.
*   Cost per million queries: **\$5.55**.

**Scenario B (High Throughput):** Batch size 8.
*   Latency: 10ms (doubled due to batching overhead).
*   Throughput: 800 req/s (quadrupled due to parallel efficiency).
*   Cost per million queries: **\$1.38**.

**The Trade-off:** Reducing latency from 10ms to 5ms increases the hardware bill by **400%**. Engineers must quantify whether that 5ms speedup generates enough business value to justify the 4x cost increase.

:::

Most production systems combine both approaches. Common queries hit a cache populated by batch inference while uncommon requests trigger dynamic computation. Understanding this spectrum is essential because it determines which subsequent optimization strategies apply. Static inference optimizes for throughput during batch computation and storage efficiency for serving. Dynamic inference optimizes for per-request latency under concurrent load, which requires understanding where time goes within each request.

### The Spectrum of Serving Architectures {#sec-serving-architectures}

While "serving" often implies a networked server processing API requests, the architectural pattern varies fundamentally by deployment environment.

**1. Networked Serving (Cloud/Datacenter)**
The model runs as a standalone service (microservice). The primary interface is the network (HTTP/gRPC). Optimization focuses on **throughput** (batching) and **concurrency).
*   *Key Constraint:* Network bandwidth and serialization cost.

**2. Application-Embedded Serving (Mobile/Edge)**
The model runs within the user application process (e.g., a smartphone app using CoreML). There is no "server." The interface is a function call. Optimization focuses on **energy** and **responsiveness** (SingleStream).
*   *Key Advantage:* **Zero-Copy Inference**. The camera buffer can often be read directly by the NPU without CPU copying or serialization.

**3. Bare-Metal Serving (TinyML)**
The model is compiled into the firmware of a microcontroller. There is no operating system or dynamic memory allocator. "Serving" is a tight loop reading sensors and invoking the interpreter. Optimization focuses on **static memory usage** (fitting in SRAM).
*   *Key Difference:* All memory is pre-allocated (Tensor Arena). Dynamic batching is impossible.

### The Load Balancer Layer {#sec-serving-load-balancer}

Production serving systems place load balancers between clients and model servers. A load balancer provides three essential functions for serving infrastructure.

**Request Distribution** routes incoming requests to available model replicas using algorithms like round-robin or least-connections. For latency-sensitive ML serving, algorithms that route away from slow or overloaded replicas improve tail latency.

**Health Monitoring** continuously verifies that replicas are ready to serve, routing traffic away from unhealthy instances. For ML systems, health checks must verify not just process liveness but model readiness, confirming that weights are loaded and warmup is complete.

**Deployment Support** enables safe model updates by gradually shifting traffic between versions. @sec-ml-operations examines deployment strategies including canary testing, blue-green deployments, and shadow mode validation.

For single-machine serving with multiple model instances, such as running several ONNX Runtime sessions, the framework and operating system handle request queuing. The full complexity of load balancing becomes essential when scaling to distributed inference systems, where multiple machines serve the same model. The implementation details of request distribution algorithms and multi-replica architectures belong to that distributed context.

**Impact on Queuing Analysis**: When capacity planning considers "the server" in this chapter, it means the single machine's model serving capacity. The queuing dynamics analyzed in @sec-serving-queuing apply to understanding single-machine behavior and determining when scaling to multiple machines becomes necessary.

### Deterministic Latency and Resource Isolation {#sec-serving-isolation}

An inference server does not operate in isolation. On a single machine, the operating system manages multiple competing processes—logging agents, monitoring tools, and system interrupts—which can intermittently steal CPU cycles from the inference pipeline. These "noisy neighbors" are a primary source of **latency jitter**, where the time required to process identical requests varies significantly, causing the 99th percentile (P99) latency to spike even when the hardware is under-utilized.

To achieve deterministic performance on a single node, systems engineers employ three primary isolation techniques:

1.  **CPU Affinity (Pinning)**: Restricting the inference server's threads to specific physical CPU cores. This prevents the operating system from context-switching the server's processes, ensuring that the "preprocessing" stage of the pipeline always has immediate access to computational resources.
2.  **Memory Locking (`mlock`)**: Instructing the OS to lock the model weights and KV caches in physical RAM. This prevents the system from "paging out" model data to slow disk storage during periods of high memory pressure, ensuring consistent microsecond-scale access times.
3.  **Interrupt Shielding**: Configuring the system to route network and storage interrupts to CPU cores not used by the inference runner. This ensures that a burst of incoming network traffic does not interrupt the GPU's command stream, protecting the "heartbeat" of the inference execution.

Understanding these isolation principles transforms a simple "model script" into a **deterministic service**, a transition essential for safety-critical applications like autonomous driving or real-time industrial control.

## Serving System Architecture {#sec-serving-architecture}

Building a high-performance serving system requires orchestrating multiple software components to minimize overhead and maximize hardware utilization.

### Anatomy of an Inference Server {#sec-serving-anatomy}

While model optimization focuses on the mathematical artifact, model serving requires a specialized software architecture to manage high-frequency request streams and hardware utilization. An inference server (such as NVIDIA Triton, TensorFlow Serving, or TorchServe) is not a simple wrapper around a model script; it is a high-performance scheduler that manages concurrency, memory, and data movement.

Understanding the internal anatomy of these servers reveals how they bridge the gap between irregular user traffic and the highly regular, batch-oriented requirements of accelerators.

### The Request Pipeline {#sec-serving-pipeline}

Every request traverses a multi-stage pipeline designed to maximize hardware throughput while minimizing latency overhead. @fig-server-anatomy visualizes this internal flow.

::: {#fig-server-anatomy fig-env="figure" fig-pos="htb" fig-cap="**Inference Server Anatomy**: Modern inference servers organize request processing into a decoupled pipeline. The Network Ingress handles high-concurrency protocols (HTTP/gRPC), the Queue buffers bursts of traffic, and the Dynamic Batcher aggregates individual requests into optimized tensors. The Inference Runner manages the low-level execution on the hardware accelerator, ensuring the GPU remains utilized through asynchronous execution."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.2cm]
  \tikzset{
    Box/.style={draw=black!70, thick, rounded corners=2pt, align=center, minimum width=2.5cm, minimum height=1cm},
    Hardware/.style={Box, fill=gray!10},
    Software/.style={Box, fill=blue!10},
    Queue/.style={draw=black!70, thick, shape=cylinder, shape border rotate=90, aspect=0.25, minimum width=1.5cm, minimum height=1.2cm, fill=yellow!10}
  }

  \node[Box, fill=white] (client) {Client\\(Request)};
  \node[Software, right=of client] (ingress) {Network Ingress\\(HTTP/gRPC)};
  \node[Queue, right=of ingress] (queue) {Request\\Queue};
  \node[Software, right=of queue] (scheduler) {Dynamic\\Batcher};
  \node[Software, below=1.5cm of scheduler] (runtime) {Inference Runner\\(TensorRT/ONNX)};
  \node[Hardware, below=1.0cm of runtime] (gpu) {Accelerator\\(GPU/TPU)};

  \draw[->, thick] (client) -- (ingress);
  \draw[->, thick] (ingress) -- (queue);
  \draw[->, thick] (queue) -- (scheduler);
  \draw[->, thick] (scheduler) -- (runtime);
  \draw[->, thick] (runtime) -- (gpu);

  % Labels
  \node[right=0.2cm of queue, font=\scriptsize, text=gray] {Request Buffering};
  \node[right=0.2cm of scheduler, font=\scriptsize, text=gray] {Throughput Opt.};
  \node[right=0.2cm of runtime, font=\scriptsize, text=gray] {Execution Opt.};

\end{tikzpicture}
```
:::

The server architecture serves three critical functions:

1.  **Concurrency Management**: Servers use asynchronous event loops or thread pools to handle thousands of concurrent client connections without blocking. This ensures that network I/O wait times do not idle the accelerator.
2.  **Request Transformation**: The server handles the conversion of network payloads (JSON/Protobuf) into the specific tensor formats (NCHW vs NHWC) required by the optimized model runtime.
3.  **Model Management**: Servers manage the lifecycle of models, including loading weights into VRAM, managing versioning, and ensuring that "warm-up" inferences are completed before exposing the model to live traffic.

### The Scheduler: Where Throughput Meets Latency

The **Scheduler** is the "brain" of the inference server. It implements the dynamic batching logic discussed in @sec-serving-throughput. The scheduler must decide: "Should I run this one request now to minimize its latency, or wait 5 milliseconds for a second request to arrive and process them together to maximize throughput?"

Systems designers use the **Batching Window** parameter to tune this trade-off. A window of 0ms optimizes for pure latency (no batching), while a window of 10-50ms is common for high-throughput cloud services. This decision determines the "duty cycle" of the GPU—the percentage of time the hardware is actually computing vs. waiting for work.

## Latency and Queuing Dynamics {#sec-serving-dynamics}

Understanding serving performance requires analyzing both the static latency of a single request and the dynamic behavior of the system under load.

### The Latency Budget {#sec-serving-latency-budget}

For dynamic inference systems, the fundamental difference from training lies in optimization objectives. Training optimizes throughput: maximizing samples processed per hour over days of computation. A training job that processes 1000 samples per second is successful regardless of how long any individual sample takes. Serving inverts this priority, optimizing latency per request under strict time constraints. A serving system with 1000ms per-request latency has failed, even if it achieves excellent throughput.

This mindset shift has concrete implications for system design [@gujarati2020serving]. The metrics that matter change from aggregate throughput to latency distributions. Mean latency tells you little about user experience; p50, p95, and p99 latencies reveal how the system performs across the full range of requests. If your mean latency is 50ms but p99 is 2 seconds, one in a hundred users waits 40 times longer than average. For consumer-facing applications, these tail latencies often determine user satisfaction and retention.[^fn-tail-latency]

[^fn-tail-latency]: **Tail Latency Impact**: Research at Google and Amazon in the mid-2000s established that users are more sensitive to latency variance than mean latency. Industry experience suggests that latency increases of 100ms can measurably impact user engagement and conversion rates for e-commerce applications, though the magnitude varies by context. This is why service level objectives (SLOs) typically specify percentile targets rather than averages.

::: {.callout-definition title="Latency Budget"}

**Latency Budget** refers to the maximum time allowed for a serving request to complete, decomposed into allocations for _preprocessing_, _inference_, and _postprocessing_ phases. Effective latency budgeting requires understanding where time is consumed and allocating resources accordingly.

:::

Every serving request decomposes into three phases that each consume part of the latency budget. Preprocessing transforms raw input such as image bytes or text strings into model-ready tensors. Inference executes the model computation. Postprocessing transforms model outputs into user-facing responses.

A common misconception is that faster hardware automatically means faster serving. In practice, preprocessing and postprocessing often dominate total latency. Studies of production systems show preprocessing consuming 60 to 70 percent of total request time when inference runs on optimized accelerators [@nvidia2024tritontutorial]. Optimizing only the inference phase yields diminishing returns when the surrounding pipeline remains bottlenecked on CPU operations.

### Latency Distribution Analysis {#sec-serving-latency-analysis}

Understanding where time goes requires instrumenting each phase independently. Consider what happens when our ResNet-50 classifier receives a JPEG image:

::: {.callout-note title="ResNet-50: Latency Budget Breakdown"}

A typical serving request for our ResNet-50 classifier shows the following latency distribution:

+--------------------+----------------------------+------------+----------------+
| **Phase**          | **Operation**              | **Time**   | **Percentage** |
+:===================+:===========================+===========:+===============:+
| **Preprocessing**  | JPEG decode                | 3.0ms      | 30%            |
| **Preprocessing**  | Resize to 224×224          | 1.0ms      | 10%            |
| **Preprocessing**  | Normalize (mean/std)       | 0.5ms      | 5%             |
| **Data Transfer**  | CPU→GPU copy               | 0.5ms      | 5%             |
| **Inference**      | **ResNet-50 forward pass** | **5.0ms**  | **50%**        |
| **Postprocessing** | Softmax + top-5            | 0.1ms      | ~0%            |
| **Total**          |                            | **10.1ms** | **100%**       |
+--------------------+----------------------------+------------+----------------+

Key insight: **Preprocessing consumes 45% of latency** despite model inference being the computationally intensive phase. With TensorRT optimization reducing inference to 2ms, preprocessing would dominate at 75%.

:::

::: {.callout-note title="Systems Perspective: The Efficiency of DSAs for Serving"}

David Patterson famously argued that general-purpose CPUs are inefficient for ML because they dedicate significant silicon area to complex logic (branch prediction, out-of-order execution) that is unnecessary for the regular, data-parallel patterns of neural networks.

For serving, this efficiency gap is magnified. A CPU executing an inference request at batch size 1 might achieve only 1--2% of its theoretical peak performance because the "killer microseconds" of instruction fetch and decode dominate the few arithmetic operations performed per token or pixel.

**Domain-Specific Architectures (DSAs)**, like Google’s TPU or NVIDIA’s Tensor Cores, solve this by replacing complex instruction logic with massive arrays of simple Multiply-Accumulate (MAC) units. This specialization allows a DSA to achieve 10--100$\times$ higher **Arithmetic Intensity**—the ratio of compute to memory access—even at the small batch sizes required for low-latency serving. Understanding this architectural advantage explains why hardware acceleration is not just a speedup, but a requirement for economically viable serving.

:::

This breakdown reveals why naive optimization efforts often fail. Engineers focus on model optimization (quantization, pruning) because that is where ML expertise applies, but the actual bottleneck is image decoding running on CPU.

::: {.callout-note title="Systems Perspective: Optimizing the Whole System"}
**Amdahl's Law at Work**: In the ResNet-50 breakdown above, preprocessing (3ms) and data transfer (0.5ms) consume 35% of the total latency. If we spend months optimizing the model to be 10× faster (reducing it from 5ms to 0.5ms), the end-to-end latency only drops from 10.1ms to 5.6ms—a disappointing **1.8× speedup**. As Dave Patterson often argues, "optimizing the common case" is essential, but Amdahl's Law reminds us that the unoptimized parts of the system will eventually dominate. Holistic serving optimization requires looking past the model to the "killer microseconds" spent in JSON parsing, JPEG decoding, and memory copies.
:::

Moving preprocessing to GPU can reduce total latency by 6x in some pipelines by eliminating CPU-GPU data transfers between stages [@nvidia2024tritontutorial]. Amdahl's Law formalizes this principle: if preprocessing consumes 45% of latency, then even infinitely fast inference can yield at most a \(1/0.45 \approx 2.22\times\) end-to-end speedup. Effective optimization targets the largest time consumers first.

**The Killer Microseconds Problem**

Barroso, Patterson, and colleagues identified a critical gap in how systems handle latency at different time scales [@barroso2017attack]. Modern systems efficiently handle nanosecond-scale events (CPU cache access, DRAM reads) through hardware mechanisms like out-of-order execution, and millisecond-scale events (disk I/O, network calls) through software techniques like threading and asynchronous I/O. But microsecond-scale events fall into an uncomfortable middle ground where neither approach works well.

ML serving lives squarely in this microsecond regime. Individual inference calls complete in 1 to 10ms, but the surrounding operations such as serialization, memory allocation, network stack processing, and encryption each add microseconds that compound into significant overhead. Google's analysis found that a significant fraction, often 20 percent or more, of datacenter CPU cycles are consumed by this "datacenter tax" rather than useful computation. For serving systems, this means a 2μs network fabric can become 100μs end-to-end through software overhead, context switching costs of 5 to 10μs can exceed the inference time for small models, and memory allocation patterns in preprocessing can add unpredictable microsecond delays. These overheads explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization through memory pooling, zero-copy data paths, and kernel bypass matters as much as model optimization.

The latency budget framework provides a systematic approach to optimization. First, measure each phase to identify the true bottleneck. Then allocate engineering effort proportionally to where time is actually spent. Finally, consider architectural changes such as GPU preprocessing or batching strategies that can shift work between phases.

### Resolution and Input Size Tradeoffs {#sec-serving-resolution}

Input resolution dramatically affects both preprocessing and inference latency, but the relationship differs depending on whether the system is compute-bound or memory-bound. @sec-ai-acceleration covers this distinction in depth through roofline model analysis; understanding it is essential for making informed resolution decisions.

For compute-bound models, @eq-resolution-throughput formalizes how throughput scales inversely with resolution squared:

$$\frac{T(r_2)}{T(r_1)} = \left(\frac{r_1}{r_2}\right)^2$$ {#eq-resolution-throughput}

Doubling resolution from 224 to 448 theoretically yields 4x slowdown (measured: 3.6x due to fixed overhead amortization). However, at high resolutions, models transition from compute-bound to memory-bound as activation tensors exceed cache capacity. @tbl-resolution-bottleneck quantifies this transition for ResNet-50, showing how arithmetic intensity decreases with resolution:

+----------------+---------------------+----------------------+----------------+
| **Resolution** | **Activation Size** | **Arith. Intensity** | **Bottleneck** |
+===============:+====================:+=====================:+:===============+
| **224×224**    | 12.5MB              | 85 FLOPS/byte        | Compute        |
| **384×384**    | 36.8MB              | 49 FLOPS/byte        | Transitional   |
| **512×512**    | 65.5MB              | 28 FLOPS/byte        | Memory BW      |
| **640×640**    | 102.4MB             | 18 FLOPS/byte        | Memory BW      |
+----------------+---------------------+----------------------+----------------+

: **Resolution and Compute Bottleneck**: ResNet-50 arithmetic intensity decreases with resolution as activation sizes grow. The V100 ridge point for FP32 operations is approximately 17 FLOPS/byte (15 TFLOPS compute divided by 900 GB/s bandwidth). At 224x224, compute dominates; by 512x512, memory bandwidth becomes the limiting factor. {#tbl-resolution-bottleneck}

**Deployment-Specific Resolution Decisions**

Different deployment contexts have fundamentally different resolution requirements. Mobile applications often accept lower resolution such as 224×224 for object detection in camera viewfinders, where latency and battery life dominate. Medical imaging requires high resolution of 512×512 or higher for diagnostic accuracy, with relaxed latency requirements. Autonomous vehicles use multiple resolutions for different tasks, with low resolution for detection and high resolution crops for recognition. Cloud APIs typically receive resolution set by client upload and must handle a range gracefully.

**Adaptive Resolution**

Production systems can select resolution dynamically based on content. One approach runs a lightweight classifier at 128×128 to categorize content type, then selects task-appropriate resolution with documents at 512×512, landscapes at 224×224, and faces at 384×384. This achieves 1.4× throughput improvement with 99.2 percent accuracy retention versus fixed high resolution. This pattern trades preprocessing cost from running the lightweight classifier for inference savings on the main model.

### Utilization and Request Pipelining {#sec-serving-concurrency}

The latency budget analysis in @sec-serving-latency-budget reveals that model inference is only one component of the request lifecycle. From a hardware perspective, the primary goal of a serving system is to maximize the **duty cycle** of the accelerator—the percentage of time the GPU is performing useful computation.

In a naive, serialized serving system, the hardware sits idle during network I/O and CPU-based preprocessing. High-performance serving systems use **Request Pipelining** to overlap these stages, ensuring the GPU is fed a continuous stream of tensors.

### Overlapping I/O and Compute {#sec-serving-overlap}

@fig-serving-pipeline-timing contrasts serial execution with pipelined execution. In the serial case (A), each request must complete its entire lifecycle (Network $\rightarrow$ CPU Preprocessing $\rightarrow$ GPU Inference $\rightarrow$ Postprocessing) before the next request begins. Even with a fast GPU, the system throughput is limited by the slowest stage, and the GPU remains idle for more than 50% of the time.

::: {#fig-serving-pipeline-timing fig-env="figure" fig-pos="htb" fig-cap="**Request Pipelining**: Pipelining hides latency by overlapping independent operations across different hardware resources. In pipelined execution (B), the CPU processes the next request's data while the GPU executes the current request's inference. This increases the GPU duty cycle toward 100%, effectively doubling or tripling throughput on the same hardware without changing the model."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  \definecolor{CPUColor}{RGB}{173,216,230}
  \definecolor{GPUColor}{RGB}{144,238,144}
  \definecolor{WaitColor}{RGB}{240,240,240}

  % Serial Execution
  \node[anchor=west] at (0, 3.5) {\textbf{A. Serial Execution} (Low Utilization)};
  \draw[fill=CPUColor] (0, 2.5) rectangle (1.5, 3) node[midway] {Pre};
  \draw[fill=GPUColor] (1.5, 2.5) rectangle (3.0, 3) node[midway] {GPU};
  \draw[fill=WaitColor] (3.0, 2.5) rectangle (4.5, 3) node[midway, text=gray] {Idle};
  \draw[fill=CPUColor] (4.5, 2.5) rectangle (6.0, 3) node[midway] {Pre};
  \draw[fill=GPUColor] (6.0, 2.5) rectangle (7.5, 3) node[midway] {GPU};

  % Overlapped Execution
  \node[anchor=west] at (0, 1.5) {\textbf{B. Pipelined Execution} (High Utilization)};
  % CPU Row
  \draw[fill=CPUColor] (0, 0.5) rectangle (1.5, 1) node[midway] {Pre 1};
  \draw[fill=CPUColor] (1.5, 0.5) rectangle (3.0, 1) node[midway] {Pre 2};
  \draw[fill=CPUColor] (3.0, 0.5) rectangle (4.5, 1) node[midway] {Pre 3};
  \draw[fill=CPUColor] (4.5, 0.5) rectangle (6.0, 1) node[midway] {Pre 4};

  % GPU Row
  \draw[fill=GPUColor] (1.5, 0) rectangle (3.0, 0.5) node[midway] {GPU 1};
  \draw[fill=GPUColor] (3.0, 0) rectangle (4.5, 0.5) node[midway] {GPU 2};
  \draw[fill=GPUColor] (4.5, 0) rectangle (6.0, 0.5) node[midway] {GPU 3};
  \draw[fill=GPUColor] (6.0, 0) rectangle (7.5, 0.5) node[midway] {GPU 4};

\end{tikzpicture}
```
:::

Pipelining is enabled by **Asynchronous I/O** and **Concurrency Models**. Instead of waiting for a GPU kernel to finish, the server's CPU thread submits the work to the GPU's command queue and immediately begins preprocessing the next incoming request.

### The Systems Metric: Hardware Duty Cycle

In the "Quantitative Approach" to ML systems, we define the efficiency of a serving system by its ability to saturate the bottleneck resource. For most ML systems, this is the GPU's compute cores or memory bandwidth.

$$\text{System Efficiency} = \frac{\sum T_{\text{compute}}}{\text{Wall Clock Time} \times \text{Resource Count}}$$

If a ResNet-50 request takes 10ms total (5ms GPU, 5ms CPU), a serial system achieves only 50% efficiency. By pipelining just two requests, efficiency approaches 100% (assuming the CPU can keep up with the GPU). If the CPU is too slow to feed the GPU, the system becomes **CPU-bound**, and further model optimization provides zero throughput gain—a direct application of Amdahl's Law to serving.

### Interface Protocols and Serialization {#sec-serving-protocols}

The mechanism used to transport data between client and server significantly impacts the latency budget. While model inference is often highly optimized, the cost of moving data into the model—serialization and network protocol overhead—can become the dominant bottleneck, especially for lightweight models where inference time is small. This "serialization tax" is a primary contributor to the "killer microseconds" problem identified in the previous section.

### The Serialization Bottleneck {#sec-serving-serialization}

Text-based formats like JSON are ubiquitous but computationally expensive. Parsing a JSON object requires reading every byte, validating syntax, and converting text representations into machine-native types. For high-throughput systems, this consumes CPU cycles that could otherwise be used for request handling or preprocessing.

Binary formats like Protocol Buffers (Protobuf) or FlatBuffers reduce this overhead by designing the wire format to map directly to in-memory data structures. This enables "zero-copy" deserialization in optimal cases, where the network buffer can be used directly without allocating new memory.

### REST vs gRPC {#sec-serving-rest-grpc}

Two dominant paradigms define modern serving interfaces, each with distinct system characteristics:

**REST (Representational State Transfer)** typically uses HTTP/1.1 and JSON. It is universally supported, human-readable, and stateless, making it the default choice for public-facing APIs. However, standard HTTP/1.1 requires a new TCP handshake for each request (unless keep-alive is carefully tuned), and JSON serialization adds significant latency for numerical data like tensors.

**gRPC (gRPC Remote Procedure Call)** uses HTTP/2 and Protobuf. HTTP/2 enables multiplexing multiple requests over a single persistent TCP connection, eliminating handshake latency and allowing efficient binary streaming. Protobuf provides strict type safety and efficient binary serialization, making it the standard for internal service-to-service communication where latency is critical.

::: {.callout-note title="Benchmarks: JSON vs Protobuf Serialization"}

Consider a request payload containing 1,000 floating point numbers (e.g., an embedding vector).

*   **JSON**: Uses ~9 KB on the wire. Requires ~50$\mu$s to parse.
*   **Protobuf**: Uses ~4 KB on the wire. Requires ~5$\mu$s to parse.

For a system processing 10,000 requests per second, switching to Protobuf saves nearly half a core of CPU time just in serialization overhead. This 10$\times$ efficiency gain makes gRPC essential for high-throughput internal microservices.

:::

**System Choice**: Use REST for public APIs to maximize developer accessibility. Use gRPC for high-performance internal communication to minimize the serialization tax.

### Queuing Fundamentals {#sec-serving-queuing}

The latency budget framework explains where time goes within a single request, but production systems must handle many concurrent requests competing for finite resources. Understanding why latency degrades under load requires queuing theory, the mathematical framework that explains how requests wait for service in any system with finite capacity. These principles apply universally, from web servers to ML inference, and explain the counterintuitive behavior that causes well-provisioned systems to suddenly violate latency SLOs when load increases modestly.

### Little's Law {#sec-serving-littles-law}

The most fundamental result in queuing theory is Little's Law, which @eq-littles-law expresses as a simple relationship between three quantities in any stable system:

$$L = \lambda \cdot W$$ {#eq-littles-law}

where $L$ is the average number of requests in the system, $\lambda$ is the arrival rate (requests per second), and $W$ is the average time each request spends in the system. This relationship holds regardless of arrival distribution, service time distribution, or scheduling policy.

Little's Law has immediate practical implications. If your inference service averages 10ms per request ($W = 0.01$s) and you observe 50 concurrent requests in the system on average ($L = 50$), then your arrival rate must be $\lambda = L/W = 5000$ requests per second. Conversely, if you need to limit concurrent requests to 10 (perhaps due to GPU memory constraints), and your service time is 10ms, you can sustain at most 1000 requests per second.

### The Utilization-Latency Relationship {#sec-serving-utilization-latency}

For a system with Poisson arrivals and exponential service times (the M/M/1 queue model), the average time in system follows:

$$W = \frac{1}{\mu - \lambda} = \frac{\text{service time}}{1 - \rho}$$ {#eq-mm1-wait}

where $\mu$ is the service rate (requests per second the server can handle), and $\rho = \lambda/\mu$ is the utilization (fraction of time the server is busy).

This equation reveals why serving systems exhibit nonlinear behavior. At 50% utilization, average wait time is $2\times$ the service time. At 80% utilization, it is $5\times$. At 90% utilization, it is $10\times$. Small increases in load near capacity cause disproportionate latency increases.

+--------------------------+------------------------+---------------------------+
| **Utilization ($\rho$)** | **Wait Time Multiple** | **Example (5ms service)** |
+=========================:+=======================:+==========================:+
| 50%                      | 2.0×                   | 10ms                      |
| 70%                      | 3.3×                   | 17ms                      |
| 80%                      | 5.0×                   | 25ms                      |
| 90%                      | 10.0×                  | 50ms                      |
| 95%                      | 20.0×                  | 100ms                     |
+--------------------------+------------------------+---------------------------+

: **Utilization-Latency Relationship**: Average wait time as a multiple of service time for an M/M/1 queue. At 50% utilization, wait time is 2x service time; at 90%, it reaches 10x. This nonlinear growth explains why systems that perform well at moderate load suddenly violate SLOs when traffic increases: moving from 80% to 90% utilization doubles wait time. {#tbl-utilization-latency}

The M/M/1 model assumes exponentially distributed service times, but ML inference typically has near-constant service time for fixed batch sizes, making the M/D/1 (deterministic service) model more accurate in practice. We use M/M/1 here because it yields closed-form solutions and produces conservative estimates. @tbl-utilization-latency reveals how average wait time grows rapidly as utilization approaches 100%. For M/D/1 queues, average wait time is approximately half of M/M/1 at the same utilization, which matters for capacity planning: M/M/1 analysis will slightly over-provision, erring on the side of meeting SLOs rather than violating them.[^fn-queuing-models]

[^fn-queuing-models]: **Queuing Model Assumptions**: The M/M/1 model assumes Poisson arrivals and exponential service times. ML inference typically has near-constant service time for fixed batch sizes, making M/D/1 (deterministic service) more accurate. We use M/M/1 because it produces conservative estimates and closed-form solutions, erring on the side of meeting SLOs. For deeper treatment including multi-server models, see Harchol-Balter's *Performance Modeling and Design of Computer Systems* [@harchol2013performance].

### Multi-Server Considerations {#sec-serving-multi-server}

While this chapter focuses on single-machine capacity planning using M/M/1 analysis, production systems at scale deploy multiple replicas. The M/M/c queuing model shows that multiple servers dramatically improve tail latency: the probability of all servers being simultaneously slow drops exponentially with server count.

**When M/M/1 Applies**: M/M/1 analysis remains valuable for single-machine sizing, worst-case analysis, and understanding when scaling to multiple machines becomes necessary. The capacity planning approach in the worked example below uses M/M/1 to determine when a single machine meets SLOs.

Multi-server queuing dynamics become essential when scaling to distributed inference systems, where they inform replica count, load balancing, and fault tolerance decisions.

### Tail Latency {#sec-serving-tail-latency}

Production SLOs typically specify percentile targets (p95, p99) rather than averages because tail latency determines user experience for the slowest requests [@dean2013tail]. For an M/M/1 queue, the p99 latency follows:

$$W_{p99} \approx \text{service time} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right)$$ {#eq-p99-latency}

At 70 percent utilization, p99 latency is approximately fifteen times the service time, while average latency is only 3.3 times. This explains why systems that seem healthy with low average latency can have unacceptable tail latency, since the average hides the experience of the unluckiest requests.

**The Tail at Scale Problem**

Dean and Barroso's foundational analysis reveals why tail latency becomes critical as systems scale beyond single machines [@dean2013tail]. The key insight is that when requests fan out to multiple servers, the probability of experiencing at least one slow response grows rapidly with server count. This "tail at scale" effect makes individual server tail latency critical for overall system performance.

For single-machine serving, this principle has two implications. First, tail latency on individual machines matters because it will compound when systems eventually scale. Second, the tail-tolerant techniques described below (hedging, graceful degradation) provide value even on single machines and become essential at scale.

**Tail-Tolerant Techniques**: Request hedging sends redundant requests after a timeout, accepting whichever response arrives first. Backup requests and load balancing away from slow servers directly address latency variance. These techniques apply to single-machine serving with multiple GPU streams or model replicas, and become essential when scaling to distributed inference systems.

::: {.callout-notebook title="Worked Example: ResNet-50 Capacity Planning" collapse="true"}

Consider designing a ResNet-50 serving system with these requirements:

- **Target p99 latency**: 50ms
- **Peak expected traffic**: 5,000 requests per second
- **Service time** (TensorRT FP16): 5ms

**Step 1: Find safe utilization**

Applying @eq-p99-latency to constrain $W_{p99} \leq 50$ms with 5ms service time and solving for $\rho$:

$$5\text{ms} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right) \leq 50\text{ms}$$

This yields $\rho \leq 0.72$ (72% maximum utilization).

**Step 2: Calculate required service rate**

$$\mu_{\text{required}} = \frac{\lambda}{\rho_{\text{safe}}} = \frac{5000}{0.72} = 6944 \text{ requests/second}$$

**Step 3: Determine GPU count**

Single V100 throughput at batch=16: 1,143 images/second

$$\text{GPUs needed} = \frac{6944}{1143} = 6.1 \rightarrow 7 \text{ GPUs}$$

**Step 4: Add headroom for variance**

Production systems add 30% headroom for traffic spikes and variance:

$$\text{Final count} = 7 \times 1.3 = 9.1 \rightarrow 10 \text{ GPUs}$$

**Step 5: Verify fault tolerance (N+1 redundancy)**

The 30% headroom addresses traffic variance, but production systems also need fault tolerance. With 10 GPUs, losing one leaves 9 GPUs handling 5,000 QPS:

$$\text{Utilization after failure} = \frac{5000 / 1143}{9} = 48.6\%$$

This remains well below the 72% safe utilization threshold, confirming N+1 redundancy is satisfied. For stricter fault tolerance requirements, N+2 redundancy (tolerating two simultaneous failures) would require 11-12 GPUs.

**Result**: Provision 10 V100 GPUs to serve 5,000 QPS at 50ms p99 latency with N+1 fault tolerance.

:::

The queuing analysis explains the capacity planning approach detailed in @sec-serving-capacity-planning and connects directly to the MLPerf Server scenario. @sec-benchmarking-ai explains how MLPerf measures throughput only for requests meeting the latency SLO: a system achieving 10,000 QPS but violating the SLO on 5% of requests reports only 9,500 valid QPS.

### Tail-Tolerant Techniques {#sec-serving-tail-tolerant}

Rather than eliminating all sources of latency variability (often impractical), production systems employ techniques that tolerate variability while still meeting SLOs [@dean2013tail; @dean2012rapid]. These techniques treat latency variance as a fact of life and design around it.

**Hedged Requests**

When a request has not completed within the expected time, send a duplicate request to another server. The client uses whichever response arrives first and cancels the other. For ML serving, this means maintaining multiple model replicas and routing slow requests to alternative replicas. The overhead is modest: if you hedge at the 95th percentile, only 5% of requests generate duplicates, increasing load by just 5% while dramatically reducing tail latency.

**Cancellation Complexity**: A critical implementation detail is that CUDA kernels cannot be interrupted mid-execution. When a hedged request completes, the duplicate must be cancelled, but if inference has already begun on the GPU, cancellation approaches include checking a cancellation flag before launching inference, accepting wasted compute for the in-flight kernel, or using request prioritization to deprioritize the duplicate. Since hedging typically applies only to the slowest 5 percent of requests, the overhead from occasional wasted compute remains acceptable.

**Tied Requests**

Send the request to multiple servers simultaneously, but include a tag allowing servers to cancel execution once another server begins processing. This eliminates the delay of waiting to detect a slow response before hedging. For inference servers with significant startup overhead from model loading and memory allocation, tied requests ensure at least one server begins immediately.

**Canary Requests**

For requests that fan out to many backends, first send the request to a small subset of 1 to 2 servers. If these return within expected time, send to the remainder. If the canary is slow, the system can take corrective action by retrying elsewhere or using cached results before committing to the full fan-out. This prevents a single slow backend from stalling an entire distributed inference request.

**Graceful Degradation**

When load exceeds capacity, return approximate results rather than timing out. For classification, return cached predictions for similar inputs. For generative models, return shorter outputs. For ensemble systems, return predictions from a subset of models. This maintains responsiveness at the cost of some accuracy, which users often prefer to outright failures.

**Admission Control**

When traffic exceeds capacity, accepting all requests can trigger widespread SLO violations. Admission control proactively rejects requests when queue depth exceeds a threshold, returning immediate 503 responses rather than accepting requests that are likely to timeout. This sacrifices throughput to protect latency for admitted requests.

**Setting the Threshold**: A practical starting point is 2 to 3 times service time multiplied by the number of workers. For a system with 4 workers and 10ms service time, this yields a queue depth threshold of 80 to 120 requests. Adaptive admission control adjusts thresholds based on observed p99 latency, tightening when latency increases above target and relaxing when latency remains healthy.

**Retry Storm Prevention**: A subtle failure mode occurs when all replicas are overloaded simultaneously. If the load balancer retries rejected requests at other replicas that are also overloaded, retry traffic amplifies the overload. Coordinated load shedding addresses this by sharing load information across replicas, enabling system-wide decisions about which requests to accept. When global load exceeds capacity, replicas collectively reject the same fraction of requests rather than each rejecting independently and triggering retries.

These techniques become essential at scale when fan-out amplification makes individual server tail latency visible to users. Single-machine serving systems can implement hedged and tied requests across GPU streams or model replicas. The queuing analysis here assumes FIFO processing, but production systems often implement priority scheduling such as deadline-aware or shortest-job-first approaches to further reduce tail latency for heterogeneous workloads [@harchol2013performance].

## Model Lifecycle Management {#sec-serving-lifecycle}

### Training-Serving Skew {#sec-serving-skew}

The queuing theory and latency analysis from previous sections assume the model produces correct predictions. However, a critical problem in production ML systems threatens this assumption.

::: {.callout-definition title="Training-Serving Skew"}

**Training-Serving Skew** occurs when a model's performance in production degrades relative to its _training validation metrics_, typically caused by _discrepancies_ between the training and serving data pipelines or environments.

:::

While @sec-ml-operations provides a comprehensive analysis of skew diagnosis and prevention, serving systems must specifically guard against *preprocessing divergence*. This occurs when the real-time inference pipeline processes raw data differently than the batch training pipeline—a common failure mode when training uses Python/Pandas while serving uses C++/Java or optimized inference servers.

::: {.callout-note title="ResNet-50: Image Preprocessing Skew"}

For ResNet-50 serving, common sources of skew include:

**Resize interpolation**: Training uses PIL.BILINEAR while OpenCV defaults to cv2.INTER_LINEAR. These produce pixel-level differences that can shift accuracy by 0.5-1%.

**Color space handling**: JPEG loading in different libraries may produce BGR vs RGB ordering. If the model trained on RGB but serves BGR inputs, predictions are essentially random.

**Normalization constants**: ImageNet normalization uses specific mean/std values. Using `mean=[0.5, 0.5, 0.5]` instead of `mean=[0.485, 0.456, 0.406]` shifts inputs out of the training distribution.

**Prevention**: The safest approach is to export the exact preprocessing code used during training and run it identically in serving, or use a framework like NVIDIA DALI that can help standardize preprocessing across training and serving environments.

:::

### Model Loading and Initialization {#sec-serving-model-loading}

With preprocessing pipelines designed to avoid training-serving skew, the next challenge is getting the model ready to serve. Before processing any request, models must load from storage into memory and prepare for inference [@romero2021infaas].

::: {.callout-definition title="Cold Start"}

**Cold Start** refers to the latency penalty incurred by the first request(s) processed by a new model instance, caused by _initialization overheads_ such as _weight loading_, _runtime compilation_, and _memory allocation_.

:::

Understanding cold start dynamics is essential for designing systems that meet latency requirements from the moment they begin serving traffic.

### Cold Start Anatomy {#sec-serving-cold-start}

Cold start latency compounds from multiple sources, each adding to the time between deployment and serving readiness. Weight loading reads model parameters from disk or network storage. Graph compilation performs just-in-time compilation of operations for the specific hardware. Memory allocation reserves GPU memory for activations and intermediate values. Warmup execution performs initial inferences that populate caches and trigger lazy initialization.

::: {.callout-note title="ResNet-50: Cold Start Timeline"}

Loading ResNet-50 for production serving involves the following cold start phases:

+---------------------------------+--------------+---------------------------------------------------+
| **Phase**                       | **Duration** | **Notes**                                         |
+:================================+=============:+:==================================================+
| **Weight loading (SSD)**        | 0.5s         | 98MB FP32 weights from local storage              |
| **Weight loading (S3)**         | 3-5s         | Network latency dominates for cloud storage       |
| **CUDA context**                | 0.3-0.5s     | CUDA 11+ lazy loading significantly reduced this  |
| **TensorRT compilation**        | 15-30s       | Converts PyTorch model to optimized engine        |
| **Warmup (10 inferences)**      | 0.2s         | Triggers remaining lazy initialization            |
| **Total (local, optimized)**    | **~1.5s**    | With pre-compiled TensorRT engine, warm container |
| **Total (cloud, first deploy)** | **~35s**     | Including compilation from cold state             |
+---------------------------------+--------------+---------------------------------------------------+

**Key insight**: Pre-compiling models and storing the optimized engine eliminates the 30-second compilation phase on subsequent deployments. Production systems use CUDA MPS (Multi-Process Service) or pre-warmed container pools to amortize CUDA initialization costs across requests.

:::

Without warmup, the first real request triggers compilation and memory allocation mid-inference, often causing timeout failures. A request that normally takes 5ms might require 500ms during cold start, violating SLOs and degrading user experience.

### Loading Strategies {#sec-serving-loading-strategies}

Different loading strategies trade off cold start duration against serving performance and memory efficiency.

**Full loading** reads the entire model into memory before serving begins. This maximizes inference speed since all weights are immediately available, but extends cold start duration and limits model size to available memory. Full loading is appropriate when cold start latency is acceptable and models comfortably fit in memory.

**Memory mapping** maps model files directly into the address space, loading pages on demand as accessed. This reduces cold start time since inference can begin before the full model loads, but causes unpredictable latency as pages fault in during initial requests. Memory mapping works well for infrequently accessed model components but can cause latency spikes if critical weights are not preloaded.

**Lazy initialization** defers compilation and allocation until first use. This minimizes startup time but shifts latency to the first request. Production systems often combine lazy initialization with synthetic warmup requests to trigger initialization before real traffic arrives.

### Model Caching Infrastructure {#sec-serving-model-caching}

Production systems cache model weights at the infrastructure level to reduce cold start for common deployment scenarios:

**Container Image Embedding**: Bundle model weights directly in the container image. This produces a single deployment artifact and eliminates network fetches at startup, but creates large images (often 10-50GB) that slow container pulls and consume registry storage. Best for models that rarely update.

**Shared Filesystem**: Mount a network filesystem (EFS, GCS FUSE) containing model weights. Multiple replicas share cached weights, and updates propagate immediately without redeployment. Network latency affects cold start, and filesystem availability becomes a critical dependency. Best for organizations with many models and frequent updates.

**Node-Local SSD Cache**: Pre-populate local SSDs on inference nodes with frequently-used models. Provides fast loading (500MB/s+ for NVMe) without network dependency, but requires cache management to handle model updates and capacity limits. Best for high-traffic models where cold start latency is critical.

The choice depends on model update frequency: infrequent updates favor container embedding, frequent updates favor shared filesystem, and performance-critical deployments benefit from local caching with background refresh.

### Multi-Model Serving {#sec-serving-multi-model}

Production systems often serve multiple models from a single machine, whether different model versions for A/B testing, ensemble components, or entirely different models sharing infrastructure. GPU memory becomes the limiting resource, requiring careful management strategies.

Strategies for multi-model serving include time-multiplexing that loads one model at a time and swaps based on request routing, memory sharing where models share GPU memory to limit concurrent execution but enable more models, and model virtualization where frameworks like Triton manage model lifecycle by loading and unloading based on traffic patterns [@nvidia2024triton]. The choice depends on request patterns. If models receive traffic evenly, concurrent loading works. If traffic is bursty and model-specific, time-multiplexing with intelligent preloading reduces average latency while maximizing GPU utilization.

**Multi-Stream Execution**

When multiple models or multiple instances of the same model must run concurrently on a single GPU, the hardware must partition resources between them. NVIDIA's Multi-Instance GPU technology enables hardware-level isolation, dividing an A100 into up to 7 independent GPU instances, each with dedicated memory and compute resources. MIG is available on A100, A30, H100, and newer data center GPUs. For older GPUs such as V100 or T4, CUDA stream scheduling provides time-multiplexed sharing without hardware isolation.

The choice depends on whether consistent latency with MIG or maximum utilization with shared streams is the priority.

**Model Swapping and Host Memory**

When the aggregate size of all models exceeds GPU memory capacity, the serving system must swap models between host memory (DRAM) and device memory (VRAM) on demand. This introduces a new latency component determined by the PCIe bus bandwidth.

For a 10 GB model on PCIe Gen4 x16 (32 GB/s theoretical bandwidth), loading takes at least:
$$ T_{\text{load}} = \frac{10 \text{ GB}}{32 \text{ GB/s}} \approx 312 \text{ ms} $$

To mitigate this, systems use **Pinned Memory** (page-locked host memory). Pinning prevents the OS from paging out model weights, enabling the GPU's DMA engine to transfer data at maximum physical bandwidth without CPU involvement, often improving transfer speeds by 2-3$\times$ compared to pageable memory.

## Throughput Optimization {#sec-serving-throughput}

Once models are loaded and ready to serve, the next optimization opportunity lies in how requests are grouped for processing. Batching differs fundamentally between training and serving [@crankshaw2017clipper]. Training batches maximize throughput, processing hundreds or thousands of samples together with no concern for individual sample latency. Serving batches must balance throughput against individual request latency, typically processing single digits of requests together while ensuring no request waits too long.

::: {.callout-definition title="Dynamic Batching"}

**Dynamic Batching** refers to a serving strategy that collects incoming requests within a _time window_ and processes them together, trading individual request latency for improved _throughput_ and _hardware utilization_. The window size and maximum batch size parameters control this tradeoff.

:::

### Why Batching Helps {#sec-serving-batching-why}

Modern accelerators achieve peak efficiency only at sufficient batch sizes [@shen2019nexus]. A single inference request leaves most compute units idle because GPUs are designed for parallel execution across thousands of threads. Batching amortizes fixed costs (kernel launch overhead, weight loading from memory) across multiple requests and enables parallel execution across the batch dimension.

::: {.callout-notebook title="Worked Example: ResNet-50 Batching Efficiency" collapse="true"}

The throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates the power of batching:

+----------------+---------------------+-----------------------+----------------+---------------+
| **Batch Size** | **Inference Time*** | **Per-Image Compute** | **Throughput** | **GPU Util.** |
+===============:+====================:+======================:+===============:+==============:+
| 1              | 5.0ms               | 5.0ms                 | 200 img/s      | 15%           |
| 4              | 7.2ms               | 1.8ms                 | 556 img/s      | 42%           |
| 8              | 9.1ms               | 1.1ms                 | 879 img/s      | 65%           |
| 16             | 14.0ms              | 0.9ms                 | 1,143 img/s    | 85%           |
| 32             | 25.0ms              | 0.8ms                 | 1,280 img/s    | 95%           |
+----------------+---------------------+-----------------------+----------------+---------------+

*Times shown are pure inference time, excluding queue wait. @sec-serving-traffic-patterns analyzes how user-perceived latency includes batching window wait.

**Key insight**: Batch size 32 achieves 6.4× higher throughput than batch size 1. However, user-perceived latency includes both queue wait and inference time. With a 10ms batching window and 25ms inference, total latency reaches 35ms versus 5ms at batch size 1.

:::

However, batching forces requests to wait. A request arriving just after a batch closes must wait for the current batch to complete plus full processing of its own batch. This waiting time directly adds to user-perceived latency, creating the fundamental tradeoff that serving system designers navigate.

### Static vs Dynamic Batching {#sec-serving-batching-types}

**Static batching** waits for a fixed batch size before processing. Simple to implement but problematic in practice: during low traffic, requests wait indefinitely for a full batch. During high traffic, large batches increase per-request latency.

**Dynamic batching** collects requests within a time window, processing whatever has arrived when the window closes [@olston2017tensorflow]. This bounds maximum wait time regardless of traffic level. The window size represents a direct tradeoff: shorter windows reduce latency but sacrifice throughput; longer windows improve throughput but increase latency.

Typical configurations use windows of 5-50ms with maximum batch sizes of 8-32 for latency-sensitive applications. The optimal configuration depends on request arrival patterns, model characteristics, and latency requirements.

### Dynamic Batching Latency-Throughput Trade-offs {#sec-serving-batching-tradeoffs}

Dynamic batching introduces a fundamental tension between throughput optimization and latency constraints. Understanding this tradeoff quantitatively enables principled configuration decisions rather than trial-and-error tuning. @eq-batching-latency decomposes the total user-perceived latency for a batched request into two components:

$$L_{\text{total}} = L_{\text{wait}} + L_{\text{compute}}(b)$$ {#eq-batching-latency}

where $L_{\text{wait}}$ is the time spent waiting in the batching queue and $L_{\text{compute}}(b)$ is the inference time for batch size $b$. The batching window $T$ bounds wait time ($L_{\text{wait}} \leq T$), while batch size affects compute time through GPU utilization characteristics.

**Queue Waiting Time Analysis**

For Poisson arrivals with rate $\lambda$ and batching window $T$, requests arrive uniformly within the window. A request arriving at time $t$ within the window waits $T - t$ for the batch to close. @eq-avg-wait shows that the average wait time is simply half the window:

$$E[L_{\text{wait}}] = \frac{T}{2}$$ {#eq-avg-wait}

This simple relationship has profound implications. A 20ms batching window adds 10ms average latency regardless of batch size achieved. If your latency SLO is 50ms and inference takes 5ms, the batching window consumes 20% of your latency budget before any computation begins.

**Batch Size Distribution**

The number of requests collected during window $T$ follows a Poisson distribution with mean $\lambda T$. @eq-batch-distribution formalizes this relationship:

$$P(\text{batch size} = k) = \frac{(\lambda T)^k e^{-\lambda T}}{k!}$$ {#eq-batch-distribution}

@tbl-batch-variability quantifies this variability, showing how batch size fluctuates for different traffic levels with a fixed 10ms window:

+------------------+----------------+-------------+----------------+---------------------+
| **Arrival Rate** | **Mean Batch** | **Std Dev** | **P(batch=0)** | **P(batch≥2×mean)** |
+=================:+===============:+============:+===============:+====================:+
| **50 QPS**       | 0.5            | 0.7         | 61%            | 1%                  |
| **200 QPS**      | 2.0            | 1.4         | 14%            | 9%                  |
| **500 QPS**      | 5.0            | 2.2         | 0.7%           | 12%                 |
| **1000 QPS**     | 10.0           | 3.2         | 0.005%         | 13%                 |
+------------------+----------------+-------------+----------------+---------------------+

: **Batch Size Variability**: At low traffic, batching windows frequently contain zero requests (wasted GPU cycles). At moderate traffic, batch sizes fluctuate significantly around the mean. High traffic provides more stable batching but still sees 13% of batches exceeding twice the mean size. {#tbl-batch-variability}

**Throughput Maximization Strategy**

Throughput optimization requires maximizing the number of requests processed per unit time. For a system with service time $S(b)$ for batch size $b$, throughput follows @eq-batch-throughput:

$$\text{Throughput}(b) = \frac{b}{T + S(b)}$$ {#eq-batch-throughput}

The numerator increases linearly with batch size while the denominator increases sub-linearly (due to GPU parallelism). This creates an optimal batch size that balances these competing effects.

For ResNet-50 on a V100 GPU, service time scales as $S(b) = 5\text{ms} + 0.6b$ (5ms fixed overhead plus 0.6ms per additional image in the batch). With $T = 10$ms batching window:

+----------------+------------------+-------------------+----------------+----------------+
| **Batch Size** | **Service Time** | **Total Latency** | **Throughput** | **Efficiency** |
+===============:+=================:+==================:+===============:+:===============+
| 1              | 5.6ms            | 15.6ms            | 64 img/s       | Low            |
| 4              | 7.4ms            | 17.4ms            | 230 img/s      | Moderate       |
| 8              | 9.8ms            | 19.8ms            | 404 img/s      | Good           |
| 16             | 14.6ms           | 24.6ms            | 650 img/s      | High           |
| 32             | 24.2ms           | 34.2ms            | 935 img/s      | Maximum        |
+----------------+------------------+-------------------+----------------+----------------+

: **Batching Throughput Analysis**: ResNet-50 throughput on V100 with 10ms batching window. Throughput increases 14.6x from batch size 1 to 32 (64 to 935 img/s), but total latency more than doubles (15.6ms to 34.2ms). The optimal configuration depends on whether the latency SLO or throughput target is the binding constraint. {#tbl-batching-throughput}

**Latency-Constrained Optimization**

When latency SLOs provide the binding constraint, the optimization problem becomes finding the maximum batch size that meets the SLO. For SLO $L_{\text{SLO}}$ and average wait time $T/2$, @eq-latency-constrained-batch defines the maximum allowable batch size:

$$b_{\text{max}} = \max\{b : \frac{T}{2} + S(b) \leq L_{\text{SLO}}\}$$ {#eq-latency-constrained-batch}

Consider a 50ms p95 latency SLO for ResNet-50 serving:

**Scenario 1: Conservative window (T = 5ms)**
- Average wait: 2.5ms
- Latency budget for inference: 47.5ms
- Maximum batch size: 71 (but typically capped at 32 for memory)
- Achieved throughput: ~1,140 img/s (batch=32)

**Scenario 2: Aggressive window (T = 25ms)**
- Average wait: 12.5ms
- Latency budget for inference: 37.5ms
- Maximum batch size: 48
- Achieved throughput: ~1,280 img/s (batch=48)

The aggressive window achieves only 12% higher throughput but increases average latency by 10ms and p99 latency by 25ms. Examine @tbl-batching-throughput: for latency-sensitive applications, the conservative window provides better user experience at modest throughput cost.

**SLO Violation Analysis**

Batch size variability causes SLO violations even when mean latency appears safe. The p99 latency includes both worst-case wait time (full window) and worst-case batch size (governed by Poisson tail). @eq-p99-batch-latency captures this relationship:

$$L_{p99} \approx T + S(b_{p99})$$ {#eq-p99-batch-latency}

where $b_{p99}$ is the 99th percentile batch size. For $\lambda = 500$ QPS and $T = 10$ms:

- Mean batch size: 5
- p99 batch size: 11 (from Poisson distribution)
- Mean latency: $5\text{ms} + 9.0\text{ms} = 14\text{ms}$
- p99 latency: $10\text{ms} + 11.6\text{ms} = 21.6\text{ms}$

The p99 latency is 1.54× the mean, reflecting both wait time variance and batch size variance. Systems that provision based on mean latency will experience SLO violations.

**Adaptive Batching Windows**

Fixed batching windows waste latency budget during high traffic when large batches form quickly. @lst-adaptive-batching demonstrates how adaptive strategies adjust the window based on queue depth.

::: {#lst-adaptive-batching lst-cap="**Adaptive Batching Window**: Dynamically adjusts batch timeout based on queue depth and arrival rate, reducing average latency by 27% compared to fixed windows while maintaining throughput."}
```{.python}
def adaptive_batching_window(queue_depth, arrival_rate, slo_ms):
    """Compute optimal batching window based on current system state."""
    target_batch_size = 16  # Optimal batch for GPU utilization

    # Fast path: batch ready, close immediately to minimize latency
    if queue_depth >= target_batch_size:
        return 0

    # Compute maximum allowable wait from SLO constraint
    # Reserve 30% of latency budget for batching, remainder for inference
    max_wait = slo_ms * 0.3

    # Estimate time to accumulate target batch at current arrival rate
    if arrival_rate > 0:
        requests_needed = target_batch_size - queue_depth
        estimated_wait = requests_needed / arrival_rate
        # Return minimum of estimated wait and SLO-constrained maximum
        return min(estimated_wait, max_wait)

    return (
        max_wait  # Low traffic: use full budget to accumulate batch
    )
```
:::

This approach reduces average wait time during high traffic while maintaining batch sizes. For traffic varying between 200-1000 QPS:

- Fixed window (10ms): Average latency 15ms, throughput 650 img/s
- Adaptive window: Average latency 11ms (27% reduction), throughput 680 img/s (5% improvement)

**Throughput-Latency Pareto Frontier**

The batching configuration space forms a Pareto frontier where improving throughput requires accepting higher latency. @tbl-pareto-batching traces this frontier across five representative configurations:

+-----------------+---------------+-----------------+-----------------+----------------+----------------------+
| **Window (ms)** | **Max Batch** | **Avg Latency** | **p99 Latency** | **Throughput** | **Configuration**    |
+================:+==============:+================:+================:+===============:+:=====================+
| 2               | 16            | 8ms             | 18ms            | 890 img/s      | Ultra-low latency    |
| 5               | 32            | 10ms            | 22ms            | 1,140 img/s    | Balanced             |
| 10              | 32            | 15ms            | 35ms            | 1,240 img/s    | Moderate latency     |
| 20              | 64            | 23ms            | 52ms            | 1,310 img/s    | Throughput-optimized |
| 50              | 128           | 38ms            | 98ms            | 1,350 img/s    | Maximum throughput   |
+-----------------+---------------+-----------------+-----------------+----------------+----------------------+

: **Batching Pareto Frontier**: Each configuration represents a different point on the throughput-latency trade-off curve. Moving from 2ms to 50ms windows improves throughput by only 52% while increasing p99 latency by 5.4×. Diminishing returns make aggressive batching costly for latency-sensitive applications. {#tbl-pareto-batching}

**Practical Configuration Guidelines**

Based on quantitative analysis, principled batching configuration follows these guidelines. Start with the latency budget by allocating 20 to 30 percent of SLO to batching wait time. Estimate traffic using the p95 arrival rate rather than average to account for traffic spikes. Calculate the maximum window as $T_{\text{max}} = 0.3 \times L_{\text{SLO}}$. Determine the batch size limit from GPU memory and p99 latency constraints. Monitor the actual distribution since batch size variance indicates whether traffic assumptions hold.

For ResNet-50 with 50ms SLO and 500 QPS traffic:

- Latency budget for batching: 15ms
- Maximum window: 15ms
- Expected batch size: 7.5
- Maximum batch size: 32 (memory limit)
- Configuration: $T = 12$ms, $b_{\text{max}} = 32$
- Predicted p99 latency: 43ms (within SLO)
- Predicted throughput: 1,180 img/s

### Continuous Batching {#sec-serving-continuous-batching}

Autoregressive models like language models generate outputs token by token, creating a batching challenge that differs fundamentally from single-pass models like ResNet-50. Traditional batching processes all sequences in a batch for all generation steps, wasting compute when sequences complete at different times [@yu2022orca]. If one sequence in a batch of 8 finishes after 10 tokens while others need 100 tokens, 87.5% of the compute for that sequence slot is wasted. This inefficiency becomes critical as language models dominate production inference workloads.

Continuous batching (also called iteration-level batching) addresses this waste by allowing new requests to join a batch between generation steps and completed sequences to exit [@kwon2023vllm]. Rather than forming static batches that persist for the entire generation process, the system manages batch composition dynamically at each decoding iteration.

The mechanism works as follows: when a sequence generates its end-of-sequence token, its slot becomes immediately available. A waiting request can fill that slot for the next iteration rather than waiting for the entire batch to complete. Similarly, the system can add new requests to available slots without interrupting ongoing generation. This dynamic approach maintains high GPU utilization even when sequence lengths vary dramatically.

Systems implementing continuous batching, such as vLLM and TensorRT-LLM, achieve 2-4× higher throughput than traditional static batching [@agrawal2024sarathi]. The improvement comes from two sources: eliminating wasted compute on completed sequences and reducing average wait time for new requests. For production language model serving where response lengths vary from single tokens to thousands, continuous batching has become essential for cost-effective deployment.

Memory management adds complexity to continuous batching. As sequences enter and exit the batch, the key-value cache that stores attention context must be dynamically allocated and freed. PagedAttention, introduced in vLLM, applies operating system paging concepts to manage this memory efficiently, avoiding fragmentation that would otherwise limit batch capacity [@kwon2023vllm]. These techniques represent the intersection of classical systems engineering with modern ML serving challenges.

::: {.callout-note title="LLM Serving: Beyond the Fundamentals"}

Language model serving introduces challenges beyond the batching and memory principles established here. The key-value cache that stores attention context scales with sequence length and batch size, often exceeding the model weights themselves in memory consumption. Techniques like speculative decoding use small draft models to propose multiple tokens that the target model verifies in parallel, achieving 2-3× latency reduction for interactive applications. Weight-only quantization (INT4 weights with FP16 activations) proves more effective than activation quantization for memory-bandwidth-bound LLM inference.

These LLM-specific optimizations build directly on the foundations this chapter establishes: queuing theory governs request scheduling, batching tradeoffs determine throughput-latency curves, and precision selection follows the same accuracy-efficiency principles. The serving fundamentals apply universally; LLM serving adds domain-specific techniques atop this foundation.

:::

While continuous batching represents the frontier for LLM serving, not all deployment scenarios benefit from batching at all. The sophistication of modern batching techniques should not obscure a fundamental question: when does batching hurt rather than help?

### When Not to Batch {#sec-serving-no-batch}

Some scenarios require single-request processing. Ultra-low latency requirements where p99 latency must stay under 10ms make any batching delay unacceptable. Highly variable request sizes where inputs vary dramatically in size cause batching to create padding overhead that wastes compute. Memory constraints where models already consume most GPU memory mean batch activations may cause out-of-memory errors.

### Session Affinity Constraints {#sec-serving-session-affinity}

When requests from the same user or session should route to the same replica, batching becomes constrained. Session affinity, also called sticky sessions, matters for three main reasons.

**KV-Cache Reuse**: For conversational AI, the key-value cache from previous turns dramatically speeds up multi-turn conversations. Routing a follow-up request to a different replica forfeits this cached context, increasing latency by 2 to 5 times for long conversations.

**User-Specific Models**: Some systems serve personalized models or adapters per user. Routing requests to the replica that has already loaded that user's adapter avoids repeated loading overhead.

**Stateful Preprocessing**: When preprocessing maintains state through tokenizer caches or session-specific normalization, routing to a different replica requires rebuilding this state.

The tension with batching is clear since strict affinity constrains which requests can be batched together, potentially reducing batch sizes and GPU utilization. Production systems often implement soft affinity where requests prefer their assigned replica but can overflow to others when that replica is overloaded. This preserves most affinity benefits while maintaining load balance.

### Traffic Patterns and Batching Strategy {#sec-serving-traffic-patterns}

The optimal batching strategy depends critically on how requests arrive. Different deployment contexts exhibit fundamentally different arrival patterns, each requiring distinct batching approaches. The MLPerf inference benchmark codifies these patterns into four scenarios that directly map to real-world deployments, as @sec-benchmarking-ai explains in detail.

**Server Traffic (Poisson Arrivals)**

Cloud APIs and web services typically receive requests following a Poisson process, where arrivals are independent and uniformly distributed over time. @eq-poisson-batch expresses the expected batch size for Poisson arrivals with rate $\lambda$ and batching window $T$:

$$E[\text{batch size}] = \lambda \cdot T$$ {#eq-poisson-batch}

The variance equals the mean (a property of Poisson distributions), so batch sizes fluctuate significantly at moderate traffic. With $\lambda = 200$ requests/second and $T = 10$ms, expected batch size is 2, but 16% of windows will have zero requests (wasted compute cycles) while others may have 4 or more.

The optimal batching window balances waiting cost against throughput benefit. @eq-optimal-window defines this optimum:

$$T_{\text{optimal}} = \min\left(L - S, \sqrt{\frac{S}{\lambda}}\right)$$ {#eq-optimal-window}

where $L$ is the latency SLO and $S$ is the service time. A counterintuitive result emerges from this equation: as traffic increases, the optimal window decreases while achieved batch sizes still grow. @tbl-traffic-adaptive demonstrates this phenomenon across four traffic levels.

+------------------+--------------------+--------------------+-----------------+
| **Arrival Rate** | **Optimal Window** | **Avg Batch Size** | **p99 Latency** |
+=================:+===================:+===================:+================:+
| **100 QPS**      | 20ms               | 2.0                | 45ms            |
| **500 QPS**      | 8ms                | 4.0                | 42ms            |
| **1,000 QPS**    | 5ms                | 5.0                | 38ms            |
| **5,000 QPS**    | 2ms                | 10.0               | 35ms            |
+------------------+--------------------+--------------------+-----------------+

: **Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time. {#tbl-traffic-adaptive}

**Streaming Traffic (Correlated Arrivals)**

Autonomous vehicles, video analytics, and robotics systems receive inputs from multiple synchronized sensors. Rather than independent arrivals, frames from all cameras for a given timestamp must be processed together as a batch.

::: {.callout-note title="Multi-Camera Autonomous Vehicle Serving"}

Consider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial fusion:

**Timeline for processing frame set N:**

+----------+-----------------------------------+
| **Time** | **Event**                         |
+=========:+:==================================+
| T = 0ms  | Cameras begin capturing frame N   |
| T = 8ms  | Camera 1 frame arrives            |
| T = 10ms | Cameras 2-5 frames arrive         |
| T = 15ms | Camera 6 arrives (jitter)         |
| T = 15ms | Batch inference begins (6 images) |
| T = 25ms | Inference complete                |
| T = 32ms | Result ready for planning module  |
+----------+-----------------------------------+

**Key constraints:**

- Hard deadline: 33ms per frame set (real-time requirement)
- Batch size: Fixed at 6 (one per camera)
- Synchronization budget: 12ms of 33ms total (36% for jitter tolerance)
- Timeout policy: If camera frame not received by T+20ms, use previous frame

Unlike Poisson traffic where dynamic batching optimizes throughput, streaming traffic requires synchronization policies that handle sensor jitter while meeting hard deadlines.

:::

**Single-User Traffic (Sequential Arrivals)**

Mobile and embedded applications serve one user at a time, with requests arriving only after the previous result is consumed. Batch size is typically 1, eliminating batching optimization entirely but raising different challenges.

::: {.callout-note title="ResNet-50: Mobile Single-User Serving (Pixel 6 NPU)"}

+------------------------+--------------+------------------------+
| **Phase**              | **Duration** | **Notes**              |
+:=======================+=============:+:=======================+
| **Camera buffer read** | 8ms          | System API overhead    |
| **JPEG decode (CPU)**  | 15ms         | Single-threaded        |
| **Resize + Normalize** | 5ms          | CPU preprocessing      |
| **NPU inference**      | 12ms         | 82% NPU utilization    |
| **Post-process + UI**  | 5ms          | Result rendering       |
| **Total**              | **45ms**     | Perceived as "instant" |
+------------------------+--------------+------------------------+

**Critical insight**: Even at batch size 1, the mobile NPU achieves 82% utilization because its compute capacity matches single-image workloads. This differs fundamentally from datacenter GPUs, which achieve only 15% utilization at batch size 1 because their massive parallelism requires larger batches to saturate.

Mobile serving optimization focuses on preprocessing efficiency and power management rather than batching strategies.

:::

@tbl-traffic-patterns-summary maps the four MLPerf scenarios to their deployment contexts and optimal batching strategies, providing a decision framework for serving system design.

+------------------+---------------------+------------------+---------------------------+
| **MLPerf**       | **Deployment**      | **Batch**        | **Optimization**          |
| **Scenario**     | **Context**         | **Strategy**     | **Focus**                 |
+:=================+:====================+:=================+:==========================+
| **Server**       | Cloud APIs,         | Dynamic batching | Window tuning,            |
|                  | web services        | with timeout     | utilization-latency curve |
+------------------+---------------------+------------------+---------------------------+
| **MultiStream**  | Autonomous driving, | Synchronized     | Jitter handling,          |
|                  | video analytics     | sensor fusion    | deadline guarantees       |
+------------------+---------------------+------------------+---------------------------+
| **SingleStream** | Mobile apps,        | No batching      | Preprocessing,            |
|                  | embedded devices    | (batch=1)        | power efficiency          |
+------------------+---------------------+------------------+---------------------------+
| **Offline**      | Batch processing,   | Maximum batch    | Throughput,               |
|                  | data pipelines      | size             | hardware utilization      |
+------------------+---------------------+------------------+---------------------------+

: **Traffic Patterns and Batching Strategies**: The four MLPerf inference scenarios map to distinct deployment contexts. Server traffic (cloud APIs) uses dynamic batching with timeout; MultiStream (autonomous driving) uses synchronized sensor fusion; SingleStream (mobile) processes requests individually; Offline (batch processing) maximizes batch size for throughput. {#tbl-traffic-patterns-summary}

## Generative Inference and Memory Management {#sec-serving-generative-memory}

Serving generative models (LLMs) introduces a unique systems challenge: **state management**. Unlike discriminative models (ResNet) where each request is independent and stateless, autoregressive generation requires maintaining context across a sequence of decoding steps. This state, known as the KV Cache, fundamentally changes the memory characteristics of the serving system.

### The KV Cache Bottleneck {#sec-serving-kv-cache}

In autoregressive generation, the model predicts the next token based on all previous tokens. Naively recomputing the attention mechanism for the entire sequence at every step would be prohibitively expensive ($O(N^2)$ compute).

To optimize this, inference systems cache the Key (K) and Value (V) tensors for all previous tokens. This reduces compute to $O(N)$ per step but shifts the bottleneck from **compute** to **memory capacity and bandwidth**.

::: {.callout-note title="Lighthouse Example: Generative Inference with Llama-3"}

For a Llama-3-7B model (32 layers, 4096 dim) serving a batch of 64 requests with sequence length 2048:

$$\text{Memory} = 2 \times \text{Layers} \times \text{Dim} \times \text{SeqLen} \times \text{Batch} \times \text{Precision}$$

$$\text{Memory} = 2 \times 32 \times 4096 \times 2048 \times 64 \times 2 \text{ bytes (FP16)} \approx 64 \text{ GB}$$

This 64 GB cache competes with model weights (14 GB) for GPU memory. The KV cache grows linearly with sequence length, making memory capacity the hard limit on batch size and throughput.

:::

### Memory Fragmentation and PagedAttention {#sec-serving-paged-attention}

Standard deep learning frameworks require tensors to be stored in contiguous memory blocks. However, the length of a generated sequence is unknown ahead of time. Systems traditionally reserved maximum-length buffers (e.g., 4096 tokens) for every request, leading to massive internal fragmentation—often wasting 20% to 40% of GPU memory on empty reservation slots.

**PagedAttention** (introduced by vLLM) solves this by applying operating system principles to ML serving. It treats the KV cache as "virtual memory" and GPU VRAM as "physical memory," partitioning the cache into fixed-size blocks (pages) that can be stored non-contiguously.

*   **Virtual Memory**: The model views the KV cache as a contiguous tensor.
*   **Physical Memory**: The block manager allocates physical pages on demand, only when new tokens are generated.

This architectural innovation allows serving systems to process requests with near-zero memory waste, enabling 2-4$\times$ higher batch sizes and throughput compared to contiguous allocation systems.

### Performance Metrics: TTFT and TPOT {#sec-serving-llm-metrics}

Measuring the performance of a generative model requires a two-part metric system that reflects the internal state transition from "prefill" to "decode."

::: {.callout-definition title="LLM Performance Metrics"}

**Time to First Token (TTFT)** measures the _latency_ from the moment a request is submitted until the first output token appears. This metric is governed by the _compute-bound_ prefill phase and determines the "responsiveness" of an interactive application.

**Time Per Output Token (TPOT)** measures the time to generate each subsequent token. This metric is governed by the _memory-bandwidth-bound_ decode phase and determines the perceived _fluidity_ of the generation.

:::

::: {.callout-note title="Lighthouse Example: Llama-3 Latency Targets"}

A production-grade LLM service typically targets the following SLOs:

- **TTFT**: < 500 ms (for a 1000-token prompt)
- **TPOT**: < 50 ms (equivalent to ~20 tokens/second, which is faster than human reading speed)
- **Throughput**: > 1000 tokens/second aggregate across all users

:::

### Context Management and Prefix Caching {#sec-serving-context-mgmt}

As conversations grow longer, the KV cache becomes the primary consumer of system memory. Efficient systems use two techniques to manage this state:

**Prefix Caching**

Many LLM prompts share a common "system prompt" or context (e.g., a long document used for RAG). Instead of re-processing these tokens for every request, the server caches the KV states for common prefixes. This eliminates the prefill compute for the cached tokens, reducing TTFT by 50-80% for repeated queries.

**Sliding Window Attention**

For very long conversations, systems may discard the KV cache for early tokens, keeping only a "window" of recent history. This bounds the memory footprint of the state at the cost of the model "forgetting" early details. Advanced implementations, such as **Rolling KV Cache**, manage this window by overwriting old pages in the PagedAttention block table, maintaining constant memory usage regardless of conversation length.

## Postprocessing {#sec-serving-postprocessing}

Batching optimizes how requests traverse the inference phase, but even optimally batched inference produces only raw tensors. These floating-point arrays carry no inherent meaning to applications or users. The final phase of the serving pipeline, postprocessing, transforms these tensors into actionable predictions: a 0.95 probability becomes a confident "dog" label, a sequence of token IDs becomes readable text, or a bounding box tensor becomes a highlighted region in an image. While often overlooked in system design, postprocessing significantly impacts both latency and the usefulness of predictions, determining whether raw model capability translates into practical value.

### From Logits to Predictions {#sec-serving-logits}

Classification models output logits or probabilities across classes. Converting these to predictions involves several potential steps including argmax selection that chooses the highest-probability class, thresholding that applies confidence thresholds before returning predictions, top-k extraction that returns multiple high-probability classes with scores, and calibration that adjusts raw probabilities to better reflect true likelihoods.

::: {.callout-note title="ResNet-50: Postprocessing Pipeline"}

For ResNet-50 image classification, typical postprocessing includes:

```{.python}
# Transform raw logits to calibrated probabilities
# Input: logits tensor of shape (batch_size, 1000) - one score per ImageNet class
probs = torch.softmax(
    logits, dim=-1
)  # Normalize to sum=1; ~0.05ms on GPU

# Extract top-5 predictions for multi-class response
# topk returns (values, indices) sorted by probability
top5_probs, top5_indices = probs.topk(5)  # ~0.02ms; GPU operation

# Map class indices to human-readable labels
# IMAGENET_CLASSES: list of 1000 class names from synset mapping
labels = [
    IMAGENET_CLASSES[i] for i in top5_indices
]  # ~0.01ms; CPU lookup

# Format response with predictions and metadata for API contract
response = {
    "predictions": [
        {"label": label, "confidence": float(prob)}
        for label, prob in zip(labels, top5_probs)
    ],
    "model_version": "resnet50-v2.1",  # Enable client-side version tracking
    "inference_time_ms": 5.2,  # Observability for latency monitoring
}
```

**Total postprocessing time**: ~0.1ms (negligible compared to preprocessing and inference)

:::

Each step adds latency but improves response utility. Calibration in particular can add significant computation but is essential when downstream systems make decisions based on confidence scores.

### Generation and Decoding {#sec-serving-decoding}

Generative models require decoding strategies that trade off quality, diversity, and latency. The choice of decoding strategy can dramatically affect both output quality and computational cost.

**Greedy decoding** selects the highest-probability token at each step. Fast but often produces repetitive, low-quality outputs because it cannot recover from early mistakes.

**Beam search** maintains multiple candidate sequences, selecting the highest-scoring complete sequence. Produces higher-quality outputs but multiplies computation by the beam width.

**Sampling** with temperature, top-k, and top-p parameters introduces randomness for diversity [@holtzman2020curious]. Temperature scales logits before softmax. Top-k limits sampling to the k highest-probability tokens. Top-p, also called nucleus sampling, limits sampling to tokens comprising probability mass p.

The choice involves latency tradeoffs [@meister2020beam]. Beam search with width 5 takes roughly 5× the compute of greedy decoding. Sampling adds minimal overhead but requires careful parameter tuning to balance quality and coherence.

### Output Formatting and Streaming {#sec-serving-output-format}

Production systems rarely return raw predictions. Outputs must conform to API contracts, often requiring JSON serialization with specific schema, confidence score formatting and thresholding, error handling for edge cases such as no confident prediction or out-of-distribution input, and metadata attachment including model version, inference time, and feature attributions.

Streaming responses for generative models add complexity. Rather than waiting for complete generation, systems return tokens as they are produced. This improves perceived latency (users see output beginning quickly) but requires infrastructure support for chunked responses and client-side incremental rendering.

## Inference Runtime Selection {#sec-serving-runtimes}

The complete preprocessing, inference, and postprocessing pipeline we have examined must execute somewhere, and that execution environment significantly impacts whether the latency budgets we established earlier are achievable. The inference runtime, the software layer that orchestrates tensor operations and manages hardware resources, can vary by an order of magnitude in performance for identical models. Choosing appropriately requires understanding the tradeoffs between framework-native serving, general-purpose optimization, and specialized inference engines.

### Framework-Native Serving {#sec-serving-framework-native}

PyTorch and TensorFlow models can serve directly using their native runtimes. This approach maximizes compatibility (any model that trains will serve) and simplifies the deployment pipeline (no export or conversion step). However, framework runtimes include training functionality that adds overhead, and default execution paths may not exploit hardware-specific optimizations.

TorchScript and TensorFlow SavedModel formats enable ahead-of-time compilation and graph optimization, improving over eager execution while maintaining framework compatibility. These formats represent the first step toward deployment optimization without abandoning the familiar framework ecosystem.

### General-Purpose Optimization {#sec-serving-onnx}

ONNX Runtime provides a hardware-agnostic optimization layer [@onnxruntime2024]. Models export to ONNX format, then ONNX Runtime applies graph optimizations and selects execution providers for the target hardware. This enables single-format deployment across CPUs, GPUs, and specialized accelerators.

### Specialized Inference Engines {#sec-serving-specialized}

TensorRT (NVIDIA GPUs), OpenVINO (Intel hardware), and similar engines optimize specifically for their target hardware [@nvidia2024tensorrt; @chen2018tvm]. They apply aggressive optimizations: layer fusion, precision calibration, kernel auto-tuning. These typically achieve 2-5× speedup over framework-native serving but require explicit export and may not support all operations.

::: {.callout-note title="ResNet-50: Runtime Comparison"}

Performance comparison for ResNet-50 inference on V100 GPU (batch size 1):

+-----------------+-------------+-------------+---------------------------+
| **Runtime**     | **Latency** | **Speedup** | **Notes**                 |
+:================+============:+============:+:==========================+
| PyTorch (eager) | 8.5ms       | 1.0×        | Baseline, no optimization |
| TorchScript     | 6.2ms       | 1.4×        | JIT compilation           |
| ONNX Runtime    | 5.1ms       | 1.7×        | Cross-platform            |
| TensorRT FP32   | 2.8ms       | 3.0×        | NVIDIA-specific           |
| TensorRT FP16   | 1.4ms       | 6.1×        | Tensor Core acceleration  |
| TensorRT INT8   | 0.9ms       | 9.4×        | Requires calibration      |
+-----------------+-------------+-------------+---------------------------+

**Key insight**: The 9.4× speedup from TensorRT INT8 comes at the cost of: (1) quantization calibration data, (2) potential accuracy loss (<1% for ResNet-50), and (3) NVIDIA-specific deployment.

:::

The optimization-compatibility tradeoff is fundamental. More aggressive optimization yields better performance but increases deployment complexity and may introduce numerical differences from training. The choice depends on latency requirements, deployment constraints, and available engineering resources.

### Runtime Configuration {#sec-serving-runtime-config}

Beyond runtime selection, configuration choices impact serving performance including thread pools that control parallelism for CPU inference, memory allocation strategies that choose between pre-allocating buffers versus dynamic allocation, execution providers that select and prioritize hardware backends, and graph optimization level that trades compilation time for runtime performance. Production deployments require experimentation to find optimal configurations for specific models and hardware combinations. A systematic approach tests key parameters and measures their impact on latency distributions.

### Precision Selection for Serving {#sec-serving-precision}

Numerical precision directly trades accuracy for throughput, connecting to the quantization techniques covered in @sec-model-optimizations. While @sec-model-optimizations focuses on training-time quantization, serving introduces additional considerations including calibration requirements, layer sensitivity, and dynamic precision selection.

**Precision-Throughput Relationship**

For memory-bandwidth-bound operations, reducing precision proportionally increases throughput by reducing data movement. @eq-precision-throughput quantifies the theoretical maximum speedup from precision reduction:

$$\frac{\text{Throughput}_{\text{INT8}}}{\text{Throughput}_{\text{FP32}}} = \frac{32}{8} = 4\times \text{ (theoretical maximum)}$$ {#eq-precision-throughput}

In practice, GPU compute pipelines and Tensor Core alignment requirements limit achieved speedup to 2.5-3.5x for INT8 versus FP32. Tensor Cores require specific alignment: INT8 operations need tensor dimensions divisible by 16, while FP16 requires divisibility by 8. @sec-ai-acceleration provides the detailed Tensor Core architecture that explains these alignment constraints.

::: {.callout-note title="ResNet-50: Precision Tradeoffs on V100"}

+----------------+-------------+------------+--------------+-----------------------+-----------------+
| **Precision**  | **Latency** | **Memory** | **Accuracy** | **Tensor Core Util.** | **Calibration** |
+===============:+============:+===========:+=============:+======================:+:================+
| **FP32**       | 2.8ms       | 98MB       | 76.13%       | 0%                    | None            |
| **FP16**       | 1.4ms       | 49MB       | 76.13%       | 85%                   | None            |
| **INT8 (PTQ)** | 0.9ms       | 25MB       | 75.80%       | 92%                   | 1,000 samples   |
| **INT8 (QAT)** | 0.9ms       | 25MB       | 76.05%       | 92%                   | Full retraining |
+----------------+-------------+------------+--------------+-----------------------+-----------------+

**Key observations:**

- INT8 achieves 3.1× speedup but loses 0.33% accuracy with post-training quantization (PTQ)
- Quantization-aware training (QAT) recovers most accuracy but requires retraining
- FP16 provides 2× speedup with no accuracy loss for most models

:::

**Layer Sensitivity**

Not all layers tolerate reduced precision equally. @eq-quant-error captures how quantization error for a layer scales with weight magnitude and gradient sensitivity:

$$\epsilon_{\text{quant}} \propto \alpha \cdot \|W\|_2 \cdot 2^{-b}$$ {#eq-quant-error}

where $\alpha$ is a layer-specific sensitivity coefficient, $\|W\|_2$ is the weight L2 norm, and $b$ is the bit width. This explains observed patterns where first convolutional layers with high gradients and large sensitivity coefficients are precision-sensitive and often kept at FP16, middle layers with stable gradients and low sensitivity coefficients tolerate INT8 well, and final classification layers with small weights but high task sensitivity benefit from FP16 or higher precision.

**Calibration Requirements**

Post-training quantization requires a calibration dataset to determine optimal scale factors for INT8 conversion. Production experience shows that calibration data must be representative of actual serving traffic, not just training data. Using ImageNet validation images to calibrate a model serving wildlife camera images resulted in 3.2% accuracy degradation in one production system.

**Dynamic Precision Selection**

Advanced serving systems select precision per request based on runtime conditions. If the system is ahead of latency SLO, it uses higher precision for better accuracy. For low-confidence INT8 results, it recomputes at FP16. Different customer tiers may receive different precision levels. This pattern enables adaptive quality-latency tradeoffs while maximizing throughput during normal operation.

The precision decision has direct infrastructure consequences: INT8 inference achieves roughly 3x higher throughput than FP32, meaning a workload requiring 30 GPUs at FP32 needs only 10 at INT8. This connection between model-level optimization and infrastructure economics is why precision selection cannot be treated as purely a model concern, and why the next section examines cost and capacity planning in detail.

## Economics and Capacity Planning {#sec-serving-economics}

The runtime and precision choices examined in previous sections determine per-inference performance, but production deployment requires translating these choices into infrastructure decisions. Serving costs scale with request volume, unlike training costs that scale with dataset size and model complexity [@zhang2019mark]. Cost structure analysis enables informed infrastructure decisions that balance performance requirements against budget constraints.

### Cost Per Inference {#sec-serving-cost-per-inference}

Total serving cost decomposes into several components including compute time for GPU or CPU per inference, memory for accelerator memory required to hold model and activations, data transfer for network bandwidth for request and response payloads, and orchestration overhead for container runtime, load balancing, and monitoring. For GPU inference, compute time dominates when utilization is high. When utilization is low, memory cost dominates because the GPU is reserved but idle. Serving infrastructure should maximize GPU utilization through batching, multi-model serving, or right-sized instance selection.

::: {.callout-note title="ResNet-50: Cost Analysis"}

Consider serving ResNet-50 on AWS infrastructure:

+---------------------------+---------------+----------------+------------------------+
| **Instance Type**         | **Cost/Hour** | **Throughput** | **Cost per 1M Images** |
+==========================:+==============:+===============:+=======================:+
| **c5.xlarge (CPU)**       | $0.17         | 50 img/s       | $0.94                  |
| **g4dn.xlarge (T4 GPU)**  | $0.53         | 400 img/s      | $0.37                  |
| **p3.2xlarge (V100 GPU)** | $3.06         | 1,200 img/s    | $0.71                  |
+---------------------------+---------------+----------------+------------------------+

**Key insight**: The T4 GPU instance achieves the lowest cost per inference despite higher hourly cost, because GPU throughput dramatically exceeds CPU throughput. The V100 is only cost-effective at very high sustained traffic where its higher throughput justifies the 6× price increase.

:::

### GPU vs CPU Economics {#sec-serving-gpu-cpu}

GPUs provide orders-of-magnitude speedup for parallel operations but cost significantly more per hour [@wu2019machine]. The crossover point depends on model characteristics and latency requirements.

CPU inference makes economic sense when models are small with few parameters and simple operations, latency requirements are relaxed with hundreds of milliseconds acceptable, request volume is low or highly variable, and models use operations that do not parallelize well. GPU inference makes economic sense when models are large with parallel-friendly operations, latency requirements are strict at tens of milliseconds, request volume is high and consistent, and batching can achieve high utilization.

**Scaling Responsiveness**: Beyond steady-state costs, startup time affects scaling economics. CPU instances typically start in 30 to 60 seconds while GPU instances take 2 to 5 minutes including driver initialization, model loading, and warmup. For variable traffic patterns, this startup latency can be more important than cost per inference. If traffic spikes arrive faster than GPU instances can scale, latency SLOs will be violated despite having sufficient eventual capacity.

This asymmetry suggests different scaling strategies where CPU instances enable reactive scaling by responding to current demand while GPU instances often require predictive scaling by provisioning based on anticipated demand. For bursty workloads, a hybrid approach uses always-on GPU capacity for baseline load plus CPU overflow capacity for spikes, trading higher per-inference cost during spikes for better responsiveness.

### Capacity Planning {#sec-serving-capacity-planning}

The GPU versus CPU decision establishes the cost per inference, but determining how much infrastructure to provision requires combining cost analysis with the queuing theory foundations from @sec-serving-queuing. Capacity planning translates latency requirements and traffic projections into infrastructure specifications. Key inputs include traffic patterns such as peak request rate, daily and weekly cycles, and growth projections, latency SLOs including p50, p95, and p99 targets, and model characteristics such as inference time distribution at various batch sizes. From these inputs, queuing theory determines required capacity [@harchol2013performance]. The equations developed in @sec-serving-queuing provide the mathematical foundation where @eq-mm1-wait shows how latency scales with utilization, while @eq-p99-latency enables calculating p99 latency for capacity planning.

The relationship between utilization and latency is nonlinear as shown in the utilization-latency table. At 70 percent utilization, p99 latency is approximately fifteen times service time. At 90 percent utilization, it reaches approximately 46 times service time. This nonlinearity explains why systems that seem healthy with low average latency can suddenly violate SLOs when traffic increases modestly.

The worked example in @sec-serving-queuing demonstrates the complete capacity planning process by starting from a 50ms p99 SLO and 5,000 QPS target, deriving the safe utilization threshold of 72 percent, calculating required service rate of 6,944 QPS, and determining GPU count with headroom of 10 V100s. Production systems typically provision for peak load plus 30 percent headroom, using auto-scaling to reduce costs during low-traffic periods while meeting latency objectives during peaks.

### Production Case Study: Serving Llama-3-7B {#sec-serving-case-study}

To synthesize the principles of latency budgeting, memory management, and hardware efficiency, we analyze the production profile of a modern Large Language Model (LLM) serving workload. This case study demonstrates how physical constraints—memory bandwidth and PCIe capacity—translate directly into service-level metrics and unit economics.

#### Workload Profile {#sec-serving-case-study-profile}

*   **Model**: Llama-3-7B (quantized to 4-bit AWQ).
*   **Hardware**: 1$\times$ NVIDIA H100 GPU (80 GB HBM3).
*   **Request Characteristics**: 1,000-token input prompt (Prefill), 256-token generated response (Decode).
*   **Target SLOs**: TTFT $<$ 200 ms, TPOT $<$ 20 ms.

#### Latency Deconstruction {#sec-serving-case-study-latency}

The end-to-end request latency is governed by the two-phase execution model of autoregressive transformers.

**1. Prefill Phase (Time to First Token)**

The model processes the 1,000-token prompt in parallel. On an H100, this compute-bound operation achieves approximately 10,000 tokens per second.
*   $T_{\text{prefill}} = \frac{1000 \text{ tokens}}{10000 \text{ tokens/s}} = 100 \text{ ms}$.
*   Accounting for 20 ms of system overhead (network ingress, tokenization), the **TTFT is 120 ms**, comfortably within the 200 ms SLO.

**2. Decode Phase (Time Per Output Token)**

The model generates 256 tokens sequentially. This phase is memory-bandwidth bound; the system must read the entire 3.5 GB weight tensor from VRAM to generate a single token.
*   $T_{\text{token}} \approx \frac{3.5 \text{ GB}}{3.35 \text{ TB/s}} \approx 1 \text{ ms}$ (theoretical limit).
*   Accounting for kernel launch overhead and attention computation, realized $T_{\text{token}}$ is approximately 10 ms.
*   Total decode time = $256 \text{ tokens} \times 10 \text{ ms/token} = 2.56 \text{ seconds}$.
*   **TPOT is 10 ms**, well within the 20 ms "fluidity" SLO.

#### Memory & Throughput {#sec-serving-case-study-throughput}

With 4-bit weights occupying 3.5 GB, the remaining ~76 GB of VRAM is available for the **KV Cache**. Using **PagedAttention**, we can allocate this memory with near-zero fragmentation.

*   Each token requires approximately 0.5 MB of KV cache (32 layers $\times$ 4096 dim $\times$ 2 vectors $\times$ 2-byte precision).
*   Total cache capacity $\approx \frac{72 \text{ GB}}{0.5 \text{ MB/token}} \approx 144,000 \text{ tokens}$.
*   At 1,256 tokens per request (input + output), the GPU can handle a **concurrent batch size of ~114 requests**.

#### Unit Economics {#sec-serving-case-study-economics}

For an H100 instance costing \$3.00 per hour:
*   Total tokens per hour = $114 \text{ batch} \times \frac{3600 \text{ s/hr}}{2.68 \text{ s/req}} \times 1256 \text{ tokens/req} \approx 190 \text{ million tokens/hour}$.
*   **Cost per million tokens**: $\frac{\$3.00}{190} \approx \mathbf{\$0.015}$.

This analysis highlights that for LLMs, **memory capacity** (the size of the KV cache) is the primary determinant of throughput and cost, while **memory bandwidth** is the primary determinant of latency.

## Fallacies and Pitfalls {#sec-serving-fallacies-pitfalls}

Serving inverts training priorities in counterintuitive ways. Intuitions from batch processing fail under latency constraints and unpredictable load, causing wasted effort, violated SLOs, and silent accuracy degradation in production.

**Fallacy:** _Faster model inference automatically means faster end-to-end serving._

Engineers assume model inference dominates serving latency. In production, preprocessing and postprocessing often consume 45 to 70 percent of total request time when inference runs on optimized accelerators, as @sec-serving-latency-budget demonstrates. A team reducing inference from 5ms to 2ms through quantization achieves only 23 percent end-to-end improvement if preprocessing remains at 8ms. Amdahl's Law formalizes this: if preprocessing consumes 50 percent of latency, even infinitely fast inference yields at most 2× speedup. For ResNet-50 with TensorRT optimization reducing inference to 2ms, preprocessing at 4.5ms still dominates at 69 percent of the 6.5ms total. Teams that optimize inference without profiling the complete request path waste engineering months achieving negligible latency improvements while the actual bottleneck remains unaddressed.

**Pitfall:** _Running serving infrastructure at high utilization to maximize cost efficiency._

Teams target 90 percent utilization to minimize idle capacity. In production, latency degrades nonlinearly as utilization approaches capacity. @eq-mm1-wait shows that at 90 percent utilization, average wait time reaches 10× service time; p99 latency becomes unacceptable for most SLOs. Moving from 70 percent to 90 percent utilization cuts infrastructure costs by 22 percent but triples average latency from 3.3× to 10× service time. For a 5ms inference service, this means p99 latency jumps from 25ms to 50ms. Systems provisioned for average load violate SLOs precisely when traffic increases during business-critical periods. Production systems targeting 60 to 70 percent utilization at peak load maintain the latency headroom needed to absorb traffic spikes without degrading user experience.

**Fallacy:** _Training accuracy guarantees serving accuracy._

Engineers assume identical model weights preserve validation set performance. In production, preprocessing differences silently shift inputs outside the training distribution. @sec-serving-skew shows how training-serving skew causes accuracy degradation despite identical weights: PIL versus OpenCV resize interpolation differs subtly, float64 versus float32 normalization produces different values, or feature computation timing changes. A model achieving 95 percent accuracy in validation drops to 90 percent in production from these preprocessing mismatches. One production system experienced 3.2 percent accuracy loss serving wildlife camera images after calibrating quantization with ImageNet validation images. Standard monitoring checking exceptions and latency violations fails to detect this silent degradation. Production systems require either identical preprocessing code for training and serving, or statistical monitoring comparing serving input distributions to training baselines to catch drift before accuracy degrades noticeably.

**Pitfall:** _Using average latency to evaluate serving system performance._

Engineers monitor average latency because it trends smoothly and is simple to compute. In production, averages hide the slowest requests that determine user satisfaction. A system with 10ms average latency might have 200ms p99 latency, meaning 1 percent of users experience 20× worse performance. For a service handling 1 million requests daily, 10,000 users encounter unacceptable latency while average metrics appear healthy. @sec-serving-tail-latency explains how queuing variability causes this divergence: at 70 percent utilization with 5ms service time, average latency is 17ms but p99 reaches 75ms. Production SLOs specify percentile targets (p95, p99) precisely because averages mask tail behavior. Systems reporting only averages pass monitoring checks while violating user experience standards, leading to customer churn that correlates with tail latency rather than average performance.

**Fallacy:** _Larger batch sizes always improve throughput._

Engineers maximize batch size assuming GPU saturation improves efficiency. In practice, throughput gains diminish while latency grows unbounded beyond the optimal point. For ResNet-50 on V100, increasing batch size from 16 to 32 improves throughput only 12 percent (1,143 to 1,280 images per second) while nearly doubling inference time from 14ms to 25ms. At batch size 64, memory fragmentation reduces effective utilization to 60 percent despite the GPU appearing busy, and variable input sizes create padding overhead that wastes compute. @sec-serving-batching-tradeoffs shows optimal batch size depends on latency SLOs, memory constraints, and traffic patterns: for 50ms p99 targets, batch sizes above 32 routinely violate SLOs. Systems maximizing batch size unconditionally sacrifice user experience for marginal throughput gains, violating latency requirements precisely when traffic increases during peak business periods.

**Pitfall:** _Calibrating quantized models with training data rather than production traffic._

Teams calibrate with training data because it is readily available and produced validation accuracy. In production, traffic distribution often differs from training data, making calibration scale factors suboptimal. Post-training quantization determines INT8 scale factors by measuring activation ranges on calibration data, but this assumes production inputs match the calibration distribution. When production traffic differs through image sources, lighting conditions, or user demographics, calibration on training data degrades accuracy. One production system experienced 3.2 percent accuracy loss serving wildlife camera images after calibrating with ImageNet validation data. @sec-model-optimizations shows quantization error scales with activation range: miscalibration amplifies errors precisely on the out-of-distribution inputs that matter most. Effective quantization requires calibrating with representative samples of actual serving traffic, capturing the distribution shifts that distinguish production deployment from controlled validation.

**Pitfall:** _Cold start latency only matters for the first request._

Engineers optimize steady-state latency assuming most requests hit warm instances. In production, cold starts affect any request arriving after inactivity, after model updates, or during auto-scaling. Systems with bursty traffic experience cold starts on 10 to 30 percent of requests during scale-up events. ResNet-50 with TensorRT requires 30 seconds for compilation if the optimized engine is not cached; during a traffic spike triggering 10 new instances, 300 seconds of user-facing latency is added across the first requests to each instance. @sec-serving-model-loading shows cold start compounds weight loading (500ms for 98MB FP32 from SSD), CUDA context initialization (300ms), and warmup (200ms). Systems ignoring cold start meet SLOs during steady state but violate them during scale-up events, deployment windows, and traffic pattern changes that trigger the scaling logic customers experience most acutely.

## Summary {#sec-serving-summary}

Serving represents the critical transition from model development to production deployment, where the optimization priorities that governed training must be fundamentally inverted. The shift from throughput maximization to latency minimization transforms every system design decision. The queuing theory foundations established here reveal why this inversion is not merely a change in metrics but a change in the governing mathematics: the nonlinear relationship between utilization and latency means that systems behaving well at moderate load can suddenly violate SLOs when traffic increases modestly. Little's Law and the M/M/1 wait time equations provide the quantitative foundation for capacity planning, replacing intuition-based provisioning with engineering rigor.

Effective serving optimization requires understanding the complete request path rather than focusing exclusively on model inference. Interface protocols like gRPC and efficient serialization formats minimize the "tax" of data movement, while preprocessing often consumes 45 to 70 percent of total latency when inference runs on optimized accelerators. The microsecond-scale overheads identified by Barroso, Patterson, and colleagues explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization matters as much as model optimization. Training-serving skew represents another dimension of this complexity, silently degrading accuracy when preprocessing logic differs between training and production environments in ways that traditional testing cannot detect.

The traffic pattern analysis reveals how deployment context fundamentally shapes batching strategy and system design. Server workloads with Poisson arrivals optimize dynamic batching windows, autonomous vehicles with streaming sensor data require synchronized batch formation, and mobile applications with single-user patterns eliminate batching entirely. The MLPerf scenarios codify these patterns for standardized benchmarking, connecting the serving principles established here to the measurement frameworks explored in @sec-benchmarking-ai. Precision selection and runtime optimization extend the quantization techniques from @sec-model-optimizations and Tensor Core capabilities from @sec-ai-acceleration into the serving domain. Finally, the translation of these technical metrics into unit economics—exemplified by the Llama-3 case study—demonstrates how engineering decisions regarding batching, precision, and hardware selection directly determine the financial viability of deployment.

::: {.callout-important title="Key Takeaways"}
* Serving inverts training priorities: latency per request matters more than aggregate throughput, requiring fundamentally different system design
* Queuing theory provides the mathematical foundation for capacity planning, with the utilization-latency relationship explaining why systems degrade nonlinearly under load
* Interface protocols and serialization formats impose a "tax" on every request that must be minimized through binary formats like Protobuf
* Preprocessing often dominates total latency (45-70%), making pipeline optimization as important as model optimization
* Traffic patterns (Poisson, streaming, single-user) determine optimal batching strategy, directly mapping to MLPerf inference scenarios
* Generative inference requires managing the KV Cache bottleneck, using techniques like PagedAttention to optimize memory efficiency
* Training-serving skew silently degrades accuracy and requires identical preprocessing code or rigorous monitoring to prevent
:::

The serving foundations established here provide the infrastructure for operational deployment. @sec-ml-operations builds on these foundations to address monitoring, versioning, and continuous validation, the operational complexities that characterize real-world ML system deployment.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
