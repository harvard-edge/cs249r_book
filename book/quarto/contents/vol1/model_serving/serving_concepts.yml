concept_map:
  source: model_serving.qmd
  generated_date: 2026-02-19
  primary_concepts:
    - Serving Inversion (Throughput to Latency)
    - Latency Budget (Preprocessing, Inference, Postprocessing)
    - Queuing Theory (Little's Law, M/M/1)
    - Dynamic Batching
    - Training-Serving Skew (Preprocessing divergence)
  secondary_concepts:
    - Deployment Spectrum (Cloud to TinyML)
    - Cold Start Dynamics
    - Resource Isolation (Pinning, Locking)
    - Serialization Bottlenecks (JSON vs Protobuf)
    - LLM Serving (TTFT, TPOT, PagedAttention)
  technical_terms:
    - SLO / SLA
    - Inference Server (Triton, TF Serving)
    - gRPC / REST
    - NCHW / NHWC
    - Zero-Copy Inference
  methodologies:
    - Capacity planning
    - Tail-tolerant execution (Hedging, Canary)
  formulas:
    - Little's Law (L = Î» * W)
    - M/M/1 Wait Time
    - p99 Latency (Tail explosion)
