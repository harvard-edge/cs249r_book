concept_map:
  source: training.qmd
  generated_date: 2026-02-19
  primary_concepts:
    - Optimization Algorithms (SGD, Adam, AdamW)
    - Distributed Training (Data, Model, Pipeline Parallelism)
    - Memory/Throughput Trade-offs
    - Mixed-precision Training
    - Activation Checkpointing
  secondary_concepts:
    - Gradient Accumulation
    - Warmup strategies
    - Convergence monitoring
    - Training bottlenecks (Compute vs Memory vs Data bound)
  technical_terms:
    - All-Reduce
    - Parameter Server
    - Gradient Clipping
    - Weight Decay
    - Momentum
  methodologies:
    - Profile-diagnose-fix-reprofile
    - Pipeline overlapping
  formulas:
    - Effective Batch Size
    - Training time estimation
    - Roofline analysis in training
