{
  "metadata": {
    "source_file": "quarto/contents/vol1/responsible_engr/responsible_engr.qmd",
    "total_sections": 5,
    "sections_with_quizzes": 5,
    "sections_without_quizzes": 0
  },
  "sections": [
    {
      "section_id": "#sec-responsible-engineering-introduction-7a3f",
      "section_title": "Introduction",
      "quiz_data": {
        "quiz_needed": true,
        "rationale": {
          "focus_areas": [
            "Definition of responsible engineering",
            "Why engineers must lead on responsibility"
          ],
          "question_strategy": "Test foundational concepts and the engineering role in responsibility.",
          "difficulty_progression": "Start with definitions and move to the 'why'.",
          "integration": "Connect responsibility to technical decision-making.",
          "ranking_explanation": "Critical introduction to the shift in engineering mindset."
        },
        "questions": [
          {
            "question_type": "MCQ",
            "question": "Why is responsible engineering particularly critical for machine learning systems compared to traditional software?",
            "choices": [
              "ML systems are more expensive to develop.",
              "ML systems fail silently through biased outputs that appear normal.",
              "Traditional software does not require any testing.",
              "ML systems always produce deterministic results."
            ],
            "answer": "The correct answer is B. ML systems fail silently through biased outputs that appear normal. Unlike traditional software that crashes visibly, ML systems can produce discriminatory results for months without triggering conventional alerts, necessitating a proactive responsibility framework.",
            "learning_objective": "Contrast failure modes of ML systems and traditional software."
          },
          {
            "question_type": "SHORT",
            "question": "Why can responsibility not be delegated exclusively to ethics boards or legal departments in an ML project?",
            "answer": "Engineers occupy a critical position because technical decisions made during inception—such as model architecture, data pipeline design, and optimization objectives—define and constrain the space for all subsequent fairness interventions. Ethics boards often only see the system after these decisive foundational choices have been made.",
            "learning_objective": "Explain the engineer's role in proactive responsibility design."
          }
        ]
      }
    },
    {
      "section_id": "#sec-responsible-engineering-engineering-responsibility-gap-4d82",
      "section_title": "The Engineering Responsibility Gap",
      "quiz_data": {
        "quiz_needed": true,
        "rationale": {
          "focus_areas": [
            "Technical correctness vs responsible outcomes",
            "Amazon and COMPAS case studies"
          ],
          "question_strategy": "Use real-world examples to illustrate the gap between optimization and outcomes.",
          "difficulty_progression": "Analyze specific failures to identify systemic patterns.",
          "integration": "Connect optimization objectives to social consequences.",
          "ranking_explanation": "Case studies are essential for illustrating how 'correct' code can fail socially."
        },
        "questions": [
          {
            "question_type": "MCQ",
            "question": "In the Amazon recruiting tool case, why did removing explicit gender labels fail to eliminate bias?",
            "choices": [
              "The model was not trained for enough epochs.",
              "The model learned proxy signals (like college names) that correlated with gender.",
              "The engineers forgot to delete the gender column.",
              "The dataset was too small to be accurate."
            ],
            "answer": "The correct answer is B. The model learned proxy signals that correlated with gender. Even without direct labels, the model reconstructed protected attributes from other data features like school names and activity descriptions that encoded historical gender patterns.",
            "learning_objective": "Understand how models learn protected attributes through proxy variables."
          },
          {
            "question_type": "SHORT",
            "question": "Explain how a 'feedback loop' in a recommendation system can lead to bias amplification.",
            "answer": "Feedback loops occur when a model's predictions influence the data it later observes as training input. For example, if a system recommends provocative content to increase watch time, and users engage with it, the system interprets this as success and recommends even more extreme content, reinforcing and amplifying the initial algorithmic bias over time.",
            "learning_objective": "Analyze the mechanics of bias amplification in ML systems."
          }
        ]
      }
    },
    {
      "section_id": "#sec-responsible-engineering-checklist-5e2c",
      "section_title": "The Responsible Engineering Checklist",
      "quiz_data": {
        "quiz_needed": true,
        "rationale": {
          "focus_areas": [
            "Model cards and documentation",
            "Fairness metrics calculation"
          ],
          "question_strategy": "Focus on the practical application of documentation and measurement tools.",
          "difficulty_progression": "Move from documentation types to calculating specific disparities.",
          "integration": "Integrate quantitative metrics with qualitative documentation standards.",
          "ranking_explanation": "These are the core tools a practitioner uses to implement responsibility."
        },
        "questions": [
          {
            "question_type": "MCQ",
            "question": "What is the primary purpose of a 'Model Card' in responsible engineering?",
            "choices": [
              "To act as a warranty for the model software.",
              "To provide a standardized format for documenting intended use, performance factors, and ethical considerations.",
              "To store the binary weights of the trained neural network.",
              "To list all the developers who worked on the project."
            ],
            "answer": "The correct answer is B. To provide a standardized format for documenting intended use, performance factors, and ethical considerations. Model cards ensure that essential context and limitations are communicated to users and auditors, preventing inappropriate model reuse.",
            "learning_objective": "Explain the role of standardized documentation in ML accountability."
          },
          {
            "question_type": "SHORT",
            "question": "Define 'Disaggregated Evaluation' and explain why aggregate accuracy metrics can be misleading.",
            "answer": "Disaggregated evaluation is the practice of breaking down performance metrics by demographic subgroups. Aggregate metrics can be misleading because a high overall accuracy (e.g., 95%) can conceal severe failures in a minority subgroup (e.g., 65% accuracy), a disparity that only becomes visible when evaluating groups separately.",
            "learning_objective": "Apply disaggregated evaluation concepts to detect performance disparities."
          }
        ]
      }
    },
    {
      "section_id": "#sec-responsible-engineering-fairness-worked-example",
      "section_title": "Fairness Worked Example",
      "quiz_data": {
        "quiz_needed": true,
        "rationale": {
          "focus_areas": [
            "Calculation of fairness metrics",
            "Demographic parity vs Equal opportunity"
          ],
          "question_strategy": "Quantitative assessment of fairness definitions.",
          "difficulty_progression": "Requires calculation based on provided confusion matrix data.",
          "integration": "Links mathematical definitions to social outcomes.",
          "ranking_explanation": "Crucial for ensuring students can quantitatively verify fairness claims."
        },
        "questions": [
          {
            "question_type": "MCQ",
            "question": "If Group A has an approval rate of 55% and Group B has an approval rate of 40%, which fairness criterion is being directly violated?",
            "choices": [
              "Equalized Odds",
              "Demographic Parity",
              "Numerical Precision",
              "Model Lineage"
            ],
            "answer": "The correct answer is B. Demographic Parity. Demographic parity requires that the probability of a positive outcome (approval) is independent of group membership. A 15 percentage point difference indicates a violation of this parity.",
            "learning_objective": "Distinguish between different mathematical definitions of fairness."
          },
          {
            "question_type": "SHORT",
            "question": "Explain the difference between 'Equal Opportunity' and 'Demographic Parity'.",
            "answer": "Demographic Parity requires equal outcome rates for all groups regardless of qualifications. Equal Opportunity is a stricter criterion focusing on qualified individuals; it requires equal true positive rates (TPR), ensuring that those who would have a positive outcome (like repaying a loan) have an equal chance of being correctly identified by the model across all groups.",
            "learning_objective": "Compare demographic parity and equal opportunity metrics."
          }
        ]
      }
    },
    {
      "section_id": "#sec-responsible-engineering-environmental-cost-awareness-8f4d",
      "section_title": "Environmental and Cost Awareness",
      "quiz_data": {
        "quiz_needed": true,
        "rationale": {
          "focus_areas": [
            "Total Cost of Ownership (TCO)",
            "Environmental impact of ML"
          ],
          "question_strategy": "Focus on the economic and environmental dimensions of responsibility.",
          "difficulty_progression": "Analyze the components of TCO and their relative weights.",
          "integration": "Connect system efficiency to sustainability and business value.",
          "ranking_explanation": "Sustainability is a core component of modern engineering responsibility."
        },
        "questions": [
          {
            "question_type": "MCQ",
            "question": "For a successful production ML system, which cost component typically dominates the Total Cost of Ownership (TCO)?",
            "choices": [
              "Initial data labeling",
              "Hyperparameter search",
              "Inference costs",
              "Academic research grants"
            ],
            "answer": "The correct answer is C. Inference costs. For high-volume production systems, inference costs can be 10x to 1000x higher than training costs, as they compound across every query served to users over the system's lifetime.",
            "learning_objective": "Analyze the components of TCO in production ML systems."
          },
          {
            "question_type": "SHORT",
            "question": "How do model optimization techniques like quantization support both financial and environmental responsibility?",
            "answer": "Optimization techniques reduce the computational resources required per inference. Quantization (e.g., FP32 to INT8) typically reduces memory and compute needs by 2-4x. This lowers the electricity consumption (reducing carbon footprint) and the hardware requirements (reducing operational expenses and TCO) simultaneously.",
            "learning_objective": "Connect model optimization to environmental and economic outcomes."
          }
        ]
      }
    }
  ]
}
