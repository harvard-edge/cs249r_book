# Principles of Deployment {.unnumbered}

These principles govern the reliability of models in the real world. They explain why systems fail even when the code is bug-free.

::: {.callout-note icon=false title="The Statistical Drift Invariant"}
**The Law**: Unlike traditional software, which fails only when code changes, ML systems fail because the *environment* changes (Data Drift). Reliability degrades monotonically over time.

**The Engineering Implication**:
**Observability** must shift from system metrics (latency, errors) to statistical metrics (distribution distance). A system without data monitoring is a system in a state of unobserved decay.
:::

::: {.callout-note icon=false title="The Training-Serving Skew Law"}
**The Law**: Model performance degrades (typically 5â€“15%) whenever the serving data distribution or feature logic diverges from the training environment.

**The Engineering Implication**:
Feature consistency is a hard architectural requirement. **Feature Stores** are not just caches; they are consistency engines that ensure the mathematical function computed at inference is identical to the one learned during training.
:::

::: {.callout-note icon=false title="The Latency Budget Invariant"}
**The Law**: In real-time serving, P99 latency is the hard constraint; throughput is the variable to be optimized within that constraint.

**The Engineering Implication**:
Serving systems must implement **Tail-Tolerant** designs (e.g., hedged requests, deadline propagation). Batching strategies must be dynamic, sacrificing throughput to meet the latency deadline of the oldest request in the queue.
:::
