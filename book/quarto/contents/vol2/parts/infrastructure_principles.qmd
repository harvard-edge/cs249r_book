# Principles of Infrastructure {.unnumbered}

We have established the algorithmic need for scale (Part I). Now we must build the physical machine to support it. Part II is about the **Machine Learning Fleet**â€”the warehouse-scale computer where the network is the bus, and power density is the speed limit.

::: {.callout-perspective title="The Thermodynamic Limit"}
**The Concept**: Computation is physically the conversion of ordered energy (electricity) into disordered energy (heat) to produce information (reduced entropy).
$$ P_{limit} = \frac{\Delta Q}{\Delta t} $$

**The Engineering Implication**:
The bottleneck of the modern AI datacenter is not FLOPS, but **Watts per square foot**. When a rack generates 100kW of heat, air cooling fails. The physical design of the fleet is dictated by the ability to move heat away from silicon.
:::

::: {.callout-note icon=false title="The Power Density Wall"}
**The Principle**: Cluster scaling is constrained by thermal dissipation limits (Watts per rack) and cooling capacity, not just silicon area or floor space.

**The Engineering Implication**:
Modern AI accelerators generate heat densities that exceed air cooling capabilities. **Liquid cooling** becomes a facility requirement, not an option, for large-scale training clusters.
:::

::: {.callout-note icon=false title="The Memory Capacity Gap"}
**The Principle**: Model parameter growth ($10\times$/year) consistently outpaces GPU memory capacity growth ($2\times$/year).

**The Engineering Implication**:
Models no longer fit on single devices. Architectures must embrace **3D Parallelism** (splitting the model itself via Tensor and Pipeline Parallelism) as the default state, breaking the abstraction of the "single device."
:::

::: {.callout-note icon=false title="The Bisection Bandwidth Theorem"}
**The Principle**: The performance of a distributed application is limited by the minimum bisection bandwidth of the network topology.

**The Engineering Implication**:
A cluster with 10,000 GPUs is useless for training if they are connected by a 1 Gbps Ethernet tree. **Non-blocking topologies** (Fat-Tree, Dragonfly) are required to ensure that the network does not become the bottleneck for AllReduce operations.
:::

## Part II Roadmap: The Hardware of Scale {#sec-principles-infrastructure-part-ii-roadmap-hardware-scale-36a9}

This section builds the physical substrate:

1.  **Infrastructure (@sec-compute)**: The silicon, power, and cooling systems of the AI Supercomputer.

2.  **Networking (@sec-networking)**: The "Gradient Bus" (InfiniBand, RoCE) that connects the fleet.

3.  **Storage (@sec-storage)**: The data hierarchy (NVMe to Object Store) that feeds the beast.

4.  **Orchestration (@sec-orchestration-resource-management)**: The "Brain" that manages resources and scheduling.
