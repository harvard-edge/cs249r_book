# Principles of Inference {.unnumbered}

These principles govern the economics and performance of serving models at scale. They explain why inference requires different optimizations than training.

::: {#nte-autoregressive-bottleneck .callout-principle icon=false title="The Autoregressive Bottleneck"}
**The Principle**: In generative models, the "Decode" phase is strictly **memory-bandwidth bound** because the entire model weight set must be loaded for *every single token generated*.

**The Engineering Implication**:
Throughput scales with batch size (sharing weight loads across multiple requests), not compute power. Techniques like **Continuous Batching** and **PagedAttention** are essential to maximize memory bandwidth utilization.
:::

::: {#nte-serving-cost-dominance .callout-principle icon=false title="The Serving Cost Dominance Law"}
**The Principle**: Over a successful model's lifetime, Inference OpEx (Operational Expenditure) exceeds Training CapEx (Capital Expenditure) by 100â€“1000 $\times$.

**The Engineering Implication**:
Inference efficiency is the primary economic driver of ML systems. Optimization efforts (Quantization, Distillation) should be disproportionately focused on the inference path, even if they increase training cost.
:::

::: {#nte-power-two-choices .callout-principle icon=false title="The Power of Two Choices (P2C)"}
**The Principle**: When load balancing, querying just two random replicas and selecting the least-loaded one exponentially reduces tail latency compared to random selection.
$$ O(\log n) \to O(\log \log n) $$

**The Engineering Implication**:
Simple, randomized load balancing algorithms are surprisingly effective at scale. P2C provides a near-optimal trade-off between load distribution quality and coordination overhead.
:::
