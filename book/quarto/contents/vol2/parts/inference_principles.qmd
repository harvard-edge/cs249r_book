# Principles of Inference {.unnumbered}

These principles govern the economics and performance of serving models at scale. They explain why inference requires different optimizations than training.

::: {.callout-note icon=false title="The Autoregressive Bottleneck"}
**The Law**: In generative models, the "Decode" phase is strictly **memory-bandwidth bound** because the entire model weight set must be loaded for *every single token generated*.

**The Engineering Implication**:
Throughput scales with batch size (sharing weight loads across multiple requests), not compute power. Techniques like **Continuous Batching** and **PagedAttention** are essential to maximize memory bandwidth utilization.
:::

::: {.callout-note icon=false title="The Serving Cost Dominance Law"}
**The Law**: Over a successful model's lifetime, Inference OpEx (Operational Expenditure) exceeds Training CapEx (Capital Expenditure) by 100â€“1000$\times$.

**The Engineering Implication**:
Inference efficiency is the primary economic driver of ML systems. Optimization efforts (Quantization, Distillation) should be disproportionately focused on the inference path, even if they increase training cost.
:::

::: {.callout-note icon=false title="The Power of Two Choices (P2C)"}
**The Law**: When load balancing, querying just two random replicas and selecting the least-loaded one exponentially reduces tail latency compared to random selection.
$$ O(\log n) \to O(\log \log n) $$

**The Engineering Implication**:
Simple, randomized load balancing algorithms are surprisingly effective at scale. P2C provides a near-optimal trade-off between load distribution quality and coordination overhead.
:::
