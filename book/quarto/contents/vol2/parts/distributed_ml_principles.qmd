# Part II: Distributed ML {.unnumbered}

Coordinating the **Fleet** requires solving problems that a single machine does not. If Part I built the physical substrate, Part II establishes the **Logic of Distribution**: the algorithms, protocols, and coordination strategies that transform a collection of independent accelerators into a singular, unified machine. This transition is governed by the **physics of distributed learning**—the first-principles reasoning that explains why adding more GPUs does not always yield linear speedups, and why communication, not computation, is often the hidden bottleneck.

At this scale, the engineering challenge is a trade-off between parallelization and coordination. We can partition a model to reduce per-device memory pressure, but we necessarily increase the communication volume required to keep the weights synchronized. We can scale to thousands of nodes to reduce training time, but we simultaneously decrease the Mean Time Between Failures (MTBF), making fault tolerance a mandatory system component rather than an operational luxury. These principles quantify the "Communication Tax" and the "Reliability Tax" of scale.

::: {#nte-universal-scaling .callout-principle icon=false title="The Universal Scaling Law"}
**The Invariant**: Model performance ($L$) improves as a power-law function of compute ($C$), dataset size ($D$), and parameters ($N$).
$$ L(C) \propto C^{-\alpha} $$

**The Implication**: Scale is not an option; it is a requirement for capability. To achieve a 10$\times$ improvement in performance, you typically need a 100$\times$–1000$\times$ increase in compute. This exponential hunger drives the transition from single-server training to warehouse-scale clusters.
:::

::: {#nte-distributed-step-time .callout-principle icon=false title="The Distributed Step Time Law (Iron Law of Scale)"}
**The Invariant**: The time to complete one training step is the sum of computation and non-overlapped communication.
$$ T_{\text{step}}(N) = \frac{T_{\text{compute}}}{N} + T_{\text{comm}}(N) - T_{\text{overlap}} $$

**The Implication**: Scaling is a race between parallelizable compute (which shrinks with $N$) and communication overhead (which grows or stays constant). To scale efficiently, algorithms must minimize $T_{\text{comm}}$ or maximize $T_{\text{overlap}}$ (e.g., through **Gradient Accumulation** or **Communication Hiding**).
:::

::: {#nte-alpha-beta-model .callout-principle icon=false title="The Bandwidth-Latency Trade-off ($\alpha$-$\beta$ Model)"}
**The Invariant**: Communication time is a function of fixed latency ($\alpha$) and message-dependent bandwidth ($\beta$).
$$ T(n) = \alpha + \frac{n}{\beta} $$

**The Implication**: Small messages (e.g., MoE routing, pipeline bubbles) are **latency-bound**; large messages (e.g., gradients) are **bandwidth-bound**. Optimization strategies must match the regime: fuse small messages to amortize $\alpha$, compress large messages to improve $\beta$.
:::

::: {#nte-young-daly .callout-principle icon=false title="The Young-Daly Checkpoint Law"}
**The Invariant**: The optimal checkpoint frequency balances the cost of writing the checkpoint ($T_{write}$) against the expected cost of reworking lost progress due to failures ($\text{MTBF}$).
$$ \tau_{opt} = \sqrt{2 \cdot T_{write} \cdot \text{MTBF}} $$

**The Implication**: Checkpointing is not "free." As cluster size grows, MTBF drops, forcing more frequent checkpoints. This demands high-bandwidth storage (Burst Buffers) to prevent I/O from dominating training time.
:::

## Part VI Roadmap: The Logic of Distribution {#sec-principles-distributed-ml-part-vi-roadmap-logic-distribution-b4e2}

Part VI establishes how we make the Fleet compute:

1.  **Distributed Training (@sec-distributed-training-systems)**: The strategies (Data, Tensor, Pipeline, Expert Parallelism) for partitioning the workload across the fleet.
2.  **Collective Communication (@sec-collective-communication)**: The collective operations (AllReduce, AllGather, AllToAll) that bind the fleet together.
3.  **Fault Tolerance (@sec-fault-tolerance-reliability)**: The mechanisms for surviving the inevitable failures of scale.
4.  **Fleet Orchestration (@sec-fleet-orchestration)**: The schedulers and resource managers (Slurm, Kubernetes) that allocate the fleet to workloads.
