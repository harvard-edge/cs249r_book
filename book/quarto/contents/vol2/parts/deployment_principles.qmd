# Part III: Deployment at Scale {.unnumbered}

Parts I and II built the Fleet and trained the model. Now the system must "earn its keep." Part III shifts the focus from the single, massive training run to the **Global Inference Fleet**: the thousands of geographically distributed servers and billions of edge devices that serve model outputs to users in real-time. This transition from training to production fundamentally changes the engineering constraints: from maximizing throughput on a static dataset to minimizing latency on a dynamic stream of requests.

In this regime, engineering becomes a negotiation with the user's experience. We can optimize for high batch throughput in the datacenter, but we risk violating the user's latency budget. We can deploy a model to the edge to eliminate network latency, but we are then constrained by the tight memory and power envelopes of mobile and IoT hardware. These principles govern the performance engineering and operational economics of deploying intelligence at scale.

::: {#nte-autoregressive-bottleneck .callout-principle icon=false title="The Autoregressive Bottleneck"}
**The Invariant**: In generative models, the "Decode" phase is strictly **memory-bandwidth bound** because the entire model weight set must be loaded for *every single token generated*.

**The Implication**: Throughput scales with batch size (sharing weight loads across multiple requests), not compute power. Techniques like **Continuous Batching**, **PagedAttention**, and **KV Cache Compression** are essential to maximize memory bandwidth utilization.
:::

::: {#nte-serving-cost-dominance .callout-principle icon=false title="The Serving Cost Dominance Law"}
**The Invariant**: Over a successful model's lifetime, Inference OpEx (Operational Expenditure) exceeds Training CapEx (Capital Expenditure) by 100$\times$–1000$\times$.

**The Implication**: Inference efficiency is the primary economic driver of ML systems. Optimization efforts (Quantization, Distillation, Sparsity) should be disproportionately focused on the inference path, even if they increase training cost or complexity.
:::

::: {#nte-power-two-choices .callout-principle icon=false title="The Power of Two Choices (P2C)"}
**The Invariant**: When load balancing, querying just two random replicas and selecting the least-loaded one exponentially reduces tail latency compared to random selection.
$$ O(\log n) \to O(\log \log n) $$

**The Implication**: Simple, randomized load balancing algorithms are surprisingly effective at scale. P2C provides a near-optimal trade-off between load distribution quality and coordination overhead, helping to tame the **Tail at Scale**.
:::

## Part III Roadmap: From Training to Production {#sec-principles-deployment-part-iii-roadmap-training-production-c7d3}

Part III takes the trained model from the cluster to the world:

1.  **Performance Engineering (@sec-performance-engineering)**: The efficiency frontier — precision engineering, FlashAttention, kernel fusion, compilation pipelines, and speculative decoding.
2.  **Inference at Scale (@sec-inference-scale)**: Serving models to millions of users — batching, KV cache management, PagedAttention, and model sharding for latency-sensitive workloads.
3.  **Edge Intelligence (@sec-edge-intelligence)**: Deploying ML where the data lives — resource-constrained devices, on-device inference, and federated learning.
4.  **Operations at Scale (@sec-ops-scale)**: Running the fleet in production — monitoring, observability, platform engineering, and compound AI orchestration.
