# Part VII: Deployment at Scale {.unnumbered}

Parts I and II built the fleet and trained the model. Now it must earn its keep. These principles govern the economics and performance of deploying models at scale — from datacenter inference to edge devices. They explain why serving requires fundamentally different optimizations than training.

::: {#nte-autoregressive-bottleneck .callout-principle icon=false title="The Autoregressive Bottleneck"}
**The Principle**: In generative models, the "Decode" phase is strictly **memory-bandwidth bound** because the entire model weight set must be loaded for *every single token generated*.

**The Engineering Implication**:
Throughput scales with batch size (sharing weight loads across multiple requests), not compute power. Techniques like **Continuous Batching** and **PagedAttention** are essential to maximize memory bandwidth utilization.
:::

::: {#nte-serving-cost-dominance .callout-principle icon=false title="The Serving Cost Dominance Law"}
**The Principle**: Over a successful model's lifetime, Inference OpEx (Operational Expenditure) exceeds Training CapEx (Capital Expenditure) by 100–1000$\times$.

**The Engineering Implication**:
Inference efficiency is the primary economic driver of ML systems. Optimization efforts (Quantization, Distillation) should be disproportionately focused on the inference path, even if they increase training cost.
:::

::: {#nte-power-two-choices .callout-principle icon=false title="The Power of Two Choices (P2C)"}
**The Principle**: When load balancing, querying just two random replicas and selecting the least-loaded one exponentially reduces tail latency compared to random selection.
$$ O(\log n) \to O(\log \log n) $$

**The Engineering Implication**:
Simple, randomized load balancing algorithms are surprisingly effective at scale. P2C provides a near-optimal trade-off between load distribution quality and coordination overhead.
:::

## Part III Roadmap: From Training to Production {#sec-principles-deployment-part-iii-roadmap-training-production-c7d3}

Part III takes the trained model from the cluster to the world:

1.  **Performance Engineering (@sec-performance-engineering)**: The efficiency frontier — precision engineering, FlashAttention, kernel fusion, compilation pipelines, and speculative decoding.
2.  **Inference at Scale (@sec-inference-scale)**: Serving models to millions of users — batching, KV cache management, PagedAttention, and model sharding for latency-sensitive workloads.
3.  **Edge Intelligence (@sec-edge-intelligence)**: Deploying ML where the data lives — resource-constrained devices, on-device inference, and federated learning.
4.  **Operations at Scale (@sec-ops-scale)**: Running the fleet in production — monitoring, observability, platform engineering, and compound AI orchestration.
