---
bibliography: distributed_training.bib
---

# Distributed Training Systems {#sec-distributed-training}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A technical illustration showing multiple interconnected GPU clusters working in coordination to train a massive neural network. The scene depicts hundreds of GPU nodes arranged in a circular pattern, connected by luminous data streams representing gradient synchronization. At the center, a giant neural network model spans across all nodes, with each node responsible for a different portion. Visual elements include ring allreduce patterns showing data flowing between nodes, pipeline stages depicted as sequential processing units, and synchronization barriers represented as glowing checkpoints. The color palette uses deep blues and electric purples for computation, with bright orange and gold for communication paths. The style is technical and precise, suitable for an advanced distributed systems textbook._
:::

\noindent
![](images/png/cover_distributed.png)

:::

## Purpose {.unnumbered}

_What makes distributed training a fundamental shift in how we approach machine learning at scale, rather than merely a technical optimization?_

Distributed training becomes an architectural necessity when training demands exceed the capabilities of individual machines, and the principles governing coordination across distributed systems fundamentally reshape how we approach optimization, fault tolerance, and cost efficiency. Moving from single-machine to distributed training requires understanding how communication overhead, synchronization costs, and system heterogeneity create entirely new constraints that affect algorithm design and system architecture. The challenges of distributed training, including parameter synchronization across thousands of devices, absorbing failures without losing progress, and balancing communication with computation, demand different engineering solutions than single-device scenarios. As models grow to billions of parameters and training datasets expand to continental scales, distributed training becomes the foundation upon which feasible machine learning systems are built, directly determining which problems can be tackled, which timelines are realistic, and which organizations can meaningfully participate in advanced AI development.

::: {.callout-tip title="Learning Objectives"}

- Implement data parallel training using gradient synchronization and AllReduce algorithms to achieve 85-95% parallel efficiency in the linear scaling regime (2-32 GPUs)

- Apply memory-efficient data parallelism techniques (ZeRO stages, FSDP) to reduce per-GPU memory requirements while quantifying communication trade-offs

- Design tensor parallelism strategies for transformer layers by partitioning matrix operations and placing AllReduce operations to minimize communication overhead

- Construct pipeline parallelism schedules with microbatching to reduce bubble overhead, calculating the trade-off between pipeline depth and memory requirements

- Select synchronization models (BSP, SSP, asynchronous) based on cluster heterogeneity and convergence requirements, quantifying staleness impacts

- Combine data, tensor, and pipeline parallelism in hybrid configurations for specific model architectures, balancing memory distribution and communication efficiency

- Measure distributed training efficiency using scaling metrics, communication-computation ratios, and bandwidth utilization to identify bottlenecks

:::

## Multi-Machine Scaling Fundamentals {#sec-distributed-training-multimachine-scaling-fundamentals}

Training frontier models requires computational resources that far exceed any single device. When a model requires $10^{25}$ FLOPS to train, or when its parameters consume terabytes of memory, distribution becomes a necessity, not an optimization. This chapter establishes the algorithmic foundations for scaling model training across multiple machines. By understanding how to partition computation and synchronize state, we define the workload requirements that will subsequently drive the physical infrastructure design examined in Part II.

The transition from single-machine to distributed training represents a major shift in optimization strategy and system complexity. Single-machine optimization focuses on efficiently utilizing available resources through techniques such as prefetching, mixed precision[^fn-mixed-precision], and gradient accumulation. Distributed training introduces different challenges in communication overhead, fault tolerance, and synchronization that require new conceptual frameworks and engineering approaches.

### Multi-Machine Training Requirements {#sec-distributed-training-multimachine-training-requirements}

Three concrete signals indicate when distributed training becomes necessary rather than merely beneficial. First, **memory exhaustion** occurs when model parameters, optimizer states, and activation storage exceed single-device capacity. It is critical to distinguish between static memory, which includes parameters and optimizer states and remains constant per batch, and dynamic memory for activations, which scales with batch size and sequence length. For long-context LLMs, activation memory often dominates. Techniques like **gradient checkpointing**[^fn-gradient-checkpointing] address this by discarding intermediate activations during the forward pass and recomputing them during the backward pass, trading 30% more compute for O(sqrt(n)) memory reduction. Even with these optimizations, models beyond 10-20 billion parameters typically exceed the 40-80GB HBM capacity of modern GPUs.

[^fn-gradient-checkpointing]: Gradient checkpointing trades approximately 30% additional compute for reduced activation memory, from O(n) to O(sqrt(n)) for n layers.

[^fn-mixed-precision]: **Mixed Precision Training**: Using lower-precision formats (FP16 or BF16) for forward/backward computation while maintaining FP32 master weights for updates. Halves memory usage and doubles compute throughput on modern GPUs with tensor cores. Introduced by NVIDIA in 2017, mixed precision is now standard practice for training large models.

For transformer models, this threshold typically occurs around 10-20 billion parameters on current generation GPUs with 40-80GB memory [@rajbhandari2020zero]. Second, unacceptable training duration emerges when single-device training would require weeks or months to converge, making iteration impossible. Training GPT-3 on a single V100 GPU would require approximately 355 years [@brown2020language], making distributed approaches not optional but essential. Third, dataset scale exceeds single-machine storage when training data reaches multiple terabytes, as occurs in large-scale vision or language modeling tasks.

### Distributed Training Complexity Trade-offs {#sec-distributed-training-complexity-tradeoffs}

Distributed training introduces three primary complexity dimensions absent from single-machine scenarios. Communication overhead emerges from gradient synchronization, where each training step must aggregate gradients across all devices. For a model with $N$ parameters distributed across $D$ devices, all-reduce operations must transfer approximately $2N(D-1)/D$ bytes per step. On commodity network infrastructure with 10-100 Gbps bandwidth, this communication can dominate computation time for models under 1 billion parameters [@sergeev2018horovod]. Fault tolerance requirements increase exponentially with cluster size. A 100-node cluster with 99.9% per-node reliability experiences failures every few hours on average, necessitating checkpoint and recovery mechanisms. Algorithmic considerations change because distributed training alters optimization dynamics. Large batch sizes from data parallelism affect convergence behavior, requiring learning rate scaling and warmup strategies that single-machine training does not require [@goyal2017accurate].

### Single-Machine to Distributed Transition {#sec-distributed-training-singlemachine-distributed-transition}

The systematic optimization methodology established for single-machine training extends to distributed environments with important adaptations. Profiling must now capture inter-device communication patterns and synchronization overhead in addition to computation and memory metrics. Tools like NVIDIA Nsight Systems and PyTorch's distributed profiler reveal whether training is communication-bound or computation-bound, guiding the choice between parallelization strategies. The solution space expands from single-machine techniques to include data parallelism for distributing training examples, model parallelism for distributing model parameters, pipeline parallelism for distributing model layers, and hybrid approaches that combine multiple strategies. The principles remain consistent in identifying bottlenecks, selecting appropriate techniques, and composing solutions, but the implementation complexity increases substantially.

::: {.callout-note title="Figure Placeholder: 3D Parallelism Cube" collapse="true"}
```{.tikz}
% TODO: 3D Cube showing Data, Tensor, and Pipeline axes
\node[draw, align=center] {3D Parallelism\nData x Tensor x Pipeline};
```
**The 3D Parallelism Cube**. A conceptual visualization of the three orthogonal scaling axes: Data Parallelism (replicating the model), Tensor Parallelism (splitting layers), and Pipeline Parallelism (splitting depth). Production training for models like GPT-4 occupies a specific point $(d, t, p)$ within this cube to balance memory usage, compute efficiency, and communication overhead.
:::

## Distributed Training Fundamentals {#sec-distributed-training-fundamentals}

The single-machine optimization techniques discussed in the previous section, such as prefetching that eliminates data loading bottlenecks, mixed-precision that doubles compute throughput, and gradient accumulation that simulates larger batches, each extract more performance from existing hardware. When these optimizations plateau, scaling horizontally becomes the path forward. Distributed training extends systematic optimization to multiple machines, introducing coordination challenges that require new conceptual frameworks.

::: {.callout-definition title="Distributed Training"}

***Distributed Training*** is the parallelization of model training across _multiple compute devices_ through coordinated _data partitioning_ and _gradient synchronization_, enabling training of models that exceed single-device memory or time constraints.

:::

The progression from single-machine to distributed training follows a natural scaling path of optimizing locally first, then scaling horizontally. This progression ensures that distributed systems operate efficiently at each node while adding the coordination mechanisms necessary for multi-machine training. Training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training[^fn-distributed-training] addresses this challenge by spreading the workload across multiple machines that coordinate to train a single model efficiently.

[^fn-distributed-training]: **Distributed Training**: Google's DistBelief [@dean2012distbelief] pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod [@sergeev2018horovod] and PyTorch's DistributedDataParallel, democratizing distributed training for researchers worldwide.

This coordination relies on consensus protocols and synchronization primitives to ensure parameter updates remain consistent across nodes. While basic barrier synchronization suffices for research, production deployments require careful fault tolerance, checkpointing, and recovery mechanisms covered in @sec-fault-tolerance.

With these coordination mechanisms in place, practitioners follow a systematic progression from single-device to distributed training, with each stage building upon the previous level's challenges. Single GPU training requires only local memory management and straightforward forward/backward passes, establishing the baseline computational patterns. Scaling to multiple GPUs within a single node introduces high-bandwidth communication requirements, typically handled through NVLink[^nvlink] or PCIe connections with NCCL[^nccl] optimization while preserving the single-machine simplicity of fault tolerance and scheduling.

[^nvlink]: NVLink provides 600 GB/s bidirectional bandwidth, enabling efficient gradient synchronization within nodes. See @sec-infrastructure for detailed coverage of GPU interconnect architectures.

[^nccl]: **NCCL (NVIDIA Collective Communications Library)**: Optimized library implementing efficient collective operations (AllReduce, Broadcast, etc.) for multi-GPU and multi-node communication. Automatically selects optimal communication patterns based on hardware topology, critical for distributed training performance.

The leap to multi-node distributed training brings substantially greater complexity in network communication overhead, fault tolerance requirements, and cluster orchestration challenges. Each scaling stage compounds the previous challenges as communication bottlenecks intensify, synchronization overhead grows, and failure probability increases. This progression underscores why practitioners should optimize single-GPU performance before scaling to ensure efficient resource utilization at each level.

::: {.callout-note title="Practical Distributed Training Complexity"}

While frameworks like PyTorch FSDP and DeepSpeed abstract away much of the complexity, implementing distributed training efficiently remains a significant engineering challenge. Production deployments require careful network configuration such as InfiniBand, infrastructure management through systems like Kubernetes or Slurm, and debugging of complex non-local issues like synchronization hangs or communication bottlenecks. For most teams, leveraging managed cloud services is often more practical than building and maintaining this infrastructure from scratch.

:::

The distributed training process itself involves splitting the dataset into non-overlapping subsets, assigning each subset to a different GPU, and performing forward and backward passes independently on each device. Once gradients are computed on each GPU, they are synchronized and aggregated before updating the model parameters, ensuring that all devices learn in a consistent manner. @fig-distributed-training illustrates this process, showing how input data is divided, assigned to multiple GPUs for computation, and later synchronized to update the model collectively.

::: {#fig-distributed-training fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{%
  mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,
minimum width=20mm,minimum height=9mm,line width=1pt},
  mycycle/.style={circle, draw=none, fill=red, minimum width=5mm},
  myline/.style={line width=1.15pt,draw=cyan},
%
  Box/.style={align= flush center,
    inner xsep=2pt,
    draw=RedLine,
    line width=0.75pt,
    fill=RedL!20,
    text width=22mm,
    minimum width=22mm, minimum height=8mm
  },
%
Line/.style={line width=1.0pt,black!50}
}

\begin{scope}[node distance=-1.7,local bounding box = SC1]]
\node[mycylinder,fill=red!30] (A) {};
\scoped[on background layer]
\node[mycylinder, above=of A,fill=red!50] (C) {};
\node[mycylinder, below=of A,fill=red!10] (B) {};
\end{scope}

\begin{scope}[node distance=0.2,shift={(3.5,5))},local bounding box = SC2]
\node[mycycle] (C1) {};
\node[mycycle,below=of C1] (C2) {};
\node[mycycle,below=of C2] (C3) {};
\node[mycycle,below=of C3] (C4) {};
\node[mycycle,fill=violet,left=0.6 of $(C1)!0.5!(C2)$] (CL1) {};
\node[mycycle,fill=violet,left=0.6 of $(C2)!0.5!(C3)$] (CL2) {};
\node[mycycle,fill=violet,left=0.6 of $(C3)!0.5!(C4)$] (CL3) {};
%
\node[mycycle,fill=green,right=0.6 of $(C1)!0.4!(C3)$] (CD1) {};
\node[mycycle,fill=green,right=0.6 of $(C2)!0.6!(C4)$] (CD2) {};
%
\foreach \x in {1,2,3,4} {
  \foreach \y in {CL1, CL2, CL3, CD1, CD2} {
    \draw[myline] (\y) -- (C\x);
  }
}
\node[Box,below=0.8 of C4](B1){GPU 1};
\draw[myline,dashed](C4)--(B1);
\end{scope}

\begin{scope}[node distance=0.2,shift={(11.5,5))},local bounding box = SC3]
\node[mycycle] (3C1) {};
\node[mycycle,below=of 3C1] (3C2) {};
\node[mycycle,below=of 3C2] (3C3) {};
\node[mycycle,below=of 3C3] (3C4) {};
\node[mycycle,fill=violet,right=0.6 of $(3C1)!0.5!(3C2)$] (3CL1) {};
\node[mycycle,fill=violet,right=0.6 of $(3C2)!0.5!(3C3)$] (3CL2) {};
\node[mycycle,fill=violet,right=0.6 of $(3C3)!0.5!(3C4)$] (3CL3) {};
%
\node[mycycle,fill=green,left=0.6 of $(3C1)!0.4!(3C3)$] (3CD1) {};
\node[mycycle,fill=green,left=0.6 of $(3C2)!0.6!(3C4)$] (3CD2) {};
%
\foreach \x in {1,2,3,4} {
  \foreach \y in {3CL1, 3CL2, 3CL3, 3CD1, 3CD2} {
    \draw[myline] (\y) -- (3C\x);
  }
}

\node[Box,below=0.8 of 3C4](3B1){GPU 1};
\draw[myline,dashed](3C4)--(3B1);
\end{scope}

\begin{scope}[node distance=0.2,shift={(20.0,5))},local bounding box = SC4]
\node[mycycle] (4C1) {};
\node[mycycle,below=of 4C1] (4C2) {};
\node[mycycle,below=of 4C2] (4C3) {};
\node[mycycle,below=of 4C3] (4C4) {};
\node[mycycle,fill=violet,left=0.6 of $(4C1)!0.5!(4C2)$] (4CL1) {};
\node[mycycle,fill=violet,left=0.6 of $(4C2)!0.5!(4C3)$] (4CL2) {};
\node[mycycle,fill=violet,left=0.6 of $(4C3)!0.5!(4C4)$] (4CL3) {};
%
\node[mycycle,fill=green,right=0.6 of $(4C1)!0.4!(4C3)$] (4CD1) {};
\node[mycycle,fill=green,right=0.6 of $(4C2)!0.6!(4C4)$] (4CD2) {};
%
%
\foreach \x in {1,2,3,4} {
  \foreach \y in {4CL1, 4CL2, 4CL3, 4CD1, 4CD2} {
    \draw[myline] (\y) -- (4C\x);
  }
}
\node[Box,below=0.8 of 4C4](4B1){GPU 1};
\draw[myline,dashed](4C4)--(4B1);
\end{scope}
\coordinate(X)at($(CD1)!0.5!(CD2)$);
\coordinate(Y)at($(3CD1)!0.5!(3CD2)$);

\node[fill=white,minimum height=45](ER)at($(X)!0.3!(Y)$){Error};
\node[fill=white,align=center,minimum height=45](CO)at($(X)!0.7!(Y)$){Compute\\ loss\\ function};
\draw[myline,-latex,shorten <=3mm](X)--(ER.west);
\draw[myline,-latex](ER.east)--(CO.west);
\draw[myline,-latex,shorten >=3mm](CO.east)--(Y);
\draw[myline,dashed](CO.south)--++(270:1)-|node[fill=white,align=center,
pos=0.25](COM){Compare\\ predicted\\ label with\\ annotation}
(ER.south);

\node[fill=white,align=center,minimum height=45](OP)at($(3CL2)!0.7!(4CL2)$){Avg\\ global\\ gradient};
\draw[myline,latex-,shorten <=1mm](4CL2)--(OP.east);
%
\draw[myline,latex-,shorten <=3mm,shorten >=3mm](CL2)--(SC1.east|-CL2);
%
\draw[myline,latex-,shorten <=3mm,shorten >=3mm](CL2)-|node[fill=white,pos=0.75]{Chunk}(SC1.north);
%
\path[myline,draw=none,dashed](OP.north west)--++(90:1.2)coordinate(OP1);
\draw[myline,dashed]($(ER.north east)!0.5!(CO.north west)$)--++(90:1.2)coordinate(ER1);
\coordinate (C) at ($(OP1) + (0,5mm)$);
\coordinate (B) at ($(ER1) + (0,5mm)$);
\path[red](C)-|coordinate(D1)(4CD1);
\path[red](B)-|coordinate(A1)(SC1);
\coordinate (D) at ($(D1) + (15mm,0)$);
\coordinate (A) at ($(A1) + (-15mm,0)$);
\draw[myline,dashed,shorten >=3mm,shorten <=3mm](B)--
node[fill=white]{Step 2 -- Compute gradients}(C);
\draw[myline,dashed,shorten >=3mm,pos=0.46](C)--
node[fill=white]{Step 3 -- Update Parameters}(D);
\draw[myline,dashed,shorten >=3mm,pos=0.46](B)--
node[fill=white]{Step 1 -- Predict a label}(A);

\node[above=0.2 of SC2]{Forward pass};
\node[above=0.2 of SC3]{Backward pass};
%%%%%%%%%%%%%%%%%%%%%%%
%down
%%%%%%%%%%%%%%%%%%%%%%%
\begin{scope}[node distance=0.2,shift={(3.5,-2))},local bounding box = DSC2]
\node[mycycle] (DC1) {};
\node[mycycle,below=of DC1] (DC2) {};
\node[mycycle,below=of DC2] (DC3) {};
\node[mycycle,below=of DC3] (DC4) {};
\node[mycycle,fill=violet,left=0.6 of $(DC1)!0.5!(DC2)$] (DCL1) {};
\node[mycycle,fill=violet,left=0.6 of $(DC2)!0.5!(DC3)$] (DCL2) {};
\node[mycycle,fill=violet,left=0.6 of $(DC3)!0.5!(DC4)$] (DCL3) {};
%
\node[mycycle,fill=green,right=0.6 of $(DC1)!0.4!(DC3)$] (DCD1) {};
\node[mycycle,fill=green,right=0.6 of $(DC2)!0.6!(DC4)$] (DCD2) {};
%
\foreach \x in {1,2,3,4} {
  \foreach \y in {DCL1, DCL2, DCL3, DCD1, DCD2} {
    \draw[myline] (\y) -- (DC\x);
  }
}
\node[Box,above=0.8 of DC1](DB1){GPU 2};
\draw[myline,dashed](DC1)--(DB1);
\end{scope}

\begin{scope}[node distance=0.2,shift={(11.5,-2))},local bounding box = DSC3]
\node[mycycle] (D3C1) {};
\node[mycycle,below=of D3C1] (D3C2) {};
\node[mycycle,below=of D3C2] (D3C3) {};
\node[mycycle,below=of D3C3] (D3C4) {};
\node[mycycle,fill=violet,right=0.6 of $(D3C1)!0.5!(D3C2)$] (D3CL1) {};
\node[mycycle,fill=violet,right=0.6 of $(D3C2)!0.5!(D3C3)$] (D3CL2) {};
\node[mycycle,fill=violet,right=0.6 of $(D3C3)!0.5!(D3C4)$] (D3CL3) {};
%
\node[mycycle,fill=green,left=0.6 of $(D3C1)!0.4!(D3C3)$] (D3CD1) {};
\node[mycycle,fill=green,left=0.6 of $(D3C2)!0.6!(D3C4)$] (D3CD2) {};
%
\foreach \x in {1,2,3,4} {
  \foreach \y in {D3CL1, D3CL2, D3CL3, D3CD1, D3CD2} {
    \draw[myline] (\y) -- (D3C\x);
  }
}

\node[Box,above=0.8 of D3C1](D3B1){GPU 2};
\draw[myline,dashed](D3C1)--(D3B1);
\end{scope}

\begin{scope}[node distance=0.2,shift={(20.0,-2))},local bounding box = DSC4]
\node[mycycle] (D4C1) {};
\node[mycycle,below=of D4C1] (D4C2) {};
\node[mycycle,below=of D4C2] (D4C3) {};
\node[mycycle,below=of D4C3] (D4C4) {};
\node[mycycle,fill=violet,left=0.6 of $(D4C1)!0.5!(D4C2)$] (D4CL1) {};
\node[mycycle,fill=violet,left=0.6 of $(D4C2)!0.5!(D4C3)$] (D4CL2) {};
\node[mycycle,fill=violet,left=0.6 of $(D4C3)!0.5!(D4C4)$] (D4CL3) {};
%
\node[mycycle,fill=green,right=0.6 of $(D4C1)!0.4!(D4C3)$] (D4CD1) {};
\node[mycycle,fill=green,right=0.6 of $(D4C2)!0.6!(D4C4)$] (D4CD2) {};
%
\foreach \x in {1,2,3,4} {
  \foreach \y in {D4CL1, D4CL2, D4CL3, D4CD1, D4CD2} {
    \draw[myline] (\y) -- (D4C\x);
  }
}
\node[Box,above=0.8 of D4C1](D4B1){GPU 2};
\draw[myline,dashed](D4C1)--(D4B1);
\end{scope}
%%%%%
\coordinate(DX)at($(DCD1)!0.5!(DCD2)$);
\coordinate(DY)at($(D3CD1)!0.5!(D3CD2)$);

\node[fill=white,minimum height=45](DER)at($(DX)!0.3!(DY)$){Error};
\node[fill=white,align=center,minimum height=45](DCO)at($(DX)!0.7!(DY)$){Compute\\ loss\\ function};
\draw[myline,-latex,shorten <=3mm](DX)--(DER.west);
\draw[myline,-latex](DER.east)--(DCO.west);
\draw[myline,-latex,shorten >=3mm](DCO.east)--(DY);
\draw[myline,dashed](DCO.north)--++(90:1)-|node[fill=white,align=center,
pos=0.25](DCOM){Compare\\ predicted\\ label with\\ annotation}(DER.north);

\node[fill=white,align=center,minimum height=45](DOP)at($(D3CL2)!0.7!(D4CL2)$){Avg\\ global\\ gradient};
\draw[myline,latex-,shorten <=1mm](D4CL2)--(DOP.east);
%
\draw[myline,latex-,shorten <=3mm,shorten >=3mm](DCL2)-|
node[fill=white,pos=0.75]{Chunk}(SC1.south);
%
\node[below=0.2 of DSC2]{Forward pass};
\node[below=0.2 of DSC3]{Backward pass};
%%%
\coordinate(S1)at($(3B1)!0.5!(4B1)$);
\coordinate(S2)at($(D3B1)!0.5!(D4B1)$);
\coordinate(S)at($(S1)!0.5!(S2)$);

\node[draw=none,fill=green!50!black!90,text=white,inner xsep=10pt,
             inner ysep=9pt, outer sep=5pt](CGG)at(S){\textbf{Calculate Global Gradients}};
%
\draw[myline,shorten <=1mm](OP.west)-|(CGG.80);
\draw[myline,-latex,shorten <=2mm](3CL2)-|(CGG.130);
%
\draw[myline,shorten <=1mm](DOP.west)-|(CGG.280);
\draw[myline,-latex,shorten <=2mm](D3CL2)-|(CGG.230);
 \end{tikzpicture}
```
Distributed machine learning scales model training by partitioning datasets across multiple GPUs, enabling concurrent computation of gradients, and then aggregating these gradients to update shared model parameters. This approach accelerates training through parallel processing while maintaining model consistency across all devices.
:::

This coordination introduces several key challenges that distributed training systems must address. A distributed training system must orchestrate multi-machine computation by splitting up the work, managing communication between machines, and maintaining synchronization throughout the training process. The AllReduce operations that aggregate gradients across devices, for example, consume 10-40% of total training time even with optimal implementation, and this overhead compounds as systems scale.

Understanding these basic requirements provides the foundation for examining the main approaches to distributed training. Each approach addresses different constraints, and understanding their trade-offs enables practitioners to select appropriate strategies for their specific workloads. Data parallelism divides the training data across machines while each maintains a full model copy, making it the simplest approach and effective for models that fit in single-device memory. Model parallelism splits the model itself across devices when parameters exceed single-device memory, addressing the memory constraint that data parallelism cannot solve. Pipeline parallelism partitions models into sequential stages that process microbatches concurrently, improving utilization over naive model parallelism. Hybrid approaches integrate multiple strategies, enabling training at scales where any single approach would fail. The following sections examine each approach in this progression: we begin with efficiency metrics that govern all distributed training decisions, then explore data parallelism as the foundational technique, followed by model and pipeline parallelism for memory-constrained scenarios, and finally hybrid approaches that combine these strategies for frontier-scale training.

## Distributed Training Efficiency Metrics {#sec-distributed-training-efficiency-metrics}

Before examining specific parallelism strategies, understanding the quantitative metrics that govern distributed training efficiency is essential. These metrics provide the foundation for making informed decisions about scaling strategies, hardware selection, and optimization approaches.

Communication overhead represents the primary bottleneck in distributed training systems. AllReduce operations consume 10-40% of total training time in data parallel systems, and this overhead increases significantly at scale. BERT-Large on 128 GPUs experiences communication overhead reaching 35% of total runtime, while GPT-3 scale models experience 55% overhead on 1,024 GPUs.

::: {.callout-note title="AllReduce Communication Complexity"}
AllReduce complexity depends on two components: latency ($\alpha$) and bandwidth ($\beta$). Ring AllReduce achieves bandwidth-optimal communication with $(N-1)/N$ utilization, while tree-based approaches offer lower latency at $O(\log N)$ steps. The choice depends on message size: tree wins for latency-dominated small messages, ring wins for bandwidth-dominated large gradients. Modern implementations like NCCL use hierarchical algorithms that combine tree latency within nodes and ring bandwidth between nodes. For detailed algorithm analysis including complexity formulas, hierarchical variants, and topology-aware optimizations, see @sec-communication.
:::

Interconnect selection is critical for large-scale deployments, as these complexity characteristics determine when communication dominates computation.

The bandwidth requirements for efficient distributed training are substantial, particularly for transformer models. Efficient systems require 100-400 GB/s aggregate bandwidth per node for transformer architectures. BERT-Base (110M parameters) requires approximately 440 MB of gradient synchronization per iteration in FP32, while BERT-Large (340M parameters) requires approximately 1.4 GB. Across 64 GPUs, these synchronization demands require 100-200 GB/s sustained bandwidth for sub-50ms synchronization latency. Language models with 175B parameters require 700 GB/s aggregate bandwidth to maintain 80% parallel efficiency, necessitating InfiniBand HDR or equivalent interconnects.

Synchronization frequency presents a trade-off between communication efficiency and convergence behavior. Gradient accumulation reduces synchronization frequency but increases memory requirements and may impact convergence. Synchronizing every 4 steps reduces communication overhead by 60% while increasing memory usage by 3x for gradient storage. Asynchronous methods eliminate synchronization costs entirely but introduce staleness that degrades convergence by 15-30% for large learning rates.

Scaling efficiency follows predictable patterns across different GPU counts. In the linear scaling regime of 2-32 GPUs, systems typically achieve 85-95% parallel efficiency because communication overhead remains minimal. The communication bound regime emerges at 64-256 GPUs, where efficiency drops to 60-80% even with optimal interconnects. Beyond 512 GPUs, coordination overhead becomes dominant and limits efficiency to 40-60% due to collective operation latency.

Hardware selection critically impacts these scaling characteristics. NVIDIA DGX systems with NVLink achieve 600 GB/s bisection bandwidth, enabling 90% parallel efficiency up to 8 GPUs per node. Multi-node scaling requires InfiniBand networks, where EDR at 100 Gbps supports efficient training up to 64 nodes, while HDR at 200 Gbps enables scaling to 256+ nodes with greater than 70% efficiency.

These efficiency metrics directly influence the choice of parallelism strategy. Data parallelism works well in the linear scaling regime but becomes communication-bound at scale. Model parallelism addresses memory constraints but introduces sequential dependencies that limit efficiency. Pipeline parallelism reduces device idle time but introduces complexity in managing microbatches. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads.

## Data Parallelism {#sec-distributed-training-data-parallelism}

Building on the efficiency characteristics outlined above, data parallelism represents the most straightforward distributed approach and proves particularly effective in the linear scaling regime where communication overhead remains manageable. This method distributes the training process across multiple devices by splitting the dataset into smaller subsets. Each device trains a complete copy of the model using its assigned subset of the data. When training an image classification model on 1 million images using 4 GPUs, each GPU processes 250,000 images while maintaining an identical copy of the model architecture.

Data parallelism proves particularly effective when the dataset size is large but the model size remains manageable, since each device must store a full copy of the model in memory. This method finds wide use in image classification and natural language processing, where the dataset can be processed in parallel without dependencies between data samples. When training a ResNet model [@he2016resnet] on ImageNet, each GPU can independently process its portion of images because the classification of one image does not depend on the results of another.

The effectiveness of data parallelism stems from a property of stochastic gradient descent established in optimization foundations. Gradients computed on different minibatches can be averaged while preserving mathematical equivalence to single-device training. This property enables parallel computation across devices, with the mathematical foundation following directly from the linearity of expectation.

Consider a model with parameters $θ$ training on a dataset $D$. The loss function for a single data point $x_i$ is $L(θ, x_i)$. In standard SGD with batch size $B$, the gradient update for a minibatch is:
$$
g = \frac{1}{B} \sum_{i=1}^B \nabla_θ L(θ, x_i)
$$

In data parallelism with $N$ devices, each device $k$ computes gradients on its own minibatch $B_k$:
$$
g_k = \frac{1}{|B_k|} \sum_{x_i \in B_k} \nabla_θ L(θ, x_i)
$$

The global update averages these local gradients:
$$
g_{\text{global}} = \frac{1}{N} \sum_{k=1}^N g_k
$$

This averaging is mathematically equivalent to computing the gradient on the combined batch $B_{\text{total}} = \bigcup_{k=1}^N B_k$:
$$
g_{\text{global}} = \frac{1}{|B_{\text{total}}|} \sum_{x_i \in B_{\text{total}}} \nabla_θ L(θ, x_i)
$$

This equivalence shows why data parallelism maintains the statistical properties of SGD training. The approach distributes distinct data subsets across devices, computes local gradients independently, and averages these gradients to approximate the full-batch gradient.

The method parallels gradient accumulation, where a single device accumulates gradients over multiple forward passes before updating parameters. Both techniques use the additive properties of gradients to process large batches efficiently.

::: {.callout-note title="Production Reality: Data Parallelism at Scale"}

Data parallelism in production environments involves several operational considerations beyond the theoretical framework:

- **Communication efficiency**: AllReduce operations for gradient synchronization become the bottleneck at scale. Production systems use optimized libraries like NCCL with ring or tree communication patterns to minimize overhead
- **Fault tolerance**: Node failures during large-scale training require checkpoint/restart strategies. Production systems implement hierarchical checkpointing with both local and distributed storage
- **Dynamic scaling**: Cloud environments require elastic scaling capabilities to add/remove workers based on demand and cost constraints, complicated by the need to maintain gradient synchronization
- **Cost optimization**: Production data parallelism considers cost per GPU-hour across different instance types and preemptible instances, balancing training time against infrastructure costs
- **Network bandwidth requirements**: Large models require careful network topology planning as gradient communication can consume 10-50% of training time depending on model size and batch size

Production teams typically benchmark communication patterns and scaling efficiency before deploying large distributed training jobs to identify optimal configurations.

:::

### Data Parallelism Implementation {#sec-distributed-training-data-parallelism-implementation}

Having established the mathematical foundation that makes data parallelism theoretically sound, where gradient averaging preserves the statistical properties of SGD, we now examine how these principles translate into concrete implementation steps. Each step in the implementation corresponds to a phase in the gradient averaging process formalized above, from distributing data subsets to synchronizing the computed gradients.

The process of data parallelism can be broken into a series of distinct steps, each with its role in ensuring the system operates efficiently. These steps are illustrated in @fig-train-data-parallelism.

::: {#fig-train-data-parallelism fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{Line/.style={line width=0.75pt,black!50,text=black
},
  Box/.style={inner xsep=2pt,
    line width=0.75pt,
    node distance=2.0,
    fill=VioletL2,
    draw=VioletLine2,
    text width=27mm,
    align=flush center,
    minimum width=27mm,
    minimum height=9mm
  },
  Box2/.style={Box,
    draw=BlueLine,
    fill=BlueL,
    text width=21mm,
    minimum width=22mm,
    minimum height=9mm
  },
  Text/.style={inner xsep=6pt,
  inner ysep=4pt,
    draw=none, line width=0.75pt,
    fill=TextColor!80,
    font=\usefont{T1}{phv}{m}{n}\footnotesize,
    align=flush center,
    minimum width=22mm, minimum height=5mm
  },
}

\node[Box,node distance=1](B1){GPU 1\\Forward \& Backward Pass};
\node[Box,node distance=1.2,right=of B1](B2){GPU 2\\Forward \& Backward Pass};
\node[Box,node distance=1.2,right=of B2](B3){GPU 3\\Forward \& Backward Pass};
\node[Box,node distance=1.2,right=of B3](B4){GPU 4\\Forward \& Backward Pass};
%
\node[Box2,above=1.06 of B1](GB1){Batch 1};
\node[Box2,above=1.06 of B2](GB2){Batch 2};
\node[Box2,above=1.06 of B3](GB3){Batch 3};
\node[Box2,above=1.06 of B4](GB4){Batch 4};
%
\node[Box2,above=1.8of $(GB2)!0.5!(GB3)$,fill=RedL,draw=RedLine](GGB1){Input Data};
%
\node[Box,below=of $(B2)!0.5!(B3)$,fill=GreenL,draw=GreenLine](DB1){Gradients GPU N};
\node[Box,below=1.05 of DB1,fill=GreenL,draw=GreenLine](DB2){Gradient Aggregation};
\node[Box,below=1.05 of DB2,fill=GreenL,draw=GreenLine](DB3){Model Update};
%
\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB2);
\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB3);
\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB4);
\draw[Line,-latex](GGB1)--node[Text,pos=0.5,anchor=center]{Split into Non-Overlapping Subsets}++(270:1.4)-|(GB1);
%%
\draw[Line,-latex](GB1)--node[Text,pos=0.45]{Assigned to GPU 1}(B1);
\draw[Line,-latex](GB2)--node[Text,pos=0.45]{Assigned to GPU 2}(B2);
\draw[Line,-latex](GB3)--node[Text,pos=0.45]{Assigned to GPU 3}(B3);
\draw[Line,-latex](GB4)--node[Text,pos=0.45]{Assigned to GPU 4}(B4);
%
\draw[Line,-latex](B3)--++(270:0.9)-|(DB1);
\draw[Line,-latex](B2)--++(270:0.9)-|(DB1);
\draw[Line,-latex](B1)--++(270:0.9)-|(DB1);
\draw[Line,-latex](B4)--++(270:0.9)-|node[Text,pos=0.72,text=black]{Compute Gradients}(DB1);
%
\draw[Line,-latex](DB1)--node[Text,pos=0.45]{Synchronize Gradients}(DB2);
\draw[Line,-latex](DB2)--node[Text,pos=0.45]{Aggregate Gradients and Update Parameters}(DB3);
%
\draw[Line,-latex](GGB1.east)--++(0:6.8)|-node[Text,pos=0.8,text=black]{Next Mini-Batch}(DB3.east);
\end{tikzpicture}
```
Distributed training replicates the model across multiple devices, each processing a subset of the data before aggregating gradients to update model parameters. This accelerates the training process compared to single-device training, and contrasts with model parallelism where the model itself is partitioned across devices.
:::

#### Dataset Splitting {#sec-distributed-training-dataset-splitting}

The first step in data parallelism involves dividing the dataset into smaller, non-overlapping subsets. This ensures that each device processes a unique portion of the data, avoiding redundancy and enabling efficient utilization of available hardware. With a dataset of 100,000 training examples and 4 GPUs, each GPU receives 25,000 examples per epoch. The DistributedSampler must ensure no overlap between subsets to maintain gradient estimation validity: if two GPUs process the same example, the resulting gradient average would overweight that example, violating the unbiased gradient assumption that makes data parallelism mathematically equivalent to single-device training.

Modern frameworks like PyTorch's DistributedSampler handle this distribution automatically, implementing prefetching and caching mechanisms to ensure data is readily available for processing. The sampler coordinates across workers using the process rank to deterministically partition indices, ensuring reproducibility when the same random seed is used. For a 1.2 million example dataset like ImageNet distributed across 32 GPUs, each GPU processes exactly 37,500 examples per epoch, with the sampler padding the final batch to maintain consistent batch sizes across all workers.

#### Device Forward Pass {#sec-distributed-training-device-forward-pass}

Once the data subsets are distributed, each device performs the forward pass independently. During this stage, the model processes its assigned batch of data, generating predictions and calculating the loss. In a ResNet-50 model, each GPU independently computes the convolutions, activations, and final loss for its batch. The forward pass is computationally intensive and benefits from hardware accelerators like NVIDIA V100 GPUs or Google TPUs optimized for matrix operations.

#### Backward Pass and Calculation {#sec-distributed-training-backward-pass-calculation}

Following the forward pass, each device computes the gradients of the loss with respect to the model's parameters during the backward pass. Modern frameworks like PyTorch and TensorFlow handle this automatically through their autograd systems. If a model has 50 million parameters, each device calculates gradients for all parameters based only on its local data subset.

#### Gradient Synchronization {#sec-distributed-training-gradient-synchronization}

To maintain consistency across the distributed system, the gradients computed by each device must be synchronized. This coordination represents a distributed systems challenge in achieving global consensus while minimizing communication complexity. The ring all-reduce algorithm exemplifies this trade-off by organizing devices in a logical ring where each GPU communicates only with its neighbors. The algorithm complexity is O(n) in communication rounds but requires sequential dependencies that can limit parallelism.

With 8 GPUs sharing gradients for a 100 MB model, ring all-reduce requires only 7 communication steps instead of the 56 steps needed for naive all-to-all synchronization. The ring topology creates bottlenecks where the slowest link in the ring determines the overall synchronization time, and network partitions can halt the entire training process. Alternative algorithms like tree-reduce achieve O(log n) latency at the cost of increased bandwidth requirements on root nodes. Modern systems often implement hierarchical topologies using high-speed links within nodes and lower-bandwidth connections between nodes to optimize these trade-offs.

::: {.callout-note title="Figure Placeholder: Collective Communication Patterns" collapse="true"}
```{.tikz}
% TODO: Compare Ring AllReduce (circular) vs Tree AllReduce (hierarchical) vs Parameter Server (star)
\node[draw, align=center] {Communication Patterns\nRing vs Tree vs Star};
```
**Gradient Synchronization Topologies**. Visual comparison of communication patterns. (A) Parameter Server uses a central node, creating bottlenecks. (B) Ring AllReduce distributes bandwidth evenly but has O(N) latency. (C) Tree AllReduce reduces latency to O(log N) but may congest root links. (D) Hierarchical AllReduce combines intra-node NVLink and inter-node InfiniBand.
:::

#### Synchronization Models {#sec-distributed-training-sync-models}

Distributed training systems operate under explicit synchronization models that govern when workers observe each other's updates. Understanding these models is essential for reasoning about correctness and performance.

The default model, Bulk Synchronous Parallel (BSP)[^fn-bsp] [@valiant1990bsp], requires all workers to complete their local computation in forward and backward passes, synchronize gradients through a barrier with AllReduce, and then simultaneously update parameters.

[^fn-bsp]: **Bulk Synchronous Parallel (BSP)**: A theoretical model for parallel computation introduced by Leslie Valiant in 1990. BSP divides computation into "supersteps" where all processors compute independently, then synchronize at a barrier before proceeding. While simple to reason about and implement, BSP's strict synchronization means the slowest worker determines iteration time, a problem that grows more severe as cluster size increases.

BSP provides strong guarantees where every worker sees identical parameter values at each step, ensuring mathematical equivalence to single-device training. The cost is that the slowest worker determines iteration time, creating the straggler problem.

Stale Synchronous Parallel (SSP) relaxes this constraint by allowing workers to proceed up to $s$ iterations ahead of the slowest worker before blocking. This bounds staleness while reducing synchronization delays. SSP requires careful learning rate tuning since workers compute gradients on slightly different parameter versions. The bounded staleness guarantee with $s$ typically set to 2-5 provides a middle ground between BSP's strong consistency and fully asynchronous approaches.

Asynchronous SGD eliminates synchronization barriers entirely as workers update parameters independently. This maximizes hardware utilization but introduces gradient staleness that can degrade convergence. When a worker computes gradients on parameters that are already $\tau$ steps stale, the effective learning rate decreases. Compensation techniques include learning rate scaling with $\eta' = \eta / \sqrt{\tau}$ or momentum correction.

::: {.callout-note title="Synchronization Model Trade-offs"}
+-----------+-------------------+---------------------------+---------------------------------+--------------------------------------+
| **Model** | **Consistency**   | **Throughput**            | **Convergence**                 | **Use Case**                         |
+:==========+:==================+:==========================+:================================+:=====================================+
| **BSP**   | Strong            | Bounded by slowest worker | Equivalent to single-GPU        | Final training runs, reproducibility |
| **SSP**   | Bounded staleness | Higher than BSP           | Near-equivalent with tuning     | Hyperparameter search                |
| **Async** | Weak              | Maximum                   | Degraded, requires compensation | Large heterogeneous clusters         |
+-----------+-------------------+---------------------------+---------------------------------+--------------------------------------+
:::

The choice of synchronization model directly affects both system throughput and model convergence. Production systems typically use BSP for final training runs to ensure reproducibility, while exploring SSP or async approaches during hyperparameter search where exact reproducibility is less critical.

#### Barrier Semantics and Failure Modes {#sec-distributed-training-barrier-failures}

AllReduce operations implement implicit barriers where no worker can proceed until all workers have contributed their gradients. This coupling creates failure modes absent from single-device training.

Worker failures during AllReduce cause all other workers to block indefinitely while waiting for the missing contribution. Without timeout mechanisms, the entire training job hangs rather than failing cleanly. Production systems implement watchdog timers typically set to 5-10 minutes to detect and terminate stuck jobs.

Gradient mismatches occur when workers disagree on which tensors to synchronize due to conditional computation paths or dynamic batching. AllReduce operations may block waiting for tensors that some workers never send. This commonly occurs with variable-length sequences in NLP models, dynamic computation graphs, and mixture-of-experts with different routing decisions.

Straggler-induced delays arise because iteration time equals the slowest worker's time plus synchronization overhead. A single slow worker, whether due to thermal throttling, network congestion, or OS jitter, delays all workers and reduces cluster utilization. At 1000 GPUs with 1% probability of straggler per GPU per step, approximately 10 GPUs straggle every iteration.

Production systems address these issues through timeouts, heartbeat monitoring, and elastic training mechanisms. See @sec-fault-tolerance for comprehensive coverage of failure detection and recovery strategies in distributed training systems.

#### Parameter Updating {#sec-distributed-training-parameter-updating}

After gradient aggregation, each device independently updates model parameters using the chosen optimization algorithm such as SGD with momentum or Adam. This decentralized update strategy, implemented in frameworks like PyTorch's DistributedDataParallel, enables efficient parameter updates without requiring a central coordination server. Since all devices have identical gradient values after synchronization, they perform mathematically equivalent updates to maintain model consistency across the distributed system.

In a system with 8 GPUs training a ResNet model, each GPU computes local gradients based on its data subset. After gradient averaging via ring all-reduce, every GPU has the same global gradient values. Each device then independently applies these gradients using the optimizer's update rule. With SGD and learning rate 0.1, the update becomes `weights = weights - 0.1 * gradients`. This process maintains mathematical equivalence to single-device training while enabling distributed computation.

This process, which involves splitting data, performing computations, synchronizing results, and updating parameters, repeats for each batch of data. Modern frameworks automate this cycle, allowing developers to focus on model architecture and hyperparameter tuning rather than distributed computing logistics.

### Data Parallelism Advantages {#sec-distributed-training-data-parallelism-advantages}

Data parallelism offers several key benefits that make it the predominant approach for distributed training. By splitting the dataset across multiple devices and allowing each device to train an identical copy of the model, this approach effectively addresses the core challenges in modern AI training systems.

The primary advantage of data parallelism is its linear scaling capability with large datasets. As datasets grow into the terabyte range, processing them on a single machine becomes prohibitively time-consuming. Training a vision transformer on ImageNet with 1.2 million images might take weeks on a single GPU but only days when distributed across 8 GPUs. This scalability proves particularly valuable in domains like language modeling, where datasets can exceed billions of tokens.

Hardware utilization efficiency represents another important benefit. Data parallelism maintains high GPU utilization rates typically above 85% by ensuring each device actively processes its data portion. Modern implementations achieve this through asynchronous data loading and gradient computation overlapping with communication. While one batch computes gradients, the next batch's data is already being loaded and preprocessed.

Implementation simplicity sets data parallelism apart from other distribution strategies. Modern frameworks have reduced complex distributed training to just a few lines of code. Converting a PyTorch model to use data parallelism often requires only wrapping it in `DistributedDataParallel` and initializing a distributed environment. This accessibility has contributed significantly to its widespread adoption in both research and industry.

The approach also offers flexibility across model architectures. Whether training a ResNet for vision, BERT for language, or Graph Neural Network for graph data, the same data parallelism principles apply without modification. This universality makes it particularly valuable as a default choice for distributed training.

Training time reduction is perhaps the most immediate benefit. Given proper implementation, data parallelism can achieve near-linear speedup with additional devices. Training that takes 100 hours on a single GPU might complete in roughly 13 hours on 8 GPUs, assuming efficient gradient synchronization and minimal communication overhead.

While these benefits make data parallelism compelling, achieving these advantages requires careful system design. Several challenges must be addressed to fully realize these benefits.

::: {.callout-tip title="GPT-2 Data Parallel Scaling: 1→8→32 GPUs" collapse="true"}

This example demonstrates how data parallelism scales in practice, including efficiency degradation.

**Single GPU Baseline**

- Batch size: 16 (with gradient checkpointing, fits in 32GB)
- Time per step: 1.8 seconds
- Training throughput: ~9 samples/second
- Time to 50K steps: **25 hours**

**8 GPUs: Single Node with NVLink**

Configuration:

- Per-GPU batch: 16, global batch: 128
- Gradient synchronization: 3GB @ 600 GB/s (NVLink) = 5ms

Performance results:

- Computation: 180ms per step
- Communication: 5ms per step
- Total: 185ms per step
- Speedup: 1.8s ÷ 0.185s = 9.7× (not quite 8×)
- Parallel efficiency: 9.7 ÷ 8 = 121%

Why over 100% efficiency? Larger global batch (128 vs 16) improves GPU utilization from 72% to 89%. This "super-linear" speedup is common in ML at small scales when the baseline has poor utilization.

Training time: 25 hours ÷ 9.7 = **2.6 hours**

**32 GPUs: 4 Nodes with InfiniBand**

Configuration:

- Per-GPU batch: 16, global batch: 512
- Intra-node communication: 5ms (NVLink)
- Inter-node communication: 3GB @ 12.5 GB/s (InfiniBand) = 240ms

Performance results:

- Computation: 180ms (42% of time)
- Communication: 245ms (58% of time)
- Total: 425ms per step
- Speedup: 1.8s ÷ 0.425s = 4.2× faster → 5.9 hours
- Parallel efficiency: 4.2 ÷ 32 = 13%

Communication dominates and becomes the bottleneck.

**Better Approach: 8 GPUs with Gradient Accumulation**

- Configuration: 8 GPUs × batch 16 × 4 accumulation steps = 512 effective batch
- Communication overhead: 5ms ÷ (4 × 180ms) = 0.7%
- Training time: 3.8 hours
- Cost: $128/hour × 3.8 hours = $486 vs. $3,021 for 32 GPUs
- Savings: $2,535 (84% reduction) with only 1 hour longer training

**Key Insights**

1. NVLink enables efficient scaling within single nodes (97% efficiency)
2. Inter-node communication kills efficiency (drops to 13%)
3. Gradient accumulation beats naive scaling for memory-bound models
4. Sweet spot for GPT-2: 8 GPUs per node with gradient accumulation, not naive scaling to 32+ GPUs

OpenAI's GPT-2 paper reports training on 32 V100s across 4 nodes using optimized communication (likely gradient accumulation combined with pipeline parallelism), not pure data parallelism.

:::

### Data Parallelism Limitations {#sec-distributed-training-data-parallelism-limitations}

While data parallelism is an effective approach for distributed training, it introduces several challenges that must be addressed to achieve efficient and scalable training systems. These challenges stem from the inherent trade-offs between computation and communication, as well as the limitations imposed by hardware and network infrastructures.

Communication overhead represents the most significant bottleneck in data parallelism. During gradient synchronization, each device must exchange gradient updates, often hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter model, each synchronization step might require transferring several gigabytes of data across the network. While high-speed interconnects like NVLink at 300 GB/s or InfiniBand[^fn-infiniband] at 200 Gb/s help, the overhead remains substantial.

[^fn-infiniband]: InfiniBand provides 200 Gb/s per port with sub-microsecond latency, enabling efficient gradient synchronization across nodes via RDMA. See @sec-infrastructure for network topology design and InfiniBand architecture details.

NCCL's ring-allreduce algorithm[^fn-allreduce-algorithm] reduces this burden by organizing devices in a ring topology, but communication costs still grow with model size and device count.

[^fn-allreduce-algorithm]: **AllReduce Algorithm**: A collective communication primitive where each process contributes data and all processes receive the same combined result (typically a sum). Naive implementations require O(n²) messages for n devices. The ring-allreduce algorithm achieves bandwidth-optimal performance by organizing devices in a logical ring where each device communicates only with its neighbors [@patarasuk2009allreduce], making it scalable for modern ML with hundreds of GPUs.

Scalability limitations become apparent as device count increases. While 8 GPUs might achieve $7\times$ speedup with 87.5% scaling efficiency, scaling to 64 GPUs typically yields only 45-50$\times$ speedup with 70-78% efficiency due to growing synchronization costs. Scaling efficiency, calculated as speedup divided by the number of devices, quantifies how effectively additional hardware translates to reduced training time. Perfect linear scaling would yield 100% efficiency, but communication overhead and synchronization barriers typically degrade efficiency as device count grows. This non-linear scaling means that doubling the number of devices rarely halves the training time, particularly in configurations exceeding 16-32 devices.

Memory constraints present a hard limit for data parallelism. Consider a transformer model with 175 billion parameters, which requires approximately 350 GB just to store model parameters in FP32. When accounting for optimizer states and activation memories, the total requirement often exceeds 1 TB per device. Since even high-end GPUs typically offer 80 GB or less, such models cannot use pure data parallelism.

Workload imbalance affects heterogeneous systems significantly. In a cluster mixing A100 and V100 GPUs, the A100s might process batches $1.7\times$ faster, forcing them to wait for the V100s to catch up. This idle time can reduce cluster utilization by 20-30% without proper load balancing mechanisms.

Finally, there are critical challenges related to fault tolerance and reliability in distributed training systems. Node failures become inevitable at scale: with 100 GPUs running continuously, hardware failures occur multiple times per week. A training run that costs millions of dollars cannot restart from scratch each time a single GPU fails. Modern distributed training systems implement sophisticated checkpointing strategies, storing model state every N iterations to minimize lost computation. Checkpoint frequency creates trade-offs: frequent checkpointing reduces the potential loss from failures but increases storage I/O overhead and training latency. Production systems typically checkpoint every 100-1000 iterations, balancing fault tolerance against performance.

Implementation complexity compounds these reliability challenges. While modern frameworks abstract much of the complexity, implementing robust distributed training systems requires significant engineering expertise. Graceful degradation when subsets of nodes fail, consistent gradient synchronization despite network partitions, and automatic recovery from transient failures demand deep understanding of both machine learning and distributed systems principles.

Despite these challenges, data parallelism remains an important technique for distributed training, with many strategies available to address its limitations. Monitoring these distributed systems requires specialized tooling for tracking gradient norms, communication patterns, and hardware utilization across nodes. Before examining model parallelism, we first explore memory optimization techniques that extend data parallelism to larger models.

### Memory-Efficient Data Parallelism: ZeRO and FSDP {#sec-distributed-training-zero-fsdp}

The memory constraints of data parallelism motivate a family of techniques that shard memory state across workers while preserving the simplicity of data parallel training. ZeRO (Zero Redundancy Optimizer)[^fn-zero] [@rajbhandari2020zero] and its PyTorch implementation FSDP (Fully Sharded Data Parallel) [@zhao2023fsdp] enable training models that would otherwise require model parallelism.

[^fn-zero]: **ZeRO (Zero Redundancy Optimizer)**: Developed by Microsoft Research for DeepSpeed, ZeRO eliminates memory redundancy in data parallel training by partitioning optimizer states, gradients, and parameters across workers instead of replicating them. ZeRO enabled training of models with over 100 billion parameters on hardware previously limited to much smaller models, fundamentally changing the economics of large-scale training.

In standard data parallelism, each GPU maintains a complete copy of:

- **Model parameters**: 4 bytes/param (FP32) or 2 bytes/param (FP16/BF16)
- **Gradients**: Same size as parameters
- **Optimizer states**: For Adam, 8 bytes/param (momentum + variance in FP32)

For a 7B parameter model with Adam optimizer, each GPU requires: $7B \times (4 + 4 + 8) = 112$ GB, exceeding A100-80GB capacity even before accounting for activations.

ZeRO addresses this redundancy through progressive sharding:

::: {.callout-note title="Figure Placeholder: ZeRO Memory Usage" collapse="true"}
```{.tikz}
% TODO: Bar chart showing memory usage of DDP vs ZeRO-1/2/3
\node[draw, align=center] {ZeRO Memory Partitioning\nOptimizer | Gradients | Parameters};
```
**ZeRO Memory Partitioning**. Visual comparison of memory usage between standard Data Parallelism (DDP) and ZeRO Stages 1, 2, and 3. In DDP, every GPU stores a full copy of parameters, gradients, and optimizer states. ZeRO progressively partitions these states across GPUs, eliminating redundancy. ZeRO-3 achieves linear memory scaling with GPU count, enabling training of trillion-parameter models on limited hardware.
:::

+-------------------+-----------------------+--------------------------+----------------------------------+
| **Stage**         | **What is Sharded**   | **Memory Reduction**     | **Communication Overhead**       |
+==================:+:======================+:=========================+:=================================+
| **ZeRO-1**        | Optimizer states only | ~4x                      | None (same as DDP)               |
| **ZeRO-2**        | + Gradients           | ~8x                      | ReduceScatter replaces AllReduce |
| **ZeRO-3 / FSDP** | + Parameters          | ~$N$ (linear in workers) | AllGather before each layer      |
+-------------------+-----------------------+--------------------------+----------------------------------+

ZeRO-1 shards optimizer states across GPUs. Each GPU stores only $1/N$ of the Adam momentum and variance tensors. After gradient AllReduce, each GPU updates only its shard of parameters, then broadcasts updates to other GPUs. Memory savings: optimizer states reduced from $8N$ bytes/param to $8$ bytes/param total across cluster.

ZeRO-2 additionally shards gradients. Instead of AllReduce, which leaves full gradients on each GPU, ZeRO-2 uses ReduceScatter so each GPU receives $1/N$ of the reduced gradients. Memory savings: gradients reduced from $4N$ bytes/param to $4$ bytes/param total.

ZeRO-3 and FSDP shard parameters themselves. Each GPU stores only $1/N$ of the model. Before each layer's forward pass, parameters are gathered via AllGather; after backward pass, gradients are reduced via ReduceScatter, then parameters are discarded. This achieves maximum memory efficiency at the cost of additional communication.

::: {.callout-note title="FSDP Communication Analysis"}
FSDP introduces communication on the critical path that DDP avoids:

- **Forward pass**: AllGather to reconstruct parameters ($M$ bytes × 2 for each layer)
- **Backward pass**: ReduceScatter for gradients ($M$ bytes × 2 for each layer)

For a model with $L$ layers, FSDP performs $2L$ collective operations per training step versus 1 AllReduce for DDP. However, FSDP enables overlapping: while layer $i$ computes, layer $i+1$ can prefetch parameters.

Total FSDP communication volume: approximately $3M$ bytes (vs. $2M$ for DDP AllReduce), but spread across more operations with overlap opportunities.
:::

The choice between FSDP and DDP depends on model size and memory constraints. Use DDP when the model fits in GPU memory with room for activations, as it has lower overhead. Use FSDP ZeRO-2 when the model barely fits or requires activation checkpointing. Use FSDP ZeRO-3 when model parameters exceed single-GPU memory. For training 70B+ models on 80GB GPUs, combine FSDP with tensor parallelism.

FSDP configuration requires careful tuning of sharding strategy (by layer, by transformer block, or flat) and mixed precision settings. The `auto_wrap_policy` determines sharding granularity, with finer sharding reducing memory but increasing communication frequency.

## Model Parallelism {#sec-distributed-training-model-parallelism}

The memory optimization techniques examined in the previous section extend data parallelism's reach by sharding optimizer states, gradients, and even parameters across workers. ZeRO and FSDP achieve remarkable memory savings, reducing per-GPU requirements from hundreds of gigabytes to manageable fractions. Yet these techniques share a fundamental assumption: during computation, each device still processes complete layers, just with sharded state that is gathered on demand. When individual layers themselves exceed device memory, or when activation memory dominates the memory budget, a different approach becomes necessary.

Even with ZeRO-3 fully deployed, sharding optimizer states, gradients, and parameters across workers, some architectures remain intractable. A 175B parameter model using FSDP across 64 GPUs still requires 700 GB / 64 = 11 GB of parameters per GPU before accounting for activations. For long-context transformers where activation memory dominates, a 2048-token sequence through 175B parameters generates 200+ GB of intermediate activations, and no amount of optimizer sharding addresses this constraint. Model parallelism addresses these limitations by splitting the model architecture itself across devices, rather than replicating it with sharded state.

Model parallelism addresses this limitation by distributing neural networks across multiple computing devices. Unlike data parallelism, where each device contains a complete model copy, model parallelism assigns different model components to different devices [@shazeer_mixture_of_experts_2017].

Several implementations of model parallelism exist. In layer-based splitting, devices process distinct groups of layers sequentially. The first device might compute layers 1-4 while the second handles layers 5-8. Channel-based splitting divides the channels within each layer across devices, where the first device processes 512 channels while the second manages the remaining ones. For transformer architectures, attention head splitting distributes different attention heads to separate devices.

This distribution method enables training of large-scale models. GPT-3, with 175 billion parameters, relies on model parallelism for training. Vision transformers processing high-resolution 16k × 16k pixel images use model parallelism to manage memory constraints. Mixture-of-Expert architectures use this approach to distribute their conditional computation paths across hardware.

Device coordination follows a specific pattern during training. In the forward pass, data flows sequentially through model segments on different devices. The backward pass propagates gradients in reverse order through these segments. During parameter updates, each device modifies only its assigned portion of the model. This coordination ensures mathematical equivalence to training on a single device while enabling the handling of models that exceed individual device memory capacities.

### Model Parallelism Implementation {#sec-distributed-training-model-parallelism-implementation}

Model parallelism divides neural networks across multiple computing devices, with each device computing a distinct portion of the model's operations. This division allows training of models whose parameter counts exceed single-device memory capacity. The technique encompasses device coordination, data flow management, and gradient computation across distributed model segments. The mechanics of model parallelism are illustrated in @fig-model-parallelism. These steps are described next:

::: {#fig-model-parallelism fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{Line/.style={line width=1.0pt,black!50,text=black
},
    Box/.style={inner xsep=2pt,
    draw=GreenLine,
    node distance=1.5,
    line width=0.75pt,
    fill=GreenL,
    anchor=west,
    text width=23mm,
    align=flush center,
    minimum width=23mm,
    minimum height=10mm
  },
  Text/.style={inner xsep=4pt,
    draw=none, line width=0.75pt,
    fill=TextColor!80,
    font=\usefont{T1}{phv}{m}{n}\footnotesize,
    align=flush center,
    minimum width=22mm, minimum height=6mm
  },
}

\node[Box](B1){Input Data};
\node[Box,right=of B1](B2){Model Part 1\\ on Device 1};
\node[Box,right=of B2](B3){Model Part 2\ on Device 2};
\node[Box,right=of B3](B4){Model Part 3\\ on Device 3};
\node[Box,right=of B4](B5){Predictions};
%
\draw[Line,-latex](B1)--++(90:12mm)
-|node[Text,pos=0.25]{Forward Pass}(B2.120);
\draw[Line,latex-](B1)--++(270:12mm)
-|node[Text,pos=0.25]{Gradient Updates}(B2.240);
%
\draw[Line,-latex](B2)--++(90:12mm)
-|node[Text,pos=0.25]{Intermediate Data}(B3.120);
\draw[Line,latex-](B2)--++(270:12mm)
-|node[Text,pos=0.25]{Gradient Updates}(B3.240);
%
\draw[Line,-latex](B3)--++(90:12mm)
-|node[Text,pos=0.25]{Intermediate Data}(B4.120);
\draw[Line,latex-](B3)--++(270:12mm)
-|node[Text,pos=0.25]{Gradient Updates}(B4.240);
%
\draw[Line,-latex](B4)--++(90:12mm)
-|node[Text,pos=0.25]{Output}(B5.120);
\draw[Line,latex-](B4)--++(270:12mm)
-|node[Text,pos=0.25]{Backward Pass}(B5.240);
\end{tikzpicture}
```
Distributing a neural network across multiple devices enables training models larger than the memory capacity of a single device. This approach requires careful coordination of data flow and gradient computation between devices to maintain training efficiency.
:::

#### Model Partitioning {#sec-distributed-training-model-partitioning}

The first step in model parallelism is dividing the model into smaller segments. In a deep neural network, layers are often divided among devices. In a system with two GPUs, the first half of the layers might reside on GPU 1 while the second half resides on GPU 2. Another approach splits computations within a single layer by dividing matrix multiplications in transformer models across devices.

#### Model Forward Pass {#sec-distributed-training-model-forward-pass}

During the forward pass, data flows sequentially through the partitions. Data processed by the first set of layers on GPU 1 is sent to GPU 2 for processing by the next set of layers. This sequential flow ensures that the entire model is used even though it is distributed across multiple devices. Efficient inter-device communication is important to minimize delays during this step [@deepspeed_training_system_2021].

#### Backward Pass and Calculation {#sec-distributed-training-backward-pass-calculation-model}

The backward pass computes gradients through the distributed model segments in reverse order. Each device calculates local gradients for its parameters and propagates necessary gradient information to previous devices. In transformer models, this means backpropagating through attention computations and feed-forward networks across device boundaries.

In a two-device setup with attention mechanisms split between devices, the backward computation works as follows. The second device computes gradients for the final feed-forward layers and attention heads, then sends the gradient tensors for the attention output to the first device. The first device uses these received gradients to compute updates for its attention parameters and earlier layer weights.

#### Parameter Updates {#sec-distributed-training-parameter-updates-model}

Parameter updates occur independently on each device using the computed gradients and an optimization algorithm. A device holding attention layer parameters applies updates using only the gradients computed for those specific parameters. This localized update approach differs from data parallelism, which requires gradient averaging across devices.

The optimization step proceeds as follows: Each device applies its chosen optimizer (such as Adam [@kingma2015adam] or AdaFactor) to update its portion of the model parameters. A device holding the first six transformer layers updates only those layers' weights and biases. This local parameter update eliminates the need for cross-device synchronization during the optimization step, reducing communication overhead.

#### Iterative Process {#sec-distributed-training-iterative-process}

Like other training strategies, model parallelism repeats these steps for every batch of data. As the dataset is processed over multiple iterations, the distributed model converges toward optimal performance.

### Parallelism Variations {#sec-distributed-training-parallelism-variations}

Model parallelism can be implemented through different strategies for dividing the model across devices. The three primary approaches are layer-wise partitioning, operator-level partitioning, and pipeline parallelism, each suited to different model structures and computational needs.

#### Layer-wise Partitioning {#sec-distributed-training-layerwise-partitioning}

Layer-wise partitioning assigns distinct model layers to separate computing devices. In transformer architectures, this translates to specific devices managing defined sets of attention and feed-forward blocks. As illustrated in @fig-layers-blocks, a 24-layer transformer model distributed across four devices assigns six consecutive transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and so forth.

::: {#fig-layers-blocks fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{Line/.style={line width=1.0pt,black!50
},
  Box/.style={inner xsep=2pt,
    draw=VioletLine2,
    line width=0.75pt,
    node distance=1.8,
    fill=VioletL2,
    align=flush center,
    text width=19mm,
    minimum width=19mm,
    minimum height=8mm
  },
}
\node[Box,fill=RedL,draw=RedLine](B1){Blocks 1-6};
\node[Box,right=of B1,fill=OrangeL,draw=OrangeLine](B2){Blocks 7-12};
\node[Box,right=of B2,fill=GreenL,draw=GreenLine](B3){Blocks 13-18};
\node[Box,right=of B3,fill=BlueL,draw=BlueLine](B4){Blocks 19-24};
%
\node[Box,below=1.3 of B1,fill=VioletL2,draw=VioletLine2](G1){GPU 1};
\node[Box,below=1.3 of B2,fill=VioletL2,draw=VioletLine2](G2){GPU 2};
\node[Box,below=1.3 of B3,fill=VioletL2,draw=VioletLine2](G3){GPU 3};
\node[Box,below=1.3 of B4,fill=VioletL2,draw=VioletLine2](G4){GPU 4};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=13,
line width=0.75pt,
inner ysep=18,
fill=BackColor,yshift=6,
fit=(B1)(G1)](BB1){};
\node[below=1pt of BB1.north,anchor=north]{Device 1};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=13,
line width=0.75pt,
inner ysep=18,
fill=BackColor,yshift=6,
fit=(B2)(G2)](BB2){};
\node[below=1pt of BB2.north,anchor=north]{Device 2};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=13,
line width=0.75pt,
inner ysep=18,
fill=BackColor,yshift=6,
fit=(B3)(G3)](BB3){};
\node[below=1pt of BB3.north,anchor=north]{Device 3};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=13,
line width=0.75pt,
inner ysep=18,
fill=BackColor,yshift=6,
fit=(B4)(G4)](BB4){};
\node[below=1pt of BB4.north,anchor=north]{Device 4};

\foreach \x in {1,2,3} {
    \pgfmathtruncatemacro{\newX}{\x + 1}
    \draw[-latex,Line] (B\x) -- (B\newX);
}
\foreach \x in {4,3,2} {
    \pgfmathtruncatemacro{\newX}{\x - 1}
\draw[red,-latex,Line](B\x.230)to[out=230,in=300](B\newX.300);
}
\end{tikzpicture}
```
**Layer-Wise Model Parallelism**: Distributing a transformer model across multiple gpus assigns consecutive layers to each device, enabling parallel processing of input data and accelerating training. This partitioning strategy allows each GPU to operate on a subset of the model's layers, reducing the memory footprint and computational load per device.
:::

This sequential processing introduces device idle time, as each device must wait for the previous device to complete its computation before beginning work. While device 1 processes the initial blocks, devices 2, 3, and 4 remain inactive. Similarly, when device 2 begins its computation, device 1 sits idle. This pattern of waiting and idle time reduces hardware utilization efficiency compared to other parallelization strategies.

#### Pipeline Parallelism {#sec-distributed-training-pipeline-parallelism}

Pipeline parallelism extends layer-wise partitioning by introducing microbatching to minimize device idle time, as illustrated in @fig-pipline-parallelism. Instead of waiting for an entire batch to sequentially pass through all devices, the computation is divided into smaller segments called microbatches. GPipe[^fn-gpipe] [@gpipe2019] introduced synchronous pipeline parallelism with micro-batch accumulation, while PipeDream [@harlap2018pipedream] developed asynchronous approaches with weight stashing.

[^fn-gpipe]: **GPipe**: Developed by Google in 2018, GPipe pioneered synchronous pipeline parallelism for training neural networks too large for single devices. By splitting models across accelerators and using micro-batches to keep all stages busy, GPipe achieved near-linear scaling for training an AmoebaNet model with 557 million parameters across 8 TPUs. This approach influenced subsequent systems including Megatron-LM and DeepSpeed.

Each device, as represented by the rows in the drawing, processes its assigned model layers for different microbatches simultaneously. The forward pass involves devices passing activations to the next stage, such as $F_{0,0}$ to $F_{1,0}$. The backward pass transfers gradients back through the pipeline, such as $B_{3,3}$ to $B_{2,3}$. This overlapping computation reduces idle time and increases throughput while maintaining the logical sequence of operations across devices.

::: {#fig-pipline-parallelism}
```{.tikz}
\begin{tikzpicture}[
    every node/.style={font=\sffamily, draw, minimum width=1cm, minimum height=0.7cm, align=center, outer sep=0},
    fill0/.style={fill=red!20}, % Complementary to lightgray
    fill1/.style={fill=blue!20}, % Complementary to orange
    fill2/.style={fill=orange!20}, % Complementary to blue
    fill3/.style={fill=yellow!20}, % Complementary to purple
    back3/.style={fill=yellow!20} % Same as fill3
]

% Row 0
\node[fill0] (F0_0) {$F_{0,0}$};
\node[fill0, right=0cm of F0_0] (F0_1) {$F_{0,1}$};
\node[fill0, right=0cm of F0_1] (F0_2) {$F_{0,2}$};
\node[fill0, right=0cm of F0_2] (F0_3) {$F_{0,3}$};

% Row 1
\node[fill1, above right=0cm and 0cm of F0_0] (F1_0) {$F_{1,0}$};
\node[fill1, right=0cm of F1_0] (F1_1) {$F_{1,1}$};
\node[fill1, right=0cm of F1_1] (F1_2) {$F_{1,2}$};
\node[fill1, right=0cm of F1_2] (F1_3) {$F_{1,3}$};

% Row 2 (stacked above F1)
\node[fill2, above right=0cm and 0cm of F1_0] (F2_0) {$F_{2,0}$};
\node[fill2, right=0cm of F2_0] (F2_1) {$F_{2,1}$};
\node[fill2, right=0cm of F2_1] (F2_2) {$F_{2,2}$};
\node[fill2, right=0cm of F2_2] (F2_3) {$F_{2,3}$};

% Row 3 (stacked above F2)
\node[fill3, above right=0cm and 0cm of F2_0] (F3_0) {$F_{3,0}$};
\node[fill3, right=0cm of F3_0] (F3_1) {$F_{3,1}$};
\node[fill3, right=0cm of F3_1] (F3_2) {$F_{3,2}$};
\node[fill3, right=0cm of F3_2] (F3_3) {$F_{3,3}$};

% Row 3 (backward pass)
\node[back3, right=0cm of F3_3] (B3_3) {$B_{3,3}$};
\node[back3, right=0cm of B3_3] (B3_2) {$B_{3,2}$};
\node[back3, right=0cm of B3_2] (B3_1) {$B_{3,1}$};
\node[back3, right=0cm of B3_1] (B3_0) {$B_{3,0}$};

% Row 2 (backward pass)
\node[fill2, below=0cm and 0cm of B3_2] (B2_3) {$B_{2,3}$};
\node[fill2, right=0cm of B2_3] (B2_2) {$B_{2,2}$};
\node[fill2, right=0cm of B2_2] (B2_1) {$B_{2,1}$};
\node[fill2, right=0cm of B2_1] (B2_0) {$B_{2,0}$};

% Row 1 (backward pass)
\node[fill1, below=0cm of B2_2] (B1_3) {$B_{1,3}$};
\node[fill1, right=0cm of B1_3] (B1_2) {$B_{1,2}$};
\node[fill1, right=0cm of B1_2] (B1_1) {$B_{1,1}$};
\node[fill1, right=0cm of B1_1] (B1_0) {$B_{1,0}$};

% Row 0 (backward pass)
\node[fill0, below=0cm of B1_2] (B0_3) {$B_{0,3}$};
\node[fill0, right=0cm of B0_3] (B0_2) {$B_{0,2}$};
\node[fill0, right=0cm of B0_2] (B0_1) {$B_{0,1}$};
\node[fill0, right=0cm of B0_1] (B0_0) {$B_{0,0}$};

% Update nodes
\node[fill0, right=0cm of B0_0] (U0_0) {Update};
\node[fill1, above=0cm of U0_0] (U0_1) {Update};
\node[fill2, above=0cm of U0_1] (U0_2) {Update};
\node[fill3, above=0cm of U0_2] (U0_3) {Update};

%\node[draw=none, minimum width=4cm, minimum height=1cm, align=center, right=1cm of F0_3] (Bubble) {Bubble};
\end{tikzpicture}
```
With model layers distributed across devices, microbatching divides each training batch into smaller segments that flow through the pipeline concurrently. This approach minimizes device idle time during both forward and backward passes by allowing multiple microbatches to be processed simultaneously at different pipeline stages. Activations flow sequentially between devices during the forward pass, while gradients propagate in reverse during backpropagation.
:::

In a transformer model distributed across four devices, device 1 would process blocks 1-6 for microbatch $N+1$ while device 2 computes blocks 7-12 for microbatch $N$. Simultaneously, device 3 executes blocks 13-18 for microbatch $N-1$, and device 4 processes blocks 19-24 for microbatch $N-2$. Each device maintains its assigned transformer blocks but operates on a different microbatch, creating a continuous flow of computation.

The transfer of hidden states between devices occurs continuously rather than in distinct phases. When device 1 completes processing a microbatch, it immediately transfers the output tensor of shape [microbatch_size, sequence_length, hidden_dimension] to device 2 and begins processing the next microbatch. This overlapping computation pattern maintains full hardware utilization while preserving the model's mathematical properties.

#### Tensor Parallelism {#sec-distributed-training-tensor-parallelism}

Pipeline parallelism, examined above, addresses device idle time by overlapping microbatch processing across stages. Each device holds complete layers and processes them sequentially, with communication only at stage boundaries when activations transfer between devices. This approach tolerates moderate interconnect bandwidth because communication occurs infrequently, once per layer boundary per microbatch. However, pipeline parallelism cannot help when individual layers themselves exceed device memory, or when the communication pattern within layers benefits from a different granularity than layer boundaries.

Tensor parallelism takes a fundamentally different approach: instead of assigning complete layers to devices, it splits the weight matrices within each layer. This operator-level parallelism (also called intra-layer parallelism) enables finer-grained distribution but requires high-bandwidth interconnects for the frequent intra-layer communication it introduces. The distinction is critical for hardware planning: tensor parallelism demands NVLink-class bandwidth, while pipeline parallelism tolerates InfiniBand between stages.

::: {.callout-note title="Terminology: Tensor Parallelism vs. Pipeline Parallelism"}
Modern literature distinguishes two forms of model parallelism:

**Tensor Parallelism** (intra-layer): Splits individual operations (matrix multiplies, attention heads) across devices. Requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer.

**Pipeline Parallelism** (inter-layer): Assigns complete layers to different devices. Requires only point-to-point communication between pipeline stages, tolerating lower bandwidth interconnects.

The Megatron-LM framework popularized this distinction, using tensor parallelism within nodes (8 GPUs on NVLink) and pipeline parallelism across nodes (InfiniBand).
:::

Megatron-style tensor parallelism[^fn-megatron] [@shoeybi2019megatron] partitions matrix multiplications in two ways.

[^fn-megatron]: **Megatron-LM**: NVIDIA's framework for training large transformer models, first published in 2019. Megatron introduced efficient tensor parallelism strategies specifically designed for transformers, strategically placing AllReduce operations to minimize communication while splitting attention heads and feed-forward layers across GPUs. Megatron-LM enabled training of models with 8.3 billion parameters and established patterns now used across the industry for multi-GPU transformer training.

Column-parallel linear layers split weights along columns. For input $X$ and weight matrix $W = [W_1 | W_2]$ split across 2 GPUs:
$$Y = XW = X[W_1 | W_2] = [XW_1 | XW_2]$$
Each GPU computes its partition independently. Outputs are concatenated (no communication needed if followed by row-parallel layer).

::: {.callout-note title="Figure Placeholder: Tensor Parallelism Matrix Split" collapse="true"}
```{.tikz}
% TODO: Visual showing Matrix A split by columns, B split by rows.
\node[draw, align=center] {Megatron-LM Partitioning\nColumn Split vs Row Split};
```
**Tensor Parallelism - Matrix Partitioning**. Illustration of Megatron-LM style tensor parallelism. The first linear layer (e.g., QKV) is split column-wise $[W_1 | W_2]$. The second layer (e.g., Output) is split row-wise $[W_1 ; W_2]$. This arrangement allows the output of the first layer to flow directly into the second without synchronization, requiring only one AllReduce after the second layer.
:::

Row-parallel linear layers split weights along rows. For $W = \begin{bmatrix} W_1 \\ W_2 \end{bmatrix}$:
$$Y = XW = X_1 W_1 + X_2 W_2$$
Each GPU computes a partial sum. Outputs require AllReduce to combine.

In transformer architectures, Megatron applies this pattern:

1. **QKV projection**: Column-parallel (weights split, outputs concatenated across heads)
2. **Attention output projection**: Row-parallel (requires AllReduce after)
3. **First FFN layer**: Column-parallel (split intermediate dimension)
4. **Second FFN layer**: Row-parallel (requires AllReduce after)

This design places AllReduce operations strategically: one after attention, one after FFN, totaling 2 AllReduce operations per transformer layer.

Communication volume per transformer layer depends on sequence length $S$, hidden dimension $H$, and batch size $B$:
$$\text{Communication} = 2 \times B \times S \times H \times \text{sizeof(dtype)}$$

With $S=2048$, $H=4096$, $B=4$, and FP16: $2 \times 4 \times 2048 \times 4096 \times 2 = 134$ MB per layer. For a 96-layer model, this totals 12.6 GB per training step, requiring NVLink bandwidth to avoid becoming the bottleneck.

Tensor parallelism scaling degrades rapidly beyond 8-way parallelism because:

- Communication volume grows linearly with tensor parallel degree
- Computation per GPU decreases (less work to hide communication latency)
- NVLink bandwidth becomes saturated

Production systems (GPT-4, LLaMA, Gemini) use 8-way tensor parallelism within nodes, combined with pipeline parallelism across nodes, achieving the best balance of memory distribution and communication efficiency.

### Model Parallelism Advantages {#sec-distributed-training-model-parallelism-advantages}

The tensor parallelism analysis above demonstrated that 8-way parallelism reduces per-GPU memory requirements proportionally while incurring 2 AllReduce operations per transformer layer. This trade-off proves favorable in practice, enabling training of architectures that would otherwise be impossible on current hardware.

Memory scaling represents the primary advantage. Consider a 175B parameter model: storing parameters alone requires 700 GB (FP32) or 350 GB (FP16). With 8-way tensor parallelism, each GPU holds 44-87 GB of parameters, within the 80 GB capacity of an A100. Without model parallelism, this model simply cannot train on current hardware regardless of optimizer sharding or gradient accumulation. When accounting for optimizer states and gradients, the total memory footprint reaches 2.1-2.8 TB, making distribution across devices not merely beneficial but essential.

Efficient utilization of device memory follows directly from this distribution. Each device stores and processes only its portion of the model, enabling larger batch sizes without exceeding memory limits. An 8-way tensor parallel configuration can support batch sizes 4-8x larger per GPU than data parallel training of the same model would allow, improving both throughput and training stability.

Model parallelism provides flexibility across architectures. Sequential models in language processing naturally map to pipeline parallelism, while attention-heavy transformers benefit from tensor parallelism that distributes the attention matrices. Mixture-of-Expert architectures use expert parallelism to distribute conditional computation paths. This adaptability, combined with quantitative guidance on communication overhead at different parallel degrees, enables practitioners to select appropriate strategies for their specific architecture.

Finally, model parallelism complements other distributed training strategies. Production systems combine 8-way tensor parallelism within nodes with 8-16 stage pipeline parallelism across nodes and data parallelism across replica groups, achieving 45-65% parallel efficiency at 1000+ GPU scale. This hybrid flexibility enables scaling both model size and dataset simultaneously.

While model parallelism offers these quantifiable benefits, its effectiveness depends on careful partitioning strategy design. The following sections examine the challenges that arise in practice.

### Model Parallelism Limitations {#sec-distributed-training-model-parallelism-limitations}

While model parallelism provides an effective approach for training large-scale models, it also introduces unique challenges. These challenges arise from the complexity of partitioning the model and the dependencies between partitions during training. Addressing these issues requires careful system design and optimization.

One major challenge in model parallelism is balancing the workload across devices. Not all parts of a model require the same amount of computation. For instance, in layer-wise partitioning, some layers may perform significantly more operations than others, leading to an uneven distribution of work. Devices responsible for the heavier computations may become bottlenecks, leaving others underutilized. This imbalance reduces overall efficiency and slows down training. Identifying optimal partitioning strategies is critical to ensuring all devices contribute evenly.

Another challenge is data dependency between devices. During the forward pass, activation tensors of shape [batch_size, sequence_length, hidden_dimension] must transfer between devices. For a typical transformer model with batch size 32, sequence length 2048, and hidden dimension 2048, each transfer moves approximately 512 MB of data at float32 precision. With gradient transfers in the backward pass, a single training step can require several gigabytes of inter-device communication. On systems using PCIe interconnects with 64 GB/s theoretical bandwidth, these transfers introduce significant latency.

Model parallelism also increases the complexity of implementation and debugging. Partitioning the model, ensuring proper data flow, and synchronizing gradients across devices require detailed coordination. Errors in any of these steps can lead to incorrect gradient updates or even model divergence. Debugging such errors is often more difficult in distributed systems, as issues may arise only under specific conditions or workloads.

A further challenge is pipeline bubbles in pipeline parallelism. With $P$ pipeline stages and $M$ microbatches per training step, the pipeline experiences idle time at the start (fill) and end (drain) of the schedule. The fraction of time spent in the bubble is given by:

$$
\text{Bubble Fraction} = \frac{P - 1}{M}
$$

In an 8-stage pipeline with $P=8$ and 32 microbatches with $M=32$, the bubble overhead is $(8-1)/32 \approx 22\%$. Increasing $M$ reduces this overhead but increases the memory required to store activations for the pending backward passes. This trade-off between throughput with high $M$ and memory with low $M$ is the central optimization constraint in pipeline parallelism.

::: {.callout-note title="Figure Placeholder: Pipeline Bubble" collapse="true"}
```{.tikz}
% TODO: Space-time diagram of pipeline schedule showing idle bubbles
\node[draw, align=center] {Pipeline Schedule\nIdle Bubbles vs Active Compute};
```
**Pipeline Bubble Analysis**. A space-time diagram comparing naive pipeline execution versus 1F1B (One-Forward-One-Backward) scheduling. The "bubble" represents idle time (gray) where accelerators wait for data dependencies. Minimizing this bubble through micro-batching and efficient scheduling is the primary optimization goal in pipeline parallelism.
:::

Finally, model parallelism may be less effective for certain architectures, such as models with highly interdependent operations. In these cases, splitting the model may lead to excessive communication overhead, outweighing the benefits of parallel computation. For such models, alternative strategies like data parallelism or hybrid approaches might be more suitable.

Despite these challenges, model parallelism remains an indispensable tool for training large models. With careful optimization and the use of modern frameworks, many of these issues can be mitigated, enabling efficient distributed training at scale.

## Hybrid Parallelism {#sec-distributed-training-hybrid-parallelism}

The preceding sections revealed a fundamental tension: data parallelism scales throughput but requires each GPU to hold the complete model, while model parallelism enables larger models but reduces throughput through sequential dependencies. For GPT-3 scale training, with 175B parameters across 300B tokens, neither approach alone suffices [@narayanan_pipeline_parallelism_2021]. Data parallelism across 1024 GPUs would require each GPU to hold 700 GB of parameters. Pure model parallelism across 64 stages would create $(64-1)/32 \approx$ 98% pipeline bubble overhead with reasonable microbatch counts.

Hybrid parallelism resolves this tension by applying both strategies orthogonally: model parallelism splits the architecture to fit available memory, while data parallelism scales throughput across multiple model replicas. Training a 175-billion parameter language model on a dataset of 300 billion tokens demonstrates this approach in practice. The neural network layers distribute across multiple GPUs through model parallelism, while data parallelism enables different GPU groups to process separate batches. This dual strategy addresses both memory constraints from model size and computational demands from dataset scale simultaneously.

### Hybrid Parallelism Implementation {#sec-distributed-training-hybrid-parallelism-implementation}

Hybrid parallelism operates by combining the processes of model partitioning and dataset splitting, ensuring efficient utilization of both memory and computation across devices. This integration allows large-scale machine learning systems to overcome the constraints imposed by individual parallelism strategies.

#### Model and Data Partitioning {#sec-distributed-training-model-data-partitioning}

Hybrid parallelism divides both model architecture and training data across devices. The model divides through layer-wise or operator-level partitioning, where GPUs process distinct neural network segments. Simultaneously, the dataset splits into subsets, allowing each device group to train on different batches. A transformer model might distribute its attention layers across four GPUs, while each GPU group processes a unique 1,000-example batch. This dual partitioning distributes memory requirements and computational workload.

#### Forward Pass {#sec-distributed-training-forward-pass-hybrid}

During the forward pass, input data flows through the distributed model. Each device processes its assigned portion of the model using the data subset it holds. In a hybrid system with four devices, two devices might handle different layers of the model through model parallelism while simultaneously processing distinct data batches through data parallelism. Communication between devices ensures that intermediate outputs from model partitions are passed seamlessly to subsequent partitions.

#### Backward Pass and Gradient Calculation {#sec-distributed-training-backward-pass-gradient-calculation-hybrid}

During the backward pass, gradients are calculated for the model partitions stored on each device. Data-parallel devices that process the same subset of the model but different data batches aggregate their gradients, ensuring that updates reflect contributions from the entire dataset. For model-parallel devices, gradients are computed locally and passed to the next layer in reverse order. In a two-device model-parallel configuration, for example, the first device computes gradients for layers 1-3, then transmits these to the second device for layers 4-6. This combination of gradient synchronization and inter-device communication ensures consistency across the distributed system.

#### Parameter Updates {#sec-distributed-training-parameter-updates-hybrid}

After gradient synchronization, model parameters are updated using the chosen optimization algorithm. Devices working in data parallelism update their shared model partitions consistently, while model-parallel devices apply updates to their local segments. Efficient communication is critical in this step to minimize delays and ensure that updates are correctly propagated across all devices.

#### Iterative Process {#sec-distributed-training-iterative-process-hybrid}

Hybrid parallelism follows an iterative process similar to other training strategies. The combination of model and data distribution allows the system to process large datasets and complex models efficiently over multiple training epochs. By balancing the computational workload and memory requirements, hybrid parallelism enables the training of advanced machine learning models that would otherwise be infeasible.

### Parallelism Variations {#sec-distributed-training-parallelism-variations-hybrid}

Hybrid parallelism can be implemented in different configurations, depending on the model architecture, dataset characteristics, and available hardware. These variations allow for tailored solutions that optimize performance and scalability for specific training requirements.

#### Hierarchical Parallelism {#sec-distributed-training-hierarchical-parallelism}

Hierarchical hybrid parallelism applies model parallelism to divide the model across devices first and then layers data parallelism on top to handle the dataset distribution. In a system with eight devices, four devices may hold different partitions of the model while each partition is replicated across the other four devices for data parallel processing. This approach is well-suited for large models with billions of parameters, where memory constraints are a primary concern.

Hierarchical hybrid parallelism ensures that the model size is distributed across devices, reducing memory requirements, while data parallelism ensures that multiple data samples are processed simultaneously, improving throughput. This dual-layered approach is particularly effective for models like transformers, where each layer may have a significant memory footprint.

#### Intra-layer Parallelism {#sec-distributed-training-intralayer-parallelism}

Intra-layer hybrid parallelism combines model and data parallelism within individual layers of the model. In a transformer architecture, the attention mechanism can be split across multiple devices using model parallelism while each device processes distinct batches of data using data parallelism. This fine-grained integration allows the system to optimize resource usage at the level of individual operations, enabling training for models with extremely large intermediate computations.

This variation is particularly useful in scenarios where specific layers, such as attention or feedforward layers, have computationally intensive operations that are difficult to distribute effectively using model or data parallelism alone. Intra-layer hybrid parallelism addresses this challenge by applying both strategies simultaneously.

#### Inter-layer Parallelism {#sec-distributed-training-interlayer-parallelism}

Inter-layer hybrid parallelism focuses on distributing the workload between model and data parallelism at the level of distinct model layers. Early layers of a neural network may be distributed using model parallelism while later layers use data parallelism. This approach aligns with the observation that certain layers in a model may be more memory-intensive while others benefit from increased data throughput.

This configuration allows for dynamic allocation of resources, adapting to the specific demands of different layers in the model. By tailoring the parallelism strategy to the unique characteristics of each layer, inter-layer hybrid parallelism achieves an optimal balance between memory usage and computational efficiency.

### Hybrid Parallelism Advantages {#sec-distributed-training-hybrid-parallelism-advantages}

The quantitative analysis of preceding sections demonstrates why hybrid parallelism becomes essential at scale. Pure data parallelism achieves 85-95% efficiency up to 32 GPUs but degrades to 40-60% beyond 512 GPUs due to communication overhead. Pure model parallelism enables training of 175B+ parameter models but introduces 20-30% pipeline bubble overhead even with optimized scheduling. Hybrid 3D parallelism, combining 8-way tensor parallelism within nodes, 8-way pipeline parallelism across nodes, and 16-way data parallelism across replica groups, enables training 175B parameter models on 1024 GPUs with 45% parallel efficiency, compared to approximately 4% efficiency for pure data parallelism at the same scale.

The primary advantage lies in simultaneous scaling across model size and dataset throughput. A 3D parallel configuration for GPT-3 training distributes the 175B parameters across 8 tensor parallel GPUs within each node, reducing per-GPU memory from 700 GB to 87.5 GB. Pipeline parallelism across 8 stages further enables processing of long sequences without activation memory overflow. Data parallelism across 16 replica groups then scales throughput, achieving effective batch sizes of 1536 while maintaining gradient quality. This orthogonal decomposition addresses memory, throughput, and convergence constraints simultaneously.

Hardware utilization improves substantially over single-strategy approaches. Pure model parallelism leaves devices idle during pipeline bubbles (20-30% of training time). Pure data parallelism leaves devices waiting during gradient synchronization (10-40% of training time). Hybrid approaches overlap these operations: while one pipeline stage computes, another synchronizes gradients within its data parallel group. Megatron-LM demonstrates approximately 50-55% MFU (Model FLOPS Utilization) on 1024 A100 GPUs using 3D parallelism [@narayanan_pipeline_parallelism_2021], compared to 30-40% MFU for pure data parallelism at similar scale. These efficiency gains stem from the hierarchical communication structure that matches bandwidth availability at each level.

Communication overhead reduces through hierarchical structuring. Tensor parallelism restricts high-frequency communication to NVLink-connected GPUs within nodes (600 GB/s bandwidth). Pipeline parallelism limits cross-node communication to activation transfers at stage boundaries (requires 100-200 GB/s per stage). Data parallelism performs AllReduce across replica groups using InfiniBand (200 GB/s aggregate). This hierarchy matches communication patterns to available bandwidth at each level, avoiding the scenario where all communication competes for the slowest interconnect.

::: {.callout-note title="Figure Placeholder: Hybrid Parallelism Physical Mapping" collapse="true"}
```{.tikz}
% TODO: Diagram showing a cluster of nodes. Tensor Parallel within Node (NVLink). Pipeline Parallel across groups of Nodes. Data Parallel across replicas.
\node[draw, align=center] {Hybrid Parallelism Mapping\nTP(Intra) -> PP(Inter) -> DP(Replicas)};
```
**3D Parallelism Physical Mapping**. How Hybrid Parallelism maps to hardware topology. Tensor Parallelism uses high-bandwidth NVLink within a node. Pipeline Parallelism connects adjacent nodes via high-speed InfiniBand. Data Parallelism replicas can be placed anywhere, synchronizing gradients via the inter-node network.
:::

Hybrid parallelism enables training at scales that would otherwise be impossible. GPT-4 training required an estimated 10,000+ H100 GPUs for months. Without hybrid approaches, memory constraints would limit training to smaller models, throughput constraints would extend training time beyond practical limits, and communication overhead would waste the majority of available compute. The combination of tensor, pipeline, and data parallelism transforms these theoretical capabilities into practical training systems.

### Hybrid Parallelism Limitations {#sec-distributed-training-hybrid-parallelism-limitations}

While hybrid parallelism provides a robust framework for scaling machine learning training, it also introduces complexities that require careful consideration. These challenges stem from the intricate coordination needed to integrate both model and data parallelism effectively. Understanding these obstacles is crucial for designing efficient hybrid systems and avoiding potential bottlenecks.

One of the primary challenges of hybrid parallelism is communication overhead. Both model and data parallelism involve significant inter-device communication. In model parallelism, devices must exchange intermediate outputs and gradients to maintain the sequential flow of computation. In data parallelism, gradients computed on separate data subsets must be synchronized across devices. Hybrid parallelism compounds these demands, as it requires efficient communication for both processes simultaneously. If not managed properly, the resulting overhead can negate the benefits of parallelization, particularly in large-scale systems with slower interconnects or high network latency.

Another critical challenge is the complexity of implementation. Hybrid parallelism demands a nuanced understanding of both model and data parallelism techniques, as well as the underlying hardware and software infrastructure. Designing efficient hybrid strategies involves making decisions about how to partition the model, how to distribute data, and how to synchronize computations across devices. This process often requires extensive experimentation and optimization, particularly for custom architectures or non-standard hardware setups. While modern frameworks like PyTorch and TensorFlow provide tools for distributed training, implementing hybrid parallelism at scale still requires significant engineering expertise.

Workload balancing also presents a challenge in hybrid parallelism. In a distributed system, not all devices may have equal computational capacity. Some devices may process data or compute gradients faster than others, leading to inefficiencies as faster devices wait for slower ones to complete their tasks. Additionally, certain model layers or operations may require more resources than others, creating imbalances in computational load. Managing this disparity requires careful tuning of partitioning strategies and the use of dynamic workload distribution techniques.

Memory constraints remain a concern, even in hybrid setups. While model parallelism addresses the issue of fitting large models into device memory, the additional memory requirements for data parallelism, such as storing multiple data batches and gradient buffers, can still exceed available capacity. This is especially true for models with extremely large intermediate computations, such as transformers with high-dimensional attention mechanisms. Balancing memory usage across devices is essential to prevent resource exhaustion during training.

Lastly, hybrid parallelism poses challenges related to fault tolerance and debugging. Distributed systems are inherently more prone to hardware failures and synchronization errors. Debugging issues in hybrid setups can be significantly more complex than in standalone model or data parallelism systems, as errors may arise from interactions between the two approaches. Ensuring robust fault-tolerance mechanisms and designing tools for monitoring and debugging distributed systems are essential for maintaining reliability.

Despite these challenges, hybrid parallelism remains an indispensable strategy for training large-scale machine learning models. By addressing these obstacles through optimized communication protocols, intelligent partitioning strategies, and robust fault-tolerance systems, practitioners can unlock the full potential of hybrid parallelism and drive innovation in AI research and applications.

## Parallelism Strategy Comparison {#sec-distributed-training-parallelism-strategy-comparison}

The features of data parallelism, model parallelism, pipeline parallelism, and hybrid parallelism are summarized in @tbl-parallelism-compare. This comparison highlights their respective focuses, memory requirements, communication overheads, scalability, implementation complexity, and ideal use cases. By examining these factors, practitioners can determine the most suitable approach for their training needs.

+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+
| **Aspect**                        | **Data Parallelism**               | **Model Parallelism**              | **Pipeline Parallelism**           | **Hybrid Parallelism**              |
+:==================================+:===================================+:===================================+:===================================+:====================================+
| **Focus**                         | Distributes dataset across         | Distributes the model across       | Distributes model stages in        | Combines multiple parallelism       |
|                                   | devices, each with a full model    | devices, each handling a portion   | pipeline, processing microbatches  | strategies for balanced             |
|                                   | copy                               | of the model                       | concurrently                       | scalability                         |
+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+
| **Memory Requirement per Device** | High (entire model on each device) | Low (model split across devices)   | Low to Moderate (stages split      | Moderate (splits model and dataset  |
|                                   |                                    |                                    | across devices)                    | across devices)                     |
+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+
| **Communication Overhead**        | Moderate to High (gradient         | High (communication for            | Moderate (activation passing       | Very High (requires synchronization |
|                                   | synchronization across devices)    | intermediate activations and       | between stages)                    | for both model and data)            |
|                                   |                                    | gradients)                         |                                    |                                     |
+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+
| **Scalability**                   | Good for large datasets with       | Good for very large models with    | Good for deep models with many     | Excellent for extremely large       |
|                                   | moderate model sizes               | smaller datasets                   | layers                             | models and datasets                 |
+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+
| **Implementation Complexity**     | Low to Moderate (relatively        | Moderate to High (requires         | Moderate to High (requires         | High (complex integration of        |
|                                   | straightforward with existing      | careful partitioning and           | pipeline scheduling and            | multiple parallelism strategies)    |
|                                   | tools)                             | coordination)                      | microbatch management)             |                                     |
+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+
| **Ideal Use Case**                | Large datasets where model fits    | Extremely large models that exceed | Deep models with sequential stages | Training massive models on vast     |
|                                   | within a single device             | single-device memory limits        | that can tolerate microbatch       | datasets in large-scale systems     |
|                                   |                                    |                                    | latency                            |                                     |
+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+

: **Parallel Training Strategies**: Data, model, pipeline, and hybrid parallelism each address the challenges of scaling machine learning training by distributing workload across devices, differing in how they partition data and model parameters to optimize memory usage, communication, and scalability. Understanding these trade-offs enables practitioners to select the most effective approach for their specific model and infrastructure. {#tbl-parallelism-compare}

@fig-parallelism-flowchart provides a general guideline for selecting parallelism strategies in distributed training systems. While the chart offers a structured decision-making process based on model size, dataset size, and scaling constraints, it is intentionally simplified. Real-world scenarios often involve additional complexities such as hardware heterogeneity, communication bandwidth, and workload imbalance, which may influence the choice of parallelism techniques. The chart is best viewed as a foundational tool for understanding the trade-offs and decision points in parallelism strategy selection. Practitioners should consider this guideline as a starting point and adapt it to the specific requirements and constraints of their systems to achieve optimal performance.

::: {#fig-parallelism-flowchart fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{Line/.style={line width=1.0pt,black!50,text=black
},
  Box/.style={inner xsep=2pt,
    node distance=11mm,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    text width=27mm,align=flush center,
    minimum width=27mm, minimum height=9mm
  },
    Box1/.style={Box,
    draw=RedLine, fill=RedL,
    text width=31mm,
    minimum width=32mm,
    minimum height=10mm
  },
  Text/.style={inner xsep=2pt,
    draw=none, line width=0.75pt,
    fill=TextColor,
    font=\footnotesize\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm,
    minimum height=5mm
  },
 decision/.style = {align=flush center,text width=42mm,diamond, aspect=2.2, node distance=6mm,
                             inner xsep=-3pt,  inner ysep=-2.95ex,fill=VioletL2, draw=VioletLine},
}
\node[Box](B1){Hybrid\\ Parallelism};
\node[Box,node distance=16mm,right=of B1](B2){Model\\Parallelism};
\node[Box,node distance=16 mm,right=of B2](B3){Data\\ Parallelism};
\node[Box,right=of B3,fill=RedL, draw=RedLine](B4){Single Device Optimization};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,
yshift=-1mm,
fill=BackColor,fit=(B1)(B3),line width=0.75pt](BB){};
\node[decision,node distance=18mm,
above=of B4](G1B4){Is\\ the dataset\\ very large?};

\node[Box1,node distance=15mm,
above=of $(B2.north)!0.5!(B3.north)$](G1B3){Is scaling the model\\ or data more critical?};
\node[decision,above=of G1B3](G2B3){Are\\ both constraints\\ significant?};
\node[decision,above=of G2B3](G3B3){Does\\ the dataset fit in a\\  single device?};
\node[decision,above=of G3B3](G4B3){Does\\ the model fit in a\\ single device?};
\node[Box,node distance=5mm,above=of G4B3,fill=BlueL, draw=BlueLine](G5B3){Start};
%
\node[Box,below=1 of B2,fill=BlueL, draw=BlueLine](DB2){End};
%
\draw[Line,-latex](G5B3)--(G4B3);
\draw[Line,-latex](G4B3)--node[right,pos=0.35]{No}(G3B3);
\draw[Line,-latex](G4B3)-|node[above,pos=0.05]{Yes}(G1B4);
\draw[Line,-latex](G3B3)--node[right,pos=0.35]{No}(G2B3);
\draw[Line,-latex](G2B3)--node[right,pos=0.35]{No}(G1B3);
\draw[Line,-latex](G1B4)--node[right,pos=0.15]{No}(B4);
%
\draw[Line,-latex](G3B3.west)--node[above,pos=0.25]{Yes}++(180:2.3)|-(B2.west);
\draw[Line,-latex](G2B3)-|node[above,pos=0.05]{Yes}(B1);
\draw[Line,-latex](G1B3.south)--node[left,align=center,pos=0.45]{Scaling Model}++(270:8mm)-|(B2);
\draw[Line,-latex](G1B3.south)--++(270:8mm)-|(B3);
\draw[Line,-latex](G1B4)-|node[above,pos=0.22,text=black]{Yes}(B3.40);
%
\draw[Line,-latex](B1)|-(DB2);
\draw[Line,-latex](B3)|-(DB2);
\draw[Line,-latex](B2)--(DB2);
\node[above=2pt of  BB.204,inner sep=0pt,anchor=south,fill=BackColor]{Parallelism Opportunities};
\end{tikzpicture}
```
Distributed training systems use data, model, or hybrid parallelism based on model size, dataset size, and scaling constraints to accelerate training and efficiently utilize resources. This flowchart guides practitioners through a decision process, recognizing that real-world deployments often require adaptation due to factors like hardware heterogeneity and workload imbalance.
:::

## Framework Integration {#sec-distributed-training-framework-integration}

While the theoretical foundations of distributed training establish the mathematical principles for scaling across multiple devices, modern frameworks provide abstractions that make these concepts accessible to practitioners. Understanding how frameworks like PyTorch translate distributed training theory into practical APIs bridges the gap between mathematical concepts and implementation.

### Data Parallel Framework APIs {#sec-distributed-training-data-parallel-framework-apis}

The data parallelism mechanisms we explored earlier—gradient averaging, AllReduce communication, and parameter synchronization—are abstracted through framework APIs that handle the complex coordination automatically. PyTorch provides two primary approaches that demonstrate different trade-offs between simplicity and performance.

`torch.nn.DataParallel` represents the simpler approach, automatically replicating the model across available GPUs within a single node. This API abstracts the gradient collection and averaging process, requiring minimal code changes to existing single-GPU training scripts. However, this simplicity comes with performance limitations, as the implementation uses a parameter server approach[^fn-parameter-server] [@li2014parameter] that can create communication bottlenecks when scaling beyond 4-8 GPUs.

[^fn-parameter-server]: **Parameter Server**: A distributed architecture where dedicated server nodes store model parameters while worker nodes compute gradients on data. Workers pull parameters, compute gradients, and push updates to servers. While intuitive and flexible, this centralized design creates bandwidth bottlenecks at servers, leading modern systems to prefer decentralized AllReduce patterns where workers communicate directly without a central coordinator.

```python
# Simple data parallelism - framework handles gradient synchronization
model = torch.nn.DataParallel(model)
# Training loop remains unchanged - framework automatically:
# 1. Splits batch across GPUs
# 2. Replicates model on each device
# 3. Gathers gradients and averages them
# 4. Broadcasts updated parameters
```

For production scale training, `torch.distributed` provides the high-performance alternative that implements the efficient AllReduce communication patterns discussed earlier. This API requires explicit initialization of process groups and distributed coordination but enables the linear scaling characteristics essential for large-scale training.

```python
# Production distributed training - explicit control over communication
import torch.distributed as dist

dist.init_process_group(backend="nccl")  # NCCL for GPU communication
model = torch.nn.parallel.DistributedDataParallel(model)
# Framework now uses optimized AllReduce instead of parameter server
```

The key insight is that `DistributedDataParallel` implements the efficient ring AllReduce algorithm automatically, transforming the O(n) communication complexity we discussed into practical code that achieves 90%+ parallel efficiency at scale. The framework handles device placement, gradient bucketing for efficient communication, and overlapping computation with communication.

### Model Parallel Framework Support {#sec-distributed-training-model-parallel-framework-support}

Model parallelism requires more explicit coordination since frameworks must manage cross-device tensor placement and data flow. PyTorch addresses this through manual device placement and the emerging `torch.distributed.pipeline` API for pipeline parallelism.

```python
# Manual model parallelism - explicit device placement
class ModelParallelNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers_gpu0 = nn.Sequential(...).to("cuda:0")
        self.layers_gpu1 = nn.Sequential(...).to("cuda:1")

    def forward(self, x):
        x = self.layers_gpu0(x.to("cuda:0"))
        x = self.layers_gpu1(
            x.to("cuda:1")
        )  # Cross-GPU data transfer
        return x
```

This manual approach exposes the sequential dependencies and communication overhead inherent in model parallelism, requiring careful management of tensor movement between devices. The framework automatically handles the backward pass gradient flow across device boundaries, but practitioners must consider the performance implications of frequent device transfers.

### Communication Primitives {#sec-distributed-training-communication-primitives}

Modern frameworks expose the communication operations that enable distributed training through high-level APIs. These primitives abstract the low-level NCCL operations while maintaining performance:

```python
# Framework-provided collective operations
dist.all_reduce(tensor)  # Gradient averaging across all devices
dist.broadcast(tensor, src=0)  # Parameter broadcasting from master
dist.all_gather(
    tensor_list, tensor
)  # Collecting tensors from all devices
```

These APIs translate directly to the NCCL collective operations that implement the efficient communication patterns discussed earlier, demonstrating how frameworks provide accessible interfaces to complex distributed systems concepts while maintaining the performance characteristics essential for production training.

The framework abstractions enable practitioners to focus on model architecture and training dynamics while leveraging sophisticated distributed systems optimizations. This separation of concerns, where mathematical foundations are handled by the framework while model design remains under practitioner control, exemplifies how modern ML systems balance accessibility with performance. Yet these abstractions cannot hide the fundamental constraints of the underlying hardware. The efficiency of distributed training ultimately depends on physical interconnect bandwidth, memory capacity, and synchronization latency, constraints that determine what the framework abstractions can actually achieve.

## Hardware Infrastructure for Scale {#sec-distributed-training-hardware-infrastructure}

While framework abstractions successfully hide implementation complexity, the physical hardware determines what these abstractions can achieve in practice. The parallelism strategies examined in previous sections, from data parallelism's gradient synchronization to hybrid parallelism's multi-dimensional coordination, all depend critically on the underlying hardware. Data parallelism requires high-bandwidth gradient synchronization, where the 10-40% communication overhead established in our efficiency analysis translates to terabytes of gradient traffic per training step at scale. Model parallelism demands low-latency point-to-point links for activation transfers between pipeline stages. Tensor parallelism pushes interconnect requirements even further, requiring NVLink-class bandwidth for the 2 AllReduce operations per transformer layer. This section examines the hardware architectures that enable these communication patterns, analyzing how multi-chip systems introduce qualitatively different constraints around communication overhead, memory coherence, and fault tolerance.

### Multi-Chip AI Acceleration {#sec-distributed-training-multichip-acceleration}

The transition from single-chip to multi-chip architectures represents more than simple replication. It requires rethinking how computations distribute across processors, how data flows between chips, and how systems maintain coherence at scale. Where single-chip optimization focuses on maximizing utilization within fixed resources, multi-chip systems must balance computational distribution against communication overhead, memory coherence costs, and synchronization complexity. These challenges fundamentally transform the optimization landscape, requiring new abstractions and techniques beyond those developed for individual accelerators.

The scaling of AI systems follows a natural progression: integration within a single package through chiplet architectures, multi-GPU configurations within a server, distributed accelerator pods, and wafer-scale integration. Each approach presents unique trade-offs between computational density, communication overhead, and system complexity. Chiplet architectures maintain high-speed interconnects within a package, while distributed systems sacrifice communication latency for massive parallelism.

#### Chiplet-Based Architectures {#sec-distributed-training-chiplet-architectures}

Chiplet architectures achieve scaling by partitioning large designs into smaller, modular dies that are interconnected within a single package. Small, specialized semiconductor dies connect together within a single package to create larger, more complex processors. AMD's EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies, with compute chiplets in 7 nm paired with I/O chiplets in 14 nm, optimizing each function independently.

Modern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM (High Bandwidth Memory)[^fn-hbm] stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning.

[^fn-hbm]: **High Bandwidth Memory (HBM)**: A 3D-stacked DRAM technology that places memory dies directly on top of or beside the processor die, connected via thousands of through-silicon vias. HBM3 provides up to 819 GB/s bandwidth per stack, while the newer HBM3e variant reaches 1.2 TB/s, roughly 10x that of GDDR6. The H100 GPU uses 80 GB of HBM3, enabling it to hold large model weights close to compute. HBM's proximity to the processor dramatically reduces memory access latency and energy compared to traditional off-chip memory.

AMD's Instinct MI300 exemplifies this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.

However, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence, and thermal management become critical factors as more chiplets are integrated. Memory coherence presents particular complexity: ensuring all processors in a system see the same consistent view of shared memory when multiple cores or chips access the same data. Traditional cache coherence protocols like MESI add 10-50 ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive, so most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.

#### Multi-GPU Systems {#sec-distributed-training-multi-gpu-systems}

Beyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.

A common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network).

NVSwitch interconnects enable high-speed communication between GPUs, reducing bottlenecks in distributed training. However, scaling up the number of GPUs introduces fundamental distributed coordination challenges that become the dominant performance constraint. The arithmetic intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient synchronization across GPUs, where AllReduce operations must aggregate 175 billion parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth, but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds available capacity.

The coordination complexity compounds exponentially. While two GPUs require a single communication channel, eight GPUs need 28 interconnect paths, and fault tolerance requirements mandate redundant communication patterns. Memory consistency protocols further complicate coordination as different GPUs may observe weight updates at different times, requiring sophisticated synchronization primitives that can add 10-50 microseconds latency per training step. These seemingly small delays aggregate to hours of training time across million-iteration runs.

#### Communication Overhead and Amdahl's Law {#sec-distributed-training-amdahl-analysis}

The fundamental limitation of distributed AI training stems from Amdahl's Law[^fn-amdahl], which quantifies how communication overhead constrains parallel speedup regardless of available compute power.

[^fn-amdahl]: **Amdahl's Law**: Named after computer architect Gene Amdahl, who formulated it in 1967 to argue against massively parallel architectures. The law states that speedup from parallelization is limited by the sequential portion of a program. If 10% of a program must run sequentially, maximum speedup is 10x regardless of processor count. In distributed training, gradient synchronization represents this sequential bottleneck, explaining why 1000 GPUs never achieve 1000x speedup.

For distributed neural network training, communication overhead during gradient synchronization creates a sequential bottleneck that limits scalability even with infinite parallelism.

The maximum speedup achievable with distributed training is bound by Amdahl's Law:
$$
\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}
$$
where $P$ is the fraction of work that can be parallelized and $N$ is the number of processors. For AI training, the correct formulation accounts for communication time that does not decrease with more workers:
$$
\text{Speedup} = \frac{T_{compute}}{T_{compute}/N + T_{comm}}
$$
where $T_{comm}$ is largely independent of $N$ for ring AllReduce (or grows as $\log N$ for tree-based approaches). This can be rewritten as:
$$
\text{Speedup} = \frac{N}{1 + N \cdot (T_{comm}/T_{compute})}
$$

Consider training a 175 B parameter model with 1000 H100 GPUs as a concrete example:

- **Computation time per iteration**: 100 ms of forward/backward passes per GPU
- **Gradient size**: 175 B parameters × 4 bytes = 700 GB in FP32
- **Ring AllReduce time**: For ring AllReduce, each GPU sends/receives $2 \times (N-1)/N \times 700\text{GB} \approx 1.4\text{TB}$

With ring AllReduce across 1000 GPUs connected via 600 GB/s links:

- **Intra-node (8 GPUs via NVLink)**: $1.4\text{TB} / 600\text{GB/s} \approx 2.3$ seconds
- **Inter-node adds latency**: $1000 \times \alpha$ where $\alpha \approx 1\mu s$ per hop

The resulting scaling efficiency is:
$$
\text{Efficiency} = \frac{T_{compute}}{T_{compute} + T_{comm}} = \frac{100\text{ms}}{100\text{ms} + 2300\text{ms}} \approx 4\%
$$

This is why real systems use mixed precision (FP16 = 350 GB, halving communication), gradient compression, and pipeline parallelism. With FP16 and 4-way pipeline parallelism reducing synchronization to 1/4 of parameters per stage, efficiency improves dramatically:
$$
\text{Efficiency} = \frac{100\text{ms}}{100\text{ms} + 290\text{ms}} \approx 26\%
$$

This demonstrates why pure data parallelism fails at scale and why hybrid strategies are essential.

Communication requirements scale superlinearly with model size and linearly with the number of parameters. Modern transformer models require gradient synchronization across all parameters during each training step:

- **GPT-3 (175 B parameters)**: 700 GB gradient exchange per step
- **GPT-4 (estimated 1.8 T parameters)**: approximately 7 TB gradient exchange per step
- **Future 10 T parameter models**: approximately 40 TB gradient exchange per step

Even with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization for 10 T parameter models would require 22+ seconds per training step, making distributed training impractical without algorithmic innovations like gradient compression or asynchronous updates.

#### TPU Pods {#sec-distributed-training-tpu-pods}

With gradient synchronization demanding terabytes of bandwidth per training step, purpose-built accelerator clusters become essential for training at scale. Google's TPU Pods[^fn-tpu-pod] represent a pioneering approach to this challenge, interconnecting hundreds of TPUs to function as a unified system [@jouppi2017tpu].

[^fn-tpu-pod]: **TPU Pod**: A tightly integrated cluster of Google Tensor Processing Units connected via custom high-bandwidth interconnects. The TPU v4 Pod contains 4,096 TPU chips providing over 1 exaflop of peak compute. Unlike loosely coupled GPU clusters, TPU Pods are designed from the ground up for collective operations, using a 3D torus network topology that provides deterministic, low-latency communication for distributed training workloads.

The architectural design of TPU Pods differs fundamentally from traditional multi-GPU systems. While multi-GPU configurations typically rely on NVLink or PCIe connections within a single machine, TPU Pods employ high-bandwidth optical links to interconnect accelerators at data center scale. TPU v4 implements a 3D torus interconnect topology, enabling efficient data exchange between accelerators while minimizing communication bottlenecks as workloads scale across nodes.

The effectiveness of this architecture is demonstrated in its performance scaling capabilities. TPU Pod performance exhibits near-linear scaling when running ResNet-50, from quarter-pod to full-pod configurations. The system achieves a remarkable 33.0x speedup when scaled to 1024 chips compared to a 16-TPU baseline.

However, distributing AI workloads across an entire data center introduces distributed coordination challenges that fundamentally differ from single-node systems. Even the 3D torus interconnect, while providing high bisection bandwidth, creates communication bottlenecks when training large transformer models that require AllReduce operations across all 4,096 TPUs. Each parameter gradient must traverse multiple hops through the torus network, with worst-case communication requiring dozens of hops between distant TPUs, creating latency penalties that compound with model size.

The energy cost of coordination also scales dramatically: moving data across the pod's optical interconnect consumes 1000x more energy than on-chip communication within individual TPUs, transforming distributed training into a careful balance between computation parallelism and communication efficiency where AllReduce bandwidth, not compute capacity, determines overall training throughput.

#### Wafer-Scale AI {#sec-distributed-training-wafer-scale}

At the frontier of AI scaling, wafer-scale integration represents a paradigm shift that abandons traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication.

Wafer-scale integration[^fn-wafer-scale] uses an entire 300 mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores, 125x more than the largest GPUs.

[^fn-wafer-scale]: **Wafer-Scale Integration**: The radical approach of fabricating an entire processor on a single silicon wafer rather than dicing it into individual chips. Cerebras pioneered this for AI with their WSE (Wafer Scale Engine), which eliminates inter-chip communication bottlenecks by keeping all cores on a single die. The engineering challenges include managing defects (solved with redundant cores), distributing power across 56,000 mm² of silicon, and cooling 23 kW of heat.

Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23 kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.

The primary advantage of wafer-scale AI is its ultra-fast on-die communication. Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries between separate devices, wafer-scale AI enables near-instantaneous data transfer across its vast compute array. This architecture drastically reduces communication latency, achieving performance levels that remain out of reach for conventional multi-chip systems.

Achieving this level of integration introduces substantial engineering challenges. Thermal dissipation, fault tolerance, and manufacturing yield become major constraints when fabricating a processor of this scale. Unlike distributed TPU systems, which mitigate failures by dynamically re-routing workloads, wafer-scale AI must incorporate built-in redundancy mechanisms to tolerate localized defects in the silicon.

### Hardware Scaling Trade-offs {#sec-distributed-training-scaling-tradeoffs}

The progressive scaling of AI acceleration introduces new challenges at each step, from single-chip processors to increasingly complex architectures. @tbl-distributed-scaling-trajectory summarizes these trade-offs across different scaling approaches.

+----------------------+-------------------------------------+-----------------------------------------------------+
| **Scaling Approach** | **Key Feature**                     | **Challenges**                                      |
+:=====================+:====================================+:====================================================+
| **Chiplets**         | Modular scaling within a package    | Inter-chiplet latency, memory coherence             |
+----------------------+-------------------------------------+-----------------------------------------------------+
| **Multi-GPU**        | External GPU interconnects (NVLink) | Synchronization overhead, communication bottlenecks |
+----------------------+-------------------------------------+-----------------------------------------------------+
| **TPU Pods**         | Distributed accelerator clusters    | Interconnect congestion, workload partitioning      |
+----------------------+-------------------------------------+-----------------------------------------------------+
| **Wafer-Scale AI**   | Entire wafer as a single processor  | Thermal dissipation, fault tolerance                |
+----------------------+-------------------------------------+-----------------------------------------------------+

: **AI Acceleration Scaling Trade-offs**: Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution. {#tbl-distributed-scaling-trajectory}

While chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.

### Multi-Chip Execution Strategies {#sec-distributed-training-execution-strategies}

As AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.

Execution mapping in multi-chip systems requires computation placement that considers workload partitioning across multiple accelerators, with explicit coordination of execution order and dependencies. Computation scheduling must be interconnect-aware to manage communication delays effectively. Load balancing across accelerators is vital, as uneven task distribution results in some accelerators remaining underutilized while others operate at full capacity.

Distributed memory allocation requires each accelerator to manage its own local memory, necessitating explicit allocation of model parameters, activations, and intermediate data across devices. Unlike single-chip execution where data is fetched once and reused, multi-chip setups require deliberate strategies to minimize redundant data transfers.

Data movement optimization addresses inter-chip data transfer, which becomes the primary bottleneck rather than memory hierarchy latency. Techniques include overlapping computation and communication so accelerators process data while simultaneously sending and receiving, and locality-aware scheduling that places computations on accelerators already holding required data.

Compiler and runtime adaptations extend single-chip execution models to handle dynamic workload distribution across accelerators. Interconnect-aware workload partitioning enables compilers to distribute computations strategically based on communication cost. In TPU Pods, the runtime schedules computations across multiple TPU cores to minimize communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution synchronizes operations across GPUs while maintaining execution order.

## Fallacies and Pitfalls {#sec-distributed-training-fallacies-pitfalls}

Distributed training involves counterintuitive behavior that leads to common misconceptions. These fallacies and pitfalls capture errors that waste compute resources and delay research progress.

Linear speedup remains theoretically impossible regardless of engineering effort. Amdahl's Law establishes hard limits: any sequential component bounds maximum speedup regardless of parallelism. In distributed training, gradient synchronization is inherently sequential since all gradients must be collected before any update proceeds.

Even with perfect overlap and optimal algorithms, communication overhead grows with cluster size. For data parallelism, AllReduce time increases logarithmically with tree algorithms or linearly in the latency term with ring algorithms as GPU count grows. A 1000-GPU cluster will never train 1000x faster than a single GPU; achieving 500x speedup would be exceptional, and 100-200x is more typical for communication-heavy workloads.

Organizations that budget projects assuming linear scaling inevitably miss deadlines and overspend on compute.

Hyperparameters tuned on small clusters fail catastrophically at large scale. The most critical is learning rate: as batch size increases with data parallelism, learning rate typically must increase proportionally to maintain convergence rate. The "linear scaling rule"[^fn-linear-scaling] [@goyal2017accurate] suggests $\eta_{large} = \eta_{base} \times (B_{large}/B_{base})$.

[^fn-linear-scaling]: **Linear Scaling Rule**: Discovered empirically by Facebook AI Research when training ImageNet in 1 hour on 256 GPUs. When batch size increases by factor k, learning rate should also increase by factor k to maintain similar convergence behavior. This rule enables efficient distributed training but breaks down beyond the "critical batch size" where gradient noise becomes too low for effective exploration of the loss landscape.

However, this rule has limits. Beyond the "critical batch size" (model and dataset dependent, often 8K-32K for vision models), increasing batch size provides diminishing returns. Larger batches find sharper minima that generalize poorly. Training that converges beautifully at 256 GPUs may diverge or produce worse models at 1024 GPUs with naive scaling.

Warmup schedules, weight decay, and dropout rates also require adjustment. The only reliable approach is systematic scaling studies that validate hyperparameters at target scale.

Data parallelism does not scale indefinitely by adding more GPUs. Data parallelism increases effective batch size proportionally with GPU count, but statistical efficiency (loss reduction per sample) decreases with batch size beyond model-specific thresholds. A 100K-sample batch may provide only 2x the gradient quality of a 10K-sample batch, not 10x.

The critical batch size defines where marginal returns collapse. Beyond this point, additional GPUs increase throughput (samples per second) but not training efficiency (loss reduction per compute dollar). For BERT, critical batch size is approximately 8K; for ResNet, approximately 32K. Scaling beyond critical batch size wastes compute.

Large organizations have trained models to convergence using 1024 GPUs in the same wall-clock time as 512 GPUs at 2x the cost, because they exceeded critical batch size.

Pipeline parallelism and tensor parallelism both distribute model weights across devices, but their memory and compute characteristics differ dramatically.

Tensor parallelism splits each layer across devices, requiring AllReduce communication within each layer. This reduces memory proportionally but introduces communication overhead on the critical path. Pipeline parallelism assigns complete layers to devices, requiring only point-to-point communication between stages but introducing bubble overhead.

For memory-constrained scenarios where a model barely fits with splitting, tensor parallelism's even distribution helps. For throughput-maximizing scenarios with adequate memory, pipeline parallelism's lower communication overhead helps. Choosing based on one dimension (memory or compute) without considering the other leads to suboptimal configurations.

FSDP and ZeRO do not always improve training efficiency. FSDP (Fully Sharded Data Parallel) and ZeRO reduce memory footprint by sharding optimizer state and gradients across GPUs. This enables larger batch sizes or larger models per GPU. However, sharding introduces communication overhead: AllGather before forward pass, ReduceScatter after backward pass.

For models that fit comfortably in GPU memory without sharding, FSDP adds overhead without benefit. A 7B model training on A100-80GB with batch size 4 runs faster with DDP than FSDP because the model fits entirely with room for activations.

FSDP provides value when:

- Model + optimizer state exceeds single-GPU memory
- Enabling larger batch sizes justifies communication overhead
- ZeRO-Offload to CPU extends effective memory

Applying FSDP universally, as some tutorials suggest, degrades performance for models that do not require it.

Parallelism overhead is roughly constant regardless of model size: AllReduce time depends on gradient size, not model computation time. For small models where forward/backward pass takes 10ms and AllReduce takes 5ms, communication overhead is 50%. For large models where forward/backward takes 1000ms and AllReduce takes 5ms, overhead is 0.5%.

Decisions made based on small-model benchmarks ("pipeline parallelism is always slower") invert at scale. The 20% overhead acceptable for a 1B model becomes 0.2% for a 100B model. Parallelism strategy evaluation must occur at target scale, or at minimum with analytical models that extrapolate appropriately.

Gradient accumulation is not free. Gradient accumulation simulates larger batch sizes by accumulating gradients across multiple forward/backward passes before synchronizing. This reduces communication frequency proportionally. However, accumulation has costs:

1. **Memory**: Accumulated gradients consume memory throughout the accumulation window
2. **Latency**: Effective step time increases proportionally with accumulation steps
3. **Precision**: Accumulated FP16 gradients may overflow or underflow

For loss-sensitive early training, gradient accumulation can introduce instability from accumulated numerical errors. Organizations that use gradient accumulation to work around infrastructure limitations (slow network, small GPUs) sometimes discover training divergence that disappears with proper infrastructure.

## Summary {#sec-distributed-training-summary}

Distributed training transforms the computational challenge of training large-scale machine learning models into an orchestration problem across multiple devices and hardware architectures. Throughout this chapter, we have examined how parallelism strategies—data, model, pipeline, and hybrid—address different constraints in the training process. Data parallelism scales dataset processing by distributing training examples across devices while synchronizing gradients, achieving near-linear speedups when communication overhead remains manageable. Model parallelism addresses memory constraints by partitioning models across devices, enabling training of architectures that exceed single-device capacity. Pipeline parallelism improves hardware utilization through microbatching, while hybrid approaches combine these strategies for the largest-scale training workloads.

The hardware infrastructure enabling distributed training spans multiple scales: chiplet-based architectures within a single package, multi-GPU systems connected via NVLink and NVSwitch, TPU Pods with 2D torus interconnects, and wafer-scale integration. Each approach presents distinct trade-offs between communication bandwidth, memory coherence, and system complexity. Amdahl's Law quantifies the fundamental scaling limits: communication overhead during gradient synchronization creates sequential bottlenecks that constrain speedup regardless of available compute power, explaining why adding GPUs beyond approximately 100 provides diminishing returns without algorithmic innovations.

The efficiency metrics governing distributed training—communication overhead, scaling efficiency, and synchronization costs—directly influence system design decisions. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads, balancing the complexity of distributed coordination against the computational benefits of parallelization. Framework abstractions like PyTorch's DistributedDataParallel translate these concepts into practical implementations, allowing practitioners to leverage sophisticated distributed systems optimizations while focusing on model development.

::: {.callout-important title="Key Takeaways"}
* Data parallelism achieves near-linear scaling when communication overhead remains below 30-40% of training time
* Model parallelism enables training of models exceeding single-device memory but introduces sequential dependencies
* Pipeline parallelism reduces device idle time through microbatching, improving hardware utilization
* Hybrid parallelism combines strategies for training the largest models on the largest datasets
* Multi-chip hardware (chiplets, multi-GPU, TPU Pods, wafer-scale) each present distinct trade-offs between integration density, interconnect bandwidth, and system complexity
* Amdahl's Law quantifies scaling limits: communication overhead constrains speedup regardless of available compute power
* Framework APIs abstract distributed complexity while preserving the performance characteristics essential for production training
:::

The parallelism strategies examined here determine what data must move between devices. Data parallelism synchronizes gradients through AllReduce operations. Model parallelism exchanges activations through point-to-point communication. Pipeline parallelism coordinates microbatch handoffs across stages. The next chapter (@sec-communication) examines these communication patterns in depth: the collective operations that enable distributed training, the algorithms that implement them efficiently, and the optimizations that hide communication latency behind computation. Understanding communication is essential because at scale, network bandwidth and latency, not compute throughput, typically determine training efficiency.

The principles established in this chapter provide the foundation for understanding fault tolerance mechanisms (@sec-fault-tolerance), which become increasingly critical as distributed training scales to thousands of devices where failures become statistically inevitable.
