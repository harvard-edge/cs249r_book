---
engine: jupyter
---

```{python}
#| echo: false
#| label: chapter-start
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER START
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter initialization and global imports
# │
# │ Why: Registers this chapter with the mlsys registry and provides shared
# │      imports for all subsequent calculation cells.
# │
# │ Imports: mlsys.registry, mlsys.constants, mlsys.formatting
# │ Exports: (none)
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.registry import start_chapter
from mlsys.constants import (
    A100_MEM_CAPACITY, H100_MEM_CAPACITY,
    NVLINK_A100_BW, NVLINK_H100_BW, H100_MEM_BW,
    GPT3_PARAMS, GPT4_EST_PARAMS,
    GB, second, Mparam, Tparam, THOUSAND,
    SEC_PER_HOUR, SEC_PER_DAY, MILLION, TRILLION, BITS_PER_BYTE, TB
)
from mlsys.formatting import fmt, sci, check

start_chapter("vol2:distributed_training")
```

# Distributed Training Systems {#sec-distributed-training-systems}

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_distributed.png){fig-alt="Distributed training across multiple nodes and accelerators." width=100%}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{30}{100}{25}{10}
\end{marginfigure}

_Why does the linear logic of "more hardware = faster training" collapse at the Bisection Bandwidth Wall?_

Distributed training appears simple: split the work across machines and combine the results. But as the Machine Learning Fleet grows, a new physics emerges. Communication costs scale with the number of machines while computation per machine shrinks, until synchronization overhead dominates and adding hardware actively degrades performance. *When* you train on a single GPU, you optimize for arithmetic intensity; *when* you train on 10,000 GPUs, you optimize for communication intensity. The scaling ceiling is not a bug to be fixed but a fundamental property of the **Reliability Gap** and **Communication-Computation Ratio**: coordinating independent machines requires moving terabytes of state across networks that are orders of magnitude slower than on-chip memory. The art of distributed training is managing this tension—partitioning work to minimize the **Coordination Tax**, overlapping communication with computation to hide latency, and choosing synchronization strategies that balance consistency against throughput. Without this understanding, organizations waste millions on hardware that sits idle waiting for gradients to arrive, or produce models that never converge because stale updates corrupted the optimization.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Apply the **Law of Distributed Efficiency** to diagnose when a workload is compute-bound, memory-bound, or communication-bound
- Implement **Data Parallelism** using gradient synchronization and AllReduce to achieve 85--95% efficiency in the linear scaling regime
- Apply **Memory-Efficient Data Parallelism** (ZeRO, FSDP) to shard optimizer states and parameters, quantifying the communication trade-off
- Design **Tensor Parallelism** strategies for transformer layers by partitioning matrix operations to exploit high-bandwidth intra-node interconnects
- Construct **Pipeline Parallelism** schedules with microbatching to minimize bubble overhead and maximize fleet utilization
- Select **Synchronization Models** (BSP, SSP, Asynchronous) based on the CAP theorem trade-offs for cluster heterogeneity and convergence
- Architect **Hybrid 3D Parallelism** configurations that combine Data, Tensor, and Pipeline strategies to train **Archetype A (GPT-4 / Llama-3)** models

:::

```{python}
#| label: dist-train-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ DISTRIBUTED TRAINING SYSTEM CONSTANTS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-distributed-training-systems-systems-multimachine-scaling-fundamentals-ff96
# │          and ZeRO/FSDP analysis paragraphs throughout the chapter.
# │
# │ Goal: Establish GPU memory limits (A100_MEM_CAPACITY, H100_MEM_CAPACITY)
# │       and intra-node interconnect bandwidth (NVLINK_A100_BW, NVLINK_H100_BW)
# │       for large-scale distributed clusters, and compute GPT-3/GPT-4 scale
# │       (GPT3_PARAMS, GPT4_EST_PARAMS) to anchor why distribution is necessary.
# │ Show: "80" GB A100 memory and "600" GB/s NVLink H100 bandwidth — inline
# │       in the memory exhaustion and interconnect hierarchy paragraphs.
# │ How: Convert Mparam -> billions via /THOUSAND; Tparam direct .m_as().
# │
# │ Imports: mlsys.constants (A100_MEM_CAPACITY, H100_MEM_CAPACITY,
# │           NVLINK_A100_BW, NVLINK_H100_BW, GPT3_PARAMS, GPT4_EST_PARAMS,
# │           GB, second, Mparam, Tparam, THOUSAND)
# │ Exports: a100_mem, h100_mem, nvlink_a100, nvlink_h100,
# │          gpt3_params_b, gpt4_est_params_t
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.constants import (
    A100_MEM_CAPACITY, H100_MEM_CAPACITY,
    NVLINK_A100_BW, NVLINK_H100_BW, H100_MEM_BW,
    GPT3_PARAMS, GPT4_EST_PARAMS,
    GB, second, Mparam, Tparam, THOUSAND,
    SEC_PER_HOUR, SEC_PER_DAY, MILLION, TRILLION, BITS_PER_BYTE, TB
)
from mlsys.formatting import fmt, sci, check

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class DistTrainSetup:
    """
    Namespace for Distributed Training reference specs.
    Scenario: GPU and interconnect parameters for large-scale clusters.
    """

    # ┌── 1. PARAMETERS (Inputs) ───────────────────────────────────────────────
    a100_cap = A100_MEM_CAPACITY
    h100_cap = H100_MEM_CAPACITY
    nvlink_a100_bw = NVLINK_A100_BW
    nvlink_h100_bw = NVLINK_H100_BW
    
    gpt3_p = GPT3_PARAMS.m_as(Mparam)
    gpt4_p = GPT4_EST_PARAMS.m_as(Tparam)

    # ┌── 2. CALCULATION (The Physics) ─────────────────────────────────────────
    gpt3_params_b_val = gpt3_p / 1000

    # ┌── 3. INVARIANTS (Guardrails) ───────────────────────────────────────────
    check(gpt3_params_b_val == 175, f"Expected 175B params, got {gpt3_params_b_val}")

    # ┌── 4. OUTPUTS (Formatting) ──────────────────────────────────────────────
    a100_mem = f"{a100_cap.m_as(GB):.0f}"
    h100_mem = f"{h100_cap.m_as(GB):.0f}"
    nvlink_a100 = f"{nvlink_a100_bw.m_as(GB/second):.0f}"
    nvlink_h100 = f"{nvlink_h100_bw.m_as(GB/second):.0f}"
    
    gpt3_params_b = f"{gpt3_params_b_val:.0f}"
    gpt4_est_params_t = f"{gpt4_p:.1f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
a100_mem = DistTrainSetup.a100_mem
h100_mem = DistTrainSetup.h100_mem
nvlink_a100 = DistTrainSetup.nvlink_a100
nvlink_h100 = DistTrainSetup.nvlink_h100
gpt3_params_b = DistTrainSetup.gpt3_params_b
gpt4_est_params_t = DistTrainSetup.gpt4_est_params_t
```

## Why Distribution Is Necessary {#sec-distributed-training-systems-systems-multimachine-scaling-fundamentals-ff96}
