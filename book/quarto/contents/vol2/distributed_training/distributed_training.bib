@article{brown2020language,
  title = {Language models are few-shot learners},
  author = {Brown, Tom and others},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
}

@article{deepspeed_training_system_2021,
  title = {
    DeepSpeed: System optimizations enable training deep learning models with over 100 billion
    parameters
  },
  author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  year = {2020},
  journal = {arXiv preprint arXiv:2020.12},
}

@inproceedings{goyal2017accurate,
  title = {Accurate, large minibatch SGD: Training ImageNet in 1 hour},
  author = {Goyal, Priya and others},
  year = {2017},
  booktitle = {arXiv preprint arXiv:1706.02677},
}

@inproceedings{harlap2018pipedream,
  title = {PipeDream: Fast and efficient pipeline parallel DNN training},
  author = {Harlap, Aaron and others},
  year = {2019},
  booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages = {1--15},
}

@article{narayanan_pipeline_parallelism_2021,
  title = {Efficient large-scale language model training on GPU clusters using Megatron-LM},
  author = {Narayanan, Deepak and others},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.04473},
}

@article{rajbhandari2020zero,
  title = {ZeRO: Memory optimizations toward training trillion parameter models},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Rber, Olatunji and He, Yuxiong},
  year = {2020},
  journal = {arXiv preprint arXiv:1910.02054},
}

@inproceedings{sergeev2018horovod,
  title = {Horovod: fast and easy distributed deep learning in TensorFlow},
  author = {Sergeev, Alexander and Del Balso, Mike},
  year = {2018},
  booktitle = {arXiv preprint arXiv:1802.05799},
}

@article{shazeer_mixture_of_experts_2017,
  title = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author = {Shazeer, Noam and others},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.06538},
}
