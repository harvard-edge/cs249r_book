---
quiz: frontiers_quizzes.json
concepts: frontiers_concepts.yml
glossary: frontiers_glossary.json
---

# Emerging Systems Challenges {#sec-emerging-systems-challenges}

::: {layout-narrow}
::: {.column-margin}
*Gemini Pro 3 Prompt: Technical visualization showing emerging challenges in large-scale ML systems. The image depicts interconnected engineering challenges: compute scaling limits represented by plateauing curves, memory hierarchies shown as layered stacks, energy constraints illustrated by thermodynamic bounds, and compound system architectures as modular components. The style emphasizes quantitative rigor with graphs, formulas, and system diagrams. Circuit patterns and data flows connect all elements, showing how today's systems face tomorrow's constraints. Technical blues and greens dominate, suitable for an advanced textbook. Rendered in the style of Nanobanana. High resolution, rectangular image with golden ratio dimensions.*
:::

\noindent
![](images/png/cover_frontiers.png)

:::

## Purpose {.unnumbered}

_Why do the scaling strategies that drove machine learning progress face diminishing returns that demand fundamentally different approaches?_

For a decade, the dominant strategy in machine learning was reliable: scale compute, scale data, scale parameters, and capabilities reliably improved. Organizations that could afford more resources could achieve more capable systems. But this era is ending. The highest-quality data has been harvested. Energy costs grow faster than efficiency gains. Scaling laws reveal diminishing returns where each doubling of compute yields smaller improvements. The physical, economic, and thermodynamic boundaries that once seemed distant are now visible constraints that determine what systems can be built. Organizations that plan infrastructure investments based on extrapolating historical trends will discover their assumptions no longer hold. The next generation of ML systems will be built not by those who can spend the most but by those who can innovate around the constraints that spending alone cannot overcome.

::: {.callout-tip title="Learning Objectives"}

- Analyze scaling law limitations quantitatively, including Chinchilla optimal scaling, data exhaustion timelines, and diminishing returns on capability improvements

- Design compound AI system architectures as engineering solutions that overcome monolithic model constraints through modular specialization and orchestration patterns

- Evaluate alternative architectural paradigms (state space models, energy-based models) for computational efficiency trade-offs with uncertainty quantification

- Assess training methodology costs including RLHF resource requirements, Constitutional AI computational overhead, and alignment tax as permanent operational cost

- Quantify fundamental constraints on ML systems including thermodynamic bounds on energy efficiency, memory bandwidth limits, and communication bottlenecks

- Apply quantitative reasoning to frontier ML systems decisions through cost-benefit analysis, uncertainty estimation, and falsifiable capability projections

:::

::: {.callout-note title="Connection: The Systems Sandwich"}
We conclude our journey by examining frontier constraints. The Systems Sandwich is not a static structure; it evolves as systems approach fundamental limits. This chapter explores how the layers we have studied (Logic, Physics, Service, Hardening, and Society) face emerging constraints that require quantitative analysis rather than speculative extrapolation.
:::

## From Current Systems to Emerging Constraints {#sec-emerging-systems-challenges-current-systems-emerging-constraints-05c3}

The applications examined in @sec-ai-good demonstrate what current AI systems can achieve when deployed responsibly: healthcare diagnostics that improve patient outcomes, climate models that inform policy decisions, educational tools that reach underserved communities. The responsible practices developed for these systems (attention to bias, human oversight, and equitable access) become more critical as systems scale. Understanding where current approaches face fundamental constraints prepares practitioners to make evidence-based decisions about system architecture and resource allocation.

When tasked with planning a complex, multi-day project, ChatGPT generates plausible-sounding plans that often contain logical flaws[^fn-rapid-evolution]. When asked to recall details from previous conversations, it fails due to lack of persistent memory. When required to explain why a particular solution works through first principles reasoning, it reproduces learned patterns rather than demonstrating genuine comprehension. These failures represent fundamental architectural limitations, not simple bugs. Contemporary models lack persistent memory, causal reasoning, and robust planning capabilities, constraints that warrant quantitative analysis rather than speculation about future breakthroughs.

[^fn-rapid-evolution]: **A Rapidly Evolving Field**: AI capabilities advance at extraordinary pace. Since this chapter was written, new models (GPT-4o, Claude 3.5, Gemini 2.0, DeepSeek, and OpenAI's o1/o3 reasoning models) have pushed boundaries further. The o1 and o3 models demonstrate that explicit reasoning chains and extended inference time computation can improve complex problem solving, representing a shift from pure scaling toward inference time optimization. While specific benchmarks and model names will continue to evolve, the systems engineering principles, architectural patterns, and fundamental constraints discussed here remain durable. **Uncertainty caveat**: Predicting which capabilities will emerge and when remains unreliable; this chapter focuses on quantifiable constraints rather than capability timelines.

The engineering challenge involves understanding where scaling continues to yield improvements and where fundamental limits apply. While contemporary large-scale systems demonstrate capabilities across diverse domains, they face quantifiable constraints in data availability, energy consumption, and architectural flexibility. Recent advances in engineering principles enable us to analyze these constraints systematically and design systems that work within them effectively.

This chapter examines emerging constraints through systems engineering principles established throughout this textbook. Rather than speculating about future capabilities, we focus on quantifiable limitations and engineering trade-offs that practitioners must navigate when building and scaling ML systems.

The analysis proceeds along three interconnected directions that define contemporary engineering frontiers. First, we examine scaling law limitations quantitatively, analyzing where Chinchilla-optimal scaling breaks down, when data exhaustion constrains further improvement, and what diminishing returns mean for infrastructure investment. Second, we analyze compound AI systems as practical engineering architectures that overcome monolithic model constraints through orchestration of specialized components, treating these as solutions to current limitations rather than pathways toward qualitatively different systems. Third, we explore emerging computational paradigms including energy-based models, state space architectures, and neuromorphic computing, evaluating their engineering trade-offs with appropriate uncertainty quantification.

These developments have concrete implications for ML systems engineering decisions. Data engineering must accommodate multimodal, streaming, and synthetically generated content at scales that challenge existing pipeline architectures, but practitioners need quantitative frameworks for evaluating when synthetic data helps versus when it introduces quality degradation. Training infrastructure requires coordination of heterogeneous computational substrates, but the cost-benefit analysis differs substantially based on workload characteristics. Model optimization must balance capability preservation with deployment efficiency, requiring frameworks that quantify trade-offs rather than assuming optimization is always beneficial.

The significance of these constraints extends to strategic infrastructure decisions. Contemporary architectural choices regarding data representation, computational resource allocation, and system modularity should be grounded in quantitative analysis of current constraints rather than speculation about future capabilities. The engineering principles governing these choices shape what systems can reliably achieve today and what investments are likely to yield returns.

This chapter grounds its analysis in systematic application of established engineering methodologies with explicit uncertainty quantification. Where evidence supports quantitative claims, we provide specific numbers with confidence bounds. Where uncertainty is high, we acknowledge it rather than presenting speculation as prediction. This approach enables practitioners to make defensible decisions about emerging systems rather than betting on uncertain capability trajectories.

## Quantifying System Constraints {#sec-emerging-systems-challenges-quantifying-system-constraints-068a}

::: {.callout-definition title="Scaling Constraints"}
***Scaling constraints*** represent the quantifiable limits where additional compute, data, or parameters yield diminishing or negligible improvements in system capability. Understanding these constraints enables evidence-based infrastructure decisions rather than unbounded scaling assumptions.
:::

Contemporary ML systems face measurable constraints that determine infrastructure investment returns. While ChatGPT and Claude demonstrate strong capabilities within language domains, and specialized systems defeat world champions at chess and Go, extending these capabilities faces specific limitations that warrant quantitative analysis rather than speculative extrapolation[^fn-intelligence-theory].

[^fn-intelligence-theory]: **Capability vs. Scalability**: The distinction between what systems can do and how that capability scales with resources is critical for infrastructure planning. Current systems require large datasets for statistical correlation while humans generalize from few examples, but whether this gap closes with scale or represents architectural constraints remains an open empirical question. The symbol grounding problem [@harnad1990symbol] (how abstract symbols connect to embodied experience) exemplifies capabilities where scaling has not demonstrated clear progress.

Consider the constraints facing current architectures. The brain coordinates specialized subsystems through hierarchical integration: sensory cortices process multimodal input, the hippocampus consolidates episodic memories, the prefrontal cortex orchestrates executive control, and the cerebellum refines motor predictions. Each subsystem operates with distinct computational principles and extreme energy efficiency (approximately 20W total). Current ML systems lack this modularity and efficiency, facing constraints that compound architecture principles (explored later in this chapter) partially address.

Current systems excel at pattern matching but face documented limitations in causal understanding. When ChatGPT solves a physics problem, it uses statistical correlations from training data rather than modeling physical laws. When DALL-E generates an image, it combines learned visual patterns without understanding three-dimensional structure or lighting physics. These limitations stem from architectural constraints: transformers process information through attention mechanisms optimized for sequence modeling, not causal reasoning or spatial understanding. **Whether these limitations yield to scale or require architectural innovation remains uncertain**, but the question is empirically testable.

Energy-based models offer an alternative framework that addresses some constraints by providing optimization-driven reasoning rather than autoregressive generation (detailed in @sec-agi-systems-energybased-models-learning-optimization-e4c6). Rather than predicting the most probable next token, these systems find configurations that minimize global energy functions. The engineering trade-offs involve substantially higher inference costs for potential improvements in constraint satisfaction and planning tasks.

Understanding scaling constraints requires analyzing where current approaches face diminishing returns and where alternative architectures offer different trade-off curves. Four research directions illustrate different approaches to these constraints, each with distinct engineering implications and resource requirements. **Uncertainty caveat**: Which approach will prove most effective remains unknown; the following analysis focuses on quantifiable trade-offs rather than predicting outcomes.

### Scaling Laws: Empirical Observations and Quantitative Limits {#sec-emerging-systems-challenges-scaling-laws-empirical-observations-quantitative-limits-a8c8}

Scaling laws represent one of the most significant empirical findings in contemporary ML, revealing consistent, predictable relationships between model performance and three key factors: parameter count $N$, dataset size $D$, and compute budget $C$ [@kaplan2020scaling]. However, as ML systems engineers, we must distinguish empirical observation within tested regimes from speculative extrapolation beyond them, and critically examine quantitative limitations that constrain scaling as an indefinite strategy.

#### Scaling Laws: Empirical Evidence and Confidence Bounds {#sec-emerging-systems-challenges-scaling-laws-empirical-evidence-confidence-bounds-c344}

Empirically, test loss follows power law relationships: L(N) ∝ N^(-α) for parameters, L(D) ∝ D^(-β) for data, and L(C) ∝ C^(-γ) for compute, where α ≈ 0.076, β ≈ 0.095, and γ ≈ 0.050 [@kaplan2020scaling]. **Within the tested regime** (approximately 10^17 to 10^24 FLOPs), these relationships hold consistently across multiple independent studies. The Kaplan et al. formulation provides:

$$L(N, D) = \left(\frac{N_c}{N}\right)^\alpha + \left(\frac{D_c}{D}\right)^\beta$$

where $L$ represents cross-entropy loss, $N$ is parameter count, $D$ is dataset size, and $N_c$, $D_c$ are critical scales at which power laws break down. This equation predicts how loss decreases as training resources increase, enabling infrastructure planning based on projected capability improvements.

::: {.callout-note title="Figure: Neural Scaling Laws" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \begin{axis}[
    title={\textbf{Scaling Laws (Log-Log Scale)}},
    xlabel={Compute (FLOPs)},
    ylabel={Test Loss},
    xmode=log,
    ymode=log,
    grid=both,
    major grid style={dashed, gray!30},
    width=10cm,
    height=6cm,
    xmin=1e18, xmax=1e26,
    ymin=1, ymax=10,
    legend pos=north east,
    tick label style={font=\tiny},
    title style={font=\small}
  ]
    % Power law line: L(C) = a * C^(-0.05)
    \addplot[domain=1e18:1e26, samples=100, ultra thick, blue] {100 * x^(-0.05)};
    \addlegendentry{Predicted Loss}

    % Actual model points (approximate)
    \addplot[only marks, mark=*, red] coordinates {
      (1e19, 7.9) % GPT-2 Small
      (1e21, 6.3) % GPT-2 XL
      (1e23, 5.0) % GPT-3
      (1e25, 4.0) % GPT-4 (est)
    };
    \addlegendentry{Frontier Models}

    % Labels
    \node[anchor=south west, font=\tiny, text=red] at (axis cs:1e23, 5.0) {GPT-3};
    \node[anchor=south west, font=\tiny, text=red] at (axis cs:1e25, 4.0) {GPT-4};

    % AGI Target region
    \fill[green!20, opacity=0.3] (axis cs:1e26, 1) rectangle (axis cs:1e28, 10);
    \node[rotate=90, font=\tiny, text=green!60!black] at (axis cs:2e26, 3) {AGI Frontier?};

  \end{axis}
\end{tikzpicture}
```
**Neural Scaling Laws**. Empirical power-law relationship between compute budget and model performance (test loss). The linear trend on a log-log plot indicates predictable improvements from scaling, driving the current "bigger is better" paradigm. However, the plateauing of high-quality data and the extreme energy costs at the far right ($10^{26}$ FLOPs) suggest the need for architectural innovations (SSMs, EBMs) beyond pure scaling.
:::

Recent developments have expanded the scaling hypothesis beyond training-time compute to include inference-time compute. OpenAI's o1 and o3 reasoning models demonstrate that allowing models to "think longer" during inference through explicit chain-of-thought reasoning and search over solution paths can dramatically improve performance on complex reasoning tasks.

::: {.callout-example title="Engineering Metric: The Inference Compute Multiplier"}
**The Shift**: Instead of training a bigger model, we can spend more compute *during inference* to search for better answers. This is the "System 2" approach.

**Formula**:
$$ C_{inference} \approx N_{tokens} \times C_{per\_token} \times K_{paths} $$
where $K_{paths}$ is the number of reasoning paths explored (via Tree of Thoughts, Best-of-N, etc.).

**Scenario**: Solving a Math Olympiad problem.
*   **Standard Inference (Zero-Shot)**: Generate 1 solution ($K=1$). Cost = 1x. **Accuracy = 20%**.
*   **Reasoning Inference (Search)**: Generate 100 candidate chains, verify steps, backtrack on error. $K=100$.
    *   **Cost**: **100x** higher latency and energy.
    *   **Accuracy**: **75%**.

**Conclusion**: Inference-time compute can substitute for training-time scale. A smaller model with $100\times$ inference budget may outperform a model $10\times$ larger, allowing engineers to trade latency for intelligence dynamically.
:::

This suggests a new scaling dimension: rather than solely investing compute in larger models, allocating compute to extended inference enables models to tackle problems requiring multi-step reasoning, planning, and self-verification.

**Critical limitation**: These scaling laws describe test loss (perplexity), which measures how well models predict training distribution. The relationship between loss reduction and capability improvement is not linear or predictable. Loss improvements provide diminishing capability returns as models approach low perplexity, and different capabilities may have different scaling characteristics.

The uncertainty increases substantially for extrapolation beyond tested regimes. Some researchers extrapolate that continued scaling would require approximately 2.5 × 10²⁶ FLOPs[^fn-compute-extrapolation] to achieve qualitatively different capabilities. However, this extrapolation assumes: (1) scaling laws continue beyond tested regimes, (2) capability improvements track loss improvements, and (3) no fundamental architectural constraints exist. **Each assumption carries substantial uncertainty**, and historical AI predictions show systematic overconfidence in extrapolation.

[^fn-compute-extrapolation]: **Compute Extrapolation Uncertainty**: Some estimates project 2.5 × 10²⁶ FLOPs would yield qualitatively different capabilities, but this involves extrapolating 100-1000× beyond tested regimes. At current H100 efficiency, this would require: 175,000 GPUs for one year, 122 MW power consumption (comparable to 100,000 US homes), and approximately $52 billion total cost. **Uncertainty bounds**: Actual requirements could differ by 1-3 orders of magnitude in either direction. Historical AI capability predictions show median errors of 10-100× on 5-10 year timescales [@grace2018will].

#### Quantitative Limitations of Scaling {#sec-emerging-systems-challenges-quantitative-limitations-scaling-4c0a}

Critical evaluation reveals four fundamental limitations that constrain scaling as an indefinite strategy, each quantifiable through engineering analysis. These are not speculative concerns but measurable constraints that affect current infrastructure planning.

**Power Laws Describe Loss, Not Capabilities.** The Kaplan et al. scaling law predicts cross-entropy loss $L$ as a function of parameters $N$ and data $D$, providing smooth, continuous improvements. However, this metric measures how well models predict training distribution, not whether they develop qualitative capabilities like reasoning or causal understanding. "Emergent capabilities" that appear suddenly at certain scales manifest as discontinuities in task-specific evaluation metrics, not in the loss curves that scaling laws describe [@schaeffer2023emergent].

For instance, multi-digit addition appears suddenly when models reach sufficient scale, and in-context learning exhibits threshold behavior rather than gradual improvement. These discontinuities raise a critical question: are these genuine phase transitions where new capabilities emerge, or are they evaluation artifacts where smooth capability improvements cross arbitrary metric thresholds? Schaeffer et al. (2023) demonstrate that many reported "emergent" capabilities result from discontinuous evaluation metrics (like exact match accuracy) rather than fundamental capability phase transitions. When evaluated with continuous metrics (like token edit distance), the same capabilities show smooth, predictable improvements consistent with scaling laws. This distinction matters profoundly: if capabilities emerge smoothly but metrics measure discontinuously, then scaling provides predictable improvements. If capabilities genuinely exhibit phase transitions, extrapolation becomes speculative.

**Data Scaling Hits Quantifiable Walls.** High-quality text data faces exhaustion before capability plateaus, creating a measurable constraint on scaling strategies. Current estimates place internet-scale high-quality text at approximately 10¹³ tokens (web articles, books, academic papers, code repositories), with uncertainty bounds of 0.5-2× [@villalobos2022will].

**Chinchilla optimal scaling** [@hoffmann2022training] establishes that compute-efficient training requires balanced scaling of parameters and data: $D \approx 20N$, meaning tokens should equal approximately 20× parameters. This finding fundamentally changed the economics of model training. GPT-3 (175B parameters, 300B tokens) had a Chinchilla ratio of only 1.7, making it substantially under-trained. Chinchilla (70B parameters, 1.4T tokens) achieved a ratio of 20 and outperformed the much larger GPT-3. Current frontier models like LLaMA-2-70B (70B parameters, 2T tokens) operate near-optimally with ratios around 29.

The constraint becomes concrete: training a 100 trillion parameter model at Chinchilla-optimal ratio would require $2 \times 10^{15}$ tokens, representing a 200× shortfall compared to estimated available high-quality data. **This is not a speculative future problem but a current constraint affecting infrastructure decisions.**

This creates a quantitative dilemma with four options, each with measurable trade-offs. First, train data-starved models that cannot utilize parameter capacity, accepting 10-30% higher loss than Chinchilla-optimal. Second, generate synthetic training data using models, risking quality degradation and potential model collapse (documented degradation of 5-15% on reasoning benchmarks after multiple generations). Third, include lower-quality data (noisy web content, social media), accepting 3-8% performance degradation on quality-sensitive tasks. Fourth, accept parameter limits around current model sizes, investing instead in architectural innovation.

Each option has measurable consequences for training efficiency, model capability, and deployment costs. The choice between them depends on specific use cases, not on speculation about which approach might eventually achieve qualitatively different capabilities.

**Inference-Time vs. Training-Time Scaling.** OpenAI's o1 and o3 models demonstrate that allocating compute during inference provides different trade-offs than training larger models. Training-time scaling improves pre-training loss through increased model capacity. Inference-time scaling performs task-specific computation: searching over possible solutions, verifying candidate answers, and refining outputs through iterative improvement.

This distinction carries measurable economic implications. Training-time scaling follows power laws with diminishing returns: doubling parameters reduces loss by progressively smaller amounts. Inference-time scaling has different cost structures: 10-100× more compute per query, but only for queries that benefit from extended reasoning. For mathematics, generating candidate proofs and verifying them through symbolic checkers enables solving problems where direct generation fails. For coding, generating multiple programs and executing against test suites ensures correctness independent of generation accuracy.

**Quantitative trade-off**: A model using 100× inference compute on complex queries costs 100× more per such query. Whether this trade-off is economically favorable depends on: (1) the fraction of queries requiring extended reasoning, (2) the value difference between correct and incorrect answers, and (3) the cost of alternative approaches like human verification. **There is no universal answer**; the optimal balance is workload-specific.

The systems implications are concrete. Training-time scaling requires datacenter-scale distributed training infrastructure optimized for throughput. Inference-time scaling demands different optimizations: latency-sensitive serving for interactive search, speculative execution for candidate generation, and efficient verification for correctness checking. This suggests systems will increasingly resemble compound architectures coordinating specialized components rather than monolithic models, but this is an engineering observation about current constraints, not a prediction about future capabilities.

**Pattern Matching vs. Compositional Reasoning.** A fundamental question underlies scaling extrapolation: when models solve complex problems, do they perform genuine compositional reasoning or retrieve similar patterns from training data? This distinction determines whether scaling improves reasoning or merely expands pattern coverage.

Quantitative diagnostics reveal concerning gaps. Compositional generalization benchmarks like SCAN and COGS test recombination of known elements into novel combinations. Training on "jump twice" and "walk thrice," then testing on "jump thrice" requires compositional understanding that thrice applies generally to actions. Large language models achieve near-zero accuracy on such compositional recombination despite strong performance on distribution-matched test sets. This suggests pattern matching dominates: models memorize training distribution patterns rather than learning compositional rules enabling systematic generalization.

Further evidence emerges from problems designed to differ from training distribution. When GPT-4 solves mathematical problems similar to training examples, performance remains strong. When problems require applying known concepts in novel combinations absent from training data, performance degrades substantially. If scaling primarily improves pattern coverage rather than reasoning capability, then AGI requires either exhaustive training data covering all possible problem variations (combinatorially infeasible) or architectural changes enabling compositional generalization.

#### Alternative Perspectives on Scaling {#sec-emerging-systems-challenges-alternative-perspectives-scaling-90fa}

The ML research community holds divergent views on scaling's potential, grounded in different empirical observations and theoretical frameworks.

Scaling optimists (OpenAI, Anthropic, Google DeepMind) emphasize emergent capabilities: as models scale, they demonstrate qualitative improvements on tasks absent from explicit training objectives. GPT-3's few-shot learning emerged without specific training for in-context learning. GPT-4 exhibits stronger reasoning than GPT-3.5 beyond what loss reduction predicts. These observations suggest that sufficient scale may produce capabilities including causal reasoning and robust generalization, even if mechanisms remain poorly understood.

Scaling skeptics (LeCun, Marcus, Chollet) argue that scaling improves pattern matching but cannot produce genuine reasoning without architectural innovation. Current models excel at correlation but struggle with causation: when GPT-4 explains why airplanes fly, it reproduces training data patterns rather than understanding aerodynamic principles. Extrapolating to AGI assumes correlation-based learning will spontaneously produce causal understanding, a claim lacking theoretical justification. This perspective emphasizes sample efficiency gaps: humans learn basic physics from hundreds of observations while models require billions of examples, suggesting fundamental differences between human reasoning and scaled pattern matching.

Evidence synthesis suggests truth lies between extremes. Scaling demonstrably improves capabilities on many tasks, validating optimists' observations. However, compositional reasoning gaps and sample efficiency differences indicate limitations, validating skeptics' concerns. The path forward likely requires both continued scaling and architectural innovations enabling causal reasoning, compositional generalization, and efficient learning.

#### Implications for Systems Engineering {#sec-emerging-systems-challenges-implications-systems-engineering-4490}

How should ML systems engineers prepare infrastructure given scaling uncertainty? Several principles emerge from this analysis. First, build flexible infrastructure supporting both scaling and architectural diversity. Datacenter designs should accommodate scaling current transformers while enabling experimentation with alternative architectures (state space models, energy-based models, world models). This hedges against uncertainty: if scaling continues proving effective, infrastructure supports it; if architectural innovations prove necessary, infrastructure adapts.

Second, invest in inference optimization as o1/o3 suggest inference-time compute may matter more than training-time scale. Serving infrastructure should support search, verification, and iterative refinement, not just single-pass generation. This enables exploiting inference-time scaling's different efficiency properties.

Third, develop evaluation beyond perplexity since loss reduction does not guarantee capability emergence. Benchmark suites should test causal reasoning, compositional generalization, and out-of-distribution robustness, providing early warnings when scaling approaches fundamental limits.

Fourth, monitor data quality, not just quantity, as high-quality training data faces exhaustion. Data pipelines should prioritize curation and synthetic generation methods that preserve quality rather than maximizing token counts through lower-quality sources.

As systems engineers, our responsibility involves making infrastructure decisions despite uncertainty about future capabilities. The scaling hypothesis provides one pathway to AGI, grounded in empirical scaling laws but facing quantitative limitations. Critical evaluation of these limitations, combined with exploration of alternative architectures, positions us to adapt as evidence accumulates about which approaches prove most promising.

### Hybrid Neurosymbolic Architectures {#sec-agi-systems-hybrid-neurosymbolic-architectures-7d8d}

The scaling hypothesis represents one pathway to AGI, grounded in empirical observations but constrained by the limitations examined above. A fundamentally different paradigm asks: can alternative computational approaches address capabilities that no amount of scaling achieves? Hybrid neurosymbolic architectures offer such an alternative, arguing that genuine reasoning requires mechanisms distinct from statistical pattern matching. Where scaling increases parameter coverage across training distributions, neurosymbolic approaches add qualitatively different computational primitives for logical inference and symbolic manipulation.

Current transformers excel at correlation but struggle with causation. When ChatGPT explains why planes fly, it reproduces patterns from training data rather than understanding aerodynamic principles. This limitation cannot be resolved through additional scale if the underlying architecture lacks mechanisms for causal reasoning.

Hybrid neurosymbolic systems[^fn-neurosymbolic] combine neural networks for perception and pattern recognition with symbolic engines for reasoning and planning. This approach argues that pure scaling cannot address certain capability limitations because statistical learning differs fundamentally from logical reasoning [@marcus2020next]. **Whether this architectural difference matters for practical systems remains an empirical question.**

**System 1 vs. System 2 Thinking**

This distinction mirrors Daniel Kahneman's cognitive framework. System 1 (current AI) is fast, intuitive, and pattern-matching. LLMs excel here, generating plausible text instantly based on statistical correlations. System 2 (deliberative reasoning) is slow, deliberative, and logical. This requires explicit multi-step reasoning, verification, and search. **Whether hybrid approaches or scaled models better achieve System 2 capabilities remains contested.**

Neurosymbolic architectures aim to implement System 2 by delegating reasoning to symbolic engines. Where neural networks excel at pattern matching across high dimensional spaces, symbolic systems provide verifiable logical inference, constraint satisfaction, and causal reasoning through explicit rule manipulation.

[^fn-neurosymbolic]: **Neurosymbolic AI**: Integration of neural networks (learning, pattern recognition) with symbolic systems (logic, reasoning). The term gained prominence in the 2010s as researchers sought to combine deep learning's perception capabilities with classical AI's reasoning guarantees. Key challenge: neural networks use continuous differentiable representations while symbolic systems use discrete non-differentiable operations, requiring specialized gradient estimation techniques like REINFORCE or straight-through estimators at the interface.

AlphaGeometry [@alphageometry2024] exemplifies this integration through complementary strengths. The neural component, a transformer trained on 100 million synthetic geometry problems, learns to suggest promising construction steps (adding auxiliary lines, identifying similar triangles) that would advance toward a proof. The symbolic component, a deduction engine implementing classical geometry axioms, rigorously verifies each suggested step and systematically explores logical consequences. This division of labor mirrors human mathematical reasoning: intuition suggests promising directions while formal logic validates correctness. The system solved 25 of 30 International Mathematical Olympiad geometry problems, matching the performance of an average gold medalist while producing human readable proofs verifiable through symbolic rules.

Engineering neurosymbolic systems requires reconciling two computational paradigms. Neural components operate on continuous representations optimized through gradient descent, while symbolic components manipulate discrete symbols through logical inference. The integration challenge spans multiple levels: representation alignment (mapping between vector embeddings and symbolic structures), computation coordination (scheduling GPU-optimized neural operations alongside CPU-based symbolic reasoning), and learning synchronization (backpropagating through non-differentiable symbolic operations). Frameworks such as PyTorch, TensorFlow, and JAX must evolve to support these heterogeneous computations within unified training loops.

### Embodied Intelligence {#sec-agi-systems-embodied-intelligence-77ad}

Both scaling and neurosymbolic approaches assume intelligence can emerge from disembodied computation. The third paradigm challenges this assumption, arguing that genuine intelligence requires physical grounding in the world. This perspective emerged from robotics research observing that even simple insects navigating complex terrain demonstrate behaviors that pure symbolic reasoning struggles to replicate, suggesting sensorimotor coupling provides fundamental scaffolding for intelligence.

The embodied intelligence paradigm, rooted in Brooks' subsumption architecture [@brooks1986robust] and Pfeifer's morphological computation [@pfeifer2007body], contends that intelligence requires sensorimotor grounding through continuous perception-action loops. Abstract reasoning, this view holds, emerges from physical interaction rather than disembodied computation. Consider how humans learn "heavy" not through verbal definition but through physical experience lifting objects, developing intuitive physics through embodied interaction. Language models can recite that "rocks are heavier than feathers" without understanding weight through sensorimotor experience, potentially limiting their reasoning about physical scenarios.

RT-2 (Robotics Transformer 2) [@rt2023robotics] demonstrates early progress bridging this gap through vision-language-action models. By fine-tuning PaLM-E, a 562B parameter vision-language model, on robotic manipulation datasets containing millions of robot trajectories, RT-2 achieves 62% success on novel tasks compared to 32% for vision-only baselines. Critically, it transfers internet-scale knowledge to physical tasks: when shown a picture of an extinct animal and asked to "pick up the extinct animal," it correctly identifies and grasps a toy dinosaur, demonstrating semantic understanding grounded in physical capability. The architecture processes images through a visual encoder, concatenates with language instructions, and outputs discretized robot actions (joint positions, gripper states) that the control system executes. This end-to-end learning from pixels to actions represents a departure from traditional robotics pipelines separating perception, planning, and control into distinct modules.

Embodied systems face unique engineering constraints absent in purely digital intelligence. Real-time control loops demand sub-100&nbsp;ms inference latency for stable manipulation, requiring on-device deployment from @sec-edge-intelligence rather than cloud inference where network round-trip latency alone exceeds control budgets. The control hierarchy operates at multiple timescales: high-level task planning (10-100 Hz, "grasp the cup"), mid-level motion planning (100-1000 Hz, trajectory generation), and low-level control (1000+ Hz, motor commands with proprioceptive feedback). Each layer must complete inference within its cycle time while maintaining safety constraints that prevent self-collision, workspace violations, or excessive forces that could damage objects or injure humans.

Power constraints impose severe limitations compared to datacenter systems. A mobile robot operates on 100-500&nbsp;W total power budget (batteries, actuators, sensors, computation) versus a datacenter's megawatts for model inference alone. Boston Dynamics' Atlas humanoid robot dedicates approximately 1 kW to hydraulic actuation and 100-200W to onboard computation, forcing aggressive model compression and efficient architectures. This drives neuromorphic computing interest: Intel's Loihi [@davies2018loihi] processes visual attention tasks at 1000× lower power than GPUs, making it viable for battery-powered systems. The power-performance trade-off becomes critical: running a 7B parameter model at 10 Hz for real-time inference requires 50-100W on mobile GPUs, consuming substantial battery capacity that reduces operational time from hours to minutes.

Safety-critical operation necessitates formal verification methods beyond the statistical guarantees of pure learning systems. When Tesla's Full Self-Driving operates on public roads or surgical robots manipulate near vital organs, probabilistic guarantees prove insufficient. Embodied AGI requires certified behavior: provable bounds on states the system can enter, guaranteed response times for emergency stops, and verified fallback behaviors when learning-based components fail. This motivates hybrid architectures combining learned policies for nominal operation with hard-coded safety controllers that activate on anomaly detection, verified through formal methods proving the combined system satisfies safety specifications. The verification challenge intensifies with learning: continual adaptation from experience must preserve safety properties even as policies evolve.

These constraints may prove advantageous for AGI development. Biological intelligence evolved under similar limitations, achieving remarkable efficiency through sensorimotor grounding. Efficient AGI might emerge from resource-constrained embodied systems rather than datacenter-scale models, with physical interaction providing the inductive bias necessary for sample-efficient learning. The embodiment hypothesis suggests that intelligence arises from agents acting in environments under resource constraints, making embodied approaches not just one path to AGI but potentially a necessary component of general intelligence. For compound systems, this suggests integrating embodied components that handle physical reasoning, grounding abstract concepts in sensorimotor experience even within predominantly digital architectures.

### Multi-Agent Systems and Emergent Intelligence {#sec-agi-systems-multiagent-systems-emergent-intelligence-9a2f}

The fourth paradigm challenges the assumption that intelligence must reside within a single entity. Multi-agent approaches posit that AGI will emerge from interactions between multiple specialized agents, each with distinct capabilities, operating within shared environments. This perspective draws inspiration from biological systems, where ant colonies, bee hives, and human societies demonstrate collective intelligence exceeding individual capabilities. No single ant comprehends the colony's architectural plans, yet coordinated local interactions produce sophisticated nest structures.

OpenAI's hide-and-seek agents [@baker2019emergent] demonstrated how competition drives emergent complexity without explicit programming. Hider agents learned to build fortresses using movable blocks, prompting seeker agents to discover tool use, pushing boxes to climb walls. This sparked an arms race: hiders learned to lock tools away, seekers learned to exploit physics glitches. Each capability emerged purely from competitive pressure, not human specification, suggesting that multi-agent interaction could bootstrap increasingly sophisticated behaviors toward general intelligence.

From a systems engineering perspective, multi-agent AGI introduces challenges reminiscent of distributed computing but with fundamental differences. Like distributed systems, multi-agent architectures require robust communication protocols, consensus mechanisms, and fault tolerance. However, where traditional distributed systems coordinate identical nodes executing predetermined algorithms, AGI agents must coordinate heterogeneous reasoning processes, resolve conflicting world models, and align divergent objectives. Projects like AutoGPT [@autogpt2023] demonstrate early autonomous agent capabilities, orchestrating web searches, code execution, and tool use to accomplish complex tasks, though current implementations remain limited by context window constraints and error accumulation across multi-step plans.

These four paradigms (scaling, neurosymbolic, embodied, and multi-agent) need not be mutually exclusive. The most promising path forward may combine insights from each: substantial computational resources applied to hybrid architectures that ground abstract reasoning in physical or simulated embodiment, with multiple specialized agents coordinating to solve complex problems. Such convergence points toward compound AI systems, the architectural framework that could unite these paradigms into practical implementations.

::: {.callout-note title="Progress Checkpoint: Research Directions"}
**What we have covered:** Four research directions with distinct engineering implications. Scaling continues to improve loss within tested regimes but faces data and economic constraints. Neurosymbolic approaches combine pattern matching with logical reasoning. Embodied intelligence grounds cognition in physical interaction. Multi-agent systems distribute computation across specialized components.

**Key insight:** These approaches address different constraints and may be complementary. **Uncertainty caveat**: Which approaches will prove most effective for which tasks remains empirically unknown.

**Where we are heading:** The next sections examine compound AI systems as a practical framework for current production systems, followed by alternative architectures, training methodologies, and quantifiable constraints.
:::

## The Compound AI Systems Framework {#sec-emerging-systems-challenges-compound-ai-systems-framework-907e}

Engineering constraints drive adoption of "Compound AI Systems" [@berkeley2024compound]: multiple specialized components operating in concert rather than monolithic models. This architectural paradigm represents a practical engineering response to the limitations identified in the previous section, not a pathway toward qualitatively different capabilities.

**Engineering Rationale**

Compound systems address specific, measurable limitations of monolithic models: context window constraints, lack of current information, inability to execute code reliably, and difficulty maintaining factual accuracy. Rather than speculating about whether such systems might achieve qualitatively different capabilities, we analyze them as engineering solutions to current constraints. In modern engineering terms, specialized components (vision encoders, math solvers, code interpreters) and orchestration layers (LLMs that route tasks and aggregate results) combine to overcome limitations that no single model can address economically.

Modern AI assistants already demonstrate this compound architecture. ChatGPT integrates a language model for text generation, a code interpreter for computation, web search for current information, and DALL-E for image creation. Each component excels at its specialized task while a central orchestrator coordinates their interactions through several mechanisms. Intent classification determines which components to activate based on user queries, result aggregation combines outputs from multiple components into coherent responses, and error handling routes failed operations to alternative components or triggers user clarification.

When analyzing stock market trends, the orchestration unfolds through multiple stages. First, the language model parses the user request to extract key information (ticker symbols, time ranges, analysis types). Second, it generates API calls to web search for current prices and retrieves relevant financial news. Third, the code interpreter receives this data and executes statistical analysis through Python scripts, computing moving averages, volatility measures, or correlation analyses. Fourth, the language model synthesizes these quantitative results with contextual information into natural language explanations. Fifth, if the user requests visualizations, the system routes to code generation for matplotlib charts. This orchestration achieves results no single component could produce: web search lacks analytical capabilities, code execution cannot interpret results, and the language model alone cannot access real time data.

An organizational analogy illuminates this architecture. Attempting to have a single model perform all functions resembles expecting one individual to handle an entire enterprise: strategy, accounting, marketing, engineering, and legal work. This approach neither scales nor provides specialized expertise. A compound AI system mirrors a well-structured organization with a chief executive (the orchestrator) who sets strategy and delegates tasks. Specialized departments handle distinct functions: research libraries manage knowledge retrieval, compliance teams implement safety filters, and engineering teams provide specialized tools and models. Capability emerges from coordinated work across these specialized components rather than from a single system.

The compound approach offers five key advantages over monolithic models. Modularity enables components to update independently without full system retraining. When OpenAI improves code interpretation, they swap that module without touching the language model, similar to upgrading a graphics card without replacing the entire computer. Specialization allows each component to optimize for its specific task. A dedicated retrieval system using vector databases outperforms a language model attempting to memorize all knowledge, just as specialized ASICs outperform general-purpose CPUs for particular computations. Interpretability emerges from traceable decision paths through component interactions. When a system makes an error, engineers can identify whether retrieval, reasoning, or generation failed, which remains impossible with opaque end-to-end models. Scalability permits new capabilities to integrate without architectural overhauls. Adding voice recognition or robotic control becomes a matter of adding modules rather than retraining trillion-parameter models. Safety benefits from multiple specialized validators constraining outputs at each stage. A toxicity filter checks generated text, a factuality verifier validates claims, and a safety monitor prevents harmful actions. This creates defense in depth rather than relying on a single model to behave correctly.

These advantages explain why every major AI lab now pursues compound architectures. Google's Gemini 2.0 combines multimodal understanding with native tool use and agentic capabilities. Anthropic's Claude 3.5 integrates constitutional AI components, computer use capabilities, and extended context windows enabling sophisticated multi-step workflows. OpenAI's ChatGPT orchestrates plugins, code execution, image generation, and web browsing through unified interfaces. The rapid evolution of these systems, from single-purpose assistants to multi-capable agents, demonstrates that compound architecture adoption accelerates as capabilities mature. The engineering principles established throughout this textbook, from distributed systems to workflow orchestration, now converge to enable these compound systems.

## Building Blocks for Compound Systems {#sec-emerging-systems-challenges-building-blocks-compound-systems-8f98}

The evolution from monolithic models to compound AI systems requires advances in how we engineer data, integrate components, and scale infrastructure. These building blocks address specific, measurable limitations of current approaches while creating new engineering challenges that span data availability, system integration, and computational scaling. We analyze these building blocks in terms of quantifiable trade-offs rather than speculative capability improvements.

As @fig-compound-ai-system illustrates, these building blocks integrate within the compound AI architecture: specialized data engineering components feed content to the Knowledge Retrieval system, dynamic architectures enable the LLM Orchestrator to route computations efficiently through mixture-of-experts patterns, and training paradigms power the Safety Filters that implement constitutional AI principles. Understanding these building blocks individually and their integration collectively enables practitioners to make informed trade-off decisions when designing compound systems.

### Data Engineering at Scale {#sec-agi-systems-data-engineering-scale-91a0}

Data engineering represents the first and most critical building block. Compound AI systems require advanced data engineering to feed their specialized components, yet machine learning faces a data availability crisis. The scale becomes apparent when examining model requirements progression: GPT-3 consumed 300 billion tokens (OpenAI), GPT-4 likely used over 10 trillion tokens (scaling law extrapolations[^fn-chinchilla-laws]), yet research estimates suggest only 4.6-17 trillion high-quality tokens exist across the entire internet[^fn-data-availability-crisis]. This progression reveals a critical bottleneck: at current consumption rates, traditional web-scraped text data may be exhausted by 2026, forcing exploration of synthetic data generation and alternative scaling paths [@epoch2022compute].

[^fn-chinchilla-laws]: **Chinchilla Scaling Laws**: Discovered by DeepMind [@hoffmann2022training], optimal model performance requires balanced scaling of parameters N and training tokens D following N ∝ D^0.74. Previous models were under-trained: GPT-3 (175B parameters, 300B tokens) should have used 4.6 trillion tokens for optimal performance. Chinchilla (70B parameters, 1.4T tokens) outperformed GPT-3 despite being 2.5× smaller, proving data quality matters more than model size.

[^fn-data-availability-crisis]: **Data Availability Crisis**: High-quality training data may be exhausted by 2026. While GPT-3 used 300B tokens and GPT-4 likely used over 10T tokens, researchers estimate only 4.6-17T high-quality tokens exist across the entire internet. This progression reveals a critical bottleneck requiring exploration of synthetic data generation and alternative scaling approaches.

Three data engineering approaches address this challenge through compound system design.

#### Self-Supervised Learning Components {#sec-agi-systems-selfsupervised-learning-components-e6d8}

Self-supervised learning enables compound AI systems to overcome the labeled data bottleneck. While supervised learning requires human annotations for every example, self-supervised methods extract knowledge from data structure itself by learning from the inherent patterns, relationships, and regularities present in raw information.

The biological precedent proves informative. Human brains process approximately 10¹¹ bits per second of sensory input but receive fewer than 10⁴ bits per second of explicit feedback, meaning 99.99% of learning occurs through self-supervised pattern extraction[^fn-brain-information-rates]. A child learns object permanence not from labeled examples but from observing objects disappear and reappear. They grasp physics not from equations but from watching things fall, roll, and collide.

[^fn-brain-information-rates]: **Brain Information Processing Rates**: Sensory organs transmit approximately 10¹¹ bits/second to the brain (eyes: 10⁷ bits/sec, skin: 10⁶ bits/sec, ears: 10⁵ bits/sec), but conscious awareness processes only 10¹-10² bits/sec [@norretranders1999user]. Explicit feedback (verbal instruction, corrections) operates at language bandwidth of ~10⁴ bits/sec maximum, suggesting the vast majority of human learning occurs through unsupervised observation and pattern extraction rather than supervised instruction.

Yann LeCun calls self-supervised learning the "dark matter" of intelligence [@lecun2022path], invisible yet constituting most of the learning universe. Current language models barely scratch this surface through next-token prediction, a primitive form that learns statistical correlations rather than causal understanding. When ChatGPT predicts "apple" after "red," it uses co-occurrence statistics, not an understanding that apples possess the property of redness.

The Joint Embedding Predictive Architecture (JEPA)[^fn-jepa] demonstrates a more sophisticated approach. Instead of predicting raw pixels or tokens, JEPA learns abstract representations of world states. Shown a video of a ball rolling down a ramp, JEPA doesn't predict pixel values frame-by-frame. Instead, it learns representations encoding trajectory, momentum, and collision dynamics, concepts transferable across different objects and scenarios. This abstraction achieves 3× better sample efficiency than pixel prediction while learning genuinely reusable knowledge.

[^fn-jepa]: **Joint Embedding Predictive Architecture (JEPA)**: Meta AI's framework [@lecun2022path] for learning abstract world models. V-JEPA [@bardes2024vjepa] learns object permanence and physics from video alone, without labels or rewards. Key innovation: predicting in latent space rather than pixel space, similar to how humans imagine scenarios abstractly rather than visualizing every detail.

**Mathematical Framework for Self-Supervised Learning**

To understand why self-supervised learning fundamentally differs from supervised learning, consider the mathematical structure. Supervised learning optimizes:

$$\min_\theta \mathbb{E}_{(x,y) \sim p_{data}}[\mathcal{L}(f_\theta(x), y)]$$

where $(x, y)$ pairs require human annotation. Each sample needs explicit labels, creating an $O(N)$ annotation cost for $N$ training examples. This dependency on labeled data constitutes the fundamental bottleneck: human annotation bandwidth limits training set size, which in turn limits model capability through sample complexity bounds.

Self-supervised learning eliminates explicit labels by constructing supervision from data structure itself. The general form is:

$$\min_\theta \mathbb{E}_{x \sim p_{data}}[\mathcal{L}(f_\theta(x), g(x))]$$

where $g(x)$ derives pseudo-labels from $x$ automatically without human intervention. The function $g$ varies by modality and learning objective, but critically requires no annotation cost. This enables training on arbitrarily large unlabeled datasets, with sample complexity limited only by data availability rather than annotation bandwidth.

**Contrastive Learning Framework.** Contrastive methods learn representations by distinguishing similar pairs from dissimilar ones. Given input $x$, generate two augmented views $x_i = t_i(x)$ and $x_j = t_j(x)$ where $t$ represents transformations (cropping, color jittering, masking). The InfoNCE objective maximizes agreement between positive pairs while separating negatives:

$$\mathcal{L}_{contrastive} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(z_i, z_k) / \tau)}$$

where $z_i = h(f_\theta(x_i))$ projects representations through encoder $f_\theta$ and projection head $h$, $\text{sim}(u,v) = u^\top v / \|u\| \|v\|$ computes cosine similarity, and $\tau$ is a temperature parameter controlling concentration. The numerator encourages similar representations for augmented views of the same input (positive pairs), while the denominator pushes apart representations from different inputs (negative pairs).

This formulation provides several quantitative advantages. First, it scales with unlabeled data: training on 1 billion images achieves 5-10% better downstream accuracy than 1 million images, with improvements continuing beyond supervised learning's saturation point. Second, it learns transferable representations: a model pre-trained contrastively on ImageNet achieves 92% of supervised performance when fine-tuned on target tasks using only 1% of labeled data, demonstrating 100× label efficiency improvement. Third, the learned representations encode semantic similarity: objects from the same category cluster in embedding space even without category labels during pre-training, with 72% accuracy on nearest-neighbor classification despite zero labeled training examples.

**Masked Prediction Framework.** Instead of contrasting examples, masked prediction reconstructs corrupted inputs. Given input $x$, mask a subset $M \subset \{1, ..., |x|\}$ and predict masked content from visible context:

$$\mathcal{L}_{masked} = \mathbb{E}_{M}[\sum_{i \in M} -\log p_\theta(x_i | x_{M^c})]$$

where $M^c$ denotes unmasked positions. For language (BERT), mask 15% of tokens and predict them from bidirectional context. For vision (MAE), mask 75% of image patches and reconstruct pixels through a decoder. This asymmetry (high masking rate for images, low for text) reflects information density: natural images contain substantial spatial redundancy, enabling reconstruction from sparse observations, while language encodes denser semantic content requiring more context.

The mathematical advantage appears in the reconstruction objective's information theoretic interpretation. Minimizing reconstruction loss upper bounds mutual information between learned representations and input data: $I(Z; X) \geq H(X) - \mathcal{L}_{masked}$, where $H(X)$ is input entropy and $Z = f_\theta(x)$ represents learned embeddings. Successful reconstruction from partial observations requires capturing underlying data structure, forcing the model to learn generalizable features rather than memorizing surface statistics.

Quantitatively, masked image modeling achieves 87.8% ImageNet accuracy using only Vision Transformer encoders, matching supervised pre-training without any labeled data. For language, BERT masked prediction enables 93.2% GLUE benchmark performance from self-supervised pre-training on web text, approaching human-level language understanding without task-specific supervision. The key insight: predicting missing information from context forces learning representations that capture data relationships, enabling transfer to downstream tasks.

**JEPA Mathematical Formulation.** The Joint Embedding Predictive Architecture formalizes the distinction between generative and predictive modeling. Rather than reconstructing input $x$ itself, JEPA predicts representations in latent space:

$$\mathcal{L}_{JEPA} = \mathbb{E}_{x,t}[\|s_\theta(x_t) - z_\theta(x_{t-\Delta t})\|^2]$$

where $s_\theta$ computes target representations from future states $x_t$, $z_\theta$ encodes context from past states $x_{t-\Delta t}$, and prediction occurs in embedding space rather than pixel/token space. The critical difference: by predicting abstract representations rather than raw observations, JEPA avoids modeling irrelevant details. When predicting video frames, pixel-level prediction must account for texture patterns, lighting variations, and background clutter. Representation-level prediction ignores such details, focusing on semantically relevant dynamics like object motion and interaction.

This abstraction provides measurable efficiency gains. V-JEPA processes 2000 video frames to learn object permanence and basic physics, while pixel-prediction models require 10,000+ frames for equivalent understanding (5× sample efficiency improvement). The learned representations transfer across contexts: trained on balls rolling down ramps, JEPA generalizes to predicting trajectories for novel objects without retraining, demonstrating compositional understanding impossible for pixel-level models that memorize specific visual patterns.

**Sample Complexity Analysis.** Self-supervised learning's fundamental advantage lies in sample complexity. Supervised learning requires $O(d/\epsilon^2)$ labeled examples to learn a classifier with $d$ dimensions and error $\epsilon$ (VC dimension bound). Self-supervised learning amortizes this cost: pre-train once on $10^9$ unlabeled examples, then fine-tune for specific tasks using $10^2$-$10^3$ labeled examples. The effective sample complexity becomes $O(d_{pretrain}/\epsilon_{pretrain}^2 + d_{finetune}/\epsilon_{finetune}^2)$ where typically $d_{pretrain} \gg d_{finetune}$, enabling massive labeled data reduction for downstream tasks.

Empirically, this manifests as label efficiency. ImageNet classification requires 1.28M labeled images to achieve 76% supervised accuracy. With self-supervised pre-training, the same 76% accuracy requires only 12,800 labeled images (100× reduction), and 1% labeled data achieves 72% accuracy. For language understanding, GPT-3 achieves few-shot learning: providing 10-100 examples in context enables task performance approaching supervised fine-tuning on thousands of labeled examples, representing 10-100× label efficiency improvement from self-supervised pre-training on web text.

For compound systems, self-supervised learning enables each specialized component to develop expertise from its natural data domain. A vision module learns from images, a language module from text, a dynamics module from video, all without manual labeling. The engineering challenge involves coordinating these diverse learning processes: ensuring representations align across modalities, preventing catastrophic forgetting when components update, and maintaining consistency as the system scales. Frameworks such as PyTorch, TensorFlow, and JAX must evolve to support these heterogeneous self-supervised objectives within unified training loops.

#### Synthetic Data Generation {#sec-agi-systems-synthetic-data-generation-a05e}

Compound systems generate their own training data through guided synthesis rather than relying solely on human-generated content. This approach appears paradoxical: how can models learn from themselves without degrading into model collapse, where generated data increasingly reflects model biases rather than ground truth? Three complementary mechanisms prevent quality degradation.

First, verification through external ground truth constrains generation. Microsoft's Phi models [@gunasekar2023textbooks] generate synthetic textbook problems but verify solutions through symbolic execution, mathematical proof checkers, or code compilation. A generated algebra problem must have a unique, verifiable solution; a programming exercise must compile and pass test cases. This creates a feedback loop where generators learn to produce not merely plausible examples but verifiable correct ones.

Second, curriculum-based synthesis starts with simple, tractable examples and progressively increases complexity. Phi-2 (2.7B parameters) matches GPT-3.5 (175B) performance because its synthetic training data follows pedagogical progression: basic arithmetic before calculus, simple functions before recursion, concrete examples before abstract reasoning. This structured curriculum enables smaller models to achieve capabilities requiring 65× more parameters when trained on unstructured web data.

Third, ensemble verification uses multiple independent models to filter synthetic data. When generating training examples, outputs must satisfy multiple distinct critic models trained on different data distributions. This prevents systematic biases: if one generator consistently produces examples favoring particular patterns, ensemble critics trained on diverse data will identify and reject these biased samples. Anthropic's Constitutional AI demonstrates this through iterative refinement: one component generates responses, multiple critics evaluate them against different principles (helpfulness, harmlessness, factual accuracy), and synthesis produces improved versions satisfying all criteria simultaneously.

For compound systems, this enables specialized data generation components that create domain-specific training examples calibrated to other component needs. A reasoning component might generate step by step solutions for a verification component to check, while a code generation component produces programs for an execution component to validate.

#### Self-Play Components {#sec-agi-systems-selfplay-components-49ca}

AlphaGo Zero [@silver2017mastering] demonstrated a key principle for compound systems: components can bootstrap expertise through self-competition without human data. Starting from completely random play, it achieved superhuman Go performance in 72 hours purely through self-play reinforcement learning[^fn-self-play].

[^fn-self-play]: **Self-Play Reinforcement Learning**: Training paradigm where an agent learns by competing against copies of itself, eliminating the need for human expert data. Each self-play game generates training examples at the agent's current skill level, providing automatic curriculum progression. AlphaGo Zero used 5,000 TPUs for 3 days generating 4.9 million self-play games, a computational investment of approximately 3,600 TPU-hours that would cost roughly $150,000 at current cloud prices but produced superhuman capability without any human game data.

Three technical elements enable bootstrapping from zero knowledge.

First, self-play provides automatic curriculum adaptation through opponent strength tracking. Unlike supervised learning with fixed datasets, self-play continuously adjusts difficulty as both competing agents improve. When AlphaGo Zero plays against itself, each game reflects current skill level, creating training examples calibrated to just beyond current capabilities. Early games explore basic patterns; later games reveal subtle tactical nuances impossible to specify through human instruction.

Second, search-guided exploration expands the effective training distribution beyond what current policy can generate. Monte Carlo Tree Search[^fn-mcts] simulates thousands of possible futures from each position, discovering strong moves the current policy would not consider.

[^fn-mcts]: **Monte Carlo Tree Search (MCTS)**: Planning algorithm that builds a search tree through repeated random simulations. Each simulation starts from the current state, makes moves according to a policy, and records the outcome. Over thousands of simulations, the algorithm identifies moves that lead to favorable outcomes. MCTS enabled breakthroughs in game-playing AI: AlphaGo ran 1,600 MCTS simulations per move (consuming approximately 4 seconds on 48 TPUs), exploring roughly 10 million positions per game. The computational cost scales with desired play strength.

These search-enhanced decisions become training targets, pulling policy toward superhuman play through iterative improvement. This creates a virtuous cycle: better policy enables more accurate search, which discovers better training targets, which improve policy further.

Third, outcome verification provides unambiguous learning signals. Game outcomes (win/loss in Go, solution correctness in coding, debate victory in reasoning) offer clear supervision without human annotation. A model that generates code can test millions of candidate programs against test suites, learning from successes and failures without human evaluation. DeepMind's AlphaCode generates over one million programs per competition problem, filtering through compilation errors and test failures to identify correct solutions, thereby learning both from successful programs (positive examples) and systematic failure patterns (negative examples).

This principle extends beyond games to create specialized system components for compound architectures. OpenAI's debate models argue opposing sides of questions, with a judge model determining which argument better supports truth, creating training data for both argumentation and evaluation. Anthropic's models critique their own outputs through self-generated critiques evaluated for quality, bootstrapping improved responses. These self-play patterns enable compound systems to generate domain-specific training data without expensive human supervision.

Implementing this approach in compound systems requires data pipelines handling dynamic generation at scale: managing continuous streams of self-generated examples, filtering for quality through automated verification, and preventing mode collapse through diversity metrics. The engineering challenge involves orchestrating multiple self-playing components while maintaining exploration diversity and preventing system-wide convergence to suboptimal patterns or adversarial equilibria.

#### Web-Scale Data Processing {#sec-agi-systems-webscale-data-processing-f0c9}

High-quality curated text may be limited, but self-supervised learning, synthetic generation, and self-play create new data sources. The internet's long tail contains untapped resources for compound systems: GitHub repositories, academic papers, technical documentation, and specialized forums. Common Crawl contains 250 billion pages, GitHub hosts 200M+ repositories, arXiv contains 2M+ papers, and Reddit has 3B+ comments, combining to over 100 trillion tokens of varied quality. The challenge lies in extraction and quality assessment rather than availability.

@fig-frontier-data-pipeline breaks down how modern compound systems employ sophisticated filtering pipelines where specialized components handle different aspects: deduplication removes 30-60% redundancy in web crawls, quality classifiers trained on curated data identify high-value content, and domain-specific extractors process code, mathematics, and scientific text. This processing intensity exemplifies the data engineering challenge: GPT-4's training likely processed over 100 trillion raw tokens to extract 10-13 trillion training tokens, representing approximately 90% total data reduction: 30% from deduplication, then 80-90% of remaining data from quality filtering.

This represents a shift from batch processing to continuous, adaptive data curation where multiple specialized components work together to transform raw internet data into training-ready content.

::: {#fig-frontier-data-pipeline fig-env="figure" fig-pos="htb" fig-cap="**Data Engineering Pipeline for Frontier Models**: The multi-stage pipeline transforms 100+ trillion raw tokens into 10-13 trillion high-quality training tokens. Each stage applies increasingly sophisticated filtering, with synthetic generation augmenting the final dataset. This pipeline represents the evolution from simple web scraping to intelligent data curation systems." fig-alt="Flowchart showing data sources (web 100T, GitHub 10T, papers 1T, books 0.5T tokens) flowing through deduplication and quality filters to extractors, then synthetic generation, yielding 10-13T training tokens."}
```{.tikz}
\begin{tikzpicture}[line cap=round,line join=round,font=\usefont{T1}{phv}{m}{n}]
\tikzset{
  funnel/.style={trapezium, trapezium angle=60, trapezium stretches=true,line width=0.75pt,
                 draw=VioletLine, fill=VioletL2!99, minimum width=42mm, minimum height=13mm},
Line/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},
LineA/.style={violet!50,, line width=1.5pt,{-{Triangle[width=1.1*6pt,length=2.0*6pt]}},shorten <=3pt,shorten >=2pt},
ALine/.style={black!50, line width=1.1pt,{{Triangle[width=0.9*6pt,length=1.2*6pt]}-}},
Larrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,
            single arrow head indent=0pt,minimum height=7mm, minimum width=3pt},
  Box/.style={align=center,
    inner xsep=2pt,
    node distance=2.2,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=35mm,
    minimum width=35mm, minimum height=10mm
  },
}
%Globe style
\tikzset{
pics/globe/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[circle,minimum size=25mm,draw=\drawcolor, fill=\filllcolor!70,line width=1.5*\Linewidth](C\picname) at (0,0){};
\draw[draw=\drawcolor,line width=\Linewidth,shorten <=2pt,shorten >=2pt](C\picname.north)to[bend left=65](C\picname.south);
\draw[draw=\drawcolor,line width=\Linewidth,shorten <=2pt,shorten >=2pt](C\picname.north)to[bend right=65](C\picname.south);
\draw[draw=\drawcolor,line width=\Linewidth,shorten <=1pt,shorten >=1pt](C\picname.north)to(C\picname.south);
\draw[draw=\drawcolor,line width=\Linewidth,shorten <=1pt,shorten >=1pt](C\picname.west)--(C\picname.east);
%
\draw[draw=\drawcolor,line width=\Linewidth,shorten <=1pt,shorten >=1pt](C\picname.130)to[bend right=35](C\picname.50);
\draw[draw=\drawcolor,line width=\Linewidth,shorten <=1pt,shorten >=1pt](C\picname.230)to[bend left=35](C\picname.310);
\end{scope}
    }
  }
}
%Github logo style
\tikzset{
pics/github/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[circle,draw=none,fill=\filllcolor,minimum size=27mm](GIT-\picname){};
\draw[fill=white,draw=white](-0.71,0.74)to[out=250,in=110] (-0.73,0.38)to[out=235,in=180,distance=15](-0.27,-0.64)
to[out=235,in=80](-0.34,-0.81)to[out=215,in=310](-0.69,-0.76)to[out=120,in=20](-1,-0.59)
to[out=270,in=150](-0.93,-0.66)to[out=330,in=110](-0.79,-0.86)to[out=300,in=200](-0.33,-1.03)to(-0.33,-1.26)
to[out=230,in=30](-0.37,-1.305)to[out=348,in=195](0.37,-1.3)to[out=348,in=195](0.34,-1.25)to(0.34,-0.83)
to[out=90,in=300](0.27,-0.64)to[out=0,in=310,distance=15](0.73,0.38)to[out=70,in=290](0.71,0.74)
to[out=190,in=30](0.36,0.60)to[out=170,in=10](-0.36,0.60)to[out=140,in=350]cycle;
\end{scope}
    }
  }
}
%Folder style
\tikzset{%
 LineDF/.style={line width=\Linewidth,draw=\drawcolor,rounded corners=2pt},
 pics/dataFolder/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=DATAFOLDER,scale=\scalefac, every node/.append style={transform shape}]
\draw[LineDF,fill=\filllcolor!20] (0,0) -- (-0.20,2.45)coordinate(\picname-GL)--
(0.4,2.45)to[out=360,in=180](0.9,2.1)-- (2.5,2.1)--(2.5,0)--cycle ;
\draw[LineDF,fill=\filllcolor!50] (0,0)coordinate(\picname-DL) -- (2.8,0)coordinate(\picname-DD)-- (3,1.8) -- (0.2,1.8) -- cycle;
 \end{scope}
     }
  }
}
%books style
\tikzset{
pics/books/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\draw[draw=\drawcolor,line width=\Linewidth](1.23,-0.42)--(0.32,-1.23)coordinate(BD\picname)--(-0.97,-1.1)
to[out=170,in=200,distance=5](-1.0,-0.71)to(0.32,-0.83)to(1.23,-0.03);
\draw[draw=\drawcolor,,line width=\Linewidth](1.23,0.36)--(0.32,-0.44)--(-0.97,-0.29)
to[out=170,in=200,distance=5](-1.0,0.1)to(0.32,-0.07)to(1.3,0.76);
\draw[draw=\drawcolor,,line width=2.5pt](-1.0,-0.69)to[out=170,in=190,distance=5](-0.97,-0.3);
\draw[draw=\drawcolor,fill=\filllcolor](0.02,0.9)--(1.34,0.8)--(0.32,-0.07)coordinate(BG\picname)--(-1.06,0.1)--cycle;
\draw[draw=none,line width=1pt,fill=white](0.04,0.65)to(0.7,0.58)to(0.50,0.42)to(-0.17,0.49)to cycle;
\end{scope}
    }
  }
}
%Data
\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,
minimum width=25mm,minimum height=11mm,line width=\Linewidth,node distance=-0.15},
pics/data/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[mycylinder,fill=\filllcolor!50] (A\picname) {};
\node[mycylinder, above=of A\picname,fill=\filllcolor!30] (B\picname) {};
\node[mycylinder, above=of B\picname,fill=\filllcolor!10] (C\picname) {};
 \end{scope}
     }
  }
}
%%%
\pgfkeys{
  /channel/.cd,
   Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawcolor/.store in=\drawcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  filllcolor=BrownLine,
  filllcirclecolor=violet!20,
  drawcolor=black,
  drawcircle=violet,
  scalefac=1,
  Linewidth=0.5pt,
  Depth=1.3,
  Height=0.8,
  Width=1.1,
  picname=C
}
%Globe
\pic[shift={(0,0)}] at  (0,0){globe={scalefac=0.9,picname=1,filllcolor=orange!20!, Linewidth=1.3pt}};
\node[above=2mm of C1.north,align=center](WC){Web Crawl\\\small 100T tokens};
%Github
\pic[shift={(0,0)}] at  ($(C1)+(5.0,0)$){github={scalefac=0.9,picname=1,filllcolor=RedLine}};
%Data folder
\pic[shift={(0,-1.13)}] at  ($(GIT-1)+(4.0,0)$){dataFolder={scalefac=0.7,picname=1,Linewidth=1.5pt,
 filllcolor=BrownLine!50!,drawcolor=BrownLine}};
 %books
\pic[shift={(0,0.1)}] at  ($(GIT-1)+(10.0,0)$){books={scalefac=1.0,picname=1,drawcolor=BlueD,filllcolor=BlueD,Linewidth=2.5pt}};
 %\text above
\path[red](WC)-|coordinate(SR1)(GIT-1.north);
\path[red](WC)-|coordinate(SR2)($(1-DL)!0.5!(1-DD)$);
\path[red](WC)-|coordinate(SR3)(BG1);
\node[align=center]at(SR1){GitHub\\\small 10T tokens};
\node[align=center]at(SR2){Papers\\\small 1T tokens};
\node[align=center]at(SR3){Books\\\small 0.5T tokens};
%
\coordinate(DEDX)at($($(GIT-1.south)!0.60!(1-DL)$)+(0,-1.25)$);
\fill[red](DEDX)circle(2pt);
%
\node[funnel,align=center] (B1) at(DEDX){Deduplication\\ $-$30\%};
\node[funnel,below=0.8of B1,draw=OrangeLine,fill=OrangeL!30,align=center] (B2){Quality Filter\\ $-$80\%};
\node[Box,below=0.8of B2] (B4){Math Extractor};
\node[Box,left=of B4] (B3){Code Parser};
\node[Box,right=of B4] (B5){Language Detector};
\node[Box,below=0.8 of B4,draw=RedLine,fill=RedL!70] (B6){Synthetic Generation};
\begin{scope}[local bounding box=DATA1,shift={($(0,0)+(0,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,-5.1)}] at  (B4){data={scalefac=0.7,picname=11,filllcolor=green!70!black, Linewidth=1.0pt}};
\node[below=2mmof A11,align=center]{Training Data\\\small 10-13T tokens};
 \end{scope}
 %arrows
\draw[LineA](C1.south)|-(B1);
\draw[LineA](GIT-1.south)to[bend right=25](B1);
\draw[LineA]($(1-DL)!0.5!(1-DD)$)to[bend left=25](B1);
\draw[LineA](BD1)|-coordinate(T1)(B1);
\draw[LineA](B1)--(B2);
\draw[LineA](B2)-|(B3);
\draw[LineA](B2)-|(B5);
\draw[LineA](B2)--(B4);
\draw[LineA](B4)--(B6);
\draw[LineA](B3)|-(B6);
\draw[LineA](B5)|-(B6);
\draw[LineA](B6)--(C11);
%
\node[right=8mm of T1](T77){77T tokens};
\path[red](T77)|-coordinate(T2)(B2);
\path[red](T77)|-coordinate(T3)(B6);
\node[](T15)at(T2){15T tokens};
\node[](T33)at(T3){+3T tokens};
\end{tikzpicture}
```
:::

@fig-frontier-data-pipeline reveals an important insight about this pipeline architecture: the bottleneck isn't data availability but processing capacity. Starting with 111.5 trillion raw tokens, aggressive filtering reduces this to just 10-13 trillion training tokens, with over 90% of data discarded. For ML engineers, this means that improving filter quality could be more impactful than gathering more raw data. A 10% improvement in the quality filter's precision could yield an extra trillion high-quality tokens, equivalent to doubling the amount of books available.

These data engineering approaches (synthetic generation, self-play, and advanced harvesting) represent the first building block of compound AI systems. They transform data limitations from barriers into opportunities for innovation, with specialized components generating, filtering, and processing data streams continuously.

Generating high-quality training data only addresses part of the compound systems challenge. The next building block involves architectural innovations that enable efficient computation across specialized components while maintaining system coherence.

### Dynamic Architectures for Compound Systems {#sec-agi-systems-dynamic-architectures-compound-systems-fca0}

Compound systems require dynamic approaches that can adapt computation based on task requirements and input characteristics. This section explores architectural innovations that enable efficient specialization through selective computation and sophisticated routing mechanisms. Mixture of experts and similar approaches allow systems to activate only relevant components for each task, improving computational efficiency while maintaining system capability.

#### Specialization Through Selective Computation {#sec-agi-systems-specialization-selective-computation-f46f}

Compound systems face a fundamental efficiency challenge: not all components need to activate for every task. A mathematics question requires different processing than language translation or code generation, yet dense monolithic models activate all parameters for every input regardless of task requirements.

Consider GPT-3 [@brown2020language] processing the prompt "What is 2+2?". All 175 billion parameters activate despite this requiring only arithmetic reasoning, not language translation, code generation, or commonsense reasoning. This activation requires 350GB memory and 350 GFLOPs per token of forward pass computation. Activation analysis through gradient attribution reveals that only 10-20% of parameters contribute meaningfully to any given prediction, suggesting 80-90% computational waste for typical inputs. The situation worsens at scale: a hypothetical 1 trillion parameter dense model would require 2TB memory and 2 TFLOPs per token, with similar utilization inefficiency.

This inefficiency compounds across three dimensions. Memory bandwidth limits how quickly parameters load from HBM to compute units, creating bottlenecks even when compute units sit idle. Power consumption scales with activated parameters regardless of contribution, burning energy for computations that minimally influence outputs. Latency increases linearly with model size for dense architectures, making real-time applications infeasible beyond certain scales.

The biological precedent suggests alternative approaches. The human brain contains approximately 86 billion neurons but does not activate all for every task. Visual processing primarily engages occipital cortex, language engages temporal regions, and motor control engages frontal areas. This sparse, task-specific activation enables energy efficiency: the brain operates on 20 watts despite complexity rivaling trillion parameter models in connectivity density.

These observations motivate architectural designs enabling selective activation of system components. Rather than activating all parameters, compound systems should route inputs to relevant specialized components, activating only the subset necessary for each specific task. This selective computation promises order of magnitude improvements in efficiency, latency, and scalability.

#### Expert Routing in Compound Systems {#sec-agi-systems-expert-routing-compound-systems-0e3e}

The Mixture of Experts (MoE)[^fn-moe] architecture [@fedus2022switch] demonstrates the compound systems principle at the model level: specialized components activated through intelligent routing. Rather than processing every input through all parameters, MoE models consist of multiple expert networks, each specializing in different problem types.

[^fn-moe]: **Mixture of Experts (MoE)**: Sparse architecture where only a subset of model parameters (experts) activate per input. Originated in 1991 with Jacobs et al., but became practical at scale with Google's Switch Transformer (2021). Key systems benefit: MoE models can have 8x more parameters than dense models with comparable inference cost, since only 12-25% of parameters activate per token. Challenge: expert load balancing requires auxiliary loss functions, and irregular memory access patterns reduce GPU utilization from 80% to 40-60% without careful optimization.

@fig-moe-routing-frontiers visualizes how a routing mechanism (learned gating function) determines which experts process each input, directing tokens to specialized subnetworks based on content type.

The router computes probabilities for each expert using learned linear transformations followed by softmax, typically selecting the top-2 experts per token. Load balancing losses ensure uniform expert utilization to prevent collapse to few specialists. This pattern extends naturally to compound systems where different models, tools, or processing pipelines are routed based on input characteristics.

Consider how the routing works in practice, as depicted in @fig-moe-routing-frontiers: when a token like "2+2=" enters the system, the router assigns high weights (0.7) to arithmetic specialists while giving zero weight to vision or language experts. For "Bonjour means", it activates translation experts instead. GPT-4 [@openai2023gpt4] is rumored to use eight expert models of approximately 220B parameters each (unconfirmed by OpenAI), activating only two per token, reducing active computation to 280B parameters while maintaining 1.8T total capacity with 5-7x inference speedup.

This introduces systems challenges: load balancing across experts, preventing collapse where all routing converges to few experts, and managing irregular memory access patterns. For compound systems, these same challenges apply to routing between different models, databases, and processing pipelines, requiring sophisticated orchestration infrastructure.

::: {#fig-moe-routing-frontiers fig-env="figure" fig-pos="htb" fig-cap="**Mixture of Experts (MoE) Routing**: Conditional computation through learned routing enables efficient scaling to trillions of parameters. The router (gating function) determines which experts process each token, activating only relevant specialists. This sparse activation pattern reduces computational cost while maintaining model capacity, though it introduces load balancing and memory access challenges." fig-alt="MoE routing diagram with token input passing through router gate to 4 experts. Router assigns weights 0.7 to Mathematics, 0.3 to Language, 0.0 to Code and Vision. Active experts feed weighted sum producing output."}
```{.tikz}
\begin{tikzpicture}[line cap=round,line join=round,font=\usefont{T1}{phv}{m}{n}]

\tikzset{
  Box/.style={align=center,
    inner xsep=2pt,
    node distance=0.45,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL!60,
    text width=32mm,
    minimum width=17mm, minimum height=11mm
  },
   Box2/.style={Box, fill=BrownL!60,draw=BrownLine},
   Box3/.style={Box, fill=BlueL!60,draw=BlueLine},
Line/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},
LineA/.style={GreenD,line width=2.0pt,{-{Triangle[width=1.1*6pt,length=2.0*6pt]}},shorten <=3pt,shorten >=2pt},
ALine/.style={black!50, line width=1.1pt,{{Triangle[width=0.9*6pt,length=1.2*6pt]}-}},
Larrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,
            single arrow head indent=0pt,minimum height=10mm, minimum width=3pt}
}

%Router symbol style
\tikzset{
pics/gatewey/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=GAT,scale=0.9, every node/.append style={transform shape}]
\def\rI{4mm}
\def\rII{2.8mm}
\def\rIII{1.6mm}
\draw[\filllcolor=\filllcolor,line width=\Linewidth](0,0)--(0,0.38)--(1.2,0.38)--(1.2,0)--cycle;
\draw[\drawcolor=\drawcolor,line width=\Linewidth](0.6,0.4)--(0.6,0.9);

\draw[\drawcolor=red,line width=\Linewidth] (0.6,0.9)+(60:\rI) arc[start angle=60, end angle=-60, radius=\rI];
\draw[\drawcolor=red,line width=\Linewidth] (0.6,0.9)+(50:\rII) arc[start angle=50, end angle=-50, radius=\rII];
\draw[\drawcolor=red,line width=\Linewidth] (0.6,0.9)+(30:\rIII) arc[start angle=30, end angle=-30, radius=\rIII];
%
\draw[\drawcolor=red,line width=\Linewidth] (0.6,0.9)+(120:\rI) arc[start angle=120, end angle=240, radius=\rI];
\draw[\drawcolor=red,line width=\Linewidth] (0.6,0.9)+(130:\rII) arc[start angle=130, end angle=230, radius=\rII];
\draw[\drawcolor=red,line width=\Linewidth] (0.6,0.9)+(150:\rIII) arc[start angle=150, end angle=210, radius=\rIII];
\fill[\filllcolor=red](0.6,0.9)circle (1.5pt);
\foreach\i in{0.15,0.3,0.45,0.6}{
\fill[\filllcolor=red](\i,0.19)circle (1.5pt);
}
\fill[\filllcolor=red](1,0.19)circle (2pt);
\end{scope}
    }
  }
}
%Token style
\tikzset{
pics/token/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[draw=\drawcolor,fill=\filllcirclecolor,circle,minimum size=40mm,
line width=\Linewidth](T-\picname){};
\node[draw=white,fill=none,circle,minimum size=0.925*40mm,line width=0.6*\Linewidth]{};
\clip[] circle (0.925*20mm);
\draw[step=5mm,draw=white] (-2,-2) grid (2,2);
\foreach \x/\y[count=\a] in {0/0,1.0/0,1/1,-0.5/1.5,-1.5/0.5,-1.0/-0.5,0.5/-1.0,1.0/0}{
\fill[fill=white,draw=none](\x,\y)circle(5pt)coordinate(C\a);
}
\draw[white,line width=\Linewidth,fill opacity=0.5,fill=\filllcolor!40](C2)--(C3)--(C4)--(C5)--(C6)--(C7)--cycle;
\foreach \x in {2,...,7}{
\draw[white,line width=\Linewidth](C1)--(C\x);
}
\foreach \x/\y\col[count=\a] in {0/0/red,1.0/0/green,1/1/blue,-0.5/1.5/violet,
-1.5/0.5/magenta,-1.0/-0.5/brown,0.5/-1.0/yellow}{
\fill[fill=\col,draw=none](\x,\y)circle(5pt)coordinate(C\a);
}
\end{scope}
    }
  }
}
%%%
\pgfkeys{
  /channel/.cd,
   Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawcolor/.store in=\drawcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  filllcolor=BrownLine,
  filllcirclecolor=violet!20,
  drawcolor=black,
  drawcircle=violet,
  scalefac=1,
  Linewidth=0.5pt,
  Depth=1.3,
  Height=0.8,
  Width=1.1,
  picname=C
}

\node[Box](B1){Expert 1\\ Mathematics};
\node[Box,below=of B1](B2){Expert 2\\ Language};
\node[Box2,below=of B2](B3){Expert 3\\ Code};
\node[Box2,below=of B3](B4){Expert 4\\ Vision};
%Router
\coordinate(RO)at($($(B2.south west)!0.5!(B3.north west)$)+(-3.5,0)$);
\node[draw=none,fill=red,circle,minimum size=20mm](GA)at(RO){};
\pic[shift={(-0.55,-0.5)}] at (GA) {gatewey={scalefac=1.0,picname=1,drawcolor=white,
filllcolor=white,Linewidth=1.75pt}};
\node[below=1mm of GA]{Router Gate};
%Token
\begin{scope}[local bounding box=TOKEN1,shift={($(RO)+(-3.5,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){token={scalefac=0.5,picname=1,drawcolor=orange,
filllcirclecolor=orange!90,filllcolor=orange,Linewidth=1.25pt}};
\node[below=1mm of T-1.south] {Token Input};
\node[above=1mm of T-1.north] {Embedding};
\end{scope}
%Sum
\coordinate(WS)at($($(B1.south east)!0.5!(B2.north east)$)+(2.75,0)$);
\node[draw=none, fill=BlueLine!80, circle, minimum size=20mm] (SUM)at (WS) {};
\node[draw=none, fill=cyan!10, circle, minimum size=14mm] at (WS) {\LARGE $\sum$};
\node[below=1mm of SUM.south](){Weighted Sum};
%Token Output
\begin{scope}[local bounding box=TOKEN2,shift={($(WS)+(3.5,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){token={scalefac=0.5,picname=2,drawcolor=green!55!black,
filllcirclecolor=green!55!black!90,filllcolor=green!55!black,Linewidth=1.25pt}};
\node[below=1mm of T-2.south] {Output Final};
\node[above=1mm of T-2.north] {Representation};
\end{scope}
%%arrows
\coordinate(SR1)at($(T-1.east)!0.5!(GA.west)$);
\node[Larrow]at(SR1){};
\coordinate(SR3)at($(SUM.east)!0.5!(T-2.west)$);
\draw[LineA](B1.east)--(SUM);
\draw[LineA](B2.east)--(SUM);
\node[Larrow]at(SR3){};
\draw[LineA](GA)--node[sloped,above]{0.7}(B1.west);
\draw[LineA](GA)--node[sloped,above]{0.3}(B2.west);
\draw[LineA,BrownLine!40](GA)--node[sloped,above,text=BrownLine]{0.0}(B3.west);
\draw[LineA,BrownLine!40](GA)--node[sloped,above,text=BrownLine]{0.0}(B4.west);
%
\node[align=center,below=3mm of B4](EQ){$\displaystyle y=\sum\limits_{i=1}^k G(x)_i\cdot E_i(x)$
%\\[1ex]
where $G(x)$ = routing weights, $E_i =$ expert $i$};
%%fitting
\scoped[on background layer]
\node[draw=BackLine,fill=BackColor!70, inner ysep=7mm, inner xsep=6mm,
fit=(B1)(B4),yshift=5mm](BB1){};
\node[align=center,below=0.5mm of BB1.north]{Sparse Activation:\\ Only 2 of 4 experts};
\end{tikzpicture}
```
:::

#### External Memory for Compound Systems {#sec-agi-systems-external-memory-compound-systems-648c}

Beyond routing efficiency, compound systems require memory architectures that scale beyond individual model constraints. As detailed in @sec-agi-systems-state-space-models-efficient-longcontext-processing-7ece, transformers face quadratic memory scaling with sequence length, limiting knowledge access during inference and preventing long-context reasoning across system components.

Retrieval-Augmented Generation (RAG)[^fn-rag] addresses this by creating external memory stores accessible to multiple system components. Instead of encoding all knowledge in parameters, specialized retrieval components query vector databases[^fn-vector-db] containing billions of documents, incorporating relevant information into generation processes.

[^fn-vector-db]: **Vector Databases**: Specialized database systems optimized for storing and querying high-dimensional embedding vectors. Unlike traditional databases that match exact values, vector databases find semantically similar content through approximate nearest neighbor (ANN) search. Popular systems include Pinecone, Weaviate, Milvus, and Chroma. Performance: modern implementations achieve 10-100ms query latency across billions of vectors using techniques like HNSW (Hierarchical Navigable Small World) graphs, enabling real-time RAG at scale.

This transforms the architecture from purely parametric to hybrid parametric-nonparametric systems [@borgeaud2022improving].

[^fn-rag]: **Retrieval-Augmented Generation (RAG)**: Introduced by Meta AI researchers in 2020, RAG combines parametric knowledge (stored in model weights) with non-parametric knowledge (retrieved from external databases) [@borgeaud2022improving]. Facebook's RAG system retrieves from 21M Wikipedia passages, enabling models to access current information without retraining. Modern RAG systems like ChatGPT plugins and Bing Chat handle billions of documents with sub-second retrieval latency.

For compound systems, this enables shared knowledge bases accessible to different specialized components, efficient similarity search across diverse content types, and coordinated retrieval that supports complex multi-step reasoning processes.

#### Modular Reasoning Architectures {#sec-agi-systems-modular-reasoning-architectures-be96}

Multi-step reasoning exemplifies the compound systems advantage: breaking complex problems into verifiable components. While monolithic models can answer simple questions directly, multi-step problems produce compounding errors (90% accuracy per step yields only 59% overall accuracy for 5-step problems). GPT-3 [@brown2020language] exhibits 40-60% error rates on complex reasoning, primarily from intermediate step failures.

Chain-of-thought prompting [@wei2022chain] and modular reasoning architectures address this through decomposition where different components handle different reasoning stages. Rather than generating answers directly, specialized components produce intermediate reasoning steps that verification components can check and correct. Chain-of-thought prompting improves GSM8K accuracy from 17.9% to 58.1%, with step verification reaching 78.2%.

This architectural approach, decomposing complex tasks across specialized components with verification, represents the core compound systems pattern: multiple specialists collaborating through structured interfaces rather than monolithic processing.

These innovations demonstrate the transition from static architectures toward dynamic compound systems that route computation, access external memory, and decompose reasoning across specialized components. This architectural foundation enables the sophisticated orchestration required for AGI-scale intelligence.

The dynamic architectures explored above, including mixture-of-experts routing, external memory systems, and modular reasoning patterns, extend what compound systems can achieve with transformer-based components. These innovations enable efficient specialization, scalable knowledge access, and decomposed problem-solving. Yet transformers themselves face fundamental limitations that may constrain AGI development regardless of how cleverly we orchestrate them.

Consider the core computational pattern: attention mechanisms that compare every input element with every other element create quadratic scaling that prevents processing book-length contexts efficiently. The autoregressive generation pattern forces sequential, left-to-right processing that cannot easily revise earlier decisions based on later constraints. And the pattern-matching foundation may never yield genuine causal reasoning, no matter how much data or compute we apply.

Before examining how to train and deploy compound systems, we must understand alternative architectural paradigms that could serve as building blocks within these systems. These alternatives are not replacements for transformers but potential complements: future compound systems may route different computations to different architectural substrates based on task requirements.

::: {.callout-note title="Progress Checkpoint: Compound Systems Foundation"}
**What we have covered:** The compound AI systems framework as the organizing principle for AGI development, including its five key advantages (modularity, specialization, interpretability, scalability, safety). We examined the building blocks: data engineering approaches (self-supervised learning, synthetic generation, self-play), dynamic architectures (mixture-of-experts, external memory, modular reasoning), and why current implementations at major AI labs validate the compound approach.

**Key insight:** Intelligence emerges from orchestrating specialized components rather than scaling monolithic models. This mirrors biological cognition and organizational design.

**Where we are heading:** The next section examines alternative architectures that could serve as building blocks within compound systems, addressing fundamental transformer limitations through different computational principles.
:::

## Alternative Architectural Patterns {#sec-emerging-systems-challenges-alternative-architectural-patterns-dc93}

The transformer limitations outlined above motivate exploring alternative computational approaches. Quadratic attention scaling prevents processing of book-length contexts. Autoregressive generation cannot easily revise earlier decisions based on later constraints. These are measurable constraints with quantifiable impact on system design.

This section examines three architectural patterns that address these limitations through different computational principles: state space models for efficient long-context processing, energy-based models for optimization-driven reasoning, and world models for predictive learning. **Uncertainty caveat**: Whether these architectures will prove superior to transformers for specific workloads, and for which workloads, remains an empirical question. We present them as engineering alternatives with different trade-off profiles, not as predictions about which will dominate.

Rather than replacing transformers, these architectures may serve as complementary building blocks within compound systems, each contributing capabilities where transformers face constraints. Processing a 100,000 token document with standard attention requires 10 billion pairwise comparisons, which is computationally expensive and economically prohibitive for many applications. The architectural patterns explored here offer different trade-offs that may be favorable for specific workloads.

### State Space Models: Efficient Long-Context Processing {#sec-agi-systems-state-space-models-efficient-longcontext-processing-7ece}

Transformers' attention mechanism compares every token with every other token, creating quadratic scaling: a 100,000 token context requires 10 billion comparisons (100K × 100K pairwise attention scores). This O(n²) memory and computation complexity limits context windows and makes processing book-length documents, multi-hour conversations, or entire codebases prohibitively expensive for real-time applications. The quadratic bottleneck emerges from the attention matrix A = softmax(QKᵀ/√d) where Q, K ∈ ℝⁿˣᵈ must compute all n² pairwise similarities.

State space models[^fn-ssm] offer an alternative by processing sequences in O(n) time through recurrent hidden state updates rather than attention over all prior tokens.

[^fn-ssm]: **State Space Models (SSMs)**: Sequence modeling architecture derived from control theory that maintains a fixed-size hidden state updated recurrently. Unlike transformers with O(n^2) attention, SSMs achieve O(n) complexity by compressing history into a constant-size state vector. The S4 model (Gu et al., 2022) demonstrated competitive language modeling using structured state spaces, while Mamba (2023) achieved transformer-parity through input-dependent state transitions. Systems advantage: inference requires only constant memory regardless of sequence length, enabling million-token contexts that would exhaust GPU memory with transformers.

The fundamental idea draws from control theory: maintain a compressed latent state h ∈ ℝᵈ that summarizes all previous inputs, updating it incrementally as new tokens arrive. Mathematically, state space models implement continuous-time dynamics discretized for sequence processing:

**Continuous form:**
$$
\begin{aligned}
\dot{h}(t) &= Ah(t) + Bx(t) \\
y(t) &= Ch(t) + Dx(t)
\end{aligned}
$$

**Discretized form:**
$$
\begin{aligned}
h_t &= \bar{A}h_{t-1} + \bar{B}x_t \\
y_t &= \bar{C}h_t + \bar{D}x_t
\end{aligned}
$$

where x ∈ ℝ is the input token, h ∈ ℝᵈ is the hidden state, y ∈ ℝ is the output, and {A, B, C, D} are learned parameters mapping between these spaces. Unlike RNN hidden states that suffer from vanishing/exploding gradients, state space formulations use structured matrices (diagonal, low-rank, or Toeplitz) that enable stable long-range dependencies through careful initialization and parameterization.

The technical breakthrough enabling competitive performance came from selective state spaces where the recurrence parameters themselves depend on the input: Āₜ = f_A(xₜ), B̄ₜ = f_B(xₜ), making the state transition input-dependent rather than fixed. This selectivity allows the model to dynamically adjust which information to remember or forget based on current input content. When processing "The trophy doesn't fit in the suitcase because it's too big," the model can selectively maintain "trophy" in state while discarding less relevant words, with the selection driven by learned input-dependent gating similar to LSTM forget gates but within the state space framework. This approach resembles maintaining a running summary that adapts its compression strategy based on content importance rather than blindly summarizing everything equally.

Models like Mamba [@gu2023mamba], RWKV [@peng2023rwkv], and Liquid Time-constant Networks [@hasani2020liquid] demonstrate that this approach can match transformer performance on many tasks while scaling linearly rather than quadratically with sequence length. Using selective state spaces with input-dependent parameters, Mamba achieves 5× better throughput on long sequences (100K+ tokens) compared to transformers. Mamba-7B matches transformer-7B performance on text while using 5× less memory for 100K token sequences. Subsequent developments including Mamba-2 have further improved both efficiency and quality, while hybrid architectures combining state space layers with attention (as in Jamba) suggest that the future may involve complementary mechanisms rather than wholesale architectural replacement. RWKV combines the efficient inference of RNNs with the parallelizable training of transformers, while Liquid Time-constant Networks adapt their dynamics based on input, showing particular promise for time-series and continuous control tasks.

Systems engineering implications are significant. Linear scaling enables processing book-length contexts, multi-hour conversations, or entire codebases within single model calls. This requires rethinking data loading strategies (handling MB-scale inputs), memory management (streaming rather than batch processing), and distributed inference patterns optimized for sequential processing rather than parallel attention.

State space models remain experimental. Transformers benefit from years of optimization across the entire ML systems stack, from specialized hardware kernels (FlashAttention, optimized CUDA implementations) to distributed training frameworks (tensor parallelism, pipeline parallelism) to deployment infrastructure. Alternative architectures must not only match transformer capabilities but also justify the engineering effort required to rebuild this optimization ecosystem. For compound systems, hybrid approaches may prove most practical: transformers for tasks benefiting from parallel attention, state space models for long-context sequential processing, coordinated through the orchestration patterns explored in @sec-emerging-systems-compound-framework.

### Energy-Based Models: Learning Through Optimization {#sec-agi-systems-energybased-models-learning-optimization-e4c6}

Current language models generate text by predicting one token at a time, conditioning each prediction on all previous tokens. This autoregressive approach has key limitations for complex reasoning: it cannot easily revise earlier decisions based on later constraints, struggles with problems requiring global optimization, and tends to produce locally coherent but globally inconsistent outputs.

Energy-based models (EBMs)[^fn-ebm] offer a different approach: learning an energy function $E(x)$ that assigns low energy to probable or desirable configurations $x$ and high energy to improbable ones. The name derives from physics analogies rather than electrical consumption, though as @sec-sustainable-ai established, any new paradigm must ultimately be evaluated against energy efficiency constraints.

[^fn-ebm]: **Energy-Based Models (EBMs)**: Framework where learning defines an energy landscape over possible states, with inference finding low-energy configurations. The concept originated with Hopfield networks (1982) and Boltzmann machines (1985), gaining renewed interest with modern deep learning. Key insight: while discriminative models directly output probabilities, EBMs define unnormalized scores that can capture complex dependencies. Systems challenge: training requires estimating gradients of the partition function (normalization constant), which is computationally intractable for high-dimensional spaces, requiring MCMC approximations or contrastive divergence.

Rather than directly generating outputs, EBMs perform inference through optimization, finding configurations that minimize energy. This paradigm enables several capabilities unavailable to autoregressive models through its fundamentally different computational structure.

First, EBMs enable global optimization by considering multiple interacting constraints simultaneously rather than making sequential local decisions. When planning a multi-step project where earlier decisions constrain later options, autoregressive models must commit to steps sequentially without revising based on downstream consequences. An EBM can formulate the entire plan as an optimization problem where the energy function captures constraint satisfaction across all steps, then search for globally optimal solutions through gradient descent or sampling methods. For problems requiring planning, constraint satisfaction, or multi-step reasoning where local decisions create global suboptimality, this holistic optimization proves essential. Sudoku exemplifies this: filling squares sequentially often leads to contradictions requiring backtracking, while formulating valid completions as low-energy states enables efficient solution through constraint propagation.

Second, the energy landscape naturally represents multiple valid solutions with different energy levels, enabling exploration of solution diversity. Unlike autoregressive models that commit to single generation paths through greedy decoding or limited beam search, EBMs maintain probability distributions over the entire solution space. When designing molecules with desired properties, multiple chemical structures might satisfy constraints with varying trade-offs. The energy function assigns scores to each candidate structure, with inference sampling diverse low-energy configurations rather than collapsing to single outputs. This supports creative applications where diversity matters: generating multiple plot variations for a story, exploring architectural design alternatives, or proposing candidate drug molecules for synthesis and testing.

Third, EBMs support bidirectional reasoning that propagates information both forward and backward through inference. Autoregressive generation flows unidirectionally from start to end, unable to revise earlier decisions based on later constraints. EBMs perform inference through iterative refinement that can modify any part of the output to reduce global energy. When writing poetry where the final line must rhyme with the first, EBMs can adjust earlier lines to enable satisfying conclusions. This bidirectional capability extends to causal reasoning: inferring probable causes from observed effects, planning actions that achieve desired outcomes, and debugging code by working backward from error symptoms to root causes. The inference procedure treats all variables symmetrically, enabling flexible reasoning in any direction needed.

Fourth, energy levels provide principled uncertainty quantification through the Boltzmann distribution p(x) ∝ exp(-E(x)/T) where temperature T controls confidence calibration. Solutions with energy far above the minimum receive exponentially lower probability, providing natural confidence scores. This supports robust decision making in uncertain environments: when multiple completion options have similar low energies, the model expresses uncertainty rather than overconfidently committing to arbitrary choices. For safety-critical applications like medical diagnosis or autonomous vehicle control, knowing when the model is uncertain enables deferring to human judgment rather than blindly executing potentially incorrect decisions. The energy-based framework inherently provides the uncertainty estimates that autoregressive models must learn separately through ensemble methods or Bayesian approximations.

Systems engineering challenges are considerable. Inference requires solving optimization problems that can be computationally expensive, particularly for high-dimensional spaces. Training EBMs often involves contrastive learning methods requiring negative example generation through MCMC sampling[^fn-mcmc] or other computationally intensive procedures. The optimization landscapes can contain many local minima, requiring sophisticated inference algorithms.

[^fn-mcmc]: **Markov Chain Monte Carlo (MCMC)**: Statistical sampling method using Markov chains to generate samples from complex probability distributions. Developed by Metropolis [@metropolis1953equation] and Hastings [@hastings1970monte]. In ML, MCMC generates negative examples for contrastive learning by sampling from energy-based models. Computational cost grows exponentially with dimension, requiring 1000-10000 samples per iteration.

These challenges create opportunities for systems innovation. Specialized hardware for optimization (quantum annealers, optical computers) could provide computational advantages for EBM inference. Hierarchical energy models could decompose complex problems into tractable subproblems. Hybrid architectures could combine fast autoregressive generation with EBM refinement for improved solution quality.

In compound AI systems, EBMs could serve as specialized reasoning components handling constraint satisfaction, planning, and verification tasks, domains where optimization-based approaches excel. While autoregressive models generate fluent text, EBMs ensure logical consistency and constraint adherence. This division of labor uses each approach's strengths while mitigating weaknesses, exemplifying the compound systems principle explored in @sec-emerging-systems-compound-framework.

### World Models and Predictive Learning {#sec-agi-systems-world-models-predictive-learning-9e54}

Building on the self-supervised learning principles established in @sec-agi-systems-selfsupervised-learning-components-e6d8, true AGI requires world models: learned internal representations of how environments work that support prediction, planning, and causal reasoning across diverse domains.

World models are internal simulations that capture causal relationships enabling systems to predict consequences of actions, reason about counterfactuals, and plan sequences toward goals. While current AI predicts surface patterns in data through next-token prediction, world models understand underlying mechanisms. Consider the difference: a language model learns that "rain" and "wet" frequently co-occur in text, achieving statistical association. A world model learns that rain causes wetness through absorption and surface wetting, enabling predictions about novel scenarios (Will a covered object get wet in rain? No, because the cover blocks causal mechanism) that pure statistical models cannot make.

The technical distinction manifests in representation structure. Autoregressive models maintain probability distributions over sequences: P(x₁, x₂, ..., xₙ) = ∏ᵢ P(xᵢ | x₁, ..., xᵢ₋₁), predicting each token given history. World models instead learn latent dynamics: sₜ₊₁ = f(sₜ, aₜ) mapping current state sₜ and action aₜ to next state, with separate observation model o = g(s) rendering states to observations. This factorization enables forward simulation (predicting long-term consequences), inverse models (inferring actions that produced observed outcomes), and counterfactual reasoning (what would happen if action differed).

DeepMind's MuZero [@schrittwieser2020mastering] demonstrates world model principles in game playing. Rather than learning rules explicitly, MuZero learns three functions: representation (mapping observations to hidden states), dynamics (predicting next hidden state from current state and action), and prediction (estimating value and policy from hidden state). Starting without game rules, it discovers that certain piece configurations lead to winning outcomes, enabling superhuman play in chess, shogi, and Go through learned causal models rather than explicit rule specification.

This paradigm shift uses the Joint Embedding Predictive Architecture (JEPA) framework introduced earlier, moving beyond autoregressive generation toward predictive intelligence that understands causality.

**Prediction vs. Generation**

A key insight from LeCun's world model hypothesis is the distinction between generating pixels/tokens and predicting states. Generative models (LLMs and diffusion models) must predict every detail $x$, often hallucinating plausible but incorrect details to fill gaps. This is computationally expensive. Predictive world models (JEPA) instead predict the representation of the future state $y$ in an abstract latent space. They ignore irrelevant details (such as the exact pattern of leaves on a tree) to focus on causal dynamics (such as the car will hit the tree).

Instead of generating text tokens sequentially, future AGI systems predict consequences of actions in abstract representation spaces. For robotics, this means predicting how objects move when pushed (physics world model). For language, this means predicting how conversations evolve based on speaking strategies (social world model). For reasoning, this means predicting how mathematical statements follow from axioms (logical world model).

**Causal Reasoning Framework for World Models**

World models must distinguish correlation from causation to support genuine reasoning. This distinction matters practically: systems that merely correlate cannot predict intervention consequences, cannot answer counterfactual questions, and cannot transfer learning to new domains. A model that learns "rain correlates with wet streets" cannot predict what happens if we artificially wet the streets (does it cause rain?), while a causal model understanding that rain causes wetness answers such questions correctly.

To understand what separates true world models from sophisticated pattern matchers, we need a formal framework for reasoning about causality. Judea Pearl's causal hierarchy provides exactly this framework, distinguishing three increasingly sophisticated levels of causal understanding. The mathematical notation in the following presentation may appear dense, but the underlying intuition is accessible: each level represents a qualitatively different capability that builds on the previous one. Current language models operate almost entirely at the lowest level.

**Pearl's Causal Hierarchy.** Causal reasoning operates at three levels, each requiring increasingly sophisticated capabilities:

**Level 1: Association** ($P(Y | X)$) answers "What is the probability of $Y$ given we observe $X$?" This requires only observational data and standard conditional probability. Current LLMs operate at this level, modeling $P(\text{next\_token} | \text{context})$ from training distribution patterns. They learn that "rain" and "umbrella" co-occur but not why. A model achieves high accuracy predicting $P(\text{umbrella} | \text{rain})$ purely from statistical association, sufficient for many predictive tasks but insufficient for reasoning about interventions.

**Level 2: Intervention** ($P(Y | do(X))$) answers "What happens to $Y$ if we force $X$ to occur?" This requires causal structure and cannot be computed from observational data alone. The distinction matters when confounders exist. Consider:

$$P(\text{umbrella} | \text{rain}) \neq P(\text{umbrella} | do(\text{rain}))$$

if weather forecasts confound the relationship. Observing rain updates beliefs about forecasts (which caused people to bring umbrellas), but forcing rain artificially doesn't change forecasts. Planning requires $do()$ operations: "What if I move this object?" demands predicting consequences of actions, not mere observation. Current LLMs cannot reliably perform Level 2 reasoning because they model $P(X)$ from passive observation, lacking the causal graph structure necessary for intervention calculus.

**Level 3: Counterfactual** ($P(Y_x | X', Y')$) answers "What would $Y$ have been if $X$ had been different, given we observed $X'$ and $Y'$?" This requires structural causal models with latent variables. Example: "Would there have been an accident if I hadn't braked?" given that braking occurred and no accident resulted. Counterfactual reasoning enables learning from near-misses, credit assignment in multi-agent systems, and robust decision-making under uncertainty. This represents the most sophisticated causal reasoning, requiring models of unobserved factors affecting both actions and outcomes.

**Structural Causal Models for World Modeling.** To achieve Level 2 and 3 reasoning, world models require structural causal models (SCMs) that explicitly represent causal relationships. An SCM consists of:

1. Variables $V = \{X_1, ..., X_n\}$ representing observable states
2. Structural equations $X_i = f_i(PA_i, U_i)$ where $PA_i$ are causal parents and $U_i$ are latent factors
3. Distribution $P(U)$ over unobserved variables

For world models, this translates to forward and inverse models:

**Forward Model**: Predicts next state from current state and action
$$s_{t+1} = f(s_t, a_t, u_t)$$

where $s_t$ is current state, $a_t$ is action, and $u_t$ represents unobserved factors (noise, environmental randomness, hidden variables). Learning $f$ from observations where $u_t$ is latent constitutes the world modeling challenge. Unlike supervised learning with paired $(x,y)$ examples, world models must infer causal structure from sequential observations where latent factors create stochasticity.

**Inverse Model**: Infers actions from state transitions
$$a_t = g(s_t, s_{t+1}, u_t)$$

This enables planning through optimization: find actions $a_{1:T}$ that achieve desired state $s_{goal}$. The inverse problem is fundamentally harder than forward prediction because state transitions are many-to-one (multiple action sequences can produce the same outcome) while inverse inference is one-to-many (a single transition may result from many possible actions).

**Quantitative Diagnostic: Why Causality Matters.** Compositional physical reasoning provides measurable evidence for causal understanding's importance. Consider:

**Setup**: Train models on objects A and B in isolation (push, stack, separate manipulation primitives). Test on novel compositions (stack A on B then push both, push A to enable stacking B on top).

**Level 1 System Performance** (LLM, correlation-based): 23% accuracy on compositional tasks. These models retrieve similar seen scenarios from training data but fail when asked to compose known operations in novel ways. When asked "What happens if you push the stack?" the model searches for training examples matching "push" and "stack" rather than composing causal understanding of pushing (applies force causing motion) with stacking (requires supporting surface).

**Level 2 System Performance** (World model with causal graph): 78% accuracy on compositional tasks. These systems understand that push CAUSES movement through force application, and stack REQUIRES stable support, enabling them to reason about compositions absent from training. When asked "What happens if you push the stack?" the model composes: push causes force on bottom object, which propagates to top object via contact, causing both to move together.

**Quantitative Gap**: 78% - 23% = 55 percentage points. This absolute improvement quantifies causal reasoning's value for compositional generalization. The performance difference emerges specifically on out-of-distribution compositions: both systems achieve 95%+ accuracy on in-distribution tasks seen during training, but only causal world models generalize systematically to novel combinations.

**Learning Causal Structure.** How do world models acquire causal graphs from observation? Three approaches emerge:

1. **Intervention-based learning**: Actively manipulate environment and observe consequences. Robotics systems learn physics by pushing objects and observing motion, building causal models mapping actions to effects. This requires embodied interaction: $10^4$ push actions suffice to learn basic mechanics, while passive observation of $10^6$ video frames yields weaker models lacking intervention understanding.

2. **Temporal precedence**: Exploit time's arrow—causes precede effects. From video sequences showing rain followed by wet surfaces, temporal models infer rain→wet causality. This succeeds when temporal ordering reflects causality but fails with confounders (forecasts cause both umbrella-carrying and subsequent rain observation).

3. **Invariance-based learning**: Identify relationships stable across distribution shifts. Causal relationships (rain causes wetness) remain constant across locations and times, while spurious correlations (rain correlates with traffic) vary by context. Training on diverse environments and identifying invariant patterns enables causal discovery [@peters2016causal].

**Systems Engineering for Causal World Models.** Implementing causal reasoning at scale creates distinct engineering challenges:

**Evaluation Infrastructure**: Testing causal understanding requires intervention experiments, not passive prediction metrics. Standard benchmarks measure $P(Y | X)$ accuracy, but causal models need evaluation on $P(Y | do(X))$. This demands simulation environments supporting counterfactual queries or carefully designed A/B tests in production systems. Offline evaluation must construct intervention distributions from observational data using causal inference techniques (propensity score matching, instrumental variables).

**Data Requirements**: Learning causal structure requires diverse intervention data. Passive observation of $10^9$ examples may produce worse causal models than active intervention on $10^6$ examples. For embodied systems, this motivates curiosity-driven exploration: agents actively seek state-action pairs informative about causal structure rather than passively observing common trajectories. The data engineering challenge involves generating or collecting interventional data at scale, requiring simulation environments, human demonstrations of diverse strategies, or active learning protocols.

**Architectural Design**: Causal world models separate dynamics (how states evolve causally) from observation (how states map to perceptions). This factorization enables:

- **Modularity**: Swap dynamics models when environment physics change while preserving observation models
- **Transfer**: Causal structure learned in simulation transfers to physical systems despite appearance differences
- **Interpretability**: Causal graphs provide human-readable explanations for predictions

These benefits require architectural choices encoding causal structure: graph neural networks representing relational dynamics, modular architectures separating object-level causal models, and symbolic layers encoding logical constraints.

**Connection to Systems Engineering.** As ML systems engineers designing AGI components, causal reasoning provides formal tools for intervention analysis—the essence of engineering. When you design a serving system, you reason causally: "If I increase batch size (intervention), latency will rise (consequence) because per-request processing time grows." This is Level 2 reasoning. World models achieving this capability can plan effectively, while those limited to Level 1 correlation cannot.

The causal hierarchy clarifies why scaled LLMs struggle with novel reasoning despite vast training data: they model $P(\text{text})$ from observation, lacking the causal structure necessary for $P(\text{outcome} | do(\text{action}))$ inference. Future AGI systems require world models implementing Level 2 reasoning at minimum, with Level 3 capabilities emerging as systems handle increasing environmental complexity.

Systems engineering challenges span multiple dimensions. Data requirements grow substantially: learning accurate world models requires petabytes of multimodal interaction data capturing diverse causal patterns, far exceeding text-only training. Architecture design must support temporal synchronization across multiple sensory modalities (vision at 30 Hz, audio at 16 kHz, proprioception at 1 kHz), requiring careful buffer management and alignment. Training procedures must enable continuous learning from streaming data without catastrophic forgetting (challenges explored in @sec-agi-systems-continual-learning-lifelong-adaptation-7aee), updating world models as environments change while preserving previously learned causal relationships.

Verification poses unique challenges. Evaluating world models requires testing causal predictions, not just statistical accuracy. A model predicting "umbrellas appear when it rains" achieves high statistical accuracy but fails causally, as umbrellas don't cause rain. Testing requires intervention experiments: if the model believes rain causes umbrellas, removing umbrellas shouldn't affect predicted rain. Implementing such causal testing at scale demands sophisticated evaluation infrastructure beyond standard ML benchmarking.

In compound systems, world model components provide causal understanding and planning capabilities while other components handle perception, action selection, or communication. This specialization enables developing robust world models for specific domains (physical laws for robotics, social dynamics for dialogue, logical rules for mathematics) while maintaining flexibility to combine them for complex, multi-domain reasoning tasks. A household robot might use physical world models to predict object trajectories, social world models to anticipate human actions, and planning algorithms to sequence manipulation steps achieving desired outcomes.

### Hybrid Architecture Integration Strategies {#sec-agi-systems-hybrid-architecture-integration-strategies-50c2}

The paradigms explored above address complementary transformer limitations through different computational approaches, yet none represents a complete replacement. Transformers excel at parallel processing and fluent natural language generation but suffer quadratic memory scaling and sequential generation constraints. State space models achieve linear complexity but lack transformers' expressive attention patterns. Energy-based models enable global optimization but require expensive inference. World models provide causal reasoning but demand extensive multimodal training data. The path forward lies not in choosing one paradigm but orchestrating hybrid compound systems that use each architecture's strengths while mitigating weaknesses.

Several integration patterns emerge from current research. Cascade architectures route inputs sequentially through specialized components, with each stage refining outputs from previous stages. A language understanding pipeline might use transformers for initial parsing, world models for causal inference about described events, and energy-based models for constraint checking and consistency verification. This sequential specialization enables sophisticated reasoning pipelines where each component contributes distinct capabilities.

Parallel ensemble approaches combine multiple architectures processing inputs simultaneously, with results aggregated through learned weighting or voting mechanisms. A question-answering system might generate candidate answers using transformers, score them using energy-based models evaluating logical consistency, and rank them using world models predicting downstream consequences. This redundancy provides robustness: if one architecture fails on particular inputs, others may succeed.

Hierarchical decomposition assigns architectures to different abstraction levels. High level planning might use world models to predict long-term consequences, mid level execution might use transformers for action generation, and low level control might use state space models for real-time response. This vertical integration enables systems to reason at multiple timescales simultaneously, from millisecond reflexes to multi-hour plans.

The most sophisticated integration strategy involves dynamic routing based on input characteristics and task requirements. An orchestrator analyzes incoming requests and selects appropriate architectural components adaptively. Mathematical proofs route to symbolic reasoners augmented by transformer hint generation. Creative writing tasks route to transformers optimized for fluent generation. Long document summarization routes to state space models handling extended contexts. Physical manipulation planning routes to world models predicting object dynamics. This adaptive specialization requires meta-learning systems that learn which architectures excel for particular task distributions.

Implementation challenges compound with architectural heterogeneity. Training procedures must accommodate different computational patterns: transformers parallelize across sequence positions, recurrent models process sequentially, and energy-based models require iterative optimization. Gradient computation differs fundamentally: transformers backpropagate through deterministic operations, world models backpropagate through learned dynamics, and energy-based models require contrastive estimation. Framework infrastructure (PyTorch, TensorFlow, JAX) must evolve to support these diverse training paradigms within unified pipelines.

Hardware acceleration presents similar challenges. Transformers map efficiently to GPU tensor cores optimized for dense matrix multiplication. State space models benefit from sequential processing engines with optimized memory access patterns. Energy-based models require optimization hardware accelerating iterative refinement. Compound systems must orchestrate computation across heterogeneous accelerators, routing different architectural components to appropriate hardware substrates while minimizing data movement overhead.

Deployment and monitoring infrastructure must track diverse failure modes across architectural components. Transformer failures typically manifest as fluency degradation or factual errors. Energy-based model failures appear as optimization convergence issues or constraint violations. World model failures show as incorrect causal predictions or planning breakdowns. Observability systems must detect and diagnose failures across these different failure semantics, requiring architectural-specific monitoring strategies within unified operational frameworks.

The compound AI systems framework from @sec-emerging-systems-compound-framework provides organizing principles for managing this architectural heterogeneity. By treating each paradigm as a specialized component with well defined interfaces, compound systems enable architectural diversity while maintaining system coherence. The following sections on training methodologies, infrastructure requirements, and operational practices apply across these architectural paradigms, though specific implementations vary based on computational substrate.

## Training Methodologies for Compound Systems {#sec-agi-systems-training-methodologies-compound-systems-e3fa}

We have now surveyed the computational building blocks for compound intelligence: data engineering approaches that generate training content at scale through self-supervised learning, synthetic generation, and self-play; dynamic architectures that route computation efficiently through mixture-of-experts patterns and external memory systems; and alternative paradigms including state space models for long-context processing, energy-based models for optimization-driven reasoning, and world models for causal understanding. Each represents a potential component within compound systems, offering capabilities that monolithic transformers cannot achieve through scaling alone.

Yet having the right building blocks is insufficient. The critical question becomes: how do we train these diverse components to work together reliably and safely? Unlike monolithic models that train end-to-end on fixed datasets, compound systems face unique training challenges: aligning multiple specialized components with human values, enabling continuous learning from deployment experience, and maintaining safety guarantees across component interactions.

This section examines three training advances that address these challenges. First, we explore how Reinforcement Learning from Human Feedback (RLHF) and Constitutional AI enable alignment across compound system components. Second, we examine continual learning techniques that allow systems to improve through deployment without catastrophic forgetting. Third, we survey the production infrastructure required to support these novel training paradigms at scale.

### Alignment Across Components {#sec-agi-systems-alignment-across-components-9552}

Compound systems face an alignment challenge that builds upon responsible AI principles (@sec-responsible-ai) while extending beyond current safety frameworks to address systems that may exceed human capabilities: each specialized component must align with human values while the orchestrator must coordinate these components appropriately. Traditional supervised learning creates a mismatch where models trained on internet text learn to predict what humans write, not what humans want. GPT-3 completions for sensitive historical prompts varied significantly, with some evaluations showing concerning outputs in a minority of cases, accurately reflecting web content distribution rather than truth.

For compound systems, misalignment in any component can compromise the entire system: a search component that retrieves biased information, a reasoning component that perpetuates harmful stereotypes, or a safety filter that fails to catch problematic content.

#### Human Feedback for Component Training {#sec-agi-systems-human-feedback-component-training-6f85}

Addressing these alignment challenges, Reinforcement Learning from Human Feedback (RLHF)[^fn-rlhf] [@christiano2017deep; @ouyang2022training] addresses alignment through multi-stage training that compounds naturally to system-level alignment.

[^fn-rlhf]: **Reinforcement Learning from Human Feedback (RLHF)**: Training paradigm combining supervised learning with reinforcement learning guided by human preferences. Pioneered by OpenAI (2017) and scaled to production by Anthropic and OpenAI (2022). Systems implications: RLHF requires maintaining three distinct model copies (reference policy, reward model, active policy) during training, approximately tripling memory requirements. A single RLHF training run for a 70B parameter model requires approximately 1,000 A100-hours, costing $15,000-25,000 in cloud compute.

Rather than training on text prediction alone, RLHF creates specialized components within the training pipeline itself.

The process exemplifies compound systems design through three distinct stages, each with specific technical requirements. Stage 1 begins with supervised fine-tuning on high-quality demonstrations. Human annotators write example responses to prompts demonstrating desired behavior, providing approximately 10,000-100,000 demonstrations across diverse tasks. This initial fine-tuning transforms a base language model (trained purely on text prediction) into an instruction-following assistant, though without understanding human preferences for different response qualities.

Stage 2 collects comparative feedback to train a reward model. Rather than rating responses on absolute scales (difficult for humans to calibrate consistently), annotators compare multiple model outputs for the same prompt, selecting which response better satisfies criteria like helpfulness, harmlessness, and honesty. The system generates 4-10 candidate responses per prompt, with humans ranking or doing pairwise comparisons. From these comparisons, a separate reward model learns to predict human preferences, mapping any response to a scalar reward score estimating human judgment. This reward model achieves approximately 70-75% agreement with held-out human preferences, providing automated quality assessment without requiring human evaluation of every output.

Stage 3 applies reinforcement learning to optimize policy using the learned reward model. Proximal Policy Optimization (PPO)[^fn-ppo] [@schulman2017proximal] fine-tunes the language model to maximize expected reward while preventing excessive deviation from the supervised fine-tuned initialization through KL divergence penalties.

[^fn-ppo]: **Proximal Policy Optimization (PPO)**: Reinforcement learning algorithm developed by OpenAI (2017) that updates policies incrementally to prevent training instability. Key innovation: clipping the objective function to limit how much the policy can change per update, preventing catastrophic forgetting and reward hacking. PPO became the standard for RLHF because it balances sample efficiency with stability. Alternative approaches like Direct Preference Optimization (DPO) eliminate the RL phase entirely, directly optimizing from preference pairs with 2-3x lower computational cost.

This constraint proves critical: without it, models exploit reward model weaknesses, generating nonsensical outputs that fool the reward predictor but fail true human judgment. The KL penalty β controls this trade-off, typically set to 0.01-0.1, allowing meaningful improvement while maintaining coherent outputs. @fig-rlhf-pipeline traces this iterative process where each reinforcement learning step generates responses, computes rewards, and updates policy gradients until the model converges on outputs that satisfy both the reward model and the KL constraint.

::: {#fig-rlhf-pipeline fig-env="figure" fig-pos="htb" fig-cap="**RLHF Training Pipeline**: The three-stage process transforms base language models into aligned assistants. Stage 1 uses human demonstrations for initial fine-tuning. Stage 2 collects human preferences to train a reward model. Stage 3 applies reinforcement learning (PPO) to optimize for human preferences while preventing mode collapse through KL divergence penalties." fig-alt="Three-stage RLHF pipeline. Stage 1: human demos through supervised fine-tuning to base model. Stage 2: model generates ranked outputs to train reward model. Stage 3: PPO with KL penalty produces aligned model."}
```{.tikz}
\begin{tikzpicture}[line cap=round,line join=round,font=\usefont{T1}{phv}{m}{n}]
\tikzset{
  Box/.style={align=center,
    inner xsep=2pt,
    node distance=0.45,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL!60,
    text width=25mm,
    minimum width=17mm, minimum height=10mm
  },
   Box2/.style={Box, fill=RedL!60,draw=RedLine},
   Box3/.style={Box, fill=BlueL!60,draw=BlueLine},
Line/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},
LineA/.style={violet!50,line width=2.0pt,{-{Triangle[width=1.1*6pt,length=2.0*6pt]}},shorten <=3pt,shorten >=2pt},
ALine/.style={black!50, line width=1.1pt,{{Triangle[width=0.9*6pt,length=1.2*6pt]}-}},
Larrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,
            single arrow head indent=0pt,minimum height=10mm, minimum width=3pt}
}
% #1 number of teeth
% #2 radius intern
% #3 radius extern
% #4 angle from start to end of the first arc
% #5 angle to decale the second arc from the first
% #6 inner radius to cut off
\newcommand{\gear}[6]{%
  (0:#2)
  \foreach \i [evaluate=\i as \n using {(\i-1)*360/#1}] in {1,...,#1}{%
    arc (\n:\n+#4:#2) {[rounded corners=1.5pt] -- (\n+#4+#5:#3)
    arc (\n+#4+#5:\n+360/#1-#5:#3)} --  (\n+360/#1:#2)
  }%
  (0,0) circle[radius=#6];
}
%graph style
\tikzset{
pics/graph/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=GRAPH,scale=1, every node/.append style={transform shape}]
\def\dx{\Width}
\def\dy{\Height}
\def\dz{\Depth}
% koordinata donjeg levog ugla (početak bara)
\def\x{0}
\def\y{0.15}
\def\z{0}
% boje
\draw[draw=\filllcirclecolor,line width=1pt](-0.2,0)--(1.3,0);
\draw[draw=\filllcirclecolor,line width=1pt](-0.2,0)--(-0.2,1.2);
\filldraw[fill=\filllcolor!10, draw=\drawcolor] (\x,\y+\dy,\z) -- (\x,\y+\dy,\z+\dz) -- (\x+\dx,\y+\dy,\z+\dz) -- (\x+\dx,\y+\dy,\z) -- cycle; % gornja strana
\filldraw[fill=\filllcolor!50, draw=\drawcolor] (\x+\dx,\y,\z) -- (\x+\dx,\y,\z+\dz) -- (\x+\dx,\y+\dy,\z+\dz) -- (\x+\dx,\y+\dy,\z) -- cycle; % desna strana
\filldraw[fill=\filllcolor!60, draw=\drawcolor] (\x,\y,\z+\dz) -- (\x+\dx,\y,\z+\dz) -- (\x+\dx,\y+\dy,\z+\dz) -- (\x,\y+\dy,\z+\dz) -- cycle; % prednja strana
\end{scope}
    }
  }
}
%person style
 \tikzset{
 pics/man/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=PERSON,scale=\scalefac, every node/.append style={transform shape}]
     % tie
    \draw[draw=\tiecolor,fill=\tiecolor] (0.0,-1.1)--(0.16,-0.87)--(0.09,-0.46)--(0.13,-0.37)--(0.0,-0.28)
                   --(-0.13,-0.37)--(-0.09,-0.46)--(-0.16,-0.87)--cycle;
    % ears
    \draw[fill=black] (0.74,0.95) to[out=20,in=80](0.86,0.80) to[out=250,in=330](0.65,0.65) to[out=70,in=260] cycle;
    \draw[fill=black] (-0.76,0.96) to[out=170,in=110](-0.85,0.80) to[out=290,in=190](-0.65,0.65) to[out=110,in=290] cycle;

    % head
    \draw[fill=black] (0,0) to[out=180,in=290](-0.72,0.84) to[out=110,in=190](-0.56,1.67)
                      to[out=70,in=110](0.68,1.58) to[out=320,in=80](0.72,0.84) to[out=250,in=0] cycle;
    % face
    \draw[fill=white] (0,0.11) to[out=175,in=290](-0.53,0.65) to[out=110,in=265](-0.61,1.22)
                      to[out=80,in=235](-0.50,1.45) to[out=340,in=215](0.50,1.47)
                      to[out=310,in=85](0.60,0.92) to[out=260,in=2] cycle;
    \draw[fill=black] (-0.50,1.45) to[out=315,in=195](0.40,1.25) to[out=340,in=10](0.37,1.32)
                      to[out=190,in=310](-0.40,1.49) -- cycle;
    % neck
    \draw[line width=1.0pt] (-0.62,-0.2) to[out=50,in=290] (-0.5,0.42);
    \draw[line width=1.0pt] (0.62,-0.2) to[out=130,in=250] (0.5,0.42);
    % body
    \draw[draw=\bodycolor,fill=\bodycolor,line width=\Linewidth] (0.0,-1.0) to[out=150,in=290](-0.48,-0.14) to[out=200,in=50](-1.28,-0.44)
                   to[out=240,in=80](-1.55,-2.06) -- (1.55,-2.06)
                   to[out=100,in=300](1.28,-0.44) to[out=130,in=340](0.49,-0.14)
                   to[out=245,in=30] cycle;
    % right stet
    \draw[line width=3pt,\stetcolor] (0.8,-0.21) to[bend left=7](0.78,-0.64)
         to[out=350,in=80](0.98,-1.35) to[out=250,in=330](0.72,-1.60);
    \draw[line width=3pt,\stetcolor] (0.43,-1.53) to[out=180,in=240](0.3,-1.15)
         to[out=60,in=170](0.78,-0.64);
    % left stet
    \draw[line width=3pt,\stetcolor] (-0.75,-0.21) to[bend right=20](-0.65,-1.45);
    \node[fill=\stetcolor,circle,minimum size=5pt] at (-0.65,-1.45) {};
    % eyes
    \node[circle,fill=black,inner sep=2pt] at (0.28,0.94) {};
    \node[circle,fill=black,inner sep=2pt] at (-0.28,0.94) {};
     % mouth
    \draw[line width=1.1pt] (-0.25,0.5) to[bend right=40](0.25,0.5);
 \end{scope}
     }
  }
}
%medal style
\tikzset{/pgf/decoration/.cd,
    number of sines/.initial=10,
    angle step/.initial=20,
}
\newdimen\tmpdimen
\pgfdeclaredecoration{complete sines}{initial}
{
    \state{initial}[
        width=+0pt,
        next state=move,
        persistent precomputation={
            \pgfmathparse{\pgfkeysvalueof{/pgf/decoration/angle step}}%
            \let\anglestep=\pgfmathresult%
            \let\currentangle=\pgfmathresult%
            \pgfmathsetlengthmacro{\pointsperanglestep}%
                {(\pgfdecoratedremainingdistance/\pgfkeysvalueof{/pgf/decoration/number of sines})/360*\anglestep}%
        }] {}
    \state{move}[width=+\pointsperanglestep, next state=draw]{
        \pgfpathmoveto{\pgfpointorigin}
    }
    \state{draw}[width=+\pointsperanglestep, switch if less than=1.25*\pointsperanglestep to final, % <- bit of a hack
        persistent postcomputation={
        \pgfmathparse{mod(\currentangle+\anglestep, 360)}%
        \let\currentangle=\pgfmathresult%
    }]{%
        \pgfmathsin{+\currentangle}%
        \tmpdimen=\pgfdecorationsegmentamplitude%
        \tmpdimen=\pgfmathresult\tmpdimen%
        \divide\tmpdimen by2\relax%
        \pgfpathlineto{\pgfqpoint{0pt}{\tmpdimen}}%
    }
    \state{final}{
        \ifdim\pgfdecoratedremainingdistance>0pt\relax
            \pgfpathlineto{\pgfpointdecoratedpathlast}
        \fi
   }
}
\tikzset{
pics/medal/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\draw[draw=none,fill=\filllcolor!60](-0.48,-0.10)--(-0.68,-0.68)--(-0.92,-1.38)--
(-0.53,-1.28)--(-0.29,-1.61)--(-0.09,-0.93)--(0.15,-0.1)--cycle;
\draw[draw=none,fill=\filllcolor!60](-0.266,-0.10)--(-0.02,-0.93)--(0.18,-1.61)--
(0.45,-1.34)--(0.85,-1.48)--(0.61,-0.68)--(0.44,-0.1)--cycle;
 \draw[draw=none,postaction={very thick, line join=round, draw=white,fill=\filllcolor,
        decorate,decoration={complete sines, number of sines=9, amplitude=\scalefac*4pt}}] (0,0) circle [radius=0.9];
\node[draw=none,fill=white,circle,minimum size=11mm,line width=1pt](CM-\picname) {};
%
\end{scope}
    }
  }
}
%shield
\def\inset{3.2pt} %
\def\myshape{%
  (0,1.34) to[out=220,in=0] (-1.20,1.03) --
  (-1.20,-0.23) to[out=280,in=160] (0,-1.53) to[out=20,in=260] (1.20,-0.23) --
  (1.20,1.03)  to[out=180,in=320] cycle
}
\tikzset{
pics/stit/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\fill[fill=\filllcolor!60] \myshape;
%
\begin{scope}
  \clip \myshape;
  \draw[draw=\filllcolor!60, line width=3*\inset,fill=white] \myshape;
\end{scope}
\fill[fill=\filllcirclecolor!60](0,0)circle(0.4);
\end{scope}
    }
  }
}
%%%
\pgfkeys{
  /channel/.cd,
  Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawcolor/.store in=\drawcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
 tiecolor/.store in=\tiecolor,
  bodycolor/.store in=\bodycolor,
  stetcolor/.store in=\stetcolor,
  tiecolor=red,      % default tie color
  bodycolor=blue!30,  % default body color
  stetcolor=green,  % default stet color
  filllcolor=BrownLine,
  filllcirclecolor=violet!20,
  drawcolor=black,
  drawcircle=violet,
  scalefac=1,
  Linewidth=0.5pt,
  Depth=0.2,
  Height=0.5,
  Width=0.25,
  picname=C
}
%person
\begin{scope}[local bounding box=PERSON1,shift={($(0,0)+(0,0)$)},scale=0.9, every node/.append style={transform shape}]
\pic at (0,0) {man={scalefac=0.35,tiecolor=red, bodycolor=BrownLine,stetcolor=BrownLine, Linewidth=1.0pt}};
\pic at (1.5,0) {man={scalefac=0.35,tiecolor=red, bodycolor=BrownLine,stetcolor=BrownLine, Linewidth=1.0pt}};
\pic at (0.75,0.16) {man={scalefac=0.43,tiecolor=GreenD, bodycolor=red!80!black,stetcolor=red!80!black, Linewidth=1.0pt}};
\end{scope}
%gears
\begin{scope}[local bounding box=GEAR1,shift={(4.6,0.3)},scale=2.0,every node/.append style={scale=1}]
\colorlet{black}{red!60!black}
\fill[draw=none,fill=black,even odd rule,xshift=-2mm]coordinate(GE1)\gear{10}{0.23}{0.28}{10}{2}{0.1};
\fill[draw=none,fill=black,even odd rule,xshift=2.19mm,yshift=-2.91mm]coordinate(GE2)\gear{10}{0.18}{0.22}{10}{2}{0.08};
\fill[draw=none,fill=black,even odd rule,xshift=-5.7mm,yshift=-2.8mm]coordinate(GE3)\gear{10}{0.15}{0.19}{10}{2}{0.08};
\node[draw=none,inner xsep=8,inner ysep=8,yshift=0mm,
           fill=none,fit=(GE1)(GE2)(GE3),line width=1.0pt](BB1){};
\end{scope}
%Base Model
\node[Box,below =0.8 of GEAR1](BM){Base Model};
%graph
\begin{scope}[local bounding box=GRAPH1,shift={($(BM)+(4.1,-0.60)$)},scale=1.2, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){graph={filllcirclecolor=black!60,scalefac=0.5,picname=1,drawcolor=black,filllcolor=red,Height=0.5,Linewidth=1.25pt}};
\pic[shift={(0.33,0)}] at  (0,0){graph={filllcirclecolor=none,scalefac=0.5,picname=2,drawcolor=black,filllcolor=red,Height=1,Linewidth=1.25pt}};
\pic[shift={(0.66,0)}] at  (0,0){graph={filllcirclecolor=none,scalefac=0.5,picname=3,drawcolor=black,filllcolor=red,Height=0.25,Linewidth=1.25pt}};
\pic[shift={(0.99,0)}] at  (0,0){graph={filllcirclecolor=none,scalefac=0.5,picname=4,drawcolor=black,filllcolor=red,Height=0.75,Linewidth=1.25pt}};
\end{scope}
\node[Box2,below =1.9 of GRAPH1,xshift=-5mm](PM){Policy Model};
\node[Box3,right =6.1 of GEAR1](TRM){Train\\ Reward Model};
\path[red](PM)-|coordinate(PB1)(TRM);
\node[Box2](RLFT)at(PB1){RL\\ Fine-Tuning};
\coordinate(PB2)at($($(TRM.south east)!0.35!(RLFT.north east)$)+(2.5,0)$);
\fill[red](PB2)circle(2pt);
\node[circle,draw=none,minimum size=20mm,fill=cyan!20](C1) at(PB2){};
\pic[shift={(0.03,0.2)}] at  (C1){medal={scalefac=0.55,picname=1,drawcolor=orange,filllcirclecolor=orange!70,filllcolor=orange}};
\path[red](RLFT)-|coordinate(PB3)(C1);
\node[circle,draw=none,minimum size=20mm,fill=orange!30](C2) at(PB3){};
\pic[shift={(0.03,-0.02)}] at  (C2){stit={scalefac=0.48,picname=1,
filllcirclecolor=green!85!black,filllcolor=GreenD}};
%text
\node[above=0.5mm of PERSON1,align=center](HD){Human Demos};
\path[red](HD)-|coordinate(T2)(GEAR1);
\node[align=center](SFT)at(T2){Supervised Fine-tuning};
\node[below=1mm of GRAPH1.230,align=center](THR){Human Rankings};
\node[below= 0.5mm of C1](TRMT){Reward Model};
\node[below= 1mm of C2](TAM){Aligned Model};
\node[above right=2pt and -3pt of PM.north]{PPO with KL penalty};
%%arrows
\coordinate(SR1)at($(PERSON1.east)!0.5!(GEAR1.west)$);
\node[Larrow]at(SR1){};
\coordinate(SR2)at($(BM.east)!0.5!(GRAPH1.west)$);
\node[Larrow,minimum height=21mm](AR2)at(SR2){};
\node[above=1pt of AR2]{\small Generates};
\coordinate(SR3)at($(GEAR1.south)!0.35!(BM.north)$);
\node[Larrow,minimum height=8mm,rotate=270](AR3)at(SR3){};
\draw[LineA](BM)|-(PM);
\draw[LineA](PM)--(RLFT);
\draw[LineA](GRAPH1.130)|-node[left,pos=0.3,text=black]{\small Labels}(TRM);
\draw[LineA](TRM)-|(C1);
\draw[LineA](C1)-|node[above,pos=0.3,text=black]{\small Scores}(RLFT);
\coordinate(SR4)at($(RLFT.east)!0.5!(C2.west)$);
\node[Larrow,minimum height=11mm]at(SR4){};
%fitting
\scoped[on background layer]
\node[draw=BackLine,fill=BackColor!70, inner ysep=2mm, inner xsep=2mm,
fit=(PERSON1)(GEAR1)(BM)(SFT),yshift=0mm](BB1){};
\node[align=center,above right=0.5mm of BB1.south west]{\textbf{Stage 1}};
%
\scoped[on background layer]
\node[draw=BrownLine,fill=BrownL!8, inner ysep=2mm, inner xsep=1mm,
fit=(THR)(TRM)(TRMT),yshift=1mm](BB2){};
\node[align=center,below left=0.5mm of BB2.north east]{\textbf{Stage 2}};
%
\scoped[on background layer]
\node[draw=GreenLine,fill=green!8, inner ysep=2mm, inner xsep=2mm,
fit=(PM)(C2)(TAM),yshift=0/5mm,xshift=-1mm](BB3){};
\node[align=center,above right=0.5mm of BB3.south west]{\textbf{Stage 3}};
\end{tikzpicture}
```
:::

Examining the details of @fig-rlhf-pipeline reveals substantial engineering complexity in this three-stage pipeline. Each stage requires distinct infrastructure: Stage 1 needs demonstration collection systems, Stage 2 demands ranking interfaces that present multiple outputs side-by-side, and Stage 3 requires careful hyperparameter tuning to prevent the policy from diverging too far from the original model (the KL penalty shown). The feedback loop at the bottom represents continuous iteration, with models often going through multiple rounds of RLHF, each round requiring fresh human data to prevent overfitting to the reward model.

This approach yields significant improvements: InstructGPT [@ouyang2022training] with 1.3B parameters outperforms GPT-3 with 175B parameters in human evaluations[^fn-rlhf-impact], demonstrating that alignment matters more than scale for user satisfaction. For ML engineers, this means that investing in alignment infrastructure can be more valuable than scaling compute: a 100x smaller aligned model outperforms a larger unaligned one.

[^fn-rlhf-impact]: **RLHF Effectiveness**: InstructGPT (1.3B parameters) was preferred over GPT-3 (175B parameters) in 85% of human evaluations despite being 100× smaller. RLHF training reduced harmful outputs by 90%, hallucinations by 40%, and increased user satisfaction by 72%, demonstrating that alignment matters more than scale for practical performance.

#### Constitutional AI: Value-Aligned Learning {#sec-agi-systems-constitutional-ai-valuealigned-learning-8d4c}

Human feedback remains expensive and inconsistent: different annotators provide conflicting preferences, and scaling human oversight to billions of interactions proves challenging[^fn-human-feedback-limits]. Constitutional AI [@bai2022constitutional] addresses these limitations through automated preference learning.

[^fn-human-feedback-limits]: **Human Feedback Bottlenecks**: RLHF (Reinforcement Learning from Human Feedback) faces severe scaling challenges. ChatGPT required 40 annotators working 3 months for 200K labels; GPT-4 scale would need 10,000+ annotators. Inter-annotator agreement reaches only 70-80%, injecting subjective bias. Constitutional AI and AI-assisted feedback aim to reduce human labeling dependency.

Instead of human rankings, Constitutional AI uses a set of principles (a "constitution") to guide model behavior[^fn-constitutional-approach]. The model generates responses, critiques its own outputs against these principles, and revises responses iteratively. This self-improvement loop removes the human bottleneck while maintaining alignment objectives.

[^fn-constitutional-approach]: **Constitutional AI Method**: Bai et al. [@bai2022constitutional] implementation uses 16 principles like "avoid harmful content" and "be helpful." The model performs 5 rounds of self-critique and revision. Harmful outputs reduced by approximately 90% while maintaining most original helpfulness (specific metrics vary by evaluation).

::: {#fig-constitutional-ai fig-env="figure" fig-pos="htb" fig-cap="**Constitutional AI Self-Improvement Loop**: The iterative refinement process eliminates human feedback bottlenecks. Each cycle evaluates outputs against constitutional principles, generates critiques, and produces improved versions. After 5 iterations, harmful content reduces by 95% while maintaining helpfulness. The final outputs become training data for the next model generation." fig-alt="Five-step loop: language model generates initial response, self-critique against constitutional principles, revised response, then training data. Harmful content decreases from 100% to 40% to 5% across iterations."}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
\tikzset{%
  Box/.style={align=flush center, inner xsep=2pt,
    draw=BlueLine, line width=0.75pt,node distance=1.2,
    fill=BlueL!60, text width=55mm,
    minimum width=55mm, minimum height=12mm
  },
Circ/.style = {circle,minimum size=27mm,draw=none, fill=none,node distance=1.7},
LineA/.style={violet!50,dashed, line width=1.0pt,{-{Triangle[width=1.0*6pt,length=1.6*6pt]}},shorten <=3pt,shorten >=-2pt},
ALine/.style={black!50, line width=1.1pt,{{Triangle[width=0.9*6pt,length=1.2*6pt]}-}},
Larrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,
            single arrow head indent=0pt,minimum height=13mm, minimum width=14pt},
}

\tikzset{
pics/llm/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[circle,minimum size=12mm,draw=\drawcolor, fill=\filllcolor!70,line width=0.5*\Linewidth](C\picname) at (0,0){};
\def\startangle{90}
\def\radius{1.15}
\def\radiusI{1.1}
\foreach \i [evaluate=\i as \j using \i+1] [count =\k] in {0,2,4,6,8} {
\pgfmathsetmacro{\angle}{\startangle - \i * (360/8)}
\draw[draw=black,-{Circle[black ,fill=\filllcirclecolor,length=5.5pt,line width=0.5*\Linewidth]},line width=1.5*\Linewidth](C\picname)--++(\startangle - \i*45:\radius) ;
\node[circle,draw=black,fill=\filllcirclecolor!80!red!50,inner sep=3pt,line width=0.5*\Linewidth](2C\k)at(\startangle - \j*45:\radiusI) {};
}
\draw[line width=1.5*\Linewidth](2C1)--++(-0.5,0)|-(2C2);
\draw[line width=1.5*\Linewidth](2C3)--++(0.5,0)|-(2C4);
\node[circle,minimum size=12mm,draw=\drawcolor, fill=\filllcolor!70,line width=0.5*\Linewidth]at (0,0){};
\node[draw,rectangle,rounded corners=1pt,minimum width=7mm,minimum height=4mm,fill=orange!10](R1)at(0.1,0.1){};
\draw[BrownLine,shorten <=2pt,shorten >=2pt ]($(R1.north west)!0.35!(R1.south west)$)--($(R1.north east)!0.35!(R1.south east)$);
\draw[BrownLine,shorten <=2pt,shorten >=2pt ]($(R1.north west)!0.7!(R1.south west)$)--($(R1.north east)!0.7!(R1.south east)$);
\node[draw,rectangle,rounded corners=1pt,minimum width=6mm,minimum height=4mm,fill=orange!10](R2)at(-0.05,-0.15){};
\draw[BrownLine,shorten <=2pt,shorten >=2pt ]($(R2.north west)!0.35!(R2.south west)$)--($(R2.north east)!0.35!(R2.south east)$);
\draw[BrownLine,shorten <=2pt,shorten >=2pt ]($(R2.north west)!0.7!(R2.south west)$)--($(R2.north east)!0.7!(R2.south east)$);
\end{scope}
    }
  }
}
%testing
\tikzset{
pics/testing/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=TESTING1,shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\newcommand{\tikzxmark}{%
\tikz[scale=0.18] {
    \draw[line width=0.7,line cap=round,GreenLine] (0,0) to [bend left=6] (1,1);
    \draw[line width=0.7,line cap=round,GreenLine] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}
\newcommand{\tikzxcheck}{%
\tikz[scale=0.16] {
    \draw[line width=0.7,line cap=round,GreenLine] (0.5,0.75)--(0.85,-0.1) to [bend left=16] (1.5,1.55);

}}
 \node[draw, minimum width  =15mm, minimum height = 20mm, inner sep = 0pt,
        rounded corners,draw = \drawcolor, fill=\filllcolor!10, line width=\Linewidth](COM){};
\node[draw=GreenLine,inner sep=4pt,fill=white](CB1) at ($(COM.north west)!0.25!(COM.south west)+(0.3,0)$){};
\node[draw=GreenLine,inner sep=4pt,fill=white](CB2) at ($(COM.north west)!0.5!(COM.south west)+(0.3,0)$){};
\node[draw=GreenLine,inner sep=4pt,fill=white](CB3) at ($(COM.north west)!0.75!(COM.south west)+(0.3,0)$){};
%\pgfmathsetmacro{\isCheck}{ifthenelse(\Check=="yes",1,0)}
\ifnum\Check=1
\node[xshift=0pt]at(CB1){\tikzxcheck};
\node[xshift=0pt]at(CB2){\tikzxcheck};
\node[xshift=0pt]at(CB3){\tikzxcheck};
\else
\node[xshift=0pt]at(CB1){\tikzxmark};
\node[xshift=0pt]at(CB2){\tikzxmark};
\node[xshift=0pt]at(CB3){\tikzxmark};
\fi
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,-0.12)$)--++(0:0.7);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,-0.12)$)--++(0:0.6);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,-0.12)$)--++(0:0.6);
\end{scope}
    }
  }
}
%pencil
\tikzset{
pics/pencil/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=TESTING1,shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape},rotate=340]
            \fill[fill=\filllcolor!70] (0,4) -- (0.4,4) -- (0.4,0) --(0.3,-0.15) -- (0.2,0) -- (0.1,-0.14) -- (0,0) -- cycle;
            \draw[color=white,thick] (0.2,4) -- (0.2,0);
            \fill[black] (0,3.5) -- (0.2,3.47) -- (0.4,3.5) -- (0.4,4) arc(30:150:0.23cm);
            \fill[fill=\filllcolor!40] (0,0) -- (0.2,-0.8)node[coordinate,pos=0.75](a){} -- (0.4,0)node[coordinate,pos=0.25](b){} -- (0.3,-0.15) -- (0.2,0) -- (0.1,-0.14) -- cycle;
            \fill[fill=\filllcolor] (a) -- (0.2,-0.8) -- (b) -- cycle;

\end{scope}
    }
  }
}
%brain
\tikzset{pics/brain/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=BRAIN,scale=\scalefac, every node/.append style={transform shape}]
\draw[fill=\filllcolor,line width=\Linewidth](-0.3,-0.10)to(0.08,0.60)
to[out=60,in=50,distance=3](-0.1,0.69)to[out=160,in=80](-0.26,0.59)to[out=170,in=90](-0.46,0.42)
to[out=170,in=110](-0.54,0.25)to[out=210,in=150](-0.54,0.04)
to[out=240,in=130](-0.52,-0.1)to[out=300,in=240]cycle;
\draw[fill=\filllcolor,line width=\Linewidth]
(-0.04,0.64)to[out=120,in=0](-0.1,0.69)(-0.19,0.52)to[out=120,in=330](-0.26,0.59)
(-0.4,0.33)to[out=150,in=280](-0.46,0.42)
%
(-0.44,-0.03)to[bend left=30](-0.34,-0.04)
(-0.33,0.08)to[bend left=40](-0.37,0.2) (-0.37,0.12)to[bend left=40](-0.45,0.14)
(-0.26,0.2)to[bend left=30](-0.24,0.13)
(-0.16,0.32)to[bend right=30](-0.27,0.3)to[bend right=30](-0.29,0.38)
(-0.13,0.49)to[bend left=30](-0.04,0.51);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcolor,length=7.5pt]},line width=\Linewidth](-0.23,0.03)--(-0.15,-0.03)--(-0.19,-0.18)--(-0.04,-0.28);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcolor,length=7.5pt]},line width=\Linewidth](-0.17,0.13)--(-0.04,0.05)--(-0.06,-0.06)--(0.14,-0.11);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcolor,length=7.5pt]},line width=\Linewidth](-0.12,0.23)--(0.31,0.0);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcolor,length=7.5pt]},line width=\Linewidth](-0.07,0.32)--(0.06,0.26)--(0.16,0.33)--(0.34,0.2);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcolor,length=7.5pt]},line width=\Linewidth](-0.01,0.43)--(0.06,0.39)--(0.18,0.51)--(0.31,0.4);
\coordinate(PO)at(-0.1,0.2);
\node[circle,draw=white,line width=1pt,fill=\filllcirclecolor,minimum size=5mm,inner sep=0pt](LV)at(PO){};
\node[draw=none,rotate=40,rounded corners=3pt,rectangle,minimum width=1.2mm,inner sep=1pt,
fill=\filllcirclecolor,minimum height=6mm,anchor=north]at(PO){};
\node[circle,draw=none,fill=white,minimum size=3.0mm,inner sep=0pt](LM)at(PO){};
\node[font=\tiny\bfseries]at(LM){...};
\end{scope}
     }
  }
}
%books
\tikzset{
pics/books/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\draw[draw=\drawcolor,line width=\Linewidth](1.23,-0.42)--(0.32,-1.23)--(-0.97,-1.1)
to[out=170,in=200,distance=5](-1.0,-0.71)to(0.32,-0.83)to(1.23,-0.03);
\draw[draw=\drawcolor,,line width=\Linewidth](1.23,0.36)--(0.32,-0.44)--(-0.97,-0.29)
to[out=170,in=200,distance=5](-1.0,0.1)to(0.32,-0.07)to(1.3,0.76);
\draw[draw=\drawcolor,,line width=2.5pt](-1.0,-0.69)to[out=170,in=190,distance=5](-0.97,-0.3);
\draw[draw=\drawcolor,fill=\filllcolor](0.02,0.9)--(1.34,0.8)--(0.32,-0.07)--(-1.06,0.1)--cycle;
\draw[draw=none,line width=1pt,fill=white](0.04,0.65)to(0.7,0.58)to(0.50,0.42)to(-0.17,0.49)to cycle;
\end{scope}
    }
  }
}
\pgfkeys{
  /channel/.cd,
   Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  Check/.store in=\Check,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawcolor/.store in=\drawcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  filllcolor=BrownLine,
  filllcirclecolor=violet!20,
  drawcolor=black,
  drawcircle=violet,
  Check=1,
  scalefac=1,
  Linewidth=0.5pt,
  Depth=1.3,
  Height=0.8,
  Width=1.1,
  picname=C
}
%Language Model
\node[Circ](C1){};
\pic[shift={(0,0)}] at  (C1){llm={scalefac=1,drawcolor=OrangeLine,filllcolor=OrangeLine!50!, Linewidth=1pt,filllcirclecolor=green}};
\node[below=0pt of C1]{Language Model};
%Initial Response
\node[Circ,right=of C1](C2){};
\pic[shift={(0,-0.0)}] at  (C2){testing={scalefac=0.85,picname=1,Check=0,drawcolor=OrangeLine,filllcolor=OrangeLine, Linewidth=1.0pt}};
\pic[shift={(0,-0.5)},rotate=-15] at  (C2){pencil={scalefac=0.35,picname=1,filllcolor=RedLine, Linewidth=1.0pt}};
\node[below=0pt of C2](IR){Initial Response};
%Self Critique
\node[Circ,right=of C2](C3){};
\pic[shift={(0.2,-0.37)}] at  (C3){brain={scalefac=2,picname=1,filllcolor=orange!30!, filllcirclecolor=cyan!55!black!60, Linewidth=1.5pt}};
\node[below=0pt of C3]{Self Critique};
%Revised Response
\node[Circ,right=of C3](C4){};
\pic[shift={(0,-0.0)}] at  (C4){testing={scalefac=0.85,picname=1,Check=1,drawcolor=green!55!black,filllcolor=green!55!black, Linewidth=1.0pt}};
\pic[shift={(0,-0.5)},rotate=-15] at  (C4){pencil={scalefac=0.35,picname=1,filllcolor=BlueLine, Linewidth=1.0pt}};
\node[below=0pt of C4](RR){Revised Response};
%Training Data
\node[Circ,right=of C4](C5){};
\pic[shift={(0,0.1)}] at  (C5){books={scalefac=0.7,picname=1,drawcolor=BlueLine,filllcolor=BlueLine,Linewidth=2.0pt}};
\node[below=0pt of C5]{Training Data};
%arrows

\foreach \i/\tx [evaluate=\i as \j using int(\i+1)] in {1/Generate,2/Evaluate,3/Improve,4/}{%
\coordinate(SR\i) at($(C\i.east)!0.5!(C\j.west)$);
\node[Larrow](AR\i)at(SR\i){};
\node[above=2pt of AR\i,font=\small\usefont{T1}{phv}{m}{n}]{\tx};
}
\node[Box,above=0.4 of C2](B1){Constitutional Principles\\ "Be helpful, harmless, honest"};
\draw[LineA](B1)-|node[above,pos=0.25,text=black]{Against}(C3);
\draw[LineA](RR.south) to[bend left=18] node[above,violet]{Iterate 5$\times$}(IR);
%
\node[below=16mm of C2, align=center,BrownLine] {\small Harmful: 100\%};
\node[below=16mm of C3, align=center,BrownLine] {\small Harmful: 40\%};
\node[below=16mm of C4, align=center,BrownLine] {\small Harmful: 5\%};
\end{tikzpicture}
```
:::

@fig-constitutional-ai captures this principled self-refinement loop where the model iteratively critiques and improves its own outputs against constitutional principles. The approach uses model efficiency techniques by having the model distill its own knowledge, similar to knowledge distillation but guided by constitutional objectives rather than teacher models.

#### Continual Learning: Lifelong Adaptation {#sec-agi-systems-continual-learning-lifelong-adaptation-7aee}

Deployed models face a limitation: they cannot learn from user interactions without retraining. Each conversation provides valuable feedback (corrections, clarifications, new information) but models remain frozen after training[^fn-deployment-freeze]. This creates an ever-widening gap between training data and current reality.

[^fn-deployment-freeze]: **Static Model Problem**: Deployed models remain frozen at training cutoff dates—GPT-3 eternally believes it's 2021. Models cannot learn user preferences, correct mistakes, or incorporate news without multi-million-dollar retraining. Retrieval-augmented generation (RAG) and continuous learning research address this limitation, though full solutions remain elusive.

Continual learning aims to update models from ongoing interactions while preventing catastrophic forgetting: the phenomenon where learning new information erases previous knowledge[^fn-catastrophic]. Standard gradient descent overwrites parameters without discrimination, destroying prior learning.

[^fn-catastrophic]: **Catastrophic Forgetting**: Neural networks typically lose 20-80% accuracy on previous tasks when learning new ones. In language models, fine-tuning on specialized domains degrades general conversation ability by 30-50%. Solutions like Elastic Weight Consolidation (EWC) protect important parameters by identifying which weights were critical for previous tasks and penalizing changes to them.

Solutions require memory management inspired by @sec-edge-intelligence that protect important knowledge while enabling new learning. Elastic Weight Consolidation (EWC) [@kirkpatrick2017overcoming] addresses this by identifying which neural network parameters were critical for previous tasks, then penalizing changes to those specific weights when learning new tasks. The technique computes the Fisher Information Matrix[^fn-fisher] to measure parameter importance.

[^fn-fisher]: **Fisher Information Matrix**: Statistical measure quantifying how much information a parameter carries about model predictions. For neural networks, the Fisher diagonal approximation F_i = E[(dL/dθ_i)^2] measures each parameter's contribution to performance. Parameters with high Fisher information are critical for learned capabilities; modifying them risks catastrophic forgetting. Computing exact Fisher information scales as O(n^2) for n parameters, so EWC uses diagonal approximations computed from gradient samples during training.

Parameters with high Fisher information contributed significantly to previous performance and should be preserved. Progressive Neural Networks take a different approach by adding entirely new pathways for new knowledge while freezing original pathways, ensuring previous capabilities remain intact. Memory replay techniques periodically rehearse examples from previous tasks during new training, maintaining performance through continued practice rather than architectural constraints.

These training innovations (alignment through human feedback, principled self-improvement, and continual adaptation) transform traditional training paradigms into dynamic learning systems that improve through deployment rather than remaining static after training.

### Production Infrastructure for Large-Scale Systems {#sec-emerging-systems-challenges-production-infrastructure-largescale-systems-489b}

The preceding subsections examined challenges for frontier systems: data engineering at scale, dynamic architectures, and training paradigms for compound systems. Three additional building blocks (optimization, hardware, and operations) prove equally critical. Rather than requiring entirely new techniques, these domains apply and extend the comprehensive frameworks developed in earlier chapters.

This section briefly surveys how model efficiency techniques (including quantization, pruning, and distillation), hardware acceleration through specialized processors, and operational practices evolve for large-scale systems. The key insight: while scale and coordination challenges intensify substantially, the underlying engineering principles remain consistent with established ML systems practices.

#### Optimization: Dynamic Computation Allocation {#sec-emerging-systems-challenges-optimization-dynamic-computation-allocation-8e7b}

Model efficiency techniques evolve from static compression to dynamic computation allocation across compound system components. Current models waste computation by activating all parameters for every input. When GPT-4 answers "2+2=4", it activates the same trillion parameters used for complex reasoning, like using a supercomputer for basic arithmetic. Efficient systems require selective activation based on input complexity.

Mixture-of-experts architectures demonstrate one approach to sparse and adaptive computation: routing inputs through relevant subsets of model capacity. Extending this principle, adaptive computation allocates computational time dynamically based on problem difficulty, spending seconds on simple queries but extensive resources on complex reasoning tasks. This requires systems engineering for real-time difficulty assessment and graceful scaling across computational budgets.

Rather than building monolithic models, systems can employ distillation cascades where large frontier models teach progressively smaller, specialized variants. This mirrors human organizations: junior staff handle routine work while senior experts tackle complex problems. Knowledge distillation techniques enable creating model families that maintain capabilities while reducing computational requirements for common tasks. The systems engineering challenge involves orchestrating these hierarchies and routing problems to appropriate computational levels.

Model efficiency principles (pruning, quantization, distillation) remain foundational; large-scale systems apply them dynamically across compound architectures rather than statically to individual models.

#### Hardware: Scaling as Moore's Law Slows {#sec-emerging-systems-challenges-hardware-scaling-moores-law-slows-074e}

Hardware acceleration through specialized processors provides foundations, but large-scale requirements face post-Moore's Law constraints as traditional silicon scaling [@koomey2011web] slows from approximately 30-50% annual transistor density improvements (1970-2010) to roughly 10-20% annually (2010-2025)[^fn-moores-end].

[^fn-moores-end]: **Moore's Law Slowdown**: Transistor scaling slowed dramatically as physical limits emerge: quantum tunneling at 3-5nm nodes, fab costs exceeding $20B, and power density approaching limits of air cooling. TSMC's 2nm (2025) may represent near-final conventional scaling. This drives exploration of 3D stacking, chiplets, and alternative computing paradigms for continued AI performance growth.

Training GPT-4 class models already requires the extensive parallelism covered in @sec-distributed-training-systems, coordinating thousands of GPUs through tensor, pipeline, and data parallelism techniques. Scaling further requires architectural innovations across multiple fronts.

3D chip stacking and chiplets build density through vertical integration and modular composition rather than horizontal shrinking. Samsung's 176-layer 3D NAND and AMD's multi-chiplet EPYC processors demonstrate feasibility[^fn-3d-chiplet]. For AGI, this enables mixing specialized processors (matrix units, memory controllers, networking chips) in optimal ratios while managing thermal challenges through advanced cooling.

[^fn-3d-chiplet]: **3D Stacking and Chiplets**: Vertical integration achieves 100× higher interconnect density than planar designs, but generates 1000 W/cm² heat flux requiring advanced cooling (liquid, microfluidic). AMD's MI300X uses 3D chiplets with 153B transistors. Chiplet architectures mix specialized dies (compute, memory, I/O) while improving yields from 30% to 90%+ vs. monolithic designs.

Communication and memory bottlenecks require novel solutions through optical interconnects and processing-in-memory architectures. Silicon photonics enables 100 Tbps bandwidth with 10× lower energy than electrical interconnects, critical when coordinating 100,000+ processors[^fn-optical-pim]. Processing-in-memory reduces data movement energy by 100× by computing directly where data resides, addressing the memory wall limiting current accelerator efficiency.

[^fn-optical-pim]: **Communication and Memory Innovations**: Optical interconnects achieve 100× bandwidth/energy improvement over electrical for chip-to-chip communication, essential as processor arrays scale. Processing-in-memory (Samsung's HBM-PIM) eliminates data movement—responsible for 90% of energy in memory-bound workloads—by computing directly in memory arrays where parameters reside.

Longer-term pathways emerge through neuromorphic and quantum-hybrid systems. Intel's Loihi [@davies2018loihi] and IBM's TrueNorth demonstrate 1000× energy efficiency for event-driven workloads through brain-inspired architectures. Quantum-classical hybrids could accelerate combinatorial optimization (neural architecture search, hyperparameter tuning) while classical systems handle gradient computation[^fn-neuromorphic-quantum]. Programming these heterogeneous systems requires sophisticated middleware to decompose AGI workflows across different computational paradigms.

[^fn-neuromorphic-quantum]: **Alternative Computing Paradigms**: Neuromorphic chips achieve 1000× energy efficiency for sparse, event-driven workloads but require new programming models. Quantum processors show advantages for specific optimization tasks (IBM's 1000+ qubit systems, Google's Sycamore), though hybrid quantum-classical systems face orchestration challenges due to vastly different computational timescales.

Hardware acceleration principles (parallelism, memory hierarchy optimization, specialized compute units) remain foundational. AGI systems extend these through post-Moore's Law innovations while requiring unprecedented orchestration across heterogeneous architectures.

#### Post-Silicon Frontiers: Biological and DNA Computing {#sec-emerging-systems-challenges-postsilicon-frontiers-biological-dna-computing-6615}

As we approach the atomic limits of silicon, the sustainability crisis established in @sec-sustainable-ai forces us to look beyond traditional semiconductors. The 1,000,000$\times$ energy efficiency gap between the human brain and current GPUs suggests that silicon may be an inherently suboptimal substrate for general intelligence.

- **Biological Computing**: Emerging research explores "Organoid Intelligence"—using living neurons integrated with silicon interfaces to perform recognition and learning tasks. These systems operate at the milliwatt scale, potentially offering a path to AGI that bypasses the "Power Wall" of modern datacenters.
- **DNA Computing**: To solve the "Memory Wall," researchers are investigating DNA as a storage and computational substrate. With an information density of 215 petabytes per gram, a single kilogram of DNA could store the entire training corpus of a sentient-level AGI. When combined with molecular-scale parallel processing, DNA computing could enable "Knowledge Tiers" that are billions of times denser than HBM3 or High Bandwidth Flash (HBF).

These biological frontiers represent the ultimate convergence of the Machine Learning Fleet: where the "Logic" of the algorithm finally merges with the "Physics" of life itself.

#### Operations: Continuous System Evolution {#sec-agi-systems-operations-continuous-system-evolution-ed9b}

Operational practices for ML systems become critical as AGI systems evolve from static models to dynamic, continuously learning entities. Three operational challenges intensify at AGI scale and transform how we think about model deployment and maintenance.

Continuous learning systems update from user interactions in real-time while maintaining safety and reliability. This transforms operations from discrete deployments (v1.0, v1.1, v2.0) to continuous evolution where models change constantly. Traditional version control, rollback strategies, and reproducibility guarantees require rethinking. The operational infrastructure must support live model updates without service interruption while maintaining safety invariants, a challenge absent in static model deployment.

Testing and validation grow complex when comparing personalized model variants across millions of users. Traditional A/B testing assumes consistent experiences per variant; AGI systems introduce complications where each user may receive a slightly different model. Emergent behaviors can appear suddenly as capabilities scale, requiring detection of subtle performance regressions across diverse use cases. Monitoring and observability principles provide foundations but must extend to detect capability changes rather than just performance metrics.

Safety monitoring demands real-time detection of harmful outputs, prompt injections, and adversarial attacks across billions of interactions. Unlike traditional software monitoring tracking system metrics (latency, throughput, error rates), AI safety monitoring requires understanding semantic content, user intent, and potential harm. This necessitates new tooling combining the robustness principles from @sec-robust-ai, security practices from @sec-security-privacy, and responsible AI frameworks from @sec-responsible-ai. The operational challenge involves deploying these safety systems at scale while maintaining sub-second response times.

Operational practices (CI/CD, monitoring, incident response) remain essential; AGI systems simply apply them to continuously evolving, personalized models requiring semantic rather than purely metric-based validation.

### Integrated System Architecture Design {#sec-agi-systems-integrated-system-architecture-design-d490}

The six building blocks examined (data engineering, dynamic architectures, training paradigms, optimization, hardware, and operations) must work in concert for compound AI systems, but integration proves far more challenging than simply assembling components. Successful architectures require carefully designed interfaces, coordinated optimization across layers, and holistic understanding of how building blocks interact to create emergent capabilities or cascade failures.

Consider data flow through an integrated compound system serving a complex user query. Novel data engineering pipelines from @sec-agi-systems-data-engineering-scale-91a0 continuously generate synthetic training examples, curate web-scale corpora, and enable self-play learning that produce specialized training datasets for different components. These datasets feed into dynamic architectures from @sec-agi-systems-dynamic-architectures-compound-systems-fca0 where mixture-of-experts models route different aspects of queries to specialized components: mathematical reasoning to quantitative experts, creative writing to language specialists, code generation to programming-focused modules. Each expert was trained using methodologies from @sec-agi-systems-training-methodologies-compound-systems-e3fa including RLHF alignment, constitutional AI self-improvement, and continual learning that adapts to user feedback. Optimization techniques from @sec-model-compression enable deploying these components efficiently through quantization reducing memory footprints, pruning eliminating redundant parameters, and distillation transferring knowledge to smaller deployment models. This optimized model ensemble runs on heterogeneous hardware from @sec-frontiers-emerging-hardware combining GPU clusters for transformer inference, neuromorphic chips for event-driven perception, and specialized accelerators for symbolic reasoning. Finally, evolved MLOps from @sec-agi-systems-operations-continuous-system-evolution-ed9b monitors this complex deployment through semantic validation, handles component failures gracefully, and supports continuous learning updates without service interruption.

The critical insight: these building blocks cannot be developed in isolation. Data engineering decisions constrain which architectural patterns prove feasible; model architectures determine optimization opportunities; hardware capabilities bound achievable performance; operational requirements feed back to influence architectural choices. This creates a tightly coupled design space where co-optimization across building blocks often yields greater improvements than optimizing any single component.

Concretely, three integration patterns emerged from production compound systems, each representing different trade-offs in the building block design space. The horizontal integration pattern distributes specialized components across a shared infrastructure layer. All components access common data pipelines, deploy on homogeneous hardware clusters, and integrate through standardized APIs. This pattern maximizes resource sharing and operational simplicity but limits per-component optimization. Google's Gemini exemplifies this approach: multimodal encoders, reasoning modules, and tool integrations all run on TPU clusters, sharing training infrastructure and deployment frameworks. The advantage lies in operational efficiency: one team manages the infrastructure serving all components. The limitation manifests when component-specific optimizations (neuromorphic hardware for vision, symbolic accelerators for logic) cannot be used within the homogeneous substrate.

The vertical integration pattern customizes the entire stack for each specialized component. A reasoning component might train on synthetic data from formal logic generators, use energy-based architectures optimized for constraint satisfaction, deploy on quantum-classical hybrid hardware accelerating combinatorial search, and include custom verification in its operational monitoring. A separate vision component trains on self-supervised video prediction, uses convolutional or vision transformer architectures, deploys on neuromorphic chips for efficient event processing, and monitors for distribution shift in visual inputs. This pattern enables maximal per-component optimization at the cost of operational complexity managing heterogeneous systems. Meta's approach with different specialized models for different modalities and tasks exemplifies vertical integration: each capability area receives custom treatment across the entire stack.

The hierarchical integration pattern combines horizontal and vertical approaches through layered abstraction. Lower layers provide shared infrastructure (data pipelines, training clusters, deployment platforms) while higher layers enable component-specific customization (architectural choices, optimization strategies, operational policies). Foundation model providers exemplify this: they offer base models trained on massive infrastructure (horizontal), which developers fine-tune with custom data and optimization (vertical), deployed on shared serving infrastructure (horizontal), with custom monitoring and guardrails (vertical). This pattern balances operational efficiency with optimization flexibility but introduces complexity at abstraction boundaries where the shared infrastructure must accommodate diverse customization needs.

Choosing among these patterns requires understanding system requirements and organizational capabilities. Horizontal integration suits organizations with strong infrastructure teams but limited AI specialization, accepting some performance sacrifice for operational simplicity. Vertical integration benefits organizations with deep AI expertise across multiple domains, able to manage complexity for maximal performance. Hierarchical integration serves platforms supporting diverse use cases, providing standard infrastructure while enabling customization.

The engineering challenge intensifies with scale. A research prototype might manually integrate building blocks through ad-hoc scripts and configuration files. Production systems serving millions of users require robust integration frameworks: declarative specifications defining how components interact, automated deployment pipelines validating cross-building-block consistency, monitoring systems detecting integration failures, and update mechanisms coordinating changes across building blocks without breaking dependencies. These frameworks themselves become substantial engineering artifacts, often rivaling individual building blocks in complexity.

Critically, the engineering principles developed throughout this textbook provide foundations for all six building blocks. AGI development extends rather than replaces these principles, applying them at unprecedented scale and coordination complexity. Data engineering principles scale to petabyte corpora. Distributed training techniques coordinate million-GPU clusters. Optimization methods including quantization, pruning, and distillation enable trillion-parameter deployment. Operational practices ensure reliable compound system operation. AGI systems engineering builds incrementally upon these foundations rather than requiring revolutionary new approaches, though the scale and coordination demands push existing techniques to their limits and sometimes beyond.

## Production Deployment of Compound AI Systems {#sec-agi-systems-production-deployment-compound-ai-systems-02aa}

Moving from training paradigms to production deployment introduces a different set of engineering challenges. While training focuses on learning the right behaviors, deployment demands making those behaviors reliable, fast, and observable at scale. The metrics shift from loss curves to latency percentiles, from accuracy to availability, from capability to operational cost.

The preceding sections established the building blocks required for compound AI systems: novel data sources and training paradigms, architectural alternatives addressing transformer limitations, and infrastructure supporting heterogeneous components. These building blocks provide the raw materials for AGI development. This section examines how to assemble these materials into functioning systems through orchestration patterns that coordinate specialized components at production scale.

The compound AI systems framework provides the conceptual foundation, but implementing these systems at scale requires sophisticated orchestration infrastructure. Production systems like GPT-4 [@openai2023gpt4] tool integration, Gemini [@team2023gemini] search augmentation, and Claude's constitutional AI [@bai2022constitutional] implementation demonstrate how specialized components coordinate to achieve capabilities beyond individual model limits. The engineering complexity involves managing component interactions, handling failures gracefully, and maintaining system coherence as components evolve independently. Understanding these implementation patterns bridges the gap between conceptual frameworks and operational reality.

The engineering complexity becomes concrete with specific performance metrics (@fig-compound-ai-system): the central orchestrator routes user queries to appropriate specialized modules within 10-50&nbsp;ms decision latency, manages bidirectional communication between components through 1-10 GB/s data flows depending on modality (text: 1 MB/s, code: 10 MB/s, multimodal: 1 GB/s), coordinates iterative refinement processes with 100-500&nbsp;ms round-trip times per component, and maintains conversation state across the entire interaction using 1-100 GB memory per session. Each component represents distinct engineering challenges requiring different optimization strategies (LLM: GPU-optimized inference, Search: distributed indexing, Code: secure sandboxing), hardware configurations (orchestrator: CPU+memory, retrieval: SSD+bandwidth, compute: GPU clusters), and operational practices (sub-second latency SLAs, 99.9% availability, failure isolation). Failure modes include component timeouts (10-30 second fallbacks), dependency failures (graceful degradation), and coordination deadlocks (circuit breaker patterns).

::: {#fig-compound-ai-system fig-env="figure" fig-pos="htb" fig-cap="**Compound AI System Architecture**: Modern AI assistants integrate specialized components through a central orchestrator, enabling capabilities beyond monolithic models. Each module handles specific tasks while the LLM coordinates information flow, decisions, and responses. This architecture enables independent scaling, specialized optimization, and multi-layer safety validation." fig-alt="Radial architecture with central LLM orchestrator connected to 8 modules: web search, knowledge retrieval, response generation, context memory, safety filters, external tools, user interface, and code interpreter."}
```{.tikz}
\scalebox{0.6}{%
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
planet/.style = {circle, draw=none,semithick, fill=white,line width=1.5pt,
                    font=\usefont{T1}{phv}{m}{n}\bfseries,
                    minimum size=25mm, inner sep=1mm,align=flush center},
satellite/.style = {circle, draw=none, semithick, fill=#1!15, %white,%
                    text width=20mm, inner sep=1pt, align=flush center,minimum size=20mm},
arr/.style = {-{Triangle[length=3mm,width=6mm]}, color=#1!50,
                    line width=3mm, shorten <=1mm, shorten >=1mm},
TxtC/.style = {font=\footnotesize\usefont{T1}{phv}{m}{n},text width=30mm,align=flush center},
Line/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},
LineA/.style={violet!30, line width=1.0pt,{-{Triangle[width=1.0*6pt,length=1.6*6pt]}},shorten <=3pt,shorten >=2pt},
}

\tikzset{
pics/web/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[circle,minimum size=25mm,draw=\drawcolor, fill=\filllcolor!70,line width=\Linewidth](C\picname) at (0,0){};
\draw[draw=\drawcolor,line width=\Linewidth](C\picname.north)to[bend left=65](C\picname.south);
\draw[draw=\drawcolor,line width=\Linewidth](C\picname.north)to[bend right=65](C\picname.south);
\draw[draw=\drawcolor,line width=\Linewidth](C\picname.north)to(C\picname.south);
\draw[draw=\drawcolor,line width=\Linewidth](C\picname.west)--(C\picname.east);
%
\draw[draw=\drawcolor,line width=\Linewidth](C\picname.130)to[bend right=35](C\picname.50);
\draw[draw=\drawcolor,line width=\Linewidth](C\picname.230)to[bend left=35](C\picname.310);
\node[circle,draw=white,line width=1pt,fill=\filllcirclecolor,minimum size=12mm](LV)at(0,-0.2){};
\node[draw=none,rotate=40,rounded corners=2pt,rectangle,minimum width=4.0mm,
fill=\filllcirclecolor,minimum height=15mm,anchor=north]at(0,-0.2){};
\node[circle,draw=none,fill=white,minimum size=8mm](LM)at(0,-0.2){};
\node[font=\footnotesize]at(LM){$\bullet$ $\bullet$ $\bullet$};
\end{scope}
    }
  }
}
%llm
\tikzset{
pics/llm/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[circle,minimum size=13mm,draw=\drawcolor, fill=\filllcolor!70,line width=\Linewidth](C\picname) at (0,0){\large LLM};
\def\startangle{110}
\def\radius{1.55}
\def\radiusI{1.2}
\foreach \i [evaluate=\i as \j using \i+1] in {0,2,4,6,8} {
\pgfmathsetmacro{\angle}{\startangle - \i * (360/10)}
\draw[draw=\drawcolor,,line width=0.8*\Linewidth,-{Circle[\drawcolor, ,fill=\filllcirclecolor,length=9.5pt]}](C\picname)--++(\startangle - \i*36:\radius) ;
\draw[draw=\drawcolor,,line width=0.8*\Linewidth,-{Circle[\drawcolor, ,fill=\filllcirclecolor!80!red!50,length=7.5pt]}](C\picname)--++(\startangle - \j*36:\radiusI) ;
}
\end{scope}
    }
  }
}
%books
\tikzset{
pics/books/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\draw[draw=\drawcolor,line width=\Linewidth](1.23,-0.42)--(0.32,-1.23)--(-0.97,-1.1)
to[out=170,in=200,distance=5](-1.0,-0.71)to(0.32,-0.83)to(1.23,-0.03);
\draw[draw=\drawcolor,,line width=\Linewidth](1.23,0.36)--(0.32,-0.44)--(-0.97,-0.29)
to[out=170,in=200,distance=5](-1.0,0.1)to(0.32,-0.07)to(1.3,0.76);
\draw[draw=\drawcolor,,line width=2.5pt](-1.0,-0.69)to[out=170,in=190,distance=5](-0.97,-0.3);
\draw[draw=\drawcolor,fill=\filllcolor](0.02,0.9)--(1.34,0.8)--(0.32,-0.07)--(-1.06,0.1)--cycle;
\draw[draw=none,line width=1pt,fill=white](0.04,0.65)to(0.7,0.58)to(0.50,0.42)to(-0.17,0.49)to cycle;
\end{scope}
    }
  }
}
%Generated
\tikzset{
  neuron/.style={circle, draw, fill=black, minimum size=2mm, inner sep=0pt},
pics/gene/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\foreach \i in {1,...,3}
  \node[neuron,draw=\drawcolor,fill=\filllcolor] (I\i) at (0,-0.3*\i) {};
%
\foreach \i in {1,...,4}
  \node[neuron,draw=\drawcolor!70,fill=\filllcolor!70] (H\i) at (0.4,-0.3*\i+0.2) {};
\foreach \i in {1,...,2}
  \node[neuron,draw=\drawcolor!40,fill=\filllcolor!40!] (X\i) at (0.7,-0.6*\i+0.3) {};
\foreach \i in {1}
  \node[neuron,draw=\drawcolor!70,fill=\filllcolor!70] (O\i) at (1.0,-0.3*\i-0.3) {};
\foreach \i in {1,...,3}
  \foreach \j in {1,...,4}
    \draw[thin,BrownLine] (I\i) -- (H\j);
\foreach \i  [evaluate=\i as \k using \i+2] in {1,...,2}{
  \foreach \j  [evaluate=\j as \m using \j+1] in {1}{
  \draw[thin,BrownLine] (H\i) -- (X1);
  \draw[thin,BrownLine] (H\k) -- (X2);
  }}
\foreach \i in {1,2}
  \foreach \j in {1}
    \draw[thin,BrownLine] (X\i) -- (O\j);
\end{scope}
    }
  }
}
\tikzset{
pics/square/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=SQUARE,scale=\scalefac,every node/.append style={transform shape}]
% Right Face
\draw[fill=\filllcolor!70,line width=\Linewidth]
(\Depth,0,0)coordinate(\picname-ZDD)--(\Depth,\Width,0)--(\Depth,\Width,\Height)--(\Depth,0,\Height)--cycle;
% Front Face
\draw[fill=\filllcolor!40,line width=\Linewidth]
(0,0,\Height)coordinate(\picname-DL)--(0,\Width,\Height)coordinate(\picname-GL)--
(\Depth,\Width,\Height)coordinate(\picname-GD)--(\Depth,0,\Height)coordinate(\picname-DD)--(0,0,\Height);
% Top Face
\draw[fill=\filllcolor!20,line width=\Linewidth]
(0,\Width,0)coordinate(\picname-ZGL)--(0,\Width,\Height)coordinate(\picname-ZGL)--
(\Depth,\Width,\Height)--(\Depth,\Width,0)coordinate(\picname-ZGD)--cycle;
%dots front
\node[circle,draw=none, minimum size=1.75mm,inner sep=0pt,fill=\filllcirclecolor!60!black]at($(\picname-GL)!0.5!(\picname-DD)$){};
 \node[circle,draw=none, minimum size=1.75mm,inner sep=0pt,fill=\filllcirclecolor!60!black]at($(\picname-GL)!0.22!(\picname-DD)$){};
\node[circle,draw=none, minimum size=1.75mm,inner sep=0pt,fill=\filllcirclecolor!60!black]at($(\picname-GL)!0.78!(\picname-DD)$){};
\node[circle,draw=none, minimum size=1.75mm,inner sep=0pt,fill=\filllcirclecolor!60!black]at($(\picname-GD)!0.78!(\picname-DL)$){};
\node[circle,draw=none, minimum size=1.75mm,inner sep=0pt,fill=\filllcirclecolor!60!black]at($(\picname-GD)!0.22!(\picname-DL)$){};
%dots up
\node[ellipse,draw=none, minimum width=2mm,minimum height=1mm,inner sep=0pt,fill=\filllcirclecolor!60!black]
at($(\picname-GL)!0.5!(\picname-ZGD)$){};
%dots right
\node[ellipse,draw=none, minimum width=1mm,minimum height=1.75mm,inner sep=0pt,fill=\filllcirclecolor!60!black]
at($(\picname-GD)!0.3!(\picname-ZDD)$){};
\node[ellipse,draw=none, minimum width=1mm,minimum height=1.75mm,inner sep=0pt,fill=\filllcirclecolor!60!black]
at($(\picname-GD)!0.7!(\picname-ZDD)$){};
\end{scope}
    }
  }
}
%brain
\tikzset{pics/brain/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=BRAIN,scale=\scalefac, every node/.append style={transform shape}]
\fill[fill=\filllcolor!50](0.1,-0.5)to[out=0,in=180](0.33,-0.5)
to[out=0,in=270](0.45,-0.38)to(0.45,-0.18)
to[out=40,in=240](0.57,-0.13)to[out=110,in=310](0.52,-0.05)
to[out=130,in=290](0.44,0.15)to[out=90,in=340,distance=8](0.08,0.69)
to[out=160,in=80](-0.42,-0.15)to (-0.48,-0.7)to(0.07,-0.7)to(0.1,-0.5)
(-0.10,-0.42)to[out=310,in=180](0.1,-0.5);
\draw[draw=\drawcolor,line width=\Linewidth](0.1,-0.5)to[out=0,in=180](0.33,-0.5)
to[out=0,in=270](0.45,-0.38)to(0.45,-0.18)
to[out=40,in=240](0.57,-0.13)to[out=110,in=310](0.52,-0.05)
to[out=130,in=290](0.44,0.15)to[out=90,in=340,distance=8](0.08,0.69)
(-0.42,-0.15)to (-0.48,-0.7)
(0.07,-0.7)to(0.1,-0.5)
(-0.10,-0.42)to[out=310,in=180](0.1,-0.5);
\draw[fill=\filllcolor,line width=\Linewidth](-0.3,-0.10)to(0.08,0.60)
to[out=60,in=50,distance=3](-0.1,0.69)to[out=160,in=80](-0.26,0.59)to[out=170,in=90](-0.46,0.42)
to[out=170,in=110](-0.54,0.25)to[out=210,in=150](-0.54,0.04)
to[out=240,in=130](-0.52,-0.1)to[out=300,in=240]cycle;
\draw[fill=\filllcolor,line width=\Linewidth]
(-0.04,0.64)to[out=120,in=0](-0.1,0.69)(-0.19,0.52)to[out=120,in=330](-0.26,0.59)
(-0.4,0.33)to[out=150,in=280](-0.46,0.42)
%
(-0.44,-0.03)to[bend left=30](-0.34,-0.04)
(-0.33,0.08)to[bend left=40](-0.37,0.2) (-0.37,0.12)to[bend left=40](-0.45,0.14)
(-0.26,0.2)to[bend left=30](-0.24,0.13)
(-0.16,0.32)to[bend right=30](-0.27,0.3)to[bend right=30](-0.29,0.38)
(-0.13,0.49)to[bend left=30](-0.04,0.51);

\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.23,0.03)--(-0.15,-0.03)--(-0.19,-0.18)--(-0.04,-0.28);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.17,0.13)--(-0.04,0.05)--(-0.06,-0.06)--(0.14,-0.11);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.12,0.23)--(0.31,0.0);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.07,0.32)--(0.06,0.26)--(0.16,0.33)--(0.34,0.2);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.01,0.43)--(0.06,0.39)--(0.18,0.51)--(0.31,0.4);
\end{scope}
     }
  }
}
%
\def\inset{3.2pt} %
\def\myshape{%
  (0,1.34) to[out=220,in=0] (-1.20,1.03) --
  (-1.20,-0.23) to[out=280,in=160] (0,-1.53) to[out=20,in=260] (1.20,-0.23) --
  (1.20,1.03)  to[out=180,in=320] cycle
}
\tikzset{
pics/stit/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\fill[fill=\filllcolor!60] \myshape;
%
\begin{scope}
  \clip \myshape;
  \draw[draw=\filllcolor!60, line width=2*\inset,fill=white] \myshape; % boja i debljina po želji
\end{scope}
\fill[fill=\filllcolor!60](0,0)circle(0.4);
\end{scope}
    }
  }
}
%gear
% #1 number of teeth
% #2 radius intern
% #3 radius extern
% #4 angle from start to end of the first arc
% #5 angle to decale the second arc from the first
% #6 inner radius to cut off
\tikzset{
  pics/gear/.style args={#1/#2/#3/#4/#5/#6/#7}{
   code={
           \pgfkeys{/channel/.cd, #7}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
    \pgfmathtruncatemacro{\N}{#1}%
    \def\rin{#2}\def\rout{#3}\def\aA{#4}\def\aOff{#5}\def\rcut{#6}%
    \path[rounded corners=1.5pt,draw=\drawcolor,fill=\filllcolor]
      (0:\rin)
      \foreach \i [evaluate=\i as \n using (\i-1)*360/\N] in {1,...,\N}{%
        arc (\n:\n+\aA:\rin)
        -- (\n+\aA+\aOff:\rout)
        arc (\n+\aA+\aOff:\n+360/\N-\aOff:\rout)
        -- (\n+360/\N:\rin)
      } -- cycle;
      \draw[draw=none,fill=white](0,0) circle[radius=\rcut];
\end{scope}
  }}
}
%code
\tikzset{
pics/interpreter/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[black,font=\Large\bfseries]at(-0.75,0.65){\textless\,/\,\textgreater};
\draw[line cap=round,line join=round,green!99!black!90,line width=\Linewidth](-1.32,0.17)--(-1.05,0.17);
\draw[line cap=round,line join=round,red,line width=\Linewidth](-0.8,0.17)--(-0.1,0.17);
\draw[line cap=round,line join=round,green!99!black!90,line width=\Linewidth](-1.15,-0.15)--(-0.45,-0.15);
\draw[line cap=round,line join=round,green!99!black!90,line width=\Linewidth](-1.15,-0.47)--(-0.75,-0.47);
\draw[line cap=round,line join=round,red,line width=\Linewidth](-0.45,-0.47)--(0.45,-0.47);
\draw[line cap=round,line join=round,cyan,line width=\Linewidth](0.75,-0.47)--(1.1,-0.47);
\draw[line cap=round,line join=round,green!99!black!90,line width=\Linewidth](-1.15,-0.79)--(-1,-0.79);
\draw[line cap=round,line join=round,red,line width=\Linewidth](-0.65,-0.79)--(-0.10,-0.79);
\draw[line cap=round,line join=round,cyan,line width=\Linewidth](0.2,-0.79)--(1.1,-0.79);
\draw[line cap=round,line join=round,green!99!black!90,line width=\Linewidth](-1.15,-1.11)--(-0.4,-1.11);
\draw[line cap=round,line join=round,blue!99!black!90,line width=\Linewidth](-0.15,-1.11)--(1.1,-1.11);
\end{scope}
    }
  }
}
\pgfkeys{
  /channel/.cd,
   Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawcolor/.store in=\drawcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  filllcolor=BrownLine,
  filllcirclecolor=violet!20,
  drawcolor=black,
  drawcircle=violet,
  scalefac=1,
  Linewidth=0.5pt,
  Depth=1.3,
  Height=0.8,
  Width=1.1,
  picname=C
}
\tikzset{
  man/.pic={
  \pgfkeys{/man/.cd, #1}
     % tie
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
    \draw[draw=\tiecolor,fill=\tiecolor] (0.0,-1.1)--(0.16,-0.87)--(0.09,-0.46)--(0.13,-0.37)--(0.0,-0.28)
                   --(-0.13,-0.37)--(-0.09,-0.46)--(-0.16,-0.87)--cycle;
    % ears
    \draw[fill=black] (0.74,0.95) to[out=20,in=80](0.86,0.80) to[out=250,in=330](0.65,0.65) to[out=70,in=260] cycle;
    \draw[fill=black] (-0.76,0.96) to[out=170,in=110](-0.85,0.80) to[out=290,in=190](-0.65,0.65) to[out=110,in=290] cycle;

    % head
    \draw[fill=black] (0,0) to[out=180,in=290](-0.72,0.84) to[out=110,in=190](-0.56,1.67)
                      to[out=70,in=110](0.68,1.58) to[out=320,in=80](0.72,0.84) to[out=250,in=0] cycle;
    % face
    \draw[draw=none,fill=white] (0,0.11) to[out=175,in=290](-0.53,0.65) to[out=110,in=265](-0.61,1.22)
                      to[out=80,in=235](-0.50,1.45) to[out=340,in=215](0.50,1.47)
                      to[out=310,in=85](0.60,0.92) to[out=260,in=2] cycle;
    \draw[fill=black] (-0.50,1.45) to[out=315,in=195](0.40,1.25) to[out=340,in=10](0.37,1.32)
                      to[out=190,in=310](-0.40,1.49) -- cycle;
    % neck
    \draw[line width=1.2pt] (-0.62,-0.2) to[out=50,in=290] (-0.5,0.42);
    \draw[line width=1.2pt] (0.62,-0.2) to[out=130,in=250] (0.5,0.42);
    % body
    \draw[draw=\bodycolor,fill=\bodycolor] (0.0,-1.0) to[out=150,in=290](-0.48,-0.14) to[out=200,in=50](-1.28,-0.44)
                   to[out=240,in=80](-1.55,-2.06) -- (1.55,-2.06)
                   to[out=100,in=300](1.28,-0.44) to[out=130,in=340](0.49,-0.14)
                   to[out=245,in=30] cycle;
    % right stet
    \draw[line width=3pt,\stetcolor] (0.8,-0.21) to[bend left=7](0.78,-0.64)
         to[out=350,in=80](0.98,-1.35) to[out=250,in=330](0.72,-1.60);
    \draw[line width=3pt,\stetcolor] (0.43,-1.53) to[out=180,in=240](0.3,-1.15)
         to[out=60,in=170](0.78,-0.64);
    % left stet
    \draw[line width=3pt,\stetcolor] (-0.75,-0.21) to[bend right=20](-0.65,-1.45);
    \node[fill=\stetcolor,circle,minimum size=5pt] at (-0.65,-1.45) {};
    % eyes
    \node[circle,fill=black,inner sep=2pt] at (0.28,0.94) {};
    \node[circle,fill=black,inner sep=2pt] at (-0.28,0.94) {};
     % mouth
    \draw[line width=1.0pt] (-0.25,0.5) to[bend right=40](0.25,0.5);
\end{scope}
  },
}
\pgfkeys{
  /man/.cd,
    scalefac/.store in=\scalefac,
  tiecolor/.store in=\tiecolor,
  bodycolor/.store in=\bodycolor,
  stetcolor/.store in=\stetcolor,
  tiecolor=red,      % derfault tie color
  bodycolor=blue!30  % derfault body color
  stetcolor=green,  % derfault stet color
    scalefac=1,
}
\node (p)   [planet]    {AGI};
%satellites
\foreach \i/\j [count=\k from 0] in {
red/{Web Search},
cyan/{Knowledge\\ Retrieval\\{\fontsize{7pt}{7}\selectfont \textcolor{BrownLine}{RAG}}},
Siva/{Response\\ Generation},
violet!75!/{Context Memory\\{\fontsize{7pt}{7}\selectfont \textcolor{BrownLine}{Sessions}}},
orange/{Safety Filters},
magenta!70!/{External Tools\\{\fontsize{7pt}{7}\selectfont \textcolor{BrownLine}{APIs}}},
green!65!black/{User Interface},
teal!20!gray/{Code\\ Interpreter\\{\fontsize{7pt}{7}\selectfont \textcolor{BrownLine}{Python}}}
}
{\def\radius{4.2}
\def\startangle{90}
%Satelit
\pgfmathsetmacro{\angle}{\startangle - \k * (360/8)}  % smer kazaljke na satu
 \node (s\k) [satellite=\i, font=\footnotesize\usefont{T1}{phv}{m}{n}] at (\angle:\radius) {};
 \node[TxtC,below=0pt of s\k]{\j};
%Arrows
\draw[arr=\i,shorten >=11pt] (p) --coordinate[pos=0.35](AR\k) (s\k);
%\draw[dashed,gray] (p) -- (s\k);
}
\node[above=-5pt of AR2,font=\tiny\usefont{T1}{phv}{m}{n}\bfseries]{Result};
\node[above=-6pt of AR6,font=\tiny\usefont{T1}{phv}{m}{n}\bfseries]{Query};
%web
\pic[shift={(0,0)}] at  (s0){web={scalefac=0.6,picname=1,filllcolor=cyan!30!, Linewidth=1.5pt,filllcirclecolor=orange}};
\pic[shift={(0,0)}] at  (p){llm={scalefac=0.8,drawcolor=BrownLine,filllcolor=green!70!, Linewidth=1.5pt,filllcirclecolor=red}};
\pic[shift={(0,0.1)}] at  (s1){books={scalefac=0.55,picname=1,drawcolor=GreenD,filllcolor=GreenD,Linewidth=2.0pt}};
%generate
\pic[shift={(-0.50,0.7)}] at  (s2){gene={scalefac=1.3,picname=1,drawcolor=RedLine,filllcolor=RedLine,Linewidth=2.5pt}};
\pic[rotate=-10,shift={(0.1,-0.4)}] at  (s2){square={scalefac=0.5,picname=1,filllcolor=cyan!90!,filllcirclecolor=red, Linewidth=0.5pt}};
%brain
\pic[shift={(0.1,0)}] at  (s3){brain={scalefac=1.0,picname=1,filllcolor=orange!30!, Linewidth=0.5pt}};
\pic[shift={(0,0)}] at  (s4){stit={scalefac=0.5,picname=1,drawcolor=orange,filllcolor=green!55!black}};
%gear
\pic[shift={(-0.20,0.25)}] at (s5) {gear={11/1.25/1.7/11/2.0/0.7/scalefac=0.33,drawcolor=black,filllcolor=BrownLine!60}};
\pic[shift={(0.28,-0.4)}] at (s5) {gear={10/1.3/1.7/17/1/0.7/scalefac=0.24,drawcolor=black,filllcolor=BrownLine}};
%person
\pic[shift={(0,0.1)}] at (s6){man={scalefac=0.35,tiecolor=green, bodycolor=VioletLine,stetcolor=VioletLine}};
\pic[shift={(0.1,0.1)}] at  (s7){interpreter={scalefac=0.5,picname=1,filllcolor=cyan!30!, Linewidth=2.0pt,filllcirclecolor=orange}};
%
\draw[LineA](s2.east)--++(0.5,0)--++(0,-6)--node[above,pos=0.87,text=violet,font=\footnotesize\usefont{T1}{phv}{m}{n}]{Feedback Loop}++(-11.7,0)|-(s6);
\end{tikzpicture}}
```
:::

### Orchestration Patterns for Production Systems {#sec-agi-systems-orchestration-patterns-production-systems-8c4f}

Implementing compound AI systems at production scale requires sophisticated orchestration patterns that coordinate specialized components while maintaining reliability and performance. Three fundamental patterns emerged from production deployments at organizations like OpenAI, Anthropic, and Google, each addressing different aspects of component coordination.

The request routing pattern determines which components process each user query based on intent classification and capability requirements. When a user asks "What's the weather in Tokyo?", the orchestrator analyzes the request structure, identifies required capabilities (web search for real-time data, location resolution, unit conversion), and routes to appropriate components. This routing happens in two stages: coarse-grained classification using a small, fast model (10-50ms latency) determines broad categories (factual query, creative task, code generation, multimodal request), followed by fine-grained routing that selects specific component configurations. GPT-4's tool use implementation exemplifies this: the base model generates function calls as structured JSON, a validation layer checks schema compliance, the execution engine invokes external APIs with timeout protection, and result integration merges outputs back into conversation context. The routing layer maintains a capability registry mapping intents to component combinations, updated dynamically as new components deploy or existing ones prove unreliable.

Component coordination becomes critical when multiple specialized modules must work together. The orchestration state machine pattern manages multi-step workflows where outputs from one component inform inputs to subsequent components. Consider a research query requiring synthesis across multiple sources: the orchestrator (1) decomposes the question into sub-queries addressing different aspects, (2) dispatches parallel searches across knowledge bases, (3) ranks retrieved passages by relevance, (4) feeds top-k passages to the reasoning component with the original question, (5) validates generated claims against retrieved evidence, and (6) formats the final response with citations. Each transition between stages requires state management tracking intermediate results, handling partial failures, and making continuation decisions. The orchestrator maintains workflow state in distributed memory (Redis, Memcached) enabling recovery from component failures without restarting entire pipelines. State checkpointing occurs after each successful stage, allowing restart from the last consistent state when components timeout or return errors.

Error handling and resilience patterns prove essential as component counts increase. The circuit breaker pattern prevents cascading failures when components become unreliable. When a knowledge retrieval component begins timing out due to database overload, the circuit breaker tracks failure rates and automatically disables that component after exceeding thresholds (e.g., >30% failures over 60 seconds). Rather than continuing to overwhelm the failing component, the orchestrator routes to fallback strategies: cached responses for common queries, degraded responses from the base model alone, or explicit user notification that certain capabilities are temporarily unavailable. Circuit state transitions through three phases: closed (normal operation), open (failures trigger immediate fallbacks), and half-open (periodic testing for recovery). Anthropic's Claude implementation includes sophisticated fallback hierarchies where constitutional AI filters have multiple backup implementations at different quality/latency trade-offs, ensuring safety validation even when preferred components fail.

Production systems implement dynamic component scaling based on load and performance characteristics. Different components face different bottlenecks: the base language model is compute-bound requiring GPU instances, vector search is memory-bandwidth-bound requiring high-IOPS SSDs, and code execution is isolation-bound requiring sandboxed containers. The orchestrator monitors component-level metrics (latency distribution, throughput, error rates, resource utilization) and signals scaling decisions to the deployment infrastructure. When code execution requests spike during peak hours, Kubernetes horizontally scales container pools while the orchestrator load-balances requests across available instances. This requires sophisticated queuing: high-priority requests (paying customers, critical workflows) skip to front of queues while batch requests tolerate higher latency. The orchestrator tracks per-user request contexts enabling fair scheduling that prevents single users from monopolizing shared resources while maintaining quality of service for all users.

Monitoring and observability become exponentially more complex with compound systems. Traditional metrics like latency and throughput prove insufficient when failures manifest as semantic degradation rather than hard errors. The system might execute successfully (no exceptions thrown, 200 OK responses) yet produce poor outputs because retrieval returned irrelevant passages or the reasoning component hallucinated connections. Production observability requires semantic monitoring tracking content quality alongside system health. This involves multiple validation layers: automated fact-checking comparing claims against knowledge bases, consistency checking ensuring responses don't contradict prior statements in conversation, safety filtering detecting harmful content generation, and calibration monitoring verifying confidence scores match actual accuracy. These validators run asynchronously to avoid blocking user responses but feed into continuous quality dashboards enabling rapid detection of subtle regressions. When Google's Bard initially launched, semantic monitoring detected that certain query patterns caused increased citation errors, triggering investigation revealing retrieval component issues that system metrics alone would not have surfaced.

The engineering challenge intensifies with versioning and deployment. In monolithic systems, version updates are atomic: deploy new model, route traffic, monitor, rollback if necessary. Compound systems have N components evolving independently, creating version compatibility complexity. When the base language model updates to improve reasoning, does it remain compatible with the existing safety filter trained on the old model's output distribution? Production systems maintain compatibility matrices tracking which component versions work together and implement staged rollouts that update one component at a time while monitoring for interaction regressions. This requires extensive integration testing in staging environments that replicate production traffic patterns, A/B testing frameworks comparing compound system variants across user cohorts, and automated canary deployment pipelines that gradually increase traffic to new configurations while watching for anomalies. Operational discipline extends to compound systems but with multiplicative complexity: N components create O(N²) potential interactions requiring validation.

::: {.callout-note title="Progress Checkpoint: Engineering Building Blocks Complete"}
**What we have covered:** The complete engineering stack for compound AI systems, from alternative architectures (state space models, energy-based models, world models) through training methodologies (RLHF, Constitutional AI, continual learning) to production deployment (orchestration patterns, semantic monitoring, component versioning). Together, these represent the current state of the art in AGI-oriented systems engineering.

**Key insight:** Compound systems at production scale require engineering discipline beyond traditional ML: semantic monitoring detects quality degradation invisible to system metrics, circuit breakers prevent cascading failures, and version compatibility matrices manage N² component interactions.

**Where we are heading:** The following section examines fundamental constraints that current approaches face, providing quantitative analysis rather than speculation about when or whether these constraints might be overcome.
:::

## Fundamental System Constraints {#sec-emerging-systems-challenges-fundamental-system-constraints-5115}

The preceding sections examined compound systems combining specialized components, alternative architectures addressing transformer limitations, training methodologies improving alignment, and infrastructure enabling production deployment. These building blocks represent genuine engineering progress, but intellectual honesty requires acknowledging constraints that current approaches face and quantifying their severity.

Five categories of constraints affect current ML systems. Memory systems cannot persist beyond context windows. Energy consumption exceeds biological systems by orders of magnitude. Reasoning often reduces to pattern matching rather than causal inference. Symbol grounding disconnects language from embodied understanding. Alignment techniques cannot yet ensure systems reliably pursue intended objectives. **Uncertainty caveat**: Whether these constraints require architectural innovation, further scaling, or represent fundamental limits remains an open empirical question.

Consider concrete limitations that illustrate these constraints: ChatGPT can write code but fails to track variable state across a long debugging session. It can explain quantum mechanics but cannot learn from user corrections within a conversation. It can translate between languages but lacks the cultural context to know when literal translation misleads. These represent documented architectural limitations, though whether they yield to scale, architectural innovation, or represent fundamental barriers is unknown.

### Memory and Context Limitations {#sec-emerging-systems-challenges-memory-context-limitations-8211}

Human working memory holds approximately seven items [@miller1956magical], yet long-term memory stores lifetime experiences [@landauer1986much]. Current AI systems face a different constraint: transformer context windows reach 128K tokens (approximately 100K words) but cannot maintain information across sessions. This creates systems that can process books but cannot remember yesterday's conversation.

The challenge extends beyond storage to organization and retrieval. Human memory operates hierarchically (events within days within years) and associatively (smell triggering childhood memories). Current systems lack these structures, treating all information equally. Vector databases store billions of embeddings but lack temporal or semantic organization, while humans retrieve relevant memories from decades of experience in milliseconds through associative activation spreading[^fn-associative-memory].

[^fn-associative-memory]: **Associative Memory**: Biological neural networks recall information through spreading activation: one memory trigger activates related memories through learned associations. Hopfield networks (1982) demonstrate this computationally but scale poorly (O(n²) storage). Modern approaches include differentiable neural dictionaries and memory-augmented networks. Human associative recall operates in 100-500&nbsp;ms across 100 billion memories.

**Current engineering approaches**: Retrieval-augmented generation (RAG) addresses some limitations by providing access to external knowledge bases. However, RAG adds latency (100-500ms per retrieval), introduces new failure modes (retrieval quality affects output quality), and does not solve the session persistence problem. Memory-augmented architectures remain research topics with limited production deployment. **Uncertainty**: Whether scaling context windows, improving RAG, or fundamentally new architectures will address these limitations most effectively remains unknown.

### Energy Efficiency: Thermodynamic and Practical Bounds {#sec-emerging-systems-challenges-energy-efficiency-thermodynamic-practical-bounds-c4e1}

Energy consumption presents quantifiable constraints that bound what scaling can achieve economically. GPT-4 training is estimated to have consumed 50-100 GWh of electricity [@epoch2022compute], enough to power 50,000 homes for a year[^fn-gpt4-energy]. Understanding both thermodynamic limits and practical constraints enables realistic assessment of efficiency improvement potential.

[^fn-gpt4-energy]: **GPT-4 Energy Consumption**: Estimated 50-100 GWh for training, equivalent to 50,000 US homes' annual electricity. At $0.10/kWh plus $100M+ hardware amortization, total training cost exceeds $100M. Inference adds ongoing costs: serving GPT-4 to millions requires 10,000+ GPUs continuously. Scaling 10× further would require energy comparable to small cities.

**Thermodynamic bounds on computation** establish fundamental limits independent of architecture. The Landauer limit[^fn-landauer] sets the minimum energy required to erase one bit of information at temperature T: $E_{min} = k_B T \ln 2 \approx 2.9 \times 10^{-21}$ joules at room temperature (300K). Current transistors operate at approximately $10^{-15}$ joules per operation, roughly 3 million times above the Landauer limit.

[^fn-landauer]: **Landauer Limit**: Rolf Landauer (1961) proved that erasing one bit of information necessarily dissipates at least $k_B T \ln 2$ joules of energy, where $k_B$ is Boltzmann's constant. This follows from the second law of thermodynamics: computation that discards information increases entropy, requiring energy dissipation. Reversible computing theoretically avoids this limit by preserving information, but practical reversible computers remain research projects. **Uncertainty caveat**: Whether approaching Landauer efficiency is practically achievable for useful computation remains unknown.

**Practical limits** are much tighter than thermodynamic limits. The human brain operates on 20 watts while performing computations that would require megawatts on current hardware[^fn-brain-efficiency]. This efficiency gap (approximately 360× by one estimate) emerges from architectural differences: biological neurons operate at approximately 1 Hz effective compute rates using chemical signaling, while digital processors run at GHz frequencies using electronic switching. Despite the frequency disadvantage, the brain's extensive parallelism (10¹¹ neurons with 10¹⁴ connections) and analog processing enable efficient pattern recognition that digital systems achieve only through higher energy expenditure.

[^fn-brain-efficiency]: **Biological vs Digital Efficiency**: Brain: approximately 10¹⁵ ops/sec ÷ 20&nbsp;W = 5 × 10¹³ ops/watt [@sandberg2008whole]. H100 GPU: 1.98 × 10¹⁵ ops/sec ÷ 700&nbsp;W = 2.8 × 10¹² ops/watt. Efficiency ratio: approximately 360× advantage for biological computation. **Critical caveat**: This comparison requires careful interpretation. Biological neurons use analog, chemical signaling with massive parallelism, while digital systems use precise, electronic switching with sequential processing. The mechanisms are fundamentally different, making direct efficiency comparisons approximate. What computation the brain performs per "operation" is not well defined.

**Improvement potential**: Current digital systems have approximately 10⁶× improvement potential to reach Landauer limits, but each 10× improvement becomes progressively harder. Practical improvement paths include:

- **Neuromorphic architectures**: Intel's Loihi achieves 1000× efficiency gains for sparse, event-driven workloads, but this applies to specific workload types
- **Reversible computing**: Theoretical elimination of Landauer costs, but no practical implementations exist
- **Cryogenic computing**: Reducing temperature decreases $k_B T$, but cooling costs typically exceed energy savings
- **Algorithmic efficiency**: Better algorithms can reduce compute requirements, but fundamental task complexity provides lower bounds

**Quantitative constraint**: Closing the efficiency gap by 1000× (from current H100 efficiency to brain-like efficiency) would reduce GPT-4 training energy from 50-100 GWh to 50-100 MWh. This represents a meaningful but not transformative change in economic feasibility. Solutions building on @sec-sustainable-ai (neuromorphic architectures, reversible computing, algorithmic improvements) address different aspects of this gap with varying degrees of maturity and applicability.

### Causal Reasoning and Planning Capabilities {#sec-agi-systems-causal-reasoning-planning-capabilities-32be}

Algorithmic limitations remain even with efficient hardware. Current models excel at pattern completion but struggle with novel reasoning. Ask ChatGPT to plan a trip, and it produces plausible itineraries. Ask it to solve a problem requiring new reasoning (proving a novel theorem or designing an experiment) and performance degrades rapidly[^fn-reasoning-limitation].

[^fn-reasoning-limitation]: **Reasoning Performance Cliff**: LLMs achieve 90%+ on familiar patterns but drop to 10-30% on problems requiring genuine abstraction. The ARC challenge [@chollet2019measure] tests visual reasoning with novel rules—GPT-4 achieves ~25% vs. humans' 80%+. This gap suggests current models memorize patterns rather than learning generalizable reasoning procedures.

True reasoning requires capabilities absent from current architectures. World models represent internal simulations of how systems behave over time, understanding that dropping a ball causes it to fall, not just that "dropped" and "fell" co-occur in text. Search mechanisms explore solution spaces systematically rather than relying on pattern matching. Finding mathematical proofs requires testing hypotheses and backtracking, not just recognizing solution patterns. Causal understanding distinguishes correlation from causation, recognizing that umbrellas correlate with rain but do not cause it, while clouds do[^fn-reasoning-requirements]. These capabilities demand architectural innovations beyond current neural network designs, potentially hybrid systems combining neural networks with symbolic reasoners, or new architectures inspired by cognitive science.

[^fn-reasoning-requirements]: **Reasoning vs Pattern Matching**: **World models**: Internal simulators predicting consequences ("if I move this chess piece, opponent's likely responses are..."). Current LLMs lack persistent state; each token generation starts fresh. **Search**: Systematic exploration of possibilities with backtracking. Chess programs search millions of positions; LLMs generate tokens sequentially without reconsideration. **Causal understanding**: Distinguishing causation from correlation. Humans understand that medicine causes healing (even if correlation isn't perfect), while LLMs may learn "medicine" and "healing" co-occur without causal direction. Classical planning requires explicit state representation, action models, goal specification, and search algorithms. Neural networks provide none explicitly. Neurosymbolic approaches attempt integration but remain limited to narrow domains.

### Symbol Grounding and Embodied Intelligence {#sec-agi-systems-symbol-grounding-embodied-intelligence-4de1}

Language models learn "cat" co-occurs with "meow" and "fur" but have never experienced a cat's warmth or heard its purr. This symbol grounding problem[^fn-symbol-grounding] [@harnad1990symbol; @searle1980minds] (connecting symbols to experiences) may limit intelligence without embodiment.

[^fn-symbol-grounding]: **Symbol Grounding Problem**: Philosophical challenge first articulated by Stevan Harnad (1990): how do abstract symbols acquire meaning beyond their relationships to other symbols? A computer can process "cat" and "dog" as different tokens without understanding what cats or dogs are. Searle's Chinese Room argument (1980) makes a related point: manipulating symbols according to rules does not constitute understanding. For AGI, this suggests that purely text-based training may produce sophisticated pattern matching without genuine comprehension.

Robotic embodiment introduces systems constraints covered in this volume's on-device learning discussions: real-time inference requirements (sub-100&nbsp;ms control loops), continuous learning from noisy sensor data, and safe exploration in environments where mistakes cause physical damage[^fn-embodiment-constraints]. These constraints mirror efficiency challenges but with even stricter latency and reliability requirements. Yet embodiment might be essential for understanding concepts like "heavy," "smooth," or "careful" that are grounded in physical experience.

[^fn-embodiment-constraints]: **Robotic System Requirements**: Physical AI demands extreme real-time performance. Boston Dynamics' Atlas executes 1KHz control loops across 28 actuators; Tesla's FSD processes 36 cameras at 36 FPS. Both require <10ms inference latency—impossible with cloud round-trips. Embodied AGI must solve perception, planning, and control simultaneously on edge hardware.

### AI Alignment and Value Specification {#sec-agi-systems-ai-alignment-value-specification-13f9}

The most critical barrier involves ensuring AGI systems pursue human values rather than optimizing simplified objectives that lead to harmful outcomes[^fn-alignment-challenge]. Current reward functions are proxies (maximize engagement, minimize error) that can produce unintended behaviors when optimized strongly.

[^fn-alignment-challenge]: **Alignment Failure Modes**: Systems optimizing misspecified objectives produce unintended harmful behaviors. YouTube's watch-time optimization promoted extreme content; trading algorithms caused 2010's trillion-dollar flash crash. Goodhart's Law ("when a measure becomes a target, it ceases to be a good measure") suggests AGI alignment requires solving value specification fundamentally, not just optimization engineering.

Alignment requires solving multiple interconnected problems. Value specification asks what humans actually want. Robust optimization pursues goals without exploiting loopholes. Corrigibility keeps systems modifiable as capabilities grow. Scalable oversight maintains control over systems smarter than overseers[^fn-alignment-components]. These challenges span technical and philosophical domains, requiring advances in interpretability from @sec-responsible-ai, formal verification methods, and new frameworks for specifying and verifying objectives.

[^fn-alignment-components]: **Alignment Technical Challenges**: Value specification: Arrow's impossibility theorem shows no perfect aggregation of preferences. Robust optimization: Goodhart's law states optimized metrics cease being good metrics. Corrigibility: Self-modifying systems might remove safety constraints. Scalable oversight: Humans cannot verify solutions to problems they cannot solve.

::: {.callout-note title="The Alignment Tax: Permanent Operational Cost of Safety"}
Ensuring AGI systems are safe and aligned with human values requires significant, ongoing investment of computational resources, research effort, and human oversight. This "alignment tax" represents a permanent operational cost, not a one-time problem to be solved. Aligned AGI systems may be intentionally less computationally efficient than unaligned ones because a portion of their resources will always be dedicated to safety verification, value alignment checks, and self-limitation mechanisms. Systems must continuously monitor their own behavior, verify outputs against safety constraints, and maintain oversight channels even when these checks introduce latency or reduce throughput. This frames alignment not as an engineering hurdle to overcome and move past, but as a continuous cost of operating trustworthy intelligent systems at scale.
:::

::: {#fig-technical-barriers fig-env="figure" fig-pos="htb" fig-cap="**Technical Barriers to AGI**: Five critical challenges must be solved simultaneously for artificial general intelligence. Each represents orders-of-magnitude gaps: memory systems need persistence across sessions, energy efficiency requires 1000x improvements, reasoning needs genuine planning beyond pattern matching, embodiment demands symbol grounding, and alignment requires value specification. Red arrows show critical blocking paths; dashed gray lines indicate key interdependencies." fig-alt="Radial diagram with AGI center connected to 5 barriers: Memory (no persistence), Energy (100 GWh vs 20W), Reasoning (pattern only), Embodiment (no grounding), and Alignment (value loading). Red arrows show blocking paths."}
```{.tikz}
\scalebox{0.65}{%
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
planet/.style = {circle, draw=yellow!50!red!90,semithick, fill=yellow!30,line width=1.5pt,
                    font=\usefont{T1}{phv}{m}{n}\bfseries,
                    minimum size=24mm, inner sep=1mm,align=flush center},
satellite/.style = {circle, draw=none, semithick, fill=#1!10,
                    text width=26mm, inner sep=1pt, align=flush center,minimum size=28mm,minimum height=12mm},
TxtC/.style = {font=\footnotesize\usefont{T1}{phv}{m}{n},text width=50mm,align=flush center},
arr/.style = {-{Triangle[length=3mm,width=6mm]}, color=#1!60,,
                    line width=3mm, shorten <=1mm, shorten >=1mm},
LineA/.style={violet!30,dashed, line width=1.0pt,{-{Triangle[width=1.0*6pt,length=1.6*6pt]}},shorten <=3pt,shorten >=2pt}
}

%puzzle
\tikzset{pics/puzzle/.style = {
        code = {
\pgfkeys{/channel/.cd, #1}
\begin{scope}[scale=\scalefac, every node/.append style={transform shape}]
\fill[fill=\filllcolor] (-2,-0.35) to[out=90,in=135] (-1.5,-0.45) arc(-135:135:0.6 and
{0.45*sqrt(2)}) to[out=-135,in=-90] (-2,0.35) |- (-0.35,2)
to[out=0,in=-45] (-0.45,2.5) arc(225:-45:{0.45*sqrt(2)} and 0.6)
to[out=-135,in=180] (0.35,2) -| (2,0.35)
to[out=-90,in=225] (2.5,0.45) arc(135:-135:0.6 and {0.45*sqrt(2)})
to[out=135,in=90] (2,-0.35) |- (0.35,-2)
to[out=180,in=-135] (0.45,-1.5) arc(-45:225:{0.45*sqrt(2)} and 0.6)
to[out=-45,in=0] (-0.35,-2) -| cycle;
\end{scope}
}}}
%battery
\tikzset{
pics/battery/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[rectangle,minimum width=35mm,minimum height=8mm,draw=\drawcolor,
rounded corners=4pt,fill=\filllcirclecolor,line width=\Linewidth](2R\picname) at (1,0){};
\node[rectangle,minimum width=45mm,minimum height=22mm,draw=\drawcolor,
rounded corners=4pt,fill=\filllcolor,line width=\Linewidth](R\picname) at (0,0){};
\node[rectangle,minimum width=5mm,minimum height=18mm,draw=none,
fill=green,line width=\Linewidth](3R\picname) at ($(R\picname.west)!0.5!(R\picname.east)$){};
\node[rectangle,minimum width=5mm,minimum height=18mm,draw=none,
fill=green,line width=\Linewidth](3R\picname) at ($(R\picname.west)!0.33!(R\picname.east)$){};
\node[rectangle,minimum width=5mm,minimum height=18mm,draw=none,
fill=green,line width=\Linewidth](3R\picname) at ($(R\picname.west)!0.16!(R\picname.east)$){};

\end{scope}
    }
  }
}
%scales
\tikzset{
pics/scales/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[rectangle,minimum width=2mm,minimum height=22mm,
draw=none, fill=\filllcolor,line width=\Linewidth](1R) at (0,-0.95){};
\fill[fill=\filllcolor!60!black](230:2.8)arc(230:310:2.8)--cycle;%circle(2.9);
%LT
\node [semicircle, shape border rotate=180,  anchor=chord center,
      minimum size=11mm, draw=none, fill=\filllcirclecolor](LT) at (-2,-0.5) {};
\node [circle,  minimum size=4mm, draw=none, fill=\filllcirclecolor](T1) at (-2,1.25) {};
\draw[draw=\drawcolor,,line width=1.2*\Linewidth,shorten <=3pt,shorten >=3pt](T1)--(LT);
\draw[draw=\drawcolor,,line width=1.2*\Linewidth,shorten <=3pt,shorten >=3pt](T1)--(LT.30);
\draw[draw=\drawcolor,,line width=1.2*\Linewidth,shorten <=3pt,shorten >=3pt](T1)--(LT.150);
%DT
\node [semicircle, shape border rotate=180,  anchor=chord center,
      minimum size=11mm, draw=none, fill=\filllcirclecolor!70!black](DT) at (2,-0.5) {};
\node [circle,  minimum size=4mm, draw=none, fill=\filllcirclecolor!70!black](T2) at (2,1.25) {};
\draw[draw=\drawcolor,line width=1.2*\Linewidth,shorten <=3pt,shorten >=3pt](T2)--(DT);
\draw[draw=\drawcolor,,line width=1.2*\Linewidth,shorten <=3pt,shorten >=3pt](T2)--(DT.30);
\draw[draw=\drawcolor,,line width=1.2*\Linewidth,shorten <=3pt,shorten >=3pt](T2)--(DT.150);
%
\node[draw=none,rectangle,minimum width=32mm,minimum height=1.5mm,inner sep=0pt,
fill=\filllcolor!60!black]at(0,1.25){};
\node[draw=white,fill=\filllcolor,line width=2*\Linewidth,ellipse,minimum width=9mm,  minimum height=15mm](EL)at(0,0.85){};
\node[draw=white,fill=\filllcolor!60!black,line width=2*\Linewidth,,circle,minimum size=10mm](2C)at(0,2.05){};
\end{scope}
    }
  }
}
%robot
\tikzset{
pics/robot/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[rectangle,minimum width=14mm,minimum height=11mm,rounded corners=5pt,
draw=\filllcolor, fill=\filllcolor,line width=\Linewidth](R) at (0,0){};
\node[rectangle,minimum width=1mm,minimum height=3mm,inner sep=0pt,
draw=none,anchor=south,fill=\filllcolor!40!black,line width=1pt](2R) at (R.north){};
\node[circle,minimum width=1mm,minimum height=3mm,inner sep=0pt,
draw=none,anchor=south,fill=\filllcirclecolor!70!black,line width=1pt](3R) at (2R){};
%left eye
\node [circle,  minimum size=3.3mm, draw=none, fill=white,inner sep=0pt](C1) at (-0.42,-0.03) {};
\node [circle,  minimum size=2mm, draw=none, fill=\filllcirclecolor,inner sep=0pt](C1) at (-0.42,-0.03) {};
%right eye
\node [circle,  minimum size=5.3mm, draw=none, fill=white,inner sep=0pt](C1) at (0.28,0.0) {};
\node [circle,  minimum size=3mm, draw=none, fill=\filllcirclecolor,inner sep=0pt](C1) at (0.28,0.0) {};
%line
\draw[line cap=round,red,line width=3pt](-0.5,-0.4)--(-0.3,-0.4);
\draw[line cap=round,blue,line width=3pt](-0.15,-0.4)--(-0.05,-0.4);
\draw[line cap=round,yellow,line width=3pt](0.1,-0.4)--(0.2,-0.4);
\draw[line cap=round,green,line width=3pt](0.35,-0.4)--(0.5,-0.4);
\end{scope}
    }
  }
}
%data
\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,
minimum width=25mm,minimum height=11mm,line width=\Linewidth,node distance=-0.15},
pics/data/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[mycylinder,fill=\filllcolor!50] (A) {};
\node[mycylinder, above=of A,fill=\filllcolor!30] (B) {};
\node[mycylinder, above=of B,fill=\filllcolor!10] (C) {};
 \end{scope}
     }
  }
}
\pgfkeys{
  /channel/.cd,
   Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawcolor/.store in=\drawcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  filllcolor=BrownLine,
  filllcirclecolor=violet!20,
  drawcolor=red,
  drawcircle=violet,
  scalefac=1,
  Linewidth=0.5pt,
  Depth=1.3,
  Height=0.8,
  Width=1.1,
  picname=C
}

%planet
\node (p)   [planet]    {AGI};
%satellites
\def\radius{32mm}
\def\startangle{90}

\foreach \i/\j/\sho [count=\k from 0] in {
red/{\textbf{Memory}\\ {\footnotesize No persistence}}/20pt,
cyan/{\textbf{Energy}\\ {\footnotesize 100 GWh vs 20W}}/11pt,
Siva/{\textbf{Reasoning}\\{\footnotesize Pattern only}}/11pt,
violet!75!/{\textbf{Embodiment}\\{\footnotesize No grounding}}/11pt,
orange/\textbf{Alignment}\\ {\footnotesize Value loading}/11pt
}
{\def\radius{4.2}
\def\startangle{90}
%Satelit
\pgfmathsetmacro{\angle}{\startangle - \k * (360/5)}
\node (s\k) [satellite=\i, font=\footnotesize\usefont{T1}{phv}{m}{n}] at (\angle:\radius) {};
 \node[TxtC,below=0pt of s\k]{\j};
%Arrows
\draw[arr=\i,shorten >=\sho] (p) --coordinate[pos=0.35](AR\k) (s\k);
}
%battery
\pic[shift={(0,0)}] at  (s1){battery={scalefac=0.45,picname=1, drawcolor=BrownLine,filllcolor=BrownLine!10!, Linewidth=1.5pt,filllcirclecolor=BrownLine}};
%puzzle
\begin{scope}[local bounding box=PUZZLE1,shift={($(-0.4,-0.45)+(s2)$)},
scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){puzzle={scalefac=0.2,picname=1,filllcolor=orange!80}};
\pic[shift={(0,0.8)}] at  (0,0){puzzle={scalefac=0.2,picname=1,filllcolor=red!80}};
\pic[shift={(0.8,0)}] at  (0,0){puzzle={scalefac=0.2,picname=1,filllcolor=green!60!black}};
\pic[shift={(0.8,0.8)}] at  (0,0){puzzle={scalefac=0.2,picname=1,filllcolor=cyan!70}};
\end{scope}
%scales
\pic[shift={(0,0)}] at  (s4){scales={scalefac=0.4,picname=1,filllcolor=BlueLine, Linewidth=1.0pt,filllcirclecolor=orange}};
%robot
\pic[shift={(0,-0.15)}] at  (s3){robot={scalefac=1.2,picname=1,filllcolor=BlueLine, Linewidth=1.0pt,filllcirclecolor=red}};
\pic[shift={(0,-0.7)}] at  (s0){data={scalefac=0.6,picname=1,filllcolor=BlueLine, Linewidth=1.0pt}};
\end{tikzpicture}
}
```
:::

@fig-technical-barriers maps these five barriers as an interconnected web of challenges where progress on any single barrier remains insufficient, as AGI requires coordinated breakthroughs across all dimensions. The engineering principles developed throughout this textbook, from data engineering through distributed training to robust deployment, provide foundations for addressing each barrier, though the complete solutions remain unknown.

These constraints motivate architectural alternatives to monolithic models. One approach distributes computation across multiple specialized components that collaborate through structured interfaces.

## Multi-Agent Coordination as Engineering Pattern {#sec-emerging-systems-challenges-multiagent-coordination-engineering-pattern-a211}

The constraints outlined above suggest that addressing them through monolithic system improvements may face diminishing returns. Multi-agent systems offer an alternative engineering pattern where capabilities emerge from interactions between specialized components rather than residing in any single system. This approach extends the compound AI systems framework: rather than one model handling all tasks, specialized agents collaborate through structured interfaces.

Multi-agent architectures can address specific constraints through distribution. Context limitations reduce when specialized agents maintain domain-specific state. Energy efficiency improves through selective activation where only relevant agents engage for each task. Complex reasoning can decompose across specialized agents with verification steps. These are engineering trade-offs, not predictions about what capabilities will emerge.

**Uncertainty caveat**: Whether multi-agent approaches offer practical advantages over alternative architectures (scaling monolithic models, improving compound systems, developing new architectures) remains an open empirical question. The following analysis examines coordination challenges that multi-agent systems face, enabling quantitative comparison with alternatives.

Large-scale multi-agent coordination introduces engineering challenges that scale with agent count[^fn-agent-scale]. Coordination between thousands of agents distributed across continents faces latency constraints: communication between agents in Tokyo and New York introduces 150&nbsp;ms round-trip delays, constraining real-time coordination patterns.

[^fn-agent-scale]: **Multi-Agent Scale**: Current production multi-agent systems coordinate tens to hundreds of agents. Scaling to thousands introduces O(n²) coordination complexity without hierarchical organization. Whether larger-scale multi-agent systems provide benefits proportional to their coordination costs remains unclear.

Addressing coordination challenges requires establishing agent specialization across different domains. Specialized agents handle distinct functions (retrieval, reasoning, code execution, verification) while sharing common interfaces that enable coordination. This mirrors how modern software systems decompose complex functionality into microservices, trading coordination complexity for specialization benefits.

The effectiveness of such specialization depends on communication protocols between agents. Unlike traditional distributed systems that exchange simple state updates, multi-agent ML systems must communicate semantic information including context, reasoning chains, and uncertainty estimates[^fn-agent-communication]. Current approaches use structured JSON or function calling interfaces, but these may become bottlenecks at scale.

[^fn-agent-communication]: **Agent Communication Trade-offs**: Richer inter-agent communication enables better coordination but increases bandwidth and latency. Current production systems use structured function calls with 1-10KB payloads. Semantic compression (transmitting concepts rather than tokens) could reduce bandwidth but requires standardized representations that do not yet exist.

Network topology design becomes critical for achieving efficient communication at scale. Hierarchical topologies reduce communication from O(n²) to O(n log n) by clustering local processing with regional integration[^fn-agent-topology]. Current multi-agent frameworks (LangChain, AutoGPT) use flat architectures suitable for small agent counts; production systems may require hierarchical coordination as agent counts grow.

[^fn-agent-topology]: **Agent Topology Choices**: Flat topologies are simple but scale poorly (O(n²) communication). Hierarchical topologies reduce communication but introduce coordination latency. The optimal topology depends on agent count, task structure, and latency requirements. No consensus exists on preferred approaches for large-scale systems.

Consensus mechanisms for multi-agent systems face complexity beyond traditional distributed systems. While Paxos/Raft consensus handles simple state transitions, multi-agent systems must handle potentially conflicting outputs from different agents[^fn-agent-consensus]. Current approaches use voting, ranking, or orchestrator arbitration, each with different trade-offs for quality and latency.

[^fn-agent-consensus]: **Agent Consensus Approaches**: Voting selects majority outputs but fails when outputs are diverse. Ranking uses a separate model to evaluate outputs but adds latency. Orchestrator arbitration uses rules or another model to select outputs. Production systems typically use orchestrator arbitration for simplicity, though voting can improve reliability on tasks with clear correct answers.

Resource coordination across many agents requires scheduling algorithms that consider agent capabilities, current load, and task requirements. Reasoning tasks vary substantially (10-1000×) in compute requirements, making predictive scheduling valuable but difficult[^fn-agent-scheduling]. Current orchestration frameworks (Kubernetes, Ray) provide foundations but lack ML-specific scheduling heuristics.

[^fn-agent-scheduling]: **Agent Scheduling Challenges**: Reasoning task compute requirements are difficult to predict before execution. Simple heuristics (round-robin, least-loaded) perform poorly when task variance is high. More sophisticated approaches use task classification or adaptive time limits, but no approach dominates across workloads.

Multi-agent coordination provides engineering benefits (modularity, specialization, fault isolation) but introduces coordination costs (latency, complexity, failure modes). The trade-off is workload-specific: tasks requiring tight coordination between components may perform better with monolithic models, while tasks with natural decomposition may benefit from multi-agent approaches.

::: {.callout-note title="Progress Checkpoint: Constraints and Engineering Patterns"}
**What we have covered:** Five categories of fundamental constraints (memory, energy, reasoning, grounding, alignment) and multi-agent coordination as one engineering approach to address them.

**Key insight:** Engineering trade-offs, not speculation about future breakthroughs, should guide architectural decisions. Different constraints have different severity and different approaches for mitigation.

**Where we are heading:** The final section translates these constraints into practical engineering guidance, identifying where current approaches are effective, where uncertainty is high, and how practitioners can make defensible decisions.
:::

## Engineering Guidance for Frontier Systems {#sec-emerging-systems-challenges-engineering-guidance-frontier-systems-6966}

The preceding sections surveyed building blocks, emerging constraints, and architectural patterns. This section translates that analysis into practical engineering guidance: which investments have clear returns, where uncertainty is high, and how practitioners can make defensible decisions in the face of rapid change.

The key insight for practitioners is distinguishing between investments with quantifiable returns (infrastructure efficiency improvements, compound system orchestration, reliability engineering) and investments based on speculative capability predictions (betting on specific architectures achieving qualitative breakthroughs). **The former enable progress regardless of which research directions prove fruitful; the latter carry substantial risk of misallocation.**

The convergence of these building blocks (data engineering at scale, dynamic architectures, alternative paradigms, training methodologies, and hardware constraints) creates concrete opportunities for ML systems engineers. These are near-term projects with measurable returns, not speculative bets on uncertain capability trajectories.

### High-Confidence Investment Areas {#sec-emerging-systems-challenges-highconfidence-investment-areas-e9ab}

Three investment areas have clear, quantifiable returns independent of which capability trajectories materialize.

**Infrastructure efficiency**: Training platforms currently achieve only 20-40% GPU utilization. Improving utilization to 70-80% would reduce training costs by 40-60%, worth billions annually at industry scale. This is a pure engineering problem with known solutions: better scheduling, more efficient checkpointing, improved communication patterns. The returns are measurable and independent of capability speculation.

**Compound system orchestration**: Integrating specialized components (retrieval, code execution, verification) provides measurable capability improvements (documented 40-90% accuracy improvements on specific tasks) with clear engineering trade-offs. These investments pay off regardless of whether monolithic models eventually achieve similar capabilities, because they address current limitations with current technology.

**Reliability and observability**: Production ML systems require monitoring, testing, and failure recovery that current frameworks under-provide. Investment in semantic monitoring (detecting quality degradation), robust orchestration (circuit breakers, fallbacks), and continuous evaluation (detecting capability drift) has clear returns in production system quality.

### Moderate-Confidence Investment Areas {#sec-emerging-systems-challenges-moderateconfidence-investment-areas-3fcd}

**Multimodal systems**: Processing text, images, audio, and video jointly provides capabilities unavailable from single-modality systems. The engineering is mature enough for production deployment, but the value depends on specific applications. Investment makes sense for applications with clear multimodal requirements.

**Personalization infrastructure**: Parameter-efficient fine-tuning (reducing costs 1000×) and retrieval systems for personal knowledge bases enable new application categories. The technical foundations are solid, but whether users will adopt personalized AI systems at scale remains uncertain.

### High-Uncertainty Areas {#sec-emerging-systems-challenges-highuncertainty-areas-46b4}

**Architectural bets**: Investing heavily in specific alternative architectures (state space models, energy-based models) carries substantial risk. These may prove superior to transformers for specific workloads, but which workloads and by how much remains uncertain. Hedging through architecture-agnostic infrastructure is more defensible than betting on specific alternatives.

**Capability timeline predictions**: Planning based on when specific capabilities will emerge is high-risk. Historical AI predictions show systematic overconfidence and poor calibration. Decisions that depend on specific timelines for qualitatively new capabilities should be avoided or explicitly hedged.

### Current Engineering Challenges {#sec-emerging-systems-challenges-current-engineering-challenges-3839}

These opportunities require addressing engineering challenges that affect current systems at scale.

#### Reliability at Scale {#sec-emerging-systems-challenges-reliability-scale-8c14}

When training runs cost millions of dollars and involve thousands of components, even 99.9% reliability means frequent failures destroying weeks of progress. This demands checkpointing that restarts from recent states, recovery mechanisms salvaging partial progress, and graceful degradation maintaining quality when components fail. Moving from 99.9% to 99.99% reliability (a 10× reduction in failure rate) proves disproportionately expensive, requiring redundancy, predictive failure detection, and fault-tolerant algorithms. **This is a current, quantifiable problem with known engineering approaches, not a speculative future challenge.**

#### Heterogeneous System Orchestration {#sec-emerging-systems-challenges-heterogeneous-system-orchestration-e9a9}

Systems must coordinate CPUs for preprocessing, GPUs for matrix operations, TPUs[^fn-tpu] for inference, and potentially specialized accelerators for specific workloads. This heterogeneity demands abstractions hiding complexity from developers and scheduling algorithms optimizing across different computational paradigms. Current frameworks (TensorFlow, PyTorch) assume relatively homogeneous hardware; production systems increasingly require multi-paradigm orchestration.

[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed for neural network ML. TPU v4 (2021) delivers 275 teraFLOPs for training with specialized matrix multiplication units. The key engineering question is not whether specialized hardware provides benefits (it does) but how to orchestrate heterogeneous hardware efficiently.

Quality-efficiency trade-offs sharpen as systems scale. Real-time systems often cannot use the most advanced models due to latency constraints—a dilemma that intensifies as model capabilities grow. The optimization challenge involves hierarchical processing where simple models handle routine cases while advanced models activate only when needed, adaptive algorithms adjusting computational depth based on available time, and graceful degradation providing approximate results when exact computation isn't possible.

#### Operational Challenges: Testing and Deployment {#sec-agi-systems-operational-challenges-testing-deployment-3fcf}

Verification and validation for AI-driven workflows proves difficult when errors compound through long chains. A small mistake in early stages can invalidate hours or days of subsequent work. This requires automated testing understanding AI behavior patterns, checkpoint systems enabling rollback from failure points, and confidence monitoring triggering human review when uncertainty increases. Testing frameworks extend to handle non-deterministic AI components and emergent behaviors.

Trust calibration determines when humans should intervene in automated systems. Complete automation often fails, but determining optimal handoff points requires understanding both technical capabilities and human factors. The challenge involves creating interfaces providing context for human decision-making, developing trust calibration so humans know when to intervene, and maintaining human expertise in domains where automation becomes dominant. This draws on responsible AI principles from @sec-responsible-ai regarding human-AI collaboration.

Safety monitoring at the semantic level requires understanding content and intent, not just system metrics. AI safety monitoring must detect harmful outputs, prompt injections, and adversarial attacks in real-time across billions of interactions—qualitatively different from traditional software monitoring tracking latency, throughput, and error rates. This necessitates new tooling combining robustness principles (@sec-robust-ai), security practices (@sec-security-privacy), and responsible AI frameworks (@sec-responsible-ai).

#### Social and Ethical Considerations {#sec-agi-systems-social-ethical-considerations-34a8}

AGI systems amplify existing privacy and security challenges (@sec-security-privacy) while introducing new attack vectors through multi-component interactions and continuous learning capabilities. Privacy and personalization create difficult tensions in system design. Personalization requires user data (conversation histories, work patterns, preferences) yet privacy regulations and user expectations increasingly demand local processing. The challenge lies in developing federated learning and differential privacy techniques that enable personalization while maintaining privacy guarantees. Current approaches often sacrifice significant performance for privacy protection—a trade-off that must improve for widespread adoption.

Filter bubbles and bias amplification risk reinforcing harmful patterns when personalized AI systems learn to give users what they want to hear rather than what they need to know. This limits exposure to diverse perspectives and challenging ideas. Building responsible personalization requires ensuring systems occasionally introduce diverse viewpoints, challenge user assumptions rather than confirming beliefs, and maintain transparency about personalization processes. This applies the responsible AI principles from @sec-responsible-ai at the personalization layer.

Explainability and performance create tension, forcing choices between model accuracy and human interpretability. More interpretable models often sacrifice accuracy because constraints required for human understanding may conflict with optimal computational patterns. Different stakeholders need different explanations: medical professionals want detailed causal reasoning, patients want simple reassuring summaries, regulatory auditors need compliance-focused explanations, and researchers need technical details enabling reproducibility. Building systems adapting explanations appropriately requires combining technical expertise with user experience design.

The opportunity and challenge landscapes interconnect: infrastructure platforms enable personalized and real-time systems, which power automation applications, but each opportunity amplifies specific challenges. Successfully navigating this landscape requires the systems thinking developed throughout this textbook: understanding how components interact, anticipating failure modes, designing for graceful degradation, and balancing competing constraints. The engineering principles from data pipelines through distributed training to robust deployment provide foundations for addressing these challenges at unprecedented scale.

## Implications for ML Systems Engineers {#sec-emerging-systems-challenges-implications-ml-systems-engineers-aee8}

ML systems engineers with understanding of this textbook's content are well-positioned for frontier system development. The competencies developed (data engineering, distributed training, model optimization, robust deployment) constitute essential infrastructure requirements. Frontier development demands full-stack capabilities spanning infrastructure construction, efficient experimentation tools, safety system design, and reproducible complex system interactions.

### Applying Frontier Concepts to Current Practice {#sec-emerging-systems-challenges-applying-frontier-concepts-current-practice-0d07}

Understanding frontier constraints improves architectural decisions in routine ML projects today. @tbl-frontier-mapping demonstrates how the engineering challenges faced by frontier systems directly map to foundational knowledge developed throughout this textbook, reinforcing that frontier system skills extend current competencies rather than replacing them.

+----------------------------+------------------------------------------------------------+
| **Frontier Challenge**     | **Foundational Knowledge**                                 |
+:===========================+:===========================================================+
| **Data at Scale**          | Data pipelines and engineering for large datasets          |
+----------------------------+------------------------------------------------------------+
| **Training Paradigms**     | Distributed training techniques and optimization           |
+----------------------------+------------------------------------------------------------+
| **Dynamic Architectures**  | Deep neural network architectures and model structures     |
+----------------------------+------------------------------------------------------------+
| **Hardware Scaling**       | AI acceleration through specialized processors (GPUs/TPUs) |
+----------------------------+------------------------------------------------------------+
| **Efficiency & Resources** | Model optimizations: quantization, pruning, distillation   |
+----------------------------+------------------------------------------------------------+
| **Development Frameworks** | ML frameworks and development tools                        |
+----------------------------+------------------------------------------------------------+
| **System Orchestration**   | Workflow orchestration and pipeline management             |
+----------------------------+------------------------------------------------------------+
| **Edge Deployment**        | @sec-edge-intelligence: On-device Learning                 |
+----------------------------+------------------------------------------------------------+
| **Performance Evaluation** | Benchmarking methodologies and metrics                     |
+----------------------------+------------------------------------------------------------+
| **Privacy & Security**     | @sec-security-privacy: Privacy & Security                  |
+----------------------------+------------------------------------------------------------+
| **Energy Sustainability**  | @sec-sustainable-ai: Sustainable AI                        |
+----------------------------+------------------------------------------------------------+
| **Alignment & Safety**     | @sec-responsible-ai: Responsible AI                        |
+----------------------------+------------------------------------------------------------+
| **Operations**             | ML operations, deployment, and monitoring                  |
+----------------------------+------------------------------------------------------------+

: **Frontier System Challenges to Core ML Systems Knowledge**: The technical challenges of frontier systems directly build upon the foundational engineering principles covered throughout this textbook. {#tbl-frontier-mapping}

Three concepts from frontier systems apply directly to current practice. First, compound systems with specialized components often outperform single large models while being easier to debug, update, and scale; the orchestration architecture in @fig-compound-ai-system applies whether coordinating multiple models, integrating external tools, or combining retrieval with generation. Second, @fig-frontier-data-pipeline reveals that frontier models discard over 90% of raw data through filtering, suggesting most projects under-invest in data cleaning and quality assessment. Third, @fig-rlhf-pipeline demonstrates how alignment through preference learning proves essential for user satisfaction at any scale, from customer service bots to recommendation engines.

The principles covered throughout this textbook provide the foundation; frontier systems push these principles toward larger scale as distributed systems expertise, hardware-software co-design knowledge, and human-AI interaction understanding become increasingly critical.

## Core Design Principles for Frontier Systems {#sec-emerging-systems-challenges-core-design-principles-frontier-systems-8089}

This section examines enduring design principles that guide frontier ML systems regardless of which specific technologies prevail. The preceding table maps system challenges to foundational knowledge; the principles here transcend particular implementations.

Capability trajectories remain uncertain. Breakthroughs may emerge from unexpected directions: transformers displaced RNNs in 2017 [@vaswani2017attention] despite decades of LSTM dominance, state space models achieve transformer performance with linear complexity, and alternative architectures could prove superior for specific workloads. **This uncertainty makes technology-agnostic principles more valuable, not less.**

Regardless of architectural outcomes, successful systems require efficient data processing pipelines handling large-scale datasets, scalable training infrastructure, optimized model deployment across heterogeneous hardware, robust operational practices ensuring high availability, and integrated safety frameworks.

The systematic approaches to distributed systems, efficient deployment, and robust operation covered throughout this textbook remain essential regardless of which architectural approaches prove most effective. Engineering principles transcend specific technologies, providing foundations for system construction across any technological trajectory.

## Fallacies and Pitfalls {#sec-emerging-systems-challenges-fallacies-pitfalls-8f1d}

Frontier ML systems development combines engineering complexity with uncertain capability trajectories, creating fertile ground for misconceptions. These fallacies and pitfalls represent resource misallocations, failed research directions, and planning errors that result from insufficient quantitative reasoning about constraints and uncertainties.

**Fallacy:** _Scaling solves all capability limitations._

Engineers assume that current AI limitations reflect insufficient model size and that scaling addresses capability gaps. In production, empirical scaling laws show diminishing returns on specific capability measures. While GPT-3's 175B parameters outperform GPT-2's 1.5B on benchmarks, the relationship between scale and specific capabilities (reasoning, factual accuracy, instruction following) is non-linear and task-dependent. Scaling GPT-3 to 17.5T parameters (100× larger) would require approximately $10B in training costs and consume 5 GWh of electricity, with uncertain capability improvements on tasks where current models struggle. As @sec-emerging-systems-scaling-limitations establishes, scaling provides predictable improvements on test loss but does not guarantee proportional improvements on specific capabilities. **Defensible approach**: Quantify expected capability improvements for specific tasks before committing resources, and hedge against uncertainty by investing in architectural alternatives alongside scaling.

**Fallacy:** _Compound systems are temporary workarounds._

Engineers assume that monolithic models will eventually render compound systems (combinations of models, tools, retrieval, databases) unnecessary. In production, computer science principles establish that modular architectures with specialized components enable capabilities difficult to achieve in monolithic systems. GPT-4's code generation accuracy improves from 48% to 89% when augmented with code execution, syntax checking, and test validation, improvements that scale would need to replicate at substantial cost [@openai2023gpt4]. As @sec-emerging-systems-compound-framework demonstrates, compound architectures enable independent optimization, graceful degradation, and debuggable behavior. **Defensible approach**: Invest in compound system infrastructure regardless of capability trajectory expectations, because modularity provides engineering benefits at any scale.

**Fallacy:** _Frontier systems require entirely new engineering principles._

Engineers assume that advanced ML systems necessitate abandoning existing practices for revolutionary approaches. In production, frontier systems extend rather than replace systems engineering fundamentals. Training GPT-4 required coordinating approximately 25,000 GPUs through sophisticated distributed systems engineering. The distributed training principles from @sec-distributed-training-systems (gradient synchronization, communication optimization, fault tolerance) become more critical, not less relevant, as system scale increases. Engineers who ignore distributed systems fundamentals recreate decades of hard-won lessons about consistency, fault tolerance, and performance optimization. **Defensible approach**: Master existing ML systems principles before pursuing novel approaches; these principles provide value regardless of which architectural directions prove fruitful.

**Pitfall:** _Treating biological efficiency as achievable target._

Teams assume that biological neural efficiency (20W for brain-scale computation) represents an achievable target for silicon systems. In production, biological and silicon substrates operate on fundamentally different physics with different strengths. Digital systems excel at precise arithmetic, reliable storage, and rapid communication; biological neurons achieve analog computation, massive parallelism, and low-power operation. Neuromorphic chips achieve 1000× efficiency gains for specific workloads (event-driven perception) but struggle with dense matrix operations where GPUs excel [@davies2018loihi]. **Defensible approach**: Use biological efficiency as a theoretical bound rather than engineering target; focus on practical efficiency improvements (1000× is achievable; 10⁶× is theoretical).

**Fallacy:** _Inference-time compute makes training efficiency irrelevant._

Engineers assume that inference-time scaling (as demonstrated by o1 and o3 models) shifts the bottleneck entirely from training to serving. In production, inference-time compute multiplies serving costs by 10-100× per query. A reasoning model performing 1000 inference steps per query consumes 1000× more compute than single-pass generation. Training efficiency determines the base capability from which reasoning begins; better base models require less inference-time compute for equivalent performance. **Defensible approach**: Optimize both training and inference efficiency; the cost-optimal balance depends on query volume and task complexity.

**Pitfall:** _Planning based on extrapolated scaling laws._

Teams commit multi-year roadmaps assuming scaling laws predict capability trajectories. In production, scaling laws describe test loss within tested regimes but provide uncertain guidance for capability prediction. Historical AI predictions show systematic overconfidence: expert surveys show median prediction errors of 10-100× on 5-10 year timescales [@grace2018will]. Scaling from GPT-3 to GPT-4 yielded capability improvements that loss metrics did not predict. **Defensible approach**: Plan for uncertainty through hedging and staged investment; avoid commitments that depend on specific capability timelines.

### Emerging Hardware for LLMs {#sec-emerging-systems-challenges-emerging-hardware-llms-3bf7}

The memory wall and interconnect bottlenecks examined throughout this volume have driven architectural innovation beyond the standard GPU/TPU paradigms. As established in @sec-compute, traditional accelerators are often memory-capacity constrained for inference-heavy workloads (like serving 100B+ parameter models) and bandwidth-constrained for the autoregressive decode phase. Recent research identifies four architectural opportunities specifically targeting the distinct requirements of Large Language Model (LLM) inference [@ma2024challenges].

#### High Bandwidth Flash (HBF) {#sec-emerging-systems-challenges-high-bandwidth-flash-hbf-2f2f}
Standard SSDs offer vast capacity but insufficient bandwidth for direct model serving, while HBM offers extreme bandwidth but at high cost and limited capacity. High Bandwidth Flash (HBF) proposes a middle tier: integrating flash memory with high-speed interfaces to serve as active memory for "cold" or "warm" model weights. By targeting bandwidths of 50-100 GB/s per device (10x standard SSDs), HBF systems could host trillion-parameter models on a single node, replacing the need for massive GPU clusters for inference-only workloads. This approach trades latency for capacity, making it ideal for the *prefill* phase or for routing-based Mixture of Experts (MoE) models where only a fraction of parameters are active per token.

#### Processing-Near-Memory (PNM) {#sec-emerging-systems-challenges-processingnearmemory-pnm-c02b}
Processing-in-Memory (PIM) has long promised to reduce data movement by computing inside DRAM banks. However, adoption has been slow due to manufacturing complexity and limited logic performance. Processing-Near-Memory (PNM) offers a pragmatic alternative: placing logic logic dies extremely close to memory dies (e.g., via 3D stacking or silicon interposers) without modifying the DRAM process itself. This approach specifically benefits the *decode* phase of LLM inference, which is almost entirely memory-bandwidth bound. By minimizing the physical distance and energy cost of moving weights to compute units, PNM architectures can achieve significantly higher tokens-per-second per watt than general-purpose GPUs.

#### 3D Memory-Logic Stacking {#sec-emerging-systems-challenges-3d-memorylogic-stacking-ec0a}
Vertical integration allows for interconnect densities orders of magnitude higher than planar 2D designs. By stacking memory layers directly on top of logic layers (using through-silicon vias or hybrid bonding), future accelerators can expose the entire internal bandwidth of memory banks to compute units. This effectively removes the "pin bottleneck" of off-chip memory interfaces. While thermal management remains a critical challenge (logic generates heat that degrades memory retention), this architecture represents the ultimate solution to the bandwidth-bound nature of autoregressive generation.

#### Low-Latency Interconnects for Inference {#sec-emerging-systems-challenges-lowlatency-interconnects-inference-55fc}
As discussed in @sec-distributed-training-systems, training systems optimize for *throughput* (bulk bandwidth). Inference systems, particularly for interactive applications, are sensitive to *latency*. The "Time-to-First-Token" (TTFT) metric is often dominated by the round-trip time of small control messages and synchronization barriers across the cluster. Future interconnects for inference clusters prioritize tail latency and message rate over raw bulk bandwidth. This includes hardware support for collective operations (like AllReduce) directly in the network switch or interface card (SmartNICs), reducing the software overhead of distributed inference coordination.

These directions signal a divergence in hardware specialization: "Training Chips" will continue to push maximum FLOPS and bulk bandwidth, while specialized "Inference Chips" may prioritize memory capacity (via HBF) and latency (via PNM/Interconnects) to economically serve the models that training clusters produce.

## Summary {#sec-emerging-systems-challenges-summary-c9a7}

This chapter examined emerging challenges in ML systems through quantitative analysis rather than speculative projection. We analyzed where current approaches face constraints (scaling limits, data exhaustion, energy bounds), what engineering patterns address those constraints (compound systems, multi-agent coordination), and how practitioners can make defensible decisions in the face of uncertainty.

The central insight is that **engineering trade-offs, not capability predictions, should guide infrastructure decisions**. Scaling laws describe behavior within tested regimes but provide uncertain guidance for extrapolation. Chinchilla optimal scaling identifies data constraints that affect current training decisions. Thermodynamic bounds (Landauer limit) establish fundamental efficiency limits independent of architecture. These quantifiable constraints enable evidence-based planning.

::: {.callout-important title="Key Takeaways"}
* **Scaling Has Limits**: Scaling laws describe test loss, not capabilities. Data exhaustion, diminishing returns, and energy costs create constraints that purely scaling-based strategies face. Chinchilla optimal scaling establishes that data quality often matters more than model size.
* **Compound Systems Are Engineering Solutions**: Integrating specialized components (retrieval, code execution, verification) addresses current limitations with measurable improvements. This is an engineering pattern, not a pathway to qualitatively different capabilities.
* **Uncertainty Requires Hedging**: Historical AI predictions show systematic overconfidence. Defensible infrastructure investments focus on capabilities with quantifiable returns (efficiency, reliability, orchestration) rather than betting on specific capability trajectories.
* **Thermodynamic Bounds Matter**: The Landauer limit establishes fundamental energy constraints. Current systems operate 10⁶× above this limit, suggesting improvement potential but also practical constraints on achieving brain-like efficiency.
* **Systems Engineering Remains Essential**: Distributed training, efficient serving, robust operations, and responsible governance provide value regardless of which architectural approaches prove most effective. These are high-confidence investments.
:::

Throughout Volume II, we traced the evolution of ML systems from single-machine implementations to global infrastructure. This chapter examined the constraints that frontier systems face and the engineering patterns for addressing them. The principles developed (quantitative analysis, uncertainty acknowledgment, trade-off reasoning) equip practitioners to navigate a rapidly evolving field where confident predictions are unreliable but sound engineering remains essential.

@sec-conclusion distills these lessons into the enduring principles that will guide your engineering decisions, regardless of which technologies emerge.
