---
bibliography: introduction.bib
---

# Introduction {#sec-vol2-introduction}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting advanced ML systems at scale, set on a crisp, clean white background. The image features interconnected data centers, distributed computing networks, and multiple deployment environments. Visual elements include server racks connected by glowing data streams, edge devices at the periphery, and governance frameworks represented as protective layers. Icons represent the three core themes: scale (expanding infrastructure), distribute (networked nodes), and govern (protective oversight). The style is clean, modern, and flat, suitable for a technical book._
:::

\noindent
![](images/png/cover_introduction.png)

:::

## Purpose {.unnumbered}

_Why must we master the engineering principles that enable machine learning systems to operate reliably at massive scale, across distributed infrastructure, and under responsible governance?_

The systems that transform industries and affect billions of lives cannot run on individual machines or small clusters. Production ML systems operate at scales where the fundamental nature of engineering challenges changes: communication dominates computation, failures become routine rather than exceptional, and architectural decisions carry consequences that reach far beyond technical performance metrics. At this frontier, the ability to coordinate learning across thousands of machines, to serve predictions to hundreds of millions of users with consistent reliability, and to ensure these systems operate fairly and sustainably determines which capabilities remain laboratory demonstrations and which reshape how humanity solves problems. Volume I established how to build, optimize, and operate ML systems; this volume extends those foundations to the production scale where most consequential AI systems must operate, introducing the three imperatives of scale, distribution, and governance that define advanced ML systems engineering. Mastering these principles determines your ability to architect the infrastructure and establish the practices that enable transformative AI capabilities to reach the people and applications that need them most.

::: {.callout-tip title="Learning Objectives"}

- Apply scaling principles to design ML infrastructure that handles datasets, models, and request volumes beyond single-machine capacity

- Design distributed training and inference systems that coordinate computation across thousands of machines while maintaining fault tolerance

- Evaluate the trade-offs between centralized and distributed architectures for different deployment contexts and constraints

- Implement security, privacy, and robustness measures that protect ML systems against adversarial conditions

- Establish governance frameworks that ensure ML systems operate fairly, sustainably, and accountably at production scale

- Synthesize the complete ML systems engineering methodology spanning both volumes to architect production-grade intelligent systems

:::

## The Scale Transformation {#sec-vol2-introduction-scale-transformation}

The history of machine learning is a history of scale. Each major capability leap has come not from algorithmic breakthroughs alone, but from the ability to apply computation at previously impossible scales. Understanding this progression reveals why systems engineering has become central to AI advancement.

Consider the trajectory of compute requirements. AlexNet (2012) trained on two GTX 580 GPUs for approximately 5 to 6 days [@krizhevsky2012imagenet]. BERT (2018) required 64 TPU chips for 4 days, roughly 6,144 chip hours [@devlin2018bert]. GPT-3 (2020) consumed an estimated 3.14×10²³ FLOPS during training, requiring thousands of V100 GPUs running for weeks [@brown2020language]. PaLM (2022) trained on 6,144 TPU v4 chips for roughly 60 days, consuming approximately 10²⁴ FLOPS [@chowdhery2022palm]. GPT-4 (2023) reportedly trained on approximately 25,000 A100 GPUs over 90 to 100 days [@openai2023gpt4]. This represents a 10,000 fold increase in training compute over a single decade.

::: {.callout-example title="Training Compute Evolution"}
```
Model           Year    GPUs/TPUs    Training Time    Estimated FLOPS
─────────────────────────────────────────────────────────────────────
AlexNet         2012    2 GPUs       5-6 days         ~10¹⁸
BERT-Large      2018    64 TPUs      4 days           ~10²⁰
GPT-3           2020    ~1000 GPUs   ~30 days         ~10²³
PaLM            2022    6144 TPUs    ~60 days         ~10²⁴
GPT-4           2023    ~25000 GPUs  ~100 days        ~10²⁵
```
:::

This exponential growth in compute requirements has transformed ML from a discipline where algorithms dominate to one where systems engineering determines success. An algorithm that cannot scale provides less value than a competent algorithm deployed across efficient infrastructure.

The transition from single-machine to distributed training introduces qualitative changes in system behavior. On a single GPU, training proceeds deterministically: the same code, data, and random seed produce identical results. At the scale of thousands of GPUs, new phenomena emerge. Network partitions can split clusters into groups that train independently, causing model divergence[^fn-network-partition]. Stragglers, workers that process data slower than peers due to hardware variation or thermal throttling, can bottleneck entire training runs[^fn-straggler-problem]. Hardware failures that occur once per machine-year become daily events when operating 10,000 machines, meaning systems must checkpoint frequently enough that losing a day's progress becomes acceptable rather than catastrophic[^fn-failure-rates].

[^fn-network-partition]: **Network Partition in Distributed Training**: When network failures isolate groups of workers, each group may continue training independently with different gradient updates, causing models to diverge. In synchronous distributed training, partitions typically halt progress until connectivity restores. In asynchronous training, partitions can cause split brain scenarios where different model versions evolve in parallel. Production systems implement partition detection through heartbeat mechanisms and quorum protocols. Google's approach with TPU pods uses dedicated high bandwidth interconnects (ICI at 4.5 TB/s bidirectional per chip) to minimize partition probability, while software level protocols detect and recover from rare partitions within minutes rather than allowing extended divergent training.

[^fn-straggler-problem]: **The Straggler Problem**: In synchronous distributed training, all workers must complete their computation before proceeding to the next iteration. If one worker runs 10% slower due to thermal throttling, memory pressure, or background processes, the entire cluster waits, reducing effective utilization. At 1,000 workers, the probability of at least one straggler per iteration approaches certainty. Solutions include backup workers that redundantly compute slow partitions, gradient coding that tolerates missing results, bounded staleness that allows proceeding with partial results, and careful hardware provisioning that minimizes variation. Meta's research found that stragglers can reduce large scale training throughput by 20 to 30% without mitigation strategies.

[^fn-failure-rates]: **Hardware Failure Rates at Scale**: Individual server components fail infrequently. Enterprise SSDs show annual failure rates of 0.5 to 2%; GPUs fail at roughly 1 to 2% annually under typical datacenter conditions, though rates can exceed 9% under intensive AI training workloads; memory DIMMs fail at 1 to 2% per year. These rates seem manageable until multiplied by scale. A cluster of 10,000 GPUs with 2% annual failure rate experiences 200 GPU failures per year, or roughly one every two days. A training run lasting 100 days will statistically encounter 50 or more GPU failures. Production systems implement automatic detection, workload migration, and checkpoint based recovery to handle failures as routine events rather than emergencies. Meta reported that during LLaMA training, they experienced hardware failures roughly every few hours, requiring automated recovery systems to maintain progress.

These scale-induced challenges explain why the largest AI organizations invest heavily in infrastructure. Meta's Research SuperCluster (RSC) contains 16,000 NVIDIA A100 GPUs connected by 200 Gb/s InfiniBand networking [@meta2022rsc]. Google's TPU v4 pods contain 4,096 chips with 1.1 exaFLOPS of aggregate compute capacity. Microsoft's Azure AI infrastructure spans multiple datacenters with tens of thousands of GPUs dedicated to AI workloads. These investments reflect that frontier AI capabilities require frontier infrastructure.

## Why Scale Changes Everything {#sec-vol2-introduction-why-scale-changes}

Scale is not merely a larger version of small. Systems that work perfectly at modest scale exhibit fundamentally different behaviors at production scale. Understanding these qualitative transitions prepares you for the engineering challenges examined throughout this volume.

### Communication Becomes Dominant

At small scale, computation dominates. Training a model on a single GPU spends most time performing matrix multiplications. Communication overhead, moving data between CPU and GPU memory, represents a small fraction of total time.

At large scale, communication often dominates. Distributed training requires synchronizing gradients across workers after each batch. For a model with 175 billion parameters using 32 bit gradients, each synchronization must transfer 700GB of data. Using the ring all-reduce algorithm[^fn-all-reduce] across 1,000 workers connected by 200 Gb/s InfiniBand, theoretical completion time approaches 56 seconds for the complete reduce-scatter and all-gather phases. Practical implementations achieve 60 to 80% of theoretical bandwidth, making communication consume a significant fraction of total training time even with optimized networks.

[^fn-all-reduce]: **Ring All-Reduce**: The dominant collective communication algorithm for distributed training gradient synchronization. In ring all-reduce, N workers arrange logically in a ring. Each worker sends 1/N of its gradient to its neighbor, which adds the received gradient to its own before forwarding. After N minus 1 steps, each worker has the sum of all gradients for 1/N of parameters. A second ring pass distributes the complete sum to all workers. The algorithm achieves optimal bandwidth utilization. Total data transferred equals 2(N minus 1)/N times the gradient size, approaching 2× regardless of worker count. For 1,000 workers with 700GB of gradients and 200 Gb/s links, theoretical completion time is approximately 2 × 700GB / 200 Gb/s = 56 seconds for the full reduce scatter plus all gather. Practical implementations achieve 60 to 80% of theoretical bandwidth, making communication a significant fraction of large scale training time.

This ratio explains why distributed training systems optimize communication aggressively. Gradient compression reduces transfer volume by 10 to 100× at the cost of some accuracy. Overlapping communication with computation hides transfer latency during the next batch's forward pass. Hierarchical aggregation reduces cross rack traffic by combining gradients locally first. These optimizations, unnecessary at small scale, become essential at production scale.

::: {.callout-definition title="Communication-Computation Ratio"}
***Communication-Computation Ratio*** describes the relative time spent transferring data versus performing computation in distributed systems. A ratio of 1 to 1 means equal time on each; higher ratios indicate communication bound workloads. Modern distributed training systems typically achieve ratios between 1 to 3 and 1 to 1, making communication optimization critical for efficiency. The ratio depends on model size (larger models have more gradients to synchronize), batch size (larger batches amortize communication over more computation), and network bandwidth (faster networks reduce communication time).
:::

### Failure Becomes Routine

At small scale, failure is exceptional. A well maintained server might run for years without hardware issues. Software bugs, once fixed, stay fixed. Administrators can manually investigate and remediate problems.

At large scale, failure becomes statistical certainty. With 10,000 GPUs, multiple failures occur weekly. With 100,000 concurrent user sessions, software edge cases that occur one in a million times happen hundreds of times daily. Manual intervention becomes impossible; systems must self heal.

This transition requires fundamental architectural changes. Small scale systems optimize for the common case and handle failures through manual recovery. Large scale systems must design for failure from the beginning:

- **Checkpointing**: Saving model state frequently enough that losing hours of progress is acceptable when failures occur
- **Redundancy**: Running extra workers that can absorb failed workers' tasks without restart
- **Isolation**: Containing failures so that one component's crash does not cascade through the system
- **Detection**: Monitoring that identifies failures within seconds
- **Recovery**: Automated procedures that restore service without human intervention

### Heterogeneity Emerges

At small scale, systems are homogeneous. A single GPU training job runs on one type of hardware with one software configuration. Behavior is predictable and reproducible.

At large scale, heterogeneity becomes unavoidable. A fleet of 10,000 GPUs contains multiple hardware generations purchased over years. Different racks have different thermal characteristics affecting clock speeds. Software updates roll out gradually, creating version skew. Network paths vary in latency and bandwidth.

This heterogeneity creates engineering challenges absent at small scale. Load balancing must account for hardware capability differences. Gradient aggregation must handle workers completing at different rates. Inference routing must direct requests to servers with appropriate model versions. Testing must verify behavior across the combinatorial explosion of configuration variants.

## Why Distribution is Hard {#sec-vol2-introduction-why-distribution-hard}

Distribution introduces challenges beyond those of scale. Coordinating computation across physically separated machines connected by finite-bandwidth, non-zero-latency networks creates fundamental constraints that no amount of engineering cleverness can eliminate.

### The CAP Theorem Reality

The CAP theorem[^fn-cap-theorem] establishes that distributed systems can provide at most two of three properties: consistency (all nodes see the same data), availability (every request receives a response), and partition tolerance (the system continues operating despite network failures). Since network partitions can always occur, practical systems must choose between consistency and availability during partitions.

[^fn-cap-theorem]: **CAP Theorem**: Proven by Eric Brewer and formalized by Gilbert and Lynch [@gilbert2002brewer], the CAP theorem states that a distributed data store cannot simultaneously provide more than two of Consistency (every read receives the most recent write), Availability (every request receives a non-error response), and Partition tolerance (the system continues operating despite network partitions). Since partitions are unavoidable in distributed systems, the practical choice is between CP (consistent but potentially unavailable during partitions) and AP (available but potentially inconsistent). Distributed ML systems make different choices. Synchronous training is CP (training halts during partitions to maintain model consistency), while asynchronous training is AP (training continues with potentially stale gradients). Understanding this trade-off informs architecture decisions throughout distributed ML systems.

ML systems make different choices depending on context. Synchronous distributed training chooses consistency: all workers see the same model state, but training halts if any worker becomes unreachable. Asynchronous training chooses availability: training continues even with stragglers or failures, but workers may operate on slightly stale model versions. Federated learning often chooses availability with eventual consistency: edge devices train locally and periodically synchronize, accepting temporary inconsistency for continuous operation.

### Coordination Overhead

Distributed systems require coordination that consumes resources. Every synchronization point introduces latency. Every consensus protocol requires network round-trips. Every distributed lock limits parallelism.

Consider the overhead of distributed training synchronization. Each training iteration requires:

1. Forward pass computation (parallelizable)
2. Loss computation (local to each worker)
3. Backward pass computation (parallelizable)
4. Gradient aggregation (requires network communication)
5. Parameter update (can parallelize with next iteration)

Steps 1-3 and 5 parallelize efficiently. Step 4, gradient aggregation, requires global coordination. Even with optimized all-reduce algorithms and high-bandwidth networks, this coordination can consume a substantial fraction of total training time for large models [@shoeybi2019megatron]. This overhead is fundamental: no algorithm can aggregate globally distributed values without communication proportional to the data volume.

### Edge Distribution Complexity

Datacenter distribution is challenging but controlled. All machines run in managed facilities with reliable power, cooling, and networking. Administrators can access any machine for diagnosis and repair.

Edge distribution amplifies every challenge. Billions of smartphones, IoT devices, and embedded systems operate in uncontrolled environments with unreliable connectivity, limited power, and heterogeneous capabilities. Google's Gboard keyboard runs on over 1 billion Android devices, each potentially participating in federated learning, a technique where models train across distributed devices without centralizing data, to improve predictions [@hard2018federated].

Edge distribution introduces unique constraints:

- **Intermittent connectivity**: Devices may be reachable only when on WiFi and charging
- **Heterogeneous hardware**: Model must run efficiently across devices spanning 100× performance range
- **Privacy requirements**: Raw data cannot leave devices, requiring on device processing
- **Update complexity**: Pushing model updates to billions of devices takes weeks
- **Monitoring limitations**: Cannot install arbitrary diagnostics on user devices

These constraints require architectural approaches fundamentally different from datacenter ML. Federated learning aggregates model updates without collecting data. On-device inference optimizes for varied hardware capabilities. Differential privacy, which adds calibrated noise to protect individual data points while preserving aggregate statistical properties, provides mathematical guarantees about information leakage. These techniques, largely unnecessary for centralized ML, become essential for edge deployment.

## Why Governance Matters at Scale {#sec-vol2-introduction-why-governance-matters}

Scale amplifies impact. A bug in a small system affects few users; a bug in a system serving billions affects society. This amplification creates governance requirements that small-scale systems can ignore.

### Security Threats Intensify

ML systems face unique security threats beyond traditional software vulnerabilities[^fn-ml-security-threats]. Model extraction attacks can steal proprietary models through query access. Researchers demonstrated extracting functionally equivalent copies of production ML models using only API access [@tramer2016stealing]. Membership inference attacks can determine whether specific data was used in training, creating privacy violations from seemingly innocuous model access [@shokri2017membership]. Adversarial examples can cause misclassification with perturbations imperceptible to humans, demonstrated against production systems including Tesla Autopilot and content moderation systems [@goodfellow2014explaining].

[^fn-ml-security-threats]: **ML-Specific Security Threats**: Traditional software security focuses on preventing unauthorized code execution and data access. ML systems face additional threats that exploit the learned behavior of models. Data poisoning attacks inject malicious training examples that cause targeted misbehavior. Researchers demonstrated that controlling 0.1% of training data can implant backdoors that cause misclassification on specific inputs. Model inversion attacks reconstruct training data from model access. Facial recognition models can leak enough information to reconstruct recognizable images of training individuals. Adversarial reprogramming hijacks models to perform unintended tasks through specially crafted inputs. These threats require defenses beyond traditional security including differential privacy, certified robustness, and continuous monitoring of model behavior in production.

At production scale, these threats become economically attractive to attackers. A model serving millions of users represents substantial intellectual property worth stealing. A model making consequential decisions (loans, hiring, content moderation) offers high-value manipulation targets. A model processing sensitive data (health records, financial information) provides valuable inference targets.

Defense requires systematic approaches including access controls that limit query rates and patterns, output perturbation that provides differential privacy guarantees, adversarial training that improves robustness to perturbations, and monitoring that detects anomalous query patterns indicative of attacks. These defenses impose overhead unnecessary for small-scale systems but essential for production deployment.

### Regulatory Requirements Emerge

Systems operating at scale attract regulatory attention. The European Union's General Data Protection Regulation (GDPR) imposes obligations for systems processing EU residents' data, including the right to explanation for automated decisions. The EU AI Act establishes risk-based requirements for AI systems, with high-risk applications (healthcare, employment, law enforcement) requiring conformity assessments, human oversight, and accuracy documentation. Similar regulations exist or are developing in jurisdictions worldwide.

Compliance requires technical capabilities:

- **Audit trails**: Recording inputs, outputs, and model versions for every decision
- **Explanation generation**: Producing human-interpretable justifications for model outputs
- **Consent management**: Tracking and honoring user preferences for data usage
- **Data deletion**: Removing specific users' data from training sets and retraining affected models
- **Bias testing**: Evaluating model performance across protected demographic groups

These capabilities impose engineering costs absent for unregulated systems but mandatory for production deployment in regulated contexts.

### Societal Impact Demands Responsibility

Beyond legal compliance, systems affecting billions of users carry ethical obligations. Recommendation algorithms shape public discourse. Researchers have documented how engagement-optimizing systems can amplify misinformation and polarization [@ribeiro2020auditing]. Hiring algorithms affect employment opportunities. Amazon discontinued an AI recruiting tool that exhibited bias against women [@dastin2018amazon]. Content moderation systems determine what speech is visible. Errors can suppress legitimate expression or fail to remove harmful content.

Responsible engineering practices address these impacts:

- **Fairness evaluation**: Testing for disparate impact across demographic groups before deployment
- **Impact assessment**: Analyzing potential harms before launching new capabilities
- **Human oversight**: Maintaining human review for high-stakes decisions
- **Incident response**: Processes for rapidly addressing identified harms
- **Transparency**: Documentation of system capabilities, limitations, and decision factors

::: {.callout-definition title="Responsible AI"}
***Responsible AI*** encompasses the practices, frameworks, and technical approaches that ensure ML systems operate in ways that are fair, transparent, accountable, and beneficial to society. Responsible AI addresses the ethical implications of automated decision making, the potential for algorithmic bias, the environmental impact of large scale computation, and the governance structures needed to maintain human oversight over consequential systems. Unlike traditional software quality assurance focused on correctness and performance, responsible AI evaluates systems against societal values and human rights considerations.
:::

## Bridging from Volume I {#sec-vol2-introduction-bridging}

Volume I established the foundations that this textbook extends. If you are beginning here, this section provides essential context. If you completed Volume I, consider this a brief reminder before we proceed to advanced topics.

**The AI Triangle** provides the organizing framework for understanding ML systems. Every machine learning system comprises three interdependent components: data that guides behavior, algorithms that learn patterns, and computational infrastructure that enables both training and inference. These components exist in dynamic tension. Larger models require more data and compute. Larger datasets enable more sophisticated models but demand storage and processing infrastructure. At the scales examined in this volume, these interdependencies intensify. Distributed training requires coordinating the AI Triangle's components across thousands of machines rather than optimizing them on a single system.

**The Five-Pillar Framework** structures the ML systems engineering discipline into interconnected domains: data engineering establishes pipelines for collecting, processing, and serving training and inference data; model development encompasses architecture design, training procedures, and validation methodologies; optimization techniques compress and accelerate models for deployment constraints; deployment infrastructure spans cloud platforms, edge devices, and embedded systems; and operations practices ensure systems remain reliable, secure, and effective throughout their lifecycle. This textbook extends each pillar to production scale, where the engineering challenges multiply.

**The Six Systems Engineering Principles** provide guidance for design decisions across all five pillars:

1. *Measure Everything*: At scale, measurement itself becomes a systems challenge requiring distributed monitoring infrastructure
2. *Design for 10x Scale*: Production deployment reveals whether 10× design was adequate or optimistic
3. *Optimize the Bottleneck*: Scale shifts bottlenecks from compute to communication to coordination
4. *Plan for Failure*: At scale, failure is not exceptional but routine
5. *Design Cost-Consciously*: Scale makes efficiency improvements worth millions of dollars
6. *Co-Design for Hardware*: Distributed hardware introduces network topology and storage hierarchy as co-design considerations

Volume I taught you to build, optimize, and operate ML systems. This textbook teaches you to scale, distribute, and govern them.

## The Structure of This Textbook {#sec-vol2-introduction-structure}

This textbook organizes around the three imperatives, progressing from infrastructure foundations through distribution techniques to governance practices. @tbl-vol2-structure summarizes the four-part structure.

+--------------------------------+-----------------------------------+-----------------------------------------+
| **Part**                       | **Theme**                         | **Key Chapters**                        |
+:===============================+:==================================+:========================================+
| **I: Foundations of Scale**    | **Scale**: Building blocks        | Infrastructure, Storage, Communication  |
+--------------------------------+-----------------------------------+-----------------------------------------+
| **II: Distributed Systems**    | **Distribute**: Coordinating      | Distributed Training, Fault Tolerance,  |
|                                | computation across machines       | Inference, Edge Intelligence            |
+--------------------------------+-----------------------------------+-----------------------------------------+
| **III: Production Challenges** | **Scale + Distribute**: Operating | On-Device Learning, Privacy & Security, |
|                                | distributed systems               | Robust AI, Operations at Scale          |
+--------------------------------+-----------------------------------+-----------------------------------------+
| **IV: Responsible Deployment** | **Govern**: Ensuring beneficial   | Responsible AI, Sustainable AI,         |
|                                | societal impact                   | AI for Good, Frontiers                  |
+--------------------------------+-----------------------------------+-----------------------------------------+

: The four parts progress from infrastructure foundations through distributed systems techniques to production challenges and responsible deployment. {#tbl-vol2-structure}

### Part I: Foundations of Scale

Before systems can scale, they require infrastructure designed for scale.

**Infrastructure** examines how datacenters, accelerators, and orchestration systems enable large-scale ML. You will understand the hardware and software stack that makes distributed ML possible, from GPU clusters to high-bandwidth networks and container orchestration.

**Storage Systems** addresses data infrastructure at scale. Training datasets for modern models exceed any single storage system, requiring distributed architectures optimized for ML access patterns. You will understand storage hierarchies, data formats, and caching strategies that enable efficient data serving.

**Communication Systems** analyzes networking for distributed ML. Collective operations like all-reduce dominate training communication, requiring network architectures optimized for these patterns. You will understand topologies, algorithms, and optimization techniques that minimize communication overhead.

### Part II: Distributed Systems

With infrastructure foundations established, distribution techniques enable ML across multiple machines.

**Distributed Training** develops techniques for training models across devices and machines. Data parallelism, model parallelism, and pipeline parallelism each address different constraints. You will understand when each applies, how they combine, and what synchronization and consistency they require.

**Fault Tolerance** ensures distributed systems continue operating despite failures. At production scale, failures occur daily. Checkpointing, redundancy, and recovery procedures enable training and inference to continue despite inevitable component failures.

**Inference at Scale** examines serving systems that deliver predictions with low latency and high throughput. Request routing, load balancing, autoscaling, and geographic distribution enable production inference to meet demanding performance requirements.

**Edge Intelligence** extends ML to resource-constrained devices at the network edge. Model compression, runtime optimization, and edge-cloud coordination enable deployment where centralized inference is infeasible.

### Part III: Production Challenges

Distribution creates operational challenges that require systematic approaches.

**On-Device Learning** enables models to learn from local data without centralized collection. Federated learning and on-device fine-tuning enable privacy-preserving personalization, but introduce challenges for convergence, coordination, and quality assurance.

**Privacy and Security** addresses threats specific to ML systems. Model extraction, membership inference, adversarial examples, and data poisoning require defenses including differential privacy, secure computation, and adversarial training.

**Robust AI** ensures reliable operation under uncertainty. Distribution shift, out-of-distribution inputs, and novel situations require systems that recognize the limits of their competence and respond appropriately.

**Operations at Scale** encompasses practices that maintain large ML systems in production. Monitoring, deployment, and incident response adapt for ML-specific requirements at production scale.

### Part IV: Responsible Deployment

Technical excellence is insufficient for systems affecting human lives at scale.

**Responsible AI** addresses fairness, transparency, and accountability. Systems must operate equitably across populations, enable stakeholder understanding, and ensure harms can be identified and remediated.

**Sustainable AI** addresses environmental impact. Efficient algorithms, appropriate model sizing, and renewable energy sourcing minimize the environmental footprint of large-scale ML.

**AI for Good** demonstrates how ML systems address societal challenges. Applications in healthcare, climate, education, and accessibility illustrate how systems engineering principles enable beneficial impact.

**Frontiers** examines emerging directions including foundation models and novel computing paradigms. Understanding these trajectories prepares you for continued learning as the field evolves.

## How to Use This Textbook {#sec-vol2-introduction-how-to-use}

This textbook supports multiple reading paths.

**Sequential readers** should proceed through chapters in order. Each chapter builds on previous material, progressing logically from infrastructure through distribution to governance.

**Practitioners with specific needs** can approach chapters selectively. The structure overview above identifies what each chapter covers. If distributed training is your immediate challenge, start with Part II. If security is paramount, prioritize the privacy and security chapter.

**Readers beginning here** will find the bridging section above provides essential context. Where deeper background is needed, references to Volume I chapters are provided.

**Online readers** can navigate the unified web presentation of both volumes, following cross-references between related topics.

**Print readers** will find this textbook designed as a standalone book, with sufficient context to proceed without constant reference to Volume I.

## The Journey Ahead {#sec-vol2-introduction-journey-ahead}

Volume I concluded with six systems engineering principles and a vision of building ML systems that matter. This textbook extends that vision to the scale at which most consequential ML systems operate.

The transition from building systems that work to building systems that scale, distribute, and govern responsibly represents a significant professional development. The ML systems that will define this decade, the foundation models serving hundreds of millions of users, the edge deployments spanning billions of devices, the AI systems making consequential decisions about human lives, all require the capabilities this textbook develops.

You will learn to architect infrastructure that processes petabytes of training data across tens of thousands of accelerators. You will design inference systems that serve billions of predictions with consistent low latency. You will implement security, privacy, and robustness measures that protect systems against adversarial conditions. You will establish governance practices that ensure systems operate fairly, sustainably, and accountably.

The engineering challenges are substantial, as is the impact of getting them right.

The path forward begins with infrastructure: the datacenters, accelerators, storage systems, and communication networks that make everything else possible. When you understand how this infrastructure enables distributed ML, you will be prepared to build systems that leverage these capabilities effectively.

Let us begin.

::: { .quiz-end }
:::
