---
bibliography: introduction.bib
---

<!--
================================================================================
VOLUME II EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY
================================================================================

PHILOSOPHY: This textbook teaches generalizable ML systems principles, NOT
"LLM infrastructure." Every technique should apply across model architectures.
Students who master these concepts can work on any production ML system.

WHEN WRITING CONTENT, ENSURE EXAMPLES SPAN THESE MODEL TYPES:

| Model Type          | Unique Systems Challenges                              |
|---------------------|--------------------------------------------------------|
| LLMs/Transformers   | KV cache memory, attention compute scaling, long       |
|                     | context, autoregressive decoding latency               |
| Recommendation      | Massive embedding tables (trillion+ params), feature   |
|                     | lookup latency, real-time updates, CTR prediction      |
| Vision (CNN/ViT)    | Data augmentation pipelines, batch size sensitivity,   |
|                     | spatial locality, multi-resolution processing          |
| Scientific/GNN      | Irregular compute patterns, graph partitioning,        |
|                     | sparse operations, physics constraints                 |
| Multimodal          | Cross-encoder coordination, modality-specific          |
|                     | preprocessing, heterogeneous compute requirements      |
| Speech/Audio        | Streaming inference, variable-length sequences,        |
|                     | real-time latency constraints                          |

CHAPTER-SPECIFIC GUIDANCE:

DISTRIBUTED TRAINING (@sec-distributed-training):

- Data parallelism: ResNet, BERT, recommendation models
- Model parallelism: GPT-3, Megatron, DLRM embedding sharding
- Pipeline parallelism: GPT, T5, large vision models
- Include: DLRM has DIFFERENT parallelism needs (embedding-heavy vs compute-heavy)

INFERENCE AT SCALE (@sec-inference-at-scale):

- Batching strategies differ: LLMs (continuous batching) vs RecSys (feature lookup)
- Latency profiles: recommendation <10ms, LLMs 100ms-seconds, vision 20-50ms
- Include ensemble serving (multiple models in pipeline)
- RecSys is the DOMINANT inference workload by volume at Meta, TikTok, Netflix

COMMUNICATION (@sec-communication):

- AllReduce for dense gradients (vision, transformers)
- AlltoAll for embedding lookups (recommendation)
- Gradient compression benefits vary by model type

STORAGE (@sec-storage):

- Feature stores: critical for RecSys, less relevant for LLMs
- Checkpoint sizes: LLMs (TB), vision (GB), recommendation (embedding tables)
- Data lakes: training data diversity matters

FAULT TOLERANCE (@sec-fault-tolerance):

- Checkpointing frequency depends on model size and iteration time
- Recommendation systems: real-time feature stores need different recovery
- Training vs serving fault tolerance requirements differ

EDGE INTELLIGENCE (@sec-edge-intelligence):

- On-device: vision (phones), speech (assistants), NLP (keyboards)
- Federated learning works differently for different model types
- Model compression: quantization effects vary by architecture

QUANTITATIVE DIVERSITY CHECKLIST:

- [ ] Does this section have examples from at least 2-3 model types?
- [ ] Are performance numbers given for multiple architectures?
- [ ] Would a RecSys engineer find this content applicable to their work?
- [ ] Would a vision ML engineer find this content applicable?
- [ ] Are we avoiding LLM-centric framing of general concepts?

EXAMPLE OF GOOD FRAMING:
  BAD:  "Training GPT-3 on a single V100 would require 355 years"
  GOOD: "Training frontier models—whether GPT-3 (175B params), ViT-22B
         (vision), or DLRM (trillion-param embeddings)—would require
         centuries on single devices, making distributed approaches essential."

EXAMPLE OF INCLUSIVE CASE STUDIES:
  - Meta DLRM: recommendation at scale
  - Google BERT/T5: NLP training infrastructure
  - Tesla Autopilot: vision model deployment
  - Spotify: audio/recommendation hybrid
  - TikTok: multimodal recommendation

================================================================================
-->

# Introduction {#sec-vol2-introduction}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting advanced ML systems at scale, set on a crisp, clean white background. The image features interconnected data centers, distributed computing networks, and multiple deployment environments. Visual elements include server racks connected by glowing data streams, edge devices at the periphery, and governance frameworks represented as protective layers. Icons represent the three core themes: scale (expanding infrastructure), distribute (networked nodes), and govern (protective oversight). The style is clean, modern, and flat, suitable for a technical book._
:::

\noindent
![](images/png/cover_introduction.png)

:::

## Purpose {.unnumbered}

_Why must we master the engineering principles that enable machine learning systems to operate reliably at massive scale, across distributed infrastructure, and under responsible governance?_

The systems that transform industries and affect billions of lives cannot run on individual machines or small clusters. Production ML systems operate at scales where the fundamental nature of engineering challenges changes: communication dominates computation, failures become routine rather than exceptional, and architectural decisions carry consequences that reach far beyond technical performance metrics. At this frontier, the ability to coordinate learning across thousands of machines, to serve predictions to hundreds of millions of users with consistent reliability, and to ensure these systems operate fairly and sustainably determines which capabilities remain laboratory demonstrations and which reshape how humanity solves problems. This textbook addresses the production scale where most consequential AI systems must operate, introducing the three imperatives of scale, distribution, and governance that define advanced ML systems engineering. This treatment assumes familiarity with single-machine ML systems, including model architectures, optimization techniques, and deployment basics. Mastering these principles determines your ability to architect the infrastructure and establish the practices that enable transformative AI capabilities to reach the people and applications that need them most.

::: {.callout-tip title="Learning Objectives"}

- Explain why ML systems exhibit qualitatively different behaviors at production scale compared to single-machine systems, including the dominance of communication over computation and the transition from exceptional to routine failure

- Analyze the historical evolution of ML compute requirements from AlexNet to frontier models, calculating the implications for infrastructure design

- Compare synchronous versus asynchronous distributed training approaches using the CAP theorem framework to evaluate consistency-availability trade-offs

- Differentiate between datacenter distribution and edge distribution challenges, identifying unique constraints for each deployment context

- Classify ML security threats (model extraction, membership inference, adversarial examples) and explain why scale amplifies their economic attractiveness to attackers

- Apply the AI Triad (data, algorithms, infrastructure) to reason about distributed systems challenges, understanding how optimizing any single element in isolation leads to suboptimal outcomes

- Evaluate the textbook's five-part structure (Foundations of Scale, Distributed Training, Deployment at Scale, Production Concerns, Responsible AI at Scale) to select appropriate chapters for specific engineering challenges

:::

## The Scale Transformation {#sec-vol2-introduction-scale-transformation}

The history of machine learning is a history of scale. Each major capability leap has come not from algorithmic breakthroughs alone, but from the ability to apply computation at previously impossible scales. Understanding this progression reveals why systems engineering has become central to AI advancement.

Consider the trajectory of compute requirements. AlexNet (2012) trained on two GTX 580 GPUs for approximately 5-6 days [@krizhevsky2012imagenet]. BERT (2018) required 64 TPU[^fn-tpu] chips for 4 days, roughly 6,144 chip hours [@devlin2018bert].

[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom-designed ASIC optimized specifically for neural network computation. Unlike general-purpose GPUs, TPUs implement a systolic array architecture that excels at the matrix multiplications dominating deep learning workloads. TPU v4 chips deliver approximately 275 TFLOPS of bfloat16 performance at 175 watts, roughly 2.5× the performance per watt of contemporary GPUs for transformer training. The latest TPU v5p chips are interconnected via Inter-Core Interconnect (ICI) providing 4,800 Gb/s of bandwidth per chip, enabling tight coupling for distributed training across TPU pods containing thousands of chips. While GPUs offer broader flexibility for varied workloads, TPUs demonstrate how hardware-software co-design enables significant efficiency gains for specific computational patterns.

GPT-3 (2020) consumed an estimated 3.14×10²³ FLOPS during training, requiring thousands of V100 GPUs running for weeks [@brown2020language]. PaLM (2022) trained on 6,144 TPU v4 chips for roughly 60 days, consuming approximately 10²⁴ FLOPS [@chowdhery2022palm]. GPT-4 (2023) reportedly trained on approximately 25,000 A100 GPUs over 90-100 days [@openai2023gpt4]. This progression represents approximately a 10-million-fold increase in training compute over a single decade, from roughly 10¹⁸ FLOPS for AlexNet to 10²⁵ FLOPS for GPT-4 [@sevilla2022compute; @amodei2018ai].

::: {.callout-example title="Training Compute Evolution"}
```
Model           Year    GPUs/TPUs    Training Time    Estimated FLOPS
─────────────────────────────────────────────────────────────────────
AlexNet         2012    2 GPUs       5-6 days         ~10¹⁸
BERT-Large      2018    64 TPUs      4 days           ~10²⁰
GPT-3           2020    ~1000 GPUs   ~30 days         ~10²³
PaLM            2022    6144 TPUs    ~60 days         ~10²⁴
GPT-4           2023    ~25000 GPUs  ~100 days        ~10²⁵
```
:::

This exponential growth in compute requirements has transformed ML from a discipline where algorithms dominate to one where systems engineering determines success. A sophisticated algorithm that cannot scale often provides less practical value than a simpler algorithm deployed efficiently across scalable infrastructure.

The transition from single-machine to distributed training introduces qualitative changes in system behavior. On a single GPU, training proceeds deterministically: the same code, data, and random seed produce identical results. At the scale of thousands of GPUs, new phenomena emerge. Network partitions can split clusters into groups that train independently, causing model divergence[^fn-network-partition]. Stragglers (workers that process data slower than peers due to hardware variation or thermal throttling) can bottleneck entire training runs[^fn-straggler-problem]. Hardware failures that occur once per machine-year become daily events when operating 10,000 machines. Systems must checkpoint frequently enough that losing a day's progress becomes acceptable rather than catastrophic[^fn-failure-rates].

[^fn-network-partition]: **Network Partition in Distributed Training**: When network failures isolate groups of workers, each group may continue training independently with different gradient updates, causing models to diverge. In synchronous distributed training, partitions typically halt progress until connectivity restores. In asynchronous training, partitions can cause split brain scenarios where different model versions evolve in parallel. Production systems implement partition detection through heartbeat mechanisms and quorum protocols. Google's approach with TPU pods uses dedicated high bandwidth interconnects (ICI at 4.5 TB/s bidirectional per chip) to minimize partition probability, while software level protocols detect and recover from rare partitions within minutes rather than allowing extended divergent training.

[^fn-straggler-problem]: **The Straggler Problem**: In synchronous distributed training, all workers must complete their computation before proceeding to the next iteration. If one worker runs 10% slower due to thermal throttling, memory pressure, or background processes, the entire cluster waits, reducing effective utilization. At 1,000 workers, the probability of at least one straggler per iteration approaches certainty. Solutions include backup workers that redundantly compute slow partitions, gradient coding that tolerates missing results, bounded staleness that allows proceeding with partial results, and careful hardware provisioning that minimizes variation. Meta's research found that stragglers can reduce large-scale training throughput by 20-30% without mitigation strategies.

[^fn-failure-rates]: **Hardware Failure Rates at Scale**: Individual server components fail infrequently. Enterprise SSDs show annual failure rates of 0.5-2% [@pinheiro2007failure]; GPUs fail at roughly 1-2% annually under typical datacenter conditions, though rates can exceed 9% under intensive AI training workloads; memory DIMMs fail at 1-2% per year [@schroeder2009dram]. These rates seem manageable until multiplied by scale. A cluster of 10,000 GPUs with 2% annual failure rate experiences 200 GPU failures per year, or roughly one every two days. A training run lasting 100 days will statistically encounter 50 or more GPU failures. Production systems implement automatic detection, workload migration, and checkpoint-based recovery to handle failures as routine events rather than emergencies. Meta reported that during LLaMA 3 training on 16,384 H100 GPUs, they experienced hardware failures roughly every three hours, requiring automated recovery systems to maintain over 90% effective training time [@dubey2024llama3].

These scale-induced challenges explain why the largest AI organizations invest heavily in infrastructure. Meta's Research SuperCluster (RSC) contains 16,000 NVIDIA A100 GPUs connected by 200 Gb/s InfiniBand[^fn-infiniband] networking [@meta2022rsc]. Google's TPU v4 pods contain 4,096 chips with 1.1 exaFLOPS[^fn-exaflops] of aggregate compute capacity. Microsoft's Azure AI infrastructure spans multiple datacenters with tens of thousands of GPUs dedicated to AI workloads. These investments reflect that frontier AI capabilities require frontier infrastructure.

[^fn-infiniband]: **InfiniBand**: A high-performance, low-latency networking technology purpose-built for data centers and supercomputing. While standard Ethernet reaches 100-400 Gb/s with 1-5 microsecond latencies, InfiniBand achieves 200-400 Gb/s with sub-microsecond latencies through features including Remote Direct Memory Access (RDMA), which bypasses the operating system to transfer data directly between application memory on different machines. For distributed ML training, InfiniBand's RDMA support enables NVIDIA Collective Communications Library (NCCL) to achieve near-optimal gradient synchronization bandwidth. The 200 Gb/s HDR InfiniBand links common in 2024 AI clusters deliver approximately 25 GB/s of usable bandwidth, while the newer 400 Gb/s NDR generation reaches 50 GB/s. These speeds determine whether training becomes communication-bound or compute-bound for large models.

[^fn-exaflops]: **ExaFLOPS**: One quintillion (10^18) floating-point operations per second. For context, a single NVIDIA H100 GPU delivers approximately 2,000 TFLOPS (teraFLOPS, or 10^12 FLOPS) of FP16 compute; achieving 1 exaFLOPS thus requires roughly 500 such GPUs operating in parallel with perfect efficiency. Google's TPU v4 pods reaching 1.1 exaFLOPS represent one of the first single-system installations to cross this threshold. The scale progression from megaFLOPS (10^6, 1980s workstations) through gigaFLOPS (10^9, 1990s servers), teraFLOPS (10^12, 2000s GPUs), petaFLOPS (10^15, 2010s supercomputers), to exaFLOPS (10^18, 2020s AI clusters) illustrates the exponential growth enabling modern ML capabilities.

## Why Scale Changes Everything {#sec-vol2-introduction-why-scale-changes}

Scale is not merely a larger version of small. Systems that work perfectly at modest scale exhibit qualitatively different behaviors at production scale. Understanding these transitions prepares you for the engineering challenges examined throughout this volume.

### Communication Becomes Dominant

At small scale, computation dominates. Training a model on a single GPU spends most time performing matrix multiplications. Communication overhead (moving data between CPU and GPU memory) represents a small fraction of total time.

At large scale, communication often dominates. Distributed training requires synchronizing gradients across workers after each batch. For a model with 175 billion parameters using 32-bit gradients, each synchronization must transfer 700GB of data (175 billion parameters × 4 bytes per parameter). Using the ring all-reduce algorithm[^fn-all-reduce] across 1,000 workers connected by 200 Gb/s InfiniBand (25 GB/s), theoretical completion time for the full synchronization approaches 56 seconds (2 × 700 GB ÷ 25 GB/s). Practical implementations achieve 60-80% of theoretical bandwidth, making communication consume a significant fraction of total training time even with optimized networks.

[^fn-all-reduce]: **Ring All-Reduce**: An efficient collective communication algorithm for gradient synchronization that achieves near-optimal bandwidth utilization regardless of worker count. For large models, this synchronization can consume a significant fraction of training time. @sec-communication examines AllReduce algorithms and their trade-offs.

This ratio explains why distributed training systems optimize communication aggressively. Gradient compression[^fn-gradient-compression] reduces transfer volume by 10× to 100× at the cost of some accuracy [@lin2018deep]. Overlapping communication with computation hides transfer latency during the next batch's forward pass. Hierarchical aggregation reduces cross-rack traffic by combining gradients locally first. These optimizations, unnecessary at small scale, become essential at production scale. @sec-communication examines these techniques in depth, developing the quantitative framework for reasoning about when communication becomes the bottleneck and how to address it.

[^fn-gradient-compression]: **Gradient Compression**: Techniques that reduce gradient data volume during distributed training, including sparsification and quantization, achieving 10-100x compression with minimal accuracy impact. @sec-communication examines compression algorithms and their trade-offs.

::: {.callout-definition title="Communication-Computation Ratio"}
***Communication-Computation Ratio*** describes the relative time spent transferring data versus performing computation in distributed systems. A ratio of 1:1 means equal time on each; higher ratios indicate communication-bound workloads. Modern distributed training systems typically achieve ratios between 1:3 and 1:1, making communication optimization critical for efficiency. The ratio depends on model size (larger models have more gradients to synchronize), batch size (larger batches amortize communication over more computation), and network bandwidth (faster networks reduce communication time).
:::

Communication optimization addresses one scale-induced challenge, but the machines generating and consuming that communication present their own transformation. As systems grow from dozens to thousands of components, the probability model for hardware reliability changes fundamentally.

### Failure Becomes Routine

At small scale, failure is exceptional. A well maintained server might run for years without hardware issues. Software bugs, once fixed, stay fixed. Administrators can manually investigate and remediate problems.

At large scale, failure becomes statistical certainty. With 10,000 GPUs, multiple failures occur weekly. With 100,000 concurrent user sessions, software edge cases that occur one-in-a-million times happen hundreds of times daily. Manual intervention becomes impossible; systems must self-heal. This transition requires architectural changes from the beginning. Small-scale systems optimize for the common case and handle failures through manual recovery, while large-scale systems embed failure handling into their core design:

- **Checkpointing**[^fn-checkpointing]: Saving model state frequently enough that losing hours of progress is acceptable when failures occur
- **Redundancy**: Running extra workers that can absorb failed workers' tasks without restart
- **Isolation**: Containing failures so that one component's crash does not cascade through the system
- **Detection**: Monitoring that identifies failures within seconds
- **Recovery**: Automated procedures that restore service without human intervention

@sec-fault-tolerance develops these principles systematically, showing how to design training systems that treat failure as expected rather than exceptional.

[^fn-checkpointing]: **Checkpointing in Distributed Training**: Periodically saving model state to persistent storage, enabling recovery from failures. For frontier models, checkpoints reach terabytes, making checkpoint management a systems challenge. @sec-fault-tolerance examines checkpointing strategies.

### Heterogeneity Emerges

At small scale, systems are homogeneous. A single GPU training job runs on one type of hardware with one software configuration. Behavior is predictable and reproducible.

At large scale, heterogeneity becomes unavoidable. A fleet of 10,000 GPUs contains multiple hardware generations purchased over years. Different racks have different thermal characteristics affecting clock speeds. Software updates roll out gradually, creating version skew. Network paths vary in latency and bandwidth.

This heterogeneity creates engineering challenges absent at small scale. Load balancing must account for hardware capability differences. Gradient aggregation must handle workers completing at different rates. Inference routing must direct requests to servers with appropriate model versions. Testing must verify behavior across the combinatorial explosion of configuration variants.

These techniques for managing heterogeneity are necessary but not sufficient. Even with perfect load balancing and configuration management, a deeper challenge remains: distribution itself introduces fundamental constraints arising from the physics of information transfer. These constraints exist regardless of scale, though they become inescapable as systems grow beyond single machines.

## Why Distribution is Hard {#sec-vol2-introduction-why-distribution-hard}

Scale forces distribution: no single machine provides the thousands of GPUs that frontier training requires, no single datacenter serves global user bases with acceptable latency, and no centralized system can collect the distributed data that edge devices generate. Coordinating computation across physically separated machines connected by finite bandwidth, non-zero latency networks creates constraints that no amount of engineering cleverness can eliminate. These constraints manifest in three forms: the impossibility theorems that bound what distributed systems can guarantee, the coordination overhead that taxes every synchronization point, and the amplified complexity when distribution extends beyond the datacenter to edge devices.

### The CAP Theorem Reality

The CAP theorem[^fn-cap-theorem] establishes that distributed systems can provide at most two of three properties: consistency (all nodes see the same data), availability (every request receives a response), and partition tolerance (the system continues operating despite network failures). Since network partitions can always occur, practical systems must choose between consistency and availability during partitions.

[^fn-cap-theorem]: **CAP Theorem**: Proven by Eric Brewer and formalized by Gilbert and Lynch [@gilbert2002brewer], the CAP theorem states that a distributed data store cannot simultaneously provide more than two of Consistency (every read receives the most recent write), Availability (every request receives a non-error response), and Partition tolerance (the system continues operating despite network partitions). Since partitions are unavoidable in distributed systems, the practical choice is between CP (consistent but potentially unavailable during partitions) and AP (available but potentially inconsistent). Distributed ML systems make different choices. Synchronous training is CP (training halts during partitions to maintain model consistency), while asynchronous training is AP (training continues with potentially stale gradients). Understanding this trade-off informs architecture decisions throughout distributed ML systems.

Distributed ML systems make different CAP trade-offs depending on their requirements. Synchronous distributed training chooses consistency: all workers see the same model state, but training halts if any worker becomes unreachable. Asynchronous training chooses availability: training continues even with stragglers or failures, but workers may operate on slightly stale model versions [@dean2012large]. Federated learning often chooses availability with eventual consistency: edge devices train locally and periodically synchronize, accepting temporary inconsistency for continuous operation.

### Coordination Overhead

Beyond the impossibility theorems, distributed systems pay a practical tax on every operation that requires agreement across machines. Every synchronization point introduces latency. Every consensus protocol requires network round-trips. Every distributed lock limits parallelism.

Consider the overhead of distributed training synchronization. Each training iteration requires:

1. Forward pass computation (parallelizable)
2. Loss computation (local to each worker)
3. Backward pass computation (parallelizable)
4. Gradient aggregation (requires network communication)
5. Parameter update (can parallelize with next iteration)

Steps 1-3 and 5 parallelize efficiently. Step 4, gradient aggregation, requires global coordination. Even with optimized all reduce algorithms and high-bandwidth networks, this coordination can consume a substantial fraction of total training time for large models [@shoeybi2019megatron]. This overhead is fundamental: no algorithm can aggregate globally distributed values without communication proportional to the data volume.

### Edge Distribution Complexity

The coordination challenges discussed so far assume datacenter distribution, where all machines run in managed facilities with reliable power, cooling, and networking. Administrators can access any machine for diagnosis and repair. These assumptions fail entirely when distribution extends to edge devices.

Edge distribution amplifies every challenge. Billions of smartphones, IoT devices, and embedded systems operate in uncontrolled environments with unreliable connectivity, limited power, and heterogeneous capabilities. Google's Gboard keyboard runs on over 1 billion Android devices, each potentially participating in federated learning[^fn-federated-learning] to improve predictions [@hard2018federated]. This deployment context introduces unique constraints:

[^fn-federated-learning]: **Federated Learning**: A distributed machine learning approach where models train across many decentralized devices holding local data samples, without exchanging the raw data. Instead of collecting user data to a central server, federated learning sends the model to devices, trains locally, and aggregates only the model updates (gradients or weight differences). This preserves data privacy while enabling learning from distributed sources. Challenges include handling heterogeneous device capabilities, intermittent connectivity, and ensuring convergence despite non-uniform data distributions. Federated learning is covered in detail in @sec-edge-intelligence.

- **Intermittent connectivity**: Devices may be reachable only when on WiFi and charging
- **Heterogeneous hardware**: Model must run efficiently across devices spanning a 100× performance range
- **Privacy requirements**: Raw data cannot leave devices, requiring on-device processing
- **Update complexity**: Pushing model updates to billions of devices takes weeks
- **Monitoring limitations**: Cannot install arbitrary diagnostics on user devices

These constraints require architectural approaches that differ from datacenter ML in essential ways. Federated learning aggregates model updates without collecting data. On-device inference optimizes for varied hardware capabilities. Differential privacy[^fn-differential-privacy] (which adds calibrated noise to protect individual data points while preserving aggregate statistical properties) provides mathematical guarantees about information leakage [@dwork2014algorithmic]. These techniques, largely unnecessary for centralized ML, become essential for edge deployment.

The edge deployment challenges just described illustrate how distribution multiplies engineering complexity. Yet the implications extend beyond technical difficulty. When ML systems operate at the scale of billions of users across distributed infrastructure, their impact on society demands consideration beyond engineering excellence.

[^fn-differential-privacy]: **Differential Privacy**: A mathematical framework providing provable privacy guarantees by ensuring query results are nearly identical whether or not any individual's data is included. @sec-security-privacy examines differential privacy implementation and trade-offs.

## Why Governance Matters at Scale {#sec-vol2-introduction-why-governance-matters}

Scale and distribution do not just create engineering challenges; they amplify impact. Systems serving billions of users at millisecond latencies affect society in ways that demand governance beyond technical excellence. A bug in a small system affects few users; a bug in a system serving billions affects society. This amplification creates governance requirements that small-scale systems can ignore.

### Security Threats Intensify

ML systems face unique security threats beyond traditional software vulnerabilities[^fn-ml-security-threats]. Model extraction attacks can steal proprietary models through query access. Researchers demonstrated extracting functionally equivalent copies of production ML models using only API access [@tramer2016stealing]. Membership inference attacks can determine whether specific data was used in training, creating privacy violations from seemingly innocuous model access [@shokri2017membership]. Adversarial examples can cause misclassification with perturbations imperceptible to humans, demonstrated against production systems including Tesla Autopilot and content moderation systems [@goodfellow2014explaining].

[^fn-ml-security-threats]: **ML-Specific Security Threats**: Traditional software security focuses on preventing unauthorized code execution and data access. ML systems face additional threats that exploit the learned behavior of models. Data poisoning attacks inject malicious training examples that cause targeted misbehavior. Researchers demonstrated that controlling 0.1% of training data can implant backdoors that cause misclassification on specific inputs. Model inversion attacks reconstruct training data from model access. Facial recognition models can leak enough information to reconstruct recognizable images of training individuals. Adversarial reprogramming hijacks models to perform unintended tasks through specially crafted inputs. These threats require defenses beyond traditional security including differential privacy, certified robustness, and continuous monitoring of model behavior in production.

At production scale, these threats become economically attractive to attackers. A model serving millions of users represents substantial intellectual property worth stealing. A model making consequential decisions (loans, hiring, content moderation) offers high-value manipulation targets. A model processing sensitive data (health records, financial information) provides valuable inference targets.

Defending against these threats requires systematic approaches including access controls that limit query rates and patterns, output perturbation that provides differential privacy guarantees, adversarial training that improves robustness to perturbations, and monitoring that detects anomalous query patterns indicative of attacks. These defenses impose overhead unnecessary for small-scale systems but essential for production deployment.

### Regulatory Requirements Emerge

Systems operating at scale attract regulatory attention. The European Union's General Data Protection Regulation (GDPR) imposes obligations for systems processing EU residents' data, including the right to explanation for automated decisions [@gdpr2016]. The EU AI Act establishes risk-based requirements for AI systems, with high-risk applications (healthcare, employment, law enforcement) requiring conformity assessments, human oversight, and accuracy documentation [@euaiact2024]. Similar regulations exist or are developing in jurisdictions worldwide.

Meeting these regulatory requirements demands technical capabilities:

- **Audit trails**: Recording inputs, outputs, and model versions for every decision
- **Explanation generation**: Producing human-interpretable justifications for model outputs
- **Consent management**: Tracking and honoring user preferences for data usage
- **Data deletion**: Removing specific users' data from training sets and retraining affected models
- **Bias testing**: Evaluating model performance across protected demographic groups

These capabilities impose engineering costs absent for unregulated systems but mandatory for production deployment in regulated contexts. Yet regulatory compliance represents only one dimension of governance; the broader question concerns how systems affecting billions of lives should be built and operated.

### Societal Impact Demands Responsibility

Beyond legal compliance, systems affecting billions of users carry ethical obligations. Recommendation algorithms shape public discourse. Researchers have documented how engagement-optimizing systems can amplify misinformation and polarization [@ribeiro2020auditing]. Hiring algorithms affect employment opportunities. Amazon discontinued an AI recruiting tool that exhibited bias against women [@dastin2018amazon]. Content moderation systems determine what speech is visible. Errors can suppress legitimate expression or fail to remove harmful content. Responsible engineering practices must address these impacts:

- **Fairness evaluation**: Testing for disparate impact across demographic groups before deployment
- **Impact assessment**: Analyzing potential harms before launching new capabilities
- **Human oversight**: Maintaining human review for high-stakes decisions
- **Incident response**: Processes for rapidly addressing identified harms
- **Transparency**: Documentation of system capabilities, limitations, and decision factors

::: {.callout-definition title="Responsible AI"}
***Responsible AI*** encompasses the practices, frameworks, and technical approaches that ensure ML systems operate in ways that are fair, transparent, accountable, and beneficial to society. Responsible AI addresses the ethical implications of automated decision making, the potential for algorithmic bias, the environmental impact of large-scale computation, and the governance structures needed to maintain human oversight over consequential systems. Unlike traditional software quality assurance focused on correctness and performance, responsible AI evaluates systems against societal values and human rights considerations.
:::

The governance challenges just described cannot be addressed in isolation from the infrastructure and distribution decisions that precede them. A security vulnerability in gradient aggregation affects both training efficiency and model confidentiality. A storage architecture decision shapes both compliance capabilities and inference latency. The communication bottlenecks that dominate large-scale training also determine what audit logging is feasible without degrading throughput. These interconnections reveal a fundamental truth: scale, distribution, and governance form an integrated system where decisions in one domain cascade through others.

## Foundational Concepts {#sec-vol2-introduction-foundational-concepts}

To reason systematically about these interconnections, we need organizing frameworks that operate at different levels of analysis. The AI Triad reveals component interdependencies at the system level, showing when a change to data affects what algorithms and infrastructure are viable. The Five-Pillar Framework organizes the engineering discipline itself, distinguishing data engineering from model development from deployment infrastructure. The Six Principles guide individual design decisions, telling you to optimize the bottleneck rather than everything equally. Together, these frameworks provide a multi-scale lens: system-level for understanding trade-offs, discipline-level for organizing expertise, and decision-level for guiding implementation choices. This section introduces each framework, showing how it helps structure the engineering decisions you will face throughout this textbook.

**The AI Triad** provides the organizing framework for understanding ML systems. Every machine learning system comprises three interdependent components: data that guides behavior, algorithms that learn patterns, and computational infrastructure that enables both training and inference. These components exist in dynamic tension. Larger models require more data and compute. Larger datasets enable more sophisticated models but demand storage and processing infrastructure.

At the scales we have examined, these interdependencies intensify and transform. The 10^25^ FLOPS required for GPT-4 training did not merely demand more infrastructure; it required infrastructure architecture that enabled efficient gradient synchronization across 25,000 GPUs. The terabytes of training data did not merely require more storage; they required distributed storage systems with access patterns optimized for ML workloads. The 175 billion parameters did not merely require more memory; they required partitioning strategies that balance computation against communication overhead. Understanding these interdependencies helps you predict when a change to one Triad component will cascade through the others, and architect systems that accommodate rather than fight these dynamics.

**The Five-Pillar Framework** structures the ML systems engineering discipline into interconnected domains: data engineering establishes pipelines for collecting, processing, and serving training and inference data; model development encompasses architecture design, training procedures, and validation methodologies; optimization techniques compress and accelerate models for deployment constraints; deployment infrastructure spans cloud platforms, edge devices, and embedded systems; and operations practices ensure systems remain reliable, secure, and effective throughout their lifecycle. This textbook extends each pillar to production scale, where the engineering challenges multiply.

**The Six Systems Engineering Principles** provide guidance for design decisions across all five pillars:

1. *Measure Everything*: At scale, measurement itself becomes a systems challenge requiring distributed monitoring infrastructure
2. *Design for 10× Scale*: Production deployment reveals whether 10× design was adequate or optimistic
3. *Optimize the Bottleneck*: Scale shifts bottlenecks from compute to communication to coordination
4. *Plan for Failure*: At scale, failure is not exceptional but routine
5. *Design Cost-Consciously*: Scale makes efficiency improvements worth millions of dollars
6. *Co-Design for Hardware*: Distributed hardware introduces network topology and storage hierarchy as co-design considerations

These frameworks assume familiarity with single-machine ML systems: how models are trained, optimized, and deployed on individual devices. This textbook teaches you to scale, distribute, and govern them across distributed infrastructure.

## The Structure of This Textbook {#sec-vol2-introduction-structure}

This textbook organizes around the three imperatives, progressing from infrastructure foundations through distribution techniques to governance practices. @tbl-vol2-structure summarizes this five-part structure.

::: {.callout-note title="Figure Placeholder: Volume 2 Roadmap" collapse="true"}
```{.tikz}
% TODO: Flowchart showing Parts I-V progression
\node[draw, align=center] {Volume 2 Roadmap\nPart I -> Part II -> Part III -> Part IV -> Part V};
```
**Volume 2 Roadmap**. This flowchart illustrates the textbooks progression from foundational infrastructure to advanced frontiers. Part I establishes the physical and data foundations. Part II enables distributed training across clusters. Part III focuses on deploying models at scale. Part IV addresses production hardening (Security, Robustness, Sustainability). Part V explores responsible AI and future AGI systems.
:::

+--------------------------------+---------------------------------+----------------------------------------+
| **Part**                       | **Theme**                       | **Key Chapters**                       |
+:===============================+:================================+:=======================================+
| **I: Foundations of Scale**    | **Scale**: Physical and data    | Infrastructure, Storage                |
|                                | foundations for distributed ML  |                                        |
+--------------------------------+---------------------------------+----------------------------------------+
| **II: Distributed Training**   | **Distribute**: Training models | Distributed Training, Communication,   |
|                                | across multiple machines        | Fault Tolerance                        |
+--------------------------------+---------------------------------+----------------------------------------+
| **III: Deployment at Scale**   | **Deploy**: Serving predictions | Inference at Scale, Edge Intelligence, |
|                                | to millions of users            | ML Operations at Scale                 |
+--------------------------------+---------------------------------+----------------------------------------+
| **IV: Production Concerns**    | **Operate**: Running systems    | Privacy & Security, Robust AI,         |
|                                | safely and sustainably          | Sustainable AI                         |
+--------------------------------+---------------------------------+----------------------------------------+
| **V: Responsible AI at Scale** | **Govern**: Ensuring beneficial | Responsible AI, AI for Good,           |
|                                | societal impact                 | AGI Systems, Conclusion                |
+--------------------------------+---------------------------------+----------------------------------------+

: The five parts progress from infrastructure foundations through distributed training and deployment to production concerns and responsible governance. {#tbl-vol2-structure}

### Part I: Foundations of Scale

The scale transformation we examined, from 10^18^ FLOPS for AlexNet to 10^25^ FLOPS for GPT-4, did not happen by adding more of the same hardware. It required fundamentally different infrastructure: datacenters designed for AI workloads, accelerators optimized for matrix operations, networks that prioritize bandwidth over latency, and storage systems built for the unique access patterns of ML training. Part I establishes this infrastructure foundation, because every technique in subsequent parts depends on it.

**Infrastructure** examines the hardware and software stack that enables scale: GPU clusters with NVLink interconnects, InfiniBand networks achieving the 200 Gb/s bandwidth we calculated as necessary for gradient synchronization, and orchestration systems that manage the failure-prone fleets we discussed.

**Storage Systems** addresses the AI Triad's data component at scale. Training datasets for frontier models exceed any single storage system's capacity; feature stores must serve the real-time lookups that inference demands; artifact management tracks the thousands of model versions that production systems generate.

### Part II: Distributed Training

With infrastructure foundations established, we turn to the distribution challenges that dominate large-scale training. The communication-computation ratio we analyzed, where gradient synchronization can consume half of training time, demands techniques that were unnecessary at single-machine scale.

**Distributed Training** develops techniques for training models across devices and machines. Data parallelism[^fn-data-parallelism], model parallelism[^fn-model-parallelism], and pipeline parallelism[^fn-pipeline-parallelism] each address different constraints. You will understand when each applies, how they combine, and what synchronization and consistency they require.

[^fn-data-parallelism]: **Data Parallelism**: A distributed training strategy where each worker processes different data batches while maintaining synchronized model copies. @sec-distributed-training examines data parallelism implementation in detail.

[^fn-model-parallelism]: **Model Parallelism**: Distributes model parameters across multiple devices, enabling training of models too large for single-device memory. @sec-distributed-training examines tensor parallelism and other model partitioning strategies.

[^fn-pipeline-parallelism]: **Pipeline Parallelism**: Partitions the model by layers across devices, with computation flowing through stages. @sec-distributed-training examines pipeline scheduling and its trade-offs.

**Communication** analyzes the collective operations that coordinate distributed training. AllReduce, AllGather[^fn-allgather], and other primitives dominate training communication. You will understand algorithms, topologies, and optimization techniques that minimize communication overhead and maximize bandwidth utilization.

[^fn-allgather]: **AllGather**: A collective operation where each worker contributes data and receives the concatenation of all contributions, essential for model parallelism. @sec-communication examines collective operations in detail.

**Fault Tolerance** ensures distributed systems continue operating despite failures. At production scale, failures occur daily. Checkpointing, redundancy, and recovery procedures enable training to continue despite inevitable component failures.

### Part III: Deployment at Scale

Training produces models; deployment delivers value to users. The edge distribution complexity we examined, where billions of heterogeneous devices operate in uncontrolled environments, requires techniques that extend far beyond datacenter serving.

**Inference at Scale** examines serving systems that deliver predictions with low latency and high throughput. Request routing, load balancing, autoscaling, and geographic distribution enable production inference to meet demanding performance requirements.

**Edge Intelligence** extends ML to resource-constrained devices at the network edge. Model compression, runtime optimization, and edge-cloud coordination enable deployment where centralized inference is infeasible.

**ML Operations at Scale** encompasses practices that maintain large ML systems in production. Monitoring, debugging, deployment pipelines, and incident response adapt for ML-specific requirements at production scale.

### Part IV: Production Concerns

The security threats and regulatory requirements we examined create operational challenges that require systematic approaches. At production scale, the economic incentives for attacks, the regulatory scrutiny, and the environmental impact all intensify.

**Privacy and Security** addresses threats specific to ML systems. Model extraction, membership inference, adversarial examples, and data poisoning require defenses including differential privacy, secure computation, and adversarial training.

**Robust AI** ensures reliable operation under uncertainty. Distribution shift, out-of-distribution inputs, and novel situations require systems that recognize the limits of their competence and respond appropriately.

**Sustainable AI** addresses environmental impact. Efficient algorithms, appropriate model sizing, and renewable energy sourcing minimize the environmental footprint of large-scale ML.

### Part V: Responsible AI at Scale

The societal impact we examined, where recommendation algorithms shape public discourse and hiring algorithms affect employment opportunities, demands governance practices that transcend technical excellence. Technical excellence is insufficient for systems affecting human lives at scale.

**Responsible AI** addresses fairness, transparency, and accountability. Systems must operate equitably across populations, enable stakeholder understanding, and ensure harms can be identified and remediated.

**AI for Good** demonstrates how ML systems address societal challenges. Applications in healthcare, climate, education, and accessibility illustrate how systems engineering principles enable beneficial impact.

**AGI Systems** examines emerging directions including foundation models, compound AI systems, and novel computing paradigms. Understanding these trajectories prepares you for continued learning as the field evolves.

This progression from infrastructure through governance reflects how production ML systems are actually built: infrastructure determines what distribution strategies are feasible, distribution capabilities shape what governance is possible, and governance requirements constrain both. For detailed guidance on reading paths, prerequisite knowledge, and navigation strategies, refer to the [About](../../frontmatter/about/about.qmd) section.

## The Journey Ahead {#sec-vol2-introduction-journey-ahead}

Having mapped the territory from infrastructure foundations through governance practices, consider what mastering this material means for your professional growth. The six systems engineering principles introduced earlier provide a vision for building ML systems that matter. This textbook extends that vision to the scale at which most consequential ML systems operate.

The transition from building systems that work to building systems that scale, distribute, and govern responsibly represents significant professional growth. The ML systems that will define this era require precisely these capabilities: foundation models serving hundreds of millions of users, edge deployments spanning billions of devices, and AI systems making consequential decisions about human lives.

Throughout this volume, you will learn to architect infrastructure that processes petabytes of training data across tens of thousands of accelerators. You will design inference systems that serve billions of predictions with consistent low latency. You will implement security, privacy, and robustness measures that protect systems against adversarial conditions. You will establish governance practices that ensure systems operate fairly, sustainably, and accountably.

The engineering challenges are substantial, and so is the impact of addressing them correctly.

The path forward begins with infrastructure: the datacenters, accelerators, storage systems, and communication networks that make everything else possible. Once you understand how this infrastructure enables distributed ML, you will be prepared to build systems that leverage these capabilities effectively.

Let us begin.

::: {.callout-important title="Key Takeaways"}
* This textbook addresses the shift from single-machine ML to distributed systems where communication costs, routine failures, and governance requirements become dominant engineering concerns
* Scale creates qualitative, not merely quantitative, changes: techniques that work for 8 GPUs may fail at 8,000 GPUs due to emergent phenomena like network congestion, straggler effects, and coordination overhead
* The three pillars of this textbook (scaling infrastructure, distributing computation, and governing responsibly) are interdependent: infrastructure determines what distribution strategies are feasible, and governance constraints shape both
* Production ML systems diverge from research prototypes in their requirements for fault tolerance, security, privacy, and accountability to stakeholders beyond the development team
:::

::: { .quiz-end }
:::
