---
bibliography: introduction.bib
---

<!--
================================================================================
VOLUME II EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY
================================================================================

PHILOSOPHY: This textbook teaches generalizable ML systems principles, NOT
"LLM infrastructure." Every technique should apply across model architectures.
Students who master these concepts can work on any production ML system.

WHEN WRITING CONTENT, ENSURE EXAMPLES SPAN THESE MODEL TYPES:

| Model Type          | Unique Systems Challenges                              |
|---------------------|--------------------------------------------------------|
| LLMs/Transformers   | KV cache memory, attention compute scaling, long       |
|                     | context, autoregressive decoding latency               |
| Recommendation      | Massive embedding tables (trillion+ params), feature   |
|                     | lookup latency, real-time updates, CTR prediction      |
| Vision (CNN/ViT)    | Data augmentation pipelines, batch size sensitivity,   |
|                     | spatial locality, multi-resolution processing          |
| Scientific/GNN      | Irregular compute patterns, graph partitioning,        |
|                     | sparse operations, physics constraints                 |
| Multimodal          | Cross-encoder coordination, modality-specific          |
|                     | preprocessing, heterogeneous compute requirements      |
| Speech/Audio        | Streaming inference, variable-length sequences,        |
|                     | real-time latency constraints                          |

CHAPTER-SPECIFIC GUIDANCE:

DISTRIBUTED TRAINING (@sec-distributed-training):

- Data parallelism: ResNet, BERT, recommendation models
- Model parallelism: GPT-3, Megatron, DLRM embedding sharding
- Pipeline parallelism: GPT, T5, large vision models
- Include: DLRM has DIFFERENT parallelism needs (embedding-heavy vs compute-heavy)

INFERENCE AT SCALE (@sec-inference-at-scale):

- Batching strategies differ: LLMs (continuous batching) vs RecSys (feature lookup)
- Latency profiles: recommendation <10ms, LLMs 100ms-seconds, vision 20-50ms
- Include ensemble serving (multiple models in pipeline)
- RecSys is the DOMINANT inference workload by volume at Meta, TikTok, Netflix

COMMUNICATION (@sec-communication):

- AllReduce for dense gradients (vision, transformers)
- AlltoAll for embedding lookups (recommendation)
- Gradient compression benefits vary by model type

STORAGE (@sec-storage):

- Feature stores: critical for RecSys, less relevant for LLMs
- Checkpoint sizes: LLMs (TB), vision (GB), recommendation (embedding tables)
- Data lakes: training data diversity matters

FAULT TOLERANCE (@sec-fault-tolerance):

- Checkpointing frequency depends on model size and iteration time
- Recommendation systems: real-time feature stores need different recovery
- Training vs serving fault tolerance requirements differ

EDGE INTELLIGENCE (@sec-edge-intelligence):

- On-device: vision (phones), speech (assistants), NLP (keyboards)
- Federated learning works differently for different model types
- Model compression: quantization effects vary by architecture

QUANTITATIVE DIVERSITY CHECKLIST:

- [ ] Does this section have examples from at least 2-3 model types?
- [ ] Are performance numbers given for multiple architectures?
- [ ] Would a RecSys engineer find this content applicable to their work?
- [ ] Would a vision ML engineer find this content applicable?
- [ ] Are we avoiding LLM-centric framing of general concepts?

EXAMPLE OF GOOD FRAMING:
  BAD:  "Training GPT-3 on a single V100 would require 355 years"
  GOOD: "Training frontier models—whether GPT-3 (175B params), ViT-22B
         (vision), or DLRM (trillion-param embeddings)—would require
         centuries on single devices, making distributed approaches essential."

EXAMPLE OF INCLUSIVE CASE STUDIES:
  - Meta DLRM: recommendation at scale
  - Google BERT/T5: NLP training infrastructure
  - Tesla Autopilot: vision model deployment
  - Spotify: audio/recommendation hybrid
  - TikTok: multimodal recommendation

================================================================================
-->

# Introduction {#sec-vol2-introduction}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting advanced ML systems at scale, set on a crisp, clean white background. The image features interconnected data centers, distributed computing networks, and multiple deployment environments. Visual elements include server racks connected by glowing data streams, edge devices at the periphery, and governance frameworks represented as protective layers. Icons represent the three core themes: scale (expanding infrastructure), distribute (networked nodes), and govern (protective oversight). The style is clean, modern, and flat, suitable for a technical book._
:::

\noindent
![](images/png/cover_introduction.png)

:::

## Purpose {.unnumbered}

_Why must we master the engineering principles that enable machine learning systems to operate reliably at massive scale, across distributed infrastructure, and under responsible governance?_

The systems that transform industries and affect billions of lives cannot run on individual machines or small clusters. Production ML systems operate at scales where the fundamental nature of engineering challenges changes: communication dominates computation, failures become routine rather than exceptional, and architectural decisions carry consequences that reach far beyond technical performance metrics. At this frontier, the ability to coordinate learning across thousands of machines, to serve predictions to hundreds of millions of users with consistent reliability, and to ensure these systems operate fairly and sustainably determines which capabilities remain laboratory demonstrations and which reshape how humanity solves problems. Volume I established how to build, optimize, and operate ML systems; this volume extends those foundations to the production scale where most consequential AI systems must operate, introducing the three imperatives of scale, distribution, and governance that define advanced ML systems engineering. Mastering these principles determines your ability to architect the infrastructure and establish the practices that enable transformative AI capabilities to reach the people and applications that need them most.

::: {.callout-tip title="Learning Objectives"}

- Explain why ML systems exhibit qualitatively different behaviors at production scale compared to single-machine systems, including the dominance of communication over computation and the transition from exceptional to routine failure

- Analyze the historical evolution of ML compute requirements from AlexNet to frontier models, calculating the implications for infrastructure design

- Compare synchronous versus asynchronous distributed training approaches using the CAP theorem framework to evaluate consistency-availability trade-offs

- Differentiate between datacenter distribution and edge distribution challenges, identifying unique constraints for each deployment context

- Classify ML security threats (model extraction, membership inference, adversarial examples) and explain why scale amplifies their economic attractiveness to attackers

- Apply the AI Triad and Five-Pillar Framework from Volume I to reason about distributed systems challenges across data, algorithms, and infrastructure

- Evaluate the textbook's five-part structure (Foundations of Scale, Distributed Training, Deployment at Scale, Production Concerns, Responsible AI at Scale) to select appropriate chapters for specific engineering challenges

:::

## The Scale Transformation {#sec-vol2-introduction-scale-transformation}

The history of machine learning is a history of scale. Each major capability leap has come not from algorithmic breakthroughs alone, but from the ability to apply computation at previously impossible scales. Understanding this progression reveals why systems engineering has become central to AI advancement.

Consider the trajectory of compute requirements. AlexNet (2012) trained on two GTX 580 GPUs for approximately 5 to 6 days [@krizhevsky2012imagenet]. BERT (2018) required 64 TPU chips for 4 days, roughly 6,144 chip hours [@devlin2018bert]. GPT-3 (2020) consumed an estimated 3.14×10²³ FLOPS during training, requiring thousands of V100 GPUs running for weeks [@brown2020language]. PaLM (2022) trained on 6,144 TPU v4 chips for roughly 60 days, consuming approximately 10²⁴ FLOPS [@chowdhery2022palm]. GPT-4 (2023) reportedly trained on approximately 25,000 A100 GPUs over 90 to 100 days [@openai2023gpt4]. This progression represents approximately a 10 million fold increase in training compute over a single decade, from roughly 10¹⁸ FLOPS for AlexNet to 10²⁵ FLOPS for GPT-4.

::: {.callout-example title="Training Compute Evolution"}
```
Model           Year    GPUs/TPUs    Training Time    Estimated FLOPS
─────────────────────────────────────────────────────────────────────
AlexNet         2012    2 GPUs       5-6 days         ~10¹⁸
BERT-Large      2018    64 TPUs      4 days           ~10²⁰
GPT-3           2020    ~1000 GPUs   ~30 days         ~10²³
PaLM            2022    6144 TPUs    ~60 days         ~10²⁴
GPT-4           2023    ~25000 GPUs  ~100 days        ~10²⁵
```
:::

This exponential growth in compute requirements has transformed ML from a discipline where algorithms dominate to one where systems engineering determines success. A sophisticated algorithm that cannot scale often provides less practical value than a simpler algorithm deployed efficiently across scalable infrastructure.

The transition from single-machine to distributed training introduces qualitative changes in system behavior. On a single GPU, training proceeds deterministically: the same code, data, and random seed produce identical results. At the scale of thousands of GPUs, new phenomena emerge. Network partitions can split clusters into groups that train independently, causing model divergence[^fn-network-partition]. Stragglers, workers that process data slower than peers due to hardware variation or thermal throttling, can bottleneck entire training runs[^fn-straggler-problem]. Hardware failures that occur once per machine-year become daily events when operating 10,000 machines, meaning systems must checkpoint frequently enough that losing a day's progress becomes acceptable rather than catastrophic[^fn-failure-rates].

[^fn-network-partition]: **Network Partition in Distributed Training**: When network failures isolate groups of workers, each group may continue training independently with different gradient updates, causing models to diverge. In synchronous distributed training, partitions typically halt progress until connectivity restores. In asynchronous training, partitions can cause split brain scenarios where different model versions evolve in parallel. Production systems implement partition detection through heartbeat mechanisms and quorum protocols. Google's approach with TPU pods uses dedicated high bandwidth interconnects (ICI at 4.5 TB/s bidirectional per chip) to minimize partition probability, while software level protocols detect and recover from rare partitions within minutes rather than allowing extended divergent training.

[^fn-straggler-problem]: **The Straggler Problem**: In synchronous distributed training, all workers must complete their computation before proceeding to the next iteration. If one worker runs 10% slower due to thermal throttling, memory pressure, or background processes, the entire cluster waits, reducing effective utilization. At 1,000 workers, the probability of at least one straggler per iteration approaches certainty. Solutions include backup workers that redundantly compute slow partitions, gradient coding that tolerates missing results, bounded staleness that allows proceeding with partial results, and careful hardware provisioning that minimizes variation. Meta's research found that stragglers can reduce large scale training throughput by 20 to 30% without mitigation strategies.

[^fn-failure-rates]: **Hardware Failure Rates at Scale**: Individual server components fail infrequently. Enterprise SSDs show annual failure rates of 0.5 to 2%; GPUs fail at roughly 1 to 2% annually under typical datacenter conditions, though rates can exceed 9% under intensive AI training workloads; memory DIMMs fail at 1 to 2% per year. These rates seem manageable until multiplied by scale. A cluster of 10,000 GPUs with 2% annual failure rate experiences 200 GPU failures per year, or roughly one every two days. A training run lasting 100 days will statistically encounter 50 or more GPU failures. Production systems implement automatic detection, workload migration, and checkpoint based recovery to handle failures as routine events rather than emergencies. Meta reported that during LLaMA training, they experienced hardware failures roughly every few hours, requiring automated recovery systems to maintain progress.

These scale-induced challenges explain why the largest AI organizations invest heavily in infrastructure. Meta's Research SuperCluster (RSC) contains 16,000 NVIDIA A100 GPUs connected by 200 Gb/s InfiniBand networking [@meta2022rsc]. Google's TPU v4 pods contain 4,096 chips with 1.1 exaFLOPS of aggregate compute capacity. Microsoft's Azure AI infrastructure spans multiple datacenters with tens of thousands of GPUs dedicated to AI workloads. These investments reflect that frontier AI capabilities require frontier infrastructure.

## Why Scale Changes Everything {#sec-vol2-introduction-why-scale-changes}

Scale is not merely a larger version of small. Systems that work perfectly at modest scale exhibit fundamentally different behaviors at production scale. Understanding these qualitative transitions prepares you for the engineering challenges examined throughout this volume.

### Communication Becomes Dominant

At small scale, computation dominates. Training a model on a single GPU spends most time performing matrix multiplications. Communication overhead, moving data between CPU and GPU memory, represents a small fraction of total time.

At large scale, communication often dominates. Distributed training requires synchronizing gradients across workers after each batch. For a model with 175 billion parameters using 32 bit gradients, each synchronization must transfer 700GB of data (175 billion parameters × 4 bytes per parameter). Using the ring all-reduce algorithm[^fn-all-reduce] across 1,000 workers connected by 200 Gb/s InfiniBand (25 GB/s), theoretical completion time for the full synchronization approaches 56 seconds (2 × 700 GB ÷ 25 GB/s). Practical implementations achieve 60 to 80% of theoretical bandwidth, making communication consume a significant fraction of total training time even with optimized networks.

[^fn-all-reduce]: **Ring All-Reduce**: The dominant collective communication algorithm for distributed training gradient synchronization. In ring all-reduce, N workers arrange logically in a ring. Each worker sends 1/N of its gradient to its neighbor, which adds the received gradient to its own before forwarding. After N minus 1 steps, each worker has the sum of all gradients for 1/N of parameters. A second ring pass distributes the complete sum to all workers. The algorithm achieves optimal bandwidth utilization. Total data transferred equals 2(N minus 1)/N times the gradient size, approaching 2× regardless of worker count. For 1,000 workers with 700GB of gradients and 200 Gb/s links, theoretical completion time is approximately 2 × 700GB / 200 Gb/s = 56 seconds for the full reduce scatter plus all gather. Practical implementations achieve 60 to 80% of theoretical bandwidth, making communication a significant fraction of large scale training time.

This ratio explains why distributed training systems optimize communication aggressively. Gradient compression reduces transfer volume by 10 to 100× at the cost of some accuracy. Overlapping communication with computation hides transfer latency during the next batch's forward pass. Hierarchical aggregation reduces cross rack traffic by combining gradients locally first. These optimizations, unnecessary at small scale, become essential at production scale.

::: {.callout-definition title="Communication-Computation Ratio"}
***Communication-Computation Ratio*** describes the relative time spent transferring data versus performing computation in distributed systems. A ratio of 1 to 1 means equal time on each; higher ratios indicate communication bound workloads. Modern distributed training systems typically achieve ratios between 1 to 3 and 1 to 1, making communication optimization critical for efficiency. The ratio depends on model size (larger models have more gradients to synchronize), batch size (larger batches amortize communication over more computation), and network bandwidth (faster networks reduce communication time).
:::

### Failure Becomes Routine

At small scale, failure is exceptional. A well maintained server might run for years without hardware issues. Software bugs, once fixed, stay fixed. Administrators can manually investigate and remediate problems.

At large scale, failure becomes statistical certainty. With 10,000 GPUs, multiple failures occur weekly. With 100,000 concurrent user sessions, software edge cases that occur one in a million times happen hundreds of times daily. Manual intervention becomes impossible; systems must self heal.

This transition requires fundamental architectural changes. Small scale systems optimize for the common case and handle failures through manual recovery. Large scale systems must design for failure from the beginning:

- **Checkpointing**: Saving model state frequently enough that losing hours of progress is acceptable when failures occur
- **Redundancy**: Running extra workers that can absorb failed workers' tasks without restart
- **Isolation**: Containing failures so that one component's crash does not cascade through the system
- **Detection**: Monitoring that identifies failures within seconds
- **Recovery**: Automated procedures that restore service without human intervention

### Heterogeneity Emerges

At small scale, systems are homogeneous. A single GPU training job runs on one type of hardware with one software configuration. Behavior is predictable and reproducible.

At large scale, heterogeneity becomes unavoidable. A fleet of 10,000 GPUs contains multiple hardware generations purchased over years. Different racks have different thermal characteristics affecting clock speeds. Software updates roll out gradually, creating version skew. Network paths vary in latency and bandwidth.

This heterogeneity creates engineering challenges absent at small scale. Load balancing must account for hardware capability differences. Gradient aggregation must handle workers completing at different rates. Inference routing must direct requests to servers with appropriate model versions. Testing must verify behavior across the combinatorial explosion of configuration variants.

## Why Distribution is Hard {#sec-vol2-introduction-why-distribution-hard}

Distribution introduces challenges beyond those of scale. Coordinating computation across physically separated machines connected by finite-bandwidth, non-zero-latency networks creates fundamental constraints that no amount of engineering cleverness can eliminate.

### The CAP Theorem Reality

The CAP theorem[^fn-cap-theorem] establishes that distributed systems can provide at most two of three properties: consistency (all nodes see the same data), availability (every request receives a response), and partition tolerance (the system continues operating despite network failures). Since network partitions can always occur, practical systems must choose between consistency and availability during partitions.

[^fn-cap-theorem]: **CAP Theorem**: Proven by Eric Brewer and formalized by Gilbert and Lynch [@gilbert2002brewer], the CAP theorem states that a distributed data store cannot simultaneously provide more than two of Consistency (every read receives the most recent write), Availability (every request receives a non-error response), and Partition tolerance (the system continues operating despite network partitions). Since partitions are unavoidable in distributed systems, the practical choice is between CP (consistent but potentially unavailable during partitions) and AP (available but potentially inconsistent). Distributed ML systems make different choices. Synchronous training is CP (training halts during partitions to maintain model consistency), while asynchronous training is AP (training continues with potentially stale gradients). Understanding this trade-off informs architecture decisions throughout distributed ML systems.

ML systems make different choices depending on context. Synchronous distributed training chooses consistency: all workers see the same model state, but training halts if any worker becomes unreachable. Asynchronous training chooses availability: training continues even with stragglers or failures, but workers may operate on slightly stale model versions. Federated learning often chooses availability with eventual consistency: edge devices train locally and periodically synchronize, accepting temporary inconsistency for continuous operation.

### Coordination Overhead

Distributed systems require coordination that consumes resources. Every synchronization point introduces latency. Every consensus protocol requires network round-trips. Every distributed lock limits parallelism.

Consider the overhead of distributed training synchronization. Each training iteration requires:

1. Forward pass computation (parallelizable)
2. Loss computation (local to each worker)
3. Backward pass computation (parallelizable)
4. Gradient aggregation (requires network communication)
5. Parameter update (can parallelize with next iteration)

Steps 1-3 and 5 parallelize efficiently. Step 4, gradient aggregation, requires global coordination. Even with optimized all-reduce algorithms and high-bandwidth networks, this coordination can consume a substantial fraction of total training time for large models [@shoeybi2019megatron]. This overhead is fundamental: no algorithm can aggregate globally distributed values without communication proportional to the data volume.

### Edge Distribution Complexity

Datacenter distribution is challenging but controlled. All machines run in managed facilities with reliable power, cooling, and networking. Administrators can access any machine for diagnosis and repair.

Edge distribution amplifies every challenge. Billions of smartphones, IoT devices, and embedded systems operate in uncontrolled environments with unreliable connectivity, limited power, and heterogeneous capabilities. Google's Gboard keyboard runs on over 1 billion Android devices, each potentially participating in federated learning[^fn-federated-learning] to improve predictions [@hard2018federated].

[^fn-federated-learning]: **Federated Learning**: A distributed machine learning approach where models train across many decentralized devices holding local data samples, without exchanging the raw data. Instead of collecting user data to a central server, federated learning sends the model to devices, trains locally, and aggregates only the model updates (gradients or weight differences). This preserves data privacy while enabling learning from distributed sources. Challenges include handling heterogeneous device capabilities, intermittent connectivity, and ensuring convergence despite non-uniform data distributions. Federated learning is covered in detail in the On-Device Learning chapter.

Edge distribution introduces unique constraints:

- **Intermittent connectivity**: Devices may be reachable only when on WiFi and charging
- **Heterogeneous hardware**: Model must run efficiently across devices spanning 100× performance range
- **Privacy requirements**: Raw data cannot leave devices, requiring on device processing
- **Update complexity**: Pushing model updates to billions of devices takes weeks
- **Monitoring limitations**: Cannot install arbitrary diagnostics on user devices

These constraints require architectural approaches fundamentally different from datacenter ML. Federated learning aggregates model updates without collecting data. On-device inference optimizes for varied hardware capabilities. Differential privacy, which adds calibrated noise to protect individual data points while preserving aggregate statistical properties, provides mathematical guarantees about information leakage. These techniques, largely unnecessary for centralized ML, become essential for edge deployment.

## Why Governance Matters at Scale {#sec-vol2-introduction-why-governance-matters}

Scale amplifies impact. A bug in a small system affects few users; a bug in a system serving billions affects society. This amplification creates governance requirements that small-scale systems can ignore.

### Security Threats Intensify

ML systems face unique security threats beyond traditional software vulnerabilities[^fn-ml-security-threats]. Model extraction attacks can steal proprietary models through query access. Researchers demonstrated extracting functionally equivalent copies of production ML models using only API access [@tramer2016stealing]. Membership inference attacks can determine whether specific data was used in training, creating privacy violations from seemingly innocuous model access [@shokri2017membership]. Adversarial examples can cause misclassification with perturbations imperceptible to humans, demonstrated against production systems including Tesla Autopilot and content moderation systems [@goodfellow2014explaining].

[^fn-ml-security-threats]: **ML-Specific Security Threats**: Traditional software security focuses on preventing unauthorized code execution and data access. ML systems face additional threats that exploit the learned behavior of models. Data poisoning attacks inject malicious training examples that cause targeted misbehavior. Researchers demonstrated that controlling 0.1% of training data can implant backdoors that cause misclassification on specific inputs. Model inversion attacks reconstruct training data from model access. Facial recognition models can leak enough information to reconstruct recognizable images of training individuals. Adversarial reprogramming hijacks models to perform unintended tasks through specially crafted inputs. These threats require defenses beyond traditional security including differential privacy, certified robustness, and continuous monitoring of model behavior in production.

At production scale, these threats become economically attractive to attackers. A model serving millions of users represents substantial intellectual property worth stealing. A model making consequential decisions (loans, hiring, content moderation) offers high-value manipulation targets. A model processing sensitive data (health records, financial information) provides valuable inference targets.

Defense requires systematic approaches including access controls that limit query rates and patterns, output perturbation that provides differential privacy guarantees, adversarial training that improves robustness to perturbations, and monitoring that detects anomalous query patterns indicative of attacks. These defenses impose overhead unnecessary for small-scale systems but essential for production deployment.

### Regulatory Requirements Emerge

Systems operating at scale attract regulatory attention. The European Union's General Data Protection Regulation (GDPR) imposes obligations for systems processing EU residents' data, including the right to explanation for automated decisions. The EU AI Act establishes risk-based requirements for AI systems, with high-risk applications (healthcare, employment, law enforcement) requiring conformity assessments, human oversight, and accuracy documentation. Similar regulations exist or are developing in jurisdictions worldwide.

Compliance requires technical capabilities:

- **Audit trails**: Recording inputs, outputs, and model versions for every decision
- **Explanation generation**: Producing human-interpretable justifications for model outputs
- **Consent management**: Tracking and honoring user preferences for data usage
- **Data deletion**: Removing specific users' data from training sets and retraining affected models
- **Bias testing**: Evaluating model performance across protected demographic groups

These capabilities impose engineering costs absent for unregulated systems but mandatory for production deployment in regulated contexts.

### Societal Impact Demands Responsibility

Beyond legal compliance, systems affecting billions of users carry ethical obligations. Recommendation algorithms shape public discourse. Researchers have documented how engagement-optimizing systems can amplify misinformation and polarization [@ribeiro2020auditing]. Hiring algorithms affect employment opportunities. Amazon discontinued an AI recruiting tool that exhibited bias against women [@dastin2018amazon]. Content moderation systems determine what speech is visible. Errors can suppress legitimate expression or fail to remove harmful content.

Responsible engineering practices address these impacts:

- **Fairness evaluation**: Testing for disparate impact across demographic groups before deployment
- **Impact assessment**: Analyzing potential harms before launching new capabilities
- **Human oversight**: Maintaining human review for high-stakes decisions
- **Incident response**: Processes for rapidly addressing identified harms
- **Transparency**: Documentation of system capabilities, limitations, and decision factors

::: {.callout-definition title="Responsible AI"}
***Responsible AI*** encompasses the practices, frameworks, and technical approaches that ensure ML systems operate in ways that are fair, transparent, accountable, and beneficial to society. Responsible AI addresses the ethical implications of automated decision making, the potential for algorithmic bias, the environmental impact of large scale computation, and the governance structures needed to maintain human oversight over consequential systems. Unlike traditional software quality assurance focused on correctness and performance, responsible AI evaluates systems against societal values and human rights considerations.
:::

## Bridging from Volume I {#sec-vol2-introduction-bridging}

Volume I established the foundations that this textbook extends. If you are beginning here, this section provides essential context. If you completed Volume I, consider this a brief reminder before we proceed to advanced topics.

**The AI Triad** provides the organizing framework for understanding ML systems. Every machine learning system comprises three interdependent components: data that guides behavior, algorithms that learn patterns, and computational infrastructure that enables both training and inference. These components exist in dynamic tension. Larger models require more data and compute. Larger datasets enable more sophisticated models but demand storage and processing infrastructure. At the scales examined in this volume, these interdependencies intensify. Distributed training requires coordinating the AI Triad's components across thousands of machines rather than optimizing them on a single system.

**The Five-Pillar Framework** structures the ML systems engineering discipline into interconnected domains: data engineering establishes pipelines for collecting, processing, and serving training and inference data; model development encompasses architecture design, training procedures, and validation methodologies; optimization techniques compress and accelerate models for deployment constraints; deployment infrastructure spans cloud platforms, edge devices, and embedded systems; and operations practices ensure systems remain reliable, secure, and effective throughout their lifecycle. This textbook extends each pillar to production scale, where the engineering challenges multiply.

**The Six Systems Engineering Principles** provide guidance for design decisions across all five pillars:

1. *Measure Everything*: At scale, measurement itself becomes a systems challenge requiring distributed monitoring infrastructure
2. *Design for 10x Scale*: Production deployment reveals whether 10× design was adequate or optimistic
3. *Optimize the Bottleneck*: Scale shifts bottlenecks from compute to communication to coordination
4. *Plan for Failure*: At scale, failure is not exceptional but routine
5. *Design Cost-Consciously*: Scale makes efficiency improvements worth millions of dollars
6. *Co-Design for Hardware*: Distributed hardware introduces network topology and storage hierarchy as co-design considerations

Volume I taught you to build, optimize, and operate ML systems. This textbook teaches you to scale, distribute, and govern them.

## The Structure of This Textbook {#sec-vol2-introduction-structure}

This textbook organizes around the three imperatives, progressing from infrastructure foundations through distribution techniques to governance practices. @tbl-vol2-structure summarizes the five-part structure.

+--------------------------------+---------------------------------+----------------------------------------+
| **Part**                       | **Theme**                       | **Key Chapters**                       |
+:===============================+:================================+:=======================================+
| **I: Foundations of Scale**    | **Scale**: Physical and data    | Infrastructure, Storage                |
|                                | foundations for distributed ML  |                                        |
+--------------------------------+---------------------------------+----------------------------------------+
| **II: Distributed Training**   | **Distribute**: Training models | Distributed Training, Communication,   |
|                                | across multiple machines        | Fault Tolerance                        |
+--------------------------------+---------------------------------+----------------------------------------+
| **III: Deployment at Scale**   | **Deploy**: Serving predictions | Inference at Scale, Edge Intelligence, |
|                                | to millions of users            | ML Operations at Scale                 |
+--------------------------------+---------------------------------+----------------------------------------+
| **IV: Production Concerns**    | **Operate**: Running systems    | Privacy & Security, Robust AI,         |
|                                | safely and sustainably          | Sustainable AI                         |
+--------------------------------+---------------------------------+----------------------------------------+
| **V: Responsible AI at Scale** | **Govern**: Ensuring beneficial | Responsible AI, AI for Good,           |
|                                | societal impact                 | AGI Systems, Conclusion                |
+--------------------------------+---------------------------------+----------------------------------------+

: The five parts progress from infrastructure foundations through distributed training and deployment to production concerns and responsible governance. {#tbl-vol2-structure}

### Part I: Foundations of Scale

Before systems can scale, they require infrastructure designed for scale.

**Infrastructure** examines how datacenters, accelerators, and orchestration systems enable large-scale ML. You will understand the hardware and software stack that makes distributed ML possible, from GPU clusters to high-bandwidth networks, resource scheduling, and container orchestration.

**Storage Systems** addresses data infrastructure at scale. Training datasets for modern models exceed any single storage system, requiring distributed architectures optimized for ML access patterns. You will understand storage hierarchies, data lakes, feature stores, and artifact management that enable efficient data serving.

### Part II: Distributed Training

With infrastructure foundations established, distribution techniques enable training across multiple machines.

**Distributed Training** develops techniques for training models across devices and machines. Data parallelism, model parallelism, and pipeline parallelism each address different constraints. You will understand when each applies, how they combine, and what synchronization and consistency they require.

**Communication** analyzes the collective operations that coordinate distributed training. AllReduce, AllGather, and other primitives dominate training communication. You will understand algorithms, topologies, and optimization techniques that minimize communication overhead and maximize bandwidth utilization.

**Fault Tolerance** ensures distributed systems continue operating despite failures. At production scale, failures occur daily. Checkpointing, redundancy, and recovery procedures enable training to continue despite inevitable component failures.

### Part III: Deployment at Scale

Training produces models; deployment delivers value to users.

**Inference at Scale** examines serving systems that deliver predictions with low latency and high throughput. Request routing, load balancing, autoscaling, and geographic distribution enable production inference to meet demanding performance requirements.

**Edge Intelligence** extends ML to resource-constrained devices at the network edge. Model compression, runtime optimization, and edge-cloud coordination enable deployment where centralized inference is infeasible.

**ML Operations at Scale** encompasses practices that maintain large ML systems in production. Monitoring, debugging, deployment pipelines, and incident response adapt for ML-specific requirements at production scale.

### Part IV: Production Concerns

Distribution creates operational challenges that require systematic approaches.

**Privacy and Security** addresses threats specific to ML systems. Model extraction, membership inference, adversarial examples, and data poisoning require defenses including differential privacy, secure computation, and adversarial training.

**Robust AI** ensures reliable operation under uncertainty. Distribution shift, out-of-distribution inputs, and novel situations require systems that recognize the limits of their competence and respond appropriately.

**Sustainable AI** addresses environmental impact. Efficient algorithms, appropriate model sizing, and renewable energy sourcing minimize the environmental footprint of large-scale ML.

### Part V: Responsible AI at Scale

Technical excellence is insufficient for systems affecting human lives at scale.

**Responsible AI** addresses fairness, transparency, and accountability. Systems must operate equitably across populations, enable stakeholder understanding, and ensure harms can be identified and remediated.

**AI for Good** demonstrates how ML systems address societal challenges. Applications in healthcare, climate, education, and accessibility illustrate how systems engineering principles enable beneficial impact.

**AGI Systems** examines emerging directions including foundation models, compound AI systems, and novel computing paradigms. Understanding these trajectories prepares you for continued learning as the field evolves.

For detailed guidance on reading paths, learning outcomes, prerequisite knowledge, and navigation strategies for both volumes, refer to the [About](../../frontmatter/about/about.qmd) section.

## The Journey Ahead {#sec-vol2-introduction-journey-ahead}

Volume I concluded with six systems engineering principles and a vision of building ML systems that matter. This volume extends that vision to the scale at which most consequential ML systems operate.

The transition from building systems that work to building systems that scale, distribute, and govern responsibly represents significant professional growth. The ML systems that will define this era require precisely these capabilities: foundation models serving hundreds of millions of users, edge deployments spanning billions of devices, and AI systems making consequential decisions about human lives.

Throughout this volume, you will learn to architect infrastructure that processes petabytes of training data across tens of thousands of accelerators. You will design inference systems that serve billions of predictions with consistent low latency. You will implement security, privacy, and robustness measures that protect systems against adversarial conditions. You will establish governance practices that ensure systems operate fairly, sustainably, and accountably.

The engineering challenges are substantial, and so is the impact of addressing them correctly.

The path forward begins with infrastructure: the datacenters, accelerators, storage systems, and communication networks that make everything else possible. Once you understand how this infrastructure enables distributed ML, you will be prepared to build systems that leverage these capabilities effectively.

Let us begin.

::: {.callout-important title="Key Takeaways"}
* Volume II addresses the fundamental shift from single-machine ML to distributed systems where communication costs, routine failures, and governance requirements become dominant engineering concerns
* Scale creates qualitative, not merely quantitative, changes: techniques that work for 8 GPUs may fail at 8,000 GPUs due to emergent phenomena like network congestion, straggler effects, and coordination overhead
* The three pillars of Volume II, scaling infrastructure, distributing computation, and governing responsibly, are interdependent: infrastructure determines what distribution strategies are feasible, and governance constraints shape both
* Production ML systems differ fundamentally from research prototypes in their requirements for fault tolerance, security, privacy, and accountability to stakeholders beyond the development team
:::

::: { .quiz-end }
:::
