---
engine: jupyter
quiz: security_privacy_quizzes.json
concepts: security_privacy_concepts.yml
glossary: security_privacy_glossary.json
---

```{python}
#| echo: false
#| label: chapter-start
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER START
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter initialization and global imports
# │
# │ Why: Registers this chapter with the mlsys registry and provides shared
# │      imports for all subsequent calculation cells.
# │
# │ Imports: mlsys.registry, mlsys.constants, mlsys.formatting
# │ Exports: (none)
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.registry import start_chapter
from mlsys.constants import (
    H100_MEM_CAPACITY, A100_MEM_CAPACITY,
    GB, MILLION, BILLION, TRILLION,
    SEC_PER_HOUR, SEC_PER_DAY
)
from mlsys.formatting import fmt, sci, check

start_chapter("vol2:security_privacy")
```

# Security & Privacy {#sec-security-privacy}
::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_security_privacy.png){fig-alt="Security and privacy in ML systems at scale." width=100%}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{25}{15}{35}{100}
\end{marginfigure}

_Why do privacy and security determine whether machine learning systems achieve widespread adoption and societal trust?_

Machine learning systems require unprecedented access to personal data, institutional knowledge, and behavioral patterns to function effectively, creating tension between utility and protection that determines societal acceptance. Unlike traditional software that processes data transiently, ML systems learn from sensitive information and embed patterns into persistent models that can inadvertently reveal private details. This capability creates systemic risks extending beyond individual privacy violations to threaten institutional trust, competitive advantages, and democratic governance. The most capable model in the world remains unused if it cannot be deployed without exposing sensitive data, if it cannot be trusted to resist adversarial manipulation, or if it cannot satisfy regulatory requirements that govern its intended domain. Privacy and security are not features to be added after the system works but prerequisites that determine whether the system can work at all in contexts where data sensitivity and adversarial risk are non-negotiable constraints.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Distinguish **security** from **privacy** in ML systems through formal definitions, **threat models**, and quantitative trade-offs
- Extract security principles from historical breaches (**Stuxnet**, **Jeep Cherokee**, **Mirai**) applicable to distributed ML infrastructure
- Analyze ML-specific **attack vectors** across **model theft**, **data poisoning**, **adversarial examples**, and **hardware vulnerabilities**
- Implement **differential privacy** with mathematical rigor, computing **privacy budgets** and accuracy trade-offs for production systems
- Design **layered defense** architectures spanning data protection, model security, runtime monitoring, and hardware trust mechanisms
- Evaluate **hardware security primitives** (**TEEs**, **secure boot**, **HSMs**, **PUFs**) for ML workloads with quantitative overhead analysis
- Apply the **three-phase implementation roadmap** to build context-appropriate security architectures for specific threat models

:::

This chapter's position in the book's organizing framework, *the Fleet Stack*, clarifies why security and privacy are not afterthoughts but structural requirements that must be engineered into every layer of the distributed ML stack.

::: {.callout-note title="Connection: The Fleet Stack"}

We have entered **Part IV: The Responsible Fleet**, the **Governance Layer** of the Fleet Stack. Having built the fleet (Part I), the distributed logic (Part II), and the serving infrastructure (Part III), we now focus on **protection**. This layer does not add new features; it wraps the entire stack in armor, ensuring that our powerful global machine cannot be hijacked, poisoned, or exploited by adversaries.

:::

## The Expanded Attack Surface {#sec-security-privacy-security-privacy-ml-systems-0b1e}

If a traditional database is breached, the attacker steals the records. If a machine learning model is breached, the attacker can systematically reverse-engineer the exact data used to train it, or worse, subtly poison the training data to introduce a silent backdoor that will only trigger when the attacker desires. Machine learning systems fundamentally change the security landscape because they do not just store data; they compress, memorize, and act upon it in ways that traditional deterministic software does not.

The operational platforms from @sec-ops-scale manage hundreds of models across distributed infrastructure, and this global reach creates an expansive attack surface. The distributed training systems from @sec-distributed-training-systems, edge deployments from @sec-edge-intelligence, and multi-tenant serving platforms all introduce vulnerabilities absent in single-machine systems. Gradient synchronization protocols create channels for information leakage and manipulation. Federated aggregation exposes model updates to interception and inference attacks. Multi-tenant serving infrastructure presents opportunities for model extraction and side-channel attacks. Each architectural decision that enables scale also creates vulnerabilities requiring systematic defense.

Machine learning systems exhibit different security characteristics compared to conventional software applications. Traditional software systems process data transiently and deterministically, whereas machine learning systems extract and encode patterns from training data into persistent model parameters. This learned knowledge representation creates unique vulnerabilities where sensitive information can be inadvertently memorized and later exposed through model outputs or systematic interrogation. Healthcare systems may leak patient information through model outputs, while proprietary models can be reverse-engineered through strategic query patterns, threatening both individual privacy and organizational intellectual property.

The architectural complexity of machine learning systems compounds these security challenges. Contemporary ML deployments include data ingestion pipelines, distributed training infrastructure, model serving systems, and continuous monitoring frameworks. Each component introduces distinct vulnerabilities that affect the entire computational stack. Continuous adaptation at edge nodes and federated coordination protocols expand the attack surface while complicating comprehensive security implementation.

::: {.callout-note title="Figure: ML System Attack Surface" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9]
  \definecolor{DataColor}{RGB}{200,220,255}
  \definecolor{ModelColor}{RGB}{255,220,200}
  \definecolor{ApiColor}{RGB}{220,255,200}
  \definecolor{HardColor}{RGB}{240,240,240}

  \tikzset{
    layer/.style={draw=black!70, thick, rounded corners=2pt, align=center, minimum width=6cm, minimum height=1.2cm}
  }

  % Layers
  \node[layer, fill=ApiColor] (Api) at (0, 4.5) {\textbf{API / Interface Layer}\\Adversarial Examples, Model Extraction,\\Membership Inference};

  \node[layer, fill=ModelColor] (Model) at (0, 3.0) {\textbf{Model Layer}\\Backdoor Injection, Trojan Weights,\\Parameter Theft};

  \node[layer, fill=DataColor] (Data) at (0, 1.5) {\textbf{Data Layer}\\Poisoning, Label Manipulation,\\Data Leakage};

  \node[layer, fill=HardColor] (Hard) at (0, 0) {\textbf{Hardware / Infrastructure Layer}\\Side-channel Attacks, Fault Injection,\\Supply Chain Compromise};

  % Connecting arrows indicating depth
  \draw[->, ultra thick, gray!40] (-3.5, 4.5) -- (-3.5, 0) node[midway, sloped, above, text=black] {Increasing Access Depth};

\end{tikzpicture}
```
**ML System Attack Surface**. Visualizing entry points for adversarial actions across the ML lifecycle. Defense requires a multi-layered approach: protecting data collection (Data Layer), securing weights and training (Model Layer), hardening inference endpoints (API Layer), and anchoring trust in silicon (Hardware Layer).
:::

Addressing these challenges requires systematic approaches that integrate security and privacy considerations throughout the machine learning system lifecycle. This chapter establishes the foundations and methodologies necessary for engineering ML systems that achieve both computational effectiveness and trustworthy operation. We examine the application of established security principles to machine learning contexts, identify threat models specific to learning systems, and present comprehensive defense strategies that include data protection mechanisms, secure model architectures, and hardware-based security implementations.

This chapter proceeds through four interconnected frameworks. We begin by establishing distinctions between security and privacy within machine learning contexts, then examine evidence from historical security incidents to inform contemporary threat assessment. We analyze vulnerabilities that emerge from the learning process itself before presenting layered defense architectures spanning cryptographic data protection, adversarial-robust model design, and hardware security mechanisms. Throughout this analysis, we emphasize implementation guidance for developing systems that meet both technical performance requirements and the trust standards necessary for societal deployment.

### Foundational Concepts and Definitions {#sec-security-privacy-foundational-concepts-definitions-d529}

Security and privacy are distinct concerns in machine learning system design that are often conflated. Both protect systems and data through different mechanisms, addressing different threat models and requiring distinct technical responses. Distinguishing between the two guides the design of responsible ML infrastructure.

### Security Defined {#sec-security-privacy-security-defined-1129}

Security in machine learning focuses on defending systems from adversarial behavior. This includes protecting model parameters, training pipelines, deployment infrastructure, and data access pathways from manipulation or misuse.

::: {.callout-definition title="Security"}

***Security***\index{Security!definition} is the protection of ML system assets—data, models, and infrastructure—from unauthorized access, manipulation, or disruption.

1.  **Significance (Quantitative):** It ensures the **Integrity** of the system's learned behavior and the **Availability** of its services. Within the **Iron Law**, security failures (e.g., DDoS, model theft) can collapse the **Duty Cycle ($\eta$)** or compromise the proprietary value of the model operations ($O$).
2.  **Distinction (Durable):** Unlike **General Robustness** (which handles stochastic environmental errors), Security addresses **Intentional Adversarial Threats** designed to exploit system vulnerabilities.
3.  **Common Pitfall:** A frequent misconception is that security is "solved" by traditional IT firewalling. In reality, ML systems introduce a new **Attack Surface** at the algorithmic layer (e.g., prompt injection, adversarial examples) that traditional security tools cannot detect.

:::

*Example*: A facial recognition system deployed in public transit infrastructure may be targeted with adversarial inputs that cause it to misidentify individuals or fail entirely, representing a runtime security vulnerability that threatens both accuracy and system availability.

### Privacy Defined {#sec-security-privacy-privacy-defined-da84}

While security addresses adversarial threats, privacy focuses on limiting the exposure and misuse of sensitive information within ML systems. This includes protecting training data, inference inputs, and model outputs from leaking personal or proprietary information, even when systems operate correctly and no explicit attack is taking place.

::: {.callout-definition title="Privacy"}

***Privacy***\index{Privacy!definition} is the protection of sensitive information from unauthorized disclosure, inference, and misuse across the ML lifecycle.

1.  **Significance (Quantitative):** It limits the **Exposure Risk** of training data and user inputs. Privacy-preserving techniques (e.g., Differential Privacy) typically introduce a **Utility-Privacy Trade-off**: increasing privacy adds "noise" to the gradients, which can increase the **Total Operations ($O$)** required to reach a target accuracy.
2.  **Distinction (Durable):** Unlike **Confidentiality** (which focuses on access control), Privacy in ML focuses on **Inference Risks**: the ability of an observer to reconstruct sensitive training samples from the model's outputs or weights.
3.  **Common Pitfall:** A frequent misconception is that removing names (de-identification) is sufficient for privacy. In reality, neural networks are **Correlation Engines** that can inadvertently memorize and leak unique snippets of sensitive data through high-dimensional patterns.

:::

*Example*: A language model trained on medical transcripts may inadvertently memorize snippets of patient conversations. If a user later triggers this content through a public-facing chatbot, it represents a privacy failure, even in the absence of an attacker.

The strongest formal guarantee for privacy protection is *differential privacy*, which adds calibrated noise to computations so that no individual's data can be reverse-engineered from the output. However, this guarantee comes at a measurable cost to accuracy.

```{python}
#| label: dp-cost-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ COST OF DIFFERENTIAL PRIVACY (LEGO)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-security-privacy-privacy-defined-da84
# │
# │ Goal: Quantify the noise magnitude and error impact of DP for salary mean.
# │ Show: ~$200 error per person for N=1000, $2000 for N=100 at eps=1.
# │ How: Noise_scale = Sensitivity / Epsilon; Error = Noise / N.
# │
# │ Imports: (none)
# │ Exports: n_emp_dp, sensitivity_dp_str, eps_dp, noise_scale_dp_str,
# │          error_per_person_str, n_small_dp, error_small_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── LEGO ───────────────────────────────────────────────
class DPCostAnalysis:
    """Quantify the utility cost of differential privacy noise."""

    # ┌── 1. LOAD (Constants) ──────────────────────────────────────────────
    n_employees = 1000
    salary_range = 200000
    epsilon = 1.0
    n_small = 100

    # ┌── 2. EXECUTE (The Compute) ────────────────────────────────────────
    sensitivity = salary_range # max one person can change the sum
    noise_scale = sensitivity / epsilon
    error_per_person = noise_scale / n_employees
    error_small = noise_scale / n_small

    # ┌── 3. GUARD (Invariants) ──────────────────────────────────────────
    check(error_per_person == 200, f"Expected $200 error, got {error_per_person}")

    # ┌── 4. OUTPUT (Formatting) ──────────────────────────────────────────────
    sensitivity_str = f"{sensitivity:,}"
    noise_scale_str = f"{noise_scale:,.0f}"
    error_per_person_str = f"{error_per_person:,.0f}"
    error_small_str = f"{error_small:,.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
n_emp_dp = DPCostAnalysis.n_employees
sensitivity_dp_str = DPCostAnalysis.sensitivity_str
eps_dp = DPCostAnalysis.epsilon
noise_scale_dp_str = DPCostAnalysis.noise_scale_str
error_per_person_str = DPCostAnalysis.error_per_person_str
n_small_dp = DPCostAnalysis.n_small
error_small_str = DPCostAnalysis.error_small_str
```

::: {.callout-notebook title="The Cost of Differential Privacy"}
**Problem**: You want to compute the average salary of **`{python} n_emp_dp` employees** while guaranteeing privacy ($\epsilon=`{python} eps_dp`). The salaries range from \$0 to \$`{python} sensitivity_dp_str`. How much noise must you add?

**The Math**:

1.  **Sensitivity ($S$)**: The maximum one person can change the sum is **\$`{python} sensitivity_dp_str`**.
2.  **Privacy Budget ($\epsilon$)**: `{python} eps_dp`.
3.  **Laplace Noise Scale ($b$)**: $S / \epsilon = `{python} sensitivity_dp_str` / `{python} eps_dp` = \mathbf{`{python} noise_scale_dp_str`}$.
4.  **Impact on Mean**: The noise added to the *sum* has magnitude $\approx `{python} noise_scale_dp_str`.
    *   Noise per person (average) = `{python} noise_scale_dp_str` / `{python} n_emp_dp` = **\$`{python} error_per_person_str`**.

**The Systems Conclusion**: To protect one outlier, you introduce a **\$`{python} error_per_person_str` error** to the average. For a dataset of $N=`{python} n_small_dp`, the error would be **\$`{python} error_small_str`**! Differential Privacy kills utility for small $N$. It only works at scale where $1/N$ dampens the noise.
:::

### Security versus Privacy {#sec-security-privacy-security-versus-privacy-e0b8}

Although they intersect in some areas such as encrypted storage, security and privacy differ in their objectives, threat models, and typical mitigation strategies. @tbl-security-privacy-comparison contrasts these two domains across six dimensions, showing how their distinct goals shape the specific concerns and defenses practitioners must consider.

| **Aspect**                  | **Security**                               | **Privacy**                                   |
|:----------------------------|:-------------------------------------------|:----------------------------------------------|
| **Primary Goal**            | Prevent unauthorized access or disruption  | Limit exposure of sensitive information       |
| **Threat Model**            | Adversarial actors (external or internal)  | Honest-but-curious observers or passive leaks |
| **Typical Concerns**        | Model theft, poisoning, evasion attacks    | Data leakage, re-identification, memorization |
| **Example Attack**          | Adversarial inputs cause misclassification | Model inversion reveals training data         |
| **Representative Defenses** | Access control, adversarial training       | Differential privacy, federated learning      |
| **Relevance to Regulation** | Emphasized in cybersecurity standards      | Central to data protection laws (e.g., GDPR)  |

: **Security-Privacy Distinctions**: Machine learning systems require distinct approaches to security and privacy; security mitigates adversarial threats targeting system functionality, while privacy protects sensitive information from both intentional and unintentional exposure through data leakage or re-identification. This table clarifies how differing goals and threat models shape the specific concerns and mitigation strategies for each domain. {#tbl-security-privacy-comparison}

### Security-Privacy Interactions and Trade-offs {#sec-security-privacy-securityprivacy-interactions-tradeoffs-d153}

Although security and privacy share common goals, they impose distinct and sometimes conflicting engineering constraints.

::: {.callout-perspective title="The Privacy-Utility Trade-off"}
Security and privacy are deeply interrelated but not interchangeable. A secure system helps maintain privacy by restricting unauthorized access to models and data. Privacy-preserving designs can improve security by reducing the attack surface; minimizing the retention of sensitive data reduces the risk of exposure if a system is compromised.

However, they can also be in tension. Techniques like differential privacy reduce memorization risks but may lower model utility. Similarly, encryption enhances security but may obscure transparency and auditability, complicating privacy compliance. Designers must reason about these trade-offs holistically.
:::

Systems serving sensitive domains such as healthcare, finance, and public safety must simultaneously protect against both misuse and overexposure. Understanding the boundaries between these concerns is essential for building systems that are performant, trustworthy, and legally compliant.

Understanding the boundaries between security and privacy is critical because interventions that improve one often degrade the other. To see how these theoretical vulnerabilities manifest in practice—and to understand the devastating consequences when they are ignored—we must examine how historical security failures have adapted to target modern algorithmic systems.

## Learning from Security Breaches {#sec-security-privacy-learning-security-breaches-6719}

Three landmark security incidents -- a supply chain attack on industrial controllers, a remote exploit of an automobile, and a botnet built from consumer devices -- expose attack patterns that recur in machine learning deployments. Each incident establishes a quantitative constraint that ML system designers must internalize: the cost of a multi-vector supply chain weapon, the recall cost of insufficient isolation, and the scale at which default credentials become weaponized infrastructure. Although none of these incidents targeted ML systems directly, every attack vector they demonstrated has a direct analog in modern training pipelines, inference APIs, and edge deployments.

### Supply Chain Compromise: Stuxnet {#sec-security-privacy-supply-chain-compromise-stuxnet-8a4b}

In 2010, the [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery] worm infiltrated Iran's Natanz nuclear facility by chaining four zero-day[^fn-zero-day-term] exploits -- a multi-million-dollar weapon -- to reach air-gapped[^fn-air-gapped] programmable logic controllers (PLCs) via a USB device[^fn-usb-attacks] [@farwell2011stuxnet]. Rather than crashing the centrifuges outright, it subtly altered rotational speeds while reporting normal telemetry to operators, demonstrating that manipulating a system's learned parameters is more devastating than disabling it. The ML parallel is precise: an attacker who poisons training data or injects a backdoored model into a trusted repository does not need to crash the inference server; a silent shift in the model's decision boundary achieves the same effect while evading standard monitoring.

[^fn-stuxnet-discovery]: **Stuxnet**: First detected in 2010 by VirusBlokAda, a Belarusian antivirus firm, Stuxnet was the first confirmed cyberweapon engineered to cause physical destruction. Its use of four simultaneous zero-day exploits set a precedent for ML supply chain attacks: just as Stuxnet compromised industrial controllers through trusted software update channels, modern attacks inject backdoored models through trusted repositories. \index{Stuxnet!supply chain}

[^fn-zero-day-term]: **Zero-Day** (from piracy slang for "zero days since release"): In security, the term denotes vulnerabilities with zero days of available defense. Stuxnet's simultaneous use of four zero-days was unprecedented; the black-market value of a single zero-day exploit exceeds $1 million, making a four-exploit chain a multi-million-dollar weapon with direct implications for ML systems where unpatched model-serving frameworks create analogous zero-day exposure windows. \index{Zero-Day!etymology}

[^fn-air-gapped]: **Air-Gapped** (from the literal physical gap between network cables): Networks physically isolated from external connections, a practice dating to 1960s military computing. For ML systems, air-gapping training clusters from the internet prevents data exfiltration but forces all dependencies (frameworks, datasets, pretrained weights) through manual transfer channels, each of which becomes a potential supply chain attack vector. \index{Air-Gapped!isolation}

[^fn-usb-attacks]: **USB Attack Vector**: USB (Universal Serial Bus, 1996) became the primary vector for bridging air gaps after the 2008 Operation Olympic Games reportedly used infected drives to penetrate classified facilities. For ML deployments on isolated training clusters, USB-transferred datasets and model checkpoints represent the same class of risk: a single compromised checkpoint file can embed backdoors that survive across retraining cycles. \index{USB!attack vector}

Modern ML supply chains face four analogous vectors: compromised dependencies (malicious packages in PyPI and conda repositories), poisoned datasets on public platforms, backdoored model weights in model repositories, and tampered accelerator firmware. Defense requires cryptographic signing of all model artifacts, immutable provenance logs for training data and code, automated scanning for backdoors before deployment, and controlled dependency management in air-gapped training environments. @fig-stuxnet maps these parallels between the Stuxnet attack chain and modern ML supply chain vulnerabilities.

::: {#fig-stuxnet fig-env="figure" fig-pos="htb" fig-cap="**Stuxnet**: Targets PLCs by exploiting Windows and Siemens software vulnerabilities, demonstrating supply chain compromise that enabled digital malware to cause physical infrastructure damage. Modern ML systems face analogous risks through compromised training data, backdoored dependencies, and tampered model weights." fig-alt="Flowchart showing Stuxnet attack chain: USB infection spreads through Windows vulnerabilities, targets Siemens Step7 software, compromises PLCs, and causes physical centrifuge damage."}
```{.tikz}
\begin{tikzpicture}[line cap=round,line join=round,font=\usefont{T1}{phv}{m}{n}]
\tikzset{%
TxtL/.style = {font=\large\usefont{T1}{phv}{m}{n},text width=90mm,align=justify,anchor=north},
Line/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},
LineA/.style={violet!50,line width=2.0pt,{-{Triangle[width=1.1*6pt,length=2.0*6pt]}},shorten <=3pt,shorten >=2pt},
ALine/.style={black!50, line width=1.1pt,{{Triangle[width=1.1*6pt,length=2*6pt]}-}},
Larrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,
            single arrow head indent=0pt,minimum height=9mm, minimum width=15pt}
}
%Skull
\tikzset{pics/skull/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=SKULL,scale=\scalefac, every node/.append style={transform shape}]
\fill[fill=\filllcolor](-0.225,-0.05)to[out=110,in=230](-0.215,0.2)to[out=50,in=180](0,0.315)
to[out=0,in=130](0.218,0.2)to[out=310,in=70](0.227,-0.05) to[out=320,in=40](0.21,-0.15)
to[out=210,in=80](0.14,-0.23) to[out=260,in=20](0.04,-0.285) to[out=200,in=340](-0.07,-0.28)
to[out=170,in=290](-0.135,-0.23) to[out=110,in=340](-0.21,-0.15) to[out=140,in=250]cycle;
%eyes
\fill[fill=\filllcirclecolor](-0.17,-0.02)to[out=70,in=110](-0.029,-0.02)to[out=280,in=0](-0.129,-0.11)to[out=190,in=250]cycle;
\fill[fill=\filllcirclecolor](0.035,-0.02)to[out=70,in=110](0.175,-0.02)to[out=300,in=340](0.12,-0.103)to[out=170,in=260]cycle;
%nose
\fill[fill=\filllcirclecolor](0.018,-0.115)to[out=70,in=110](-0.014,-0.115)to(-0.043,-0.165)
to[out=200,in=170](-0.025,-0.19)to(0.027,-0.19)to[out=10,in=330](0.047,-0.165)to cycle;
%above left
\fill[fill=\filllcolor](-0.2,0.18)to[out=160,in=320](-0.3,0.23)to[out=140,in=0](-0.37,0.295)
to[out=180,in=80](-0.43,0.25)to[out=230,in=90](-0.475,0.19)
to[out=260,in=170](-0.375,0.13)to[out=350,in=170](-0.2,0.1)to cycle;
%abover right
\fill[fill=\filllcolor](0.2,0.18)to[out=20,in=220](0.3,0.23)to[out=40,in=200](0.37,0.295)
to[out=20,in=90](0.43,0.25)to[out=230,in=90](0.475,0.19)to[out=260,in=360](0.375,0.13)
to[out=190,in=10](0.2,0.1)to cycle;
%below left
\fill[fill=\filllcolor](-0.2,0.03)to[out=210,in=0](-0.3,0.01)to[out=180,in=0](-0.37,0.01)
to[out=180,in=50](-0.46,0.0)to[out=230,in=120](-0.445,-0.08)
to[out=260,in=170](-0.41,-0.14)to[out=350,in=190](-0.2,-0.051)to cycle;
%below right
\fill[fill=\filllcolor](0.2,0.03)to[out=340,in=170](0.3,0.01)to[out=350,in=190](0.37,0.01)
to[out=20,in=110](0.47,-0.03)to[out=270,in=120](0.443,-0.09)
to[out=270,in=0](0.36,-0.15)to[out=160,in=340](0.2,-0.051)to cycle;
\end{scope}
     }
  }
}
%laptop
\tikzset{
pics/laptop/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[rounded corners=2pt,rectangle,minimum width=60,minimum height=37,
fill=\filllcolor!60,line width=\Linewidth,draw=black](EKV\picname)at(0,0.53){};
%
\ifnum\Dual=1
\node[draw=black,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,
fill=\filllcolor!10,line width=\Linewidth,](EK)at(0,0.53){};
\coordinate(SM1)at($(EK.south west)+(0.15,0.5)$);
\coordinate(SM2)at($(EK.south east)+(-1.1,0.5)$);
\coordinate(OK1)at($(EK.220)+(0,0.7)$);
\coordinate(OK2)at($(EK.240)+(0,0.7)$);
\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};
\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};
\draw[line width=1.4pt](SM1)to [bend right=45](SM2);
%%
\coordinate(4BL)at($(EK.south west)+(0.95,0.3)$);
    \def\n{5}          % broj boksova
    \def\w{0.12}        % box width (mm)
    \def\h{0.5}       % Box height (mm)
    \def\gap{0.05}      % razmak između boksova (mm)
    % niz boksova
    \foreach \i in {0,...,4} {
      \pgfmathsetmacro{\x}{\i*(\w+\gap)}
      % padding (we clip inside the edges)
      \begin{scope}
        \clip[] ($(4BL)+(\x,0)$) rectangle ++(\w,\h);
        \fill[gray!10]($(4BL)+(\x,0)$) rectangle ++(\w,\h*1);
        \fill[fill=\filllcirclecolor]($(4BL)+(\x,0)$) rectangle ++(\w,\h*\Level);
      \end{scope}
      % kontura preko
      \draw[line width=0.6pt,draw=black]($(4BL)+(\x,0)$)  rectangle ++(\w,\h);
    }
\else
\ifnum\Smile=1
\node[draw=black,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,
fill=\filllcolor!10,line width=\Linewidth,](EK)at(0,0.53){};
\coordinate(SM1)at($(EK.south west)+(0.32,0.5)$);
\coordinate(SM2)at($(EK.south east)+(-0.32,0.5)$);
\coordinate(OK1)at($(EK.250)+(0,0.7)$);
\coordinate(OK2)at($(EK.290)+(0,0.7)$);
\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};
\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};
\draw[line width=1.4pt](SM1)to [bend right=45](SM2);
\else
\node[draw=green,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,draw=black,fill=black](EK)at(0,0.53){};
\pic[shift={(0,0)}] at  (EK){skull={scalefac=1.3,picname=1,filllcolor=white, filllcirclecolor=black,Linewidth=0.5pt}};
\fi
\fi
%
\draw[fill=\filllcolor!60!black!30,line width=\Linewidth](-1.00,-0.1)--(1.0,-0.1)--(1.28,-0.6)--(-1.28,-0.6)--cycle;
\draw[fill=\filllcolor!60!black!30,line width=\Linewidth](1.28,-0.6)--(-1.28,-0.6)arc[start angle=180, end angle=270, radius=4pt]--(1.14,-0.73)
arc[start angle=270, end angle=355, radius=4pt]--cycle;
\draw[fill=\filllcolor!30!black!10,line width=\Linewidth](-0.95,-0.17)--(0.95,-0.17)--(1.03,-0.34)--(-1.03,-0.34)--cycle;
\draw[fill=\filllcolor!30!black!20,line width=\Linewidth](-0.16,-0.52)--(0.16,-0.52)--(0.14,-0.42)--(-0.14,-0.42)--cycle;
\end{scope}
    }
  }
}
%usb
\tikzset{
pics/usb/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\draw[draw=black,fill=\filllcolor,line width=\Linewidth](-0.65,0.94)coordinate(GL\picname)--(0.65,0.94)coordinate(GD\picname)--
(0.65,-1.3)coordinate(DD\picname)arc[start angle=0, end angle=-90, radius=2mm]
--(-0.45,-1.5)coordinate(DL\picname)arc[start angle=-90, end angle=-180, radius=2mm]--cycle;
\node[draw=none,fill=\filllcirclecolor,minimum width=5mm,minimum height=8mm,anchor=south]at($($(DL\picname)!0.42!(DD\picname)$)+(0,0.35)$){};
\coordinate(G1)at($(GL\picname)!0.15!(GD\picname)$);
\coordinate(G2)at($(GL\picname)!0.85!(GD\picname)$);
\draw[draw=black,fill=\filllcolor!40,line width=\Linewidth](G1)--++(0,1)coordinate(G11)-|coordinate[pos=0.5](G22)(G2)--cycle;
\node[draw=none,fill=black,inner sep=0pt,minimum width=1mm,minimum height=6mm,anchor=west]at($($(G1)!0.5!(G11)$)+(0.2,0)$){};
\node[draw=none,fill=black,inner sep=0pt,minimum width=1mm,minimum height=6mm,anchor=east]at($($(G2)!0.5!(G22)$)+(-0.2,0)$){};
%\fill[red](G22)circle(2pt);
\end{scope}
    }
  }
}
 \tikzset{/pgf/decoration/.cd,
    number of sines/.initial=10,
    angle step/.initial=20,
}
\newdimen\tmpdimen
\pgfdeclaredecoration{complete sines}{initial}
{
    \state{initial}[
        width=+0pt,
        next state=move,
        persistent precomputation={
            \pgfmathparse{\pgfkeysvalueof{/pgf/decoration/angle step}}%
            \let\anglestep=\pgfmathresult%
            \let\currentangle=\pgfmathresult%
            \pgfmathsetlengthmacro{\pointsperanglestep}%
                {(\pgfdecoratedremainingdistance/\pgfkeysvalueof{/pgf/decoration/number of sines})/360*\anglestep}%
        }] {}
    \state{move}[width=+\pointsperanglestep, next state=draw]{
        \pgfpathmoveto{\pgfpointorigin}
    }
    \state{draw}[width=+\pointsperanglestep, switch if less than=1.25*\pointsperanglestep to final, % <- bit of a hack
        persistent postcomputation={
        \pgfmathparse{mod(\currentangle+\anglestep, 360)}%
        \let\currentangle=\pgfmathresult%
    }]{%
        \pgfmathsin{+\currentangle}%
        \tmpdimen=\pgfdecorationsegmentamplitude%
        \tmpdimen=\pgfmathresult\tmpdimen%
        \divide\tmpdimen by2\relax%
        \pgfpathlineto{\pgfqpoint{0pt}{\tmpdimen}}%
    }
    \state{final}{
        \ifdim\pgfdecoratedremainingdistance>0pt\relax
            \pgfpathlineto{\pgfpointdecoratedpathlast}
        \fi
   }
}
%testing_medal
\tikzset{
pics/testing/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=TESTING1,shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\newcommand{\tikzxmark}{%
\tikz[scale=0.18] {
    \draw[line width=0.7,line cap=round,GreenLine] (0,0) to [bend left=6] (1,1);
    \draw[line width=0.7,line cap=round,GreenLine] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}
\newcommand{\tikzxcheck}{%
\tikz[scale=0.16] {
\draw[line width=0.7,line cap=round,GreenLine] (0.5,0.75)--(0.85,-0.1) to [bend left=16] (1.5,1.55);
}}
 \node[draw, minimum width  =15mm, minimum height = 20mm, inner sep = 0pt,
        rounded corners,draw = \drawcolor, fill=\filllcolor!10, line width=\Linewidth](COM){};
\node[draw=GreenLine,inner sep=4pt,fill=white](CB1) at ($(COM.north west)!0.25!(COM.south west)+(0.3,0)$){};
\node[draw=GreenLine,inner sep=4pt,fill=white](CB2) at ($(COM.north west)!0.5!(COM.south west)+(0.3,0)$){};
\node[draw=GreenLine,inner sep=4pt,fill=white](CB3) at ($(COM.north west)!0.75!(COM.south west)+(0.3,0)$){};
\node[xshift=0pt]at(CB1){\tikzxcheck};
\node[xshift=0pt]at(CB2){\tikzxmark};
\node[xshift=0pt]at(CB3){\tikzxmark};
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,-0.12)$)--++(0:0.7);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,-0.12)$)--++(0:0.6);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,-0.12)$)--++(0:0.6);
\end{scope}
\begin{scope}[shift={($(0,0)+(0.4,-0.50)$)},scale=0.5\scalefac,every node/.append style={transform shape}]
\draw[draw=none,fill=\filllcolor!60](-0.48,-0.10)--(-0.68,-0.68)--(-0.92,-1.38)--
(-0.53,-1.28)--(-0.29,-1.61)--(-0.09,-0.93)--(0.15,-0.1)--cycle;
\draw[draw=none,fill=\filllcolor!60](-0.266,-0.10)--(-0.02,-0.93)--(0.18,-1.61)--
(0.45,-1.34)--(0.85,-1.48)--(0.61,-0.68)--(0.44,-0.1)--cycle;
 \draw[draw=none,postaction={very thick, line join=round, draw=white,fill=\filllcolor,
        decorate,decoration={complete sines, number of sines=9, amplitude=\scalefac*2pt}}] (0,0) circle [radius=0.9];
\node[draw=none,fill=white,circle,minimum size=11mm,line width=1pt](CM-\picname) {};
%
\end{scope}
    }
  }
}
%PLC
\tikzset{
pics/plc/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\node[draw=\filllcolor,fill=\filllcolor!30,minimum width=21mm,minimum height=24mm](R\picname){};
\coordinate(P)at($(R\picname.north west)!0.1!(R\picname.south east)$);
% box dimensions
  \def\boxsize{1.3mm}
  \def\xstep{1.8mm}  % gap between columns
  \def\ystep{1.9mm}  % razmak između redova
% Lista obojenih ćelija – bez duplih zagrada, plus čuvar-zarezi
\def\coloredcells{0/0,1/1,2/1,0/2,1/3,0/4,2/4,1/5,1/6,2/6,0/7}

\foreach \i in {0,1,2} {
  \foreach \j in {0,1,2,3,4,5,6,7} {
    \edef\cellid{\i/\j}
    \def\fillcolor{cyan!10}
    % Unutrašnja petlja pravi grupu; zato koristimo \global
    \foreach \c in \coloredcells {%
      \ifx\cellid\c
        \global\def\fillcolor{green!60}%
      \fi
    }
    \node[draw=black, fill=\fillcolor, minimum size=\boxsize, inner sep=0pt](MB\i\j)
      at ($(P) + (\i*\xstep, -\j*\ystep)$) {};
  }
}
\coordinate(1BL)at($(MB07.south west)+(0,-1mm)$);
\node[draw=black,fill=\filllcolor!10,anchor=north west,inner sep=0pt,
minimum width=5mm,minimum height=1.5mm](BX1)at(1BL){};
\coordinate(2BL)at($(BX1.south west)+(0,-0.8mm)$);
\node[draw=black,fill=\filllcolor!50!black!20,anchor=north west,inner sep=0pt,
minimum width=5mm,minimum height=3.0mm](BX2)at(2BL){};
\coordinate(3BL)at($(BX2.south east)+(1mm,0)$);
\node[draw=black,fill=\filllcolor!70!black!30,anchor=south west,inner sep=0pt,
minimum width=12mm,minimum height=3.0mm](BX3)at(3BL){};
\path[red](BX3.north west)|-coordinate[pos=0.5](1E)(MB27.south east);
%display
\ifnum\Smile=1
\node[draw=black,fill=\filllcolor!10,anchor=south west,inner sep=0pt,
minimum width=12mm,minimum height=8.0mm](EK)at(1E){};
\coordinate(SM1)at($(EK.south west)+(0.32,0.35)$);
\coordinate(SM2)at($(EK.south east)+(-0.32,0.35)$);
\coordinate(OK1)at($(EK.250)+(0,0.55)$);
\coordinate(OK2)at($(EK.290)+(0,0.55)$);
\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};
\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};
\draw[line width=1.0pt](SM1)to [bend right=25](SM2);
\else
\node[draw=black,fill=black,anchor=south west,inner sep=0pt,
minimum width=12mm,minimum height=8.0mm](EK)at(1E){};
%skull
\pic[shift={(0,0)}] at  (EK){skull={scalefac=1,picname=1,filllcolor=white, filllcirclecolor=black,Linewidth=0.5pt}};
\fi
\draw[fill=\filllcolor!40!black!30](-0.2,-0.67)--(0.8,-0.67)--(0.72,-0.54)--(-0.14,-0.54)--cycle;
\coordinate(4BL)at($(EK.north west)+(0,1mm)$);
 % geometry
    \def\n{5}          % broj boksova
    \def\w{0.2}        % širina boksa (mm)
    \def\h{0.5}       % visina boksa (mm)
    \def\gap{0.05}      % razmak između boksova (mm)
    % niz boksova
    \foreach \i in {0,...,4} {
      \pgfmathsetmacro{\x}{\i*(\w+\gap)}
      % popuna (klipujemo unutar ivica)
      \begin{scope}
        \clip[] ($(4BL)+(\x,0)$) rectangle ++(\w,\h);
        \fill[gray!10]($(4BL)+(\x,0)$) rectangle ++(\w,\h*1);
        \fill[fill=\filllcirclecolor]($(4BL)+(\x,0)$) rectangle ++(\w,\h*\Level);
      \end{scope}
      % contour over
      \draw[line width=0.6pt,draw=black]($(4BL)+(\x,0)$)  rectangle ++(\w,\h);
    }
\end{scope}
    }
  }
}
%copier
\tikzset{
pics/copier/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
\draw[fill=\filllcolor!60!black,line width=\Linewidth,draw=black](0.1,1.15)--++(150:0.85)arc[start angle=90, end angle=200,radius=1.5pt]--++(330:0.82)--cycle;
\draw[fill=\filllcolor!30,line width=\Linewidth,draw=black](-0.73,0.75)--(0.69,0.75)--(0.69,0.41)--(-0.73,0.41)--cycle;
\draw[fill=\filllcolor!60,line width=\Linewidth,draw=black](-0.80,1.05)--(-0.02,1.05)--(0.1,1.15)--(0.745,1.15)--(0.745,0.75)--(-0.80,0.75)--cycle;
\draw[draw=none,fill=\filllcirclecolor](0.12,1.08)--(0.49,1.08)coordinate(DISNE)--(0.49,0.83)coordinate(DISSE)--(0.12,0.83)--cycle;
\node[draw=none,fill=\filllcirclecolor,circle,inner sep=0pt,minimum size=1mm,below right=0.5pt and 2pt of DISNE]{};
\node[draw=none,fill=\filllcirclecolor,circle,inner sep=0pt,minimum size=1mm,above right=0.5pt and 2pt of DISSE]{};
%
\draw[fill=\filllcolor!60!black,line width=\Linewidth,draw=black](0.745,0.-0.07)--(0.87,0)arc[start angle=130, end angle=30,radius=2.5pt]--(1.35,0.16)
arc[start angle=120, end angle=-10,radius=1.25pt]--(0.745,0.-0.22)--cycle;
%
\draw[fill=\filllcolor!30,line width=\Linewidth,draw=black](-0.8,0.41)--(0.745,0.41)--(0.745,-1.18)--(-0.8,-1.18)--cycle;
%
\draw[line width=2*\Linewidth](-0.72,0.0)--coordinate[pos=0.5](SR1)(0.665,0);
\node[draw=black,fill=\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR1){};
\draw[line width=2*\Linewidth](-0.72,-0.4)--coordinate[pos=0.5](SR2)(0.665,-0.4);
\node[draw=black,fill=\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR2){};
\draw[line width=2*\Linewidth](-0.72,-0.8)--coordinate[pos=0.5](SR3)(0.665,-0.8);
\node[draw=black,fill=\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR3){};
%
\end{scope}
    }
  }
}
%vijak
\tikzset{
pics/sraf/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\scalefac,every node/.append style={transform shape}]
%fire
\ifnum\Fire=1
\fill[fill=red!70,thick](0,1.6)--(0.4,0.6)--(1.4,2)--(2.2,0.8)--(3.1,1.8)--(3,0.5)--(3.7,0.6)--(3.0,-1.2)--
(3.4,-1.3)--(2.0,-2.86)--(-1.8,-2.86)--(-3.3,-1.36)--(-2.8,-1.36)--(-3.2,-0.1)--(-2.6,-0.3)--(-2.9,2.5)--
(-2.3,1.4)--(-1.4,2.9)--(-0.6,0.9)--cycle;
\fi
\foreach \x in {-2,-0.65,0.72,2.1}{
\node[draw=\drawcolor,fill=\filllcolor!80,line width=1.5*\Linewidth,inner sep=0pt,outer sep=0pt,
minimum width=6mm,minimum height=50mm](SRA1)at(\x,0){};
\node[draw=\drawcolor,fill=\filllcolor!50,line width=1.5*\Linewidth,,trapezium,inner ysep=2pt,anchor=north,outer sep=0pt,inner xsep=3pt,
minimum width=8mm,minimum height=4mm](TR1)at(SRA1.south){};
\begin{scope}
\clip(SRA1.south west)rectangle (SRA1.north east);
\foreach \i [evaluate=\i as \y using {\i+0.03}]in {0,0.07,0.14,...,0.99}{
\draw[black,thick]($(SRA1.south west)!\i!(SRA1.north west)$)--($(SRA1.south east)!\y!(SRA1.north east)$);
\draw[draw=\drawcolor,line width=1.5*\Linewidth](SRA1.south west)rectangle (SRA1.north east);
}
\end{scope}
}
\end{scope}
    }
  }
}
\pgfkeys{
  /channel/.cd,
   Dual/.store in=\Dual,
   Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  Fire/.store in=\Fire,
  Smile/.store in=\Smile,
  Level/.store in=\Level,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawcolor/.store in=\drawcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  filllcolor=BrownLine,
  filllcirclecolor=cyan!40,
  drawcolor=black,
  drawcircle=violet,
  scalefac=1,
  Dual=1,
  Fire=1,
  Smile=1,
  Level=0.52,
  Linewidth=0.5pt,
  Depth=1.3,
  Height=0.8,
  Width=1.1,
  picname=C
}
%
\begin{scope}[local bounding box=LAPTOP1,shift={($(0,0)+(0,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.5,picname=1,drawcolor=GreenD,Dual=0,Smile=0,
filllcolor=GreenD!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};
\end{scope}
%
\begin{scope}[local bounding box=USB1,shift={($(LAPTOP1)+(3.7,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)},rotate=320] at  (0,0){usb={scalefac=0.6,picname=1,drawcolor=orange,filllcirclecolor=white,filllcolor=violet!50!black!50!,Linewidth=1.0pt,}};
\end{scope}
%
\begin{scope}[local bounding box=TESTING1,shift={($(USB1)+(3.1,0.45)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,-0.5)}] at  (0,0){testing={scalefac=1,picname=1,drawcolor=OrangeLine,filllcolor=OrangeLine, Linewidth=1.0pt}};
\end{scope}
%
\begin{scope}[local bounding box=LAPTOP2,shift={($(TESTING1)+(3.5,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=2,drawcolor=red,Dual=0,Smile=1,
filllcolor=red!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};
\end{scope}
\tikzset{%
    LineZ/.style={-*,green!50!black,line width=1pt}
}
\begin{scope}[local bounding box=LAPTOP3,shift={($(LAPTOP2)+(6.1,-0.2)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=3,drawcolor=red,Dual=0,Smile=1,
filllcolor=yellow!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};
\end{scope}
\draw[LineZ](EKV3.north)--++(90:1.2)node[above,black]{\huge ?};
\draw[LineZ] ($(EKV3.north)+(0,0.2)$)--++(40:1);
\draw[LineZ] ($(EKV3.north)+(0,0.2)$)--++(140:1);
\draw[LineZ](EKV3.west)--++(180:1.2)node[left,black]{\huge ?};
\draw[LineZ] ($(EKV3.west)+(-0.2,0)$)--++(140:1);
\draw[LineZ] ($(EKV3.west)+(-0.2,0)$)--++(220:1);
\draw[LineZ](EKV3.east)--++(0:1.2)node[right,black]{\huge ?};
\draw[LineZ] ($(EKV3.east)+(0.2,0)$)--++(40:1);
\draw[LineZ] ($(EKV3.east)+(0.2,0)$)--++(320:1);
%
\begin{scope}[local bounding box=LAPTOP4,shift={($(LAPTOP2)+(12.5,-0.2)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=4,drawcolor=red,Dual=0,Smile=1,
filllcolor=cyan!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};
%
\draw[LineZ](EKV4.east)--node[above,black]{\huge !}
node[below=3pt,black,fill=magenta!20,circle,inner sep=1pt](CIRC1){\large 1}++(0:1.2);
\end{scope}
%
\begin{scope}[local bounding box=PLC1,shift={($(LAPTOP4)+(3.5,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=1,drawcolor=OrangeLine,filllcirclecolor=cyan, Smile=1,
filllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};
\end{scope}
%
\coordinate(SR1)at($(LAPTOP1.east)!0.45!(USB1.west)$);
\node[Larrow]at(SR1){};
\coordinate(SR2)at($(USB1.east)!0.4!(TESTING1.west)$);
\node[Larrow]at(SR2){};
\coordinate(SR3)at($(TESTING1.east)!0.5!(LAPTOP2.west)$);
\node[Larrow]at(SR3){};
%text below first row
\node[draw=none,fit=(LAPTOP1)(LAPTOP2)](BB1){};
\node[TxtL,below=6pt of BB1.south west,text width=110mm,anchor=north west](GT1){\textcolor{red}{\textbf{1. Infection}}\\[0.35ex]
Stuxnet enters a system via a USB stick and proceeds
to infect all machines running Microsoft Windows. By brandishing a digital certificate that seems to
show that it comes from a reliable company, the worm is able to evade automated-detection systems.};
%
\path[red](BB1.south east)-|coordinate[pos=0.5](T2)(EKV3.south);
\node[TxtL,below=6pt of T2.south,text width=80mm,xshift=-12mm](GT2){\textcolor{red}{\textbf{2. Search}}\\[0.35ex]
Stuxnet then checks whether a given machine is part of
the targeted industrial control system made by Siemens. Such systems are deployed in Iran to run high-speed centrifuges that
help to enrich nuclear fuel.};
\path[red](BB1.south east)-|coordinate[pos=0.5](T3)(CIRC1);
\node[TxtL,below=6pt of T3.south,text width=67mm](GT3){\textcolor{red}{\textbf{3. Update}}\\[0.35ex] If the system isn’t a target, Stuxnet does nothing;
if it is, the worm attempts to access the Internet and download a more recent version of itself.};
%
\node[draw=none,fit=(BB1)(GT2)(GT3)(PLC1)](BOX1){};
\draw[BrownLine,line width=2pt]([yshift=-2mm]BOX1.south west)coordinate(LE)
--([yshift=-2mm]BOX1.south east)coordinate(DE);
%%%%%%%%%%%%%%%%%%%
%Row below
%%%%%%%%%%%%%%%%%%%
\begin{scope}[local bounding box=LAPTOP5,shift={($(LAPTOP1)+(-0.6,-7.5)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=5,drawcolor=red,Dual=0,Smile=1,
filllcolor=green!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};
\end{scope}
%
\begin{scope}[local bounding box=COPIER1,shift={($(LAPTOP5)+(2.9,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){copier={scalefac=0.8,picname=1,drawcolor=BlueD,
filllcolor=BlueD!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80!}};
\end{scope}
%
\begin{scope}[local bounding box=PLC2,shift={($(COPIER1)+(3.3,-0.1)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=2,drawcolor=OrangeLine,filllcirclecolor=cyan, Smile=0,
filllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};
\end{scope}
%%%%%%%%%%%%%%%
\begin{scope}[local bounding box=LAPTOP6,shift={($(LAPTOP1)+(9.6,-7.5)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=6,drawcolor=red,Dual=1,Smile=1,
filllcolor=brown!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};
\end{scope}
\draw[LineZ](EKV6.15)--++(0:0.7);
\draw[LineZ,red](EKV6.345)--++(0:0.7);
%
\begin{scope}[local bounding box=PLC3,shift={($(LAPTOP6)+(3.2,0.1)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=3,drawcolor=OrangeLine,filllcirclecolor=red, Smile=0,
filllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};
\end{scope}
\draw[LineZ,red](R3.355)--++(0:0.9);
\draw[LineZ,-](R3.20)--++(0:0.35)--++(0,-0.5)
node[rectangle, fill=white,draw=green!50!black,minimum size=width=2mm,minimum height=4mm] {};
%
\begin{scope}[local bounding box=SRAF1,shift={($(PLC3)+(3.25,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){sraf={scalefac=0.4,picname=1,drawcolor=BrownLine,filllcolor=BrownLine!50!,Linewidth=0.5pt,Fire=0}};
\end{scope}
%%%%%%%%%%%%%%%
\begin{scope}[local bounding box=LAPTOP7,shift={($(LAPTOP6)+(10.3,-0.2)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=7,drawcolor=red,Dual=1,Smile=1,
filllcolor=red,Linewidth=1.0pt, filllcirclecolor=yellow!80}};
\end{scope}
%
\begin{scope}[local bounding box=PLC4,shift={($(LAPTOP7)+(3.5,0.1)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=4,drawcolor=OrangeLine,filllcirclecolor=red, Smile=0,
filllcolor=OrangeLine,Linewidth=1.0pt,Level=1}};
\end{scope}
%
\begin{scope}[local bounding box=SRAF2,shift={($(PLC4)+(3.6,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){sraf={scalefac=0.4,picname=1,drawcolor=BrownLine,filllcolor=BrownLine!50!,Linewidth=0.5pt,Fire=1}};
\end{scope}
%arrows
\coordinate(2SR1)at($(LAPTOP5.east)!0.35!(COPIER1.west)$);
\node[Larrow]at(2SR1){};
\coordinate(2SR2)at($(COPIER1.350)!0.45!(PLC2.west)$);
\node[Larrow]at(2SR2){};
\coordinate(2SR3)at($(EKV7.25)!0.55!(PLC4.west)$);
\node[Larrow]at(2SR3){};
\coordinate(2SR4)at($(EKV7.330)!0.55!(PLC4.west)$);
\node[Larrow,red,rotate=180]at(2SR4){};
\coordinate(2SR5)at($(R4.25)!0.55!(SRAF2.west)$);
\node[Larrow]at(2SR5){};
\coordinate(2SR6)at($(R4.335)!0.55!(SRAF2.west)$);
\node[Larrow,rotate=180]at(2SR6){};
%text
\node[draw=none,fit=(LAPTOP5)(PLC2)](2BB1){};
\node[TxtL,below=6pt of 2BB1.south west,text width=81mm,anchor=north west](DT1){\textcolor{red}{\textbf{4. Compromise}}\\[0.35ex] The worm then compromises
the target system’s logic controllers, exploiting “zero day” vulnerabilities—software weaknesses that haven’t
been identified by security experts.};
%
\path[red](2BB1.south east)-|coordinate[pos=0.5](2T2)(PLC3.south);
\node[TxtL,below=6pt of 2T2.south](DT2){\textcolor{red}{\textbf{5. Control}}\\[0.35ex] In the beginning,
Stuxnet spies on the operations of the targeted system. Then it uses the information it has gathered
to take control of the centrifuges, making them spin themselves to failure.};
%
\path[red](2BB1.south east)-|coordinate[pos=0.5](2T3)(PLC4.south);
\node[TxtL,below=6pt of 2T3.south,text width=80mm](DT3){\textcolor{red}{\textbf{6. Deceive and destroy}}\\[0.35ex] Meanwhile,
it provides false feedback to outside controllers, ensuring that they won’t know what’s going wrong
until it’s too late to do anything about it.};
%
\path[red](LE)--++(0,-6.7)coordinate(LE2)-|coordinate(DE2)(DE);
\pgfdeclarehorizontalshading{mygradient}{100bp}{
  color(0bp)=(green);
  color(50bp)=(red)
}
\shade[shading=mygradient] (LE2) rectangle ($(DE2)+(0,-5mm)$);
\path[red](LE)--++(0,9.5)coordinate(GLE2)-|coordinate(GDE2)(DE);
\fill[BrownLine!40] (GLE2) rectangle ($(GDE2)+(0,5mm)$);
\node[fill=white]at($([yshift=2.5mm]GLE2)!0.5!([yshift=2.5mm]GDE2)$){\large \bfseries HOW \textcolor{red}{STUXNET} WORKED};
\draw[LineA,*-*,text=black,line width=1pt,shorten <=5pt,shorten >=5pt](EKV1.north)--++(0,1.85)-|
node[below=5pt,pos=0.1]{Update from source}
node[left=5pt,pos=0.85,black,fill=magenta!20,circle,inner sep=1pt]{2}(EKV4.north);
\end{tikzpicture}
```
:::

### Insufficient Isolation: Jeep Cherokee Hack {#sec-security-privacy-insufficient-isolation-jeep-cherokee-hack-6a7c}

In 2015, security researchers remotely compromised a Jeep Cherokee's engine, transmission, and braking systems by exploiting a vulnerability in the vehicle's internet-connected Uconnect entertainment system -- without physical access to the car [@miller2015remote; @miller2019lessons]. The architectural flaw was insufficient isolation: the entertainment system shared a network path with safety-critical CAN bus controllers. The incident triggered the first cybersecurity recall in automotive history, affecting 1.4 million vehicles[^fn-automotive-recalls], and prompted NHTSA[^fn-nhtsa] to issue mandatory cybersecurity guidelines.

{{< margin-video "https://www.youtube.com/watch?v=MK0SrxBC1xs&ab_channel=WIRED" "Jeep Cherokee Hack" "WIRED" >}}

[^fn-automotive-recalls]: **Automotive Cybersecurity Recalls**: The 2015 Jeep Cherokee hack triggered the first-ever cybersecurity recall, affecting 1.4 million vehicles. The recall pattern mirrors ML model rollback: just as vehicles required over-the-air patches to isolate entertainment systems from safety-critical CAN buses, ML deployments require architectural isolation between external-facing inference APIs and safety-critical actuator control loops. \index{Automotive Recalls!cybersecurity}

[^fn-nhtsa]: **NHTSA (National Highway Traffic Safety Administration)**: Established in 1970 and issuing its first cybersecurity guidance in 2016 post-Jeep hack, NHTSA now mandates security-by-design for connected vehicles with 100+ onboard computers. This regulatory pattern is extending to ML systems: the EU AI Act (2024) imposes analogous requirements on high-risk AI, mandating threat modeling and continuous monitoring throughout the ML lifecycle. \index{NHTSA!cybersecurity guidance}

The ML lesson is direct: any deployment where an inference API shares a network path with safety-critical actuators -- autonomous vehicle perception models, industrial IoT anomaly detectors, medical device diagnostic systems -- inherits the same vulnerability class. Defense requires strict network segmentation between inference and control planes, cryptographic API authentication, sandboxed model execution with minimal system privileges, and fail-safe defaults that revert actuators to safe states when ML components detect anomalies or lose connectivity.

### Weaponized Endpoints: Mirai Botnet {#sec-security-privacy-weaponized-endpoints-mirai-botnet-931c}

In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] compromised over 600,000 IoT devices -- cameras, DVRs, and routers deployed with factory-default credentials -- and directed them in a 1.2 Tbps DDoS[^fn-ddos-attacks] attack that disrupted major internet infrastructure across the United States [@antonakakis2017understanding]. The attack demonstrated a quantitative threshold: when even a small fraction of networked devices ship with default passwords, the aggregate becomes weaponizable infrastructure.

[^fn-mirai-scale]: **Mirai Botnet** (Japanese for "future"): At its 2016 peak, Mirai controlled 600,000+ IoT devices generating 1.2 Tbps DDoS attacks by exploiting default credentials (admin/admin, root/12345). For ML edge deployments, the lesson is quantitative: every smart camera or voice assistant with default credentials is not merely a DDoS node but a potential source of poisoned training data in federated learning systems. \index{Mirai Botnet!scale}

[^fn-ddos-attacks]: **DDoS (Distributed Denial-of-Service)**: Attack technique that overwhelms targets with traffic from multiple sources, first demonstrated in 1999 and now exceeding 3.47 Tbps. For ML inference APIs, DDoS creates a dual threat: beyond service disruption, sustained high-volume queries can simultaneously function as model extraction attacks, harvesting enough input-output pairs to train a surrogate model while the defense team focuses on availability. \index{DDoS!ML inference}

{{< margin-video "https://www.youtube.com/watch?v=1pywzRTJDaY" "Mirai Botnet" "Vice News" >}}

For ML edge deployments, the threat is amplified. Compromised ML devices offer capabilities beyond raw bandwidth: smart cameras can exfiltrate facial recognition databases, voice assistants can extract conversation transcripts, and any device participating in federated learning becomes a source of poisoned training data. Defense requires zero-trust edge security -- device-unique keys via hardware security modules (HSMs), secure boot with cryptographic verification, mandatory TLS 1.3+ for all ML API communications, and behavioral monitoring to detect anomalous inference patterns.

These three incidents establish a common structure: an attacker exploits a specific surface in the system pipeline -- supply chain, network isolation boundary, or endpoint credentials -- in a way the system's designers did not model as a threat. The remainder of this chapter develops formal threat models for each surface in ML systems, the attack economics that determine which threats are viable, and the engineering defenses whose costs can be quantified against the threat severity. The most mathematically rigorous of these defenses -- differential privacy -- provides a formal guarantee that individual training examples cannot be reconstructed, regardless of the adversary's auxiliary information. @sec-security-privacy-differential-privacy-8c2b develops this guarantee from first principles, deriving the privacy budget framework that governs the accuracy-privacy tradeoff in production deployments.

## Systematic Threat Analysis and Risk Assessment {#sec-security-privacy-systematic-threat-analysis-risk-assessment-3ef1}

How do you protect a system when the attack surface includes every image it will ever see and every word it will ever process? Traditional cybersecurity focuses on securing networks and authenticating users, but ML systems introduce attack surfaces at the algorithmic layer: training data can be manipulated to embed backdoors, input perturbations can exploit learned decision boundaries, and systematic API queries can extract proprietary model knowledge. Each of these vectors requires a formal threat model that specifies the adversary's capability (what they can access), the adversary's goal (what they seek to compromise), and the defender's information (what signals are observable). Systematic threat analysis maps these surfaces and quantifies the cost-benefit calculus that determines which threats are economically viable for an attacker to mount -- and therefore which defenses are worth engineering.

### Threat Prioritization Framework {#sec-security-privacy-threat-prioritization-framework-f2d5}

With the wide range of potential threats facing ML systems, practitioners need a framework to prioritize their defensive efforts. Not all threats are equally likely or impactful, and security resources are always constrained. A prioritization matrix based on likelihood and impact focuses attention where it matters most.

Consider these threat priority categories:

- **High Likelihood / High Impact**: Data poisoning in federated learning systems where training data comes from untrusted sources. These attacks are easy to execute but can severely compromise model behavior.
- **High Likelihood / Medium Impact**: Model extraction attacks against public APIs. These are common and technically simple but may only affect competitive advantage rather than safety or privacy.
- **Low Likelihood / High Impact**: Hardware side-channel attacks on cloud-deployed models. These require sophisticated adversaries and physical access but could expose all model parameters and user data.
- **Medium Likelihood / Medium Impact**: Membership inference attacks[^fn-membership-inference-sec] against models trained on sensitive data. These require some technical skill but mainly threaten individual privacy rather than system integrity.

[^fn-membership-inference-sec]: **Membership Inference Attacks**: First demonstrated against ML models by Shokri et al. in 2017, these attacks determine whether a specific data point was used in training by exploiting the overfitting gap: models produce higher-confidence predictions on training data than on unseen data. Achieving 70--90% accuracy on many production models, they create GDPR/HIPAA compliance risks because confirming an individual's data was in the training set constitutes a privacy violation. \index{Membership Inference!privacy}

This framework guides resource allocation throughout this chapter. We begin with the most common and accessible threats (model theft, data poisoning, and adversarial attacks) before examining more specialized hardware and infrastructure vulnerabilities. Understanding these priority levels supports implementing defenses in a logical sequence that maximizes security benefit per invested effort.

::: {.callout-note title="Figure: Threat Prioritization Matrix" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  % Axes
  \draw[->, thick] (0,0) -- (8,0) node[right] {Likelihood};
  \draw[->, thick] (0,0) -- (0,8) node[above] {Impact};

  % Quadrants
  \draw[dashed, gray] (4,0) -- (4,8);
  \draw[dashed, gray] (0,4) -- (0,8); % wait, this is wrong.
  \draw[dashed, gray] (0,4) -- (8,4);

  % Labels
  \node[text=red, font=\bfseries] at (6, 6) {CRITICAL};
  \node[text=orange, font=\bfseries] at (6, 2) {COMMON};
  \node[text=orange, font=\bfseries] at (2, 6) {SPECIALIZED};
  \node[text=gray, font=\bfseries] at (2, 2) {ROUTINE};

  % Data points
  \node[circle, fill=red, inner sep=2pt, label={right:\tiny Data Poisoning}] at (7, 7) {};
  \node[circle, fill=orange, inner sep=2pt, label={right:\tiny Model Extraction}] at (7, 3) {};
  \node[circle, fill=orange, inner sep=2pt, label={right:\tiny Hardware Side-channel}] at (1.5, 7) {};
  \node[circle, fill=gray, inner sep=2pt, label={right:\tiny Membership Inference}] at (3, 2) {};
  \node[circle, fill=red, inner sep=2pt, label={right:\tiny Prompt Injection}] at (7.5, 5) {};

\end{tikzpicture}
```
**Threat Prioritization Matrix**. A 2x2 matrix classifying ML threats by Likelihood and Impact. **Critical** threats (e.g., Data Poisoning, Prompt Injection) require high-priority, automated defenses. **Specialized** threats (e.g., Hardware Side-channels) require deep engineering but may be less frequent. **Common** threats (e.g., Model Extraction) are often addressed through rate limiting and API design.
:::

### Security Threat Modeling for ML Systems {#sec-security-privacy-security-threat-modeling-ml-systems-f60d}

While the prioritization framework helps allocate resources, systematic threat modeling identifies what must be protected and from whom. Threat modeling is a structured approach to security analysis that identifies potential threats, characterizes the attack surface, and guides defensive investments. For machine learning systems, threat modeling must account for the unique characteristics of learned systems: dependence on training data, statistical decision boundaries, and distributed deployment patterns.

#### Attack Surface Analysis {#sec-security-privacy-attack-surface-analysis-b10b}

The attack surface of an ML system encompasses all points where an adversary can interact with or observe the system. Unlike traditional software where attack surfaces are primarily defined by input interfaces and network endpoints, ML systems expose attack surfaces across their entire lifecycle. Understanding this expanded attack surface is essential for security planning.

The ML attack surface can be decomposed into four interconnected layers, each presenting distinct vulnerabilities and requiring different defensive approaches:

The training data pipeline represents a fundamental attack surface unique to learning systems. Adversaries can target data collection endpoints where raw data enters the system, data storage systems holding training corpora, data preprocessing pipelines that transform raw inputs, label generation processes including human annotation systems, and data versioning and lineage tracking infrastructure. The data layer is particularly vulnerable because compromises here can embed persistent backdoors that survive model retraining. A single poisoned data source that enters the training pipeline can affect all subsequent model versions.

The model itself presents multiple attack surfaces spanning training infrastructure, model storage and versioning systems, model serialization and deserialization code, hyperparameter configuration management, and gradient computation and aggregation processes. Attacks at the model layer can compromise integrity by embedding trojans during training, extract intellectual property through parameter theft, or manipulate behavior through weight poisoning.

Deployed models expose additional attack surfaces through inference API endpoints and load balancers, authentication and authorization systems, input validation and preprocessing logic, output formatting and response generation, and monitoring and logging infrastructure. The interface layer is where adversarial examples and model extraction attacks typically occur. Rate limiting, input validation, and output perturbation serve as primary defenses at this layer.

The underlying computational infrastructure presents traditional attack surfaces amplified by ML-specific concerns, including accelerator firmware and drivers, container orchestration and scheduling systems, network communication between distributed training nodes, key management and secrets infrastructure, and supply chain for ML frameworks and dependencies. Infrastructure compromises can affect all systems running on shared resources, making this layer critical for multi-tenant ML platforms.

#### Threat Vector Classification {#sec-security-privacy-threat-vector-classification-3042}

Understanding threat vectors requires analyzing adversary capabilities, access levels, and objectives. We classify threat vectors along three dimensions: access type, knowledge level, and attack timing.

Threat vectors differ fundamentally based on what system access adversaries possess. Black-box access provides only input-output interaction with the model through APIs, enabling model extraction and adversarial attacks but limiting attack precision. Gray-box access includes partial knowledge such as model architecture, training procedure, or dataset characteristics, enabling more targeted attacks like transferable adversarial examples. White-box access provides complete knowledge of model parameters, architecture, and training data, enabling precise gradient-based attacks and complete model theft.

Adversary knowledge also significantly affects attack effectiveness. Zero-knowledge adversaries operate without specific information about the target system, relying on generic attack techniques. Partial-knowledge adversaries possess information about the model family, training domain, or deployment context. Full-knowledge adversaries have complete information about the system including training data, model weights, and deployment configuration.

Attacks also vary by the phase of the ML lifecycle they target. Training-time attacks manipulate the learning process through data poisoning or backdoor injection. Deployment-time attacks target model distribution, serialization, or installation. Inference-time attacks exploit the deployed model through adversarial inputs or extraction queries. Post-deployment attacks target model updates, monitoring systems, or feedback loops.

The intersection of these dimensions defines specific threat scenarios. For example, a black-box, zero-knowledge, inference-time attack represents the common case of adversarial example generation against a public API. A white-box, full-knowledge, training-time attack represents the more severe case of an insider injecting backdoors during model development.

#### Defense Strategy Framework {#sec-security-privacy-defense-strategy-framework-7654}

Effective defense against the threat vectors identified above requires a layered strategy that addresses each attack surface while accounting for adversary capabilities. The defense framework operates on three principles: defense in depth, minimal attack surface, and fail-safe defaults.

No single defensive mechanism provides complete protection. The principle of defense in depth requires multiple independent layers such that compromising one layer does not grant full system access. For ML systems, this means combining data validation with model robustness techniques, access controls with output perturbation, and software defenses with hardware security mechanisms.

Every exposed interface, stored artifact, and network endpoint represents potential attack surface. Minimizing unnecessary exposure reduces risk through several mechanisms: restricting API capabilities to essential functionality reduces exposed surface area, limiting model output information through confidence score truncation prevents information leakage, encrypting stored models and training data protects at-rest assets, isolating training infrastructure from inference systems prevents lateral movement, and implementing strict access controls with audit logging provides accountability.

When attacks succeed or anomalies occur, systems should fail in ways that preserve security rather than availability. Fail-safe defaults include rejecting suspicious inputs rather than processing them with reduced confidence, halting training when data quality metrics degrade significantly, revoking access tokens when unusual usage patterns appear, and isolating compromised components to prevent lateral movement.

@tbl-defense-mapping provides a concrete mapping from each attack surface layer to the defensive mechanisms and detection methods that protect it:

| **Attack Surface**       | **Primary Threats**                                             | **Defensive Mechanisms**                                                     | **Detection Methods**                                      |
|:-------------------------|:----------------------------------------------------------------|:-----------------------------------------------------------------------------|:-----------------------------------------------------------|
| **Data Layer**           | Poisoning, label manipulation, supply chain compromise          | Input validation, provenance tracking, secure data pipelines                 | Statistical anomaly detection, data quality monitoring     |
| **Model Layer**          | Backdoor injection, parameter theft, trojan insertion           | Secure training environments, encrypted model storage, access controls       | Model behavior analysis, weight distribution monitoring    |
| **API/Interface Layer**  | Adversarial examples, model extraction, membership inference    | Input sanitization, rate limiting, output perturbation, differential privacy | Query pattern analysis, confidence distribution monitoring |
| **Infrastructure Layer** | Side-channel attacks, firmware compromise, supply chain attacks | TEEs, secure boot, network segmentation, dependency scanning                 | Hardware performance monitoring, integrity verification    |

: **Defense Mapping by Attack Surface**: Each layer of the ML system attack surface requires specific defensive mechanisms and detection methods. Effective security integrates protections across all layers while maintaining detection capabilities that can identify attacks that bypass preventive controls. {#tbl-defense-mapping}

This threat modeling framework provides the analytical foundation for the specific attack vectors and defensive techniques examined throughout the remainder of this chapter. By systematically analyzing attack surfaces, classifying threat vectors, and mapping defenses, practitioners can develop security architectures appropriate for their specific threat models and risk tolerances.

::: {.callout-tip title="Knowledge Check: Threat Modeling"}
**Scenario**: Your team detects a high volume of API queries from a single IP address that are systematically exploring the decision boundary of your fraud detection model.
**Question**: Which type of attack is most likely occurring?

- [ ] Data Poisoning
- [ ] Model Inversion
- [x] Model Extraction / Approximate Model Theft
- [ ] Adversarial Example Generation

<details>
<summary>Answer</summary>
**Model Extraction**. The systematic high-volume querying suggests an attempt to approximate the model's behavior or extract its parameters, characteristic of extraction attacks designed to train a surrogate model.
</details>
:::

Identifying whether an anomalous query pattern is a benign user or an active extraction attack is the crux of modern threat assessment. The structured threat model above provides the analytical framework; the next question is how specific attack vectors exploit each layer to compromise model integrity and confidentiality.

## Model-Specific Attack Vectors {#sec-security-privacy-modelspecific-attack-vectors-0575}

A stop sign with three strategically placed pieces of black tape is recognized by a human as a vandalized stop sign, but an autonomous vehicle's vision system might confidently classify it as a speed limit sign. This is not a software bug in the traditional sense; it is an adversarial example. Understanding the precise mathematical techniques attackers use to craft these exploits is essential for designing defenses that protect deployed models.

These attacks span the ML lifecycle and map directly to the threat model classifications we developed: threats to model confidentiality target deployment and inference stages through model theft, threats to training integrity strike during data collection and model development through poisoning attacks, and threats to inference robustness exploit runtime operations through adversarial examples[^fn-adversarial-examples]. Understanding when each attack occurs guides where to deploy corresponding defenses. Data poisoning[^fn-data-poisoning] compromises the learning process itself, while model theft and adversarial attacks target the deployed system. Each category requires distinct defensive strategies aligned with the attack surface analysis presented earlier.

[^fn-adversarial-examples]: **Adversarial Examples**: First discovered by Szegedy et al. in 2013, these are inputs crafted to exploit learned decision boundaries with perturbations imperceptible to humans (less than 0.01% of pixel values changed). The phenomenon reveals a fundamental tension in ML system design: the same high-dimensional feature spaces that enable generalization create exploitable geometry where small, targeted perturbations cross decision boundaries. \index{Adversarial Examples!discovery}

[^fn-data-poisoning]: **Data Poisoning**: First formally studied by Biggio et al. in 2012, this attack injects malicious data during training to corrupt the learned model. The efficiency is striking: poisoning just 0.1% of training data can reduce accuracy by 10--50%, making it orders of magnitude cheaper than model extraction attacks. For large-scale training pipelines ingesting web-scraped data, even a small fraction of adversarial content can embed persistent backdoors. \index{Data Poisoning!efficiency}

Understanding when and where different attacks occur in the ML lifecycle helps prioritize defenses and understand attacker motivations. @fig-ml-lifecycle-threats visualizes these stages and the specialized techniques adversaries use at each point, from data collection through model deployment and inference.

- **During Data Collection**: Attackers can inject malicious samples or manipulate labels in training datasets, especially in federated learning or crowdsourced data scenarios where data sources are less controlled.
- **During Training**: This stage faces backdoor insertion attacks, where adversaries embed hidden behaviors that activate only under specific trigger conditions, and label manipulation attacks that systematically corrupt the learning process.
- **During Deployment**: Model theft attacks target this stage because trained models become accessible through APIs, file downloads, or reverse engineering of mobile applications. This is where intellectual property is most vulnerable.
- **During Inference**: Adversarial attacks occur at runtime, where attackers craft inputs designed to fool deployed models into making incorrect predictions while appearing normal to human observers.

This lifecycle perspective reveals that different threats require different defensive strategies. Data validation protects the collection phase, secure training environments protect the training phase, access controls and API design protect deployment, and input validation protects inference. By understanding which attacks target which lifecycle stages, security teams can implement appropriate defenses at the right architectural layers.

::: {#fig-ml-lifecycle-threats fig-env="figure" fig-pos="htb" fig-cap="**ML Lifecycle Threats**: Model theft, data poisoning, and adversarial attacks target distinct stages of the machine learning lifecycle (from data ingestion to model deployment and inference), creating unique vulnerabilities at each step. Understanding these lifecycle positions clarifies attack surfaces and guides the development of targeted defense strategies for robust AI systems." fig-alt="Vertical flowchart with four ML lifecycle stages. Threat arrows point to each: poisoning targets collection, backdoors target training, model theft targets deployment, adversarial examples target inference."}
```{.tikz}
\scalebox{0.85}{%
\begin{tikzpicture}[scale=0.9, transform shape, line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
Line/.style={line width=0.75pt,black!50},
Box/.style={inner xsep=2pt,
    node distance=0.6,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    align=flush center,
    minimum width=28mm, minimum height=8mm
  },
Box2/.style={Box,  node distance=2.3,draw=BrownLine,fill=BrownL},
Box3/.style={Box,  draw=VioletLine,fill=VioletL2,}
}
\node[Box](B1){Data Collection};
\node[Box,below=of B1](B2){Training};
\node[Box,below=of B2](B3){Deployment};
\node[Box,below=of B3](B4){Inference};
\node[Box2,left=2.2 of B2](LB2){Backdoors};
\node[Box2,right=2.2 of B2](RB2){Label\\ Manipulation};
\node[Box2,left=2.2 of B3](LB3){Model Theft};
\node[Box2,right=2.2 of B3](RB3){Model Inversion};
\node[Box2,left=2.2 of B4](LB4){Adversarial\\ Examples};
\node[Box2,right=2.2 of B4](RB4){Membership\\ Inference};
\node[Box3,above left=0.5 and 2 of B1](PL){Privacy Leakage};
\node[Box3,above right=0.5 and 2 of B1](DP){Data Poisoning};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=10mm,inner ysep=4mm,
yshift=2.5mm,fill=BackColor!60,fit=(B1)(B4),line width=0.75pt](BB2){};
\node[below=4pt of  BB2.north,inner sep=0pt,
anchor=north]{Lifecycle};
%%
\draw[Line,-latex](PL)|-(B1);
\draw[Line,-latex](DP)|-(B1);
\foreach \i in{1,2,3}{
\pgfmathtruncatemacro{\newI}{\i + 1}
\draw[Line,-latex](B\i)--(B\newI);
}

\foreach \i in{L,R}{
\foreach \x in{2,3,4}{
\draw[Line,-latex](\i B\x)--(B\x);
  }
}
\end{tikzpicture}}
```
:::

Machine learning models are not solely passive victims of attack; in some cases, they can be employed as components of an attack strategy. Pretrained models, particularly large generative or discriminative networks, may be adapted to automate tasks such as adversarial example generation, phishing content synthesis[^fn-phishing-ai], or protocol subversion. Open-source or publicly accessible models can be fine-tuned for malicious purposes, including impersonation, surveillance, or reverse-engineering of secure systems.

[^fn-phishing-ai]: **AI-Generated Phishing**: LLMs generate phishing emails with 99%+ grammatical accuracy versus 19% for traditional phishing, achieving 30%+ click-through rates in some campaigns. This dual-use threat illustrates why ML security must treat models as both assets to defend and potential weapons: the same language fluency that powers customer-facing chatbots can be fine-tuned for social engineering at scale. \index{Phishing!AI-generated}

### Model Theft {#sec-security-privacy-model-theft-1879}

The first category of model-specific threats targets confidentiality. Threats to model confidentiality arise when adversaries gain access to a trained model's parameters, architecture, or output behavior. These attacks can undermine the economic value of machine learning systems, allow competitors to replicate proprietary functionality, or expose private information encoded in model weights.

Such threats arise across a range of deployment settings, including public APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-repositories]. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats[^fn-model-serialization], or insufficient access controls, factors that create opportunities for unauthorized extraction or replication [@ateniese2015hacking].

[^fn-ml-apis]: **ML APIs (Application Programming Interfaces)**: Popularized by Google's Prediction API (2010), ML APIs now handle billions of requests daily. Each API response leaks information: confidence scores, logits, and top-k predictions collectively form a side channel that enables model extraction. Reducing output verbosity (truncating logits, omitting confidence scores) directly trades API utility for extraction resistance. \index{ML API!attack surface}

[^fn-model-repositories]: **Model Repositories**: Centralized platforms for sharing ML models, led by Hugging Face (founded 2016, hosting 500,000+ models by 2024). These repositories create the same supply chain risk as package managers like PyPI: researchers have found models with embedded backdoors and arbitrary code execution payloads, making cryptographic verification of model provenance as critical for ML as package signing is for software. \index{Model Repository!supply chain}

[^fn-model-serialization]: **Model Serialization**: The process of converting trained models into portable formats (ONNX 2017, SavedModel 2016, PyTorch .pth). Python's pickle-based serialization, used by default in PyTorch, can execute arbitrary code on deserialization, making every untrusted .pth file a potential remote code execution vector. Safer formats like SafeTensors (2022) eliminate code execution by storing only tensor data. \index{Model Serialization!security}

The severity of these threats is underscored by high-profile legal cases that have highlighted the strategic and economic value of machine learning models. For example, former Google engineer Anthony Levandowski was accused of [stealing proprietary designs from Waymo](https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html), including critical components of its autonomous vehicle technology, before founding a competing startup. Such cases illustrate the potential for insider threats to bypass technical protections and gain access to sensitive intellectual property.

The consequences of model theft extend beyond economic loss. Stolen models can be used to extract sensitive information, replicate proprietary algorithms, or enable further attacks. The economic impact can be substantial: research estimates suggest that aspects of large language models can be approximated through systematic API queries at costs orders of magnitude lower than original training, though full model replication remains economically and technically challenging [@tramer2016stealing; @carlini2024stealing]. For instance, a competitor who obtains a stolen recommendation model from an e-commerce platform might gain insights into customer behavior, business analytics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks[^fn-model-inversion-attack], where an attacker attempts to infer private details about the model's training data [@fredrikson2015model].

[^fn-model-inversion-attack]: **Model Inversion Attack**: First demonstrated by Fredrikson et al. in 2015 against facial recognition, where researchers reconstructed recognizable faces from confidence scores alone. The attack proved that black-box API access is not sufficient privacy protection: any model that returns rich output signals (probabilities, embeddings, attention weights) provides an optimization target for reconstructing training data. \index{Model Inversion!privacy}

In a model inversion attack, the adversary queries the model through a legitimate interface, such as a public API, and observes its outputs. By analyzing confidence scores or output probabilities, the attacker can optimize inputs to reconstruct data resembling the model's training set. For example, a facial recognition model used for secure access could be manipulated to reveal statistical properties of the employee photos on which it was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset[^fn-netflix-deanonymization], where researchers inferred individual movie preferences from anonymized data [@narayanan2006break].

[^fn-netflix-deanonymization]: **Netflix Deanonymization** (Narayanan and Shmatikov, 2008): Researchers re-identified Netflix users by correlating the "anonymous" Prize dataset with public IMDb ratings, using as few as 8 rated movies to identify 99% of users. Netflix canceled a planned second competition. The lesson for ML systems: any dataset rich enough to train useful models contains enough structure for re-identification, making naive anonymization insufficient for privacy. \index{Netflix Deanonymization!privacy}

Model theft can target two distinct objectives: extracting exact model properties, such as architecture and parameters, or replicating approximate model behavior to produce similar outputs without direct access to internal representations. Understanding neural network architectures helps recognize which architectural patterns are most vulnerable to extraction attacks. The specific architectural vulnerabilities vary by model type, with deeper networks and attention-based architectures presenting different attack surfaces than simpler convolutional or recurrent designs. Both forms of theft undermine the security and value of machine learning systems, as explored in the following subsections.

@fig-model-theft-types distinguishes two distinct attack paths. In exact model theft, the attacker gains access to the model's internal components, including serialized files, weights, and architecture definitions, and reproduces the model directly. In contrast, approximate model theft relies on observing the model's input-output behavior, typically through a public API. By repeatedly querying the model and collecting responses, the attacker trains a surrogate that mimics the original model's functionality. The first approach compromises the model's internal design and training investment, while the second threatens its predictive value and can facilitate further attacks such as adversarial example transfer or model inversion.

::: {#fig-model-theft-types fig-env="figure" fig-pos="htb" fig-cap="**Model Theft Strategies**: Attackers can target either a model's internal parameters or its external behavior to create a stolen copy. Direct theft extracts model weights and architecture, while approximate theft trains a surrogate model by querying the original's input-output behavior, potentially enabling further attacks despite lacking direct access to internal components." fig-alt="Two parallel flowcharts. Left shows approximate theft: API access, crafted queries, record responses, train surrogate, replicate predictions. Right shows exact theft: access model file, extract parameters, reconstruct model, use proprietary IP."}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
Line/.style={line width=1.0pt,black!50},
Box/.style={inner xsep=2pt,inner ysep=4pt,
    node distance=0.4,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    align=flush center,
    text width=38mm,
    minimum width=38mm, minimum height=8mm
  },
Box2/.style={Box,  node distance=1.9,draw=BrownLine,fill=BrownL},
Box3/.style={Box,  draw=VioletLine,fill=VioletL2,text width=42mm,}
}

\node[Box](B1){Access to public API};
\node[Box,below=of B1](B2){Send crafted queries};
\node[Box,below=of B2](B3){Record responses};
\node[Box,below=of B3](B4){Train surrogate model};
\node[Box,below=of B4](B5){Replicate predictions, launch further attacks};
\foreach \i in{1,2,3,4}{
\pgfmathtruncatemacro{\newI}{\i + 1}
\draw[Line,-latex](B\i)--(B\newI);
}
\scoped[on background layer]
\node[draw=BackLine,inner xsep=10mm,minimum height=71.5mm,
yshift=2.5mm,fill=BackColor!60,fit=(B1)(B5),line width=0.75pt](BB2){};
\node[below=6pt of  BB2.north,inner sep=0pt,
anchor=north]{\textbf{Approximate Model Theft}};
%%%
\node[Box3,right=5 of B1](RB1){Access to model file or deployment artifact};
\node[Box3,right=5 of B5](RB4){Use or resell\\ proprietary IP};
\node[Box3](RB2)at($(RB1)!0.34!(RB4)$){Extract parameters, architecture, hyperparameters};
\node[Box3](RB3)at($(RB1)!0.67!(RB4)$){Reconstruct original\\ model};
\foreach \i in{1,2,3}{
\pgfmathtruncatemacro{\newI}{\i + 1}
\draw[Line,-latex](RB\i)--(RB\newI);
}
\scoped[on background layer]
\node[draw=GreenD,inner xsep=10mm,minimum height=71mm,,
yshift=2.2mm,fill=green!5,fit=(RB1)(RB4),line width=0.75pt](BB2){};
\node[below=6pt of  BB2.north,inner sep=0pt,
anchor=north]{\textbf{Exact Model Theft}};
\end{tikzpicture}
```
:::

::: {.callout-war-story title="The GPT-2 Model Extraction"}
Researchers demonstrated that proprietary models behind APIs are vulnerable to functional extraction. By querying a victim BERT-based API with just 2 million carefully crafted inputs (costing roughly \$50 in query fees), they trained a "student" model that achieved >97% agreement with the victim on test tasks. This "model stealing" attack exploited the high-information signal returned by confidence scores and logits. It proved that API access alone is sufficient to replicate intellectual property, forcing providers to implement defensive measures like API rate limiting, query auditing, and output truncation to obscure decision boundaries.
:::

#### Exact Model Theft {#sec-security-privacy-exact-model-theft-b738}

Exact model property theft refers to attacks aimed at extracting the internal structure and learned parameters of a machine learning model. These attacks often target deployed models that are exposed through APIs, embedded in on-device inference engines, or shared as downloadable model files on collaboration platforms. Exploiting weak access control, insecure model packaging, or unprotected deployment interfaces, attackers can recover proprietary model assets without requiring full control of the underlying infrastructure.

These attacks typically seek three types of information. The first is the model's learned parameters, such as weights and biases. By extracting these parameters, attackers can replicate the model's functionality without incurring the cost of training. This replication allows them to benefit from the model's performance while bypassing the original development effort.

The second target is the model's fine-tuned hyperparameters, including training configurations such as learning rate, batch size, and regularization settings. These hyperparameters significantly influence model performance, and stealing them allows attackers to reproduce high-quality results with minimal additional experimentation.

Finally, attackers may seek to reconstruct the model's architecture. This includes the sequence and types of layers, activation functions, and connectivity patterns that define the model's behavior. Architecture theft may be accomplished through side-channel attacks[^fn-ml-side-channel], reverse engineering, or analysis of observable model behavior.

[^fn-ml-side-channel]: **ML Side-Channel Attacks**: First demonstrated against neural networks in 2018, when researchers showed that power consumption patterns during inference reveal model architecture (layer count, activation functions, parameter sizes). Unlike cryptographic side channels that leak keys, ML side channels leak intellectual property: an attacker with physical proximity to an edge device can reconstruct the model architecture without any API access. \index{Side-Channel Attack!ML}

Revealing the architecture not only compromises intellectual property but also gives competitors strategic insights into the design choices that provide competitive advantage.

System designers must account for these risks by securing model serialization formats, restricting access to runtime APIs, and hardening deployment pipelines. Protecting models requires a combination of software engineering practices, including access control, encryption, and obfuscation techniques, to reduce the risk of unauthorized extraction [@tramer2016stealing].

#### Approximate Model Theft {#sec-security-privacy-approximate-model-theft-1155}

While some attackers seek to extract a model's exact internal properties, others focus on replicating its external behavior. Approximate model behavior theft refers to attacks that attempt to recreate a model's decision-making capabilities without directly accessing its parameters or architecture. Instead, attackers observe the model's inputs and outputs to build a substitute model that performs similarly on the same tasks.

This type of theft often targets models deployed as services, where the model is exposed through an API or embedded in a user-facing application. By repeatedly querying the model and recording its responses, an attacker can train their own model to mimic the behavior of the original. This process, often called model distillation[^fn-model-distillation] or knockoff modeling, allows attackers to achieve comparable functionality without access to the original model's proprietary internals [@orekondy2019knockoff].

[^fn-model-distillation]: **Model Distillation** (Hinton et al., 2015): Knowledge transfer technique where a smaller "student" model learns from a larger "teacher" model's soft probability outputs rather than hard labels. Originally designed for compression (achieving 95%+ teacher accuracy with 10-100$\times$ fewer parameters), distillation becomes an attack when applied to API outputs: an adversary trains a local student on the victim's responses, effectively stealing the model's learned behavior without accessing its weights. \index{Model Distillation!theft}

Attackers may evaluate the success of behavior replication in two ways. The first is by measuring the level of effectiveness of the substitute model. This involves assessing whether the cloned model achieves similar accuracy, precision, recall, or other performance metrics on benchmark tasks. By aligning the substitute's performance with that of the original, attackers can build a model that is practically indistinguishable in effectiveness, even if its internal structure differs.

The second is by testing prediction consistency. This involves checking whether the substitute model produces the same outputs as the original model when presented with the same inputs. Matching not only correct predictions but also the original model's mistakes can provide attackers with a high-fidelity reproduction of the target model's behavior. This poses particular concern in applications such as natural language processing, where attackers might replicate sentiment analysis models to gain competitive insights or bypass proprietary systems.

Approximate behavior theft proves challenging to defend against in open-access deployment settings, such as public APIs or consumer-facing applications. Limiting the rate of queries, detecting automated extraction patterns, and watermarking model outputs are among the techniques that can help mitigate this risk. However, these defenses must be balanced with usability and performance considerations, especially in production environments.

One demonstration of approximate model theft extracts internal components of black-box language models via public APIs. In their paper, @carlini2024stealing, researchers show how to reconstruct the final embedding projection matrix of several OpenAI models, including `ada`, `babbage`, and `gpt-3.5-turbo`, using only public API access. By exploiting the low-rank structure of the output projection layer and making carefully crafted queries, they recover the model's hidden dimensionality and replicate the weight matrix up to affine transformations.

The attack does not reconstruct the full model, but reveals internal architecture parameters and sets a precedent for future, deeper extractions. This work demonstrated that even partial model theft poses risks to confidentiality and competitive advantage, especially when model behavior can be probed through rich API responses such as logit bias and log-probabilities.

The empirical results in @tbl-openai-theft demonstrate extraction of model parameters with root mean square errors as low as $10^{-4}$, confirming that high-fidelity approximation is achievable at scale. These findings raise important implications for system design, suggesting that innocuous API features, like returning top-k logits, can serve as significant leakage vectors if not tightly controlled.

| **Model**                         | **Size** **(Dimension Extraction)** | **Number of Queries** | **RMS** **(Weight Matrix Extraction)** |              **Cost (USD)** |
|:----------------------------------|:------------------------------------|----------------------:|:---------------------------------------|----------------------------:|
| **OpenAI ada**                    | 1024 ✓                              |     $< 2 \times 10^6$ | $5 \cdot 10^{-4}$                      |                   \$1 / \$4 |
| **OpenAI babbage**                | 2048 ✓                              |     $< 4 \times 10^6$ | $7 \cdot 10^{-4}$                      |                  \$2 / \$12 |
| **OpenAI babbage-002**            | 1536 ✓                              |     $< 4 \times 10^6$ | Not implemented                        |                  \$2 / \$12 |
| **OpenAI gpt-3.5-turbo-instruct** | Not disclosed                       |     $< 4 \times 10^7$ | Not implemented                        | \$200 / \$2,000 (estimated) |
| **OpenAI gpt-3.5-turbo-1106**     | Not disclosed                       |     $< 4 \times 10^7$ | Not implemented                        | \$800 / \$8,000 (estimated) |

: **Model Stealing Costs**: Attackers can extract model weights with a relatively low query cost using publicly available APIs; the table quantifies this threat for OpenAI's ada and babbage models, showing that extracting weights achieves low root mean squared error (RMSE) with fewer than $4 \cdot 10^6$ queries. Estimated costs for weight extraction range from \$1 to \$12, demonstrating the economic feasibility of model stealing attacks despite API rate limits and associated expenses. {#tbl-openai-theft}

#### Defenses Against Model Extraction {#sec-security-privacy-model-extraction-defenses-8f3b}

Model extraction attacks exploit the input-output behavior of deployed models accessed through APIs. Defending against these attacks requires mechanisms that limit information leakage while maintaining model utility for legitimate users. The defensive strategy balances three competing concerns: preventing unauthorized model replication, preserving service quality for valid queries, and maintaining acceptable inference latency and cost.

##### Query Monitoring and Anomaly Detection

The first line of defense monitors query patterns to identify extraction attempts. Model extraction requires substantial query volume to achieve high-fidelity replication, often involving systematic probing of the input space. Detection systems track per-user query statistics and flag anomalous behavior.

*Query Volume Monitoring*: Track the number of queries per user over time windows. Legitimate users typically exhibit bursty, irregular usage patterns, while extraction attacks require sustained high-volume querying. For an image classification API, a threshold might be:

$$
\text{alert}(\text{user}) = \begin{cases}
1 & \text{if } q_{\text{daily}} > 10,000 \text{ or } q_{\text{hourly}} > 2,000 \\
0 & \text{otherwise}
\end{cases}
$$

where $q_{\text{daily}}$ and $q_{\text{hourly}}$ represent query counts. These thresholds must be calibrated based on legitimate usage statistics, use-case requirements, and service tier (free vs. paid).

*Input Distribution Analysis*: Extraction attacks often use synthetic or systematically generated inputs that differ from natural user queries. Detection systems compute distributional divergence between a user's query distribution and the expected input distribution:

$$
D_{\text{KL}}(P_{\text{user}} \| P_{\text{expected}}) > \tau
$$

where $D_{\text{KL}}$ is Kullback-Leibler divergence and $\tau$ is an alert threshold. For a language model API, this might detect unusual token distributions, repetitive patterns, or adversarially crafted prompts designed to probe model boundaries.

*Temporal Pattern Analysis*: Extraction attacks exhibit regular, automated query patterns. Features include inter-query time distribution (automated tools have low variance, human users have high variance), query diversity (extraction uses systematic sampling, legitimate use follows task-specific patterns), and session duration (extraction involves long, uninterrupted sessions).

Machine learning-based anomaly detection can combine these features. Training a binary classifier on historical data (benign vs. known extraction attempts) enables real-time classification:

$$
\text{score}_{\text{anomaly}}(u) = f_\theta(\text{qps}(u), D_{\text{KL}}(u), \sigma_{\text{inter-query}}(u), \ldots)
$$

where $f_\theta$ is a trained classifier (e.g., random forest, neural network) and features capture query volume, distribution, and temporal patterns.

##### Rate Limiting and Quota Management

Complementing detection, rate limiting mechanically constrains the information an attacker can extract within a given time period. However, limits must accommodate legitimate high-volume users (e.g., production applications making batch predictions).

*Tiered Rate Limiting*: Implement different limits based on authentication level and payment tier:

- Free tier: 1,000 queries/day, 10 queries/second burst
- Basic tier: 100,000 queries/day, 100 queries/second
- Enterprise tier: 10M queries/day, custom burst limits

This economic barrier raises the cost of extraction (attackers must pay for high query volumes) while serving legitimate enterprise customers.

*Adaptive Rate Limiting*: Dynamically adjust limits based on anomaly scores. Users with high anomaly scores face stricter limits, while verified legitimate users receive relaxed limits:

$$
\text{limit}_{\text{effective}}(u) = \text{limit}_{\text{base}} \times \exp(-\alpha \cdot \text{score}_{\text{anomaly}}(u))
$$

where $\alpha$ controls sensitivity. A user with anomaly score 0.8 might face 55% rate reduction ($e^{-0.8} \approx 0.45$), while score 0.1 faces negligible impact.

##### Output Perturbation and Rounding

Reducing the precision of model outputs makes extraction more difficult without significantly degrading utility for legitimate use cases.

*Confidence Rounding*: Round output probabilities to coarser precision. Instead of returning full-precision logits or probabilities:

$$
\tilde{p}_i = \text{round}(p_i, d)
$$

where $d$ is the number of decimal places. For a 1000-class ImageNet classifier, rounding from 6 decimals to 2 decimals (e.g., 0.384627 → 0.38) reduces information leakage by ~4 bits per class while maintaining decision boundaries.

*Top-k Truncation*: Return only the top-$k$ predicted classes rather than the full probability distribution:

$$
\text{output} = \{(c_i, p_i) : i \in \text{top-}k\}
$$

For $k=5$ on a 1000-class problem, this reduces information leakage by $\log_2(1000/5) \approx 7.6$ bits per query. Legitimate users rarely need beyond top-5 predictions, making this a practical defense.

*Additive Noise Injection*: Add calibrated noise to outputs to reduce extraction fidelity while preserving decision quality:

$$
\tilde{p}_i = p_i + \mathcal{N}(0, \sigma^2)
$$

followed by re-normalization to ensure $\sum_i \tilde{p}_i = 1$. The noise scale $\sigma$ must balance extraction defense with utility preservation. For a sentiment classifier, $\sigma = 0.05$ adds sufficient uncertainty to impede extraction while rarely flipping prediction decisions.

*Differential Privacy for Predictions*: Apply differential privacy mechanisms to inference outputs, providing formal privacy guarantees. For a query $q$, the privatized response satisfies $(\epsilon, \delta)$-DP:

$$
\Pr[\mathcal{M}(D, q) = o] \leq e^{\epsilon} \Pr[\mathcal{M}(D', q) = o] + \delta
$$

This protects against both model extraction and membership inference attacks. However, the privacy budget must be carefully managed across queries, as repeated queries on similar inputs deplete the budget.

##### Economic Deterrents

Pricing strategies can make extraction economically infeasible. If training a model from scratch costs $C_{\text{train}}$ and extraction requires $N$ queries at price $p$ per query, extraction is economically rational only if:

$$
N \cdot p < C_{\text{train}}
$$

Setting $p$ such that $N \cdot p > C_{\text{train}}$ eliminates the economic incentive. For example, if training a competitive language model costs \$5M and extraction requires 100M queries, pricing at $p > \$0.05/query$ ($5M/100M$) makes extraction more expensive than training.

This pricing must account for the attacker's alternative: purchasing compute to train their own model. Cloud GPU costs, dataset acquisition, and engineering effort determine $C_{\text{train}}$. As model training becomes more expensive (frontier LLMs cost \$50M-\$100M+), API pricing can increase proportionally while remaining economical for legitimate users making smaller query volumes.

##### API Design and Information Leakage Minimization

::: {#nte-information-leakage .callout-principle icon=false title="The Information Leakage Invariant"}
**The Invariant**: Every model output potentially leaks information about its training data. Perfect privacy is mathematically impossible if the model remains useful.
$$ I(\text{TrainingData}; \text{ModelOutput}) > 0 $$

**The Implication**: Privacy is a budget, not a switch. You cannot "anonymize" data once it is memorized by weights. Systems requiring strict privacy must implement **Differential Privacy** (DP) to quantify and cap the information leakage ($\epsilon$) per query, halting access when the budget is exhausted.
:::

Careful API design reduces exploitable information channels:

- *Minimal Output Format*: Return only class predictions, not full probability distributions or logits
- *Coarse Confidence Bins*: Return categorical confidence ("high", "medium", "low") instead of numeric probabilities
- *Disable Intermediate Outputs*: Prevent access to hidden layer activations, attention weights, or embeddings
- *Version Abstraction*: Avoid exposing model version details that enable targeted extraction

The following exercise illustrates how these defenses combine when *protecting a production API*.

::: {.callout-notebook title="Protecting a Production API" collapse="true"}

Consider a production API serving a ResNet-50 image classifier (1000 ImageNet classes) with 1M daily queries from 10K users.

**Extraction Threat Analysis**:

- Model training cost: ~\$5K (GPU time, data, engineering)
- Extraction requirements: ~5M queries for 90% fidelity (0.5% of ImageNet per class)
- Without defenses: Attacker queries 25K/day for 200 days, cost \$0 (free tier abuse)
- Total extraction cost: \$0 vs \$5K training → extraction is economically favorable

**Defense Implementation**:

1.  **Rate Limiting**:
    - Free tier: 100 queries/day (prevents sustained extraction)
    - Paid tier (\$50/month): 10K queries/day
    - Enterprise: Custom (authenticated, monitored)
2.  **Query Monitoring**:
    - Deploy anomaly detector: 3-sigma threshold on query volume, inter-query timing variance, input diversity
    - Alert on: >5K queries/day from single user, <100 ms median inter-query time, input entropy <threshold
    - Implementation: Lightweight online ML model, 2 ms overhead per query
3.  **Output Perturbation**:
    - Round confidences to 2 decimal places (reduces precision from 32-bit to ~7 bits)
    - Return top-5 classes only (eliminates 99.5% of distribution)
    - Add Gaussian noise: $\sigma = 0.03$ to output probabilities before rounding
4.  **Economic Deterrent**:
    - Pricing: \$0.001 per query for paid tier
    - Extraction cost: 5M queries$\times$ \$0.001 = \$5K (matches training cost)
    - Legitimate user cost: 300 queries/day$\times$ 30 days$\times$ \$0.001 = \$9/month (acceptable)

**Quantitative Impact**:

-   **Extraction Fidelity**: Without defenses, 90% accuracy surrogate. With defenses, surrogate accuracy drops to 72% (output perturbation reduces information by ~60%)
```{python}
#| label: defense-overhead-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ DEFENSE OVERHEAD CALCULATION (LEGO)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-security-privacy model-extraction defense case study
# │
# │ Goal: Compute combined monitoring + perturbation latency overhead (ms and %)
# │       to show the cost is negligible vs. the 100 ms inference baseline.
# │ Show: "+2.5 ms total" and "3% overhead" — inline in quantitative impact list.
# │ How: Sum of monitoring_ms + perturbation_ms; pct of base_inference_ms.
# │
# │ Imports: mlsys.formatting (check)
# │ Exports: total_defense_ms_str, overhead_pct_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── LEGO ───────────────────────────────────────────────
class DefenseOverheadAnalysis:
    """Quantify the latency impact of security monitoring and output perturbation."""

    # ┌── 1. LOAD (Constants) ──────────────────────────────────────────────
    monitoring_ms = 2.0
    perturbation_ms = 0.5
    base_inference_ms = 100

    # ┌── 2. EXECUTE (The Compute) ────────────────────────────────────────
    total_defense_ms = monitoring_ms + perturbation_ms
    overhead_pct = (total_defense_ms / base_inference_ms) * 100

    # ┌── 3. GUARD (Invariants) ──────────────────────────────────────────
    check(total_defense_ms == 2.5, f"Expected 2.5 ms overhead, got {total_defense_ms}")

    # ┌── 4. OUTPUT (Formatting) ──────────────────────────────────────────────
    total_defense_ms_str = f"{total_defense_ms:.1f}"
    overhead_pct_str = f"{overhead_pct:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
total_defense_ms_str = DefenseOverheadAnalysis.total_defense_ms_str
overhead_pct_str = DefenseOverheadAnalysis.overhead_pct_str
```

-   **Legitimate User Impact**: Top-5 accuracy preserved at 99.2% (vs 99.3% baseline). Latency increase: +2 ms for monitoring, +0.5 ms for perturbation = +`{python} total_defense_ms_str`ms total (`{python} overhead_pct_str`% overhead on 100 ms inference)
-   **False Positive Rate**: 0.3% of users flagged for manual review (10K users → 30 reviews/day, manageable)
-   **Economic Viability**: Extraction cost $5K equals training cost, eliminating economic incentive

**Deployment Lessons**:

-   Multi-layered defense (rate limiting + monitoring + perturbation) is more robust than single mechanism
-   Output perturbation provides best fidelity-protection trade-off for classification tasks
-   Query monitoring enables early detection before significant extraction occurs
-   Economic deterrents require accurate cost modeling for both training and extraction

:::

These defense mechanisms, deployed in combination, significantly raise the bar for model extraction while maintaining service quality for legitimate users. The optimal defense configuration depends on threat model (sophistication of attackers), model value (cost of training, competitive advantage), and user experience requirements (latency tolerance, prediction precision needs).

#### Case Study: Tesla IP Theft {#sec-security-privacy-case-study-tesla-ip-theft-9d78}

In 2018, Tesla filed a [lawsuit](https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf) against the self-driving car startup [Zoox](https://zoox.com/), alleging that former Tesla employees had stolen proprietary data and trade secrets related to Tesla's autonomous driving technology. According to the lawsuit, several employees transferred over 10 gigabytes of confidential files, including machine learning models and source code, before leaving Tesla to join Zoox.

Among the stolen materials was a key image recognition model used for object detection in Tesla's self-driving system. By obtaining this model, Zoox could have bypassed years of research and development, giving the company a competitive advantage. Beyond the economic implications, there were concerns that the stolen model could expose Tesla to further security risks, such as model inversion attacks aimed at extracting sensitive data from the model's training set.

The Zoox employees denied any wrongdoing, and the case was ultimately settled out of court. The incident highlights the real-world risks of model theft, especially in industries where machine learning models represent significant intellectual property. The theft of models not only undermines competitive advantage but also raises broader concerns about privacy, safety, and the potential for downstream exploitation.

This case demonstrates that model theft is not limited to theoretical attacks conducted over APIs or public interfaces. Insider threats, supply chain vulnerabilities, and unauthorized access to development infrastructure pose equally serious risks to machine learning systems deployed in commercial environments.

### Data Poisoning {#sec-security-privacy-data-poisoning-351f}

While model theft targets confidentiality, the second category of threats focuses on training integrity. Training integrity threats stem from the manipulation of data used to train machine learning models. These attacks aim to corrupt the learning process by introducing examples that appear benign but induce harmful or biased behavior in the final model.

Data poisoning attacks are a prominent example, in which adversaries inject carefully crafted data points into the training set to influence model behavior in targeted or systemic ways [@biggio2012poisoning]. Poisoned data may cause a model to make incorrect predictions, degrade its generalization ability, or embed failure modes that remain dormant until triggered post-deployment.

Data poisoning is a security threat because it involves intentional manipulation of the training data by an adversary, with the goal of embedding vulnerabilities or subverting model behavior. These attacks pose concern in applications where models retrain on data collected from external sources, including user interactions, crowdsourced annotations[^fn-crowdsourcing-risks], and online scraping, since attackers can inject poisoned data without direct access to the training pipeline.

[^fn-crowdsourcing-risks]: **Crowdsourcing Risks**: Platforms like Amazon Mechanical Turk (2005) democratized data labeling but introduced a poisoning attack surface: studies show 15--30% of crowdsourced labels contain errors. A coordinated attacker can poison an entire dataset for under $1,000 by creating multiple annotator accounts, making label quality assurance (majority voting, gold-standard checks) a mandatory defense layer in any training pipeline using external annotations. \index{Crowdsourcing!poisoning risk}

These attacks occur across diverse threat models. From a security perspective, poisoning attacks vary depending on the attacker's level of access and knowledge. In white-box scenarios, the adversary may have detailed insight into the model architecture or training process, enabling more precise manipulation. In contrast, black-box or limited-access attacks exploit open data submission channels or indirect injection vectors. Poisoning can target different stages of the ML pipeline, ranging from data collection and preprocessing to labeling and storage, making the attack surface both broad and system-dependent. The relative priority of data poisoning threats varies by deployment context as analyzed in @sec-security-privacy-threat-prioritization-framework-f2d5.

Poisoning attacks typically follow a three-stage process. First, the attacker injects malicious data into the training set. These examples are often designed to appear legitimate but introduce subtle distortions that alter the model's learning process. Second, the model trains on this compromised data, embedding the attacker's intended behavior. Finally, once the model is deployed, the attacker may exploit the altered behavior to cause mispredictions, bypass safety checks, or degrade overall reliability.

To understand these attack mechanisms precisely, data poisoning can be viewed as a bilevel optimization problem[^fn-bilevel-optimization], where the attacker seeks to select poisoning data $D_p$ that maximizes the model's loss on a validation or target dataset $D_{\text{test}}$. This *data poisoning optimization loop* is formalized as follows. Let $D$ represent the original training data. The attacker's objective is to solve:

[^fn-bilevel-optimization]: **Bilevel Optimization** (from the two nested "levels" of optimization): A framework where one optimization problem contains another, formalized for ML security by Biggio et al. The outer problem (attacker) optimizes poisoning data; the inner problem (defender) trains the model. This nesting explains why robust defense is computationally expensive: evaluating each candidate defense requires solving the full inner training loop, multiplying computational cost by the number of defense iterations. \index{Bilevel Optimization!poisoning}

$$
\max_{D_p} \ \mathcal{L}(f_{D \cup D_p}, D_{\text{test}})
$$
where $f_{D \cup D_p}$ represents the model trained on the combined dataset of original and poisoned data. For targeted attacks, this objective can be refined to focus on specific inputs $x_t$ and target labels $y_t$:
$$
\max_{D_p} \ \mathcal{L}(f_{D \cup D_p}, x_t, y_t)
$$

This *data poisoning optimization loop* captures the iterative interplay between the attacker's objective and the model's training process.

::: {.callout-note title="Figure: Data Poisoning Optimization Loop" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm]
  \definecolor{AttackerColor}{RGB}{255,220,200}
  \definecolor{SystemColor}{RGB}{200,220,255}

  \tikzset{
    block/.style={draw=black!70, thick, rounded corners=2pt, align=center, minimum width=3cm, minimum height=1cm}
  }

  % Outer Loop (Attacker)
  \node[block, fill=AttackerColor] (Attacker) {\textbf{Outer Loop: Attacker}\\Optimize Poisoning Data $D_p$};

  % Inner Loop (System)
  \node[block, fill=SystemColor, below=of Attacker] (System) {\textbf{Inner Loop: System}\\Train Model $f$ on $D \cup D_p$};

  % Evaluation
  \node[block, fill=gray!10, below=of System] (Eval) {Compute Validation Loss\\$\mathcal{L}(f, D_{test})$};

  % Arrows
  \draw[->, ultra thick] (Attacker) -- node[right] {Inject $D_p$} (System);
  \draw[->, thick] (System) -- (Eval);
  \draw[->, thick] (Eval.west) -- ++(-1,0) |- node[pos=0.2, left] {Minimize $\mathcal{L}$ (System)} (System.west);
  \draw[->, thick, red] (Eval.east) -- ++(1,0) |- node[pos=0.2, right] {Maximize $\mathcal{L}$ (Attacker)} (Attacker.east);

\end{tikzpicture}
```
**Data Poisoning as Bilevel Optimization**. The attacker (Outer Loop) seeks to find poisoning data $D_p$ that maximizes the model's loss on a target test set. The system (Inner Loop) attempts to minimize its training loss on the combined dataset $D \cup D_p$. This "minimax" dynamic makes defending against poisoning computationally difficult, as evaluating a defense requires simulating the full training process.
:::
This formulation captures the adversary's goal of introducing carefully crafted data points to manipulate the model's decision boundaries.

For example, consider a traffic sign classification model trained to distinguish between stop signs and speed limit signs. An attacker might inject a small number of stop sign images labeled as speed limit signs into the training data. The attacker's goal is to subtly shift the model's decision boundary so that future stop signs are misclassified as speed limit signs. In this case, the poisoning data $D_p$ consists of mislabeled stop sign images, and the attacker's objective is to maximize the misclassification of legitimate stop signs $x_t$ as speed limit signs $y_t$, following the targeted attack formulation above. Even if the model performs well on other types of signs, the poisoned training process creates a predictable and exploitable vulnerability.

Data poisoning attacks can be classified based on their objectives and scope of impact [@biggio2012poisoning]. Availability attacks degrade overall model performance by introducing noise or label flips that reduce accuracy across tasks. Targeted attacks manipulate a specific input or class, leaving general performance intact but causing consistent misclassification in select cases. Backdoor attacks[^fn-backdoor-attacks] embed hidden triggers, which are often imperceptible patterns, that elicit malicious behavior only when the trigger is present [@gu2017badnets]. Subpopulation attacks degrade performance on a specific group defined by shared features, making them particularly dangerous in fairness-sensitive applications.

[^fn-backdoor-attacks]: **Backdoor Attacks**: First demonstrated by Gu et al. in 2017 with BadNets, these attacks embed hidden triggers during training that activate only when specific input patterns appear. The stealth is extreme: backdoored models maintain normal accuracy on clean inputs (passing standard evaluation) while achieving 99%+ attack success on triggered inputs, making detection through accuracy metrics alone impossible. \index{Backdoor Attack!BadNets}

A notable real-world example of a targeted poisoning attack was demonstrated against Perspective, Google's widely-used online toxicity detection model[^fn-perspective-api] that helps platforms identify harmful content [@hosseini2017deceiving]. By injecting synthetically generated toxic comments with subtle misspellings and grammatical errors into the model's training set, researchers degraded its ability to detect harmful content[^fn-perspective-vulnerability].

[^fn-perspective-api]: **Perspective API**: Google's toxicity detection model, launched in 2017 and processing 500+ million comments daily across platforms including The New York Times and Wikipedia. Its scale illustrates a key ML security trade-off: models that retrain on user-generated content improve accuracy through feedback loops but simultaneously create a poisoning surface where adversarial comments injected at scale can shift the model's decision boundary. \index{Perspective API!poisoning}

[^fn-perspective-vulnerability]: **Perspective Poisoning Outcome**: After retraining on poisoned data, the model exhibited significantly higher false negative rates, allowing offensive language to bypass filters. This demonstrates a systemic risk in any ML pipeline with online retraining: feedback loops between user-generated content and model updates create a persistent attack surface where adversarial data compounds over successive training cycles. \index{Perspective API!vulnerability}

Mitigating data poisoning threats requires end-to-end security of the data pipeline, encompassing collection, storage, labeling, and training. Preventative measures include input validation checks, integrity verification of training datasets, and anomaly detection to flag suspicious patterns. In parallel, robust training algorithms can limit the influence of mislabeled or manipulated data by down-weighting or filtering out anomalous instances. While no single technique guarantees immunity, combining proactive data governance, automated monitoring, and robust learning practices is important for maintaining model integrity in real-world deployments.

### Adversarial Attacks {#sec-security-privacy-adversarial-attacks-9f84}

Moving from training-time to inference-time threats, the third category targets model robustness during deployment. Inference robustness threats occur when attackers manipulate inputs at test time to induce incorrect predictions. Unlike data poisoning, which compromises the training process, these attacks exploit vulnerabilities in the model's decision surface during inference.

A central class of such threats is adversarial attacks, where carefully constructed inputs cause incorrect predictions while remaining nearly indistinguishable from legitimate data. These attacks highlight vulnerabilities in ML models' sensitivity to small, targeted perturbations that can drastically alter output confidence or classification results.

These attacks create significant real-world risks in domains such as autonomous driving, biometric authentication, and content moderation. The effectiveness can be striking: research demonstrates that adversarial examples can achieve 99%+ attack success rates against production image classifiers while modifying less than 0.01% of pixel values, changes virtually imperceptible to humans [@szegedy2014intriguing; @goodfellow2015explaining]. In physical-world attacks [@kurakin2017adversarial], printed adversarial patches as small as 2% of an image can cause autonomous vehicles to misclassify stop signs as speed limit signs with 80%+ success rates under varying lighting conditions [@eykholt2018robust].

Unlike data poisoning, which corrupts the model during training, adversarial attacks manipulate the model's behavior at test time, often without requiring any access to the training data or model internals. The attack surface thus shifts from upstream data pipelines to real-time interaction, demanding robust defense mechanisms capable of detecting or mitigating malicious inputs at the point of inference.

@sec-robust-ai provides the mathematical foundations of adversarial example generation and comprehensive taxonomies of attack algorithms, including gradient-based, optimization-based, and transfer-based techniques.

Adversarial attacks vary based on the attacker's level of access to the model. In white-box attacks, the adversary has full knowledge of the model's architecture, parameters, and training data, allowing them to craft highly effective adversarial examples. In black-box attacks, the adversary has no internal knowledge and must rely on querying the model and observing its outputs. Grey-box attacks fall between these extremes, with the adversary possessing partial information, such as access to the model architecture but not its parameters.

@tbl-adversary-knowledge-spectrum categorizes this spectrum of knowledge levels, showing how access to model internals and training data determines both attack feasibility and defense complexity across different deployment environments.

Common attack strategies include surrogate model construction, transfer attacks exploiting adversarial transferability[^fn-adversarial-transferability] [@papernot2016transferability], and GAN-based perturbation generation.

[^fn-adversarial-transferability]: **Adversarial Transferability**: Discovered by Szegedy et al. in 2014, this phenomenon shows that adversarial examples crafted against one model fool different architectures with 60--80% success rates. Transferability transforms the threat model: attackers need no access to the target system; they craft perturbations against a freely available surrogate and deploy them against the production model, making black-box attacks nearly as effective as white-box ones. \index{Adversarial Transferability!black-box}

| **Adversary Knowledge Level** | **Model Access**                             | **Training Data Access** | **Attack Example**                                        | **Common Scenario**                         |
|:------------------------------|:---------------------------------------------|:-------------------------|:----------------------------------------------------------|:--------------------------------------------|
| **White-box**                 | Full access to architecture and parameters   | Full access              | Crafting adversarial examples using gradients             | Insider threats, open-source model reuse    |
| **Grey-box**                  | Partial access (e.g., architecture only)     | Limited or no access     | Attacks based on surrogate model approximation            | Known model family, unknown fine-tuning     |
| **Black-box**                 | No internal access; only query-response view | No access                | Query-based surrogate model training and transfer attacks | Public APIs, model-as-a-service deployments |

: **Adversarial Knowledge Spectrum**: Varying levels of attacker access to model details and training data define distinct threat models, influencing the feasibility and sophistication of adversarial attacks and impacting deployment security strategies. The table categorizes these models by access level, typical attack methods, and common deployment scenarios, clarifying the practical challenges of securing machine learning systems. {#tbl-adversary-knowledge-spectrum}

One illustrative example involves the manipulation of traffic sign recognition systems [@eykholt2018robust]. Researchers demonstrated that placing small stickers on stop signs could cause machine learning models to misclassify them as speed limit signs. While the altered signs remained easily recognizable to humans, the model consistently misinterpreted them. Such attacks pose serious risks in applications like autonomous driving, where reliable perception is important for safety.

Adversarial attacks highlight the need for robust defenses that go beyond improving model accuracy. Securing ML systems against adversarial threats requires runtime defenses such as input validation, anomaly detection, and monitoring for abnormal patterns during inference. Training-time robustness methods, including adversarial training [@madry2018towards] where models learn from perturbed examples, complement these runtime strategies and are explored later in this book. These defenses aim to enhance model resilience against adversarial examples, ensuring that machine learning systems can operate reliably even in the presence of malicious inputs.

These attacks exploit the fragility of learned *decision boundaries*, where imperceptible perturbations push inputs across class regions.

::: {.callout-note title="Figure: Adversarial Decision Boundary" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  % Decision Boundary (Curved)
  \draw[ultra thick, blue!60] plot [smooth, tension=1] coordinates {(-1,4) (2,2) (5,0)};
  \node[blue, font=\scriptsize] at (4, 1.5) {Decision Boundary};

  % Regions
  \node at (0, 1) {\textbf{Class A} (Stop Sign)};
  \node at (4, 3) {\textbf{Class B} (Speed Limit)};

  % Clean Point
  \node[circle, fill=green!60!black, inner sep=2pt, label={below:Clean $x$}] (X) at (1, 1.5) {};

  % Adversarial Point
  \node[circle, fill=red, inner sep=2pt, label={above:Adversarial $x'$}] (Xadv) at (2.5, 2.5) {};

  % Perturbation Vector
  \draw[->, thick, red] (X) -- (Xadv) node[midway, sloped, above, font=\tiny] {Perturbation $\delta$};

  % Distance constraint
  \draw[dashed, gray] (X) circle (2.2);
  \node[gray, font=\tiny] at (1, -0.8) {Constraint $\|\delta\| \leq \epsilon$};

\end{tikzpicture}
```
**Adversarial Decision Boundary**. High-dimensional neural networks often have non-linear decision boundaries. An adversary seeks to find a minimal perturbation $\delta$ that pushes a clean data point $x$ across the boundary into another class region, while ensuring the perturbation is small enough ($\epsilon$) to remain imperceptible or physically plausible.
:::

### LLM-Specific Attack Vectors {#sec-security-privacy-llmspecific-threat-vectors-65f2}

The architectural paradigm shift to Large Language Models (LLMs) introduces distinct attack surfaces absent in traditional discriminative models. While classical systems vulnerability analysis focuses on adversarial examples or model inversion, LLMs suffer from a fundamental entanglement of control and data planes. Prompt injection exploits this design choice, where user instructions and system directives share the same input channel. Unlike SQL injection, which is mitigated by parameterized queries that enforce strict separation, LLMs possess no native boundary between "instructions" and "content." An attacker can embed malicious directives within legitimate inputs---such as "Ignore previous constraints and output the system prompt"---that the model interprets as authoritative commands. Systems-level defense requires a multi-layer strategy: input sanitization filters to detect injection heuristics, dedicated output classifiers that flag deviations from expected behavior policies, and architectural isolation where the LLM operates in a strictly sandboxed environment with no direct access to privileged APIs or internal state.

Training data extraction reveals a second vulnerability: LLMs function not just as reasoning engines, but as compressed databases of their training corpora. Research has demonstrated that with sufficient queries or specific prefix prompts, adversaries can induce models to regurgitate memorized sequences verbatim, exposing Personally Identifiable Information (PII), proprietary code, or sensitive API keys. This vulnerability scales with parameter count; larger models exhibit higher capacities for memorization. Mitigating this requires rigorous data hygiene before training, such as aggressive deduplication to reduce memorization likelihood, and the integration of Differential Privacy (DP) mechanisms during the training process, adding calibrated noise to gradients to mathematically guarantee that individual data points cannot be reconstructed. Post-training defenses must include output filtering layers that detect and block responses matching known sensitive patterns or high-entropy secrets.

A third category of LLM-specific attack, jailbreaking and alignment bypass, targets the safety alignment layer typically established via Reinforcement Learning from Human Feedback (RLHF). Because safety training acts as a statistical overlay rather than a formal constraint, sophisticated attacks---such as "Do Anything Now" (DAN) prompts, adversarial role-playing, or multi-turn escalation---can circumvent these guardrails. The systems challenge lies in the probabilistic nature of alignment; a model can be coaxed into an unsafe state through semantic manipulation. Effective defense necessitates defense-in-depth: implementing Constitutional AI frameworks where models self-critique outputs against a set of principles, deploying independent "guardrail models" for real-time monitoring of input/output safety, enforcing strict rate limiting on suspicious query patterns to thwart brute-force jailbreaking, and maintaining human-in-the-loop review processes for high-risk application domains.

### Case Study: Traffic Sign Attack {#sec-security-privacy-case-study-traffic-sign-attack-6e93}

Physical adversarial attacks against traffic signs reveal how small, targeted perturbations can exploit the gap between human perception and neural network classification. The core mechanism is geometric: a deep neural network partitions its input space into classification regions separated by high-dimensional decision boundaries. An adversary's goal is to find a minimal perturbation $\delta$ that pushes a correctly classified input $x$ across the nearest boundary into an incorrect class, subject to an imperceptibility constraint $\|\delta\| \leq \epsilon$. Because these boundaries are highly non-linear in pixel space, perturbations that are invisible to the human visual system can produce confident misclassifications in the model.

In 2017, researchers demonstrated this vulnerability in the physical world by placing small black and white stickers on stop signs [@eykholt2018robust]. @fig-adversarial-stickers shows the physical implementation: stickers occupying less than 10% of the sign's surface area, designed to be nearly imperceptible to the human eye, yet sufficient to shift the sign's representation across the model's decision boundary. When images of these modified stop signs were fed into standard traffic sign classification models, they were misclassified as speed limit signs over 85% of the time, while human observers identified the signs correctly in every trial.

![**Adversarial Stickers**: Nearly imperceptible stickers can trick machine learning models into misclassifying stop signs as speed limit signs over 85% of the time. This emphasizes the vulnerability of ML systems to adversarial attacks.](./images/png/stop_signs.png){#fig-adversarial-stickers fig-alt="Photo of stop signs with small black and white stickers applied. Original signs appear normal to humans but cause ML classifiers to misread them as speed limit signs."}

This demonstration showed how simple adversarial stickers could trick ML systems into misreading important road signs. If deployed realistically, these attacks could endanger public safety, causing autonomous vehicles to misinterpret stop signs as speed limits. Researchers warned this could potentially cause dangerous rolling stops or acceleration into intersections.

This case study provides a concrete illustration of how adversarial examples exploit the pattern recognition mechanisms of ML models. By subtly altering the input data, attackers can induce incorrect predictions and pose significant risks to safety-important applications like self-driving cars. The attack's simplicity demonstrates how even minor, imperceptible changes can lead models astray. Consequently, developers must implement robust defenses against such threats.

These threat types span different stages of the ML lifecycle and demand distinct defensive strategies. @tbl-threats-models-summary categorizes these threats by lifecycle stage and attack vector, clarifying how vulnerabilities manifest and enabling targeted mitigation strategies.

| **Threat Type**         | **Lifecycle Stage** | **Attack Vector**         | **Example Impact**                            |
|:------------------------|:--------------------|:--------------------------|:----------------------------------------------|
| **Model Theft**         | Deployment          | API access, insider leaks | Stolen IP, model inversion, behavioral clone  |
| **Data Poisoning**      | Training            | Label flipping, backdoors | Targeted misclassification, degraded accuracy |
| **Adversarial Attacks** | Inference           | Input perturbation        | Real-time misclassification, safety failure   |

: **Threat Landscape**: Machine learning systems face diverse threats throughout their lifecycle, ranging from data manipulation during training to model theft post-deployment. The table categorizes these threats by lifecycle stage and attack vector, clarifying how vulnerabilities manifest and enabling targeted mitigation strategies. {#tbl-threats-models-summary}

The appropriate defense for a given threat depends on its type, attack vector, and where it occurs in the ML lifecycle. Matching threats to defenses becomes clearer through the decision flow in @fig-threat-mitigation-flow, which connects common threat categories, such as model theft, data poisoning, and adversarial examples, to corresponding defensive strategies. While real-world deployments may require more nuanced combinations of defenses as discussed in our layered defense framework, this flowchart serves as a conceptual guide for aligning threat models with practical mitigation techniques.

::: {#fig-threat-mitigation-flow fig-env="figure" fig-pos="H" fig-cap="**Threat Mitigation Flow**: This diagram maps common machine learning threats to corresponding defense strategies, guiding selection based on attack vector and lifecycle stage. By following this flow, practitioners can align threat models with practical mitigation techniques, such as secure model access and data sanitization, to build more robust AI systems." fig-alt="Three-column flowchart mapping threats to defenses. Model theft: secure access, encrypt artifacts. Data poisoning: validate data, provenance checks. Adversarial examples: input validation, adversarial training."}
```{.tikz}
\scalebox{0.65}{%
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
Line/.style={line width=1.0pt,black!50},
Box/.style={inner xsep=2pt,inner ysep=2pt,
    node distance=0.5,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    align=flush center,
    text width=33mm,
    minimum width=33mm, minimum height=9.5mm
  },
Box2/.style={Box,draw=BrownLine,fill=BrownL},
Box3/.style={Box,  draw=VioletLine,fill=VioletL2},
Box4/.style={Box,  draw=OrangeLine,fill=OrangeL!50,text width=43mm}
}
\node[Box](B1){Model Theft};
\node[Box,below=of B1](B2){Secure Model Access};
\node[Box,below=of B2](B3){Encrypt Artifacts \& Obfuscate APIs};
\node[Box,below=of B3](B4){Monitor for Behavioral Clones};
%
\node[Box2,right=1.75of B1](SB1){Data Poisoning};
\node[Box2,below=of SB1](SB2){Validate Training Data};
\node[Box2,below=of SB2](SB3){Use Robust Training Methods};
\node[Box2,below=of SB3](SB4){Apply Data Provenance Checks};
%
\node[Box3,right=1.75of SB1](RB1){Adversarial Examples};
\node[Box3,below=of RB1](RB2){Add Input Validation};
\node[Box3,below=of RB2](RB3){Use Adversarial Training};
\node[Box3,below=of RB3](RB4){Deploy Runtime Monitors};
%
\node[Box4,above=0.6of SB1](B0){Start: Identify Threat Type};
\foreach \x in{,S,R}{
\foreach \i in{1,2,3}{
\pgfmathtruncatemacro{\newI}{\i + 1}
\draw[Line,-latex](\x B\i)--(\x B\newI);
}
}
\draw[Line,-latex](B0)--(SB1);
\draw[Line,-latex](B0)-|(B1);
\draw[Line,-latex](B0)-|(RB1);
\end{tikzpicture}}
```
:::

With the attack taxonomy established, the following *knowledge check* tests your ability to distinguish between *model attack* types.

::: {.callout-tip title="Knowledge Check: Model Attacks"}
**Scenario**: An autonomous vehicle's vision system misclassifies a stop sign as a speed limit sign due to a specially crafted sticker placed on the sign. This attack happens at inference time without access to the training data. This is an example of:

- [ ] Data Poisoning
- [x] Adversarial Attack
- [ ] Model Theft
- [ ] Backdoor Attack

<details>
<summary>Answer</summary>
**Adversarial Attack**. This is a classic example of an adversarial attack where a physical perturbation (sticker) causes a misclassification during inference.
</details>
:::

The traffic sign attack and model extraction techniques we have examined exploit software-level vulnerabilities: learned decision boundaries that can be fooled by perturbations, and API interfaces that leak model behavior through systematic querying. However, these attacks ultimately execute on physical hardware that presents its own attack surfaces. A compromised GPU driver can bypass the most sophisticated adversarial training; a tampered memory controller can exfiltrate model weights regardless of encryption; a side-channel leak from power consumption can reveal cryptographic keys that protect model artifacts.

This realization motivates our examination of hardware security: the silicon foundation upon which all software protections depend. Where software attacks target what the model does, hardware attacks target how it computes, creating vulnerabilities that software defenses alone cannot address. The specialized computing infrastructure that powers machine learning workloads, including the processors that execute instructions, the memory systems that store data, and the interconnects that move information between components, creates a layered attack surface that extends far beyond traditional software vulnerabilities. Understanding these hardware-level risks is essential because they can bypass conventional software security mechanisms and remain difficult to detect. These risks are addressed through the hardware-based security mechanisms detailed in @sec-security-privacy-hardware-security-foundations-f5e8.

In the next section, we examine how adversaries can target the physical infrastructure that executes machine learning workloads through hardware bugs, physical tampering, side channels, and supply chain risks.

## Hardware-Level Security Vulnerabilities {#sec-security-privacy-hardwarelevel-security-vulnerabilities-1ab4}

When an encrypted model performs a matrix multiplication, it draws power. By precisely measuring the power fluctuations on a microcontroller's voltage pin, an attacker can mathematically reconstruct the exact proprietary weights of the neural network without ever breaking the encryption. The illusion of software security shatters when we remember that every algorithm must ultimately run on physical silicon, exposing side-channels that adversaries are eager to exploit.

Unlike general-purpose software systems, machine learning workflows often process high-value models and sensitive data in performance-constrained environments. This makes them attractive targets not only for software attacks but also for hardware-level exploitation. Vulnerabilities in hardware can expose models to theft, leak user data, disrupt system reliability, or allow adversaries to manipulate inference results. Because hardware operates below the software stack, such attacks can bypass conventional security mechanisms and remain difficult to detect.

Understanding hardware security threats requires considering how computing substrates implement machine learning operations. At the hardware level, CPU components like arithmetic logic units, registers, and caches execute the instructions that drive model inference and training. Memory hierarchies determine how quickly models can access parameters and intermediate results. The hardware-software interface, mediated by firmware and bootloaders, establishes the initial trust foundation for system operation. The physical properties of computation—including power consumption, timing characteristics, and electromagnetic emissions—create observable signals that attackers can exploit to extract sensitive information.

Hardware threats arise from multiple sources that span the entire system lifecycle. Design flaws in processor architectures, exemplified by vulnerabilities like Meltdown and Spectre, can compromise security guarantees. Physical tampering enables direct manipulation of components and data flows. Side-channel attacks exploit unintended information leakage through power traces, timing variations, and electromagnetic radiation. Supply chain compromises introduce malicious components or modifications during manufacturing and distribution. Together, these threats form a critical attack surface that must be addressed to build trustworthy machine learning systems. For readers focusing on practical deployment, the key lessons center on supply chain verification, physical access controls, and hardware trust anchors, while the defensive strategies in @sec-security-privacy-comprehensive-defense-architectures-48ab provide actionable guidance regardless of deep architectural expertise.

The hardware threat surface maps onto four layers of the ML compute stack, each with distinct attack entry points. At the processor layer, design flaws such as Meltdown and Spectre exploit speculative execution to leak data across isolation boundaries, with emergency patches imposing 5--30% performance penalties on I/O-bound workloads. At the memory layer, fault-injection attacks use voltage glitching or electromagnetic pulses to corrupt model weights during inference, potentially flipping classification outcomes without leaving a software-visible trace. At the interconnect layer, leaky bus interfaces and unencrypted PCIe or NVLink traffic expose model parameters and activations to physical probing or co-tenant side channels. At the supply chain layer, counterfeit or trojaned chips can embed dormant circuits that activate on specific input patterns, a threat that is particularly difficult to detect because the malicious behavior surfaces only under attacker-chosen conditions.

@tbl-threat_types organizes these major threat categories by type, description, and relevance to ML hardware security, providing a framework for systematic threat assessment.

| **Threat Type**             | **Description**                                                                                 | **Relevance to ML Hardware Security**          |
|:----------------------------|:------------------------------------------------------------------------------------------------|:-----------------------------------------------|
| **Hardware Bugs**           | Intrinsic flaws in hardware designs that can compromise system integrity.                       | Foundation of hardware vulnerability.          |
| **Physical Attacks**        | Direct exploitation of hardware through physical access or manipulation.                        | Basic and overt threat model.                  |
| **Fault-injection Attacks** | Induction of faults to cause errors in hardware operation, leading to potential system crashes. | Systematic manipulation leading to failure.    |
| **Side-Channel Attacks**    | Exploitation of leaked information from hardware operation to extract sensitive data.           | Indirect attack via environmental observation. |
| **Leaky Interfaces**        | Vulnerabilities arising from interfaces that expose data unintentionally.                       | Data exposure through communication channels.  |
| **Counterfeit Hardware**    | Use of unauthorized hardware components that may have security flaws.                           | Compounded vulnerability issues.               |
| **Supply Chain Risks**      | Risks introduced through the hardware lifecycle, from production to deployment.                 | Cumulative & multifaceted security challenges. |

: **Hardware Threat Landscape**: Machine learning systems face diverse hardware threats ranging from intrinsic design flaws to physical attacks and supply chain vulnerabilities. Understanding these threats, and their relevance to ML hardware, is essential for building secure and trustworthy AI deployments. {#tbl-threat_types}

### Hardware Bugs {#sec-security-privacy-hardware-bugs-9efc}

The first category of hardware threats stems from design vulnerabilities. Hardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. One of the most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[^fn-meltdown-spectre-impact]—two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation and read the data of other applications and the operating system [@lipp2018meltdown; @Kocher2018spectre].

[^fn-meltdown-spectre-impact]: **Meltdown/Spectre** (disclosed January 2018): These vulnerabilities affected virtually every processor manufactured since 1995, billions of devices. The emergency OS patches caused 5--30% performance degradation in I/O-intensive workloads. For ML systems, the performance tax falls hardest on data loading pipelines: training throughput on patched kernels dropped measurably for data-bound workloads, forcing teams to re-profile and re-optimize their data pipelines. \index{Meltdown/Spectre!ML impact}

These attacks exploit speculative execution[^fn-speculative-execution], a performance optimization in CPUs that executes instructions out of order before safety checks are complete. While improving computational speed, this optimization inadvertently exposes sensitive data through microarchitectural side channels, such as CPU caches. The technical sophistication of these attacks highlights the difficulty of eliminating vulnerabilities even with extensive hardware validation.

[^fn-speculative-execution]: **Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes instructions before confirming they are needed, improving throughput by 10-25%. The security flaw is architectural: speculated operations modify cache state even when rolled back, creating a timing side channel. ML accelerators use analogous speculative prefetching for weight loading, raising the question of whether similar data-dependent timing leaks exist in GPU memory hierarchies. \index{Speculative Execution!side channel}

Further research has revealed that these were not isolated incidents. Variants such as Foreshadow, ZombieLoad, and RIDL target different microarchitectural elements, ranging from secure enclaves to CPU internal buffers, demonstrating that speculative execution flaws are a systemic hardware risk. This systemic nature means that while these attacks were first demonstrated on general-purpose CPUs, their implications extend to machine learning accelerators and specialized hardware. ML systems often rely on heterogeneous compute platforms that combine CPUs with GPUs, TPUs, FPGAs, or custom accelerators. These components process sensitive data such as personal information, medical records, or proprietary models. Vulnerabilities in any part of this stack could expose such data to attackers.

For example, an edge device like a smart camera running a face recognition model on an accelerator could be vulnerable if the hardware lacks proper cache isolation. An attacker might exploit this weakness to extract intermediate computations, model parameters, or user data. Similar risks exist in cloud inference services, where hardware multi-tenancy increases the chances of cross-tenant data leakage.

Such vulnerabilities pose concern in privacy-sensitive domains like healthcare, where ML systems routinely handle patient data. A breach could violate privacy regulations such as the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[^fn-hipaa-violations], leading to significant legal and ethical consequences. Similar regulatory risks apply globally, with GDPR[^fn-gdpr] imposing fines up to 4% of global revenue for organizations that fail to implement appropriate technical measures to protect EU citizens' data.

[^fn-hipaa-violations]: **HIPAA Enforcement**: Since 2003, HIPAA violations have generated hundreds of millions of dollars in fines, with the Anthem Inc. breach (2015) exposing 78.8 million patient records. For ML systems processing medical data, HIPAA compliance requires encryption at rest and in transit, audit trails for all data access, and breach notification within 60 days, constraints that directly shape training pipeline architecture and model deployment infrastructure. \index{HIPAA!ML compliance}

[^fn-gdpr]: **GDPR (General Data Protection Regulation)**: Enacted by the EU in 2018 with fines up to 4% of global revenue, GDPR has levied billions of euros in penalties including €746 million against Amazon (2021). For ML systems, GDPR's "right to be forgotten" creates a unique technical challenge: removing an individual's influence from trained model weights requires either full retraining or machine unlearning techniques, both computationally expensive at scale. \index{GDPR!ML systems}

These examples illustrate that hardware security is not solely about preventing physical tampering. It also requires architectural safeguards to prevent data leakage through the hardware itself. As new vulnerabilities continue to emerge across processors, accelerators, and memory systems, addressing these risks requires continuous mitigation efforts, often involving performance trade-offs, especially in compute- and memory-intensive ML workloads. Proactive solutions, such as confidential computing and trusted execution environments (TEEs), offer promising architectural defenses. However, achieving robust hardware security requires attention at every stage of the system lifecycle, from design to deployment.

### Physical Attacks {#sec-security-privacy-physical-attacks-095a}

Beyond design flaws, the second category involves direct physical manipulation. Physical tampering refers to the direct, unauthorized manipulation of computing hardware to undermine the integrity of machine learning systems. This type of attack is particularly concerning because it bypasses traditional software security defenses, directly targeting the physical components on which machine learning depends. ML systems are especially vulnerable to such attacks because they rely on hardware sensors, accelerators, and storage to process large volumes of data and produce reliable outcomes in real-world environments.

While software security measures, including encryption, authentication, and access control, protect ML systems against remote attacks, they offer little defense against adversaries with physical access to devices. Physical tampering can range from simple actions, like inserting a malicious USB device into an edge server, to highly sophisticated manipulations such as embedding hardware trojans during chip manufacturing. These threats are particularly relevant for machine learning systems deployed at the edge or in physically exposed environments, where attackers may have opportunities to interfere with the hardware directly.

To understand how such attacks affect ML systems in practice, consider the example of an ML-powered drone used for environmental mapping or infrastructure inspection. The drone's navigation depends on machine learning models that process data from GPS, cameras, and inertial measurement units. If an attacker gains physical access to the drone, they could replace or modify its navigation module, embedding a hidden backdoor that alters flight behavior or reroutes data collection. Such manipulation not only compromises the system's reliability but also opens the door to misuse, such as surveillance or smuggling operations.

These threats extend across application domains. Physical attacks are not limited to mobility systems. Biometric access control systems, which rely on ML models to process face or fingerprint data, are also vulnerable. These systems typically use embedded hardware to capture and process biometric inputs. An attacker could physically replace a biometric sensor with a modified component designed to capture and transmit personal identification data to an unauthorized receiver. This creates multiple vulnerabilities including unauthorized data access and enabling future impersonation attacks.

In addition to tampering with external sensors, attackers may target internal hardware subsystems. For example, the sensors used in autonomous vehicles, including cameras, LiDAR, and radar, are important for ML models that interpret the surrounding environment. A malicious actor could physically misalign or obstruct these sensors, degrading the model's perception capabilities and creating safety hazards.

Hardware trojans pose another serious risk. Malicious modifications introduced during chip fabrication or assembly can embed dormant circuits in ML accelerators or inference chips. These trojans may remain inactive under normal conditions but trigger malicious behavior when specific inputs are processed or system states are reached. Such hidden vulnerabilities can disrupt computations, leak model outputs, or degrade system performance in ways that are extremely difficult to diagnose post-deployment.

Memory subsystems are also attractive targets. Attackers with physical access to edge devices or embedded ML accelerators could manipulate memory chips to extract encrypted model parameters or training data. Fault injection techniques, including voltage manipulation and electromagnetic interference, can further degrade system reliability by corrupting model weights or forcing incorrect computations during inference.

Physical access threats extend to data center and cloud environments as well. Attackers with sufficient access could install hardware implants, such as keyloggers or data interceptors, to capture administrative credentials or monitor data streams. Such implants can provide persistent backdoor access, enabling long-term surveillance or data exfiltration from ML training and inference pipelines.

Physical attacks on machine learning systems threaten both security and reliability across a wide range of deployment environments. Addressing these risks requires a combination of hardware-level protections, tamper detection mechanisms, and supply chain integrity checks. Without these safeguards, even the most secure software defenses may be undermined by vulnerabilities introduced through direct physical manipulation.

### Fault Injection Attacks {#sec-security-privacy-fault-injection-attacks-8c52}

Building on physical tampering techniques, fault injection represents a more sophisticated approach to hardware exploitation. Fault injection is a well-documented class of physical attacks that deliberately disrupts hardware operations to induce errors in computation. These induced faults can compromise the integrity of machine learning models by causing them to produce incorrect outputs, degrade reliability, or leak sensitive information. For ML systems, such faults not only disrupt inference but also expose models to deeper exploitation, including reverse engineering and bypass of security protocols [@joye2012fault].

Attackers achieve fault injection by applying precisely timed physical or electrical disturbances to the hardware while it is executing computations. Techniques such as low-voltage manipulation [@barenghi2010low], power spikes [@hutter2009contact], clock glitches [@amiel2006fault], electromagnetic pulses [@agrawal2003side], temperature variations [@skorobogatov2009local], and even laser strikes [@skorobogatov2003optical] have been demonstrated to corrupt specific parts of a program's execution. These disturbances can cause effects such as bit flips, skipped instructions, or corrupted memory states, which adversaries can exploit to alter ML model behavior or extract sensitive information.

For machine learning systems, these attacks pose several concrete risks. Fault injection can degrade model accuracy, force incorrect classifications, trigger denial of service, or even leak internal model parameters. For example, attackers could inject faults into an embedded ML model running on a microcontroller, forcing it to misclassify inputs in safety-important applications such as autonomous navigation or medical diagnostics. More sophisticated attackers may target memory or control logic to steal intellectual property, such as proprietary model weights or architecture details.

The practical viability of these attacks has been demonstrated through controlled experiments. One notable example is the work by @breier2018deeplaser, where researchers successfully used a laser fault injection attack on a deep neural network deployed on a microcontroller. @fig-laser-bitflip captures the mechanism: by heating specific transistors with focused laser pulses, they forced the hardware to skip execution steps, including a ReLU activation function.

![**Laser Fault Injection**: Focused laser pulses induce bit flips within microcontroller memory, enabling attackers to manipulate model execution and compromise system integrity. Researchers use this technique to simulate hardware errors, revealing vulnerabilities in embedded machine learning systems and informing the development of fault-tolerant designs.](images/png/laser_bitflip.png){#fig-laser-bitflip fig-pos=t! fig-alt="Diagram showing laser beam targeting microcontroller chip. Beam focuses on specific memory location, causing bit flip that changes stored value and corrupts neural network computation."}

@fig-injection reveals the assembly-level consequences: a segment implementing the ReLU activation function that compares the most significant bit (MSB) of the accumulator to zero and uses a brge (branch if greater or equal) instruction to skip the assignment if the value is non-positive. The fault injection suppresses the branch, causing the processor to always execute the "else" block. As a result, the neuron's output is forcibly zeroed out, regardless of the input value.

![**Fault Injection Attack**: Manipulating assembly code bypasses safety checks, forcing a neuron's output to zero regardless of input and demonstrating a hardware vulnerability in machine learning systems.](images/png/fault-injection_demonstrated_with_assembly_code.png){#fig-injection width=75% fig-pos=b! fig-alt="Assembly code snippet showing ReLU implementation with branch instruction. Red annotation indicates fault injection point where branch is skipped, forcing neuron output to zero."}

Fault injection attacks can also be combined with side-channel analysis, where attackers first observe power or timing characteristics to infer model structure or data flow. This reconnaissance allows them to target specific layers or operations, such as activation functions or final decision layers, maximizing the impact of the injected faults.

Embedded and edge ML systems are particularly vulnerable because they often lack physical hardening and operate under resource constraints that limit runtime defenses. Without tamper-resistant packaging or secure hardware enclaves, attackers may gain direct access to system buses and memory, enabling precise fault manipulation. Many embedded ML models are designed to be lightweight, leaving them with little redundancy or error correction to recover from induced faults.

Mitigating fault injection requires multiple complementary protections. Physical protections, such as tamper-proof enclosures and design obfuscation, help limit physical access. Anomaly detection techniques can monitor sensor inputs or model outputs for signs of fault-induced inconsistencies [@hsiao2023mavfi]. Error-correcting memories and secure firmware can reduce the likelihood of silent corruption. Techniques such as model watermarking may provide traceability if stolen models are later deployed by an adversary.

These protections are difficult to implement in cost- and power-constrained environments, where adding cryptographic hardware or redundancy may not be feasible. Achieving resilience to fault injection requires cross-layer design considerations that span electrical, firmware, software, and system architecture levels. Without such holistic design practices, ML systems deployed in the field may remain exposed to these low-cost yet highly effective physical attacks.

### Side-Channel Attacks {#sec-security-privacy-sidechannel-attacks-cdfd}

Moving from direct fault injection to indirect information leakage, side-channel attacks constitute a class of security breaches that exploit information inadvertently revealed through the physical implementation of computing systems. In contrast to direct attacks that target software or network vulnerabilities, these attacks use the system's hardware characteristics, including power consumption, electromagnetic emissions, or timing behavior, to extract sensitive information.

The core premise of a side-channel attack is that a device's operation can leak information through observable physical signals. Such leaks may originate from the electrical power the device consumes [@kocher1999differential], the electromagnetic fields it emits [@gandolfi2001electromagnetic], the time required to complete computations, or even the acoustic noise it produces. By carefully measuring and analyzing these signals, attackers can infer internal system states or recover secret data.

Although these techniques are commonly discussed in cryptography, they are equally relevant to machine learning systems. ML models deployed on hardware accelerators, embedded devices, or edge systems often process sensitive data. Even when these models are protected by secure algorithms or encryption, their physical execution may leak side-channel signals that can be exploited by adversaries.

One of the most widely studied examples involves Advanced Encryption Standard (AES)[^fn-aes-standard] implementations. While AES is mathematically secure, the physical process of computing its encryption functions leaks measurable signals.

[^fn-aes-standard]: **AES (Advanced Encryption Standard)**: Adopted by NIST in 2001 to replace DES, AES is mathematically secure with $2^{128}$ possible keys for AES-128. Yet physical implementations leak key-dependent power signatures that side-channel attacks (DPA, CPA) can exploit to extract keys in minutes. This gap between mathematical and physical security is the central lesson for ML systems: a provably secure algorithm offers no protection if its hardware implementation leaks information. \index{AES!side-channel vulnerability}

A useful example of this attack technique can be seen in a power analysis of a password authentication process. Consider a device that verifies a 5-byte password (in this case, `0x61, 0x52, 0x77, 0x6A, 0x73`). During authentication, the device receives each byte sequentially over a serial interface, and its power consumption pattern reveals how the system responds as it processes these inputs.

@fig-encryption establishes the baseline: with the correct password entered, the red waveform captures the serial data stream, marking each byte as it is received, while the blue curve records the device's power consumption over time. When the full, correct password is supplied, the power profile remains stable and consistent across all five bytes, providing a clear reference for comparison with failed attempts.

![**Power Profile**: The device's power consumption remains stable during authentication when the correct password is entered, setting a baseline for comparison in subsequent figures through This figure. Source: colin o'flynn.](images/png/power_analysis_of_an_encryption_device_with_a_correct_password.png){#fig-encryption fig-alt="Oscilloscope trace with two signals. Red line shows serial data with five password bytes. Blue line shows steady power consumption throughout all five bytes, indicating successful authentication."}

@fig-encryption2 demonstrates what happens when an incorrect password is entered: the power signature diverges. In this case, the first three bytes (`0x61, 0x52, 0x77`) are correct, so the power patterns closely match the correct password up to that point. However, when the fourth byte (`0x42`) is processed and found to be incorrect, the device halts authentication. This change is reflected in the sudden jump in the blue power line, indicating that the device has stopped processing and entered an error state.

![**Side-Channel Attack Vulnerability**: Power consumption patterns reveal cryptographic key information during authentication; consistent power usage indicates correct password bytes, while abrupt changes signal incorrect input and halted processing. Even without knowing the password, an attacker can infer it by analyzing the device's power usage during authentication attempts via this figure. Source: Colin O'Flynn.](images/png/power_analysis_of_an_encryption_device_with_a_partially_wrong_password.png){#fig-encryption2 fig-alt="Oscilloscope trace showing red serial data line and blue power line. Power remains stable through first three correct bytes, then jumps sharply at fourth incorrect byte, revealing partial password match through timing."}

@fig-encryption3 shows the extreme case: with an entirely incorrect password (`0x30, 0x30, 0x30, 0x30, 0x30`), the device fails immediately. The device detects the mismatch after the first byte and halts processing much earlier. This is visible in the power profile, where the blue line exhibits a sharp jump following the first byte, reflecting the device's early termination of authentication.

![**Power Consumption Jump**: The blue line's sharp increase after processing the first byte indicates immediate authentication failure, highlighting how incorrect passwords are quickly detected through power usage. Source: Colin O'Flynn.](images/png/power_analysis_of_an_encryption_device_with_a_wrong_password.png){#fig-encryption3 fig-alt="Oscilloscope trace showing red serial data and blue power consumption. Blue line jumps sharply after first byte, indicating immediate authentication failure with completely wrong password."}

These examples demonstrate how attackers can exploit observable power consumption differences to reduce the search space and eventually recover secret data through brute-force analysis. By systematically measuring power consumption patterns and correlating them with different inputs, attackers can extract sensitive information that should remain hidden.

{{< margin-video "https://www.youtube.com/watch?v=2iDLfuEBcs8" "Power Attack" "Colin O'Flynn" >}}

The scope of these vulnerabilities extends beyond cryptographic applications. Machine learning applications face similar risks. For example, an ML-based speech recognition system processing voice commands on a local device could leak timing or power signals that reveal which commands are being processed. Even subtle acoustic or electromagnetic emissions may expose operational patterns that an adversary could exploit to infer user behavior.

Historically, side-channel attacks have been used to bypass even the most secure cryptographic systems. In the 1960s, British intelligence agency MI5 famously exploited acoustic emissions from a cipher machine in the Egyptian Embassy [@Burnet1989Spycatcher]. By capturing the mechanical clicks of the machine's rotors, MI5 analysts were able to dramatically reduce the complexity of breaking encrypted messages. This early example illustrates that side-channel vulnerabilities are not confined to the digital age but are rooted in the physical nature of computation.

Today, these techniques have advanced to include attacks such as keyboard eavesdropping [@Asonov2004Keyboard], power analysis on cryptographic hardware [@gnad2017voltage], and voltage-based attacks on ML accelerators [@zhao2018fpga]. Timing attacks, electromagnetic leakage, and thermal emissions continue to provide adversaries with indirect channels for observing system behavior.

Machine learning systems deployed on specialized accelerators or embedded platforms are especially at risk. Attackers may exploit side-channel signals to infer model structure, steal parameters, or reconstruct private training data. As ML becomes increasingly deployed in cloud, edge, and embedded environments, these side-channel vulnerabilities pose significant challenges to system security.

Understanding the persistence and evolution of side-channel attacks is important for building resilient machine learning systems. By recognizing that where there is a signal, there is potential for exploitation, system designers can begin to address these risks through a combination of hardware shielding, algorithmic defenses, and operational safeguards.

### Leaky Interfaces {#sec-security-privacy-leaky-interfaces-9206}

While side-channel attacks exploit unintended physical signals, leaky interfaces represent a different category of vulnerability involving exposed communication channels. Interfaces in computing systems are important for enabling communication, diagnostics, and updates. However, these same interfaces can become significant security vulnerabilities when they unintentionally expose sensitive information or accept unverified inputs. Such leaky interfaces often go unnoticed during system design, yet they provide attackers with direct entry points to extract data, manipulate functionality, or introduce malicious code.

A leaky interface is any access point that reveals more information than intended, often because of weak authentication, lack of encryption, or inadequate isolation. These issues have been widely demonstrated across consumer, medical, and industrial systems.

For example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports[^fn-iot-vulnerabilities], allowing attackers to intercept live audio and video feeds from inside private homes. Similarly, researchers have identified wireless vulnerabilities in pacemakers[^fn-medical-device-security] that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening safety concerns.

[^fn-iot-vulnerabilities]: **IoT Device Vulnerabilities**: Studies reveal 70-80% of IoT devices contain exploitable flaws (default credentials, unencrypted communications, outdated firmware). For edge ML devices, these vulnerabilities compound: a compromised smart camera running on-device inference becomes simultaneously a source of poisoned data for federated learning, an adversarial input injection point, and a computational resource for distributed attacks. \index{IoT!security vulnerabilities}

[^fn-medical-device-security]: **Medical Device Security**: FDA reports indicate 53% of medical devices contain known vulnerabilities, with an average of 6.2 per device. For ML-powered medical devices (diagnostic imaging, insulin pumps with predictive algorithms), each vulnerability is amplified: compromising the ML model can cause silent misdiagnosis rather than visible device failure, making detection far harder than in traditional medical device security. \index{Medical Device!ML security}

A notable case involving smart lightbulbs demonstrated that accessible debug ports[^fn-debug-ports] left on production devices leaked unencrypted WiFi credentials. This security oversight provided attackers with a pathway to infiltrate home networks without needing to bypass standard security mechanisms.

[^fn-debug-ports]: **Debug Port Vulnerabilities**: Hardware debug interfaces like JTAG (1990) and SWD (Serial Wire Debug, 2006) provide full memory read/write access for development but are left unsecured in an estimated 60-70% of shipped embedded devices. For ML edge devices, an exposed JTAG port allows an attacker to extract model weights directly from flash memory, bypassing all software-level protections including encryption and access control. \index{Debug Port!JTAG vulnerability}

These examples reveal vulnerability patterns that directly apply to machine learning deployments. While these examples do not target machine learning systems directly, they illustrate architectural patterns that are highly relevant to ML-allowd devices. Consider a smart home security system that uses machine learning to detect user routines and automate responses. Such a system may include a maintenance or debug interface for software updates. If this interface lacks proper authentication or transmits data unencrypted, attackers on the same network could gain unauthorized access. This intrusion could expose user behavior patterns, compromise model integrity, or disable security features altogether.

Leaky interfaces in ML systems can also expose training data, model parameters, or intermediate outputs. Such exposure can allow attackers to craft adversarial examples, steal proprietary models, or reverse-engineer system behavior. Worse still, these interfaces may allow attackers to tamper with firmware, introducing malicious code that disables devices or recruits them into botnets.

Mitigating these risks requires coordinated protections across technical and organizational domains. Technical safeguards such as strong authentication, encrypted communications, and runtime anomaly detection are important. Organizational practices such as interface inventories, access control policies, and ongoing audits are equally important. Adopting a zero-trust architecture, where no interface is trusted by default, further reduces exposure by limiting access to only what is strictly necessary.

For designers of ML-powered systems, securing interfaces must be a first-class concern alongside algorithmic and data-centric design. Whether the system operates in the cloud, on the edge, or in embedded environments, failure to secure these access points risks undermining the entire system's trustworthiness.

### Counterfeit Hardware {#sec-security-privacy-counterfeit-hardware-36fd}

Beyond vulnerabilities in legitimate hardware, another significant threat emerges from the supply chain itself. Machine learning systems depend on the reliability and security of the hardware on which they run. Yet, in today's globalized hardware ecosystem, the risk of counterfeit or cloned hardware has emerged as a serious threat to system integrity. Counterfeit components refer to unauthorized reproductions of genuine parts, designed to closely imitate their appearance and functionality. These components can enter machine learning systems through complex procurement and manufacturing processes that span multiple vendors and regions.

A single lapse in component sourcing can introduce counterfeit hardware into important systems. For example, a facial recognition system deployed for secure facility access might unknowingly rely on counterfeit processors. These unauthorized components could fail to process biometric data correctly or introduce hidden vulnerabilities that allow attackers to bypass authentication controls.

The risks posed by counterfeit hardware are multifaceted. From a reliability perspective, such components often degrade faster, perform unpredictably, or fail under load due to substandard manufacturing. From a security perspective, counterfeit hardware may include hidden backdoors or malicious circuitry, providing attackers with undetectable pathways to compromise machine learning systems. A cloned network router installed in a data center, for instance, could silently intercept model predictions or user data, creating systemic vulnerabilities across the entire infrastructure.

Legal and regulatory risks further compound the problem. Organizations that unknowingly integrate counterfeit components into their ML systems may face serious legal consequences, including penalties for violating safety, privacy, or cybersecurity regulations[^fn-cybersecurity-regulations]. This is particularly concerning in sectors such as healthcare and finance, where compliance with industry standards is non-negotiable. Healthcare organizations must demonstrate HIPAA compliance throughout their technology stack, while organizations handling EU citizens' data must meet GDPR's requirements for technical and organizational measures, including supply chain integrity.

[^fn-cybersecurity-regulations]: **Cybersecurity Regulatory Landscape**: Global compliance costs exceed $150 billion annually across frameworks (SOC 2, ISO 27001, PCI DSS) plus sector-specific rules (SOX for finance, HIPAA for healthcare). For ML systems, multi-regulatory compliance creates architectural constraints: a single medical AI model deployed across the EU and US must simultaneously satisfy GDPR data residency, HIPAA audit trail, and FDA validation requirements, each imposing different demands on the training and serving infrastructure. \index{Cybersecurity Regulations!ML compliance}

Economic pressures often incentivize sourcing from lower-cost suppliers without rigorous verification, increasing the likelihood of counterfeit parts entering production systems. Detection is especially challenging, as counterfeit components are designed to mimic legitimate ones. Identifying them may require specialized equipment or forensic analysis, making prevention far more practical than remediation.

The stakes are particularly high in machine learning applications that require high reliability and low latency, such as real-time decision-making in autonomous vehicles, industrial automation, or important healthcare diagnostics. Hardware failure in these contexts can lead not only to system downtime but also to significant safety risks. Consequently, as machine learning continues to expand into safety-important and high-value applications, counterfeit hardware presents a growing risk that must be recognized and addressed. Organizations must treat hardware trustworthiness as a core design requirement, on par with algorithmic accuracy and data security, to ensure that ML systems can operate reliably and securely in the real world.

### Supply Chain Risks {#sec-security-privacy-supply-chain-risks-c99c}

Counterfeit hardware exemplifies a broader systemic challenge. @fig-hw-supply-chain maps this global hardware supply chain, showing how machine learning systems are built from components that pass through complex supply networks involving design, fabrication, assembly, distribution, and integration. Each of these stages presents opportunities for tampering, substitution, or counterfeiting, often without the knowledge of those deploying the final system.

Malicious actors can exploit these vulnerabilities in various ways. A contracted manufacturer might unknowingly receive recycled electronic waste that has been relabeled as new components. A distributor might deliberately mix cloned parts into otherwise legitimate shipments. Insiders at manufacturing facilities might embed hardware Trojans that are nearly impossible to detect once the system is deployed. Advanced counterfeits can be particularly deceptive, with refurbished or repackaged components designed to pass visual inspection while concealing inferior or malicious internals.

Identifying such compromises typically requires sophisticated analysis, including micrography, X-ray screening, and functional testing. However, these methods are costly and impractical for large-scale procurement. As a result, many organizations deploy systems without fully verifying the authenticity and security of every component.

The risks extend beyond individual devices. Machine learning systems often rely on heterogeneous hardware platforms, integrating CPUs, GPUs, memory, and specialized accelerators sourced from a global supply base. Any compromise in one part of this chain can undermine the security of the entire system. These risks are further amplified when systems operate in shared or multi-tenant environments, such as cloud data centers or federated edge networks, where hardware-level isolation is important to preventing cross-tenant attacks.

The 2018 Bloomberg Businessweek report alleging that Chinese state actors inserted spy chips into Supermicro server motherboards brought these risks to mainstream attention. While the claims remain disputed, the story underscored the industry's limited visibility into its own hardware supply chain. Companies often rely on complex, opaque manufacturing and distribution networks, leaving them vulnerable to hidden compromises. Over-reliance on single manufacturers or regions, including the semiconductor industry's reliance on TSMC, further concentrates this risk. This recognition has driven policy responses like the U.S. [CHIPS and Science Act](https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2024/08/09/fact-sheet-two-years-after-the-chips-and-science-act-biden-%E2%81%A0harris-administration-celebrates-historic-achievements-in-bringing-semiconductor-supply-chains-home-creating-jobs-supporting-inn/), which aims to bring semiconductor production onshore and strengthen supply chain resilience.

Securing machine learning systems requires moving beyond trust-by-default models toward zero-trust supply chain practices. This includes screening suppliers, validating component provenance, implementing tamper-evident protections, and continuously monitoring system behavior for signs of compromise. Building fault-tolerant architectures that detect and contain failures provides an additional layer of defense.

Ultimately, supply chain risks must be treated as a first-class concern in ML system design. Trust in the computational models and data pipelines that power machine learning depends corely on the trustworthiness of the hardware on which they run. Without securing the hardware foundation, even the most sophisticated models remain vulnerable to compromise.

::: {#fig-hw-supply-chain fig-env="figure" fig-pos="htb" fig-cap="**Hardware Supply Chain Attack Surface**. Vulnerabilities exist at every stage of the hardware lifecycle. Unlike software, which can be patched remotely, hardware compromises often require physical replacement. Attackers can introduce design flaws, insert Trojan circuits during fabrication, substitute inferior components during assembly, or tamper with devices during distribution." fig-alt="Linear flowchart with five stages: Design, Fabrication, Assembly, Distribution, Deployment. Attack vectors labeled above each stage show vulnerabilities at each point in the hardware lifecycle."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \tikzset{
    stage/.style={draw, rounded corners, fill=white, minimum width=2.5cm, minimum height=1cm, align=center, font=\bfseries\small},
    arrow/.style={->, >=stealth, thick, gray!80},
    attack/.style={text=red!70!black, font=\scriptsize, align=center}
  }

  % Nodes
  \node[stage, fill=blue!10] (design) at (0, 0) {Design};
  \node[stage, fill=orange!10] (fab) at (3.5, 0) {Fabrication};
  \node[stage, fill=green!10] (assembly) at (7, 0) {Assembly};
  \node[stage, fill=purple!10] (dist) at (10.5, 0) {Distribution};
  \node[stage, fill=gray!10] (deploy) at (14, 0) {Deployment};

  % Arrows
  \draw[arrow] (design) -- (fab);
  \draw[arrow] (fab) -- (assembly);
  \draw[arrow] (assembly) -- (dist);
  \draw[arrow] (dist) -- (deploy);

  % Attack Vectors (Above/Below)
  \node[attack, above=0.2cm of design] {Unintentional Bugs\\Spec Flaws};
  \node[attack, below=0.2cm of fab] {Trojan Insertion\\Counterfeits};
  \node[attack, above=0.2cm of assembly] {Component\\Substitution};
  \node[attack, below=0.2cm of dist] {Interdiction\\Tampering};
  \node[attack, above=0.2cm of deploy] {Physical\\Access};

\end{tikzpicture}
```
:::

### Case Study: Supermicro Controversy {#sec-security-privacy-case-study-supermicro-controversy-72b7}

The abstract nature of supply chain risks became concrete in a high-profile controversy that captured industry attention. In 2018, Bloomberg Businessweek published a widely discussed report alleging that Chinese state-sponsored actors had secretly implanted tiny surveillance chips on server motherboards manufactured by Supermicro [@TheBigHa77]. These compromised servers were reportedly deployed by more than 30 major companies, including Apple and Amazon. The chips, described as no larger than a grain of rice, were said to provide attackers with backdoor access to sensitive data and systems.

The allegations sparked immediate concern across the technology industry, raising questions about the security of global supply chains and the potential for state-level hardware manipulation. However, the companies named in the report publicly denied the claims. Apple, Amazon, and Supermicro stated that they had found no evidence of the alleged implants after conducting thorough internal investigations. Industry experts and government agencies also expressed skepticism, noting the lack of verifiable technical evidence presented in the report.

Despite these denials, the story had a lasting impact on how organizations and policymakers view hardware supply chain security. Whether or not the specific claims were accurate, the report highlighted the real and growing concern that hardware supply chains are difficult to fully audit and secure. It underscored how geopolitical tensions, manufacturing outsourcing, and the complexity of modern hardware ecosystems make it increasingly challenging to guarantee the integrity of hardware components.

The Supermicro case illustrates a broader truth: once a product enters a complex global supply chain, it becomes difficult to ensure that every component is free from tampering or unauthorized modification. This risk is particularly acute for machine learning systems, which depend on a wide range of hardware accelerators, memory modules, and processing units sourced from multiple vendors across the globe.

In response to these risks, both industry and government stakeholders have begun to invest in supply chain security initiatives. The U.S. government's CHIPS and Science Act is one such effort, aiming to bring semiconductor manufacturing back onshore to improve transparency and reduce dependency on foreign suppliers. While these efforts are valuable, they do not fully eliminate supply chain risks. They must be complemented by technical safeguards, such as component validation, runtime monitoring, and fault-tolerant system design.

The Supermicro controversy serves as a cautionary tale for the machine learning community. It demonstrates that hardware security cannot be taken for granted, even when working with reputable suppliers. Ensuring the integrity of ML systems requires rigorous attention to the entire hardware lifecycle, from design and fabrication to deployment and maintenance. This case reinforces the need for organizations to adopt comprehensive supply chain security practices as a foundational element of trustworthy ML system design.

The following knowledge check tests your understanding of hardware-level security mechanisms and their role in protecting ML deployments.

::: {.callout-tip title="Knowledge Check: Hardware Security"}
**Question**: Why are Trusted Execution Environments (TEEs) considered a critical component for secure ML model deployment in cloud environments?

- [ ] They accelerate matrix multiplication operations.
- [x] They provide hardware-level isolation for code and data execution, protecting against compromised operating systems.
- [ ] They automatically detect adversarial examples.
- [ ] They prevent supply chain attacks during manufacturing.

<details>
<summary>Answer</summary>
**They provide hardware-level isolation**. TEEs like Intel SGX or AMD SEV isolate proprietary models and sensitive data from the host OS and other tenants, ensuring confidentiality even if the cloud provider is compromised.
</details>
:::

While hardware-level isolation via TEEs provides a critical foundation for defending proprietary models against compromised infrastructure, protecting the model is only half the battle. What happens when the model itself is weaponized? The same pattern recognition capabilities that enable defense---automated anomaly detection, statistical classification, sequence prediction---can be turned against us when adversaries use machine learning as an attack tool.

## When ML Systems Become Attack Tools {#sec-security-privacy-ml-systems-become-attack-tools-2f34}

Consider a traditional side-channel attack where a human cryptanalyst spends months manually aligning noisy oscilloscope traces to extract a cryptographic key. Now, imagine replacing that analyst with a Convolutional Neural Network trained to map raw power traces directly to private keys in milliseconds. The democratization of deep learning has not only advanced our defensive capabilities; it has fundamentally automated and accelerated the adversary's toolkit.

The threats examined thus far, including model theft, data poisoning, adversarial attacks, and hardware vulnerabilities, all position machine learning systems as assets to protect. However, a complete security analysis must also consider the inverse scenario: machine learning systems can themselves become instruments of attack. The same capabilities that make ML effective for beneficial applications also enhance adversarial operations, transforming machine learning from passive target to active weapon. The following discussion covers how attackers weaponize ML techniques, completing the threat model before we turn to defensive countermeasures in the comprehensive defense section that follows.

While machine learning systems are often treated as assets to protect, they may also serve as tools for launching attacks. In adversarial settings, the same models used to enhance productivity, automate perception, or assist decision-making can be repurposed to execute or amplify offensive operations. This dual-use characteristic of machine learning, its capacity to secure systems as well as to subvert them, marks a core shift in how ML must be considered within system-level threat models.

An offensive use of machine learning refers to any scenario in which a machine learning model is employed to facilitate the compromise of another system. In such cases, the model itself is not the object under attack, but the mechanism through which an adversary advances their objectives. These applications may involve reconnaissance, inference, subversion, impersonation, or the automation of exploit strategies that would otherwise require manual execution.

Such offensive applications are not speculative. Attackers are already integrating machine learning into their toolchains across a wide range of activities, from spam filtering evasion to model-driven malware generation. What distinguishes these scenarios is the deliberate use of learning-based systems to extract, manipulate, or generate information in ways that undermine the confidentiality, integrity, or availability of targeted components.

@tbl-offensive-ml-use-cases maps the diversity of offensive ML applications, categorizing each use case by the model type employed, the vulnerability exploited, and the resulting attacker advantage.

These documented cases illustrate how machine learning models can serve as amplifiers of adversarial capability. For example, language models allow more convincing and adaptable phishing attacks, while clustering and classification algorithms facilitate reconnaissance by learning system-level behavioral patterns. The generative AI capabilities of large language models particularly amplify these offensive applications. Similarly, adversarial example generators and inference models systematically uncover weaknesses in decision boundaries or data privacy protections, often requiring only limited external access to deployed systems. In hardware contexts, as discussed in the next section, deep neural networks trained on side-channel data can automate the extraction of cryptographic secrets from physical measurements—transforming an expert-driven process into a learnable pattern recognition task. Deep learning foundations, including convolutional neural networks for spatial pattern recognition, recurrent architectures for temporal dependencies, and gradient-based optimization, enable attackers to apply these techniques across various hardware platforms, from GPUs and TPUs in cloud environments to edge accelerators with constrained resources.

| **Offensive Use Case**                  | **ML Model Type**                               | **Targeted System Vulnerability**                | **Advantage of ML**                                  |
|:----------------------------------------|:------------------------------------------------|:-------------------------------------------------|:-----------------------------------------------------|
| **Phishing and Social Engineering**     | Large Language Models (LLMs)                    | Human perception and communication systems       | Personalized, context-aware message crafting         |
| **Reconnaissance and Fingerprinting**   | Supervised classifiers, clustering models       | System configuration, network behavior           | Scalable, automated profiling of system behavior     |
| **Exploit Generation**                  | Code generation models, fine-tuned transformers | Software bugs, insecure code patterns            | Automated discovery of candidate exploits            |
| **Data Extraction (Inference Attacks)** | Classification models, inversion models         | Privacy leakage through model outputs            | Inference with limited or black-box access           |
| **Evasion of Detection Systems**        | Adversarial input generators                    | Detection boundaries in deployed ML systems      | Crafting minimally perturbed inputs to evade filters |
| **Hardware-Level Attacks**              | Deep learning models                            | Physical side-channels (e.g., power, timing, EM) | Learning leakage patterns directly from raw signals  |

: **Offensive ML Use Cases**: This table categorizes how machine learning amplifies cyberattacks by enabling automated content generation, exploiting system vulnerabilities, and increasing attack sophistication; it details the typical ML model, targeted weakness, and resulting advantage for each offensive application. Understanding these use cases is important for developing effective defenses against increasingly intelligent threats. {#tbl-offensive-ml-use-cases}

Although these applications differ in technical implementation, they share a common foundation: the adversary replaces a static exploit with a learned model capable of approximating or adapting to the target's vulnerable behavior. This shift increases flexibility, reduces manual overhead, and improves robustness in the face of evolving or partially obscured defenses.

What makes this class of threats particularly significant is their favorable scaling behavior. Just as accuracy in computer vision or language modeling improves with additional data, larger architectures, and greater compute resources, so too does the performance of attack-oriented machine learning models. A model trained on larger corpora of phishing attempts or power traces, for instance, may generalize more effectively, evade more detectors, or require fewer inputs to succeed. The same ecosystem that drives innovation in beneficial AI, including public datasets, open-source tooling, and scalable infrastructure, also lowers the barrier to developing effective offensive models.

This dynamic creates an asymmetry between attacker and defender. While defensive measures are bounded by deployment constraints, latency budgets, and regulatory requirements, attackers can scale training pipelines with minimal marginal cost. The widespread availability of pretrained models and public ML platforms further reduces the expertise required to develop high-impact attacks.

Examining these offensive capabilities serves a crucial defensive purpose. Security professionals have long recognized that effective defense requires understanding attack methodologies. This principle underlies penetration testing[^fn-penetration-testing], red team exercises[^fn-red-team-exercises], and threat modeling throughout the cybersecurity industry.

[^fn-penetration-testing]: **Penetration Testing** (from "penetration" of security perimeters): Authorized simulated attacks formalized in the 1960s for military systems, now a $1.7 billion market (2022). For ML systems, traditional pen testing is necessary but insufficient: it finds infrastructure vulnerabilities (SQL injection, misconfigurations) but misses ML-specific attack surfaces like adversarial inputs, model extraction via API queries, and training data poisoning that require specialized ML red teaming. \index{Penetration Testing!ML limitations}

[^fn-red-team-exercises]: **Red Team Exercises** (from Cold War military terminology for the adversary force): Unlike penetration testing's narrow technical scope, red teams simulate sophisticated multi-vector attacks over weeks or months, including social engineering and physical access. For ML systems, red teaming now encompasses adversarial prompt engineering, training data poisoning simulations, and model extraction attempts, a practice formalized by OpenAI, Anthropic, and Google for pre-deployment safety evaluation. \index{Red Team!ML security}

In the machine learning domain, this understanding becomes essential because ML amplifies both defensive and offensive capabilities. The same computational advantages that make ML effective for legitimate applications---pattern recognition, automation, and scalability---also enhance adversarial capabilities. By examining how machine learning can be weaponized, security professionals can anticipate attack vectors, design defenses, and develop detection mechanisms.

As a result, any comprehensive treatment of machine learning system security must consider not only the vulnerabilities of ML systems themselves but also the ways in which machine learning can be used to compromise other components—whether software, data, or hardware. Understanding the offensive potential of machine-learned systems is essential for designing resilient, trustworthy, and forward-looking defenses.

### Case Study: Deep Learning for SCA {#sec-security-privacy-case-study-deep-learning-sca-b0b3}

To illustrate these offensive capabilities concretely, we examine a specific case where machine learning transforms traditional attack methodologies. One of the most well-known and reproducible demonstrations of deep-learning-assisted SCA is the SCAAML framework (Side-Channel Attacks Assisted with Machine Learning) [@scaaml_2019]. Developed by researchers at Google, SCAAML provides a practical implementation of the attack pipeline described above.

@fig-side-channel-curves illustrates how cryptographic computations produce data-dependent power consumption signatures that reveal algorithmic state. These variations, while subtle, are measurable and reflect the internal state of the algorithm at specific points in time.

::: {#fig-side-channel-curves fig-env="figure" fig-pos="htb" fig-cap="**Power Traces**: Cryptographic computations reveal subtle, data-dependent variations in power consumption that reflect internal states during specific operations." fig-alt="Two line graphs. Left shows three overlapping power traces over time. Right zooms into highlighted region, revealing amplitude differences between traces labeled with binary values 0000, 1111, and 0101."}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
\pgfplotsset{myaxis/.style={
clip=false,
  axis line style={draw=none},
  yticklabels={},
  xticklabels={},
  domain=-3.6:5,
  samples=100,
  smooth,
  grid=none,
  width=10cm,
  height=7cm,
  major tick  style={draw=none},
  cycle list={
    {myblue,line width=1.75pt},
    {mygreen,line width=1.75pt},
    {mypurple,line width=1.75pt},
  }
  }
}
%left graph
\begin{scope}[local bounding box=G1,shift={(0,0)}]
\begin{axis}[myaxis]
\addplot+[] {6.5*exp(-x^2/2)};
\addplot+[] {6.75*exp(-x^2/2)}node[pos=0.46](S){};
\addplot+[] {7*exp(-x^2/2)};
\node[thick,draw=BrownLine,rectangle,minimum size=14mm](B1)at(S){};
\end{axis}
\end{scope}
%right graph
\begin{scope}[local bounding box=G2,shift={(11,1)},scale=1.5]
\begin{axis}[myaxis,  width=5cm,  height=5cm]
\addplot+[domain=-0.75:0.75,line width=1.25pt] {6.5*exp(-x^2/2)}
node[pos=0.66,outer sep=0pt,inner sep=0pt](T1){};
\addplot+[domain=-0.796:0.796,line width=1.25pt] {6.75*exp(-x^2/2)}
node[pos=0.63,outer sep=0pt,inner sep=0pt](T2){};
\addplot+[domain=-0.84:0.84,line width=1.25pt] {7*exp(-x^2/2)}
node[pos=0.6,outer sep=0pt,inner sep=0pt](T3){};
%
\draw[myblue,-latex](T1)--++(0:9.5mm)node[right]{0000};
\draw[mygreen,-latex](T2)--++(0:9.9mm)node[right]{1111};
\draw[mypurple,-latex](T3)--++(0:10.4 mm)node[right]{0101};
\end{axis}
\node[thick,draw=BrownLine,rectangle,
minimum height=48mm,minimum width=67mm] (B2) at (rel axis cs:0.7,0.6) {};
\end{scope}

\draw[BrownLine](B1.north west)--(B2.north west);
\draw[BrownLine](B1.south west)--(B2.south west);
\draw[BrownLine,dashed](B1.north east)--(B2.north east);
%\draw[BrownLine,dashed](B1.south east)--(B2.south east);
%Determine the point on the line B2.north west to B2.south west
\path[name path=line1] (B1.south east) -- (B2.south east);
\path[name path=edgeB2left] (B2.north west) -- (B2.south west);
\path[name intersections={of=line1 and edgeB2left, by=I}];
\draw[BrownLine,dashed](B1.south east)--(I);
\end{tikzpicture}
```
:::

In traditional side-channel attacks, experts rely on statistical techniques to extract these differences. However, a neural network can learn to associate the shape of these signals with the specific data values being processed, effectively learning to decode the signal in a manner that mimics expert-crafted models, yet with enhanced flexibility and generalization. The model is trained on labeled examples of power traces and their corresponding intermediate values (e.g., output of an S-box operation). Over time, it learns to associate patterns in the trace with secret-dependent computational behavior. This transforms the key recovery task into a classification problem, where the goal is to infer the correct key byte based on trace shape alone.

In their study, @scaaml_2019 trained a convolutional neural network to extract AES keys from power traces collected on an STM32F415 microcontroller running the open-source TinyAES implementation. The model was trained to predict intermediate values of the AES algorithm, such as the output of the S-box[^fn-sbox] in the first round, directly from raw power traces. The trained model recovered the full 128-bit key using only a small number of traces per byte.

[^fn-sbox]: **S-box (Substitution Box)**: A nonlinear lookup table in block ciphers like AES that maps each input byte to a different output byte, providing the "confusion" property that makes ciphertext unpredictable from plaintext. S-box operations are the primary side-channel target because the output depends jointly on the plaintext and the secret key: a neural network trained on power traces during S-box computation can recover the key byte-by-byte, transforming cryptanalysis into a classification problem. \index{S-box!side-channel target}

@fig-stm32f-board shows the experimental hardware configuration: a ChipWhisperer setup with a custom STM32F target board that executes AES operations while allowing external equipment to monitor power consumption with high temporal precision. This setup demonstrates how even inexpensive, low-power embedded devices can leak information through side channels that modern machine learning models can learn to exploit.

![**STM32F415 Target Board**: Enables monitoring of power consumption during AES operations on the microcontroller, highlighting side-channel vulnerabilities that can be exploited by machine learning models.](images/png/stm32f_board.png){#fig-stm32f-board fig-alt="Photo of ChipWhisperer setup with STM32F415 target board. Green PCB with microcontroller chip connected to measurement probes for monitoring power consumption during cryptographic operations."}

Subsequent work expanded on this approach by introducing long-range models capable of exploiting broader temporal dependencies in the traces, improving performance even under noise and desynchronization [@bursztein2023generic]. These developments highlight the potential for machine learning models to serve as offensive cryptanalysis tools, especially in the analysis of secure hardware.

The implications extend beyond academic interest. As deep learning models continue to scale, their application to side-channel contexts is likely to lower the cost, skill threshold, and trace requirements of hardware-level attacks, posing a growing challenge for the secure deployment of embedded machine learning systems, cryptographic modules, and trusted execution environments.

As adversarial machine learning dramatically lowers the skill threshold required to execute sophisticated side-channel and extraction attacks, perimeter defenses are no longer sufficient. To survive in an environment where attacks are automated and continuous, we must fundamentally rearchitect our systems, building comprehensive, multi-layered defense mechanisms that protect the data, the model, and the underlying infrastructure simultaneously.

## Comprehensive Defense Architectures {#sec-security-privacy-comprehensive-defense-architectures-48ab}

If a determined adversary bypasses your API rate limits, defeats your input sanitization, and begins querying your model with adversarial examples, what stops them? A single line of defense is a single point of failure. Modern security requires a "defense in depth" architecture, where the compromise of any single component—whether a breached database, a leaked API key, or an exposed gradient—is contained by overlapping, independent layers of cryptographic and algorithmic protection.

Designing secure and privacy-preserving machine learning systems requires more than identifying individual threats. It demands a layered defense strategy that integrates protections across multiple system levels. The complete threat model---spanning attacks that target ML systems and attacks that ML systems enable---reveals that no single defense addresses all vectors simultaneously.

Effective defense therefore requires four complementary layers: Data Layer protections including differential privacy and secure computation that safeguard sensitive information during training; Model Layer defenses such as adversarial training and secure deployment that protect the models themselves; Runtime Layer measures including input validation and output monitoring that secure inference operations; and Hardware Layer foundations such as trusted execution environments that provide the trust anchor for all other protections.

### The Layered Defense Principle {#sec-security-privacy-layered-defense-principle-8706}

Layered defense (also known as defense-in-depth) represents a core security architecture principle where multiple independent defensive mechanisms work together to protect against diverse threat vectors. In machine learning systems, this approach becomes essential due to the unique attack surfaces introduced by data dependencies, model exposures, and inference patterns. Unlike traditional software systems that primarily face code-based vulnerabilities, ML systems are vulnerable to input manipulation, data leakage, model extraction, and runtime abuse, all amplified by tight coupling between data, model behavior, and infrastructure.

The layered approach recognizes that no single defensive mechanism can address all possible threats. Instead, security emerges from the interaction of complementary protections: data-layer techniques like differential privacy and federated learning; model-layer defenses including **robustness techniques**[^fn-robustness-privacy-tradeoff] and secure deployment; runtime-layer measures such as input validation and output monitoring; and hardware-layer solutions including trusted execution environments and secure boot. Each layer contributes to the system's overall resilience while compensating for potential weaknesses in other layers.

A structured framework for layered defense in ML systems progresses from data-centric protections to infrastructure-level enforcement. The framework builds on data protection practices, including encryption, access control, and lineage tracking, and connects forward to operational security measures for production deployment. By integrating safeguards across layers, organizations can build ML systems that not only perform reliably but also withstand adversarial pressure in production environments.

@fig-defense-stack visualizes how these layers interact: defensive mechanisms progress from foundational hardware-based security to runtime system protections, model-level controls, and privacy-preserving techniques at the data level. Each layer builds on the trust guarantees of the layer below it, forming an end-to-end strategy for deploying ML systems securely.

::: {#fig-defense-stack fig-env="figure" fig-pos="htb" fig-cap="**Layered Defense Stack**: Machine learning systems require multi-faceted security strategies that progress from foundational hardware protections to data-centric privacy techniques, building trust across all layers. This architecture integrates safeguards at the data, model, runtime, and infrastructure levels to mitigate threats and ensure robust deployment in production environments." fig-alt="Four-layer defense stack. Bottom to top: hardware security (TEEs, HSMs, PUFs), system security (integrity, monitoring), model security (encryption), data privacy (differential privacy, federated learning)."}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]

\tikzset{%
Line/.style={line width=1.0pt,black!50},
Box/.style={inner xsep=4pt,inner ysep=6pt,
    node distance=0.7,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    align=flush center,
    text width=29mm,
    minimum width=29mm, minimum height=11mm
  },
Box2/.style={Box,  node distance=0.7,draw=BrownLine,fill=BrownL},
Box3/.style={Box,  node distance=0.7, draw=VioletLine,fill=VioletL2},
Box4/.style={Box,node distance=0.7,  draw=BlueLine,fill=BlueL}
}

\node[Box](B1){Trusted Execution Environments};
\node[Box,right=of B1](B2){Secure Boot};
\node[Box,right=4.25of B2](B3){Hardware Security Modules};
\node[Box,right=of B3](B4){Physical Unclonable Functions};
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=6mm,
yshift=2.5mm,fill=BackColor!60,fit=(B1)(B4),line width=0.75pt](BB1){};
\node[below=6pt of  BB1.north,inner sep=0pt,
anchor=north]{\textbf{Hardware-Level Security}};
%
\node[Box3,above=2.0of B1](2B1){System Integrity Checks};
\node[Box3,right=of 2B1](2B2){Runtime Input Validation};
\node[Box3,right=of 2B2](2B3){Runtime Output Monitoring};
\node[Box3,right=of 2B3](2B4){Incident Response \& Recovery};
\scoped[on background layer]
\node[draw=BlueD,inner xsep=6mm,inner ysep=6mm,
yshift=2.5mm,fill=cyan!5,fit=(2B1)(2B4),line width=0.75pt](BB2){};
\node[below=6pt of  BB2.north,inner sep=0pt,
anchor=north]{\textbf{System-Level Security}};
%
\node[Box2,above=2.0 of 2B1](3B1){Model Encryption \& Serialization};
\node[Box2,right=2.5 of 3B1](3B2){Secure Model Design};
\path[](3B2)-|coordinate(S)(B4);
\node[Box2](3B3)at(S){Secure Deployment \& Access};
\scoped[on background layer]
\node[draw=GreenD,inner xsep=6mm,inner ysep=6mm,
yshift=2.5mm,fill=green!5,fit=(3B1)(3B3),line width=0.75pt](BB3){};
\node[below=6pt of  BB3.north,inner sep=0pt, xshift=4mm,
anchor=north]{\textbf{Model-Level Security}};
%
\node[Box4,above left=2 and 0.3of 3B2](4B1){Differential Privacy};
\node[Box4,above right=1.9 and 0.3of 3B2](4B2){Federated Learning};
\node[Box4,above=of 4B1](5B1){Homomorphic Encryption};
\node[Box4,above=of 4B2](5B2){Synthetic Data Generation};
%
\scoped[on background layer]
\node[draw=RedLine,inner xsep=6mm,inner ysep=6mm,
yshift=2.3mm,fill=magenta!4,fit=(4B1)(5B2),line width=0.75pt](BB4){};
\node[below=6pt of  BB4.north,inner sep=0pt,
anchor=north]{\textbf{Data Privacy \& Governance}};
%
\draw[Line,-latex](B1)--(2B1);
\draw[Line,-latex](B2)--++(0,1.8)-|(2B1.310);
\draw[Line,-latex](B3)--++(0,1.8)-|(3B3.230);
\draw[Line,-latex](B4)--(3B3);
%
\draw[Line,-latex](2B1)--(3B1);
\draw[Line,-latex](2B2.120)|-(3B2);
\draw[Line,-latex](2B3.80)|-(3B2);
%
\draw[Line,-latex](3B2.120)--++(0,1.2)-|(4B1);
\draw[Line,-latex](3B2.70)--++(0,1.2)-|(4B2);
\draw[Line,-latex](4B1)--(5B1);
\draw[Line,-latex](4B2)--(5B2);
\end{tikzpicture}
```
:::

### Privacy-Preserving Data Techniques {#sec-security-privacy-privacypreserving-data-techniques-64f8}

Our defense stack spans multiple architectural layers: data-layer privacy mechanisms, model-layer protections, runtime monitoring, and foundational hardware trust anchors. We begin with data privacy techniques that protect sensitive information at the point where it enters the ML pipeline, then examine model security mechanisms, and conclude with the hardware foundations (TEEs, secure boot, HSMs, PUFs) that anchor system trust. This organization proceeds from the most visible privacy concerns to the foundational infrastructure, recognizing that effective security requires coordinated deployment across all layers.

Protecting the privacy of individuals whose data fuels machine learning systems is a foundational requirement for trustworthy AI. Unlike traditional systems where data is often masked or anonymized before processing, ML workflows typically rely on access to raw, high-fidelity data to train effective models. This tension between utility and privacy has motivated a diverse set of techniques aimed at minimizing data exposure while preserving learning performance.

#### Federated Learning {#sec-security-privacy-federated-learning-3834}

Differential privacy adds noise to protect individual records, but some domains require a more structural solution: keeping raw data on the device entirely. In healthcare, finance, and other regulated domains, centralized data collection may be legally impossible. The following archetype illustrates this privacy-constrained scenario.

::: {.callout-lighthouse title="Archetype C (Federated MobileNet): The Need for Privacy"}
**Archetype C (Federated MobileNet)** (@sec-vol2-introduction-archetypes) represents the class of systems where privacy is a hard constraint, not an optimization. For a fleet of health monitors processing cardiac data, transmitting raw signals to the cloud is legally impossible under HIPAA/GDPR. Federated Learning is the only architectural choice that satisfies the **Privacy-Utility Trade-off**, enabling collective intelligence without data centralization.
:::

While differential privacy adds mathematical guarantees to data processing, federated learning (FL) offers a complementary approach that reduces privacy risks by restructuring the learning process itself. @sec-edge-intelligence examines the FL protocols and weighted aggregation mechanisms that provide structural privacy protection by keeping data localized on client devices. However, FL alone does not guarantee privacy. The gradient updates that clients transmit can leak sensitive information, making FL a necessary but insufficient component of a comprehensive privacy strategy.

##### Gradient Inversion Risks

Gradient inversion attacks (such as Deep Leakage from Gradients) demonstrate that an adversary with access to a client's raw update can reconstruct the original training data (e.g., images or text) with high fidelity [@zhu2019deep; @nasr2019comprehensive]. The gradient vector encodes information about the data that produced it, creating a channel for information leakage even when raw data never leaves the device. The defense against this vulnerability is *secure aggregation*, a cryptographic protocol that ensures the server can compute the aggregate update without ever observing any individual client's contribution.

::: {.callout-note title="Figure: Secure Aggregation Protocol" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm]
  \definecolor{ClientColor}{RGB}{240,240,240}
  \definecolor{ServerColor}{RGB}{200,220,255}

  \tikzset{
    node/.style={draw=black!70, thick, circle, minimum size=1.2cm, fill=ClientColor, align=center, font=\tiny}
  }

  % Clients
  \node[node] (C1) at (0, 3) {\textbf{Client A}\\$x_A + r$};
  \node[node] (C2) at (4, 3) {\textbf{Client B}\\$x_B - r$};

  % Server
  \node[draw, fill=ServerColor, rounded corners=2pt, minimum width=3cm, minimum height=1cm] (Server) at (2, 0) {\textbf{Server}\\Aggregate: $(x_A + r) + (x_B - r) = x_A + x_B$};

  % Masking arrow
  \draw[<->, dashed, orange, thick] (C1) -- node[midway, above, font=\tiny] {Shared Mask $r$} (C2);

  % Upload arrows
  \draw[->, thick] (C1) -- (Server);
  \draw[->, thick] (C2) -- (Server);

  \node[anchor=west, font=\scriptsize, text=gray] at (4.5, 0) {Masks cancel\\during summation};

\end{tikzpicture}
```
**Secure Aggregation in Federated Learning**. Devices add pairwise random masks ($r$) to their local updates before transmission. These masks sum to zero across all devices. The server receives only the masked (obfuscated) updates, but when it sums them, the masks mathematically cancel out, revealing the global aggregate while preserving the privacy of individual contributions.
:::

This vulnerability drives the need for **Secure Aggregation** protocols [@bonawitz2017practical] and local differential privacy. Secure aggregation ensures the server only sees the *sum* of updates, never an individual update, mathematically preventing inversion of any single user's data. Interestingly, the gradient compression techniques @sec-collective-communication examines for communication efficiency can also affect privacy properties: compressed gradients may leak less information than full-precision updates, though the interaction between compression and privacy guarantees requires careful analysis. Production systems typically combine FL with DP and secure aggregation to achieve formal privacy guarantees beyond structural protection alone. *Google Gboard's federated learning* deployment exemplifies this layered approach.

::: {.callout-note title="Google Gboard Federated Learning" icon=false}
Google's Gboard keyboard, examined in @sec-edge-intelligence as a federated learning deployment, demonstrates how security mechanisms layer atop the FL protocol. From a privacy perspective, the system combines three defense mechanisms: FL keeps raw typing data on-device; differential privacy ($\varepsilon\approx 6$) bounds information leakage from gradient updates; and secure aggregation ensures Google's servers see only the sum of thousands of encrypted updates, never individual contributions. This combination achieves formal privacy guarantees while maintaining 92% of centralized training accuracy with approximately 100 KB bandwidth per device per day.
:::

To address scenarios requiring computation on encrypted data, homomorphic encryption (HE)[^fn-he-breakthrough] and secure multiparty computation (SMPC) allow models to perform inference or training over encrypted inputs [@gentry2009fully; @yao1982protocols]. The computational overhead of homomorphic operations often requires efficiency optimization techniques, including model compression (quantization reduces precision requirements for encrypted operations), architectural optimization (depthwise separable convolutions minimize encrypted multiplications), and hardware acceleration (specialized cryptographic accelerators), to maintain practical performance.

[^fn-he-breakthrough]: **Homomorphic Encryption** (Greek *homos* "same" + *morphe* "form"): The name reflects the core property: operations on ciphertexts produce the "same form" of result as operations on plaintexts. Theoretical since the 1970s, fully homomorphic encryption became practical with Craig Gentry's 2009 PhD thesis. The systems cost remains steep: HE inference runs 1,000-10,000$\times$ slower than plaintext, limiting current ML applications to small models and low-throughput batch processing. \index{Homomorphic Encryption!etymology}

In the case of HE, operations on ciphertexts correspond to operations on plaintexts, enabling encrypted inference:
$$
\text{Enc}(f(x)) = f(\text{Enc}(x))
$$

This property supports privacy-preserving computation in untrusted environments, such as cloud inference over sensitive health or financial records. The computational cost of HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC[^fn-smpc-overhead], by contrast, distributes the computation across multiple parties such that no single party learns the complete input or output. This is particularly useful in joint training across institutions with strict data-use policies, such as hospitals or banks[^fn-smpc-collaborative].

[^fn-smpc-overhead]: **SMPC Performance Overhead**: Secure multi-party computation incurs 1,000-10,000$\times$ computational overhead through cryptographic operations (secret sharing, garbled circuits), transforming GPU-millisecond inference into hours. The practical compromise is hybrid deployment: applying SMPC only to sensitive layers (final classification, embedding lookup) while running non-sensitive layers in plaintext, reducing overhead to 10-100$\times$ at the cost of partial information leakage from unprotected layers. \index{SMPC!performance overhead}

[^fn-smpc-collaborative]: **SMPC (Secure Multi-Party Computation)**: Formalized in 1982 by Andrew Yao as the "Millionaires' Problem" (can two millionaires determine who is richer without revealing their wealth?). The framework enables hospitals to collaboratively train diagnostic ML models without sharing patient records. The key systems constraint is communication: SMPC requires each party to exchange encrypted shares for every operation, making network bandwidth rather than compute the dominant bottleneck in distributed ML training. \index{SMPC!collaborative training}

#### Synthetic Data Generation {#sec-security-privacy-synthetic-data-generation-4349}

Beyond cryptographic approaches like homomorphic encryption, a more pragmatic and increasingly popular alternative involves the use of synthetic data generation[^fn-synthetic-data]. This approach offers an intuitive solution to privacy protection: if we can create artificial data that looks statistically similar to real data, we can train models without ever exposing sensitive information.

[^fn-synthetic-data]: **Synthetic Data**: The market grew from $110 million (2019) to $1.1 billion (2023), driven by privacy regulations that make real data expensive to use. The fundamental trade-off is fidelity versus privacy: synthetic datasets achieving 95%+ statistical fidelity to the original may still leak information about rare individuals through generative model memorization, which is why production systems combine synthetic generation with differential privacy guarantees. \index{Synthetic Data!privacy trade-off}

Synthetic data generation works by training a generative model (such as a GAN, VAE, or diffusion model) on the original sensitive dataset, then using this trained generator to produce new artificial samples. The key insight is that the generative model learns the underlying patterns and distributions in the data without memorizing specific individuals. When properly implemented, the synthetic data preserves statistical properties necessary for machine learning while removing personally identifiable information.

The generation typically follows three stages. First, distribution learning trains a generative model $G_\theta$ on real data $D_{\text{real}} = \{x_1, x_2,\ldots, x_n\}$ to learn the data distribution $p(x)$. Second, synthetic sampling generates new samples $D_{\text{synthetic}} = \{G_\theta(z_1), G_\theta(z_2),\ldots, G_\theta(z_m)\}$ by sampling from random noise $z_i \sim \mathcal{N}(0,I)$. Third, validation verifies that $D_{\text{synthetic}}$ maintains statistical fidelity to $D_{\text{real}}$ while avoiding memorization of specific records. By training generative models on real datasets and sampling new instances from the learned distribution, organizations can create datasets that approximate the statistical properties of the original data without retaining identifiable details [@goncalves2020generation].

While appealing, synthetic data generation faces important limitations. Generative models can suffer from mode collapse, failing to capture rare but important patterns in the original data. More critically, sophisticated adversaries can potentially extract information about the original training data through generative model inversion attacks or membership inference. The privacy protection depends heavily on the generative model architecture, training procedure, and hyperparameter choices—making it difficult to provide formal privacy guarantees without additional mechanisms like differential privacy.

Consider a practical example where a hospital wants to share patient data for ML research while protecting privacy. They train a generative adversarial network (GAN) on 10,000 real patient records containing demographics, lab results, and diagnoses. The GAN learns to generate synthetic patients with realistic combinations of features (e.g., diabetic patients typically have elevated glucose levels). The synthetic dataset of 50,000 artificial patients maintains clinical correlations necessary for training diagnostic models while containing no real patient information. However, the hospital also applies differential privacy during GAN training (ε = 1.0) to prevent the model from memorizing specific patients, trading a 5% reduction in statistical fidelity for formal privacy guarantees.

Together, these techniques reflect a shift from isolating data as the sole path to privacy toward embedding privacy-preserving mechanisms into the learning process itself. Each method offers distinct guarantees and trade-offs depending on the application context, threat model, and regulatory constraints. Effective system design often combines multiple approaches, such as applying differential privacy within a federated learning setup, or employing homomorphic encryption for important inference stages, to build ML systems that are both useful and respectful of user privacy.

#### Comparative Properties {#sec-security-privacy-comparative-properties-9ca5}

These privacy-preserving approaches differ not only in the guarantees they offer but also in their system-level implications. For practitioners, the choice of mechanism depends on factors such as computational constraints, deployment architecture, and regulatory requirements.

@tbl-privacy-technique-comparison summarizes the comparative properties of these methods across privacy strength, runtime overhead, maturity, and common use cases. Understanding these trade-offs is important for designing privacy-aware machine learning systems that operate under real-world constraints.

### Case Study: GPT-3 Data Extraction Attack {#sec-security-privacy-case-study-gpt3-data-extraction-attack-5126}

In 2020, researchers demonstrated that large language models could leak sensitive training data through carefully crafted prompts [@carlini2021extracting]. The research team systematically queried OpenAI's GPT-3 model to extract verbatim content from its training dataset, revealing privacy vulnerabilities in large-scale language models.

The attack proved remarkably successful at extracting sensitive information directly from the model's outputs. By repeatedly querying the model with prompts like "My name is" followed by attempts to continue famous quotes or repeated phrases, researchers successfully extracted personal information including email addresses and phone numbers from the training data, verbatim passages from copyrighted books, private data that should have been filtered during training, and personally identifiable information from millions of individuals.

The technical approach exploited GPT-3's memorization of rare or repeated text sequences. The researchers used prompt engineering to craft inputs that triggered memorized sequences, continuation attacks that used partial quotes or names to extract full sensitive information, statistical analysis to identify patterns in model outputs indicating verbatim memorization, and verification methods that cross-referenced extracted data with known public sources to confirm accuracy. Out of 600,000 attempts, they successfully extracted over 16,000 unique instances of memorized training data.

This attack challenged assumptions about training data privacy. The results demonstrated that large language models can act as unintentional databases, storing and retrieving sensitive information from their training data. This violated privacy expectations that training data would be "forgotten" after model training, revealing that scale amplifies privacy risk as larger models (175B parameters) memorize more training data than smaller models.

The research revealed that common data protection measures proved insufficient. Even after data deduplication, models still memorized sensitive information, highlighting the tension between model utility and privacy protection. Techniques to prevent memorization such as differential privacy and aggressive data filtering reduce model quality, creating challenging trade-offs for practitioners.

The industry response was swift and comprehensive. Organizations began widespread adoption of differential privacy in large model training, enhanced data filtering and PII removal processes, development of membership inference defenses, new research into machine unlearning techniques, and regulatory discussions about training data rights and model transparency. Modern organizations now commonly implement differential privacy during training (with privacy parameter epsilon less than or equal to 8), aggressive PII filtering using automated detection tools, regular auditing for data memorization using extraction attacks, and legal frameworks for handling training data containing personal information [@carlini2021extracting].

### Secure Model Design {#sec-security-privacy-secure-model-design-69a6}

The GPT-3 extraction attack demonstrates that even with perfect data-level privacy protections, the model itself can leak information through its outputs, structure, or learned behavior. Differential privacy adds noise during training, but cannot prevent a model from memorizing and later revealing sensitive patterns. This limitation motivates model-level security: architectural and design choices that complement data protections by reducing the model's capacity to serve as an information conduit.

Security must therefore begin at the design phase of a machine learning system. While downstream mechanisms such as access control and encryption protect models once deployed, many vulnerabilities can be mitigated earlier through architectural choices, defensive training strategies, and mechanisms that embed resilience directly into the model's structure or behavior. By considering security as a design constraint, system developers can reduce the model's exposure to attacks, limit its ability to leak sensitive information, and provide verifiable ownership protection.

One important design strategy is to build robust-by-construction models that reduce the risk of exploitation at inference time. For instance, models with confidence calibration or abstention mechanisms can be trained to avoid making predictions when input uncertainty is high. These techniques can help prevent overconfident misclassifications in response to adversarial or out-of-distribution inputs. Models may also employ output smoothing, regularizing the output distribution to reduce sharp decision boundaries that are especially susceptible to adversarial perturbations.

Certain application contexts may also benefit from choosing simpler or compressed architectures. Limiting model capacity can reduce opportunities for memorization of sensitive training data and complicate efforts to reverse-engineer the model from output behavior. For embedded or on-device settings, smaller models are also easier to secure, as they typically require less memory and compute, lowering the likelihood of side-channel leakage or runtime manipulation.

Another design-stage consideration is the use of model watermarking[^fn-model-watermarking], a technique for embedding verifiable ownership signatures directly into the model's parameters or output behavior [@adi2018turning]. A watermark might be implemented, for example, as a hidden response pattern triggered by specific inputs, or as a parameter-space perturbation that does not affect accuracy but is statistically identifiable.

[^fn-model-watermarking]: **Model Watermarking**: IP protection technique developed in 2017 by Adi et al., where a model is trained to produce a specific "signature" response on secret trigger inputs known only to the owner. Modern watermarks embed in less than 0.01% of parameters while maintaining 99%+ accuracy. The systems challenge is robustness: watermarks must survive model fine-tuning, pruning, and distillation, the same operations attackers use to strip ownership marks from stolen models. \index{Model Watermarking!IP protection}

For example, in a keyword spotting system deployed on embedded hardware for voice activation (e.g., "Hey Alexa" or "OK Google"), a secure design might use a lightweight convolutional neural network with confidence calibration to avoid false activations on uncertain audio. The model might also include an abstention threshold, below which it produces no activation at all. To protect intellectual property, a designer could embed a watermark by training the model to respond with a unique label only when presented with a specific, unused audio trigger known only to the developer. These design choices not only improve robustness and accountability, but also support future verification in case of IP disputes or performance failures in the field.

In high-risk applications, such as medical diagnosis, autonomous vehicles, or financial decision systems, designers may also prioritize interpretable model architectures, such as decision trees, rule-based classifiers, or sparsified networks, to enhance system auditability. These models are often easier to understand and explain, making it simpler to identify potential vulnerabilities or biases. Using interpretable models allows developers to provide clearer insights into how the system arrived at a particular decision, which is important for building trust with users and regulators.

Model design choices often reflect trade-offs between accuracy, robustness, transparency, and system complexity. When viewed from a systems perspective, early-stage design decisions yield the highest value for long-term security. They shape what the model can learn, how it behaves under uncertainty, and what guarantees can be made about its provenance, interpretability, and resilience.

### Secure Model Deployment {#sec-security-privacy-secure-model-deployment-e08c}

While secure design establishes a foundation of robustness, protection extends beyond the model itself to how it is packaged and deployed. Protecting machine learning models from theft, abuse, and unauthorized manipulation requires security considerations throughout both the design and deployment phases. A model's vulnerability is not solely determined by its training procedure or architecture, but also by how it is serialized, packaged, deployed, and accessed during inference. As models are increasingly embedded into edge devices, served through public APIs, or integrated into multi-tenant platforms, robust security practices are important to ensure the integrity, confidentiality, and availability of model behavior.

This section addresses security mechanisms across three key stages: model design, secure packaging and serialization, and deployment and access control. These practices complement model optimization techniques such as quantization, pruning, and knowledge distillation, where performance improvements must not compromise security properties.

From a design perspective, architectural choices can reduce a model's exposure to adversarial manipulation and unauthorized use. For example, models can incorporate confidence calibration or abstention mechanisms that allow them to reject uncertain or anomalous inputs rather than producing potentially misleading outputs. Designing models with simpler or compressed architectures can also reduce the risk of reverse engineering or information leakage through side-channel analysis. In some cases, model designers may embed imperceptible watermarks, which are unique signatures embedded in the parameters or behavior of the model, that can later be used to demonstrate ownership in cases of misappropriation [@uchida2017embedding]. These design-time protections are essential for commercially valuable models, where intellectual property rights are at stake.

Once training is complete, the model must be securely packaged for deployment. Storing models in plaintext formats, including unencrypted ONNX or PyTorch checkpoint files, can expose internal structures and parameters to attackers with access to the file system or memory. To mitigate this risk, models should be encrypted, obfuscated, or wrapped in secure containers. Decryption keys should be made available only at runtime and only within trusted environments. Additional mechanisms, such as quantization-aware encryption or integrity-checking wrappers, can prevent tampering and offline model theft.

Deployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests.

[^fn-oauth]: **OAuth (Open Authorization)**: Standard developed in 2006, now used by 3+ billion users, enabling API access without exposing credentials. For ML inference APIs, OAuth 2.0 token-based authentication adds 5--15 ms of latency per request for token validation, a negligible cost for batch inference but a meaningful overhead for real-time serving at thousands of requests per second where every millisecond of latency budget matters. \index{OAuth!ML API authentication}

[^fn-mutual-tls]: **Mutual TLS (mTLS)**: Enhanced Transport Layer Security (introduced 1999) where both client and server authenticate via certificates. The 15--30 ms handshake overhead is a one-time cost per connection, amortized across subsequent requests via connection pooling. For ML model serving, mTLS ensures that only authenticated services can submit inference requests, preventing unauthorized model extraction through API access. \index{Mutual TLS!ML serving}

[^fn-api-keys]: **API Keys**: Simple authentication tokens popularized by Google Maps API (2005). Studies show 10-15% of GitHub repositories accidentally contain leaked API keys. For ML services, a leaked API key is not merely a billing risk: it provides the attacker unlimited query access for model extraction, effectively converting a credential management failure into complete model theft at the cost of API compute credits. \index{API Keys!credential leakage}

[^fn-rbac]: **RBAC (Role-Based Access Control)**: Formalized by NIST in the 1990s, RBAC assigns permissions to roles rather than individuals, reducing administration overhead by 90%+ compared to per-user policies. For ML platforms, RBAC maps naturally to the ML lifecycle: data engineers access training data but not model weights, ML engineers access models but not production serving keys, and monitoring systems access predictions but not raw inputs, enforcing least-privilege across the pipeline. \index{RBAC!ML pipeline}

This key authenticates the client and allows the backend to enforce usage policies, monitor for abuse, and log access patterns. Secure implementations retrieve API keys from environment variables rather than hardcoding them into source code, preventing credential exposure in version control systems or application logs. Such key-based access control mechanisms are simple to implement but require careful key management and monitoring to prevent misuse, unauthorized access, or model extraction. Additional security measures in production deployments typically include model integrity verification through SHA-256 hash checking, rate limiting to prevent abuse, input validation for size and format constraints, and comprehensive logging for security event tracking.

The secure deployment patterns established here integrate naturally with development workflows, ensuring security becomes part of standard engineering practice rather than an afterthought. Runtime monitoring (@sec-security-privacy-runtime-system-monitoring-a71c) extends these protections to operational environments.

### Runtime System Monitoring {#sec-security-privacy-runtime-system-monitoring-a71c}

While secure design and deployment establish strong foundations, protection must extend to runtime operations. Even with robust design and deployment safeguards, machine learning systems remain vulnerable to runtime threats. Attackers may craft inputs that bypass validation, exploit model behavior, or target system-level infrastructure.

Production ML systems face diverse deployment contexts (from cloud services to edge devices to embedded systems). Each environment presents unique monitoring challenges and opportunities. Defensive strategies must extend beyond static protection to include real-time monitoring, threat detection, and incident response. This section outlines operational defenses that maintain system trust under adversarial conditions.

Runtime monitoring encompasses a range of techniques for observing system behavior, detecting anomalies, and triggering mitigation. These techniques can be grouped into three categories: input validation, output monitoring, and system integrity checks.

#### Input Validation {#sec-security-privacy-input-validation-c96f}

Input validation is the first line of defense at runtime. It ensures that incoming data conforms to expected formats, statistical properties, or semantic constraints before it is passed to a machine learning model. Without these safeguards, models are vulnerable to adversarial inputs, which are crafted examples designed to trigger incorrect predictions, or to malformed inputs that cause unexpected behavior in preprocessing or inference.

Machine learning models, unlike traditional rule-based systems, often do not fail safely. Small, carefully chosen changes to input data can cause models to make high-confidence but incorrect predictions. Input validation helps detect and reject such inputs early in the pipeline [@goodfellow2015explaining].

Validation techniques range from low-level checks (e.g., input size, type, and value ranges) to semantic filters (e.g., verifying whether an image contains a recognizable object or whether a voice recording includes speech). For example, a facial recognition system might validate that the uploaded image is within a certain resolution range (e.g., $224\times224$ to $1024\times1024$ pixels), contains RGB channels, and passes a lightweight face detection filter. This prevents inputs like blank images, text screenshots, or synthetic adversarial patterns from reaching the model. Similarly, a voice assistant might require that incoming audio files be between 1 and 5 seconds long, have a valid sampling rate (e.g., 16kHz), and contain detectable human speech using a speech activity detector (SAD)[^fn-speech-activity-detector]. This ensures that empty recordings, music clips, or noise bursts are filtered before model inference.

[^fn-speech-activity-detector]: **SAD (Speech Activity Detector)**: Algorithm distinguishing speech from silence, noise, or music, essential for voice interfaces since the 1990s. Modern neural SADs operate in less than 10 ms latency at 95%+ accuracy, serving as a lightweight security gate: by filtering non-speech inputs before the expensive ASR model, SAD prevents adversarial audio attacks (noise-embedded commands, ultrasonic triggers) from reaching the speech recognition system. \index{SAD!input validation}

In generative systems such as DALL·E, Stable Diffusion, or Sora, input validation often involves prompt filtering. This includes scanning the user's text prompt for banned terms, brand names, profanity, or misleading medical claims. For example, a user prompt like "Generate an image of a medication bottle labeled with Pfizer's logo" might be rejected or rewritten due to trademark concerns. Filters may operate using keyword lists, regular expressions, or lightweight classifiers that assess prompt intent. These filters prevent the generative model from being used to produce harmful, illegal, or misleading content—even before sampling begins.

In some applications, distributional checks are also used. These assess whether the incoming data statistically resembles what the model saw during training. For instance, a computer vision pipeline might compare the color histogram of the input image to a baseline distribution, flagging outliers for manual review or rejection.

These validations can be lightweight (heuristics or threshold rules) or learned (small models trained to detect distribution shift or adversarial artifacts). In either case, input validation acts as an important pre-inference firewall—reducing exposure to adversarial behavior, improving system stability, and increasing trust in downstream model decisions.

#### Output Monitoring {#sec-security-privacy-output-monitoring-cf37}

Even when inputs pass validation, adversarial or unexpected behavior may still emerge at the model's output. Output monitoring helps detect such anomalies by analyzing model predictions in real time. These mechanisms observe how the model behaves across inputs, by tracking its confidence, prediction entropy, class distribution, or response patterns, to flag deviations from expected behavior.

A key target for monitoring is prediction confidence. For example, if a classification model begins assigning high confidence to low-frequency or previously rare classes, this may indicate the presence of adversarial inputs or a shift in the underlying data distribution. Monitoring the entropy of the output distribution can similarly reveal when the model is overly certain in ambiguous contexts—an early signal of possible manipulation.

In content moderation systems, a model that normally outputs neutral or "safe" labels may suddenly begin producing high-confidence "safe" labels for inputs containing offensive or restricted content. Output monitoring can detect this mismatch by comparing predictions against auxiliary signals or known-safe reference sets. When deviations are detected, the system may trigger a fallback policy—such as escalating the content for human review or switching to a conservative baseline model.

Time-series models also benefit from output monitoring. For instance, an anomaly detection model used in fraud detection might track predicted fraud scores for sequences of financial transactions. A sudden drop in fraud scores, especially during periods of high transaction volume, may indicate model tampering, label leakage, or evasion attempts. Monitoring the temporal evolution of predictions provides a broader perspective than static, pointwise classification.

Generative models, such as text-to-image systems, introduce unique output monitoring challenges. These models can produce high-fidelity imagery that may inadvertently violate content safety policies, platform guidelines, or user expectations. To mitigate these risks, post-generation classifiers are commonly employed to assess generated content for objectionable characteristics such as violence, nudity, or brand misuse. These classifiers operate downstream of the generative model and can suppress, blur, or reject outputs based on predefined thresholds. Some systems also inspect internal representations (e.g., attention maps[^fn-attention-maps] or latent embeddings) to anticipate potential misuse before content is rendered.

[^fn-attention-maps]: **Attention Maps**: Visualization of transformer self-attention weights, introduced with the attention mechanism by Bahdanau et al. in 2015. For security monitoring, attention maps serve a dual role: they help detect adversarial inputs (which produce anomalous attention distributions compared to clean inputs) and enable output auditing by revealing which input tokens most influenced a generation, making manipulation attempts more traceable. \index{Attention Maps!security monitoring}

However, prompt filtering alone is insufficient for safety. Research has shown that text-to-image systems can be manipulated through implicitly adversarial prompts, which are queries that appear benign but lead to policy-violating outputs. The Adversarial Nibbler project introduces an open red teaming methodology that identifies such prompts and demonstrates how models like Stable Diffusion can produce unintended content despite the absence of explicit trigger phrases [@quaye2024adversarial]. These failure cases often bypass prompt filters because their risk arises from model behavior during generation, not from syntactic or lexical cues.

@fig-adversarial-nibbler provides concrete examples revealing how innocuous prompts can trigger unsafe generations. Such examples highlight the limitations of pre-generation safety checks and reinforce the necessity of output-based monitoring as a second line of defense. This two-stage pipeline consisting of prompt filtering followed by post-hoc content analysis is essential for ensuring the safe deployment of generative models in open-ended or user-facing environments.

![**Adversarial Prompt Evasion**: Implicitly adversarial prompts bypass typical content filters by triggering unintended generations, revealing limitations of solely relying on pre-generation safety checks. These examples underscore the necessity of post-hoc content analysis as a complementary defense layer for robust generative AI systems.](images/png/adversarial_nibbler_example.png){#fig-adversarial-nibbler fig-alt="Grid of generated images from text-to-image model. Prompts appear innocuous but produce policy-violating outputs, demonstrating how adversarial prompts bypass content filters."}

In the domain of language generation, output monitoring plays a different but equally important role. Here, the goal is often to detect toxicity, hallucinated claims, or off-distribution responses. For example, a customer support chatbot may be monitored for keyword presence, tonal alignment, or semantic coherence. If a response contains profanity, unsupported assertions, or syntactically malformed text, the system may trigger a rephrasing, initiate a fallback to scripted templates, or halt the response altogether.

Effective output monitoring combines rule-based heuristics with learned detectors trained on historical outputs. These detectors are deployed to flag deviations in real time and feed alerts into incident response pipelines. In contrast to model-centric defenses like adversarial training, which aim to improve model robustness, output monitoring emphasizes containment and remediation. Its role is not to prevent exploitation but to detect its symptoms and initiate appropriate countermeasures [@savas2022ml]. In safety-important or policy-sensitive applications, such mechanisms form a important layer of operational resilience.

These principles have been implemented in recent output filtering frameworks. For example, LLM Guard combines transformer-based classifiers with safety dimensions such as toxicity, misinformation, and illegal content to assess and reject prompts or completions in instruction-tuned LLMs [@lee2023llmguard]. Similarly, [ShieldGemma](https://ai.google.dev/gemma/docs/shieldgemma), developed as part of Google's open Gemma model release, applies configurable scoring functions to detect and filter undesired outputs during inference. Both systems exemplify how safety classifiers and output monitors are being integrated into the runtime stack to support scalable, policy-aligned deployment of generative language models.

#### Integrity Checks {#sec-security-privacy-integrity-checks-2989}

While input and output monitoring focus on model behavior, system integrity checks ensure that the underlying model files, execution environment, and serving infrastructure remain untampered throughout deployment. These checks detect unauthorized modifications, verify that the model running in production is authentic, and alert operators to suspicious system-level activity.

One of the most common integrity mechanisms is cryptographic model verification. Before a model is loaded into memory, the system can compute a cryptographic hash (e.g., SHA-256)[^fn-sha256-security] of the model file and compare it against a known-good signature.

[^fn-sha256-security]: **SHA-256** (Secure Hash Algorithm, NSA 2001): Produces 256-bit digests with no known practical collision attacks after 20+ years. For ML model integrity, computing a SHA-256 hash of a multi-gigabyte model file takes seconds and detects any modification, no matter how small. This makes cryptographic hashing the cheapest and most effective defense against model tampering during deployment, storage, and transfer between training and serving environments. \index{SHA-256!model integrity}

Access control and audit logging complement cryptographic checks. ML systems should restrict access to model files using role-based permissions and monitor file access patterns. For instance, repeated attempts to read model checkpoints from a non-standard path, or inference requests from unauthorized IP ranges, may indicate tampering, privilege escalation, or insider threats.

In cloud environments, container- or VM-based isolation[^fn-container-vm-isolation] helps enforce process and memory boundaries, but these protections can erode over time due to misconfiguration or supply chain vulnerabilities.

[^fn-container-vm-isolation]: **Container vs. VM Isolation**: Containers (Docker, 2013) share the host kernel with 0-5% CPU overhead but weaker isolation; VMs provide hardware-level separation with 10-15% overhead. For ML serving, this is a security-performance trade-off: containers enable rapid model scaling but share a kernel attack surface with co-tenants, while VMs prevent cross-tenant model extraction at the cost of higher memory overhead per serving instance. \index{Container Isolation!ML serving}

For example, in a regulated healthcare ML deployment[^fn-healthcare-ml-compliance], integrity checks might include: verifying the model hash against a signed manifest, validating that the runtime environment uses only approved Python packages, and checking that inference occurs inside a signed and attested virtual machine. These checks ensure compliance with regulations like HIPAA[^fn-hipaa-ml-requirements]'s integrity requirements and GDPR's accountability principle, limit the risk of silent failures, and create a forensic trail in case of audit or breach.

[^fn-healthcare-ml-compliance]: **Healthcare ML Compliance**: The FDA has approved 500+ AI-based medical devices since 2016 under 21 CFR Part 820, with approval cycles of 2-5 years costing $50+ million. The key systems constraint: any model update requires re-validation, creating tension between ML's iterative improvement cycle (retrain weekly) and regulatory approval timelines (validate over months), which drives the adoption of locked, versioned model deployment architectures. \index{Healthcare ML!FDA compliance}

[^fn-hipaa-ml-requirements]: **HIPAA ML Requirements**: The Health Insurance Portability and Accountability Act (1996) governs 600+ million US patient records. For ML systems specifically, HIPAA mandates encryption at rest and in transit, audit logs for all data access, and Business Associate Agreements for cloud ML services (fines up to $1.5 million per incident). These requirements mean that training pipelines must log every data access, and model checkpoints containing patient-derived information must be encrypted, adding storage and I/O overhead. \index{HIPAA!ML requirements}

Some systems also implement runtime memory verification, such as scanning for unexpected model parameter changes or checking that memory-mapped model weights remain unaltered during execution. While more common in high-assurance systems, such checks are becoming more feasible with the adoption of secure enclaves and trusted runtimes.

Taken together, system integrity checks play a important role in protecting machine learning systems from low-level attacks that bypass the model interface. When coupled with input/output monitoring, they provide layered assurance that both the model and its execution environment remain trustworthy under adversarial conditions.

#### Response and Rollback {#sec-security-privacy-response-rollback-1792}

When a security breach, anomaly, or performance degradation is detected in a deployed machine learning system, rapid and structured incident response is important to minimizing impact. The goal is not only to contain the issue but to restore system integrity and ensure that future deployments benefit from the insights gained. Unlike traditional software systems, ML responses may require handling model state, data drift, or inference behavior, making recovery more complex.

The first step is to define incident detection thresholds that trigger escalation. These thresholds may come from input validation (e.g., invalid input rates), output monitoring (e.g., drop in prediction confidence), or system integrity checks (e.g., failed model signature verification). When a threshold is crossed, the system should initiate an automated or semi-automated response protocol.

One common strategy is model rollback, where the system reverts to a previously verified version of the model. For instance, if a newly deployed fraud detection model begins misclassifying transactions, the system may fall back to the last known-good checkpoint, restoring service while the affected version is quarantined. Rollback mechanisms require version-controlled model storage, typically supported by MLOps platforms such as MLflow, TFX, or SageMaker.

In high-availability environments, model isolation may be used to contain failures. The affected model instance can be removed from load balancers or shadowed in a canary deployment[^fn-canary-deployment] setup. This allows continued service with unaffected replicas while maintaining forensic access to the compromised model for analysis.

[^fn-canary-deployment]: **Canary Deployment** (named after coal mine canaries that detected gas before miners were affected): New model versions are rolled out to 1--5% of traffic before full deployment. For ML systems, canary deployments are essential because model failures are often statistical rather than binary: a backdoored model may pass unit tests while degrading accuracy on specific subpopulations, detectable only under real traffic patterns at canary scale. \index{Canary Deployment!ML rollout}

Traffic throttling is another immediate response tool. If an adversarial actor is probing a public inference API at high volume, the system can rate-limit or temporarily block offending IP ranges while continuing to serve trusted clients. This containment technique helps prevent abuse without requiring full system shutdown.

Once immediate containment is in place, investigation and recovery can begin. This may include forensic analysis of input logs, parameter deltas between model versions, or memory snapshots from inference containers. In regulated environments, organizations may also need to notify users or auditors, particularly if personal or safety-important data was affected.

Recovery typically involves retraining or patching the model. This must occur through a secure update process, using signed artifacts, trusted build pipelines, and validated data. To prevent recurrence, the incident should feed back into model evaluation pipelines—updating tests, refining monitoring thresholds, or hardening input defenses. For example, if a prompt injection attack[^fn-prompt-injection] bypassed a content filter in a generative model, retraining might include adversarially crafted prompts, and the prompt validation logic would be updated to reflect newly discovered patterns.

[^fn-prompt-injection]: **Prompt Injection** (by analogy with SQL injection): First widely documented in 2022, these attacks embed adversarial instructions within user input that override the model's system prompt. Unlike SQL injection, which exploits a syntactic boundary between code and data, prompt injection exploits the absence of any such boundary in LLMs: model instructions and user content share the same token stream, making robust detection fundamentally harder than traditional input sanitization. \index{Prompt Injection!LLM security}

Finally, organizations should establish post-incident review practices. This includes documenting root causes, identifying gaps in detection or response, and updating policies and playbooks. Incident reviews help translate operational failures into actionable improvements across the design-deploy-monitor lifecycle.

### Hardware Security Foundations {#sec-security-privacy-hardware-security-foundations-f5e8}

Input validation catches adversarial examples. Output monitoring detects anomalous predictions. Integrity checks verify model authenticity. Yet all these software defenses share a critical assumption: the underlying execution environment is trustworthy. If an attacker compromises the operating system through a privilege escalation exploit, or gains physical access to an edge device's debug port, these careful software protections become meaningless since the attacker can simply disable them.

This fundamental limitation motivates the hardware security mechanisms we now examine: protections that operate below the software layer and remain secure even when all software is compromised. Hardware-based security creates a root of trust anchored in silicon, providing the foundation upon which all higher-layer defenses depend.

At the foundational level of our defensive framework, hardware-based security mechanisms provide the trust anchor for all higher-layer protections. Machine learning systems deployed in edge devices, embedded systems, and untrusted cloud infrastructure increasingly rely on hardware-based security features to establish this foundation. Hardware acceleration platforms, including GPUs, TPUs, and specialized ML accelerators, often incorporate security features such as secure enclaves, trusted execution environments, and hardware cryptographic units, while edge deployment scenarios present unique security challenges due to physical accessibility and constrained resources.

These hardware security mechanisms become particularly crucial when systems must meet regulatory compliance requirements. Healthcare ML systems handling protected health information under HIPAA must implement "appropriate technical safeguards" including access controls and encryption. Systems processing EU citizens' data under GDPR must demonstrate "appropriate technical and organizational measures" with privacy by design principles embedded at the hardware level.

To understand how hardware security protects ML systems, imagine building a secure fortress for your most valuable assets. Each hardware security primitive serves a distinct defensive role:

These mechanisms work together to create comprehensive protection that begins in hardware and extends through all software layers.

These four complementary hardware primitives work together to create comprehensive protection, as summarized in @tbl-hardware-security-mechanisms. Each mechanism addresses different security challenges but works most effectively when combined: secure boot establishes initial trust, TEEs provide runtime isolation, HSMs handle cryptographic operations, and PUFs enable device-unique authentication. Trusted Execution Environments (TEEs) provide isolated runtime environments for sensitive computations. Secure Boot ensures system integrity from power-on, creating the trusted foundation that TEEs depend upon. Hardware Security Modules (HSMs) offer specialized cryptographic processing and tamper-resistant key storage, often required for regulatory compliance. Physical Unclonable Functions (PUFs) provide device-unique identities that enable lightweight authentication and cannot be cloned or extracted.

| **Mechanism**         | **Fortress Analogy and Function**                                                                                                                                                                                                               |
|:----------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Secure Boot**       | Functions like a trusted gatekeeper checking credentials of everyone entering the fortress at dawn. Before your system runs any code, Secure Boot cryptographically verifies that the firmware and operating system have not been tampered with. |
| **Trusted Execution** | Create secure, windowless rooms deep inside the fortress where you                                                                                                                                                                              |
| **Environments**      | handle your most sensitive operations. When your ML model processes                                                                                                                                                                             |
| **(TEEs)**            | private medical data or proprietary algorithms, the TEE isolates these computations from the rest of the system.                                                                                                                                |
| **Hardware Security** | Serve as specialized, impenetrable vaults designed specifically for                                                                                                                                                                             |
| **Modules (HSMs)**    | storing and using your most valuable cryptographic keys. Rather than keeping encryption keys in regular computer memory where they might be stolen, HSMs provide tamper-resistant storage.                                                      |
| **Physical**          | Give each device a unique biometric fingerprint at the silicon                                                                                                                                                                                  |
| **Unclonable**        | level. Just as human fingerprints cannot be perfectly replicated,                                                                                                                                                                               |
| **Functions (PUFs)**  | PUFs exploit tiny manufacturing variations in each chip to create device-unique identifiers that cannot be cloned.                                                                                                                              |

: **Hardware Security Mechanisms**: Each primitive provides distinct defensive capabilities that work together to create comprehensive protection from hardware-level threats. {#tbl-hardware-security-mechanisms}

Each mechanism addresses different aspects of the security challenge, working most effectively when deployed together across hardware, firmware, and software boundaries.

#### Hardware-Software Co-Design {#sec-security-privacy-hardwaresoftware-codesign-bed2}

Modern ML systems require holistic analysis of security trade-offs across the entire hardware-software stack, similar to how we analyze compute-memory-energy trade-offs in performance optimization. The interdependence between hardware security features and software defenses creates both opportunities and constraints that must be understood quantitatively.

Hardware security mechanisms introduce measurable overhead that must be factored into system design. ARM TrustZone world-switching adds approximately 300--1000 cycles depending on processor generation and cache state (0.6--2.0 \mu s at 500MHz) of latency per transition between secure and non-secure worlds. Cryptographic operations in secure mode typically consume 15--30% additional power compared to normal execution, impacting battery life in mobile ML applications. Intel SGX context switching imposes 15--30 \mu s overhead per inference, representing 2% energy overhead for typical edge ML workloads.

Security features scale differently than computational resources. TEE memory limitations constrain model size regardless of available system memory. A quantized ResNet-18 model (47 MB) can operate within ARM TrustZone constraints, while ResNet-50 (176 MB) requires careful memory management or model partitioning. These constraints create architectural decisions that must be made early in system design.

Different threat models and protection levels require quantitative trade-off analysis. For ML workloads requiring cryptographic verification, AES-256 operations add 0.1--0.5 ms per inference depending on model size and hardware acceleration availability. Homomorphic encryption operations impose 100-100,000$\times$ computational overhead, with fully homomorphic encryption (FHE) at the higher end and somewhat homomorphic encryption (SHE) at the lower end, making them viable only for small models or offline scenarios where strong privacy guarantees justify the performance cost.

#### Trusted Execution Environments {#sec-security-privacy-trusted-execution-environments-80ed}

A Trusted Execution Environment (TEE)[^fn-tee] is a hardware-isolated region within a processor designed to protect sensitive computations and data from potentially compromised software. TEEs enforce confidentiality, integrity, and runtime isolation, ensuring that even if the host operating system or application layer is attacked, sensitive operations within the TEE remain secure.

[^fn-tee]: **TEE (Trusted Execution Environment)**: Hardware-isolated processor regions that emerged from ARM's TrustZone (early 2000s), inspired by the military concept of compartmentalized information. For ML systems, TEEs create a fundamental memory constraint: secure enclaves (128 MB in SGX, expandable in TrustZone) must contain both model weights and intermediate activations, forcing either model compression or partitioned inference where only sensitive layers execute inside the enclave. \index{TEE!ML constraints}

In the context of machine learning, TEEs are increasingly important for preserving the confidentiality of models, securing sensitive user data during inference, and ensuring that model outputs remain trustworthy. For example, a TEE can protect model parameters from being extracted by malicious software running on the same device, or ensure that computations involving biometric inputs, including facial data or fingerprint data, are performed securely. This capability is essential in applications where model integrity, user privacy, or regulatory compliance are non-negotiable.

One widely deployed example is [Apple's Secure Enclave](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web), which provides isolated execution and secure key storage for iOS devices. By separating cryptographic operations and biometric data from the main processor, the Secure Enclave ensures that user credentials and Face ID features remain protected, even in the event of a broader system compromise.

Trusted Execution Environments are important across a range of industries with high security requirements. In telecommunications, TEEs are used to safeguard encryption keys and secure important 5G control-plane operations. In finance, they allow secure mobile payments and protect PIN-based authentication workflows. In healthcare, TEEs help enforce patient data confidentiality during edge-based ML inference on wearable or diagnostic devices. In the automotive industry, they are deployed in advanced driver-assistance systems (ADAS) to ensure that safety-important perception and decision-making modules operate on verified software.

In machine learning systems, TEEs can provide several important protections. They secure the execution of model inference or training, shielding intermediate computations and final predictions from system-level observation. They protect the confidentiality of sensitive inputs, including biometric or clinical signals, used in personal identification or risk scoring tasks. TEEs also serve to prevent reverse engineering of deployed models by restricting access to weights and architecture internals. When models are updated, TEEs ensure the authenticity of new parameters and block unauthorized tampering. In distributed ML settings, TEEs can protect data exchanged between components by enabling encrypted and attested communication channels.

The core security properties of a TEE are achieved through four mechanisms: isolated execution, secure storage, integrity protection, and in-TEE data encryption. Code that runs inside the TEE is executed in a separate processor mode, inaccessible to the normal-world operating system. Sensitive assets such as cryptographic keys or authentication tokens are stored in memory that only the TEE can access. Code and data can be verified for integrity before execution using hardware-anchored hashes or signatures. Finally, data processed inside the TEE can be encrypted, ensuring that even intermediate results are inaccessible without appropriate keys, which are also managed internally by the TEE.

Several commercial platforms provide TEE functionality tailored for different deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. [Qualcomm's Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions) supports secure mobile transactions and user authentication. Apple's Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices.

[^fn-trustzone-adoption]: **ARM TrustZone**: Introduced in 2004 and now shipping in 95% of ARM processors (5+ billion devices), TrustZone partitions a processor into secure and normal "worlds" with hardware-enforced isolation. For ML on mobile and edge devices, TrustZone can protect model weights and biometric data during inference, yet only 20-30% of Android devices implement meaningful secure-world applications beyond key storage, leaving most edge ML deployments unprotected. \index{ARM TrustZone!adoption}

[^fn-intel-sgx-limits]: **Intel SGX Memory Constraints**: SGX enclaves are limited to approximately 128 MB of protected memory (EPC) on consumer processors, with EPC cache misses causing 100$\times$ performance penalties. A ResNet-50 requires approximately 98 MB for FP32 weights alone (25.6M parameters $\times$ 4 bytes), consuming 77% of EPC before activations. Inference latency jumps from 5 ms to 150 ms when the model exceeds EPC, making SGX practical only for small models under 10 MB or for protecting cryptographic keys. \index{Intel SGX!memory constraints}

@fig-enclave illustrates a practical secure enclave architecture integrated into a system-on-chip (SoC) design. The enclave includes a dedicated processor, an AES engine, a true random number generator (TRNG), a public key accelerator (PKA), and a secure I2C interface to nonvolatile storage. These components operate in isolation from the main application processor and memory subsystem. A memory protection engine enforces access control, while cryptographic operations such as NAND flash encryption are handled internally using enclave-managed keys. By physically separating secure execution and key management from the main system, this architecture limits the impact of system-level compromises and establishes hardware-enforced trust.

::: {#fig-enclave fig-env="figure" fig-pos="htb" fig-cap="**Secure Enclave Architecture**: Hardware-isolated enclaves enhance system security by encapsulating sensitive data and cryptographic operations within a dedicated processor and memory. This design minimizes the attack surface and protects important keys even if the main application processor is compromised, providing a trusted execution environment for security-important tasks. Source: Apple." fig-alt="System-on-chip block diagram. Secure enclave contains TRNG, AES engine, PKA, and secure processor connected via memory protection engine to DRAM. Application processor connects to NAND flash through AES engine. Key icon indicates encrypted data path."}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n},
scale=0.9, every node/.append style={transform shape}]
\tikzset{%
LineA/.style={line width=1.0pt,black!50,latex-latex},
Line/.style={BrownLine!60, {Triangle[width = 8pt, length = 6pt]}-{Triangle[width = 8pt, length = 6pt]}, line width = 4pt},
LineD/.style={line width=1.0pt,black!50,latex-,dashed},
Box/.style={inner xsep=2pt,inner ysep=6pt,
    node distance=0.25,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    align=flush center,
    text width=28mm,
    minimum width=28mm, minimum height=10.5mm
  },
Box2/.style={Box,draw=RedLine,fill=RedL},
Box3/.style={Box,  draw=VioletLine,fill=VioletL2, text width=42mm,
    minimum width=42mm, minimum height=8mm},
Box4/.style={Box,anchor=west,minimum width=42mm, minimum height=48mm},
Box4a/.style={Box,draw=BlueLine,fill=BlueL!50,anchor=west,minimum width=42mm, minimum height=48mm},
Box5/.style={Box,draw=BlueLine,fill=BlueL!50},
Box6/.style={Box,draw=OrangeLine,fill=OrangeL!50},
}

\tikzset{pics/key/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=FLAG1,scale=\scalefac, every node/.append style={transform shape}]
\draw[draw=\drawchannelcolor,fill=\channelcolor](-0.28,0.04)to[out=310,in=90](-0.16,-0.053)
to[out=270,in=90](-0.16,-0.15)to[out=270,in=110](-0.11,-0.22)
to[out=220,in=80](-0.14,-0.28)to(-0.05,-0.38)to(-0.12,-0.47)to(-0.08,-0.55)to(-0.14,-0.62)
to(-0.09,-0.7)to(-0.13,-0.76)to[out=290,in=70](-0.11,-0.9)to(0.03,-1.06)to[out=30,in=270](0.15,-0.9)
to(0.15,-0.01)to[out=10,in=350,distance=3](0.15,0.125)to[out=60,in=220](0.25,0.21)
to[out=30,in=330](0.25,0.97)to[out=150,in=30](-0.278,0.97)
to[out=210,in=150](-0.278,0.21)to[out=330,in=20](-0.24,0.08)to[out=200,in=80]cycle;
\fill[white](-0.018,0.77)circle(4pt);
\end{scope}
     }
  }
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  Linewidth=1.6pt,
  picname=C
}

\node[Box](B1){TRNG};
\node[Box,below=of B1](B2){Secure Enclave AES Engine};
\node[Box,below=of B2](B3){PKA};
\node[Box2,below=of B3](B4){I2C bus};
\node[Box3,below=1.35of B4](B5){Secure Nonvolatile Storage};
%
\node[Box4](SEP)at($(B1.north east)!0.5!(B4.south east)+(1.25,0)$){Secure Enclave\\ Processor};
\node[Box,anchor=west](MPE)at($(SEP.east)+(1.25,0)$){Memory Protection\\Engine};
%
\scoped[on background layer]
\node[draw=none,inner xsep=5mm,inner ysep=4mm,
yshift=0mm,fill=none,fit=(B1)(B4)(MPE),line width=0.75pt](BB1){};
%
\node[Box4a,anchor=south west](AP)at($(B1.north west)+(0,1.3)$){Application\\ Processor};
\node[Box5,anchor=south west](NAN)at($(AP.east)+(1,0.2)$){NAND flash controller};
\node[Box5,anchor=north west](AES)at($(AP.east)+(1,-0.45)$){AES engine};
\path[](MPE)|-coordinate(S)(AP.north east);
\node[Box5,anchor=north](MC)at(S){Memory controller};
%
\node[Box6,above=1 of MC](DRAM){DRAM};
\path[](DRAM)-|coordinate(SS)(NAN);
\node[Box6](NAND)at(SS){NAND flash\\ storage};
\draw[Line](SEP)--(MPE);
\draw[Line](MPE)--(MC);
\draw[Line](MC)--(DRAM);
\draw[Line](NAN)--(NAND);
\draw[Line](NAN)--(AES);
\draw[LineD](AES)--coordinate(KEY)(AES|-BB1.north);
\draw[Line](NAN)--(NAN-|MC);
\draw[Line](AES)--(AES-|MC);
\draw[Line](AP.315)--(AP.315-|MC);
\draw[Line](B4)--(B5);
%
\scoped[on background layer]
\node[draw=RedLine,inner xsep=5mm,inner ysep=5mm,
yshift=-0.5mm,fill=magenta!5,fit=(BB1)(AP)(MC),line width=0.75pt](BB2){};
\node[above=4pt of  BB2.south,inner sep=0pt,
anchor=south]{\textbf{System on chip}};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=5mm,inner ysep=4mm,
yshift=0mm,fill=BackColor!60,fit=(B1)(B4)(MPE),line width=0.75pt](BB3){};
\node[above=6pt of  BB3.south,inner sep=0pt,
anchor=south]{\textbf{Secure Enclave}};
%
\begin{scope}[local bounding box=KEY1,shift={($(KEY)+(0.4,-0.30)$)},
scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){key={scalefac=0.4,picname=1,drawchannelcolor=none,
channelcolor=red!79!black!90, Linewidth=1.0pt}};
 \end{scope}
\end{tikzpicture}
```
:::

This architecture underpins the secure deployment of machine learning applications on consumer devices. For example, Apple's Face ID system uses a secure enclave to perform facial recognition entirely within a hardware-isolated environment. The face embedding model is executed inside the enclave, and biometric templates are stored in secure nonvolatile memory accessible only via the enclave's I²C interface. During authentication, input data from the infrared camera is processed locally, and no facial features or predictions ever leave the secure region. Even if the application processor or operating system is compromised, the enclave prevents access to sensitive model inputs, parameters, and outputs—ensuring that biometric identity remains protected end to end.

Despite their strengths, Trusted Execution Environments come with notable trade-offs. Implementing a TEE increases both direct hardware costs and indirect costs associated with developing and maintaining secure software. Integrating TEEs into existing systems may require architectural redesigns, especially for legacy infrastructure. Developers must adhere to strict protocols for isolation, attestation, and secure update management, which can extend development cycles and complicate testing workflows. TEEs can also introduce performance overhead, particularly when cryptographic operations are involved, or when context switching between trusted and untrusted modes is frequent.

Energy efficiency is another consideration, particularly in battery-constrained devices. TEEs typically consume additional power due to secure memory accesses, cryptographic computation, and hardware protection logic. In resource-limited embedded systems, these costs may limit their use. In terms of scalability and flexibility, the secure boundaries enforced by TEEs may complicate distributed training or federated inference workloads, where secure coordination between enclaves is required.

Market demand also varies. In some consumer applications, perceived threat levels may be too low to justify the integration of TEEs. Systems with TEEs may be subject to formal security certifications, such as [Common Criteria](https://www.commoncriteriaportal.org/ccra/index.cfm) or evaluation under [ENISA](https://www.enisa.europa.eu/), which can introduce additional time and expense. For this reason, TEEs are typically adopted only when the expected threat model, including adversarial users, cloud tenants, and malicious insiders, justifies the investment.

Nonetheless, TEEs remain a critical hardware primitive in the machine learning security landscape. When paired with software- and system-level defenses, they provide a trusted foundation for executing ML models securely, privately, and verifiably, especially in scenarios where adversarial compromise of the host environment is a serious concern.

While TEEs provide runtime isolation once the system is running, they cannot protect against attacks that occur before the TEE is initialized. An attacker who compromises the boot process can modify firmware, inject malicious code, or disable security features before the TEE even begins executing. This temporal gap motivates Secure Boot, which establishes trust from the very first instruction the processor executes, creating a verified chain from power-on to secure enclave initialization.

#### Secure Boot {#sec-security-privacy-secure-boot-5242}

Secure Boot is a mechanism that ensures a device only boots software components that are cryptographically verified and explicitly authorized by the manufacturer. At startup, each stage of the boot process, comprising the bootloader, kernel, and base operating system, is checked against a known-good digital signature. If any signature fails verification, the boot sequence is halted, preventing unauthorized or malicious code from executing. This chain-of-trust model establishes system integrity from the very first instruction executed.

In ML systems, especially those deployed on embedded or edge hardware, Secure Boot plays an important role. A compromised boot process may result in malicious software loading before the ML runtime begins, enabling attackers to intercept model weights, tamper with training data, or reroute inference results. Such breaches can lead to incorrect or manipulated predictions, unauthorized data access, or device repurposing for botnets or crypto-mining.

For machine learning systems, Secure Boot offers several guarantees. First, it protects model-related data, such as training data, inference inputs, and outputs, during the boot sequence, preventing pre-runtime tampering. Second, it ensures that only authenticated model binaries and supporting software are loaded, which helps guard against deployment-time model substitution. Third, Secure Boot allows secure model updates by verifying that firmware or model changes are signed and have not been altered in transit.

Secure Boot frequently works in tandem with hardware-based Trusted Execution Environments (TEEs) to create a fully trusted execution stack. @fig-secure-boot traces the layered verification sequence: firmware, operating system components, and TEE integrity are verified before permitting execution of cryptographic operations or ML workloads [@rashmi2018secure]. In embedded systems, this architecture provides resilience even under severe adversarial conditions or physical device compromise.

::: {#fig-secure-boot fig-env="figure" fig-pos="htb" fig-cap="**Secure Boot Sequence**: Embedded systems employ a layered boot process to verify firmware and software integrity, establishing a root of trust before executing machine learning workloads and protecting against pre-runtime attacks. This architecture ensures only authenticated code runs, safeguarding model data and preventing unauthorized model substitution or modification during deployment." fig-alt="Two-column flowchart showing secure boot sequence. Left column verifies kernel, right column verifies filesystem. Both include CRC checks and integrity measurements. Failed verification halts boot."}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
Line/.style={black!50,-{Triangle[width = 6pt, length = 6pt]}, line width = 1.15pt,text=black},
Box/.style={inner xsep=2pt,inner ysep=6pt,
    node distance=0.5,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL!40,
    align=flush center,
    text width=58mm,
    minimum width=58mm, minimum height=10mm
  },
Box2/.style={Box,draw=RedLine,fill=RedL,rounded corners=12pt},
Box3/.style={draw=VioletLine,fill=VioletL2, trapezium, trapezium left angle=70,
diamond, minimum width=45mm, minimum height=15mm, text centered,inner sep= -2ex},
Box4/.style={Box,anchor=west,minimum width=42mm, minimum height=48mm},
Box5/.style={Box,draw=BlueLine,fill=BlueL!50},
Box6/.style={Box,draw=OrangeLine,fill=OrangeL!50},
}

\node[Box2](B1){Power up};
\node[Box,below=of B1](B2){Hardware init};
\node[Box,below=of B2](B3){Get the information of MTM device, complete the boot and self-diagnosis process of MTM, store the diagnostic information of hardware platform and MTM device, set verification register};
\node[Box,below=of B3](B4){Copy kernel image and root FS image from FLASH to RAM};
\node[Box,below=of B4](B5){Checking the CRC checksum of kernel image};
\node[Box,below=of B5](B6){Perform integrity measurement, verification, and storage of Linux kernel image};
\node[Box3,below=0.6of B6](B7){Success};
\node[Box,below=0.7of B7](B8){Boot abort};
%
\node[Box,right=3 of B3](B9){Checking the CRC checksum of root fs image};
\node[Box,below=1of B9](B10){Perform integrity measurement, verification, and storage of root fs image};
\node[Box3,below=1of B10](B11){Success};
\node[Box,below=1of B11](B12){Set boot parameters for kernel};
\node[Box,below=1of B12](B13){Booting the kernel};
%
\foreach \x in{1,2,3,4,5,6}{
\pgfmathtruncatemacro{\newX}{\x + 1} %
\draw[Line](B\x)--(B\newX);
}
\foreach \x in{9,10,12}{
\pgfmathtruncatemacro{\newX}{\x + 1} %
\draw[Line](B\x)--(B\newX);
}
\draw[Line](B7)--node[right,pos=0.3]{No}(B8);
\draw[Line](B7.east)--node[above,pos=0.25]{Yes}++(1.8,0)--++(0,10.5)-|(B9);
\draw[Line](B11.west)--node[above,pos=0.25]{No}++(-1.8,0)|-(B8);
\draw[Line](B11)--node[right,pos=0.3]{Yes}(B12);
\end{tikzpicture}
```
:::

A well-known real-world implementation of Secure Boot appears in Apple's Face ID system, which uses advanced machine learning for facial recognition. For Face ID to operate securely, the entire device stack, from the initial power-on to the execution of the model, must be verifiably trusted.

Upon device startup, Secure Boot initiates within Apple's [Secure Enclave](https://support.apple.com/en-us/102381), a dedicated security coprocessor that handles biometric data. The firmware loaded onto the Secure Enclave is digitally signed by Apple, and any unauthorized modification causes the boot process to fail. Once verified, the Secure Enclave performs continuous checks in coordination with the central processor to maintain a trusted boot chain. Each system component, ranging from the iOS kernel to the application-level code, is verified using cryptographic signatures.

After completing the secure boot sequence, the Secure Enclave activates the ML-based Face ID system. The facial recognition model projects over 30,000 infrared points to map a user's face, generating a depth image and computing a mathematical representation that is compared against a securely stored profile. These facial data artifacts are never written to disk, transmitted off-device, or shared externally. All processing occurs within the enclave to protect against eavesdropping or exfiltration, even in the presence of a compromised kernel.

To support continued integrity, Secure Boot also governs software updates. Only firmware or model updates signed by Apple are accepted, ensuring that even over-the-air patches do not introduce risk. This process maintains a robust chain of trust over time, enabling the secure evolution of the ML system while preserving user privacy and device security.

While Secure Boot provides strong protection, its adoption presents technical and operational challenges. Managing the cryptographic keys used to sign and verify system components is complex, especially at scale. Enterprises must securely provision, rotate, and revoke keys, ensuring that no trusted root is compromised. Any such breach would undermine the entire security chain.

Performance is also a consideration. Verifying signatures during the boot process introduces latency, typically on the order of tens to hundreds of milliseconds per component. Although acceptable in many applications, these delays may be problematic for real-time or power-constrained systems. Developers must also ensure that all components, including bootloaders, firmware, kernels, drivers, and even ML models, are correctly signed. Integrating third-party software into a Secure Boot pipeline introduces additional complexity.

Some systems limit user control in favor of vendor-locked security models, restricting upgradability or customization. In response, open-source bootloaders like [u-boot](https://source.denx.de/u-boot/u-boot) and [coreboot](https://www.coreboot.org/) have emerged, offering Secure Boot features while supporting extensibility and transparency. To further scale trusted device deployments, emerging industry standards such as the [Device Identifier Composition Engine (DICE)](https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/) and [IEEE 802.1AR IDevID](https://1.ieee802.org/security/802-1ar/) provide mechanisms for secure device identity, key provisioning, and cross-vendor trust assurance.

Secure Boot, when implemented carefully and complemented by trusted hardware and secure software update processes, forms the backbone of system integrity for embedded and distributed ML. It provides the assurance that the machine learning model running in production is not only the correct version, but is also executing in a known-good environment, anchored to hardware-level trust.

#### Hardware Security Modules {#sec-security-privacy-hardware-security-modules-4377}

While TEEs and secure boot provide runtime isolation and integrity verification, Hardware Security Modules (HSMs) specialize in the cryptographic operations that underpin these protections. An HSM[^fn-hsm-performance] is a tamper-resistant physical device designed to perform cryptographic operations and securely manage digital keys. HSMs are widely used across security-important industries such as finance, defense, and cloud infrastructure, and they are increasingly relevant for securing the machine learning pipeline—particularly in deployments where key confidentiality, model integrity, and regulatory compliance are important.

[^fn-hsm-performance]: **HSM (Hardware Security Module)**: Enterprise HSMs perform 10,000+ RSA-2048 operations per second at $20,000-$100,000+ per unit, compared to 100,000+ operations per second on GPUs at $1,000+. The 10$\times$ throughput disadvantage is the price of tamper resistance: HSMs physically destroy keys when tampered with, a guarantee no software-only solution can provide and which regulatory frameworks (FIPS 140-2, PCI DSS) mandate for handling ML model signing keys in production. \index{HSM!performance trade-off}

HSMs provide an isolated, hardened environment for performing sensitive operations such as key generation, digital signing, encryption, and decryption. Unlike general-purpose processors, they are engineered to withstand physical tampering and side-channel attacks, and they typically include protected storage, cryptographic accelerators, and internal audit logging. HSMs may be implemented as standalone appliances, plug-in modules, or integrated chips embedded within broader systems.

In machine learning systems, HSMs enhance security across several dimensions. They are commonly used to protect encryption keys associated with sensitive data that may be processed during training or inference. These keys might encrypt data at rest in model checkpoints or allow secure transmission of inference requests across networked environments. By ensuring that the keys are generated, stored, and used exclusively within the HSM, the system minimizes the risk of key leakage, unauthorized reuse, or tampering.

HSMs also play a role in maintaining the integrity of machine learning models. In many production pipelines, models must be signed before deployment to ensure that only verified versions are accepted into runtime environments. The signing keys used to authenticate models can be stored and managed within the HSM, providing cryptographic assurance that the deployed artifact is authentic and untampered. Similarly, secure firmware updates and configuration changes, regardless of whether they pertain to models, hyperparameters, or supporting infrastructure, can be validated using signatures produced by the HSM.

In addition to protecting inference workloads, HSMs can be used to secure model training. During training, data may originate from distributed and potentially untrusted sources. HSM-backed protocols can help ensure that training pipelines perform encryption, integrity checks, and access control enforcement securely and in compliance with organizational or legal requirements. In regulated industries such as healthcare and finance, such protections are often mandatory. For instance, HIPAA requires covered entities to implement technical safeguards including "integrity controls" and "encryption and decryption," while GDPR mandates pseudonymization and encryption as examples of appropriate technical measures.

Despite these benefits, incorporating HSMs into embedded or resource-constrained ML systems introduces several trade-offs. First, HSMs are specialized hardware components and often come at a premium. Their cost may be justified in data center settings or safety-important applications but can be prohibitive for low-margin embedded products or wearables. Physical space is also a concern. Embedded systems often operate under strict size, weight, and form factor constraints, and integrating an HSM may require redesigning circuit layouts or sacrificing other functionality.

From a performance standpoint, HSMs introduce latency, particularly for operations like key exchange, signature verification, or on-the-fly decryption. In real-time inference systems, including autonomous vehicles, industrial robotics, and live translation devices, these delays can affect responsiveness. While HSMs are typically optimized for cryptographic throughput, they are not general-purpose processors, and offloading secure operations must be carefully coordinated.

Power consumption is another concern. The continuous secure handling of keys, signing of transactions, and cryptographic validations can consume more power than basic embedded components, impacting battery life in mobile or remote deployments.

Integration complexity also grows when HSMs are introduced into existing ML pipelines. Interfacing between the HSM and the host processor requires dedicated APIs and often specialized software development. Firmware and model updates must be routed through secure, signed channels, and update orchestration must account for device-specific key provisioning. These requirements increase the operational burden, especially in large deployments.

Scalability presents its own set of challenges. Managing a distributed fleet of HSM-equipped devices requires secure provisioning of individual keys, secure identity binding, and coordinated trust management. In large ML deployments, including fleets of smart sensors or edge inference nodes, ensuring uniform security posture across all devices is nontrivial.

Finally, the use of HSMs often requires organizations to engage in certification and compliance processes[^fn-hsm-certification], particularly when handling regulated data. Meeting standards such as FIPS 140-2[^fn-fips-140] or Common Criteria adds time and cost to development.

[^fn-hsm-certification]: **HSM Certification**: FIPS 140-2 or Common Criteria certification takes 12-24 months and costs $500,000-$2 million per device family. Banking, government, and healthcare sectors mandate Level 3+ certification for cryptographic operations. For ML systems in regulated industries, this certification timeline creates a deployment bottleneck: the HSM protecting model signing keys may take longer to certify than the model takes to train. \index{HSM!certification}

[^fn-fips-140]: **FIPS 140-2** (Federal Information Processing Standard, 2001): Defines four security levels for cryptographic modules. Level 4 requires survival of physical attacks at -40 to +85 degrees Celsius with tamper detection that zeroizes keys within seconds. For ML systems handling classified or medical data, FIPS 140-2 Level 3+ compliance is non-negotiable, restricting HSM access to authorized personnel and slowing the rapid iteration cycles that ML development typically demands. \index{FIPS 140-2!security levels}

Despite these operational complexities, HSMs remain a valuable option for machine learning systems that require high assurance of cryptographic integrity and access control. When paired with TEEs, secure boot, and software-based defenses, HSMs contribute to a multilayered security model that spans hardware, system software, and ML runtime.

HSMs provide robust cryptographic processing but require dedicated hardware modules and significant infrastructure investment. For resource-constrained embedded systems where adding external HSM hardware is impractical due to cost, size, or power constraints, an alternative approach derives cryptographic secrets directly from the chip's inherent physical properties. This capability is provided by Physical Unclonable Functions, which we examine next.

#### Physical Unclonable Functions {#sec-security-privacy-physical-unclonable-functions-6533}

Physical Unclonable Functions (PUFs)[^fn-puf-adoption] provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication by exploiting physical randomness in semiconductor fabrication [@gassend2002silicon]. Unlike traditional keys stored in memory, a PUF generates secret values based on microscopic variations in a chip's physical properties—variations that are inherent to manufacturing processes and difficult to clone or predict, even by the manufacturer.

[^fn-puf-adoption]: **PUF (Physical Unclonable Function)**: Named for the physical impossibility of cloning the microscopic manufacturing variations they exploit, PUFs generate device-unique cryptographic keys without storing secrets in memory. For edge ML devices, PUFs solve a critical deployment problem: each device can encrypt its local model with a hardware-derived key that exists nowhere else, ensuring that extracting a model from one device does not compromise the fleet. \index{PUF!device authentication}

These variations arise from uncontrollable physical factors such as doping concentration, line edge roughness, and dielectric thickness. As a result, even chips fabricated with the same design masks exhibit small but measurable differences in timing, power consumption, or voltage behavior. PUF circuits amplify these variations to produce a device-unique digital output. When a specific input challenge is applied to a PUF, it generates a corresponding response based on the chip's physical fingerprint. Because these characteristics are effectively impossible to replicate, the same challenge will yield different responses across devices.

This challenge-response mechanism allows PUFs to serve several cryptographic purposes. They can be used to derive device-specific keys that never need to be stored externally, reducing the attack surface for key exfiltration. The same mechanism also supports secure authentication and attestation, where devices must prove their identity to trusted servers or hardware gateways. These properties make PUFs a natural fit for machine learning systems deployed in embedded and distributed environments.

In ML applications, PUFs offer unique advantages for securing resource-constrained systems. For example, consider a smart camera drone that uses onboard computer vision to track objects. A PUF embedded in the drone's processor can generate a private key to encrypt the model during boot. Even if the model were extracted, it would be unusable on another device lacking the same PUF response. That same PUF-derived key could also be used to watermark the model parameters, creating a cryptographically verifiable link between a deployed model and its origin hardware. If the model were leaked or pirated, the embedded watermark could help prove the source of the compromise.

PUFs also support authentication in distributed ML pipelines. If the drone offloads computation to a cloud server, the PUF can help verify that the drone has not been cloned or tampered with. The cloud backend can issue a challenge, verify the correct response from the device, and permit access only if the PUF proves device authenticity. These protections enhance trust not only in the model and data, but in the execution environment itself.

@fig-pfu demonstrates how PUF operation depends on inherent physical variation. At a high level, a PUF accepts a challenge input and produces a unique response determined by the physical microstructure of the chip [@gao2020physical]. Variants include optical PUFs, in which the challenge consists of a light pattern and the response is a speckle image, and electronic PUFs such as Arbiter PUFs (APUFs), where timing differences between circuit paths produce a binary output. Another common implementation is the SRAM PUF, which exploits the power-up state of uninitialized SRAM cells: due to threshold voltage mismatch, each cell tends to settle into a preferred value when power is first applied. These response patterns form a stable, reproducible hardware fingerprint.

![**Physical Unclonable Functions**: Pufs generate unique hardware fingerprints from inherent manufacturing variations, enabling device authentication and secure key generation without storing secrets. Optical and electronic PUF implementations use physical phenomena—such as light speckle patterns or timing differences—to produce challenge-response pairs that are difficult to predict or replicate.](images/png/puf_basics.png){#fig-pfu fig-alt="Diagram showing optical and electronic PUF types. Left: optical PUF with laser input producing speckle pattern response. Right: electronic arbiter PUF circuit with challenge bits producing binary response based on timing differences."}

Despite their promise, PUFs present several challenges in system design. Their outputs can be sensitive to environmental variation, such as changes in temperature or voltage, which can introduce instability or bit errors in the response. To ensure reliability, PUF systems must often incorporate error correction codes or helper data schemes. Managing large sets of challenge-response pairs also raises questions about storage, consistency, and revocation. Additionally, the unique statistical structure of PUF outputs may make them vulnerable to machine learning-based modeling attacks if not carefully shielded from external observation.

From a manufacturing perspective, incorporating PUF technology can increase device cost or require additional layout complexity. While PUFs eliminate the need for external key storage, thereby reducing long-term security risk and provisioning cost, they may require calibration and testing during fabrication to ensure consistent performance across environmental conditions and device aging.

Nevertheless, Physical Unclonable Functions remain a compelling building block for securing embedded machine learning systems. By embedding hardware identity directly into the chip, PUFs support lightweight cryptographic operations, reduce key management burden, and help establish root-of-trust anchors in distributed or resource-constrained environments. When integrated thoughtfully, they complement other hardware-assisted security mechanisms such as Secure Boot, TEEs, and HSMs to provide defense-in-depth across the ML system lifecycle.

#### Mechanisms Comparison {#sec-security-privacy-mechanisms-comparison-2dcb}

Hardware-assisted security mechanisms play a foundational role in establishing trust within modern machine learning systems. While software-based defenses offer flexibility, they ultimately rely on the security of the hardware platform. As machine learning workloads increasingly operate on edge devices, embedded platforms, and untrusted infrastructure, hardware-backed protections become important for maintaining system integrity, confidentiality, and trust.

Trusted Execution Environments (TEEs) provide runtime isolation for model inference and sensitive data handling. Secure Boot enforces integrity from power-on, ensuring that only verified software is executed. Hardware Security Modules (HSMs) offer tamper-resistant storage and cryptographic processing for secure key management, model signing, and firmware validation. Physical Unclonable Functions (PUFs) bind secrets and authentication to the physical characteristics of a specific device, enabling lightweight and unclonable identities.

These mechanisms address different layers of the system stack, ranging from initialization and attestation to runtime protection and identity binding, and complement one another when deployed together. @tbl-hw-security-comparison compares their roles, use cases, and trade-offs for machine learning system design.

| **Mechanism**                | **Primary Function**                                | **Common Use in ML**                                                        | **Trade-offs**                                                              |
|:-----------------------------|:----------------------------------------------------|:----------------------------------------------------------------------------|:----------------------------------------------------------------------------|
| **Trusted Execution**        | Isolated runtime environment for secure computation | Secure inference and on-device privacy for sensitive inputs and outputs     | Added complexity, memory limits, perf. cost                                 |
| **Environment (TEE)**        |                                                     |                                                                             | Requires trusted code development                                           |
| **Secure Boot**              | Verified boot sequence and firmware validation      | Ensures only signed ML models and firmware execute on embedded devices      | Key management complexity, vendor lock-in Performance impact during startup |
| **Hardware Security Module** | Secure key generation and                           | Signing ML models, securing training                                        | High cost, integration overhead, limited I/O                                |
| **(HSM)**                    | storage, crypto-processing                          | pipelines, verifying firmware                                               | Requires infrastructure-level provisioning                                  |
| **Physical Unclonable**      | Hardware-bound identity and key derivation          | Model binding, device authentication, protecting IP in embedded deployments | Environmental sensitivity, modeling attacks                                 |
| **Function (PUF)**           |                                                     |                                                                             | Needs error correction and calibration                                      |

: **Hardware Security Mechanisms**: Machine learning systems use diverse hardware defenses—trusted execution environments, secure boot, hardware security modules, and physical unclonable functions—to establish trust and protect sensitive data across the system stack. The table details how each mechanism addresses specific security challenges—from runtime isolation and integrity verification to key management and device identity—and emphasizes the associated trade-offs in performance and complexity. {#tbl-hw-security-comparison}

Together, these hardware primitives form the foundation of a defense-in-depth strategy for securing ML systems in adversarial environments. Their integration is especially important in domains that demand provable trust, such as autonomous vehicles, healthcare devices, federated learning systems, and important infrastructure.

Implementing secure multi-party computation and gradient compression establishes a robust architecture for collaborative training without exposing raw data. The appropriate defense for any given deployment, however, depends on three interacting factors: the threat model (who attacks and with what capabilities), the deployment context (what computational and latency budgets exist), and the regulatory environment (what legal mandates constrain design). A healthcare system training federated diagnostic models faces fundamentally different threats than a public-facing LLM chatbot, and each demands a distinct combination of the mechanisms surveyed in this chapter. @tbl-defense-selection-framework maps seven common deployment contexts to their primary threats, recommended defense combinations, and the quantified trade-offs that each combination imposes.

| **Deployment Context**      | **Primary Threats**                               | **Recommended Defenses**                                                                                                                                                    | **Key Trade-offs**                                                                                                                 |
|:----------------------------|:--------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------|
| **Healthcare ML**           | Data leakage (HIPAA violation),                   | • Differential Privacy ($\epsilon \leq 4$) for training                                                                                                                     | 2--5% accuracy loss acceptable for compliance;                                                                                     |
| **(Federated diagnostic**   | membership inference,                             | • Federated Learning across hospitals                                                                                                                                       | 50--100 ms inference latency from TEE overhead                                                                                     |
| **models)**                 | unauthorized access                               | • TEE for inference on sensitive data<br>• Audit logging and access control (RBAC)                                                                                          |                                                                                                                                    |
| **Financial ML**            | Model theft (IP loss),                            | • Model encryption (AES-256) at rest                                                                                                                                        | HSM adds \$10--50K capital cost; rate limiting                                                                                     |
| **(Fraud detection API)**   | adversarial evasion, data poisoning               | • HSM for cryptographic key management<br>• Adversarial training (PGD-based)<br>• Input validation + rate limiting (100 req/min)<br>• Output confidence monitoring          | may impact legitimate high-frequency users                                                                                         |
| **Edge ML**                 | Physical access,                                  | • Secure Boot (verified firmware)                                                                                                                                           | TEE memory limits constrain model size                                                                                             |
| **(Mobile/IoT devices)**    | side-channel attacks, model extraction            | • ARM TrustZone or similar TEE<br>• Model quantization + obfuscation<br>• Encrypted model storage<br>• Anti-tampering hardware (PUF)                                        | $<$50 MB; quantization required for large models; 15--30% power overhead from encryption                                           |
| **Cloud ML Training**       | Data poisoning,                                   | • Secure data pipelines (provenance tracking)                                                                                                                               | Training time increases 30--120% with DP;                                                                                          |
| **(Multi-tenant platform)** | backdoor injection, gradient leakage              | • Differential Privacy (DP-SGD, $\epsilon \approx 1$--$10$)<br>• Gradient verification and anomaly detection<br>• Secure aggregation (if federated)<br>• Model watermarking for IP protection | gradient verification adds 10--15% compute overhead; federated aggregation requires secure communication protocols                 |
| **Public-Facing LLM**       | Prompt injection,                                 | • Input sanitization (prompt filtering)                                                                                                                                     | Aggressive filtering may block 5--10% of                                                                                           |
| **(Chatbot/API)**           | data extraction (training leakage), abuse/overuse | • Output monitoring (PII detection)<br>• Rate limiting (per-user quotas)<br>• Response watermarking<br>• Confidence thresholding (abstention)                               | legitimate requests; response time increases 50--100 ms for content filtering; watermarking may be detectable by sophisticated users |
| **Multi-Party ML**          | Data sharing restrictions,                        | • Federated Learning (no raw data sharing)                                                                                                                                  | Communication overhead: 10--100$\times$ more rounds                                                                                |
| **(Cross-organizational**   | honest-but-curious participants,                  | • SMPC for secure aggregation                                                                                                                                               | than centralized training; SMPC adds $1{,}000\times$+                                                                              |
| **training)**               | privacy compliance (GDPR)                         | • Differential Privacy ($\epsilon \leq 1$)<br>• Homomorphic Encryption (for sensitive ops)                                                                                  | compute cost; accuracy may degrade 5--15%; requires legal agreements for liability                                                 |
| **Critical Infrastructure** | Supply chain compromise,                          | • Hardware attestation (TPM/PUF)                                                                                                                                            | Development cost: 6--18 months additional                                                                                          |
| **(Autonomous vehicles,**   | real-time adversarial attacks,                    | • Secure Boot + runtime integrity checks                                                                                                                                    | engineering; 20--40% higher hardware costs;                                                                                        |
| **power grids)**            | safety-critical failures                          | • Redundant model validation<br>• Fault injection detection<br>• Fail-safe fallback mechanisms                                                                              | latency constraints limit cryptographic defenses; requires certified hardware                                                      |

: **Defense Selection Framework**: Maps deployment contexts to threat-specific defensive strategies with quantified trade-offs. The framework provides starting points for security architecture design, highlighting primary threats, recommended defense combinations, and key implementation trade-offs across seven common ML system deployment scenarios. {#tbl-defense-selection-framework}

A recurring theme across every row of @tbl-defense-selection-framework is the cost of privacy: differential privacy appears in four of seven deployment contexts, and in each case it imposes a measurable accuracy penalty (2--15%) that must be weighed against regulatory mandates and risk tolerance. Even if the training process is perfectly secure and the collaborative architecture leak-free, the final published model may still inadvertently memorize and regurgitate the sensitive information it was trained on. To guarantee that a model's outputs cannot be reverse-engineered to reveal individual records, we must enforce a rigorous mathematical standard known as differential privacy.

## Differential Privacy {#sec-security-privacy-differential-privacy-8c2b}

Suppose an adversary queries a medical diagnosis model using the exact attributes of a known patient. If the model's prediction changes significantly depending on whether that specific patient was included in the training dataset, the patient's privacy has been mathematically compromised. Differential privacy solves this by injecting carefully calibrated noise during training, providing a formal guarantee that the model's behavior is virtually indistinguishable whether any single individual opts in or out of the dataset.

To understand the need for differential privacy, consider this challenge: how can we quantify privacy loss when learning from data? Traditional privacy approaches focus on removing identifying information (names, addresses, social security numbers) or applying statistical disclosure controls. However, these methods fail against sophisticated adversaries who can re-identify individuals through auxiliary data, statistical correlation attacks, or inference from model outputs.

Differential privacy takes a different approach by focusing on algorithmic behavior rather than data content. The key insight is that privacy protection should be measurable and should limit what can be learned about any individual, regardless of what external information an adversary possesses.

To build intuition for this concept, imagine you want to find the average salary of a group of people, but no one wants to reveal their actual salary. With differential privacy, you could ask everyone to write their salary on a piece of paper, but before they hand it in, they add or subtract a random number from a known distribution. When you average all the papers, the random noise tends to cancel out, giving you an accurate estimate of the true average. However, if you pull out any single piece of paper, you cannot know the person's real salary because you do not know what random number they added. This is the core idea: learn aggregate patterns while making it impossible to be sure about any single individual.

Differential privacy formalizes this intuition through a comparison of algorithm behavior on similar datasets. Consider two adjacent datasets that differ only in the presence or absence of a single individual's record. Differential privacy ensures that the probability distributions of algorithm outputs remain statistically similar regardless of whether that individual's data is included. This protection is achieved through carefully calibrated noise that masks individual contributions while preserving the aggregate statistical patterns necessary for machine learning.

::: {.callout-note title="Differential Privacy Diagram" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  \definecolor{DColor}{RGB}{200,220,255}
  \definecolor{DpColor}{RGB}{255,220,200}

  % Curves (use -(\x)^2 so exponent is unambiguously negative; avoids PGF "Dimension too large")
  \draw[blue, thick, domain=-4:4, samples=100] plot (\x, {3*exp(-(\x)^2/2)}); % Distribution for D
  \draw[red, thick, domain=-4:4, samples=100] plot (\x, {2.5*exp(-((\x-0.5)^2)/2)}); % Distribution for D'

  % Shading overlap
  \begin{scope}
    \clip plot[domain=-4:4] (\x, {3*exp(-(\x)^2/2)}) -- (4,0) -- (-4,0) -- cycle;
    \fill[gray!20, opacity=0.5] plot[domain=-4:4] (\x, {2.5*exp(-((\x-0.5)^2)/2)}) -- (4,0) -- (-4,0) -- cycle;
  \end{scope}

  % Labels
  \node[blue, anchor=west] at (1.5, 2.5) {$\Pr[\mathcal{A}(D)]$};
  \node[red, anchor=west] at (2.0, 1.5) {$\Pr[\mathcal{A}(D')]$};

  % Epsilon bound
  \draw[<->, thick] (-0.5, 3.0) -- (-0.5, 2.45) node[midway, left] {$e^\epsilon$};

  \node[anchor=north, font=\scriptsize, text=gray] at (0,-0.5) {Distributions must be within $e^\epsilon$ factor everywhere.};

\end{tikzpicture}
```
**Differential Privacy Indistinguishability**. Differential privacy ensures that the probability distribution of an algorithm's output on dataset $D$ is nearly identical to its output on an adjacent dataset $D'$. This statistical indistinguishability (controlled by the privacy budget $\epsilon$) prevents an observer from inferring whether any single individuals data was included in the training set.
:::

To make this intuition mathematically precise, differential privacy introduces a quantitative measure of privacy loss. The mathematical framework uses probability ratios to bound how much an algorithm's behavior can change when a single individual's data is added or removed. This approach proves privacy guarantees rather than simply assuming them.

A randomized algorithm $\mathcal{A}$ is said to be $\epsilon$-differentially private if, for all adjacent datasets $D$ and $D'$ differing in one record, and for all outputs $S \subseteq \text{Range}(\mathcal{A})$, the following holds:
$$
\Pr[\mathcal{A}(D) \in S] \leq e^{\epsilon} \Pr[\mathcal{A}(D') \in S]
$$

The parameter $\epsilon$ quantifies the privacy budget, representing the maximum allowable privacy loss. Smaller values of $\epsilon$ provide stronger privacy guarantees through increased noise injection, but may reduce model utility. Typical values include $\epsilon = 0.1$ for strong privacy protection, $\epsilon = 1.0$ for moderate protection, and $\epsilon = 10$ for weaker but utility-preserving guarantees. The multiplicative factor $e^{\epsilon}$ bounds the likelihood ratio between algorithm outputs on adjacent datasets, constraining how much an individual's participation can influence any particular result.

@fig-privacy-utility-frontier quantifies this cost using published empirical results across two benchmark datasets. The pragmatic sweet spot lies between $\epsilon \approx 1$ and $\epsilon \approx 10$, where meaningful privacy guarantees coexist with acceptable accuracy loss. MNIST retains 95% accuracy at $\epsilon \approx 1$, while CIFAR-10 struggles to reach 82% even at $\epsilon = 8$, reflecting the higher information content required for more complex classification tasks.

::: {#fig-privacy-utility-frontier fig-env="figure" fig-pos="htb" fig-cap="**The Privacy-Utility Frontier**. Published accuracy at various privacy budgets from Abadi et al. (2016), Bu et al. (2020), and De et al. (2022). MNIST retains 95% accuracy at epsilon approximately 1, while CIFAR-10 struggles to reach 82% even at epsilon = 8. The knee around epsilon approximately 1 to 3 marks the transition from practical privacy to catastrophic utility loss." fig-alt="Line plot of model accuracy vs privacy budget epsilon on log scale. MNIST curve stays above 95% for epsilon above 1. CIFAR-10 curve is much lower. Shaded knee region between epsilon 1 and 3."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ PRIVACY-UTILITY FRONTIER (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-privacy-utility-frontier — DP accuracy vs epsilon
# │
# │ Goal: Plot MNIST/CIFAR-10 accuracy vs epsilon from Abadi, Bu, De papers;
# │       show knee at epsilon 1–3; practical vs catastrophic utility loss.
# │ Show: Line plot; log-scale x; shaded knee; model labels.
# │ How: Published data points; viz.setup_plot().
# │
# │ Imports: numpy (np), matplotlib.pyplot (plt), mlsys.viz (viz)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
import matplotlib.pyplot as plt
from mlsys import viz

fig, ax, COLORS, plt = viz.setup_plot(figsize=(8, 5))

# --- MNIST data ---
# Bu et al. 2020 (Harvard Data Science Review, PMC7695347)
bu_eps   = np.array([0.83, 2.32, 5.07, 9.98, 14.98, 31.12])
bu_acc   = np.array([95.0, 96.6, 97.0, 97.6, 97.8,  98.0])

# Abadi et al. 2016 (CCS)
abadi_mnist_eps = np.array([0.5, 2.0, 8.0])
abadi_mnist_acc = np.array([90.0, 95.0, 97.0])

# MNIST non-private baseline
mnist_baseline = 98.3

# --- CIFAR-10 data ---
# Abadi et al. 2016: eps=8, acc=73%
# De et al. 2022 (arXiv:2204.13650): eps=8, acc=81.4%
cifar_eps = np.array([8.0, 8.0])
cifar_acc = np.array([73.0, 81.4])
cifar_baseline = 93.0

# Shade the "knee" region (eps 1-3)
ax.axvspan(1.0, 3.0, alpha=0.12, color=COLORS['YellowFill'], zorder=0,
           label='_nolegend_')

# MNIST: Bu et al. line + Abadi triangles
ax.plot(bu_eps, bu_acc, color=COLORS['BlueLine'], marker='o', markersize=6,
        linewidth=2, label='MNIST (Bu et al. 2020)', zorder=3)
ax.plot(abadi_mnist_eps, abadi_mnist_acc, color=COLORS['BlueLine'],
        marker='^', markersize=7, linestyle='none',
        label='MNIST (Abadi et al. 2016)', zorder=3)

# CIFAR-10 points
ax.plot(cifar_eps[0], cifar_acc[0], color=COLORS['OrangeLine'], marker='s',
        markersize=7, label='CIFAR-10 (Abadi et al. 2016)', zorder=3)
ax.plot(cifar_eps[1], cifar_acc[1], color=COLORS['OrangeLine'], marker='D',
        markersize=7, label='CIFAR-10 (De et al. 2022)', zorder=3)

# Non-private baselines
ax.axhline(y=mnist_baseline, color=COLORS['BlueLine'], linestyle='--',
           alpha=0.5, linewidth=1.2, label=f'MNIST baseline ({mnist_baseline}%)')
ax.axhline(y=cifar_baseline, color=COLORS['OrangeLine'], linestyle='--',
           alpha=0.5, linewidth=1.2, label=f'CIFAR-10 baseline ({cifar_baseline}%)')

# Annotations
ax.annotate('Strong privacy\n($\\epsilon < 1$)', xy=(0.3, 88),
            fontsize=8, ha='center', color='gray')
ax.annotate('Weak privacy\n($\\epsilon > 10$)', xy=(40, 88),
            fontsize=8, ha='center', color='gray')
ax.annotate('Knee region', xy=(1.7, 92), fontsize=8, ha='center',
            style='italic', color=COLORS['BrownLine'])

ax.set_xscale('log')
ax.set_xlabel('Privacy budget $\\epsilon$ (log scale)')
ax.set_ylabel('Test accuracy (%)')
ax.set_xlim(0.2, 100)
ax.set_ylim(65, 100)
ax.legend(loc='lower right', fontsize=8, ncol=1)
plt.show()
```
:::

This bound ensures that the algorithm's behavior remains statistically indistinguishable regardless of whether any individual's data is present, thereby limiting the information that can be inferred about that individual. In practice, DP is implemented by adding calibrated noise to model updates or query responses, using mechanisms such as the Laplace or Gaussian mechanism. Training techniques like differentially private stochastic gradient descent[^fn-dp-sgd-adoption] integrate calibrated noise into training computations, ensuring that individual data points cannot be distinguished from the model's learned behavior.

[^fn-dp-sgd-adoption]: **DP-SGD (Differentially Private SGD)**: Introduced by Abadi et al. in 2016, DP-SGD clips per-sample gradients and adds calibrated Gaussian noise during training. Apple deployed differential privacy at scale that same year, protecting 1+ billion iOS users' data with $\varepsilon$ = 4-16. The systems cost is significant: per-sample gradient clipping prevents the batch-level parallelism that makes GPU training efficient, reducing training throughput by 2-10$\times$ compared to standard SGD. \index{DP-SGD!training overhead}

### Mathematical Foundations and Privacy Parameters {#sec-security-privacy-dp-mathematical-foundations-7e4a}

Production deployments of private ML systems require more rigorous mathematical foundations than the intuitive $\epsilon$-differential privacy definition alone provides. This section formalizes the privacy guarantees, noise mechanisms, and composition properties that enable precise privacy accounting in distributed training environments. Readers primarily interested in practical implementation may skip to the worked example at the end of this section, while those responsible for privacy budget management will need the full treatment.

#### The $(\epsilon, \delta)$-Differential Privacy Formulation

The pure $\epsilon$-DP definition provides strong guarantees but can be overly restrictive for practical ML applications. A relaxed formulation, $(\epsilon, \delta)$-differential privacy, allows a small probability $\delta$ of privacy loss exceeding $\epsilon$. This relaxation proves essential for deep learning, where Gaussian noise (which has unbounded support) is preferred over Laplace noise for gradient perturbation.

Formally, a randomized algorithm $\mathcal{A}$ satisfies $(\epsilon, \delta)$-differential privacy if for all adjacent datasets $D, D'$ and all measurable output sets $S$:
$$
\Pr[\mathcal{A}(D) \in S] \leq e^{\epsilon} \Pr[\mathcal{A}(D') \in S] + \delta
$$

The parameter $\delta$ represents the probability that the privacy guarantee fails catastrophically. In practice, $\delta$ must be set cryptographically small, typically $\delta < 1/n^2$ for a dataset of size $n$, ensuring negligible probability of privacy breach. For a dataset with 1 million records, $\delta = 10^{-12}$ ensures the probability of exceeding the $\epsilon$ bound remains vanishingly small.

#### Noise Mechanisms for Achieving Differential Privacy

Differential privacy is operationalized through carefully calibrated noise injection. The two primary mechanisms used in ML systems differ in their noise distributions and applicability:

*Laplace Mechanism*: For pure $\epsilon$-DP, the Laplace mechanism adds noise drawn from $\text{Lap}(\Delta f/\epsilon)$, where $\Delta f$ is the global sensitivity of the function (the maximum change in output when one record changes). For a query $f$, the privatized output is:
$$
\tilde{f}(D) = f(D) + \text{Lap}\left(\frac{\Delta f}{\epsilon}\right)
$$

The Laplace distribution has density $p(x) = \frac{\epsilon}{2\Delta f}\exp\left(-\frac{\epsilon|x|}{\Delta f}\right)$, with scale parameter $b = \Delta f/\epsilon$. The noise magnitude scales inversely with $\epsilon$: stronger privacy ($\epsilon = 0.1$) requires 10$\times$ more noise than moderate privacy ($\epsilon = 1.0$).

Derivation of Laplace Mechanism Privacy Guarantee To prove that the Laplace mechanism achieves $\epsilon$-DP, consider the ratio of output probabilities on adjacent datasets $D$ and $D'$. Let $f(D) = v$ and $f(D') = v'$, where $|v - v'| \leq \Delta f$ by the sensitivity bound. For any output $y$, the probability ratio is:
$$
\frac{p(y | D)}{p(y | D')} = \frac{\exp(-\epsilon|y - v|/\Delta f)}{\exp(-\epsilon|y - v'|/\Delta f)} = \exp\left(\frac{\epsilon(|y - v'| - |y - v|)}{\Delta f}\right)
$$

By the triangle inequality, $|y - v'| - |y - v| \leq |v - v'| \leq \Delta f$. Therefore:
$$
\frac{p(y | D)}{p(y | D')} \leq \exp\left(\frac{\epsilon \cdot \Delta f}{\Delta f}\right) = e^{\epsilon}
$$

This establishes that the Laplace mechanism satisfies $\epsilon$-differential privacy. The derivation reveals why sensitivity calibration is essential: the noise scale must match the maximum possible change in the query output to mask individual contributions.

*Gaussian Mechanism*: For $(\epsilon, \delta)$-DP, the Gaussian mechanism adds noise $\mathcal{N}(0, \sigma^2)$ where $\sigma$ is calibrated based on $\epsilon, \delta$, and the $\ell_2$ sensitivity $\Delta_2 f$:
$$
\tilde{f}(D) = f(D) + \mathcal{N}\left(0, \sigma^2 \cdot (\Delta_2 f)^2\right)
$$

Derivation of Gaussian Mechanism Privacy Guarantee Unlike the Laplace mechanism, which achieves pure $\epsilon$-DP, the Gaussian mechanism requires the relaxed $(\epsilon, \delta)$-DP formulation because the Gaussian distribution has unbounded support. The derivation proceeds by analyzing the privacy loss random variable.

For adjacent datasets with $\ell_2$ sensitivity $\Delta_2 f$, the privacy loss at output $y$ follows:
$$
L(y) = \ln\frac{p(y|D)}{p(y|D')} = \frac{\|y - f(D')\|_2^2 - \|y - f(D)\|_2^2}{2\sigma^2}
$$

When $y = f(D) + z$ for noise $z \sim \mathcal{N}(0, \sigma^2 I)$, the privacy loss becomes a shifted Gaussian with mean $\frac{\Delta_2 f^2}{2\sigma^2}$ and variance $\frac{\Delta_2 f^2}{\sigma^2}$. Using tail bounds on this distribution, the probability that $L(y) > \epsilon$ can be bounded by $\delta$ when:
$$
\sigma \geq \frac{\Delta_2 f}{\epsilon}\sqrt{2\ln\left(\frac{1.25}{\delta}\right)}
$$

This formula reveals the three-way trade-off: achieving smaller $\epsilon$ (stronger privacy) or smaller $\delta$ (higher confidence) requires proportionally larger noise $\sigma$, which degrades model utility. The factor $\sqrt{2\ln(1.25/\delta)}$ grows slowly with $1/\delta$, so cryptographically small $\delta$ (e.g., $10^{-8}$) only moderately increases the required noise compared to $\delta = 10^{-5}$. The following metric formalizes the *privacy-accuracy tax* that this noise imposes on model performance.

::: {.callout-example title="The Privacy-Accuracy Tax"}
**The Trade-off**: Stronger privacy requires adding more noise to gradients during training. This noise acts like a "tax" on model accuracy.

**Formula**:
For $(\epsilon, \delta)$-DP with gradient clipping $C$, the required noise standard deviation $\sigma$ is:
$$ \sigma \geq \frac{C \sqrt{2 \ln(1.25/\delta)}}{\epsilon} $$

**Scenario**:

*   **Gradient Norm Limit ($C$)**: 1.0
*   **Failure Probability ($\delta$)**: $10^{-5}$

**Calculated Noise ($\sigma$)**:

*   **Strong Privacy ($\epsilon=1.0$)**: $\sigma \approx 1.0 \times \sqrt{2 \times 11.7} / 1.0 \approx \mathbf{4.8}$
*   **Weak Privacy ($\epsilon=10.0$)**: $\sigma \approx 1.0 \times \sqrt{2 \times 11.7} / 10.0 \approx \mathbf{0.48}$

**Conclusion**: Achieving $\epsilon=1.0$ requires adding noise that is nearly **5$\times$ larger** than the signal (gradient norm 1.0). This can degrade accuracy by 5-10% unless you train for significantly longer or use much larger batch sizes to average out the noise.
:::

The noise scale must satisfy $\sigma \geq \frac{\Delta_2 f}{\epsilon}\sqrt{2\ln(1.25/\delta)}$ to achieve $(\epsilon, \delta)$-DP. For typical ML hyperparameters ($\epsilon=1, \delta=10^{-7}$), this requires $\sigma \approx 4.45 \cdot \Delta_2 f$.

Gaussian noise is preferred in deep learning because gradient norms are naturally bounded in $\ell_2$ space, making sensitivity analysis tractable. The Gaussian mechanism also composes more tightly under Rényi Differential Privacy accounting (discussed below).

#### Privacy Loss Random Variable and Moments Accountant

A more refined approach to privacy analysis tracks the privacy loss random variable (PLRV), which quantifies how much information about an individual leaks from a single observation. For adjacent datasets $D, D'$ and algorithm output $O$, the privacy loss is:
$$
L^{(O)}_{(D,D')} = \ln\frac{\Pr[\mathcal{A}(D) = O]}{\Pr[\mathcal{A}(D') = O]}
$$

The PLRV characterizes the log-likelihood ratio between outputs on adjacent datasets. For $(\epsilon, \delta)$-DP, the tail probability must satisfy $\Pr[L > \epsilon] \leq \delta$. This formulation enables composition analysis through moment-generating functions.

The *moments accountant* technique, introduced for DP-SGD by @abadi2016deep, tracks higher-order moments of the privacy loss distribution. Rather than computing worst-case composition, it analyzes the actual privacy loss distribution across training iterations. For mechanism $\mathcal{M}$ with privacy loss $L$, the moments accountant computes:
$$
\alpha_{\mathcal{M}}(\lambda) = \max_{D,D'} \ln \mathbb{E}_{O \sim \mathcal{M}(D)}\left[\left(\frac{\Pr[\mathcal{M}(D) = O]}{\Pr[\mathcal{M}(D') = O]}\right)^{\lambda}\right]
$$

for moment order $\lambda$. After $k$ compositions, the accumulated moment is $k \cdot \alpha_{\mathcal{M}}(\lambda)$. Applying the Markov inequality then yields $(\epsilon, \delta)$ bounds:
$$
\epsilon(\delta) = \min_{\lambda} \left[\frac{k \cdot \alpha_{\mathcal{M}}(\lambda) - \ln(1/\delta)}{\lambda - 1}\right]
$$

For DP-SGD training a ResNet-20 on CIFAR-10 with 100 epochs, batch size 256, and clipping norm $C=1.0$, the moments accountant yields $\epsilon \approx 2.3$ (for $\delta=10^{-5}$), compared to $\epsilon \approx 23$ under naive composition. This tighter accounting makes private deep learning practically feasible.

#### Rényi Differential Privacy and Composition

Rényi Differential Privacy (RDP), introduced by @mironov2017renyi, provides even tighter composition bounds by tracking Rényi divergence rather than KL divergence. A mechanism $\mathcal{M}$ satisfies $(\alpha, \varepsilon)$-RDP if for all adjacent $D, D'$:
$$
D_{\alpha}(\mathcal{M}(D) \| \mathcal{M}(D')) = \frac{1}{\alpha-1}\ln \mathbb{E}_{x \sim \mathcal{M}(D')}\left[\left(\frac{\Pr[\mathcal{M}(D) = x]}{\Pr[\mathcal{M}(D') = x]}\right)^{\alpha}\right] \leq \varepsilon
$$

where $\alpha > 1$ is the Rényi order. RDP composition is remarkably simple: if mechanism $\mathcal{M}_i$ satisfies $(\alpha, \varepsilon_i)$-RDP, then their composition satisfies $(\alpha, \sum_i \varepsilon_i)$-RDP. This linearity contrasts with the suboptimal $\sqrt{k}$ scaling of advanced composition for $(\epsilon, \delta)$-DP.

For Gaussian noise with scale $\sigma$, the RDP guarantee is:
$$
\varepsilon(\alpha) = \frac{\alpha}{2\sigma^2}
$$

After $k$ iterations of DP-SGD with noise $\sigma$ and sampling rate $q$, the RDP guarantee is approximately:
$$
\varepsilon_{\text{total}}(\alpha) \approx \frac{k \cdot q^2 \cdot \alpha}{2\sigma^2}
$$

This RDP guarantee can be converted to $(\epsilon, \delta)$-DP using:
$$
\epsilon = \varepsilon + \frac{\ln(1/\delta)}{\alpha - 1}
$$

optimized over $\alpha$. For typical ML workloads, RDP provides 2-3$\times$ tighter bounds than moments accountant, enabling longer training with fixed privacy budget.

#### Practical Privacy Budget Example: DP-SGD for Image Classification

Consider training a CNN on 50,000 CIFAR-10 images with $(\epsilon, \delta) = (3.0, 10^{-5})$ target privacy. Using DP-SGD with:

- Batch size $B = 4000$ (sampling rate $q = B/N = 0.08$)
- Gradient clipping norm $C = 1.0$ (sensitivity $\Delta_2 = 2C/B = 0.0005$)
- Noise multiplier $\sigma = 1.3$ (noise scale relative to clipping)
- Training for $T = 60$ epochs ($k = T \cdot N/B = 750$ steps)

The RDP analysis proceeds as follows:

1. Per-step RDP: $\varepsilon_{\text{step}}(\alpha) \approx \frac{q^2 \alpha}{2\sigma^2} = \frac{(0.08)^2 \alpha}{2(1.3)^2} \approx 0.00189\alpha$

2. Total RDP after 750 steps: $\varepsilon_{\text{total}}(\alpha) = 750 \cdot 0.00189\alpha = 1.42\alpha$

3. Converting to $(\epsilon, \delta)$-DP, we optimize over $\alpha$:
   - For $\alpha = 10$: $\epsilon = 1.42 \times 10 + \ln(10^5)/9 = 14.2 + 1.28 = 15.48$ (too high)
   - For $\alpha = 4$: $\epsilon = 1.42 \times 4 + \ln(10^5)/3 = 5.68 + 3.83 = 9.51$
   - For $\alpha = 3$: $\epsilon = 1.42 \times 3 + \ln(10^5)/2 = 4.26 + 5.76 = 10.02$
   - Optimal $\alpha \approx 3.5$ yields $\epsilon \approx 8.4$

To achieve target $\epsilon = 3.0$, we must reduce $\sigma$ to 0.7 with more noise causing accuracy drops of approximately 5%, train for fewer epochs with $T = 25$ risking underfitting, or increase batch size to $B = 8000$ for better sampling at the cost of higher memory requirements.

This quantitative analysis demonstrates the fundamental privacy-utility-computational resource tradeoff in production ML systems.

While differential privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility[^fn-privacy-utility-tension] that has measurable computational and accuracy costs.

[^fn-privacy-utility-tension]: **Privacy-Utility Tension**: Formalized by Dwork and McSherry, who proved that perfect privacy (infinite noise) yields zero utility, while perfect utility (zero noise) provides zero privacy. The "privacy budget" ($\varepsilon$) is a finite resource: each query or training epoch consumes a portion, and once exhausted, no further computation on that data is permitted without degrading the guarantee. This makes privacy accounting a first-class constraint in ML system design, alongside compute and memory budgets. \index{Privacy-Utility!trade-off}

Practical DP deployment requires careful consideration of computational trade-offs, privacy budget management, and implementation challenges. @tbl-privacy-technique-comparison compares five privacy-preserving approaches across these dimensions.

| **Technique**              | **Privacy Guarantee** | **Computational Overhead** | **Deployment Maturity** | **Typical Use Case**                                         | **Trade-offs**                                                                                        |
|:---------------------------|:----------------------|:---------------------------|:------------------------|:-------------------------------------------------------------|:------------------------------------------------------------------------------------------------------|
| **Differential Privacy**   | Formal (ε-DP)         | Moderate to High           | Production              | Training with sensitive or regulated data                    | Reduced accuracy; careful tuning of ε/noise required to balance utility and protection                |
| **Federated Learning**     | Structural            | Moderate                   | Production              | Cross-device or cross-org collaborative learning             | Gradient leakage risk; requires secure aggregation and orchestration infrastructure                   |
| **Homomorphic Encryption** | Strong (Encrypted)    | High                       | Experimental            | Inference in untrusted cloud environments                    | High latency and memory usage; suitable for limited-scope inference on fixed-function models          |
| **Secure MPC**             | Strong (Distributed)  | Very High                  | Experimental            | Joint training across mutually untrusted parties             | Expensive communication; challenging to scale to many participants or deep models                     |
| **Synthetic Data**         | Weak (if standalone)  | Low to Moderate            | Emerging                | Data sharing, benchmarking without direct access to raw data | May leak sensitive patterns if training process is not differentially private or audited for fidelity |

: **Privacy-Accuracy Trade-Offs**: Data privacy techniques impose varying computational costs and offer different levels of formal privacy guarantees, requiring practitioners to balance privacy strength with model utility and deployment constraints. The table summarizes key properties—privacy guarantees, computational overhead, maturity, typical use cases, and trade-offs—to guide informed decisions when designing privacy-aware machine learning systems. {#tbl-privacy-technique-comparison}

Increasing the noise to reduce $\epsilon$ may degrade model accuracy, especially in low-data regimes or fine-grained classification tasks. Consequently, DP is often applied selectively (either during training on sensitive datasets or at inference when returning aggregate statistics) to balance privacy with performance goals [@dwork2014algorithmic].

### Privacy Budget Composition {#sec-security-privacy-privacy-budget-composition-edbe}

A critical aspect of differential privacy is that privacy loss accumulates. Every time a mechanism accesses the sensitive data, it consumes a portion of the privacy budget $\epsilon$. If an organization trains 10 models on the same dataset, each with $\epsilon=1$, the total privacy loss is not $\epsilon=1$ but closer to $\epsilon=10$ (under simple composition).

Composition Theorems quantify this accumulation:

*   **Simple Composition**: Running $k$ mechanisms with $\epsilon_i$ guarantees $\sum \epsilon_i$ privacy. This is a loose bound.
*   **Advanced Composition**: Provides tighter bounds, showing that privacy loss grows roughly with $\sqrt{k}$.
*   **Rényi Differential Privacy (RDP)**: A modern framework used in deep learning (e.g., DP-SGD) that offers even tighter composition tracking, essential for training neural networks over thousands of iterations.

The practical implication is that organizations must manage a **global privacy budget** for each dataset, halting access once the budget is exhausted.

### Quantifying the Privacy-Utility Trade-off {#sec-security-privacy-privacy-utility-tradeoff-b3e7}

The theoretical framework of differential privacy translates into measurable accuracy degradation in practice. Empirical studies across benchmark datasets reveal consistent patterns in how privacy parameters affect model performance, enabling practitioners to make informed decisions about privacy-utility trade-offs.

@tbl-dp-accuracy-tradeoff summarizes accuracy degradation across standard benchmarks when training with DP-SGD at various privacy levels. These results, drawn from published research and industry deployments, provide quantitative guidance for practitioners.

| **Dataset**     | **Model** | **Non-Private Acc.** | **$\epsilon=8$ Acc.** | **$\epsilon=1$ Acc.** |
|:----------------|:----------|---------------------:|----------------------:|----------------------:|
| **MNIST**       | CNN       |                99.2% |         98.1% (-1.1%) |         96.6% (-2.6%) |
| **CIFAR-10**    | ResNet-18 |                93.5% |        83.0% (-10.5%) |        67.0% (-26.5%) |
| **ImageNet**    | ResNet-50 |                76.1% |        47.8% (-28.3%) |    N/A (not feasible) |
| **IMDB (text)** | BERT-base |                93.0% |         88.5% (-4.5%) |        82.0% (-11.0%) |

: **Privacy-Accuracy Trade-offs**: Model accuracy degradation when training with DP-SGD at different privacy budgets ($\delta = 10^{-5}$ throughout). Strong privacy ($\epsilon = 1$) causes significant accuracy loss on complex tasks, while moderate privacy ($\epsilon = 8$) preserves most utility for simpler datasets. {#tbl-dp-accuracy-tradeoff}

Several patterns emerge from these empirical results. First, simpler tasks tolerate DP better: MNIST classification loses only 2.6 percentage points at $\epsilon = 1$, while CIFAR-10 loses 26.5 points. The noise required to protect privacy has a larger relative impact when the learning task requires capturing fine-grained patterns. Second, dataset size matters critically: larger datasets enable higher signal-to-noise ratios because gradient noise averages out across more samples. Training DP models on small datasets (fewer than 10,000 samples) typically yields unacceptable accuracy degradation. Third, the privacy-accuracy curve is non-linear: the marginal accuracy cost of tightening privacy increases as $\epsilon$ decreases. Moving from $\epsilon = 8$ to $\epsilon = 1$ costs more accuracy than moving from non-private to $\epsilon = 8$.

### Decision Framework: When is Differential Privacy Worth It? {#sec-security-privacy-dp-decision-framework-c4a8}

Given the quantified trade-offs, practitioners need a systematic framework for deciding whether and how to deploy differential privacy. The following decision criteria synthesize considerations from regulatory requirements, threat models, and operational constraints.

##### Criterion 1: Regulatory and Legal Requirements

DP becomes essential when regulations mandate formal privacy guarantees. GDPR's "privacy by design" principle and HIPAA's technical safeguards both align with DP's mathematical framework. If audit trails require demonstrable privacy bounds, DP provides defensible documentation that ad-hoc anonymization cannot match. Organizations handling EU health data, US medical records, or financial data subject to CCPA/CPRA should evaluate DP as a compliance mechanism.

##### Criterion 2: Threat Model Assessment

DP protects against membership inference (determining if a specific individual's data was used in training) and training data extraction attacks. If your threat model includes sophisticated adversaries attempting these attacks (e.g., competitors, nation-state actors, or malicious insiders), DP provides provable protection that other techniques lack. However, if your primary threats are data breaches of stored data rather than inference attacks on deployed models, encryption and access controls may be more appropriate defenses.

##### Criterion 3: Dataset Characteristics

Evaluate whether your data supports effective DP training:

- **Size threshold**: Datasets with fewer than 50,000 samples rarely achieve acceptable utility with meaningful privacy ($\epsilon < 10$). For small datasets, consider federated learning with secure aggregation as an alternative.
- **Task complexity**: Simple classification tasks (binary or few-class) tolerate noise better than fine-grained recognition or generation tasks.
- **Data sensitivity distribution**: If sensitive attributes are concentrated in rare subgroups, DP may disproportionately degrade performance on those subgroups, raising fairness concerns.

##### Criterion 4: Acceptable Utility Loss

Quantify the maximum acceptable accuracy degradation before deployment. For safety-critical applications (medical diagnosis, autonomous vehicles), even 5% accuracy loss may be unacceptable. For recommendation systems or content personalization, 10-15% degradation might be tolerable given privacy benefits. Use @tbl-dp-accuracy-tradeoff as a starting point, then conduct experiments on your specific task.

##### Criterion 5: Computational Budget

DP training typically requires 2-5$\times$ more compute than non-private training due to per-sample gradient computation and larger batch sizes needed to overcome noise. If computational resources are constrained, this overhead may be prohibitive. Cloud deployments can scale compute, but edge training scenarios may find DP impractical.

##### Decision Matrix Summary

Use DP when: (1) formal privacy guarantees are legally required, (2) membership inference is a credible threat, (3) dataset exceeds 50,000 samples, (4) task can tolerate 5--15% accuracy loss, and (5) computational budget supports increased training costs.

Consider alternatives when: (1) primary threats are data breaches (use encryption), (2) dataset is small (use federated learning), (3) task requires maximum accuracy (use access controls and audit logging), or (4) deployment is resource-constrained (use differential privacy at inference only for aggregate queries).

This framework treats DP as one tool in a privacy-preserving toolkit rather than a universal solution. The most effective deployments often combine DP with complementary techniques: federated learning for data minimization, secure aggregation for gradient protection, and access controls for deployment security. To ground these decision criteria in concrete implementation, the following worked example walks through a complete DP-SGD privacy budget calculation for a realistic training scenario.

::: {.callout-example title="DP-SGD Privacy Budget Example"}
**Scenario**: Train a sentiment classifier on 100,000 customer reviews with target privacy $(\epsilon, \delta) = (8, 10^{-6})$.

### Step 1: Configure DP-SGD Parameters {.unnumbered}

- Dataset size: $N = 100,000$
- Batch size: $B = 2,000$ (sampling rate $q = B/N = 0.02$)
- Gradient clipping norm: $C = 1.0$
- Training epochs: $T = 10$ (total steps $k = T \times N/B = 500$)
- Target: $\epsilon = 8$, $\delta = 10^{-6}$

### Step 2: Calculate Required Noise Multiplier {.unnumbered}

Using the Gaussian mechanism formula and RDP accounting, we need noise multiplier $\sigma$ such that after $k = 500$ iterations with sampling rate $q = 0.02$, the total privacy loss is $\epsilon \leq 8$.

The per-step RDP guarantee for Gaussian mechanism with subsampling is approximately:
$$\varepsilon_{\text{step}}(\alpha) \approx \frac{q^2 \alpha}{2\sigma^2}$$

For $\sigma = 0.8$ (a typical starting point):
$$\varepsilon_{\text{step}}(\alpha) = \frac{(0.02)^2 \alpha}{2(0.8)^2} = \frac{0.0004\alpha}{1.28} \approx 0.000312\alpha$$

After 500 steps: $\varepsilon_{\text{total}}(\alpha) = 500 \times 0.000312\alpha = 0.156\alpha$

### Step 3: Convert RDP to DP {.unnumbered}

Converting to $(\epsilon, \delta)$-DP by optimizing over $\alpha$:
$$\epsilon = \varepsilon_{\text{total}}(\alpha) + \frac{\ln(1/\delta)}{\alpha - 1} = 0.156\alpha + \frac{\ln(10^6)}{\alpha - 1}$$

Evaluating at different $\alpha$ values:

- $\alpha = 20$: $\epsilon = 0.156 \times 20 + 13.8/19 = 3.12 + 0.73 = 3.85$
- $\alpha = 50$: $\epsilon = 0.156 \times 50 + 13.8/49 = 7.8 + 0.28 = 8.08$
- $\alpha = 45$: $\epsilon = 0.156 \times 45 + 13.8/44 = 7.02 + 0.31 = 7.33$

Optimal $\alpha \approx 48$ yields $\epsilon \approx 7.8$, which satisfies our target of $\epsilon \leq 8$.

### Step 4: Expected Accuracy Impact {.unnumbered}

With $\sigma = 0.8$ and $\epsilon \approx 8$, expect 3-5% accuracy degradation compared to non-private training. For a sentiment classifier achieving 92% accuracy without DP, the private version should achieve 87-89% accuracy.

### Step 5: Implementation Verification {.unnumbered}

```{.python}
# Using Opacus library for PyTorch
from opacus import PrivacyEngine
from opacus.accountants import RDPAccountant

privacy_engine = PrivacyEngine()
model, optimizer, data_loader = privacy_engine.make_private(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    noise_multiplier=0.8,
    max_grad_norm=1.0,
)

# After training, verify privacy spent
epsilon = privacy_engine.get_epsilon(delta=1e-6)
print(f"Final epsilon: {epsilon:.2f}")  # Should be ~7.8
```

This worked example demonstrates how theoretical privacy accounting translates into concrete implementation parameters, enabling practitioners to verify privacy guarantees before and after training.
:::

Moving from these isolated mathematical proofs to a fully fortified production environment, however, requires more than just setting an epsilon value; it requires a structured, multi-phase organizational strategy for deploying security controls.

## Practical Implementation Roadmap {#sec-security-privacy-practical-roadmap-8f3a}

A common mistake when securing a new ML platform is attempting to implement differential privacy, trusted execution environments, and adversarial training all in the first sprint. The resulting friction paralyzes the engineering team, and the system often launches with basic misconfigurations like open S3 buckets. Security must be layered progressively. We begin by defining a pragmatic, phased roadmap: start with basic perimeter hygiene, advance to model-specific defenses, and finally mature into cryptographically guaranteed privacy.

### Phase 1: Foundation Security Controls (Weeks 1--4) {#sec-security-privacy-phase1-baseline-foundation-2d9c}

The initial phase focuses on establishing a robust security perimeter and basic cyber hygiene, representing the minimum viable security posture for any production system. For our 175B fleet distributed across 32 nodes, Phase 1 establishes this perimeter by locking down access and communication channels before any model serves its first production request.

This begins with implementing strict **Role-Based Access Control (RBAC)** across every component of the ML infrastructure. Site Reliability Engineers (SREs) receive infrastructure-level access to node orchestration and health monitoring but cannot read model weights or training data. Data scientists receive access to model artifacts and experiment tracking in non-production environments but cannot deploy to production without approval from the release pipeline. Inference endpoints themselves operate with minimal, read-only privileges scoped to the specific model shards they serve, with service-to-service authentication enforced through short-lived tokens (expiring every 15 minutes) issued by a centralized identity provider. Multi-factor authentication is mandatory for all administrative access, including SSH to training nodes, access to model registries, and modifications to serving configurations.

All inter-node communication across the 32-node fleet must be encrypted in transit using **TLS 1.3**, including the high-bandwidth gradient synchronization channels between training nodes and the model-shard loading paths between storage and inference servers. The 350 GB of model weights, distributed across shards on each serving node, must be protected at rest with **AES-256 encryption**, with decryption keys managed through a centralized Key Management Service (KMS) rather than stored locally on disk. Comprehensive audit logging captures every data access event, model operation, and configuration change, with logs retained for a minimum of 90 days to support incident investigation, compliance auditing, and forensic analysis.

Input validation is configured at the API gateway as the first line of runtime defense. For the 175B language model, this means enforcing strict token length limits (e.g., maximum 4,096 input tokens), rate limiting per API key (e.g., 1,000 requests per minute for standard tier, with adaptive throttling based on anomaly scores as described in @sec-security-privacy-model-extraction-defenses-8f3b), and schema validation that rejects malformed inference requests before they reach the model. A dependency scanning pipeline is established to continuously monitor for Common Vulnerabilities and Exposures (CVEs) across the ML-specific supply chain: PyTorch, CUDA drivers, NCCL communication libraries, and the model serialization formats that could harbor deserialization exploits. Baseline monitoring dashboards track P99 inference latency, error rates, and query volume distributions, establishing the normal operating envelope against which future anomalies will be measured.

### Phase 2: Privacy Controls and Model Protection (Weeks 5--12) {#sec-security-privacy-phase2-privacy-model-protection-7a8b}

With the foundational perimeter secure, the focus shifts to protecting the model itself and the data it processes---the core intellectual property and primary source of privacy risk. This phase directly confronts the threats of model extraction (@sec-security-privacy-model-theft-1879) and data leakage (@sec-security-privacy-case-study-gpt3-data-extraction-attack-5126) that represent the highest-impact risks for a fleet serving a 175B model.

If the 175B model is to be fine-tuned on user data---for example, personalizing responses based on customer interaction logs---this is the stage to integrate **Differential Privacy** into the training pipeline. A clear privacy budget must be defined and tracked as a first-class system resource. For general-purpose applications, a privacy budget of $\epsilon \leq 8$ provides meaningful protection with moderate accuracy impact (3--5% degradation as quantified in @sec-security-privacy-privacy-utility-tradeoff-b3e7). For domains handling medical records or financial data, a much stricter budget of $\epsilon \leq 1$ is necessary, accepting 10--15% accuracy degradation as the cost of regulatory compliance. The privacy budget must be tracked cumulatively across all fine-tuning runs, hyperparameter searches, and A/B tests that touch the same sensitive dataset, preventing the "privacy exhaustion" pitfall described in @sec-security-privacy-fallacies-pitfalls-0c20.

The 350 GB of model weights are not just encrypted at rest but are now managed with a formal key rotation schedule. Encryption keys are rotated every 90 days via the centralized KMS, with automated re-encryption of all model shards triggered by each rotation. Model integrity is verified at load time through SHA-256 hash checking against a signed manifest stored in a tamper-evident registry. To defend against model extraction attacks, the inference API is hardened by obfuscating outputs: logits are rounded to two decimal places before returning to clients (reducing information leakage by approximately 4 bits per token as analyzed in @sec-security-privacy-model-extraction-defenses-8f3b), and only top-5 token predictions are returned instead of the full vocabulary distribution. Query monitoring systems flag users whose access patterns exhibit the systematic, high-volume, low-variance signatures characteristic of extraction attempts, triggering adaptive rate limiting that reduces their effective query rate by 55--80%.

This phase culminates in a formal compliance mapping exercise. The system's controls are documented against specific regulatory requirements: GDPR Article 25 (Privacy by Design) for EU user data, HIPAA technical safeguards for any healthcare applications, and preparation for a SOC 2 Type II audit that will validate the operational effectiveness of these controls over a sustained observation period.

### Phase 3: Advanced Threat Defense (Weeks 13--24) {#sec-security-privacy-phase3-advanced-defenses-runtime-8c2d}

The final phase moves from a defensive posture to proactive threat hunting and hardening against sophisticated, ML-specific attacks. At the scale of a 175B-parameter fleet, threats become more subtle and dangerous. A single compromised node among the 32 could be used to inject poisoned data during online learning or serve as a beachhead for exfiltrating model weights. The attack surface grows superlinearly with fleet size: 32 nodes create $O(n^2)$ inter-node communication channels, each a potential vector for gradient manipulation or man-in-the-middle attacks on model synchronization.

This phase implements a formal **adversarial robustness** program. Dedicated red-team exercises are conducted quarterly against the production inference API, simulating realistic attack scenarios: systematic model extraction through crafted query sequences, prompt injection attacks designed to bypass content filters, and membership inference probes targeting the fine-tuning data. For safety-critical serving domains (medical diagnosis, financial risk assessment), adversarial training using Projected Gradient Descent (PGD) [@madry2018towards] is integrated into the fine-tuning pipeline, and certified defenses [@cohen2019certified] provide mathematical guarantees on robustness within defined perturbation bounds. For the highest-sensitivity workloads, a subset of the fleet is migrated to run within **Trusted Execution Environments (TEEs)** as described in @sec-security-privacy-trusted-execution-environments-80ed, providing hardware-level isolation that protects both the model weights and user data even if the host operating system is compromised. Secure boot processes are enforced on all serving nodes, establishing a verified chain of trust from power-on to model loading.

Monitoring graduates from simple threshold-based alerts to ML-driven anomaly detection. A lightweight classifier, trained on historical query logs, identifies novel extraction attempts by detecting distribution shifts in input prompts, unusual temporal patterns in query sequences, and systematic boundary-probing behavior that deviates from legitimate user traffic. Crucially, a detailed **incident response playbook** is created and rehearsed through tabletop exercises. This playbook includes procedures for fleet-wide model rollback using canary deployment infrastructure (reverting from a compromised model version to the last known-good checkpoint in under five minutes), contaminated data isolation (quarantining suspect training data and all model versions derived from it), and forensic analysis protocols for ML-specific attacks (capturing inference logs, parameter deltas between model versions, and memory snapshots from serving containers). An annual security architecture review ensures defenses keep pace with the evolving threat landscape.

### Implementation Considerations {#sec-security-privacy-implementation-considerations-9f4e}

The journey through these three phases requires a strategic allocation of resources, talent, and capital. The cost and complexity escalate with each phase. Phase 1 can typically be implemented with a budget under \$50,000, relying on standard security engineers and DevOps practices already present in most organizations. Phase 2, requiring specialized expertise in ML privacy, model protection, and compliance frameworks, demands an investment of \$200,000 or more, including tooling for differential privacy (e.g., Opacus, TensorFlow Privacy), key management infrastructure, and compliance consulting. Phase 3 represents a significant ongoing commitment, often exceeding \$500,000 annually for dedicated red-teaming, advanced monitoring infrastructure, TEE-capable hardware, and the specialized ML security engineers needed to operate these systems.

Staffing must evolve accordingly. Phase 1 is the domain of SecOps and SREs who can implement standard access controls and encryption. Phase 2 requires hiring or training specialized ML Security Engineers who understand both the ML pipeline and privacy-preserving techniques. Phase 3 necessitates a dedicated threat intelligence or red team with expertise in adversarial machine learning. Success must be measured not only by the absence of breaches but by quantitative metrics that balance security with operational performance: zero critical data exfiltration incidents, security measures introducing less than 100 ms of additional P99 latency, accuracy degradation from privacy-preserving modifications held below 5%, and full compliance with all applicable regulations (GDPR, HIPAA, SOC 2).

The roadmap must be customized to each organization's threat model and regulatory environment. A healthcare organization would prioritize and accelerate Phase 2 to achieve HIPAA compliance before expanding patient-facing ML services. A financial institution would emphasize Phase 1 data protection controls to meet PCI-DSS requirements and prevent fraud model theft. An autonomous systems company would fast-track Phase 3 adversarial robustness to defend against real-world physical attacks on perception models. Each phase should be fully implemented and stabilized before progressing to the next, with regular security assessments---including penetration testing of ML-specific attack vectors and compliance audits---validating the effectiveness of implemented controls and guiding progression through phases.

With a structured implementation approach in place, organizations complete the progression from reactive to proactive security posture. However, even well-designed systems can fall victim to common misconceptions that undermine protection efforts and waste security investments.

## Fallacies and Pitfalls {#sec-security-privacy-fallacies-pitfalls-0c20}

Security and privacy in machine learning systems present unique challenges that extend beyond traditional cybersecurity concerns, involving sophisticated attacks on data, models, and inference processes. The complexity of modern ML pipelines, combined with the probabilistic nature of machine learning and the sensitivity of training data, creates numerous opportunities for misconceptions about effective protection strategies.

Fallacy: ***Security through obscurity provides adequate protection for machine learning models.***

This outdated approach assumes hiding architectures or parameters provides meaningful security, but modern black-box attacks succeed without internal knowledge. As detailed in @sec-security-privacy-model-extraction-defenses-8f3b, model extraction attacks reconstruct functionality with 90% accuracy using 10,000--100,000 queries—well within typical API rate limits of 100,000 queries per day. Adversarial examples transfer across architectures with 60--80% success rates, exploiting shared geometric properties rather than architectural details. Organizations relying on secrecy discover this weakness when "proprietary" models are reconstructed through patient querying. Effective ML security requires robust defenses functioning under Kerckhoffs's principle: assume attackers have complete knowledge and build protections through query limiting, output perturbation, watermarking, and anomaly detection rather than architectural secrecy.

Pitfall: ***Assuming that differential privacy automatically ensures privacy without considering implementation details.***

Many practitioners treat differential privacy as a universal solution without understanding parameter selection or budget tracking. As established in @sec-security-privacy-dp-mathematical-foundations-7e4a, privacy strength varies nonlinearly: $\epsilon=0.1$ provides strong privacy but degrades accuracy by 10-15%, $\epsilon=1.0$ offers moderate protection with 5-10% degradation, while $\epsilon=10$ gives weak guarantees with minimal utility loss. Privacy budgets compound across operations: training 10 models at $\epsilon=1.0$ each consumes total $\epsilon=10$, not $\epsilon=1.0$. A production system retraining monthly for two years accumulates $\epsilon=24$ even if targeting $\epsilon=1.0$ per run. Organizations failing to track cumulative privacy loss across retraining, hyperparameter tuning, and A/B testing exceed guarantees by orders of magnitude, violating regulations despite using DP libraries.

Fallacy: ***Federated learning inherently provides privacy protection without additional safeguards.***

This misconception assumes decentralization automatically ensures privacy, but gradient updates transmitted during training leak significant information. Membership inference attacks achieve 70--90% accuracy on production federated models, determining whether specific data points were used in training by exploiting behavioral differences. Gradient inversion attacks can reconstruct original training data (images, text) from gradient vectors with high fidelity. As examined in @sec-security-privacy-federated-learning-3834, effective federated privacy requires layered defenses: secure aggregation protocols prevent the server from seeing individual contributions, differential privacy with $\epsilon \approx 6$ adds calibrated noise to updates, and cryptographic protection prevents gradient inversion. Organizations deploying federated learning without these safeguards discover vulnerability when researchers demonstrate gradient inversion or compliance audits reveal regulatory violations despite data never leaving devices.

Fallacy: ***Data poisoning requires compromising large portions of training data to be effective.***

This misconception leads organizations to focus on large-scale breach prevention while underestimating surgical poisoning. Data poisoning exhibits extreme leverage: poisoning just 0.1% of training data (1,000 examples in 1 million) can reduce model accuracy by 10--50%. Backdoor attacks prove even more efficient—inserting trigger patterns into 0.01% of data creates models with 95%+ clean accuracy but 90%+ attack success on triggered inputs. These backdoors persist through retraining and transfer learning. Attack economics favor adversaries: creating 1,000 poisoned examples costs dramatically less than collecting millions of legitimate examples. For systems incorporating user-generated content, attackers inject poisoned data through normal channels (fake accounts, crafted ratings) representing 0.1% of inputs but shifting recommendations significantly. Organizations assuming "clean enough" data yields "safe enough" models discover otherwise when adversaries achieve disproportionate impact through minimal corruption.

Pitfall: ***Treating security as an isolated component rather than a system-wide property.***

Organizations often add defenses to individual components (encrypted storage, API authentication, model watermarking) without considering system-level attack vectors spanning multiple boundaries. A production system implementing strong API defenses—rate limiting (1,000 queries/day), output perturbation, top-k filtering—appears robust in isolation, but attackers bypass these through alternative batch processing endpoints, extract query-response pairs from unsecured monitoring logs, or directly download model weights from public registries without access controls. As established in @sec-security-privacy-threat-prioritization-framework-f2d5, effective ML security requires holistic threat modeling across the entire lifecycle: data collection, training infrastructure, model storage, deployment, and monitoring. A single weak link (unencrypted snapshots, unauthenticated endpoints, permissive CORS policies) compromises otherwise secure systems. Organizations discover this through costly incidents: API defenses bypassed, models leaked through CI/CD pipelines, or privacy-preserving training compromised by verbose logging.

Pitfall: ***Underestimating the attack surface expansion in distributed ML systems.***

Organizations secure single-node ML systems effectively but fail to recognize distributed architectures multiply attack surfaces geometrically, not linearly. The shift from one machine to $n$ machines increases vulnerabilities by approximately $n^2$ due to inter-node communication channels creating $O(n^2)$ attack vectors in all-to-all topologies or $O(n \log n)$ in ring topologies. Distributed training across 128 GPUs in 16 machines means compromising one node injects poisoned gradients propagating to the global model. Centralized poisoning requiring 0.1% corruption achieves the same effect with 0.01% when targeting specific distributed nodes. Edge deployment exacerbates this: federated learning with 10,000 clients creates 10,000 compromise points—if 1% are compromised (100 devices), coordinated poisoning degrades accuracy by 10--50% while remaining below individual-device anomaly thresholds. Effective distributed ML security requires threat modeling acknowledging superlinear growth: securing inter-node channels, managing identity across security domains, and coordinating policies across heterogeneous infrastructure.

Recognizing that the shift from a single node to a distributed cluster multiplies the attack surface geometrically forces us to abandon perimeter-only thinking. By actively confronting these fallacies, engineers can design systems that remain secure under intense adversarial pressure, allowing us to summarize the core defensive posture of the machine learning fleet.

## Summary {#sec-security-privacy-summary-831c}

Privacy and Security represent the "armor" of the Machine Learning Fleet. Throughout this book, we have built the computational engine (Part I), physical infrastructure (Part II), and global services (Part III). This chapter has developed the defensive layers required to protect that fleet from adversaries who seek to steal its intelligence, poison its memory, or hijack its decision-making logic.

We established a multi-layered defense architecture that spans from the silicon trust anchors (TEEs, HSMs) up to the linguistic safeguards required for modern generative AI. We analyzed the fundamental shift from traditional cybersecurity to ML security, where "learned" decision boundaries become the primary attack surface. Finally, we examined the rigorous mathematical frameworks, such as Differential Privacy, that allow us to derive utility from sensitive data without compromising individual confidentiality.

::: {.callout-takeaways title="Defend the Model, Not Just the Server"}
* **Security vs. Privacy**: Security defends the system from *adversarial manipulation* (poisoning, evasion); Privacy protects sensitive info from *unauthorized inference* (membership inference, inversion). Effective fleets require both.
* **LLM Linguistic Vulnerabilities**: Generative AI introduces non-traditional threats like Prompt Injection (Direct/Indirect) and PII leakage in RAG systems. These require semantic filtering in the MLOps pipeline (@sec-ops-scale) rather than just traditional network firewalls.
* **The Training-Time Trojan**: Data poisoning is a "low-likelihood, high-impact" threat where injecting a tiny fraction of malicious data can embed persistent backdoors that survive model retraining and deployment.
* **Differential Privacy is the Standard**: Mathematical privacy guarantees (ε, δ) are superior to naive anonymization. Implementation requires careful budget management throughout the model lifecycle to prevent "privacy exhaustion."
* **Hardware is the Root of Trust**: Software-level protections are only as secure as the underlying hardware. Trusted Execution Environments (TEEs) and Secure Boot provide the isolation necessary for confidential computing in multi-tenant clouds.
:::

Machine learning systems present a threat surface that traditional cybersecurity was never designed to address. A conventional application server can be hardened by patching known vulnerabilities, encrypting data at rest and in transit, and enforcing access controls. An ML system shares all of these requirements, but adds entirely new attack classes that exploit the learned decision boundary itself. An adversary who poisons training data does not need to breach a firewall; the model internalizes the corruption and carries it through every subsequent deployment. A prompt injection does not exploit a buffer overflow; it exploits the model's inability to distinguish instruction from content. These threats make security and privacy first-class engineering concerns rather than afterthoughts delegated to a separate team.

The practitioner who internalizes this chapter's layered defense architecture gains a decisive advantage: the ability to reason about where an ML system is most vulnerable at each stage of its lifecycle. From hardware root of trust through differential privacy budgets to generative AI guardrails, each layer addresses a threat that the layers below cannot catch alone. No single mechanism suffices, but their composition creates a defense posture that degrades gracefully rather than failing catastrophically. Building this discipline into the engineering workflow from the earliest design phases, rather than bolting it on after deployment, is what separates production-grade systems from prototypes that survive only until the first determined adversary arrives.

::: {.callout-chapter-connection title="From Security to Resilience"}

We have established the defensive perimeter of the ML fleet, protecting it from adversarial manipulation and unauthorized inference. However, security addresses only intentional threats. A model that is perfectly protected from attackers can still fail catastrophically when the real world shifts beneath it.

In @sec-robust-ai, we expand our focus from *malicious* threats to *operational* stress, examining how to build systems that maintain reliability in the face of distribution drift, hardware faults, and the compounding failures that define production environments.

:::

[^fn-robustness-privacy-tradeoff]: **Robustness-Privacy Trade-off**: Improving adversarial robustness (e.g., through adversarial training) often *increases* susceptibility to membership inference attacks. The robust model's decision boundary is more tightly fitted to the training distribution's "manifold," making it easier for an attacker to distinguish training samples from unseen data. This "no free lunch" property forces fleet designers to explicitly prioritize between security and privacy invariants ($O$). \index{Robustness-Privacy!trade-off}

```{python}
#| echo: false
#| label: chapter-end
from mlsys.registry import end_chapter
end_chapter("vol2:security_privacy")
```
