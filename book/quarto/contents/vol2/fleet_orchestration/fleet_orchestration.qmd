---
engine: jupyter
---

```{python}
#| echo: false
#| label: chapter-start
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER START
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter initialization and global imports
# │
# │ Why: Registers this chapter with the mlsys registry and provides shared
# │      imports for all subsequent calculation cells.
# │
# │ Imports: mlsys.registry, mlsys.constants, mlsys.formatting
# │ Exports: (none)
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.registry import start_chapter
from mlsys.constants import (
    A100_MEM_CAPACITY, H100_MEM_CAPACITY,
    A100_FLOPS_FP16_TENSOR, H100_FLOPS_FP16_TENSOR,
    NVLINK_A100_BW, NVLINK_H100_BW,
    INFINIBAND_HDR_BW, INFINIBAND_NDR_BW,
    GPT3_PARAMS,
    GiB, GB, TB, second, Gbps, TFLOPs, param,
    BILLION, BITS_PER_BYTE, SEC_PER_HOUR, SEC_PER_DAY
)
from mlsys.formatting import fmt, sci, check

start_chapter("vol2:fleet_orchestration")
```

# Fleet Orchestration {#sec-fleet-orchestration}

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_orchestration.png){fig-alt="Stylized datacenter visualization with blue glowing server towers, hexagonal node clusters connected by bright network pathways, and circuit board patterns below." width=100%}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{30}{100}{40}{15}
\end{marginfigure}

_Why does resource allocation become the primary bottleneck when hardware is plentiful?_

A thousand GPUs sitting idle waiting for scheduling decisions cost the same as a thousand GPUs computing useful work. At scale, the limiting factor shifts from having enough hardware to using it efficiently: jobs waiting in queues while resources sit idle, fragmentation leaving gaps too small for any pending job, deadlocks where multiple jobs each hold partial resources while waiting for more. Orchestration is the discipline of extracting useful work from shared infrastructure, deciding which jobs run on which nodes, when preemption serves the greater good, how to balance fairness across teams against raw utilization, and how to prevent the coordination mechanisms themselves from becoming bottlenecks. Poor orchestration transforms expensive hardware into expensive waste; effective orchestration transforms a collection of machines into a coherent computing resource where capacity translates reliably into completed work.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Implement **gang scheduling** policies to prevent resource fragmentation and deadlocks in distributed training jobs
- Design multi-tenant **quota systems** using hierarchical fair-share and borrowing to maximize cluster utilization
- Differentiate between HPC (**Slurm**) and Cloud Native (**Kubernetes**) scheduling paradigms and their trade-offs for ML workloads
- Architect **auto-scaling** policies for inference workloads based on custom metrics like queue depth and GPU memory pressure
- Apply **topology-aware scheduling** to place distributed training jobs with respect to NVLink domains and switch hierarchy
- Evaluate **elastic training** strategies that allow jobs to grow and shrink dynamically without full restarts
- Quantify the economics of fleet scheduling including **spot instances**, preemption costs, and capacity reservation strategies

:::

```{python}
#| label: fleet-orchestration-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET ORCHESTRATION: CHAPTER-WIDE CONSTANTS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter-wide setup for Fleet Orchestration (@sec-fleet-orchestration).
# │
# │ Goal: Compute cluster GPU utilization under Kubernetes gang scheduling vs
# │       Slurm backfill to show the ~15–20% utilization gap from scheduling
# │       overhead, plus cluster economics and fragmentation numbers.
# │ Show: "~65% utilization" (Kubernetes) vs "~80%" (Slurm backfill) — inline
# │       in scheduling comparison paragraph; $480K/day operating cost for
# │       10,000-GPU cluster; 25% fragmentation from 6-GPU jobs on 8-GPU nodes.
# │ How: Direct arithmetic on cluster_size, gpu_hour_cost_usd, utilization
# │      fractions; pint .m_as() for hardware specs (GiB, GB/s, Gbps).
# │
# │ Imports: mlsys.constants (A100_MEM_CAPACITY, H100_MEM_CAPACITY,
# │          NVLINK_A100_BW, NVLINK_H100_BW, INFINIBAND_NDR_BW,
# │          GiB, GB, second, Gbps, BITS_PER_BYTE)
# │ Exports: cluster_size_str, daily_cost_str, annual_cost_str,
# │          daily_waste_str, annual_waste_str, equivalent_gpus_str,
# │          spot_price_str, frag_pct_str, total_gpus_str,
# │          nvlink_a100_str, nvlink_h100_str, ib_ndr_str, ib_ndr_gbs_str,
# │          a100_mem_str, h100_mem_str
# └─────────────────────────────────────────────────────────────────────────────

class FleetOrchestrationSetup:
    """Cluster economics, fragmentation, and hardware specs for Fleet Orchestration chapter."""
    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    a100_mem    = A100_MEM_CAPACITY.m_as(GiB)
    h100_mem    = H100_MEM_CAPACITY.m_as(GiB)
    nvlink_a100 = NVLINK_A100_BW.m_as(GB / second)
    nvlink_h100 = NVLINK_H100_BW.m_as(GB / second)
    ib_ndr      = INFINIBAND_NDR_BW.m_as(Gbps)
    ib_ndr_gbs  = ib_ndr / BITS_PER_BYTE

    cluster_size       = 10000   # GPUs in reference cluster
    gpu_hour_cost_usd  = 2.0     # $/GPU-hour (cloud equivalent)
    hours_per_day      = 24
    days_per_year      = 365

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────
    daily_cost  = cluster_size * gpu_hour_cost_usd * hours_per_day
    annual_cost = daily_cost * days_per_year

    idle_fraction = 0.30
    daily_waste   = daily_cost * idle_fraction
    annual_waste  = daily_waste * days_per_year

    util_low  = 0.50
    util_high = 0.80
    equivalent_gpus = cluster_size * (util_high - util_low) / util_low

    spot_discount = 0.65
    spot_price    = gpu_hour_cost_usd * (1 - spot_discount)

    nodes_total    = 64
    gpus_per_node  = 8
    total_gpus     = nodes_total * gpus_per_node
    job_gpus       = 6
    wasted_per_node = gpus_per_node - job_gpus
    frag_pct       = wasted_per_node / gpus_per_node * 100

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    _c1 = check(daily_cost > 0, "Daily cost must be positive")
    _c2 = check(annual_waste > 1_000_000, "Annual waste should be millions at this scale")
    _c3 = check(0 < frag_pct < 100, "Fragmentation must be a valid percentage")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    cluster_size_str    = fmt(cluster_size, precision=0)
    daily_cost_str      = fmt(daily_cost, precision=0)
    annual_cost_str     = f"{annual_cost / 1e6:.0f}"
    daily_waste_str     = fmt(daily_waste, precision=0)
    annual_waste_str    = f"{annual_waste / 1e6:.0f}"
    equivalent_gpus_str = fmt(equivalent_gpus, precision=0)
    spot_price_str      = f"{spot_price:.2f}"
    frag_pct_str        = f"{frag_pct:.0f}"
    total_gpus_str      = fmt(total_gpus, precision=0)
    nvlink_a100_str     = fmt(nvlink_a100, precision=0)
    nvlink_h100_str     = fmt(nvlink_h100, precision=0)
    ib_ndr_str          = fmt(ib_ndr, precision=0)
    ib_ndr_gbs_str      = fmt(ib_ndr_gbs, precision=0)
    a100_mem_str        = fmt(a100_mem, precision=0)
    h100_mem_str        = fmt(h100_mem, precision=0)

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
cluster_size_str    = FleetOrchestrationSetup.cluster_size_str
daily_cost_str      = FleetOrchestrationSetup.daily_cost_str
annual_cost_str     = FleetOrchestrationSetup.annual_cost_str
daily_waste_str     = FleetOrchestrationSetup.daily_waste_str
annual_waste_str    = FleetOrchestrationSetup.annual_waste_str
equivalent_gpus_str = FleetOrchestrationSetup.equivalent_gpus_str
spot_price_str      = FleetOrchestrationSetup.spot_price_str
frag_pct_str        = FleetOrchestrationSetup.frag_pct_str
total_gpus_str      = FleetOrchestrationSetup.total_gpus_str
nvlink_a100_str     = FleetOrchestrationSetup.nvlink_a100_str
nvlink_h100_str     = FleetOrchestrationSetup.nvlink_h100_str
ib_ndr_str          = FleetOrchestrationSetup.ib_ndr_str
ib_ndr_gbs_str      = FleetOrchestrationSetup.ib_ndr_gbs_str
a100_mem_str        = FleetOrchestrationSetup.a100_mem_str
h100_mem_str        = FleetOrchestrationSetup.h100_mem_str
# Also export gpu_hour_cost_usd and idle_fraction for inline use in prose
gpu_hour_cost_usd   = FleetOrchestrationSetup.gpu_hour_cost_usd
idle_fraction       = FleetOrchestrationSetup.idle_fraction
util_low            = FleetOrchestrationSetup.util_low
util_high           = FleetOrchestrationSetup.util_high
```

## The Scheduling Problem {#sec-fleet-orchestration-introduction}

The preceding chapters established how to distribute computation (@sec-distributed-training-systems), synchronize it efficiently (@sec-collective-communication), and recover when hardware fails (@sec-fault-tolerance-reliability). Fault tolerance keeps individual jobs alive; orchestration decides which jobs run, where they run, and how resources are shared among competing demands.

Imagine two research teams sharing a 1,000-GPU cluster: Team A submits a 512-GPU job that will run for a month, while Team B submits hundreds of 8-GPU experiments that each run for an hour. If the scheduler simply processes jobs as they arrive, Team B's tiny experiments might indefinitely block Team A's massive training run. The scheduling problem is the challenge of navigating this multi-dimensional conflict between throughput, fairness, and cluster utilization.

In the **Fleet Stack** (@sec-vol2-introduction), orchestration operates at the Distribution Layer, but its decisions are fundamentally constrained by the Infrastructure Layer. The scheduler must reason about physical realities: GPU heterogeneity, NVLink topology, rack power limits, and network bisection bandwidth. When scheduling goes wrong, debugging requires examining all four layers to identify whether the root cause is infrastructure (hardware constraints), distribution (algorithm limitations), serving (workload mismatch), or governance (policy misconfiguration). Orchestration is where physical infrastructure meets the demands of distributed algorithms, translating resource requests into placement decisions that respect both hardware topology and organizational policy.

To make the scheduling problem concrete, consider a research organization operating a `{python} cluster_size_str`-GPU cluster. Training GPT-3 required 1,024 A100 GPUs running continuously for 34 days [@brown2020language]. Imagine 100 such training jobs queued, alongside thousands of smaller experiments and inference workloads all competing for the same resources. How should the system decide which jobs run, when they run, and where they run? A naive first-come-first-served policy would let the first few large jobs monopolize the cluster for weeks, starving smaller experiments that researchers need to iterate quickly. A strict fair-share policy would fragment GPUs across many small allocations, preventing any large job from assembling the contiguous thousand-GPU block it needs. Neither extreme works. The scheduler must navigate a multi-dimensional trade-off space where every decision affects throughput, fairness, cost, and researcher productivity simultaneously.

The economic stakes make these decisions consequential at every scale. A `{python} cluster_size_str`-GPU cluster at $`{python} f"{gpu_hour_cost_usd:.0f}"` per GPU-hour costs $`{python} daily_cost_str` per day to operate, whether the GPUs are computing useful work or sitting idle in a queue. If scheduling inefficiencies leave `{python} f"{idle_fraction*100:.0f}"` percent of GPUs idle, that translates to $`{python} daily_waste_str` per day in wasted capacity, or over $`{python} annual_waste_str` million annually. Conversely, improving utilization from `{python} f"{util_low*100:.0f}"` percent to `{python} f"{util_high*100:.0f}"` percent effectively adds `{python} equivalent_gpus_str` GPUs worth of productive capacity without purchasing additional hardware. At this scale, a one-percentage-point improvement in utilization is worth more annually than the salary of the engineer who achieves it. Scheduling is not operational overhead; it is one of the highest-leverage engineering investments in ML infrastructure.

What makes ML scheduling fundamentally harder than traditional job scheduling are two requirements that conventional schedulers were never designed for. The first is all-or-nothing allocation: a distributed training job needing 1,024 GPUs cannot make progress with 1,023, so the scheduler must allocate the entire group atomically. The second is topology awareness: to support tensor parallelism, the scheduler must ensure GPUs are placed within the same high-bandwidth NVLink domain rather than scattered across the datacenter. Tools like Slurm (from the HPC world) and Kubernetes (from the cloud-native world) are the two primary vehicles for implementing these policies, and each makes different trade-offs that shape how these constraints are enforced.

ML workloads present scheduling challenges that distinguish them fundamentally from traditional high-performance computing and cloud computing. **Gang scheduling**[^fn-gang-scheduling] represents the most critical difference: a distributed training job requiring 1,024 GPUs cannot make progress with only 512. Unlike traditional HPC simulations that can often scale to whatever resources are available, synchronous data parallelism demands all-or-nothing allocation. Every worker must participate in every AllReduce operation, and a missing worker blocks all others. A scheduler that partially allocates resources creates deadlocks where multiple jobs each hold some GPUs while waiting for more, with none able to proceed. This all-or-nothing requirement means the scheduler cannot simply "pack jobs tightly" as a traditional bin packer would; it must reason about atomic, multi-resource allocations across the entire cluster.

[^fn-gang-scheduling]: **Gang Scheduling**: Formalized by John Ousterhout in 1982 for parallel systems, where the "Ousterhout matrix" co-schedules related threads across processors in synchronized time slices. In ML clusters, the constraint is stricter: synchronous AllReduce requires *all* workers simultaneously, so partial allocation wastes 100% of held resources rather than merely degrading throughput. A 1,024-GPU job holding 900 GPUs while waiting for 124 more burns approximately $1,800/hour in idle capacity. \index{Gang Scheduling!scheduling}

GPU heterogeneity adds a second challenging dimension. Modern clusters contain mixtures of A100 and H100 accelerators with different memory capacities (`{python} a100_mem_str` GB versus `{python} h100_mem_str` GB), compute throughput, and interconnect bandwidth. An H100 provides roughly twice the training throughput of an A100 for transformer workloads but costs proportionally more. Some models fit in A100 memory with appropriate parallelism strategies; others require the larger H100 memory to avoid excessive model sharding. The scheduler must match workloads to appropriate hardware based on actual requirements, not user preferences, while maintaining high utilization across heterogeneous pools. When users request specific GPU types out of habit rather than necessity, the mismatch between requested and required resources can leave entire hardware pools idle while queues overflow on preferred hardware.

Job duration unpredictability compounds these challenges further. A training run may converge early and complete in days rather than weeks. Hardware failures may abort jobs unexpectedly, returning resources at unpredictable times. Hyperparameter searches may reveal early that certain configurations will not succeed, leading to voluntary termination. Traditional scientific computing workloads, such as weather simulations or molecular dynamics, have more predictable runtimes that enable better scheduling decisions: the scheduler can project when resources will become available and plan accordingly. ML workloads deny this luxury, forcing schedulers to operate with deep uncertainty about when current jobs will release resources.

The interaction between these challenges creates combinatorial complexity. A scheduler must simultaneously handle gang scheduling constraints (atomic allocation), heterogeneous resources (capability matching), topology requirements (NVLink locality for tensor parallelism), priority and fairness policies (organizational equity), preemption decisions (which running jobs to interrupt), and cost optimization (spot versus on-demand placement). Each dimension constrains the others: a topology-optimal placement may violate fairness policies, a cost-optimal spot placement may violate reliability requirements, and a fairness-driven allocation may fragment resources in ways that prevent gang scheduling.

This chapter examines the algorithms and trade-offs for ML cluster orchestration. We begin with the core algorithmic challenges that make scheduling intrinsically hard, then explore the two dominant orchestration paradigms, Slurm and Kubernetes, and their ML-specific extensions. From there, we analyze topology-aware scheduling, elastic training, the economics of cost optimization, custom ML schedulers, serving resource management, and multi-tenant quota systems. A detailed debugging example integrates these concepts by diagnosing and resolving a realistic cluster utilization problem. By the end, you will understand how to design scheduling policies that maximize both efficiency and fairness for ML workloads at scale.

### The Scheduler's Challenge {#sec-fleet-orchestration-schedulers-challenge}

Why is cluster scheduling so hard? After all, operating systems have scheduled processes on CPUs for decades. The answer lies in a qualitative shift that occurs at fleet scale: the scheduling problem transitions from an optimization challenge (finding good assignments) to a distributed systems challenge (maintaining consistent state and coordinating decisions across thousands of machines). Before examining specific scheduling systems, understanding why cluster scheduling is intrinsically hard clarifies the design constraints that every solution must navigate.

### Distributed Scheduling Complexity {#sec-fleet-orchestration-distributed-scheduling}

A single-machine scheduler enjoys luxuries that a cluster scheduler cannot: it has instantaneous, consistent visibility into all resource states; it can make atomic decisions that take effect immediately; and failures are binary (the machine is up or it is down). Cluster scheduling surrenders all three of these properties, and several fundamental distributed systems problems emerge as a result.

Partial failures pose the first challenge. A node can fail between allocation and job start, creating a gap between the scheduler's decision and its effect. The scheduler may successfully allocate 32 GPUs across 4 nodes, only to have one node fail before the job launches. The remaining 24 GPUs sit idle while the scheduler detects the failure, updates its state, and re-plans the allocation. Meanwhile, other jobs that could have used those 24 GPUs have already been placed elsewhere, potentially on suboptimal hardware. The fault tolerance mechanisms from @sec-fault-tolerance-reliability handle failures *during* execution; the scheduler must handle failures *during* placement, a fundamentally different problem because the job has not yet established any state to recover from.

Network partitions create a second problem that is unique to distributed scheduling. The scheduler may lose connectivity to a subset of nodes while those nodes continue operating normally. From the scheduler's perspective, the nodes appear failed and their GPUs appear unavailable. From the nodes' perspective, jobs may still be running and producing useful work. This ambiguity creates a dilemma with no universally correct resolution: should the scheduler reallocate those GPUs to new jobs, risking double-allocation if the partition heals? Or should it wait for reconnection, wasting capacity that may be perfectly functional? The duration of the partition is unknowable in advance, so any fixed timeout represents a guess about network behavior that may prove wrong.

State inconsistency emerges as a third challenge that compounds the first two. Resource state may differ between the scheduler's view and reality on individual nodes. A GPU the scheduler believes is free may still be cleaning up from a previous job: flushing caches, deallocating device memory, running a zombie process from a failed container, or completing a CUDA driver reset after an error. Conversely, a GPU marked as "in use" may have been freed by a job that terminated between heartbeat intervals. This inconsistency means the scheduler operates on a *model* of cluster state that is always slightly stale, and the degree of staleness varies across nodes depending on heartbeat frequency, network latency, and the complexity of cleanup operations.

Ordering without global time presents the fourth fundamental issue. Without a global clock, determining whether allocation A happened before allocation B across different nodes requires careful protocol design using logical clocks or consensus algorithms. Two jobs may both believe they "own" the same GPU if the system does not enforce ordering through consensus protocols or centralized coordination. This scenario is not theoretical: during recovery from a network partition, allocation messages from before and after the partition may arrive in the wrong order at different nodes, creating conflicting resource assignments that must be detected and resolved.

These four challenges connect directly to the **CAP theorem**[^fn-cap-scheduling], which applies to cluster scheduling with particular force. A scheduler cannot simultaneously provide perfect consistency (every allocation reflects true cluster state), availability (every scheduling request gets a response), and partition tolerance (the system continues operating despite network failures). Production schedulers make different trade-offs along this spectrum, and those trade-offs shape their behavior in fundamental ways. Slurm prioritizes consistency, blocking allocations during uncertainty rather than risking conflicting assignments. This means Slurm may fail to schedule jobs during transient network issues, but the jobs it does schedule always have valid, non-conflicting resource assignments. Kubernetes prioritizes availability, using eventual consistency with reconciliation loops that continuously drive actual state toward desired state. This means Kubernetes remains responsive during partial failures, but may temporarily have inconsistent views of resource allocation that are resolved asynchronously. Custom ML schedulers often accept bounded inconsistency in exchange for scheduling throughput, using optimistic concurrency where conflicts are detected and resolved after the fact rather than prevented through locking.

[^fn-cap-scheduling]: **CAP Theorem**: Conjectured by Eric Brewer (2000) and proven by Gilbert and Lynch (2002), CAP establishes that no distributed system can simultaneously guarantee Consistency, Availability, and Partition tolerance. For ML schedulers, this trade-off is acute: Slurm sacrifices availability (blocking allocations during uncertainty) to prevent double-booking GPUs worth thousands of dollars per hour, while Kubernetes sacrifices consistency (allowing temporary resource overcommit) to keep scheduling responsive during partial failures. \index{CAP Theorem!scheduling}

### Failure Rates at Scale {#sec-fleet-orchestration-failure-rates}

At scale, failure is normal operation, not exceptional. This principle, established in the reliability analysis of @sec-fault-tolerance-reliability, has direct consequences for scheduling: the scheduler must not merely tolerate failure but actively plan for it in every allocation decision. Component reliability does not change with cluster size, but aggregate system reliability degrades multiplicatively. With 99.9 percent annual GPU reliability (typical for datacenter hardware), a 4,096-GPU cluster experiences:

$$ \text{Expected failures per day} = 4096 \times \frac{0.001}{365} \approx 0.01 \text{ GPU failures/day} $$

This calculation captures only GPU hardware failures. More realistically, including software failures (driver crashes, CUDA context corruption), thermal events (throttling, emergency shutdowns), network interface failures, host OS issues, and container runtime errors, production clusters see 1 to 4 failures per day per 1,000 GPUs. A `{python} cluster_size_str`-GPU cluster thus experiences 10 to 40 component failures daily. A multi-week training run on 4,096 GPUs will encounter multiple failures with near certainty.

The scheduling implications are profound and shape nearly every design decision in the scheduler. The scheduler cannot treat the cluster as a static resource pool where resources, once allocated, remain available until voluntarily released. Instead, it must anticipate that allocated resources will disappear during job execution, maintain spare capacity or implement rapid re-scheduling to keep jobs running through the constant churn of hardware entering and leaving operational status, and distinguish between transient failures (which may self-resolve within minutes and should not trigger expensive reallocation) and permanent failures (which require new resource allocation and checkpoint recovery).

Consider the scheduler's decision when a node heartbeat is missed. The scheduler has three options: (1) wait and see if the heartbeat resumes (risking wasted GPU time if the node is truly dead), (2) immediately reallocate the node's resources to other jobs (risking conflict if the node recovers and its original jobs are still running), or (3) mark the node as suspect and begin pre-positioning replacement resources while waiting for confirmation (consuming spare capacity that could be used for other work). Each option trades off between responsiveness and correctness, and the optimal choice depends on the failure mode distribution for the specific hardware in the cluster, information the scheduler must learn from historical failure data.

These requirements connect the scheduler directly to the checkpoint and recovery infrastructure from @sec-fault-tolerance-reliability: scheduling decisions about spare capacity, preemption policies, and elastic training support determine how quickly the system recovers from the failures that are statistically guaranteed to occur. A well-designed scheduler that maintains 5 percent spare capacity and supports elastic recovery can restore training throughput within minutes of a failure, while a poorly designed scheduler that has no spare capacity and requires full job restart may waste hours of GPU time per failure event.

### Scheduling Objectives and Their Conflicts {#sec-fleet-orchestration-objectives}

Every scheduling decision represents a trade-off between four fundamental and often contradictory objectives. **Throughput** measures the total useful work completed per unit time, naturally favoring large, long-running jobs that saturate hardware and minimize the overhead of context switching and data movement. A throughput-maximal policy would fill the cluster with massive training runs, achieving near-perfect arithmetic intensity but forcing every other user to wait weeks for a slot. **Fairness**, conversely, measures the equitable distribution of resources across users or teams, often implementing policies like dominant resource fairness to ensure that a single large team cannot monopolize the fleet. This inevitably fragments resources, leaving small gaps that cannot be filled by large jobs, thereby sacrificing aggregate throughput for social harmony.

**Latency** creates a third tension, prioritizing the rapid completion of short jobs to maintain high researcher velocity. A latency-optimal scheduler, similar to Shortest Job First, would aggressively preempt long-running training jobs to service interactive debugging sessions or small experiments. While this minimizes average wait time and clears queues quickly, it can lead to starvation for the large training jobs that are often the organization's most critical deliverables. Finally, **cost efficiency** introduces financial constraints, favoring the use of interruptible spot instances, off-peak scheduling, and packing jobs tightly to minimize fragmentation. Optimizing for cost often means accepting higher failure rates and longer completion times, trading researcher productivity for budget preservation.

Optimizing all four simultaneously is impossible; every scheduling policy represents a specific point in this four-dimensional trade-off space. The most direct conflict exists between throughput and latency, a tension analogous to the throughput-latency trade-off in computer architecture. A throughput-optimal scheduler prioritizes the largest, most parallelizable jobs because they utilize the hardware most efficiently. However, this policy is latency-catastrophic for small jobs: a researcher submitting a 10-minute debugging task might wait days for the large job to finish. Conversely, a latency-optimal scheduler minimizes average wait time but potentially starves large jobs indefinitely, reducing aggregate cluster throughput by leaving large blocks of resources idle while waiting for "just one more" small job to finish.

Consider our 175B model training on 64 GPUs. From a throughput perspective, this is an ideal job: it runs for weeks with high arithmetic intensity and zero scheduling overhead once started. From a latency perspective, it is a boulder in the stream. To schedule it, the system might need to drain 64 GPUs of all other work, forcing hundreds of smaller jobs to wait. Once running, it occupies those resources immovably. If the scheduler prioritizes the 175B model (throughput), the P99 latency for small jobs explodes. If it prioritizes small jobs (latency) by allowing them to preempt or fragmentation-fill the cluster, the 175B model may never assemble the contiguous block it needs to start. This zero-sum game forces organizations to make explicit policy choices -- often encoded in Quality of Service classes -- about which objective wins when they collide.

::: {.callout-notebook title="The Queuing Theory of GPU Clusters"}

A GPU cluster can be modeled as an M/G/1 queue, where jobs arrive according to a Poisson process ($\lambda$) and service times follow a general distribution ($G$) with mean $1/\mu$ and standard deviation $\sigma$. The Pollaczek-Khinchine formula defines the expected waiting time in the queue ($W_q$):

$$ W_q = \frac{\rho}{1-\rho} \cdot \frac{1+C_s^2}{2\mu} $$

Here, $\rho = \lambda/\mu$ represents cluster utilization, and $C_s = \sigma \mu$ is the **coefficient of variation** of job duration.

In standard web serving, requests are roughly uniform, yielding $C_s \approx 1$. In ML clusters, job durations follow a heavy-tailed distribution: a vast number of short debugging jobs (minutes) mixed with a few massive training runs (weeks). This extreme variance typically yields $C_s$ values between 3 and 5. The impact on wait times is multiplicative.

At 80 percent utilization ($\rho = 0.8$): with a uniform workload ($C_s = 1$), $W_q = 4 \times$ the average job duration. With a typical ML workload ($C_s = 3$), $W_q = 20 \times$ the average job duration. This explains the "utilization wall" in ML infrastructure. A web server cluster feels responsive at 80 percent load, but an ML cluster at the same utilization feels broken, with jobs languishing in the queue for days. The heavy tail of the service distribution acts as a latency multiplier, forcing operators to run ML clusters at lower utilization (often 60 to 70 percent) to maintain acceptable responsiveness for interactive users.

:::

### Bin Packing {#sec-fleet-orchestration-bin-packing}

The most fundamental scheduling algorithm is **bin packing**[^fn-bin-packing-scheduling]: fitting jobs of varying sizes into fixed-capacity nodes. This problem is NP-hard in its general form, meaning no known algorithm finds optimal solutions in polynomial time. Fortunately, practical heuristics such as first-fit decreasing and best-fit decreasing achieve near-optimal results for the workload distributions typical of ML clusters, where job sizes follow a heavy-tailed distribution (many small jobs, few large ones).

[^fn-bin-packing-scheduling]: **Bin Packing**: The one-dimensional version is NP-hard; ML scheduling adds four to five dimensions (GPU, CPU, memory, network, topology), making exact solutions intractable for clusters above a few hundred nodes. First-fit-decreasing heuristics achieve within 11/9 of optimal for typical workloads, but the real cost in ML clusters is not suboptimality in packing but *fragmentation*: stranded GPUs that individually satisfy no pending job yet collectively represent millions of dollars in idle capacity. \index{Bin Packing!scheduling}

Consider a 64-node cluster with 8 GPUs per node, totaling `{python} total_gpus_str` GPUs. If jobs request 6 GPUs each, each job occupies one full node but wastes 2 GPUs per node, reducing effective capacity to `{python} f"{100 - float(frag_pct_str):.0f}"` percent. The remaining 2 GPUs per node cannot be combined across nodes because GPU workloads require local memory access. This **fragmentation** grows worse with heterogeneous job sizes: a mix of 1-GPU, 3-GPU, and 7-GPU jobs creates irregular gaps distributed across many nodes, where no single pending job can fit into any individual gap, yet the total free capacity would be sufficient if the gaps were contiguous.

ML workloads make bin packing multi-dimensional in ways that traditional scheduling rarely encounters. Each job requires not just GPUs but also CPU cores for data preprocessing, host memory for data loading and augmentation pipelines, local SSD storage for dataset caching, and network bandwidth for gradient synchronization. A job requesting 4 GPUs, 32 CPU cores, and 256 GB of RAM may not fit on a node that has 4 free GPUs but only 16 free CPU cores because other jobs have consumed the host CPU for preprocessing. The scheduler must simultaneously satisfy all resource dimensions, and the job fits only if *every* dimension has sufficient capacity on the selected node. This multi-dimensional constraint dramatically reduces the solution space compared to single-dimensional packing, because a bottleneck in any single resource dimension can strand capacity in all others.

Locality constraints further restrict placement beyond simple resource availability. A training job using tensor parallelism requires GPUs connected via NVLink within the same node, since cross-node communication over InfiniBand is an order of magnitude slower (`{python} nvlink_h100_str` GB/s intra-node versus `{python} ib_ndr_gbs_str` GB/s inter-node). A job requesting "4 GPUs with NVLink connectivity" cannot use 2 GPUs from node A and 2 from node B, even if all 4 GPUs are individually available and the total capacity is sufficient. These topology constraints transform a packing problem into a placement problem where *which* specific resources matter as much as *how many* resources are available. The placement problem is strictly harder than the packing problem because it adds spatial constraints to the existing capacity constraints.

Production schedulers address fragmentation through several complementary strategies, each targeting a different aspect of the problem.

**Backfill scheduling** allows smaller jobs to fill gaps while larger jobs wait for contiguous resources, improving utilization without violating priority ordering. The insight is that small jobs can execute and complete in the gaps, freeing those resources before the large job's target start time. Backfill scheduling requires estimating when current jobs will complete (to determine whether a backfill candidate will finish before the large job can start), which is why accurate runtime estimates are so important and why Tiresias' approach of eliminating runtime estimates (discussed in @sec-fleet-orchestration-tiresias) represents a fundamental alternative.

**Defragmentation** periodically migrates or preempts low-priority jobs to consolidate free resources into contiguous blocks, analogous to memory compaction in operating systems. The scheduler identifies nodes where partial allocations leave stranded resources, preempts the jobs occupying those resources, and re-schedules them on nodes where they can be packed more efficiently. The cost of defragmentation (preempting running work, which wastes compute between the last checkpoint and the preemption event) must be weighed against the benefit (enabling large jobs to start sooner, improving overall throughput). Production systems typically run defragmentation during low-demand periods (late night, weekends) when the impact of preemption is minimal.

**Over-subscription** allows more jobs to be admitted than strictly fit, relying on statistical multiplexing to avoid simultaneous peak usage across all resource dimensions. If each job's GPU utilization averages 70 percent (alternating between compute-intensive and data-loading phases), the cluster can support approximately 1.4$\times$ as many jobs as the strict GPU count would allow. Over-subscription works well when resource utilization is genuinely bursty and jobs' peak usage periods are uncorrelated, but can cause severe performance degradation (thrashing, memory pressure, network congestion) when multiple jobs peak simultaneously. The key to safe over-subscription is monitoring actual utilization in real time and throttling admission when contention is detected.

::: {#fig-fragmentation-problem fig-env="figure" fig-pos="htb" fig-cap="The Fragmentation Problem. The heatmap shows a snapshot of an 8-node cluster (rows) with 8 GPUs each (columns). Different colors represent distinct active jobs, while gray slots represent free GPUs. Although 19 GPUs (~30% of capacity) are available globally, they are fragmented across nodes. A new job requiring 8 contiguous GPUs (a full node) cannot be scheduled because no single row is completely empty, illustrating how bin-packing inefficiencies reduce effective cluster capacity." fig-alt="8-by-8 heatmap of cluster GPU allocation. Rows are nodes, columns are GPU slots. Colored cells show jobs; gray shows free. Status panel indicates 19 free GPUs but no contiguous block of 8."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FRAGMENTATION PROBLEM (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-fragmentation-problem — bin-packing and over-subscription
# │
# │ Goal: Heatmap of 8×8 cluster; show fragmented free slots; illustrate why
# │       8-contiguous-GPU job cannot be placed despite 30% free.
# │ Show: pcolormesh; job colors; gray = free; status text.
# │ How: row_patterns grid; ListedColormap; matplotlib.
# │
# │ Imports: matplotlib.pyplot (plt), matplotlib.colors (mcolors),
# │          matplotlib.patches (mpatches), numpy (np)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.patches as mpatches
import numpy as np

plt.style.use('seaborn-v0_8-whitegrid')

n_nodes = 8
gpus_per_node = 8

grid = np.zeros((n_nodes, gpus_per_node))

row_patterns = [
    [1, 1, 0, 0, 2, 2, 2, 0],
    [3, 3, 3, 3, 0, 0, 4, 4],
    [5, 5, 0, 6, 6, 6, 6, 6],
    [7, 7, 7, 0, 0, 8, 8, 8],
    [9, 9, 0, 0, 0, 10, 10, 10],
    [11, 11, 11, 11, 0, 0, 12, 12],
    [13, 13, 13, 13, 13, 0, 0, 0],
    [14, 14, 14, 14, 15, 0, 0, 0]
]
grid = np.array(row_patterns)

fig, ax = plt.subplots(figsize=(10, 5))

cmap_name = 'tab20'
base_cmap = plt.cm.get_cmap(cmap_name)
colors = ['#E0E0E0'] + [base_cmap(i/20) for i in range(20)]
cmap = mcolors.ListedColormap(colors)

c = ax.pcolormesh(grid, cmap=cmap, edgecolors='white', linewidth=1.5, vmin=0, vmax=20)

ax.invert_yaxis()

ax.set_title('Cluster Fragmentation State', fontsize=13, pad=10, fontweight='bold')
ax.set_xlabel('GPU Slot (Local Rank)', fontsize=11)
ax.set_ylabel('Node ID', fontsize=11)

ax.set_xticks(np.arange(gpus_per_node) + 0.5)
ax.set_xticklabels(np.arange(gpus_per_node))
ax.set_yticks(np.arange(n_nodes) + 0.5)
ax.set_yticklabels([f'Node {i}' for i in range(n_nodes)])

ax.grid(False)

props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.95, edgecolor='#cccccc')
status_text = (
    "SYSTEM STATUS\n"
    "──────────────\n"
    "Total GPUs:  64\n"
    "Allocated:   45 (70%)\n"
    "Free:        19 (30%)\n\n"
    "PENDING REQUEST\n"
    "──────────────\n"
    "Size: 8 GPUs (1 Node)\n"
    "Result: BLOCKED\n"
    "(No contiguous block)"
)
ax.text(8.5, 0, status_text, fontsize=10, verticalalignment='top', bbox=props, fontfamily='monospace')

legend_patches = [
    mpatches.Patch(facecolor='#E0E0E0', edgecolor='#999999', label='Free Slot'),
    mpatches.Patch(facecolor=base_cmap(0.05), edgecolor='white', label='Occupied (Job)')
]
ax.legend(handles=legend_patches, loc='lower right', bbox_to_anchor=(1.35, 0))

plt.subplots_adjust(right=0.75)

fig = plt.gcf()
```
:::

### Gang Scheduling {#sec-fleet-orchestration-gang-scheduling}

Gang scheduling addresses the most distinctive aspect of ML workload scheduling: the requirement for atomic, all-or-nothing resource allocation. The term comes from the idea that all members of a "gang" (the workers in a distributed training job) must be present before any useful work can begin. Gang scheduling ensures that all $N$ workers for a distributed training job start simultaneously. Without this guarantee, a job requesting 1,024 GPUs might receive 768 immediately while waiting indefinitely for the remaining 256, wasting the already-allocated resources. Worse, if two such jobs each receive partial allocations, neither can proceed, creating a classic **hold-and-wait deadlock**[^fn-deadlock-scheduling].

[^fn-deadlock-scheduling]: **Hold-and-Wait Deadlock**: One of the four conditions identified by Coffman et al. (1971) that are jointly sufficient for deadlock. In GPU clusters, this condition is uniquely expensive: two jobs each holding 500 GPUs while waiting for 200 more strand 1,000 GPUs indefinitely, burning approximately $2,000/hour. Gang scheduling eliminates hold-and-wait by construction, requiring atomic all-or-nothing allocation at the cost of reduced packing flexibility. \index{Deadlock!scheduling}

The formal requirement is straightforward: for a job $J$ requesting $N$ GPUs, the scheduler must guarantee that either all $N$ GPUs are allocated atomically, or the job remains in the queue without holding any resources. This binary outcome eliminates deadlock by construction, since a job that holds no resources cannot block other jobs. Implementing this guarantee efficiently, however, is the challenge that defines much of cluster scheduling algorithm design.

Naive gang scheduling is wasteful in predictable ways. If the cluster has 900 free GPUs and a job requests 1,024, the 900 GPUs sit idle until 124 more become available, potentially wasting thousands of GPU-hours. **Backfill scheduling** addresses this by identifying jobs in the queue that can fit within the 900 available GPUs without delaying the large job's expected start time. A 128-GPU job with estimated runtime of 2 hours can safely backfill if the 124 additional GPUs needed by the large job will become available within 2 hours anyway (from other completing jobs). The backfilled job uses resources that would otherwise be idle, improving utilization without violating the large job's scheduling guarantee.

The accuracy of runtime estimates determines backfill effectiveness, and this creates a game-theoretic challenge. If users consistently overestimate runtimes, backfill slots are artificially narrow and fewer jobs fit, reducing utilization. If users underestimate, backfilled jobs may still be running when the large job's resources become available, forcing preemption that wastes the backfilled job's progress. In practice, users have strong incentives to overestimate (to avoid being killed before completion) and weak incentives to estimate accurately, leading to systematic inflation that degrades backfill performance. Research schedulers like Tiresias [@gu2019tiresias] address this fundamental problem by eliminating the requirement for runtime estimates entirely, instead using observed resource consumption to dynamically adjust priority. This approach, discussed in detail in @sec-fleet-orchestration-tiresias, turns the runtime estimation problem from a user-facing burden into a system-level observation.

::: {.callout-perspective title="The Economics of Idle GPUs"}

The cost of poor scheduling compounds rapidly. Consider a `{python} cluster_size_str`-GPU cluster:

- **Operating cost**: $`{python} daily_cost_str`/day ($`{python} annual_cost_str`M/year)
- **At 60% utilization**: 4,000 GPUs productive, 6,000 idle = \$288,000/day wasted
- **At 80% utilization**: 8,000 GPUs productive, 2,000 idle = \$96,000/day wasted
- **Improvement value**: Moving from 60% to 80% saves \$192,000/day, or \$70M/year

This savings exceeds the cost of a dedicated scheduling engineering team by orders of magnitude. Every percentage point of utilization improvement on a large cluster translates to millions of dollars annually. Scheduling is not overhead; it is one of the highest-leverage engineering investments in ML infrastructure.

:::

### Deadlock Prevention and Detection {#sec-fleet-orchestration-deadlock}

While gang scheduling eliminates the **hold-and-wait** condition -- one of the four formal Coffman conditions for deadlock -- it does not immunize the cluster against all forms of resource contention. Deadlocks can still emerge from the interaction of priority rules, preemption policies, and auxiliary resource dependencies. In a cluster with thousands of GPUs and petabytes of state, these edge cases transition from theoretical curiosities to daily operational incidents.

The most pernicious of these is **priority inversion**, a scenario borrowed from real-time systems where a high-priority job is indefinitely blocked by a low-priority job. Consider a 175B parameter training run (high priority) requesting a gang of 64 GPUs. The scheduler has reserved 60 available GPUs, but the remaining 4 are held by a low-priority data processing job. Normally, the scheduler would preempt the low-priority job to satisfy the high-priority request. However, if a stream of medium-priority development jobs saturates the cluster's CPU or network bandwidth -- resources the low-priority job needs to checkpoint and exit -- the low-priority job stalls. It cannot release the GPUs because it cannot complete its exit sequence. The high-priority job waits for the low-priority job, which is effectively blocked by the medium-priority jobs. The result is a 60-GPU idle block that persists until an operator manually intervenes.

::: {.callout-definition title="Priority Inversion"}

***Priority Inversion***\index{Priority Inversion!definition} is a scheduling pathology in which a high-priority task is forced to wait for a lower-priority task to release a shared resource.

1.  **Significance (Quantitative):** It reduces the progress rate of the entire fleet to that of the lowest-priority job. In ML clusters, this typically occurs when a low-priority job holding GPUs is starved of auxiliary resources (e.g., $BW$ for checkpointing), preventing it from finishing and releasing the accelerators needed by high-priority workloads.
2.  **Distinction (Durable):** Unlike **Standard Queuing** (where tasks wait their turn), Priority Inversion involves an **Active Blockage**: the high-priority task is ready to run but is transitively dependent on a task that the scheduler does not prioritize.
3.  **Common Pitfall:** A frequent misconception is that strict priority levels solve this. In reality, without **Holistic Preemption** (reserving all resources needed for a task to exit), increasing priority levels can actually *increase* the likelihood of inversion by creating more complex dependency chains.

:::

Solving this requires a choice between prevention and detection. Prevention mechanisms, like strict gang scheduling and aggressive timeouts, eliminate deadlock states by construction but often sacrifice utilization. Detection mechanisms allow the system to enter potentially unsafe states to maximize throughput, relying on monitoring to identify and resolve deadlocks when they occur.

For large-scale fleets, detection is computationally non-trivial. A cluster with 10,000 GPUs and 500 pending jobs has a state space exceeding $10^{15}$ possible allocations. Constructing and traversing a global "wait-for" graph to detect cycles in real time is often intractable. Production systems therefore rely on heuristics. **Lease-based resources** assign all allocations a time-to-live (TTL); if a job effectively deadlocks, its lease expires, and the scheduler reclaims the GPUs. **Progress monitoring** flags jobs whose GPU utilization drops to zero for a threshold period (e.g., 10 minutes) as "zombies" and terminates them regardless of their reported status. **Holistic preemption** reserves all necessary resources (CPU, network, storage bandwidth) for the victim to exit gracefully when preempting for high-priority jobs, preventing the priority inversion trap. These strategies accept a non-zero rate of false positives (killing healthy but slow jobs) to guarantee cluster liveness.

The tension between gang scheduling's safety guarantees and backfill's utilization gains defines the core trade-off in training cluster scheduling. Gang scheduling prevents deadlock but wastes resources; backfill improves utilization but requires runtime estimates that users cannot accurately provide. The next section examines how the two dominant orchestration paradigms resolve this tension through fundamentally different architectural philosophies.

## Orchestration Paradigms {#sec-fleet-orchestration-paradigms}

When a researcher needs 64 GPUs, should they submit a script detailing exactly how many nodes they want (imperative), or should they declare their desired state and let a control loop figure out the provisioning (declarative)? Two dominant philosophies have emerged for cluster orchestration—the HPC-oriented Slurm and the cloud-native Kubernetes—each reflecting the culture and requirements of its origin domain.

The fundamental distinction is between *imperative* and *declarative* resource management. In an imperative system, the user specifies exactly what resources they need and the system allocates them directly. In a declarative system, the user specifies what they want running and the system figures out how to make it happen. This distinction, familiar from programming language design, has profound consequences for how ML workloads are scheduled, monitored, and recovered from failure.

### Slurm: The HPC Heritage {#sec-fleet-orchestration-slurm}

**Slurm**[^fn-slurm-hpc] (Simple Linux Utility for Resource Management) [@yoo2003slurm] dominates HPC environments and extends naturally to GPU-intensive ML training. Originating from Lawrence Livermore National Laboratory in 2002, Slurm was designed for the distinctive requirements of scientific computing: long-running batch jobs, expensive shared hardware, and users who understand their resource needs. Its partition-based architecture maps well to heterogeneous accelerator pools, and its decades of deployment in national laboratories, university research clusters, and increasingly in industry training infrastructure have produced a mature, well-understood system.

[^fn-slurm-hpc]: **Slurm (Simple Linux Utility for Resource Management)**: Developed at Lawrence Livermore National Laboratory beginning in 2002, Slurm's centralized controller (`slurmctld`) provides strong consistency guarantees by maintaining a single authoritative view of all resource allocations. This architecture scales to approximately 10,000 nodes before scheduling throughput becomes a bottleneck, which is why the largest ML training clusters (10,000+ GPUs) often partition into multiple Slurm federations. \index{Slurm!scheduling}

Slurm's scheduling model is fundamentally **imperative**: users submit job scripts specifying exact resource requirements, and the scheduler places jobs into partitions based on priority, fairness, and resource availability. This directness makes reasoning about allocation guarantees straightforward. When a user submits `sbatch --gres=gpu:8 --nodes=4`, Slurm guarantees that if the job starts, it will have exactly 32 GPUs across 4 nodes. The user knows precisely what they are getting, and the scheduler knows precisely what it must provide. The cost of this directness is that users must understand their resource needs precisely; requesting more than needed wastes resources, while requesting less causes out-of-memory errors or reduced performance.

Slurm's architecture consists of a central controller daemon (`slurmctld`) that maintains the global view of cluster state and makes all scheduling decisions, and per-node daemons (`slurmd`) that manage local resources and execute jobs. This centralized architecture provides the strong consistency guarantees discussed in @sec-fleet-orchestration-distributed-scheduling: the controller has authoritative knowledge of every resource allocation, preventing the double-allocation problems that plague eventually-consistent systems. The cost is a potential single point of failure (addressed through active-passive failover) and a scheduling throughput limit (the controller must evaluate every scheduling decision sequentially). For clusters up to approximately 10,000 nodes, this centralized throughput is sufficient; beyond that, scheduling latency becomes measurable and organizations may need to partition the cluster into multiple Slurm federations.

A typical ML cluster configuration defines partitions by accelerator type and interconnect:

| **Partition** | **GPUs/Node**   | **Interconnect** | **Typical Use**    |
|:--------------|:----------------|:-----------------|:-------------------|
| **dgx-a100**  | 8$\times$ A100 | NVLink + IB NDR  | Large LLM training |
| **a100-pcie** | 4$\times$ A100 | PCIe + IB HDR    | Medium training    |
| **inference** | 2$\times$ A10G | Ethernet         | Model serving      |
| **debug**     | 1$\times$ V100 | Ethernet         | Development        |

: **Slurm Partition Configuration**: Partitions organize heterogeneous accelerators into logical pools matched to workload characteristics. NVLink-connected partitions support tensor parallelism, while PCIe partitions serve workloads that rely primarily on data parallelism. Separating inference and debug partitions prevents experimental workloads from impacting production serving. {#tbl-fleet-orchestration-slurm-partitions}

GPU allocation strategies significantly impact utilization, and Slurm provides several mechanisms for controlling GPU placement. The `--gres=gpu:N` flag requests N GPUs, but naive allocation can fragment nodes. If jobs request 6 GPUs on 8-GPU nodes, each job wastes 2 GPUs per node, reducing effective capacity to 75 percent. Slurm's `SelectTypeParameters=CR_GPU` enables GPU-level scheduling, tracking individual GPU availability rather than treating nodes as indivisible units. The `--gpus-per-node` flag ensures jobs receive full nodes when NVLink communication patterns make partial-node allocation counterproductive, while `--gpus-per-task` distributes GPUs evenly across tasks for data-parallel workloads.

The interaction between GPU allocation and node selection creates subtleties that affect both performance and utilization. A job requesting `--gpus=16 --gpus-per-node=8` will always receive exactly 2 complete nodes, ensuring all 8 GPUs within each node communicate via NVLink. The same job requesting `--gpus=16` without the per-node constraint might receive GPUs spread across 3 or 4 partially occupied nodes, degrading intra-node communication performance. For training jobs using tensor parallelism, the per-node constraint is essential; for data-parallel jobs that communicate only through AllReduce, the flexibility of unconstrained placement improves the scheduler's ability to find valid allocations and reduces fragmentation.

**Fair-share scheduling** prevents any single user or project from monopolizing resources over time. The core insight is that past usage should affect future priority: heavy users should yield to lighter users when the cluster is contended. The classic fair-share formula computes effective priority as:

$$ P_{effective} = P_{base} \times \frac{F_{target}}{F_{actual} + \epsilon} $$ {#eq-fleet-orchestration-fair-share}

where $F_{target}$ represents the user's allocated share (determined by organizational policy), $F_{actual}$ their recent resource consumption (measured in GPU-hours over a configurable time window), and $\epsilon$ is a small constant preventing division by zero for new users. This formula naturally deprioritizes heavy users while allowing burst access when resources are idle. A researcher who has consumed twice their fair share sees their priority halved, pushing new submissions behind colleagues who have used less. Conversely, a researcher who has used no resources recently receives maximum priority, allowing rapid access when they return to active work.

The time decay of usage history determines how quickly the system "forgives" past heavy usage. A half-life of one week means that a researcher's heavy usage from two weeks ago contributes only 25 percent to their current usage calculation. Short half-lives create rapid rebalancing but can lead to oscillatory behavior where users alternate between starving and gorging on resources. Long half-lives create stable priority ordering but may penalize researchers who completed a legitimate large project and now want resources for a smaller one. Most production clusters configure half-lives between 3 and 14 days, balancing responsiveness against stability.

**Preemption policies** enable high-priority jobs to reclaim resources from running workloads, and for ML training, this requires careful coordination with the checkpoint infrastructure. Jobs receive `SIGTERM` with configurable grace periods, typically 60 to 300 seconds, to save checkpoints before receiving `SIGKILL`. The `PreemptMode=REQUEUE` setting automatically restarts preempted jobs from their last checkpoint, while `GraceTime` controls the checkpoint window. The checkpoint and recovery infrastructure developed in @sec-fault-tolerance-reliability makes this preemption practical; without reliable checkpointing, preemption would lose all progress since the last manually triggered save, making preemption economically ruinous for long-running training jobs.

Preemption introduces a scheduling paradox: it improves the scheduler's ability to serve high-priority work but can degrade overall cluster throughput. Every preemption wastes the compute between the last checkpoint and the preemption event, plus the time to restart and reload from checkpoint. If checkpoint intervals are long (e.g., hourly) and preemptions are frequent, the wasted compute can be substantial. Production systems balance preemption frequency against checkpoint overhead, often configuring preemption cooldown periods that prevent the same job from being preempted more than once within a configurable interval.

Slurm's strengths for ML training are clear: predictable allocation guarantees, mature fair-share policies, straightforward integration with MPI-based distributed training frameworks, and decades of operational experience in managing large-scale scientific computing. Its weaknesses emerge for inference workloads and mixed training-serving clusters, where the batch-oriented model struggles with the dynamic, latency-sensitive nature of serving traffic. Slurm has no native concept of "a service that should always be running," making it awkward for inference endpoints that must respond to requests continuously. Adding or removing inference replicas requires submitting or canceling Slurm jobs, a much heavier operation than scaling a Kubernetes deployment.

### Advanced Slurm Configuration for ML {#sec-fleet-orchestration-slurm-advanced}

While standard partitions and fair-share policies handle basic batch workloads, optimizing Slurm for large-scale deep learning requires configuring logic that understands the specific parallelism and failure modes of distributed training. Production ML clusters rely on three advanced capabilities -- job arrays, heterogeneous steps, and lifecycle hooks -- to bridge the gap between HPC conventions and AI requirements.

**Job arrays and hyperparameter sweeps** transform the chaotic submission of thousands of experimental trials into a coherent, schedulable unit. Instead of flooding the controller with individual `sbatch` requests, a user submits a single array job: `#SBATCH --array=0-99`. Slurm treats this as a single object for parsing and queuing overhead but schedules each element as an independent task, allowing the scheduler to backfill small holes in the cluster with individual trials. Each task receives a unique `SLURM_ARRAY_TASK_ID` environment variable, which the training script uses to index into a hyperparameter configuration file (e.g., selecting learning rate $\lambda = 10^{-4}$ for task 0 and $\lambda = 3 \times 10^{-4}$ for task 1). This mechanism is the standard for large-scale ablation studies, enabling researchers to launch 1,000 experiments with a single command while preserving scheduler throughput.

**Heterogeneous job steps** address the preprocessing bottleneck where expensive GPUs sit idle while CPUs prepare data. Traditional jobs allocate symmetric resources to every node, forcing users to reserve GPU nodes for the entire pipeline. Slurm's `--het-group` feature enables a single job to span disparate resource types, allocating CPU-only nodes for data ingestion and GPU nodes for training within the same allocation. Our 175B model's training job uses this pattern: it requests a heterogeneous allocation of 8 GPU nodes (64 A100s) for the model and 2 CPU-only nodes for on-the-fly data tokenization. The `srun --het-group=0` command launches the training process on the GPUs, while `srun --het-group=1` launches the data workers, allowing the expensive accelerators to focus purely on gradient computation while cheaper CPU nodes feed them data.

**Prolog and epilog scripts** serve as the cluster's immune system, running administrative code before a job starts and after it finishes. In ML clusters, the standard prolog script performs a pre-flight check on allocated hardware: it validates NVLink topology (using `nvidia-smi topo -m`), checks ECC error counters, and verifies InfiniBand link width. If a node fails these checks, the prolog returns a non-zero exit code, causing Slurm to drain the node and requeue the job elsewhere, preventing a silent hardware fault from corrupting a week-long training run. The epilog script ensures hygiene by killing orphaned Python processes, clearing shared memory segments (`/dev/shm`), and logging final GPU utilization metrics to the accounting database. For our 175B model, the prolog script specifically validates that all 8 GPUs on each node have full P2P bandwidth access, preventing a single degraded NVLink lane from bottlenecking the entire 64-GPU tensor parallel group.

**Trackable Resources (TRES)** extend Slurm's accounting beyond simple CPU/memory tracking to arbitrary assets like GPU-hours, license tokens, or power budget. Organizations use TRES to implement project billing for expensive resources. By defining a TRES type `billing=gpu`, administrators can charge different rates for A100s versus V100s, or track the consumption of specific software licenses. This granular accounting enables the implementation of fair-share policies based on actual economic cost rather than just core counts, ensuring that a team using 100 older GPUs does not deplete its budget as fast as a team using 100 flagship H100s.

### Kubernetes: The Cloud Native Standard {#sec-fleet-orchestration-kubernetes}

Kubernetes has become the dominant platform for ML infrastructure, particularly for organizations requiring unified management of training and serving workloads. Where Slurm's model is imperative ("run this job on these resources"), Kubernetes is **declarative** ("ensure this state exists"), using control loops that continuously reconcile desired state with actual state. This fundamental difference shapes every aspect of ML workload management, from job submission to failure recovery.

The declarative model's power lies in its approach to failure handling. When a node fails in Slurm, an administrator or monitoring system must detect the failure, cancel affected jobs, and resubmit them. In Kubernetes, the control loop detects the divergence between desired state ("4 replicas of this training worker should be running") and actual state ("only 3 are running") and automatically creates a replacement pod on a healthy node. This self-healing behavior reduces operational burden but introduces complexity: the system's actions are emergent from control loop interactions rather than explicitly commanded, making debugging more challenging when things go wrong.

Kubernetes' control loop architecture is built on the **controller pattern**[^fn-controller-k8s]: a controller watches the cluster's actual state (stored in etcd), compares it to desired state (specified by users through API objects like Deployments, Jobs, and StatefulSets), and takes corrective action to close the gap. This pattern repeats at every level: the Deployment controller ensures the right number of pods exist, the scheduler assigns pods to nodes, the kubelet on each node ensures containers are running, and the node controller detects node failures. Each controller operates independently, communicating only through shared state in etcd, which makes the system resilient to individual controller failures but can create emergent behaviors when multiple controllers interact.

[^fn-controller-k8s]: **Controller Pattern (Reconciliation Loop)**: A "level-triggered" design where the controller continuously compares desired state to actual state, as opposed to "edge-triggered" systems that react to individual events. The distinction matters for ML clusters: if a controller crashes and restarts, it re-reads current state and self-heals without needing an event log. The trade-off is that emergent interactions between multiple independent controllers can create unexpected scheduling behaviors that are difficult to debug sequentially. \index{Controller Pattern!Kubernetes}

Native Kubernetes lacks ML-aware scheduling, but a growing ecosystem of extensions addresses this gap. GPU scheduling relies on **device plugins** that expose accelerators as schedulable extended resources. The NVIDIA device plugin registers GPUs with the kubelet (the node-level agent), enabling pod specifications that request GPU resources declaratively through the standard Kubernetes resource model.

::: {#lst-fleet-orchestration-k8s-gpu lst-cap="**Kubernetes GPU Allocation**: Pod resource specification requesting GPUs through the NVIDIA device plugin. The `nvidia.com/gpu` resource name follows Kubernetes extended resource conventions, where the domain prefix identifies the device plugin vendor. This declarative syntax enables portable GPU workload definitions across any Kubernetes cluster with the NVIDIA device plugin installed."}
```{.yaml}
# Kubernetes pod resource specification for GPU allocation
resources:
  limits:
    nvidia.com/gpu: 4  # Request exactly 4 GPUs for this pod
  requests:
    cpu: "32"
    memory: "256Gi"
```
:::

This binary allocation model creates a significant inefficiency for inference workloads. A small model serving occasional requests might need only a fraction of a GPU's compute and memory capacity, yet Kubernetes allocates an entire GPU, wasting the remaining capacity. For training workloads that saturate GPU compute, full-device allocation is appropriate. For inference workloads with variable and often modest resource needs, it creates the same kind of fragmentation that partial-node allocation creates in Slurm.

**Multi-Instance GPU (MIG)**[^fn-mig-isolation] technology addresses this inefficiency by partitioning A100 and H100 GPUs into hardware-isolated instances. An A100 with `{python} a100_mem_str` GB of memory can be divided into configurations ranging from 7 small instances of approximately 10 GB each to 2 large instances of approximately 40 GB each. Unlike software-based GPU sharing (time-slicing), MIG provides hardware isolation: each instance has dedicated memory, cache, and compute resources, preventing one workload from interfering with another. The device plugin exposes MIG instances as separate schedulable resources, allowing the scheduler to treat each instance as an independent GPU for allocation purposes.

[^fn-mig-isolation]: **Multi-Instance GPU (MIG)**: Introduced with the A100 (2020), MIG partitions a single GPU into up to 7 hardware-isolated instances with dedicated memory controllers, L2 cache slices, and compute units. The isolation is enforced at the hardware level, not by time-slicing, eliminating the noisy-neighbor interference that degrades P99 latency by 10 to 20 percent under software sharing. The trade-off is rigidity: partition profiles cannot change without draining all workloads, making MIG poorly suited for clusters with rapidly shifting workload mixes. \index{MIG!GPU isolation}

| **MIG Profile** | **GPU Memory** | **SM Count** | **Typical Workload** |
|:----------------|:---------------|:-------------|:---------------------|
| **1g.10gb**     | 10 GB          | 14 SMs       | Small inference      |
| **2g.20gb**     | 20 GB          | 28 SMs       | Medium inference     |
| **3g.40gb**     | 40 GB          | 42 SMs       | Large inference      |
| **7g.80gb**     | 80 GB          | 98 SMs       | Training             |

: **A100 MIG Profiles**: Multi-Instance GPU configurations trade GPU memory and compute resources for increased sharing density. Each profile provides hardware-isolated resources, making MIG suitable for multi-tenant inference clusters where workloads from different teams or customers must not interfere. The SM (Streaming Multiprocessor) count determines compute throughput within each instance. {#tbl-fleet-orchestration-mig-profiles}

**Gang scheduling in Kubernetes** requires extensions beyond the default scheduler, which was designed for microservice workloads where individual pods are independently schedulable. The default scheduler evaluates pods one at a time, placing each on the best available node. For a distributed training job with 64 pods, this means 64 independent scheduling decisions, with no guarantee that all 64 will succeed. If the cluster has resources for 60 pods but not 64, the default scheduler will place 60 pods and leave the remaining 4 pending, creating exactly the partial-allocation waste that gang scheduling prevents.

The **Volcano**[^fn-volcano-k8s] batch scheduler and **Coscheduling** scheduler plugin address this gap by implementing gang semantics through **PodGroup** abstractions. A PodGroup declares that a set of pods must be scheduled together, specifying a minimum member count that must be satisfiable before any pod in the group starts. The scheduler evaluates the entire group atomically: either all minimum members can be placed, and they are placed simultaneously, or none are placed and the group waits in the queue. This transforms Kubernetes' pod-level scheduling into job-level scheduling that respects the all-or-nothing requirements of distributed training.

[^fn-volcano-k8s]: **Volcano**: Open-sourced by Huawei in 2019 and now a CNCF incubating project, Volcano replaces the Kubernetes default scheduler entirely to add gang scheduling via its PodGroup CRD. The replacement approach provides strong atomicity guarantees for multi-pod ML training jobs but carries operational risk: a bug in Volcano affects *all* scheduling decisions on the cluster, not just batch workloads. \index{Volcano!Kubernetes scheduling}

**Priority classes** control preemption behavior in Kubernetes, establishing a hierarchy that determines which workloads yield resources to which others. A typical production configuration assigns inference workloads high priority (ensuring serving SLOs are met), training workloads medium priority, and development jobs low priority. The `preemptionPolicy: PreemptLowerPriority` setting enables inference pods to reclaim resources from training when latency SLOs are threatened, while `NonPreempting` prevents development jobs from ever preempting other workloads. Organizations must carefully balance the priority hierarchy: overly aggressive inference preemption can thrash training jobs (repeatedly preempting and restarting them), while overly permissive training priorities can starve inference during demand spikes. The Kubernetes priority and preemption system integrates with the checkpoint-aware preemption patterns established in @sec-fault-tolerance-reliability, where preempted training jobs save state before yielding resources, minimizing wasted computation.

**Kueue**[^fn-kueue-k8s], a newer Kubernetes-native job queueing system, represents an emerging approach that separates *admission control* from *scheduling*. Rather than replacing the Kubernetes scheduler entirely (as Volcano does), Kueue manages when jobs enter the cluster, while the default scheduler (or any compatible scheduler) handles where pods are placed. This separation of concerns has practical advantages: Kueue can be deployed alongside existing Kubernetes infrastructure without disrupting running workloads, and it integrates naturally with the ecosystem of scheduler plugins for topology-aware placement and other features.

Kueue provides fair-share scheduling through ClusterQueues that represent organizational teams or projects. Each queue has resource quotas, borrowing policies that allow queues to use idle capacity from other queues, and preemption policies that reclaim borrowed resources when the owning queue needs them. A research team queue might borrow idle capacity from a production team queue during off-peak hours, automatically returning resources through preemption when the production team submits urgent work. This borrowing mechanism mirrors the hierarchical fair-share approaches in Slurm but expressed through Kubernetes' declarative model rather than Slurm's imperative configuration.

[^fn-kueue-k8s]: **Kueue**: Introduced by Google in 2022, Kueue separates *admission control* (when jobs enter the cluster) from *scheduling* (where pods are placed), unlike Volcano which replaces the scheduler entirely. This less-invasive design can be deployed alongside existing Kubernetes infrastructure without disrupting running workloads, but it lacks Volcano's strong gang scheduling guarantees, forcing teams to choose between deployment safety and scheduling atomicity. \index{Kueue!Kubernetes scheduling}

### Kubernetes Ecosystem for ML Training {#sec-fleet-orchestration-k8s-ecosystem}

While Kubernetes provides the orchestration primitives, a specialized ecosystem of operators and plugins is required to bridge the gap between microservice orchestration and high-performance training. This ecosystem augments Kubernetes with the batch processing, hardware acceleration, and observability capabilities found in HPC environments.

The **Training Operator ecosystem** simplifies distributed training by extending Kubernetes with job-specific Custom Resource Definitions (CRDs). The Kubeflow Training Operator provides controllers for `PyTorchJob`, `TFJob`, and `MPIJob` resources, automating the complex lifecycle of distributed workloads. A `PyTorchJob` specification defines the number of workers, the container image, and resource requirements; the operator then creates the necessary pods, configures the distributed environment variables (`MASTER_ADDR`, `MASTER_PORT`, `WORLD_SIZE`, `RANK`), and manages fault tolerance. This declarative approach replaces brittle manual scripting with a reconciled state model: if a worker pod fails, the operator detects the deviation and restarts the replica to restore the desired job state.

**Network performance in Kubernetes** requires bypassing the virtualization layers standard for microservices. The default Container Network Interface (CNI) introduces 1 to 5 percent latency overhead and CPU switching costs that degrade gradient synchronization performance. For bandwidth-sensitive training, **host networking** (`hostNetwork: true`) allows pods to bypass the CNI entirely, granting direct access to the host's network namespace and RDMA devices. The NVIDIA Network Operator automates the low-level plumbing required for this performance, managing RDMA device plugins, SR-IOV virtual function assignment, and GPUDirect RDMA configuration. This bypass capability allows containerized workloads to achieve near-bare-metal interconnect performance.

**Storage integration** addresses the "checkpoint storm" problem where thousands of GPUs simultaneously write terabytes of state. Kubernetes PersistentVolumeClaims (PVCs) abstract the underlying storage fabric, which may be a parallel file system like Lustre or GPFS, or high-performance object storage via CSI drivers. The critical architectural challenge is preventing these synchronized writes from saturating the storage backend. Storage classes with Quality of Service (QoS) policies and client-side throttling are essential to ensure that a 1,024-GPU training job writing a 2 TB checkpoint does not starve other workloads or crash the storage metadata server.

The **observability stack** for training clusters combines general-purpose cluster monitoring with specialized hardware telemetry. Prometheus and Grafana provide the collection and visualization layer, while the DCGM (Data Center GPU Manager) exporter exposes granular GPU metrics -- utilization, memory bandwidth, temperature, and ECC errors. This stack enables the utilization dashboards essential for multi-tenant efficiency, allowing operators to distinguish between true compute saturation and I/O-bound idleness.

Our 175B model training on Kubernetes utilizes this full stack. The workload is defined as a `PyTorchJob` with 128 worker pods, each requesting 8 GPUs. Host networking is enabled to allow direct InfiniBand access for the NVLink-connected nodes. A PVC backed by a high-throughput Lustre file system provides access to the 100 TB training dataset and absorbs periodic checkpoints. DCGM metrics stream to Prometheus, triggering alerts if any GPU reports uncorrectable ECC errors or if NVLink bandwidth drops below the expected baseline.

### Choosing Between Paradigms {#sec-fleet-orchestration-paradigm-choice}

The choice between Slurm and Kubernetes is rarely binary; it depends on workload composition, organizational context, and the relative importance of different system qualities. Understanding where each paradigm excels guides the architectural decision.

Slurm excels for pure training clusters where predictability and bare-metal performance matter most. Its direct resource management avoids the container overhead that Kubernetes introduces (container networking adds microseconds of latency and reduces effective network bandwidth by 1 to 5 percent), and its batch scheduling model aligns naturally with long-running training jobs that require guaranteed, uninterrupted access to specific hardware. Slurm's centralized scheduler has global visibility into cluster state, enabling sophisticated multi-factor priority decisions that account for fair-share, job size, queue depth, and resource availability simultaneously. National laboratories, university research clusters, and dedicated training infrastructure typically favor Slurm because these environments prioritize maximum hardware utilization and minimal overhead for a relatively homogeneous set of batch workloads.

Kubernetes excels for mixed training-serving clusters where unified management, rolling updates, and service mesh integration reduce operational complexity. Organizations running both training pipelines and production inference endpoints benefit from a single control plane that manages both workload types through a consistent API. Kubernetes' declarative model simplifies operational workflows: deploying a new model version means updating a Deployment specification, and Kubernetes handles rolling updates, health checks, and rollback automatically. Cloud-native organizations, teams deploying ML as microservices, and organizations with existing Kubernetes expertise typically favor Kubernetes because the marginal cost of adding ML workloads to an existing platform is lower than operating a separate scheduling system.

Many production environments recognize that neither paradigm alone satisfies all requirements, and they deploy both systems in complementary roles. A common architecture uses Slurm for dedicated training clusters where raw performance and scheduling sophistication matter, and Kubernetes for inference serving, pipeline orchestration, and lighter training workloads (fine-tuning, small-scale experimentation). The emerging pattern is to use Kubernetes as the outer orchestration layer, submitting Slurm jobs for large training runs through Kubernetes operators. This hybrid architecture achieves unified management and observability without sacrificing Slurm's batch scheduling capabilities for the workloads that need them most.

The following table summarizes the key trade-offs between the two paradigms across the dimensions most relevant to ML workloads:

| **Dimension**          | **Slurm**                                                               | **Kubernetes**                                                            |
|:-----------------------|:------------------------------------------------------------------------|:--------------------------------------------------------------------------|
| **Scheduling model**   | Imperative: user specifies exact resources and scheduler allocates them | Declarative: user specifies desired state, system reconciles continuously |
| **Failure handling**   | External monitoring and manual resubmission                             | Self-healing through control loops                                        |
| **Gang scheduling**    | Native support through backfill scheduler                               | Requires extensions (Volcano, Coscheduling)                               |
| **Fair-share**         | Mature multi-factor priority with configurable decay                    | Kueue provides basic fair-share with ClusterQueue borrowing               |
| **Container overhead** | None (bare-metal execution)                                             | 1 to 5% network overhead, container startup time                          |
| **Service management** | No native support for long-running services                             | Native Deployment, rolling updates, health checks                         |
| **Ecosystem**          | MPI, OpenMP, scientific computing libraries                             | Cloud-native, microservices mesh, observability stack                     |

: **Slurm vs. Kubernetes for ML Workloads**: The two paradigms reflect different design philosophies optimized for different workload characteristics. Most production ML platforms use elements of both, with Slurm managing dedicated training clusters and Kubernetes managing inference serving and pipeline orchestration. {#tbl-fleet-orchestration-paradigm-comparison}

The choice also reflects organizational trajectory. Teams starting with HPC-focused training infrastructure may begin with Slurm and add Kubernetes for serving. Teams starting from cloud-native application infrastructure may begin with Kubernetes and add HPC-oriented extensions (Volcano, Kueue) for training. Either path can reach a functional hybrid architecture, but the starting point determines which capabilities are strongest and which require the most investment to develop.

### Hybrid Architectures in Practice {#sec-fleet-orchestration-hybrid-architectures}

While the choice between Slurm and Kubernetes often appears binary, sophisticated engineering organizations increasingly deploy **hybrid architectures** that leverage the strengths of both systems. These architectures acknowledge a fundamental reality: training workloads behave like HPC applications (batch, synchronous, monolithic), while serving workloads behave like microservices (continuous, asynchronous, elastic). Forcing both into a single paradigm often results in a "lowest common denominator" system that serves neither well.

The **Kubernetes-over-Slurm** pattern wraps the HPC scheduler within a cloud-native control plane. In this architecture, Kubernetes acts as the outer orchestration layer, managing the lifecycle of training jobs through custom operators while delegating the actual pod placement to Slurm. A developer submits a `TrainingJob` Custom Resource Definition (CRD) to Kubernetes, which the operator translates into a Slurm `sbatch` script. The operator then monitors the job status via Slurm's REST API, streaming logs and metrics back to the Kubernetes control plane. This approach provides the best of both worlds: the unified observability, CI/CD integration, and access control of Kubernetes, combined with the gang scheduling sophistication, topology awareness, and bare-metal performance of Slurm.

The **Slurm-alongside-Kubernetes** pattern takes a coarser-grained approach, maintaining separate clusters for training and serving that draw from a shared pool of physical nodes. A meta-scheduler or capacity broker acts as an arbitrator, dynamically reassigning nodes between the Slurm partition and the Kubernetes cluster based on demand signals. During intensive training campaigns, nodes are drained from Kubernetes and joined to Slurm; during product launches or traffic spikes, the flow reverses. The primary challenge is the mode-switching cost: draining a Kubernetes node, re-imaging or reconfiguring it, and joining it to Slurm (or vice versa) typically requires 5 to 15 minutes. This latency limits the granularity of sharing -- the system can adapt to diurnal patterns or weekly cycles, but cannot use Slurm capacity to absorb second-by-second inference bursts.

**Emerging unified platforms** like Ray attempt to eliminate the dichotomy entirely by providing a single runtime for both training and serving. Ray implements its own distributed scheduler that handles actor placement, task scheduling, and resource management natively. This eliminates the impedance mismatch between training (stateful actors) and serving (stateless replicas), allowing a single Python script to define a training loop that seamlessly transitions into a deployment pipeline. The trade-off is maturity: while Ray excels at orchestration flexibility, its scheduling logic lacks the decades of hardening found in Slurm and the massive ecosystem support of Kubernetes, effectively requiring the platform team to assume more responsibility for reliability and isolation.

Threading these patterns together, consider the lifecycle of our 175B parameter model. The training phase runs on Slurm to maximize NVLink topology efficiency and gang scheduling reliability for the multi-month run. Once trained, the model artifacts are handed off to Kubernetes for serving, where rolling updates and autoscaling ensure high availability for end users. The bridge between them is a Kubernetes operator that monitors training progress; when validation loss stabilizes, it automatically triggers a quantization pipeline and deploys the resulting model to the inference cluster.

The operational impact of these choices is quantifiable. A pure Slurm-only environment minimizes scheduling latency (microseconds) and maximizes utilization (greater than 90 percent) but requires higher engineering headcount to build custom serving infrastructure. A Kubernetes-only environment minimizes headcount (leveraging standard DevOps tools) but often struggles to exceed 75 percent training utilization due to scheduling fragmentation. The hybrid approach targets the efficient frontier: preserving 90 percent training utilization and low operational overhead, at the cost of increased architectural complexity.

::: {.callout-perspective title="The Convergence of HPC and Cloud"}
The sharp distinction between HPC and cloud-native scheduling is rapidly blurring as both communities adopt features from the other. Kubernetes is evolving batch capabilities through the Volcano and Kueue projects, introducing concepts like queues, priorities, and gang scheduling that were previously unique to HPC. Conversely, Slurm is adopting cloud features like REST APIs, container runtime support, and elasticity. The industry is converging toward a unified fleet operating system: a system that offers the declarative API and fault tolerance of the cloud with the strict placement guarantees and performance of the supercomputer.
:::

::: {.callout-checkpoint title="Scheduling Paradigm Trade-offs"}

Before proceeding to topology-aware scheduling, verify your understanding of the core trade-offs:

- [ ] Can you explain why gang scheduling is essential for distributed training but not for inference workloads? What property of synchronous data parallelism creates the all-or-nothing requirement?
- [ ] What CAP theorem trade-off does Slurm make differently from Kubernetes? How does this difference manifest during a network partition between the scheduler and a subset of nodes?
- [ ] When would MIG partitioning improve cluster utilization (more concurrent inference workloads on fewer GPUs), and when would it reduce it (training jobs that need full GPU memory)?
- [ ] Why does backfill scheduling depend on accurate job runtime estimates, and what game-theoretic incentives cause users to provide inaccurate estimates?

:::

The scheduling paradigms discussed above provide the infrastructure for resource allocation, but they treat GPUs as interchangeable units within a partition. The next section examines a critical refinement: how the *physical location* of allocated GPUs within the cluster's network topology affects training performance, and how topology-aware scheduling algorithms exploit this structure.

## Topology-Aware Scheduling {#sec-fleet-orchestration-topology-aware}

If you randomly assign 64 GPUs across a massive datacenter, the resulting network latency will cripple a synchronous training job, causing it to run 30% slower than if those same 64 GPUs were packed into a single rack. The scheduling algorithms discussed so far treat GPUs as interchangeable units, but topology-aware scheduling recognizes that physical proximity in the network fabric is just as critical as raw compute.

### The Topology Hierarchy {#sec-fleet-orchestration-topology-hierarchy}

Modern GPU clusters exhibit a multi-level communication hierarchy, where bandwidth decreases and latency increases at each level. This hierarchy is not an implementation detail that the scheduler can ignore; it is a physical constraint that directly determines training throughput for communication-intensive workloads. Understanding the hierarchy is essential for making placement decisions that translate allocated resources into useful work.

Within a single node, GPUs communicate via **NVLink**, providing `{python} nvlink_a100_str` GB/s per GPU on A100 systems and `{python} nvlink_h100_str` GB/s on H100 systems. This high-bandwidth, low-latency interconnect enables efficient tensor parallelism, where matrix operations are split across GPUs that must exchange intermediate results (activations and gradients) at every transformer layer. The NVLink bandwidth is sufficient to overlap communication with computation for most model architectures, meaning tensor parallel operations can proceed at near-ideal throughput when confined to a single node.

Between nodes within the same **rack** or **leaf switch domain**, communication traverses InfiniBand or RoCE at `{python} ib_ndr_str` Gbps (NDR InfiniBand), roughly `{python} ib_ndr_gbs_str` GB/s per link. This represents an order of magnitude reduction from NVLink bandwidth. Data parallelism, which requires AllReduce of gradients after each training step, operates efficiently at this level because the gradient synchronization pattern allows overlap between communication and the next iteration's forward pass. The latency penalty is modest (microseconds versus nanoseconds), and the aggregate bandwidth is sufficient for gradient tensors that are typically smaller than the activation tensors exchanged in tensor parallelism.

Between racks connected through **spine switches**, communication crosses additional switch hops, introducing both higher latency and potentially reduced effective bandwidth due to network congestion. The fat-tree topologies analyzed in @sec-network-fabrics provide full bisection bandwidth in theory, but in practice, congestion from concurrent flows, routing inefficiencies, and switch buffer limitations reduce effective cross-rack bandwidth by 10 to 30 percent compared to intra-rack communication. More importantly, cross-rack communication adds tail latency variability: while median latency may be only slightly higher, P99 latency can spike during congestion events, creating straggler effects in synchronous training where the slowest communicator determines the pace for all workers.

This hierarchy means that a 64-GPU job allocated entirely within a single rack (8 nodes, all under one leaf switch) will outperform the same job spread across 8 racks (1 node per rack, crossing spine switches for every AllReduce). The performance difference stems directly from the communication analysis in @sec-collective-communication: ring AllReduce latency scales with the slowest link in the ring, and cross-rack hops introduce both higher latency and increased contention risk. For synchronous training, every iteration waits for the slowest worker to complete its AllReduce, so even occasional cross-rack congestion events create persistent throughput degradation.

The scheduler must model the cluster not as a flat list of compute slots, but as a hierarchical graph with distinct bandwidth cliffs: GPU, Node, Rack, and Pod. At the lowest level, GPUs within a single node (e.g., an HGX H100 system) communicate via NVLink/NVSwitch at 900 GB/s, enabling virtually seamless tensor parallelism. Crossing the node boundary forces traffic onto InfiniBand or RoCE adapters, dropping effective bandwidth to 200–400 GB/s and introducing significant latency penalties for synchronous operations like AllReduce. At the rack and pod levels, network oversubscription—often 2:1 or 4:1 at the spine switches—further restricts bisection bandwidth. A topology-aware scheduler explicitly optimizes for these constraints, treating "locality" as a first-class resource. For a distributed training job requiring 1,024 GPUs, a fragmented placement that scatters workers across random racks can degrade training throughput by 30–50% compared to a compact placement within a single pod.

@fig-topology-placement-paths contrasts optimal and scattered placement for an 8-GPU tensor parallel group, showing how the communication path determines whether GPUs use NVLink or must traverse slower InfiniBand links.

::: {#fig-topology-placement-paths fig-env="figure" fig-pos="htb" fig-cap="**Topology-Aware vs. Scattered Placement**. An 8-GPU tensor parallel group placed within a single node (left, green paths) communicates exclusively via NVLink at 900 GB/s. The same group scattered across two nodes (right, red paths) must route half its traffic through InfiniBand at 50 GB/s, an 18$\\times$ bandwidth reduction that compounds at every transformer layer." fig-alt="Two side-by-side diagrams. Left shows 8 GPUs in one node connected by green NVLink paths through a central NVSwitch. Right shows 8 GPUs split across two nodes with red InfiniBand paths crossing between them."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{BlueL}{HTML}{D1E6F3}
  \definecolor{GreenLine}{HTML}{008F45}
  \definecolor{GreenL}{HTML}{D4EFDF}
  \definecolor{RedLine}{HTML}{CB202D}
  \definecolor{RedL}{HTML}{F5D2D5}
  \definecolor{OrangeLine}{HTML}{E67817}
  \definecolor{BrownLine}{HTML}{78492A}
  \definecolor{BrownL}{HTML}{E3D3C8}

  \tikzset{
    gpu/.style={draw=black!60, fill=BlueL, thick, rounded corners=2pt,
                minimum width=0.6cm, minimum height=0.5cm, font=\tiny\bfseries},
    switch/.style={draw=black!60, fill=BrownL, thick, rounded corners=2pt,
                   minimum width=1.6cm, minimum height=0.4cm, font=\tiny\bfseries},
    nodebox/.style={draw=black!30, fill=black!3, rounded corners=6pt,
                    minimum width=4.5cm, minimum height=2.5cm},
    nvpath/.style={draw=GreenLine, line width=1.5pt},
    ibpath/.style={draw=RedLine, line width=1.5pt, dashed}
  }

  % === LEFT: Optimal Placement ===
  \node[font=\small\bfseries, text=GreenLine] at (2.25, 3.5) {Optimal (Single Node)};

  % Node box
  \node[nodebox] (opt_node) at (2.25, 1.2) {};
  \node[font=\tiny, anchor=south] at (2.25, -0.2) {Node 0};

  % NVSwitch
  \node[switch, fill=GreenL] (nvsw) at (2.25, 1.8) {NVSwitch};

  % GPUs (2 rows of 4)
  \foreach \i in {0,...,3} {
    \pgfmathsetmacro{\xpos}{0.6 + \i * 1.1}
    \node[gpu, fill=GreenL] (og\i) at (\xpos, 0.5) {G\i};
    \draw[nvpath] (og\i) -- (nvsw);
  }
  \foreach \i in {4,...,7} {
    \pgfmathsetmacro{\xpos}{0.6 + (\i-4) * 1.1}
    \pgfmathtruncatemacro{\idx}{\i}
    \node[gpu, fill=GreenL] (og\idx) at (\xpos, -0.3) {G\idx};
    \draw[nvpath] (og\idx) -- (nvsw);
  }

  % Bandwidth label
  \node[font=\scriptsize\bfseries, text=GreenLine] at (2.25, -1.0) {900 GB/s (NVLink)};

  % === RIGHT: Scattered Placement ===
  \node[font=\small\bfseries, text=RedLine] at (9.25, 3.5) {Scattered (Two Nodes)};

  % Node A
  \node[nodebox, minimum width=3.2cm, minimum height=2.0cm] (sn_a) at (7.5, 1.4) {};
  \node[font=\tiny, anchor=south] at (7.5, 0.1) {Node 0};
  \node[switch, fill=GreenL, minimum width=1.2cm] (nvsw_a) at (7.5, 1.8) {NVSw};
  \foreach \i in {0,...,3} {
    \pgfmathsetmacro{\xpos}{6.3 + \i * 0.8}
    \node[gpu, fill=GreenL] (sg\i) at (\xpos, 0.7) {G\i};
    \draw[nvpath] (sg\i) -- (nvsw_a);
  }

  % Node B
  \node[nodebox, minimum width=3.2cm, minimum height=2.0cm] (sn_b) at (11.0, 1.4) {};
  \node[font=\tiny, anchor=south] at (11.0, 0.1) {Node 1};
  \node[switch, fill=GreenL, minimum width=1.2cm] (nvsw_b) at (11.0, 1.8) {NVSw};
  \foreach \i in {4,...,7} {
    \pgfmathsetmacro{\xpos}{9.8 + (\i-4) * 0.8}
    \pgfmathtruncatemacro{\idx}{\i}
    \node[gpu, fill=RedL] (sg\idx) at (\xpos, 0.7) {G\idx};
    \draw[nvpath] (sg\idx) -- (nvsw_b);
  }

  % InfiniBand link between nodes
  \draw[ibpath] (nvsw_a.east) -- node[above, font=\tiny\bfseries, text=RedLine] {IB 50 GB/s} (nvsw_b.west);

  % Bandwidth label
  \node[font=\scriptsize\bfseries, text=RedLine] at (9.25, -1.0) {50 GB/s cross-node (18$\times$ slower)};

\end{tikzpicture}
```
:::

### Placement Algorithms {#sec-fleet-orchestration-placement-algorithms}

Topology-aware placement algorithms assign jobs to nodes that minimize communication cost, transforming the abstract bin packing problem into a graph optimization problem on the cluster's physical topology. The simplest approach uses a **locality score** that penalizes allocations spanning multiple topology domains:

$$ S_{locality} = \sum_{i < j} w(d(g_i, g_j)) $$ {#eq-fleet-orchestration-locality-score}

where $g_i$ and $g_j$ are GPUs assigned to the job, $d(g_i, g_j)$ is their topological distance (0 for same node, 1 for same rack, 2 for different racks), and $w(\cdot)$ is a weighting function that increases with distance. The scheduler minimizes this score when selecting an allocation from the set of feasible placements. The weighting function encodes the performance impact of each topology level: $w(0) = 0$ (same node, NVLink, no penalty), $w(1) = 1$ (same rack, one switch hop), $w(2) = 10$ (different racks, multiple switch hops). The tenfold weight difference between same-rack and cross-rack placements reflects the disproportionate performance impact of crossing spine switches.

More sophisticated algorithms distinguish between parallelism strategies, recognizing that different communication patterns have different bandwidth and latency requirements. Tensor parallelism requires the highest bandwidth and lowest latency because it exchanges activation tensors at every layer; it should be confined to NVLink domains within a single node. Pipeline parallelism tolerates moderate latency because it sends activations between stages less frequently (once per microbatch per stage boundary rather than at every layer); it can span nodes within a rack where InfiniBand provides sufficient bandwidth. Data parallelism, with its bulk gradient synchronization that can overlap with computation, operates efficiently across racks because the communication is less latency-sensitive and can utilize the available bandwidth effectively.

A **hierarchical placement** algorithm assigns resources in layers that match this parallelism hierarchy. First, it assigns tensor parallel groups to individual nodes, ensuring that all GPUs in a tensor parallel group share NVLink connectivity. Second, it groups those nodes into pipeline stages within racks, placing consecutive pipeline stages on nodes connected through the same leaf switch. Third, it distributes data parallel replicas across racks, ensuring that each data parallel group has access to the cluster's full bisection bandwidth for AllReduce operations. This three-level placement mirrors the three-level topology and the three-level parallelism strategy, creating a natural alignment between logical and physical structure.

::: {.callout-notebook}
## Napkin Math: The Topology Placement Impact
The physical location of GPUs on the network switch fabric dramatically impacts collective performance. Consider an AllReduce operation across 8 GPUs:

*   **Random Placement:** Spread across 8 different racks $\rightarrow$ Traffic traverses core switches $\rightarrow$ **120ms** latency.
*   **Rack-Aware:** Confined to a single rack (Top-of-Rack switch) $\rightarrow$ **85ms**.
*   **Rail-Optimized:** Aligned to specialized "rails" (dedicated standard IB switches) $\rightarrow$ **40ms**.
*   **Topology-Optimal:** All GPUs on the same leaf switch $\rightarrow$ **25ms**.
Proper scheduling yields a **4.8$\times$ speedup** purely by respecting the network topology, with zero changes to the model code.
:::

The computational cost of topology-aware placement is non-trivial. Evaluating all possible placements for a 256-GPU job across a 10,000-GPU cluster is combinatorially intractable. Production schedulers use heuristic approaches: greedy algorithms that build placements bottom-up (first fill NVLink domains, then fill racks, then spread across racks), tree-search algorithms with pruning that explore the most promising placements first, or scoring-based approaches that evaluate a sample of feasible placements and select the best. The scheduling latency introduced by topology-aware algorithms (milliseconds to seconds, depending on cluster size and algorithm complexity) is negligible compared to job runtimes of hours to weeks, making the throughput improvement worth the scheduling overhead.

::: {.callout-perspective title="Topology Placement Impact"}

Consider a 256-GPU training job using 3D parallelism: 8-way tensor parallel, 4-way pipeline parallel, 8-way data parallel.

**Good placement** (topology-aware):

- Tensor parallel groups: 32 groups of 8 GPUs, each confined to one 8-GPU node (NVLink: `{python} nvlink_h100_str` GB/s)
- Pipeline stages: 4 consecutive nodes within the same rack (1 switch hop, InfiniBand: `{python} ib_ndr_gbs_str` GB/s)
- Data parallel replicas: 8 racks (2 switch hops for cross-rack AllReduce)

**Bad placement** (topology-unaware):

- Tensor parallel groups split across 2 nodes (InfiniBand instead of NVLink)
- Pipeline stages scattered across racks (2 switch hops instead of 1)
- Data parallel replicas randomly distributed

The tensor parallel communication alone illustrates the impact. With NVLink at `{python} nvlink_h100_str` GB/s, a 1 GB activation transfer takes approximately 1.1 ms. Over InfiniBand at `{python} ib_ndr_gbs_str` GB/s, the same transfer takes approximately 20 ms, an 18$\times$ slowdown. Since tensor parallelism communicates at every transformer layer (typically 80+ layers for large models), this difference compounds to 15 to 30 percent total training throughput degradation.

:::

::: {#fig-topology-placement fig-env="figure" fig-pos="htb" fig-cap="Topology Placement Impact: Reducing AllReduce latency by 4.8x through topology-aware scheduling (Rack/Rail optimization) compared to random placement for a 1GB gradient sync on 64 GPUs." fig-alt="Bar chart comparing AllReduce latency for Random, Rack-Aware, Rail-Optimized, and Topology-Optimal placement. 4.8x speedup from Random to Topology-Optimal."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ TOPOLOGY PLACEMENT (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-topology-placement — AllReduce latency vs placement strategy
# │
# │ Goal: Bar chart comparing Random, Rack-Aware, Rail-Optimized, Topology-
# │       Optimal; show 4.8× speedup for 1GB gradient sync on 64 GPUs.
# │ Show: Four bars with latency (ms) and effective bandwidth labels.
# │ How: strategies/times; ax.bar; matplotlib.
# │
# │ Imports: matplotlib.pyplot (plt), numpy (np)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt
import numpy as np

plt.style.use('seaborn-v0_8-whitegrid')

strategies = ['Random', 'Rack-Aware', 'Rail-Optimized', 'Topology-Optimal']
times = [120, 85, 40, 25]
bandwidths = [2.0 / (t / 1000.0) for t in times]

colors = ['#d62728', '#ff7f0e', '#bcbd22', '#2ca02c']

fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(strategies, times, color=colors, alpha=0.85, width=0.6, edgecolor='black', linewidth=1)

ax.set_ylabel('AllReduce Latency (ms)\n(Lower is Better)', fontsize=12, fontweight='bold')
ax.set_xlabel('Placement Strategy', fontsize=12, fontweight='bold')
ax.set_title('Impact of Topology-Aware Job Placement\n(64 GPUs, 1GB Gradient Sync)', fontsize=14, pad=15)
ax.set_ylim(0, 150)

for bar, time, bw in zip(bars, times, bandwidths):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height + 3,
            f'{time} ms\n({bw:.1f} GB/s)',
            ha='center', va='bottom', fontsize=11, fontweight='bold', color='#333333')

ax.annotate('4.8x Speedup',
            xy=(3, 25), xycoords='data',
            xytext=(1.5, 120), textcoords='data',
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=-0.2", color='#2ca02c', lw=2.5),
            fontsize=13, fontweight='bold', color='#2ca02c',
            bbox=dict(boxstyle="round,pad=0.4", fc="white", ec="#2ca02c", alpha=0.9))

plt.tight_layout()
fig = plt.gcf()
```
:::

### Rail-Optimized Scheduling {#sec-fleet-orchestration-rail-optimized}

Large GPU clusters increasingly use **rail-optimized topologies** where each GPU in a node connects to a different network switch, creating parallel "rails" through the network fabric. This design, analyzed in @sec-network-fabrics, provides balanced bandwidth across all GPUs by distributing traffic across independent switch hierarchies. However, it also creates a scheduling constraint: AllReduce operations are most efficient when communicating GPUs are on the same rail (connected to the same switch chain), because same-rail communication avoids cross-rail traffic that could create congestion at shared switch ports.

Rail-aware scheduling assigns data parallel replicas such that corresponding GPUs across nodes share the same rail. For an 8-GPU-per-node cluster with 8 rails, GPU 0 on every node connects to rail 0, GPU 1 to rail 1, and so on. The scheduler places data parallel rank $r$ on GPU $r \bmod 8$ across selected nodes, ensuring that AllReduce for each data parallel group traverses only a single rail switch hierarchy rather than crossing between rails. This alignment means that AllReduce traffic is confined to independent switch trees, eliminating contention between data parallel groups and providing each group with the full bandwidth of its dedicated rail.

The interaction between rail-aware scheduling and topology-aware placement creates a multi-dimensional optimization problem. The scheduler must simultaneously optimize for intra-node NVLink locality (tensor parallelism requires GPUs on the same node), intra-rack switch locality (pipeline parallelism benefits from minimal switch hops between stages), and rail alignment (data parallelism benefits from same-rail communication). These three objectives are partially conflicting: constraining data parallel replicas to specific GPU positions within each node (for rail alignment) reduces the scheduler's flexibility to choose nodes based on rack locality, and requiring full-node allocations for NVLink locality prevents sharing nodes between jobs that could improve packing efficiency.

Production schedulers resolve these conflicts using weighted scoring functions that prioritize constraints based on workload characteristics. For large training jobs using 3D parallelism, tensor parallel locality receives the highest weight (because NVLink-to-InfiniBand performance degradation is the largest), followed by rail alignment (because it eliminates congestion for the most frequent communication pattern), followed by rack locality (because the performance benefit is smaller and can be partially compensated by compute-communication overlap). For pure data parallel jobs, rail alignment receives the highest weight. For single-node jobs, topology constraints are irrelevant and the scheduler optimizes purely for packing efficiency.

### Topology Discovery and Dynamic Adaptation {#sec-fleet-orchestration-topology-discovery}

Static configuration files like Slurm's `topology.conf` or Kubernetes node labels provide a baseline map of the cluster, but they represent the *intended* state, not the *effective* state. True **topology discovery** requires runtime introspection to validate that the physical hardware matches the architectural diagram. Libraries like NCCL bypass static configurations entirely, parsing the Linux virtual filesystem at `/sys/class/infiniband` to map the PCIe bus hierarchy, enumerate NVLink connections, and verify NIC proximity. This creates a ground-truth graph where edges represent verified, operational links rather than theoretical cables.

The cluster topology is a living organism, not a fixed schematic. Hardware failures, maintenance windows, and thermal throttling constantly reshape the effective topology available to the scheduler. A single spine switch failure in a Clos network does not sever connectivity entirely, but it drastically reduces the bisection bandwidth available to jobs spanning the affected racks. If a spine switch serving our 175B parameter model's data-parallel AllReduce group fails, cross-rack bandwidth might drop by 50 percent. The scheduler faces a critical optimization decision: continue training at reduced throughput -- effectively wasting 25 percent of the expensive GPU cycles waiting for straggling gradients -- or initiate a topology-aware migration.

Migration is not free; it requires a coordinated checkpoint, a job kill, a rescheduling event on healthy nodes, and a warmup phase. This process typically consumes 5 to 15 minutes of idle time. However, for a multi-week training run, the math often favors migration. If the network degradation creates a 20 percent throughput penalty, the break-even point for a 10-minute migration is less than an hour of training. Sophisticated orchestrators continuously monitor effective bandwidth metrics reported by the collective communication library. When the delta between the assigned topology score and the realized throughput exceeds a threshold, the scheduler triggers a drain-and-migrate workflow, treating network degradation with the same severity as a GPU hardware fault.

::: {.callout-war-story title="The Silent Switch Failure"}
A major research lab spent a week debugging a 30 percent performance regression in their flagship foundation model training run. The GPUs were healthy, the code was unchanged, and the job was placed on a "prime" pod with full bisection bandwidth. The culprit was a single top-of-rack switch with a corrupted transceiver. The switch was technically "up" and passing standard health checks (ping, link status), but it was silently dropping 50 percent of packets due to CRC errors, forcing massive TCP retransmissions.

Because the scheduler relied on a static topology map, it continued to schedule the most communication-intensive jobs on the degraded rack, assuming full 800 Gb/s connectivity. The fix required shifting from binary health checks (up/down) to **performance-aware topology monitoring**, where the scheduler integrates real-time bandwidth counters from the switch fabric. Nodes connected to a "limping" switch are now dynamically tainted, forcing the scheduler to route high-bandwidth jobs elsewhere until the hardware is replaced.
:::

The topology-awareness discussed here improves performance for allocated jobs by matching logical communication patterns to physical network structure. The next section addresses a complementary question: how should jobs adapt when the resources available to them change dynamically? Rather than treating resource allocation as fixed for a job's lifetime, elastic training allows jobs to grow, shrink, and recover without full restarts.

## Elastic Training {#sec-fleet-orchestration-elastic-training}

::: {.callout-definition title="Elastic Training"}

***Elastic Training***\index{Elastic Training!definition} is the capability of a distributed training job to dynamically adjust its worker count during execution without requiring a full restart.

1.  **Significance (Quantitative):** It maximizes the **System Duty Cycle ($\eta$)** by allowing training to continue through node failures and by absorbing idle capacity in the cluster. It requires **Learning Rate Recalibration** and gradient accumulation adjustment to maintain mathematical consistency as the global batch size changes.
2.  **Distinction (Durable):** Unlike **Standard Fault Tolerance** (which relies on checkpoint-restart), Elastic Training is **Always-Live**: the optimization loop never stops, it only changes its parallel width.
3.  **Common Pitfall:** A frequent misconception is that elasticity is "automatic." In reality, it is a **Distributed Coordination Problem**: the training framework, the data loader, and the orchestrator must all synchronize their state to ensure that no data samples are lost or double-counted during worker scaling.

:::

Consider what happens when a top-of-rack switch fails, taking out 8 GPUs from a 1,024-GPU training run. Under traditional rigid scheduling, the entire job crashes and must wait for 8 new GPUs to become available. Elastic training shatters this rigid contract, allowing the model to dynamically downscale to 1,016 GPUs and continue learning without interruption.

When a node fails, the entire job must stop, synchronize all remaining workers, reload the most recent checkpoint, and restart with a new allocation that replaces the failed node. The restart overhead (checkpoint loading, optimizer state reconstruction, data pipeline warmup) wastes minutes to hours depending on model size and checkpoint storage bandwidth. During this time, all the surviving workers sit idle, waiting for the new allocation to be provisioned and the new worker to catch up. For a 1,024-GPU job where a single node (8 GPUs) fails, the remaining 1,016 GPUs are entirely unproductive during recovery.

When cluster demand fluctuates, rigid allocation creates a second category of waste. During off-peak hours, a job running on 256 GPUs cannot absorb idle resources to speed up training. During peak demand, the same job cannot release excess resources to accommodate higher-priority work without being fully preempted. The scheduler's only options are "let it keep all its resources" or "kill it entirely," with nothing in between.

**Elastic training** eliminates this binary choice by enabling jobs to dynamically adjust their worker count without full restart. A training job might start with 128 GPUs (the minimum needed for reasonable throughput), scale up to 256 when resources become available, and scale back to 192 when higher-priority work arrives, all while maintaining continuous training progress. This flexibility transforms the scheduler's action space from a binary {continue, preempt} to a continuous spectrum of resource allocation levels.

### The Elastic Training Contract {#sec-fleet-orchestration-elastic-contract}

Elastic training requires a well-defined contract between the scheduler and the training framework, specifying what each side guarantees and what each side must handle. The scheduler guarantees that resource changes are communicated through defined signals, with sufficient advance notice for graceful adaptation. The framework guarantees that it can handle resource changes without corrupting model state. The framework must implement three fundamental operations.

**Scale-up** (adding workers) is the simpler direction but still requires coordination across the entire training group. The framework must redistribute the dataset across the new worker count (so no data shard is orphaned or duplicated), adjust the effective batch size or per-worker batch size (depending on the scaling strategy), initialize new workers with current model parameters (requiring a parameter broadcast from existing workers), and join new workers into the communication group (reconstructing the AllReduce ring or tree). The initialization step is the most expensive: transferring the full model state to a new worker over InfiniBand takes seconds for a 70B parameter model, during which the existing workers must either pause or buffer gradient updates.

**Scale-down** (removing workers) is conceptually the reverse but operationally more challenging because it must handle both voluntary departures (the scheduler reclaiming resources) and involuntary departures (worker failure). The framework must redistribute work from departing workers to remaining ones, adjust batch size to maintain convergence properties, cleanly remove workers from the communication group, and optionally save a checkpoint to simplify recovery if scale-down was triggered by preemption. The critical requirement is that the communication group reconstruction is atomic with respect to gradient updates: no gradient update should be in flight during the transition, because a partially completed AllReduce with a changing participant set would produce incorrect gradients.

**Worker replacement** (substituting failed workers) combines scale-down and scale-up atomically, removing the failed worker and adding a replacement without changing the total worker count. This operation is the most common in practice and is the foundation of fault-tolerant training, connecting directly to the elastic recovery concepts introduced in @sec-fault-tolerance-reliability. Worker replacement must handle the additional complexity that the failed worker may not have completed its most recent communication, requiring the remaining workers to detect the incomplete operation (through timeouts) and reinitialize the communication group before adding the replacement.

The mathematical implications of elastic scaling interact with the optimization dynamics of stochastic gradient descent in ways that affect convergence. The effective batch size changes with worker count according to $B_{effective} = B_{per\_worker} \times N_{workers}$. Changing $N_{workers}$ mid-training alters the gradient noise level, which can affect convergence speed and final model quality. Two strategies handle this tension, each with distinct trade-offs.

**Constant batch size** scaling adjusts $B_{per\_worker}$ inversely with $N_{workers}$ to maintain $B_{effective}$ at its original value. When workers are added, each worker processes fewer samples per step; when workers are removed, each processes more. This approach preserves the optimization dynamics exactly (the gradient noise level is unchanged), but changes per-worker compute and memory requirements. If $B_{per\_worker}$ becomes too small after aggressive scale-up, per-GPU utilization drops because the batch cannot saturate the GPU's compute units. If $B_{per\_worker}$ becomes too large after scale-down, it may exceed per-GPU memory capacity.

**Adaptive batch size** scaling keeps $B_{per\_worker}$ constant and adjusts the learning rate using scaling rules like the linear scaling rule of Goyal et al. [@goyal2017accurate], which increases the learning rate proportionally with batch size. This approach accepts changed optimization dynamics (larger batch sizes produce lower-variance gradient estimates, which may require different learning rate schedules) in exchange for simpler worker management (each worker's compute and memory requirements remain constant regardless of group size). For many practical workloads, adaptive batch size with linear learning rate scaling produces equivalent convergence outcomes, but the interaction between batch size scaling and other training hyperparameters (warmup schedule, weight decay, momentum) requires careful tuning.

### Framework Support {#sec-fleet-orchestration-elastic-frameworks}

**TorchElastic** (torchelastic), integrated into PyTorch's `torch.distributed.elastic` module, provides the runtime infrastructure for elastic training. Jobs specify minimum and maximum worker counts, and TorchElastic manages worker lifecycle:

::: {#lst-fleet-orchestration-torchelastic lst-cap="**TorchElastic Launch Configuration**: Elastic training specification allowing a job to run with between 32 and 128 GPUs. The rendezvous backend coordinates worker group membership changes, while the framework handles gradient synchronization group reconstruction when workers join or leave."}
```{.bash}
# Launch elastic training with min 32 to max 128 workers
torchrun \
  --nnodes=4:16 \
  --nproc_per_node=8 \
  --rdzv_backend=c10d \
  --rdzv_endpoint=head-node:29500 \
  --max_restarts=3 \
  train.py
```
:::

The `--nnodes=4:16` specification declares that the job can operate with anywhere from 4 to 16 nodes (32 to 128 GPUs with 8 GPUs per node). When a node fails, TorchElastic triggers a **rendezvous**[^fn-rendezvous-elastic-fleet] where surviving workers re-form the communication group and continue training. When additional nodes become available, new workers join through the same rendezvous mechanism.

::: {.callout-perspective title="Elastic vs. Rigid Scheduling"}

Consider a congested cluster where a 128-GPU job waits 8 hours for a full gang allocation.

**Rigid scheduling**: Wait 8 hours, then train for 24 hours at full throughput. Total wall-clock: 32 hours.

**Elastic scheduling** (min 32, max 128):

- Start immediately with 32 GPUs (throughput = 25% of full)
- After 2 hours, scale to 64 (throughput = 50%)
- After 4 hours, scale to 128 (throughput = 100%)
- Training progresses during the entire wait period

Approximate work completed during the 8-hour "queue wait":

- Hours 0 to 2: 25% throughput$\times$ 2 hours = 0.5 hours equivalent
- Hours 2 to 4: 50% throughput$\times$ 2 hours = 1.0 hours equivalent
- Hours 4 to 8: 100% throughput$\times$ 4 hours = 4.0 hours equivalent
- Total: 5.5 hours of equivalent full-scale work

The elastic job completes in approximately 26.5 hours (8 hours of scaled-up training plus 18.5 hours at full scale), saving 5.5 hours compared to rigid scheduling. This 17% improvement comes entirely from using resources productively during what would otherwise be idle queue time.

:::

[^fn-rendezvous-elastic-fleet]: **Rendezvous**: From French *rendez-vous* ("present yourselves"), a coordination barrier where all participants must arrive before any can proceed. In TorchElastic, the rendezvous protocol must handle concurrent worker arrivals and departures without deadlock, typically taking 10 to 60 seconds depending on group size. When 1,000 workers simultaneously attempt rendezvous after a switch failure, this "thundering herd" can overwhelm the coordination service (etcd), turning a single hardware fault into a cascading scheduling failure. \index{Rendezvous!elastic training}

**Elastic Horovod** [@sergeev2018horovod] provides similar capabilities for Horovod-based training. Horovod's ring AllReduce implementation can dynamically reconstruct the communication ring when workers change, though the reconstruction incurs a brief pause (typically seconds) while the new ring topology is established and all workers synchronize their state.

The key architectural difference between TorchElastic and Elastic Horovod lies in their coordination models. TorchElastic uses a centralized rendezvous mechanism (backed by etcd or the C10d backend) where all workers agree on group membership through a coordinator. Elastic Horovod uses a driver process that manages the ring topology externally and notifies workers of changes. The centralized rendezvous approach is more robust to simultaneous failures (multiple workers failing at the same time) but introduces a single point of coordination that must itself be fault-tolerant. The driver-based approach is simpler to implement but may produce inconsistent states if the driver and workers disagree about group membership during rapid changes.

Both frameworks share a common limitation: the elastic transition period (the time between detecting a change and completing the group reconfiguration) is a period of reduced or zero training throughput. For TorchElastic, this transition typically takes 10 to 60 seconds depending on the number of workers and the rendezvous backend's performance. For Elastic Horovod, the transition is faster (5 to 30 seconds) but requires the driver process to be reachable. Minimizing transition time is important because frequent transitions (e.g., in a highly volatile spot instance environment) can accumulate enough overhead to negate the cost savings from elastic scheduling.

### Elastic Training Limitations and Failure Modes {#sec-fleet-orchestration-elastic-limitations}

While elastic training offers theoretical resilience, the physics of distributed deep learning imposes hard constraints on its practical utility. The most significant limitation is **incompatibility with model parallelism**. Elastic scaling is fundamentally a data-parallel operation: we add or remove replicas of the model, changing the global batch size but keeping the model architecture constant. For our 175B-parameter example, which relies on 8-way tensor parallelism within each node, elasticity operates strictly at the *node* level. The job can scale from 128 nodes (1,024 GPUs) to 120 nodes (960 GPUs), changing the data-parallel degree. However, it cannot elastically remove a single GPU from a tensor-parallel group; doing so would require re-sharding the 350 GB of model weights across the remaining 7 GPUs, a process equivalent to a full checkpoint-and-restart cycle. Similarly, pipeline parallelism allows for limited elasticity -- adding or removing pipeline stages is possible in principle, but it forces a complete recalculation of the pipeline schedule to balance computation, often negating the benefits of dynamic scaling.

At scale, the mechanics of recovery introduce their own failure modes, most notably the **communication group reconstruction storm**. When a top-of-rack switch fails, causing 64 GPUs to vanish simultaneously, the remaining 960 workers do not immediately detect the loss. Instead, they hang on collective operations until a timeout expires (typically 30 to 60 seconds). Once the timeout triggers, every surviving worker simultaneously attempts to reconnect to the coordination service (e.g., etcd or ZooKeeper) to establish a new rendezvous. This thundering herd of nearly 1,000 concurrent requests can overwhelm the key-value store, causing the rendezvous itself to time out and triggering a cascading failure loop where the job never successfully restabilizes. Production systems mitigate this through exponential backoff with jitter on rendezvous retries and by deploying high-availability coordination services with sufficient capacity to absorb burst traffic.

The mathematical integrity of the training process is also compromised by **gradient staleness during transitions**. When new workers join the fleet to replace failed ones, they receive the latest model parameters but often miss the gradient updates that were in-flight during the transition window. This results in a temporary inconsistency in the global optimizer state, manifesting as a sharp spike in the training loss curve. For a 175B model training on 1,024 GPUs, a recovery event typically incurs 30 to 90 seconds of rendezvous latency and 15 to 45 seconds of parameter broadcast overhead (saturating InfiniBand links). The resulting loss spike from stale gradients usually self-corrects within 50 to 200 iterations, but if this occurs during a critical phase -- such as a learning rate warmup or decay -- it can permanently destabilize convergence. Careful scheduling of elastic transitions to avoid sensitive training phases, combined with gradient clipping that limits the impact of stale updates, reduces this risk in practice.

### Scheduler Integration {#sec-fleet-orchestration-elastic-scheduler-integration}

Elastic training's value depends critically on scheduler integration. Without scheduler awareness, elastic training is merely a fault tolerance mechanism (replacing failed workers). With scheduler integration, it becomes a powerful scheduling optimization that fundamentally changes the economics of cluster utilization. The scheduler must understand that elastic jobs can operate within a range of resource allocations, specified as $[N_{min}, N_{max}]$ workers, enabling three key scheduling optimizations.

**Faster job start** is the most immediately valuable benefit. An elastic job requesting 32 to 128 GPUs can start as soon as 32 GPUs are available, rather than waiting in the gang scheduling queue for 128 contiguous GPUs. In congested clusters where large-job queue times can be hours to days, starting immediately at reduced scale often completes the job sooner than waiting for full-scale allocation. Consider a job that completes in 24 hours at 128 GPUs or 72 hours at 32 GPUs: if the queue wait for 128 GPUs is more than 48 hours, starting immediately at 32 GPUs and scaling up as resources become available produces faster total completion. The scheduler can compute these trade-offs dynamically, choosing the start time and initial scale that minimizes expected total time (wait time plus execution time).

**Opportunistic scaling** enables the scheduler to use elastic jobs as flexible capacity absorbers. When resources become available (other jobs complete, spot instances are provisioned, preempted jobs release resources), the scheduler can expand running elastic jobs to improve their throughput. When resources are needed (higher-priority job arrives, reserved capacity is reclaimed), the scheduler can shrink elastic jobs without preempting them entirely. This bidirectional flexibility converts elastic jobs into a buffer that absorbs utilization variance, smoothing the gap between allocated and productive capacity.

**Graceful preemption** transforms the scheduler's response to resource pressure. Instead of killing a training job outright to reclaim resources, the scheduler can request a scale-down, reducing the job's resource allocation to release capacity for higher-priority work. The job continues with fewer workers, maintaining progress at reduced throughput rather than losing all progress since the last checkpoint and incurring restart overhead. This provides the scheduler with a continuous spectrum between "full resources" and "fully preempted," rather than the binary choice of traditional scheduling. Importantly, graceful preemption composes well with checkpoint-based preemption: if the scheduler needs to reclaim all resources, it first scales the job down to its minimum size, then initiates a checkpoint-and-preempt sequence, minimizing the amount of lost work.

The trade-off for all of these benefits is complexity. Elastic training requires framework support (not all training frameworks support it), introduces potential convergence concerns from batch size changes (requiring validation that elastic scaling does not degrade model quality), and complicates performance modeling (a job's throughput is no longer constant, making capacity planning harder). Not all workloads benefit equally from elastic scaling: jobs with high communication-to-computation ratios see diminishing returns from additional workers (because communication overhead grows with worker count) and thus benefit less from scale-up, while compute-bound jobs with low communication overhead scale more linearly and benefit more from opportunistic scaling. The scheduler must model these scaling characteristics per-job to make optimal elastic scaling decisions.

### Elastic Scaling Policies {#sec-fleet-orchestration-elastic-policies}

Does adding resources to a running job always accelerate time-to-convergence? The naive assumption that throughput scales linearly with worker count collapses under the weight of communication overhead, requiring the scheduler to enforce elastic scaling policies based on empirical efficiency curves rather than resource availability alone. A job's **scaling efficiency** $E(k)$ -- defined as the ratio of observed throughput at $k$ workers to the ideal linear speedup -- typically exhibits three distinct phases: a linear phase where computation dominates communication, a sublinear phase where gradient synchronization latency begins to mask compute, and a saturation phase where the AllReduce ring becomes the bottleneck. A scheduler that blindly assigns available GPUs to a job in the saturation phase wastes cluster capacity that could have cleared the queue; conversely, running a job below its **minimum viable scale** forces it to hold onto memory and network leases for disproportionately low progress.

The decision to scale is therefore a function of marginal utility, not just vacancy. For our 175B parameter model, the efficiency curve is derived from profiling: it scales linearly from 64 to 512 GPUs, becomes sublinear between 512 and 1,024, and hits diminishing returns beyond 1,024. Consequently, the scheduler enforces an elastic range of $[256, 1024]$. If the cluster has free resources but the job is already at 1,024 GPUs, the scheduler withholds the extra nodes, predicting that the 5 percent marginal throughput gain does not justify the re-sharding overhead or the opportunity cost of starving a pending job. Similarly, if only 128 GPUs are available, the scheduler may choose to queue the job rather than run it, as the training intensity per GPU would be too low to hide the communication latency.

This logic extends to the temporal dimension, where the scheduler must weigh the immediate progress of a smaller allocation against the deferred gratification of waiting for a larger one. If a 512-GPU job can start immediately with 128 GPUs, or wait two hours for the full request, the optimal choice depends on the integration of throughput over time. The preemption overhead -- the cost of stopping, checkpointing, and restarting to resize -- must be amortized over the subsequent run duration. A policy of "opportunistic scaling" that resizes a job every time a single node becomes free often yields net negative progress due to the constant thrashing of the distributed process group.

::: {.callout-notebook title="The Elastic Scaling Decision"}

Consider a large language model training job with a target batch size that requires 512 A100s for peak efficiency. Three scheduling strategies are evaluated over a 24-hour window.

**Assumptions**: Throughput at 128 GPUs is 1.0 epoch/hour (baseline). Throughput at 256 GPUs is 1.8 epochs/hour (90 percent scaling efficiency). Throughput at 512 GPUs is 3.2 epochs/hour (80 percent scaling efficiency). Re-scaling cost is 10 minutes (0.17 hours) to checkpoint, re-shard, and restart.

**Strategy 1 -- Immediate start (no scaling)**: The job starts immediately with 128 GPUs and runs for the full 24 hours. Total work: $24 \times 1.0 = 24.0$ epochs.

**Strategy 2 -- Wait for capacity (gang scheduling)**: The job waits in the queue for 4 hours until 512 GPUs become available, then runs for 20 hours. Total work: $20 \times 3.2 = 64.0$ epochs.

**Strategy 3 -- Elastic scaling (step-up)**: The job starts immediately with 256 GPUs. After 4 hours, 256 more GPUs become available. The job pauses, scales up, and resumes. Phase 1: $4 \times 1.8 = 7.2$ epochs. Phase 2: $(24 - 4 - 0.17) \times 3.2 \approx 63.5$ epochs. Total work: $7.2 + 63.5 = 70.7$ epochs.

The elastic strategy outperforms waiting by approximately 10 percent and the immediate small-scale run by nearly 3$\times$. However, if the re-scaling cost exceeded 2 hours (due to slow checkpoint transfer or compilation), Strategy 2 would become superior.

:::

The placement and elasticity mechanisms discussed so far address individual job efficiency, optimizing how each job uses its allocated resources. The next section shifts perspective to examine how schedulers make collective decisions that affect the entire fleet's economics, treating the cluster as a financial asset whose return on investment depends on scheduling policy.

## Cost Optimization {#sec-fleet-orchestration-cost-optimization}

A 10,000-GPU cluster represents a capital investment of roughly \$300 million and consumes millions more in monthly power and cooling. If poor scheduling leaves 20% of those GPUs idling in fragmentation gaps or waiting for data, the company is incinerating \$60 million of compute capacity. Cost optimization forces the fleet orchestrator to treat the cluster not just as a computer, but as a massive financial asset whose ROI must be maximized.

@fig-cost-idle-gpus makes this waste concrete by plotting annual wasted cost as a function of cluster size and utilization level. At 10,000 GPUs with 50% utilization, the annual waste exceeds \$87 million. Published measurements from production clusters suggest this is not hypothetical: Microsoft's Philly cluster study [@jeon2019analysis] measured median utilization of approximately 52%, and Bhatele et al. (2023) found that 50% of jobs on NERSC's Perlmutter supercomputer used less than 25% of allocated GPU memory. The gap between 30% and 70% utilization, shaded in the figure, represents the operational reality for most organizations. Moving from 50% to 70% utilization on a 10,000-GPU cluster saves over \$35 million annually without purchasing a single additional GPU, making scheduling optimization one of the highest-leverage investments in ML infrastructure.

::: {#fig-cost-idle-gpus fig-env="figure" fig-pos="htb" fig-cap="**The Cost of Idle GPUs**. Annual wasted cost as a function of cluster size for different GPU utilization levels, assuming \$2/GPU-hour (cloud equivalent). The shaded region between 30% and 70% utilization represents the typical operating range for production clusters. At 10,000 GPUs with 50% utilization, organizations waste \$87.6M annually. The Microsoft Philly data point marks the published median cluster utilization of 52%. Data sources: Jeon et al. (ATC 2019), Bhatele et al. (ISC HPC 2023)." fig-alt="Line chart of annual wasted cost in millions versus cluster size with four utilization curves at 30, 50, 70, and 90 percent and shaded typical range."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ COST OF IDLE GPUS (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-cost-idle-gpus — utilization and waste economics
# │
# │ Goal: Plot annual wasted cost vs cluster size for 30/50/70/90% utilization;
# │       show $87.6M at 10K GPUs/50%; Microsoft Philly data point.
# │ Show: Four curves; shaded 30–70% typical range; annotations.
# │ How: waste = N*(1-util)*cost*HOURS_PER_YEAR; viz.setup_plot().
# │
# │ Imports: numpy (np), matplotlib.pyplot (plt), mlsys.viz (viz),
# │          mlsys.constants (HOURS_PER_YEAR)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
import matplotlib.pyplot as plt
from mlsys import viz
from mlsys.constants import HOURS_PER_YEAR

fig, ax, COLORS, plt = viz.setup_plot(figsize=(8, 5))

# Parameters
gpu_cost_per_hour = 2.0  # $/GPU-hour (from constants.py CLOUD_GPU_TRAINING_PER_HOUR)
cluster_sizes = np.linspace(1000, 100000, 200)

# Utilization levels and their colors
util_configs = [
    (0.30, COLORS['RedLine'],    '30% utilization'),
    (0.50, COLORS['OrangeLine'], '50% utilization'),
    (0.70, COLORS['GreenLine'],  '70% utilization'),
    (0.90, COLORS['BlueLine'],   '90% utilization'),
]

# Plot lines
for util, color, label in util_configs:
    annual_waste = cluster_sizes * (1 - util) * gpu_cost_per_hour * HOURS_PER_YEAR / 1e6
    ax.plot(cluster_sizes / 1000, annual_waste, color=color, linewidth=2, label=label)

# Shade typical range (30% to 70%)
waste_30 = cluster_sizes * (1 - 0.30) * gpu_cost_per_hour * HOURS_PER_YEAR / 1e6
waste_70 = cluster_sizes * (1 - 0.70) * gpu_cost_per_hour * HOURS_PER_YEAR / 1e6
ax.fill_between(cluster_sizes / 1000, waste_70, waste_30, alpha=0.12,
                color=COLORS['OrangeLine'], label='Typical range')

# Microsoft Philly data point: ~52% utilization
# Place on appropriate line (interpolate between 50% and 70%)
philly_cluster = 4  # ~4K GPUs (representative Philly-scale cluster)
philly_waste = philly_cluster * 1000 * (1 - 0.52) * gpu_cost_per_hour * HOURS_PER_YEAR / 1e6
ax.scatter([philly_cluster], [philly_waste], color=COLORS['OrangeLine'], s=100, zorder=5,
           marker='D', edgecolors='white', linewidth=1.0)
ax.annotate('Microsoft Philly\n(52% median util.)',
            xy=(philly_cluster, philly_waste), xytext=(12, philly_waste + 50),
            fontsize=8, fontweight='bold', color=COLORS['OrangeLine'],
            arrowprops=dict(arrowstyle='->', color=COLORS['OrangeLine'], lw=1.0))

# Key annotation: waste at 10K GPUs, 50% utilization
waste_10k_50 = 10000 * (1 - 0.50) * gpu_cost_per_hour * HOURS_PER_YEAR / 1e6
ax.annotate(f'At 10K GPUs, 50% util.\n= ${waste_10k_50:.1f}M wasted/year',
            xy=(10, waste_10k_50), xytext=(20, waste_10k_50 - 100),
            fontsize=8, fontweight='bold', color='crimson',
            bbox=dict(facecolor=COLORS['RedL'], edgecolor='none', alpha=0.8, pad=3),
            arrowprops=dict(arrowstyle='->', color='crimson', lw=1.2))

ax.set_xlabel('Cluster Size (thousands of GPUs)')
ax.set_ylabel('Annual Wasted Cost ($M)')
ax.legend(loc='upper left', fontsize=8)
ax.set_xlim(0, 100)
ax.set_ylim(0, max(waste_30) * 1.05)

plt.tight_layout()
plt.show()
```
:::

### Spot Instances and Preemptible VMs {#sec-fleet-orchestration-spot-instances}

Cloud providers offer **spot instances**[^fn-spot-instances-cloud] (AWS) or **preemptible VMs** (GCP) at 60 to 70 percent discounts compared to on-demand pricing, with the caveat that instances can be reclaimed with as little as 30 seconds to 2 minutes notice when the provider needs the capacity for on-demand customers. At $`{python} spot_price_str` per GPU-hour instead of $`{python} f"{gpu_hour_cost_usd:.2f}"`, spot instances transform the economics of large-scale training, potentially reducing the cost of a multi-week pre-training run from hundreds of thousands of dollars to under one hundred thousand.

[^fn-spot-instances-cloud]: **Spot Instances**: Named after commodity trading's "spot price" (the current market price for immediate delivery), spot instances sell spare cloud capacity at 60 to 90 percent discounts with the provider retaining reclamation rights. The critical asymmetry for ML training is the interruption notice window: AWS provides 2 minutes, GCP provides 30 seconds. A 175B-parameter model requires 3 to 5 minutes to checkpoint, so GCP's 30-second notice forces either continuous checkpointing overhead or acceptance of lost work on every interruption. \index{Spot Instances!cost optimization}

The viability of spot instances for ML training depends on two factors that determine whether the discount translates into actual savings: the cost of each interruption (checkpoint overhead plus restart time plus lost work since the last checkpoint) and the frequency of interruption (which varies by instance type, region, time of day, and overall cloud demand). The **effective cost** of spot training is:

$$ C_{effective} = C_{spot} \times \frac{T_{total}}{T_{productive}} $$ {#eq-fleet-orchestration-spot-cost}

where $T_{total}$ includes productive training time plus checkpoint overhead plus restart overhead after interruptions, and $T_{productive}$ is the time spent advancing training. If interruptions are rare (less than once per day) and checkpointing is efficient (less than 5 percent overhead), effective cost remains close to the spot price and the savings are nearly the full discount. If interruptions are frequent (every few hours) and restarts are expensive (large model, slow checkpoint loading, cold GPU cache), effective cost can approach or even exceed on-demand pricing, turning the apparent discount into a hidden premium.

The relationship between spot savings and fault tolerance infrastructure is circular in an important way. The fault tolerance mechanisms from @sec-fault-tolerance-reliability (frequent asynchronous checkpointing, fast checkpoint loading, elastic recovery) directly enable spot instance usage by minimizing the cost of each interruption. Elastic training allows jobs to continue with reduced worker count when some spot instances are reclaimed, avoiding full restart entirely. Organizations that invested in fault tolerance for reliability reasons discover that the same infrastructure unlocks significant cost savings through spot instance utilization, providing a return on their fault tolerance investment that goes far beyond avoiding lost work from hardware failures.

The strategic insight is that fault tolerance infrastructure has a double return: it both protects against involuntary losses (hardware failure) and enables voluntary cost savings (spot instances). This double return often justifies fault tolerance investments that would be hard to justify on reliability grounds alone, because the cost savings from spot utilization are concrete and measurable while the cost avoidance from hardware failure protection is statistical and harder to quantify.

### Spot Instance Strategies for ML Training {#sec-fleet-orchestration-spot-strategies}

To maximize spot instance utility while minimizing disruption, sophisticated schedulers employ diversification and predictive strategies that go beyond simple "retry on failure" logic.

**Availability zone diversification** reduces the probability of simultaneous fleet-wide reclamation. Cloud providers typically manage spot pools independently per availability zone (AZ). If a training job requires 1,024 GPUs, allocating them as a single block in one zone exposes the job to a complete stop if that specific zone's spot pool is reclaimed. Spreading the job across multiple AZs ensures that a reclamation event in one zone only affects a fraction of the fleet. Training our 175B model on 1,024 spot GPUs across 3 availability zones, where each AZ has a 5 percent hourly interruption probability, the probability of losing more than 128 GPUs (one full data-parallel group) in a single hour is less than 0.1 percent, making elastic recovery sufficient for most interruption events.

**Instance type diversification** exploits the fact that different GPU SKUs have independent spot markets. A job that strictly requests one instance type competes in a single, crowded market. A job that accepts multiple equivalent instance types dramatically increases its scheduling probability. The scheduler should maintain a priority-ordered list of acceptable instance types, falling back to larger-memory instances when the primary type is unavailable. While this mix requires careful peer-to-peer bandwidth management, it effectively uses the "upgrade" to maintain progress at a blended cost still far below on-demand rates.

**Spot interruption prediction** leverages the data that cloud providers expose about reclamation likelihood, such as the AWS Spot Placement Score or GCP preemptibility data. Advanced schedulers ingest this feed to estimate the expected interruptions per day for a given instance type and region. If the expected interruption rate rises above a threshold where the overhead of restarts exceeds the cost savings (typically more than 4 interruptions per day for large models), the scheduler should automatically migrate the job to on-demand capacity or a different region, preventing thrashing where a job spends more time recovering than training.

**Checkpoint frequency optimization** for spot instances differs from the standard reliability logic. The Young-Daly formula (@sec-fault-tolerance-reliability) optimizes for unpredicted hardware failures. Spot interruptions, however, come with a warning: 2 minutes on AWS, 30 seconds on GCP. The optimal strategy is a **two-tier checkpointing** approach: a periodic checkpoint at the Young-Daly interval to protect against hard crashes, combined with an immediate checkpoint triggered by the termination notice. This "panic checkpoint" captures the exact state of training seconds before the node vanishes, reducing lost work to near zero and transforming preemption from a rollback event into a pause-and-resume event.

::: {.callout-war-story title="The Spot Market Crash"}
A major AI lab configured their entire research fleet to use spot instances in a single cloud region to save costs. Two days before a top-tier conference deadline, a massive wave of last-minute experiments from thousands of researchers worldwide hit the same region. The spot market "crashed": prices spiked to equal on-demand rates, and the provider reclaimed 90 percent of the lab's spot instances within a 15-minute window. Dozens of paper-critical training runs died simultaneously. The lab had no fallback strategy to shift to on-demand capacity or other regions. The lesson: spot capacity is correlated with global research deadlines. A robust scheduler must have a "break glass" capability to flee to on-demand capacity when the spot market becomes toxic.
:::

### Capacity Reservation Strategies {#sec-fleet-orchestration-capacity-reservation}

Organizations that rely on cloud infrastructure (or that think about on-premise infrastructure in economic terms) balance three tiers of compute capacity, each offering a different trade-off between cost, availability, and commitment.

**Reserved capacity** (1 to 3 year commitments) provides guaranteed availability at 30 to 60 percent discounts compared to on-demand pricing. The discount reflects the provider's reduced risk: guaranteed revenue enables long-term capacity planning and hardware procurement. Reserved capacity forms the baseline for predictable, continuous workloads: production inference serving that must be available 24/7, regularly scheduled training runs that happen weekly or monthly, continuous integration and regression testing pipelines, and any workload where interruption would violate service level agreements. The drawback is commitment: reserved capacity costs money whether used or not, so over-reservation wastes budget while under-reservation forces reliance on more expensive tiers during peak demand.

**On-demand capacity** provides immediate availability at full price, serving as the safety valve for workloads that cannot tolerate queuing or interruption. Typical uses include urgent training runs triggered by production incidents (e.g., retraining a model after discovering data quality issues), debugging sessions where researchers need interactive GPU access, and workloads that are too short-lived to justify the complexity of spot instance management. The cost premium (30 to 60 percent more than reserved, 200 to 300 percent more than spot) limits on-demand to genuinely time-sensitive work.

**Spot capacity** provides the lowest cost for workloads that can tolerate interruption and restart, which includes a surprisingly large fraction of ML compute. Hyperparameter sweeps (individual trials are independent and can be restarted), ablation studies (same independence property), pre-training runs with robust checkpointing (the fault tolerance investment pays for itself), and exploratory research experiments all fit this category. The discount is substantial (60 to 90 percent depending on market conditions), but the availability is not guaranteed and varies with cloud demand patterns.

The optimal mix depends on workload characteristics and organizational priorities. An organization with 60 percent steady-state utilization and 40 percent burst demand might reserve capacity for 60 percent of peak need, use on-demand for latency-sensitive bursts that require immediate provisioning, and use spot for the remainder. The scheduling system must understand these capacity tiers and route workloads accordingly: inference workloads to reserved capacity for guaranteed availability, large training jobs to a mix of reserved (for the base allocation) and spot (for additional workers via elastic scaling), and exploratory work to spot exclusively where the cost savings are maximized and the interruption tolerance is highest.

::: {.callout-perspective title="Spot vs. On-Demand Training"}

Consider training a 70B parameter model requiring 512 GPUs for 14 days.

**On-demand**: 512 GPUs$\times$ 336 hours$\times$ \$2.00/GPU-hour = **\$344,064**

**Spot** (65% discount, 5% checkpoint overhead, 1 interruption/day with 30 min restart):

- Base cost: 512$\times$ 336$\times$ \$0.70 = \$120,422
- Checkpoint overhead: 5%$\times$ 336 hours = 16.8 extra hours
- Restart overhead: 14 interruptions$\times$ 0.5 hours = 7 hours
- Total time: 336 + 16.8 + 7 = 359.8 hours
- Total cost: 512$\times$ 359.8$\times$ \$0.70 = **\$128,872**

**Savings**: \$215,192 (63%), at the cost of approximately 7% longer wall-clock time.

The economics are compelling when fault tolerance infrastructure is already in place. Without checkpointing, a single interruption forces restart from scratch, potentially wasting days of compute and negating all savings.

:::

### Scheduling for Cost Efficiency {#sec-fleet-orchestration-cost-scheduling}

Cost-aware scheduling extends the scheduler's optimization objective beyond utilization and fairness to include financial efficiency. Traditional schedulers minimize a single objective (e.g., average job completion time or maximum wait time), but cost-aware schedulers must navigate multi-objective trade-offs where time, money, and reliability are all decision variables. The scheduler must routinely make decisions such as: Should this job run on reserved A100s now, or wait 2 hours for spot H100s that would complete the job 3$\times$ faster at half the cost? Should a low-priority hyperparameter sweep be preempted to make room for a high-priority training run on reserved capacity, or should the training run be placed on more expensive on-demand instances? When spot capacity is reclaimed, should the scheduler migrate the job to on-demand (maintaining throughput at higher cost) or scale down elastically (reducing throughput but maintaining low cost)?

These decisions require the scheduler to model both the time-value of computation (how much does a 2-hour delay cost the organization in terms of researcher productivity, product launch schedules, or competitive positioning?) and the financial cost of different resource allocations. The time-value varies enormously by workload type: delaying a production model retrain by 2 hours might violate an SLA and cost thousands in penalty fees, while delaying an exploratory experiment by 2 hours costs nothing but researcher patience.

Production systems typically define **cost policies** per workload class that encode these trade-offs as configuration rather than requiring per-decision human judgment. Inference workloads are pinned to reserved capacity for guaranteed availability and predictable latency, since the cost of an inference outage (lost revenue, degraded user experience) far exceeds the premium of reserved pricing. Large training runs use a mix of reserved capacity (for the minimum viable allocation) and spot capacity (for elastic expansion), with automatic fallback to on-demand if spot is reclaimed and the job is within a configurable deadline. Exploratory workloads are restricted to spot to minimize cost, with the understanding that they may be interrupted and must tolerate variable completion times.

Technical optimization must be paired with financial accountability through **ML FinOps**. In many organizations, GPU costs are pooled into a generic "infrastructure" bucket, creating a tragedy of the commons where teams lack visibility into their resource consumption and have no incentive to optimize. Effective governance requires granular tagging to implement **showback** (reporting costs to teams) or **chargeback** (billing teams' budgets). Metrics must evolve from "GPU hours" to business-aligned units: "Cost per Model Version," "Cost per Experiment," and "Cost per 1M Predictions." This shift transforms cost from an opaque infrastructure expense into a measurable input to product decisions, enabling teams to make informed trade-offs between model quality, training speed, and infrastructure cost.

### The Total Cost of Ownership {#sec-fleet-orchestration-tco}

Optimizing for spot instance availability or scheduling density addresses only the visible tip of the infrastructure iceberg. The **Total Cost of Ownership (TCO)** for a machine learning fleet extends far beyond simple GPU-hours. In high-performance clusters, the compute silicon typically accounts for only 40 to 60 percent of the total system cost. The remainder is consumed by the supporting infrastructure required to keep that silicon fed and cool: high-bandwidth networking (InfiniBand switches, optics, and cabling), parallel file systems for checkpointing, and specialized power and cooling delivery. A single rack of H100 nodes can draw upwards of 40 kW -- ten times the density of a standard web server rack -- forcing facilities to deploy liquid cooling or rear-door heat exchangers that radically alter the facility's capital requirements.

For an organization deciding between building a private cluster or renting cloud capacity, the decision hinges on the **amortization equation**. The TCO of an on-premise cluster over a typical three-year hardware lifecycle is the sum of Capital Expenditure ($C_{cap}$), Operational Expenditure ($C_{ops}$), and the often-overlooked Opportunity Cost ($C_{opp}$) of tying up capital in depreciating assets:

$$ \text{TCO}_{3\text{yr}} = C_{cap} + 3 \times C_{ops} + C_{opp} $$ {#eq-fleet-orchestration-tco}

$C_{cap}$ is substantial: a cluster of 1,024 H100 GPUs requires approximately \$40M in servers and another \$10M in networking and storage fabric. $C_{ops}$ scales with utilization but includes a high fixed baseline for datacenter space, power availability guarantees, and the Site Reliability Engineering teams required to manage the complex software stack.

The break-even point between cloud and on-premise is strictly a function of utilization. Cloud pricing models charge a premium for elasticity -- the ability to scale to zero. Owning hardware eliminates the premium but introduces the risk of idle silicon. If a cluster runs at 80 percent utilization, the effective hourly cost of on-premise hardware drops to nearly half that of on-demand cloud instances, typically reaching break-even within 12 to 18 months. However, if utilization falters below 40 percent due to poor scheduling or fragmentation, the fixed amortization costs of owned hardware make it significantly more expensive than the cloud, where you only pay for what you use.

Consider our 175B parameter model. Training requires approximately 34 days on 1,024 H100 GPUs. On a cloud provider using reserved instances at approximately \$4 per GPU-hour, a single training run costs roughly \$3.3 million. Building the cluster requires a \$50M upfront investment. To justify this capital outlay, the organization must run this training workload (or equivalent jobs) at least 15 times over the hardware's life just to match the raw cloud cost, before factoring in electricity (\$0.10/kWh$\times$ 1.5 MW $\approx$ \$100K per run) and engineering salaries. If the team only trains one large model every quarter, the on-premise cluster becomes a financial liability; if they are training continuously, the owned infrastructure eventually yields effectively free compute after the break-even date.

::: {.callout-checkpoint title="Cost-Aware Scheduling Trade-offs"}

Before proceeding to ML-specific schedulers, verify your understanding of cost optimization:

- [ ] A training job requires 512 GPUs for 14 days. Spot instances offer 65% discount but have 1 interruption per day with 30-minute restart overhead. Under what checkpoint frequency does spot training become more expensive than on-demand?
- [ ] Why does fault tolerance infrastructure have a "double return" for both reliability and cost savings? What is the minimum fault tolerance investment needed to make spot instances viable?
- [ ] An organization runs 40% of its workloads on reserved capacity. How would you determine whether increasing the reservation to 60% would save or waste money?

:::

General-purpose orchestration and cost-saving measures treat ML models as opaque boxes, unaware of their convergence properties. However, significant efficiency gains remain locked inside the training loop itself. The next section explores custom ML schedulers -- specialized policies like Tiresias and Gandiva -- that peer inside the workload to optimize for time-to-accuracy rather than just time-to-completion, prioritizing jobs based on their real-time learning dynamics.

## Custom ML Schedulers {#sec-fleet-orchestration-custom-schedulers}

If a scheduler knew that a deep learning model was in the final, diminishing-returns phase of its learning curve, it could dynamically shrink its GPU allocation and give those resources to a brand-new experiment that would benefit more. General-purpose schedulers treat jobs as opaque black boxes, but custom ML schedulers peer inside the training loop to optimize for time-to-accuracy rather than just time-to-completion.

### Tiresias: Duration-Agnostic Scheduling {#sec-fleet-orchestration-tiresias}

**Tiresias** [@gu2019tiresias] addresses the fundamental problem identified in @sec-fleet-orchestration-gang-scheduling: ML job durations are unpredictable, and backfill scheduling degrades when users cannot provide accurate runtime estimates. Users consistently overestimate or underestimate runtimes by 2 to 5$\times$, and the game-theoretic incentives encourage overestimation (to avoid being killed before completion), further degrading scheduler performance.

Rather than fighting this information asymmetry, Tiresias eliminates the requirement for duration estimates entirely. It uses a **two-dimensional attained service**[^fn-attained-service-scheduling] scheduler that makes priority decisions based on what a job has already consumed, not what it claims it will consume. Jobs accumulate "service" based on GPU-time consumed, with priority decreasing as service increases. The two dimensions are time (how long the job has been running) and resources (how many GPUs the job uses), capturing both elapsed duration and resource intensity.

[^fn-attained-service-scheduling]: **Attained Service Scheduling**: A discipline from OS queuing theory where priority decreases with cumulative resource consumption, approximating the theoretically optimal Shortest Remaining Processing Time (SRPT) policy without requiring the future knowledge that SRPT demands. In ML clusters, this approximation is particularly effective because job durations are heavy-tailed: the majority of submitted jobs are short experiments, so deprioritizing high-service jobs correctly identifies the long-running outliers without requiring users to provide runtime estimates they cannot accurately compute. \index{Attained Service!scheduling}

A discretized version groups jobs into service bins (e.g., less than 1 GPU-hour, 1 to 10 GPU-hours, 10 to 100 GPU-hours, greater than 100 GPU-hours), promoting jobs in lower bins to the front of the queue. This bin structure means that all short jobs (the majority of submitted work) receive high priority and run quickly, while the few long jobs that consume most cluster resources gradually lose priority and are deprioritized relative to new arrivals. The key insight is that this behavior approximates the theoretically optimal Shortest Remaining Processing Time (SRPT) policy without requiring the future knowledge that SRPT demands: jobs that have consumed little service are statistically likely to be short jobs, so prioritizing them is a good heuristic for minimizing average completion time.

Experiments on production cluster traces from Microsoft and Alibaba show 40 to 60 percent reduction in average job completion time compared to FIFO scheduling, with the largest improvements for short jobs that previously waited behind long-running training runs. The improvement is not free: long-running training jobs experience increased completion times because they are systematically deprioritized. However, the aggregate benefit is positive because there are many more short jobs than long ones, and the short-job improvement exceeds the long-job penalty in total.

### Gandiva: Iteration-Aware Scheduling {#sec-fleet-orchestration-gandiva}

**Gandiva** [@xiao2018gandiva] exploits a characteristic that general-purpose schedulers completely ignore: the iterative nature of deep learning training. Each training iteration follows a predictable, repeating pattern: a GPU-intensive forward pass, a GPU-intensive backward pass, a communication-intensive gradient synchronization, and then a CPU-intensive data loading and preprocessing phase before the next iteration. During the data loading phase, the GPU sits partially idle, computing at less than full utilization while waiting for the next batch of data to be prepared.

Gandiva **time-slices GPU access at iteration boundaries**, enabling higher utilization through controlled oversubscription. A cluster with 100 GPUs might support 120 concurrent jobs if each spends 20 percent of its iteration time waiting for data, because the scheduler can interleave data loading from one job with GPU computation from another on the same device. The critical insight that makes this practical is that iteration boundaries provide natural preemption points where GPU state is minimal. Between iterations, only model weights and optimizer state reside on the GPU; the intermediate activations from forward and backward passes have been freed. This minimal state makes context switching cheap (seconds rather than minutes), unlike preempting mid-iteration when the full activation memory is in use.

Gandiva also implements **grow-shrink elasticity** at a finer granularity than the framework-level elastic training discussed in @sec-fleet-orchestration-elastic-training. Gandiva automatically adjusts data parallelism degree based on real-time resource availability, using profiled iteration times to predict the throughput impact of adding or removing workers. When a high-priority job arrives, Gandiva shrinks lower-priority jobs by reducing their worker count rather than killing them entirely, preserving their progress while freeing resources. When resources become available again, it grows jobs back to their preferred parallelism degree. This fine-grained elasticity, informed by actual iteration-level profiling data, enables scheduling decisions that general-purpose systems cannot make because they lack visibility into job internals.

### Themis: Finish-Time Fairness {#sec-fleet-orchestration-themis}

Traditional fair-share scheduling treats all GPU-seconds equally: a job consuming 100 GPU-hours is treated the same whether those hours represent the first 10 percent of a long training run or the last 10 percent of a nearly complete one. From a resource accounting perspective, 100 GPU-hours is 100 GPU-hours regardless of context. **Themis** [@mahajan2020themis] argues this resource-centric view is fundamentally unfair because it ignores the sunk cost of work already performed.

Themis defines a **finish-time fairness** metric that allocates resources to minimize the maximum slowdown any job experiences relative to exclusive access (the hypothetical scenario where the job has the entire cluster to itself). Under this metric, a job that is 90 percent complete and needs only 10 more GPU-hours to finish receives higher priority than a job that is 10 percent complete and needs 90 more GPU-hours. The reasoning is economic: delaying the nearly-complete job by an hour wastes the 900 GPU-hours already invested (because those hours only produce value when the job completes), while delaying the early-stage job by an hour has a proportionally smaller impact on the total investment's return.

This approach benefits shorter jobs and nearly-complete jobs without excessive penalty to longer ones, and it aligns scheduling decisions with the economic value of completing work rather than the simple accounting of resource consumption. Themis implements this metric through an auction mechanism where jobs bid for resources based on their current marginal value (how much their completion time improves per GPU-hour allocated), creating a market-like dynamic that naturally directs resources to their highest-value use.

### Pollux: Adaptive Resource Allocation {#sec-fleet-orchestration-pollux}

**Pollux** [@qiao2021pollux] takes the most aggressive approach of the four research schedulers by jointly optimizing resource allocation and training hyperparameters. Where Tiresias, Gandiva, and Themis make scheduling decisions *about* jobs (when to run, how to share), Pollux makes decisions *within* jobs (how many GPUs and what batch size). The key observation is that the optimal number of GPUs for a training job depends on the current batch size, learning rate, and gradient noise level, all of which change during training as the model moves through different phases of convergence.

Pollux dynamically adjusts each job's GPU allocation and batch size to maximize **cluster-wide goodput**, defined as the rate of useful training progress across all jobs simultaneously. The goodput metric accounts for statistical efficiency (how much each gradient step advances convergence, which depends on batch size) and system throughput (how many gradient steps per second, which depends on GPU count and communication overhead). A job experiencing diminishing returns from its current GPU count (because communication overhead is growing superlinearly) may have GPUs reassigned to a job that would benefit more (because it is in a phase of training where larger batches improve statistical efficiency).

This co-optimization of scheduling and hyperparameters achieves 37 to 50 percent improvement in average job completion time compared to scheduling with fixed resource allocations. The improvement comes from two sources: better resource allocation (GPUs go to jobs where they produce the most progress) and better batch size tuning (each job runs at a batch size that balances statistical and computational efficiency for its current training phase). The combined effect is greater than either optimization alone.

### Scheduler Comparison Framework {#sec-fleet-orchestration-scheduler-comparison}

Custom ML schedulers represent a progressive trade-off between implementation complexity and scheduling precision. While general-purpose schedulers treat jobs as opaque boxes, these research systems open the box to exploit specific characteristics of deep learning workloads: their iterative nature, their malleability (elasticity), and their predictability.

| **Scheduler** | **Key Insight**                            | **Scheduling Signal** | **Optimization Target**  | **Overhead**                | **Production Readiness**    |
|:--------------|:-------------------------------------------|:----------------------|:-------------------------|:----------------------------|:----------------------------|
| **Tiresias**  | Attained service predicts remaining time   | GPU-hours consumed    | Minimize avg JCT         | Low (no profiling)          | Medium                      |
| **Gandiva**   | Iteration boundaries are preemption points | Iteration timing      | Maximize utilization     | Medium (profiling)          | Medium                      |
| **Themis**    | Sunk cost matters for fairness             | Completion percentage | Minimize max slowdown    | Low (job metadata)          | Low (auction complexity)    |
| **Pollux**    | Batch size and allocation are coupled      | Goodput (stat + sys)  | Maximize cluster goodput | High (continuous profiling) | Low (framework integration) |

: **Custom ML Scheduler Comparison**: A taxonomy of research schedulers based on the specific workload characteristic they exploit. Tiresias and Themis optimize for queuing metrics (JCT, fairness) using external signals, while Gandiva and Pollux optimize for efficiency using internal profiling data. {#tbl-fleet-orchestration-scheduler-comparison}

This design space reveals a fundamental tension: scheduler complexity versus scheduling quality. Tiresias operates with the least information -- it essentially guesses that "young" jobs will finish soon -- yet achieves significant gains simply by preventing large jobs from blocking small ones. It is robust because it requires no cooperation from the user or the training framework. At the other extreme, Pollux requires deep bidirectional integration: the scheduler must know the job's scaling curve and gradient noise scale, and the job must accept commands to change its batch size and learning rate on the fly. When this integration works, it produces the highest cluster-wide throughput; when the assumptions are violated (e.g., a model with unstable convergence dynamics that cannot tolerate batch size changes), the optimization collapses.

Failure modes generally track this complexity gradient. Tiresias fails gracefully: if its assumption (that past usage predicts future usage) is wrong, it simply degrades to a standard fair-share policy. Gandiva's failure mode is performance jitter: if the profiling phase misestimates iteration variance, it may pack incompatible jobs onto the same node, causing interference. Pollux has the most brittle failure mode because it modifies the training semantics themselves; an incorrect goodput model could theoretically harm model convergence, a risk that most production teams are unwilling to take for a 15 percent efficiency gain.

For our 175B parameter model, these trade-offs dictate a hybrid lifecycle. Pollux offers the highest potential improvement during the early, unstable phase of training where batch sizes are small and the job is elastic; it could dynamically resize the job to fit available holes in the cluster. However, effectively using Pollux requires modifying the training loop to be elasticity-aware. Tiresias offers the simplest deployment path: it would treat the 175B model as a "heavy" job and deprioritize it relative to small experiments, ensuring the cluster remains responsive to researchers without requiring any changes to the 175B model's code. In practice, most infrastructure teams start with Tiresias-like policies to solve the "blocked queue" problem before attempting the deep integration required by Pollux.

The research scheduler landscape demonstrates a consistent theme across all four systems: exploiting domain-specific knowledge about ML workloads enables dramatically better scheduling outcomes than generic approaches. General-purpose cluster schedulers like Kubernetes and Slurm treat machine learning jobs as opaque boxes with static resource requirements, leading to severe inefficiency in deep learning clusters. They fail to exploit three fundamental properties of ML workloads: predictable iteration times, diminishing returns from parallel scaling, and inherent checkpoint-restart capability. A generic scheduler sees a job requesting 64 GPUs for 3 days; an ML-aware scheduler sees a stochastic gradient descent process that converges non-linearly, releases resources if performance plateaus, and can be preempted with minimal loss.

Tiresias optimizes for job completion time (JCT) by leveraging the observation that ML job duration is often unpredictable *a priori* but becomes predictable as training progresses. By using "attained service" as a proxy for remaining time, it prioritizes jobs with the least attained service, preventing long-running experimental trials from blocking short debugging jobs. Gandiva takes a different approach, optimizing for cluster-wide efficiency through aggressive time-slicing and migration. It packs multiple jobs onto the same GPUs to maximize utilization during I/O stalls, performing "suspend-resume" operations with low overhead to service interactive jobs immediately. Themis introduces fairness into this equation using an auction-based mechanism where users bid for resources, ensuring that no single hyperparameter sweep dominates the cluster, while Pollux advances the paradigm by co-optimizing system-level resource allocation with model-level batch size adaptation. Pollux dynamically resizes jobs based on their "goodput" (useful statistical progress), acknowledging that doubling GPUs rarely yields a 2× convergence speedup due to synchronization overhead.

The decision framework for selecting a scheduler depends on the primary bottleneck. For exploratory clusters with high churn and many interactive debugging sessions, Gandiva's time-slicing minimizes queuing delay. For production training clusters where job completion time is the primary SLA, Tiresias's age-based prioritization prevents starvation. For large-scale training where resource efficiency is paramount, Pollux's adaptive scaling offers the highest ROI, effectively reclaiming the "elasticity gap" that static allocation leaves on the table.

The challenge for production systems is incorporating these insights while maintaining the operational reliability, policy flexibility, and organizational governance that production environments require. No production system has fully adopted any single research scheduler, but the ideas from these systems are gradually being incorporated into extensions for Slurm and Kubernetes.

### From Research to Production {#sec-fleet-orchestration-research-to-production}

While research schedulers like Tiresias and Pollux offer theoretically optimal placement strategies, deploying them into a live production environment reveals a chasm between the clean abstractions of a paper and the messy reality of a datacenter. In research, the cluster is often assumed to be a homogeneous pool of accelerators with uniform interconnect topology, and jobs are well-behaved entities that declare their resource needs accurately. In production, the "cluster" is a geological formation of hardware generations -- V100s alongside A100s and H100s -- connected by a patchwork of InfiniBand and Ethernet, running jobs that frequently crash, hang, or misrepresent their memory requirements. Consequently, no major hyperscaler simply "installs" a research scheduler; instead, they selectively transplant specific algorithms into standard orchestrators via Kubernetes operators or Slurm plugins.

Consider our 175B-parameter model training run. A pure implementation of Pollux might aggressively resize the job based on global cluster load, scaling the number of GPUs up and down to maximize goodput. However, changing the worker count for a synchronous training job requires a checkpoint-restart cycle, which for a model of this size involves writing terabytes of optimizer state to persistent storage. If the re-scaling interval is too short, the I/O overhead of checkpointing negates the throughput gains. In practice, production teams adopt a hybrid approach: they use Pollux-style adaptive allocation primarily during the volatile early phase of training or for hyperparameter sweeps, but lock the 175B model into a static, topology-aware allocation once the learning rate warmup completes and the job enters its months-long steady state.

The build-versus-buy decision for a custom scheduler is a substantial capital investment. Developing a robust, fault-tolerant scheduler that outperforms the default bin-packing behavior of Kubernetes requires a specialized team of 3 to 5 systems engineers working for 6 to 12 months. Is this effort justified? If the team can improve overall cluster utilization by just 10 percent on a fleet of 10,000 H100 GPUs (approximately \$25,000 per hour in capital depreciation and power), the savings amount to roughly \$22 million per year. This stark return on investment drives the development of internal scheduling platforms at every major AI lab, transforming the scheduler from a mere utility into a primary lever for operational efficiency.

::: {.callout-war-story title="The Scheduler That Optimized the Wrong Metric"}
A platform team at a major autonomous vehicle company deployed a custom scheduler designed to minimize average job completion time (JCT), a standard academic metric. The logic was sound: by prioritizing shorter jobs, the queue clears faster, and developer velocity increases.

The result was a disaster for the model architecture team. The scheduler correctly identified that their massive Transformer pre-training jobs would take weeks to complete. To minimize average JCT, it continuously preempted these long-running jobs to squeeze in thousands of short, minute-long debugging and visualization tasks submitted by other teams. While the *average* wait time dropped by 40 percent, the P99 wait time for critical-path training jobs exploded from 4 hours to 9 days. The core product -- the autonomous driving model -- effectively stopped learning for two weeks. The team learned that optimizing for an aggregate metric without distinct Quality of Service (QoS) tiers for different workload classes (interactive versus batch versus production) is a recipe for organizational paralysis.
:::

::: {.callout-checkpoint title="Custom Scheduler Design Space"}

Consider the trade-offs between the four research schedulers examined above:

- [ ] Tiresias eliminates runtime estimates. What information does it sacrifice, and when would this sacrifice hurt scheduling quality?
- [ ] Gandiva time-slices at iteration boundaries. For which workloads would this approach fail, and why?
- [ ] Themis prioritizes nearly-complete jobs. Could this starve long-running pre-training jobs indefinitely?
- [ ] Pollux adjusts both allocation and hyperparameters. What happens if the goodput model is wrong?

:::

While custom schedulers maximize throughput for long-running training jobs, the operational reality changes drastically when models are deployed for inference. The objective function flips from maximizing cluster utilization to guaranteeing millisecond-level responsiveness. The next section examines serving resource management, where the challenge lies in handling bursty, unpredictable traffic patterns while adhering to strict Service Level Objectives.

## Serving Resource Management {#sec-fleet-orchestration-serving}

Training jobs are predictable, batch-oriented beasts that run for weeks; serving workloads are volatile, user-facing services that must respond in milliseconds to unpredictable traffic spikes. When an e-commerce platform launches a major sale, the serving fleet must instantly autoscale to handle thousands of requests per second. Serving resource management forces the orchestrator to balance these aggressive latency constraints against raw hardware utilization.

The serving infrastructure analyzed in @sec-inference-scale provides the architectural foundations for efficient inference execution on individual accelerators. This section examines how the fleet orchestrator manages the resource envelope around those serving systems: scaling replica counts up and down with demand, isolating serving workloads from interference, and balancing inference resource needs against training's claims on the same shared infrastructure.

### Autoscaling for Inference {#sec-fleet-orchestration-autoscaling}

**Horizontal Pod Autoscaling (HPA)** adjusts replica counts based on observed metrics, adding instances when load increases and removing them when load decreases. The default autoscaling metric for general cloud workloads is CPU utilization, with targets of 50 to 70 percent. This default poorly reflects GPU inference workloads for two reasons. First, GPU utilization is a noisy metric that can be high even when the system is not processing user requests (background maintenance, model warmup). Second, the relationship between GPU utilization and inference latency is highly nonlinear: latency can spike dramatically when utilization crosses a threshold (typically 70 to 80 percent for GPU inference), making utilization-based scaling reactive rather than predictive. Effective ML autoscaling uses custom metrics that better predict user-facing performance before degradation occurs:

| **Metric**              | **Target Range**  | **Considerations**                                             |
|:------------------------|:------------------|:---------------------------------------------------------------|
| **GPU utilization**     | 60 to 80%         | Varies by model batch efficiency                               |
| **Request queue depth** | 10 to 50 requests | Prevents latency spikes before they manifest in P99 metrics    |
| **P99 latency**         | Below SLO target  | Reactive metric that lags demand changes by seconds to minutes |
| **Pending tokens**      | Model-specific    | LLM-specific metric that accounts for KV cache memory growth   |

: **Inference Autoscaling Metrics**: Custom metrics for GPU inference workloads capture the relationship between load and user-facing performance more accurately than default CPU utilization. Queue depth provides a leading indicator of latency degradation, while pending tokens captures the memory pressure unique to autoregressive LLM serving. {#tbl-fleet-orchestration-autoscaling-metrics}

**Vertical Pod Autoscaling (VPA)** adjusts resource requests and limits for individual pods, operating on a different axis than HPA. Where HPA changes how many instances run, VPA changes how much resource each instance receives. For inference, VPA can right-size memory allocations based on observed usage patterns, preventing over-provisioning of CPU memory and host resources that reduces the number of inference pods that fit on each node. GPU resources cannot be vertically scaled without pod restart (the CUDA context must be reinitialized), limiting VPA's utility for accelerated workloads where model loading takes minutes. However, VPA is valuable for the CPU components of inference pipelines, such as preprocessing, postprocessing, and tokenization, where resource requirements may differ significantly from initial estimates and can be adjusted with a brief pod restart.

LLM inference requires specialized scaling considerations that neither HPA nor VPA fully address, due to the **key-value (KV) cache**[^fn-kv-cache-serving-fleet] memory growth pattern. A 70B parameter model serving long-context requests may require more than 80 GB of GPU memory for KV cache alone, even with PagedAttention optimizations detailed in @sec-inference-scale. This means that the GPU memory bottleneck for LLM serving is not the model weights (which are fixed) but the KV cache (which grows with request count and context length). Scaling decisions must account for both request rate and context length distribution, not just computational load. A sudden increase in requests with long contexts (e.g., a shift from short chatbot queries to document summarization workloads) can exhaust GPU memory even at moderate request rates, requiring scale-out despite low GPU compute utilization. Conversely, a burst of many short-context requests may stress compute without threatening memory limits.

[^fn-kv-cache-serving-fleet]: **Key-Value (KV) Cache**: Stores precomputed attention key and value tensors to avoid recomputing attention over the entire sequence for each new token. KV cache grows linearly with sequence length and batch size; for a 70B model with 128K context, the cache can exceed the model weights in memory consumption. This growth pattern creates a scheduling failure mode invisible to compute-based autoscalers: GPU memory exhaustion at low compute utilization, causing requests to stall even when the GPU appears idle. \index{KV Cache!serving}

This dual resource dimension (compute and memory) suggests that effective LLM autoscaling should monitor both GPU compute utilization and GPU memory pressure, scaling out when either approaches critical thresholds. Some production systems define a composite scaling metric that combines these dimensions, triggering scale-out when the maximum of normalized compute utilization and normalized memory pressure exceeds a threshold. This composite approach prevents the system from being surprised by either type of resource exhaustion.

Autoscaling for inference must also handle **cold start latency**, which creates a tension between cost efficiency and responsiveness that has no simple resolution. The "cold start" problem in serving is quantitatively more severe than in standard web services due to the sheer mass of model weights. Loading a large model into GPU memory takes 30 seconds to several minutes, depending on model size, quantization level, and storage bandwidth (NVMe SSD versus network storage). A 70B parameter model stored in FP16 occupies approximately 140 GB on disk and requires transferring all weights to GPU memory before the first inference request can be served. Loading a 70B parameter model (approx. 140GB in FP16) into VRAM is bound by PCIe or NVLink bandwidth. Even at a high-end PCIe Gen4 speed of 64GB/s, simple data transfer takes over 2 seconds; in practice, model initialization, graph compilation, and safety checks often extend this to 60-120 seconds. Even with NVMe SSDs delivering 7 GB/s read bandwidth, loading takes approximately 20 seconds; with network storage, it can exceed 2 minutes.

Aggressive scale-down policies that terminate replicas during brief traffic lulls create painful cold starts when traffic returns minutes later, producing latency spikes that violate SLOs and degrade user experience. The fundamental problem is that inference traffic exhibits burstiness at multiple timescales (seconds, minutes, hours), and short-term lulls do not reliably predict sustained low demand. A five-minute quiet period might be followed by a traffic spike, and the cost of a cold start during that spike (degraded latency for all requests while the model loads) can exceed the cost savings from the few minutes of freed GPU capacity.

Production systems address cold start through several complementary strategies. Maintaining a **minimum replica count** above zero ensures at least one warm replica is always available, eliminating cold starts for the first burst of traffic but incurring a baseline cost even during truly idle periods. **Predictive scaling** based on historical traffic patterns (diurnal cycles, day-of-week patterns, known events) scales up before expected peak hours rather than reacting to load, avoiding the cold-start window entirely for predictable traffic patterns. **Pre-warming** model replicas on standby GPUs loads the model into GPU memory without actively serving traffic, reducing the cost of activation from minutes (model loading) to seconds (process initialization). The trade-off is that standby replicas consume GPU memory that cannot be used for other workloads. Organizations with multiple models competing for the same GPU pool must carefully balance the pre-warming overhead against the cold-start penalty, choosing which models to keep warm based on their traffic patterns and latency requirements.

### Predictive Autoscaling and SLO Management {#sec-fleet-orchestration-predictive-autoscaling}

**Reactive autoscaling** (like Kubernetes HPA) is inherently backward-looking: it observes a metric breach (e.g., GPU utilization exceeding 80 percent), waits for a stabilization window, and then triggers a scale-up event. For a container that starts in milliseconds, this lag is negligible. For a 175B parameter model that takes 3 minutes to load weights from disk to GPU memory, this lag is fatal to SLOs. If traffic doubles in 30 seconds -- a realistic scenario during a product launch or viral event -- the reactive scaler will provision new replicas only *after* the surge has already saturated the existing fleet, causing a 3-minute window of degraded latency and dropped requests.

**Predictive autoscaling** decouples scaling actions from current load. By analyzing historical traffic patterns (diurnal cycles, day-of-week seasonality) and incorporating real-time leading indicators (e.g., a surge in login requests often precedes a surge in inference queries), the scheduler can pre-provision capacity *before* the demand arrives. Serving our 175B model with a 500 ms P99 SLO requires this anticipation. If the model takes 3 minutes to become ready, the predictive scaler must issue the scale-up command at least 4 minutes before the expected traffic ramp. This transforms the scaling problem from a control theory problem (reacting to error) to a forecasting problem (predicting the future).

Effective scaling policies must be **SLO-driven** rather than utilization-driven. Targeting a fixed utilization (e.g., "keep GPU at 70 percent") is a proxy that often fails: a model might hit its latency SLO at 60 percent utilization due to memory bandwidth contention, or it might safely run at 90 percent utilization if the requests are compute-bound and uniform. An SLO-driven policy explicitly targets the metric that matters: "Scale up when P99 latency exceeds 400 ms (80 percent of the 500 ms target)." This approach automatically adapts to changes in workload characteristics. If a new model version is less efficient, the latency metric will rise faster, and the scaler will provision more replicas to maintain the SLO, without requiring manual tuning of utilization thresholds.

To prevent **oscillation**, where the scaler rapidly adds and removes replicas during noisy traffic, production systems implement **hysteresis** and **cooldown periods**. A typical policy scales up aggressively (no stabilization window) to protect the SLO, but scales down conservatively (15-minute cooldown) to avoid thrashing. This asymmetry acknowledges that the cost of an unnecessary scale-up (wasted compute for 15 minutes) is far lower than the cost of a missed scale-up (SLO violation and user churn).

::: {.callout-notebook title="The Autoscaling Lag"}

Consider a serving fleet with 10 replicas, each capable of 5 queries per second (QPS) while meeting the 500 ms SLO. Total capacity: 50 QPS.

**Scenario**: Traffic ramps from 40 QPS to 80 QPS linearly over 60 seconds. Model load time: 3 minutes (180 seconds).

**Reactive scaling**: HPA detects overload at $t = 15$ s (when traffic hits 50 QPS). It requests 10 new replicas. These replicas become ready at $t = 195$ s (15 + 180). From $t = 15$ s to $t = 195$ s (3 minutes), demand exceeds capacity. At peak (80 QPS), 30 QPS are queued or dropped. Over this 3-minute window, approximately 3,600 requests (30 QPS$\times$ 120 s average excess) violate the SLO.

**Predictive scaling**: The scaler forecasts the ramp and triggers scale-up at $t = -180$ s. The 10 new replicas come online at $t = 0$ s, pushing capacity to 100 QPS just as the ramp begins. Zero SLO violations.

:::

### Resource Isolation {#sec-fleet-orchestration-resource-isolation}

When multiple inference workloads share the same physical hardware, **noisy neighbor problems** arise when one workload's resource consumption degrades another's performance. On GPUs, interference manifests through several shared resource channels: memory bandwidth (one workload's data movement saturates the HBM interface), L2 cache contention (one workload's working set evicts another's cached data), PCIe bottlenecks (concurrent host-device transfers compete for bus bandwidth), and even thermal effects (one workload's heat generation causes thermal throttling that affects all workloads on the same GPU). For latency-sensitive inference, even minor interference can push P99 latency above SLO thresholds, turning a seemingly well-provisioned system into an SLO-violating one.

MIG provides hardware isolation that eliminates most interference channels, as discussed in @sec-fleet-orchestration-kubernetes. Each MIG instance has dedicated memory, cache, and compute resources, preventing cross-instance interference at the hardware level. However, MIG has limitations: it is available only on A100 and later GPUs, it requires pre-configured partition profiles that cannot change without draining all workloads from the GPU, and the fixed partition sizes may not match workload requirements (a workload that needs 15 GB of GPU memory wastes 5 GB on a 20 GB MIG instance or cannot fit on a 10 GB instance).

Software approaches to isolation provide more flexibility at the cost of weaker guarantees. Efficiently sharing GPUs for inference requires navigating a hierarchy of isolation mechanisms, each trading performance for security. At the simplest level, **CUDA time-slicing** allows multiple processes to share a GPU by context-switching the compute resources. While flexible, this incurs high latency penalties (often 10-20ms) and offers no memory isolation. NVIDIA's **Multi-Process Service (MPS)**[^fn-mps-gpu-sharing] improves this by allowing kernels from different processes to run concurrently on the same GPU, improving throughput for small batch inference but still sharing the same memory address space. The gold standard for multi-tenancy is **Multi-Instance GPU (MIG)**, which physically partitions the GPU's compute units and memory into isolated instances.

**GPU memory isolation** prevents one model from consuming memory needed by another through explicit memory limits enforced by the container runtime. Without such limits, a memory leak, an unexpectedly large batch (triggered by a request with unusually long context), or a misbehaving custom kernel can consume all available GPU memory and crash colocated workloads. MPS improves utilization for small workloads by eliminating the context-switching overhead of time-slicing, but it adds latency overhead of approximately 5 to 10 microseconds per kernel launch and provides limited protection against memory bandwidth interference.

Furthermore, long-running serving instances suffer from dynamic memory fragmentation. The key culprit is the Key-Value (KV) cache in Transformer inference, which grows and shrinks with request sequence length. Standard allocators struggle with these highly variable lifetimes, leaving "holes" in GPU memory too small for new requests but collectively wasting gigabytes. Advanced serving engines now employ paged memory management to allocate KV cache in non-contiguous blocks, virtually eliminating fragmentation-induced OOMs.

[^fn-mps-gpu-sharing]: **CUDA MPS (Multi-Process Service)**: Enables concurrent kernel execution from multiple processes on a single GPU, eliminating the 10 to 20 ms context-switch penalty of time-slicing. MPS adds approximately 5 to 10 microseconds of overhead per kernel launch but provides no memory isolation: a process that exhausts GPU memory crashes all colocated workloads. This makes MPS suitable for trusted, same-team inference workloads but dangerous for multi-tenant environments where a single misbehaving model can take down its neighbors. \index{MPS!GPU sharing}

**CPU pinning** assigns specific CPU cores to inference pods, preventing the Linux scheduler from migrating processes between cores in ways that invalidate processor caches. For latency-sensitive workloads, isolating cores using `isolcpus` kernel parameters and `taskset` affinity removes OS scheduling jitter that manifests as random latency spikes. Combined with **NUMA-aware**[^fn-numa-inference] placement that ensures inference pods access memory through the nearest memory controller (avoiding remote NUMA access that adds 50 to 100 ns per access), this reduces P99 latency by 10 to 30 percent for sub-millisecond inference tasks. The principle is straightforward: inference latency is dominated by memory access patterns, and any source of memory access unpredictability, whether cache eviction from core migration, remote NUMA access, or TLB misses from address space switching, directly degrades tail latency.

[^fn-numa-inference]: **NUMA (Non-Uniform Memory Access)**: Local memory access takes approximately 100 ns versus 150 to 200 ns for remote-socket access, a 50 to 100 percent penalty. For ML inference preprocessing, placing CPU cores on the wrong NUMA domain relative to the GPU's PCIe connection adds this penalty to every host-device transfer, inflating P99 latency by 10 to 30 percent for sub-millisecond inference tasks. The fix is straightforward (CPU pinning and NUMA-aware pod scheduling) but often overlooked in Kubernetes deployments where the default scheduler ignores NUMA topology entirely. \index{NUMA!inference latency}

The isolation techniques discussed here represent a spectrum from strong (MIG, hardware isolation) to flexible (MPS, software sharing), and the choice depends on the trust boundary between colocated workloads. Multi-tenant inference platforms serving different external customers typically require MIG for security isolation. Single-tenant platforms serving different internal models on the same GPU can use MPS or time-slicing where the isolation requirements are weaker.

### GPU Sharing Economics {#sec-fleet-orchestration-gpu-sharing}

The decision to co-locate multiple models on a single GPU defines the efficiency frontier of an inference fleet. This choice operates on a spectrum: at one end, **exclusive access** guarantees isolation but strands capacity; at the other, **aggressive sharing** (via MPS or time-slicing) maximizes utilization but risks latency interference. The physics of this trade-off are dictated by memory fragmentation and compute contention.

Consider an 80 GB A100 GPU serving a 7B parameter model. The model weights in FP16 consume approximately 14 GB. With a typical KV cache and activation overhead, the total runtime footprint is roughly 26 GB. Under an exclusive access policy, this single model leaves 54 GB (67 percent) of the GPU's high-bandwidth memory dark -- a waste of silicon capital. Partitioning the GPU with MIG (e.g., a `3g.40gb` profile) tightens the container, reducing the waste to roughly 35 percent within the partition, but still leaves the remainder of the GPU strictly segmented. Enabling MPS to pack two such models onto the same 80 GB device consumes 52 GB, dropping the aggregate memory waste to just 35 percent. For fleets running hundreds of small models, shifting from exclusive to shared hosting can reduce the required GPU count by a factor of 2$\times$ to 3$\times$.

This density comes at the cost of **interference**. When two workloads share a GPU via MPS, they compete for Streaming Multiprocessors (SMs) and memory bandwidth. The degradation is workload-dependent: two compute-bound models (e.g., large batch processing) fighting for ALUs often see 15 to 20 percent throughput degradation each. However, a compute-bound model co-located with a memory-bound model (e.g., decoding-heavy generation) often coexist peacefully, seeing only 5 percent degradation because they bottleneck on different hardware resources. The scheduler's job thus becomes a multi-dimensional bin packing problem: finding complementary workloads that maximize density while respecting latency SLOs.

For a heterogeneous fleet, this logic dictates a bifurcated strategy. Our 175B parameter model, requiring 350 GB of memory, spans 4 to 8 GPUs via tensor parallelism and saturates them completely; it is a candidate for exclusive access. Conversely, the dozens of 1B to 7B parameter support models -- toxicity classifiers, query embedding encoders, draft models for speculative decoding -- are prime candidates for sharing. By packing these smaller models onto shared "utility nodes," the orchestrator liberates high-performance clusters for the heavy lifting of foundation model inference.

::: {.callout-notebook title="GPU Sharing ROI"}

Consider a fleet serving 100 distinct 7B models (26 GB footprint each) and 10 distinct 175B models (350 GB footprint each).

**Strategy A -- Exclusive access (one model per GPU)**: The 7B models require 100 A100 GPUs (80 GB each), achieving only 32 percent memory utilization (100$\times$ 26 GB used out of 8,000 GB total). The 175B models require 80 GPUs (10 models$\times$ 8 GPUs each for tensor parallelism). Total fleet: 180 GPUs.

**Strategy B -- Mixed sharing (MPS for small, exclusive for large)**: Pack 2 models per A100 for the 7B models (2$\times$ 26 = 52 GB, well within 80 GB), requiring only 50 GPUs at 65 percent memory utilization. The 175B models remain unchanged at 80 GPUs. Total fleet: 130 GPUs.

Sharing reduces the fleet size by 27 percent (50 fewer GPUs). At \$2 per GPU-hour, this saves approximately \$876,000 annually in compute costs, purely through scheduling policy.

:::

### Model Routing and Traffic Management {#sec-fleet-orchestration-model-routing}

Once resources are isolated, the orchestrator faces the challenge of directing inference queries to the correct model replicas. For a monolithic web service, simple round-robin load balancing suffices. For a 175B parameter language model served across thousands of GPUs, routing becomes a complex distributed systems problem where the "unit of serving" is no longer a single container but a **tensor parallel group** -- a collection of 8 GPUs that must function as a single logical entity. The load balancer cannot simply route a request to "GPU 12"; it must route to "TP Group 3," ensuring the request arrives at the exact moment the group is ready to process it.

Request routing strategies determine cluster-wide throughput. Naive **round-robin** distribution fails for generative workloads because request processing times vary by orders of magnitude based on output token length. A round-robin scheduler inevitably sends new requests to replicas backed up with long-generation tasks, causing tail latency to spike. **Least-outstanding-requests** (LOR) routing improves this by directing traffic to the idlest replicas. However, for large language models, **memory-aware routing** is superior. By tracking the KV cache occupancy of each replica, the router can send long-context requests to replicas with ample free memory and short requests to those near capacity, preventing fragmentation-induced out-of-memory errors and increasing effective batch sizes by 30 to 40 percent compared to blind routing.

**Traffic shifting** mechanisms like canary deployments and A/B testing are essential for safe model updates. When deploying Model v2, the orchestrator does not simply replace Model v1. Instead, it spins up v2 replicas alongside v1, creating a shadow fleet. The traffic manager routes 1 percent of live requests to v2 (often in "shadow mode," where the response is computed but discarded) to validate latency and error rates against v1. Only after statistical verification does the orchestrator gradually shift weights -- 5 percent, 25 percent, 100 percent -- draining v1 replicas only as v2 proves stable.

**Multi-model serving** complicates this further. A single cluster often hosts diverse models: a product recommendation model (10 ms P99 SLO, high throughput) alongside a coding assistant model (2,000 ms P99 SLO, bursty usage). Placing these on the same node invites interference; placing them on separate clusters strands capacity. The optimal strategy uses **latency-aware bin packing**: the orchestrator co-locates latency-critical models with batch-processing jobs (like embedding generation) that can be throttled instantly, ensuring high utilization without violating strict SLOs.

### The Training-Serving Resource Boundary {#sec-fleet-orchestration-boundary}

The deepest utilization divide in the ML fleet is the wall between training and serving. Most organizations manage these as separate fiefdoms: a "Production Serving" cluster that must handle peak traffic (and sits 50 percent idle at night) and a "Research Training" cluster that is perpetually backlogged. This **static partitioning** is safe but economically inefficient. **Dynamic sharing** unifies these pools, but it introduces a fundamental friction: the mode-switching cost.

Repurposing a GPU from training to serving is not instantaneous. The orchestrator must checkpoint the training job (1 to 5 minutes), kill the container, launch the inference server, load the model weights from storage (2 to 10 minutes), and run warmup queries (1 to 5 minutes) to populate compilation caches. This 5-to-20-minute switching latency means GPUs cannot react to second-by-second traffic bursts. Instead, they must follow diurnal patterns.

Inference traffic typically follows a "human curve": low at 3 AM, ramping up at 8 AM, peaking at 2 PM, and tapering off at 10 PM. A smart orchestrator forecasts this curve 30 minutes in advance. At 7:30 AM, it preempts low-priority training jobs (like hyperparameter sweeps) to warm up inference replicas. At 10:30 PM, as traffic drops, it drains inference replicas and releases GPUs back to the training scheduler.

::: {.callout-notebook title="The Diurnal GPU Shift"}

Consider a fleet of 1,000 GPUs managed under two regimes:

**Scenario A -- Static Partitioning**: The serving pool reserves 400 GPUs to handle peak demand, but average usage is only 150 GPUs (37.5 percent utilization). The training pool reserves 600 GPUs, and backlog ensures 100 percent utilization. Result: 150 + 600 = 750 active GPUs. Fleet utilization: **75 percent**.

**Scenario B -- Dynamic Sharing**: Serving claims GPUs strictly as needed (average 150). Training reclaims unused serving GPUs (average 250), minus a safety buffer. The orchestrator shifts 200 GPUs between roles twice daily based on diurnal forecasts. Result: serving (150) + training (600 + 130 reclaimed) = 880 active GPUs. Fleet utilization: **88 percent**.

Dynamic sharing recovers 130 GPUs of effective capacity. At \$2 per GPU-hour, this creates approximately \$2.3 million per year in value without purchasing a single additional accelerator.

:::

Optimizing isolation for a single deployment is straightforward, but maintaining strict guarantees becomes volatile when thousands of training and serving workloads coexist on the same infrastructure. The collision of high-priority inference SLAs with resource-hungry training jobs creates a noisy neighbor problem that simple containerization cannot solve. The next section addresses multi-tenancy and quotas, the governance layer responsible for arbitrating fair access and preventing tragedy-of-the-commons scenarios in shared fleets.

## Multi-Tenancy and Quotas {#sec-fleet-orchestration-multi-tenancy}

What happens when the computer vision team hoards 500 GPUs for a week "just in case" they need to run an experiment, while the NLP team sits completely blocked trying to push a critical bug fix to production? Without strict governance, shared clusters devolve into a tragedy of the commons. Multi-tenancy and quotas provide the organizational and technical firewalls needed to ensure fair access and high overall utilization.

Without explicit resource management policies, resource allocation degrades to a tragedy of the commons. Teams with the most aggressive job submission rates, the largest jobs, or the most persistent resubmission scripts consume disproportionate resources, while teams with more modest or intermittent needs find the cluster perpetually occupied. This creates organizational friction, political escalation to management, and underinvestment in slower-moving but potentially higher-value projects whose teams lack the engineering effort to compete for resources. The multi-tenancy and quota systems discussed in this section prevent this degradation by establishing formal policies for resource allocation, borrowing, and reclamation.

### Hierarchical Fair-Share {#sec-fleet-orchestration-fair-share-multi}

GPU quota allocation typically operates at the namespace or project level, defining how much of the cluster each team is entitled to use. The simplest approach allocates fixed GPU counts per team: Team A gets 500 GPUs, Team B gets 300 GPUs, Team C gets 200 GPUs. This approach is easy to understand and implement but leads to systematic underutilization when teams have variable workloads. If Team A's 500-GPU allocation sits 50 percent idle during a quiet period while Team B's queue overflows with urgent work, the cluster wastes 250 GPUs of capacity that rigid quotas forbid Team B from using. The alternative, allocating less quota to each team, creates a different problem: teams are blocked from running legitimate work during peak periods even when the cluster has overall spare capacity.

**Hierarchical quotas** resolve this tension by enabling departmental limits with sub-team flexibility and cross-team borrowing. The effective quota for any team is bounded by both its own allocation and the remaining capacity within its parent organization:

$$ Q_{effective} = \min\left(Q_{team},\ Q_{department} - \sum_{\text{other teams}} U_{allocated}\right) $$ {#eq-fleet-orchestration-quota}

When aggregate demand exceeds capacity, each team receives resources proportional to its share allocation, ensuring that no team is systematically disadvantaged. Unused capacity borrows down the hierarchy: if Team A uses only 60 percent of its 500-GPU allocation, the remaining 200 GPUs become available to other teams within the same department. These borrowed resources carry lower preemption priority than owned resources: when Team A later submits jobs that need its full allocation, the scheduler reclaims borrowed capacity by preempting jobs running on those resources, returning capacity to its rightful owner.

This borrowing mechanism is essential for achieving high utilization in multi-tenant clusters. Without borrowing, organizations face a painful choice between allocating enough quota to handle each team's peak demand (guaranteeing low utilization when demand is uneven, since peaks rarely coincide across all teams) or allocating less (guaranteeing that some team is always quota-blocked during their peak periods). Borrowing resolves this dilemma by allowing peak-level allocations as *guarantees* while permitting idle capacity to *flow* to where it is needed. A team that is guaranteed 500 GPUs can always get 500 GPUs when needed (through preemption of borrowed jobs), but it does not waste the cluster's capacity when its actual demand is lower.

The preemption dynamics of borrowing require careful integration with the checkpoint infrastructure from @sec-fault-tolerance-reliability. Jobs running on borrowed capacity must checkpoint frequently enough that preemption (when the owning team reclaims resources) does not waste significant work. Production systems typically distinguish between "guaranteed" and "opportunistic" scheduling tiers: guaranteed jobs run on the team's owned quota and face preemption only from higher-priority work, while opportunistic jobs run on borrowed capacity and accept that they may be preempted when the owning team needs its resources back. Training frameworks that support elastic scaling can respond to borrowed-capacity reclamation by shrinking rather than terminating, further reducing the cost of the borrowing mechanism.

### Burst Capacity and Over-Subscription {#sec-fleet-orchestration-burst-capacity}

In a shared GPU cluster, static quotas inevitably lead to the "burst capacity" paradox: Team A has a quota of 64 GPUs but needs 256 for a weekend-long training run, while Team B's allocated 64 GPUs sit idle. The solution requires decoupling "guaranteed quota" from "limit quota." A team is guaranteed 64 GPUs, which they can access instantly, but can burst up to 512 GPUs if the cluster has slack capacity. This "over-subscription" model relies on preemption: if Team B wakes up and reclaims their guaranteed share, the scheduler must immediately terminate Team A's burst jobs.

**Burst capacity** handling enables teams to temporarily exceed their quotas when cluster-wide resources are available, providing a more aggressive sharing mechanism than quota borrowing. Where borrowing reassigns idle capacity from one team to another, burst capacity allows teams to exceed the total allocated capacity by exploiting the gap between *requested* resources and *actual* utilization. Over-commitment ratios of 1.2 to 1.5$\times$ are common in production clusters, with admission controllers tracking actual versus requested resources and intervening when contention materializes. When contention occurs, jobs using burst capacity face preemption first, ensuring that guaranteed allocations within each team's owned quota are protected.

To manage this safely, clusters must implement strict priority classes. **Production Serving** (Priority 0) is non-preemptible. **Production Training** (Priority 1) handles core model updates and is only preemptible by serving outages. **Interactive/Debug** (Priority 2) gets high scheduling precedence but short time limits. **Batch/Research** (Priority 3) is fully preemptible and fault-tolerant. This hierarchy ensures that a massive hyperparameter sweep fills the cluster's cracks but evaporates instantly when a critical retraining job arrives.

The appropriate over-commitment ratio depends on workload characteristics and requires careful observation of actual resource utilization patterns. If most jobs request 100 percent GPU utilization but actually achieve 70 percent (due to data loading phases, communication overhead, memory allocation gaps, or suboptimal kernel scheduling), a 1.3$\times$ over-commitment ratio improves cluster-wide utilization without significant contention because the total actual demand (70 percent$\times$ 1.3 = 91 percent) remains below physical capacity. If jobs are genuinely compute-bound and sustain near-100 percent GPU utilization, over-commitment leads to resource contention and performance degradation for all colocated workloads. The over-commitment ratio should therefore be calibrated empirically based on cluster-wide utilization telemetry, not set based on theoretical assumptions about workload behavior.

### Resource Accounting and Observability {#sec-fleet-orchestration-resource-accounting}

Who pays for the 64 GPUs that sat idle for a weekend because a researcher forgot to cancel a reservation? In a dedicated cluster, the waste is visible: the lights are on, but the fans are quiet. In a multi-tenant cloud fleet, this waste is invisible -- and expensive. Effective governance requires moving beyond simple "allocation" metrics to a tiered accounting model that distinguishes between what was requested, what was used, and what was useful.

**Allocated capacity** measures what the scheduler has reserved for a job -- the resources that are unavailable to others. **Compute utilization** measures the percentage of time the GPU kernels are active. **Productive utilization** measures the fraction of time the GPU is advancing the model state, excluding data loading pauses, communication overhead, and checkpointing. The distinction is financial, not just technical. A team might be "allocated" 500 GPUs but only "utilizing" 350 (70 percent compute utilization) and only "productively utilizing" 280 (56 percent of allocation). If the organization pays for allocation but measures success by training progress, this 44 percent gap represents pure burn.

Attribution in a shared fleet presents a forensic challenge. A single "training" job might be a shared experiment between three teams, or a platform test run by an SRE. Without granular tagging at the job level, costs default to the "infrastructure" bucket, creating a tragedy of the commons where no one owns the bill. Kubernetes labels and Slurm accounts provide the mechanism for attribution, but the organizational discipline to apply them consistently is the harder problem. Mature organizations enforce "no tag, no schedule" policies, rejecting untagged jobs at the admission controller level.

This data feeds the utilization dashboard, the cluster operator's primary instrument for fleet health. This view must synthesize per-team utilization against quotas, queue depth by priority class, and the **fragmentation index** -- a measure of free GPUs that cannot be allocated due to topology constraints. Crucially, it must track the "cost of chaos": preemption rates, spot instance interruption frequencies, and the recovery overhead they induce. When these metrics are visible, they drive behavior. Our 175B training job consumes 64 GPUs for 34 days. Without proper accounting, this \$131,000 cost (64 GPUs$\times$ 816 hours$\times$ \$2.50/GPU-hour) is invisible to the team that requested it. With chargeback, the team must justify this expenditure against the model's expected business value.

Automated anomaly detection turns these metrics into actionable signals. A sudden spike in a team's consumption to 3$\times$ their historical norm might indicate a runaway script spawning infinite jobs. A queue that grows faster than it drains signals a capacity shortfall that auto-scaling must address. Conversely, a sudden drop in cluster-wide power consumption typically precedes a mass job failure event, often due to a shared dependency like a storage outage. These alerts allow operators to intervene before the budget is drained or the queue becomes unmanageable.

::: {.callout-perspective title="The Three Utilization Metrics"}
To debug cluster efficiency, measure utilization at three distinct layers. **Allocated utilization** is the percentage of physical GPUs reserved by the scheduler; low values indicate a lack of demand or overly restrictive quotas. **Compute utilization** is the percentage of time allocated GPUs are executing kernels (reported by `nvidia-smi`); low values indicate inefficient code, I/O bottlenecks, or communication stalls. **Productive utilization** is the percentage of allocated time spent effectively training (excluding overhead); low values indicate poor distributed scaling, excessive checkpointing, or frequent restarts.
:::

### Security and Namespace Isolation {#sec-fleet-orchestration-security-isolation}

Multi-tenancy requires not only fair resource allocation but also security isolation that prevents teams from interfering with or observing each other's workloads. The stakes are real: ML models represent significant intellectual property, training data may contain sensitive information subject to privacy regulations, and the models themselves may encode proprietary business logic. Without proper isolation, a compromised or misconfigured workload in one team's namespace could access another team's model weights, training data, or inference traffic.

In Kubernetes environments, namespace separation provides the fundamental isolation boundary. Each team operates within dedicated namespaces with role-based access control (RBAC) limiting visibility to their own resources. RBAC policies control who can submit jobs, view logs, access model artifacts, and modify scheduling policies within each namespace, providing organizational governance over cluster usage.

**Network policies** extend isolation to the network layer, preventing cross-namespace communication except through explicitly permitted services. Network policies for ML workloads must balance isolation against the communication requirements of distributed training. A practical policy might allow unrestricted all-to-all communication within a namespace (necessary for ring AllReduce as analyzed in @sec-collective-communication, where every worker must communicate with every other worker) while blocking all ingress from other namespaces (preventing external workloads from intercepting gradient traffic or model updates). Egress policies can prevent training jobs from accessing external networks, reducing data exfiltration risk from compromised training code or poisoned dependencies.

**GPU virtualization options** represent a spectrum of isolation strength versus resource efficiency. Time-slicing (software-based GPU sharing) provides low isolation but high flexibility, suitable for trusted workloads from the same team that need to share a GPU for cost efficiency. MIG (hardware-partitioned GPU sharing) provides strong isolation with fixed partitions, suitable for multi-tenant inference where different customers' workloads share the same physical GPU. Full device passthrough provides complete isolation (each workload gets exclusive access to one or more complete GPUs) at the cost of the lowest packing efficiency, suitable for training workloads that saturate GPU resources and cannot tolerate any interference. The choice depends on the trust boundary between colocated workloads and the performance sensitivity of each workload type.

Security in multi-tenant environments extends beyond simple resource fairness. Side-channel attacks on shared GPUs are a documented vulnerability; by monitoring contention on shared caches or memory controllers, a malicious tenant can infer the architecture or even data properties of a co-resident model. In highly sensitive environments, hardware isolation mechanisms like MIG or strictly dedicating entire GPU nodes to single tenants become mandatory requirements.

### Priority Preemption Cascades {#sec-fleet-orchestration-preemption-cascades}

When a high-priority job enters a saturated cluster, the scheduler faces a critical decision: which running workloads must be terminated to free up resources? This decision is rarely isolated. In tightly packed clusters, evicting a medium-priority job to accommodate a high-priority request often triggers a **preemption cascade**, where the evicted job immediately attempts to reschedule itself by displacing lower-priority workloads. Without dampening controls, a single urgent inference service deployment can ripple through the queue, destabilizing dozens of training jobs and forcing a storm of checkpoint reloads that saturate storage bandwidth.

The engineering cost of preemption extends far beyond the scheduling latency. The **preemption tax** is the sum of lost computation since the last checkpoint, the overhead of persisting state, and the "cold start" penalty upon resumption. For large-scale distributed training, this tax is non-linear. Consider our 175B parameter model training on 64 GPUs. If preempted, the job loses the work performed since the last snapshot (average 15 minutes). Upon restarting, it requires 20 minutes to reload the massive optimizer state from distributed storage and another 10 minutes of warmup time -- a period where data pipelines refill pre-fetch buffers, JIT compilers re-optimize kernels, and GPU caches re-populate.

During this 45-minute recovery window, the GPUs are active but effectively unproductive. At \$2 per GPU-hour, a single preemption event costs \$96 in wasted compute capital. If the scheduler allows unlimited preemption, a cluster averaging 12 preemptions per day wastes 8.5 hours -- over 35 percent of total capacity -- on recovery overhead. To mitigate this, production systems enforce **preemption budgets**. These rate limits constrain disruption frequency, ensuring that a job cannot be preempted more than once per hour, or that total cluster preemption churn cannot exceed 5 percent of capacity in any 10-minute window. This forces the scheduler to wait for natural job completions rather than triggering a cascade, trading slightly higher pending times for significantly higher aggregate throughput.

### Quota Governance and Organizational Dynamics {#sec-fleet-orchestration-quota-governance}

While schedulers manage the second-by-second allocation of silicon, **quota governance** manages the month-by-month allocation of organizational intent. Quotas are the interface between engineering constraints and business priorities. In mature ML organizations, these are rarely static; they are adjusted through periodic **quota review cycles** where allocations are recalibrated based on realized utilization rather than forecasted demand. A team that consistently utilizes only 40 percent of their 128-GPU allocation is effectively blocking other teams from launching experiments, creating a phantom scarcity that drives up queue times despite ample physical capacity.

The central tension in quota management is the **hoarding problem**. Engineering teams, rational in their desire to minimize wait times, often request maximum capacity "just in case" a project accelerates. This leads to low average utilization and high burst demand. To counter this, platform teams implement **utilization-based reclamation**: if a quota pool remains underutilized for a set period, the system automatically reduces the allocation, returning resources to the general pool. This technical enforcement shifts the burden of proof back to the team to justify their reservation.

Financial accountability further aligns incentives. Organizations typically begin with **showback**, a reporting mechanism that exposes the dollar cost of GPU usage to engineering managers without directly impacting their budgets. This fosters awareness but lacks teeth. As spending scales, organizations transition to **chargeback**, where compute costs are deducted from team budgets. Chargeback creates immediate pressure to optimize code and release unused quota, though it risks discouraging speculative research if the internal pricing model is too aggressive.

::: {.callout-war-story title="The Quota Hoarding Incident"}
A computer vision team at a major logistics company reserved a block of 500 GPUs for a "critical" urgent model refresh. Due to upstream data delays, the training jobs were postponed, but the team retained the reservation to ensure availability "when the data arrives." The GPUs sat idle for three weeks -- a capital waste of roughly \$500,000 -- while the NLP team's queues overflowed, delaying a chatbot release by a month.

The infrastructure team implemented a "use it or lose it" policy. Any reserved quota group operating below 20 percent utilization for 72 consecutive hours is now automatically reclaimed and converted to spot capacity for the general pool. Reclaiming the quota requires VP-level approval, effectively eliminating "parking" on idle hardware.
:::

::: {.callout-checkpoint title="Multi-Tenancy Design Decisions"}

Consider a 2,000-GPU cluster shared between a research team (60% allocation) and a production team (40% allocation):

- [ ] The research team is using only 30% of the cluster. Should the production team be able to use the idle 30%? What happens when the research team submits a large job?
- [ ] A production inference workload needs 100 GPUs with guaranteed latency SLOs. A research training job can tolerate preemption. How should the priority system be configured?
- [ ] What over-commitment ratio is appropriate if research jobs average 65% GPU utilization and production jobs average 85%?

:::

Even with sophisticated multi-tenancy and quota policies in place, the cluster can still fall victim to complex, systemic bottlenecks that defy simple monitoring. When utilization drops inexplicably despite full queues, operators must roll up their sleeves and dive into debugging cluster utilization.

## Debugging Cluster Utilization {#sec-fleet-orchestration-debugging}

Imagine opening your cluster dashboard to find that average GPU utilization is stuck at 60%, yet developers are complaining about three-day queue times for small debugging jobs. You have hundreds of idle GPUs, but no jobs are starting. Bridging this gap between theoretical capacity and production reality requires methodical forensic analysis to untangle the interactions between hardware topology, scheduler policies, and user behavior.

::: {#fig-utilization-paradox fig-env="figure" fig-pos="htb" fig-cap="The Utilization Paradox: As cluster utilization approaches saturation (100%), job wait times increase exponentially, not linearly. Operating above 80% utilization (the Danger Zone) causes unpredictable scheduling delays, especially for large jobs, illustrating why 100% utilization is an anti-pattern for interactive research clusters." fig-alt="Semi-log plot of job wait time versus cluster utilization. Three curves for small, mixed, and large jobs. Wait times explode near 100%. Danger Zone shaded above 80%."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ UTILIZATION PARADOX (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-utilization-paradox — wait time vs utilization
# │
# │ Goal: Plot job wait time vs cluster utilization; show M/M/1-style explosion
# │       near 100%; Danger Zone >80%; Target 75%.
# │ Show: Three curves (small/mixed/large jobs); log-scale y; shaded zone.
# │ How: wait = rho/(1-rho)*avg_time; matplotlib.
# │
# │ Imports: numpy (np), matplotlib.pyplot (plt)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
import matplotlib.pyplot as plt

plt.style.use('seaborn-v0_8-whitegrid')

utilization = np.linspace(0, 0.98, 500)
rho = utilization

wait_small = (rho / (1 - rho)) * 1.0
wait_mixed = (rho / (1 - rho)) * 4.0
wait_large = (rho / (1 - rho)) * 12.0

fig, ax = plt.subplots(figsize=(10, 6))

ax.plot(utilization * 100, wait_small, label='Small Jobs (Avg 1h)', linewidth=2, color='#2ca02c')
ax.plot(utilization * 100, wait_mixed, label='Mixed Workload (Avg 4h)', linewidth=2, color='#1f77b4')
ax.plot(utilization * 100, wait_large, label='Large Jobs (Avg 12h)', linewidth=2, color='#d62728')

target_util = 75
ax.axvline(x=target_util, color='gray', linestyle='--', alpha=0.7, label='Target Utilization (75%)')

ax.axvspan(80, 100, alpha=0.15, color='red', label='Danger Zone (>80%)')

ax.text(82, 500, 'Danger Zone:\nWait times explode', color='#8B0000', fontweight='bold')
ax.text(73, 0.5, 'Target Zone', color='gray', ha='right', va='bottom', rotation=90)

ax.set_yscale('log')
ax.set_xlim(0, 100)
ax.set_ylim(0.1, 1000)
ax.set_xlabel('Cluster Utilization (%)', fontsize=12)
ax.set_ylabel('Average Job Wait Time (Hours, Log Scale)', fontsize=12)
ax.set_title('The Utilization Paradox: Efficiency vs. Latency', fontsize=14, pad=15)
ax.legend(frameon=True, fontsize=10, loc='upper left')

ax.grid(True, which="both", ls="-", alpha=0.2)
ax.minorticks_on()

fig = plt.gcf()
```
:::

::: {.callout-example title="Debugging Low GPU Utilization"}

**The Problem**: A 1,000-GPU cluster reports 60% average utilization despite a full job queue with over 50 pending jobs. Engineering leadership expects greater than 85% utilization given the capital investment. The standard monitoring dashboards show plenty of idle GPUs, yet users complain about long queue times. Where is the problem, and how do we systematically diagnose it?

The **Fleet Stack** framework (@sec-vol2-introduction) provides a structured approach: analyze the Infrastructure Layer first to understand hardware constraints, then the Distribution Layer to understand scheduling logic, and finally the interaction between layers to identify the root cause.

**Infrastructure Layer Analysis**: The cluster contains heterogeneous hardware acquired over three procurement cycles:

| **GPU Type**  | **Count** | **Memory**  | **Interconnect**    | **Nodes**                |
|:--------------|----------:|:------------|:--------------------|:-------------------------|
| **A100-80GB** |       400 | 80 GB HBM2e | NVLink (600 GB/s)   | 50 nodes$\times$ 8 GPUs |
| **A100-40GB** |       400 | 40 GB HBM2e | NVLink (600 GB/s)   | 50 nodes$\times$ 8 GPUs |
| **V100-32GB** |       200 | 32 GB HBM2  | PCIe Gen3 (32 GB/s) | 50 nodes$\times$ 4 GPUs |

: **Cluster Hardware Inventory**: Three procurement generations create distinct resource pools with different capabilities. A100 nodes support tensor parallelism via NVLink, while V100 nodes are limited to data parallelism. {#tbl-fleet-orchestration-debug-inventory}

The physical topology creates three distinct resource pools with different capabilities. The A100 nodes support efficient tensor parallelism via NVLink, while V100 nodes are limited to data parallelism due to PCIe bandwidth constraints.

**Distribution Layer Analysis**: The Slurm scheduler implements gang scheduling with strict resource type matching. Examining the job specifications reveals the demand pattern:

- 15 large training jobs requesting 64+ A100-80GB GPUs (total demand: 1,200 A100-80GB GPUs)
- 8 medium jobs requesting 32 A100-40GB GPUs (total demand: 256 A100-40GB)
- Zero jobs explicitly requesting V100 resources

The scheduler's allocation log shows A100-80GB GPUs at 95% allocation (380/400), but with only 5 jobs actually running because gang scheduling holds resources for jobs that cannot yet be fully satisfied. The remaining 10 large jobs each hold partial allocations (64 to 96 GPUs reserved) while waiting for additional A100-80GB resources that never become available, creating a **hold-and-wait deadlock pattern**.

**Diagnosis**: Multiple pathologies compound to create low utilization:

1. **Over-specification**: Model memory analysis reveals that 12 of the 15 large jobs actually require only 45 GB peak memory per GPU, well within A100-40GB capacity. Users copied job templates specifying A100-80GB without recalculating requirements.
2. **Pool fragmentation**: The strict homogeneity requirement means a 64-GPU job requesting "A100-80GB" cannot use any A100-40GB GPUs, even when 300 A100-40GB GPUs sit idle.
3. **Stranded resources**: No jobs target V100 hardware because researchers perceive it as "legacy." The 200 V100 GPUs contribute zero productive work despite consuming power and cooling.
4. **Gang scheduling deadlock**: Without a timeout or preemption policy, partially-satisfied jobs hold resources indefinitely, blocking other work.

**Root Cause (Policy-Infrastructure Mismatch)**: Job templates and organizational practices evolved for a homogeneous A100-80GB cluster. When infrastructure expanded with heterogeneous hardware, the Distribution Layer policies were never updated. The scheduler correctly implements its configured policy; the policy itself creates the utilization gap.

**Solution**: Implement tiered resource matching with topology awareness. The fix addresses all four pathologies through complementary policy changes:

1. **Capability-based scheduling**: Replace exact GPU type requests with capability requirements. Instead of `--gres=gpu:a100-80g:64`, jobs specify `--constraint="gpu_mem>=40GB"` and let the scheduler select appropriate hardware. This eliminates over-specification by expressing what the job *needs* rather than what hardware it *prefers*. The scheduler can then match jobs to the cheapest available hardware that meets their requirements.
2. **Topology-aware placement**: For jobs requiring tensor parallelism (identified by requesting more than 1 GPU per node), add constraint `--constraint="nvlink"` to ensure placement on NVLink-connected nodes. Data-parallel jobs that request only 1 GPU per node can omit this constraint, enabling scheduling on V100 nodes where NVLink is absent but data parallelism works efficiently over any network.
3. **Gang scheduling with timeout**: Configure `SchedulerParameters=bf_continue,bf_window=7200` to enable backfill scheduling and prevent indefinite resource holding. Jobs waiting more than 2 hours for gang completion release their partial allocations back to the free pool. This timeout prevents the accumulating deadlock where partially-satisfied jobs hold resources indefinitely.
4. **V100 workload seeding**: Create a "legacy-gpu" queue with lower cost accounting (reflecting V100's lower capability) to incentivize researchers to submit appropriate workloads to V100 hardware. Small-scale experiments, single-GPU development, and data-parallel training of models under 30B parameters can run effectively on V100s.
5. **Workload-hardware matching guidance**: Provide researchers with a decision matrix mapping model sizes to minimum GPU memory requirements, reducing over-specification by making the correct resource request the easiest choice.

```{python}
#| label: debug-value-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ DEBUGGING EXAMPLE: UTILIZATION RECOVERY
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-fleet-orchestration-debugging worked example (callout)
# │
# │ Goal: Quantify the annual dollar value recovered by fixing a 1,000-GPU
# │       cluster from 60% to 84% effective utilization via policy changes.
# │ Show: ~400 equivalent GPUs recovered; ~$7M/year in recovered value —
# │       inline in the "Quantified Impact" paragraph after the results table.
# │ How: recovered_gpus = cluster × (util_after − util_before) / util_before;
# │      annual_value = recovered_gpus × $2/GPU-hr × 8,760 hr/yr.
# │
# │ Imports: (uses gpu_hour_cost_usd from fleet-orchestration-setup)
# │ Exports: debug_recovered_str, debug_annual_str
# └─────────────────────────────────────────────────────────────────────────────

class DebugValueCalc:
    """Annual dollar value recovered by raising a 1,000-GPU cluster from 60% to 84% utilization."""
    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    debug_cluster      = 1000
    debug_util_before  = 0.60
    debug_util_after   = 0.84
    _gpu_rate          = gpu_hour_cost_usd   # from fleet-orchestration-setup exports

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────
    debug_improvement  = debug_util_after - debug_util_before
    debug_recovered    = debug_cluster * debug_improvement / debug_util_before
    debug_annual       = debug_recovered * _gpu_rate * 8760

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    debug_recovered_str = f"{int(debug_recovered)}"
    debug_annual_str    = f"{debug_annual / 1e6:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
debug_recovered_str = DebugValueCalc.debug_recovered_str
debug_annual_str    = DebugValueCalc.debug_annual_str
```

**Quantified Impact**: After implementing tiered matching and backfill scheduling over a two-week validation period:

| **Pool**         | **Before**                   | **After**                    | **Change**    |
|:-----------------|:-----------------------------|:-----------------------------|:--------------|
| **A100-80GB**    | 95% allocated, 72% effective | 89% allocated, 94% effective | +5% effective |
| **A100-40GB**    | 64% allocated                | 88% allocated                | +24%          |
| **V100**         | 0% allocated                 | 71% allocated                | +71%          |
| **Cluster-wide** | **60%**                      | **84%**                      | **+40%**      |

: **Utilization Recovery Results**: The distinction between "allocated" and "effective" utilization captures the gang scheduling deadlock: before the fix, A100-80GB GPUs showed high allocation in Slurm but low actual compute utilization because allocated jobs could not start. {#tbl-fleet-orchestration-debug-results}

This 40% improvement in effective cluster capacity equals approximately `{python} debug_recovered_str` additional GPUs worth of productive work. At \$2 per GPU-hour, this represents approximately \$`{python} debug_annual_str` million in annual recovered value, achieved through policy changes requiring zero additional hardware investment.

**The Fleet Stack Lesson**: Surface-level diagnosis suggested a scheduling algorithm problem (Distribution Layer), perhaps requiring a more sophisticated scheduler like those discussed in @sec-fleet-orchestration-custom-schedulers. Management initially proposed evaluating Pollux or a custom scheduling solution, which would have required months of engineering effort and introduced operational risk. Deeper analysis revealed the root cause as a **mismatch between Infrastructure Layer heterogeneity and Distribution Layer policies designed for homogeneous infrastructure**. The scheduler algorithm was not the problem; the policies it implemented were the problem. The scheduler correctly implemented its configured policy; the policy itself created the utilization gap.

Effective debugging required examining both layers and their interaction, recognizing that infrastructure evolution (adding A100-40GB and V100 nodes to an originally homogeneous A100-80GB cluster) had invalidated assumptions embedded in job templates and scheduling configuration. The fix addressed the policy layer, not the algorithm layer, and was implemented through configuration changes requiring zero code modifications and zero additional hardware investment. This pattern recurs across production ML infrastructure: the most common root cause of scheduling problems is not algorithmic insufficiency but policy-infrastructure mismatch, where operational policies designed for one infrastructure configuration are applied without adaptation to a changed infrastructure.

:::

### Common Utilization Anti-Patterns {#sec-fleet-orchestration-anti-patterns}

High-level metrics often mask deep inefficiencies. While a cluster dashboard may report 85 percent allocation, the productive utilization -- cycles actually advancing model state -- can be significantly lower. Debugging these gaps requires identifying specific signatures in the GPU telemetry that reveal the underlying bottleneck.

**The zombie job** represents the most egregious waste: a training process that has crashed or deadlocked but continues to hold GPU resources. In a distributed run training a 175B parameter model, a single rank failure can leave 128 GPUs allocated but idling if the process cleanup fails. The signature is high memory allocation (greater than 300 GB) paired with near-zero SM utilization (less than 5 percent) persisting for longer than a heartbeat timeout (typically 10 minutes). This state often results from CUDA context corruption or a driver-level deadlock that prevents the container runtime from successfully reaping the process, requiring forceful node-level remediation to reclaim the device.

**The data starvation pattern** manifests as a high-frequency sawtooth wave in GPU utilization, where the device oscillates between 100 percent load (forward/backward pass) and 0 percent load (waiting for the next batch). While the average utilization might appear acceptable at 60 to 70 percent, the GPU is effectively idling for a third of its life. This occurs when the CPU-bound data pipeline cannot sustain the throughput required by the GPU, a common scenario in computer vision where JPEG decoding and augmentation saturate host CPUs. A utilization variance exceeding 30 percent standard deviation is the primary diagnostic indicator, signaling the need for active prefetching, increased loader parallelism, or local NVMe caching to saturate the accelerator.

**The communication bottleneck** reveals itself through periodic, synchronized drops in SM utilization across all ranks in a distributed group. For the 175B model, synchronizing gradients for 350 GB of parameters requires massive bandwidth; if the network is undersized or congested, the GPUs spend more time waiting for AllReduce to complete than performing computation. The diagnostic metric is the step time ratio: if the total iteration time exceeds 1.5$\times$ the compute-only time (forward plus backward passes in isolation), the network has become the dominant constraint. Remediation requires topology-aware placement to keep traffic on high-bandwidth switch tiers or algorithmic changes like gradient accumulation to increase the compute-to-communication ratio.

**The memory fragmentation trap** is an insidious failure mode in long-running inference servers. A 175B model serving diverse request lengths can end up with 80 percent of its HBM allocated to the KV cache, yet be unable to accept a new request because the free memory is scattered in small, non-contiguous blocks. The GPU appears "full" to the scheduler but "underutilized" in terms of throughput. This resembles the classic heap fragmentation problem but with higher stakes -- a single restart to defragment dumps hundreds of gigabytes of state. Monitoring the ratio of requested token slots to allocated memory bytes often exposes this gap, which modern serving engines address through paging mechanisms (like PagedAttention in vLLM) that decouple logical sequence contiguity from physical memory layout, recovering this "dark matter" memory and boosting serving throughput by 2 to 4$\times$ without adding hardware.

The debugging example illustrates a broader principle that applies to every system discussed in this chapter: scheduling systems are only as effective as the policies they implement, and policies must evolve alongside infrastructure. When infrastructure changes (new hardware generations are added, network topologies are upgraded, workload mixes shift from training-dominant to serving-dominant), the policies encoded in scheduler configuration, job templates, and organizational practices must be re-evaluated and updated. The most expensive scheduling bug is often not a software defect but a policy that was correct for the old infrastructure and was never updated for the new one.

This principle connects to the broader governance themes explored later in Volume II. Technical policies (GPU type matching, gang scheduling timeouts, preemption grace periods) interact with organizational policies (team quotas, priority hierarchies, cost allocation) and human behavior (job template reuse, resource request habits, queue submission patterns). Effective fleet orchestration requires attending to all three layers simultaneously.

The next section examines common misconceptions that lead to policy-infrastructure mismatches and other costly scheduling errors.

## Fallacies and Pitfalls {#sec-fleet-orchestration-fallacies}

It is tempting to look at a cluster running at 98% utilization and declare victory, only to discover that researchers have stopped submitting jobs entirely because wait times have stretched into weeks. Cluster orchestration is riddled with counterintuitive traps like this, where optimizing a single metric destroys overall system value.

**Fallacy:** *More sophisticated scheduling algorithms always improve utilization.*

Engineers facing low utilization often reach for more advanced schedulers, assuming the algorithm is the bottleneck. As the debugging example in @sec-fleet-orchestration-debugging demonstrates, the root cause is frequently policy misconfiguration, not algorithmic limitation. A simple FIFO scheduler with correct policies (capability-based matching, backfill with timeouts, appropriate gang scheduling constraints) often outperforms a sophisticated scheduler with incorrect policies. Before upgrading the scheduler, audit the policies: are users requesting resources they do not need? Are heterogeneous resources properly exposed? Are gang scheduling timeouts configured?

**Pitfall:** *Treating all GPU-hours as equal when measuring utilization.*

Cluster dashboards commonly report "GPU utilization" as a single percentage, averaging across all GPUs and all workloads. This metric hides critical information. A cluster might report 80% utilization where 60% is productive training, 15% is idle GPUs allocated to jobs waiting for gang completion, and 5% is GPUs running data loading with no active computation. Effective monitoring distinguishes between **allocated utilization** (GPUs assigned to jobs), **compute utilization** (GPUs executing kernels), and **productive utilization** (GPUs advancing useful training or serving requests). Only the last metric correlates with actual value delivered.

**Fallacy:** *Gang scheduling is always necessary for distributed training.*

Gang scheduling prevents deadlock for synchronous training, but not all distributed training is synchronous. Asynchronous training methods tolerate worker arrivals and departures, and elastic training frameworks handle variable worker counts. For workloads that can operate asynchronously or elastically, relaxing the gang scheduling requirement dramatically improves scheduling flexibility and reduces queue wait times. The trade-off is potential convergence degradation from stale gradients or batch size variability, but for many practical workloads (hyperparameter sweeps, fine-tuning, pre-training with adaptive batch size), this trade-off is favorable.

**Pitfall:** *Setting static quotas based on peak demand.*

Organizations commonly set team quotas to handle worst-case demand, reasoning that teams need guaranteed access during crunch periods. If Team A's peak demand is 500 GPUs and Team B's is 300 GPUs, the cluster needs 800 GPUs with static quotas. In practice, peaks rarely coincide. Hierarchical fair-share with borrowing can serve both teams' peak demands with a 600-GPU cluster, because when Team A peaks, Team B is typically at moderate demand and can yield borrowed capacity. Static quotas at peak levels waste 25 to 40 percent of cluster capacity on guaranteed-but-unused allocations.

**Fallacy:** *Spot instances are always cheaper for training.*

The 60 to 70 percent discount on spot instances creates the impression of automatic savings. The effective cost depends on interruption frequency, checkpoint overhead, and restart time. For a large model with 15-minute checkpoint times and 30-minute restart times, a spot interruption costs 45 minutes of productive time. If interruptions occur every 2 hours, the effective overhead is 37 percent, reducing the 65 percent discount to a net savings of only 28 percent. For very large models with slow checkpointing on clusters with frequent spot interruptions, on-demand instances can actually be cheaper when accounting for all overhead. The decision requires quantitative analysis of the specific workload and spot market conditions, not blanket assumptions about cost savings.

**Pitfall:** *Ignoring topology when scheduling distributed training.*

Schedulers that treat GPUs as interchangeable units create allocations where tensor parallel groups span nodes (forcing NVLink-speed operations over InfiniBand) or AllReduce groups cross spine switches (adding latency and congestion). The 15 to 30 percent throughput penalty from poor topology placement accumulates over multi-week training runs, potentially wasting more resources than would be lost by waiting for a topology-optimal allocation. Topology-aware scheduling increases scheduling complexity and may reduce packing efficiency, but for large distributed training jobs, the throughput improvement almost always justifies the trade-off.

**Pitfall:** *Autoscaling inference workloads based on GPU utilization alone.*

GPU utilization is a poor proxy for inference health. A GPU can be 90 percent utilized but still violating latency SLOs because the utilization comes from processing a backlog of queued requests. Conversely, a GPU at 40 percent utilization might be perfectly healthy if it is serving low-latency requests with headroom for bursts. The right metrics are queue depth, P99 latency relative to SLO, and KV cache memory pressure for LLM workloads. Relying on utilization creates a lagging indicator that only triggers scaling after performance has already degraded, whereas queue depth and SLO margin provide leading indicators that allow the fleet to scale *before* the user experience suffers. Furthermore, for LLMs, memory exhaustion from KV cache growth can occur at low compute utilization, causing requests to stall or fail. An autoscaler watching only compute utilization will miss this failure mode entirely, leaving the service degraded despite apparent capacity.

**Fallacy:** *Elastic training eliminates the need for gang scheduling.*

Elastic training complements but does not replace gang scheduling. While it allows jobs to resize, the resizing steps themselves often have atomic requirements. More importantly, elastic training works for data parallelism but not tensor parallelism (which requires a fixed number of GPUs per group). A 175B model using 8-way tensor parallelism needs exactly 8 GPUs per tensor-parallel group to function; this is a gang constraint that elastic training cannot relax. Elastic training adjusts the number of data-parallel replicas, not the intra-replica parallelism configuration, meaning the scheduler must still enforce gang constraints for the base unit of the model. If a scheduler naively places these 8 GPUs across different racks or fails to allocate them atomically, the tensor parallel group will either fail to initialize or suffer catastrophic performance degradation.

**Pitfall:** *Treating the scheduler as a black box that "just works."*

Scheduler configuration requires continuous tuning as workloads evolve. Default configurations in Slurm or Kubernetes are optimized for generic workloads, often prioritizing simple fairness over throughput. ML-specific tuning -- including gang scheduling timeouts, topology-aware placement weights, preemption grace periods, and fair-share decay rates -- can improve utilization by 15 to 25 percent. Organizations that deploy a scheduler and never revisit its configuration leave significant value on the table. A default preemption grace period might be too short for a large model to checkpoint, causing wasted work, while an untuned topology weight might fragment the cluster unnecessarily. As the cluster grows and the workload mix shifts (e.g., from training-heavy to inference-heavy), the optimal scheduling parameters shift with it. Treating the scheduler as a set-and-forget appliance guarantees that the fleet will eventually drift into inefficiency.

**Fallacy:** *High utilization means the cluster is well-managed.*

A cluster running at 95% utilization sounds efficient but may indicate a problem: insufficient capacity that is choking experimentation velocity. When utilization consistently exceeds 85 to 90 percent, queue times grow rapidly following the well-known M/M/1 queuing result from stochastic process theory. In this model, average wait time is proportional to $\rho / (1 - \rho)$ where $\rho$ is utilization; at 90% utilization, average wait is 9$\times$ the service time, and at 95%, it is 19$\times$. Researchers waiting hours or days for resources conduct fewer experiments, test fewer hypotheses, and iterate more slowly on model designs. This reduced experimentation velocity has real costs that are harder to quantify than GPU-hours but may be larger in aggregate. The optimal utilization target balances resource efficiency against researcher productivity, typically landing between 70 and 85 percent for training clusters where queue responsiveness directly affects research output. Inference clusters, where requests are served immediately or not at all, typically target even lower utilization (50 to 70 percent) to maintain latency headroom for traffic spikes.

Recognizing these fallacies prevents platform teams from chasing false optimizations that look good on dashboards but degrade developer velocity. We will now summarize the core orchestration principles that successfully navigate these trade-offs to build a productive, high-utilization ML fleet.

## Summary {#sec-fleet-orchestration-summary}

Fleet orchestration transforms the raw capacity of datacenter hardware into productive ML infrastructure. The scheduling algorithms, placement strategies, and resource management policies examined in this chapter determine whether thousands of isolated GPUs function as a single coherent computing platform or as a fragmented collection of expensive servers. The difference between these outcomes is not hardware capability but scheduling sophistication.

We began with the fundamental distributed systems challenges that make cluster scheduling intrinsically hard: partial failures, network partitions, state inconsistency, and the CAP theorem trade-offs that force every scheduler to choose between consistency and availability. These are not theoretical concerns but daily realities in production clusters where failure is normal operation. Slurm and Kubernetes resolve these challenges through fundamentally different architectural philosophies: Slurm's imperative model prioritizes predictable allocation guarantees for batch workloads, while Kubernetes' declarative model prioritizes self-healing availability through continuous reconciliation. The choice between them depends on workload composition, and many production environments use both in complementary roles.

Topology-aware scheduling bridges the gap between abstract resource allocation and physical performance by exploiting the multi-level communication hierarchy of modern GPU clusters. Matching parallelism strategies to topology levels, confining tensor parallelism to NVLink domains, pipeline parallelism to rack-local nodes, and data parallelism to rail-aligned cross-rack communication, improves training throughput by 15 to 30 percent through intelligent placement alone. Elastic training extends this flexibility by allowing jobs to dynamically adjust their resource consumption, enabling faster job starts through immediate allocation at minimum scale, opportunistic scaling when resources become available, and graceful preemption that degrades throughput rather than destroying progress. Cost optimization strategies, particularly spot instances combined with robust fault tolerance infrastructure, can reduce training costs by 50 to 65 percent, demonstrating that investments in fault tolerance yield returns beyond reliability protection.

For inference workloads, we analyzed autoscaling based on custom metrics (particularly the KV cache memory pressure unique to LLM serving), resource isolation through hardware (MIG) and software (MPS, CPU pinning) mechanisms, and the cold-start challenge that creates tension between cost efficiency and responsiveness. Multi-tenant quota systems with hierarchical fair-share and borrowing balance guaranteed access against fleet-wide utilization, preventing the tragedy of the commons without sacrificing capacity to rigid allocation boundaries.

Research schedulers like Tiresias, Gandiva, Themis, and Pollux demonstrate that exploiting ML-specific workload characteristics can improve average job completion time by 40 to 60 percent compared to general-purpose scheduling. The common theme across all four systems is that ML workloads are not opaque resource consumers; they have exploitable structure, iterative computation, heavy-tailed durations, sunk-cost economics, and convergence-dependent resource efficiency, that enables dramatically better scheduling decisions when the scheduler is designed to observe and exploit these patterns.

Effective resource management extends beyond training clusters to the inference serving plane, where the objective function shifts from maximizing throughput to minimizing tail latency. While training workloads are elastic and batch-oriented, serving requires strict SLO compliance under fluctuating arrival rates, necessitating autoscaling strategies driven by custom metrics like queue depth rather than simple GPU utilization. Multi-tenancy introduces the economic necessity of GPU sharing, using mechanisms like MIG or temporal slicing to co-locate low-utilization models, though this often incurs a performance penalty that must be weighed against hardware savings. The training-serving boundary becomes a critical friction point; unified clusters allow for opportunistic scavenging of idle serving capacity for training, but require robust preemption logic to ensure inference requests always take precedence. Implementing hierarchical fair-share scheduling with borrowing enables high utilization across diverse teams, yet technical isolation is insufficient without organizational governance -- chargeback models and quota review cycles are the ultimate enforcers of efficiency in a shared environment.

Scheduling is ultimately an economic exercise where the return on algorithmic sophistication often exceeds the return on additional hardware procurement. The spot market offers significant cost reductions -- often 60 to 90 percent -- but demands fault-tolerant architectures capable of handling interruptions gracefully. Strategies like instance diversification and two-tier checkpointing (fast local, durable remote) mitigate the impact of preemption, turning volatility into a manageable engineering constraint. Financial efficiency further relies on a tiered capacity strategy, balancing expensive on-demand instances for critical paths with reserved instances for baselines and spot for burst capacity. The choice between cloud elasticity and on-premise control is governed by the total cost of ownership, where the break-even point is determined not just by utilization rates, but by the hidden costs of operations, power, cooling, and the opportunity cost of idle silicon.

Mastery of fleet orchestration requires a shift from algorithm design to rigorous system diagnostics. The Fleet Stack framework provides a structured approach to debugging, isolating failures across the hardware, scheduler, and application layers. Common anti-patterns like zombie jobs that consume resources without progress, data starvation where GPUs wait on I/O, and fragmentation that leaves scattered resources unusable are rarely solved by adding more nodes. Instead, they reveal a fundamental mismatch between scheduler policy and infrastructure reality. The most valuable skill in this domain is the ability to trace low utilization to its root cause -- whether it is a communication bottleneck in a distributed training job or an overly aggressive packing algorithm -- proving that a well-tuned scheduler on modest hardware outperforms a naive scheduler on a supercomputer.

::: {.callout-takeaways title="Scheduling Is Systems Engineering"}

* **Distributed Scheduling is Fundamentally Hard**: Cluster scheduling faces challenges (partial failures, network partitions, state inconsistency) that single-machine schedulers never encounter. The CAP theorem forces trade-offs between consistency and availability that shape every scheduling system's design.
* **Gang Scheduling Prevents Deadlock but Reduces Flexibility**: Distributed training requires atomic resource allocation to prevent hold-and-wait deadlocks, but rigid gang scheduling wastes resources when combined with backfill timeouts and elastic training alternatives.
* **Topology Determines Performance**: Where GPUs are placed within the cluster hierarchy matters as much as how many GPUs a job receives. NVLink versus InfiniBand placement decisions can create 15 to 30 percent throughput differences for the same job on the same hardware.
* **ML-Specific Scheduling Outperforms Generic Approaches**: Exploiting workload characteristics (predictable resource needs, iterative computation, diminishing returns) enables 40 to 60 percent improvements in job completion time compared to general-purpose FIFO or fair-share policies.
* **Utilization is a First-Order Economic Driver**: Improving cluster utilization from 50% to 80% on a large cluster effectively adds thousands of GPUs worth of capacity. Scheduling sophistication is one of the highest-leverage engineering investments in ML infrastructure.
* **Policies Must Evolve with Infrastructure**: Scheduling algorithms are only as effective as the policies they implement. When infrastructure changes (heterogeneous hardware additions, topology upgrades, workload mix shifts), policies must be re-evaluated and updated.

:::

With the fleet built, compute nodes defined, networks connected, storage configured, fault tolerance in place, and schedulers running, the machine is ready. The infrastructure layers of Volume II's Fleet Stack are now complete: physical compute (@sec-compute-infrastructure), networking (@sec-network-fabrics), storage (@sec-data-storage), distributed algorithms (@sec-distributed-training-systems, @sec-collective-communication), reliability (@sec-fault-tolerance-reliability), and orchestration (this chapter). Together, these layers transform racks of individual servers into a coherent computing platform capable of training the largest models and serving them to millions of users.

But a training cluster is only a means to an end: producing models that serve users. The transition from training infrastructure to serving infrastructure introduces entirely new challenges. Latency constraints replace throughput optimization as the primary objective. Unpredictable, bursty traffic from real users replaces the deterministic batch scheduling of training jobs. User-facing SLOs with contractual obligations replace the flexible timelines of research experiments. The autoscaling discussion in @sec-fleet-orchestration-autoscaling provided an introduction to these challenges; the next chapter examines them in depth.

::: {.callout-chapter-connection title="From Orchestration to Optimization"}

We have organized the *operational* layer of the Machine Learning Fleet -- the "who gets what, when, and where" of resource management. But the fleet's purpose is not to run training jobs; it is to produce models that serve users. The transition from training to deployment introduces entirely new engineering challenges.

In **Performance Engineering** (@sec-performance-engineering), we examine the techniques that extract maximum throughput from the hardware we have orchestrated: operator fusion, precision engineering, graph compilation, and speculative decoding. These optimizations determine whether the fleet's expensive silicon delivers its theoretical potential or wastes cycles on inefficient execution. The orchestration layer ensures the right resources are allocated; performance engineering ensures those resources are used effectively.

:::

```{=latex}
\part{key:vol2_deployment}
```
