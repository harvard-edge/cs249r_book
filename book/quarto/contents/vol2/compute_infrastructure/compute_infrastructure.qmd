---
engine: jupyter
---

# Compute Infrastructure {#sec-compute-infrastructure}

```{python}
#| echo: false
#| label: chapter-start
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER START
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter initialization and global imports
# │
# │ Why: Registers this chapter with the mlsys registry and provides shared
# │      imports for all subsequent calculation cells.
# │
# │ Imports: mlsys.registry, mlsys.constants, mlsys.formatting
# │ Exports: (none)
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.registry import start_chapter
from mlsys.constants import (
    GB, TB, PB, Gbps, byte, second, GiB, MB, watt, hour, kilowatt,
    BILLION, TRILLION, SEC_PER_HOUR, SEC_PER_DAY, BITS_PER_BYTE, KIB_TO_BYTES,
    A100_MEM_CAPACITY, H100_MEM_CAPACITY, H100_MEM_BW, H100_TDP,
    A100_FLOPS_FP16_TENSOR, A100_MEM_BW, A100_TDP,
    B200_FLOPS_FP16_TENSOR, B200_MEM_BW, B200_TDP,
    V100_FLOPS_FP16_TENSOR, V100_MEM_BW, V100_TDP,
    TPUV1_FLOPS_INT8, TPUV1_TDP,
    TPUV2_FLOPS_BF16, TPUV2_MEM_BW, TPUV2_MEM_CAPACITY,
    TPUV3_FLOPS_BF16, TPUV3_MEM_BW, TPUV3_MEM_CAPACITY,
    TPUV4_FLOPS_BF16, TPUV4_MEM_BW,
    TPUV5P_FLOPS_BF16, TPUV5P_MEM_BW, TPUV5P_MEM_CAPACITY,
    WSE1_CORES, WSE1_MEM_CAPACITY, WSE1_MEM_BW, WSE1_TDP,
    WSE2_CORES, WSE2_MEM_CAPACITY, WSE2_MEM_BW, WSE2_TDP,
    WSE3_CORES, WSE3_MEM_CAPACITY, WSE3_MEM_BW, WSE3_TDP,
    TPUV5P_MEM_BW, NVLINK_H100_BW, INFINIBAND_NDR_BW, PCIE_GEN5_BW, SYSTEM_MEMORY_BW,
    CLOUD_ELECTRICITY_PER_KWH, USD, GPT3_PARAMS,
    Mparam, Bparam, TFLOPs, flop, param, MFLOPs, GFLOPs
)
from mlsys.formatting import fmt, sci, check, md, md_math

start_chapter("vol2:compute_infrastructure")
```

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_infrastructure.png){fig-alt="Isometric datacenter visualization with rows of server racks connected by glowing blue network paths, cooling infrastructure below, and monitoring dashboards displaying utilization graphs above." width=100%}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{100}{30}{15}{10}
\end{marginfigure}

_Why does infrastructure—not algorithms—increasingly determine who can participate in advancing machine learning?_

Machine learning systems have a physical reality that transcends code. While the algorithms for training large models are open, the ability to execute them at scale is gated by the **physics of infrastructure**. Every training run is a race against thermal limits, power delivery stability, and the crushing cost of data movement. When algorithms were the primary bottleneck, progress could happen anywhere. Now that scale dominates, the constraint is who can build and operate the physical systems that make scale possible. Building an ML fleet requires engineering across four physical levels: the **Accelerator** (silicon physics), the **Node** (interconnect density), the **Rack** (thermal management), and the **Pod** (warehouse-scale networking). At each level, the goal is the same: to minimize the time data spends in transit and maximize the time it spends in computation. The economics are equally unforgiving: a 10,000-GPU cluster represents hundreds of millions in capital and megawatts of continuous power. Infrastructure has become the moat that separates organizations that can define the frontier from those that can only follow it. This chapter maps that physical stack, from the stacking of memory dies to the stabilization of grid-scale power ramps.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Explain the **Accelerator Spectrum** (GPU vs. TPU vs. ASIC) and how workload dataflows drive architectural specialization
- Quantify the **Memory Wall** by comparing HBM generations and calculating token generation latency
- Apply the **Roofline Model** to diagnose whether a cluster is throttled by arithmetic units or memory bandwidth
- Analyze the **Bandwidth Hierarchy** (HBM $\to$ NVLink $\to$ InfiniBand) and its impact on parallel strategy selection
- Evaluate **Cooling Architectures** (Air vs Liquid) and calculate datacenter power requirements using **PUE**
- Perform a **Total Cost of Ownership (TCO)** analysis to determine the break-even point for build-vs-buy decisions

:::

```{python}
#| label: infra-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ INFRASTRUCTURE SPECIFICATIONS AND HIERARCHY
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-compute-infrastructure — chapter-wide hardware specs and TCO
# │
# │ Goal: Establish accelerator specs (H100/A100/B200/V100/TPU), bandwidth
# │       hierarchy (HBM→NVLink→PCIe→InfiniBand), and build-vs-buy TCO for a
# │       1-node cluster to support all quantitative claims in the chapter.
# │ Show: H100 ~1979 TFLOPs, ~3.35 TB/s HBM, ridge ~591 FLOP/byte; 8-GPU node
# │       ~640 GiB HBM; cloud ~\$280k/yr vs on-prem ~\$166k/yr at 80% utilization.
# │ How: Pull from mlsys.constants; ridge via (flops/bw).to(flop/byte).m_as(flop/byte);
# │       TCO from CLOUD_ELECTRICITY_PER_KWH + amortized CapEx vs cloud hourly rate.
# │
# │ Imports: mlsys.constants (H100_MEM_BW, H100_FLOPS_FP16_TENSOR,
# │           H100_MEM_CAPACITY, H100_TDP, A100_FLOPS_FP16_TENSOR, A100_MEM_BW,
# │           A100_TDP, B200_FLOPS_FP16_TENSOR, B200_MEM_BW, B200_TDP,
# │           V100_FLOPS_FP16_TENSOR, V100_MEM_BW, V100_TDP,
# │           NVLINK_H100_BW, INFINIBAND_NDR_BW, PCIE_GEN5_BW, SYSTEM_MEMORY_BW,
# │           CLOUD_ELECTRICITY_PER_KWH, GPT3_PARAMS, TPUV3_FLOPS_BF16,
# │           TPUV4_FLOPS_BF16, TPUV5P_FLOPS_BF16, WSE3_CORES, WSE3_MEM_CAPACITY)
# │ Exports: h100_tflops, h100_bw, h100_ridge, ddr_bw_str, nvlink_bw_str,
# │          pcie_bw_str, ib_bw_str, ib_bw_gbs, node_hbm_cap, rack_power_str,
# │          h100_ef, a100_ef, b200_ef, v100_ef, annual_elec_cost_str,
# │          onprem_node_annual_str, cloud_node_annual_str, gpt3_params_b,
# │          t_mem_str, t_comp_str, dominance_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class InfraSetup:
    """
    Namespace for global infrastructure constants and specs.
    """
    # Level 1: Accelerator Specs
    h100_flops = 1979 * TFLOPs/second # H100 Dense FP16
    h100_bw = H100_MEM_BW
    h100_cap = H100_MEM_CAPACITY
    h100_tdp = H100_TDP

    a100_flops = A100_FLOPS_FP16_TENSOR
    a100_bw = A100_MEM_BW
    a100_tdp = A100_TDP

    b200_flops = B200_FLOPS_FP16_TENSOR
    b200_bw = B200_MEM_BW
    b200_tdp = B200_TDP

    v100_flops = V100_FLOPS_FP16_TENSOR
    v100_bw = V100_MEM_BW
    v100_tdp = V100_TDP

    # TPU Specs
    tpuv3_flops = TPUV3_FLOPS_BF16
    tpuv3_bw = TPUV3_MEM_BW
    tpuv4_flops = TPUV4_FLOPS_BF16
    tpuv4_bw = TPUV4_MEM_BW
    tpuv5p_flops = TPUV5P_FLOPS_BF16
    tpuv5p_bw = TPUV5P_MEM_BW

    # Wafer-Scale Specs (WSE-3)
    wse3_cores = WSE3_CORES
    wse3_mem = WSE3_MEM_CAPACITY
    wse3_bw = WSE3_MEM_BW
    wse3_tdp = WSE3_TDP

    ddr_bw = SYSTEM_MEMORY_BW # ~50 GB/s

    # Ridge points
    h100_ridge_val = (h100_flops / h100_bw).m_as(flop/byte)

    # Level 2: Node
    gpus_per_node = 8
    node_hbm_capacity = h100_cap * gpus_per_node
    nvlink_bw = NVLINK_H100_BW
    pcie_bw = PCIE_GEN5_BW
    ib_bw = INFINIBAND_NDR_BW

    # Level 3: Rack
    nodes_per_rack = 4
    rack_power_kw = (h100_tdp * gpus_per_node * nodes_per_rack).m_as(kilowatt) * 1.2
    pue_efficient = 1.10

    # Economics
    dgx_h100_price = 350000 # USD
    electricity_rate = CLOUD_ELECTRICITY_PER_KWH.m_as(USD/(kilowatt*hour))
    utilization = 0.80
    amortization_years = 3

    # Annual OpEx for 1 Node
    node_power_kw = (h100_tdp * gpus_per_node).m_as(kilowatt) * 1.2
    annual_elec_cost = node_power_kw * 8760 * utilization * electricity_rate * pue_efficient

    # Break-even build vs buy
    cloud_h100_hourly = 4.0 # USD/hr/GPU
    cloud_node_annual = cloud_h100_hourly * 8 * 8760 * utilization

    onprem_node_annual_capex = dgx_h100_price / amortization_years
    onprem_node_total_annual = onprem_node_annual_capex + annual_elec_cost

    # Efficiency
    h100_tflops_per_watt = h100_flops.m_as(TFLOPs/second) / h100_tdp.m_as(watt)
    a100_tflops_per_watt = a100_flops.m_as(TFLOPs/second) / a100_tdp.m_as(watt)
    b200_tflops_per_watt = b200_flops.m_as(TFLOPs/second) / b200_tdp.m_as(watt)
    v100_tflops_per_watt = v100_flops.m_as(TFLOPs/second) / v100_tdp.m_as(watt)

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class TokenLatency:
    """
    Scenario: The Physics of Token Latency (Memory Wall).
    Calculates time for loading weights vs performing compute.
    """
    # PARAMETERS
    p_params = 70 * BILLION
    weight_bytes = 2 # FP16
    hbm_bw = 3.35 * (TB/second)
    peak_tflops = 1979 * (TFLOPs/second)

    # CALCULATION
    weight_vol = p_params * weight_bytes
    ops_per_token = 2 * p_params

    t_mem_ms = (weight_vol / hbm_bw.to(byte/second)).m_as('millisecond')
    t_comp_ms = (ops_per_token / peak_tflops.to(flop/second)).m_as('millisecond')

    mem_dominance = (t_mem_ms / (t_mem_ms + t_comp_ms)) * 100

    # OUTPUTS handled in EXPORTS

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
h100_tflops = f"{InfraSetup.h100_flops.m_as(TFLOPs/second):.0f}"
h100_bw = f"{InfraSetup.h100_bw.m_as(TB/second):.2f}"
h100_ridge = f"{InfraSetup.h100_ridge_val:.0f}"
ddr_bw_str = f"{InfraSetup.ddr_bw.m_as(GB/second):.0f}"
nvlink_bw_str = f"{InfraSetup.nvlink_bw.m_as(GB/second):.0f}"
pcie_bw_str = f"{InfraSetup.pcie_bw.m_as(GB/second):.0f}"
ib_bw_str = f"{InfraSetup.ib_bw.m_as(Gbps):.0f}"
ib_bw_gbs = f"{InfraSetup.ib_bw.m_as(Gbps) / 8:.0f}"
node_hbm_cap = f"{InfraSetup.node_hbm_capacity.m_as(GiB):.0f}"
rack_power_str = f"{InfraSetup.rack_power_kw:.0f}"
breakeven_util = f"{int(InfraSetup.utilization * 100)}" # Placeholder logic
h100_ef = f"{InfraSetup.h100_tflops_per_watt:.2f}"
a100_ef = f"{InfraSetup.a100_tflops_per_watt:.2f}"
b200_ef = f"{InfraSetup.b200_tflops_per_watt:.2f}"
v100_ef = f"{InfraSetup.v100_tflops_per_watt:.2f}"
annual_elec_cost_str = f"{InfraSetup.annual_elec_cost:,.0f}"
onprem_node_annual_str = f"{InfraSetup.onprem_node_total_annual:,.0f}"
cloud_node_annual_str = f"{InfraSetup.cloud_node_annual:,.0f}"
gpt3_params_b = f"{GPT3_PARAMS.m_as(Bparam):.0f}"

t_mem_str = f"{TokenLatency.t_mem_ms:.1f}"
t_comp_str = f"{TokenLatency.t_comp_ms:.2f}"
dominance_str = f"{TokenLatency.mem_dominance:.1f}"

# Math built in Python (vol1 pattern: no {python} inside $...$ or $$...$$)
flops_per_token_math = md_math(f"6 \\times {gpt3_params_b} \\times 10^9")
t_comp_math = md_math(f"T_{{\\text{{comp}}}} \\approx \\mathbf{{{t_comp_str} \\text{{ ms}}}}")
t_mem_math = md_math(f"T_{{\\text{{mem}}}} \\approx \\mathbf{{{t_mem_str} \\text{{ ms}}}}")
inf_flops_math = md_math(f"2 \\times {gpt3_params_b} \\times 10^9 = 350 \\times 10^9")
inf_throughput_math = md_math(f"{h100_bw} \\times 10^{{12}} \\times 1.0 = 3.35")
train_intensity_math = md_math(f"2048 \\times 1.0 = 2{{,}}048")
t_intra_math = md_math(f"T_{{\\text{{intra}}}} = 1\\text{{ GB}} / {nvlink_bw_str}\\text{{ GB/s}} \\approx \\mathbf{{1.1 \\text{{ ms}}}}")
t_inter_math = md_math(f"T_{{\\text{{inter}}}} = 1\\text{{ GB}} / {ib_bw_gbs}\\text{{ GB/s}} \\approx \\mathbf{{20 \\text{{ ms}}}}")
pod_flops_math = md_math(f"6 \\times {gpt3_params_b} \\times 10^9 \\times 300 \\times 10^9 \\approx 3.15 \\times 10^{{23}}")
total_flops_math = md_math(f"6 \\times {gpt3_params_b} \\times 10^9 \\times 300 \\times 10^9 = 3.15 \\times 10^{{23}}")
cluster_throughput_math = md_math(f"8{{,}}192 \\times {h100_tflops} \\times 10^{{12}} \\times 0.45 \\approx 7.3")
compute_per_step_math = md_math(f"C = 6 \\times {gpt3_params_b} \\times 10^9 \\times 2 \\times 10^6 = 2.1 \\times 10^{{18}}")
t_compute_math = md_math(f"T_{{\\text{{compute}}}} = 2.1 \\times 10^{{18}} / (1024 \\times {h100_tflops} \\times 10^{{12}} \\times 0.5) \\approx 2.1")
t_comm_math = md_math(f"T_{{\\text{{comm}}}} \\approx 0.5 \\times 350 / {ib_bw_gbs} \\approx 3.5")
comm_time_formula_math = md_math(f"2 \\times (N-1)/N \\times 350 \\text{{ GB}} / ({ib_bw_gbs} \\text{{ GB/s}})")
comm_approx_math = md_math(f"2 \\times 350 / {ib_bw_gbs} \\approx 14")
n_params_math = md_math(f"N = {gpt3_params_b} \\times 10^9")

# TCO display strings (avoid $ in inline expr to prevent validator false positive)
cloud_cost_display = f"${cloud_node_annual_str}"
annual_elec_display = f"${annual_elec_cost_str}"
onprem_display = f"${onprem_node_annual_str}"
util_pct_str = f"{InfraSetup.utilization*100:.0f}"
```

## The Physics of Scale {#sec-compute-introduction}

In the Introduction to this volume (@sec-vol2-introduction), we defined the Machine Learning Fleet not merely as a collection of servers, but as a holistic warehouse-scale computer. This chapter explores the physical foundation of that machine: the **Compute Infrastructure**. While algorithms define *how* a model learns, infrastructure determines *if* it can learn at all. The transition from academic prototypes to frontier systems is fundamentally a transition from software constraints to physical ones -- where the limitations of silicon, electricity, and heat become the primary obstacles to progress.

To understand the magnitude of this challenge, consider our running example: a `{python} gpt3_params_b`B-parameter language model. In standard half-precision (FP16), the model's weights alone occupy 350 GB of memory. A single NVIDIA H100 accelerator -- the current standard for high-performance training -- provides 80 GB of High Bandwidth Memory. The model is more than 4$\times$ larger than the hardware intended to run it. When we account for the optimizer states and gradients required for training, the memory footprint exceeds 2 TB. This single arithmetic reality drives every architectural decision in this chapter: because the model cannot fit on a single chip, we are forced to build a distributed system.

Building this system requires us to expand outward from the silicon die in a series of concentric physical layers, each designed to overcome a specific fundamental constraint:

1.  **The Accelerator**: At the center, we confront the **Memory Wall**. Specialized silicon (GPUs, TPUs) maximizes arithmetic throughput, but is constantly starved for data. The engineering response is High Bandwidth Memory (HBM) and extreme on-chip caching.
2.  **The Node**: To solve the capacity deficit, we aggregate multiple accelerators into a single chassis. Here we face the bandwidth limits of standard interfaces like PCIe. The response is high-speed proprietary interconnects (NVLink, Infinity Fabric) that bind distinct chips into a unified memory space.
3.  **The Rack**: Concentrating this much compute density triggers the **Power Wall**. A single rack of ML servers consumes as much power as a residential neighborhood. The response shifts from electrical engineering to thermodynamics, necessitating liquid cooling and advanced power delivery networks.
4.  **The Pod**: Finally, to train in weeks rather than decades, we scale to thousands of accelerators. Here we hit the **Communication Wall**, where network latency threatens to idle the entire cluster. The response is the creation of a "supercomputer-in-a-warehouse" topology.

::: {.callout-note title="Connection: The Fleet Stack"}

In the Fleet Stack model introduced in @sec-vol2-introduction, **Compute Infrastructure** serves as the foundation layer -- the physical silicon and power systems upon which everything else is built. It provides the raw execution capability that **Network Fabrics** (@sec-network-fabrics) wire together and **Data Storage** (@sec-data-storage) feeds. Without understanding the physics of this compute layer, the constraints that shape every subsequent chapter -- from the necessity of distributed training algorithms to the complexity of fleet orchestration -- remain invisible.

:::

Even for machine learning engineers who will never crimp an Ethernet cable or debug a liquid cooling loop, the physical reality of the datacenter defines the *constraints* of their software. The decision to shard our `{python} gpt3_params_b`B model is not merely algorithmic but a direct response to the bandwidth hierarchy: tensor parallelism exploits ultra-fast intra-node NVLink, while pipeline parallelism mitigates the latency of slower inter-node links. The optimal batch size is determined by the Roofline Model, ensuring arithmetic intensity prevents the accelerator from stalling on memory fetches. The necessity of quantization is a concession to the Memory Wall. At scale, the frequency of checkpointing is a function of the Mean Time Between Failures (MTBF) of a 1,000-GPU pod. Every software optimization in distributed training is ultimately an attempt to navigate these physical boundaries, transforming system configuration from ad hoc heuristics into principled engineering grounded in the physics of latency, bandwidth, and failure rates.

We begin our exploration at the center of this hierarchy, where the struggle to bridge the gap between CPU capability and AI demand first began: the accelerator.

## How ML Workloads Invert Compute Assumptions {#sec-compute-workload-inversion}

A computer architect from the era of general-purpose supercomputing would find the design of a modern ML cluster both familiar and deeply strange. While both systems are built for high-performance parallel processing, the underlying assumptions about the workload are fundamentally different. Traditional High-Performance Computing (HPC) for scientific simulation often requires high-precision floating-point arithmetic (FP64), deals with irregular data access patterns (e.g., sparse matrix solvers, graph traversals), and balances computation with significant I/O for loading initial conditions and saving simulation state.

ML workloads, particularly for training large neural networks, systematically invert these assumptions. This inversion is not a minor difference in emphasis; it is a profound shift in the computational paradigm that has driven the design of every successful ML accelerator, from GPUs to TPUs to custom ASICs.

| **Workload Pattern**    | **Traditional HPC Assumption**    | **ML Reality**                              |
|:------------------------|:----------------------------------|:--------------------------------------------|
| **Numerical Precision** | FP64 for accuracy                 | FP16/BF16/FP8 tolerated, even preferred     |
| **Instruction Mix**     | Diverse, complex instruction set  | Dominated by one operation: matrix multiply |
| **Control Flow**        | Branch-heavy, data-dependent      | Uniform, predictable (data-parallel)        |
| **Data Reuse**          | Varies; often limited by locality | Extreme (weights reused for every sample)   |
| **Fault Tolerance**     | Must be perfect or restart        | Checkpoint/resume is part of the workflow   |

: **ML Workloads Invert Traditional Compute Assumptions**: Where HPC prioritizes precision and generality, ML training sacrifices both for massive throughput on a narrow set of operations. {#tbl-compute-assumptions}

**The Precision Inversion**: Traditional scientific computing demands 64-bit precision (FP64) to accurately simulate physical phenomena where small errors can compound over time. ML training, by contrast, has demonstrated remarkable tolerance for lower precision. The stochastic nature of the training process means that the "noise" introduced by 16-bit (FP16/BF16) or even 8-bit (FP8) arithmetic is often absorbed by the optimization algorithm. This tolerance is the single most important economic and architectural feature of ML compute. It allows architects to build hardware (like Tensor Cores) that performs four FP16 operations or eight FP8 operations in the same silicon area and power budget as a single FP32 operation, yielding a 4--8$\times$ improvement in throughput for free.

**The Instruction Mix Inversion**: General-purpose CPUs and HPC systems are designed to execute a vast and complex instruction set, from integer arithmetic to floating-point division to complex conditional branching. Profiling a large neural network training run reveals a starkly different picture: over 90% of the computation is spent on a single operation—the matrix-multiply-accumulate (GEMM). This extreme regularity allows for radical hardware specialization. Instead of building flexible, general-purpose ALUs, architects can dedicate silicon to hyper-optimized matrix engines (Tensor Cores, Systolic Arrays), achieving 100--1000$\times$ the throughput of a CPU for that specific operation.

**The Control Flow Inversion**: Traditional parallel programs often involve complex, data-dependent control flow, where different processing elements take different paths through the code. ML training, particularly data-parallel training, is the opposite: every accelerator executes the exact same sequence of operations on different data. This "Single Program, Multiple Data" (SPMD) model is a hardware architect's dream, as it eliminates the need for complex branch prediction and reordering logic, freeing up silicon and power for more arithmetic units.

These inversions explain why CPUs are poorly suited for large-scale ML and why a specialized ecosystem of accelerators has emerged. The "generality tax" paid by CPUs—the silicon dedicated to handling a wide range of tasks—is a price that ML workloads do not need to pay.

## The Accelerator {#sec-compute-accelerator-core}

Suppose we want to train a `{python} gpt3_params_b`B-parameter language model. The arithmetic is straightforward: each training step requires roughly `{python} flops_per_token_math` floating-point operations per token, and we need to process trillions of tokens. A modern server CPU can deliver perhaps 1--2 TFLOPS of matrix throughput. At that rate, training would take decades.

To put this in perspective, training GPT-3 on a single modern CPU would require approximately 355 years. Even with an aggressive 64-core server running at 2 TFLOPS aggregate, the computation would take over 5 years. The gap between CPU capability and the compute requirements of frontier models is not a small inefficiency that clever software can bridge; it is a chasm of 3--4 orders of magnitude that can only be crossed by fundamentally different hardware.

This gap has widened over time, not narrowed. In 2012, the compute required to train AlexNet (61 million parameters on ImageNet) was approximately $10^{15}$ FLOPs, achievable on a single GPU in a few days. By 2020, GPT-3 required approximately $3.1 \times 10^{23}$ FLOPs, an increase of 8 orders of magnitude in 8 years. CPU performance over the same period improved by perhaps 5$\times$, following the slowdown of Moore's Law and the end of Dennard Scaling. The accelerator's entire purpose is to bridge this exponentially growing gap between what models demand and what general-purpose hardware can deliver.

The question that launched an entire hardware industry is therefore not *whether* to accelerate, but *how much generality to sacrifice* in exchange for the throughput that makes large-scale training feasible. This question does not have a single answer; it depends on the workload's predictability, the organization's scale, and the time horizon of the investment.

In practice, the "build versus buy" calculus for hardware acceleration is dominated not by silicon efficiency but by **software ecosystem lock-in**. While a custom ASIC might offer a 10$\times$ improvement in performance per watt for specific tensor operations by discarding the generality tax of GPU display engines and rasterization logic, the switching costs are often prohibitive. Migrating the training stack for our `{python} gpt3_params_b`B model from NVIDIA's CUDA to a proprietary ASIC environment requires rewriting highly optimized kernels for attention mechanisms, porting communication primitives like NCCL, and debugging numerical instabilities that vanish on established platforms. The **Non-Recurring Engineering (NRE)** cost for a modern 5nm ASIC design ranges from \$50 million to over \$200 million, with a design cycle of 24--36 months. By the time a custom chip optimized for a specific Transformer architecture returns from the fab, the state of the art may have shifted to Mixture-of-Experts or state-space models, rendering the hardware assumptions obsolete. Consequently, only hyperscalers with stable, massive-scale workloads can amortize this risk. For most organizations, the GPU premium is effectively an insurance policy against software incompatibility and architectural obsolescence.

This question sits at the foundation of every fleet. The smallest physical unit of the machine learning fleet is the accelerator[^fn-accelerator-generality], and every accelerator is a physical response to three fundamental constraints that limit computation:

1. **The Memory Wall**: Data movement from storage to arithmetic units consumes more time and energy than the arithmetic itself.
2. **The Power Wall**: The maximum heat that silicon can dissipate sets an upper bound on clock frequency and transistor density.
3. **The Communication Wall**: Synchronizing partial results across thousands of chips introduces overhead that can dominate the computation.

::: {.callout-important title="The Three Walls of Fleet Computing"}

Every architecture decision in this chapter is a response to one or more of three fundamental physical constraints. The **Memory Wall** limits per-device throughput: HBM bandwidth, not arithmetic capability, determines token generation latency for inference workloads. The **Power Wall** limits per-rack compute density: thermal dissipation at 700--1,000 W per accelerator forces the transition from air cooling to liquid cooling and drives megawatt-scale power delivery engineering. The **Communication Wall** limits per-cluster scaling: network bandwidth between nodes determines whether adding more accelerators produces proportional speedup or diminishing returns. These three walls are not independent problems to be solved in isolation; they interact at every level of the infrastructure stack, and the design of each level represents a negotiated response to all three simultaneously.

:::

@fig-three-walls-grid illustrates how these three constraints cascade through every level of the infrastructure stack, from a single accelerator to a full pod.

::: {#fig-three-walls-grid fig-env="figure" fig-pos="htb" fig-cap="**The Three Walls Across Infrastructure Levels**. The Memory Wall, Power Wall, and Communication Wall are not independent problems; they interact at every level of the physical stack. Each row shows how the three constraints manifest at a given scale, and the downward arrows indicate that solving the constraint at one level creates the dominant constraint at the next. The rightmost column shows why the Communication Wall ultimately dominates at cluster scale." fig-alt="Grid with four rows for Accelerator, Node, Rack, and Pod levels and three columns for Memory, Power, and Communication walls. Downward arrows connect rows showing constraint cascade."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{BlueL}{HTML}{D1E6F3}
  \definecolor{RedLine}{HTML}{CB202D}
  \definecolor{RedL}{HTML}{F5D2D5}
  \definecolor{GreenLine}{HTML}{008F45}
  \definecolor{GreenL}{HTML}{D4EFDF}
  \definecolor{OrangeLine}{HTML}{E67817}
  \definecolor{OrangeL}{HTML}{FCE4CC}
  \definecolor{BrownLine}{HTML}{78492A}
  \definecolor{BrownL}{HTML}{E3D3C8}

  \tikzset{
    cell/.style={draw=#1, fill=#1!15, rounded corners=2pt, minimum width=3.4cm,
                 minimum height=1.1cm, align=center, font=\scriptsize, thick},
    levelbox/.style={draw=BrownLine, fill=BrownL!40, rounded corners=2pt,
                     minimum width=1.8cm, minimum height=1.1cm, align=center,
                     font=\small\bfseries},
    wallhdr/.style={font=\small\bfseries, align=center},
    cascade/.style={-{Triangle[width=5pt,length=4pt]}, thick, black!40}
  }

  % Column headers
  \node[wallhdr, text=BlueLine] at (3.2, 7.2) {Memory Wall};
  \node[wallhdr, text=OrangeLine] at (7.0, 7.2) {Power Wall};
  \node[wallhdr, text=GreenLine] at (10.8, 7.2) {Communication Wall};

  % Row 1: Accelerator
  \node[levelbox] (L1) at (0, 5.8) {Accelerator};
  \node[cell=BlueLine] (m1) at (3.2, 5.8) {HBM bandwidth\\bottleneck\\(3.35 TB/s)};
  \node[cell=OrangeLine] (p1) at (7.0, 5.8) {700 W TDP\\per chip};
  \node[cell=GreenLine] (c1) at (10.8, 5.8) {On-die SRAM\\capacity limit};

  % Row 2: Node
  \node[levelbox] (L2) at (0, 3.8) {Node\\(8 GPUs)};
  \node[cell=BlueLine] (m2) at (3.2, 3.8) {640 GB aggregate\\HBM capacity\\$<$ model size};
  \node[cell=OrangeLine] (p2) at (7.0, 3.8) {5.6 kW compute\\forces liquid\\cooling};
  \node[cell=GreenLine] (c2) at (10.8, 3.8) {NVLink 900 GB/s\\intra-node only};

  % Row 3: Rack
  \node[levelbox] (L3) at (0, 1.8) {Rack\\(4 nodes)};
  \node[cell=BlueLine] (m3) at (3.2, 1.8) {Host DRAM\\needed for\\optimizer state};
  \node[cell=OrangeLine] (p3) at (7.0, 1.8) {40+ kW rack\\power delivery\\and cooling};
  \node[cell=GreenLine] (c3) at (10.8, 1.8) {PCIe between\\nodes: 64 GB/s};

  % Row 4: Pod
  \node[levelbox] (L4) at (0, -0.2) {Pod\\(1000+ GPUs)};
  \node[cell=BlueLine] (m4) at (3.2, -0.2) {Distributed\\storage: PFS\\and object};
  \node[cell=OrangeLine] (p4) at (7.0, -0.2) {Megawatt facility\\PUE $\approx$ 1.1--1.3};
  \node[cell=GreenLine] (c4) at (10.8, -0.2) {InfiniBand\\50 GB/s per link\\dominates cost};

  % Cascade arrows
  \foreach \i/\j in {1/2, 2/3, 3/4} {
    \draw[cascade] (m\i.south) -- (m\j.north);
    \draw[cascade] (p\i.south) -- (p\j.north);
    \draw[cascade] (c\i.south) -- (c\j.north);
  }

  % Emphasis bracket on Communication Wall
  \draw[RedLine, ultra thick, decorate, decoration={brace, amplitude=6pt, mirror}]
    (12.7, -0.8) -- (12.7, 6.4)
    node[midway, right=8pt, align=left, font=\scriptsize\bfseries, text=RedLine]
    {Dominates\\at scale};

\end{tikzpicture}
```
:::

This chapter begins at the silicon die and expands outward through four physical levels. At each level, we encounter the same engineering pattern: a constraint becomes intolerable, and the solution creates the next level of infrastructure. The Node aggregates accelerators to overcome memory capacity limits. The Rack concentrates nodes and confronts power delivery and cooling. The Pod wires racks into a warehouse-scale computer and faces the communication wall at full force. By the end, we will have mapped the complete physical stack from transistor to datacenter, with our `{python} gpt3_params_b`B model serving as the thread that connects each level.

[^fn-accelerator-generality]: **Accelerator (ML Context)**: A CPU devotes 5--10% of die area to arithmetic; a GPU devotes 50--60%; a TPU 70--80%. This progression quantifies the generality-efficiency trade-off: every percentage point of die area reclaimed from control logic and caches becomes additional multiply-accumulate units, directly increasing peak TFLOPS per watt but narrowing the set of operations the chip can execute efficiently. \index{Accelerator!generality trade-off}

### Accelerator Spectrum {#sec-compute-accelerator-spectrum}

\index{Accelerator!spectrum}
\index{Generality Tax}

Consider what happens when a CPU executes a matrix multiplication. The processor fetches an instruction, decodes it, checks for data hazards, routes operands through a deep pipeline, and writes the result back to a register file. For each multiply-accumulate operation, the chip expends energy on branch prediction, speculative execution, out-of-order scheduling, and cache coherence.

These mechanisms exist because a CPU must handle *any* instruction sequence efficiently, from pointer-chasing linked-list traversals to system calls. For neural network workloads, however, the operation is almost always the same: multiply two matrices of known dimensions, add a bias, and apply a nonlinear function. The CPU's elaborate control logic represents a tax on every operation, one that buys flexibility the workload does not need. This concept mirrors the classic RISC vs. CISC debate in computer architecture: just as RISC processors achieved higher throughput by simplifying the instruction set, ML accelerators achieve higher throughput by simplifying the computational model to match the dominant workload pattern.

::: {.callout-perspective title="The Generality Tax"}

A modern server CPU devotes roughly 30--40% of its die area to caches, 20--30% to control logic (branch predictors, reorder buffers, instruction decoders), and only 5--10% to arithmetic units. This allocation makes sense for general-purpose code, where branches are unpredictable and data access patterns are irregular. For matrix multiplication, however, the access pattern is perfectly regular and the control flow is trivially predictable. Every transistor spent on branch prediction or speculative execution is a transistor that could have been a multiplier. This is the **Generality Tax**: the silicon area that a processor wastes on capabilities irrelevant to the dominant workload.

The accelerator revolution is, at its core, an exercise in eliminating this tax. Each step along the spectrum from CPU to custom ASIC reclaims more die area for arithmetic by removing another layer of general-purpose control logic. The progression is quantifiable: a CPU devotes roughly 5--10% of its die to arithmetic, a GPU devotes roughly 50--60%, a TPU devotes roughly 70--80%, and a purpose-built ASIC can devote over 90% of its die to the target computation.

:::

The transition from general-purpose to specialized silicon follows a logical progression. At one end of the spectrum, GPUs retain substantial programmability while providing 10--100$\times$ the matrix throughput of CPUs. At the other end, custom ASICs hardwire a specific dataflow for maximum efficiency at the cost of flexibility.

Understanding *where* each architecture falls on this spectrum, and *why*, is essential for selecting hardware that matches a given workload. The spectrum is not a ranking from "bad" (general-purpose CPU) to "good" (custom ASIC), but a continuum of trade-offs where each position offers the best match for a different combination of workload characteristics and deployment constraints.

*How can we gain massive parallelism while retaining programmability?*

**GPUs** represent the first major step away from general-purpose computing. An NVIDIA H100, for example, contains 16,896 CUDA cores organized into 132 Streaming Multiprocessors (SMs). Each SM can execute thousands of threads simultaneously using the **Single Instruction, Multiple Thread (SIMT)**[^fn-simt-warp] execution model. Unlike a CPU, which optimizes for single-thread latency, a GPU hides memory latency by maintaining thousands of threads in flight and switching between them when one stalls on a memory access.

The programmer writes a single function (a *kernel*), and the hardware maps it across a massive grid of threads. This model is flexible enough to run any mathematical kernel, from convolutions to attention mechanisms to custom loss functions, while providing orders of magnitude more throughput than a CPU for data-parallel computation.

The trade-off is that irregular, branch-heavy code runs poorly because divergent threads within a warp (a group of 32 threads that execute in lockstep) must execute serially. When threads in the same warp take different branches of an if-else statement, the hardware must execute both branches sequentially, disabling the threads that took the other path during each branch. For ML workloads, this divergence penalty is usually small because neural network operations have highly uniform control flow: every element in a matrix multiplication follows the same computation path.

[^fn-simt-warp]: **SIMT (Single Instruction, Multiple Thread)**: Coined by NVIDIA to distinguish their GPU model from classical SIMD. In SIMT, 32 threads form a *warp* that shares an instruction stream but can diverge at branches; divergence forces serial execution of both paths, halving throughput per branch. For ML workloads, where matrix operations have uniform control flow, warp divergence is rare, which is why GPUs achieve near-peak utilization on GEMMs but degrade on irregular operations like sparse attention or dynamic routing. \index{SIMT!warp divergence}

*What if we sacrifice some programmability for maximum efficiency on a single operation type?*

**TPUs** go a step further by hardwiring the dataflow itself. Google's Tensor Processing Units use a **systolic array**[^fn-systolic-array-tpu] architecture: a fixed grid of multiply-accumulate (MAC) units where data flows between neighboring cells in a regular, wave-like pattern. Instead of fetching and decoding instructions for each operation, the systolic array receives a matrix at one edge and pulses it through the grid, with each cell performing one MAC and passing the result to its neighbor. This eliminates the instruction fetch and decode overhead entirely, and it avoids writing intermediate results to memory because each partial sum flows directly to the next computation.

The cost of this efficiency is reduced programmability. Models must be compiled through XLA (Accelerated Linear Algebra), which maps high-level operations onto the fixed dataflow. Workloads with irregular control flow or dynamic shapes may not map well to this architecture, and the compilation process itself can be time-consuming (minutes to hours for complex models), which slows the iteration cycle during model development.

The programming model distinction between GPUs and TPUs has practical implications for organizational decisions. Research teams that frequently modify model architectures (adding custom attention patterns, experimenting with new activation functions, prototyping novel training algorithms) generally prefer the GPU's CUDA ecosystem because new operations can be implemented as custom kernels without waiting for compiler support. Teams running established architectures at scale (standard Transformer training, large-scale fine-tuning) may prefer TPUs because the XLA compiler can optimize the entire computation graph, often achieving better hardware utilization than hand-written CUDA kernels for standard operations.

[^fn-systolic-array-tpu]: **Systolic Array**: From Greek *systole* (contraction) -- coined by H. T. Kung and Leiserson in 1978 because data pulses through a grid of processing elements in a heart-like rhythm. The metaphor explains the design: each cell performs one MAC and passes the result to its neighbor, eliminating register-file round-trips. Google's TPU v1 deployed a 256$\times$256 array (65,536 MACs), achieving 92 TOPS within 75 W by hardwiring this dataflow -- the architectural bet that made datacenter-scale inference economically viable. \index{Systolic Array!etymology}

::: {.callout-war-story title="The TPU Origin Story"}

In 2013, Google engineers projected that if users spoke to their Android phones for just three minutes per day using voice search, the company would need to double its datacenter compute capacity to handle the inference load. The cost was prohibitive. When they profiled their production workloads, a striking pattern emerged: over 90% of inference cycles were spent on matrix multiplications in neural network models. The existing GPU fleet was powerful but expensive, and its general-purpose SIMT architecture carried the generality tax for capabilities that inference workloads never used. This observation led directly to the TPU v1, a purpose-built inference accelerator with a 256$\times$ 256 systolic array that could deliver 92 TOPS (INT8) within a 75 W power envelope. The first TPUs were deployed in 2015, and within a year they were processing every Google Search query, every Google Translate request, and every AlphaGo move [@jouppi2017datacenter].

:::

*When does it make economic sense to abandon general-purpose programmability entirely?*

**Custom ASICs** represent the extreme end of the spectrum. When an organization runs a single model architecture at enormous scale, the economics of silicon justify designing a chip specifically for that workload. By stripping away every feature not required by the target computation, custom ASICs achieve the lowest energy per operation and the highest sustained utilization of any architecture on the spectrum.

Tesla's Dojo D1 chip, for example, is optimized for the spatial and temporal convolutions in video-based vision models. Its 1,024 custom cores are designed to execute the exact computational primitives needed by Tesla's vision pipeline, with on-chip SRAM sized to hold the working set of a single spatial tile. This eliminates the repeated round-trips to off-chip memory that a general-purpose GPU would require for the same computation.

AWS Trainium takes a different approach, targeting the broad category of Transformer training rather than a single model. Trainium chips include hardware support for attention-pattern-specific memory access (prefetching the KV cache entries in the order the attention mechanism will access them) and custom collective communication engines that implement AllReduce in hardware rather than software.

The risk of custom silicon is equally clear: if the dominant model architecture shifts (as it did from CNNs to Transformers between 2017 and 2020), a custom ASIC designed for the old architecture becomes a stranded asset. The design cycle for a new ASIC is 2--3 years from conception to deployment, which means the architecture decision must anticipate workload trends several years into the future. This prediction challenge is non-trivial: in 2015, few would have predicted that attention-based Transformers would replace CNNs as the dominant architecture within five years, and organizations that committed to CNN-optimized ASICs during that period found their hardware stranded by the architectural shift.

Custom silicon is therefore a bet on workload stability, and the organizations that make this bet are typically those with enough scale to justify the \$50--200 million development cost and enough workload volume to amortize the per-chip NRE (non-recurring engineering) cost across millions of chips. Google can justify the TPU's development cost because every Google Search query, every YouTube recommendation, and every Gmail spam filter uses the same accelerator. A research lab running a few hundred GPUs cannot justify the same investment.

The bet pays off handsomely when workloads are stable: a purpose-built ASIC can deliver 5--10$\times$ the energy efficiency of a general-purpose GPU for its target operation. But the consequences of a wrong bet are severe, as the chip's fixed dataflow cannot be reprogrammed to accommodate a fundamentally new computational pattern.

*Can we scale the silicon die to the size of the entire wafer?*

### Wafer-Scale Engines {#sec-compute-wafer-scale}

\index{Wafer-Scale Engine}
\index{Cerebras}

**Wafer-Scale Engines (WSE)** represent the ultimate pursuit of data locality. While every other architecture on the spectrum relies on chiplets or discrete dies connected by relatively slow PCB-level or package-level interconnects, a wafer-scale engine (like the Cerebras WSE-3) is a single, continuous piece of silicon the size of a dinner plate. By avoiding the need to "dice" the wafer into individual chips, a WSE can maintain a single, massive on-chip interconnect across its entire surface.

The WSE-3 contains 900,000 AI-optimized cores and 44 GB of on-chip SRAM, all connected by a silicon fabric that delivers 21 PB/s of memory bandwidth. To put this in perspective, a single WSE-3 has roughly the same compute and memory bandwidth as a cluster of hundreds of H100 GPUs, but because the entire system resides on a single piece of silicon, the communication latency between any two cores is measured in nanoseconds rather than microseconds.

The challenge of wafer-scale integration is physical: manufacturing yield, power delivery, and thermal expansion. A single defect on a standard chip might render it useless, but on a wafer-scale engine, the software must be "defect-aware," routing around local manufacturing flaws in the silicon fabric. Delivering 23 kW of power to a single piece of silicon and cooling it requires specialized manifold-level liquid cooling that is closer to industrial plumbing than traditional computer engineering.

Wafer-scale engines sit at a unique point on the spectrum: they are highly specialized in their *physical* architecture but flexible in their *computational* model, as the underlying cores are often general-purpose enough to execute diverse ML kernels. They represent a "Scale Up" philosophy that attempts to eliminate the Communication Wall by making the cluster the chip.

::: {#fig-wafer-scale-engine fig-env="figure" fig-pos="htb" fig-cap="**Wafer-Scale Engine (WSE) Architecture**: Unlike traditional processors, a WSE uses the entire 300mm silicon wafer as a single continuous compute fabric. This eliminates the 'Dicing' process and replaces PCB-level interconnects with a high-bandwidth on-chip mesh. The software must be defect-aware, routing data through a unified fabric that treats the entire wafer as a single logical processor with uniform memory access (UMA) characteristics across 900,000 cores." fig-alt="Diagram showing a large circular silicon wafer covered in a grid of hundreds of thousands of small compute cores. A red line shows a data path routing around a black 'defect' node in the grid. Inset shows a magnifying glass over a 4x4 grid of cores connected by high-speed local links."}
```{.tikz}
\begin{tikzpicture}[line join=round, font=\usefont{T1}{phv}{m}{n}\small, scale=0.8]
\tikzset{
  core/.style={rectangle, draw=BlueLine, fill=BlueL!40, minimum size=3.5mm, line width=0.4pt, rounded corners=0.5pt},
  defect/.style={rectangle, draw=RedLine, fill=RedL, minimum size=3.5mm, line width=0.4pt, rounded corners=0.5pt},
  link/.style={line width=0.6pt, black!30},
  route/.style={line width=1.2pt, OrangeLine, -{Triangle[length=3pt, width=4pt]}}
}

% Draw the Wafer (circular boundary)
\draw[line width=1.5pt, black!10, fill=black!2] (0,0) circle (4.2);
\draw[line width=0.8pt, black!20] (0,0) circle (4.15);

% Draw Grid of Cores (subset for visualization)
\foreach \x in {-8,-7,...,8} {
  \foreach \y in {-8,-7,...,8} {
    \pgfmathsetmacro{\dist}{sqrt(\x*\x + \y*\y)*0.45}
    \ifdim \dist pt < 4.0pt
      \node[core] (c\x\y) at (\x*0.45, \y*0.45) {};
    \fi
  }
}

% Simulate a couple of defects
\node[defect] at (0.45*2, 0.45*1) {};
\node[defect] at (0.45*-3, 0.45*-4) {};

% Example routing path
\draw[route] (c-2-2) -- (c-2-1) -- (c-1-1) -- (c0-1) -- (c1-1) -- (c1-2) -- (c2-2) -- (c3-2);

% Zoom inset
\begin{scope}[shift={(5.5, 1.5)}, scale=1.2]
  \draw[black!20, fill=white, rounded corners=3pt] (-1.2,-1.2) rectangle (1.2,1.2);
  \foreach \x in {-1,0,1} {
    \foreach \y in {-1,0,1} {
      \node[core, minimum size=6mm] (z\x\y) at (\x*0.7, \y*0.7) {};
    }
  }
  % Links in zoom
  \foreach \x in {-1,0,1} {
    \draw[link] (\x*0.7, -0.9) -- (\x*0.7, 0.9);
    \draw[link] (-0.9, \x*0.7) -- (0.9, \x*0.7);
  }
  \node[font=\fontsize{6pt}{6}\selectfont] at (0,-1.05) {Inter-core Links (PB/s)};
\end{scope}

% Labels
\node[below] at (0,-4.4) {\textbf{300mm Silicon Wafer}};
\node[above right] at (5.6, 2.8) {\textbf{Unified Compute Fabric}};

\end{tikzpicture}
```
:::

| **Feature**         |    **CPU**    | **GPU** (H100) | **TPU** (v5p)  | **Wafer-Scale** | **Custom ASIC** |
|:--------------------|:-------------:|:--------------:|:--------------:|:---------------:|:---------------:|
| **Arithmetic Core** | Scalar/Vector |  Tensor Core   | Systolic Array | RISC-style Core |  Fixed Dataflow |
| **Execution**       |  Instruction  |      SIMT      |  Data-Driven   |     Dataflow    |    Hardwired    |
| **Memory Control**  |     Cache     |  L1/L2 + HBM   |   Scratchpad   |   On-Chip SRAM  |  Explicit Mesh  |
| **Flexibility**     |  **Extreme**  |      High      |    Moderate    |       High      |       Low       |
| **Efficiency**      |      Low      |      High      |   Very High    |       High      |   **Extreme**   |

: **The Accelerator Spectrum**. As we move from left to right, we trade general-purpose programmability for compute density and power efficiency. Wafer-scale engines occupy a distinct niche, providing cluster-scale performance on a single piece of silicon. {#tbl-accelerator-comparison}

For our `{python} gpt3_params_b`B model, the choice is not purely about peak FLOPS. If we are a research lab experimenting with novel architectures weekly, the GPU's flexibility justifies its generality tax. If we are deploying a fixed Transformer at scale for years, the TPU's dataflow efficiency or a custom ASIC's power advantage may dominate total cost. The accelerator spectrum is ultimately an economic question: *how much flexibility can we afford to surrender, given the stability of our workload?*

An emerging trend in accelerator design is the **chiplet** architecture, exemplified by NVIDIA's Blackwell and AMD's Instinct MI300 series. Rather than fabricating a single monolithic die, chiplet-based designs partition the processor into multiple smaller dies connected by a high-bandwidth die-to-die interconnect on a common package substrate. This approach addresses two physical limitations that constrain monolithic designs.

First, the maximum die size is limited by the reticle size of lithographic equipment, roughly 800 mm$^2$ for current EUV scanners. A monolithic GPU cannot exceed this area, which caps the number of Tensor Cores, SMs, and HBM stacks that can be integrated. Chiplet designs bypass this limit by placing multiple dies on a single package, with the B200's dual-die design effectively creating a 1,600 mm$^2$ equivalent processor.

Second, manufacturing yield decreases exponentially with die area because a single defect anywhere on the die renders the entire chip unusable. Smaller chiplets have higher individual yield, and the package-level integration allows partial yields (a defective chiplet can be replaced with a good one). This yield advantage translates directly to lower manufacturing cost per unit of compute, which is particularly important as transistor densities continue to increase and process nodes become more expensive.

The trade-off is the die-to-die interconnect. Communication between chiplets within a package is faster than communication between packages (NVLink) but slower than communication within a single monolithic die (the on-die mesh network). Workloads that generate frequent, fine-grained communication between processing elements (such as operations that share data between non-adjacent SMs) may experience a latency penalty when those SMs reside on different chiplets. GPU vendors mitigate this by making the die-to-die link transparent to the programmer, so the software sees a single logical GPU regardless of the underlying chiplet topology.

#### Evolution of GPU Architectures

The rapid evolution of NVIDIA's GPU architectures over the past decade illustrates how accelerator design has adapted to the changing demands of ML workloads. Each generation has not merely increased transistor counts but has introduced architectural innovations targeted at the specific bottlenecks revealed by production ML workloads.

The **Volta** architecture (2017) introduced the first-generation Tensor Core, recognizing that neural network training spends the majority of its time in matrix multiplications. By adding dedicated matrix-multiply-accumulate (MMA) hardware alongside the existing CUDA cores, Volta could accelerate the dominant workload pattern without sacrificing the general-purpose programmability that made GPUs attractive for research. The V100, Volta's flagship, delivered 125 TFLOPS of FP16 Tensor Core throughput at 300 W.

The **Ampere** architecture (2020) expanded Tensor Core support to additional data types (TF32, BF16, INT8, INT4), reflecting the growing importance of mixed-precision training and quantized inference. The A100 also introduced Multi-Instance GPU (MIG), which partitions a single GPU into up to seven isolated instances, enabling efficient sharing of expensive hardware across multiple inference workloads. Perhaps most significantly for infrastructure, Ampere introduced third-generation NVLink with 600 GB/s bidirectional bandwidth per GPU, up from 300 GB/s in Volta, directly addressing the communication wall for tensor parallelism.

The **Hopper** architecture (2022) added the Transformer Engine, which dynamically selects between FP8 and FP16 precision on a per-layer basis, doubling the effective throughput for Transformer models without requiring manual precision tuning. Hopper also introduced NVLink 4.0 at 900 GB/s per GPU and the NVLink Switch, enabling NVLink connectivity beyond the 8-GPU boundary.

The H100's `{python} h100_tflops` TFLOPS of FP16 Tensor Core throughput at 700 W represents a 6.8$\times$ efficiency improvement over V100, achieved through a combination of process technology (TSMC 4N), architectural innovation (Transformer Engine), and precision engineering (FP8 support).

The **Blackwell** architecture (2024) continued this trajectory with the B200, which pairs two GPU dies in a single package via a high-bandwidth chip-to-chip link, effectively creating a "dual-die GPU" that delivers 4,500 TFLOPS of FP16 throughput at 1,000 W.

The dual-die approach acknowledges that single-die GPU sizes are approaching the reticle limit of lithographic equipment (roughly 800 mm$^2$), and further scaling requires chiplet-based designs. The reticle limit is a fundamental constraint of photolithography: the lens system in the EUV scanner can only project a pattern onto an area of approximately 26 mm$\times$ 33 mm (858 mm$^2$) in a single exposure. A die larger than this area would require multiple exposures with stitching, which is technically possible but dramatically increases cost and reduces yield.

Blackwell also introduced fifth-generation NVLink at 1,800 GB/s per GPU, doubling the intra-node bandwidth again. The die-to-die link within the B200 package operates at 10 TB/s, fast enough that the two dies appear as a single logical GPU to the software, with no performance penalty for operations that span the die boundary.

This progression reveals a consistent pattern: each generation addresses the bottleneck exposed by the previous generation. Volta solved the compute bottleneck for matrix multiplication. Ampere solved the precision bottleneck for mixed-precision training. Hopper solved the Transformer-specific precision bottleneck with FP8. Blackwell is pushing against the die-size limit with multi-die packaging.

At each step, the accelerator's design reflects the dominant workload of its era, illustrating how the physics of the workload drives the physics of the silicon. This co-evolution between workloads and hardware is not accidental: GPU architects profile production ML workloads to identify the next bottleneck, and then design the next generation to address it. The result is a hardware evolution that is tightly coupled to the model architecture evolution, which is why the Transformer's dominance since 2017 has so profoundly shaped the trajectory of accelerator design.

The evolution also reveals a sobering trend for infrastructure planners: the useful lifetime of each generation is shrinking. The V100 remained the state of the art for approximately three years (2017--2020). The A100 held that position for roughly two years (2020--2022). The H100's reign is similarly expected to last two years before being superseded by Blackwell for new deployments.

This accelerating cadence means that the hardware a team deploys today will be two generations behind within three to four years, at which point the electricity cost of operating it may exceed the cost of replacing it. Hardware refresh planning is therefore not a future concern but an integral part of the initial procurement decision.

A subtlety that affects fleet consistency is the **silicon lottery** -- the manufacturing reality where microscopic variance in 4nm lithography produces a distribution of chip quality across each wafer. NVIDIA manages this yield curve through aggressive binning: dies capable of sustaining high clock frequencies at strictly controlled voltages under the full TDP are designated as premium H100 SXM modules, while those with higher leakage currents or minor defects become H100 PCIe cards or are fused down to lower-tier products. Even within the top-tier SXM bin, the achievable boost clock varies based on silicon characteristics. In a synchronous training cluster, the collective communication primitives are blocked by the slowest participant. A single chip running 50 MHz below the fleet average can degrade the effective throughput of the entire cluster by 5--10%, which is why sophisticated fleet management systems track per-GPU performance metrics and quarantine underperforming silicon to inference pools where the impact of individual chip variance is less pronounced.

| **Architecture**     | **Year** | **Key Innovation**      | **FP16 Tensor** | **TDP** | **NVLink BW** |
|:---------------------|:--------:|:------------------------|:---------------:|:-------:|:-------------:|
| **Volta** (V100)     |   2017   | First Tensor Core       |    125 TFLOPS   |  300 W  |    300 GB/s   |
| **Ampere** (A100)    |   2020   | Multi-precision, MIG    |    312 TFLOPS   |  400 W  |    600 GB/s   |
| **Hopper** (H100)    |   2022   | Transformer Engine, FP8 |   1,979 TFLOPS  |  700 W  |    900 GB/s   |
| **Blackwell** (B200) |   2024   | Dual-die, NVLink 5      |   4,500 TFLOPS  | 1,000 W |   1,800 GB/s  |

: **NVIDIA GPU Architecture Evolution**. Each generation approximately doubles efficiency (TFLOPS/W) while introducing architectural features targeted at the dominant ML workload pattern of its era. The NVLink bandwidth doubles each generation, tracking the growth in model sizes that require increasingly aggressive tensor parallelism. {#tbl-gpu-evolution}

@tbl-gpu-evolution compresses four hardware generations into a few columns. @fig-accelerator-efficiency-wall unpacks two of those columns, raw throughput and power efficiency, to reveal a divergence that shapes every infrastructure decision in this volume.

::: {#fig-accelerator-efficiency-wall fig-env="figure" fig-pos="htb" fig-cap="**The Accelerator Efficiency Wall**. FP16 throughput (blue, left axis) and power efficiency (green, right axis) for six generations of NVIDIA datacenter GPUs, both on logarithmic scales. Raw throughput has grown 236$\\times$ from the P100 to the B200, but efficiency (TFLOPS per watt) has grown only 70$\\times$ over the same period. The shaded region highlights the widening gap that the power grid, cooling systems, and datacenter infrastructure must absorb." fig-alt="Dual-axis log-scale plot showing GPU FP16 TFLOPS and TFLOPS per watt from 2016 to 2024 across six NVIDIA GPU generations."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ACCELERATOR EFFICIENCY WALL (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-accelerator-efficiency-wall — throughput vs efficiency
# │
# │ Goal: Dual-axis plot: FP16 TFLOPS (236×) vs TFLOPS/Watt (70×) P100→B200;
# │       show widening gap.
# │ Show: Semilogy; left/right axes; shaded divergence; GPU labels.
# │ How: Verified tflops, tdp_w, eff; viz.setup_plot().
# │
# │ Imports: numpy (np), mlsys.viz (viz)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
from mlsys import viz

fig, ax1, COLORS, plt = viz.setup_plot(figsize=(8, 5))

# --- Verified data (NVIDIA datacenter GPUs, official datasheets) ---
gpus    = ["P100\nSXM2", "V100\nSXM2", "A100\nSXM4", "H100\nSXM5", "B100", "B200"]
years   = [2016,          2017,          2020,          2022,          2024,   2024  ]
tflops  = [21.2,          125,           312,           989,           1750,   5000  ]
tdp_w   = [300,           300,           400,           700,           700,    1000  ]
eff     = [t / w for t, w in zip(tflops, tdp_w)]  # TFLOPS/W

years_arr = np.array(years, dtype=float)

# --- Left axis: FP16 TFLOPS ---
ax1.semilogy(years_arr, tflops, "o-", color=COLORS["BlueLine"], markersize=8,
             linewidth=2, label="FP16 TFLOPS", zorder=5)
ax1.set_ylabel("FP16 TFLOPS (log scale)", color=COLORS["BlueLine"])
ax1.tick_params(axis="y", labelcolor=COLORS["BlueLine"])
ax1.set_ylim(10, 10000)

# --- Right axis: TFLOPS/Watt ---
ax2 = ax1.twinx()
ax2.semilogy(years_arr, eff, "s-", color=COLORS["GreenLine"], markersize=8,
             linewidth=2, label="TFLOPS / Watt", zorder=5)
ax2.set_ylabel("TFLOPS / Watt (log scale)", color=COLORS["GreenLine"])
ax2.tick_params(axis="y", labelcolor=COLORS["GreenLine"])
ax2.set_ylim(0.01, 10)
ax2.spines["right"].set_visible(True)
ax2.spines["right"].set_color(COLORS["GreenLine"])

# --- Shade the divergence gap ---
ax1.fill_between(years_arr, tflops, [e * tflops[0] / eff[0] for e in eff],
                 alpha=0.08, color=COLORS["OrangeLine"])

# --- Label each GPU ---
for i, g in enumerate(gpus):
    ax1.annotate(g, (years[i], tflops[i]), textcoords="offset points",
                 xytext=(10, -5), fontsize=7.5, color=COLORS["BlueLine"])

# --- Growth annotations ---
ax1.annotate("236$\\times$ raw TFLOPS",
             xy=(2023.2, 3500), fontsize=9, color=COLORS["BlueLine"],
             fontstyle="italic",
             bbox=dict(boxstyle="round,pad=0.3", fc="white", ec=COLORS["BlueLine"], alpha=0.7))
ax2.annotate("70$\\times$ efficiency",
             xy=(2022.7, 0.035), fontsize=9, color=COLORS["GreenLine"],
             fontstyle="italic",
             bbox=dict(boxstyle="round,pad=0.3", fc="white", ec=COLORS["GreenLine"], alpha=0.7))

ax1.set_xlabel("Release Year")
ax1.set_xlim(2015, 2025)

# --- Combined legend ---
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc="upper left",
           frameon=True, fancybox=True, framealpha=0.9)
ax1.set_title("")
plt.show()
```
:::

@fig-accelerator-efficiency-wall quantifies a critical asymmetry: while each GPU generation delivers dramatically more raw throughput, the power required to sustain that throughput grows nearly as fast. The 236$\times$ increase in FP16 TFLOPS from P100 to B200 is accompanied by only a 70$\times$ increase in TFLOPS per watt, which means that the datacenter's power grid and cooling infrastructure must absorb the difference. This divergence is the physical root of the Power Wall discussed in the next sections and explains why liquid cooling, megawatt-scale power delivery, and thermal management dominate modern datacenter design.

#### Evolution of TPU Architectures {#sec-compute-tpu-evolution}

\index{TPU}
\index{Google Cloud TPU}

Google's TPU trajectory follows a different path, focusing on **distributed efficiency** and **XLA compiler integration**. While GPUs emphasize peak per-chip throughput and software flexibility (CUDA), the TPU is designed from the beginning as a "Pod-scale" resource. Every TPU chip is born as part of a larger cluster, with dedicated inter-chip interconnect (ICI) links that bypass the host CPU entirely.

The **TPU v1** (2015) was a dedicated inference chip with a 256$\times$ 256 systolic array, delivering 92 TOPS (INT8) for the matrix operations that dominated Google's production inference workloads.

The **TPU v2** (2017) and **TPU v3** (2018) transitioned to training, introducing bfloat16 (BF16) precision to eliminate the dynamic range issues of standard FP16. These generations pioneered the TPU Pod, with v3 pods scaling to 1,024 chips and 100+ PFLOPS of aggregate compute.

The **TPU v4** (2021) introduced an optical circuit switch (OCS) in the pod-scale network, allowing the physical topology of the pod to be reconfigured for different workloads. This generation also brought significant per-chip improvements, delivering 275 TFLOPS of BF16 compute.

The **TPU v5p** (2023) is the current state-of-the-art for large-scale training. It features 459 TFLOPS of BF16 compute and 95 GB of HBM, but its true advantage lies in its interconnect: 1,600 GB/s of ICI bandwidth, optimized for the massive AllReduce operations required by billion-parameter models.

| **Generation** | **Year** | **Key Innovation**         | **Peak BF16** | **HBM** | **ICI BW** |
|:---------------|:--------:|:---------------------------|:-------------:|:-------:|:----------:|
| **TPU v1**     |   2015   | Systolic Array (Inference) |    92 TOPS*   |   ---   |    ---     |
| **TPU v2**     |   2017   | High-Bandwidth Memory      |   45 TFLOPS   |  16 GB  |  600 GB/s  |
| **TPU v3**     |   2018   | Liquid Cooling, Pod Scale  |   105 TFLOPS  |  32 GB  |  650 GB/s  |
| **TPU v4**     |   2021   | Optical Circuit Switching  |   275 TFLOPS  |  32 GB  | 1,200 GB/s |
| **TPU v5p**    |   2023   | SparseCore, HBM3           |   459 TFLOPS  |  95 GB  | 1,600 GB/s |

: **Google TPU Architecture Evolution**. The TPU's development highlights a "Pod-first" design philosophy where inter-chip interconnect bandwidth and compiler-level optimization (XLA) are as important as peak per-chip arithmetic. *Note: TPU v1 was INT8 only.* {#tbl-tpu-evolution}

This comparison reveals a divergence in philosophy. GPU evolution is a race to pack more arithmetic and higher-precision Tensor Cores onto a single package, using chiplets (Blackwell) to overcome physical die-size limits. TPU evolution is a race to build more efficient warehouse-scale computers, where the individual chip's performance is secondary to the pod's collective bandwidth and reconfigurability.

The accelerator's arithmetic engine is now extraordinarily powerful -- capable of nearly 2,000 TFLOPS on the H100. But raw arithmetic throughput is meaningless if the data cannot reach the compute units fast enough. The next section examines the fundamental bottleneck that limits every accelerator's effective performance: the Memory Wall.

## The Memory Wall {#sec-compute-memory-wall}

\index{Memory Wall}

With the accelerator's arithmetic engine selected, we confront a paradox: the faster we make our logic, the more it idles waiting for data. This diverging trajectory between processor throughput and data access speed is formally known as the **Memory Wall**. While transistor scaling has driven logic performance up by orders of magnitude, the physical interconnects that feed data to these cores have failed to keep pace. This bottleneck is existential for machine learning: unlike traditional software that benefits heavily from caching and data reuse, neural networks must stream billions of weights from memory for every inference pass, often making bandwidth -- not compute -- the governing constraint on performance.

The implications are concrete and perceptible in our running example. Our `{python} gpt3_params_b`B model's weights occupy 350 GB in FP16. During autoregressive decoding, this entire 350 GB tensor must be streamed from off-chip memory into the processor's registers for *every single token* generated. Even at state-of-the-art HBM bandwidths (~3.35 TB/s on an H100), this data movement alone dictates a latency floor of over 100 ms per token. The Memory Wall is not an abstract architectural concept -- it is the physical reason your chatbot takes a perceptible pause between words. To navigate this constraint, we turn to three critical engineering responses: **High Bandwidth Memory (HBM)** to widen the data pipe, the **Roofline Model** to rigorously diagnose whether a workload is starving for data or for compute, and **Tensor Cores** to maximize the arithmetic value of every byte fetched.

### HBM: Breaking the Memory Wall {#sec-compute-hbm}

\index{HBM}

With the accelerator's arithmetic engine selected, we confront a paradox. We can pack thousands of multipliers onto a single die, but if we cannot feed them with data fast enough, most of those multipliers sit idle on every clock cycle. Consider our `{python} gpt3_params_b`B model: its weights alone occupy 350 GB in FP16 precision. During the decode phase of text generation, the processor must stream the *entire* weight tensor through the arithmetic units for every single output token. The bottleneck is not the speed of multiplication; it is the speed of *delivery*. This is the Memory Wall in action: the processor is starved for data, not for arithmetic capability. No amount of additional Tensor Cores can help, because the existing ones are already idle for most of each cycle, waiting for the next tile of data to arrive from memory. The technology that defines the modern accelerator's response to this fundamental limitation is High Bandwidth Memory[^fn-hbm-origin].

The memory hierarchy within a single accelerator spans orders of magnitude in both capacity and bandwidth. At the top sits the **register file** -- approximately 20--30 MB distributed across all SMs -- with effectively infinite bandwidth (hundreds of TB/s) but minuscule capacity. Below this lies the L1 cache and **shared memory** (SRAM), offering roughly 256 KB per SM (approximately 33 MB total) with an aggregate bandwidth of ~19 TB/s. Further down is the 50 MB L2 cache (~12 TB/s), and finally the 80 GB of HBM3 at 3.35 TB/s. The bandwidth gap between registers and HBM is approximately 1,000$\times$. If an operand must be fetched from HBM for a single operation, the arithmetic unit spends 99.9% of its time stalling. High Model FLOPS Utilization (MFU) is only possible through aggressive **tiling**: breaking the massive weight matrices into small tiles that fit entirely within shared memory and registers, then performing as many multiply-accumulate operations on each tile as possible before evicting it. Despite the `{python} gpt3_params_b`B model's massive total memory footprint, the active working set at any given microsecond must be meticulously managed to reside in that top 30 MB of register space, or the chip's theoretical performance becomes a mirage.

[^fn-hbm-origin]: **HBM (High Bandwidth Memory)**: Standardized by JEDEC in 2013 as a joint development between AMD and SK Hynix, originally for graphics cards. ML accelerators adopted HBM because neural networks exhibit the same bandwidth-hungry, capacity-moderate access pattern as high-end rendering. Each HBM generation has roughly doubled bandwidth (128 GB/s in HBM1 to 1.2 TB/s per stack in HBM3e), yet the gap between memory bandwidth and arithmetic throughput continues to widen -- making HBM a necessary but never sufficient response to the Memory Wall. \index{HBM!origin}

::: {.callout-definition title="High Bandwidth Memory (HBM)"}

***High Bandwidth Memory (HBM)***\index{High Bandwidth Memory!definition} is a specialized DRAM architecture that uses 3D-stacked memory dies connected directly to the processor via an interposer.

1.  **Significance (Quantitative):** By minimizing the physical distance data travels, HBM provides the **Memory Bandwidth ($BW$)** required to feed massive arithmetic units ($R_{peak}$), achieving $10\times$ to $100\times$ higher throughput than conventional DDR memory.
2.  **Distinction (Durable):** Unlike **DDR Memory**, which connects through long PCB traces, HBM is integrated into the same package as the accelerator, trading **Capacity** for **Bandwidth and Energy Efficiency** ($\eta$).
3.  **Common Pitfall:** A frequent misconception is that HBM "solves" the memory wall. In reality, it only **Moves the Wall**: while bandwidth is higher, the **Arithmetic Intensity** required to saturate modern GPUs continues to outpace even HBM scaling.

:::

Traditional DDR memory connects to the processor through pins on the edge of a printed circuit board (PCB). Each DIMM communicates over a 64-bit bus, and even with multiple channels (8 channels is typical for a high-end server CPU), a modern server tops out at roughly `{python} ddr_bw_str` GB/s of aggregate memory bandwidth.

The physical distance from the DIMM slot to the processor die is measured in centimeters, and every centimeter of copper trace introduces capacitance, signal attenuation, and energy loss. At DDR5 data rates (4,800--6,400 MT/s per pin), the signal conditioning circuits must compensate for significant channel impairment, consuming substantial power per bit transferred. Increasing the data rate on these long traces requires progressively more power for signal conditioning, creating a diminishing-returns curve that DDR5 is already approaching.

HBM solves this problem by changing the physical topology entirely. Instead of routing signals horizontally across a PCB, HBM stacks multiple DRAM dies *vertically*, one on top of another, and connects them with **Through-Silicon Vias (TSVs)**[^fn-tsv-stacking]: microscopic copper pillars etched through the silicon substrate itself. This vertical stacking is a paradigm shift in memory architecture: rather than increasing bandwidth by pushing signals faster through long copper traces (the DDR approach, which has diminishing returns), HBM increases bandwidth by multiplying the number of parallel signal paths through extremely short vertical connections.

A single HBM stack contains 8--12 DRAM dies. Each die is thinned to approximately 30--50 micrometers (roughly the thickness of a human hair) using a chemical-mechanical polishing process. This thinning is necessary because the TSVs must pass through the full thickness of each die, and thinner dies allow shorter, lower-resistance vias. The thinned dies are then vertically aligned with sub-micrometer precision and bonded to the die below using thermocompression bonding at temperatures of 300--400 degrees Celsius.

Thousands of TSVs pass through each die, providing a 1024-bit-wide interface per stack, compared to 64 bits for a DDR5 channel. This 16$\times$ wider interface, combined with higher per-pin signaling rates, is the source of HBM's bandwidth advantage. Each TSV is approximately 5--10 micrometers in diameter (invisible to the naked eye), and the pitch between adjacent TSVs is 40--50 micrometers, enabling thousands to fit within the area of a single HBM die.

Because the vias travel *through* the silicon rather than *across* a PCB, the signal path is measured in tens of micrometers rather than centimeters. This represents approximately a 1000$\times$ reduction in physical distance compared to DDR5 signal paths.

The shortened distance has three direct physical benefits. First, the energy per bit transferred drops by an order of magnitude (from approximately 20 pJ/bit for DDR5 to approximately 2 pJ/bit for HBM) because shorter traces have lower capacitance and require less driving current. Second, the latency decreases because electrical signals propagate through silicon at roughly two thirds the speed of light, and the shorter path means proportionally shorter propagation time. Third, the reduced capacitance allows higher signaling frequencies without the sophisticated equalization circuits required by long PCB traces, enabling the high per-pin data rates that complement the wide bus width.

[^fn-tsv-stacking]: **TSV (Through-Silicon Via)**: A vertical copper pillar, 5--10 micrometers in diameter, etched through a silicon die to connect stacked layers. Originally developed for CMOS image sensors in smartphone cameras, TSVs enabled HBM by replacing centimeters of PCB trace with tens of micrometers of vertical silicon -- a 1,000$\times$ reduction in signal path that drops energy per bit from ~20 pJ (DDR5) to ~2 pJ (HBM), making terabyte-per-second bandwidth economically feasible within an accelerator's power budget. \index{TSV!3D stacking}

The entire HBM assembly sits on the same silicon interposer as the processor die, connected via microbumps. This on-package placement means data travels from DRAM cell to arithmetic unit in nanoseconds rather than the tens of nanoseconds required by off-package DDR.

The interposer itself is a passive silicon substrate with etched wiring layers that connect the HBM stacks to the processor, forming what is effectively a miniature PCB made of silicon rather than fiberglass. The interposer's silicon construction allows much finer trace widths and tighter pitches than a fiberglass PCB, enabling thousands of parallel connections between the HBM stacks and the processor die in a physical area of just a few hundred square millimeters.

| **Metric**          |     **Host DRAM** (DDR5)    | **Accelerator HBM** (HBM3e)  |     **Scaling Factor**    |
|:--------------------|:---------------------------:|:----------------------------:|:-------------------------:|
| **Mechanism**       |        2D PCB Traces        |     **3D Die Stacking**      |             -             |
| **Placement**       |        Socketed DIMMs       |  **On-package (Substrate)**  |   **Physical Proximity**  |
| **Bandwidth**       | ~`{python} ddr_bw_str` GB/s | **~`{python} h100_bw` TB/s** |   **~50$\times$ Faster**  |
| **Interface Width** |            64-bit           |    **1024-bit per stack**    |    **16$\times$ Wider**   |
| **Energy**          |          ~20 pJ/bit         |        **~2 pJ/bit**         | **10$\times$ Efficiency** |

: **HBM vs. Standard DRAM Comparison**. HBM achieves its bandwidth advantage through three simultaneous innovations: 3D die stacking (more bits per package), TSV interconnects (shorter signal paths), and on-package placement (proximity to the processor). {#tbl-hbm-comparison}

This performance comes at a price. HBM costs approximately \$10--15 per GB, compared to roughly \$3 per GB for DDR5 server memory. For an H100 with 80 GB of HBM3, the memory alone represents approximately \$800--1,200 of the accelerator's manufacturing cost. For a B200 with 192 GB of HBM3e, the memory cost rises to \$1,920--2,880 per accelerator, making HBM one of the most expensive components in the system.

The advanced packaging process, which requires precise alignment of thousands of TSVs across multiple die layers, has lower manufacturing yields and higher complexity. Each step in the stacking process (die thinning, alignment, bonding, and TSV etching) can introduce defects. The cumulative yield across 12 stacking steps means that the overall yield for a complete HBM3e stack is substantially lower than the yield for a single DRAM die. The silicon interposer itself must be large enough to accommodate both the processor die and multiple HBM stacks (often exceeding 1,000 mm$^2$ in total area), and any defect in a TSV can render an entire stack unusable.

The supply chain dynamics of HBM production further affect its cost and availability. The HBM supply chain is highly concentrated among a small number of manufacturers, and the production processes (die thinning, TSV etching, die-to-die bonding) require specialized capital equipment that is fundamentally different from standard DRAM manufacturing. Expanding HBM production capacity requires 12--18 months of equipment procurement and qualification, which means that production cannot rapidly scale in response to demand surges. When demand outpaces supply, as it did following the explosion of interest in large language models in 2023, lead times stretch to 12--18 months and prices can double.

For infrastructure planners, this supply chain concentration means that HBM availability, not just its specifications, can determine the timeline for building a training cluster. Organizations planning large deployments must secure HBM allocations 12--18 months in advance, committing capital before the rest of the system is designed. This procurement lead time is longer than for any other component in the stack, making HBM the pacing element for fleet expansion.

For our `{python} gpt3_params_b`B model, the HBM alone in a cluster of 1,000 accelerators might represent \$50--80 million in memory cost. This cost-capacity trade-off explains why accelerators typically offer 80--192 GB of HBM while the host server provides 512 GB to 2 TB of DDR: the *fast* memory holds the active computation (weights, activations, gradients that are accessed every cycle), and the *cheap* memory holds everything else (optimizer states, checkpoint buffers, data loading queues).

The boundary between what resides in HBM and what resides in DDR is a critical design parameter for training frameworks, and managing this boundary efficiently is one of the key challenges addressed by ZeRO optimization and offloading strategies (@sec-distributed-training-systems). Getting this boundary wrong in either direction is costly: placing too much data in HBM wastes expensive capacity, while placing too much in DDR creates bandwidth stalls that idle the arithmetic units.

#### HBM Generations and the Scaling Frontier

The evolution of HBM tracks the growth of model sizes with remarkable correspondence. Each generation increases the number of stacked dies, the signaling rate per pin, and the total capacity per stack, driven by the relentless growth in model parameters.

| **Metric**           | **HBM2e (A100)** | **HBM3 (H100)** | **HBM3e (B200)** | **HBM4 (Future)** |
|:---------------------|:----------------:|:---------------:|:----------------:|:-----------------:|
| **Peak Bandwidth**   |    ~2.0 TB/s     |    ~3.3 TB/s    |  **>4.8 TB/s**   |   **>6.0 TB/s**   |
| **Typical Capacity** |    40--80 GB     |    80--96 GB    |    **192 GB**    |    **288 GB+**    |
| **Interface Width**  |     1024-bit     |     1024-bit    |     1024-bit     |    **2048-bit**   |
| **Stack Height**     |      8 dies      |    8--12 dies   |     12 dies      |      16 dies      |

: **Evolution of High Bandwidth Memory**. Each generation roughly doubles bandwidth, tracking the doubling of frontier model sizes every 12--18 months. The jump to HBM4 doubles the interface width for the first time since HBM's introduction, signaling that pin-rate increases alone can no longer sustain the required bandwidth growth. {#tbl-hbm-evolution}

The transition from HBM3 to HBM3e is particularly significant for our running example. An A100 with 80 GB of HBM2e can hold only 23% of our `{python} gpt3_params_b`B model's weights (at FP16). An H100 with 80 GB of HBM3 can hold the same fraction but deliver the data 65% faster. A B200 with 192 GB of HBM3e can hold 55% of the weights and deliver them at over 4.8 TB/s. Neither can hold the full model, which is precisely why we need multiple accelerators in a node, a topic we address in @sec-compute-node.

However, the capacity story changes significantly when quantization is applied. The same `{python} gpt3_params_b`B model quantized to INT8 requires only 175 GB, fitting in 3 H100 GPUs or a single B200. Quantized to INT4, it requires only 87.5 GB, fitting in a single H100. The capacity constraints that drive the need for multi-accelerator nodes for training (where FP16 or BF16 precision is typically required) are substantially relaxed for inference (where INT8 or INT4 quantization is often acceptable). This is another reason why training and inference infrastructure have different optimal configurations.

The bandwidth improvement matters independently of capacity. Each generation of HBM produces a nearly proportional reduction in per-token latency during autoregressive inference. A 70B model on an A100 (2.0 TB/s HBM2e bandwidth) generates tokens at roughly 70 GB / 2.0 TB/s = 35 ms per token. The same model on an H100 (3.35 TB/s HBM3) generates tokens at 70 GB / 3.35 TB/s = 20.9 ms per token, a 1.67$\times$ improvement. On a B200 (4.8 TB/s HBM3e), latency drops further to 70 GB / 4.8 TB/s = 14.6 ms per token. For interactive applications (chatbots, code assistants, real-time translation), where users perceive delays above 50 ms as "slow," these bandwidth improvements translate directly into better user experience and into the ability to serve larger models within latency budgets.

The forward-looking jump to HBM4 doubles the interface width from 1024 bits to 2048 bits for the first time since HBM's introduction. This signals that per-pin signaling rate increases alone can no longer sustain the required bandwidth growth, and the industry must widen the bus. The doubling of interface width requires a correspondingly larger interposer area for routing, which is one reason that next-generation accelerator packages are expected to grow even larger.

HBM4's projected 6+ TB/s bandwidth will be essential for the next generation of frontier models, which are expected to exceed 1 trillion parameters and require over 2 TB of weight storage in FP16. At current HBM3e bandwidths (4.8 TB/s), serving such a model at batch size 1 would produce tokens at 2,000 GB / 4.8 TB/s = 417 ms per token, far too slow for interactive applications. HBM4 at 6 TB/s would reduce this to 333 ms, still slow, which suggests that trillion-parameter models will require either aggressive quantization or multi-accelerator tensor parallelism even for single-request inference. The co-evolution of model scale and memory technology continues to drive infrastructure requirements.

An important architectural consideration is the number of HBM stacks per accelerator and how they connect to the processor. The H100 has 5 HBM3 stacks, each providing approximately 670 GB/s, for a total of 3.35 TB/s aggregate bandwidth. The B200 has 8 HBM3e stacks per die (16 total for the dual-die package), each providing approximately 600 GB/s, for an aggregate exceeding 8 TB/s. The number of stacks is constrained by the interposer area available for HBM placement: each HBM stack occupies approximately 100 mm$^2$ of interposer area, and the total interposer must accommodate both the processor die(s) and all HBM stacks. Larger interposers allow more HBM stacks (and therefore higher bandwidth and capacity) but are more expensive and harder to manufacture.

This interposer area constraint creates an interesting design tension. Making the processor die larger (more Tensor Cores, more SMs) leaves less interposer area for HBM stacks, potentially reducing bandwidth. Making the processor die smaller (fewer Tensor Cores) frees interposer area for more HBM but reduces peak compute throughput. The optimal balance depends on the target workload's position on the Roofline plot: compute-bound workloads benefit from a larger processor die (more Tensor Cores), while bandwidth-bound workloads benefit from more HBM stacks.

#### Memory Bandwidth and Token Latency

The distinction between memory *capacity* (how many gigabytes the HBM can store) and memory *bandwidth* (how many terabytes per second it can deliver) is one of the most practically important concepts in ML infrastructure. Capacity determines whether a model's weights *fit*. Bandwidth determines how fast the model can *run*. For autoregressive text generation, where each token requires a full pass through the model's weights, bandwidth is almost always the binding constraint.

::: {#nb-token-latency .callout-notebook title="The Physics of Token Latency"}

**Scenario**: Generating one token for Llama-3 70B (140 GB weights in FP16) on an NVIDIA H100.

1. **Compute Time**: Each token requires $2 \times 70\text{B} = 1.4 \times 10^{11}$ floating-point operations. At `{python} h100_tflops` TFLOPS peak throughput, the arithmetic takes:
   `{python} t_comp_math`
2. **Memory Time**: Loading 140 GB of weights from HBM at 3.35 TB/s takes:
   `{python} t_mem_math`

**Conclusion**: The processor spends **`{python} dominance_str`%** of its time waiting for data from memory. The arithmetic units are idle for almost the entire token generation. Even a hypothetical processor with *infinite* compute throughput would generate tokens only negligibly faster, because the memory transfer time dominates completely. This is why HBM bandwidth improvements deliver nearly linear speedups for inference workloads.

:::

This napkin math reveals a profound asymmetry at the heart of modern ML infrastructure. The accelerator vendors invest billions of dollars in designing faster arithmetic units (more Tensor Cores, higher clock speeds, wider datapaths), yet for single-request inference, the arithmetic completes in a fraction of a millisecond while the memory transfer takes tens of milliseconds. The arithmetic units are over 500$\times$ faster than the memory system for this workload, meaning that over 99% of the silicon dedicated to computation is idle during inference.

This asymmetry is the single most important physical fact about ML inference, and it shapes every architectural and economic decision about serving infrastructure.

The ratio between memory time and compute time, `{python} dominance_str`% in this example, is called the **memory-boundedness** of the workload. A workload that is 99% memory-bound will see almost no benefit from a faster processor (more TFLOPS) but will see nearly linear speedup from faster memory (more TB/s bandwidth).

This is a quantitative expression of the Memory Wall: the gap between processing speed and memory speed that has been widening for decades and is particularly acute for ML inference workloads. The Memory Wall was first identified by Wulf and McKee in 1995, who observed that processor speed was improving at 60% per year while DRAM speed improved at only 7% per year. This growing disparity meant that processors would increasingly spend their time waiting for data rather than computing on it. Thirty years later, their prediction has proven remarkably accurate, and the ML inference workload is perhaps the most extreme manifestation of the memory wall in modern computing.

The practical implication is that accelerator selection for inference workloads should prioritize bandwidth-per-dollar over FLOPS-per-dollar. An older-generation GPU with high memory bandwidth but moderate compute throughput may deliver better inference performance per dollar than a cutting-edge GPU with extreme compute but insufficient bandwidth improvement.

@fig-compute-memory-divergence makes this divergence visible across four GPU generations. While compute throughput has grown 36$\times$ from V100 to B200, memory bandwidth has grown only 9$\times$ over the same period. The widening gap between these two curves *is* the Memory Wall: each generation of accelerator becomes more powerful in arithmetic but proportionally more starved for data.

::: {#fig-compute-memory-divergence fig-env="figure" fig-pos="htb" fig-cap="**The Compute-Memory Divergence**. GPU compute throughput (FP16 Tensor TFLOPS) has grown approximately 36x from V100 to B200, while HBM bandwidth has grown only 9x over the same period. The widening gap between these curves is the Memory Wall: each generation of accelerator becomes proportionally more starved for data." fig-alt="Dual-axis log-scale plot showing GPU compute growth at 36x versus memory bandwidth growth at 9x from 2017 to 2024, with diverging slopes illustrating the Memory Wall."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ COMPUTE-MEMORY DIVERGENCE (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-compute-memory-divergence — Memory Wall quantification
# │
# │ Goal: Dual-axis plot: compute (36×) vs memory BW (9×) V100→B200; show
# │       diverging slopes.
# │ Show: Log-scale; two curves; growth annotations.
# │ How: gpus, years, tflops, bandwidth; twinx; matplotlib.
# │
# │ Imports: matplotlib.pyplot (plt)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt

gpus = ['V100', 'A100', 'H100', 'B200']
years = [2017, 2020, 2022, 2024]
tflops = [125, 312, 1979, 4500]
bandwidth = [900, 2039, 3350, 8000]

fig, ax1 = plt.subplots(figsize=(8, 5))

color_compute = 'tab:blue'
ax1.set_xlabel('Release Year', fontsize=10, fontweight='bold')
ax1.set_ylabel('FP16 Tensor (TFLOPS)', color=color_compute, fontsize=10, fontweight='bold')
l1 = ax1.plot(years, tflops, color=color_compute, marker='o', markersize=8,
              linewidth=2.5, label='Compute (TFLOPS)')
ax1.tick_params(axis='y', labelcolor=color_compute)
ax1.set_yscale('log')
ax1.set_ylim(80, 10000)

ax2 = ax1.twinx()
color_mem = 'tab:red'
ax2.set_ylabel('Memory Bandwidth (GB/s)', color=color_mem, fontsize=10, fontweight='bold')
l2 = ax2.plot(years, bandwidth, color=color_mem, marker='s', markersize=8,
              linewidth=2.5, linestyle='--', label='Memory BW (GB/s)')
ax2.tick_params(axis='y', labelcolor=color_mem)
ax2.set_yscale('log')
ax2.set_ylim(500, 15000)

ax1.set_xticks(years)
ax1.set_xticklabels([f"{y}\n({g})" for y, g in zip(years, gpus)], fontsize=10)

ax1.grid(True, which="major", ls="-", alpha=0.3)
ax1.grid(True, which="minor", ls=":", alpha=0.1)

lines = l1 + l2
labels = [l.get_label() for l in lines]
ax1.legend(lines, labels, loc='upper left', frameon=True, framealpha=0.9)

ax1.annotate(
    '~36x Compute Growth',
    xy=(2024, 4500), xycoords='data',
    xytext=(2021.5, 4500), textcoords='data',
    arrowprops=dict(arrowstyle="->", color=color_compute, lw=1.5),
    ha='right', va='center', fontsize=9, color=color_compute, fontweight='bold'
)

ax2.annotate(
    '~9x Bandwidth Growth',
    xy=(2024, 8000), xycoords='data',
    xytext=(2021.5, 12000), textcoords='data',
    arrowprops=dict(arrowstyle="->", color=color_mem, lw=1.5),
    ha='right', va='center', fontsize=9, color=color_mem, fontweight='bold'
)

plt.tight_layout()
plt.show()
```
:::

For training workloads, where large batch sizes increase arithmetic intensity, the calculus shifts toward compute: peak TFLOPS per dollar becomes the relevant metric because the weight data loaded from HBM is amortized across many tokens in the batch. The Roofline Model, which we examine next, provides the formal framework for making this trade-off precise and for determining which metric matters for any given workload.

### Roofline Model {#sec-compute-roofline}

\index{Roofline Model}
\index{Arithmetic Intensity}

The token latency calculation above demonstrated that a 70B model on an H100 is overwhelmingly memory-bound. But how do we determine this systematically for *any* workload on *any* hardware? The **Roofline Model**[^fn-roofline-origin-ci], introduced by Williams, Waterman, and Patterson [@williams2009roofline], provides a visual and analytical framework that answers this question with a single number: the arithmetic intensity of the workload.

[^fn-roofline-origin-ci]: **Roofline Model**: Introduced by Williams, Waterman, and Patterson at UC Berkeley in 2009, the model plots attainable FLOPS against arithmetic intensity on a log-log scale, producing two intersecting lines whose crossover point -- the *ridge point* -- separates memory-bound from compute-bound regimes. For an H100 (989 TFLOPS FP16, 3.35 TB/s HBM), the ridge point is ~295 FLOP/byte; most LLM inference operators fall well below this threshold, which is why the Roofline remains the first diagnostic tool for identifying whether more compute or more bandwidth will improve performance. \index{Roofline Model!origin}

::: {.callout-definition title="Arithmetic Intensity"}

***Arithmetic Intensity ($I$)***\index{Arithmetic Intensity!definition} is the ratio of floating-point operations performed to the number of bytes transferred from memory ($FLOP/\text{byte}$).

1.  **Significance (Quantitative):** It characterizes the **Computational Density** of a workload. It is the independent variable in the **Roofline Model**, determining whether a system operates in the **Bandwidth-Bound** ($BW$) or **Compute-Bound** ($R_{peak}$) regime.
2.  **Distinction (Durable):** Unlike **Peak Throughput** (a hardware property), Arithmetic Intensity is an **Algorithmic Property** that measures how effectively a workload reuses data once it is loaded into the processor.
3.  **Common Pitfall:** A frequent misconception is that AI is fixed for a model. In reality, it varies by **Implementation**: techniques like operator fusion increase AI by keeping data in local registers, while increasing batch size increases AI for layers with high parameter reuse.

:::

The Roofline Model expresses the maximum achievable performance of a workload as the lesser of two ceilings:

$$ \text{Achievable FLOPS} = \min\left(\text{Peak Compute},\ \text{Memory BW} \times I\right) $$ {#eq-roofline}

This equation has an intuitive physical interpretation. If the workload's arithmetic intensity is low (it needs many bytes per operation), then performance is limited by how fast memory can deliver those bytes. The achievable FLOPS grows linearly with $I$, tracing a sloped line on a log-log plot. If the arithmetic intensity is high (each byte fuels many operations), then performance plateaus at the hardware's peak compute rate, regardless of further increases in $I$. This plateau is the flat "roof" of the model.

::: {.callout-definition title="Ridge Point"}

***Ridge Point***\index{Ridge Point!definition} is the specific arithmetic intensity where the memory bandwidth ceiling meets the compute ceiling ($I_{\text{ridge}} = \frac{R_{peak}}{BW}$).

1.  **Significance (Quantitative):** It defines the **Hardware Efficiency Threshold**. Workloads with an intensity below the ridge point are **Bandwidth-Bound** ($BW$), while those above are **Compute-Bound** ($R_{peak}$).
2.  **Distinction (Durable):** Unlike **Peak FLOPs** (which only describes the horizontal ceiling), the Ridge Point describes the **Balance** of the architecture. A rising ridge point over hardware generations indicates that compute is growing faster than bandwidth, making utilization harder.
3.  **Common Pitfall:** A frequent misconception is that all GPUs have the same ridge point. In reality, it varies by **Precision**: because $R_{peak}$ is higher for INT8 than FP32 while $BW$ is constant, the ridge point for INT8 is much higher, requiring more data reuse to saturate the hardware.

:::

Where do specific ML workloads fall on this plot? The answer depends on both the operation and the batch size:

- **LLM decode (batch size 1)**: Each token requires loading the full weight tensor (~2 bytes per parameter) for just 2 FLOPs per parameter, yielding $I \approx 1$ FLOP/byte. This is deep in the memory-bound region, well below the ridge point of `{python} h100_ridge`. Our token latency calculation confirmed this: the arithmetic finished in microseconds while the memory transfer took milliseconds.
- **LLM prefill (large context)**: Processing a long input sequence in parallel increases the FLOPs (matrix-matrix multiply instead of matrix-vector) without proportionally increasing memory traffic, pushing $I$ to 100--500 FLOP/byte. This crosses the ridge point and enters the compute-bound region.
- **CNN training (large batch)**: Convolution with large spatial dimensions and batch sizes achieves $I$ of 50--200 FLOP/byte, placing it near or above the ridge point for most accelerators.
- **Attention (long sequences)**: The self-attention mechanism scales quadratically with sequence length in FLOPs but linearly in memory traffic for the KV cache, making its arithmetic intensity sequence-length-dependent. Short sequences are memory-bound; long sequences are compute-bound.

Reading a Roofline plot requires understanding what each axis represents. The horizontal axis is arithmetic intensity (FLOP/byte), plotted on a log scale. The vertical axis is achievable performance (FLOP/s), also on a log scale. Two lines define the "roofline" shape: a diagonal line with slope 1 (on the log-log plot) representing the memory bandwidth limit, and a horizontal line representing the peak compute limit. These two lines meet at the ridge point.

Any workload can be plotted as a single point on this chart by computing its arithmetic intensity and measuring its achieved performance. If the point lies on the diagonal line, the workload is memory-bound and would benefit from faster memory. If it lies on the horizontal line, the workload is compute-bound and would benefit from more arithmetic units.

If the point lies below either line, the workload is not fully using the available resource. This gap indicates an optimization opportunity in the software: kernel inefficiency, poor memory access patterns, or excessive synchronization. Closing this gap is the province of kernel engineering and communication optimization, topics examined in @sec-distributed-training-systems.

The Roofline's diagnostic power extends beyond individual kernels to entire training runs. For our `{python} gpt3_params_b`B model, the computation graph contains thousands of distinct operations with different arithmetic intensities. The dense Feed-Forward Network (FFN) layers are dominated by large GEMMs with high arithmetic intensity, placing them firmly in the compute-bound regime where Tensor Core utilization is the bottleneck. Conversely, operations like layer normalization and element-wise activations possess very low arithmetic intensity, sitting deep in the memory-bound region where the compute units idle while waiting for data. The self-attention mechanism fluctuates between regimes depending on sequence length: while the quadratic complexity of attention scores suggests a compute bound, the loading of Key and Value matrices creates memory pressure at shorter sequences. This diagnostic distinction dictates the optimization strategy: memory-bound layers benefit from kernel fusion (reducing HBM round-trips), while compute-bound layers benefit from precision reduction (moving from FP16 to FP8, which effectively raises the hardware's compute ceiling).

@fig-roofline-landscape makes these relationships visible for the H100. Notice how LLM decode at batch size 1 sits deep in the memory-bound region, achieving less than 1% of peak compute, while LLM training at large batch sizes crosses the ridge point into the compute-bound regime. The 591$\times$ gap between these two workloads' arithmetic intensities explains why the same hardware that delivers excellent training throughput can appear woefully underutilized during inference.

::: {#fig-roofline-landscape fig-env="figure" fig-pos="htb" fig-cap="**The Roofline Landscape: NVIDIA H100**. The roofline is defined by two ceilings: memory bandwidth (diagonal) and peak compute (horizontal), meeting at the ridge point (~591 FLOP/byte). ML workloads span the full range: LLM decode at batch size 1 is deeply memory-bound, while LLM training at large batch sizes is compute-bound. The position of each workload determines whether faster memory or faster compute would improve performance." fig-alt="Log-log roofline plot for H100 showing memory-bound and compute-bound regions with five ML workloads plotted from LLM decode at 1 FLOP per byte to LLM training at 2000 FLOP per byte."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ROOFLINE LANDSCAPE (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-roofline-landscape — H100 roofline and workload placement
# │
# │ Goal: H100 roofline; plot LLM decode/CNN/LLM training workloads; show
# │       memory-bound vs compute-bound; ridge ~591 FLOP/byte.
# │ Show: Log-log; roofline; filled regions; workload scatter.
# │ How: y = min(BW*x, P_peak); workloads dict; matplotlib.
# │
# │ Imports: matplotlib.pyplot (plt), numpy (np)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt
import numpy as np

P_PEAK_TFLOPS = 1979.0
BW_TBS = 3.350
RIDGE_POINT = P_PEAK_TFLOPS / BW_TBS

workloads = {
    "LLM Decode\n(Batch=1)": 1.0,
    "LLM Decode\n(Batch=32)": 32.0,
    "CNN Training\n(ResNet-50)": 200.0,
    "LLM Prefill": 500.0,
    "LLM Training\n(175B)": 2000.0
}

x_min, x_max = 0.5, 10000
x = np.logspace(np.log10(x_min), np.log10(x_max), 500)
y = np.minimum(BW_TBS * x, P_PEAK_TFLOPS)

plt.figure(figsize=(10, 6))

plt.plot(x, y, color='#333333', linewidth=3, label='H100 Roofline')

plt.fill_between(x, 0, y, where=(x <= RIDGE_POINT),
                 color='#e6f3ff', alpha=0.5, label='Memory Bound')
plt.fill_between(x, 0, y, where=(x > RIDGE_POINT),
                 color='#fff0e6', alpha=0.5, label='Compute Bound')

plt.axvline(x=RIDGE_POINT, color='#666666', linestyle='--', linewidth=1.5, alpha=0.7)
plt.text(RIDGE_POINT * 1.1, 10, f'Ridge Point\n~{int(RIDGE_POINT)} FLOP/B',
         color='#666666', verticalalignment='bottom')

for label, intensity in workloads.items():
    perf = min(BW_TBS * intensity, P_PEAK_TFLOPS)
    if intensity < 10:
        xytext = (10, -20)
        ha = 'left'
    elif intensity > 1000:
        xytext = (-10, -30)
        ha = 'right'
    else:
        xytext = (0, 15)
        ha = 'center'
    plt.scatter(intensity, perf, color='#D62728', s=80, zorder=5, edgecolors='white')
    plt.annotate(label, (intensity, perf), xytext=xytext, textcoords='offset points',
                 ha=ha, fontsize=9, fontweight='bold',
                 bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="none", alpha=0.8))

plt.xscale('log')
plt.yscale('log')
plt.xlim(x_min, x_max)
plt.ylim(1, P_PEAK_TFLOPS * 2.5)
plt.xlabel('Arithmetic Intensity (FLOPs / Byte)', fontweight='bold')
plt.ylabel('Attainable Performance (TFLOPS)', fontweight='bold')
plt.grid(True, which="both", ls="-", alpha=0.15)
plt.grid(True, which="major", ls="-", alpha=0.3, color='#999999')
plt.text(0.6, 2.5, f'Bandwidth Slope\n{BW_TBS:.2f} TB/s', rotation=45, color='#006395', fontsize=10)
plt.text(3000, P_PEAK_TFLOPS * 1.1, f'Peak Compute\n{int(P_PEAK_TFLOPS)} TFLOPS', color='#006395', ha='right', fontsize=10)
plt.tight_layout()
plt.show()
```
:::

The Roofline Model also reveals a subtle but important insight about the interaction between batch size and hardware utilization. Increasing the batch size for a given model raises the arithmetic intensity because the weight matrix is loaded once but multiplied against a larger activation matrix (more FLOPs for the same bytes transferred). This shifts the workload point rightward on the plot, potentially crossing the ridge point from memory-bound to compute-bound territory. The arithmetic intensity grows linearly with batch size (doubling the batch size doubles the FLOPs while keeping the weight loading unchanged), creating a simple and predictable relationship between batch size and hardware utilization.

For inference serving, this means that batching multiple requests together can dramatically increase hardware utilization and throughput. A model that achieves 1% of peak FLOPS at batch size 1 might achieve 50% of peak FLOPS at batch size 64, simply because the weight data loaded from HBM is reused across 64 independent requests rather than one. This 50$\times$ improvement in hardware utilization comes at a cost: the 64 requests must wait until a full batch is assembled before processing begins, introducing queuing latency. Serving systems must carefully balance this trade-off between throughput (larger batches, higher utilization) and latency (smaller batches, faster response per request).

For training, the batch size is a hyperparameter that affects both statistical convergence and hardware efficiency, creating a trade-off that practitioners must navigate carefully. Larger batch sizes improve hardware utilization (pushing the workload into the compute-bound regime) but may require learning rate adjustments and warmup schedules to maintain training quality. The optimal batch size depends on both the model architecture and the hardware's Roofline characteristics, creating a cross-disciplinary optimization problem that spans ML theory and systems engineering.

The practical value of the Roofline Model is that it tells us *which resource to optimize*. If a workload is memory-bound, buying a faster accelerator (more TFLOPS) yields no benefit; only higher memory bandwidth will help. Conversely, if a workload is compute-bound, upgrading HBM generations is wasted money. This diagnostic power is the reason that experienced infrastructure engineers always begin a hardware selection process by computing the arithmetic intensity of their target workload and plotting it against the candidate hardware's Roofline: the plot immediately reveals which hardware characteristic matters and which is irrelevant.

For our `{python} gpt3_params_b`B model, training with large batch sizes is compute-bound (optimize for TFLOPS), while serving individual requests is memory-bound (optimize for bandwidth). This duality explains why some organizations use different hardware generations for training and inference. An A100, with its lower cost and adequate memory bandwidth, may be more cost-effective for inference than an H100, despite the H100's higher peak FLOPS.

The Roofline Model also provides a quantitative framework for evaluating the return on investment of different optimizations. If a workload is 10$\times$ below the compute ceiling but already touching the bandwidth ceiling, spending engineering effort on kernel optimization (moving toward the compute ceiling) yields no benefit. The effort should instead be directed toward reducing memory traffic (shifting the workload rightward on the plot) through techniques like batching, kernel fusion, or quantization.

This diagnostic power makes the Roofline Model one of the most practically useful analytical tools in the infrastructure engineer's toolkit. Before committing to any hardware purchase or optimization effort, plotting the workload on the Roofline reveals immediately whether the investment will produce returns. Teams that skip this analysis risk spending months optimizing the wrong resource, an expensive mistake when GPU-hours cost thousands of dollars per day.

::: {.callout-example title="Roofline Analysis: Training vs. Inference"}

Consider our `{python} gpt3_params_b`B model on an H100 with `{python} h100_tflops` TFLOPS peak compute and `{python} h100_bw` TB/s memory bandwidth. The ridge point is `{python} h100_ridge` FLOP/byte.

**Inference (Decode, Batch Size 1)**: Each token loads 350 GB of FP16 weights and performs `{python} inf_flops_math` FLOPs. Arithmetic intensity: $I = 350 \times 10^9 / 350 \times 10^9 = 1.0$ FLOP/byte. Since $1.0 \ll$ `{python} h100_ridge` (the ridge point), the workload is deeply memory-bound. The achievable throughput is `{python} inf_throughput_math` TFLOPS, which is less than 0.2% of the H100's peak `{python} h100_tflops` TFLOPS. No amount of additional compute will help; only more memory bandwidth improves throughput.

**Training (Forward Pass, Batch Size 2048)**: The same weight tensor is multiplied by a batch of 2048 activation vectors simultaneously, turning matrix-vector operations into matrix-matrix operations. The FLOPs increase by 2048$\times$ while the weight loading remains constant: $I =$ `{python} train_intensity_math` FLOP/byte. Since $2{,}048 \gg$ `{python} h100_ridge`, the workload is compute-bound. The achievable throughput is now limited by the peak compute ceiling of `{python} h100_tflops` TFLOPS, and the memory bandwidth is irrelevant. Adding more TFLOPS (through a newer GPU generation) directly improves performance.

This contrast explains why organizations sometimes use different hardware for training and inference: training benefits from peak FLOPS, while inference benefits from memory bandwidth per dollar.

:::

::: {.callout-checkpoint title="Roofline Diagnosis"}

A team is serving a 13B-parameter model at batch size 1 on an H100 and observing 25 ms per token. They propose upgrading to a B200 with 2.3$\times$ the peak TFLOPS. Estimate the arithmetic intensity of their workload and predict whether the upgrade will achieve the expected 2.3$\times$ speedup. What alternative upgrade would be more effective?

:::

### Tensor Cores and Matrix Units {#sec-compute-tensor-cores}

The Roofline Model tells us *whether* a workload can reach peak compute. But what determines that peak in the first place? The answer lies in the specialized arithmetic units that occupy the majority of a modern accelerator's die area: **Tensor Cores**[^fn-tensor-core-evolution] on NVIDIA GPUs and **Matrix Multiply Units (MXUs)** on Google TPUs.

[^fn-tensor-core-evolution]: **Tensor Core**: Introduced with NVIDIA Volta (2017) as 4$\times$4$\times$4 FP16 fused matrix-multiply-accumulate units. Each generation widened the tile: Turing (2018) added INT8, Ampere (2020) added TF32/BF16, and Hopper (2022) reached 16$\times$16$\times$16 with FP8 support. This precision cascade tracks ML's own shift toward lower-precision training and inference -- each new format unlocks a 2$\times$ throughput gain on the same silicon, making Tensor Core generations a proxy for how quickly the hardware-software co-design loop can widen the Roofline's compute ceiling. \index{Tensor Core!evolution}

A standard CPU floating-point unit performs one multiply-accumulate per cycle per lane. Even with wide SIMD units (AVX-512 provides 16 FP32 lanes), a CPU core performs at most 16 MACs per cycle. With 32 cores, a high-end server CPU achieves approximately 512 MACs per cycle, a respectable number for general-purpose computation but wholly inadequate for the demands of matrix multiplication at neural network scale.

A Tensor Core, by contrast, performs a complete matrix-multiply-accumulate (MMA) of the form $D = A \times B + C$ on small tile matrices in a single cycle. On the H100, each Tensor Core computes a 16$\times$ 16$\times$ 16 MMA per cycle, producing 4,096 multiply-accumulate results simultaneously. This is equivalent to 8$\times$ what an entire CPU can produce per cycle, concentrated in a single hardware unit that occupies a tiny fraction of the chip area.

With 528 Tensor Cores across the chip (4 per SM, 132 SMs), the H100 can execute over 2 million MACs per cycle, which is the physical basis for its `{python} h100_tflops` TFLOPS peak throughput. This represents approximately 4,000$\times$ the throughput of a high-end server CPU, achieved through radical specialization of the arithmetic hardware.

Google's MXUs take the same concept further by organizing the multipliers into a systolic array. In a systolic MXU, one matrix is loaded into the array's weight registers while the other matrix streams through the array one row at a time. Each cell multiplies the incoming activation by its stored weight, adds the result to the partial sum flowing from the cell above, and passes both the activation rightward and the partial sum downward. This pipelined flow means the array is performing useful computation on every cycle, with no idle cells once the pipeline is full. The TPU v5p contains two 128$\times$ 128 MXUs per chip, providing 459 TFLOPS of BF16 throughput. The systolic dataflow eliminates the register file accesses between operations that a Tensor Core still requires, achieving marginally higher energy efficiency at the cost of the flexibility to run non-matrix workloads.

For our `{python} gpt3_params_b`B model, the choice between Tensor Cores and MXUs manifests in the compiler stack. CUDA kernels can intermix Tensor Core operations with arbitrary thread-level code, enabling fused kernels that combine matrix multiplies with activation functions, dropout, and layer normalization in a single launch. This fusion is critical for performance because it eliminates the intermediate memory reads and writes that would otherwise occur between operations, keeping data in registers and shared memory where access is fastest.

XLA compilation for TPUs must decompose the computation into sequences of matrix operations that map onto the systolic dataflow, which can be more efficient for standard Transformer architectures but less flexible for custom operations. The XLA compiler performs whole-program optimization, analyzing the entire computation graph to find the optimal tiling, memory layout, and execution schedule for the systolic array. For standard Transformer layers, this whole-program optimization can achieve higher hardware utilization than hand-tuned CUDA kernels, because the compiler can reason about the entire computation rather than optimizing individual operations in isolation.

The hardware dictates the software abstraction, which in turn shapes what architectures are practical to experiment with. This coupling between hardware and software is a defining characteristic of ML infrastructure and explains why hardware selection has downstream effects on research velocity and model design flexibility.

::: {.callout-definition title="Tensor Core"}

***Tensor Core***\index{Tensor Core!definition} is a specialized hardware unit that performs a fused **Matrix-Multiply-Accumulate (MMA)** operation on small tile matrices in a single clock cycle ($D = A \times B + C$).

1.  **Significance (Quantitative):** They provide the majority of the **Peak Performance ($R_{peak}$)** in modern GPUs. By operating on matrices rather than individual vectors, they achieve $10\times$ to $16\times$ higher throughput than traditional SIMD units for neural network workloads.
2.  **Distinction (Durable):** Unlike **General-Purpose SIMD/SIMT** units, which operate on individual elements (scalars) or vectors, Tensor Cores operate on **Tiled Matrices**, trading flexibility for massive arithmetic throughput.
3.  **Common Pitfall:** A frequent misconception is that all GPU operations use Tensor Cores. In reality, they are **Operator-Specific**: only operations that can be decomposed into matrix multiplications (e.g., dense layers, convolutions) benefit from Tensor Core acceleration; others fall back to slower vector units.

:::

\index{Tensor Core}

The precision support of Tensor Cores has expanded with each generation, driven by the discovery that neural network training and inference can tolerate lower numerical precision than was previously assumed. The Volta generation supported only FP16 accumulation. Ampere added BF16, TF32, and INT8. Hopper added FP8 (both E4M3 and E5M2 formats) with dynamic per-tensor scaling through the Transformer Engine.

The Transformer Engine automatically monitors the magnitude distribution of each tensor and selects FP8 when precision is adequate or FP16 when higher precision is needed, doubling the effective throughput for layers where FP8 suffices. This hardware-assisted precision management represents a convergence of arithmetic design and model-level insight: the hardware is no longer a passive executor of the programmer's precision choices but an active participant in the precision decision.

Understanding the distinction between the two FP8 formats clarifies why this matters for practitioners. The E4M3 format (4 exponent bits, 3 mantissa bits) can represent values from approximately $10^{-9}$ to $4.5 \times 10^{2}$, with 3 bits of mantissa providing about 1 part in 8 relative precision. This range and precision are adequate for most neural network weights and activations, which typically fall within a narrow dynamic range after batch normalization or layer normalization.

The E5M2 format (5 exponent bits, 2 mantissa bits) provides a wider dynamic range (from approximately $10^{-14}$ to $5.7 \times 10^{4}$) at the cost of reduced precision (only 2 mantissa bits, or about 1 part in 4 relative precision). This wider range is important for gradients during the backward pass, which can span more orders of magnitude than forward-pass activations, particularly in the early and late layers of deep networks.

The Transformer Engine selects between these formats on a per-layer basis, and the per-tensor scaling factors are maintained in a small metadata buffer that adds negligible memory overhead. The scaling factors are updated every few iterations based on the observed tensor value distributions, ensuring that the limited precision of FP8 is focused on the range of values that actually appear in each tensor.

The practical implication for fleet design is that peak TFLOPS specifications are precision-dependent. The H100 delivers `{python} h100_tflops` TFLOPS in FP16 but twice that (3,958 TFLOPS) in FP8. A fleet designed for FP8 training effectively has twice the compute density of the same fleet running FP16, with no additional hardware. This makes precision engineering a first-class optimization lever for infrastructure planners, not just a model accuracy concern.

To achieve this peak throughput in practice, the entire accelerator must be viewed as a rigid pipeline where data flows from HBM through a deepening hierarchy of caches before reaching the Tensor Cores. The fundamental constraint is **pipeline balance**: the rate at which data is staged into registers must match or exceed the rate at which the arithmetic units consume it. When this balance breaks -- when the arithmetic intensity of an operation falls below the ridge point -- the pipeline stalls, leaving teraflops of compute potential idle while waiting for data. This makes **kernel fusion** the single most critical software optimization for large-scale training. By fusing multiple operations (matrix multiplication, bias addition, activation function) into a single kernel, the system eliminates the round-trips to HBM that would occur if each operation were executed sequentially. Consider the attention mechanism: a naive, unfused implementation writes the $N \times N$ attention matrix to HBM only to read it back for the softmax operation, a round trip capped by the 3.35 TB/s memory bandwidth. A fused implementation like FlashAttention keeps these intermediate matrices entirely in on-chip SRAM, bypassing HBM and allowing the Tensor Cores to run near their theoretical peak. For our `{python} gpt3_params_b`B model, where each training step involves thousands of matrix operations across 96 Transformer layers, the difference between fused and unfused kernels can be a 2--3$\times$ throughput improvement -- the difference between a 2-week and a 6-week training run.

#### Peak vs. Sustained Throughput

A critical distinction for infrastructure planning is the difference between *peak* throughput (the maximum the hardware can achieve on a synthetic benchmark) and *sustained* throughput (what the hardware actually delivers during real training runs). Peak TFLOPS assumes that the Tensor Cores are fed with data on every cycle, which requires perfect scheduling, zero memory stalls, and no communication overhead. In practice, sustained throughput during Transformer training is typically 30--50% of peak for compute-bound operations and less than 5% of peak for memory-bound operations.

The gap between peak and sustained throughput arises from several sources, each of which represents a different physical or software limitation.

**Memory stalls** occur when the Tensor Cores complete their current tile multiplication before the next tile has been loaded from HBM. Even with the H100's 3.35 TB/s HBM bandwidth, the Tensor Cores can consume data faster than HBM can deliver it for certain matrix dimensions, creating idle cycles while the arithmetic units wait for data.

**Pipeline bubbles** arise when one operation must complete before the next can begin, leaving some processing elements idle. In a Transformer layer, the attention computation must complete before the feed-forward network can begin, and the feed-forward computation must complete before the next layer's attention can start. These sequential dependencies create brief periods where some hardware resources are unused.

::: {.callout-definition title="Pipeline Bubble"}

***Pipeline Bubble***\index{Pipeline Bubble!definition} is the idle time in pipeline-parallel training caused by stages waiting for inputs from upstream workers during the fill and drain phases of a micro-batch cycle.

1.  **Significance (Quantitative):** It represents a direct loss in **System Efficiency ($\eta$)**. For a model with $p$ pipeline stages and $m$ micro-batches, the bubble fraction is approximately **$(p-1)/m$**, dictating the maximum theoretical utilization of the cluster.
2.  **Distinction (Durable):** Unlike **Network Latency** (which is a data transfer delay), a Pipeline Bubble is a **Temporal Dependency** delay: workers are idle not because the network is slow, but because the next piece of work has not yet been computed.
3.  **Common Pitfall:** A frequent misconception is that bubbles can be eliminated by "faster networking." In reality, the bubble is an **Algorithmic Constraint**: it can only be reduced by increasing the number of micro-batches ($m$), which in turn increases the **Memory Footprint** for stored activations.

:::

**Communication overhead** from AllReduce operations (for tensor parallelism) or gradient synchronization (for data parallelism) pauses computation entirely while data is exchanged between accelerators. Even when communication is overlapped with computation (using separate communication and compute streams), there are typically operations that cannot be overlapped because they depend on the result of the communication.

**Software overhead** from kernel launch latency, memory allocation, and Python-level control flow contributes an additional 5-10% overhead. Each CUDA kernel launch incurs approximately 5-10 microseconds of latency on the host side, and a single Transformer layer may involve 20-30 kernel launches. For very small microbatches where each kernel completes in microseconds, the launch overhead can become a meaningful fraction of total execution time.

Kernel fusion partially addresses this gap by combining multiple operations (matrix multiply, bias add, activation function, dropout) into a single GPU kernel, eliminating the intermediate memory reads and writes that would otherwise stall the Tensor Cores between operations. The art of achieving high sustained utilization is largely the art of minimizing the time Tensor Cores spend idle, which is why specialized kernel libraries like FlashAttention and the Transformer Engine exist.

For capacity planning, the sustained throughput rate, not the peak rate, should be used. A training run estimated at 1,000 GPU-hours using peak FLOPS will actually require 2,000--3,000 GPU-hours when accounting for real-world utilization. Experienced infrastructure teams track their **Model FLOPS Utilization (MFU)**, defined as the ratio of achieved FLOPS (computed from training throughput) to the hardware's peak FLOPS, as the primary metric for infrastructure efficiency. MFU values of 40--50% are considered good for large-scale Transformer training; values above 50% indicate excellent software optimization.

::: {.callout-definition title="Model FLOPS Utilization (MFU)"}

***Model FLOPS Utilization (MFU)***\index{Model FLOPS Utilization!definition} is the ratio of useful model FLOPs to the hardware's theoretical peak throughput ($R_{peak}$).

1.  **Significance (Quantitative):** It is the most precise measure of **System Efficiency ($\eta$)** because it excludes "waste" FLOPs from recomputation, padding, or speculative execution.
2.  **Distinction (Durable):** Unlike **Hardware Utilization** (which reports how often the GPU is "busy"), MFU reports how much of that busyness contributes to **Model Convergence**.
3.  **Common Pitfall:** A frequent misconception is that 100% MFU is possible. In reality, production training typically achieves **30–50% MFU**; the remainder is lost to inescapable memory stalls ($BW$), communication overhead ($L_{lat}$), and pipeline bubbles.

:::

The Memory Wall constrains how fast data reaches the compute units; the Roofline Model diagnoses whether compute or memory is the binding constraint; and Tensor Cores maximize the arithmetic value of every byte fetched. But there is a third physical constraint that limits the accelerator's performance: the heat generated by all this computation. Every FLOP dissipates energy, and the faster we compute, the more heat we must remove. This is the Power Wall.

## Thermal Design Power {#sec-compute-tdp}

\index{TDP}
\index{Power Wall}

Every floating-point operation dissipates energy as heat. A Tensor Core performing 4,096 MACs per cycle at 2 GHz generates heat proportional to the switching activity of billions of transistors. The **Thermal Design Power (TDP)**[^fn-tdp-thermal] of an accelerator is the maximum sustained heat dissipation that the cooling system must handle, and it has become the single most consequential specification in fleet design.

[^fn-tdp-thermal]: **TDP (Thermal Design Power)**: Often misunderstood as "power consumption," TDP specifies the maximum sustained thermal load (in watts) that the cooling solution must remove. Accelerators routinely exceed their TDP during transient power spikes -- an H100 can spike to 900 W during a large GEMM before firmware throttles back to 700 W -- which forces rack-level VRM and cooling designs to provision for worst-case, not average, power draw. \index{TDP!thermal constraint}

::: {.callout-definition title="Thermal Design Power (TDP)"}

***Thermal Design Power (TDP)***\index{Thermal Design Power!definition} is the maximum sustained thermal load (in watts) that a cooling solution is designed to remove.

1.  **Significance (Quantitative):** It defines the **Physical Ceiling** for performance. Within the **Iron Law**, TDP limits the **Peak Performance ($R_{peak}$)** because every floating-point operation dissipates energy as heat; exceeding the TDP triggers **Thermal Throttling**, which reduces the duty cycle ($\eta$).
2.  **Distinction (Durable):** Unlike **Peak Power Consumption** (which captures transient spikes), TDP describes the **Steady-State Thermal Contract** between the silicon and the environment.
3.  **Common Pitfall:** A frequent misconception is that TDP is a "power limit" for the user. In reality, it is a **Cooling Requirement**: a processor can only reach its rated speed if the cooling system can remove the TDP-equivalent amount of heat continuously.

:::

The trajectory of TDP tells the story of a physical law reaching its limits. For three decades, **Dennard scaling**[^fn-dennard-scaling] allowed chip designers to shrink transistors while maintaining constant power density: smaller transistors required lower voltage, so packing more transistors into the same area did not increase heat output per unit area. This scaling broke down around 2006 when transistor dimensions reached the point where quantum effects (subthreshold leakage, gate oxide tunneling) prevented further voltage reduction. Since then, each generation of accelerator has increased power consumption roughly in proportion to its performance gains. The era of "free" performance improvements through process shrinks is over; every gain in throughput now comes with a proportional gain in power consumption and heat generation.

[^fn-dennard-scaling]: **Dennard Scaling**: Named after Robert Dennard (IBM, 1974), who observed that shrinking transistors could maintain constant power density because voltage and current scale with dimensions. The breakdown around 2006 -- when subthreshold leakage and gate-oxide tunneling prevented further voltage reduction below ~28 nm -- is the direct cause of the V100-to-B200 TDP trajectory (300 W to 1,000 W). For ML infrastructure, the post-Dennard era means every TFLOPS gain arrives with a proportional watt increase, making cooling and power delivery co-equal design constraints with the silicon itself. \index{Dennard Scaling!power wall}

The numbers are stark. The V100 (2017) operated at 300 W TDP. The A100 (2020) increased to 400 W. The H100 (2022) reached 700 W. The B200 (2024) draws 1,000 W. In seven years, TDP has more than tripled, growing at approximately 18% per year.

If this growth rate continues, the next generation after Blackwell will approach 1,200--1,400 W per accelerator, and the generation after that may reach 1,500--2,000 W. At 2,000 W per accelerator, a single 8-GPU node would consume 16 kW of GPU power alone, requiring liquid cooling infrastructure that can remove heat at rates approaching those of industrial process cooling equipment.

::: {.callout-perspective title="The End of Dennard Scaling"}

For three decades, Dennard scaling allowed architects to increase transistor count without increasing power density, as smaller transistors required proportionally less voltage. That free lunch ended around 2006 when process nodes dropped below 28nm, where quantum effects like subthreshold leakage and gate oxide tunneling prevent further voltage reduction. The result is that power density now rises with transistor density: every new generation of accelerator that delivers more TFLOPS also demands more watts. The transition from the V100 (12nm) to the H100 (4nm) illustrates this physics: while transistor density tripled, the inability to scale voltage meant TDP grew from 300 W to 700 W. The "free" efficiency gains of the silicon era are over; today, the only path to higher performance per watt is architectural specialization -- Tensor Cores, systolic arrays, precision engineering -- or massive increases in power and cooling infrastructure. For ML infrastructure planners, this means that power and cooling capacity must grow at least as fast as compute demand, and facility designs that assume future hardware will be more power-efficient are building on a foundation that no longer exists.

:::

This is not merely a specification on a data sheet; it is a physical constraint that propagates through every level of the infrastructure stack. The power delivery chain, cooling system, rack design, and facility electrical infrastructure must all be designed to accommodate not just today's TDP but the projected TDP of hardware two or three generations into the future.

Modern GPUs implement sophisticated power management to operate within their TDP envelope. When the workload does not fully use all SMs (for example, during the communication phase of a training step when the GPUs are waiting for AllReduce to complete), the GPU firmware reduces the clock frequency and voltage of idle SMs through a process called *clock gating*. This reduces instantaneous power consumption and allows the active SMs to run at a higher frequency. Conversely, during a large matrix multiplication that activates all SMs simultaneously, the firmware reduces the clock frequency to keep total power within TDP.

This dynamic power management means that the actual power draw of a GPU varies continuously during training. A typical training step might see power swing from 400 W (during communication phases when most SMs are idle) to 700 W (during the forward and backward pass when all Tensor Cores are active) and back, with each transition occurring in microseconds. The VRMs on the baseboard must respond to these transitions fast enough to maintain a stable supply voltage despite the rapidly changing current draw.

To appreciate the physical challenge, consider what 700 W means in terms of heat flux. An H100 die measures approximately 814 mm$^2$ (roughly 29 mm$\times$ 28 mm). Dissipating 700 W from this area produces a heat flux of approximately 86 W/cm$^2$. For comparison, the surface of an electric stovetop burner at high heat produces approximately 10 W/cm$^2$. An H100 die generates 8.6$\times$ the heat flux of a stove burner, concentrated on an area the size of a large postage stamp.

The B200 pushes even further: at 1,000 W across a dual-die package, the average heat flux remains comparable, but the total energy that must be removed per package increases by 43%. Removing this heat without allowing the junction temperature to exceed 83 degrees Celsius (the typical operating maximum for HBM reliability) requires thermal solutions with extremely low thermal resistance from die to coolant.

The critical component in this thermal path is the **Thermal Interface Material (TIM)** -- the microscopic layer of phase-change material or liquid metal between the silicon die and the cold plate (for liquid cooling) or heatsink (for air cooling). Under the extreme thermal cycling of deep learning workloads, where die temperatures spike from 40 degrees C to 80 degrees C in milliseconds during matrix multiplications and drop back during communication phases, inferior TIM can pump out (migrate away from the contact surface) or crack. A mere 5% degradation in TIM thermal conductivity forces the GPU to thermally throttle, reducing clock frequency by hundreds of MHz to protect the silicon. This degradation is a classic gray failure: the GPU continues to function, but at reduced performance that silently degrades the entire cluster's throughput. Fleet management systems that track per-GPU junction temperatures over time can detect TIM degradation as a gradual upward trend in temperature at constant workload, enabling proactive replacement before the performance impact becomes severe.

At rack scale, four nodes of eight such chips each generate `{python} rack_power_str` kW, enough to heat several homes in winter. To put this in more tangible terms, a single ML rack at full power could run about 33 household space heaters simultaneously, concentrated in a volume roughly the size of a large refrigerator.

At pod scale, the power demand reaches megawatts, requiring dedicated electrical substations and industrial cooling plants. A 10,000-GPU cluster consumes 7--10 MW, comparable to the electrical demand of a large factory or a small town of several thousand residents.

The physical volume of a 10,000-GPU pod operating at this power density is surprisingly compact: the compute hardware (excluding networking and storage) fits in approximately 300 racks, occupying a datacenter floor area of roughly 1,500 square meters (about one third of an American football field). The density is the point: keeping the accelerators physically close minimizes cable lengths, which reduces signal propagation delay and enables higher interconnect bandwidth.

But this density makes power delivery and cooling exponentially harder, creating the engineering challenges that define the rack and pod levels of the hierarchy. The paradox of modern ML infrastructure is that computational efficiency demands physical proximity, while thermal management demands physical separation. Every rack design is a negotiation between these opposing forces.

Modern accelerators employ **dynamic voltage and frequency scaling (DVFS)** to manage the trade-off between performance and power in real time. When all Tensor Cores are active and drawing maximum current, the GPU reduces its clock frequency to stay within the TDP envelope. When some SMs are idle (during communication phases, for instance), the active SMs can boost their frequency because the total chip power is below TDP. This dynamic behavior means that the actual clock frequency, and therefore the actual throughput, varies throughout a training step.

For ML workloads, DVFS creates a subtle interaction with software optimization. A kernel that achieves high SM occupancy (many active warps per SM) draws more power, potentially triggering a frequency reduction that partially offsets the occupancy benefit. Conversely, a kernel with moderate occupancy may run at a higher clock frequency, achieving comparable throughput at lower power. This is why power-normalized throughput (TFLOPS per watt) is a more robust comparison metric than raw TFLOPS, and why the same GPU can show different sustained clock frequencies depending on the workload and the cooling solution.

At fleet scale, DVFS also affects power planning. The datacenter's power delivery must be designed for the *worst-case* power draw (all GPUs at TDP simultaneously), but the average power draw is typically 70--85% of TDP because DVFS reduces frequency during communication phases and because not all SMs are fully occupied at all times. Some operators deliberately set lower power caps on their GPUs (for example, capping an H100 at 600 W instead of 700 W), accepting a 10--15% reduction in peak performance in exchange for 15% lower power consumption and the ability to fit more GPUs within a fixed power budget. This power capping is a form of the compute-efficiency trade-off at the operational level.

The power efficiency trajectory provides a partial counterweight to rising TDP. While absolute power has increased, the TFLOPS delivered per watt has improved dramatically: from `{python} v100_ef` TFLOPS/W (V100) to `{python} h100_ef` TFLOPS/W (H100) to `{python} b200_ef` TFLOPS/W (B200). This means that for a fixed compute budget, each generation requires fewer chips and therefore less total power. But frontier models grow faster than efficiency improves, so the *absolute* power demand of training the latest model increases with every generation. This is the treadmill of infrastructure: efficiency gains buy time, but scale growth consumes it.

The implications of TDP for fleet design are profound and will recur throughout this chapter. At the node level, TDP determines how many accelerators can share a single chassis (we examine this in @sec-compute-node). At the rack level, TDP dictates the transition from air cooling to liquid cooling (@sec-compute-rack). At the pod level, TDP drives the megawatt-scale power delivery systems that connect the fleet to the electrical grid (@sec-compute-pod). The accelerator is the engine, but TDP is the exhaust, and every level of infrastructure above the die exists, in part, to manage that exhaust.

### Accelerator Selection for ML Workloads {#sec-compute-accelerator-selection}

\index{Accelerator!selection}

The preceding sections have established the physical characteristics of accelerators: compute throughput, memory bandwidth, memory capacity, power consumption, and the trade-offs between generality and efficiency. How do these characteristics map to the workloads that a fleet must support? The answer depends on the workload's position on the Roofline plot and its specific resource requirements.

**Large Language Model Training** (our `{python} gpt3_params_b`B running example) is typically compute-bound during the forward and backward passes because large batch sizes push the arithmetic intensity above the ridge point. The dominant hardware requirement is peak TFLOPS, with memory capacity being a constraint that determines the minimum number of accelerators needed to hold the model state. The H100 and B200 are well-suited for this workload, as are TPU v5p pods for organizations willing to accept the XLA compilation requirement. The choice between GPU and TPU often comes down to software ecosystem compatibility: PyTorch-native research teams prefer GPUs, while JAX-based teams can take advantage of TPU's cost efficiency.

**Large Language Model Inference (Serving)** is overwhelmingly memory-bound during the autoregressive decode phase, as we demonstrated in the token latency analysis. The key hardware metric is memory bandwidth per dollar, not peak TFLOPS. For batch-1 serving (single user), the H100's `{python} h100_bw` TB/s bandwidth determines throughput, and the compute units sit almost entirely idle. For batched serving (multiple concurrent users), increasing the batch size raises the arithmetic intensity, gradually shifting the workload toward the compute-bound regime. This shift explains why serving systems aggressively batch requests: it transforms a memory-bound workload into one that can actually use the expensive compute silicon.

**Recommendation Models** (the Meta use case) have a split personality. The embedding table lookup phase involves random memory accesses across terabytes of data, which is memory-capacity-bound (not bandwidth-bound, because each lookup retrieves a small amount of data from a random location). The dense neural network phase that operates on the retrieved embeddings is compute-bound. This duality drives Meta's hybrid CPU-GPU architecture: CPUs handle the capacity-bound embedding lookups, and GPUs handle the compute-bound dense layers.

**Vision Models and Diffusion Models** exhibit high arithmetic intensity during both training and inference because convolution and attention operations involve substantial data reuse. These workloads are typically compute-bound on modern accelerators, making peak TFLOPS the primary selection criterion. They are also less memory-capacity-constrained because vision models are typically 1--10B parameters rather than 100B+.

Diffusion models add a distinctive characteristic: inference requires multiple iterative denoising steps (typically 20--50), each of which involves a full forward pass through the model. The total computation per generated image is therefore 20--50$\times$ that of a single forward pass, making inference significantly more compute-intensive than for autoregressive language models of comparable size. This compute intensity shifts the hardware selection criterion firmly toward TFLOPS per dollar rather than bandwidth per dollar.

**Mixture-of-Experts (MoE) Models** introduce a qualitatively different infrastructure challenge. In a dense Transformer, every token activates every parameter. In an MoE model, each token activates only a subset of "expert" subnetworks (typically 2 out of 8 or 16 experts), reducing the computation per token while maintaining a large total parameter count. A 1 trillion parameter MoE model with 2-of-16 routing activates only 125 billion parameters per token, achieving the perplexity of a dense 1T model with the per-token compute cost of a 125B model.

The infrastructure implications are significant. MoE models require **AllToAll** communication to route tokens to the correct experts, which is fundamentally different from the AllReduce pattern used by dense models. AllToAll sends different data to each recipient, creating a communication pattern where every GPU must exchange data with every other GPU simultaneously. This pattern requires high bisection bandwidth (the total bandwidth available across any bisection of the network), which is the strength of fat-tree topologies and the weakness of torus topologies. Organizations planning to train MoE models at scale must account for this when selecting their network topology.

MoE models also create uneven memory and compute loads. If token routing is not perfectly balanced, some experts receive more tokens than others, creating a load imbalance where some GPUs are busy while others wait. Dynamic load balancing mechanisms (auxiliary loss terms that encourage balanced routing, or capacity factors that cap the number of tokens per expert) address this problem in software, but the infrastructure must be designed to handle the worst-case load distribution.

**Multi-Modal Models** (combining text, image, audio, and video) present the most complex hardware selection challenge because each modality has different computational characteristics. The text processing component of a multi-modal model behaves like an LLM (memory-bound during decode, compute-bound during prefill). The vision component (typically a Vision Transformer or ViT) is compute-bound during both training and inference. The audio component involves 1D convolutions and attention with different arithmetic intensities than the text or vision paths. A single accelerator must handle all these patterns efficiently, which favors the GPU's general-purpose flexibility over the TPU's specialized dataflow.

::: {.callout-perspective title="Matching Hardware to Workload"}

The fundamental insight from the Roofline Model is that *no single accelerator is optimal for all workloads*. An H100 that delivers outstanding training throughput for a 175B LLM achieves less than 1% utilization when serving that same model at batch size 1. A TPU pod that provides exceptional cost efficiency for Transformer training may be poorly suited for a recommendation model with irregular memory access patterns.

The fleet that a mature organization deploys is therefore heterogeneous by design:

- **Training clusters** optimized for compute throughput (peak TFLOPS, NVLink bandwidth)
- **Serving clusters** optimized for memory bandwidth per dollar (HBM bandwidth, inference-specific accelerators)
- **Embedding systems** optimized for memory capacity (large DRAM, CPU-GPU hybrid)
- **Fine-tuning clusters** that balance compute and memory (fewer GPUs per node, more HBM per GPU)

The accelerator spectrum is not just a menu of options but a design space that must be navigated for each workload independently. Organizations that deploy a single hardware configuration for all workloads inevitably overpay: either the hardware is overprovisioned for simple workloads (wasting expensive compute silicon) or underprovisioned for demanding workloads (failing to meet performance requirements).

:::

#### Precision and Throughput Trade-offs

\index{Precision!FP8}
\index{Precision!BF16}

The relationship between numerical precision and hardware throughput is one of the most important considerations for infrastructure planning, because it directly affects how much useful work each accelerator can perform per second. Lower precision representations use fewer bits per number, which has two compounding benefits: more numbers fit in the same HBM capacity (reducing memory pressure), and the arithmetic units can process more operations per cycle (increasing throughput).

The precision landscape for ML has evolved rapidly. The following table summarizes the available precisions on modern accelerators and their typical use cases:

| **Format**     | **Bits** | **Exponent** | **Mantissa** | **H100 TFLOPS** | **Typical Use**                  |
|:---------------|:--------:|:------------:|:------------:|:---------------:|:---------------------------------|
| **FP32**       |    32    |      8       |      23      |       989       | Master weights, loss computation |
| **TF32**       |    19    |      8       |      10      |       989       | Training (NVIDIA default)        |
| **BF16**       |    16    |      8       |      7       |      1,979      | Training, fine-tuning            |
| **FP16**       |    16    |      5       |      10      |      1,979      | Training with loss scaling       |
| **FP8 (E4M3)** |    8     |      4       |      3       |      3,958      | Forward pass, weights            |
| **FP8 (E5M2)** |    8     |      5       |      2       |      3,958      | Backward pass, gradients         |
| **INT8**       |    8     |     N/A      |     N/A      |      3,958      | Post-training quantization       |
| **INT4**       |    4     |     N/A      |     N/A      |      7,916      | Weight-only quantization         |

: **Precision Formats and Hardware Throughput on H100**. Each halving of precision approximately doubles the hardware throughput. The choice of precision affects both the training dynamics (convergence, numerical stability) and the infrastructure requirements (memory capacity, bandwidth utilization). Note that FP32 and TF32 share the same Tensor Core throughput because TF32 uses the same exponent width as FP32, differing only in mantissa precision. {#tbl-precision-throughput}

The infrastructure implications are substantial. A fleet designed for BF16 training (the current standard for most large models) delivers `{python} h100_tflops` TFLOPS per GPU. The same fleet running FP8 training delivers 3,958 TFLOPS per GPU, effectively doubling the compute density without additional hardware. For a 10,000-GPU cluster, the difference between BF16 and FP8 training is equivalent to adding 10,000 more GPUs, a savings of roughly \$3.5 billion in hardware cost.

However, not all workloads can use FP8 without accuracy degradation. The reduced mantissa precision (3 bits in E4M3) means that values must be scaled carefully to avoid overflow (values exceeding the representable range are clamped to infinity) or underflow (small values rounded to zero). The Transformer Engine's dynamic per-tensor scaling addresses this challenge for standard Transformer architectures, but custom model architectures with unusual activation distributions may require manual precision tuning. The infrastructure team must therefore work closely with the model team to determine the lowest precision that maintains acceptable accuracy, as this decision directly affects the effective throughput and therefore the required cluster size.

For inference, **quantization** stands as the single most impactful optimization for serving economics, directly altering the hardware topology required for large models. Our `{python} gpt3_params_b`B model at FP16 requires 350 GB, necessitating a minimum of five 80 GB H100 GPUs simply to load the parameters. At INT8, this drops to 175 GB, fitting on three GPUs. At INT4, it shrinks to 87.5 GB, fitting on two GPUs. Because inference is strictly memory-bandwidth-bound, reading 4-bit weights instead of 16-bit weights effectively quadruples the available bandwidth for weight loading, proportionally reducing per-token latency. A production team deploying at INT4 rather than FP16 reduces their inference fleet by 4$\times$, saving millions of dollars annually at scale while incurring less than 1% quality loss on standard benchmarks when using group quantization techniques.

For training, modern workloads use **mixed-precision training**, a strategy that decouples storage precision from arithmetic precision. A "master copy" of weights remains in FP32 for numerical stability, while the computationally intensive forward and backward passes are cast to BF16 or FP16 to exploit the full throughput of Tensor Cores. Gradients are accumulated in FP32 to prevent underflow before the optimizer updates the master weights. The H100's Transformer Engine extends this paradigm by dynamically selecting between FP8 and FP16 on a per-layer basis, potentially doubling throughput again for layers resilient to reduced precision. The cumulative effect on training velocity is profound: a training run that demands four weeks in pure FP32 can complete in approximately one week using a mixed FP16/FP8 strategy, with negligible quality degradation.

#### Energy Cost of Data Movement

\index{Data Movement!energy cost}

The Roofline Model and token latency analysis demonstrate that data movement limits performance. But data movement also dominates *energy* consumption, a fact with profound implications for fleet economics. Moving a byte of data from HBM to a register file consumes approximately 20 picojoules (pJ). Performing a floating-point multiply-accumulate operation on that byte consumes approximately 1 pJ. The energy cost of *moving* data is therefore 20$\times$ the energy cost of *computing* on it.

This 20:1 ratio explains why accelerator architects devote so much silicon area to data reuse. A Tensor Core's tile-based execution model loads a small matrix into local registers and reuses each element hundreds of times (once per element in the opposing matrix), amortizing the 20 pJ memory access cost across hundreds of 1 pJ compute operations. Without this reuse, the energy budget would be dominated by memory access, and most of the chip's power would be spent heating wires rather than switching transistors.

At fleet scale, the energy cost of data movement determines the electricity bill. For a 10,000-GPU cluster running at 700 W per GPU, approximately 400--500 W per GPU is consumed by the memory subsystem (HBM reads and writes, on-chip network transfers, register file accesses), and only 150--250 W is consumed by the Tensor Cores performing useful arithmetic. The rest goes to clock distribution, I/O, and leakage. Improving data reuse, through techniques like kernel fusion, FlashAttention, and activation checkpointing, reduces the energy wasted on data movement and increases the fraction of power that performs useful computation.

This energy perspective also provides a physical foundation for understanding why different parallelism strategies have different efficiencies. Tensor parallelism requires data movement over NVLink (approximately 5 pJ per bit at the link level, plus the energy of the NVSwitch chips). Data parallelism requires data movement over InfiniBand (approximately 15--20 pJ per bit, including the energy of the HCA and switch). Pipeline parallelism moves less data but requires more complex scheduling. Each parallelism strategy represents a different point in the trade-off between communication energy and computation efficiency.

::: {.callout-notebook title="The Energy Cost of Data Movement"}

The iron law of modern computing is that moving data costs significantly more energy than manipulating it. We can prove this by analyzing the energy hierarchy of a single operation at the 4nm process node.

Performing one FP16 multiply-accumulate (**MAC**) operation -- the atomic unit of deep learning -- consumes approximately **1 picojoule (pJ)**. This is the baseline cost of useful work. Reading a single FP16 operand (16 bits) from HBM consumes roughly 4 pJ per bit, totaling **64 pJ**. Reading that same operand from off-package DRAM costs approximately 20 pJ per bit, or **320 pJ**. The asymmetry is staggering: reading a value from HBM costs 64$\times$ more energy than computing on it. Retrieving it from standard DRAM costs 320$\times$ more.

The macro impact appears when serving our `{python} gpt3_params_b`B model. Generating a single token requires loading the entire 350 GB weight tensor from HBM:

$$\text{Data Movement Energy} = 350 \text{ GB} \times 8 \text{ bits/byte} \times 4 \text{ pJ/bit} \approx \mathbf{11.2 \text{ Joules}}$$

The computational cost for the associated $\approx$350 billion MACs is trivial by comparison:

$$\text{Computation Energy} = 350 \times 10^9 \text{ MACs} \times 1 \text{ pJ/MAC} \approx \mathbf{0.35 \text{ Joules}}$$

Moving the weights to the compute units consumes **32$\times$** more energy than the math itself. This energy penalty is why HBM is engineered for physical proximity to the GPU die -- shorter traces mean less capacitance and lower energy per bit. It also explains why techniques that reduce data movement, such as **quantization** (moving fewer bits) and **kernel fusion** (preventing round-trips to memory), are the primary levers for improving both system performance and power efficiency.

:::

#### Benchmarking Accelerator Performance

Given the complexity of the accelerator spectrum, how should practitioners compare accelerators objectively? Raw specifications (peak TFLOPS, memory bandwidth, TDP) are necessary but insufficient, because they describe *potential* rather than *achieved* performance. Two industry-standard benchmarks provide more meaningful comparisons.

**MLPerf Training** is a benchmark suite maintained by MLCommons that measures the time required to train a set of reference models to a target accuracy on a given system. The benchmark covers multiple model types (image classification, object detection, natural language processing, recommendation) and requires submissions to report both the hardware configuration and the training time. MLPerf results are particularly valuable because they capture the *complete system* performance, including software stack efficiency, communication overhead, and memory management, not just the raw silicon capabilities.

**MLPerf Inference** measures the throughput and latency of serving trained models under realistic conditions, including batching, multi-stream scenarios, and offline processing. For infrastructure procurement, the inference benchmarks are especially useful because they reveal how well the accelerator's memory bandwidth supports real-world serving workloads, which is often more important than peak FLOPS for inference-oriented hardware.

A critical subtlety in interpreting MLPerf results is the distinction between **closed** and **open** divisions. The closed division requires all submissions to use the same model architecture, hyperparameters, and training recipe, isolating the hardware and system software as the only variables. This makes closed-division results directly comparable across vendors. The open division allows arbitrary model modifications, software optimizations, and custom kernels, which can demonstrate the ceiling of a platform's capability but makes cross-vendor comparison unreliable. Infrastructure teams evaluating procurement decisions should weight closed-division results more heavily, as they reflect the performance an organization will achieve with standard frameworks and configurations, not the performance achievable only by the vendor's own optimization team.

For our `{python} gpt3_params_b`B model, no single benchmark captures the full picture. The training phase is compute-bound at large batch sizes, making peak TFLOPS and scaling efficiency the dominant metrics. The inference phase at batch size 1 is memory-bandwidth-bound, making GB/s per dollar the relevant metric. The fine-tuning phase falls between these extremes, with moderate batch sizes placing the workload near the Roofline ridge point. A comprehensive evaluation must benchmark all three phases on the candidate hardware, using the organization's actual model and data pipeline rather than relying solely on published MLPerf numbers.

::: {.callout-perspective title="Beyond Peak Specifications"}

When evaluating accelerator options, the following metrics provide a more complete picture than peak TFLOPS alone:

- **Model FLOPS Utilization (MFU)**: The ratio of achieved FLOPS during real training to peak hardware FLOPS. An MFU of 50% means the hardware spends half its time on useful computation and half waiting for data, synchronizing, or idling.
- **Time-to-Train (TTT)**: The wall-clock time to train a reference model to a target metric. This captures all system-level effects that MFU alone does not, including I/O stalls, checkpointing overhead, and job restart delays.
- **Cost per Token**: For language model training, the total cost (hardware amortization plus electricity) divided by the number of tokens processed. This metric normalizes across different hardware generations, cluster sizes, and pricing models.
- **Tokens per Second per Dollar**: The inverse of cost per token, useful for comparing the economic efficiency of different systems.

No single metric tells the whole story. A system with high MFU but high cost per GPU-hour may be less economical than a system with lower MFU but cheaper hardware. The right metric depends on whether the organization is optimizing for time (training must finish by a deadline), cost (minimize total expenditure), or throughput (maximize tokens processed per unit time).

:::

#### Inference-Specific Infrastructure Considerations

\index{Inference!infrastructure}

While this chapter focuses primarily on training infrastructure (because training drives the most demanding infrastructure requirements), the infrastructure for serving trained models at scale deserves separate attention because the design constraints differ fundamentally from training.

Training infrastructure is optimized for *throughput*: maximizing the total number of tokens processed per unit time across the entire cluster. The workload is a single, long-running job that occupies the full cluster for days or weeks. The communication pattern is predictable and repetitive. The acceptable latency for any individual operation is milliseconds to seconds.

Serving infrastructure is optimized for *latency* and *cost per query*: minimizing the time each user waits for a response while keeping the per-query cost low enough for the business model to be viable. The workload consists of millions of independent, short-lived requests arriving at irregular intervals. The communication pattern is minimal (each request is processed independently on a single node or small group of nodes). The acceptable latency is tens of milliseconds for interactive applications.

These different requirements drive different hardware and architecture choices:

- **GPU utilization**: Training clusters achieve 30--50% MFU because large batch sizes push workloads into the compute-bound regime. Serving clusters at low batch sizes achieve less than 5% MFU because single-request inference is deeply memory-bound. Serving systems therefore aggressively batch requests to improve utilization, which introduces a trade-off between latency (requests must wait to be batched) and throughput (larger batches achieve higher utilization).
- **Model replication**: Training distributes a single model across many GPUs using tensor and pipeline parallelism. Serving replicates the entire model (or a significant portion of it) across many GPUs, with each replica handling independent requests. This replication approach means that the total GPU memory required for serving is proportional to the number of concurrent requests the system must support, not just the model size.
- **Network requirements**: Training requires high-bandwidth, low-latency inter-GPU communication for gradient synchronization. Serving requires minimal inter-GPU communication (each request is independent) but substantial client-facing network bandwidth to handle millions of API requests per second.
- **Cost metric**: Training cost is measured in dollars per training run (a fixed, one-time cost). Serving cost is measured in dollars per 1,000 tokens generated (a variable, ongoing cost that accumulates over the model's operational lifetime). For popular models served at scale, the cumulative serving cost can exceed the training cost within weeks.

These differences mean that the optimal serving infrastructure often uses different hardware, different software, and different facility designs than the training infrastructure. Some organizations use previous-generation GPUs (A100s) for serving because the memory bandwidth per dollar is competitive with current-generation GPUs, and the lower TDP (400 W vs. 700 W) allows higher rack density and lower cooling costs.

The inference workload itself bifurcates into two distinct phases with opposing bottlenecks. The **prefill** phase processes the input prompt, performing a large matrix-matrix multiplication that is compute-bound and achieves high Tensor Core utilization. The **decode** phase generates tokens one by one, performing matrix-vector multiplications that are deeply memory-bandwidth-bound. To maximize hardware utilization, modern serving systems employ **continuous batching**, which schedules requests at the iteration level rather than the request level. This allows the engine to inject a new prefill computation for a waiting request into the idle compute slots of an ongoing decode batch, dynamically filling the GPU's arithmetic pipelines. For our `{python} gpt3_params_b`B model, serving 10,000 requests per second with a 50 ms time-to-first-token SLA requires distributing traffic across hundreds of 8-GPU replicas (each holding the full model via tensor parallelism), with a load balancer routing requests to the replica with the lowest current queue depth. Unlike training, where 99.9% reliability is acceptable via checkpointing, inference architectures must account for tail latency, where a single slow replica can violate the SLA for the entire request batch.

#### The Accelerator Decision Matrix

The preceding sections have established the physical characteristics, analytical tools, and workload-specific considerations that govern accelerator selection. Synthesizing these into a practical decision framework requires mapping each workload archetype to the hardware characteristic that dominates its performance and cost.

| **Workload**                    | **Binding Constraint**       | **Key Metric**         | **Recommended Class**     |
|:--------------------------------|:-----------------------------|:-----------------------|:--------------------------|
| **LLM Training (>100B)**        | Compute (high batch size)    | Peak TFLOPS, NVLink BW | H100/B200, TPU v5p        |
| **LLM Inference (batch=1)**     | Memory bandwidth             | GB/s per dollar        | A100, H100 (bandwidth/\$) |
| **LLM Inference (batched)**     | Compute + bandwidth          | TFLOPS and GB/s        | H100, B200                |
| **Vision Model Training**       | Compute (large spatial dims) | Peak TFLOPS            | H100/B200, GPU preferred  |
| **Recommendation (embeddings)** | Memory capacity              | GB per dollar          | CPU DRAM + GPU hybrid     |
| **Fine-tuning (<13B)**          | Memory capacity              | HBM capacity           | A100 (cost-effective)     |
| **Research / Prototyping**      | Flexibility                  | Software ecosystem     | GPU (CUDA), avoid ASICs   |

: **The Accelerator Decision Matrix**. The optimal accelerator depends not on peak specifications but on which physical resource -- compute, bandwidth, or capacity -- is the binding constraint for the target workload. The Roofline Model provides the diagnostic: compute the arithmetic intensity, locate the workload relative to the ridge point, and select hardware that maximizes the binding resource per dollar. {#tbl-accelerator-decision}

For our `{python} gpt3_params_b`B model, the decision depends on the phase of the model's lifecycle. During training, large batch sizes push the workload into the compute-bound regime, making peak TFLOPS the dominant metric. The H100 or B200, with their high Tensor Core throughput and fast NVLink for tensor parallelism, are the natural choices. During inference at low batch sizes, the same model becomes deeply memory-bound, and the relevant metric shifts to bandwidth per dollar. An A100 with adequate HBM bandwidth at a lower price point may deliver better cost-per-token than an H100 whose additional TFLOPS go unused.

The organizational dimension adds a further consideration. Research labs that modify model architectures weekly need the flexibility of the GPU's CUDA ecosystem, where custom kernels can be written and tested in hours. Production teams running a fixed Transformer architecture at scale for months may benefit from TPUs, where the XLA compiler's whole-program optimization can achieve higher sustained utilization than hand-tuned CUDA kernels for standard operations. Custom ASICs make economic sense only for organizations with enough scale to amortize the \$50--200 million NRE cost and enough workload stability to justify a 2--3 year design cycle. The accelerator spectrum is ultimately an economic question answered at the intersection of workload physics, organizational scale, and time horizon.

::: {.callout-checkpoint title="Accelerator Selection"}

Your team needs to deploy a 70B-parameter model for both training and inference. Training will use batch size 2048 across 256 GPUs for 3 months. Inference will serve 10,000 requests per second at batch size 1 for 2 years. Using the Roofline Model, determine the arithmetic intensity of each workload on an H100. Should you use the same hardware for both, or different hardware? Calculate the cost difference over the 2-year inference period between using H100s (\$4.00/GPU-hour) and A100s (\$2.00/GPU-hour), accounting for the throughput difference at the workload's arithmetic intensity.

:::

With the accelerator's physics established, we face a concrete problem. Our `{python} gpt3_params_b`B model requires 350 GB of memory for its weights alone in FP16, and training with Adam optimizer states roughly triples that requirement to over 1 TB. A single H100 provides 80 GB of HBM. No single accelerator can hold this model. We must expand to the next physical level: the node.

## The Node {#sec-compute-node}

\index{Node}
\index{NVLink}
\index{NVSwitch}

Our `{python} gpt3_params_b`B model needs 350 GB for its FP16 weights alone. With Adam optimizer states (which store first and second moments of every parameter), the total memory requirement for training exceeds 1 TB. A single H100 provides 80 GB of HBM. We need at least 5 accelerators just for the weights, and with optimizer state, all 8 in a dense node are barely sufficient. This arithmetic forces us beyond the single die to the next physical level: the **node**.

::: {.callout-definition title="Node"}

***Node***\index{Node!definition} is a single physical server chassis containing multiple accelerators (typically 8), a high-speed intra-node interconnect, and host memory.

1.  **Significance (Quantitative):** It is the fundamental **Failure Domain** and **Scheduling Unit** for the fleet. Within the **Iron Law**, the node defines the maximum **Intra-Node Bandwidth** ($BW_{intra}$) available for model parallelism before traffic must cross the slower network fabric.
2.  **Distinction (Durable):** Unlike a **Single Accelerator**, a Node provides the aggregate **Memory Capacity** required to hold the parameters and optimizer state of frontier models.
3.  **Common Pitfall:** A frequent misconception is that nodes are independent. In reality, for large training jobs, nodes are **Synchronously Coupled**: a failure or performance drop in one node can stall the entire multi-node cluster.

:::

The node exists because of a gap in the memory hierarchy. HBM provides enough bandwidth to feed the accelerator's arithmetic units, but not enough *capacity* to hold a frontier model. Host DRAM provides capacity (512 GB to 2 TB per server) but not enough bandwidth.

The node bridges this gap by aggregating the HBM of multiple accelerators into a shared pool, connected by an interconnect fast enough to allow cooperative computation across all of them. The key engineering insight is that by placing 8 accelerators in a single chassis and connecting them with a high-speed fabric, the node creates a virtual accelerator with 8$\times$ the memory capacity and 8$\times$ the compute throughput of any individual chip, while the fast interconnect ensures that this virtual accelerator can operate nearly as efficiently as a single monolithic device.

The economic argument for multi-accelerator nodes is equally compelling. Consider the alternative: building a single accelerator with enough HBM to hold a 175B model (350 GB in FP16). This would require approximately 4--5 HBM stacks at current capacities, with a total interposer area exceeding 2,000 mm$^2$. The manufacturing cost of such a package would be prohibitive (yield decreases exponentially with area), and the power delivery to a single chip with enough Tensor Cores to use all that bandwidth would exceed any practical cooling solution. By distributing the computation across 8 accelerators connected by NVLink, the node achieves the aggregate memory capacity and compute throughput of this hypothetical super-chip while remaining within manufacturable and coolable boundaries.

Understanding how the node partitions memory across its components is essential for selecting parallelism strategies. Consider the memory budget for training our `{python} gpt3_params_b`B model. The model weights in FP16 require 350 GB. The Adam optimizer maintains two additional copies of every parameter (first and second moments), adding another 700 GB in FP32 precision. Gradients require another 350 GB. The total is approximately 1.4 TB, far exceeding a single accelerator's 80 GB HBM but within reach of a node's aggregate capacity.

When using ZeRO optimization (which shards optimizer states, gradients, and optionally parameters across data-parallel workers), each GPU in an 8-GPU node holds only 1/8 of the optimizer state, reducing the per-GPU memory requirement to roughly 175 GB of equivalent storage. This still exceeds 80 GB per GPU, necessitating techniques like activation checkpointing (recomputing intermediate activations during the backward pass rather than storing them) and offloading optimizer states to host DRAM over PCIe.

The node's internal memory hierarchy, consisting of HBM (fast, small), host DRAM (medium speed, larger), and NVMe storage (slow, very large), forms a tiered system that distributed training frameworks exploit aggressively. @sec-distributed-training-systems examines these memory optimization strategies in detail.

### Bandwidth Hierarchy {#sec-compute-bandwidth-hierarchy}

\index{Bandwidth Hierarchy}

The defining characteristic of multi-accelerator systems is not the number of chips but the *speed at which they can exchange data*. Data movement speed drops by orders of magnitude as it crosses physical boundaries. Each boundary represents a different physical medium, a different connector technology, and a different set of engineering constraints. Understanding these boundaries is essential because they dictate *where* in the hierarchy each type of parallelism can operate efficiently.

The bandwidth hierarchy is best understood as a series of concentric zones around each accelerator. The innermost zone is the HBM on the same package, with terabytes per second of bandwidth and sub-microsecond latency. The next zone encompasses the other accelerators within the same node, reachable over NVLink at hundreds of gigabytes per second with microsecond-scale latency. The outermost zone spans all other nodes in the cluster, connected by InfiniBand at tens of gigabytes per second with latency measured in single-digit microseconds. At each zone boundary, the bandwidth drops by roughly an order of magnitude while the latency increases by roughly an order of magnitude.

| **Domain**          | **Interconnect**   |           **Bandwidth**            | **Latency** | **Scaling Limit**  |
|:--------------------|:-------------------|:----------------------------------:|:-----------:|:-------------------|
| **Intra-Package**   | Silicon Interposer |           **~3.3 TB/s**            |   <100 ns   | Single Chip        |
| **Intra-Node**      | NVLink / ICI       | **~`{python} nvlink_bw_str` GB/s** |    ~1 μs    | Node (8--16 Chips) |
| **Intra-Node (IO)** | PCIe Gen5 x16      |  **~`{python} pcie_bw_str` GB/s**  |    ~2 μs    | CPU-GPU, NIC-GPU   |
| **Inter-Node**      | InfiniBand NDR     |   **~`{python} ib_bw_gbs` GB/s**   |  ~5--10 μs  | Pod (Thousands)    |

: **The Bandwidth Hierarchy**. Each physical boundary introduces an order-of-magnitude bandwidth cliff. These cliffs are not engineering failures to be optimized away; they reflect fundamental differences in the physics of each interconnect medium. The cliffs dictate model partitioning: Tensor Parallelism, which requires AllReduce after every layer, is strictly confined to the intra-node domain. {#tbl-bandwidth-hierarchy-compute}

::: {.callout-definition title="The Bandwidth Hierarchy"}

***Bandwidth Hierarchy***\index{Bandwidth Hierarchy!definition} is the physical ordering of data transfer rates across system boundaries, from on-chip SRAM (fastest) to the wide-area network (slowest).

1.  **Significance (Quantitative):** It dictates the **Scaling Ceiling** for distributed training. At each physical boundary (die edge, package edge, chassis, rack), the **Effective Bandwidth ($BW$)** drops by approximately one order of magnitude while latency ($L_{lat}$) increases.
2.  **Distinction (Durable):** Unlike **Idealized Networking Models**, the Bandwidth Hierarchy captures the **Physical Cost of Distance**: shorter signal paths over denser wiring achieve higher throughput at lower energy per bit.
3.  **Common Pitfall:** A frequent misconception is that all cluster communication is equal. In reality, parallelism strategies must be **Hierarchy-Aware**: placing high-frequency synchronization (e.g., Tensor Parallelism) on a slow tier will rapidly idle the compute units ($R_{peak}$), collapsing system efficiency ($\eta$).

:::

*Why* does each cliff exist? The answer lies in the physics of signal propagation at each distance scale.

The **intra-package** interconnect achieves terabytes per second because signals travel through lithographically patterned copper traces on a silicon interposer, with path lengths measured in millimeters. At these distances, signal attenuation is negligible, no amplification is needed, and the signaling rate is limited only by the trace geometry and the transceiver design.

**NVLink** achieves hundreds of gigabytes per second using high-speed SerDes[^fn-serdes-signaling] transceivers over short copper cables or traces within a chassis, with path lengths of tens of centimeters. At these distances, signal attenuation is measurable but manageable with simple equalization circuits. The SerDes transceivers use PAM-4 (4-level pulse amplitude modulation) signaling at 112 Gbps per lane, packing 2 bits per symbol to double the data rate relative to NRZ (non-return-to-zero) signaling.

**PCIe** uses a standardized protocol with flow control overhead, reducing effective bandwidth. While PCIe Gen5 uses the same 32 GT/s signaling rate as NVLink's individual lanes, the protocol overhead (packet headers, flow control credits, error checking) consumes approximately 20% of the raw bandwidth. The standardization that makes PCIe universally compatible also makes it less efficient than proprietary interconnects.

**InfiniBand** crosses meters of cable between racks, requiring signal amplification, error correction, and switch hops that add both latency and protocol overhead. Active optical cables (AOCs) convert electrical signals to light at the transmitter, propagate through optical fiber, and convert back to electrical signals at the receiver. Each electro-optical conversion adds approximately 2--5 nanoseconds of latency and consumes power for the laser driver and photodetector. Switch hops add another 100--300 nanoseconds each for packet routing and buffering.

[^fn-serdes-signaling]: **SerDes (Serializer/Deserializer)**: From Latin *serialis* (in a row) and *de-* (reverse). A circuit pair that converts parallel data to a serial bit stream for transmission and back again. Modern SerDes in NVLink operate at 112 Gbps per lane using PAM-4 signaling (4-level pulse amplitude modulation, encoding 2 bits per symbol), but each lane consumes 5--7 W -- so the 72 lanes of a single NVLink 4.0 connection draw 360--500 W in SerDes power alone, a non-trivial fraction of the node's total power budget. \index{SerDes!signaling}

The practical consequence is that parallelism strategies must respect these boundaries. Tensor parallelism (TP), which splits individual matrix multiplications across accelerators and requires an AllReduce operation after every layer, generates communication volume proportional to the model's hidden dimension hundreds of times per second. At NVLink bandwidth, this synchronization takes roughly 1 ms per layer. At InfiniBand bandwidth, the same synchronization takes 10--20 ms, which would leave the accelerators idle for the majority of each training step.

This is why TP is confined to within a single node, while data parallelism (which synchronizes gradients only once per training step) spans the inter-node network. The bandwidth hierarchy is not just a table of specifications; it is the physical law that determines the topology of distributed training.

The hierarchy also explains why pipeline parallelism occupies an intermediate position in the bandwidth requirements. In pipeline parallelism, the model is divided into sequential stages, with each stage assigned to a different group of accelerators. The communication between stages consists of activations flowing forward during the forward pass and gradients flowing backward during the backward pass, with a volume proportional to the batch size times the hidden dimension.

For our `{python} gpt3_params_b`B model with hidden dimension 12,288 and a microbatch of 4 sequences at 2,048 tokens, the activation tensor at each stage boundary is approximately $4 \times 2{,}048 \times 12{,}288 \times 2$ bytes (FP16) $\approx$ 200 MB. This is far smaller than the 350 GB gradient AllReduce required by data parallelism, which is why pipeline parallelism places less demanding requirements on the inter-node network.

This communication occurs once per microbatch per stage boundary, which is far less frequent than tensor parallelism's per-layer AllReduce. Pipeline parallelism can therefore tolerate the lower bandwidth of inter-node links, making it the preferred strategy for spanning multiple nodes when the model's depth exceeds a single node's capacity.

The result is a natural mapping between parallelism types and interconnect domains: tensor parallelism within the node (NVLink), pipeline parallelism across nearby nodes (InfiniBand), and data parallelism across the full cluster (InfiniBand with gradient compression). This mapping is so fundamental that it has become a de facto standard in production training systems. Meta's LLaMA training, Google's PaLM training, and OpenAI's GPT-4 training all follow variations of this hierarchical parallelism assignment, with the details differing primarily in the number of pipeline stages and the degree of data parallelism.

The mapping also determines how the job scheduler assigns nodes to training jobs. Nodes within the same tensor-parallel group should be physically adjacent (ideally in the same chassis, connected via NVLink). Nodes within the same pipeline-parallel group should be in the same rack or adjacent racks (minimizing InfiniBand switch hops). Data-parallel groups can span the entire cluster because their communication (gradient AllReduce) is the least bandwidth-intensive and most latency-tolerant of the three parallelism types.

@sec-distributed-training-systems formalizes this mapping and examines the interactions between these parallelism strategies.

#### The PCIe Hierarchy Within a Node

While NVLink provides a high-bandwidth freeway for GPU-to-GPU communication, **PCIe Gen5** serves as the universal glue connecting the heterogeneous components of the node. It links GPUs to the host CPU, GPUs to InfiniBand Host Channel Adapters (HCAs), the host CPU to NVMe storage, and the host CPU to system DRAM. Understanding this topology is critical because PCIe bandwidth -- though substantial at ~64 GB/s bidirectional per x16 link -- is often the bottleneck for operations that cross the accelerator boundary, such as data loading, checkpointing, and inter-node gradient synchronization.

A standard DGX H100 node features two host CPUs, each managing a **PCIe root complex** with 128 lanes. These lanes are distributed to maximize simultaneous throughput: 8 GPUs each receive a dedicated x16 link, and 8 InfiniBand HCAs each receive their own x16 link. This configuration provides an aggregate theoretical bandwidth of nearly 2 TB/s within the chassis. However, this bandwidth is shared among competing traffic streams. During a single training step, the PCIe bus simultaneously carries host-to-GPU data batches, GPU-to-HCA gradient shards for inter-node AllReduce (via GPUDirect RDMA), and periodic CPU-to-NVMe checkpoint writes. Contention between these streams can cause unexpected pipeline stalls, particularly when gradient synchronization saturates the links typically used for data loading.

The dual-CPU architecture introduces significant **Non-Uniform Memory Access (NUMA)** effects. The 8 GPUs are physically partitioned, with 4 connected to CPU 0's root complex and 4 to CPU 1's. A GPU connected to CPU 0 can access CPU 0's DRAM at full PCIe bandwidth but must traverse the inter-processor interconnect (UPI or Infinity Fabric) to access CPU 1's DRAM. This traversal incurs additional latency and reduces effective bandwidth by up to 50%. Robust data loading pipelines must employ *NUMA-aware scheduling*, pinning data loader worker processes to the CPU cores physically closest to the target GPUs. Empirical benchmarks show that aligning data loaders with the correct NUMA domain can improve data ingestion throughput by 20--30%, preventing the CPU from becoming the bottleneck in high-throughput training runs.

### Dense Node Designs {#sec-compute-dense-nodes}

\index{DGX}
\index{Crossbar Switch}

Given the bandwidth hierarchy, the engineering challenge within a node is clear: connect 8 accelerators with enough bandwidth that they can function as a single logical device for tensor-parallel operations. The solution is a specialized switching fabric that provides full bisection bandwidth between all accelerator pairs. Understanding the design of this fabric requires appreciating why simpler alternatives fail.

The simplest approach would be to connect the 8 GPUs in a ring, where each GPU has links to its two neighbors. A ring is inexpensive (requiring only $N$ links for $N$ GPUs) and works well for ring-AllReduce, where data flows sequentially around the ring in a circular pattern. The ring-AllReduce algorithm achieves optimal bandwidth utilization for gradient synchronization because each GPU simultaneously sends data to its right neighbor and receives data from its left neighbor, keeping all links busy throughout the operation.

However, tensor parallelism requires *all-to-all* communication: every GPU must exchange partial results with every other GPU after each layer. This is a fundamentally different communication pattern from the sequential flow of ring-AllReduce, and the ring topology handles it poorly.

In a ring of 8 GPUs, communicating between opposite sides requires 4 hops, reducing effective bandwidth by 4$\times$ compared to a direct link. A fully connected mesh (where every GPU has a direct link to every other GPU) eliminates this problem but requires $N \times (N-1) / 2 = 28$ links for 8 GPUs, a wiring challenge that becomes impractical as the link count grows quadratically.

The NVSwitch crossbar provides the best of both worlds: full bisection bandwidth with a manageable number of switch chips.

Consider the NVIDIA DGX H100 architecture. Eight H100 GPUs sit on a single baseboard, each connected to four NVSwitch chips via 18 NVLink 4.0 lanes. The NVSwitch chips form a non-blocking crossbar[^fn-crossbar-nvswitch]: any GPU can communicate with any other GPU at the full `{python} nvlink_bw_str` GB/s bidirectional bandwidth simultaneously, without contention. This is the equivalent of giving every pair of GPUs their own private highway, rather than forcing them to share a single road.

[^fn-crossbar-nvswitch]: **Crossbar Switch**: A topology providing a dedicated path between every input-output pair simultaneously. An $N \times N$ crossbar has $N^2$ crosspoints, achieving full bisection bandwidth. NVSwitch implements a 3-stage crossbar using four switch chips (each with 64 NVLink ports), consuming ~800 W total -- more than an entire V100 GPU -- but this power cost is justified because it enables all-to-all tensor parallelism within a node, the communication pattern that would otherwise dominate training time. \index{Crossbar Switch!NVSwitch}

The result is that the 8 GPUs within a DGX H100 can be treated as a single logical device with `{python} node_hbm_cap` GB of aggregate HBM and a combined compute throughput of 8$\times$ `{python} h100_tflops` TFLOPS. This abstraction is critical for software: the training framework can partition a model across the 8 GPUs using tensor parallelism as if they were a single larger processor, with the NVSwitch fabric making the partitioning nearly transparent from a performance perspective.

The physical layout of the DGX H100 baseboard reflects this design goal. The 8 GPUs are arranged on the baseboard with the 4 NVSwitch chips positioned centrally. Each GPU connects to all 4 NVSwitch chips via NVLink lanes, and each NVSwitch chip has 64 NVLink ports that cross-connect the GPUs.

The NVSwitch chips themselves consume approximately 200 W each (800 W total for the NVSwitch fabric), a non-trivial power overhead that is justified by the bisection bandwidth it provides. To put this in perspective, the NVSwitch fabric alone consumes more power than an entire previous-generation GPU (the V100 drew 300 W). Without NVSwitch, achieving full all-to-all connectivity would require 28 direct GPU-to-GPU links, a wiring nightmare that would also consume more total NVLink lane capacity.

During a tensor-parallel forward pass, each GPU holds one shard of each weight matrix and computes its portion of the matrix multiplication. For a Transformer layer with hidden dimension 12,288 (typical for a 175B model), each of the 8 GPUs holds a 12,288$\times$ 1,536 slice of the weight matrix. After each GPU computes its partial result, an AllReduce operation sums the partial results across all 8 GPUs over NVLink.

The AllReduce for a single layer's output involves exchanging approximately $2 \times \text{batch\_size} \times 12{,}288 \times 2$ bytes (FP16) across the NVLink fabric. For a typical microbatch size of 4 sequences, this is roughly 200 KB per AllReduce, which completes in under 1 microsecond at `{python} nvlink_bw_str` GB/s.

Even with 96 layers (forward pass) and 96 layers (backward pass), the cumulative AllReduce time is well under 1 ms per training step. This is possible because the NVSwitch crossbar allows all 8 GPUs to communicate simultaneously without contention, unlike a ring topology where data must traverse multiple hops sequentially.

This communication overhead is small relative to the compute time per layer, which is approximately 2--5 ms depending on the sequence length and batch size. The resulting tensor-parallel scaling efficiency within the node is typically 85--95%, meaning that using 8 GPUs achieves 6.8--7.6$\times$ the throughput of a single GPU.

The remaining 5--15% efficiency loss comes from two sources. First, the AllReduce communication itself, even at NVLink bandwidth, consumes a fraction of each layer's execution time. Second, some operations in the Transformer (layer normalization, dropout, activation functions) are not tensor-parallelized because their computation is small relative to the matrix multiplications. These sequential operations must complete on every GPU before the next layer's tensor-parallel computation can begin, introducing small but cumulative idle time.

::: {.callout-war-story title="The NVLink Bandwidth Surprise"}

Early users of multi-GPU nodes often attempted tensor parallelism over PCIe, expecting the `{python} pcie_bw_str` GB/s bandwidth to suffice. In practice, tensor parallelism requires AllReduce after *every* Transformer layer, not just once per training step. A model with 96 layers generates 96 AllReduce operations per forward pass and another 96 during the backward pass, totaling 192 synchronization events per step. At PCIe bandwidth, each AllReduce for a 12,288-dimensional hidden state takes approximately 4 ms, accumulating to 768 ms of pure communication overhead per step. The compute per step is roughly 200 ms. The accelerators were spending nearly 80% of their time waiting for PCIe transfers. Switching to NVLink reduced each AllReduce to 0.2 ms, bringing total communication overhead to 38 ms and restoring accelerator utilization to over 80%. The lesson: for tensor parallelism, the interconnect *is* the bottleneck, and the difference between PCIe and NVLink is not incremental; it is the difference between a functional and a non-functional system.

:::

Beyond NVLink, the node also connects to the external network via InfiniBand Host Channel Adapters (HCAs) and to host storage via PCIe. The DGX H100 includes eight InfiniBand ConnectX-7 HCAs, one per GPU, enabling **GPUDirect RDMA**: the network adapter can read from and write to GPU HBM directly, without staging data through host DRAM or involving the host CPU in the data path.

This capability is critical for inter-node gradient synchronization, where each GPU's gradient shard must be sent to its peers on other nodes. Without GPUDirect, each gradient transfer would require two extra copies: first from GPU HBM to host DRAM (over PCIe), and then from host DRAM to the network adapter (over another PCIe path). This double-copy adds latency (two PCIe traversals instead of one direct DMA operation) and halves the effective bandwidth (each byte traverses the PCIe bus twice).

GPUDirect RDMA eliminates both copies by allowing the InfiniBand HCA to read directly from GPU HBM, bypassing the host CPU and host DRAM entirely. The data path goes from GPU HBM through the NVLink-to-PCIe bridge to the InfiniBand HCA in a single transfer. This optimization is one of the reasons that modern training clusters achieve inter-node communication bandwidth close to the raw InfiniBand line rate.

The host CPU manages job scheduling, data preprocessing, data loading from storage, and network protocol processing, but it sits outside the critical path of the GPU-to-GPU computation. This separation is deliberate and reflects a fundamental architectural principle: the host handles the "slow" operations (disk I/O, network management, job coordination) while the GPU-NVSwitch fabric handles the "fast" operations (matrix math, gradient synchronization).

The host CPU's role is analogous to an operating system kernel in a traditional computer: it manages resources, handles exceptions, and coordinates I/O, but it does not perform the application's core computation. In a well-optimized training loop, the host CPU is almost never on the critical path, and its performance specifications (core count, clock speed) are less important than its I/O capabilities (PCIe lane count, memory channels, NVMe controller bandwidth).

The host CPU also runs the training framework's Python runtime, which orchestrates kernel launches, manages the computation graph, and coordinates collective communication operations. In well-optimized training loops, the host CPU is pipelining the next batch's data loading and preprocessing while the GPUs are executing the current batch's forward and backward passes, keeping all components busy simultaneously.

As the "datacenter tax" on host CPUs has grown -- with up to 30% of CPU cycles consumed by network protocol processing, storage virtualization, and security functions -- modern ML nodes increasingly incorporate **Data Processing Units (DPUs)** or SmartNICs. Devices like the NVIDIA BlueField DPU offload these infrastructure tasks to dedicated ARM cores and hardware accelerators integrated into the network adapter itself. By moving the control plane for RDMA, firewall rules, and storage protocols to the DPU, the host CPU recovers nearly all its cycles for the ML pipeline: data loading, tokenization, and kernel orchestration. The DPU also acts as an isolated security domain, allowing cloud providers to maintain control over the network and storage layer via the DPU while granting customers full access to the host CPU and GPUs. For our `{python} gpt3_params_b`B model training, the DPU's offloading of RDMA protocol processing ensures that the host CPU is never on the critical path for gradient synchronization, which flows directly from GPU HBM through the DPU to the InfiniBand fabric without host CPU involvement.

#### Alternative Node Architectures

The DGX architecture is not the only approach to building dense multi-accelerator nodes. Google's TPU v4 and v5p pods use a different philosophy: instead of connecting chips through a central switch (NVSwitch), each TPU chip has direct links to its neighbors through the Inter-Chip Interconnect (ICI). Within a "tray" (Google's equivalent of a node), four TPU chips are connected in a small mesh, with each chip directly connected to the others via ICI. Multiple trays are then connected into larger topologies at the pod level. This eliminates the switch as a separate component, reducing cost and latency at the expense of the universal any-to-any connectivity that NVSwitch provides.

AMD's MI300X represents a particularly interesting alternative. The MI300X is a chiplet-based design that integrates 8 GPU compute dies (XCDs) and 4 I/O dies on a single package, with 192 GB of HBM3 memory providing 5.3 TB/s of aggregate bandwidth. The intra-package communication between XCDs uses AMD's Infinity Fabric, achieving bandwidth comparable to NVLink within the package. For multi-accelerator nodes, AMD uses Infinity Fabric links between packages, similar in concept to NVLink but with different bandwidth characteristics.

The MI300X's 192 GB of HBM per accelerator is particularly notable. While the H100 offers 80 GB, the MI300X's larger capacity means that a single 8-GPU node can hold 1,536 GB of HBM, enough to fit the full weight tensor of our `{python} gpt3_params_b`B model (350 GB in FP16) with substantial room for optimizer state, gradients, and activations. This capacity advantage can reduce the number of nodes needed for training (fewer data-parallel groups means less inter-node communication) or enable larger batch sizes (more activation memory available), both of which improve scaling efficiency.

Intel's Gaudi architecture represents yet another approach: Gaudi 2 and Gaudi 3 accelerators use an integrated RDMA-capable Ethernet NIC (RoCE) built directly into each accelerator chip, combined with high-bandwidth on-die interconnects. This allows Gaudi to bypass the need for both NVSwitch and separate InfiniBand HCAs, using a single network fabric for both intra-node and inter-node communication. The trade-off is that the intra-node bandwidth is lower than NVLink (roughly 300 GB/s vs. 900 GB/s), limiting the efficiency of tensor parallelism within a node.

These alternative architectures illustrate a recurring theme in infrastructure design: there is no single optimal architecture, only optimal architectures *for specific workload and scale regimes*. The DGX approach maximizes intra-node bandwidth at higher cost. The TPU approach eliminates switches but constrains topology. The Gaudi approach simplifies the network at the expense of intra-node bandwidth. The AMD approach prioritizes HBM capacity per accelerator, reducing the need for complex multi-node parallelism.

Each approach reflects a different bet about which constraint will be most binding for the target workload. NVIDIA bets on bandwidth (NVLink provides the fastest intra-node fabric). Google bets on systolic efficiency (the MXU achieves higher utilization for regular workloads). AMD bets on memory capacity (192 GB HBM reduces the need for complex memory optimization). Intel bets on network simplicity (integrated Ethernet eliminates the need for separate InfiniBand infrastructure).

For our `{python} gpt3_params_b`B model, the choice matters primarily for the tensor-parallel portion of the training computation, where the accelerator interconnect is on the critical path. But the secondary effects (software ecosystem maturity, supply chain availability, total cost of ownership) often dominate the primary performance differences in practice.

#### Node Health and Reliability

The node is the fundamental failure domain in the fleet hierarchy. When a single GPU fails within a node, the entire node typically becomes unusable for the training job, because tensor parallelism requires all 8 GPUs to participate in every AllReduce operation. A single missing GPU breaks the collective communication, stalling all other GPUs in the node. This failure coupling means that the node's effective MTBF is determined by the *least reliable* component, not the average.

The components most prone to failure within a node are, in order of frequency: GPU memory (ECC-uncorrectable HBM errors), NVLink connections (signal integrity degradation over time), power supplies (capacitor aging under sustained high-load operation), and cooling components (pump failures in liquid-cooled systems, fan bearing failures in air-cooled systems).

A well-managed fleet tracks the error rates of each component and preemptively migrates workloads away from nodes showing early warning signs. The most important early warning indicators are increasing corrected ECC error rates (which often precede uncorrectable errors by hours to days), rising junction temperatures at constant workload (indicating cooling degradation), and intermittent NVLink retraining events (indicating cable or connector degradation). Proactive replacement of components showing these warning signs can prevent the much more costly outcome of an unplanned job failure during a multi-week training run.

Node-level health monitoring is therefore a critical operational practice. Modern fleet management systems continuously collect telemetry from each GPU (temperature, power draw, ECC error counts, NVLink error rates) and from the host BMC (baseboard management controller). Automated health checkers run short diagnostic workloads on idle nodes to verify that all GPUs, NVLinks, and InfiniBand connections are functioning correctly before the job scheduler assigns training work to that node. Without this proactive monitoring, a silently degraded node can corrupt training gradients (if the error is in the arithmetic path) or slow the entire job (if the error causes NVLink retraining, which temporarily reduces bandwidth). @sec-fault-tolerance-reliability discusses fleet-level fault tolerance strategies in detail.

For our `{python} gpt3_params_b`B model training across 128 nodes, the probability of at least one node experiencing a hardware issue during a two-week training run is substantial. If each node has an MTBF of 1,000 hours (a reasonable estimate for a DGX H100 under sustained load), the expected number of node failures during a 336-hour training run across 128 nodes is $128 \times 336 / 1{,}000 \approx 43$ failures. This means the training run will experience, on average, roughly three node failures per day. Each failure requires detecting the degraded node, draining its workload, substituting a spare node, and resuming from the last checkpoint -- a process that takes 10--30 minutes with automated tooling. Without spare nodes and automated recovery, these failures would extend the training run by 20--30%, consuming millions of dollars in wasted GPU-hours.

### Node Memory Partitioning {#sec-compute-node-memory}

Understanding how a large model's memory footprint maps onto a node's physical resources is essential for capacity planning. The memory required to train a model far exceeds the weight storage, and different components of the training state reside in different tiers of the node's memory hierarchy.

For our `{python} gpt3_params_b`B model, the training memory breaks down as follows:

| **Component**            | **Precision** | **Memory per Parameter** | **Total for `{python} gpt3_params_b`B Model** |
|:-------------------------|:-------------:|:------------------------:|:---------------------------------------------:|
| **Model Weights**        |      FP16     |         2 bytes          |                     350 GB                    |
| **Gradients**            |      FP16     |         2 bytes          |                     350 GB                    |
| **Optimizer (Adam $m$)** |      FP32     |         4 bytes          |                     700 GB                    |
| **Optimizer (Adam $v$)** |      FP32     |         4 bytes          |                     700 GB                    |
| **Activations**          |     Mixed     |         Variable         |                  100--400 GB                  |
| **Total**                |               |                          |                **2.2--2.5 TB**                |

: **Training Memory Breakdown for a `{python} gpt3_params_b`B-Parameter Model**. The optimizer state alone (first and second moments in FP32) consumes 4$\times$ the memory of the model weights, making optimizer memory the dominant component of training memory. This is why memory optimization techniques focus heavily on optimizer state sharding and offloading. {#tbl-memory-breakdown}

The model weights in FP16 occupy 350 GB ($`{python} gpt3_params_b` \times 10^9 \times 2$ bytes). The Adam optimizer maintains two FP32 state tensors per parameter (first moment $m$ and second moment $v$), adding $`{python} gpt3_params_b` \times 10^9 \times 4 \times 2 = 1{,}400$ GB. The gradients in FP16 add another 350 GB. Intermediate activations (needed for the backward pass) depend on the batch size and sequence length but typically require 100--400 GB for practical training configurations. The total memory footprint for training is therefore 2.2--2.5 TB, roughly 6--7$\times$ the raw weight size.

A DGX H100 node provides `{python} node_hbm_cap` GB of aggregate HBM across its 8 GPUs, plus 2 TB of host DDR5 DRAM. The HBM capacity alone is insufficient for the full training state. To appreciate why, consider the memory arithmetic for different parallelism strategies. With **pure data parallelism**, each GPU must hold the complete model: 350 GB weights + 350 GB gradients + 700 GB optimizer states = 1,400 GB -- physically impossible on an 80 GB device. Even **ZeRO Stage 3**, which shards all three components across $N$ GPUs, yields 1,400/8 = 175 GB per GPU with 8-way sharding, still more than double the available HBM. The solution is to combine **tensor parallelism** (which splits the model's layers across GPUs, reducing per-GPU weights to 350/8 = 43.75 GB) with **ZeRO-1** (which shards only the optimizer state across the data-parallel dimension). In a configuration with TP-8 within the node and 128-way data parallelism across the cluster, each GPU holds 43.75 GB of weight shards plus approximately 700/128 = 5.5 GB of optimizer state, totaling roughly 50 GB -- comfortably within the 80 GB HBM envelope with 30 GB remaining for activations and temporary buffers. This arithmetic is why the combination of tensor parallelism and optimizer sharding has become the standard approach for frontier model training. Several additional strategies further optimize this memory budget:

- **ZeRO Stage 3 (Full Sharding)**: The optimizer states, gradients, and parameters are sharded across all GPUs participating in data parallelism. With 8-way data parallelism within a node, each GPU holds only 1/8 of each component, reducing per-GPU memory to roughly 275 GB of equivalent storage. This still exceeds 80 GB per GPU, requiring the combination with other techniques.
- **Activation Checkpointing**: Instead of storing all intermediate activations for the backward pass, only a subset of "checkpoint" activations are stored. The remaining activations are recomputed from the nearest checkpoint during the backward pass. This trades compute (roughly 33% more FLOPs) for memory (up to 10$\times$ reduction in activation memory).
- **CPU Offloading**: Optimizer states, which are accessed only during the parameter update step (once per training step), can be stored in host DDR5 DRAM and transferred to GPU HBM only when needed. The PCIe Gen5 link at `{python} pcie_bw_str` GB/s introduces a transfer delay, but since the parameter update is a small fraction of total step time, the overhead is manageable.
- **NVMe Offloading**: For even larger models, optimizer states can be stored on NVMe SSDs (with sequential read bandwidth of 5--7 GB/s), though the latency penalty is more significant and requires careful pipelining to overlap with computation.

The node's memory hierarchy thus operates as a tiered system. HBM holds the active computation: weight shards that are needed for the current layer's matrix multiplication, the current microbatch's activations, and the gradients being accumulated during the backward pass. DDR5 holds the bulk optimizer state, which is accessed only during the parameter update step at the end of each training iteration. NVMe provides overflow capacity for the largest models, storing seldom-accessed shards that can be prefetched during computation.

The training framework's memory manager orchestrates the flow of data between these tiers, operating much like a hardware cache controller but at a coarser granularity. The analogy to hardware caching is instructive: just as a CPU's cache controller uses prefetching to hide memory latency by loading data before the processor needs it, the training framework's memory manager uses software-level prefetching to hide the PCIe transfer latency by loading weights before the GPU needs them.

During the forward pass of layer $l$, the manager simultaneously prefetches the weights for layer $l+1$ from DDR5 to HBM (if they have been offloaded) and evicts the weights for layer $l-2$ from HBM back to DDR5 (if memory is tight).

This pipelining ensures that each layer's computation can proceed without waiting for data transfers, at the cost of increased software complexity and careful tuning of the prefetch depth. If the prefetch is too shallow, the computation stalls waiting for data. If the prefetch is too aggressive, HBM fills up with data that will not be used for several layers, crowding out the activations and gradients needed for the current computation.

The following table summarizes the memory tiers available within a DGX H100 node:

| **Memory Tier** |       **Capacity**       |   **Bandwidth**   | **Latency** | **Role**              |
|:----------------|:------------------------:|:-----------------:|:-----------:|:----------------------|
| **GPU HBM3**    | 80 GB$\times$ 8 = 640 GB | 3.35 TB/s per GPU |    <1 μs    | Active computation    |
| **Host DDR5**   |           2 TB           |      ~50 GB/s     |   ~100 ns   | Optimizer state       |
| **NVMe SSD**    |         8--30 TB         |     5--7 GB/s     |    ~10 μs   | Overflow, checkpoints |

: **Node Memory Hierarchy**. Each tier trades capacity for bandwidth. The training framework's memory manager must orchestrate data flow across these tiers to fit models whose total state exceeds the aggregate HBM capacity. {#tbl-node-memory}

This orchestration is complex but essential: without it, training our `{python} gpt3_params_b`B model on currently available hardware would be impossible. @sec-distributed-training-systems examines these memory optimization strategies and their interaction with parallelism in full detail.

#### Putting It Together: Memory Budget Exercise

To solidify the memory planning concepts, consider a concrete sizing exercise for our `{python} gpt3_params_b`B model on a DGX H100 node.

::: {.callout-example title="Memory Budget for 175B Training on DGX H100"}

**Given**:

- Model: `{python} gpt3_params_b`B parameters
- Node: 8$\times$ H100 GPUs, 80 GB HBM each (640 GB total HBM)
- Host: 2 TB DDR5
- Parallelism: 8-way tensor parallelism within the node

**Step 1: Weight Distribution**

With 8-way TP, each GPU holds 1/8 of every weight tensor.
Per-GPU weight memory: 350 GB / 8 = 43.75 GB (FP16).

**Step 2: Optimizer State with ZeRO Stage 1**

ZeRO Stage 1 shards the optimizer states across data-parallel workers. Within a single node using only TP (no data parallelism), the optimizer is *not* sharded. Each GPU stores the full optimizer state for its TP shard: 43.75 GB$\times$ 4 (FP32 $m$ and $v$) = 175 GB.

This exceeds the 80 GB HBM capacity. **Solution**: offload optimizer states to host DDR5. The 2 TB of DDR5 can hold the full 1,400 GB of optimizer state with room to spare.

**Step 3: Activations**

With activation checkpointing (recomputing every other layer), the activation memory for a microbatch of 4 sequences at 2,048 token length is approximately 15--25 GB per GPU. Without checkpointing, it would be 100--200 GB per GPU, which is impossible.

**Step 4: Gradient Accumulation**

Gradients match the weight size: 43.75 GB per GPU in FP16.

**Total per-GPU HBM usage**:

- Weights: 43.75 GB
- Activations: ~20 GB (with checkpointing)
- Gradients: 43.75 GB
- Buffers and fragmentation: ~5 GB
- **Total: ~113 GB** (exceeds 80 GB HBM)

**Resolution**: Use gradient accumulation (accumulate over 2--4 microbatches before AllReduce, reducing peak activation memory) and gradient offloading (store inactive gradient shards in DDR5). With these techniques, the per-GPU HBM usage drops to approximately 70--75 GB, fitting within the 80 GB envelope with a small margin for NCCL communication buffers.

:::

This example illustrates why memory planning for frontier models is a careful engineering exercise rather than a simple capacity calculation. Every component of the training state must be assigned to a specific memory tier (HBM, DDR5, or NVMe), and the timing of transfers between tiers must be coordinated with the computation schedule to avoid stalls.

::: {#nb-comm-overhead .callout-notebook title="The Cost of Crossing the Cliff"}

**Scenario**: Synchronizing a 1 GB gradient buffer during a training step.

1. **Intra-Node (NVLink)**: Bandwidth = `{python} nvlink_bw_str` GB/s.
   `{python} t_intra_math`
2. **Inter-Node (InfiniBand NDR)**: Bandwidth = `{python} ib_bw_gbs` GB/s.
   `{python} t_inter_math`

**Conclusion**: Crossing the node boundary increases communication time by approximately 18$\times$. In a training loop where gradient synchronization happens at every step, this penalty would stall the accelerators for the majority of each iteration, reducing utilization to well below 50%. This is why training frameworks use tensor parallelism within the node (fast NVLink) and data parallelism across nodes (tolerant of slower InfiniBand).

:::

### Data Loading and I/O Pipeline {#sec-compute-data-loading}

\index{Data Loading}
\index{I/O Pipeline}

The discussion so far has focused on how data moves *within* the node (HBM to Tensor Core, GPU to GPU via NVLink, GPU to host via PCIe). But training also requires feeding the node with a continuous stream of training data from external storage. If the data pipeline cannot keep up with the GPU's consumption rate, the most expensive component in the system (the GPU) sits idle waiting for data. This is the **I/O bottleneck**, and avoiding it requires careful engineering of the data loading pipeline.

For language model training, the data consumption rate is determined by the training throughput. If the cluster processes 500,000 tokens per second and each token requires 2 bytes of input data (token ID), the raw data ingestion rate is only 1 MB/s, which is trivially handled by any storage system. However, the actual I/O demand is much larger because training data must be tokenized, shuffled, batched, and transferred to GPU memory. A realistic data pipeline includes:

1. **Reading**: Raw text is read from a distributed file system (NFS, Lustre, or cloud object storage like S3) into host memory. For large datasets (terabytes of text), the data is typically stored in a binary format (TFRecord, WebDataset, or memory-mapped files) to minimize parsing overhead.

2. **Preprocessing**: Tokenization, sequence packing, and random cropping are performed on the host CPU. These operations are parallelized across multiple CPU cores using data loader workers (typically 4--8 workers per GPU).

3. **Batching**: Individual sequences are assembled into microbatches, padded to uniform length, and organized into tensors.

4. **Transfer**: The assembled batch is transferred from host DRAM to GPU HBM over PCIe. For a microbatch of 4 sequences at 2,048 tokens with FP16 embeddings, this transfer is approximately 100--200 MB, completing in 3--6 ms at PCIe Gen5 bandwidth.

5. **Prefetching**: While the GPU processes the current batch, the data loader prefetches and preprocesses the next 2--4 batches in parallel, ensuring that the next batch is already in GPU HBM when the current computation completes.

The critical design goal is to make the data pipeline *invisible* to the GPU: the prefetching must be deep enough that a batch is always ready when the GPU needs it, regardless of transient variations in storage bandwidth or preprocessing time. When this pipeline is well-tuned, the GPU utilization is limited only by compute and communication, not by data loading. When it is poorly tuned, the GPU can spend 10--30% of its time waiting for data, which directly reduces training throughput.

The traditional data path routes every byte through the host CPU: storage controller to host DRAM (one PCIe traversal), then host DRAM to GPU HBM (a second PCIe traversal). This "bounce buffer" architecture doubles the traffic on the PCIe root complex and makes the CPU a bottleneck for I/O-intensive workloads, capping effective bandwidth at 3--4 GB/s per NVMe drive due to kernel context switching and interrupt handling overhead. **GPUDirect Storage (GDS)** eliminates this inefficiency by establishing a direct DMA path between the NVMe controller and GPU memory, bypassing the host CPU entirely. A single NVMe drive can deliver 5--7 GB/s directly to GPU HBM through GDS, saturating the drive's internal bandwidth rather than the host's I/O subsystem. For a standard node equipped with four NVMe drives, GDS unlocks an aggregate storage-to-GPU bandwidth of 20--28 GB/s, freeing the CPU to focus exclusively on preprocessing tasks like tokenization and sequence packing. @sec-data-storage examines GDS and the broader storage hierarchy in detail.

The required prefetch depth is not arbitrary but follows from the statistics of I/O latency variance. If the GPU processes one batch every $T_{compute}$ seconds and the storage system delivers a batch every $T_{io}$ seconds with standard deviation $\sigma_{io}$, the minimum prefetch depth $k$ required to maintain a 99.7% probability of zero stalls is $k \ge \lceil T_{io} / T_{compute} + 3\sigma_{io} / T_{compute} \rceil$. For our 175B model training run where $T_{compute} = 2.0$ seconds and the storage layer delivers batches at $T_{io} = 1.5 \pm 0.5$ seconds, the required depth is $\lceil 0.75 + 0.75 \rceil = 2$ batches. In production environments where "noisy neighbors" on shared file systems induce heavy tail latencies, engineers typically over-provision this buffer to 4--8 batches to insulate the GPU from the erratic physics of distributed storage. The memory cost of this prefetch buffer (each batch may occupy 100--200 MB of host DRAM) is negligible compared to the cost of a GPU stall.

As cluster sizes expand, a new bottleneck emerges at the storage layer: the **I/O Wall**. When training our `{python} gpt3_params_b`B model across 128 nodes, the system acts as a synchronized "thundering herd" -- all 128 nodes simultaneously demand the next microbatch of tokens at the start of every training step. A shared parallel filesystem like Lustre, even with 500 GB/s of aggregate throughput, will buckle when 128 clients simultaneously pull data, causing read latencies to spike from milliseconds to seconds. The architectural solution is **tiered storage** with aggressive local caching. By provisioning each training node with local NVMe SSDs capable of delivering 25 GB/s per node, the cluster decouples its immediate data dependency from the shared filesystem. A background process prefetches data from the central store to the local NVMe cache asynchronously, smoothing out I/O spikes. For the 128-node cluster, local NVMe creates an aggregate read bandwidth of 3.2 TB/s (128$\times$ 25 GB/s), eclipsing the capability of even the most expensive centralized storage arrays and ensuring the GPUs are never starved.

::: {.callout-warning title="The Data Loading Trap"}

A subtle failure mode occurs when data loading *appears* fast in benchmarks but degrades under production conditions. Single-node benchmarks often read data from local NVMe SSDs, achieving 5--7 GB/s read bandwidth. In production, where data resides on a shared storage system accessed by hundreds of nodes simultaneously, the effective per-node bandwidth may drop to 100--500 MB/s due to network congestion and storage controller contention. Teams that design their data pipelines based on single-node benchmarks may discover that their GPUs are I/O-starved in production. @sec-data-storage examines the storage architecture strategies that address this bottleneck, including the tiered storage hierarchies that ensure training data reaches GPUs without stalling.

:::

For our `{python} gpt3_params_b`B model, a single DGX H100 node provides `{python} node_hbm_cap` GB of aggregate HBM, enough to hold the model weights and a portion of the optimizer state. But training requires processing trillions of tokens, and a single node's compute throughput limits training time to months. To complete training in a reasonable timeframe (weeks rather than months), we need tens or hundreds of nodes. Stacking those nodes into a physical enclosure brings us to the next level of infrastructure, where the constraints shift from bandwidth and capacity to raw power and heat.

## The Rack {#sec-compute-rack}

\index{Rack}
\index{Power Delivery}
\index{Cooling}

A standard 42U server rack in a traditional datacenter draws 5--10 kW and can be cooled by room-temperature air pushed through perforated floor tiles. Now place four DGX H100 nodes in that same rack: 32 GPUs, each drawing 700 W, plus host CPUs, memory, networking, and power conversion losses. The rack power reaches `{python} rack_power_str` kW, an order of magnitude beyond what traditional datacenter infrastructure was designed to deliver or cool. At this density, the engineering constraints shift from silicon and signal integrity to power delivery and thermodynamics. The rack is where the Power Wall and the laws of heat transfer become the dominant design forces.

For our `{python} gpt3_params_b`B model, training across 1,024 GPUs requires approximately 128 racks. Each rack dissipates `{python} rack_power_str` kW as heat -- the thermal output of a small industrial furnace. The aggregate power draw of the training cluster approaches 5 MW, enough to power a small town. Delivering this power reliably, converting it efficiently, and removing the resulting heat without allowing any component to exceed its thermal limit is a multi-disciplinary engineering challenge that spans electrical, mechanical, and civil engineering. A failure at any point in the power delivery chain -- from the utility substation to the individual GPU voltage regulator -- can halt the entire training run, wasting hours of computation and potentially corrupting the training state.

::: {.callout-definition title="Rack"}

***Rack***\index{Rack!definition} is the physical and electrical unit of aggregation that houses multiple compute nodes, network switches, and power distribution units.

1.  **Significance (Quantitative):** It defines the **Power and Cooling Boundary** for the fleet. Within the **Iron Law**, the rack limits the maximum **Peak Performance ($R_{peak}$)** of the nodes it houses based on the available kilowatts per rack (typically 40–100 kW for AI workloads).
2.  **Distinction (Durable):** Unlike a **Node** (the compute unit), a Rack is the **Infrastructure Unit**: it is the level at which liquid cooling manifolds and Top-of-Rack (ToR) network switches are integrated.
3.  **Common Pitfall:** A frequent misconception is that rack placement does not matter. In reality, **Rack Locality** is critical for performance: nodes within the same rack often have significantly higher bandwidth and lower latency ($L_{lat}$) than nodes in different rows of the datacenter.

:::

### Power Delivery {#sec-compute-power-delivery}

\index{Power Distribution Unit}
\index{UPS}

Electricity flows through a chain of transformations before it reaches a GPU's voltage regulators, and understanding this chain is essential for capacity planning. The chain has five stages, each with its own conversion technology, efficiency, and failure modes.

**Stage 1: Grid Connection.** Utility power arrives at the datacenter as high-voltage AC, typically 13.8--69 kV depending on the country and the size of the facility. A dedicated substation or transformer yard steps this down to medium voltage (480 V AC in North America, 400 V AC in Europe). The largest ML facilities require their own substation, which takes 18--24 months to build and requires coordination with the local utility. The grid connection is the ultimate bottleneck: no amount of engineering inside the building can deliver more power than the grid provides.

**Stage 2: UPS and Power Conditioning.** An Uninterruptible Power Supply (UPS) sits between the utility feed and the IT equipment. The UPS serves two functions: it conditions the incoming power (removing voltage fluctuations and frequency variations) and provides battery backup during brief outages. Modern online (double-conversion) UPS systems convert AC to DC, charge a battery bank, and then convert back to AC, ensuring clean power but losing 3--5% efficiency. Newer high-efficiency "eco-mode" UPS designs bypass the double conversion during normal operation, achieving 98--99% efficiency but providing slightly less protection against input power anomalies.

**Stage 3: Power Distribution Unit (PDU).** The PDU distributes power from the UPS to individual racks. In traditional datacenters, the PDU provides the final AC-to-AC step-down (from 480 V to 208/240 V for servers).

Some modern ML facilities use **48 V DC distribution**, which eliminates one conversion stage and improves efficiency by 2--3%. The principle is straightforward: each AC-to-DC or DC-to-DC conversion stage loses 1--5% of the power as heat. By reducing the number of conversion stages, 48 V DC distribution directly increases the fraction of utility power that reaches the GPU. Google pioneered 48 V DC distribution in their datacenters, and the Open Compute Project (OCP) has standardized rack-level 48 V DC power buses for high-density compute.

At ML rack power densities, the difference between 95% and 98% distribution efficiency is meaningful. For a 33 kW rack, a 3% efficiency improvement saves approximately 1 kW of power per rack. Across a facility with 300 racks, this saves 300 kW, enough to power 10 additional GPU nodes. Over a 3-year lifecycle at \$0.07/kWh, the efficiency improvement saves approximately \$550,000 in electricity costs.

**Stage 4: Server Power Supply.** Each server (or node) contains power supply units (PSUs) that convert the rack-level voltage to 12 V DC or directly to the multiple voltages needed by the baseboard components. The DGX H100 uses multiple high-efficiency PSUs rated for 10 kW total, with N+1 redundancy (one spare PSU per node to tolerate a single PSU failure without downtime).

**Stage 5: Voltage Regulators.** On the baseboard, voltage regulator modules (VRMs) perform the final conversion from 12 V DC to the 0.7--1.0 V required by the GPU core and the 1.1--1.2 V required by the HBM. These regulators must respond to load changes within microseconds (when the GPU transitions between idle and full-load computation), and they operate at 90--95% efficiency.

At 700 W per GPU, the VRM dissipates 35--70 W of heat, which must be cooled along with the GPU itself. This VRM heat is sometimes overlooked in thermal design: in a liquid-cooled system where cold plates cover the GPUs, the VRMs are typically still air-cooled by small fans, creating a thermal management challenge for the remaining components that do not have direct liquid cooling contact.

The cumulative efficiency across all five stages is typically 85--90%, meaning that for every 100 W of useful computation, 10--15 W is lost as heat in the power delivery chain itself. This overhead is part of the PUE calculation and represents an irreducible cost of converting utility power to useful computation.

To make this concrete, consider the power budget for a single rack containing four DGX H100 nodes:

| **Component**                      | **Power (kW)** | **% of Rack Total** |
|:-----------------------------------|:--------------:|:-------------------:|
| **GPU compute (32$\times$ 700 W)** |      22.4      |         67%         |
| **Host CPUs and DRAM**             |      3.2       |         10%         |
| **NVSwitch fabric**                |      1.6       |          5%         |
| **InfiniBand HCAs**                |      0.8       |          2%         |
| **Power conversion losses**        |      2.8       |          8%         |
| **Cooling overhead (PUE 1.1)**     |      2.7       |          8%         |
| **Total**                          |    **33.5**    |       **100%**      |

: **Power Budget for a Four-Node DGX H100 Rack**. GPUs consume two thirds of total rack power, but the remaining one third (host systems, networking, power conversion, and cooling) cannot be eliminated and must be budgeted for when sizing the facility's electrical infrastructure. The power conversion losses (8%) represent the cumulative inefficiency of the five-stage delivery chain. {#tbl-rack-power}

This breakdown reveals that the GPU power alone (22.4 kW) understates the true rack power requirement by roughly 50%. Infrastructure planners who size their facility based on GPU TDP alone will underestimate the electrical load and may discover during commissioning that their power capacity is insufficient.

ML training workloads impose a unique challenge on this power chain: **synchronous transients**. In traditional web-serving datacenters, thousands of servers handle independent requests with uncorrelated power draws. The aggregate load is smooth and predictable, varying by perhaps 10--20% over the course of a day.

In a training cluster, the picture is radically different. All accelerators execute the same computation in lockstep. When a large matrix multiplication begins, thousands of Tensor Cores across the cluster activate simultaneously, and power demand surges by 40--60% within microseconds. When the computation pauses for gradient synchronization, demand drops just as sharply. These **power ramps** stress every component in the delivery chain, from the VRMs on the baseboard to the transformers in the substation.

::: {.callout-war-story title="The Power Ramp Crash"}

In one of the early large-scale training deployments, a 512-GPU cluster experienced intermittent node failures during the first week of operation. The failures occurred at random intervals, with no apparent correlation to the model or software. Hardware diagnostics showed no component defects. The root cause turned out to be the datacenter's power infrastructure: the cluster's synchronous training steps created 2 MW power ramps that exceeded the response time of the facility's UPS battery inverters. The resulting voltage sags (drops of 5--8% lasting 10--50 ms) triggered the GPUs' undervoltage protection circuits, forcing hard resets. The fix required installing supercapacitor banks capable of delivering 500 kW for 100 ms, smoothing the transient until the UPS could respond. The lesson was that ML clusters do not behave like traditional datacenter workloads: the *temporal correlation* of power demand across thousands of chips creates a qualitatively different electrical engineering challenge.

:::

Modern ML datacenters address power transients through a layered defense strategy, with each layer covering a different timescale.

**Supercapacitor banks** provide the first line of defense, delivering hundreds of kilowatts within microseconds to smooth the initial surge. Unlike batteries, which have response times measured in milliseconds, supercapacitors store energy electrostatically and can discharge instantaneously. A typical installation places 50--100 kJ of supercapacitor storage per rack, enough to sustain a 100 kW transient for 0.5--1.0 seconds.

**Battery-backed UPS systems** with fast inverter response (under 10 ms switching time) handle longer transients and provide ride-through capability during brief grid disturbances lasting up to several minutes.

**Dedicated electrical substations** with custom transformer designs serve the largest installations. These transformers are rated for the high di/dt (rate of current change) characteristic of ML workloads, with custom winding configurations that can handle rapid load swings without voltage distortion. Standard utility transformers are designed for slowly varying loads and can experience magnetic saturation when subjected to the rapid load changes that ML training creates.

To appreciate the magnitude of these transients, consider a 1,024-GPU cluster transitioning from communication phase (400 W per GPU average) to matrix multiplication phase (700 W per GPU). The power delta is 300 W$\times$ 1,024 = 307 kW, and this transition occurs in approximately 100 microseconds. The rate of power change is therefore 307 kW / 100 $\mu$s = 3.07 GW/s. No passive electrical component can respond at this rate; only energy storage devices (supercapacitors) positioned physically close to the load can absorb the transient before it propagates into the building's electrical distribution.

The power delivery chain itself introduces inefficiencies at each stage. Utility-to-medium-voltage transformation loses 1--2%. The UPS loses 3--5% (modern double-conversion designs) or 1--2% (eco-mode designs that bypass the inverter during normal operation). The PDU loses 2--3%. Voltage regulation on the baseboard loses another 5--8%. Cumulatively, 10--15% of the power drawn from the grid is dissipated as heat in the delivery chain before it ever reaches a transistor. This overhead is part of the PUE calculation: a PUE of 1.10 implies that the delivery chain and cooling together consume 10% above the IT load.

At the largest scales, the datacenter's power draw represents a significant fraction of the local electrical grid's capacity. A 100,000-GPU cluster at 700 W per GPU consumes 70 MW of IT power, and with PUE the total facility draw approaches 80 MW. This is equivalent to powering a small city. Such installations require dedicated feeds from the electrical grid, often with purpose-built substations and transmission lines. The lead time for grid interconnection can exceed two years, making power availability one of the longest-lead-time constraints in building ML infrastructure.

The energy consumption of ML training has grown rapidly enough to become a topic of public concern. Training GPT-3 consumed approximately 1,287 MWh of electricity, equivalent to the annual energy consumption of roughly 120 US households. Training GPT-4, while not officially disclosed, is estimated to have consumed 10--100$\times$ more energy.

The scaling of energy consumption with model size is approximately linear: doubling the model parameters roughly doubles the training energy (assuming the same number of training tokens per parameter). However, the trend in frontier model development has been to increase both model size and dataset size simultaneously, leading to super-linear growth in total training energy. A model with 10$\times$ the parameters, trained on 10$\times$ the data, requires approximately 100$\times$ the energy.

As models continue to scale, the energy requirements scale proportionally, raising questions about the environmental sustainability of frontier model development. The electricity consumed by a single frontier model training run is no longer negligible relative to the output of a single power plant. A 100 MW training facility operating at full capacity for one year consumes 876 GWh, which is comparable to the annual output of a 100 MW wind farm (at 30% capacity factor, a wind farm produces about 263 GWh per year, so the training facility would require the equivalent of approximately 3.3 large wind farms).

This energy growth rate cannot continue indefinitely without addressing the source of the energy. Organizations training frontier models increasingly seek to match their electricity consumption with renewable energy generation, either by locating datacenters near renewable sources (hydro, wind, solar) or by purchasing renewable energy certificates (RECs) to offset their grid consumption.

Some have invested directly in new renewable energy projects, adding generation capacity to the grid specifically to power their ML workloads. Microsoft, for example, has signed agreements to purchase nuclear energy from restarted reactors, recognizing that the scale and consistency of ML training loads require baseload power sources that renewable intermittent sources alone cannot provide. Google has similarly invested in geothermal energy projects, which provide consistent power output independent of weather conditions.

The **carbon intensity** of the energy grid dictates the true environmental cost of a training run. A facility powered by hydroelectric dams in the Pacific Northwest emits approximately 50g CO$_2$/kWh, while a coal-heavy grid can produce 400g CO$_2$/kWh or more. A single training run for our `{python} gpt3_params_b`B model, consuming approximately 1,287 MWh, implies a carbon impact ranging from 64 tonnes to 515 tonnes depending solely on location -- an 8$\times$ variance that makes site selection the single most effective tool for decarbonization. This environmental calculus increasingly drives infrastructure decisions: organizations that can locate training clusters in low-carbon regions achieve both lower electricity costs (hydroelectric power is typically cheaper than fossil-fuel generation) and lower carbon footprints, a rare alignment of economic and environmental incentives. @sec-sustainable-ai examines these sustainability considerations in detail, including the carbon accounting for different energy sources and strategies for reducing the environmental footprint of ML infrastructure.

### Cooling {#sec-compute-cooling}

Every watt of electrical power delivered to a GPU is ultimately converted to heat. The first law of thermodynamics guarantees this: the electrical energy is converted to computational work (switching transistors), but the "work" product is just bit flips in memory, which themselves have negligible energy. All of the input energy exits the system as thermal energy that must be physically removed from the chip, transported out of the rack, and rejected to the environment. The fundamental physics of heat transfer establishes an unavoidable constraint: the heat generated by `{python} rack_power_str` kW of compute must be absorbed by a cooling medium and carried away at the same rate, continuously. If cooling falls behind even briefly, chip temperatures rise, triggering thermal throttling that reduces clock speeds and throughput. At extreme temperatures, the silicon can sustain permanent damage.

Air cooling, the dominant technology for decades, works by blowing room-temperature air across heat sinks attached to the chips. The air absorbs heat at a rate determined by its specific heat capacity, roughly 1.0 kJ/kg/K. The heated air is exhausted from the rear of the rack, typically 15--20 degrees Celsius warmer than the inlet, and directed to a computer room air conditioning (CRAC) unit that cools it before recirculating.

The fundamental problem is that air is a poor thermal conductor. Its thermal conductivity is only 0.026 W/m/K, compared to 0.6 W/m/K for water and 400 W/m/K for copper. To remove 100 kW from a rack using air alone, the fans must move enormous volumes of air at high velocity, consuming 30--40% of the rack's total power budget just for cooling.

The physics can be made precise with a simple calculation. The heat removal capacity of a fluid flow is:

$$Q = \dot{m} \times c_p \times \Delta T$$

where $Q$ is the heat removed (watts), $\dot{m}$ is the mass flow rate (kg/s), $c_p$ is the specific heat capacity (J/kg/K), and $\Delta T$ is the temperature difference between outlet and inlet. For air with $c_p = 1{,}005$ J/kg/K and a typical $\Delta T$ of 15 K (inlet at 20 degrees C, outlet at 35 degrees C), removing 100 kW requires a mass flow rate of $100{,}000 / (1{,}005 \times 15) \approx 6.6$ kg/s. At sea-level air density of 1.2 kg/m$^3$, this corresponds to a volumetric flow rate of 5.5 m$^3$/s, or approximately 11,600 CFM (cubic feet per minute). Moving this much air through the confined space of a server rack requires powerful fans that themselves consume substantial power.

At higher power densities (above 30 kW per rack), the fan power begins to approach or exceed the compute power, at which point the cooling system is consuming more energy than the computation it supports.

This is the origin of the **Power Usage Effectiveness (PUE)**[^fn-pue-metric] metric: a PUE of 1.5 means that for every watt consumed by compute, an additional 0.5 watts is consumed by cooling and power distribution overhead.

Reducing PUE is a primary engineering objective for ML datacenters because the cooling overhead is essentially wasted energy that produces no useful computation. At the scale of a 10,000-GPU cluster consuming 7 MW of IT power, the difference between PUE 1.5 and PUE 1.1 is 2.8 MW of wasted power, costing approximately \$1.7 million per year in electricity and requiring proportionally more cooling infrastructure to dissipate.

[^fn-pue-metric]: **PUE (Power Usage Effectiveness)**: Defined by The Green Grid consortium in 2007 as $P_{\text{total}} / P_{\text{IT}}$. Google's fleet-wide PUE averages 1.10; the industry average is ~1.58. For a 10,000-GPU cluster consuming 7 MW of IT power, reducing PUE from 1.58 to 1.10 saves 3.4 MW of cooling overhead -- roughly \$2.1 million per year in electricity and the equivalent of removing 1,000 residential homes from the grid. ML-specific facilities with direct-to-chip liquid cooling have demonstrated PUE values of 1.03--1.08. \index{PUE!efficiency metric}

::: {.callout-definition title="Power Usage Effectiveness (PUE)"}

***Power Usage Effectiveness (PUE)***\index{Power Usage Effectiveness!definition} is the ratio of total facility power consumption to the power consumed specifically by IT equipment ($P_{\text{facility}} / P_{\text{IT}}$).

1.  **Significance (Quantitative):** It measures the **Infrastructure Overhead** of the datacenter. A PUE of 1.0 is the theoretical ideal; a PUE of 1.10 means that for every 100 watts of computation ($\eta$), an additional 10 watts are required for cooling and power distribution.
2.  **Distinction (Durable):** Unlike **Computing Efficiency** (which focuses on FLOPs per Watt), PUE focuses on **Facility Efficiency**: it captures how much energy is "wasted" before it even reaches the processor.
3.  **Common Pitfall:** A frequent misconception is that a low PUE means a "green" datacenter. In reality, PUE only measures **Efficiency**, not the **Carbon Intensity** of the energy source; a coal-powered datacenter can have a better PUE than a solar-powered one while having a much higher environmental impact.

:::

Water has a specific heat capacity of 4.18 kJ/kg/K, over four times that of air, and a thermal conductivity roughly 25$\times$ higher. These physical properties make water an inherently superior heat transfer medium. To appreciate the magnitude of the difference, consider how much fluid must flow to remove 700 W from a single GPU. Air at a 15-degree temperature rise requires approximately 45 liters per second of airflow (a small wind tunnel). Water at the same temperature rise requires only 0.01 liters per second (a thin stream). This 4,500$\times$ difference in volumetric flow rate is why air cooling requires massive fans and carefully designed airflow paths, while liquid cooling requires only thin pipes and small pumps.

**Direct-to-chip liquid cooling** routes chilled water (or a specialized dielectric coolant) through machined copper cold plates mounted directly on each GPU package. The cold plate makes physical contact with the GPU's heat spreader through a thin layer of thermal interface material, creating a thermal path with a resistance of less than 0.1 K/W. The coolant absorbs heat within millimeters of the die surface and carries it via manifolds and pipes to a Coolant Distribution Unit (CDU) at the rack or row level.

The CDU transfers heat from the chip-level coolant loop (a closed loop using deionized water or dielectric fluid) to the building's chilled water loop, which rejects the heat to the outside environment via cooling towers or dry coolers. This two-loop design isolates the chip-level coolant (which must be ultra-pure to avoid mineral deposits on the cold plates) from the building-level water (which is less rigorously filtered).

Because liquid coolant is far more effective at absorbing heat per unit volume, the server-level fans are eliminated entirely (or reduced to small units for auxiliary components like DIMMs and VRMs). The cooling power overhead drops to 3--8% of IT power, yielding PUE values of 1.03--1.08. An additional benefit is noise reduction: liquid-cooled datacenters are dramatically quieter than their air-cooled counterparts, which matters for facilities co-located with offices or in noise-regulated areas.

A more aggressive approach, **immersion cooling**, submerges entire server boards in a tank of non-conductive dielectric fluid. The fluid absorbs heat through direct contact with every component surface, eliminating the need for cold plates, fans, and even heat sinks. The principle is simple: if every surface of the board is in contact with coolant, the heat has nowhere to accumulate and is removed uniformly across the entire assembly.

Single-phase immersion cooling uses a fluid that remains liquid throughout the process, with heat carried away by convection currents in the tank. The heated fluid rises to the surface, is pumped through a heat exchanger to reject the heat to the building's chilled water loop, and returns to the bottom of the tank.

Two-phase immersion cooling takes this further: the fluid boils at the chip surface, absorbing the latent heat of vaporization (roughly 100$\times$ more energy per gram than a simple temperature change), condenses on a cold surface at the top of the tank, and drips back down. This cycle is self-sustaining and remarkably efficient, removing over 200 kW per rack with PUE values approaching 1.02.

The trade-off is serviceability: accessing a failed component requires draining or partially submerging in fluid, and the dielectric fluids themselves are expensive (\$20--50 per liter). A single immersion tank holding four server boards may contain 500-1,000 liters of fluid, representing \$10,000-50,000 in coolant cost alone.

The operational procedures for immersion-cooled facilities differ fundamentally from air-cooled ones, requiring specialized training for technicians and different approaches to cable management, since all connectors must be compatible with prolonged fluid exposure. Standard copper cables and connectors can corrode or swell when exposed to some dielectric fluids, necessitating the use of specialized fluid-resistant materials that add cost and reduce the available supply chain options.

| **Metric**             | **Air Cooling (Legacy)** | **Liquid Cooling (Modern)** |
|:-----------------------|:-------------------------|:----------------------------|
| **Max Power Density**  | ~20--30 kW / Rack        | **>120 kW / Rack**          |
| **Cooling Efficiency** | PUE ~1.5--2.0            | **PUE ~1.05--1.10**         |
| **Mechanism**          | Forced-Air Fans          | **Direct-to-Chip Coolant**  |
| **Heat Carrier**       | Air (1.0 kJ/kg/K)        | **Water (4.18 kJ/kg/K)**    |
| **Fan Power**          | 30--40% of IT load       | **<5% of IT load**          |

: **The Shift to Liquid Cooling**. At rack power densities above 30 kW, air cooling requires fan power that approaches the power consumed by the GPUs themselves. Liquid cooling is not a premium option; it is a thermodynamic requirement for modern ML racks. {#tbl-cooling-limits}

The capital cost of these cooling technologies spans an order of magnitude. Standard air cooling infrastructure costs \$2,000--5,000 per rack (fans, CRAC units, raised floor tiles). Direct-to-chip liquid cooling costs \$15,000--25,000 per rack (cold plates, manifolds, CDUs, piping). Full immersion cooling costs \$30,000--50,000 per tank (dielectric fluid, sealed tanks, specialized heat exchangers). The break-even analysis between air and liquid cooling depends on rack power density: at 20 kW per rack, air cooling's lower CapEx wins over a 3-year lifecycle. At 40 kW per rack, the electricity savings from liquid cooling's lower PUE (1.08 vs. 1.5) offset the higher CapEx within 18--24 months. At 60+ kW per rack -- the regime of modern ML infrastructure -- air cooling is physically impossible, making the comparison moot. For our `{python} gpt3_params_b`B model's 128-rack training cluster at `{python} rack_power_str` kW per rack, direct-to-chip liquid cooling is the standard choice, balancing density, serviceability, and cost. Immersion cooling offers marginal PUE improvement (1.03 vs. 1.08) but introduces operational complexity that most organizations find unjustified at current rack densities.

::: {.callout-notebook title="The Cooling Tax"}

Consider a 1,000-GPU cluster of H100s at 700 W TDP each.

- **IT Power**: 1,000$\times$ 700 W = 700 kW
- **Air Cooling (PUE 1.5)**: Total facility power = 700 kW$\times$ 1.5 = 1,050 kW. Cooling overhead = 350 kW.
- **Liquid Cooling (PUE 1.08)**: Total facility power = 700 kW$\times$ 1.08 = 756 kW. Cooling overhead = 56 kW.

**Savings**: Liquid cooling saves 294 kW of continuous power. At \$0.07/kWh, the annual savings are approximately \$180,000. Over a 3-year hardware lifecycle, the cooling savings alone total \$540,000, which often exceeds the capital cost of installing the liquid cooling infrastructure.

:::

The final link in the cooling chain is **heat rejection**: getting the heat from the building's chilled water loop to the outside environment. The dominant technology is the cooling tower, which sprays warm water over a fill medium and uses evaporation to carry heat into the atmosphere. Evaporative cooling is remarkably efficient (the latent heat of vaporization of water is 2,260 kJ/kg, compared to 4.18 kJ/kg/K for sensible heating), but it consumes water.

A 10 MW datacenter with evaporative cooling towers can consume 10--20 million liters of water per year, which has become a significant concern in water-stressed regions. To put this in perspective, 20 million liters is roughly the annual water consumption of 150 households. As ML datacenters grow to 100 MW and beyond, their water footprint becomes a meaningful factor in local resource planning.

**Dry coolers**, which use fans to blow air over a radiator without evaporation, eliminate water consumption but work efficiently only when the ambient air temperature is well below the coolant temperature, limiting their effectiveness in hot climates. Many modern facilities use hybrid approaches: dry coolers during cool weather and evaporative towers during heat waves.

An increasingly attractive alternative is **waste heat reuse**, which treats the datacenter's thermal output as a resource rather than a waste product. A liquid-cooled ML facility produces hot water at 40-60 degrees Celsius, a temperature range suitable for district heating systems, greenhouse agriculture, and industrial process heat. Several Nordic datacenters already supply their waste heat to municipal heating networks, offsetting the natural gas or electricity that would otherwise be required to heat buildings during winter. A 10 MW datacenter can supply approximately 8-9 MW of useful heat (accounting for heat pump efficiency), enough to heat several thousand apartments. When the economic value of the waste heat is credited against the datacenter's operating costs, the effective PUE can drop below 1.0, meaning the facility produces more useful energy (computation plus heat) than it consumes from the grid.

The viability of waste heat reuse depends on the proximity of heat consumers. Urban datacenters, despite their higher land and electricity costs, are often better positioned for waste heat reuse than remote facilities because they are close to residential and commercial heating loads. This creates an interesting economic optimization: a datacenter in a Nordic city may have higher electricity costs but lower net operating costs after waste heat revenues, compared to a remote facility with cheaper electricity but no heat consumers nearby.

@sec-sustainable-ai examines the environmental implications of datacenter cooling in detail, including the carbon accounting for waste heat reuse and the life-cycle analysis of different cooling technologies.

For our `{python} gpt3_params_b`B model training cluster, the choice between cooling technologies is not optional. A cluster of 1,000 H100s dissipates 700 kW of heat from the GPUs alone, before accounting for CPUs, memory, networking, and power conversion losses. Only liquid cooling can remove this heat at the required density. The rack is the level at which the problem shifts from *computation* to *physics*, and the design of the cooling infrastructure often determines whether a training cluster can operate at full utilization or must be throttled to prevent thermal runaway.

#### Cooling System Reliability

Cooling system failures have more severe consequences in ML clusters than in traditional datacenters because of the higher power density. In a traditional air-cooled datacenter at 10 kW per rack, losing a CRAC unit causes temperatures to rise gradually over tens of minutes, providing ample time for operators to respond. In a liquid-cooled ML rack at 100+ kW, losing coolant flow causes temperatures to reach the GPU's thermal shutdown threshold within 30--60 seconds, because the heat capacity of the cold plate and the small volume of coolant in the pipes provides very little thermal buffer.

This rapid thermal runaway drives several design decisions. Coolant loops are designed with N+1 redundancy: each CDU has a backup pump, and the piping manifold includes bypass valves that can reroute coolant around a failed CDU. Temperature sensors at each cold plate trigger immediate alerts when the coolant outlet temperature exceeds a threshold (typically 65 degrees Celsius), and the GPU firmware will throttle power within milliseconds if the junction temperature approaches the 83-degree limit.

Some facilities also maintain an emergency air cooling capability as a last-resort backup. Even though air cooling cannot sustain full-power operation at ML rack densities, it can keep the hardware below damage thresholds (at reduced clock speeds) long enough for operators to repair the liquid cooling system. This defense-in-depth approach to cooling reliability reflects the fact that a cooling failure in a 10,000-GPU cluster can simultaneously affect hundreds of GPUs, making the potential financial impact of a cooling outage far greater than the cost of the redundancy.

The failure modes of liquid cooling systems are qualitatively different from those of air cooling. Air cooling fails gracefully: a fan failure reduces airflow, causing temperatures to rise slowly over minutes, providing ample time for automated load shedding. Liquid cooling can fail catastrophically: a coolant leak can simultaneously damage hardware (if the coolant is conductive) and remove cooling capacity (if the leak drains the loop). Quick-disconnect fittings, which allow hot-swapping of server nodes without draining the entire coolant loop, are a critical design feature that reduces maintenance downtime from hours to minutes. However, these fittings are also the most common point of failure in the coolant loop, as the O-ring seals degrade over thousands of connect/disconnect cycles. Facilities that perform frequent hardware swaps (common in research environments where nodes are regularly reconfigured) must budget for quarterly O-ring replacement and maintain a stock of spare fittings.

The economics of cooling reliability shift dramatically when moving from independent inference servers to tightly coupled training clusters. In a distributed training run using synchronous parallelism, a single rack failure halts the entire job. Consider a cooling failure that triggers a thermal shutdown of a 256-GPU rack within a 10,000-GPU cluster. The direct hardware cost is negligible, but the opportunity cost is immense. If the repair time for a CDU pump is four hours, the immediate loss of 256 GPUs at \$4 per GPU-hour is only \$4,096. However, because the training algorithm requires all workers to proceed in lockstep, the remaining 9,744 GPUs also sit idle, burning electricity without making progress. This straggler effect inflates the cost to \$160,000 in lost compute time. When adding the overhead of checkpoint retrieval and the rollback to the last saved state -- often losing 30 to 60 minutes of computation -- the total financial impact of a single cooling component failure easily exceeds $200,000. This non-linear scaling of failure costs makes N+1 redundancy in cooling loops not just an insurance policy but a mathematical necessity for training economics.

Maintaining the physical integrity of the liquid loop requires managing complex hydro-chemical dynamics. The fluid circulating through direct-to-chip systems is typically **deionized water** mixed with specific corrosion inhibitors, not simple tap water. The conductivity must be rigorously maintained below 1 microsiemens per centimeter ($\mu$S/cm) to prevent **galvanic corrosion**, where the electrical potential difference between dissimilar metals in the loop (copper cold plates and stainless steel manifolds) eats away at the cooling surfaces. This chemical balance is unstable: inhibitors are consumed over time and dissolved gases accumulate, necessitating monthly quality testing and annual full-volume replacement. Biological contamination poses an equally severe threat. **Biofilm** growth on the internal micro-fins of a cold plate acts as a thermal insulator; a mere 50-micron layer of organic growth can degrade heat transfer coefficients by 30%, forcing pumps to run at maximum power to compensate. Regular biocide treatments and periodic system flushing are therefore as critical to cluster performance as driver updates or firmware patches.

### Rack Design Considerations {#sec-compute-rack-design}

The physical design of an ML rack differs substantially from a traditional server rack in ways that go beyond power and cooling. In a conventional 42U rack, servers are installed as independent 1U or 2U pizza-box units, each with its own fans, power supplies, and cable connections. ML racks use a fundamentally different form factor. A DGX H100 occupies 8U and contains 8 GPUs, the NVSwitch fabric, multiple power supplies, and (in liquid-cooled configurations) coolant manifolds with quick-connect fittings. Four DGX units in a 42U rack leave only 10U for networking switches, cable management, and PDUs.

The transition from traditional server form factors to ML-optimized designs is driven by the same power density challenge that forced the transition from air cooling to liquid cooling. A traditional 1U server dissipates 300-500 W and requires only modest airflow for cooling. A DGX H100 node dissipates approximately 10 kW across its 8 GPUs, NVSwitches, and supporting components, requiring either massive airflow (in air-cooled configurations) or liquid cooling plumbing (in liquid-cooled configurations). The form factor must accommodate both the compute hardware and the thermal management infrastructure, which is why ML nodes are substantially taller (8U vs. 1U) than traditional servers.

Cable management is a non-trivial engineering challenge. Each DGX H100 node has 8 InfiniBand cables (one per GPU), 2 Ethernet management cables, power cables, and (for liquid-cooled units) coolant hoses. A fully populated rack has over 40 high-speed cables, each of which must be routed carefully.

Three physical constraints govern cable routing. First, cables must not obstruct airflow in air-cooled designs, as even a partial blockage can create hot spots that throttle nearby components. Second, high-speed cables have a minimum bend radius (typically 10--15$\times$ the cable diameter for active optical cables) below which the signal path is damaged, causing bit errors or complete link failure. Third, bundling too many high-speed copper cables together creates electromagnetic interference that can degrade signal quality on neighboring links. In large installations, the cable plant alone can take weeks to install and test, and cable routing errors are one of the most common causes of post-installation debugging delays.

As rack power densities climb toward 100 kW for liquid-cooled clusters, the inefficiency of per-server AC conversion becomes untenable. Traditional designs dedicate volume in every chassis to redundant AC power supply units (PSUs), but modern ML infrastructure is standardizing on **rack-level DC power distribution**. In this architecture, a centralized power shelf converts mains AC to a **48V DC busbar** that runs the height of the rack, replacing individual server PSUs. This consolidation eliminates one conversion stage, yielding a system-wide efficiency gain of 2--3% -- a meaningful reduction in thermal load when training a 175B-parameter model across thousands of GPUs. Power is delivered via the 48V bus directly to the server backplane, where compact DC-to-DC converters step it down to 12V and finally to the GPU's sub-1V operating voltage. Pioneered by Google and Meta through the Open Compute Project (OCP), this topology reduces failure points and recovers valuable chassis volume for hydraulic cooling loops and high-bandwidth interconnects.

The **Top-of-Rack (ToR) switch** deserves special attention. In traditional datacenters, a ToR switch provides 1--10 Gbps Ethernet connectivity to each server, aggregated into uplinks to higher-level switches. In an ML rack, the ToR switch is an InfiniBand or high-speed Ethernet switch providing `{python} ib_bw_str` Gbps per port, and it must handle the bursty, synchronized traffic patterns of distributed training.

The placement and configuration of ToR switches directly affects the network topology. Modern **rail-optimized** designs (used by Meta and others) assign each GPU within a node to a specific "rail" that connects to a dedicated ToR switch. In a conventional design, all 8 GPUs in a node connect to the same ToR switch, creating a potential bottleneck when multiple GPUs simultaneously send AllReduce traffic. In a rail-optimized design, GPU 0 in every node connects to ToR switch 0, GPU 1 to ToR switch 1, and so on. This ensures that AllReduce traffic within a parallelism group (where all participants are the same GPU index across different nodes) is confined to a single switch rather than traversing the full fabric.

The rail-optimized design requires 8 ToR switches per rack group instead of 1, which increases switch cost and cabling complexity. However, it eliminates cross-switch congestion for the most common communication pattern (data-parallel AllReduce across nodes), improving scaling efficiency by 5--15% compared to a conventional single-ToR design. @sec-network-fabrics examines rail-optimized topologies in detail, including the formal analysis of their bandwidth properties and the conditions under which they outperform fat-tree alternatives.

#### Physical Security and Environmental Controls

Datacenters housing ML infrastructure typically implement multiple layers of physical security and environmental monitoring, not only for regulatory compliance but also for practical risk management. The value of a single rack of DGX H100 nodes (\$1.4 million in hardware alone) justifies security measures that would be excessive for traditional server equipment.

Environmental monitoring includes:

- **Water detection sensors** beneath raised floor tiles and around liquid cooling piping, capable of detecting leaks within seconds and triggering automatic coolant isolation valves.
- **Smoke and fire detection** using aspirating smoke detection (ASD) systems that continuously sample air from inside racks, detecting combustion byproducts at concentrations far below what sprinkler-activating detectors can sense.
- **Vibration sensors** on the building structure and rack frames, detecting seismic events or mechanical vibrations from construction activity that could damage disk drives or loosen cable connections.
- **Humidity control** maintaining relative humidity between 40--60% to prevent both static discharge (low humidity) and condensation (high humidity).

These environmental controls are not afterthoughts but integral parts of the infrastructure design that affect reliability and uptime. A water leak that reaches a DGX H100 baseboard can cause millions of dollars in damage and weeks of downtime. A fire in a single rack can shut down an entire datacenter hall for days while the fire suppression system is recharged and the affected area is decontaminated.

Datacenter reliability is classified by the Uptime Institute into tiers, with **Tier III** and **Tier IV** representing the standard options for mission-critical compute. A Tier III facility offers $N+1$ redundancy (concurrently maintainable) with an expected uptime of 99.982%, translating to roughly 1.6 hours of unplanned downtime per year. Tier IV provides $2N$ fully fault-tolerant redundancy for every power and cooling path, boosting uptime to 99.995% (approximately 26 minutes of unplanned downtime per year). Achieving Tier IV requires doubling the electrical and mechanical infrastructure, driving CapEx 30--50% higher than a comparable Tier III build.

For ML training infrastructure, the calculus differs from traditional enterprise computing. A training run is a long-running batch process, not a real-time transaction system. If a facility-level outage occurs during a two-week training run, the cost is not data loss but a restart from the last checkpoint -- typically losing 10--30 minutes of computation. The probability of a facility-level outage during any specific two-week window in a Tier III facility is less than 0.1%. The ML infrastructure industry therefore overwhelmingly favors Tier III designs paired with aggressive application-level fault tolerance: rather than spending millions on redundant generators and chillers to prevent a rare power interruption, engineers invest in robust checkpointing systems that save the model state every 10--30 minutes. This software resilience commoditizes the facility risk, treating the datacenter as a fallible utility rather than a fortress. The savings from choosing Tier III over Tier IV -- often $10--20 million for a large facility -- can be redirected toward additional GPU capacity, which directly accelerates training.

::: {.callout-checkpoint title="Infrastructure Physics"}

A team is planning to deploy 256 H100 GPUs (32 nodes) in an existing air-cooled datacenter that has 250 kW of available power capacity and cooling rated for 200 kW at PUE 1.5. Can this facility support the deployment? Calculate the total power draw (including cooling overhead) and identify which constraint (power or cooling) is binding. What modifications would be needed to support the full deployment?

:::

The rack concentrates power and heat into a physical volume where thermodynamics, not software, sets the limits. But a single rack of 32 GPUs is far from sufficient for our `{python} gpt3_params_b`B model, which may require thousands of accelerators to train in a reasonable timeframe. The next level of infrastructure aggregates hundreds of racks into a unified computing system: the pod.

## The Pod {#sec-compute-pod}

\index{Pod}
\index{Warehouse-Scale Computer}

Training our `{python} gpt3_params_b`B model on a single DGX H100 node (8 GPUs, roughly 16,000 TFLOPS aggregate) would take several months, assuming we can fit the model at all. Reducing training time to weeks requires 100--1,000 nodes operating in concert, and doing so demands that these nodes be wired together into a network fast enough to keep gradient synchronization from becoming the bottleneck. This is the engineering challenge of the **pod**: aggregating hundreds of racks into a single, coordinated computing system where the network fabric serves the same role that the system bus serves within a single machine.

The scale of this challenge is worth appreciating concretely. A 1,024-node DGX H100 cluster contains 8,192 GPUs, 4,096 NVSwitch chips, 8,192 InfiniBand HCAs, several hundred InfiniBand switches, tens of thousands of cables, and consumes approximately 7--10 MW of power. It occupies roughly 250 racks across one or more datacenter halls.

The physical weight of such a cluster is also substantial. Each DGX H100 node weighs approximately 130 kg (287 pounds), so 1,024 nodes weigh over 133 metric tons. Combined with racks, switches, cables, and cooling infrastructure, the total weight can exceed 300 metric tons. The datacenter floor must be structurally engineered to support this concentrated load, with typical floor loading requirements of 1,500--2,500 kg per square meter for ML clusters, compared to 500--1,000 kg per square meter for traditional server installations. This structural requirement is often overlooked during facility planning and can be a blocking constraint for retrofitting existing buildings.

The physical layout of the datacenter hall reflects these density constraints. The extreme power density of ML training racks necessitates rigid hot aisle/cold aisle containment in air-cooled sections, where cold air is forced into the enclosed front of the rack and waste heat is captured immediately at the rear exhaust. The "spine" of the hall -- the central cable corridor connecting all racks to the aggregation switches -- must accommodate thousands of fiber optic cables and power feeds. Overhead cable trays are preferred over under-floor routing to improve airflow and accessibility, carrying the heavy copper power feeds and fragile fiber interconnects that form the nervous system of the cluster. The layout must also accommodate the liquid cooling infrastructure: CDU placement, coolant piping runs, and isolation valves that allow individual racks to be serviced without draining the entire cooling loop. These physical layout decisions, made during facility design, constrain the network topology options available to the training team years later.

Training a `{python} gpt3_params_b`B model on this cluster produces approximately 1.3 EFLOPS (exaFLOPS) of sustained throughput. This sounds enormous until one considers that the training requires `{python} pod_flops_math` total floating-point operations (assuming 300 billion training tokens). At 1.3 EFLOPS, this takes approximately $3.15 \times 10^{23} / 1.3 \times 10^{18} \approx 242{,}000$ seconds, or roughly 67 hours, under idealized conditions.

In practice, communication overhead, pipeline bubbles, and maintenance windows extend this to 2--4 weeks. The gap between the idealized 67 hours and the practical 2--4 weeks is a factor of 5--10$\times$, which illustrates the central theme of this section: at pod scale, the infrastructure's imperfections dominate over the raw capability of the silicon. Recovering this lost factor is the central challenge of distributed systems engineering, and the solutions span hardware (better networks), software (overlapped communication), and operations (proactive maintenance to minimize downtime).

::: {.callout-notebook title="Training Time for 175B"}

We can derive the training time for our `{python} gpt3_params_b`B model from first principles.

1. **Total FLOPs**: Using the approximation $6 \times N \times P$, where `{python} n_params_math` and $P = 300 \times 10^9$ tokens:
   `{python} total_flops_math` FLOPs

2. **Cluster Throughput**: 8,192 H100 GPUs at `{python} h100_tflops` TFLOPS peak, operating at 45% MFU:
   `{python} cluster_throughput_math` EFLOPS sustained

3. **Idealized Training Time** (the "physics limit"):
   $3.15 \times 10^{23} / 7.3 \times 10^{18} \approx 43{,}150$ seconds $\approx$ **12 hours**

4. **Real-World Multipliers**:
   - Communication overhead (scaling efficiency $\approx 0.85$): $\div 0.85 \rightarrow$ 14.1 hours
   - Pipeline bubbles ($\approx 5\%$): $\times$ $1.05$ $\rightarrow$ 14.8 hours
   - Checkpoint overhead ($\approx 3\%$): $\times$ $1.03$ $\rightarrow$ 15.3 hours
   - Hardware failures and restarts ($\approx 10\%$ of wall time): $\times$ $1.10$ $\rightarrow$ 16.8 hours
   - Maintenance windows ($\approx 5\%$): $\times$ $1.05$ $\rightarrow$ 17.6 hours

**Total**: ~18 hours (system minimum) $\rightarrow$ 2--3 days (typical operations) $\rightarrow$ 1--2 weeks (conservative schedule with operational margin). The gap between the 12-hour physics limit and the practical 1--2 week schedule is a factor of 15--25$\times$. Recovering this lost time is the domain of distributed systems engineering -- the subject of @sec-distributed-training-systems.

:::

::: {.callout-definition title="Warehouse-Scale Computer (WSC)"}

***Warehouse-Scale Computer (WSC)***\index{Warehouse-Scale Computer!definition} is a distributed architecture where thousands of compute nodes are operated as a single coherent machine.

1.  **Significance (Quantitative):** In a WSC, the network fabric replaces the **System Bus**, distributed storage replaces the **Local Disk**, and a fleet orchestrator replaces the **Operating System**. It is the only architecture capable of providing the exa-scale **Peak Performance ($R_{peak}$)** required for frontier model training.
2.  **Distinction (Durable):** Unlike a **Horizontal Cluster** (which houses many independent applications), a WSC is designed for **Synchronous Tight Coupling**, where the components are connected by wires rather than silicon traces but must act with the same coordination as a single chip.
3.  **Common Pitfall:** A frequent misconception is that WSC design is just "IT management." In reality, it is **Building-Level Computer Architecture**: the physical layout, power delivery, and cooling must be optimized as a single system to prevent the **Bisection Bandwidth ($BW$)** from becoming the binding constraint.

:::

The concept was formalized by Luiz Andre Barroso and Urs Holzle at Google in their 2009 monograph. The key insight is that at Google's scale, the datacenter *is* the computer, and traditional computer architecture principles must be applied at the building level rather than the chip level.

### WSC Architecture Principles {#sec-compute-wsc-principles}

The concept of the Warehouse-Scale Computer, articulated by Barroso and Holzle at Google, reframes the datacenter from a *room full of computers* to a *single computer that happens to fill a room*. This reframing has profound implications for design.

In a traditional datacenter, each server is an independent unit with its own operating system, storage, and network identity. The facility simply provides power, cooling, and physical security. Servers can be added, removed, or replaced independently, and the workloads running on different servers are unrelated to each other.

In a WSC, the individual server is analogous to a core in a multicore processor: it has no independent utility and exists only as part of the larger system. A DGX H100 node running in isolation cannot train a 175B model; it becomes useful only when connected to hundreds of other nodes through a carefully designed network fabric, with a distributed storage system providing training data and checkpoint storage, and a fleet orchestrator coordinating the work across all nodes.

This perspective shift has practical implications for how infrastructure teams think about component failures, performance optimization, and capacity planning. Just as a chip designer does not optimize individual transistors but optimizes the overall circuit, a WSC engineer does not optimize individual servers but optimizes the overall system throughput and efficiency. Three design principles follow from this shift in perspective.

The first principle is that **the network is the bottleneck, not the compute**. Within a single node, NVLink provides enough bandwidth for tensor parallelism. Across nodes, the network fabric must carry gradient tensors, activation checkpoints, and pipeline stage outputs.

For our `{python} gpt3_params_b`B model with data parallelism across 128 nodes, each training step requires an AllReduce of approximately 350 GB of gradients. If the network *cannot* overlap this communication with the next forward pass, the GPUs sit idle during synchronization.

The fraction of time spent on communication (as opposed to computation) is the single largest determinant of cluster efficiency, often mattering more than the peak FLOPS of the individual accelerators. This is a counterintuitive result for teams accustomed to thinking about hardware in terms of compute specifications. A cluster with slightly slower GPUs but a better network fabric can outperform a cluster with faster GPUs but an inadequate network, because the faster GPUs spend more of their time idle, waiting for gradient synchronization.

@sec-network-fabrics examines the network engineering in detail; here we focus on how the physical topology of the pod determines what communication patterns are efficient.

The second principle is that **failure is routine, not exceptional**. A cluster of 10,000 GPUs, each with a mean time between failures (MTBF) of approximately 10,000 hours, will experience a GPU failure roughly once per hour on average.

\index{MTBF}

To put this in perspective: if training our `{python} gpt3_params_b`B model takes 2 weeks (336 hours) on a 10,000-GPU cluster, we should expect approximately 336 individual GPU failures during the training run. Each failure potentially corrupts the training state and requires either rolling back to a checkpoint or reorganizing the cluster to work around the failed node.

The checkpointing frequency determines the cost of each failure. If checkpoints are taken every 10 minutes, a failure costs at most 10 minutes of lost work plus the time to restart. If checkpoints are taken every hour, each failure costs up to an hour of lost work plus restart overhead, and with 336 failures, the cumulative lost time could exceed 336 hours, *doubling the training time*.

This arithmetic makes checkpointing strategy a first-order performance concern, not an afterthought. Nodes fail, switches fail, power supplies fail, and cables degrade. The WSC must be designed so that individual component failures do not halt the entire training run.

This requires redundancy at every level of the physical infrastructure. Power feeds to each rack are typically dual-path (A+B feeds from independent UPS systems), so a single UPS failure does not cause a rack outage. Network paths between nodes include redundant switch fabrics, so a single switch failure does not partition the cluster. At the application level, periodic checkpointing saves the training state to persistent storage, and automatic restart mechanisms can reassemble the training job on healthy nodes after a failure, resuming from the most recent checkpoint.

The interaction between failure rate and checkpointing frequency creates an optimization problem. More frequent checkpoints reduce the amount of work lost per failure but consume GPU time (to serialize the training state) and storage bandwidth (to write the checkpoint). Less frequent checkpoints reduce overhead but increase the expected work lost per failure. @sec-fault-tolerance-reliability examines checkpointing strategies and elastic training systems in detail.

Checkpointing also creates a significant *storage* requirement that demands its own dedicated infrastructure. A full checkpoint for our `{python} gpt3_params_b`B model includes the model weights (350 GB), optimizer states (1,400 GB), and the random number generator states needed to resume training deterministically. The total is approximately 1.75 TB per checkpoint. To keep the checkpoint overhead below 2% of total training time, this write operation must complete in under 30 seconds, demanding a sustained aggregate write bandwidth of approximately 60 GB/s. Standard NFS or object storage implementations collapse under this "thundering herd" pattern, where all GPUs transition from compute to I/O simultaneously. The solution requires a high-performance parallel file system like Lustre or IBM Spectrum Scale (GPFS), architected with hundreds of Object Storage Targets (OSTs) striping data across thousands of NVMe drives. A failure to architect for this burst bandwidth results in "I/O jitter," where training hangs unpredictably for minutes as the checkpoint write saturates the storage backend, effectively burning millions of dollars in idle GPU cycles. @sec-data-storage examines the storage architectures that meet these throughput requirements in detail. If checkpoints are taken every 10 minutes during a 2-week training run, the system must write:

- **Checkpoints per run**: (14 days$\times$ 24 hours$\times$ 6 per hour) = 2,016 checkpoints
- **Total checkpoint data**: 2,016$\times$ 1.75 TB = 3,528 TB

Even retaining only the most recent 10 checkpoints (17.5 TB), the storage system must sustain a continuous write throughput of 1.75 TB every 10 minutes, or approximately 3 GB/s. Organizations that undersize their checkpoint storage discover that checkpointing itself becomes a bottleneck, as the GPUs must wait for the previous checkpoint to complete writing before starting the next one. @sec-data-storage examines the storage architectures that meet these throughput requirements.

\index{Checkpointing}

The third principle is that **homogeneity enables efficiency**. Unlike a general-purpose cloud that serves heterogeneous workloads (web servers, databases, batch processing) with heterogeneous hardware, an ML training pod runs a single workload (or a small number of workloads) across all its nodes simultaneously. This homogeneity allows the network topology, cooling system, and power delivery to be co-designed for the specific communication patterns of the target workload.

A pod optimized for Transformer training, where the dominant communication pattern is AllReduce along well-defined parallelism groups, can use a topology with high nearest-neighbor bandwidth (like a torus). A pod optimized for recommendation models, where the dominant pattern is AllToAll for embedding shard exchange, needs high bisection bandwidth (like a fat-tree). A pod designed for mixed workloads (training during the week, inference during peak serving hours) may adopt a fat-tree for flexibility, accepting the higher switch cost in exchange for the ability to serve any communication pattern efficiently.

Both specialized designs outperform a general-purpose topology for their target workload, illustrating the same generality-efficiency trade-off we observed at the accelerator level. The Generality Tax applies not only to silicon but to every layer of the infrastructure stack: network topologies, cooling systems, and even building designs become more efficient when optimized for a known workload pattern.

These principles (that the network is the bottleneck, failure is routine, and homogeneity enables efficiency) are not just qualitative observations. They can be quantified by a single metric: **scaling efficiency**, which determines whether adding more nodes to a pod produces proportional returns or diminishing waste.

#### The Homogeneity Advantage

The homogeneity principle deserves deeper examination because it has practical consequences that extend beyond network topology into scheduling, debugging, and fleet economics.

In a homogeneous pod, every node has identical compute, memory, and network capabilities. The job scheduler can assign any subset of $N$ nodes to any training job without worrying about capability mismatches. In a heterogeneous fleet with mixed GPU generations (V100, A100, H100), the scheduler must solve a constrained bin-packing problem: a training job that requires 256 GPUs of the same generation may wait in the queue even when 256 GPUs are available across generations, because mixing generations within a single data-parallel group creates a straggler problem (the oldest GPU determines the step time for the entire group). This fragmentation can reduce effective fleet utilization by 10--20%, as capacity that is technically available cannot be allocated to waiting jobs.

Debugging benefits equally from homogeneity. When all nodes are identical, a performance anomaly on one node can be immediately compared against the fleet baseline. If node 347 achieves 85% of the throughput that nodes 1--346 and 348--1024 achieve, the anomaly is clearly hardware-related and the node can be quarantined for diagnosis. In a heterogeneous fleet, distinguishing between "this node is slow because it has older hardware" and "this node is slow because it has a failing component" requires much more sophisticated analysis, and subtle degradation can go undetected for days or weeks.

Network topology optimization also benefits from homogeneity. If every node runs the same model with the same parallelism configuration, the traffic matrix is predictable: tensor-parallel AllReduce within nodes, pipeline-parallel point-to-point between specific node groups, and data-parallel AllReduce across the remaining dimension. The topology can be designed to minimize congestion for this specific pattern, using rail-optimized or hierarchical designs that exploit the known communication structure. Heterogeneous workloads produce unpredictable traffic matrices that require general-purpose topologies (fat-trees) with their higher switch cost and lower peak efficiency for any single pattern.

The cost of this homogeneity is inflexibility. A pod designed for Transformer training with 8-way tensor parallelism may be poorly suited for Mixture-of-Experts models that require AllToAll communication, or for recommendation models that need large CPU memory pools for embedding tables. Organizations that run diverse workloads must either accept the generality tax of a flexible topology or maintain multiple specialized pods, each optimized for a different workload class. The choice mirrors the accelerator spectrum: specialization yields efficiency, generality yields flexibility, and the optimal point depends on workload stability.

The software infrastructure of a WSC functions less like a collection of servers and more like a single distributed operating system, with the **job scheduler** acting as the kernel. Systems like Slurm or Kubernetes do not merely assign tasks; they orchestrate the rigid resource geometry required for distributed training. The critical constraint is **gang scheduling**: a training job for our `{python} gpt3_params_b`B model requiring 1,024 GPUs cannot start if only 1,023 are available. The scheduler must allocate the entire cohort simultaneously, because the synchronous nature of the training algorithm means a single missing node halts the progress of the entire fleet. Beyond binary allocation, the scheduler must enforce **topology awareness**, placing the job on a contiguous block of high-speed interconnects to minimize cross-sectional bandwidth bottlenecks. Allocating 1,024 GPUs scattered randomly across the datacenter introduces hop-count penalties that can reduce effective AllReduce bandwidth by 30--50%. In a multi-tenant environment, this creates complex queue management challenges where high-priority jobs may need to preempt lower-priority experiments, triggering automated checkpointing and eviction to free up the required contiguous hardware blocks. @sec-fleet-orchestration examines the scheduling algorithms and resource management strategies for WSC-scale clusters in detail.

### Scaling Efficiency {#sec-compute-scaling-efficiency}

Before examining specific pod architectures, we must understand the metric that determines whether scaling from one node to many is worthwhile: **scaling efficiency**. If a model trains in time $T_1$ on one node, ideal (linear) scaling would train it in time $T_1 / N$ on $N$ nodes. In practice, communication overhead, pipeline bubbles, and load imbalances reduce the speedup. Scaling efficiency is defined as:

$$\eta = \frac{T_1}{N \times T_N}$$ {#eq-scaling-efficiency}

where $T_N$ is the training time on $N$ nodes. An efficiency of 1.0 means perfect linear scaling; an efficiency of 0.5 means we achieve only half the expected speedup.

::: {.callout-definition title="Scaling Efficiency"}

***Scaling Efficiency ($\eta$)***\index{Scaling Efficiency!definition} is the ratio of actual throughput to ideal linear throughput when increasing the number of compute nodes ($N$).

1.  **Significance (Quantitative):** It is the most important metric for cluster productivity ($\eta = T_1 / (N \cdot T_N)$). A scaling efficiency of 0.50 means that a 10,000-GPU cluster is delivering only the same useful work as a 5,000-GPU cluster, wasting 50% of the hardware investment.
2.  **Distinction (Durable):** Unlike **Single-Node Efficiency** (which captures local bottlenecks like $BW$), Scaling Efficiency captures the **Cluster-Level Overhead** of communication ($L_{lat}$) and synchronization.
3.  **Common Pitfall:** A frequent misconception is that scaling efficiency is constant. In reality, it is a **Function of Problem Size**: as $N$ increases, the communication-to-compute ratio typically worsens (Amdahl's Law), making it harder to maintain high efficiency for small models.

:::

For data-parallel training of our `{python} gpt3_params_b`B model, the communication cost per step is dominated by the AllReduce of 350 GB of gradients. Using ring-AllReduce over InfiniBand at `{python} ib_bw_gbs` GB/s effective bandwidth, the communication time is approximately `{python} comm_time_formula_math`, which for large $N$ approaches `{python} comm_approx_math` seconds. If the compute time per step is 20 seconds, the total step time is 34 seconds, yielding a scaling efficiency of $20/34 \approx 0.59$. In practice, communication can be overlapped with the backward pass (sending gradients for early layers while computing gradients for later layers), which recovers much of this loss, achieving 70--90% scaling efficiency for well-optimized training systems.

The scaling efficiency depends critically on the ratio of computation to communication. This ratio is determined by three factors:

1. **Model size**: Larger models have more computation per training step (more FLOPs per weight update), so the same communication overhead represents a smaller fraction of total step time. This is sometimes called the *model-communication ratio*, and it is why scaling efficiency improves as models grow larger. Paradoxically, larger models are *easier* to scale than smaller ones.

2. **Batch size**: Larger batches increase the computation per step without proportionally increasing communication (gradients are the same size regardless of batch size). This improves the computation-to-communication ratio. However, large batch sizes can harm convergence (the model may overfit to the batch), requiring learning rate tuning and warmup schedules.

3. **Network bandwidth**: Doubling the InfiniBand bandwidth halves the communication time, directly improving scaling efficiency. This is why the network fabric is not a secondary concern but a first-order determinant of cluster productivity. The cost of the network fabric (10--15% of total system cost) is easily justified if it improves scaling efficiency by even a few percentage points, because poor scaling efficiency wastes the other 85--90% of the investment.

These three factors interact in important ways. Larger models with larger batch sizes achieve better scaling efficiency, which means that frontier-scale training runs (the most expensive workloads) are also the ones that benefit most from scale. This creates a virtuous cycle for large-scale infrastructure: the workloads that justify building thousand-GPU clusters are also the workloads that use them most efficiently. Conversely, small models and small batch sizes scale poorly, which is why researchers training 1B-parameter models on 64 GPUs often achieve only 40--60% scaling efficiency.

However, even for large models, scaling does not continue indefinitely. There exists a **scaling cliff** beyond which adding more GPUs actually reduces cost-efficiency. For our `{python} gpt3_params_b`B model, the optimal cluster size is approximately 1,024--4,096 GPUs, where the communication-to-compute ratio remains favorable and scaling efficiency stays above 70%. Beyond 8,192 GPUs, the AllReduce communication time begins to dominate the backward pass computation time, and the efficiency drops below 50%. While the wall-clock training time may still decrease slightly with more GPUs, the *cost per useful FLOP* increases -- the organization is paying for 8,000 GPUs to do the work of 4,000. This non-linear relationship dictates that the economic viability of training frontier models is bounded not just by hardware availability but by the physics of interconnect latency. The cluster must be sized to operate in the linear regime of the scaling curve, and the model architecture (batch size, sequence length, parallelism dimensions) must be co-designed with the cluster size to maintain this balance.

::: {.callout-notebook title="Scaling Efficiency for a 175B Model"}

**Setup**: Training a `{python} gpt3_params_b`B model on a DGX H100 cluster with `{python} ib_bw_str` Gbps InfiniBand per GPU.

- **Compute per step** (assuming batch size 2M tokens, 6 FLOPs per parameter per token):
  `{python} compute_per_step_math` FLOPs

- **Per-GPU compute time** on 1,024 GPUs, each at `{python} h100_tflops` TFLOPS (50% utilization):
  `{python} t_compute_math` seconds

- **AllReduce time** for 350 GB of gradients using ring-AllReduce with overlap:
  `{python} t_comm_math` seconds (with 50% overlap)

- **Scaling efficiency**: $\eta = T_{compute} / (T_{compute} + T_{comm}) = 2.1 / 5.6 \approx 0.375$

This low efficiency (37.5%) shows why naive data parallelism at this scale is insufficient. Production systems achieve 70--90% efficiency by combining data parallelism with tensor parallelism (which communicates over NVLink) and pipeline parallelism (which overlaps computation with communication).

:::

#### Parallelism-Infrastructure Interaction {#sec-compute-parallelism-infrastructure}

\index{Parallelism!infrastructure interaction}

The scaling efficiency analysis reveals a deeper insight: the optimal parallelism strategy is not determined by the model architecture alone but by the *interaction* between the model's communication requirements and the infrastructure's bandwidth hierarchy. Each combination of parallelism strategy and infrastructure topology produces a different scaling efficiency curve, and selecting the wrong combination can waste a significant fraction of the cluster's capacity.

To illustrate this interaction concretely, consider three parallelism configurations for training our `{python} gpt3_params_b`B model on a 1,024-GPU cluster organized as 128 nodes of 8 GPUs each.

**Configuration A: Pure Data Parallelism (DP-1024).** All 1,024 GPUs replicate the full model (using ZeRO to shard optimizer states), and each GPU processes a different data shard. The gradient AllReduce exchanges 350 GB across the full InfiniBand fabric. As the napkin math above showed, this achieves approximately 37.5% efficiency because the inter-node communication dominates.

**Configuration B: TP-8, DP-128.** Within each node, 8 GPUs use tensor parallelism over NVLink. Across nodes, 128 data-parallel groups synchronize gradients over InfiniBand. Each data-parallel group now needs to AllReduce only the gradients for its 1/8 shard of the model (43.75 GB instead of 350 GB), and the TP communication is contained within the fast NVLink domain. The inter-node AllReduce time drops from 14 seconds to approximately 1.75 seconds. If the compute time is still 2.1 seconds, the efficiency improves to $2.1 / (2.1 + 1.75) \approx 54.5\%$, and with communication-computation overlap, practical efficiency reaches 75-85%.

**Configuration C: TP-8, PP-4, DP-32.** Within each node, 8 GPUs use tensor parallelism. Across 4 nodes, pipeline parallelism divides the model into 4 stages. Across the remaining 32 data-parallel groups, gradient synchronization occurs over InfiniBand. Pipeline parallelism reduces the gradient AllReduce volume further (each stage has roughly 1/4 of the parameters), and the pipeline communication (forwarding activations between stages) has lower volume than a full AllReduce. The trade-off is the **pipeline bubble**: at the beginning and end of each microbatch, some pipeline stages are idle while waiting for activations from earlier stages or gradients from later stages. The bubble fraction is approximately $1 / \text{num\_microbatches}$, so with 32 microbatches per training step, the bubble wastes roughly 3% of compute.

This three-way comparison reveals a clear pattern: configurations that keep high-bandwidth communication (tensor parallelism) within the fast NVLink domain and push only low-bandwidth communication (data-parallel AllReduce of smaller gradient shards) onto the slower InfiniBand fabric achieve the highest efficiency. The infrastructure hierarchy *dictates* the parallelism hierarchy. This principle is sometimes called **hierarchy-aware parallelism**, and it is the standard approach for all production-scale training systems.

::: {.callout-definition title="Hierarchy-Aware Parallelism"}

***Hierarchy-Aware Parallelism***\index{Hierarchy-Aware Parallelism!definition} is the strategy of mapping different parallel execution modes to the physical bandwidth tiers of the cluster.

1.  **Significance (Quantitative):** It ensures that high-frequency synchronization (e.g., Tensor Parallelism) stays on the fastest links (NVLink), while lower-frequency tasks (e.g., Data Parallelism) use slower tiers (InfiniBand). This alignment maximizes the **System Efficiency ($\eta$)** by minimizing communication stalls ($L_{lat}$).
2.  **Distinction (Durable):** Unlike **Uniform Parallelism**, which treats all node-to-node links as equal, Hierarchy-Aware strategies respect the **Bandwidth Cliffs** between die, node, and rack boundaries.
3.  **Common Pitfall:** A frequent misconception is that any model can be sharded across any number of nodes. In reality, if the **Hierarchy Mapping** is wrong (e.g., sharding a large tensor across a slow inter-rack link), the communication time will dwarf the compute time, making the scale-out useless.

:::

The interaction also flows in the reverse direction: the choice of parallelism strategy influences the optimal infrastructure design. A training system that uses TP-8, PP-4, DP-32 generates a communication pattern where the most bandwidth-intensive traffic (tensor-parallel AllReduce) is confined to within each node, the moderate-bandwidth traffic (pipeline stage communication) flows between groups of 4 neighboring nodes, and the lowest-bandwidth traffic (data-parallel AllReduce of reduced gradient shards) flows across the full cluster.

This layered communication pattern favors a hierarchical network topology where nearby nodes have higher bandwidth between them (a "locality-aware" topology) over a flat topology where all node pairs have equal bandwidth (a uniform fat-tree). Rail-optimized and hierarchical fat-tree designs exploit this locality, placing the nodes that communicate most frequently on the same switch or in the same rack, minimizing the number of switch hops for the most bandwidth-intensive traffic.

The practical implication for infrastructure procurement is that the network topology must be co-designed with the parallelism strategy, not selected independently. An organization that purchases a flat fat-tree fabric (optimized for any-to-any communication) but trains exclusively with hierarchy-aware parallelism (where most traffic is local) has over-provisioned the network's global bandwidth while potentially under-provisioning local bandwidth. Conversely, an organization that purchases a rail-optimized fabric (optimized for local communication) but later needs to run Mixture-of-Experts models with AllToAll communication (which requires global bandwidth) will find the fabric inadequate. The network fabric, which represents 10--15% of total system cost, must be matched to the anticipated workload mix, and changing the fabric after deployment is prohibitively expensive and disruptive.

@sec-distributed-training-systems provides the formal framework for selecting and combining parallelism strategies, and @sec-network-fabrics examines how network topology is co-designed with the parallelism mapping to maximize scaling efficiency.

### Fleet Monitoring and Observability {#sec-compute-fleet-monitoring}

\index{Fleet Monitoring}
\index{Observability}

Because failure is routine at pod scale, the monitoring system is not an operational convenience but a prerequisite for productive training. A 10,000-GPU cluster experiences hardware failures roughly once per hour, and the difference between a 10-minute recovery and a multi-hour disruption depends on whether the operations team can detect degradation before it becomes failure. The monitoring system is itself a distributed system, collecting telemetry from every GPU, switch, and cooling component in the fleet.

The telemetry spans three physical levels. At the GPU level, the most diagnostic signals are junction temperature (which reveals cooling degradation), ECC error counts in HBM and SRAM (where a rising rate of correctable errors often precedes a crash-inducing uncorrectable error), and SM utilization (which distinguishes hardware faults from software inefficiencies). At the network level, packet retry rates on individual ports reveal failing cables or switches. At the facility level, coolant temperature deviations trigger automated load shedding before thermal damage occurs. The challenge is not collecting these signals individually but correlating them across spatial and temporal dimensions.

The most operationally valuable monitoring capability is **correlation analysis** across these telemetry streams. A single GPU showing elevated temperature might indicate a failing fan or a coolant flow restriction. But if 8 GPUs in the same node simultaneously show elevated temperatures, the cause is more likely a node-level cooling issue. If all GPUs in a rack show elevated temperatures, the cause is probably a rack-level CDU problem. And if GPUs across multiple racks show coordinated temperature changes, the cause is likely a facility-level event such as a chiller failure. The monitoring system must detect and classify these patterns at the correct spatial scale to direct operators to the actual root cause, rather than generating hundreds of independent alerts for what is a single underlying problem.

The scale of this telemetry is non-trivial. A 10,000-GPU cluster generates approximately 1 TB of metric data per day -- junction temperatures sampled every second across 10,000 GPUs alone produce 864 million data points daily, before accounting for ECC counters, power readings, NVLink error rates, and network port statistics. Ingesting, indexing, and querying this firehose of time-series data with sub-second latency requires its own dedicated infrastructure. Organizations typically dedicate 1--2% of the cluster's total compute and storage capacity to the monitoring stack itself: time-series databases (Prometheus, InfluxDB, or custom solutions), alerting engines, and dashboard systems. This operational overhead is the cost of visibility at scale; without it, the fleet is flying blind.

Before a training job is allocated to a set of nodes, the scheduler employs **automated pre-flight checks** to verify hardware health. The control plane runs a battery of short, intensive diagnostics: GEMM benchmarks to verify Tensor Core throughput, NCCL AllReduce tests to validate NVLink and InfiniBand bandwidth, and memory stress tests to catch weak HBM bit cells that might produce uncorrectable errors under sustained load. A node that underperforms on any diagnostic is automatically quarantined for repair, and a healthy replacement is substituted before the job launches. This validation process adds 5--10 minutes to job startup time, a negligible cost compared to the hours of wasted computation when a training run crashes mid-flight due to a degraded GPU that passed a simple power-on self-test but fails under sustained arithmetic load.

The most insidious adversary in a large fleet is not the hard failure but the **gray failure** -- a component that continues to function but at degraded performance. A single GPU with a partially failed HBM stack might operate at only 75% of its peak bandwidth. An NVLink with marginal signal integrity might force frequent link retraining, causing microsecond stalls that accumulate into seconds of lost time per training step. In a synchronous data-parallel workload, a single straggler slows the entire cluster, because every other GPU must wait for the slowest participant to complete its AllReduce contribution. These gray failures are invisible to simple "up/down" health checks and require continuous, fine-grained performance benchmarking to detect. The most effective approach is to run periodic micro-benchmarks on idle nodes (or during scheduled maintenance windows) and compare each node's performance against the fleet baseline. A node whose GEMM throughput drops below 90% of the fleet median, or whose NVLink bandwidth drops below 85%, is flagged for investigation even though it has not experienced any hard error.

::: {.callout-perspective title="Proactive vs. Reactive Maintenance"}

Fleet operators have learned, often through costly experience, that proactive maintenance dramatically reduces the impact of hardware failures on training productivity. The three pillars of proactive maintenance are: **predictive diagnostics** (using models trained on historical telemetry to predict component failures 24--72 hours before they occur), **scheduled burn-in testing** (running benchmark workloads on newly installed nodes before assigning production work), and **rolling maintenance windows** (cycling 2--5% of nodes through health checks without reducing available capacity). Organizations that invest in proactive maintenance typically achieve 95--98% effective fleet utilization, compared to 80--90% for organizations that rely on reactive maintenance.

:::

In multi-tenant clusters where multiple training jobs share the same physical infrastructure, the **noisy neighbor** problem introduces a performance hazard that is invisible to individual job metrics. While containerization strictly limits CPU and memory usage, the network fabric is often a shared resource susceptible to interference. If Job A initiates a massive AllReduce operation across the spine switches just as Job B attempts to fetch training data from networked storage, the resulting micro-bursts of packet contention can throttle Job B's throughput by 30--40%. This interference is particularly pernicious in RDMA-enabled clusters where traffic bypasses the host CPU, rendering standard OS-level packet scheduling ineffective. Modern orchestration mitigates this via **static rail alignment** -- physically dedicating specific InfiniBand subnets to specific jobs -- or by deploying congestion notification protocols that throttle aggressive flows at the switch hardware level. For organizations running our `{python} gpt3_params_b`B model training alongside smaller research experiments, the safest approach is to physically partition the cluster into isolated "islands" with dedicated network fabrics, accepting the utilization penalty of fragmentation in exchange for performance predictability.

The reliability challenge is formalized by the **checkpoint-compute trade-off**, which balances the time lost to saving state against the time lost to recomputing work after a failure. The **Young/Daly formula** defines the optimal checkpoint interval $\tau$ as $\sqrt{2\delta M}$, where $\delta$ is the time to write a checkpoint and $M$ is the Mean Time Between Failures. For our `{python} gpt3_params_b`B model, the full training state -- weights, optimizer moments, and gradients -- approaches 2 TB. Even with a high-performance parallel filesystem capable of 100 GB/s write throughput, committing this state to persistent storage takes approximately $\delta \approx 20$ seconds. In a cluster of 10,000 GPUs where the system-wide MTBF is approximately 1 hour, the formula yields an optimal checkpoint interval of roughly 8.5 minutes. This creates a relentless cadence: every 8.5 minutes, the entire cluster pauses for 20 seconds to serialize its state to disk, introducing a sustained training overhead of approximately 4%. If the interval is too long, the cost of recomputing lost work after a failure exceeds the savings from fewer checkpoints; if too short, the I/O overhead dominates the training budget. @sec-fault-tolerance-reliability examines checkpointing strategies, including asynchronous checkpointing and incremental state saving, that reduce this overhead.

The choice of network topology is a direct reflection of the dominant communication patterns in the distributed training workload. A **fat-tree** topology offers full bisection bandwidth and non-blocking any-to-any communication, making it theoretically ideal for algorithms like AllReduce and AllToAll. However, the cost of the upper-tier spine switches makes it expensive at scale. A **torus or mesh** topology provides excellent nearest-neighbor bandwidth at a fraction of the switch cost but suffers severe congestion penalties for global traffic patterns. A **rail-optimized** topology physically wires the network so that each GPU rank connects to a dedicated leaf switch, maximizing the efficiency of structured AllReduce within parallelism groups. For our `{python} gpt3_params_b`B model configured with TP-8, PP-4, and DP-32, the rail-optimized topology is the natural match: the bandwidth-heavy tensor parallelism traffic is entirely contained within the NVLink domain of a single node, the pipeline parallelism traffic flows between adjacent nodes requiring only moderate point-to-point bandwidth, and the data-parallel gradient reduction happens across the full fleet but with reduced volume (each DP group synchronizes only 1/32 of the total gradients). The rail-optimized design physically matches this communication structure, placing the nodes that communicate most frequently on the same switch.

### Fleet Architecture Case Studies {#sec-compute-fleet-case-studies}

Leading organizations have made different architectural choices for their pods, and these differences are not arbitrary. Each reflects the organization's dominant workload, its hardware supply chain, and its willingness to trade flexibility for efficiency. Examining these choices concretely illustrates how the WSC principles manifest in practice.

#### NVIDIA DGX SuperPOD

The DGX SuperPOD is designed for organizations that run diverse workloads: large language models one week, vision models the next, and scientific simulations on weekends. Its network uses a **fat-tree** topology[^fn-fat-tree-topology] built from InfiniBand NDR switches, providing high-bandwidth, any-to-any connectivity with low hop counts. In a fat-tree, the aggregate bandwidth at each level of the tree matches the bandwidth of the level below, so no bottleneck exists regardless of which pairs of nodes are communicating. A typical SuperPOD configuration connects 32 DGX H100 nodes (256 GPUs) through a two-tier InfiniBand fabric, with larger installations using a three-tier fat-tree to scale to thousands of GPUs.

[^fn-fat-tree-topology]: **Fat-Tree**: Proposed by Charles Leiserson (MIT, 1985), named because links grow "fatter" (higher bandwidth) at each level of the tree hierarchy, preventing upper-tier congestion. A non-blocking fat-tree gives every node full bisection bandwidth for arbitrary traffic patterns, which is why it dominates ML clusters running diverse AllReduce and AllToAll workloads. The cost constraint is switch count: $O(N^{3/2})$ switches for $N$ endpoints in a three-tier design, making the network fabric 10--15% of total cluster cost at scale. \index{Fat-Tree!topology}

The fat-tree's strength is its flexibility. Whether the training framework uses ring-AllReduce, tree-AllReduce, or AllToAll communication patterns, the fat-tree's any-to-any connectivity provides near-optimal bandwidth for all of them. This makes it the natural choice for research environments where the communication pattern changes with each experiment.

The fat-tree also simplifies job scheduling significantly. Because any subset of nodes can communicate efficiently with any other subset, the scheduler can place jobs on whatever nodes are available without worrying about locality constraints. This scheduling flexibility improves cluster utilization because jobs do not need to wait for a specific set of contiguous nodes to become available. In a torus topology, by contrast, a training job that requires 64 nodes works most efficiently when those nodes form a compact sub-torus within the larger topology, which constrains the scheduler and can leave nodes idle even when the cluster has sufficient total capacity.

For our `{python} gpt3_params_b`B model, a SuperPOD configuration of 128 DGX H100 nodes (1,024 GPUs) connected via a two-tier InfiniBand fat-tree provides the baseline training platform. The fat-tree's full bisection bandwidth ensures that the data-parallel AllReduce of 350 GB of gradients across 128 nodes proceeds at the full InfiniBand line rate, regardless of which specific nodes are assigned to the job. This topology flexibility is particularly valuable when nodes are periodically quarantined for maintenance: the scheduler simply substitutes healthy nodes from elsewhere in the fabric without any performance penalty, maintaining training continuity.

The trade-off is cost and complexity. A fat-tree requires a large number of expensive InfiniBand switches (each costing $15,000--30,000), and the switch count grows super-linearly with the number of endpoints. For a 1,024-GPU cluster, the switch fabric alone can cost $10--20 million, representing 10--15% of total system cost. Cable management also becomes formidable: a three-tier fat-tree for 1,024 GPUs requires thousands of individual cables, each of which must be precisely routed, labeled, and tested. @sec-network-fabrics examines the fat-tree topology and its alternatives in full quantitative detail.

#### Google TPU Pod

Google's TPU pods take a fundamentally different approach to scaling. Instead of connecting chips through external switches, each TPU chip has direct **Inter-Chip Interconnect (ICI)** links built into the silicon, connecting it to its neighbors in a 3D torus[^fn-torus-topology] topology. This eliminates the switch as a separate component, reducing cost and latency at the expense of the universal any-to-any connectivity that NVSwitch provides.

[^fn-torus-topology]: **3D Torus**: From Latin *torus* (cushion, ring-shaped surface) -- a topology where nodes form a three-dimensional grid with wraparound edges. The torus eliminates switches entirely (each chip connects directly to its six neighbors), which is why Google chose it for TPU pods: at 8,960-chip scale, the switch savings over a fat-tree are substantial. The trade-off is that global communication patterns like AllToAll must traverse $O(\sqrt[3]{N})$ hops, degrading latency for non-local traffic. \index{Torus!topology}

The architectural bet is clear: Google optimizes for the predictable, nearest-neighbor communication patterns that dominate Transformer training, accepting reduced flexibility for irregular patterns like AllToAll (used by Mixture-of-Experts models). This trade-off between topology specialization and workload generality mirrors the accelerator spectrum's trade-off between efficiency and flexibility, applied at the network level rather than the silicon level.

A TPU v5p pod can scale to 8,960 chips in a single torus, providing an aggregate of approximately 459 PFLOPS of BF16 throughput. The torus topology's key advantage is its *constant bisection bandwidth per chip* as the pod scales: adding more chips to the torus extends the mesh without creating bottleneck layers (unlike a fat-tree, where the spine switches must scale with the number of endpoints). This property makes the torus economically attractive at very large scale, where the switch cost of a fat-tree becomes a significant fraction of total system cost. The trade-off is that non-local communication patterns (where data must traverse many hops across the torus) experience higher latency than in a fat-tree, which is why Google's XLA compiler includes topology-aware placement algorithms that map the model's communication graph onto the physical torus to minimize hop count for the most bandwidth-intensive operations.

| **System**            | **NVIDIA SuperPOD** | **Google TPU Pod**      | **Meta Grand Teton**      |
|:----------------------|:--------------------|:------------------------|:--------------------------|
| **Interconnect**      | InfiniBand Fat-Tree | **3D Torus ICI**        | RoCE (RDMA over Ethernet) |
| **Switching**         | External Switches   | **Direct Chip-to-Chip** | Leaf/Spine Ethernet       |
| **Topology**          | Any-to-any          | Nearest-Neighbor Mesh   | Hierarchical Rail         |
| **Optimization**      | Flexibility         | **Dataflow Throughput** | Embedding Memory          |
| **Dominant Workload** | Diverse Research    | **LLM Training**        | Recommendation            |

: **WSC Architecture Comparison**. The choice of pod architecture is not a matter of preference but of workload economics. NVIDIA prioritizes generality because their customers run diverse workloads. Google prioritizes Transformer efficiency because that is their dominant workload. Meta prioritizes embedding capacity because recommendation models are their primary revenue driver. {#tbl-pod-comparison}

#### Meta's Hybrid Recommendation Infrastructure

Meta's production ML workload is fundamentally different from language model training. Their recommendation and ranking models use embedding tables that can reach terabytes in size, far exceeding the HBM capacity of any GPU. The dense neural network layers that operate on the embeddings are relatively small and compute-intensive.

Meta addresses this with a hybrid CPU-GPU architecture called **Grand Teton**. Terabyte-scale embedding tables reside in host CPU DRAM, which costs $3--5/GB, while the dense layers execute on GPUs where the compute density justifies the $10--15/GB cost of HBM.

The embedding lookups are essentially random memory accesses: each user request retrieves a different set of embeddings based on the user's features and the items being ranked. These accesses are memory-capacity-bound (the table must be large enough to hold all embeddings) but not bandwidth-bound (each lookup retrieves only a small number of embeddings, perhaps 100--1,000 entries from a table of billions). CPU DRAM, despite its lower per-socket bandwidth, is adequate for this pattern because the data volume per lookup is small.

This hybrid approach achieves roughly 3$\times$ better cost efficiency than a GPU-only design. In a GPU-only design, the embedding table would consume most of the HBM capacity, leaving little room for the activations and gradients of the dense layers. The expensive Tensor Cores would sit idle during the memory-bound embedding lookup phase, which can represent 30--50% of total inference time for recommendation models.

By offloading the embedding table to cheap DDR, the GPUs are freed to spend 100% of their time on compute-intensive dense layer operations, where their Tensor Cores achieve high utilization. This is another manifestation of the Roofline Model's insight: rather than forcing a memory-capacity-bound workload onto a compute-optimized processor, the system assigns each workload component to the hardware whose strengths match its demands.

Meta's network architecture for these systems uses RoCE (RDMA over Converged Ethernet) rather than InfiniBand, reflecting a preference for supply chain diversity and cost over raw per-port performance. This is acceptable because recommendation model training involves less inter-node communication than LLM training. The choice also reflects Meta's scale: operating hundreds of thousands of GPUs across multiple datacenters, Meta benefits from the commodity economics and multi-vendor supply chain of Ethernet switches, which are manufactured by dozens of companies, compared to InfiniBand's single-vendor ecosystem (NVIDIA/Mellanox). At Meta's scale, the 30--50% cost savings per port on Ethernet switches translates to hundreds of millions of dollars in networking CapEx savings. @sec-network-fabrics examines the RoCE vs. InfiniBand trade-off in detail, including the protocol-level differences that affect latency and congestion behavior.

The Grand Teton architecture illustrates a broader principle: the optimal infrastructure is workload-specific, not hardware-specification-maximizing. An organization that deploys DGX H100 nodes (optimized for dense Transformer training) for recommendation model serving would waste the majority of the GPU's Tensor Core throughput on a workload that is fundamentally memory-capacity-bound. Meta's willingness to design custom server platforms -- rather than purchasing off-the-shelf DGX nodes -- reflects the economic reality that at sufficient scale, the engineering cost of a custom design is amortized across enough units to justify the investment.

#### Tesla Dojo

Tesla's Dojo represents the extreme end of the custom silicon approach at the pod level. Designed specifically for processing the millions of video clips collected daily from Tesla's vehicle fleet, Dojo uses a custom D1 chip with 1,024 cores optimized for the spatial and temporal convolution operations central to video-based vision models.

The D1 chip's architecture is radically different from both GPUs and TPUs. Instead of large matrix multiply units, each D1 core contains a small but versatile compute engine optimized for the fused convolution-activation operations that dominate video processing. The cores communicate through a 2D mesh interconnect with 4 TB/s of aggregate on-die bandwidth, enabling efficient spatial partitioning where each core processes a region of the video frame and exchanges boundary data with its neighbors.

Twenty-five D1 chips are assembled into a "training tile," with the chips arranged in a 5$\times$ 5 grid and connected via inter-chip links that extend the 2D mesh seamlessly across chip boundaries. Multiple tiles aggregate into an ExaPOD, creating a system with hundreds of thousands of cores operating as a single large mesh. The architecture is optimized for the specific communication pattern of spatially-partitioned video processing, where data locality is high and most communication occurs between neighboring tiles.

Dojo's efficiency for its target workload is exceptional, but the system cannot run Transformer-based language models efficiently because the 2D mesh topology and small per-core memory are poorly matched to the all-to-all communication and large weight matrices characteristic of Transformer training. Training our `{python} gpt3_params_b`B model on Dojo would be impractical: the 350 GB weight tensor would need to be distributed across thousands of tiles, and the all-to-all communication required for tensor parallelism would traverse dozens of hops in the 2D mesh, creating latencies that would dominate the training step. This makes Dojo a high-stakes bet on the continued centrality of vision models to Tesla's autonomous driving stack, and it illustrates the extreme end of the generality-efficiency trade-off at the system level. If Tesla's workload shifts toward large language models (as the industry trend suggests), the Dojo architecture would need fundamental redesign -- a risk that custom silicon always carries.

The choice between fat-tree, torus, and rail-optimized topologies has quantitative implications for training throughput that can alter AllReduce time by 2--3$\times$ for the same cluster size. The key insight for infrastructure planning is that topology choice is not separable from workload selection: Transformer training with its regular AllReduce patterns favors different topologies than Mixture-of-Experts models with their AllToAll communication. The network fabric cost (10--15% of total system cost) is easily justified if it improves scaling efficiency by even a few percentage points, because poor scaling wastes the other 85--90% of the investment. @sec-network-fabrics provides the rigorous quantitative comparison of these topologies, including formal bandwidth analysis and the mathematical framework for computing bisection bandwidth and path diversity.

The cost structure of the network fabric itself warrants quantitative examination. An InfiniBand NDR switch with 64 ports of 400 Gbps costs \$15,000--30,000, with specialized active optical cables adding \$500--1,000 per link. A two-tier fat-tree for 1,024 GPUs (128 nodes) requires approximately 64 leaf switches and 32 spine switches, plus roughly 4,000 cables, bringing the total fabric cost to \$5--15 million depending on the oversubscription ratio. Ethernet-based alternatives using RoCE (RDMA over Converged Ethernet) reduce the per-port cost by 30--50%, but introduce a latency and reliability penalty. Ethernet is inherently lossy: when switch buffers overflow under the bursty, synchronized traffic patterns of distributed training, packets are dropped and must be retransmitted. For our `{python} gpt3_params_b`B model training on 1,024 GPUs, even a 1% packet retransmission rate on a RoCE fabric can degrade effective AllReduce throughput by 10--20%, because thousands of GPUs wait for the slowest participant. The cost-performance frontier analysis asks whether the \$3--5 million savings of Ethernet justifies the potential 10--20% throughput loss. For frontier model training where time-to-market is paramount and the GPU fleet represents a \$35+ million investment, the InfiniBand premium pays for itself by ensuring the network never becomes the bottleneck.

#### Site Selection and Physical Constraints

Before any accelerators are purchased or racks are populated, the most consequential decision is *where* to build the pod. Site selection for ML training infrastructure is driven by three physical constraints that are often in tension.

**Power availability** is the first and often most binding constraint. A 10,000-GPU pod requires 10--15 MW of continuous power, equivalent to a medium-sized factory. Locations near hydroelectric dams (Pacific Northwest, Scandinavia, Quebec) offer abundant power at low carbon intensity, but these power-rich sites are often remote from talent pools and existing fiber optic infrastructure. The trade-off between cheap power and proximity to engineers is one of the most consequential site-selection decisions, and at megawatt scale the difference between a favorable and unfavorable electricity rate can represent tens of millions of dollars over a hardware lifecycle.

**Cooling environment** is the second constraint. Sites in temperate or cold climates can achieve lower PUE through **free cooling** (using outside air directly, without mechanical refrigeration) for much of the year. Meta's datacenter in Lulea, Sweden, where the average annual temperature is 1 degree Celsius, achieves PUE values approaching 1.03 through year-round free cooling. Sites in hot, arid regions face both higher cooling costs and potential water scarcity for evaporative cooling towers.

**Network connectivity** is the third constraint. Training clusters that ingest data from geographically distributed sources need substantial WAN bandwidth, while serving clusters need proximity to major internet exchange points to minimize end-user latency. Some organizations separate training and serving infrastructure entirely, placing training pods in power-rich remote locations and serving pods near population centers, connected by dedicated fiber links for model checkpoint transfer.

These three constraints are frequently in tension, and no single site optimizes all of them simultaneously. Organizations that anticipate building multiple facilities often diversify geographically, placing training-heavy workloads in power-rich regions and latency-sensitive serving workloads near users. The right trade-off depends on the organization's workload portfolio, financial structure, and time horizon.

Regulatory and geopolitical boundaries increasingly dictate site selection as rigidly as physics. Export controls on high-performance compute act as a hard filter: a cluster designed for 175B-parameter model training requires H100-class silicon that is legally restricted from export to certain nations, rendering otherwise power-rich geographies viable only for inference on older hardware. Data sovereignty laws like GDPR (Europe) or local residency mandates create a parallel fragmentation, compelling organizations to keep both training data and model weights within specific legal jurisdictions. This regulatory layer often forces a suboptimal thermodynamic choice, requiring the construction of facilities in expensive, power-constrained regions to satisfy compliance rather than efficiency.

The resulting **power alley** phenomenon has concentrated gigawatt-scale capacity in a few specific zones: Northern Virginia (Ashburn) for connectivity, the Pacific Northwest for cheap hydroelectric power, and the Nordic countries for free cooling and renewable energy. This geographic clustering creates its own crisis: local grids in these hubs face 3--5 year waitlists for new substation capacity, and land prices have appreciated dramatically. As prime zones saturate, the frontier of compute infrastructure is shifting to unconventional "brownfield" sites -- retired aluminum smelters, defunct coal plants, and industrial parks with existing high-voltage transmission -- where power capacity exists but fiber and cooling infrastructure must be built from scratch.

Water scarcity is the final, increasingly volatile constraint. A 10 MW training facility using standard evaporative cooling towers consumes 10--20 million liters of water annually. In water-stressed regions like the American Southwest or parts of the Middle East, this consumption places datacenters in direct competition with municipal and agricultural needs, leading to moratoriums on new construction permits. Operators are increasingly forced to adopt closed-loop liquid cooling or fully adiabatic dry cooler systems, which reduce water consumption to near zero but increase capital intensity and may impose a PUE penalty of 10--15% compared to evaporative systems in hot climates.

#### Construction and Deployment Timelines

The timeline from decision to first computation is a critical planning constraint that many organizations underestimate. Building a new datacenter facility from scratch takes 18--30 months, with the longest-lead items being the electrical substation (18--24 months) and the building shell (12--18 months). GPU lead times during periods of high demand can reach 6--12 months and run concurrently with facility construction. The coordination challenge is ensuring that hardware arrives when the facility is ready to receive it, neither too early (requiring expensive temporary storage) nor too late (leaving a completed facility idle).

The practical consequence is that infrastructure decisions must be made 18--30 months before the infrastructure is needed. For an organization planning to train a frontier model in 2027, the datacenter decisions must be made in 2025. Given that GPU architecture generations span roughly two years, the infrastructure team is often selecting a facility design for hardware that does not yet exist, based on preliminary specifications from the manufacturer. To mitigate this temporal mismatch, experienced infrastructure teams design facilities with **headroom** for future generations: oversizing the electrical infrastructure by 30--50%, designing the cooling plant for higher heat densities than the initial deployment requires, and specifying flexible rack layouts. The additional upfront cost of this headroom (typically 10--20% of facility CapEx) is justified by the ability to upgrade compute hardware without reconstructing the facility.

The construction timeline is governed by a **critical path** that aligns two disparate workstreams: the facility track (site selection, power provisioning, building shell, cooling) and the hardware track (silicon allocation, server assembly, network integration). While GPU procurement often captures headlines, the true bottleneck is invariably the electrical substation, which requires 18--24 months for permitting and transformer delivery -- a timeline that cannot be compressed by spending more. This creates a high-stakes synchronization problem. An organization that secures a 20,000-GPU allocation ($500M+) before its facility is ready risks depreciating state-of-the-art silicon in a warehouse for months. Conversely, completing a $200M datacenter shell only to wait 6--12 months for an accelerator allocation leaves massive capital assets idle.

To mitigate these synchronization risks, experienced infrastructure teams use **phased deployment**. Rather than targeting a single "go-live" date for the entire cluster, the facility is commissioned in waves -- often bringing up an initial 10--20% of capacity (2,000 GPUs out of 10,000) while the remaining construction continues. This early phase serves a dual purpose: it allows the software team to begin validating the distributed training stack and tuning collective communication kernels on the specific hardware topology, while simultaneously stress-testing the physical infrastructure. Issues like cooling hotspots, phase imbalances in power distribution, or cabling defects are identified and remediated at small scale, preventing catastrophic failures when the full load is applied.

These logistical contortions are driven by the **time value of compute**. In the race for frontier model development, the cost of delay is not merely the interest on capital but the loss of strategic position. If a frontier model is projected to generate \$10 million per month in value -- whether through direct inference revenue or downstream product capabilities -- a three-month construction delay represents a \$30 million loss, often exceeding the cost of the facility's electrical infrastructure entirely. This calculus justifies aggressive expediting measures, such as paying 2--3$\times$ premiums for pre-fabricated modular datacenters or leasing temporary colocation space to bridge the gap between silicon delivery and facility readiness.

The question that ultimately determines whether any of this infrastructure gets built is: *what does it cost?*

## Economics: Total Cost of Ownership {#sec-compute-tco}

\index{Total Cost of Ownership}
\index{CapEx}
\index{OpEx}

The preceding sections have mapped the physical infrastructure from the transistor to the datacenter, establishing the engineering constraints at each level. But engineering constraints are ultimately economic constraints: every design decision trades cost against performance, and the "right" infrastructure is the one that maximizes useful computation per dollar over the system's lifetime. A 10,000-GPU cluster of H100s represents roughly \$3.5 billion in hardware acquisition cost. But the purchase price is only the beginning. Over a typical three-year hardware lifecycle, power and cooling costs can equal or exceed the original hardware investment. Facility construction, networking, staffing, and maintenance add further. Understanding the full financial picture requires **Total Cost of Ownership (TCO)**[^fn-tco-framework] analysis, which encompasses every cost incurred from the day the first shovel breaks ground to the day the last GPU is decommissioned.

The economics of ML infrastructure differ from traditional IT in three fundamental ways. First, the hardware depreciates faster than any other IT category, with each new accelerator generation delivering 2--3$\times$ the performance per watt and rendering previous generations economically obsolete within 3--4 years. Second, the power consumption is an order of magnitude higher per rack, making electricity a first-order cost rather than a rounding error. Third, the utilization sensitivity is extreme: the same cluster can be a brilliant investment at 80% utilization or a financial disaster at 20% utilization, with no change in hardware or facility costs. These three factors make TCO analysis not just useful but essential for any organization considering ML infrastructure investment.

[^fn-tco-framework]: **TCO (Total Cost of Ownership)**: A financial framework, formalized by Gartner in the 1980s for IT procurement, that sums CapEx (one-time acquisition) and OpEx (recurring operation) over a system's lifecycle. For ML clusters, TCO analysis is uniquely consequential because power costs can equal or exceed hardware cost over three years, and a 60-percentage-point swing in utilization (20% to 80%) can flip the build-vs-buy decision entirely without changing any hardware specification. \index{TCO!financial framework}

### TCO Breakdown {#sec-compute-tco-breakdown}

The total cost of an ML cluster decomposes into two broad categories. **Capital expenditure (CapEx)** covers the one-time costs of building the infrastructure: accelerators, servers, networking equipment, facility construction, and installation. **Operational expenditure (OpEx)** covers the recurring costs of running it: electricity, cooling, network bandwidth, staffing, maintenance, and software licenses. For a large on-premises cluster, the approximate breakdown is:

- **Accelerators and servers** (50--60% of CapEx): The GPUs or TPUs themselves, along with the host servers, baseboard management controllers, and local storage.
- **Networking** (10--15% of CapEx): InfiniBand switches, HCAs, cables, and optical transceivers. A fat-tree fabric for 1,000 GPUs can cost \$10--20 million.
- **Facility** (15--25% of CapEx): Building construction or retrofit, electrical infrastructure (transformers, UPS, PDUs), and cooling plant (chillers, piping, pumps). Liquid cooling infrastructure adds 10--15% to facility costs but reduces long-term OpEx.
- **Electricity** (60--70% of OpEx): At \$0.07/kWh, a 1,000-GPU H100 cluster consuming 1 MW (including cooling at PUE 1.1) costs approximately \$615,000 per year in electricity alone.
- **Staffing and maintenance** (20--30% of OpEx): System administrators, hardware technicians, replacement parts, and software license fees.

For our `{python} gpt3_params_b`B model, a minimum viable training cluster requires approximately 1,000 H100 GPUs spread across 125 nodes to complete a training run in 2--4 weeks. Evaluating the TCO over a three-year lifecycle reveals a stark utilization dependency. The hardware CapEx dominates at \$43.75 million (\$350,000 per node), supported by a \$5 million investment in a two-tier InfiniBand fat-tree network and a proportional \$10 million facility allocation. Operational costs add approximately \$1.5 million annually for electricity (at \$0.07/kWh with a PUE of 1.1) and specialized staffing, bringing the three-year total to roughly \$63 million. If this dedicated cluster only trains six frontier models per year, the effective cost per run is \$3.5 million. Conversely, executing the same workload on the public cloud at \$4.00 per GPU-hour with 80% utilization would cost approximately \$1.075 million per run, totaling \$19.3 million over three years -- less than a third of the on-premises investment. The economic advantage of owning hardware only materializes at *continuous utilization*: if the cluster runs 24/7 (supporting not just training but also inference, fine-tuning, and experimentation), the effective on-premises cost drops to approximately \$2.40 per GPU-hour, significantly undercutting the cloud rate. This utilization dependency is the central tension in every build-vs-buy analysis.

### Build vs. Buy {#sec-compute-build-vs-buy}

The most consequential infrastructure decision is whether to build an on-premises cluster or rent capacity from a cloud provider. This decision depends primarily on one variable: sustained utilization.

Cloud providers charge by the GPU-hour. At approximately \$4.00 per H100-hour, a single 8-GPU node running at `{python} util_pct_str`% utilization costs `{python} cloud_cost_display` per year. An on-premises DGX H100 node costs approximately \$350,000 to purchase. Amortized over three years and combined with electricity costs of `{python} annual_elec_display` per node per year, the total annual on-premises cost is approximately `{python} onprem_display` per node. On-premises infrastructure becomes favorable when sustained utilization exceeds roughly `{python} breakeven_util`%.

Several factors complicate this simple comparison. Cloud pricing is highly dynamic, with multiple pricing tiers that can significantly shift the economics. Reserved instances (1--3 year commitments) reduce the effective hourly rate by 40--60%, narrowing the gap between cloud and on-premises costs. For a 3-year commitment, the effective cloud cost for our 8-GPU node drops from `{python} cloud_cost_display` to approximately \$100,000--140,000 per year, which approaches the on-premises cost.

Spot or preemptible instances offer even deeper discounts of 60--80%, but they can be interrupted at any time when the cloud provider needs the capacity for higher-paying customers. This makes them suitable for fault-tolerant workloads (where frequent checkpointing allows the job to resume after interruption) but risky for long-running training jobs where interruption costs hours of lost computation and checkpoint restoration time.

On the on-premises side, the analysis above excludes facility construction costs, which can add \$500--1,000 per kW of IT capacity, as well as networking equipment, staffing, and maintenance. A comprehensive TCO analysis must also account for the time value of money: CapEx is a large upfront payment, while cloud OpEx is spread over time, which matters for organizations that value capital preservation or have limited access to financing.

Hardware obsolescence risk further complicates the comparison. A three-year-old GPU may be economically stranded if a new generation delivers 3$\times$ more performance per watt. The organization has already paid for the hardware and cannot recover its investment, whereas a cloud customer simply switches to the latest instance type when the new generation launches.

The build-vs-buy decision is therefore not a simple arithmetic exercise but a strategic choice that depends on an organization's financial structure, workload predictability, and tolerance for operational complexity. An organization with access to cheap capital (low interest rates on debt or strong cash reserves) and predictable, sustained GPU demand benefits from ownership. An organization with limited capital, uncertain demand, or rapidly evolving hardware requirements benefits from the cloud's pay-as-you-go flexibility.

Many organizations find that a hybrid approach, using owned infrastructure for predictable baseline workloads and cloud for peak demand or experimental workloads, provides the best balance of cost and flexibility. The hybrid model captures the cost advantage of ownership for the steady-state workload (which justifies the capital investment) while using the cloud's elasticity for the variable portion (avoiding the risk of over-provisioning owned hardware for peak demand that occurs only occasionally).

For our `{python} gpt3_params_b`B model, the build-vs-buy decision depends critically on the organization's training cadence. An organization that trains one frontier model per year and serves it for the remaining 11 months faces a fundamentally different calculus than one that continuously trains, fine-tunes, and experiments. The single-model organization would achieve perhaps 15--20% utilization on owned hardware (2--4 weeks of training out of 52 weeks), making cloud rental overwhelmingly cheaper. The continuous-training organization, running back-to-back experiments, hyperparameter sweeps, and model variants, can sustain 70--80% utilization, making owned hardware the clear winner. The hybrid approach serves the middle ground: own enough hardware for the continuous baseline workload (fine-tuning, inference, experimentation) and burst to the cloud for the periodic frontier training runs that temporarily require 5--10$\times$ the baseline capacity.

#### Operational Complexity

The TCO analysis often understates the operational complexity of running an on-premises ML cluster. Unlike traditional server infrastructure, which can be managed with general-purpose IT skills, ML clusters require specialized expertise in several areas.

**Hardware maintenance** for ML clusters involves diagnosing and replacing failed GPUs, NVLink cables, InfiniBand HCAs, and cooling components. A 10,000-GPU cluster experiencing one GPU failure per hour (as predicted by MTBF analysis) requires a hardware team that can diagnose, isolate, and replace the failed component within hours, not days. This requires maintaining an inventory of spare parts on site, including spare GPUs (\$20,000--30,000 each), NVLink cables, InfiniBand switches, and cooling components.

**Software stack management** encompasses the CUDA toolkit, GPU drivers, InfiniBand drivers, container runtime (typically Docker or Enroot), job scheduler (Slurm, Kubernetes, or a custom scheduler), monitoring and alerting systems, and the training frameworks themselves. These components interact in complex ways, and version incompatibilities between the GPU driver and the CUDA toolkit, for example, can silently degrade performance or cause intermittent failures. A typical large-scale training cluster employs 5--15 infrastructure engineers per 10,000 GPUs to manage these systems.

**Performance optimization** is an ongoing activity, not a one-time setup. As models, frameworks, and hardware configurations change, the cluster's communication patterns change, requiring updates to the network configuration, parallelism strategies, and memory management settings. Teams that treat infrastructure as "set and forget" after initial deployment typically achieve 30--40% lower utilization than teams with dedicated performance engineers who continuously monitor and tune the system.

Cloud providers absorb all of this operational complexity into their service, which is one reason that cloud pricing includes a substantial margin above the raw hardware and electricity costs. For organizations that lack the specialized engineering talent to operate a large cluster, the cloud's operational simplicity may justify its higher per-GPU-hour cost.

The staffing challenge deserves particular emphasis. The labor market for engineers who can operate large-scale GPU clusters is extraordinarily thin. The required skill set spans GPU kernel optimization, InfiniBand network administration, distributed systems debugging, liquid cooling maintenance, and power systems engineering -- a combination that few individuals possess and that no university program explicitly trains. Organizations building on-premises clusters often find that recruiting and retaining this talent is more difficult and more expensive than procuring the hardware itself. A single senior infrastructure engineer with experience operating 10,000+ GPU clusters can command compensation exceeding \$500,000 per year, and the loss of such an individual can measurably degrade cluster utilization for months until a replacement is found and onboarded.

#### From TCO to Total Value of Ownership

A more complete framework than TCO is **Total Value of Ownership (TVO)**, which considers not only the costs but also the *value* generated by the infrastructure. Two clusters with identical TCO may generate very different value if one achieves higher utilization, faster time-to-result, or better model quality.

The value side of the equation includes:

- **Time-to-market advantage**: An organization that trains a model 2 weeks faster than competitors may capture significantly more market share, generating value that dwarfs the infrastructure cost difference.
- **Model quality**: A cluster with better scaling efficiency (due to superior networking or more recent GPUs) can train for more steps within the same time budget, potentially producing a higher-quality model.
- **Experimentation velocity**: Infrastructure that enables rapid iteration (quick job startup, fast checkpointing, seamless scaling) accelerates the research cycle, allowing more experiments per unit time. The value of each experiment is hard to quantify but compounds over time.
- **Inference cost**: For organizations that serve trained models at scale, the cost of inference hardware and electricity may dominate the training cost by 10$\times$ or more over the model's lifetime. An infrastructure decision that reduces inference cost per token (for example, by training a more efficient model using more training compute) can generate enormous downstream value.

This value-oriented perspective often changes the optimal infrastructure decision. A team that evaluates infrastructure purely on TCO may choose the cheapest option, which saves money on infrastructure but produces a slower research cycle and lower-quality models. A team that evaluates on TVO may choose a more expensive infrastructure option that pays for itself through faster iteration and better models.

The inference dimension of TVO deserves particular emphasis because it often dominates the total economic picture. Training our `{python} gpt3_params_b`B model is a one-time cost -- even at \$5 million per training run, it is a bounded expenditure. Serving the trained model, however, is an ongoing operational cost that accumulates indefinitely. A popular LLM serving 10 million queries per day, with each query generating an average of 500 tokens, processes 5 billion tokens daily. At an inference cost of \$2.00 per million tokens on H100 hardware, the daily serving cost is \$10,000, or approximately \$3.6 million per year. The cumulative inference cost exceeds the training cost within 6 months. This inversion means that infrastructure decisions optimized for training (maximizing TFLOPS per dollar) may be suboptimal for the model's total lifecycle cost. An organization that spends an additional \$2 million on training infrastructure to produce a model that is 20% more efficient at inference (through better architecture search enabled by faster experimentation) can recover that investment within months of deployment at scale.

::: {.callout-notebook title="The 10,000-GPU Cluster"}

Consider a cluster of 1,250 DGX H100 nodes (10,000 GPUs) for training our `{python} gpt3_params_b`B model.

**On-Premises (3-year lifecycle)**:

- **Hardware CapEx**: 1,250 nodes$\times$ \$350,000 = \$437.5M
- **Network CapEx**: ~\$25M (InfiniBand fat-tree fabric)
- **Facility CapEx**: ~\$75M (liquid-cooled datacenter hall)
- **Annual Electricity**: 10,000 GPUs$\times$ 700 W$\times$ PUE 1.1$\times$ 8,760 h$\times$ \$0.07/kWh = \$4.7M/year
- **Annual Staffing**: ~\$5M/year
- **3-Year Total**: \$537.5M + 3$\times$ \$9.7M = **~\$567M**

**Cloud (3-year rental at 80% utilization)**:

- **Annual Cost**: 10,000 GPUs$\times$ \$4.00/hr$\times$ 8,760 h$\times$ 0.80 = \$280M/year
- **3-Year Total**: **~\$840M**

**Break-Even**: On-premises saves approximately \$273M over three years at 80% utilization. The savings disappear if utilization drops below ~40%, because the on-premises hardware still incurs facility and staffing costs regardless of load.

:::

::: {.callout-perspective title="The Infrastructure Moat"}

The economics of ML infrastructure create a self-reinforcing advantage for organizations that can sustain high utilization. Building a 10,000-GPU cluster saves hundreds of millions over cloud rental, but only if the organization has enough workloads to keep it busy. Large technology companies with continuous training pipelines, frequent model refreshes, and massive inference workloads achieve 70--90% utilization, making on-premises infrastructure highly cost-effective. Smaller organizations with sporadic training needs may achieve only 20--30% utilization, making cloud rental cheaper despite the higher per-hour cost. This dynamic creates an infrastructure moat: organizations with scale can afford to build, and building makes scale cheaper, which enables more ambitious models, which require more infrastructure. The gap compounds over time, explaining why a small number of organizations dominate frontier model development.

:::

### Depreciation and Lifecycle {#sec-compute-depreciation}

\index{Depreciation}

ML accelerators depreciate faster than any other category of IT equipment. Traditional servers have useful lifetimes of 5--7 years; network switches last even longer. ML accelerators become economically obsolete in 3--4 years because each new generation delivers 2--3$\times$ the performance per watt. An organization operating V100 GPUs in 2024 is paying the same electricity bill per GPU as in 2017 but receiving only 1/15 the throughput per watt compared to H100s. The electricity cost per unit of useful computation is therefore 15$\times$ higher than a team with current-generation hardware.

The economic impact of this depreciation is significant. A V100 GPU that cost $10,000--12,000 in 2018 could be purchased on the secondary market for $2,000--3,000 in 2023, a depreciation of 70--80% in five years. An A100 GPU that cost $15,000--20,000 in 2021 traded for $8,000--12,000 in 2024, after just three years. These depreciation rates are far steeper than traditional IT equipment, reflecting the rapid pace of accelerator innovation.

This rapid depreciation has several implications for fleet management:

- **Accelerated write-off**: Most organizations depreciate ML accelerators over 3 years for accounting purposes, even if the hardware physically functions for longer. After 3 years, the residual value of the hardware is near zero because the secondary market is flooded with previous-generation equipment.
- **Staggered refresh**: Rather than replacing the entire fleet simultaneously (which requires enormous CapEx outlays), some organizations refresh one third of their fleet each year, maintaining a mixed-generation cluster. This approach reduces peak CapEx but complicates software optimization, as training frameworks must handle heterogeneous hardware with different compute capabilities, memory capacities, and communication bandwidths.
- **Resale and repurposing**: Older accelerators that are inefficient for training may still be cost-effective for inference, where memory bandwidth (which improves more slowly across generations) is the binding constraint. Some organizations sell their training-retired GPUs to smaller companies or repurpose them for less compute-intensive tasks like fine-tuning or small-model serving.
- **Lease-back arrangements**: Some GPU-as-a-service providers offer lease-back programs where an organization purchases GPUs, leases them back to the provider when not in use, and receives the GPUs on demand when needed. This reduces the effective cost of ownership by generating revenue during idle periods.

The depreciation challenge is particularly acute for organizations that train models infrequently. If a team trains one frontier model per year, the GPUs sit idle for months between training runs. The depreciation during those idle months represents pure loss, as the hardware's value decreases with calendar time regardless of whether it is being used.

This is one reason why the cloud is often more cost-effective for organizations with bursty workloads: they pay only for the GPU-hours they use, and the cloud provider ensures the hardware is in use during the gaps between one customer's training runs. The cloud provider amortizes the hardware across multiple customers, achieving 70-90% utilization even when each individual customer uses the hardware only intermittently.

Organizations with bursty workloads that still prefer owned hardware can partially mitigate the depreciation cost by renting out their idle capacity to other organizations through GPU-as-a-service platforms. Several companies have built businesses around this model, purchasing GPU clusters, renting them to training customers, and achieving economics that work only because high utilization across multiple customers amortizes the depreciation effectively.

For our `{python} gpt3_params_b`B model, the depreciation calculus is stark. A 1,000-GPU H100 cluster purchased in 2024 for $35 million will have a resale value of approximately $7--10 million by 2027, when the next-next-generation accelerators (post-Blackwell) are expected to deliver 3--4$\times$ the performance per watt. If the cluster trained only two frontier models during its lifetime, each model effectively cost $12--14 million in depreciated hardware alone -- before accounting for electricity, staffing, or facility costs. If the same cluster ran continuously at 80% utilization for three years (training, fine-tuning, inference, and experimentation), the depreciated hardware cost per GPU-hour drops to approximately $1.50, well below the cloud rate. The depreciation math reinforces the central lesson of TCO analysis: utilization is the single most important variable in determining whether owned infrastructure is economically viable.

### Power Efficiency Trajectory {#sec-compute-power-efficiency}

\index{Power Efficiency}

The trajectory of power efficiency across accelerator generations provides a quantitative framework for making cluster refresh decisions. Replacing an older cluster with a newer generation can pay for itself through electricity savings alone, particularly at scale.

| **Generation**  | **TFLOPS (FP16)** | **TDP (W)** |    **TFLOPS/W**    | **Relative Efficiency** |
|:----------------|------------------:|------------:|:------------------:|:-----------------------:|
| **V100 (2017)** |               125 |         300 | `{python} v100_ef` |       1.0$\times$       |
| **A100 (2020)** |               312 |         400 | `{python} a100_ef` |       1.9$\times$       |
| **H100 (2022)** |              1979 |         700 | `{python} h100_ef` |       6.8$\times$       |
| **B200 (2024)** |              4500 |        1000 | `{python} b200_ef` |       10.8$\times$      |

: **Power Efficiency Across GPU Generations**. Each generation delivers substantially more computation per watt, meaning that for a fixed power budget, newer hardware provides multiplicatively more throughput. A facility that draws 10 MW can train models roughly 10$\times$ faster with B200s than with V100s, without any increase in electricity cost. {#tbl-power-efficiency}

The implication is that hardware refresh cycles are not merely about getting more FLOPS; they are about getting more *work per dollar of electricity*. At scale, the electricity savings from upgrading to a more efficient generation can amortize a significant fraction of the new hardware's purchase cost within the first year. This economic dynamic drives the rapid depreciation of ML accelerators: a three-year-old GPU is not just slower than the current generation; it is *more expensive to operate* per unit of useful computation.

Consider a concrete refresh scenario. An organization operating 1,000 V100 GPUs (300 W each, `{python} v100_ef` TFLOPS/W) consumes 300 kW of IT power for 125,000 TFLOPS of aggregate throughput. Replacing them with 1,000 H100 GPUs (700 W each, `{python} h100_ef` TFLOPS/W) increases power consumption to 700 kW but delivers 1,979,000 TFLOPS, a 15.8$\times$ throughput increase for a 2.3$\times$ power increase.

Alternatively, the organization could match the V100 fleet's throughput with roughly 63 H100 GPUs, consuming only 44 kW and freeing 256 kW of power capacity for other workloads. The optimal strategy depends on whether the organization is throughput-constrained (wants to train larger models) or power-constrained (has a fixed electrical budget).

Both scenarios demonstrate that generational efficiency improvements fundamentally reshape the economics of the fleet. The power-constrained case is particularly instructive: a datacenter with a fixed 300 kW power budget can deliver 15.8$\times$ more computation by replacing V100s with H100s, even though it can only install 43% as many GPUs. Power, not procurement budget, is increasingly the binding constraint for fleet expansion.

The interplay between CapEx and OpEx also shapes procurement strategy. Cloud providers amortize their hardware over shorter periods (often 18--24 months) because they can sell older-generation instances at lower prices to price-sensitive customers, extracting residual value. On-premises operators typically amortize over 3--5 years, accepting that the hardware's relative performance declines over time.

Some organizations adopt a hybrid approach: running baseline workloads on owned infrastructure for cost efficiency and bursting to the cloud for peak demand or for early access to the latest hardware generation before committing to a large purchase. This hybrid model is increasingly common among mid-sized AI companies that have a steady-state training workload (justifying owned hardware) but periodically need 2--3$\times$ their base capacity for new model training campaigns.

The power efficiency trajectory has a direct implication for our `{python} gpt3_params_b`B model's training economics. Training on 1,000 V100 GPUs would require approximately 300 kW of IT power and take roughly 8 months (given the V100's lower throughput). Training on 1,000 H100 GPUs requires 700 kW but completes in approximately 2--4 weeks. The H100 cluster consumes 2.3$\times$ more power per unit time but finishes 8--16$\times$ faster, resulting in a net energy reduction of 3.5--7$\times$ for the same training run. When electricity costs $0.07/kWh, the V100 training run costs approximately $120,000 in electricity while the H100 run costs approximately $25,000. The newer hardware is simultaneously faster, cheaper to operate, and more energy-efficient -- a rare alignment that makes hardware refresh decisions straightforward for organizations with the capital to invest.

### GPU Procurement and Supply Chain {#sec-compute-procurement}

\index{GPU Procurement}
\index{Supply Chain}

The economics of ML infrastructure are not purely about hardware specifications and electricity rates. The *procurement process itself* introduces costs, risks, and constraints that must be factored into infrastructure planning.

The GPU supply chain is unusually concentrated. NVIDIA holds approximately 80--90% of the datacenter GPU market for ML workloads. TSMC manufactures essentially all high-end GPU dies. A small number of companies (SK Hynix, Samsung, and Micron) produce the HBM stacks. This concentration means that a disruption at any single point in the supply chain, whether a natural disaster at a fab, an equipment failure at an HBM manufacturer, or a geopolitical event affecting chip exports, can delay GPU deliveries across the entire industry.

The practical consequence of this concentration is that procurement timelines for large deployments (1,000+ GPUs) typically span 6--12 months from purchase decision to first production training job, encompassing negotiation, manufacturing, shipping, installation, and burn-in testing. During periods of intense demand (such as 2023--2024), this can stretch to 12--18 months. These lead times make GPU procurement a first-order planning constraint: organizations must commit capital and secure allocations months before the hardware is needed, often before the facility to house it is complete. The 6--12 month procurement horizon, combined with the 18--30 month facility construction timeline discussed earlier, means that infrastructure teams must plan two to three years ahead, making procurement strategy inseparable from the broader capacity planning process.

For our `{python} gpt3_params_b`B model, the procurement challenge is acute. A minimum viable training cluster of 1,000 H100 GPUs represents approximately $35 million in hardware alone. At this scale, the organization is not purchasing off-the-shelf products but negotiating directly with NVIDIA for an allocation from a constrained production pipeline. The negotiation involves not just price but delivery schedule, warranty terms, and often a commitment to purchase future generations. Organizations that delay procurement by even one quarter may find their allocation pushed back by 6--12 months, during which time a competitor with earlier access to the same hardware can complete a training run and capture the market advantage. This first-mover dynamic has led some organizations to commit hundreds of millions of dollars to GPU procurement before their models or training recipes are fully designed, treating hardware access as a strategic asset rather than a commodity input.

### Cloud Infrastructure Options {#sec-compute-cloud-options}

\index{Cloud Computing!ML infrastructure}

For organizations that choose the cloud path, the major providers (AWS, Google Cloud, Microsoft Azure) offer GPU and custom accelerator instances with different trade-offs in performance, availability, and cost. The specific instance types and pricing change frequently, but three enduring architectural distinctions shape the cloud ML landscape.

First, providers differ in their **interconnect fabric**. Some offer InfiniBand connectivity between GPU instances (providing RDMA with the same performance characteristics as on-premises clusters), while others use custom Ethernet-based fabrics with RDMA-like capabilities. The choice of fabric affects scaling efficiency for large training jobs: InfiniBand-connected instances can match on-premises AllReduce performance, while Ethernet-based fabrics may introduce 10--20% higher communication latency.

Second, providers differ in their **provisioning granularity**. Some provision individual GPU instances that the user must configure into a cluster, while others offer pod-level provisioning where the provider delivers a pre-configured, pre-connected cluster as a single resource. Pod-level provisioning simplifies networking configuration (one of the most error-prone aspects of cloud-based training) but reduces flexibility in cluster composition.

Third, all major providers offer **custom accelerator** options (purpose-built training and inference ASICs) alongside GPU instances. These custom chips apply the same generality-efficiency trade-off we observed at the silicon level: lower cost per operation for supported workloads, but reduced flexibility for non-standard architectures. The choice between GPU and custom accelerator instances mirrors the on-premises accelerator selection decision.

The pricing models across providers share three common tiers. **On-demand** instances provide immediate access at the highest per-hour cost. **Reserved instances** (1--3 year commitments) reduce costs by 40--60% but require upfront commitment and risk hardware obsolescence. **Spot/preemptible** instances offer 60--80% discounts but can be interrupted with minimal notice, making them suitable only for fault-tolerant workloads with frequent checkpointing. The choice between tiers is a risk management decision, balancing cost against the probability and impact of interruption.

The economics of spot instances deserve particular attention because they can dramatically reduce training costs for organizations with the engineering sophistication to exploit them. At a 70% discount, spot H100 instances cost approximately $1.20 per GPU-hour instead of $4.00. For our `{python} gpt3_params_b`B model requiring approximately 25,000 GPU-hours of training, the savings are substantial: $30,000 on spot versus $100,000 on demand. However, the stochastic nature of preemption transforms training from a deterministic process into a fault-tolerance engineering problem. If the cloud provider reclaims 5% of the nodes mid-training, a standard training job crashes instantly. Leveraging spot economics requires an elastic training framework (such as TorchElastic) that can dynamically rebalance the computation graph when nodes are added or removed. The economic viability hinges on the **checkpoint tax**: if the system must checkpoint every 10 minutes to limit data loss from preemption, and each checkpoint takes 30 seconds, approximately 5% of the "cheap" compute is consumed by I/O overhead. There exists a break-even point where the frequency of preemption events combined with checkpoint overhead makes spot instances more expensive in wall-clock time than reserved instances, despite the lower hourly rate.

A critical consideration for cloud-based ML infrastructure is **networking between instances**. Unlike on-premises clusters where the network topology is custom-designed, cloud instances share the provider's network fabric with other tenants. Most cloud providers now offer placement groups or dedicated networking fabrics that guarantee InfiniBand or equivalent bandwidth between instances within the same group.

However, cross-group or cross-zone communication may traverse shared infrastructure with lower bandwidth and higher latency. Training frameworks that span multiple placement groups or availability zones must account for this heterogeneous bandwidth topology, a challenge that does not arise in dedicated on-premises clusters. The practical consequence is that cloud-based training jobs must be sized to fit within a single placement group whenever possible, as crossing group boundaries can reduce scaling efficiency by 20--40%.

For our `{python} gpt3_params_b`B model, the cloud path presents a specific challenge: securing 1,000+ GPUs in a single placement group for a multi-week training run. Cloud providers typically limit placement group sizes to 256--512 GPUs, meaning that a frontier training run must either negotiate a custom allocation (often requiring a multi-million dollar commitment) or accept the performance penalty of spanning multiple groups. The availability of large contiguous GPU allocations varies by region and time of day, and organizations have reported waiting weeks for a sufficiently large allocation to become available during periods of peak demand. This availability uncertainty is a hidden cost of the cloud path that does not appear in the per-GPU-hour pricing but can delay training timelines significantly.

::: {.callout-checkpoint title="TCO Decision Framework"}

Your organization needs to train 10 models per year, each requiring 1,000 GPU-hours on H100s. You are evaluating whether to purchase a 128-GPU on-premises cluster or use cloud instances at $4.00/GPU-hour. Calculate the annual cost of each option (assume on-premises costs of $350,000 per 8-GPU node, $0.07/kWh electricity at PUE 1.1, and 700W per GPU). At what number of annual training runs does on-premises become cheaper?

:::

### Infrastructure Planning Methodology {#sec-compute-planning}

\index{Infrastructure Planning}

The preceding sections have examined each level of the infrastructure stack in isolation: accelerator selection, node architecture, rack power and cooling, pod topology, and economics. In practice, infrastructure planning requires considering all these levels simultaneously, because decisions at one level constrain choices at every other level. This section outlines a systematic planning methodology that integrates the concepts from the chapter.

The planning process begins with the **workload characterization**. Before any hardware is selected, the team must answer several questions:

- **What is the target model size?** This determines the memory requirements and therefore the minimum number of accelerators.
- **What is the training dataset size and target throughput?** This determines the total compute budget (in FLOP-hours) and, combined with the hardware's sustained throughput, the required cluster size.
- **What is the acceptable training time?** This, combined with the total compute budget, determines the minimum aggregate throughput and therefore the minimum cluster size.
- **What communication patterns will dominate?** AllReduce (dense models), AllToAll (MoE models), or point-to-point (pipeline parallelism)? This drives the network topology selection.
- **What is the inference serving requirement?** If the trained model will be served at scale, the inference hardware may differ from the training hardware.

With the workload characterized, the planning follows a **bottom-up sizing** approach:

**Step 1: Accelerator Selection.** Based on the Roofline analysis of the workload (compute-bound or memory-bound), select the accelerator type. For compute-bound training: optimize for TFLOPS/dollar. For memory-bound inference: optimize for bandwidth/dollar.

**Step 2: Node Sizing.** Determine the number of accelerators per node and the memory tier allocation. For our `{python} gpt3_params_b`B model: 8 GPUs per node with tensor parallelism, 2 TB host DRAM for optimizer state offloading.

**Step 3: Cluster Sizing.** Divide the total compute budget by the per-node sustained throughput (accounting for MFU) to determine the number of nodes. Add 5--10% overhead for maintenance pool and spares.

**Step 4: Network Design.** Based on the communication pattern and cluster size, select the network topology (fat-tree, torus, or rail-optimized) and size the switch fabric. Calculate the expected scaling efficiency using @eq-scaling-efficiency to verify that the cluster size is within the efficient scaling regime.

**Step 5: Power and Cooling.** Calculate the total facility power requirement (IT load$\times$ PUE) using @tbl-rack-power as a reference for per-rack power budgets. Verify that the target facility (existing or planned) can deliver this power and cooling capacity.

**Step 6: TCO Analysis.** Compute the 3-year total cost of ownership, including hardware CapEx, facility CapEx, electricity OpEx, and staffing OpEx. Compare against cloud alternatives at the expected utilization rate.

**Step 7: Timeline and Risk Assessment.** Map the procurement, construction, and deployment timelines to the project schedule. Identify the longest-lead items (typically electrical infrastructure and GPU procurement) and start those procurement processes first.

To illustrate this methodology concretely, consider planning the infrastructure for training our `{python} gpt3_params_b`B model. **Step 1**: The training workload at large batch sizes is compute-bound, so we optimize for TFLOPS per dollar and select the H100 (highest available TFLOPS at reasonable cost). **Step 2**: The 2.2 TB training state requires 8-way tensor parallelism within each node, yielding a DGX H100 configuration. **Step 3**: The total compute budget is $6 \times 175 \times 10^9 \times 300 \times 10^9 = 3.15 \times 10^{23}$ FLOPs. At 45% MFU on H100s, each GPU delivers $1,979 \times 10^{12} \times 0.45 \approx 891$ TFLOPS sustained. With 1,024 GPUs, the cluster delivers $891 \times 1,024 \approx 912$ PFLOPS sustained, completing training in $3.15 \times 10^{23} / 9.12 \times 10^{17} \approx 345,000$ seconds or roughly 4 days of idealized compute time (2--4 weeks with operational overhead). **Step 4**: The TP-8, PP-4, DP-32 configuration generates structured AllReduce traffic suited to a rail-optimized InfiniBand fabric. **Step 5**: 128 nodes at `{python} rack_power_str` kW per 4-node rack requires 32 racks drawing approximately 1.1 MW total at PUE 1.08, necessitating liquid cooling. **Step 6**: The 3-year TCO is approximately $63M on-premises vs. $19M cloud for this specific workload, with the break-even depending on utilization beyond the initial training run. **Step 7**: GPU procurement (6--12 months) and facility preparation (if needed) must begin immediately, with phased deployment targeting initial capacity within 3 months.

::: {.callout-checkpoint title="Infrastructure Planning Exercise"}

Your team needs to train a 70B-parameter model on 1 trillion tokens within 4 weeks. Using the following specifications:

- H100 GPU: `{python} h100_tflops` TFLOPS peak, assume 45% MFU
- Compute budget: $6 \times 70 \times 10^9 \times 10^{12} = 4.2 \times 10^{23}$ FLOPs
- Available power: 2 MW

Determine: (a) the minimum number of GPUs needed, (b) whether the 2 MW power budget is sufficient (assume PUE 1.1 and 700 W per GPU with 50% overhead for non-GPU components), and (c) the approximate cost of the training run at $4.00/GPU-hour for cloud or $350,000 per 8-GPU node for on-premises.

:::

The economics of infrastructure complete our tour of the physical stack. From the transistors in a Tensor Core to the transformers in a datacenter's electrical substation, every level of the ML fleet exists because a physical constraint made the previous level insufficient. Before examining common misconceptions, we consider emerging technologies that may reshape the infrastructure landscape in the coming years.

## Emerging Infrastructure Technologies {#sec-compute-emerging}

\index{CXL}
\index{Optical Interconnects}
\index{Disaggregated Memory}

The infrastructure stack described in the preceding sections represents the state of the art as of 2024. But several technologies under active development could fundamentally alter the constraints that shape fleet design. Understanding these emerging directions is important for infrastructure planners because the decisions made today (facility design, power provisioning, cooling architecture) must accommodate hardware that does not yet exist. A datacenter built in 2025 will host three or four generations of accelerators over its 15-year structural lifetime, and each generation may demand capabilities that the facility must already support.

Each of the technologies below targets one of the three walls that have governed this chapter. CXL and disaggregated architectures attack the Memory Wall by decoupling capacity from individual accelerators. Optical interconnects attack the Communication Wall by narrowing the bandwidth gap between intra-node and inter-node links. Wafer-scale integration attacks all three walls simultaneously by eliminating the off-chip boundary entirely. For our `{python} gpt3_params_b`B model, these technologies would progressively relax the constraints that currently force complex multi-node parallelism: if CXL memory pooling provides terabyte-scale capacity accessible from any accelerator, and optical interconnects deliver NVLink-class bandwidth between nodes, the distinction between "within a node" and "across nodes" that drives hierarchy-aware parallelism would begin to dissolve.

### Compute Express Link (CXL) {#sec-compute-cxl}

\index{CXL!memory pooling}

The most significant near-term change to the node architecture is **Compute Express Link (CXL)**[^fn-cxl-coherent], a cache-coherent interconnect built on the PCIe physical layer that enables CPUs and accelerators to share a unified, coherent memory space. CXL addresses a fundamental limitation of the current architecture: the rigid boundary between GPU HBM and host DRAM.

[^fn-cxl-coherent]: **CXL (Compute Express Link)**: An open standard (CXL Consortium, founded by Intel in 2019) that adds cache-coherent memory semantics atop the PCIe physical layer via three sub-protocols: CXL.io, CXL.cache, and CXL.mem. CXL 3.0 (ratified 2023) enables memory pooling, allowing terabyte-scale capacity shared across nodes at ~64 GB/s -- roughly 50$\times$ slower than HBM3 but sufficient for optimizer states that are accessed only during the parameter update step, potentially decoupling the memory-capacity constraint from the per-node HBM budget. \index{CXL!coherent interconnect}

In the current node design, GPU HBM and host DRAM are separate address spaces. Moving data between them requires explicit copies over PCIe, managed by the training framework's memory manager. This explicit management is a source of both software complexity and performance overhead: every byte that moves between HBM and DRAM incurs a PCIe transfer latency of several microseconds, and the programmer must carefully schedule these transfers to overlap with computation.

CXL changes this by creating a **unified memory fabric** where the GPU can directly access host-attached CXL memory (and vice versa) through load and store instructions, without explicit copies. The CXL protocol maintains cache coherence across the fabric, ensuring that all devices see a consistent view of the shared memory. For ML workloads, this means that optimizer states stored in CXL-attached memory can be accessed by the GPU at CXL bandwidth (approximately 64 GB/s for CXL 3.0, compared to the current PCIe Gen5 effective bandwidth of 50-60 GB/s) without the programming complexity of explicit memory management.

The more transformative application of CXL is **memory pooling**. CXL 3.0 enables a pool of memory devices (CXL memory expanders) connected to a CXL switch, with the pooled memory accessible by any processor or accelerator connected to the same switch. This decouples memory capacity from the number of compute devices: instead of each node having a fixed amount of DDR, a CXL memory pool can be dynamically allocated to whichever nodes need it most.

For ML training, memory pooling addresses the optimizer state problem directly. The 1,400 GB of optimizer state for our `{python} gpt3_params_b`B model could reside in a shared CXL memory pool rather than being replicated or sharded across individual nodes. Nodes that are executing the compute-intensive forward and backward passes would access only their weight and activation shards from HBM, while the optimizer state would be fetched from the CXL pool only during the parameter update step. This architecture would reduce the per-node memory requirement and potentially allow training of larger models on fewer nodes.

The challenge is bandwidth. CXL 3.0 over a PCIe Gen5 x16 link provides approximately 64 GB/s of read bandwidth, which is roughly 50$\times$ slower than HBM3 (3.35 TB/s). Data that must be accessed at HBM speeds (weights and activations during the forward and backward passes) cannot reside in CXL memory without creating severe bottlenecks. CXL memory is therefore a complement to HBM, not a replacement: it extends the capacity of the memory hierarchy without competing with HBM's bandwidth tier.

The practical timeline for CXL adoption in ML infrastructure is 2025-2027 for initial deployments, with CXL 3.0 memory pooling expected to reach production readiness by 2026-2027. Infrastructure planners designing facilities today should ensure that their server platforms support CXL and that their rack layouts can accommodate CXL memory expander modules.

### Optical Interconnects {#sec-compute-optical}

\index{Optical Interconnects!co-packaged optics}

The bandwidth hierarchy described in @sec-compute-bandwidth-hierarchy has a physical limitation: electrical signaling over copper traces becomes increasingly power-hungry and distance-limited as data rates increase. At 112 Gbps per lane (the current PAM-4 signaling rate used by NVLink 4.0 and InfiniBand NDR), copper SerDes transceivers consume approximately 7-10 pJ per bit and are limited to distances of 2-3 meters before signal integrity degrades beyond the point where equalization can recover the data.

The next generation of links (224 Gbps per lane, required for NVLink 5.0 and InfiniBand XDR) pushes copper even harder. At 224 Gbps, the SerDes power consumption roughly doubles to 15-20 pJ per bit, and the reach shrinks to 1-1.5 meters over passive copper cables. Active electrical cables (which include retimer chips to regenerate the signal mid-cable) extend the reach but add latency and power.

**Co-packaged optics (CPO)** addresses these limitations by integrating optical transceivers directly into the switch or accelerator package, rather than placing them at the end of a pluggable cable module. In CPO, a silicon photonics chip sits on the same package substrate as the processor, converting electrical signals to light at the die boundary. The optical signal then travels through a fiber at the speed of light, with negligible attenuation over distances of tens of meters, and is converted back to electrical at the receiving package.

The benefits are threefold. First, the electrical signal path from the SerDes to the optical transceiver is reduced from centimeters (in a pluggable module at the faceplate of a switch) to millimeters (on the package substrate), reducing the power consumed by electrical signal conditioning. Second, optical fiber has no distance-dependent bandwidth degradation over datacenter-relevant distances (up to 100 meters), eliminating the need for retimers and enabling more flexible physical layouts. Third, fiber is lighter and thinner than copper cables, dramatically simplifying the cable management challenge in dense racks.

For ML infrastructure, CPO could flatten the bandwidth hierarchy by narrowing the gap between intra-node and inter-node bandwidth. If inter-node links achieve bandwidth comparable to current NVLink (hundreds of GB/s per GPU), the constraint that confines tensor parallelism to within a single node would relax. This would enable new parallelism strategies where tensor parallelism spans 2-4 nodes rather than being confined to a single 8-GPU node, potentially improving scaling efficiency for very large models.

NVIDIA's NVLink 5.0 (expected with the next GPU generation after Blackwell) and Broadcom's co-packaged optics switches are among the first products targeting this transition. The industry roadmap envisions CPO becoming standard for datacenter switches by 2026-2027, with GPU-integrated optics following by 2028-2030.

However, CPO introduces new challenges. Optical components are sensitive to temperature (laser wavelength shifts with temperature, requiring active thermal management), and integrating photonics on the same package as a 1,000 W GPU creates a hostile thermal environment for the optical components. The manufacturing processes for silicon photonics and CMOS transistors are similar but not identical, requiring separate fabrication steps that increase package cost and complexity. These challenges are being addressed through hybrid integration approaches where the photonics chiplet is placed on a cooler region of the package substrate, thermally isolated from the GPU die.

### Disaggregated and Composable Architectures {#sec-compute-disaggregated}

\index{Disaggregated Architecture}

The current node architecture bundles a fixed ratio of compute, memory, and networking in each server: 8 GPUs, 640 GB of HBM, 2 TB of DDR, and 8 InfiniBand HCAs in a DGX H100. This fixed bundling is suboptimal for workloads that need a different ratio. Training a 175B model needs more memory capacity per GPU than training a 7B model. Inference serving needs more network bandwidth per GPU (to handle request traffic) than training (which has predictable, batch-oriented communication). The fixed node architecture forces organizations to provision for the most demanding ratio across all workloads, wasting resources on workloads that need a different balance.

**Disaggregated architectures** decouple compute, memory, and networking into separate, independently scalable pools. A disaggregated system might have a pool of GPU compute modules, a pool of CXL-attached memory modules, and a pool of network interface modules, all connected through a high-bandwidth fabric. The orchestration layer composes these pools into virtual nodes with the exact ratio of resources that each workload requires.

This approach has several potential advantages. Upgrading compute (replacing GPU modules with a newer generation) does not require replacing the memory or networking components. Scaling memory capacity (for larger models) does not require purchasing additional GPUs. Failure of a compute module does not take down the associated memory or networking resources.

The concept is not purely theoretical. Several startups and research projects are exploring disaggregated designs for ML infrastructure. Intel's data-centric architecture strategy and AMD's MI300X (which combines GPU compute dies and CPU dies on the same package with a unified memory controller) represent partial steps toward disaggregation. Full disaggregation, where GPUs are separated from memory by a fabric rather than co-located on the same interposer, requires CXL-class bandwidth and latency to be competitive, which is why disaggregated architectures and CXL are deeply intertwined.

The timeline for production disaggregated ML infrastructure is likely 2028-2032, as the technology depends on CXL maturation, silicon photonics for inter-pool communication, and software stack evolution to manage the additional complexity of resource composition. Infrastructure planners should be aware of this direction because facilities built today will likely host disaggregated systems during the later years of their operational life.

### Wafer-Scale Integration {#sec-compute-wafer-scale}

\index{Wafer-Scale Integration}

At the opposite extreme from disaggregation, **wafer-scale integration** maximizes the amount of compute on a single piece of silicon. Cerebras Systems' Wafer-Scale Engine (WSE) is the most prominent example: a single chip fabricated on an entire 300 mm silicon wafer, containing approximately 900,000 processing cores and 44 GB of on-chip SRAM distributed across the cores. The WSE eliminates the off-chip memory bottleneck entirely for models that fit in its on-chip SRAM, because every core can access any SRAM bank through the on-die mesh network at multi-hundred-TB/s aggregate bandwidth.

The approach directly attacks the Memory Wall. By replacing HBM (which is off-chip, connected via an interposer) with distributed on-chip SRAM (which is on-die, connected via a few-millimeter mesh), the WSE reduces data access latency from nanoseconds (HBM) to sub-nanosecond (SRAM) and increases aggregate bandwidth to levels that no discrete accelerator can match. For models that fit within the 44 GB of on-chip memory, the WSE can achieve near-100% utilization of its arithmetic units, because the memory system is never the bottleneck.

The limitation is that 44 GB of SRAM is insufficient for frontier models. Our `{python} gpt3_params_b`B model requires 350 GB for weights alone, far exceeding the WSE's capacity. Cerebras addresses this through a model-parallel scheme where the WSE processes layers sequentially, with weights streamed from external memory (MemoryX units) to the chip. This weight streaming approach works well for inference (where each layer is used once per token) but introduces pipelining complexity for training (where each layer is used in both forward and backward passes with different activations).

Wafer-scale integration also faces manufacturing challenges. Because the chip spans an entire wafer, individual defective cores must be disabled and routed around using redundant interconnect paths. The yield model is fundamentally different from conventional chips: rather than discarding entire dies with defects, the WSE includes spare cores and spare interconnect links that can be activated to replace defective ones. This redundancy-based yield strategy works because neural network workloads are tolerant of slight variations in available compute (losing 1% of cores has negligible impact on training throughput).

The wafer-scale approach is instructive even for organizations that do not deploy WSE systems, because it illustrates the fundamental trade-off between memory proximity and memory capacity. The WSE achieves extraordinary bandwidth by placing memory on the same die as compute, but at the cost of capacity. HBM achieves a middle ground: off-chip but on-package, with moderate bandwidth and moderate capacity. DDR achieves the other extreme: off-package, with low bandwidth but high capacity. Every point on this continuum represents a different resolution of the Memory Wall trade-off, and the optimal choice depends on the model's memory footprint and access pattern.

Underpinning all of these technologies is the evolution of **advanced packaging**, which determines how efficiently multiple silicon dies can communicate within a single package. The reticle limit of approximately 858 mm$^2$ forces next-generation accelerators into multi-die (chiplet) designs, where the packaging technology determines the effective bandwidth between dies. Current **2.5D interposers** (such as TSMC's CoWoS) mount multiple compute dies and HBM stacks on a passive silicon bridge, achieving bandwidth densities orders of magnitude higher than PCB traces. The next frontier is **3D stacking** (SoIC), where SRAM caches or compute logic are vertically stacked directly atop one another, reducing data travel distance to microns. This vertical integration attacks the Memory Wall by placing cache memory directly above the compute logic, but introduces a "Thermal Wall": dissipating heat from the bottom layer of a 3D stack without overheating the top layer requires exotic solutions like microfluidic cooling channels etched directly into the silicon substrate. For infrastructure planners, the implication is that next-generation accelerators will be larger, more complex packages that demand even more sophisticated cooling solutions and power delivery -- extending the trends already visible in the transition from H100 to B200.

These emerging technologies share a common theme: they all seek to flatten one or more of the bandwidth cliffs that define the current infrastructure hierarchy. CXL blurs the boundary between HBM and DRAM. Optical interconnects narrow the gap between intra-node and inter-node bandwidth. Disaggregated architectures decouple the fixed ratios that current node designs impose. Wafer-scale integration eliminates the off-chip memory boundary entirely. None of these technologies will eliminate the fundamental physics of data movement (signals travel at a finite speed, and moving data over longer distances consumes more energy), but each promises to shift the location and magnitude of the bottlenecks, which in turn will reshape the optimal fleet architecture.

The next section examines common misconceptions that arise when the physical realities described in this chapter are overlooked.

## Fallacies and Pitfalls {#sec-compute-fallacies-pitfalls}

**Fallacy:** *More GPUs always means faster training.*

Engineers frequently assume that training time scales linearly with GPU count: doubling the GPUs should halve the training time. In practice, communication overhead grows with cluster size and eventually dominates.

Amdahl's Law establishes the theoretical limit: the sequential fraction of the computation (gradient synchronization, pipeline bubble time, data loading stalls) bounds the maximum speedup regardless of parallelism. For a `{python} gpt3_params_b`B-parameter model with 350 GB of gradients, the AllReduce at each training step requires every GPU to exchange data with every other GPU.

On a cluster of 1,024 GPUs, this synchronization can consume 30--50% of the total step time if the network is not carefully engineered. Scaling from 1,024 to 2,048 GPUs doubles the hardware cost but may reduce training time by only 30--40%, yielding rapidly diminishing returns. The scaling efficiency curve is concave: the first 100 GPUs provide nearly linear speedup, the next 900 provide diminishing returns, and beyond 2,000--4,000 GPUs the marginal benefit per GPU approaches zero for most model sizes.

The correct approach is to profile the communication-to-computation ratio at the current scale and use @eq-scaling-efficiency to predict whether additional GPUs will provide sufficient return on investment before committing to procurement.

**Fallacy:** *Peak FLOPS determines training throughput.*

Procurement teams often select accelerators by comparing peak FLOPS specifications. This reasoning ignores the Roofline Model's central insight: if the workload's arithmetic intensity falls below the ridge point, memory bandwidth, not compute, limits performance. The H100's ridge point is approximately `{python} h100_ridge` FLOP/byte. LLM inference operates at 1--2 FLOP/byte, achieving less than 1% of peak FLOPS. Even LLM training, which benefits from batching, typically achieves 30--50% of peak FLOPS due to memory-bound attention kernels, activation recomputation, and communication stalls. Selecting hardware by peak FLOPS alone is analogous to selecting a car by top speed while ignoring fuel efficiency for a daily commute.

**Pitfall:** *Ignoring power and cooling constraints during planning.*

Teams plan GPU purchases based on compute requirements without verifying that the target facility can deliver the required power and cooling. These are hard physical limits with long lead times.

A single rack of four DGX H100 systems requires `{python} rack_power_str` kW of power. Upgrading a datacenter's electrical infrastructure (new transformers, switchgear, and UPS capacity) requires 12--18 months of lead time. Cooling plant upgrades (chillers, piping, pumps) require 6--12 months. Even seemingly simple changes, such as upgrading the PDU in a single rack from 20 kW to 60 kW, can require new cabling and circuit breaker panels.

Organizations that order hardware without confirming facility readiness risk having millions of dollars of GPUs sitting in shipping containers in the parking lot while the building is retrofitted. This scenario is not hypothetical: several organizations experienced exactly this situation during the 2023--2024 GPU rush, when GPU delivery times shortened faster than datacenter construction timelines.

**Pitfall:** *Buying the latest accelerator without checking the workload's arithmetic intensity.*

A team deploys B200 GPUs (4,500 TFLOPS peak) for an inference workload serving a 7B-parameter model at batch size 1. The arithmetic intensity is approximately 1 FLOP/byte, placing the workload deep in the memory-bound region. The B200's memory bandwidth (8 TB/s) is only 2.4$\times$ higher than the H100's (3.35 TB/s), so the workload runs only 2.4$\times$ faster despite the 2.3$\times$ higher peak FLOPS. The team paid for compute they cannot use. An H100 with its lower cost per unit of memory bandwidth would have been a more cost-effective choice for this specific workload.

**Fallacy:** *Liquid cooling is too expensive for our budget.*

Organizations often reject liquid cooling based on the upfront capital cost (10--15% higher than air-cooled facilities) without performing a TCO analysis. As the cooling tax napkin math showed, liquid cooling saves 294 kW of continuous power for a 1,000-GPU cluster, translating to approximately \$180,000 per year in electricity savings. Over a three-year lifecycle, the savings of \$540,000 typically exceed the incremental capital cost of the liquid cooling infrastructure. At larger scales, the savings multiply proportionally, making liquid cooling not just thermodynamically necessary but economically advantageous.

Beyond the direct electricity savings, liquid cooling enables higher rack densities (120+ kW per rack versus 20--30 kW for air), which reduces the datacenter floor space required. Smaller floor areas reduce building construction costs, land acquisition costs, and cabling distances. When all these secondary savings are included, liquid cooling is often cheaper on a total-cost basis even for clusters as small as 64 GPUs.

**Pitfall:** *Treating all GPU-hours as equivalent in TCO analysis.*

A common error in cost comparisons is to treat one GPU-hour on an H100 as interchangeable with one GPU-hour on an A100 or V100. In reality, the effective cost per unit of useful computation varies enormously across GPU generations because of the efficiency differences documented in @tbl-power-efficiency. An H100 GPU-hour delivers 6.3$\times$ the TFLOPS of an A100 GPU-hour and 15.8$\times$ the TFLOPS of a V100 GPU-hour. A training run that takes 10,000 V100 GPU-hours would require only approximately 630 H100 GPU-hours to complete the same amount of computation. The relevant metric is not cost per GPU-hour but cost per useful TFLOP-hour, which accounts for both the price per hour and the effective throughput delivered. Organizations that benchmark cloud options by GPU-hour cost alone systematically overestimate the expense of newer, more efficient hardware.

**Fallacy:** *A faster network always improves training throughput.*

Teams sometimes invest heavily in upgrading their network fabric (from 200 Gbps to 400 Gbps InfiniBand, for example) expecting a proportional improvement in training speed. The improvement depends entirely on whether the training loop is communication-bound at the current network bandwidth. If the compute time per training step is 20 seconds and the communication time is 2 seconds, the workload is compute-bound: doubling the network speed reduces total step time from 22 to 21 seconds, a mere 4.5% improvement. The same investment in additional GPUs (reducing compute time) or higher-MFU software optimization (reducing wasted compute) would yield far greater returns. The Roofline Model applies to the cluster as a whole, not just to individual chips: diagnose whether the bottleneck is compute or communication before investing in either.

**Pitfall:** *Designing the cooling system for average power draw rather than peak power draw.*

Accelerator power draw varies dynamically between communication phases (lower power) and compute phases (higher power, at or near TDP). The average power draw across a training step is typically 70-85% of TDP. Teams that design their cooling infrastructure for this average, rather than for the TDP-level peak, discover that chip temperatures spike during compute phases, triggering thermal throttling that reduces sustained throughput. The cooling system must be sized for the worst-case thermal load (all GPUs at TDP simultaneously), even though this peak is sustained for only a fraction of each training step. The penalty for undersized cooling is insidious: the system appears to function normally (no crashes, no errors), but MFU quietly degrades by 10-20% because the GPUs are throttling their clock frequency to stay within thermal limits.

**Fallacy:** *HBM capacity is the only memory metric that matters for model selection.*

Procurement teams sometimes focus exclusively on whether a model's weights "fit" in the available HBM, neglecting the bandwidth dimension. A model that fits in HBM but cannot be served at acceptable latency due to insufficient bandwidth is just as unusable as a model that does not fit. Consider two scenarios for serving a 70B model: (a) an accelerator with 192 GB of HBM2e at 2.0 TB/s bandwidth, and (b) an accelerator with 80 GB of HBM3 at 3.35 TB/s bandwidth. Option (a) has more than enough capacity (the model requires only 140 GB in FP16) but delivers tokens at 140 GB / 2.0 TB/s = 70 ms per token. Option (b) requires careful quantization to fit (140 GB in FP16 would not fit in 80 GB, but 70 GB in INT8 does) but delivers tokens at 70 GB / 3.35 TB/s = 21 ms per token, which is 3.3$\times$ faster. For latency-sensitive serving, option (b) is clearly superior despite its smaller capacity. The right accelerator is the one whose bandwidth and capacity *jointly* satisfy the workload's requirements.

**Pitfall:** *Neglecting the data loading pipeline until after deployment.*

Teams that focus all their pre-deployment optimization on GPU kernel performance and communication efficiency sometimes discover, post-deployment, that their GPUs are starved for data. The data loading pipeline (storage I/O, preprocessing, host-to-GPU transfer) must sustain a throughput equal to or greater than the GPU cluster's consumption rate. For language model training, the raw data ingestion rate is modest (megabytes per second), but the preprocessing pipeline (tokenization, sequence packing, shuffling) can become a CPU bottleneck when each GPU consumes data faster than a single CPU core can prepare it. The fix is straightforward but must be planned in advance: allocate sufficient CPU cores for data loading workers (typically 4-8 per GPU), stage training data on fast local NVMe storage rather than relying on shared network filesystems, and pipeline the data loading to overlap with GPU computation.

**Fallacy:** *Homogeneous clusters are always better than heterogeneous ones.*

The intuition that uniform hardware simplifies scheduling and reduces stragglers often leads organizations to retire capable older generations prematurely. For our `{python} gpt3_params_b`B model, the cost-optimal strategy frequently involves a mixed-generation fleet. While training requires the raw FLOPS and interconnect bandwidth of H100s to minimize synchronization overhead, inference serving is memory-bandwidth-bound rather than compute-bound. An A100 offers 2.0 TB/s of bandwidth at a significantly lower capital expenditure than the H100's 3.35 TB/s. By dedicating H100 nodes to training and A100 nodes to inference, an organization can reduce TCO by 15--25% compared to an all-H100 fleet. The fallacy lies in conflating *job-level* homogeneity -- which is critical to prevent stragglers within a single distributed training run -- with *cluster-level* homogeneity. A sophisticated scheduler can effectively manage a heterogeneous fleet, routing bandwidth-intensive inference jobs to older hardware where the bandwidth-per-dollar ratio is competitive, while reserving peak compute nodes for training throughput.

**Pitfall:** *Underestimating the time and cost of the "last mile" -- installation, burn-in, and commissioning.*

Engineering teams often allocate 90% of their planning effort to hardware selection and facility design, assuming that racking and stacking is a deterministic commodity task. In reality, the last mile -- physically installing servers, routing cables, filling coolant loops, and running burn-in tests -- frequently delays production availability by 2--4 months. A cluster for our `{python} gpt3_params_b`B model involves thousands of cables; a single loose InfiniBand connection or a pinched fiber optic cable can degrade effective bisection bandwidth by 50%, stalling distributed training. Insidious issues like firmware incompatibilities between GPU driver versions and InfiniBand switch firmware, or NUMA misconfigurations in the BIOS, often manifest only under sustained load. Experienced infrastructure teams allocate 15--20% of the total project timeline specifically for commissioning and burn-in, running synthetic stress tests (NCCL-tests, HPL benchmarks) for weeks to weed out "infant mortality" failures before a single production job is scheduled.

## Summary {#sec-compute-summary}

\index{Compute Infrastructure!summary}

This chapter has traced the physical infrastructure of machine learning from the transistor to the datacenter, using the running example of training a `{python} gpt3_params_b`B-parameter model to ground each concept in quantitative reality. The infrastructure stack is not a collection of independent components but an integrated system where decisions at each level constrain and enable choices at every other level. The accelerator's TDP determines the rack's cooling requirements. The rack's power density determines the pod's physical layout. The pod's network topology determines the achievable scaling efficiency. And the scaling efficiency determines the economics that make the entire enterprise viable or futile.

The ML fleet is built through a series of progressive responses to physical constraints. Each constraint at one level of the hierarchy creates the engineering motivation for the next level:

- Matrix multiplication workloads demanded specialized arithmetic units, producing Tensor Cores and systolic arrays that trade general-purpose flexibility for orders-of-magnitude throughput gains.
- Weight loading during inference exposed the Memory Wall, driving the development of HBM with 3D-stacked dies and through-silicon vias that deliver 50$\times$ the bandwidth of conventional DRAM.
- Model sizes exceeding single-chip memory capacity required multi-accelerator nodes connected by NVLink, creating the bandwidth hierarchy that dictates parallelism strategy.
- Synchronous training across dense nodes created power transients that stressed datacenter electrical infrastructure, demanding supercapacitor banks and fast-response UPS systems.
- Rack power densities exceeding 100 kW forced the transition from air cooling to direct-to-chip liquid cooling, reducing the cooling tax from 50% to under 10% of IT power.

Several quantitative themes emerge from this progression. The Generality Tax is not a one-time cost but a recurring choice at every level. Just as the accelerator trades general-purpose control logic for arithmetic throughput, the network topology trades general-purpose any-to-any connectivity for workload-specific efficiency, and the cooling system trades general-purpose air cooling for workload-specific liquid cooling. At each level, the question is the same: *how much generality can we afford to surrender, given the stability of our workload?*

The Roofline Model provides the unifying analytical framework across all levels: at every boundary, the question is whether the system is limited by compute or by data movement. The answer determines which resource to invest in.

The four physical levels of the fleet form a hierarchy that mirrors the classical memory hierarchy of computer architecture, but at a radically different scale. Just as a CPU manages data flow between registers, L1 cache, L2 cache, and main memory, the fleet manages data flow between HBM (fast, small), NVLink (fast within a node), InfiniBand (slower across nodes), and distributed storage (slowest, largest). At each boundary, the bandwidth drops and the capacity increases.

The engineer's task at every level is the same: maximize the fraction of time that the arithmetic units spend computing rather than waiting for data. The techniques differ at each level (batching for HBM, tensor parallelism for NVLink, overlapping communication with computation for InfiniBand), but the underlying principle is universal. This universality is what makes the Roofline Model such a powerful diagnostic tool: the same framework that diagnoses whether a single kernel is memory-bound or compute-bound can be applied to an entire cluster by treating the network as the "memory" and the aggregate GPU FLOPS as the "compute."

The physical infrastructure is not merely a container for software; it is a constraint boundary that shapes every algorithmic decision. The bandwidth hierarchy dictates the parallelism hierarchy: tensor parallelism maps to NVLink, pipeline parallelism maps to InfiniBand within a rack, and data parallelism spans the full fabric. The power budget constrains the numerical precision: FP8 operations consume significantly less energy per operation than FP32, making the shift toward lower-precision training not just a speed optimization but a power optimization that keeps the cluster within its thermal envelope. The failure rate at scale determines the checkpointing frequency: checkpoint too often and I/O overhead degrades throughput; checkpoint too rarely and a hardware fault wastes hours of irreplaceable computation. Every software parameter in a distributed training system -- the parallelism dimensions, the precision format, the checkpoint interval, the batch size, the gradient accumulation steps -- is ultimately a response to a physical constraint imposed by the infrastructure described in this chapter.

Our `{python} gpt3_params_b`B-parameter model has served as the persistent architectural forcing function connecting every layer of this hierarchy. At the accelerator level, processing a single token requires 350 billion floating-point operations, demanding specialized Tensor Cores that deliver nearly `{python} h100_tflops` TFLOPS to achieve interactive latency. At the memory level, the 350 GB weight tensor saturates HBM interfaces; streaming this volume at `{python} h100_bw` TB/s creates a hard latency floor of over 100 ms per token, regardless of arithmetic throughput. Moving to the node level, the full training state -- comprising weights, gradients, and optimizer states -- expands to over 2 TB, shattering the 80 GB limit of individual chips and necessitating 8-way tensor parallelism across NVLink. At the rack level, the thermal density of 32 such accelerators drawing 700 W each necessitates `{python} rack_power_str` kW of power delivery and liquid cooling infrastructure. At the pod level, training within a viable two-week window requires synchronizing over 1,000 GPUs across a non-blocking InfiniBand fabric, where the statistical certainty of hardware failure forces a checkpointing strategy that trades compute cycles for reliability. And at the economics level, the 3-year TCO of the required cluster ranges from $63M (on-premises at high utilization) to hundreds of millions (cloud), with the break-even utilization determining which path is viable. Every constraint in this chapter -- from the width of the HBM bus to the topology of the datacenter network -- traces back to the single arithmetic fact that this model does not fit on a single chip.

For our `{python} gpt3_params_b`B model, the chapter has shown that infrastructure selection is not a single decision but a cascade of interdependent choices. The accelerator determines the per-chip throughput and memory bandwidth. The node design determines how many accelerators can cooperate efficiently on tensor-parallel operations. The rack design determines how much power and cooling is available per unit of floor space. The pod design determines how many nodes can synchronize gradients without communication overhead dominating the training loop.

The TCO analysis determines whether the entire enterprise makes financial sense. Each choice constrains the next, creating a system where no component can be optimized in isolation. Selecting the fastest accelerator is counterproductive if the cooling infrastructure cannot remove its heat. Building the densest rack is futile if the power grid cannot supply its demand. Deploying the widest network is wasteful if the workload's communication pattern does not use the bandwidth. Infrastructure engineering is systems engineering in its purest form: every component exists in a web of physical, economic, and operational constraints that must be satisfied simultaneously.

::: {.callout-takeaways title="The Datacenter Is the Computer"}

- **The Generality Tax**: Accelerators achieve 10--100$\times$ higher throughput than CPUs by dedicating silicon area to matrix arithmetic rather than branch prediction and out-of-order execution. The trade-off is reduced flexibility. Each step along the spectrum from GPU to TPU to custom ASIC reclaims more die area for arithmetic.
- **Memory Wall Dominance**: For single-request LLM inference, memory bandwidth determines latency because the arithmetic completes in microseconds while data transfer takes milliseconds. HBM costs 3--5$\times$ more per GB than DDR but provides 50$\times$ the bandwidth. The energy cost of moving data (20 pJ/byte) exceeds the energy cost of computing on it (1 pJ/MAC) by 20$\times$.
- **Roofline Model**: The ridge point ($I_{ridge}$ = peak FLOPS / memory BW) divides workloads into memory-bound and compute-bound regimes. LLM decode is deeply memory-bound ($I \approx 1$); LLM training is compute-bound ($I > 2{,}000$). Always diagnose before optimizing.
- **Bandwidth Hierarchy Dictates Parallelism**: The 18$\times$ bandwidth cliff between NVLink and InfiniBand confines tensor parallelism to within a node and data parallelism to across nodes. Pipeline parallelism spans nodes when model depth exceeds a single node's capacity.
- **Peak vs. Sustained**: Model FLOPS Utilization (MFU) of 40--50% is considered good for large-scale training. Capacity planning must use sustained throughput, not peak specifications.
- **PUE as Efficiency Metric**: Liquid cooling achieves PUE of 1.05--1.10, delivering 90--95% of facility power to computation, compared to 50--65% for air-cooled facilities. The transition from air to liquid cooling is a thermodynamic necessity, not an optional upgrade, for modern ML racks.
- **Failure at Scale**: A 10,000-GPU cluster experiences roughly one GPU failure per hour. Checkpointing strategy is a first-order performance concern that must be designed into the system from the start.
- **TCO, Not CapEx**: The total cost of ownership includes electricity (which can equal hardware cost over a 3-year lifecycle), facility construction, and staffing. On-premises infrastructure becomes cost-effective only above approximately `{python} breakeven_util`% sustained utilization.
- **WSC Mindset**: At pod scale, the datacenter is the computer. Network topology, cooling architecture, and power delivery must be co-designed for the dominant workload.

:::

The practitioner's central takeaway from this infrastructure stack is that no component exists in isolation. The `{python} gpt3_params_b`B-parameter running example was chosen precisely because its scale forces engagement with every level of the hierarchy: a model that fit comfortably on a single chip would never expose the cooling constraints, network topology trade-offs, or failure-rate arithmetic that dominate real production systems. By tracing a single model from the arithmetic unit through HBM, NVLink, the rack power envelope, and the datacenter fabric, the chapter demonstrates a mode of reasoning that transfers to any model at any scale. The specific numbers will change as hardware generations advance, but the method of analysis remains: identify the binding constraint at each level, quantify its impact, and propagate its consequences upward through the stack.

This systems-level perspective is what separates infrastructure engineering from hardware shopping. An engineer who understands only accelerator FLOPS will over-provision compute and under-provision cooling. An engineer who understands only network bandwidth will design a topology that exceeds the facility's power budget. The discipline this chapter cultivates is the ability to hold the entire constraint graph in mind simultaneously, reasoning about how a change in numerical precision at the chip level alters the power draw at the rack level, the cooling load at the facility level, and the TCO at the business level. That integrative judgment, grounded in quantitative analysis rather than intuition, is the foundation on which every subsequent chapter in this volume builds.

::: {.callout-chapter-connection title="From Silicon to Wires"}

We have mapped the physical infrastructure from the transistors in a Tensor Core to the megawatt transformers in a datacenter's electrical substation. But a fleet of disconnected nodes is not a computer; it is a warehouse of expensive space heaters. What transforms these nodes into a unified training system is the network fabric that carries gradients, activations, and checkpoints between them.

Throughout this chapter, we have repeatedly encountered the same constraint: communication bandwidth limits scaling efficiency. The bandwidth hierarchy table showed an 18$\times$ cliff between NVLink and InfiniBand. The scaling efficiency analysis showed that naive data parallelism across 1,024 GPUs achieves only 37.5% efficiency. The pod topology comparison showed that the choice between fat-tree, torus, and rail-optimized designs can alter AllReduce time by 2--3$\times$. All of these observations point to the same conclusion: the network is not a passive connector between nodes but an active determinant of system performance.

@sec-network-fabrics examines this fabric in detail: the physics of high-speed signaling over copper and optical media, the protocols that enable kernel-bypass data transfer (RDMA), the collective communication algorithms that distribute gradient synchronization across the fabric, and the topology decisions that determine whether a 10,000-GPU cluster achieves 80% scaling efficiency or 30%. The emerging technologies discussed in this chapter (co-packaged optics, CXL memory pooling, disaggregated architectures) all aim to flatten the bandwidth hierarchy that currently dictates these topology choices. As these technologies mature, the boundary between "the compute chapter" and "the network chapter" will blur, because the same physical fabric will carry both intra-node and inter-node communication. Until then, the distinction remains sharp, and mastering the network fabric is essential for anyone building or operating ML infrastructure at scale.

:::
