---
---

# Compute {#sec-infrastructure}

::: {layout-narrow}
::: {.column-margin}
_Gemini Pro 3 Prompt: A sweeping architectural visualization of a modern AI datacenter infrastructure. The scene reveals a massive facility with rows of GPU clusters arranged in pods, connected by high-bandwidth networking fabric depicted as glowing fiber optic pathways. Cooling systems appear as flowing blue currents between server racks. The visualization includes multiple layers: physical infrastructure at the bottom with power and cooling, compute infrastructure in the middle with thousands of interconnected accelerators, and orchestration software at the top represented as an abstract control plane. Visual elements include resource managers allocating workloads, capacity graphs showing utilization, and geographic connections to other datacenters. The color scheme uses industrial grays and silvers with accent colors of electric blue for networking and amber for active computation. Photorealistic technical illustration style suitable for infrastructure engineering documentation. Rendered in the style of Nanobanana._
:::

\noindent
![](images/png/cover_infrastructure.png)

:::

## Purpose {.unnumbered}

_Why does the ability to build and manage computational infrastructure determine which organizations can realize their most ambitious machine learning goals?_

::: {.callout-tip title="Learning Objectives"}

- Calculate datacenter power requirements using PUE and thermal dissipation metrics

- Evaluate cooling architectures for high-density GPU clusters using thermal capacity constraints

- Apply the roofline model to identify compute-bound versus memory-bound workloads

- Analyze accelerator selection by matching workload characteristics to hardware capabilities

- Compute total cost of ownership by combining amortized CapEx and OpEx

- Evaluate the trade-offs between GPU, TPU, and custom ASIC architectures for specific workloads

:::

## Datacenter Architecture for ML Workloads {#sec-datacenter-architecture}

::: {.callout-note title="Connection: The Systems Sandwich"}
We are now building the **Physical Layer (Infrastructure)** of the Systems Sandwich. This layer provides the raw capabilities—FLOPS, Watts, and Bandwidth—that the **Operational Layer** (Part I) demands. The physics of cooling and power density here set the hard limits on how large our logical clusters can scale.
:::

In Part I, we established the algorithmic necessity of distributed training. We analyzed how strategies like Data, Tensor, and Pipeline parallelism allow us to split massive models across thousands of devices. However, these algorithms impose severe physical demands: power consumption densities that melt traditional racks and cooling requirements that challenge facility limits.

This chapter examines the physical foundation of the Machine Learning Fleet: the compute nodes and the datacenters that house them. We transition from abstract communication graphs to concrete engineering, analyzing how power, cooling, and accelerator hardware combine to support the distributed workloads defined in Part I. Managing such infrastructure demands expertise in datacenter design, thermal management, and hardware selection. The infrastructure must satisfy competing requirements: maximizing compute density while managing extreme heat flux, and selecting accelerators that balance raw FLOPS against memory bandwidth and cost.

### Physical Infrastructure Fundamentals

ML datacenters differ from traditional cloud facilities in three critical dimensions. Power density per rack reaches 5 to 10 times higher than conventional levels. Cooling requirements demand liquid rather than air-based solutions, as @fig-cooling-topology contrasts traditional air-cooled architectures with direct-to-chip liquid systems capable of managing 700W per GPU. Physical layout must optimize for high-bandwidth interconnects rather than flexible networking.

#### Power Delivery and Distribution

A single NVIDIA DGX H100 system consumes 10.2 kW at peak load. A rack containing four such systems requires over 40 kW, compared to 5-10 kW for traditional server racks. This power density[^fn-power-density] fundamentally changes power infrastructure design.

[^fn-power-density]: **Power density**: The power consumption per unit of datacenter floor space, typically measured in kW per rack or kW per square meter. Traditional enterprise datacenters design for 5-10 kW per rack. Modern GPU clusters require 40-100+ kW per rack, demanding specialized power distribution and cooling infrastructure. Power density directly limits how many GPUs can be deployed in existing facilities.

**Utility and Backup Power.** Production ML facilities require redundant power feeds, typically N+1 or 2N configurations[^fn-redundancy] where N represents the load requirement. Uninterruptible power supplies bridge the gap during utility failures, though the massive power draw of GPU clusters limits battery backup duration to minutes rather than hours. Diesel generators provide extended backup. Automatic transfer switches complete failover within 10 to 15 seconds.

[^fn-redundancy]: **N+1 and 2N redundancy**: Power redundancy configurations where N is the capacity required to serve the load. N+1 provides one backup component (e.g., 3 UPS units where 2 are required), allowing single-failure tolerance. 2N provides complete duplication (e.g., 4 UPS units), allowing maintenance on one path while maintaining failure protection on the other. ML training runs spanning weeks justify 2N redundancy to prevent catastrophic job loss.

**Power Distribution Architecture.** Modern ML datacenters use a tiered distribution model:

+-----------------------------+---------------------+-------------------------+
| **Distribution Level**      | **Typical Voltage** | **Purpose**             |
+:============================+====================:+:========================+
| **Utility feed**            | 13.8-69 kV          | Grid connection         |
+-----------------------------+---------------------+-------------------------+
| **Substation transformer**  | 480V (US)           | Building distribution   |
+-----------------------------+---------------------+-------------------------+
| **PDU (Power Distribution** | 208V                | Rack-level distribution |
| **Unit)**                   |                     |                         |
+-----------------------------+---------------------+-------------------------+
| **Server PSU**              | 12V DC              | Component-level power   |
+-----------------------------+---------------------+-------------------------+

@fig-power-hierarchy traces power from grid connection at 13.8 kV through four voltage step-downs to the 12V DC rails that feed individual GPU components:

::: {#fig-power-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**Datacenter Power Distribution**: Hierarchical power delivery for ML infrastructure. Voltage steps down from utility-level 13.8 kV through 480V building distribution to 208V rack distribution and finally 12V DC at the component level. The 2N redundancy pattern at UPS and PDU tiers ensures training jobs survive single-component failures during multi-week runs. Each DGX H100 system draws 10.2 kW, requiring robust power infrastructure to support racks exceeding 40 kW total load."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{GreenLine}{RGB}{34,139,34}
  \definecolor{GreenL}{RGB}{220,245,220}
  \definecolor{BlueLine}{RGB}{0,80,180}
  \definecolor{BlueL}{RGB}{225,240,255}
  \definecolor{RedLine}{RGB}{200,30,30}
  \definecolor{RedL}{RGB}{255,235,235}
  \definecolor{VioletLine}{RGB}{138,43,226}
  \definecolor{VioletL2}{RGB}{240,230,255}

  \tikzset{
    Box/.style={
      draw=GreenLine,
      fill=GreenL,
      line width=0.75pt,
      rounded corners=2pt,
      align=center,
      minimum width=3.5cm,
      minimum height=1.0cm,
      inner sep=4pt
    },
    BoxBlue/.style={Box, draw=BlueLine, fill=BlueL},
    BoxRed/.style={Box, draw=RedLine, fill=RedL},
    Arrow/.style={->, >=stealth, thick, color=black!70},
    Label/.style={font=\footnotesize\sffamily, color=black!70}
  }

  % Nodes
  \node[Box] (utility) {\textbf{Utility Feed}\\High Voltage (13.8kV)};
  \node[Box, below=1.0cm of utility] (substation) {\textbf{Substation}\\Step-down to 480V};
  \node[BoxBlue, below=1.0cm of substation] (pdu) {\textbf{PDU}\\Rack Dist. (208V)};
  \node[BoxRed, below=1.0cm of pdu] (psu) {\textbf{Server PSU}\\Component (12V DC)};

  % Edges
  \draw[Arrow] (utility) -- node[right, font=\footnotesize, xshift=2mm] {Grid Input} (substation);
  \draw[Arrow] (substation) -- node[right, font=\footnotesize, xshift=2mm] {Building Power} (pdu);
  \draw[Arrow] (pdu) -- node[right, font=\footnotesize, xshift=2mm] {Rack Power} (psu);

  % Annotations
  \node[right=2cm of utility, align=left, font=\footnotesize] (gen) {Backup Generator\\(Diesel/Gas)};
  \draw[dashed, ->] (gen) -- (utility);

  \node[right=2cm of pdu, align=left, font=\footnotesize] (ups) {UPS Battery\\Backup};
  \draw[dashed, ->] (ups) -- (pdu);

\end{tikzpicture}
```
:::

**Power Usage Effectiveness.** The PUE metric[^fn-pue], developed by The Green Grid consortium in 2007 [@thegreengrid2007pue], quantifies datacenter energy efficiency:

[^fn-pue]: **Power Usage Effectiveness (PUE)**: An industry-standard metric where values closer to 1.0 indicate greater efficiency. A PUE of 2.0 means half the power goes to overhead (cooling, lighting, power distribution), while 1.1 means only 10% goes to overhead. Google's most efficient datacenters achieve PUE of 1.06, while typical enterprise facilities operate at 1.5-2.0.

$$
\text{PUE} = \frac{\text{Total Facility Power}}{\text{IT Equipment Power}}
$$

A PUE of 1.0 represents perfect efficiency where all power goes to computing. Traditional datacenters achieve PUE values of 1.5 to 2.0, while hyperscale facilities target 1.1 to 1.2. ML datacenters face a challenge. The extreme heat density of GPU clusters increases cooling overhead, pushing PUE higher unless advanced cooling technologies are deployed.

#### Cooling Systems at Scale

Heat dissipation represents the primary constraint on ML cluster density. An H100 GPU generates 700W of thermal output from a surface area smaller than a dinner plate, producing heat flux comparable to a nuclear reactor's fuel rod surface.

**Air Cooling Limitations.** Traditional air cooling becomes impractical above 30 to 40 kW per rack. The physics are straightforward. Air's low heat capacity of approximately 1 kJ/kg-K requires massive airflow to remove heat. A 40 kW rack requires roughly 10,000 CFM of airflow, creating acoustic levels exceeding 80 dB and significant fan power overhead.

**Hot Aisle/Cold Aisle Containment.** This architectural pattern separates cold supply air from hot exhaust air using physical barriers. Cold air enters through raised floor vents or overhead ducts, passes through servers front-to-back, and exhausts into a contained hot aisle. Containment improves cooling efficiency by preventing mixing, but cannot solve the fundamental heat density challenge of modern GPU clusters. Examine @fig-cooling-topology to see how containment manages airflow separation in panel A, while panel B illustrates the direct-to-chip liquid loops that bypass air entirely.

::: {#fig-cooling-topology fig-env="figure" fig-pos="htb" fig-cap="**Datacenter Cooling Architectures**: Comparison of traditional hot-aisle/cold-aisle air cooling versus modern direct-to-chip liquid cooling. The air cooling diagram shows airflow management separating intake and exhaust streams, while the liquid cooling diagram illustrates coolant loops directly contacting high-power components to manage extreme heat density."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{BlueLine}{RGB}{0,80,180}
  \definecolor{BlueL}{RGB}{225,240,255}
  \definecolor{RedLine}{RGB}{200,30,30}
  \definecolor{RedL}{RGB}{255,235,235}

  \tikzset{
    rack/.style={draw=black!60, thick, fill=gray!5, minimum width=2cm, minimum height=3.5cm},
    server/.style={draw=black!40, fill=white, minimum width=1.8cm, minimum height=0.3cm},
    arrowCold/.style={->, >=stealth, thick, text=BlueLine, color=BlueLine},
    arrowHot/.style={->, >=stealth, thick, text=RedLine, color=RedLine}
  }

  % Left Panel: Air Cooling (Hot/Cold Aisle)
  \begin{scope}[local bounding box=Air]
    \node[anchor=south] at (0,4) {\textbf{A. Air Cooling (Hot/Cold Aisle)}};

    % Racks
    \node[rack] (r1) at (-1.5, 2) {};
    \node[rack] (r2) at (1.5, 2) {};

    % Servers inside
    \foreach \y in {0.5, 1.0, ..., 3.0} {
        \node[server] at (-1.5, \y) {};
        \node[server] at (1.5, \y) {};
    }

    % Cold Aisle (Middle)
    \node[text=BlueLine, font=\footnotesize] at (0, 0.5) {Cold Aisle};
    \draw[arrowCold] (0, 3.5) -- (0, 1.5);
    \draw[arrowCold] (0, 1.5) -- (-1, 1.5);
    \draw[arrowCold] (0, 1.5) -- (1, 1.5);

    % Hot Aisles (Sides)
    \draw[arrowHot] (-2, 1.5) -- (-3, 1.5);
    \draw[arrowHot] (2, 1.5) -- (3, 1.5);
    \node[text=RedLine, font=\footnotesize, rotate=90] at (-3.2, 2) {Hot Exhaust};
    \node[text=RedLine, font=\footnotesize, rotate=90] at (3.2, 2) {Hot Exhaust};

    % CRAC Unit Symbol
    \node[draw=black, fill=BlueL, minimum width=4cm, minimum height=0.5cm] at (0, -0.5) {CRAC / CRAH Unit};
  \end{scope}

  % Right Panel: Liquid Cooling
  \begin{scope}[shift={(6.5,0)}, local bounding box=Liquid]
    \node[anchor=south] at (0,4) {\textbf{B. Direct-to-Chip Liquid Cooling}};

    % Board
    \node[draw=black!50, fill=green!5, minimum width=4cm, minimum height=3cm] (board) at (0,2) {};
    \node[anchor=north] at (0, 3.4) {Server Blade / GPU Tray};

    % Chips
    \node[draw=black, fill=gray!30, minimum size=1cm] (c1) at (-1, 2) {GPU};
    \node[draw=black, fill=gray!30, minimum size=1cm] (c2) at (1, 2) {GPU};

    % Cold Plates (on top)
    \node[draw=BlueLine, fill=BlueL!50, minimum size=0.6cm, circle] (p1) at (-1, 2) {};
    \node[draw=BlueLine, fill=BlueL!50, minimum size=0.6cm, circle] (p2) at (1, 2) {};

    % Piping
    \draw[ultra thick, BlueLine, ->] (-2.5, 1.5) -- (-1.3, 1.8) node[midway, below, font=\scriptsize] {Cool In};
    \draw[ultra thick, BlueLine] (-1.3, 1.8) -- (p1);
    \draw[ultra thick, BlueLine] (p1) -- (p2);
    \draw[ultra thick, RedLine, ->] (p2) -- (2.5, 1.8) node[midway, below, font=\scriptsize] {Warm Out};

    % CDU
    \node[draw=black, fill=gray!20, minimum width=3cm, minimum height=0.8cm] at (0, -0.5) {Coolant Dist. Unit (CDU)};
    \draw[dashed, thick, black!50] (0, 0) -- (0, 1.5);
  \end{scope}

\end{tikzpicture}
```
:::

**Direct-to-Chip Liquid Cooling.** Liquid cooling addresses heat density through water's superior heat capacity of 4.2 kJ/kg-K, roughly four times that of air. Cold plates mounted directly on GPUs and CPUs transfer heat to circulating coolant, which flows to facility-level heat exchangers. This approach enables rack densities exceeding 100 kW while reducing cooling power consumption by 30 to 40 percent compared to air cooling.

::: {.callout-note}
## Liquid Cooling Adoption

As of 2024, liquid cooling has transitioned from specialty option to requirement for large-scale ML clusters. NVIDIA's GB200 NVL72 systems require liquid cooling, with no air-cooled option available. Facilities planning for next-generation hardware must include liquid cooling infrastructure from initial design.
:::

**Immersion Cooling.** The most aggressive thermal solution submerges entire servers in dielectric fluid. Single-phase immersion uses non-conductive oils that remain liquid. Two-phase systems use fluids that boil at low temperatures, leveraging latent heat of vaporization for efficient heat transfer. Immersion enables rack densities exceeding 200 kW but requires specialized maintenance procedures and component compatibility.

#### Physical Layout Optimization

ML cluster performance depends critically on physical topology. Unlike web serving workloads where any server can handle any request, distributed training requires specific communication patterns between specific nodes.

**Rack Density Considerations.** Higher density reduces cable lengths and switch hops but concentrates power and cooling requirements. Production deployments balance these factors based on workload characteristics:

+---------------------+---------------------+---------------------+
| **Workload Type**   | **Typical Density** | **Limiting Factor** |
+:====================+====================:+:====================+
| **LLM training**    | 80-120 kW/rack      | Cooling capacity    |
+---------------------+---------------------+---------------------+
| **Recommendation**  | 30-50 kW/rack       | CPU/memory balance  |
| **inference**       |                     |                     |
+---------------------+---------------------+---------------------+
| **Vision training** | 60-80 kW/rack       | Network bandwidth   |
+---------------------+---------------------+---------------------+

**Cable Management.** High-bandwidth interconnects like InfiniBand use copper cables for distances under 3 meters and fiber optics beyond. Cable routing must maintain bend radius requirements, typically 10 times cable diameter, while enabling airflow for any air-cooled components. Active optical cables simplify routing but add latency and power consumption compared to passive copper.

### Compute Infrastructure Design

ML clusters combine multiple node types, each optimized for different phases of the training and inference pipeline. Understanding these roles clarifies infrastructure design decisions.

#### GPU Cluster Architectures

Modern GPU clusters are built from dense multi-GPU nodes connected via high-bandwidth fabrics. Two reference architectures dominate production deployments.

**DGX-Style Dense Nodes.** NVIDIA's DGX systems package 8 GPUs with NVLink[^fn-nvlink] interconnects, high-bandwidth networking, and substantial local storage in a single chassis. The DGX H100 provides 8 H100 GPUs with 640GB total HBM3[^fn-hbm] memory, an NVSwitch fabric enabling 900 GB/s GPU-to-GPU bandwidth, 8 ports at 400 Gbps for InfiniBand or Ethernet, 2 Intel Xeon CPUs for preprocessing, and 30TB of NVMe storage for dataset staging.

::: {.callout-note title="Archetype A (Scaled Lighthouse): The Need for Scale-Up"}
**Archetype A (The Trillion-Parameter LLM)** critically depends on architectures like the DGX. The 900 GB/s NVLink bandwidth is not a luxury but a requirement for **Tensor Parallelism**. When a single model layer is split across 8 GPUs, every forward and backward pass requires synchronizing activations. Standard PCIe Gen5 (64 GB/s) would bottleneck these operations, stalling the training of Archetype A models.
:::

[^fn-nvlink]: **NVLink**: NVIDIA's proprietary high-bandwidth interconnect for GPU-to-GPU communication, providing 900 GB/s bidirectional bandwidth in NVLink 4.0 (H100), compared to 64 GB/s for PCIe Gen5. NVLink enables efficient tensor parallelism by allowing GPUs to share memory across the interconnect with near-local-memory latency.

[^fn-hbm]: **High Bandwidth Memory (HBM)**: A 3D-stacked DRAM technology that places memory dies vertically atop the GPU die, connected via thousands of through-silicon vias (TSVs). HBM3 provides 3.4 TB/s bandwidth per GPU compared to 400 GB/s for DDR5. This bandwidth is essential for memory-bound ML workloads where data movement, not computation, limits performance.

This integrated design simplifies deployment but limits flexibility. Each DGX H100 costs approximately $300,000, with pricing reflecting 2024 market conditions and fluctuating significantly based on supply, demand, and generation transitions. This cost makes component-level upgrades economically impractical.

**HGX Baseboard Designs.** For organizations building custom infrastructure, NVIDIA's HGX baseboards provide the GPU and interconnect components for integration into custom server designs. Cloud providers and large enterprises use HGX to optimize for their specific power, cooling, and networking requirements while maintaining compatibility with NVIDIA's software stack.

**PCIe vs. NVLink Configurations.** The choice between PCIe and NVLink connectivity involves fundamental trade-offs that @fig-interconnect-topology visualizes by contrasting host-centric PCIe trees with peer-to-peer NVLink meshes:

::: {#fig-interconnect-topology fig-env="figure" fig-pos="htb" fig-cap="**Node Interconnect Topologies**: Contrast between PCIe-based commodity servers and NVLink-based dense nodes. The PCIe topology shows CPU-centric communication with higher latency and shared bandwidth bottlenecks, while the NVLink topology demonstrates a high-bandwidth mesh allowing direct peer-to-peer GPU communication essential for tensor parallelism."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{GreenLine}{RGB}{34,139,34}
  \definecolor{GreenL}{RGB}{230,250,230}
  \definecolor{VioletLine}{RGB}{138,43,226}
  \definecolor{VioletL}{RGB}{240,230,255}

  \tikzset{
    component/.style={draw=black!60, fill=white, rounded corners=1pt, minimum width=1.0cm, minimum height=0.6cm, font=\scriptsize},
    root/.style={component, fill=GreenL, draw=GreenLine, font=\footnotesize\bfseries},
    gpu/.style={component, fill=VioletL, draw=VioletLine, font=\scriptsize\bfseries}
  }

  % Left: PCIe Tree
  \begin{scope}
    \node[anchor=south] at (0,3.5) {\textbf{A. PCIe Tree (Host-Centric)}};

    \node[root] (cpu) at (0, 3) {CPU / Root};

    \node[component] (sw1) at (-1.5, 2) {PCIe Sw};
    \node[component] (sw2) at (1.5, 2) {PCIe Sw};

    \node[gpu] (g1) at (-2.2, 0.5) {GPU 0};
    \node[gpu] (g2) at (-0.8, 0.5) {GPU 1};
    \node[gpu] (g3) at (0.8, 0.5) {GPU 2};
    \node[gpu] (g4) at (2.2, 0.5) {GPU 3};

    \draw[thick] (cpu) -- (sw1);
    \draw[thick] (cpu) -- (sw2);
    \draw[thick] (sw1) -- (g1);
    \draw[thick] (sw1) -- (g2);
    \draw[thick] (sw2) -- (g3);
    \draw[thick] (sw2) -- (g4);

    % Communication bottleneck annotation
    \draw[<->, dashed, red] (g1) to[bend right=20] node[below, font=\tiny] {Long Path} (g3);
  \end{scope}

  % Right: NVLink Mesh
  \begin{scope}[shift={(6,0)}]
    \node[anchor=south] at (0,3.5) {\textbf{B. NVLink Mesh (Peer-to-Peer)}};

    % GPU Ring/Mesh
    \node[gpu] (ng1) at (-1.5, 2) {GPU 0};
    \node[gpu] (ng2) at (1.5, 2) {GPU 1};
    \node[gpu] (ng3) at (1.5, 0.5) {GPU 2};
    \node[gpu] (ng4) at (-1.5, 0.5) {GPU 3};

    \node[root, anchor=center] (ncpu) at (0, 1.25) {CPU};

    % PCIe Links (thinner)
    \draw[thin, gray] (ncpu) -- (ng1);
    \draw[thin, gray] (ncpu) -- (ng2);
    \draw[thin, gray] (ncpu) -- (ng3);
    \draw[thin, gray] (ncpu) -- (ng4);

    % NVLink Links (thicker, colored)
    \draw[ultra thick, VioletLine, <->] (ng1) -- (ng2);
    \draw[ultra thick, VioletLine, <->] (ng2) -- (ng3);
    \draw[ultra thick, VioletLine, <->] (ng3) -- (ng4);
    \draw[ultra thick, VioletLine, <->] (ng4) -- (ng1);
    \draw[ultra thick, VioletLine, <->] (ng1) -- (ng3);
    \draw[ultra thick, VioletLine, <->] (ng2) -- (ng4);

    \node[text=VioletLine, font=\scriptsize, align=center] at (0, 2.5) {High BW Die-to-Die};
  \end{scope}

\end{tikzpicture}
```
:::

+---------------------+---------------+------------------+----------------------+
| **Interconnect**    | **Bandwidth** | **Latency**      | **Use Case**         |
+:====================+==============:+=================:+:=====================+
| **PCIe Gen5 x16**   | 64 GB/s       | ~1 microsecond   | Inference, small     |
|                     |               |                  | models               |
+---------------------+---------------+------------------+----------------------+
| **NVLink 4.0**      | 900 GB/s      | ~0.5 microsecond | Large model training |
| **(bidirectional)** |               |                  |                      |
+---------------------+---------------+------------------+----------------------+

For models requiring tensor parallelism across GPUs, NVLink's 14x bandwidth advantage directly translates to training throughput. @sec-distributed-training examines how tensor parallelism splits individual layers across GPUs, creating the tight synchronization requirements that make NVLink essential. PCIe-based systems suffice for data-parallel workloads where gradient synchronization occurs less frequently.

#### CPU Infrastructure Roles

While GPUs dominate ML computation, CPUs perform essential supporting functions that bottleneck overall system performance if under-provisioned.

**Preprocessing and Data Preparation.** Training data pipelines involve decompression, augmentation, tokenization, and batching. These operations execute on CPUs, which must supply data fast enough to keep GPUs utilized. A common rule of thumb allocates 4 to 8 CPU cores per GPU for training workloads, though data-intensive pipelines handling video or large images may require more.

**Feature Serving for Recommendation Systems.** Recommendation models present a distinct infrastructure pattern. These systems combine deep learning components with massive embedding tables that may exceed 1TB. The embedding lookups are memory-bound CPU operations, while neural network components benefit from GPU acceleration. Production recommendation systems often use CPU-heavy nodes for embedding serving alongside GPU nodes for model computation, connected via low-latency networks.

**Control Plane and Orchestration.** Cluster management, job scheduling, and monitoring run on dedicated CPU nodes separate from the training cluster. This isolation prevents resource contention and enables management operations even when the training cluster is fully utilized.

#### Hybrid Architectures

Real production systems rarely use homogeneous hardware throughout. Workload-aware placement matches job characteristics to appropriate resources.

**Embedding Table Placement.** For recommendation systems with embedding tables exceeding GPU memory, hybrid architectures place embeddings in CPU DRAM while compute-intensive layers execute on GPUs. Facebook's DLRM architecture [@naumov2019dlrm] pioneered this pattern, with embeddings distributed across CPU nodes communicating with GPU nodes via high-bandwidth networks.

**Heterogeneous Scheduling.** Modern orchestration systems support mixed node types within a single cluster. Kubernetes with GPU support and Slurm with Generic Resource Scheduling enable jobs to request specific hardware combinations. A training job might request 64 GPU nodes for model computation plus 16 high-memory CPU nodes for embedding tables, scheduled as a coordinated allocation.

### Accelerator Selection by Workload Type

Having examined how different compute resources combine within a cluster, we now turn to a more fundamental question: which accelerator technologies should comprise that infrastructure? The accelerator landscape has expanded beyond NVIDIA GPUs to include Google TPUs, custom ASICs, and emerging architectures. Selection requires matching accelerator characteristics to workload requirements.

#### NVIDIA GPU Ecosystem

NVIDIA maintains market dominance through integrated hardware-software offerings. Understanding the architecture evolution clarifies capability differences.

**Architecture Progression.** Each generation brings substantial improvements in compute density and memory bandwidth:

+-----------+------------+--------------+---------------+---------+
| **GPU**   | **FP16**   | **HBM**      | **Memory**    | **TDP** |
|           | **Tensor** | **Capacity** | **Bandwidth** |         |
|           | **TFLOPS** |              |               |         |
+==========:+===========:+=============:+==============:+========:+
| **A100**  | 312        | 80 GB HBM2e  | 2.0 TB/s      | 400W    |
+-----------+------------+--------------+---------------+---------+
| **H100**  | 990        | 80 GB HBM3   | 3.4 TB/s      | 700W    |
+-----------+------------+--------------+---------------+---------+
| **B100*** | ~1,800     | 192 GB HBM3e | 8.0 TB/s      | ~700W   |
+-----------+------------+--------------+---------------+---------+

*B100 specifications are preliminary estimates based on NVIDIA announcements. Verify against official specifications for production planning.

::: {.callout-perspective title="The Case for Bfloat16"}
Deep learning training is resilient to low precision but sensitive to dynamic range.
Standard FP16 (IEEE 754) uses 5 bits for the exponent and 10 bits for the mantissa. This limited exponent range often causes gradients to underflow to zero or overflow to infinity during training, requiring complex loss scaling techniques to maintain stability.

**Bfloat16 (Brain Floating Point)**, developed by Google, reallocates bits to use 8 bits for the exponent, matching FP32, and 7 bits for the mantissa. This matches the dynamic range of FP32, meaning numbers that fit in FP32 also fit in BF16. This eliminates the need for loss scaling and stabilizes training for large transformers. The trade-off is lower precision from fewer mantissa bits, but neural networks typically do not need high precision for individual weights, only for aggregate accumulation, which is usually done in FP32. Virtually all modern large language models, including GPT-4, PaLM, and Llama, are trained using BF16.
:::

The H100 delivers approximately 3x the tensor TFLOPS of A100 at 1.75x the power. To derive the efficiency improvement: A100 achieves 312 TF / 400W = 0.78 TF/W, while H100 achieves 990 TF / 700W = 1.41 TF/W, yielding approximately 80% improvement in FLOPS/watt. Memory bandwidth increases proportionally, maintaining the compute-to-memory ratio critical for transformer models.

**Tensor Core Utilization.** Tensor Cores[^fn-tensor-cores] accelerate matrix operations but require specific data layouts and sizes for full utilization. Dimensions should be multiples of 8 for FP16 or 16 for INT8 for optimal performance. Underutilized Tensor Cores represent the most common source of poor GPU efficiency in production. Many workloads achieve only 30 to 50 percent of theoretical peak FLOPS.

[^fn-tensor-cores]: **Tensor Cores**: Specialized matrix-multiply-accumulate units introduced in NVIDIA Volta (2017) that perform 4x4 matrix operations in a single clock cycle. Unlike general-purpose CUDA cores, Tensor Cores are optimized for the fused multiply-add pattern $D = A \times B + C$ that dominates neural network computation. H100 Tensor Cores support FP8, FP16, BF16, TF32, and INT8 formats.

**NVLink Topology.** Within a node, NVSwitch provides full-bandwidth connectivity between all GPUs. Across nodes, NVLink Network, available in H100 and later, extends high-bandwidth connectivity, though at reduced bandwidth compared to intra-node links. @sec-communication examines how topology-aware job placement exploits these bandwidth asymmetries to minimize communication overhead in multi-node training.

#### Google TPU Infrastructure

Google's Tensor Processing Units offer an alternative architecture optimized for matrix operations with a distinct programming model.

**TPU Pod Architecture.** TPUs connect via proprietary Inter-Chip Interconnect[^fn-ici] forming 2D or 3D torus[^fn-torus] topologies. A TPU v4 pod contains 4,096 chips with 1.1 exaFLOPS of aggregate compute. Unlike GPU clusters where networking is separate from compute nodes, TPU pods integrate the interconnect directly into the chip design.

#### The Systolic Array Advantage

To understand why TPUs achieve such high efficiency for matrix operations, we must examine their distinctive hardware design. The defining feature of the TPU architecture is the **systolic array**. Unlike CPUs or GPUs that function as general-purpose instruction processors, a systolic array is a specialized grid of arithmetic units designed for massive matrix multiplication.

The name "systolic" refers to the way data flows through the chip in rhythmic waves, analogous to blood pumped by a heart. The architecture employs a weight-stationary design. In a matrix multiply $C = A \times B$, weights from matrix $B$ are loaded into the array and held stationary in local registers. Data from matrix $A$ flows in from the left, and partial sums flow down.

This design drastically reduces energy consumption. In a standard architecture, every operation requires reading operands from registers or memory, incurring high energy cost. In a systolic array, operands are passed directly to the next neighbor unit, incurring very low energy cost. A single memory access effectively amortizes over hundreds of operations. This architecture explains why TPUs achieve extremely high FLOPS/watt for dense matrix operations but struggle with sparse or irregular computations that break the rhythmic data flow.

[^fn-ici]: **Inter-Chip Interconnect (ICI)**: Google's proprietary chip-to-chip communication fabric integrated directly into TPU silicon. ICI provides 6 links per chip at 100 GB/s each, enabling the torus topology without external switches. This integration reduces latency and power compared to discrete networking but limits flexibility in topology configuration.

[^fn-torus]: **Torus topology**: A network topology where nodes connect in a ring structure along each dimension, with the last node wrapping around to connect to the first. A 3D torus creates a cube-like structure where each node connects to 6 neighbors. This topology provides consistent latency for nearest-neighbor communication patterns common in model parallelism, though AllReduce operations require O(sqrt(N)) hops compared to O(log(N)) for fat-tree.

**TPU Slices and Multislice.** Users allocate TPU slices, contiguous subsets of a pod. Multislice training connects multiple slices via datacenter network for jobs exceeding single-slice capacity. The programming model using JAX with pjit abstracts the physical topology, enabling code portability across slice sizes.

**TPU vs. GPU Trade-offs.** TPUs excel for large-scale training with regular computation patterns:

+------------------------+-------------------------+--------------------------+
| **Factor**             | **TPU Advantage**       | **GPU Advantage**        |
+:=======================+:========================+:=========================+
| **Large transformer**  | Optimized matrix units, | Broader operator support |
| **training**           | integrated interconnect |                          |
+------------------------+-------------------------+--------------------------+
| **Custom operations**  | Limited flexibility     | CUDA extensibility       |
+------------------------+-------------------------+--------------------------+
| **Software ecosystem** | JAX-centric             | PyTorch, TensorFlow,     |
|                        |                         | many frameworks          |
+------------------------+-------------------------+--------------------------+
| **Availability**       | Google Cloud only       | Multiple cloud and       |
|                        |                         | on-premise options       |
+------------------------+-------------------------+--------------------------+

#### Custom ASICs and Specialized Accelerators

The ML accelerator landscape continues to diversify as organizations optimize for specific workloads.

**Inference-Optimized Accelerators.** Training and inference present different requirements. Training needs high-precision arithmetic, large memory for activations and optimizer state, and high interconnect bandwidth. Inference prioritizes low latency, high throughput, and power efficiency.
Recent analysis by Ma and Patterson [@ma2024challenges] further refines this distinction by separating the *prefill* phase (compute-bound processing of input tokens) from the *decode* phase (memory-bandwidth-bound generation of output tokens). The decode phase, being autoregressive, requires reading the entire model weight set for every token generated, making memory bandwidth—not FLOPS—the primary determinant of performance. This "memory wall" drives the design of inference-specialized chips that sacrifice raw compute density for massive memory bandwidth and capacity, optimizing for the unique token-by-token access pattern of large language models.
Accelerators like Google's TPU Inference chips and AWS Inferentia optimize for these characteristics, achieving 2-4x better performance per watt than training-focused hardware for appropriate workloads.

**Emerging Architectures.** Several companies offer alternative approaches. Cerebras WSE uses wafer-scale integration[^fn-wafer-scale] to place an entire ML accelerator on a single silicon wafer, eliminating chip-to-chip communication for models that fit on-chip. Graphcore IPU employs a Bulk Synchronous Parallel[^fn-bsp] execution model with distributed on-chip memory targeting sparse and dynamic workloads. SambaNova provides reconfigurable dataflow architecture for enterprise AI applications.

[^fn-wafer-scale]: **Wafer-scale integration**: Building an entire processor on a full silicon wafer (850 cm^2^) rather than dicing it into individual chips. Cerebras's WSE-2 contains 850,000 cores and 40 GB of on-chip SRAM, eliminating off-chip memory bandwidth bottlenecks. The approach requires novel solutions for defect tolerance and power delivery, as traditional packaging assumes perfect dies.

[^fn-bsp]: **Bulk Synchronous Parallel (BSP)**: An execution model where computation proceeds in supersteps: parallel computation, global communication, then barrier synchronization. Graphcore's IPU implements BSP at the hardware level, with all 1,472 cores executing the same phase simultaneously. This deterministic execution simplifies debugging but requires all operations to complete within time bounds.

These alternatives find niches where their architectural trade-offs align with workload requirements, though NVIDIA and Google maintain dominant market positions for general ML training.

### Quantitative Infrastructure Analysis

Effective infrastructure decisions require quantitative comparison across accelerator options and workload types.

**FLOPS per Watt Comparison.** Energy efficiency varies significantly across accelerator types and precision levels:

+----------------------+-----------------+-----------------+-----------------+
| **Accelerator**      | **FP16 TFLOPS** | **TDP (Watts)** | **TFLOPS/Watt** |
+=====================:+================:+================:+================:+
| **NVIDIA H100 SXM**  | 990             | 700             | 1.41            |
+----------------------+-----------------+-----------------+-----------------+
| **NVIDIA H100 PCIe** | 756             | 350             | 2.16            |
+----------------------+-----------------+-----------------+-----------------+
| **Google TPU v5p**   | 459             | 250-400*        | 1.1-1.8         |
+----------------------+-----------------+-----------------+-----------------+
| **AWS Trainium**     | 210             | 150 (estimated) | 1.40            |
+----------------------+-----------------+-----------------+-----------------+

*TPU power varies significantly by deployment configuration and is not officially published. Direct TFLOPS/Watt comparisons across architectures are problematic because utilization profiles differ. These figures should be treated as approximate.

The PCIe variant's higher efficiency reflects reduced interconnect power, acceptable for inference but limiting for distributed training.

**Memory Bandwidth Utilization.** Different model types exhibit distinct memory access patterns. LLM training is memory-bound for attention computation, achieving 70 to 85 percent bandwidth utilization. CNN training is compute-bound for convolutions, reaching 30 to 50 percent bandwidth utilization. Recommendation inference is memory-bound for embeddings, often exceeding available bandwidth. Understanding these patterns guides accelerator selection. Memory-bound workloads benefit from HBM3's bandwidth improvements, while compute-bound workloads prioritize FLOPS per dollar.

**The Roofline Model.** The roofline model[^fn-roofline] [@williams2009roofline] provides a systematic framework for understanding whether workloads are compute-bound or memory-bound. Achievable performance is limited by the minimum of peak compute and memory bandwidth:

::: {.callout-definition title="Arithmetic Intensity"}
**Arithmetic Intensity** is the ratio of floating-point operations (FLOPs) performed to bytes of memory accessed (Bytes) during a computation. It determines whether a workload is _compute-bound_ (limited by processor speed) or _memory-bound_ (limited by bandwidth).
:::

::: {.callout-definition title="Domain-Specific Architecture (DSA)"}
***Domain-Specific Architecture (DSA)*** is a class of processors tailored to a specific domain of workloads (like deep learning) rather than general-purpose computation. DSAs (such as TPUs) trade flexibility for performance per watt by optimizing memory hierarchies and arithmetic units specifically for the domain's dominant operations (e.g., matrix multiplication).
:::

::: {#fig-roofline-model fig-env="figure" fig-pos="htb" fig-cap="**Roofline Performance Model**: A visual representation of performance limits plotting achievable FLOPS against arithmetic intensity (FLOPS/Byte). The slanted \"roof\" represents the memory-bound region where bandwidth constrains performance, while the flat \"roof\" represents the compute-bound region limited by peak processor throughput. Operational points for LLMs typically fall under the slanted roof, indicating memory bandwidth dependence."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, every node/.style={transform shape}]
    % Define standard colors locally for now
    \definecolor{BlueLine}{RGB}{0,80,180}
    \definecolor{RedLine}{RGB}{200,30,30}

    % Axes
    \draw[->, thick] (0,0) -- (7,0) node[below, midway, yshift=-0.3cm] {Arithmetic Intensity (FLOP/Byte)};
    \draw[->, thick] (0,0) -- (0,5.0) node[above, rotate=90, midway, yshift=0.3cm] {Performance (OPS)};

    % Ticks
    \node[below] at (0,0) {$10^{-1}$};
    \node[below] at (2,0) {$10^{1}$};
    \node[below] at (4,0) {$10^{2}$};
    \node[below] at (6,0) {$10^{3}$}; % Log scale approximation

    \node[left] at (0,1) {$10^{12}$};
    \node[left] at (0,3) {$10^{14}$};
    \node[left] at (0,4.5) {$10^{15}$};

    % Coordinates for Roofline
    \coordinate (ridge) at (4.0, 4.0);
    \coordinate (start) at (0,0);
    \coordinate (end) at (7.0, 4.0);

    % Regions
    \fill[BlueLine!10] (start) -- (ridge) -- (4.0, 0) -- cycle;
    \node[BlueLine, rotate=45, font=\bfseries] at (2.5, 1.5) {Memory Bound};

    \fill[RedLine!10] (4.0, 0) -- (ridge) -- (end) -- (7.0, 0) -- cycle;
    \node[RedLine, font=\bfseries] at (5.5, 2.0) {Compute Bound};

    % Roofline Plot
    \draw[ultra thick, BlueLine] (start) -- (ridge) node[midway, above, rotate=45] {Bandwidth Limited};
    \draw[ultra thick, RedLine] (ridge) -- (end) node[midway, above] {Compute Limited};

    % Ridge Point
    \fill[black] (ridge) circle (2pt);
    \node[above right] at (ridge) {Ridge Point};
    \draw[dashed, thin] (ridge) -- (4.0, 0);

    % Workload dots
    \node[circle, fill=orange, inner sep=2pt, label={right:LLM (Attention)}] at (2.5, 2.5) {};
    \node[circle, fill=green!60!black, inner sep=2pt, label={below:ConvNet}] at (5.5, 3.5) {};

\end{tikzpicture}
```
:::

[^fn-roofline]: **Roofline model**: A visual performance model developed at Berkeley that plots achievable FLOPS against arithmetic intensity (FLOPS per byte of memory traffic). The "roofline" consists of a sloped region (memory-bound, limited by bandwidth) and a flat region (compute-bound, limited by peak FLOPS). The intersection point, called the ridge point, indicates where workloads transition between these regimes. This model helps identify whether to optimize for compute or memory access.

$$
\text{Achievable FLOPS} = \min\left(\text{Peak Compute}, \text{Memory Bandwidth} \times \text{Arithmetic Intensity}\right)
$$

Arithmetic intensity measures FLOPS per byte of memory traffic. @fig-roofline-model reveals that workloads fall into two distinct regimes: the sloped region where memory bandwidth limits performance (LLM attention at 50-100 FLOP/byte) and the flat region where peak compute limits performance (convolutions at 200+ FLOP/byte). The "ridge point" where these limits intersect determines which workloads benefit from each resource:

+-----------------+------------------+---------------+-----------------+
| **Accelerator** | **Peak Compute** | **Memory BW** | **Ridge Point** |
|                 | **(TF FP16)**    | **(TB/s)**    | **(FLOP/byte)** |
+================:+=================:+==============:+================:+
| **H100 SXM**    | 990              | 3.4           | 291             |
+-----------------+------------------+---------------+-----------------+
| **A100 80GB**   | 312              | 2.0           | 156             |
+-----------------+------------------+---------------+-----------------+
| **TPU v4**      | 275              | 1.2           | 229             |
+-----------------+------------------+---------------+-----------------+

Most LLM training operates at 50 to 100 FLOP/byte arithmetic intensity, well below the ridge point, making these workloads memory-bound. At 75 FLOP/byte on H100, achievable performance is $3.4 \times 75 = 255$ TF, only 26 percent of peak compute. This explains why production training achieves 30 to 50 percent of theoretical FLOPS. The bottleneck is memory bandwidth, not compute capacity.

CNN training with large batch sizes operates near 200 FLOP/byte, approaching the ridge point where both resources limit performance. Recommendation inference with random embedding lookups operates at extremely low arithmetic intensity of 1 to 10 FLOP/byte, fundamentally memory-bound regardless of accelerator choice.

::: {.callout-notebook title="Engineering Metric: Model FLOPs Utilization (MFU)"}
**The Problem**: You buy an H100 GPU advertised at 990 TFLOPS (FP16). Your training logs show it's only processing data at a rate equivalent to 350 TFLOPS. Where did the performance go?

**Definition**:
$$ \text{MFU} = \frac{\text{Achieved FLOPS}}{\text{Peak Theoretical FLOPS}} $$

**Calculation for Transformer Training**:
For a model with $P$ parameters trained on $D$ tokens in $T$ seconds:
$$ \text{Achieved FLOPS} \approx \frac{6 \cdot P \cdot D}{T} $$
*(Factor of 6 accounts for forward + backward pass operations per parameter)*.

**Worked Example**:
*   **Model**: 70B parameters ($70 \times 10^9$)
*   **Data**: 100B tokens
*   **Time**: 14 days ($1.2 \times 10^6$ seconds) on 512 H100s.

1.  **Total FLOPs Required**: $6 \cdot (70 \times 10^9) \cdot (100 \times 10^9) = 4.2 \times 10^{22}$ FLOPs.
2.  **Achieved System Throughput**: $\frac{4.2 \times 10^{22}}{1.2 \times 10^6} = 3.5 \times 10^{16}$ FLOPS ($35$ PFLOPS).
3.  **Per-GPU Throughput**: $35 \text{ PFLOPS} / 512 = 68 \text{ TFLOPS}$.
4.  **MFU**: $68 / 990 \approx \mathbf{6.8\%}$.

**Conclusion**: This low MFU indicates severe bottlenecks—likely memory bandwidth (waiting for weights) or communication overhead (waiting for AllReduce). Good MFU for LLMs is 40-50%.
:::

**Cost per PFLOP.** Infrastructure economics depend on utilization and workload fit:

$$
\text{Effective Cost per PFLOP} = \frac{\text{Hardware Cost} + \text{3-year OpEx}}{\text{Peak PFLOPS} \times \text{Average Utilization} \times 3 \text{ years}}
$$

For a DGX H100 at $300,000 with $50,000 annual power and cooling costs, achieving 50 percent average utilization yields an effective cost of approximately $0.35 per PFLOP-hour. Cloud instances at $30 per hour for equivalent hardware cost $0.15 per PFLOP-hour at 100 percent utilization, but on-premise becomes favorable above 40 percent sustained utilization over three years.

::: {.callout-warning}
## Utilization Reality

Quoted peak FLOPS numbers assume perfect utilization. Production training jobs typically achieve 30 to 50 percent of peak [@mattson2020mlperf] due to communication overhead, data pipeline stalls, and suboptimal kernel efficiency. Infrastructure planning must account for realistic utilization rates rather than theoretical peaks.
:::

This infrastructure foundation enables the distributed training strategies examined in subsequent chapters. @sec-distributed-training builds on these physical capabilities to develop data, tensor, and pipeline parallelism strategies, while @sec-communication analyzes the collective operations that these networks must support. The physical constraints examined here, particularly power delivery, cooling capacity, and interconnect topology, ultimately determine what scale of training is achievable.

## Total Cost of Ownership Analysis

Understanding the complete financial picture of ML infrastructure requires moving beyond simple hardware acquisition costs to comprehensive Total Cost of Ownership (TCO) analysis. This section provides quantitative frameworks for evaluating infrastructure investments, comparing deployment strategies, and optimizing long-term operational efficiency.

### Capital Expenditure Components

Capital expenditure[^fn-capex-opex] encompasses all upfront investments required to establish ML infrastructure. These costs are typically amortized over 3 to 5 years, though the rapid pace of GPU advancement often compresses effective useful life.

[^fn-capex-opex]: **CapEx vs OpEx**: Capital expenditure (CapEx) covers upfront asset purchases (hardware, construction) that are depreciated over time, while operational expenditure (OpEx) covers ongoing costs (power, staff, cloud fees) that are expensed immediately. Cloud computing shifts costs from CapEx to OpEx, which affects financial planning, tax treatment, and budget approval processes differently across organizations.

#### Hardware Costs

GPU and accelerator acquisition represents the dominant CapEx component for ML infrastructure. Current market pricing reflects both performance capabilities and supply constraints.

+----------------------+---------------+-------------------+--------------------+
| **System**           | **Base Cost** | **Memory Config** | **Cost per PFLOP** |
+=====================:+==============:+==================:+===================:+
| **DGX H100**         | ~$300,000     | 640 GB HBM3       | ~$75,000           |
+----------------------+---------------+-------------------+--------------------+
| **DGX B100***        | ~$450,000     | 1.4 TB HBM3e      | ~$25,000           |
+----------------------+---------------+-------------------+--------------------+
| **HGX H100 (8-way)** | ~$250,000     | 640 GB HBM3       | ~$62,500           |
+----------------------+---------------+-------------------+--------------------+
| **TPU v5p Pod**      | Variable      | 95 GB HBM         | ~$30,000           |
+----------------------+---------------+-------------------+--------------------+

*B100 pricing is estimated. All prices reflect approximate 2024 market conditions and should be verified for current planning. GPU pricing fluctuates 20 to 40 percent based on supply constraints and generation transitions.

Server and storage costs add substantial overhead beyond accelerators. A complete DGX H100 deployment requires NVMe storage at $15,000 to 30,000 per node, high-speed networking cards at $8,000 to 15,000, and rack infrastructure at $5,000 to 10,000. Storage architecture for large-scale training demands parallel file systems capable of sustaining the I/O bandwidth required by hundreds of GPUs, with enterprise solutions like Lustre or GPFS adding $500 to 1,000 per terabyte of high-performance capacity.

Networking equipment costs scale superlinearly with cluster size due to the hierarchical nature of high-bandwidth fabrics. A 256-GPU cluster using InfiniBand HDR requires approximately $800,000 to 1,200,000 in networking equipment.

$$C_{\text{network}} = N_{\text{switches}} \cdot P_{\text{switch}} + N_{\text{cables}} \cdot P_{\text{cable}} + N_{\text{adapters}} \cdot P_{\text{adapter}}$$

For a 256-GPU deployment with 2:1 oversubscription:

$$C_{\text{network}} \approx 32 \times \$15,000 + 512 \times \$800 + 256 \times \$3,000 \approx \$1,660,000$$

Refresh cycle planning significantly impacts TCO calculations. GPU generations advance every 2-3 years with typical performance improvements of 2-3x per generation. Organizations must balance the benefits of newer hardware against the disruption costs of migration. A common strategy employs staggered refresh cycles, replacing 25-33% of infrastructure annually to maintain competitive capability while avoiding wholesale replacement costs.

#### Facility Costs

Datacenter construction costs range from $7-12 million per megawatt of IT capacity for purpose-built facilities. ML workloads, with their high power density requirements (30-50 kW per rack versus 5-10 kW for traditional compute), demand specialized cooling infrastructure that increases construction costs by 20-40%.

Power infrastructure represents a substantial portion of facility investment. Electrical distribution systems including transformers, switchgear, uninterruptible power supplies (UPS), and power distribution units (PDUs) typically cost $2-4 million per megawatt. Redundancy requirements (N+1 or 2N configurations) can double these costs for mission-critical deployments.

Cooling systems for high-density ML infrastructure increasingly require liquid cooling solutions. Direct-to-chip liquid cooling adds $50,000-100,000 per rack in capital costs but enables the power densities required for modern GPU configurations. The DGX H100 systems referenced in our datacenter architecture discussion require liquid cooling for sustained operation, representing a non-optional facility cost.

### Operational Expenditure Components

Operational expenditure (OpEx) captures ongoing costs that accumulate throughout infrastructure lifetime. For ML systems, power costs and specialized staffing dominate this category.

#### Power Costs

Electricity represents the largest operational cost for ML infrastructure. Power costs vary dramatically by geography, with industrial rates ranging from $0.04/kWh in regions with abundant hydroelectric power to $0.20/kWh in constrained markets.

::: {.callout-notebook title="Engineering Calculation: Annual Power Costs" collapse="true"}
**Scenario**: Calculating the electricity bill for a single DGX H100 system.
**Parameters**:

- **Power**: 10.2 kW (max load)
- **PUE**: 1.15 (efficient liquid cooling)
- **Utilization**: 80% (high sustained load)
- **Rate**: $0.08/kWh (industrial rate)

**Calculation**:
$$ \text{Annual Cost} = P_{\text{system}} \times \text{PUE} \times H_{\text{hours}} \times R_{\text{rate}} \times U_{\text{util}} $$
$$ = 10.2 \times 1.15 \times 8760 \times 0.08 \times 0.8 $$
$$ \approx \mathbf{\$6,575 \text{ per year}} $$

**Total 3-Year OpEx**: $\approx \$20,000$ (just for power). Start-up CapEx is ~$300k.
:::

Electricity pricing models significantly impact operational costs. Time-of-use pricing creates opportunities for training workload scheduling during off-peak hours (typically nights and weekends), potentially reducing power costs by 20-40%. Demand charges, which price peak power consumption, incentivize workload smoothing to avoid utilization spikes.

Renewable energy considerations extend beyond environmental responsibility to economic optimization. Power Purchase Agreements (PPAs) for renewable energy often provide long-term price stability, hedging against electricity market volatility. Many organizations target 100% renewable energy matching through a combination of on-site generation, PPAs, and Renewable Energy Certificates (RECs). @sec-sustainable-ai develops the comprehensive framework for quantifying environmental impact, including lifecycle carbon assessment and the geographic optimization strategies that can reduce emissions by 50-80% through thoughtful infrastructure placement.

#### Staffing and Operations

ML infrastructure requires specialized operational expertise across multiple domains. Staffing costs often represent 15-25% of total operational expenditure for well-run facilities.

Hardware operations teams manage physical infrastructure including installation, maintenance, and failure response. For clusters of 500+ GPUs, dedicated hardware technicians are essential, with typical ratios of 1 technician per 200-400 GPUs depending on hardware heterogeneity and SLA requirements.

Software platform teams maintain the scheduling systems, container infrastructure, and ML frameworks that enable productive use of hardware resources. These roles command premium compensation due to the specialized intersection of systems engineering and ML expertise required.

Utilization monitoring represents both a staffing function and a key lever for TCO optimization. Continuous monitoring of GPU utilization, memory bandwidth, and job efficiency enables identification of optimization opportunities. Organizations achieving 70%+ sustained GPU utilization versus the more common 30-50% effectively halve their per-computation infrastructure costs.

### Build vs. Buy Analysis

The fundamental infrastructure decision is whether to operate private infrastructure or consume cloud capacity. This choice involves complex trade-offs that depend on workload characteristics, scale, and organizational capabilities.

#### Cloud vs. On-Premises Trade-offs

Cloud computing offers compelling advantages for specific use cases. Variable workloads with unpredictable demand benefit from cloud elasticity, avoiding stranded capacity during low-demand periods. Experimentation and research phases, where hardware requirements remain uncertain, benefit from the ability to test different configurations without capital commitment. Geographic distribution requirements for inference serving often favor cloud deployment due to the substantial investment required for multi-region presence.

On-premises infrastructure wins economically under sustained high utilization. The break-even analysis requires comparing amortized CapEx plus OpEx against equivalent cloud costs:

$$\text{Break-even utilization} = \frac{C_{\text{cloud}} \times H_{\text{annual}}}{\frac{C_{\text{capex}}}{Y_{\text{amortization}}} + C_{\text{opex}}}$$

Consider a DGX H100 system with $300,000 CapEx, 3-year amortization, and $25,000 annual OpEx (power, maintenance, proportional staff). Cloud equivalent (8x H100 instance at ~$25/hour):

$$\text{Break-even} = \frac{25 \times 8,760}{\frac{300,000}{3} + 25,000} = \frac{219,000}{125,000} \approx 1.75$$

This calculation suggests on-premises becomes favorable when utilization exceeds approximately 57% (1/1.75). In practice, organizations report break-even utilization thresholds of 40-60% depending on specific cloud pricing and operational efficiency.

+----------------------+-------------------+------------------------+
| **Factor**           | **Favors Cloud**  | **Favors On-Premises** |
+:=====================+:==================+:=======================+
| **Utilization**      | &lt;40% average   | &gt;60% sustained      |
+----------------------+-------------------+------------------------+
| **Workload pattern** | Variable, bursty  | Steady, predictable    |
+----------------------+-------------------+------------------------+
| **Data volume**      | Moderate          | Petabyte-scale         |
+----------------------+-------------------+------------------------+
| **Time horizon**     | &lt;2 years       | &gt;3 years            |
+----------------------+-------------------+------------------------+
| **Team capability**  | Limited ops staff | Strong infrastructure  |
+----------------------+-------------------+------------------------+

Hybrid strategies combine cloud burst capacity with on-premises baseline infrastructure. Organizations maintain on-premises systems sized for typical load (e.g., 60th percentile demand) while using cloud for peak periods. This approach captures most on-premises economic benefits while retaining cloud flexibility.

#### Reserved Capacity vs. Spot Instances

Cloud providers offer commitment discount programs that substantially reduce effective pricing. Reserved instances with 1-year commitments typically offer 30-40% discounts, while 3-year commitments reach 50-60% discounts relative to on-demand pricing. These discounts shift cloud economics but introduce utilization risk similar to on-premises ownership.

Spot instance strategies enable dramatic cost reduction (60-80% below on-demand) for fault-tolerant training workloads. Effective spot utilization requires:

1. **Checkpoint integration**: Training frameworks must save state frequently enough that spot interruption costs remain acceptable. Modern distributed training checkpoints every 10-30 minutes, limiting maximum lost computation.

2. **Fallback mechanisms**: Automated job migration to alternative instance types or regions when spot capacity becomes unavailable.

3. **Heterogeneous training**: Frameworks capable of operating across mixed instance types to maximize spot availability.

The effective spot discount must account for interruption overhead:

$$C_{\text{effective}} = C_{\text{spot}} \times (1 + R_{\text{interrupt}} \times T_{\text{recovery}})$$

where $R_{\text{interrupt}}$ is the hourly interruption rate and $T_{\text{recovery}}$ is recovery time as a fraction of checkpoint interval. With 5% hourly interruption rate and 10-minute recovery on 30-minute checkpoints:

$$C_{\text{effective}} = 0.30 \times C_{\text{ondemand}} \times (1 + 0.05 \times 0.33) \approx 0.31 \times C_{\text{ondemand}}$$

Even accounting for interruption overhead, spot instances provide compelling economics for training workloads with proper checkpoint infrastructure.

### Comprehensive TCO Model

A complete TCO model integrates capital and operational components across the infrastructure lifetime:

$$\text{TCO} = \sum_{t=1}^{Y} \frac{C_{\text{capex}}^{(t)} + C_{\text{opex}}^{(t)}}{(1+r)^t}$$

where $r$ is the discount rate reflecting cost of capital. For a 256-GPU cluster over 4 years:

+-------------------------+-------------+------------+------------+------------+
| **Component**           | **Year 1**  | **Year 2** | **Year 3** | **Year 4** |
+:========================+============:+===========:+===========:+===========:+
| **Hardware CapEx**      | $9,600,000  | $0         | $0         | $3,200,000 |
| **Network CapEx**       | $1,660,000  | $0         | $0         | $0         |
| **Power (at 70% util)** | $1,690,000  | $1,690,000 | $1,690,000 | $1,690,000 |
| **Maintenance**         | $480,000    | $576,000   | $691,000   | $829,000   |
| **Staff (allocated)**   | $800,000    | $840,000   | $882,000   | $926,000   |
| **Annual Total**        | $14,230,000 | $3,106,000 | $3,263,000 | $6,645,000 |
+-------------------------+-------------+------------+------------+------------+

@fig-tco-breakdown reveals the cost structure's evolution over a four-year lifecycle: hardware CapEx dominates Year 1, but accumulated operational expenses (power and staffing) eventually exceed the initial capital investment, making sustained utilization the critical economic lever:

::: {#fig-tco-breakdown fig-env="figure" fig-pos="htb" fig-cap="**Total Cost of Ownership Breakdown**. Analysis of infrastructure costs over a 4-year lifecycle. While hardware CapEx is the largest initial outlay, operational costs (Power and Staffing) accumulate to exceed hardware costs over the system's life. High utilization is key to amortizing these fixed and ongoing costs."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{CapExColor}{RGB}{70,130,180}
  \definecolor{OpExColor}{RGB}{205,92,92}
  \definecolor{OpExColor2}{RGB}{240,128,128}

  % Axes
  \draw[->, thick, black!70] (0,0) -- (6.5,0) node[right, font=\footnotesize] {Year};
  \draw[->, thick, black!70] (0,0) -- (0,5.5) node[above, font=\footnotesize, rotate=90, anchor=south, xshift=-0.5cm] {Cumulative Cost (\$M)};

  % Ticks
  \foreach \y in {1,2,3,4,5} \draw[gray!30] (0,\y) -- (6,\y);
  \node[left, font=\scriptsize] at (0,1) {2M};
  \node[left, font=\scriptsize] at (0,2) {4M};
  \node[left, font=\scriptsize] at (0,3) {6M};
  \node[left, font=\scriptsize] at (0,4) {8M};
  \node[left, font=\scriptsize] at (0,5) {10M};

  % Stacked Bar Data
  % Y1: HW(4M/2=2.0) + Power(0)
  % Y2: + Power(1.0)
  % Y3: + Power(1.0)
  % Y4: + Power(1.0)
  % Wait, CapEx is typically upfront.
  % Let's visualize cumulative.

  % Bar 1 (Year 1)
  \draw[fill=CapExColor] (1,0) rectangle (1.8, 2.0); % 4M HW
  \node[below, font=\scriptsize] at (1.4,0) {Y1};

  % Bar 2 (Year 2)
  \draw[fill=CapExColor] (2.5,0) rectangle (3.3, 2.0);
  \draw[fill=OpExColor] (2.5, 2.0) rectangle (3.3, 2.6); % +1.2M Power/Ops
  \node[below, font=\scriptsize] at (2.9,0) {Y2};

  % Bar 3
  \draw[fill=CapExColor] (4.0,0) rectangle (4.8, 2.0);
  \draw[fill=OpExColor] (4.0, 2.0) rectangle (4.8, 2.6);
  \draw[fill=OpExColor2] (4.0, 2.6) rectangle (4.8, 3.2); % + 1.2M more
  \node[below, font=\scriptsize] at (4.4,0) {Y3};

  % Bar 4
  \draw[fill=CapExColor] (5.5,0) rectangle (6.3, 2.0);
  \draw[fill=OpExColor] (5.5, 2.0) rectangle (6.3, 2.6);
  \draw[fill=OpExColor2] (5.5, 2.6) rectangle (6.3, 3.2);
  \draw[fill=OpExColor!80!black] (5.5, 3.2) rectangle (6.3, 3.8); % + 1.2M more
  \node[below, font=\scriptsize] at (5.9,0) {Y4};

  % Legend
  \node[anchor=west, font=\scriptsize] at (7, 4) {\textbf{Legend}};
  \node[fill=CapExColor, minimum size=0.2cm, label={right:\scriptsize Hardware (CapEx)}] at (7.1, 3.5) {};
  \node[fill=OpExColor, minimum size=0.2cm, label={right:\scriptsize Power \& Ops (Accum.)}] at (7.1, 3.0) {};

  % Annotation
  \draw[<-, thick, black!80] (6.4, 3.5) to[bend left] (8.5, 4.5) node[right, align=left, font=\scriptsize, text width=2.5cm] {OpEx eventually\\exceeds CapEx};

\end{tikzpicture}
```
:::

The NPV at 8% discount rate equals approximately $24.1 million, yielding a 4-year cost per GPU-hour of $4.30 at 70% utilization. This compares favorably to cloud A100 pricing of $3-4/hour only when accounting for the H100's 3x performance advantage, yielding effective cost per computation approximately 40% below cloud alternatives at this utilization level.

Power cost sensitivity analysis reveals the importance of electricity pricing in deployment decisions. A $0.04/kWh difference in electricity rates shifts the 4-year TCO by approximately $2.7 million for a 256-GPU cluster, potentially changing the optimal deployment strategy. Organizations with access to low-cost renewable energy enjoy structural cost advantages that compound over multi-year infrastructure investments.

## Case Studies

The infrastructure patterns examined in previous sections combine in different configurations depending on workload characteristics and organizational constraints. Four production deployments illustrate how datacenter architecture, networking, and resource management decisions interact to enable distinct ML workloads. Each case study represents a different point in the design space: GPU-centric dense training, TPU-based transformer optimization, hybrid CPU-GPU recommendation serving, and custom silicon for domain-specific acceleration.

### NVIDIA DGX SuperPOD Architecture

The DGX SuperPOD represents NVIDIA's reference architecture for large-scale training, combining the dense GPU packaging of DGX systems with purpose-built networking. While @sec-cluster-networking examines the SuperPOD's networking topology, this case study addresses the complete system architecture including physical deployment, management infrastructure, and operational characteristics.

#### Physical Layout and Cooling Integration

A production SuperPOD deployment with 512 DGX H100 systems (4096 GPUs) occupies approximately 2000 square meters of datacenter floor space. The layout follows a pod-based organization where groups of 32 DGX systems share common power and cooling infrastructure. Each pod dissipates over 300 kW, requiring direct liquid cooling loops with facility-level heat exchangers.

The cooling architecture uses a closed-loop system with water temperature maintained at 35-45C entering the cold plates. Unlike traditional datacenter cooling that targets low air temperatures, warm-water cooling improves efficiency by enabling free cooling in moderate climates. Heat removed from GPU cold plates transfers to building cooling towers without mechanical refrigeration for ambient temperatures below 25C.

Power distribution follows the N+1 redundancy model at the pod level, with each DGX system receiving dual power feeds. A complete SuperPOD installation requires 5-7 MW of utility power including cooling overhead, corresponding to PUE values of 1.2-1.3 for liquid-cooled deployments.

#### Management Plane Architecture

SuperPOD management integrates multiple control systems spanning hardware, networking, and workload orchestration. Base Controller Manager (BCM) provides hardware-level management including firmware updates, health monitoring, and out-of-band access. The Unified Fabric Manager coordinates InfiniBand network configuration, adaptive routing policies, and link health monitoring.

At the workload level, SuperPOD deployments typically integrate with either Slurm or Kubernetes for job scheduling. The NVIDIA GPU Operator handles GPU driver installation, monitoring integration, and device plugin management for Kubernetes environments. Slurm configurations use GRES scheduling with topology-aware placement to ensure jobs receive contiguous GPU allocations that minimize inter-node communication.

Storage integration varies by deployment, but reference architectures include NVIDIA's GPUDirect Storage for direct data paths between NVMe storage and GPU memory. A typical SuperPOD includes 30-50 PB of high-performance storage providing 200+ GB/s aggregate throughput, staging training data close to compute.

### Google TPU Pod Infrastructure

Google's TPU pods represent an alternative architectural philosophy: vertically integrated accelerators designed specifically for transformer training, with interconnect capabilities built into the chip rather than added as external networking.

#### TPU v4 Pod Architecture

A TPU v4 pod contains 4096 TPU chips arranged in a 3D torus topology. Each chip provides approximately 275 TFLOPS of bfloat16 compute with 32 GB of HBM2e memory, yielding aggregate pod capacity of 1.1 exaFLOPS and 128 TB of memory. The power envelope for a complete pod is approximately 4-5 MW, competitive with GPU-based systems at similar compute density.

The physical packaging differs fundamentally from GPU systems. TPU chips mount in trays of 4, with trays assembled into racks of 64 chips each. Sixty-four racks form the complete pod, arranged in a cube topology that matches the 3D torus interconnect structure. Cooling uses rear-door heat exchangers with facility water, maintaining chip temperatures below 85C under sustained load.

#### Inter-Chip Interconnect Topology

The ICI (Inter-Chip Interconnect) fabric provides direct chip-to-chip connectivity without external switches. Each TPU v4 chip has six ICI links at 100 GB/s each, enabling 3D torus connectivity:

$$
\text{Bisection Bandwidth} = 2 \times \sqrt[3]{N} \times B_{\text{link}} \times N/2
$$

For N=4096 chips with 100 GB/s links, the torus bisection bandwidth reaches approximately 32 TB/s. While lower than fat-tree alternatives, the consistent latency characteristics of torus topology benefit the regular communication patterns of transformer training.

The topology choice optimizes for AllReduce patterns where each chip communicates with neighbors rather than arbitrary endpoints. For a model using 3D parallelism with 4 tensor-parallel chips, 16 pipeline stages, and 64-way data parallelism, the workload maps naturally onto a 4x16x64 slice of the pod topology.

#### Software Stack Integration

TPU software centers on JAX and XLA[^fn-xla], with pjit (partitioned JIT compilation) managing distributed execution. XLA compiles high-level model descriptions to TPU-specific operations, automatically inserting communication collectives based on partition specifications. This approach differs from the explicit communication programming required for GPU clusters.

[^fn-xla]: **XLA (Accelerated Linear Algebra)**: A domain-specific compiler for linear algebra operations that optimizes entire computation graphs rather than individual operations. XLA performs operator fusion (combining multiple ops into single kernels), buffer reuse optimization, and automatic layout transformation. Originally developed for TPUs, XLA now supports GPUs and CPUs, enabling framework-agnostic optimization of ML computations.

Multislice training extends beyond single pods by connecting multiple TPU slices via datacenter network. A PaLM-scale training run might utilize four TPU v4 pods (16,384 chips) with cross-slice communication at lower bandwidth than intra-slice ICI. The software stack handles this hierarchy transparently, using different collective algorithms for intra-slice versus inter-slice operations.

### Meta Recommendation Infrastructure

Meta's recommendation systems illustrate infrastructure optimized for a fundamentally different workload pattern: models combining massive embedding tables with relatively modest dense computation. This architecture serves billions of daily recommendation queries across products including Facebook Feed, Instagram, and Reels.

::: {.callout-note title="Archetype B (Scaled Lighthouse): The Memory Capacity Wall"}
**Archetype B (The Global Real-Time Recommendation Engine)** defines this architectural split. Unlike Archetype A (which is compute-bound), Archetype B is **memory-capacity bound**. The embedding tables representing billions of users and items can exceed 10TB. Since this cannot fit in GPU HBM, the system must adopt a hybrid design: storing embeddings in massive CPU DRAM tiers while using GPUs only for the dense compute layers.
:::

#### CPU-GPU Hybrid Architecture

Recommendation models like DLRM[^fn-dlrm] (Deep Learning Recommendation Model) partition naturally between embedding operations and dense neural network computation. Embedding tables for production systems can exceed 10 TB, far exceeding GPU memory capacity. The hybrid architecture addresses this by placing embeddings in CPU DRAM while dense layers execute on GPUs.

[^fn-dlrm]: **DLRM (Deep Learning Recommendation Model)**: Meta's open-source recommendation model architecture that became the reference design for industry recommendation systems. DLRM processes both dense features (through MLPs) and sparse features (through embedding tables), combining them via a factorization machine-inspired interaction layer. The architecture explicitly acknowledges the embedding-memory bottleneck, making it a natural fit for hybrid CPU-GPU deployment.

A production recommendation training node combines multiple CPUs totaling 2-4 TB of DRAM with 8 GPUs for dense computation. The CPUs handle embedding lookups, concatenation, and feature preprocessing. Resulting feature vectors transfer to GPUs via PCIe for the dense forward and backward passes. Gradient updates for embeddings return to CPU memory via the same path.

This architecture requires careful balancing. The ratio of embedding lookups to dense computation determines optimal CPU-to-GPU allocation. For Meta's workloads, approximately 4:1 CPU socket to GPU ratios provide balanced utilization, though this varies by model architecture.

#### Embedding Table Serving at Scale

Inference architecture differs from training by emphasizing latency over throughput. Production serving distributes embedding tables across a fleet of CPU-based servers using consistent hashing for shard assignment. A single recommendation query may access hundreds of embedding shards, requiring parallel lookups that complete within the 50-100 ms latency budget.

The embedding serving tier operates separately from the dense model serving tier. This separation enables independent scaling: embedding servers scale with table size and query rate, while dense model servers scale with compute requirements. Cross-tier communication uses low-latency RPC, typically completing in under 5 ms for local datacenter deployments.

Feature stores cache frequently accessed embeddings and precomputed features, reducing embedding server load for popular items. A tiered caching architecture places hot embeddings in GPU memory (microsecond access), warm embeddings in CPU DRAM (sub-millisecond), and cold embeddings in distributed storage (milliseconds). Cache hit rates above 90% are typical for recommendation workloads due to power-law popularity distributions.

#### Training and Serving Coordination

The separation between training and serving infrastructure creates coordination challenges for model updates. Meta's approach uses a staged rollout pipeline: models train on dedicated GPU clusters, export to serving format, deploy to staging clusters for validation, then gradually roll out to production serving. The complete pipeline from training completion to full production deployment spans hours to days depending on model criticality.

Training clusters optimize for throughput using large batch sizes and aggressive gradient accumulation. Serving clusters optimize for latency using quantized models, batched inference, and result caching. The different optimization targets justify separate infrastructure rather than shared clusters.

### Tesla Dojo for Vision Training

Tesla's Dojo system represents the custom silicon approach to ML infrastructure: building purpose-designed chips and packaging for a specific workload rather than using general-purpose accelerators.

#### Custom Silicon Architecture

The Dojo D1 chip provides 1024 custom-designed cores in a 645 mm^2^ die. Each core combines an 8-wide vector unit, 64-bit scalar unit, and 1.25 MB of SRAM, yielding approximately 22.6 TFLOPS of BF16[^fn-bf16] compute per chip. The design optimizes for convolutional and attention operations typical of vision models, with dataflow execution patterns that minimize memory traffic.

[^fn-bf16]: **BF16 (Brain Floating Point)**: A 16-bit floating point format with the same 8-bit exponent as FP32 but only 7 mantissa bits (versus 23 in FP32). Developed by Google for TPUs, BF16 matches FP32's dynamic range, avoiding the overflow/underflow issues of FP16 that require loss scaling. BF16 has become the default training precision for transformers, offering 2x memory savings with minimal accuracy impact.

Twenty-five D1 chips mount on a single training tile, connected via a 2D mesh interconnect providing 4 TB/s aggregate bandwidth. Six tiles combine into a system tray, and multiple trays assemble into a complete ExaPOD delivering over 1 exaFLOP of aggregate compute. The modular architecture enables deployments from single tiles (0.5 PFLOPS) to multi-ExaPOD installations.

#### Wafer-Scale Considerations

While Dojo uses conventional chip packaging, the architecture addresses similar challenges to wafer-scale integration: maximizing on-chip bandwidth while managing thermal and yield constraints. The 2D mesh topology within each tile provides nearest-neighbor bandwidth of 18 GB/s between chips, avoiding the bottlenecks of hierarchical topologies for spatially-local operations common in vision processing.

Power density presents the primary challenge: a fully populated system tray dissipates over 100 kW in a compact form factor. Tesla's thermal solution uses direct liquid cooling with custom manifolds delivering coolant to each training tile. The aggressive cooling enables sustained operation at power densities exceeding traditional datacenter limits.

Yield management for custom silicon requires careful attention. Unlike commodity GPU purchases where defective units return to the vendor, custom chip production creates internal yield loss. Dojo's design includes redundant cores and interconnect paths, enabling graceful degradation when manufacturing defects occur. Production testing identifies defective units, and the software stack maps computation around unavailable resources.

#### Training Video Data at Scale

Dojo's primary workload is training vision models on Tesla's fleet data: over 1 million video clips per day from vehicles worldwide. The data pipeline presents distinct challenges from text or image training. Video requires decompression, temporal alignment, sensor calibration, and often 3D scene reconstruction before training.

The preprocessing pipeline runs on CPU clusters adjacent to Dojo compute, staging prepared batches to high-speed storage. Storage bandwidth of 10+ GB/s per training tile ensures compute utilization despite the data-intensive nature of video processing. The complete system integrates 10 PB of flash storage providing over 100 GB/s aggregate throughput.

This infrastructure supports auto-labeling workflows where preliminary models identify scenarios of interest in raw video, generating training data for improved models. The closed-loop between deployment, data collection, and training enables rapid iteration cycles measured in days rather than weeks.

---

These case studies demonstrate that production ML infrastructure defies one-size-fits-all solutions. DGX SuperPOD optimizes for flexible general-purpose training with emphasis on GPU density and high-bandwidth networking. TPU pods sacrifice flexibility for vertical integration that excels at transformer workloads. Meta's hybrid architecture addresses the embedding-heavy patterns unique to recommendation systems. Tesla's Dojo pursues custom silicon for domain-specific acceleration where scale justifies development costs. The choice among these approaches depends on workload characteristics, scale requirements, and organizational capabilities rather than any universal optimum. Understanding these trade-offs enables informed infrastructure decisions as models and training requirements continue to evolve. @sec-distributed-training provides the implementation details for the parallelism strategies that leverage these infrastructure platforms, while @sec-communication examines the collective operations and algorithms that govern network-level performance.

## Fallacies and Pitfalls {#sec-infrastructure-fallacies-pitfalls}

Infrastructure for distributed ML systems involves counterintuitive interactions between compute, networking, power, and cooling that lead to costly miscalculations. These fallacies and pitfalls capture errors that waste millions in infrastructure investment, delay projects by months, or cause production systems to achieve only 30-50% of planned capacity.

**Fallacy:** _More GPUs always means faster training._

Engineers assume training time scales linearly with GPU count. In production, communication overhead dominates beyond modest cluster sizes. Amdahl's Law establishes the hard limit: gradient synchronization is inherently sequential since all gradients must be collected before any update proceeds. For a 175B parameter model (350 GB gradients) on 64 GPUs with 400 Gbps InfiniBand, AllReduce requires 14 seconds while compute takes 1 second, yielding 6.7% efficiency. Organizations frequently discover that 512-GPU clusters train slower than optimized 128-GPU deployments. At $30/hour per H100, a 512-GPU cluster achieving 25% efficiency wastes $11,520/hour in idle capacity.

**Fallacy:** _Peak FLOPS determines training throughput._

Procurement teams select accelerators by comparing peak FLOPS: H100 delivers 990 TF of FP16 compute. In production workloads, memory bandwidth limits performance. The roofline model in @sec-datacenter-architecture shows that arithmetic intensity below the ridge point (~290 FLOP/byte for H100) makes accelerators memory-bound. Most LLM training operates at 50-100 FLOP/byte, achieving only 168-335 TF effective throughput—17-34% of peak. Organizations that compare accelerators by peak FLOPS alone make purchasing decisions that cost 1.5-2x more per actual training FLOP delivered. For attention-dominated workloads, lower-FLOPS accelerators with higher memory bandwidth often outperform higher-FLOPS alternatives.

**Fallacy:** _All ML infrastructure should be GPU-based._

Teams assume GPUs optimize every ML workload. At scale, hybrid architectures deliver superior economics for embedding-heavy workloads. Recommendation systems, which constitute 80-90% of inference volume at Meta and Google, require random access to terabytes of embedding tables that cannot fit in GPU HBM. These memory-bound lookups perform poorly on GPUs optimized for dense compute. Meta's production infrastructure uses CPU clusters for embedding lookups while GPU clusters process dense neural network layers, achieving 3x better cost efficiency than GPU-only deployments. Organizations that deploy GPU-only infrastructure for diverse workloads waste 40-60% of capacity on tasks CPUs handle more efficiently.

**Pitfall:** _Ignoring power and cooling constraints during infrastructure planning._

Teams plan GPU purchases based on compute requirements without verifying datacenter capacity. Power and cooling represent hard physical limits that cannot be resolved through software optimization. A single rack of 4 DGX H100 systems requires 40 kW of power and generates equivalent thermal load, compared to 5-10 kW for traditional server racks. Organizations discover their facility cannot support the power density only after hardware arrives. Thermal throttling causes GPUs to reduce clock speeds when cooling is inadequate. A cluster designed for 100% utilization may achieve only 70% sustained throughput due to thermal constraints, costing $9,000/hour in lost productivity for a 1000-GPU cluster at $30/hour per H100.

**Pitfall:** _Underestimating network requirements for distributed training._

Operators calculate network bandwidth but ignore latency, topology, and software overhead. As shown in @sec-networking-ml, a cluster with 400 Gbps InfiniBand achieves only 300 Gbps effective throughput due to NCCL protocol overhead, suboptimal job placement, and contention from concurrent jobs. Topology choice critically impacts performance: fat-tree topologies provide full bisection bandwidth at significant switch cost ($800K-$1.2M for 256 GPUs), while rail-optimized topologies reduce hardware cost but constrain job placement. NCCL introduces 5-20 microseconds of software latency per collective operation; for small message sizes common in pipeline parallelism, this overhead dominates transfer time. The crossover where bandwidth dominates latency occurs at approximately 250 KB message size.

**Fallacy:** _Cloud computing costs scale linearly with usage._

Organizations assume cloud expenses grow proportionally with compute consumption. In production, data egress fees and storage costs create nonlinear scaling. Training a 70B parameter model generates 200-400 GB of checkpoints every few hours; with 5 checkpoint retention, storage reaches 1-2 TB per experiment. Cloud storage at $0.023/GB-month costs $23-46/month per experiment, but egress fees for downloading checkpoints cost $0.09/GB—$18-36 per full checkpoint download. Organizations running 100 concurrent experiments accumulate $2,300-4,600/month in storage costs alone. The break-even point where on-premises infrastructure becomes more economical occurs at 40-60% sustained utilization. Teams that migrate to cloud assuming linear scaling discover total costs 1.5-2.5x higher than projected.

**Pitfall:** _Underestimating operational complexity of distributed schedulers._

Teams deploy Kubernetes or Slurm expecting resource management to be "solved." In production, achieving 60%+ GPU utilization requires continuous tuning of gang scheduling policies, preemption strategies, and job bin-packing algorithms. Naive first-come-first-served scheduling fragments nodes: with 8-GPU nodes and 6-GPU jobs, each job wastes 2 GPUs per node, reducing effective capacity to 75%. The CAP theorem established in @sec-vol2-introduction forces schedulers to trade off consistency versus availability. At 4096 GPUs with 99.9% annual reliability per GPU, clusters experience approximately 4 failures per year, requiring robust fault tolerance mechanisms. Organizations that treat scheduling as solved discover their $50M GPU cluster achieves only 35-45% utilization versus the 65-75% possible with dedicated tuning, wasting $5-9M annually.

## Summary

Compute infrastructure represents the physical foundation upon which all distributed training and serving systems operate. The constraints examined in this chapter, from power delivery and cooling capacity to accelerator selection, ultimately determine what scale of ML systems an organization can build and operate effectively.

We examined how power density and cooling represent hard physical limits that constrain cluster design independent of budget. The transition from air-cooled 10kW racks to liquid-cooled 100kW racks is not just an upgrade but a fundamental architectural shift required by modern accelerators.

We analyzed the trade-offs between different accelerator architectures—GPUs, TPUs, and custom ASICs—and how they map to specific workloads. The distinction between training-optimized (high precision, bandwidth) and inference-optimized (low latency, efficiency) hardware is driving specialized fleet designs.

Finally, we established that Total Cost of Ownership extends far beyond hardware acquisition. Power consumption, cooling infrastructure, and operational overhead often exceed the initial cost of GPUs over a three-year lifecycle.

::: {.callout-important title="Key Takeaways"}

* Power density and cooling capacity represent hard physical limits that constrain cluster design independent of budget
* Hybrid CPU-GPU architectures outperform GPU-only configurations for recommendation systems and other embedding-heavy workloads
* Total cost of ownership must include power, cooling, operations, and realistic utilization rates rather than theoretical peak performance
* Accelerator selection requires matching hardware characteristics (memory bandwidth vs. FLOPS) to workload bottlenecks (decode vs. prefill)

:::

With the compute nodes defined, powered, cooled, and racked, we have built the individual engines of the fleet. But these engines cannot operate in isolation. Distributed training requires them to function as a single, coordinated machine.

The next chapter, @sec-cluster-networking, examines the high-bandwidth networking fabrics—InfiniBand, RoCE, and specialized topologies—that bind these nodes together into a unified supercomputer.
