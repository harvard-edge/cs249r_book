---
title: "Large-Scale ML Infrastructure"
bibliography: infrastructure.bib
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR INFRASTRUCTURE
================================================================================

CORE PRINCIPLE: Infrastructure requirements vary by workload type.
Training clusters differ from serving infrastructure. Different model
types have different compute, memory, and networking needs.

MODEL-SPECIFIC INFRASTRUCTURE CONSIDERATIONS:

| Model Type      | Compute Profile     | Memory Profile      | Network Need        |
|-----------------|---------------------|---------------------|---------------------|
| LLMs            | GPU-heavy           | HBM-bound           | High (tensor par.)  |
| Recommendation  | CPU+GPU hybrid      | DRAM for embeddings | Moderate            |
| Vision          | GPU-heavy           | Moderate            | Moderate            |
| Scientific      | Varies              | Often huge          | Problem-dependent   |

REQUIRED COVERAGE FOR THIS CHAPTER:

DATACENTER ARCHITECTURE:

- GPU clusters: Dominant for training transformers, vision
- CPU clusters: Feature serving, preprocessing (RecSys)
- Hybrid: Recommendation training (embedding on CPU, dense on GPU)
- Include: Why different workloads need different architectures

ACCELERATOR SELECTION:

- GPU (NVIDIA): General-purpose ML, dominant for training
- TPU: Large-scale training, specific model types
- Custom ASICs: Inference optimization (recommendation, vision)
- Include: Different accelerators suit different workloads

NETWORKING:

- InfiniBand: Training clusters, high-bandwidth collective ops
- Ethernet: Serving infrastructure, feature stores
- Include: Why training and serving have different network needs

RESOURCE MANAGEMENT:

- Batch scheduling: Training jobs (Slurm, Kubernetes)
- Online serving: Request routing, autoscaling
- Include: Different scheduling for different workload types

CASE STUDIES TO INCLUDE:

- NVIDIA DGX SuperPOD architecture
- Google TPU pod infrastructure
- Meta recommendation infrastructure (CPU+GPU hybrid)
- Tesla Dojo for vision training

QUANTITATIVE ANALYSIS:

- TCO breakdown by workload type
- Power/performance efficiency for different accelerators
- Network utilization patterns by model type
- Include: Same cluster, different efficiency for different models

ANTI-PATTERNS TO AVOID:

- Assuming all ML infrastructure is GPU clusters
- Ignoring CPU infrastructure for recommendation
- One-size-fits-all datacenter design
- Only discussing training infrastructure (serving matters too)

================================================================================
-->

# Large-Scale ML Infrastructure {#sec-infrastructure}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A sweeping architectural visualization of a modern AI datacenter infrastructure. The scene reveals a massive facility with rows of GPU clusters arranged in pods, connected by high-bandwidth networking fabric depicted as glowing fiber optic pathways. Cooling systems appear as flowing blue currents between server racks. The visualization includes multiple layers: physical infrastructure at the bottom with power and cooling, compute infrastructure in the middle with thousands of interconnected accelerators, and orchestration software at the top represented as an abstract control plane. Visual elements include resource managers allocating workloads, capacity graphs showing utilization, and geographic connections to other datacenters. The color scheme uses industrial grays and silvers with accent colors of electric blue for networking and amber for active computation. Photorealistic technical illustration style suitable for infrastructure engineering documentation._
:::

\noindent
![](images/png/cover_infrastructure.png)

:::

## Purpose {.unnumbered}

_Why does the ability to build and manage computational infrastructure determine which organizations can realize their most ambitious machine learning goals?_

::: {.callout-tip title="Learning Objectives"}

After completing this chapter, you will be able to:

- Calculate power and cooling requirements for GPU cluster deployments using PUE and thermal dissipation constraints
- Compare network topology trade-offs (fat-tree, rail-optimized, torus) using bisection bandwidth and hop count metrics
- Analyze total cost of ownership for infrastructure decisions by computing amortized CapEx plus OpEx
- Design resource management policies that achieve target GPU utilization (70%+) in multi-tenant clusters
- Evaluate accelerator options (GPU, TPU, custom ASIC) by matching workload characteristics to hardware capabilities
- Apply the roofline model to determine whether workloads are compute-bound or memory-bound

:::

Machine learning systems that transform industries operate on infrastructure far exceeding the scale of single machines or small clusters. Training a frontier model may require thousands of GPUs coordinated across multiple datacenters, each machine contributing to a unified computation that can span weeks or months. Managing such infrastructure demands expertise in datacenter design, high-bandwidth networking, and distributed systems orchestration. The infrastructure must satisfy competing requirements: computational efficiency alongside fault tolerance, coordination across thousands of machines without prohibitive communication overhead, and dynamic capacity provisioning that controls costs. These challenges have become central to machine learning advancement, as organizations cannot access frontier capabilities without mastering the physical and software systems that make large-scale computation possible. Understanding infrastructure architecture is essential for building systems beyond prototype experiments, shaping whether organizations deploy efficiently, scale reliably, and compete effectively in an increasingly infrastructure-dependent landscape.

## Datacenter Architecture for ML Workloads {#sec-datacenter-architecture}

The transition from single-machine ML systems to distributed training fundamentally changes the infrastructure requirements. Single-node optimization focuses on GPU utilization, memory bandwidth, and efficient data loading. Production ML at scale demands something different: purpose-built datacenters designed around the unique characteristics of ML workloads, including massive power consumption, extreme heat density, and communication patterns that differ markedly from traditional cloud computing.

This section examines the physical and compute infrastructure that enables large-scale ML, providing the foundation for understanding how distributed training systems leverage these resources.

### Physical Infrastructure Fundamentals

ML datacenters differ from traditional cloud facilities in three critical dimensions: power density per rack is 5-10x higher, cooling requirements demand liquid rather than air-based solutions, and physical layout must optimize for high-bandwidth interconnects rather than flexible networking.

#### Power Delivery and Distribution

A single NVIDIA DGX H100 system consumes 10.2 kW at peak load. A rack containing four such systems requires over 40 kW, compared to 5-10 kW for traditional server racks. This power density[^fn-power-density] fundamentally changes power infrastructure design.

[^fn-power-density]: **Power density**: The power consumption per unit of datacenter floor space, typically measured in kW per rack or kW per square meter. Traditional enterprise datacenters design for 5-10 kW per rack. Modern GPU clusters require 40-100+ kW per rack, demanding specialized power distribution and cooling infrastructure. Power density directly limits how many GPUs can be deployed in existing facilities.

**Utility and Backup Power.** Production ML facilities require redundant power feeds, typically N+1 or 2N configurations[^fn-redundancy] where N represents the load requirement. Uninterruptible power supplies (UPS) bridge the gap during utility failures, but the massive power draw of GPU clusters limits battery backup duration to minutes rather than hours. Diesel generators provide extended backup, with automatic transfer switches completing failover within 10-15 seconds.

[^fn-redundancy]: **N+1 and 2N redundancy**: Power redundancy configurations where N is the capacity required to serve the load. N+1 provides one backup component (e.g., 3 UPS units where 2 are required), allowing single-failure tolerance. 2N provides complete duplication (e.g., 4 UPS units), allowing maintenance on one path while maintaining failure protection on the other. ML training runs spanning weeks justify 2N redundancy to prevent catastrophic job loss.

**Power Distribution Architecture.** Modern ML datacenters use a tiered distribution model:

+-----------------------------+---------------------+-------------------------+
| **Distribution Level**      | **Typical Voltage** | **Purpose**             |
+:============================+====================:+:========================+
| **Utility feed**            | 13.8-69 kV          | Grid connection         |
+-----------------------------+---------------------+-------------------------+
| **Substation transformer**  | 480V (US)           | Building distribution   |
+-----------------------------+---------------------+-------------------------+
| **PDU (Power Distribution** | 208V                | Rack-level distribution |
| **Unit)**                   |                     |                         |
+-----------------------------+---------------------+-------------------------+
| **Server PSU**              | 12V DC              | Component-level power   |
+-----------------------------+---------------------+-------------------------+

**Power Usage Effectiveness.** The PUE metric[^fn-pue], developed by The Green Grid consortium in 2007 [@thegreengrid2007pue], quantifies datacenter energy efficiency:

[^fn-pue]: **Power Usage Effectiveness (PUE)**: An industry-standard metric where values closer to 1.0 indicate greater efficiency. A PUE of 2.0 means half the power goes to overhead (cooling, lighting, power distribution), while 1.1 means only 10% goes to overhead. Google's most efficient datacenters achieve PUE of 1.06, while typical enterprise facilities operate at 1.5-2.0.

$$
\text{PUE} = \frac{\text{Total Facility Power}}{\text{IT Equipment Power}}
$$

A PUE of 1.0 represents perfect efficiency where all power goes to computing. Traditional datacenters achieve PUE values of 1.5-2.0, while hyperscale facilities target 1.1-1.2. ML datacenters face a challenge: the extreme heat density of GPU clusters increases cooling overhead, pushing PUE higher unless advanced cooling technologies are deployed.

#### Cooling Systems at Scale

Heat dissipation represents the primary constraint on ML cluster density. An H100 GPU generates 700W of thermal output from a surface area smaller than a dinner plate, producing heat flux comparable to a nuclear reactor's fuel rod surface.

**Air Cooling Limitations.** Traditional air cooling becomes impractical above 30-40 kW per rack. The physics are straightforward: air's low heat capacity (approximately 1 kJ/kg-K) requires massive airflow to remove heat. A 40 kW rack requires roughly 10,000 CFM (cubic feet per minute) of airflow, creating acoustic levels exceeding 80 dB and significant fan power overhead.

**Hot Aisle/Cold Aisle Containment.** This architectural pattern separates cold supply air from hot exhaust air using physical barriers. Cold air enters through raised floor vents or overhead ducts, passes through servers front-to-back, and exhausts into a contained hot aisle. Containment improves cooling efficiency by preventing mixing, but cannot solve the fundamental heat density challenge of modern GPU clusters.

::: {.callout-note title="Figure Placeholder: Cooling Topology" collapse="true"}
```{.tikz}
% TODO: Diagram showing hot/cold aisle airflow vs liquid cooling loops
\node[draw, align=center] {Datacenter Cooling\nAir vs Liquid};
```
**Datacenter Cooling Architectures**: Comparison of traditional hot-aisle/cold-aisle air cooling versus modern direct-to-chip liquid cooling. The air cooling diagram shows airflow management separating intake and exhaust streams, while the liquid cooling diagram illustrates coolant loops directly contacting high-power components to manage extreme heat density.
:::

**Direct-to-Chip Liquid Cooling.** Liquid cooling addresses heat density through water's superior heat capacity (4.2 kJ/kg-K, roughly four times air). Cold plates mounted directly on GPUs and CPUs transfer heat to circulating coolant, which flows to facility-level heat exchangers. This approach enables rack densities of 100+ kW while reducing cooling power consumption by 30-40% compared to air cooling.

::: {.callout-note}
## Liquid Cooling Adoption

As of 2024, liquid cooling has transitioned from specialty option to requirement for large-scale ML clusters. NVIDIA's GB200 NVL72 systems require liquid cooling, with no air-cooled option available. Facilities planning for next-generation hardware must include liquid cooling infrastructure from initial design.
:::

**Immersion Cooling.** The most aggressive thermal solution submerges entire servers in dielectric fluid. Single-phase immersion uses non-conductive oils that remain liquid, while two-phase systems use fluids that boil at low temperatures, leveraging latent heat of vaporization for efficient heat transfer. Immersion enables rack densities exceeding 200 kW but requires specialized maintenance procedures and component compatibility.

#### Physical Layout Optimization

ML cluster performance depends critically on physical topology. Unlike web serving workloads where any server can handle any request, distributed training requires specific communication patterns between specific nodes.

**Rack Density Considerations.** Higher density reduces cable lengths and switch hops but concentrates power and cooling requirements. Production deployments balance these factors based on workload characteristics:

+---------------------+---------------------+---------------------+
| **Workload Type**   | **Typical Density** | **Limiting Factor** |
+:====================+====================:+:====================+
| **LLM training**    | 80-120 kW/rack      | Cooling capacity    |
+---------------------+---------------------+---------------------+
| **Recommendation**  | 30-50 kW/rack       | CPU/memory balance  |
| **inference**       |                     |                     |
+---------------------+---------------------+---------------------+
| **Vision training** | 60-80 kW/rack       | Network bandwidth   |
+---------------------+---------------------+---------------------+

**Cable Management.** High-bandwidth interconnects like InfiniBand use copper cables for distances under 3 meters and fiber optics beyond. Cable routing must maintain bend radius requirements (typically 10x cable diameter) while enabling airflow for any air-cooled components. Active optical cables (AOCs) simplify routing but add latency and power consumption compared to passive copper.

### Compute Infrastructure Design

ML clusters combine multiple node types, each optimized for different phases of the training and inference pipeline. Understanding these roles clarifies infrastructure design decisions.

#### GPU Cluster Architectures

Modern GPU clusters are built from dense multi-GPU nodes connected via high-bandwidth fabrics. Two reference architectures dominate production deployments.

**DGX-Style Dense Nodes.** NVIDIA's DGX systems package 8 GPUs with NVLink[^fn-nvlink] interconnects, high-bandwidth networking, and substantial local storage in a single chassis. The DGX H100 provides:

[^fn-nvlink]: **NVLink**: NVIDIA's proprietary high-bandwidth interconnect for GPU-to-GPU communication, providing 900 GB/s bidirectional bandwidth in NVLink 4.0 (H100), compared to 64 GB/s for PCIe Gen5. NVLink enables efficient tensor parallelism by allowing GPUs to share memory across the interconnect with near-local-memory latency.

- 8x H100 GPUs with 640GB total HBM3[^fn-hbm] memory

[^fn-hbm]: **High Bandwidth Memory (HBM)**: A 3D-stacked DRAM technology that places memory dies vertically atop the GPU die, connected via thousands of through-silicon vias (TSVs). HBM3 provides 3.4 TB/s bandwidth per GPU compared to 400 GB/s for DDR5. This bandwidth is essential for memory-bound ML workloads where data movement, not computation, limits performance.
- NVSwitch fabric enabling 900 GB/s GPU-to-GPU bandwidth
- 8x 400 Gbps InfiniBand or Ethernet ports
- 2x Intel Xeon CPUs for preprocessing
- 30TB NVMe storage for dataset staging

This integrated design simplifies deployment but limits flexibility. Each DGX H100 costs approximately $300,000 (pricing reflects 2024 market conditions and fluctuates significantly based on supply, demand, and generation transitions), making component-level upgrades economically impractical.

**HGX Baseboard Designs.** For organizations building custom infrastructure, NVIDIA's HGX baseboards provide the GPU and interconnect components for integration into custom server designs. Cloud providers and large enterprises use HGX to optimize for their specific power, cooling, and networking requirements while maintaining compatibility with NVIDIA's software stack.

**PCIe vs. NVLink Configurations.** The choice between PCIe and NVLink connectivity involves fundamental trade-offs:

::: {.callout-note title="Figure Placeholder: Interconnect Topology" collapse="true"}
```{.tikz}
% TODO: Diagram contrasting PCIe tree vs NVLink mesh topology
\node[draw, align=center] {Interconnect Topology\nPCIe Tree vs NVLink Mesh};
```
**Node Interconnect Topologies**: Contrast between PCIe-based commodity servers and NVLink-based dense nodes. The PCIe topology shows CPU-centric communication with higher latency and shared bandwidth bottlenecks, while the NVLink topology demonstrates a high-bandwidth mesh allowing direct peer-to-peer GPU communication essential for tensor parallelism.
:::

+---------------------+---------------+------------------+----------------------+
| **Interconnect**    | **Bandwidth** | **Latency**      | **Use Case**         |
+:====================+==============:+=================:+:=====================+
| **PCIe Gen5 x16**   | 64 GB/s       | ~1 microsecond   | Inference, small     |
|                     |               |                  | models               |
+---------------------+---------------+------------------+----------------------+
| **NVLink 4.0**      | 900 GB/s      | ~0.5 microsecond | Large model training |
| **(bidirectional)** |               |                  |                      |
+---------------------+---------------+------------------+----------------------+

For models requiring tensor parallelism across GPUs (as detailed in @sec-distributed-training), NVLink's 14x bandwidth advantage directly translates to training throughput. PCIe-based systems suffice for data-parallel workloads where gradient synchronization occurs less frequently.

#### CPU Infrastructure Roles

While GPUs dominate ML computation, CPUs perform essential supporting functions that bottleneck overall system performance if under-provisioned.

**Preprocessing and Data Preparation.** Training data pipelines involve decompression, augmentation, tokenization, and batching. These operations execute on CPUs, which must supply data fast enough to keep GPUs utilized. A common rule of thumb allocates 4-8 CPU cores per GPU for training workloads, though data-intensive pipelines (video, large images) may require more.

**Feature Serving for Recommendation Systems.** Recommendation models present a distinct infrastructure pattern. These systems combine deep learning components with massive embedding tables that may exceed 1TB. The embedding lookups are memory-bound CPU operations, while neural network components benefit from GPU acceleration. Production recommendation systems often use CPU-heavy nodes for embedding serving alongside GPU nodes for model computation, connected via low-latency networks.

**Control Plane and Orchestration.** Cluster management, job scheduling, and monitoring run on dedicated CPU nodes separate from the training cluster. This isolation prevents resource contention and enables management operations even when the training cluster is fully utilized.

#### Hybrid Architectures

Real production systems rarely use homogeneous hardware throughout. Workload-aware placement matches job characteristics to appropriate resources.

**Embedding Table Placement.** For recommendation systems with embedding tables exceeding GPU memory, hybrid architectures place embeddings in CPU DRAM while compute-intensive layers execute on GPUs. Facebook's DLRM architecture pioneered this pattern, with embeddings distributed across CPU nodes communicating with GPU nodes via high-bandwidth networks.

**Heterogeneous Scheduling.** Modern orchestration systems support mixed node types within a single cluster. Kubernetes with GPU support and Slurm with GRES (Generic Resource Scheduling) enable jobs to request specific hardware combinations. A training job might request 64 GPU nodes for model computation plus 16 high-memory CPU nodes for embedding tables, scheduled as a coordinated allocation.

### Accelerator Selection by Workload Type

The accelerator landscape has expanded beyond NVIDIA GPUs to include Google TPUs, custom ASICs, and emerging architectures. Selection requires matching accelerator characteristics to workload requirements.

#### NVIDIA GPU Ecosystem

NVIDIA maintains market dominance through integrated hardware-software offerings. Understanding the architecture evolution clarifies capability differences.

**Architecture Progression.** Each generation brings substantial improvements in compute density and memory bandwidth:

+-----------+------------+--------------+---------------+---------+
| **GPU**   | **FP16**   | **HBM**      | **Memory**    | **TDP** |
|           | **Tensor** | **Capacity** | **Bandwidth** |         |
|           | **TFLOPS** |              |               |         |
+==========:+===========:+=============:+==============:+========:+
| **A100**  | 312        | 80 GB HBM2e  | 2.0 TB/s      | 400W    |
+-----------+------------+--------------+---------------+---------+
| **H100**  | 990        | 80 GB HBM3   | 3.4 TB/s      | 700W    |
+-----------+------------+--------------+---------------+---------+
| **B100*** | ~1,800     | 192 GB HBM3e | 8.0 TB/s      | ~700W   |
+-----------+------------+--------------+---------------+---------+

*B100 specifications are preliminary estimates based on NVIDIA announcements. Verify against official specifications for production planning.

::: {.callout-note title="Numerics: The Case for Bfloat16"}
Deep learning training is resilient to low precision but sensitive to dynamic range.
Standard FP16 (IEEE 754) uses 5 bits for the exponent and 10 bits for the mantissa. This limited exponent range often causes gradients to underflow to zero or overflow to infinity during training, requiring complex loss scaling techniques to maintain stability.

**Bfloat16 (Brain Floating Point)**, developed by Google, reallocates bits: 8 bits for the exponent (matching FP32) and 7 bits for the mantissa.
- **Advantage**: Matches the dynamic range of FP32, meaning numbers that fit in FP32 also fit in BF16. This eliminates the need for loss scaling and stabilizes training for large transformers.
- **Trade-off**: Lower precision (fewer mantissa bits), but neural networks typically don't need high precision for individual weights, only for the aggregate accumulation (which is usually done in FP32).
Virtually all modern large language models (GPT-4, PaLM, Llama) are trained using BF16.
:::

The H100 delivers approximately 3x the tensor TFLOPS of A100 at 1.75x the power. To derive the efficiency improvement: A100 achieves 312 TF / 400W = 0.78 TF/W, while H100 achieves 990 TF / 700W = 1.41 TF/W, yielding approximately 80% improvement in FLOPS/watt. Memory bandwidth increases proportionally, maintaining the compute-to-memory ratio critical for transformer models.

**Tensor Core Utilization.** Tensor Cores[^fn-tensor-cores] accelerate matrix operations but require specific data layouts and sizes for full utilization. Dimensions should be multiples of 8 (FP16) or 16 (INT8) for optimal performance. Underutilized Tensor Cores represent the most common source of poor GPU efficiency in production, with many workloads achieving only 30-50% of theoretical peak FLOPS.

[^fn-tensor-cores]: **Tensor Cores**: Specialized matrix-multiply-accumulate units introduced in NVIDIA Volta (2017) that perform 4x4 matrix operations in a single clock cycle. Unlike general-purpose CUDA cores, Tensor Cores are optimized for the fused multiply-add pattern $D = A \times B + C$ that dominates neural network computation. H100 Tensor Cores support FP8, FP16, BF16, TF32, and INT8 formats.

**NVLink Topology.** Within a node, NVSwitch provides full-bandwidth connectivity between all GPUs. Across nodes, NVLink Network (available in H100 and later) extends high-bandwidth connectivity, though at reduced bandwidth compared to intra-node links. Topology-aware job placement, discussed in @sec-communication, is essential for multi-node training performance.

#### Google TPU Infrastructure

Google's Tensor Processing Units offer an alternative architecture optimized for matrix operations with a distinct programming model.

**TPU Pod Architecture.** TPUs connect via proprietary Inter-Chip Interconnect (ICI)[^fn-ici] forming 2D or 3D torus[^fn-torus] topologies. A TPU v4 pod contains 4,096 chips with 1.1 exaFLOPS of aggregate compute. Unlike GPU clusters where networking is separate from compute nodes, TPU pods integrate interconnect into the chip design.

#### The Systolic Array Advantage

The defining feature of the TPU architecture is the **systolic array**. Unlike CPUs or GPUs that function as general-purpose instruction processors, a systolic array is a specialized grid of arithmetic units designed for massive matrix multiplication.

*   **Data Flow**: The name "systolic" refers to the way data flows through the chip in rhythmic waves, analogous to blood pumped by a heart.
*   **Weight Stationary**: In a matrix multiply $C = A \times B$, weights from matrix $B$ are loaded into the array and held stationary in local registers. Data from matrix $A$ flows in from the left, and partial sums flow down.
*   **Energy Efficiency**: This design drastically reduces energy consumption. In a standard architecture, every operation requires reading operands from registers or memory (high energy cost). In a systolic array, operands are passed directly to the next neighbor unit (very low energy cost). A single memory access effectively amortizes over hundreds of operations.

This architecture explains why TPUs achieve extremely high FLOPS/watt for dense matrix operations but struggle with sparse or irregular computations that break the rhythmic data flow.

[^fn-ici]: **Inter-Chip Interconnect (ICI)**: Google's proprietary chip-to-chip communication fabric integrated directly into TPU silicon. ICI provides 6 links per chip at 100 GB/s each, enabling the torus topology without external switches. This integration reduces latency and power compared to discrete networking but limits flexibility in topology configuration.

[^fn-torus]: **Torus topology**: A network topology where nodes connect in a ring structure along each dimension, with the last node wrapping around to connect to the first. A 3D torus creates a cube-like structure where each node connects to 6 neighbors. This topology provides consistent latency for nearest-neighbor communication patterns common in model parallelism, though AllReduce operations require O(sqrt(N)) hops compared to O(log(N)) for fat-tree.

**TPU Slices and Multislice.** Users allocate TPU slices, contiguous subsets of a pod. Multislice training connects multiple slices via datacenter network for jobs exceeding single-slice capacity. The programming model (JAX with pjit) abstracts the physical topology, enabling code portability across slice sizes.

**TPU vs. GPU Trade-offs.** TPUs excel for large-scale training with regular computation patterns:

+------------------------+-------------------------+--------------------------+
| **Factor**             | **TPU Advantage**       | **GPU Advantage**        |
+:=======================+:========================+:=========================+
| **Large transformer**  | Optimized matrix units, | Broader operator support |
| **training**           | integrated interconnect |                          |
+------------------------+-------------------------+--------------------------+
| **Custom operations**  | Limited flexibility     | CUDA extensibility       |
+------------------------+-------------------------+--------------------------+
| **Software ecosystem** | JAX-centric             | PyTorch, TensorFlow,     |
|                        |                         | many frameworks          |
+------------------------+-------------------------+--------------------------+
| **Availability**       | Google Cloud only       | Multiple cloud and       |
|                        |                         | on-premise options       |
+------------------------+-------------------------+--------------------------+

#### Custom ASICs and Specialized Accelerators

The ML accelerator landscape continues to diversify as organizations optimize for specific workloads.

**Inference-Optimized Accelerators.** Training and inference present different requirements. Training needs high-precision arithmetic, large memory for activations and optimizer state, and high interconnect bandwidth. Inference prioritizes low latency, high throughput, and power efficiency. Accelerators like Google's TPU Inference chips and AWS Inferentia optimize for inference characteristics, achieving 2-4x better performance per watt than training-focused hardware for appropriate workloads.

**Emerging Architectures.** Several companies offer alternative approaches:

- **Cerebras WSE**: Wafer-scale integration[^fn-wafer-scale] places an entire ML accelerator on a single silicon wafer, eliminating chip-to-chip communication for models that fit on-chip
- **Graphcore IPU**: Bulk Synchronous Parallel (BSP)[^fn-bsp] execution model with distributed on-chip memory targeting sparse and dynamic workloads

[^fn-wafer-scale]: **Wafer-scale integration**: Building an entire processor on a full silicon wafer (850 cm^2^) rather than dicing it into individual chips. Cerebras's WSE-2 contains 850,000 cores and 40 GB of on-chip SRAM, eliminating off-chip memory bandwidth bottlenecks. The approach requires novel solutions for defect tolerance and power delivery, as traditional packaging assumes perfect dies.

[^fn-bsp]: **Bulk Synchronous Parallel (BSP)**: An execution model where computation proceeds in supersteps: parallel computation, global communication, then barrier synchronization. Graphcore's IPU implements BSP at the hardware level, with all 1,472 cores executing the same phase simultaneously. This deterministic execution simplifies debugging but requires all operations to complete within time bounds.
- **SambaNova**: Reconfigurable dataflow architecture for enterprise AI applications

These alternatives find niches where their architectural trade-offs align with workload requirements, though NVIDIA and Google maintain dominant market positions for general ML training.

### Quantitative Infrastructure Analysis

Effective infrastructure decisions require quantitative comparison across accelerator options and workload types.

**FLOPS per Watt Comparison.** Energy efficiency varies significantly across accelerator types and precision levels:

+----------------------+-----------------+-----------------+-----------------+
| **Accelerator**      | **FP16 TFLOPS** | **TDP (Watts)** | **TFLOPS/Watt** |
+=====================:+================:+================:+================:+
| **NVIDIA H100 SXM**  | 990             | 700             | 1.41            |
+----------------------+-----------------+-----------------+-----------------+
| **NVIDIA H100 PCIe** | 756             | 350             | 2.16            |
+----------------------+-----------------+-----------------+-----------------+
| **Google TPU v5p**   | 459             | 250-400*        | 1.1-1.8         |
+----------------------+-----------------+-----------------+-----------------+
| **AWS Trainium**     | 210             | 150 (estimated) | 1.40            |
+----------------------+-----------------+-----------------+-----------------+

*TPU power varies significantly by deployment configuration and is not officially published. Direct TFLOPS/Watt comparisons across architectures are problematic because utilization profiles differ. These figures should be treated as approximate.

The PCIe variant's higher efficiency reflects reduced interconnect power, acceptable for inference but limiting for distributed training.

**Memory Bandwidth Utilization.** Different model types exhibit distinct memory access patterns:

- **LLM training**: Memory-bound for attention computation, achieving 70-85% bandwidth utilization
- **CNN training**: Compute-bound for convolutions, 30-50% bandwidth utilization
- **Recommendation inference**: Memory-bound for embeddings, often exceeding available bandwidth

Understanding these patterns guides accelerator selection: memory-bound workloads benefit from HBM3's bandwidth improvements, while compute-bound workloads prioritize FLOPS per dollar.

**The Roofline Model.** The roofline model[^fn-roofline] [@williams2009roofline] provides a systematic framework for understanding whether workloads are compute-bound or memory-bound. Achievable performance is limited by the minimum of peak compute and memory bandwidth:

::: {.callout-note title="Figure Placeholder: Roofline Model" collapse="true"}
```{.tikz}
% TODO: Log-log plot of FLOPS vs Arithmetic Intensity
\node[draw, align=center] {Roofline Model\nMemory vs Compute Bound};
```
**Roofline Performance Model**: A visual representation of performance limits plotting achievable FLOPS against arithmetic intensity (FLOPS/Byte). The slanted "roof" represents the memory-bound region where bandwidth constrains performance, while the flat "roof" represents the compute-bound region limited by peak processor throughput. Operational points for LLMs typically fall under the slanted roof, indicating memory bandwidth dependence.
:::

[^fn-roofline]: **Roofline model**: A visual performance model developed at Berkeley that plots achievable FLOPS against arithmetic intensity (FLOPS per byte of memory traffic). The "roofline" consists of a sloped region (memory-bound, limited by bandwidth) and a flat region (compute-bound, limited by peak FLOPS). The intersection point, called the ridge point, indicates where workloads transition between these regimes. This model helps identify whether to optimize for compute or memory access.

$$
\text{Achievable FLOPS} = \min\left(\text{Peak Compute}, \text{Memory Bandwidth} \times \text{Arithmetic Intensity}\right)
$$

Arithmetic intensity measures FLOPS per byte of memory traffic. The "ridge point" where compute and memory limits intersect determines which workloads benefit from each resource:

+-----------------+------------------+---------------+-----------------+
| **Accelerator** | **Peak Compute** | **Memory BW** | **Ridge Point** |
|                 | **(TF FP16)**    | **(TB/s)**    | **(FLOP/byte)** |
+================:+=================:+==============:+================:+
| **H100 SXM**    | 990              | 3.4           | 291             |
+-----------------+------------------+---------------+-----------------+
| **A100 80GB**   | 312              | 2.0           | 156             |
+-----------------+------------------+---------------+-----------------+
| **TPU v4**      | 275              | 1.2           | 229             |
+-----------------+------------------+---------------+-----------------+

Most LLM training operates at 50-100 FLOP/byte arithmetic intensity, well below the ridge point, making these workloads memory-bound. At 75 FLOP/byte on H100, achievable performance is $3.4 \times 75 = 255$ TF, only 26% of peak compute. This explains why production training achieves 30-50% of theoretical FLOPS: the bottleneck is memory bandwidth, not compute capacity.

CNN training with large batch sizes operates near 200 FLOP/byte, approaching the ridge point where both resources limit performance. Recommendation inference with random embedding lookups operates at extremely low arithmetic intensity (1-10 FLOP/byte), fundamentally memory-bound regardless of accelerator choice.

**Cost per PFLOP.** Infrastructure economics depend on utilization and workload fit:

$$
\text{Effective Cost per PFLOP} = \frac{\text{Hardware Cost} + \text{3-year OpEx}}{\text{Peak PFLOPS} \times \text{Average Utilization} \times 3 \text{ years}}
$$

For a DGX H100 at $300,000 with $50,000 annual power and cooling costs, achieving 50% average utilization yields an effective cost of approximately $0.35 per PFLOP-hour. Cloud instances at $30 per hour for equivalent hardware cost $0.15 per PFLOP-hour at 100% utilization, but on-premise becomes favorable above 40% sustained utilization over three years.

::: {.callout-warning}
## Utilization Reality

Quoted peak FLOPS numbers assume perfect utilization. Production training jobs typically achieve 30-50% of peak due to communication overhead, data pipeline stalls, and suboptimal kernel efficiency. Infrastructure planning must account for realistic utilization rates rather than theoretical peaks.
:::

This infrastructure foundation enables the distributed training strategies explored in @sec-distributed-training and the communication patterns detailed in @sec-communication. The physical constraints examined here, particularly power delivery, cooling capacity, and interconnect topology, ultimately determine what scale of training is achievable.

## Networking for Large-Scale ML {#sec-networking-ml}

The networking fabric connecting accelerators determines whether a distributed training job achieves near-linear scaling or collapses into communication-bound inefficiency. While the previous section examined intra-node connectivity through NVLink and NVSwitch, this section extends to the datacenter-scale networks that enable training across hundreds or thousands of accelerators. The transition from intra-node to inter-node communication introduces fundamentally different constraints: where NVLink provides 900 GB/s between GPUs on a single baseboard, inter-node networks must traverse switches, cables, and protocol stacks that introduce both bandwidth limitations and latency penalties.

### Training Network Requirements

Large-scale training workloads impose unique demands on network infrastructure. Unlike traditional datacenter traffic patterns dominated by short flows and request-response interactions, distributed training generates sustained, synchronized bulk transfers. A single AllReduce operation across 1024 GPUs may move terabytes of gradient data, with all participants blocked until the collective completes. This pattern demands networks optimized for bandwidth rather than connection establishment latency.

#### High-Bandwidth Interconnects

InfiniBand[^fn-infiniband] has emerged as the dominant interconnect for ML training clusters due to its RDMA (Remote Direct Memory Access)[^fn-rdma] capabilities and consistent low latency [@infiniband2000spec]. The technology enables direct memory-to-memory transfers without CPU involvement, reducing both latency and processor overhead.

[^fn-infiniband]: **InfiniBand**: A high-speed interconnect standard originally developed for HPC, now dominant in ML clusters. Unlike Ethernet, InfiniBand provides guaranteed delivery with hardware-based reliability, eliminating software retransmission overhead. Current NDR (Next Data Rate) generation provides 400 Gb/s per port, with XDR (800 Gb/s) emerging.

[^fn-rdma]: **Remote Direct Memory Access (RDMA)**: A network capability that allows one machine to read from or write to another machine's memory without involving either CPU. RDMA bypasses the kernel network stack, reducing latency from tens of microseconds to sub-microsecond. For ML, RDMA enables GPUs to exchange gradient data with minimal CPU intervention, critical for overlapping communication with computation.

+-----------------------+---------------+-------------+-----------------------+
| **Generation**        | **Bandwidth** | **Latency** | **Common Deployment** |
+======================:+==============:+============:+:======================+
| **HDR (200 Gb/s)**    | 25 GB/s       | 0.6 μs      | Legacy clusters       |
+-----------------------+---------------+-------------+-----------------------+
| **HDR100 (100 Gb/s)** | 12.5 GB/s     | 0.6 μs      | Cost-optimized        |
+-----------------------+---------------+-------------+-----------------------+
| **NDR (400 Gb/s)**    | 50 GB/s       | 0.5 μs      | Current standard      |
+-----------------------+---------------+-------------+-----------------------+
| **NDR200 (200 Gb/s)** | 25 GB/s       | 0.5 μs      | Disaggregated         |
+-----------------------+---------------+-------------+-----------------------+
| **XDR (800 Gb/s)**    | 100 GB/s      | &lt; 0.5 μs | Emerging              |
+-----------------------+---------------+-------------+-----------------------+

RoCE (RDMA over Converged Ethernet)[^fn-roce] provides an alternative that leverages existing Ethernet infrastructure. RoCEv2 operates over UDP/IP, enabling RDMA semantics across routed networks. While RoCE offers lower capital costs and operational familiarity, it requires careful configuration of Priority Flow Control (PFC) and Explicit Congestion Notification (ECN) to prevent packet loss. In ML workloads, even small packet loss rates cause significant performance degradation because collective operations must wait for retransmissions.

[^fn-roce]: **RoCE (RDMA over Converged Ethernet)**: A protocol that implements RDMA semantics over Ethernet networks. RoCEv2 uses UDP encapsulation for routability across L3 networks. RoCE reduces infrastructure costs by using commodity Ethernet switches but requires lossless Ethernet configuration (PFC/ECN) to achieve RDMA performance. Cloud providers typically offer RoCE-based networking (AWS EFA, Azure NDR) as a lower-cost alternative to InfiniBand.

The choice between InfiniBand and RoCE involves trade-offs beyond raw performance:

$$
\text{Effective Bandwidth} = \text{Link Rate} \times (1 - \text{Loss Rate}) \times \text{Protocol Efficiency}
$$

InfiniBand achieves protocol efficiencies above 95%, while RoCE typically operates at 85-92% depending on network congestion and flow control configuration. For a 400 Gb/s link, this difference translates to 47.5 GB/s versus 42.5 GB/s effective throughput, a gap that compounds across thousands of collective operations per training step.

Network interface cards for ML workloads increasingly integrate compute capabilities. NVIDIA's ConnectX-7 adapters include programmable engines for in-network aggregation, enabling switch-based gradient reduction that reduces traffic volumes. These SmartNICs offload collective operations from the GPU, overlapping communication with computation more effectively than software-only approaches.

#### Network Topology Design

The physical arrangement of switches and links fundamentally constrains distributed training performance. Fat-tree topologies[^fn-fat-tree] [@leiserson1985fattrees], derived from Clos network theory, provide full bisection bandwidth[^fn-bisection-bw]: any partition of the network can communicate at full link rate with the other half. For AllReduce operations that require all-to-all communication patterns, this property ensures no bottlenecks regardless of job placement.

[^fn-fat-tree]: **Fat-tree topology**: A hierarchical network structure where bandwidth increases toward the root, resembling a tree with thicker branches near the top. Unlike traditional trees where aggregation creates bottlenecks, fat-trees use multiple uplinks per switch to maintain bandwidth at each tier. The name comes from Charles Leiserson's 1985 paper that applied this concept to parallel computing.

[^fn-bisection-bw]: **Bisection bandwidth**: The minimum bandwidth available when a network is divided into two equal halves. If bisection bandwidth equals the sum of all edge bandwidths, the network is non-blocking, meaning any communication pattern can proceed at full speed. This metric predicts worst-case performance for collective operations like AllReduce that involve all-to-all communication.

A three-tier fat-tree with radix-64 switches supports 65,536 endpoints while maintaining non-blocking connectivity. The bandwidth at each tier equals:

$$
B_{\text{tier}} = \frac{k}{2} \times B_{\text{link}} \times N_{\text{switches}}
$$

where $k$ is the switch radix and $N_{\text{switches}}$ is the number of switches at that tier. For NDR InfiniBand with 64-port switches, each spine switch contributes 1.6 TB/s of bisection bandwidth.

Rail-optimized topologies offer an alternative for workloads dominated by tensor parallelism. In these designs, GPUs at the same position across multiple nodes connect through dedicated "rails" with minimal switch hops. An 8-rail design connects GPU 0 from each of 32 nodes through a single leaf switch, enabling efficient pipeline parallelism where activations flow between corresponding GPUs across nodes. This approach sacrifices the flexibility of fat-tree networks for reduced latency on structured communication patterns.

+-----------------------+------------------+--------------------+-------------------+
| **Topology**          | **Bisection BW** | **Latency (hops)** | **Best Workload** |
+:======================+:=================+===================:+:==================+
| **Fat-tree (3-tier)** | Full             | 4-6                | General/AllReduce |
+-----------------------+------------------+--------------------+-------------------+
| **Rail-optimized**    | Rail-limited     | 2                  | Tensor parallel   |
+-----------------------+------------------+--------------------+-------------------+
| **Dragonfly**         | Variable         | 3-5                | Large scale       |
+-----------------------+------------------+--------------------+-------------------+
| **Torus (TPU)**       | Dimension-based  | O(√N)              | Structured comms  |
+-----------------------+------------------+--------------------+-------------------+

The distinction between non-blocking and oversubscribed networks carries significant implications for ML workloads. A 2:1 oversubscription ratio halves the effective bisection bandwidth, potentially doubling AllReduce time for large collectives. While oversubscription reduces infrastructure costs, the impact on training throughput often negates the savings. Most production ML clusters deploy non-blocking networks for training, reserving oversubscribed designs for serving traffic where request-response patterns tolerate contention.

#### Multi-Rack and Multi-Datacenter Training

Scaling beyond a single rack introduces inter-rack connectivity as a potential bottleneck. Even with non-blocking leaf-spine architectures, cable lengths increase from meters to tens of meters, adding propagation delay. More significantly, the spine layer becomes a shared resource across all training jobs, requiring careful traffic engineering to prevent interference.

Cross-datacenter training enables access to geographically distributed GPU resources but faces fundamental latency constraints. A 100 km fiber link introduces approximately 0.5 ms round-trip latency from propagation alone, before considering switch processing or protocol overhead. For synchronous training with tight AllReduce coupling, this latency directly extends iteration time:

$$
T_{\text{iteration}} = T_{\text{compute}} + T_{\text{comm}} + T_{\text{latency}} \times N_{\text{rounds}}
$$

Asynchronous methods like Local SGD reduce communication frequency but introduce staleness that affects convergence. Practical cross-datacenter training typically employs hierarchical aggregation: workers synchronize within each datacenter, then datacenters exchange aggregated gradients at lower frequency.

Network partitions present a more severe challenge than performance degradation. When connectivity between datacenters fails, training jobs must either pause (wasting expensive GPU time) or continue with partial gradients (risking divergence). Partition-tolerant training algorithms remain an active research area, with approaches ranging from elastic data parallelism to speculative gradient accumulation. We examine fault tolerance mechanisms in detail in @sec-fault-tolerance.

### Serving Network Requirements

Inference serving demands different network characteristics than training. Where training optimizes for bulk throughput, serving prioritizes latency consistency across diverse request patterns. A recommendation model serving millions of queries per second cannot tolerate the tail latency variance acceptable in batch training.

#### Load Balancer Architectures

ML serving deployments require load balancers that understand model-specific traffic patterns. Layer 4 (L4) load balancers operate on TCP/UDP flows, distributing connections based on IP addresses and ports. They offer high throughput with minimal latency overhead but cannot inspect request content for intelligent routing.

Layer 7 (L7) load balancers parse application protocols, enabling routing decisions based on request characteristics. For ML serving, this enables routing requests to model versions, directing traffic based on input features, or implementing request coalescing for batch inference. The cost is increased latency, typically 0.5-2 ms per hop for TLS termination and HTTP parsing.

Consistent hashing provides session affinity for stateful inference scenarios. When serving autoregressive language models, subsequent tokens in a generation session should route to the same replica to reuse KV cache state. The hash function maps session identifiers to replicas:

$$
\text{replica} = \text{hash}(\text{session\_id}) \mod N_{\text{replicas}}
$$

Virtual nodes improve load distribution when replicas have heterogeneous capacity. Each physical replica appears multiple times in the hash ring proportional to its capacity, naturally directing more traffic to more capable instances.

Geographic load distribution becomes essential for global ML services. DNS-based global load balancing directs users to nearby deployments, reducing round-trip latency. However, model updates must propagate across all regions consistently, requiring coordination between deployment systems and traffic management.

#### Service Mesh for ML

Service mesh architectures insert proxy sidecars alongside ML services, enabling consistent observability and traffic management without application changes. For ML deployments, sidecars capture request latencies, model versions, and input characteristics that feed monitoring and debugging systems.

Traffic routing through service mesh enables sophisticated A/B testing beyond simple traffic splitting. Requests can route based on user segments, input features, or model confidence scores. A recommendation system might route uncertain predictions to an ensemble while serving confident predictions from a faster single model.

Circuit breaker patterns prevent cascade failures when model replicas become unhealthy. When error rates exceed thresholds, the circuit opens and redirects traffic to healthy replicas or fallback models. For ML serving, circuit breakers must account for model-specific health indicators: high latency might indicate GPU memory pressure rather than failure, warranting throttling rather than failover.

### Network Performance Analysis

Quantitative understanding of network performance enables informed decisions about infrastructure investment and training configurations. The interplay between model architecture, parallelism strategy, and network capability determines overall system efficiency.

#### Bandwidth Utilization Patterns

AllReduce[^fn-allreduce] bandwidth requirements scale with model size and parallelism configuration. For data parallelism with $N$ workers and model parameters $P$, each worker must send and receive approximately $2P$ bytes per iteration (assuming ring AllReduce [@patarasuk2009bandwidth]). The required bandwidth to hide communication behind computation is:

[^fn-allreduce]: **AllReduce**: A collective communication operation that combines data from all participating processes and distributes the result back to all. In ML training, AllReduce averages gradients across all workers. The ring AllReduce algorithm achieves optimal bandwidth utilization of 2(N-1)/N by passing partial sums around a logical ring. See @sec-communication for detailed implementation analysis.

$$
B_{\text{required}} = \frac{2P}{T_{\text{compute}}}
$$

For a 175B parameter model with FP16 gradients (350 GB), achieving 50% compute utilization on hardware with 1 second compute time requires 700 GB/s aggregate bandwidth, far exceeding single-link capacity and motivating sophisticated parallelism strategies.

::: {.callout-warning}
## Practical AllReduce Efficiency

Theoretical bandwidth calculations assume ideal conditions. Production AllReduce operations achieve 60-80% of theoretical bandwidth due to multiple overheads:

- **Startup latency**: Each ring stage incurs 5-20 μs fixed overhead per chunk
- **Memory copy overhead**: CPU-GPU and GPU-NIC transfers add latency
- **Protocol overhead**: NCCL/Gloo software stack processing
- **Network contention**: Shared fabric with concurrent jobs reduces effective bandwidth

Well-tuned systems on dedicated InfiniBand fabric achieve 75-85% efficiency. Many deployments, particularly those using RoCE or shared networks, achieve only 40-60%. Always benchmark actual collective performance rather than relying on theoretical link rates.
:::

NCCL is the standard communication library for NVIDIA GPU clusters.[^fn-nccl]

[^fn-nccl]: **NCCL (NVIDIA Collective Communications Library)**: NVIDIA's optimized library for multi-GPU and multi-node collective operations. NCCL automatically selects algorithms based on topology (ring, tree, or hybrid) and handles GPU-to-GPU communication via NVLink, PCIe, or network. It achieves 85-95% of theoretical bandwidth on well-configured systems, making it the default communication backend for PyTorch and TensorFlow distributed training.

Gradient compression reduces bandwidth requirements at the cost of computation and potential accuracy impact. Top-k sparsification transmits only the largest gradient components, achieving 100-1000x compression ratios for some models. Error feedback mechanisms accumulate untransmitted gradients, maintaining convergence despite aggressive compression:

$$
\tilde{g}_t = \text{TopK}(g_t + e_{t-1}), \quad e_t = g_t + e_{t-1} - \tilde{g}_t
$$

Pipeline parallelism communication patterns differ fundamentally from data parallelism. Rather than bulk AllReduce, pipeline stages exchange activation tensors between adjacent stages. The communication volume depends on activation size rather than parameter count, favoring models with small intermediate representations.

#### Latency Analysis

Network latency accumulates from multiple sources, each contributing to collective operation time:

+-----------------------+---------------------+-----------------------------------+
| **Source**            | **Typical Latency** | **Mitigation**                    |
+:======================+====================:+:==================================+
| **Switch hop**        | 100-400 ns          | Topology optimization             |
+-----------------------+---------------------+-----------------------------------+
| **Cable propagation** | 5 ns/m              | Compact layout                    |
+-----------------------+---------------------+-----------------------------------+
| **NIC processing**    | 1-2 μs              | Hardware offload                  |
+-----------------------+---------------------+-----------------------------------+
| **NCCL software**     | 5-20 μs             | Kernel fusion, persistent kernels |
+-----------------------+---------------------+-----------------------------------+
| **Memory copy**       | Variable            | Zero-copy RDMA                    |
+-----------------------+---------------------+-----------------------------------+

For small messages, latency dominates over bandwidth. The crossover point where bandwidth becomes the limiting factor occurs at:

$$
M_{\text{crossover}} = \text{Latency} \times \text{Bandwidth}
$$

For a system with 5 μs latency and 50 GB/s bandwidth, messages smaller than 250 KB are latency-bound. This motivates message aggregation in collective implementations, batching small gradient tensors to amortize latency overhead.

Congestion control algorithms significantly impact performance under contention. Traditional TCP congestion control, designed for fairness across independent flows, performs poorly for synchronized ML traffic where all flows compete simultaneously. DCQCN (Data Center Quantized Congestion Notification)[^fn-dcqcn] [@zhu2015dcqcn] for RoCE and hardware-based credit flow control for InfiniBand provide faster response to congestion, reducing tail latency. NCCL implements topology-aware algorithms that schedule transfers to minimize contention, exploiting knowledge of collective patterns that general-purpose congestion control lacks. We examine these collective operation implementations in detail in @sec-communication.

[^fn-dcqcn]: **DCQCN (Data Center Quantized Congestion Notification)**: A congestion control algorithm designed for RoCE networks. DCQCN uses ECN (Explicit Congestion Notification) marks from switches to throttle senders before packet loss occurs. Unlike TCP's reactive approach, DCQCN responds in microseconds, critical for the synchronized traffic patterns in distributed training where a single slow flow can delay all workers.

### Case Study: NVIDIA DGX SuperPOD Networking

The DGX SuperPOD architecture illustrates production-scale ML networking design. A SuperPOD combines multiple DGX systems into a unified training cluster, with networking designed to maintain near-linear scaling.

The baseline SuperPOD configuration connects 32 DGX H100 systems (256 GPUs) through a two-tier InfiniBand network. Each DGX H100 node provides eight NVIDIA ConnectX-7 adapters, one per GPU, each delivering 400 Gb/s (NDR) bandwidth. The leaf tier consists of 32 QM9700 switches, each connecting eight GPUs from a single node. The spine tier uses eight QM9700 switches, each connecting to all 32 leaf switches.

This configuration provides 1:1 bandwidth between any GPU pair, enabling efficient AllReduce regardless of job placement. The aggregate bisection bandwidth reaches 51.2 TB/s, supporting concurrent training of multiple large models.

Larger SuperPOD deployments extend to 4096 GPUs across 512 DGX systems. At this scale, three-tier fat-tree topologies maintain non-blocking connectivity while managing cable plant complexity. The network includes 256 leaf switches, 128 spine switches, and 32 super-spine switches, totaling over 50,000 optical connections.

Rail-optimized variants reduce switch count by exploiting structured communication patterns. In tensor-parallel configurations, each GPU primarily communicates with corresponding GPUs in other nodes, a pattern well-served by dedicated rails with single-hop connectivity. The trade-off is reduced flexibility: jobs with different parallelism configurations may experience suboptimal placement.

Network management in SuperPOD deployments integrates with cluster schedulers to enable topology-aware job placement. The Unified Fabric Manager monitors link health, detects failures, and can reroute traffic around failed components. Adaptive routing distributes load across multiple paths, improving utilization when traffic patterns create hotspots.

The SuperPOD design embodies the principles examined throughout this section: high-bandwidth interconnects (NDR InfiniBand), topology optimization (configurable fat-tree or rail), and integration with higher-level systems (NCCL, scheduler). These infrastructure choices directly determine training efficiency, making networking architecture a critical factor in ML system design.

## Resource Management and Scheduling

The datacenter infrastructure and high-speed networks discussed in @sec-datacenter-architecture and @sec-networking-ml provide the physical foundation for large-scale ML. However, translating these resources into productive workloads requires sophisticated scheduling systems that balance utilization, fairness, and job completion time. This section examines the scheduling challenges unique to ML workloads and the systems designed to address them.

ML workloads present scheduling challenges distinct from traditional computing. Training jobs require coordinated access to multiple GPUs, often spanning nodes connected via the InfiniBand fabric discussed in the previous section. Inference workloads demand consistent latency while handling unpredictable traffic patterns. Both compete for the same accelerator resources, creating tension between throughput-oriented batch processing and latency-sensitive serving.

### Why Distributed Scheduling is Hard

Before examining specific schedulers, understanding why cluster scheduling differs fundamentally from single-machine scheduling clarifies the design constraints these systems face.

**Distributed Systems Challenges.** Cluster scheduling is not merely "putting jobs on machines" at larger scale. Several fundamental distributed systems problems make it intrinsically harder:

1. **Partial failures**: A node can fail between allocation and job start. The scheduler may successfully allocate 32 GPUs across 4 nodes, only to have one node fail before the job launches. The remaining 24 GPUs sit idle while the scheduler detects the failure and re-allocates.

2. **Network partitions**: The scheduler may lose connectivity to a subset of nodes while those nodes continue operating. From the scheduler's perspective, the nodes appear failed. From the nodes' perspective, jobs may still be running.

3. **State inconsistency**: Resource state may differ between the scheduler's view and reality on individual nodes. A GPU the scheduler believes is free may still be cleaning up from a previous job.

4. **Ordering without global time**: Without a global clock, determining whether allocation A happened before allocation B across different nodes requires careful protocol design. Two jobs may both believe they "own" the same GPU if the system is not carefully designed.

**CAP Theorem Implications.** The CAP theorem[^fn-cap] applies directly to cluster scheduling: a scheduler cannot simultaneously provide perfect consistency (every allocation reflects true cluster state), availability (every request gets a response), and partition tolerance (system operates despite network failures).

[^fn-cap]: **CAP theorem**: A fundamental distributed systems result proving that no system can simultaneously guarantee Consistency (all nodes see the same data), Availability (every request receives a response), and Partition tolerance (the system operates despite network failures). First proven by Eric Brewer in 2000, CAP forces designers to choose which property to sacrifice when partitions occur. Most modern systems choose availability over strict consistency, using eventual consistency models.

Production schedulers make different trade-offs:

- **Slurm** prioritizes consistency, blocking allocations during uncertainty
- **Kubernetes** prioritizes availability, using eventual consistency with reconciliation loops
- **Custom ML schedulers** often accept bounded inconsistency for performance

**Failure Rates at Scale.** At scale, failure is normal operation, not exceptional. With 99.9% annual GPU reliability (typical for datacenter hardware), a 4096-GPU cluster experiences:

$$
\text{Expected failures per day} = 4096 \times \frac{0.001}{365} \approx 0.01 \text{ GPU failures/day}
$$

More realistically, including software failures, driver issues, and thermal events, production clusters see 1-4 failures per day per 1000 GPUs. A multi-week training run on 4096 GPUs will experience multiple failures. Infrastructure and scheduling systems must anticipate this reality.

### Batch Scheduling for Training

Training large models requires allocating substantial GPU resources for extended periods. A 175B parameter model may require 1024 GPUs for weeks, while smaller experiments need only a few GPUs for hours. Scheduling systems must efficiently pack these diverse workloads while respecting resource constraints and fairness policies.

#### Slurm for HPC-Style ML

Slurm[^fn-slurm] (Simple Linux Utility for Resource Management) [@yoo2003slurm] dominates HPC environments and extends naturally to GPU-intensive ML training. Its partition-based architecture maps well to heterogeneous accelerator pools.

[^fn-slurm]: **Slurm**: An open-source job scheduler originating from Lawrence Livermore National Laboratory (2002) that manages compute resources across thousands of nodes. Slurm's design prioritizes predictability and fairness for long-running scientific workloads. Unlike Kubernetes' declarative model, Slurm uses imperative job submission with explicit resource requests, making it easier to reason about allocation guarantees but less flexible for dynamic workloads.

A typical ML cluster configuration defines partitions by accelerator type and interconnect:

+---------------+---------------+------------------+--------------------+
| **Partition** | **GPUs/Node** | **Interconnect** | **Typical Use**    |
+:==============+==============:+:=================+:===================+
| **dgx-a100**  | 8 x A100      | NVLink + IB NDR  | Large LLM training |
+---------------+---------------+------------------+--------------------+
| **a100-pcie** | 4 x A100      | PCIe + IB HDR    | Medium training    |
+---------------+---------------+------------------+--------------------+
| **inference** | 2 x A10G      | Ethernet         | Model serving      |
+---------------+---------------+------------------+--------------------+
| **debug**     | 1 x V100      | Ethernet         | Development        |
+---------------+---------------+------------------+--------------------+

GPU allocation strategies significantly impact utilization. The `--gres=gpu:N`[^fn-gres] flag requests N GPUs, but naive allocation can fragment nodes. Consider a 64-node cluster with 8 GPUs each (512 total). If jobs request 6 GPUs, each job wastes 2 GPUs per node, reducing effective capacity to 75%. Slurm's `SelectTypeParameters=CR_GPU` enables GPU-level scheduling, while `--gpus-per-node` ensures jobs receive full nodes when beneficial for NVLink communication.

[^fn-gres]: **GRES (Generic Resource Scheduling)**: Slurm's mechanism for scheduling non-CPU resources like GPUs, FPGAs, or specialized accelerators. GRES tracks resource availability per node and enforces exclusive allocation. The gres.conf file defines available resources, while jobs request them via `--gres=resource_type:count`. This abstraction allows Slurm to manage diverse accelerator types without code changes.

Fair-share scheduling prevents any single user or project from monopolizing resources. The classic fair-share formula computes effective priority as:

$$P_{effective} = P_{base} \times \frac{F_{target}}{F_{actual} + \epsilon}$$

where $F_{target}$ represents the user's allocated share and $F_{actual}$ their recent usage. This naturally deprioritizes heavy users while allowing burst access when resources are idle.

Preemption policies enable high-priority jobs to reclaim resources from running workloads. For ML training, this requires checkpoint-aware preemption. Jobs receive SIGTERM with configurable grace periods (typically 60-300 seconds) to save checkpoints before SIGKILL. The `PreemptMode=REQUEUE` setting automatically restarts preempted jobs, while `GraceTime` controls the checkpoint window.

#### Kubernetes for ML Workloads

Kubernetes has become the standard platform for ML infrastructure, particularly for organizations requiring unified management of training and serving workloads. Native Kubernetes lacks ML-aware scheduling, but extensions address this gap.

GPU scheduling relies on device plugins that expose accelerators as extended resources. The NVIDIA device plugin registers GPUs with the kubelet, enabling pod specifications like:

```yaml
resources:
  limits:
    nvidia.com/gpu: 4
```

However, this binary allocation model wastes resources when workloads need less than a full GPU. Multi-Instance GPU (MIG)[^fn-mig] technology addresses this by partitioning A100 and H100 GPUs into isolated instances. An A100-80GB can be divided into configurations ranging from 7 small instances (10GB each) to 2 large instances (40GB each). The device plugin exposes MIG instances as separate resources:

[^fn-mig]: **Multi-Instance GPU (MIG)**: NVIDIA hardware feature (A100 and later) that partitions a single GPU into up to 7 isolated instances, each with dedicated memory, cache, and compute resources. Unlike software-based GPU sharing, MIG provides hardware isolation that prevents memory access between instances. This enables secure multi-tenant GPU sharing but requires workloads to fit within instance memory limits.

+-----------------+----------------+--------------+----------------------+
| **MIG Profile** | **GPU Memory** | **SM Count** | **Typical Workload** |
+================:+===============:+=============:+:=====================+
| **1g.10gb**     | 10 GB          | 14 SMs       | Small inference      |
+-----------------+----------------+--------------+----------------------+
| **2g.20gb**     | 20 GB          | 28 SMs       | Medium inference     |
+-----------------+----------------+--------------+----------------------+
| **3g.40gb**     | 40 GB          | 42 SMs       | Large inference      |
+-----------------+----------------+--------------+----------------------+
| **7g.80gb**     | 80 GB          | 98 SMs       | Training             |
+-----------------+----------------+--------------+----------------------+

Gang scheduling[^fn-gang] ensures distributed training jobs receive all requested resources simultaneously. Without gang scheduling, a job requesting 32 GPUs might receive 24 immediately while waiting indefinitely for the remaining 8, wasting the already-allocated resources. The Volcano batch scheduler and scheduler plugins like Coscheduling implement gang semantics through PodGroup abstractions. Jobs specify minimum member counts, and the scheduler delays placement until all pods can be scheduled together.

[^fn-gang]: **Gang scheduling**: A scheduling policy that allocates resources for multi-component jobs atomically, ensuring all components start simultaneously or none do. First developed for parallel computing in the 1980s, gang scheduling prevents deadlock scenarios where jobs partially acquire resources and block each other. For ML training, gang scheduling ensures all workers are ready before training begins, avoiding wasted GPU cycles.

Priority classes control preemption behavior. A typical hierarchy assigns training workloads medium priority, inference high priority, and development jobs low priority. The `preemptionPolicy: PreemptLowerPriority` setting enables inference pods to reclaim resources from training when latency SLOs are threatened, though organizations must balance this against training job completion times.

#### Custom ML Schedulers

Research schedulers have demonstrated significant improvements over general-purpose systems by exploiting ML-specific characteristics.

**Tiresias** [@gu2019tiresias] observes that ML training jobs have predictable resource requirements after initial epochs. Rather than requiring users to estimate job duration (often inaccurate by 2-5x), Tiresias uses a two-dimensional attained service[^fn-attained-service] scheduler. Jobs accumulate "service" based on GPU-time consumed, with priority decreasing as service increases. A discretized version groups jobs into service bins, promoting short jobs without requiring duration estimates. Experiments show 40-60% reduction in average job completion time compared to FIFO scheduling.

[^fn-attained-service]: **Attained service scheduling**: A scheduling discipline where job priority decreases with cumulative resource usage. Originally developed for processor sharing, attained service naturally prioritizes short jobs without requiring duration estimates. For ML workloads where job length correlates with model complexity, this approach provides near-optimal average completion times while remaining robust to inaccurate user estimates.

**Gandiva** exploits the iterative nature of deep learning. Training alternates between GPU-intensive forward/backward passes and CPU-intensive data loading. Gandiva time-slices GPU access at iteration boundaries, enabling higher utilization through oversubscription. A cluster with 100 GPUs might support 120 concurrent jobs if each spends 20% of time waiting for data. Gandiva also implements grow-shrink elasticity, automatically adjusting data parallelism degree based on resource availability.

**Themis** addresses fairness in long-running ML workloads. Traditional fair-share treats all GPU-seconds equally, but ML jobs have diminishing returns as training progresses. Themis defines a finish-time fairness metric, allocating resources to minimize the maximum slowdown any job experiences relative to exclusive access. This approach benefits shorter jobs without excessive penalty to longer ones.

Locality-aware scheduling recognizes that communication topology matters for distributed training. A 64-GPU job performs better on 8 nodes of 8 GPUs each than 16 nodes of 4 GPUs, due to higher NVLink bandwidth within nodes. Advanced schedulers consider the fat-tree topology discussed in @sec-networking-ml, preferring allocations that share fewer switch hops. Experiments show 15-30% training throughput improvement from topology-aware placement.

### Online Serving Resource Management

Inference workloads require different scheduling strategies than training. Latency matters more than throughput, traffic fluctuates unpredictably, and resource requirements vary by model size and request characteristics.

#### Autoscaling for Inference

Horizontal Pod Autoscaling (HPA) adjusts replica counts based on metrics. Default CPU utilization targets (often 50-70%) poorly reflect GPU inference workloads. Effective ML autoscaling uses custom metrics:

+-------------------------+------------------+----------------------------------+
| **Metric**              | **Target Range** | **Considerations**               |
+:========================+:=================+:=================================+
| **GPU utilization**     | 60-80%           | Varies by model batch efficiency |
+-------------------------+------------------+----------------------------------+
| **Request queue depth** | 10-50 requests   | Prevents latency spikes          |
+-------------------------+------------------+----------------------------------+
| **P99 latency**         | &lt; SLO target  | Reactive, lags demand changes    |
+-------------------------+------------------+----------------------------------+
| **Pending tokens**      | Model-specific   | LLM-specific, accounts for KV    |
+-------------------------+------------------+----------------------------------+

Vertical Pod Autoscaling (VPA) adjusts resource requests and limits for individual pods. For inference, VPA can right-size memory allocations based on observed usage. However, GPU resources cannot be vertically scaled without pod restart, limiting VPA's utility for accelerated workloads.

LLM inference requires specialized scaling due to the key-value cache[^fn-kv-cache]. A 70B parameter model serving long-context requests may require 80GB+ of GPU memory for KV cache alone, even with PagedAttention optimizations. Scaling decisions must account for both request rate and context length distribution.

[^fn-kv-cache]: **Key-Value (KV) cache**: Memory that stores computed attention key and value tensors from previous tokens during autoregressive generation. Without caching, each new token would require recomputing attention over the entire sequence. KV cache grows linearly with sequence length and batch size; for a 70B model with 128K context, the cache can exceed model weights in memory usage. PagedAttention (vLLM) manages this memory more efficiently through virtual memory techniques.

#### Resource Isolation

Noisy neighbor problems occur when colocated workloads interfere with each other. On GPUs, interference manifests through shared memory bandwidth, L2 cache contention, and PCIe bottlenecks. MIG provides hardware isolation but at the cost of flexibility. Software approaches include careful placement policies and time-based resource contracts.

GPU memory isolation prevents one model from consuming memory needed by another. Without explicit limits, a memory leak or unexpectedly large batch can crash colocated workloads. Container runtimes can enforce memory limits through CUDA MPS (Multi-Process Service)[^fn-mps], though this adds latency overhead of approximately 5-10 microseconds per kernel launch.

[^fn-mps]: **CUDA MPS (Multi-Process Service)**: A CUDA feature that enables multiple processes to share a GPU with reduced context switching overhead. Unlike time-slicing where only one process accesses the GPU at a time, MPS allows concurrent kernel execution from different processes. MPS improves utilization for small workloads but provides limited isolation compared to MIG; a memory-intensive process can still impact colocated workloads through shared cache pressure.

CPU pinning assigns specific cores to inference pods, preventing scheduler migration that causes cache invalidation. For latency-sensitive workloads, isolating cores using `isolcpus` kernel parameters and `taskset` affinity removes OS scheduling jitter. Combined with NUMA-aware[^fn-numa] placement, this reduces P99 latency by 10-30% for sub-millisecond inference tasks.

[^fn-numa]: **NUMA (Non-Uniform Memory Access)**: A memory architecture where access time depends on memory location relative to the processor. Modern multi-socket servers have distinct memory controllers per CPU socket; accessing local memory takes approximately 100ns, while accessing remote memory via the interconnect takes 150-200ns. For ML inference, placing GPU workloads on CPU cores closest to the GPU's PCIe connection minimizes data transfer latency.

### Multi-Tenancy Considerations

Production ML platforms serve multiple teams with competing priorities. Quota systems balance guaranteed access against overall utilization, while security isolation protects sensitive models and data.

#### Quota Management

GPU quota allocation typically operates at the namespace or project level. A simple approach allocates fixed GPU counts, but this leads to underutilization when teams have variable workloads. Hierarchical quotas enable departmental limits with sub-team flexibility:

$$Q_{effective} = \min(Q_{team}, Q_{department} - \sum_{other\ teams} U_{allocated})$$

Fair-share across teams extends the single-user formula to organizational hierarchies. When aggregate demand exceeds capacity, each team receives resources proportional to their share allocation. Unused capacity borrows down the hierarchy, maximizing utilization while respecting priorities.

Burst capacity handling enables teams to temporarily exceed quotas when resources are available. Overcommitment ratios of 1.2-1.5x are common, with admission controllers tracking actual versus requested resources. When contention occurs, jobs using burst capacity face preemption first.

#### Security Isolation

Namespace separation provides the fundamental isolation boundary in Kubernetes. Each team operates within dedicated namespaces with role-based access control (RBAC) limiting visibility to their own resources. Network policies extend isolation to the network layer, preventing cross-namespace communication except through explicitly permitted services.

Network policies for ML workloads must balance isolation with distributed training requirements. A policy might allow all-to-all communication within a namespace (for ring-AllReduce) while blocking ingress from other namespaces. Egress policies prevent training jobs from accessing external networks, reducing data exfiltration risk.

GPU virtualization options range from time-slicing (low isolation, high flexibility) to MIG (hardware isolation, fixed partitions) to full device passthrough (complete isolation, lowest utilization). The choice depends on workload sensitivity and trust boundaries. Multi-tenant inference platforms typically use MIG for isolation between tenants, while single-tenant training clusters favor device passthrough for maximum performance.

The scheduling and resource management infrastructure discussed here enables efficient use of the datacenter resources and networks from previous sections. Effective schedulers achieve 70-85% GPU utilization in production clusters, compared to 30-50% with naive approaches. This efficiency translates directly to cost: a 1000-GPU cluster at 80% utilization delivers the equivalent capacity of 1600 GPUs at 50% utilization. As organizations scale ML infrastructure, scheduling sophistication becomes a primary determinant of both cost efficiency and researcher productivity. These resource management capabilities also provide the foundation for the fault-tolerant systems discussed in @sec-fault-tolerance and the operational practices covered in @sec-ops-scale.

## Total Cost of Ownership Analysis

Understanding the complete financial picture of ML infrastructure requires moving beyond simple hardware acquisition costs to comprehensive Total Cost of Ownership (TCO) analysis. This section provides quantitative frameworks for evaluating infrastructure investments, comparing deployment strategies, and optimizing long-term operational efficiency.

### Capital Expenditure Components

Capital expenditure (CapEx)[^fn-capex-opex] encompasses all upfront investments required to establish ML infrastructure. These costs are typically amortized over 3-5 years, though the rapid pace of GPU advancement often compresses effective useful life.

[^fn-capex-opex]: **CapEx vs OpEx**: Capital expenditure (CapEx) covers upfront asset purchases (hardware, construction) that are depreciated over time, while operational expenditure (OpEx) covers ongoing costs (power, staff, cloud fees) that are expensed immediately. Cloud computing shifts costs from CapEx to OpEx, which affects financial planning, tax treatment, and budget approval processes differently across organizations.

#### Hardware Costs

GPU and accelerator acquisition represents the dominant CapEx component for ML infrastructure. Current market pricing reflects both performance capabilities and supply constraints.

+----------------------+---------------+-------------------+--------------------+
| **System**           | **Base Cost** | **Memory Config** | **Cost per PFLOP** |
+=====================:+==============:+==================:+===================:+
| **DGX H100**         | ~$300,000     | 640 GB HBM3       | ~$75,000           |
+----------------------+---------------+-------------------+--------------------+
| **DGX B100***        | ~$450,000     | 1.4 TB HBM3e      | ~$25,000           |
+----------------------+---------------+-------------------+--------------------+
| **HGX H100 (8-way)** | ~$250,000     | 640 GB HBM3       | ~$62,500           |
+----------------------+---------------+-------------------+--------------------+
| **TPU v5p Pod**      | Variable      | 95 GB HBM         | ~$30,000           |
+----------------------+---------------+-------------------+--------------------+

*B100 pricing is estimated. All prices reflect approximate 2024 market conditions and should be verified for current planning. GPU pricing fluctuates 20-40% based on supply constraints and generation transitions.

Server and storage costs add substantial overhead beyond accelerators. A complete DGX H100 deployment requires NVMe storage ($15,000-30,000 per node), high-speed networking cards ($8,000-15,000), and rack infrastructure ($5,000-10,000). Storage architecture for large-scale training demands parallel file systems capable of sustaining the I/O bandwidth required by hundreds of GPUs, with enterprise solutions like Lustre or GPFS adding $500-1,000 per terabyte of high-performance capacity.

Networking equipment costs scale superlinearly with cluster size due to the hierarchical nature of high-bandwidth fabrics. A 256-GPU cluster using InfiniBand HDR requires approximately $800,000-1,200,000 in networking equipment.

$$C_{\text{network}} = N_{\text{switches}} \cdot P_{\text{switch}} + N_{\text{cables}} \cdot P_{\text{cable}} + N_{\text{adapters}} \cdot P_{\text{adapter}}$$

For a 256-GPU deployment with 2:1 oversubscription:

$$C_{\text{network}} \approx 32 \times \$15,000 + 512 \times \$800 + 256 \times \$3,000 \approx \$1,660,000$$

Refresh cycle planning significantly impacts TCO calculations. GPU generations advance every 2-3 years with typical performance improvements of 2-3x per generation. Organizations must balance the benefits of newer hardware against the disruption costs of migration. A common strategy employs staggered refresh cycles, replacing 25-33% of infrastructure annually to maintain competitive capability while avoiding wholesale replacement costs.

#### Facility Costs

Datacenter construction costs range from $7-12 million per megawatt of IT capacity for purpose-built facilities. ML workloads, with their high power density requirements (30-50 kW per rack versus 5-10 kW for traditional compute), demand specialized cooling infrastructure that increases construction costs by 20-40%.

Power infrastructure represents a substantial portion of facility investment. Electrical distribution systems including transformers, switchgear, uninterruptible power supplies (UPS), and power distribution units (PDUs) typically cost $2-4 million per megawatt. Redundancy requirements (N+1 or 2N configurations) can double these costs for mission-critical deployments.

Cooling systems for high-density ML infrastructure increasingly require liquid cooling solutions. Direct-to-chip liquid cooling adds $50,000-100,000 per rack in capital costs but enables the power densities required for modern GPU configurations. The DGX H100 systems referenced in our datacenter architecture discussion require liquid cooling for sustained operation, representing a non-optional facility cost.

### Operational Expenditure Components

Operational expenditure (OpEx) captures ongoing costs that accumulate throughout infrastructure lifetime. For ML systems, power costs and specialized staffing dominate this category.

#### Power Costs

Electricity represents the largest operational cost for ML infrastructure. Power costs vary dramatically by geography, with industrial rates ranging from $0.04/kWh in regions with abundant hydroelectric power to $0.20/kWh in constrained markets.

The total power cost calculation must account for PUE overhead. As established in our datacenter architecture discussion, hyperscale facilities achieve PUE values of 1.1-1.2, meaning 10-20% additional power supports cooling and infrastructure. The annual power cost for a single DGX H100 system can be calculated as:

$$C_{\text{power}} = P_{\text{system}} \times \text{PUE} \times H_{\text{annual}} \times R_{\text{electricity}} \times U$$

where $P_{\text{system}}$ is system power (10.2 kW for DGX H100), $H_{\text{annual}}$ is hours per year (8,760), $R_{\text{electricity}}$ is the electricity rate, and $U$ is utilization factor.

For a DGX H100 at 80% utilization with $0.08/kWh electricity and 1.15 PUE:

$$C_{\text{power}} = 10.2 \times 1.15 \times 8,760 \times 0.08 \times 0.80 \approx \$6,600 \text{ annually}$$

Electricity pricing models significantly impact operational costs. Time-of-use pricing creates opportunities for training workload scheduling during off-peak hours (typically nights and weekends), potentially reducing power costs by 20-40%. Demand charges, which price peak power consumption, incentivize workload smoothing to avoid utilization spikes.

Renewable energy considerations extend beyond environmental responsibility to economic optimization. Power Purchase Agreements (PPAs) for renewable energy often provide long-term price stability, hedging against electricity market volatility. Many organizations target 100% renewable energy matching through a combination of on-site generation, PPAs, and Renewable Energy Certificates (RECs). Environmental implications of energy choices are examined comprehensively in @sec-sustainable-ai.

#### Staffing and Operations

ML infrastructure requires specialized operational expertise across multiple domains. Staffing costs often represent 15-25% of total operational expenditure for well-run facilities.

Hardware operations teams manage physical infrastructure including installation, maintenance, and failure response. For clusters of 500+ GPUs, dedicated hardware technicians are essential, with typical ratios of 1 technician per 200-400 GPUs depending on hardware heterogeneity and SLA requirements.

Software platform teams maintain the scheduling systems, container infrastructure, and ML frameworks that enable productive use of hardware resources. These roles command premium compensation due to the specialized intersection of systems engineering and ML expertise required.

Utilization monitoring represents both a staffing function and a key lever for TCO optimization. Continuous monitoring of GPU utilization, memory bandwidth, and job efficiency enables identification of optimization opportunities. Organizations achieving 70%+ sustained GPU utilization versus the more common 30-50% effectively halve their per-computation infrastructure costs.

### Build vs. Buy Analysis

The fundamental infrastructure decision is whether to operate private infrastructure or consume cloud capacity. This choice involves complex trade-offs that depend on workload characteristics, scale, and organizational capabilities.

#### Cloud vs. On-Premises Trade-offs

Cloud computing offers compelling advantages for specific use cases. Variable workloads with unpredictable demand benefit from cloud elasticity, avoiding stranded capacity during low-demand periods. Experimentation and research phases, where hardware requirements remain uncertain, benefit from the ability to test different configurations without capital commitment. Geographic distribution requirements for inference serving often favor cloud deployment due to the substantial investment required for multi-region presence.

On-premises infrastructure wins economically under sustained high utilization. The break-even analysis requires comparing amortized CapEx plus OpEx against equivalent cloud costs:

$$\text{Break-even utilization} = \frac{C_{\text{cloud}} \times H_{\text{annual}}}{\frac{C_{\text{capex}}}{Y_{\text{amortization}}} + C_{\text{opex}}}$$

Consider a DGX H100 system with $300,000 CapEx, 3-year amortization, and $25,000 annual OpEx (power, maintenance, proportional staff). Cloud equivalent (8x H100 instance at ~$25/hour):

$$\text{Break-even} = \frac{25 \times 8,760}{\frac{300,000}{3} + 25,000} = \frac{219,000}{125,000} \approx 1.75$$

This calculation suggests on-premises becomes favorable when utilization exceeds approximately 57% (1/1.75). In practice, organizations report break-even utilization thresholds of 40-60% depending on specific cloud pricing and operational efficiency.

+----------------------+-------------------+------------------------+
| **Factor**           | **Favors Cloud**  | **Favors On-Premises** |
+:=====================+:==================+:=======================+
| **Utilization**      | &lt;40% average   | &gt;60% sustained      |
+----------------------+-------------------+------------------------+
| **Workload pattern** | Variable, bursty  | Steady, predictable    |
+----------------------+-------------------+------------------------+
| **Data volume**      | Moderate          | Petabyte-scale         |
+----------------------+-------------------+------------------------+
| **Time horizon**     | &lt;2 years       | &gt;3 years            |
+----------------------+-------------------+------------------------+
| **Team capability**  | Limited ops staff | Strong infrastructure  |
+----------------------+-------------------+------------------------+

Hybrid strategies combine cloud burst capacity with on-premises baseline infrastructure. Organizations maintain on-premises systems sized for typical load (e.g., 60th percentile demand) while using cloud for peak periods. This approach captures most on-premises economic benefits while retaining cloud flexibility.

#### Reserved Capacity vs. Spot Instances

Cloud providers offer commitment discount programs that substantially reduce effective pricing. Reserved instances with 1-year commitments typically offer 30-40% discounts, while 3-year commitments reach 50-60% discounts relative to on-demand pricing. These discounts shift cloud economics but introduce utilization risk similar to on-premises ownership.

Spot instance strategies enable dramatic cost reduction (60-80% below on-demand) for fault-tolerant training workloads. Effective spot utilization requires:

1. **Checkpoint integration**: Training frameworks must save state frequently enough that spot interruption costs remain acceptable. Modern distributed training checkpoints every 10-30 minutes, limiting maximum lost computation.

2. **Fallback mechanisms**: Automated job migration to alternative instance types or regions when spot capacity becomes unavailable.

3. **Heterogeneous training**: Frameworks capable of operating across mixed instance types to maximize spot availability.

The effective spot discount must account for interruption overhead:

$$C_{\text{effective}} = C_{\text{spot}} \times (1 + R_{\text{interrupt}} \times T_{\text{recovery}})$$

where $R_{\text{interrupt}}$ is the hourly interruption rate and $T_{\text{recovery}}$ is recovery time as a fraction of checkpoint interval. With 5% hourly interruption rate and 10-minute recovery on 30-minute checkpoints:

$$C_{\text{effective}} = 0.30 \times C_{\text{ondemand}} \times (1 + 0.05 \times 0.33) \approx 0.31 \times C_{\text{ondemand}}$$

Even accounting for interruption overhead, spot instances provide compelling economics for training workloads with proper checkpoint infrastructure.

### Comprehensive TCO Model

A complete TCO model integrates capital and operational components across the infrastructure lifetime:

$$\text{TCO} = \sum_{t=1}^{Y} \frac{C_{\text{capex}}^{(t)} + C_{\text{opex}}^{(t)}}{(1+r)^t}$$

where $r$ is the discount rate reflecting cost of capital. For a 256-GPU cluster over 4 years:

| Component           | Year 1      | Year 2      | Year 3      | Year 4      |
|---------------------|-------------|-------------|-------------|-------------|
| Hardware CapEx      | $9,600,000  | $0          | $0          | $3,200,000  |
| Network CapEx       | $1,660,000  | $0          | $0          | $0          |
| Power (at 70% util) | $1,690,000  | $1,690,000  | $1,690,000  | $1,690,000  |
| Maintenance         | $480,000    | $576,000    | $691,000    | $829,000    |
| Staff (allocated)   | $800,000    | $840,000    | $882,000    | $926,000    |
| **Annual Total**    | $14,230,000 | $3,106,000  | $3,263,000  | $6,645,000  |

The NPV at 8% discount rate equals approximately $24.1 million, yielding a 4-year cost per GPU-hour of $4.30 at 70% utilization. This compares favorably to cloud A100 pricing of $3-4/hour only when accounting for the H100's 3x performance advantage, yielding effective cost per computation approximately 40% below cloud alternatives at this utilization level.

Power cost sensitivity analysis reveals the importance of electricity pricing in deployment decisions. A $0.04/kWh difference in electricity rates shifts the 4-year TCO by approximately $2.7 million for a 256-GPU cluster, potentially changing the optimal deployment strategy. Organizations with access to low-cost renewable energy enjoy structural cost advantages that compound over multi-year infrastructure investments.

## Case Studies

The infrastructure patterns examined in previous sections combine in different configurations depending on workload characteristics and organizational constraints. Four production deployments illustrate how datacenter architecture, networking, and resource management decisions interact to enable distinct ML workloads. Each case study represents a different point in the design space: GPU-centric dense training, TPU-based transformer optimization, hybrid CPU-GPU recommendation serving, and custom silicon for domain-specific acceleration.

### NVIDIA DGX SuperPOD Architecture

The DGX SuperPOD represents NVIDIA's reference architecture for large-scale training, combining the dense GPU packaging of DGX systems with purpose-built networking. While the previous section examined SuperPOD networking topology, this case study addresses the complete system architecture including physical deployment, management infrastructure, and operational characteristics.

#### Physical Layout and Cooling Integration

A production SuperPOD deployment with 512 DGX H100 systems (4096 GPUs) occupies approximately 2000 square meters of datacenter floor space. The layout follows a pod-based organization where groups of 32 DGX systems share common power and cooling infrastructure. Each pod dissipates over 300 kW, requiring direct liquid cooling loops with facility-level heat exchangers.

The cooling architecture uses a closed-loop system with water temperature maintained at 35-45C entering the cold plates. Unlike traditional datacenter cooling that targets low air temperatures, warm-water cooling improves efficiency by enabling free cooling in moderate climates. Heat removed from GPU cold plates transfers to building cooling towers without mechanical refrigeration for ambient temperatures below 25C.

Power distribution follows the N+1 redundancy model at the pod level, with each DGX system receiving dual power feeds. A complete SuperPOD installation requires 5-7 MW of utility power including cooling overhead, corresponding to PUE values of 1.2-1.3 for liquid-cooled deployments.

#### Management Plane Architecture

SuperPOD management integrates multiple control systems spanning hardware, networking, and workload orchestration. Base Controller Manager (BCM) provides hardware-level management including firmware updates, health monitoring, and out-of-band access. The Unified Fabric Manager coordinates InfiniBand network configuration, adaptive routing policies, and link health monitoring.

At the workload level, SuperPOD deployments typically integrate with either Slurm or Kubernetes for job scheduling. The NVIDIA GPU Operator handles GPU driver installation, monitoring integration, and device plugin management for Kubernetes environments. Slurm configurations use GRES scheduling with topology-aware placement to ensure jobs receive contiguous GPU allocations that minimize inter-node communication.

Storage integration varies by deployment, but reference architectures include NVIDIA's GPUDirect Storage for direct data paths between NVMe storage and GPU memory. A typical SuperPOD includes 30-50 PB of high-performance storage providing 200+ GB/s aggregate throughput, staging training data close to compute.

### Google TPU Pod Infrastructure

Google's TPU pods represent an alternative architectural philosophy: vertically integrated accelerators designed specifically for transformer training, with interconnect capabilities built into the chip rather than added as external networking.

#### TPU v4 Pod Architecture

A TPU v4 pod contains 4096 TPU chips arranged in a 3D torus topology. Each chip provides approximately 275 TFLOPS of bfloat16 compute with 32 GB of HBM2e memory, yielding aggregate pod capacity of 1.1 exaFLOPS and 128 TB of memory. The power envelope for a complete pod is approximately 4-5 MW, competitive with GPU-based systems at similar compute density.

The physical packaging differs fundamentally from GPU systems. TPU chips mount in trays of 4, with trays assembled into racks of 64 chips each. Sixty-four racks form the complete pod, arranged in a cube topology that matches the 3D torus interconnect structure. Cooling uses rear-door heat exchangers with facility water, maintaining chip temperatures below 85C under sustained load.

#### Inter-Chip Interconnect Topology

The ICI (Inter-Chip Interconnect) fabric provides direct chip-to-chip connectivity without external switches. Each TPU v4 chip has six ICI links at 100 GB/s each, enabling 3D torus connectivity:

$$
\text{Bisection Bandwidth} = 2 \times \sqrt[3]{N} \times B_{\text{link}} \times N/2
$$

For N=4096 chips with 100 GB/s links, the torus bisection bandwidth reaches approximately 32 TB/s. While lower than fat-tree alternatives, the consistent latency characteristics of torus topology benefit the regular communication patterns of transformer training.

The topology choice optimizes for AllReduce patterns where each chip communicates with neighbors rather than arbitrary endpoints. For a model using 3D parallelism with 4 tensor-parallel chips, 16 pipeline stages, and 64-way data parallelism, the workload maps naturally onto a 4x16x64 slice of the pod topology.

#### Software Stack Integration

TPU software centers on JAX and XLA[^fn-xla], with pjit (partitioned JIT compilation) managing distributed execution. XLA compiles high-level model descriptions to TPU-specific operations, automatically inserting communication collectives based on partition specifications. This approach differs from the explicit communication programming required for GPU clusters.

[^fn-xla]: **XLA (Accelerated Linear Algebra)**: A domain-specific compiler for linear algebra operations that optimizes entire computation graphs rather than individual operations. XLA performs operator fusion (combining multiple ops into single kernels), buffer reuse optimization, and automatic layout transformation. Originally developed for TPUs, XLA now supports GPUs and CPUs, enabling framework-agnostic optimization of ML computations.

Multislice training extends beyond single pods by connecting multiple TPU slices via datacenter network. A PaLM-scale training run might utilize four TPU v4 pods (16,384 chips) with cross-slice communication at lower bandwidth than intra-slice ICI. The software stack handles this hierarchy transparently, using different collective algorithms for intra-slice versus inter-slice operations.

### Meta Recommendation Infrastructure

Meta's recommendation systems illustrate infrastructure optimized for a fundamentally different workload pattern: models combining massive embedding tables with relatively modest dense computation. This architecture serves billions of daily recommendation queries across products including Facebook Feed, Instagram, and Reels.

#### CPU-GPU Hybrid Architecture

Recommendation models like DLRM[^fn-dlrm] (Deep Learning Recommendation Model) partition naturally between embedding operations and dense neural network computation. Embedding tables for production systems can exceed 10 TB, far exceeding GPU memory capacity. The hybrid architecture addresses this by placing embeddings in CPU DRAM while dense layers execute on GPUs.

[^fn-dlrm]: **DLRM (Deep Learning Recommendation Model)**: Meta's open-source recommendation model architecture that became the reference design for industry recommendation systems. DLRM processes both dense features (through MLPs) and sparse features (through embedding tables), combining them via a factorization machine-inspired interaction layer. The architecture explicitly acknowledges the embedding-memory bottleneck, making it a natural fit for hybrid CPU-GPU deployment.

A production recommendation training node combines multiple CPUs totaling 2-4 TB of DRAM with 8 GPUs for dense computation. The CPUs handle embedding lookups, concatenation, and feature preprocessing. Resulting feature vectors transfer to GPUs via PCIe for the dense forward and backward passes. Gradient updates for embeddings return to CPU memory via the same path.

This architecture requires careful balancing. The ratio of embedding lookups to dense computation determines optimal CPU-to-GPU allocation. For Meta's workloads, approximately 4:1 CPU socket to GPU ratios provide balanced utilization, though this varies by model architecture.

#### Embedding Table Serving at Scale

Inference architecture differs from training by emphasizing latency over throughput. Production serving distributes embedding tables across a fleet of CPU-based servers using consistent hashing for shard assignment. A single recommendation query may access hundreds of embedding shards, requiring parallel lookups that complete within the 50-100 ms latency budget.

The embedding serving tier operates separately from the dense model serving tier. This separation enables independent scaling: embedding servers scale with table size and query rate, while dense model servers scale with compute requirements. Cross-tier communication uses low-latency RPC, typically completing in under 5 ms for local datacenter deployments.

Feature stores cache frequently accessed embeddings and precomputed features, reducing embedding server load for popular items. A tiered caching architecture places hot embeddings in GPU memory (microsecond access), warm embeddings in CPU DRAM (sub-millisecond), and cold embeddings in distributed storage (milliseconds). Cache hit rates above 90% are typical for recommendation workloads due to power-law popularity distributions.

#### Training and Serving Coordination

The separation between training and serving infrastructure creates coordination challenges for model updates. Meta's approach uses a staged rollout pipeline: models train on dedicated GPU clusters, export to serving format, deploy to staging clusters for validation, then gradually roll out to production serving. The complete pipeline from training completion to full production deployment spans hours to days depending on model criticality.

Training clusters optimize for throughput using large batch sizes and aggressive gradient accumulation. Serving clusters optimize for latency using quantized models, batched inference, and result caching. The different optimization targets justify separate infrastructure rather than shared clusters.

### Tesla Dojo for Vision Training

Tesla's Dojo system represents the custom silicon approach to ML infrastructure: building purpose-designed chips and packaging for a specific workload rather than using general-purpose accelerators.

#### Custom Silicon Architecture

The Dojo D1 chip provides 1024 custom-designed cores in a 645 mm^2^ die. Each core combines an 8-wide vector unit, 64-bit scalar unit, and 1.25 MB of SRAM, yielding approximately 22.6 TFLOPS of BF16[^fn-bf16] compute per chip. The design optimizes for convolutional and attention operations typical of vision models, with dataflow execution patterns that minimize memory traffic.

[^fn-bf16]: **BF16 (Brain Floating Point)**: A 16-bit floating point format with the same 8-bit exponent as FP32 but only 7 mantissa bits (versus 23 in FP32). Developed by Google for TPUs, BF16 matches FP32's dynamic range, avoiding the overflow/underflow issues of FP16 that require loss scaling. BF16 has become the default training precision for transformers, offering 2x memory savings with minimal accuracy impact.

Twenty-five D1 chips mount on a single training tile, connected via a 2D mesh interconnect providing 4 TB/s aggregate bandwidth. Six tiles combine into a system tray, and multiple trays assemble into a complete ExaPOD delivering over 1 exaFLOP of aggregate compute. The modular architecture enables deployments from single tiles (0.5 PFLOPS) to multi-ExaPOD installations.

#### Wafer-Scale Considerations

While Dojo uses conventional chip packaging, the architecture addresses similar challenges to wafer-scale integration: maximizing on-chip bandwidth while managing thermal and yield constraints. The 2D mesh topology within each tile provides nearest-neighbor bandwidth of 18 GB/s between chips, avoiding the bottlenecks of hierarchical topologies for spatially-local operations common in vision processing.

Power density presents the primary challenge: a fully populated system tray dissipates over 100 kW in a compact form factor. Tesla's thermal solution uses direct liquid cooling with custom manifolds delivering coolant to each training tile. The aggressive cooling enables sustained operation at power densities exceeding traditional datacenter limits.

Yield management for custom silicon requires careful attention. Unlike commodity GPU purchases where defective units return to the vendor, custom chip production creates internal yield loss. Dojo's design includes redundant cores and interconnect paths, enabling graceful degradation when manufacturing defects occur. Production testing identifies defective units, and the software stack maps computation around unavailable resources.

#### Training Video Data at Scale

Dojo's primary workload is training vision models on Tesla's fleet data: over 1 million video clips per day from vehicles worldwide. The data pipeline presents distinct challenges from text or image training. Video requires decompression, temporal alignment, sensor calibration, and often 3D scene reconstruction before training.

The preprocessing pipeline runs on CPU clusters adjacent to Dojo compute, staging prepared batches to high-speed storage. Storage bandwidth of 10+ GB/s per training tile ensures compute utilization despite the data-intensive nature of video processing. The complete system integrates 10 PB of flash storage providing over 100 GB/s aggregate throughput.

This infrastructure supports auto-labeling workflows where preliminary models identify scenarios of interest in raw video, generating training data for improved models. The closed-loop between deployment, data collection, and training enables rapid iteration cycles measured in days rather than weeks.

---

These case studies demonstrate that production ML infrastructure defies one-size-fits-all solutions. DGX SuperPOD optimizes for flexible general-purpose training with emphasis on GPU density and high-bandwidth networking. TPU pods sacrifice flexibility for vertical integration that excels at transformer workloads. Meta's hybrid architecture addresses the embedding-heavy patterns unique to recommendation systems. Tesla's Dojo pursues custom silicon for domain-specific acceleration where scale justifies development costs. The choice among these approaches depends on workload characteristics, scale requirements, and organizational capabilities rather than any universal optimum. Understanding these trade-offs enables informed infrastructure decisions as models and training requirements continue to evolve. For implementation details of the distributed training algorithms that leverage these infrastructure platforms, see @sec-distributed-training. Network-level considerations for collective operations are examined in @sec-communication.

## Fallacies and Pitfalls

The complexity of large-scale ML infrastructure creates numerous opportunities for costly miscalculations. These misconceptions often stem from oversimplified mental models that fail to account for the non-linear interactions between compute, networking, power, and cooling systems. Understanding these fallacies and pitfalls helps practitioners avoid expensive mistakes that can waste millions of dollars in infrastructure investment or months of delayed projects.

**Fallacy:** _More GPUs always means faster training._

This intuition fails catastrophically beyond modest cluster sizes. Distributed training introduces communication overhead, and the relationship between GPU count and training speed is far from linear. For a 175B parameter model using data parallelism, each training step requires exchanging approximately 350 GB of gradient data (FP16). Ring AllReduce achieves near-optimal bandwidth utilization: each GPU sends and receives $2P \cdot (N-1)/N$ bytes, approaching $2P$ as $N$ grows. With 64 GPUs connected via 400 Gbps (50 GB/s) InfiniBand, the communication time for 350 GB approaches:

$$
T_{\text{comm}} = \frac{2P}{B} = \frac{700 \text{ GB}}{50 \text{ GB/s}} \approx 14 \text{ seconds}
$$

Critically, this communication time is bounded as $N$ increases. The scaling efficiency depends on the ratio of compute time to total time:

$$
\text{Efficiency} = \frac{T_{\text{compute}}}{T_{\text{compute}} + T_{\text{comm}}}
$$

For data parallelism, $T_{\text{compute}}$ scales as $1/N$ while $T_{\text{comm}}$ approaches a constant. When compute time per GPU drops below communication time, adding more GPUs yields diminishing returns. For a workload with 60 seconds single-GPU compute time, scaling to 64 GPUs reduces compute to ~1 second while communication remains ~14 seconds, yielding only 7% efficiency. Many organizations discover that their 512-GPU cluster trains models slower than a well-optimized 128-GPU deployment due to crossing this inflection point. The solution requires moving beyond naive data parallelism to hybrid parallelism strategies that minimize cross-node communication, topics explored in @sec-distributed-training.

**Fallacy:** _Peak FLOPS determines training throughput._

Vendors advertise peak FLOPS prominently: the H100 delivers 990 TF of FP16 compute. But production training jobs typically achieve 30-50% of peak due to memory bandwidth limitations, communication overhead, and kernel efficiency. For attention-dominated transformer training, memory bandwidth, not compute, is the limiting factor.

The roofline model reveals this clearly: with arithmetic intensity below the ridge point (~290 FLOP/byte for H100), the accelerator is memory-bound. Most LLM training operates at 50-100 FLOP/byte, achieving only 170-340 TF effective throughput on H100, roughly 20-35% of peak. Comparing accelerators by peak FLOPS alone misleads: a lower-FLOPS accelerator with higher memory bandwidth may outperform a higher-FLOPS alternative for memory-bound workloads.

**Fallacy:** _All ML infrastructure should be GPU-based._

The assumption that GPUs represent the optimal accelerator for all ML workloads ignores the fundamental architectural differences between model types. Recommendation systems, which drive the majority of inference cycles at companies like Meta and Google, exhibit a workload profile where embedding table lookups dominate compute time. These lookups are memory-bound operations that GPUs handle poorly since they require random access to terabytes of embedding tables that cannot fit in GPU HBM.

Meta's production recommendation infrastructure uses a hybrid architecture where CPU clusters handle embedding lookups from DRAM-based feature stores while GPU clusters process the dense neural network layers. This split architecture achieves 3x better cost efficiency than GPU-only deployments for their workload. Similarly, preprocessing pipelines for training data, including decompression, tokenization, and augmentation, execute more efficiently on CPUs. A DGX H100 with only 2 Intel Xeon CPUs can become CPU-bottlenecked on data preprocessing, starving the 8 H100 GPUs that represent 95% of the system cost.

**Pitfall:** _Ignoring power and cooling constraints during infrastructure planning._

Power and cooling represent hard physical limits that cannot be resolved through software optimization. A single rack of 4 DGX H100 systems requires over 40 kW of power and generates equivalent thermal load. Many organizations plan GPU purchases based on compute requirements without verifying datacenter capacity, only to discover their facility cannot support the power density.

The consequences compound over time. Current-generation H100 GPUs consume 700W each, but next-generation B100 GPUs maintain similar power envelopes while delivering 2x compute. However, the industry trend toward higher-density deployments means power requirements per rack continue to increase. Organizations that build datacenters for 30 kW per rack today will face costly retrofits within 2-3 years as GPU density increases.

Thermal throttling presents an equally insidious challenge. When cooling systems cannot remove heat fast enough, GPUs reduce clock speeds to prevent damage. A cluster designed for 100% utilization may achieve only 70% sustained throughput due to thermal constraints. At $30 per GPU-hour for H100 instances, this represents $7.20 per hour per GPU in wasted capacity. For a 1000-GPU cluster running training jobs, thermal inefficiency costs over $170,000 per month.

**Pitfall:** _Underestimating network requirements for distributed training._

Network bandwidth is rarely the only consideration. Latency, topology, and software overhead combine to determine actual training throughput. A cluster with theoretical 400 Gbps InfiniBand connectivity may achieve only 300 Gbps effective throughput due to protocol overhead in NCCL, suboptimal job placement, and contention from concurrent training jobs.

Fat-tree topologies provide full bisection bandwidth at significant switch cost, while rail-optimized topologies reduce hardware requirements but constrain job placement flexibility. Choosing the wrong topology for a workload can halve training throughput. For tensor-parallel workloads where adjacent GPUs exchange activations frequently, rail-optimized networks with 2-hop maximum latency outperform fat-tree networks with 4-6 hops. For data-parallel workloads with AllReduce patterns, fat-tree's guaranteed bisection bandwidth prevents congestion-induced slowdowns.

Software overhead adds latency that theoretical bandwidth calculations ignore. NCCL introduces 5-20 microseconds of software latency per collective operation. For small message sizes common in pipeline parallelism, this overhead dominates transfer time, reducing effective bandwidth by 50% or more. The crossover point where bandwidth dominates latency occurs at approximately 250 KB message size for typical InfiniBand configurations. Operators that fail to aggregate small tensors before collective operations waste substantial network capacity. These communication optimization strategies are examined in detail in @sec-communication.

## Summary

Large-scale ML infrastructure represents the physical foundation upon which all distributed training and serving systems operate. The constraints examined in this chapter, from power delivery and cooling capacity to network topology and accelerator selection, ultimately determine what scale of ML systems an organization can build and operate effectively.

Infrastructure design must match workload characteristics. LLM training demands GPU-dense configurations with high-bandwidth NVLink and InfiniBand connectivity, while recommendation systems require hybrid CPU-GPU architectures optimized for embedding table access. Vision workloads fall between these extremes, benefiting from GPU acceleration but tolerating moderate network bandwidth. Attempting to serve all workload types from homogeneous infrastructure wastes resources and constrains performance.

Training and serving have fundamentally different infrastructure requirements. Training workloads tolerate batch scheduling, require sustained high bandwidth for collective operations, and can checkpoint through failures. Serving workloads demand low-latency networking, consistent response times, and immediate failover capabilities. Organizations that attempt to share infrastructure between training and serving often compromise both workloads.

Total cost of ownership extends far beyond hardware acquisition. Power consumption for a 1000-GPU cluster can exceed $2 million annually at typical datacenter rates. Cooling infrastructure may cost more than the GPUs it supports. Operational overhead including monitoring, maintenance, and administration adds 20-30% to hardware costs over a three-year depreciation cycle. Cloud versus on-premises decisions depend critically on utilization rates, with break-even typically occurring around 40% sustained utilization.

Network topology choices determine distributed training efficiency. The decision between fat-tree and rail-optimized topologies, InfiniBand and RoCE, 2-tier and 3-tier switching architectures shapes what parallelism strategies perform well on the resulting infrastructure. Topology decisions made during datacenter construction constrain training architectures for years afterward.

Different model types require different infrastructure patterns. The SuperPOD architecture optimized for LLM training differs fundamentally from Meta's recommendation serving infrastructure or Tesla's Dojo system optimized for video processing. No single infrastructure design serves all ML workloads optimally, and organizations must either specialize their infrastructure or accept efficiency losses from generalization.

::: {.callout-important title="Key Takeaways"}

* Power density and cooling capacity represent hard physical limits that constrain cluster design independent of budget
* Network topology determines training efficiency, with fat-tree providing flexibility while rail-optimized reduces latency for structured communication patterns
* Hybrid CPU-GPU architectures outperform GPU-only configurations for recommendation systems and other embedding-heavy workloads
* Communication overhead limits scaling efficiency, with Amdahl's Law applying to gradient synchronization and collective operations
* Total cost of ownership must include power, cooling, operations, and realistic utilization rates rather than theoretical peak performance

:::

Yet infrastructure alone cannot serve ML workloads. The accelerators, networks, and cooling systems examined here require data: petabytes of training examples that must flow efficiently from persistent storage to GPU memory. The next chapter (@sec-storage) examines storage systems for large-scale ML, addressing how distributed file systems, object stores, checkpoint mechanisms, and feature stores provide the data infrastructure upon which training and serving depend. Storage bandwidth, not capacity, typically limits ML training throughput, making storage architecture decisions as consequential as accelerator selection.

The infrastructure foundations established here enable the distributed training strategies in @sec-distributed-training, which examines how parallelism approaches map onto physical hardware. The communication patterns and collective operations detailed in @sec-communication depend directly on the network topologies and bandwidth characteristics discussed in this chapter. For production deployments, @sec-fault-tolerance addresses the reliability requirements that infrastructure must satisfy to support multi-week training runs across thousands of accelerators.
