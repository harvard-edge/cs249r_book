---
title: "Large-Scale ML Infrastructure"
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR INFRASTRUCTURE
================================================================================

CORE PRINCIPLE: Infrastructure requirements vary by workload type.
Training clusters differ from serving infrastructure. Different model
types have different compute, memory, and networking needs.

MODEL-SPECIFIC INFRASTRUCTURE CONSIDERATIONS:

| Model Type      | Compute Profile     | Memory Profile      | Network Need        |
|-----------------|---------------------|---------------------|---------------------|
| LLMs            | GPU-heavy           | HBM-bound           | High (tensor par.)  |
| Recommendation  | CPU+GPU hybrid      | DRAM for embeddings | Moderate            |
| Vision          | GPU-heavy           | Moderate            | Moderate            |
| Scientific      | Varies              | Often huge          | Problem-dependent   |

REQUIRED COVERAGE FOR THIS CHAPTER:

DATACENTER ARCHITECTURE:

- GPU clusters: Dominant for training transformers, vision
- CPU clusters: Feature serving, preprocessing (RecSys)
- Hybrid: Recommendation training (embedding on CPU, dense on GPU)
- Include: Why different workloads need different architectures

ACCELERATOR SELECTION:

- GPU (NVIDIA): General-purpose ML, dominant for training
- TPU: Large-scale training, specific model types
- Custom ASICs: Inference optimization (recommendation, vision)
- Include: Different accelerators suit different workloads

NETWORKING:

- InfiniBand: Training clusters, high-bandwidth collective ops
- Ethernet: Serving infrastructure, feature stores
- Include: Why training and serving have different network needs

RESOURCE MANAGEMENT:

- Batch scheduling: Training jobs (Slurm, Kubernetes)
- Online serving: Request routing, autoscaling
- Include: Different scheduling for different workload types

CASE STUDIES TO INCLUDE:

- NVIDIA DGX SuperPOD architecture
- Google TPU pod infrastructure
- Meta recommendation infrastructure (CPU+GPU hybrid)
- Tesla Dojo for vision training

QUANTITATIVE ANALYSIS:

- TCO breakdown by workload type
- Power/performance efficiency for different accelerators
- Network utilization patterns by model type
- Include: Same cluster, different efficiency for different models

ANTI-PATTERNS TO AVOID:

- Assuming all ML infrastructure is GPU clusters
- Ignoring CPU infrastructure for recommendation
- One-size-fits-all datacenter design
- Only discussing training infrastructure (serving matters too)

================================================================================
-->

# Large-Scale ML Infrastructure {#sec-infrastructure}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: A sweeping architectural visualization of a modern AI datacenter infrastructure. The scene reveals a massive facility with rows of GPU clusters arranged in pods, connected by high-bandwidth networking fabric depicted as glowing fiber optic pathways. Cooling systems appear as flowing blue currents between server racks. The visualization includes multiple layers: physical infrastructure at the bottom with power and cooling, compute infrastructure in the middle with thousands of interconnected accelerators, and orchestration software at the top represented as an abstract control plane. Visual elements include resource managers allocating workloads, capacity graphs showing utilization, and geographic connections to other datacenters. The color scheme uses industrial grays and silvers with accent colors of electric blue for networking and amber for active computation. Photorealistic technical illustration style suitable for infrastructure engineering documentation._
:::

<!-- TODO: Add cover image when generated -->
<!-- ![](images/png/cover_infrastructure.png) -->

:::

## Purpose {.unnumbered}

_Why does the ability to build and manage computational infrastructure determine which organizations can realize their most ambitious machine learning goals?_

Machine learning systems that transform industries operate on infrastructure far exceeding the scale of single machines or small clusters. Training a frontier model may require thousands of GPUs coordinated across multiple datacenters, each machine contributing to a unified computation that can span weeks or months. Managing such infrastructure demands expertise in datacenter design, high-bandwidth networking, and distributed systems orchestration. The infrastructure must satisfy competing requirements: computational efficiency alongside fault tolerance, coordination across thousands of machines without prohibitive communication overhead, and dynamic capacity provisioning that controls costs. These challenges have become central to machine learning advancement, as organizations cannot access frontier capabilities without mastering the physical and software systems that make large-scale computation possible. Understanding infrastructure architecture is essential for building systems beyond prototype experiments, shaping whether organizations deploy efficiently, scale reliably, and compete effectively in an increasingly infrastructure-dependent landscape.

## Coming 2026

This chapter will cover datacenter architecture, cluster design, and resource management for ML workloads.
