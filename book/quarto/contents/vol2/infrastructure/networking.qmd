---
---

# Networking {#sec-networking}

::: {layout-narrow}
:::.column-margin
_DALL·E 3 Prompt: A visualization of a high-speed optical network connecting thousands of GPU nodes. The image focuses on the interconnect fabric, showing layers of switches in a fat-tree topology with glowing data packets moving between racks. Above the physical network, a digital control plane represents the scheduler, optimizing traffic flow and job placement. The style is technical and schematic, emphasizing connectivity and coordination._
:::

\noindent
![](images/png/cover_infrastructure.png){fig-alt=""}

:::

## Purpose {.unnumbered}

_Why does the network connecting accelerators matter more than the accelerators themselves at scale?_

A single GPU can perform trillions of operations per second, but distributed training requires those operations to coordinate across thousands of devices. Every synchronization point—gradient averaging, activation exchange, parameter update—depends on network bandwidth and latency. When network capacity cannot keep pace with accelerator throughput, GPUs sit idle waiting for data to arrive, and adding more GPUs makes the problem worse rather than better. At sufficient scale, network design dominates system performance: the topology determines which communication patterns are efficient, the bandwidth determines how large models can be partitioned, and the latency determines how tightly coupled training can be. Organizations that treat networking as an afterthought discover that their expensive accelerators deliver a fraction of theoretical performance because the network became the bottleneck nobody planned for.

::: {.callout-tip title="Learning Objectives"}

- Compare network topologies (Fat-Tree, Torus, Rail-Optimized) in terms of bisection bandwidth and latency trade-offs
- Analyze the impact of tail latency on distributed training synchronization
- Design resource allocation policies for multi-tenant GPU clusters using Slurm and Kubernetes
- Evaluate the trade-offs between gang scheduling and elastic scaling for training workloads
- Explain how congestion control algorithms like DCQCN prevent packet loss in high-speed Ethernet fabrics

:::

In the **Systems Sandwich** (@sec-vol2-introduction), Networking is the **Gradient Bus**—the high-speed interconnect that binds thousands of accelerators into a single logical computer. If the **Iron Law** defines the speed limit as $L = (\text{Data} + \text{Compute}) / \text{Bandwidth}$, this chapter is about maximizing that **Bandwidth** term.

The Introduction established that communication becomes dominant at scale. The networking fabric connecting accelerators determines whether that communication enables near-linear scaling or collapses the system into communication-bound inefficiency. While @sec-compute examined intra-node connectivity through NVLink and NVSwitch, this chapter extends to the datacenter-scale networks that enable training across hundreds or thousands of accelerators.

## The Physics of the Wire: Signal Integrity and Optics {#sec-networking-physics-of-wire}

Before analyzing protocols and topologies, we must understand the physical medium. At 400 Gbps and beyond, the physics of signal transmission imposes hard limits on cable length, power consumption, and error rates.

### Signal Integrity and PAM4 {#sec-networking-pam4}

To achieve 400 Gbps (and soon 800 Gbps/1.6 Tbps), we cannot simply toggle a voltage on and off faster. The signal attenuation in copper and the chromatic dispersion in fiber limit the symbol rate. Modern high-speed links use **PAM4 (Pulse Amplitude Modulation 4-level)**.

Instead of binary (0/1), PAM4 encodes two bits per symbol using four voltage levels (00, 01, 10, 11). This doubles the data rate for the same bandwidth but significantly reduces the Signal-to-Noise Ratio (SNR). The gap between voltage levels is smaller, making the link more susceptible to noise.

**The Consequence:** Modern high-speed links are lossy. They require **Forward Error Correction (FEC)** at the physical layer (Reed-Solomon codes) to recover from bit errors. This FEC adds **latency** (typically 100ns–200ns per hop) to every packet, contributing to the $\alpha$ term in our performance models.

### Copper vs. Optics {#sec-networking-copper-vs-optics}

*   **DAC (Direct Attach Copper)**: Passive copper cables. Cheapest and lowest latency, but limited to < 3 meters. Used exclusively within a rack (ToR switch to Server).
*   **AOC (Active Optical Cable)**: Fiber with transceivers permanently attached. Used for 3m–30m runs (Rack-to-Rack or Rack-to-Spine).
*   **Pluggable Optics**: Transceivers + Fiber. Used for long-distance datacenter runs.

These physical media differ not only in reach but also in cost and power, and these differences directly shape cluster geometry. The economic implications of distance are profound:

::: {.callout-perspective title="The Cost of Distance"}
In a Machine Learning Fleet, distance is money.

*   **Copper (Rack-Scale)**: $50/cable. 0W power. 0 latency.
*   **Optics (Row-Scale)**: $500/cable. 5W power. ~10ns latency.

This physics dictates the **Cluster Geometry**. We pack GPUs as densely as possible (70-100kW racks) not just to save floor space, but to maximize the use of cheap, fast copper and minimize expensive, power-hungry optics.
:::

## Protocols: InfiniBand vs. RoCE {#sec-networking-protocols}

Large-scale training requires sustained, synchronized bulk transfers. A single AllReduce operation across 1024 GPUs may move terabytes of gradient data. This pattern demands networks optimized for **Lossless Delivery** and **Remote Direct Memory Access (RDMA)**.

### The Need for RDMA {#sec-networking-rdma-need}

Standard TCP/IP is CPU-intensive. Processing a 400 Gbps stream through the Linux kernel would consume multiple CPU cores just for interrupt handling and buffer copying. **RDMA** bypasses the CPU entirely, allowing the Network Interface Card (NIC) to write directly into the application's (or GPU's) memory.

### InfiniBand (Native RDMA) {#sec-networking-infiniband}

InfiniBand[^fn-infiniband] is a network architecture designed from the ground up for HPC. It uses credit-based flow control at the hardware level to ensure **lossless** transmission—a switch will never drop a packet due to buffer overflow; it simply stops sending credits to the upstream sender.

[^fn-infiniband]: **InfiniBand**: A high-speed interconnect standard. Unlike Ethernet, it provides guaranteed delivery with hardware-based reliability. Current NDR (Next Data Rate) generation provides 400 Gb/s per port.

*   **Pros**: Native lossless, ultra-low latency (~500ns switch hop), mature ecosystem (Subnet Manager).
*   **Cons**: Expensive, requires specialized switches, distinct from standard datacenter Ethernet.

### RoCE (RDMA over Converged Ethernet) {#sec-networking-roce}

RoCE[^fn-roce] implements RDMA semantics over standard Ethernet frames. It wraps the InfiniBand transport packet in an Ethernet/UDP header.

[^fn-roce]: **RoCE**: RDMA over Converged Ethernet. Uses UDP encapsulation. Requires a lossless Ethernet fabric (PFC/ECN) to function efficiently.

*   **Pros**: Uses commodity Ethernet switches, shares infrastructure with other traffic.
*   **Cons**: Ethernet is inherently lossy. Dropping an RDMA packet forces a costly "Go-Back-N" retransmission, destroying tail latency. Making Ethernet "lossless" requires complex configuration (PFC/ECN).

@fig-ib-roce-stack compares the two protocol stacks side by side, highlighting that both expose the same Verbs API to applications despite their fundamentally different transport layers.

::: {#fig-ib-roce-stack fig-env="figure" fig-pos="htb" fig-cap="**High-Performance Networking Stacks**. Comparison of InfiniBand and RoCE protocol stacks. InfiniBand uses a native lossless fabric, while RoCE encapsulates RDMA traffic within UDP/IP packets. Both expose the same Verbs API to applications, but RoCE relies on Priority Flow Control (PFC) in the Ethernet layer to approximate InfiniBand's lossless guarantees." fig-alt="Two protocol stacks side by side. Left: InfiniBand with 5 native layers. Right: RoCEv2 with IB transport over UDP/IP and Ethernet. Orange arrows show kernel bypass path on both. Dashed lines connect shared Verbs API and IB Transport layers."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{Orchid}{RGB}{218,112,214}
  \definecolor{Slate}{RGB}{112,128,144}
  \definecolor{OrangeLine}{RGB}{255,140,0}

  \tikzset{
    layer/.style={draw=black!70, fill=white, minimum width=3cm, minimum height=0.7cm, font=\sffamily\footnotesize},
    header/.style={font=\bfseries\sffamily, align=center}
  }

  % InfiniBand Stack
  \begin{scope}[local bounding box=IB]
    \node[header] at (0, 5) {InfiniBand};
    \node[layer, fill=Orchid!20] (ib_verbs) at (0, 4) {Verbs API};
    \node[layer, fill=Orchid!10] (ib_trans) at (0, 3) {IB Transport};
    \node[layer, fill=Orchid!10] (ib_net) at (0, 2) {IB Network};
    \node[layer, fill=Orchid!10] (ib_link) at (0, 1) {IB Link};
    \node[layer, fill=Orchid!10] (ib_phy) at (0, 0) {IB Physical};

    % RDMA bypass arrow
    \draw[->, thick, OrangeLine] (-1.8, 4.3) -- (-1.8, 0.5) node[midway, left, align=center, font=\scriptsize] {Kernel\\Bypass};
  \end{scope}

  % RoCE Stack
  \begin{scope}[shift={(5,0)}, local bounding box=RoCE]
    \node[header] at (0, 5) {RoCEv2};
    \node[layer, fill=Orchid!20] (roce_verbs) at (0, 4) {Verbs API};
    \node[layer, fill=Orchid!10] (roce_trans) at (0, 3) {IB Transport};
    \node[layer, fill=Slate!10] (roce_udp) at (0, 2) {UDP / IP};
    \node[layer, fill=Slate!10] (roce_eth) at (0, 1) {Ethernet Link (PFC)};
    \node[layer, fill=Slate!10] (roce_phy) at (0, 0) {Ethernet Physical};

    % RDMA bypass arrow
    \draw[->, thick, OrangeLine] (1.8, 4.3) -- (1.8, 0.5) node[midway, right, align=center, font=\scriptsize] {Kernel\\Bypass};
  \end{scope}

  % Connectors
  \draw[dashed, gray] (ib_verbs) -- (roce_verbs);
  \draw[dashed, gray] (ib_trans) -- (roce_trans);

\end{tikzpicture}
```
:::

## Network Topology Design {#sec-networking-network-topology-design-0f1e}

The physical arrangement of switches determines the **Bisection Bandwidth**: the worst-case bandwidth available when the cluster is split in half.

### Fat-Tree (Clos) Topology {#sec-networking-fat-tree}

The standard for general-purpose clusters. It uses multiple layers of switches (Leaf, Spine, Core) to provide non-blocking connectivity.
*   **Properties**: Can be built with any radix switch. Full bisection bandwidth if configured with 1:1 oversubscription.
*   **Bandwidth Calculation**: For a 3-tier tree with radix $k$ switches:
    $$ N_{hosts} = k^3 / 4 $$
    With $k=64$ (NDR InfiniBand), a 3-tier tree supports 65,536 ports.

### Rail-Optimized Topology {#sec-networking-rail-optimized}

Optimized for **Tensor Parallelism**. In TP, GPUs of the same rank (e.g., all GPU-0s) talk frequently. Rail-optimized networks connect all GPU-0s to the same leaf switch, all GPU-1s to a second leaf switch, etc.
*   **Benefit**: Minimizes hop count for the most frequent communication patterns.
*   **Risk**: If a "rail" switch fails, the entire rail (1/8th of the cluster) is partitioned.

@fig-network-topologies illustrates these three topology families and their structural differences.

::: {#fig-network-topologies fig-env="figure" fig-pos="htb" fig-cap="**Network Topologies for ML**. (A) Fat-Tree provides full bisection bandwidth. (B) Torus connects neighbors, optimizing for local patterns. (C) Rail-Optimized designs prioritize dedicated paths between corresponding GPUs across nodes." fig-alt="Three network topology diagrams. A shows hierarchical fat-tree with switch layers. B shows 2D torus grid with wraparound connections. C shows rail-optimized with direct GPU-to-GPU paths across nodes."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{NodeColor}{RGB}{200,200,200}
  \definecolor{SwitchColor}{RGB}{100,150,200}

  \tikzset{
    switch/.style={circle, fill=SwitchColor, draw=black!50, inner sep=2pt, minimum size=0.4cm},
    node/.style={rectangle, fill=NodeColor, draw=black!50, inner sep=2pt, minimum size=0.4cm}
  }

  % Fat Tree
  \begin{scope}
    \node[anchor=south] at (2, 3.5) {\textbf{A. Leaf-Spine (Fat-Tree)}};
    % Spine
    \foreach \x in {0.5, 1.5, 2.5, 3.5} \node[switch] (s\x) at (\x, 3) {};
    % Leaf
    \foreach \x in {0, 1, 3, 4} \node[switch] (l\x) at (\x, 1.5) {};
    % Nodes
    \foreach \x in {0, 0.5, 1, 1.5} \node[node] (n\x) at (\x-0.25, 0) {};
    \foreach \x in {3, 3.5, 4, 4.5} \node[node] (nm\x) at (\x-0.25, 0) {};

    % Connections
    \foreach \s in {0.5, 1.5, 2.5, 3.5} {
        \draw[gray, thin] (s\s) -- (l0);
        \draw[gray, thin] (s\s) -- (l1);
        \draw[gray, thin] (s\s) -- (l3);
        \draw[gray, thin] (s\s) -- (l4);
    }
    \draw[gray] (l0) -- (n0); \draw[gray] (l0) -- (n0.5);
    \draw[gray] (l1) -- (n1); \draw[gray] (l1) -- (n1.5);
  \end{scope}

  % Rail Optimized
  \begin{scope}[shift={(6,0)}]
    \node[anchor=south] at (1.5, 3.5) {\textbf{C. Rail-Optimized}};

    % Rail Switches
    \foreach \x in {0, 1, 2, 3} \node[switch, fill=orange!50] (rs\x) at (\x, 3) {R\x};

    % Nodes
    \node[draw, fit={(0,-0.5) (3, 0.5)}, inner sep=4pt] (host1) {};
    \node[anchor=west] at (host1.west) {Node 1};

    \node[draw, fit={(0,-2.0) (3, -1.0)}, inner sep=4pt] (host2) {};
    \node[anchor=west] at (host2.west) {Node 2};

    % GPUs
    \foreach \x in {0, 1, 2, 3} {
        \node[node, fill=violet!30] (g1\x) at (\x, 0) {};
        \node[node, fill=violet!30] (g2\x) at (\x, -1.5) {};

        \draw[thick, orange] (g1\x) -- (rs\x);
        \draw[thick, orange] (g2\x) -- (rs\x);
    }
  \end{scope}
\end{tikzpicture}
```
:::

The topology choice has quantitative consequences. To see why oversubscription is dangerous for ML workloads, consider what happens when bisection bandwidth is constrained, a scenario we call the *bisection bottleneck*.

::: {.callout-notebook title="The Bisection Bottleneck"}
**Problem**: You have **1,024 GPUs** (128 nodes). Each node has **400 GB/s** injection bandwidth. You run an AllReduce job that requires full bisection bandwidth.

*   **Scenario A (Non-Blocking Fat-Tree)**: 1:1 subscription. Bisection BW = $128 \times 400 = \mathbf{51.2 \text{ TB/s}}$.
*   **Scenario B (Cost-Optimized)**: 4:1 oversubscription at the spine. Bisection BW = $51.2 / 4 = \mathbf{12.8 \text{ TB/s}}$.

**The Math**:

*   Job requires exchanging **100 TB** of gradients.
*   **Time on A**: $100 \text{ TB} / 51.2 \text{ TB/s} \approx \mathbf{2 \text{ seconds}}$.
*   **Time on B**: $100 \text{ TB} / 12.8 \text{ TB/s} \approx \mathbf{8 \text{ seconds}}$.

**The Systems Conclusion**: Saving money on spine switches (Scenario B) slows down your entire \$300M supercomputer by **4x** during communication phases. For training workloads, **network oversubscription is false economy**.
:::

## Congestion Control: The Silent Killer {#sec-networking-congestion-control}

In a shared network, collisions are inevitable. When two packets target the same output port, one must wait in a buffer. If the buffer fills, the switch must either drop the packet (TCP) or pause the sender (Lossless/PFC).

### Priority Flow Control (PFC) {#sec-networking-pfc}
PFC creates a "Lossless" Ethernet. When a switch buffer fills, it sends a `PAUSE` frame to the upstream switch.
*   **The Danger**: **Congestion Spreading**. The pause can propagate upstream all the way to the source, blocking traffic that wasn't even destined for the congested link (Head-of-Line Blocking). In extreme cases, this causes a **PFC Storm** that deadlocks the entire fabric.

### DCQCN and ECN {#sec-networking-dcqcn}
Modern RoCE networks use **DCQCN (Data Center Quantized Congestion Notification)**.
1.  **Switch**: Detects queue buildup and marks packets with the **ECN (Explicit Congestion Notification)** bit.
2.  **Receiver**: Sees ECN, sends a CNP (Congestion Notification Packet) back to the sender.
3.  **Sender**: Reduces injection rate.

This control loop effectively throttles traffic *before* buffers overflow, preventing packet loss without the sledgehammer of PFC pauses.

## Summary {#sec-networking-summary-0172}

The high-bandwidth networks examined here are the nervous system of the machine learning fleet. They determine whether thousands of isolated GPUs function as a single coherent supercomputer or as a fragmented collection of servers.

We explored how InfiniBand and RoCE fabrics enable memory-to-memory transfer at 400+ Gbps, bypassing CPU bottlenecks to keep pace with accelerator throughput. We saw how topology optimization—fat-trees for general workloads, rail-optimized networks for large models, and torus meshes for TPUs—physically shapes the communication patterns available to distributed algorithms.

Key takeaways from this chapter:

::: {.callout-takeaways title="Key Takeaways"}
* **Network as Computer**: At scale, the interconnect is not just plumbing but a primary component of the compute engine. Bandwidth and topology determine training speed as much as GPU FLOPS.
* **Latency Matters**: For inference and small-message training, tail latency and protocol overhead dominate raw bandwidth.
* **Topology Choice is Workload-Dependent**: Fat-tree provides flexibility; rail-optimized and torus topologies provide efficiency for structured patterns.
:::

The next chapter, @sec-storage, examines the high-performance storage architectures—parallel filesystems, tiered caches, and checkpointing pipelines—that keep our massive compute fleet fed with data.
