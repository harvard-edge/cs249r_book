---
---

# Networking {#sec-cluster-networking}

::: {layout-narrow}
:::.column-margin
_DALL·E 3 Prompt: A visualization of a high-speed optical network connecting thousands of GPU nodes. The image focuses on the interconnect fabric, showing layers of switches in a fat-tree topology with glowing data packets moving between racks. Above the physical network, a digital control plane represents the scheduler, optimizing traffic flow and job placement. The style is technical and schematic, emphasizing connectivity and coordination._
:::

\noindent
![](images/png/cover_infrastructure.png)

:::

## Purpose {.unnumbered}

_How do we connect thousands of accelerators into a unified supercomputer and orchestrate workloads to utilize them efficiently?_

::: {.callout-tip title="Learning Objectives"}

- Compare network topologies (Fat-Tree, Torus, Rail-Optimized) in terms of bisection bandwidth and latency trade-offs
- Analyze the impact of tail latency on distributed training synchronization
- Design resource allocation policies for multi-tenant GPU clusters using Slurm and Kubernetes
- Evaluate the trade-offs between gang scheduling and elastic scaling for training workloads
- Explain how congestion control algorithms like DCQCN prevent packet loss in high-speed Ethernet fabrics

:::

## Networking for Large-Scale ML {#sec-networking-ml}

The Introduction established that communication becomes dominant at scale. The networking fabric connecting accelerators determines whether that communication enables near-linear scaling or collapses the system into communication-bound inefficiency. While @sec-infrastructure examined intra-node connectivity through NVLink and NVSwitch, this chapter extends to the datacenter-scale networks that enable training across hundreds or thousands of accelerators. The transition from intra-node to inter-node communication introduces fundamentally different constraints: where NVLink provides 900 GB/s between GPUs on a single baseboard, inter-node networks must traverse switches, cables, and protocol stacks that introduce both bandwidth limitations and latency penalties.

### Training Network Requirements

Large-scale training workloads impose unique demands on network infrastructure. Unlike traditional datacenter traffic patterns dominated by short flows and request-response interactions, distributed training generates sustained, synchronized bulk transfers. A single AllReduce operation across 1024 GPUs may move terabytes of gradient data, with all participants blocked until the collective completes. This pattern demands networks optimized for bandwidth rather than connection establishment latency.

#### High-Bandwidth Interconnects

InfiniBand[^fn-infiniband] has emerged as the dominant interconnect for ML training clusters due to its RDMA (Remote Direct Memory Access)[^fn-rdma] capabilities and consistent low latency [@infiniband2000spec]. The technology enables direct memory-to-memory transfers without CPU involvement, reducing both latency and processor overhead.

[^fn-infiniband]: **InfiniBand**: A high-speed interconnect standard originally developed for HPC, now dominant in ML clusters. Unlike Ethernet, InfiniBand provides guaranteed delivery with hardware-based reliability, eliminating software retransmission overhead. Current NDR (Next Data Rate) generation provides 400 Gb/s per port, with XDR (800 Gb/s) emerging.

[^fn-rdma]: **Remote Direct Memory Access (RDMA)**: A network capability that allows one machine to read from or write to another machine's memory without involving either CPU. RDMA bypasses the kernel network stack, reducing latency from tens of microseconds to sub-microsecond. For ML, RDMA enables GPUs to exchange gradient data with minimal CPU intervention, critical for overlapping communication with computation.

+-----------------------+---------------+-------------+-----------------------+
| **Generation**        | **Bandwidth** | **Latency** | **Common Deployment** |
+======================:+==============:+============:+:======================+
| **HDR (200 Gb/s)**    | 25 GB/s       | 0.6 μs      | Legacy clusters       |
+-----------------------+---------------+-------------+-----------------------+
| **HDR100 (100 Gb/s)** | 12.5 GB/s     | 0.6 μs      | Cost-optimized        |
+-----------------------+---------------+-------------+-----------------------+
| **NDR (400 Gb/s)**    | 50 GB/s       | 0.5 μs      | Current standard      |
+-----------------------+---------------+-------------+-----------------------+
| **NDR200 (200 Gb/s)** | 25 GB/s       | 0.5 μs      | Disaggregated         |
+-----------------------+---------------+-------------+-----------------------+
| **XDR (800 Gb/s)**    | 100 GB/s      | &lt; 0.5 μs | Emerging              |
+-----------------------+---------------+-------------+-----------------------+

::: {#fig-ib-roce-stack fig-env="figure" fig-pos="htb" fig-cap="**High-Performance Networking Stacks**. Comparison of InfiniBand and RoCE protocol stacks (see @fig-ib-roce-stack). InfiniBand uses a native lossless fabric, while RoCE encapsulates RDMA traffic within UDP/IP packets over Ethernet. Both expose the same Verbs API to applications, but RoCE relies on Priority Flow Control (PFC) in the Ethernet layer to approximate InfiniBand's lossless guarantees."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{Orchid}{RGB}{218,112,214}
  \definecolor{Slate}{RGB}{112,128,144}
  \definecolor{OrangeLine}{RGB}{255,140,0}

  \tikzset{
    layer/.style={draw=black!70, fill=white, minimum width=3cm, minimum height=0.7cm, font=\sffamily\footnotesize},
    header/.style={font=\bfseries\sffamily, align=center}
  }

  % InfiniBand Stack
  \begin{scope}[local bounding box=IB]
    \node[header] at (0, 5) {InfiniBand};
    \node[layer, fill=Orchid!20] (ib_verbs) at (0, 4) {Verbs API};
    \node[layer, fill=Orchid!10] (ib_trans) at (0, 3) {IB Transport};
    \node[layer, fill=Orchid!10] (ib_net) at (0, 2) {IB Network};
    \node[layer, fill=Orchid!10] (ib_link) at (0, 1) {IB Link};
    \node[layer, fill=Orchid!10] (ib_phy) at (0, 0) {IB Physical};

    % RDMA bypass arrow
    \draw[->, thick, OrangeLine] (-1.8, 4.3) -- (-1.8, 0.5) node[midway, left, align=center, font=\scriptsize] {Kernel\\Bypass};
  \end{scope}

  % RoCE Stack
  \begin{scope}[shift={(5,0)}, local bounding box=RoCE]
    \node[header] at (0, 5) {RoCEv2};
    \node[layer, fill=Orchid!20] (roce_verbs) at (0, 4) {Verbs API};
    \node[layer, fill=Orchid!10] (roce_trans) at (0, 3) {IB Transport};
    \node[layer, fill=Slate!10] (roce_udp) at (0, 2) {UDP / IP};
    \node[layer, fill=Slate!10] (roce_eth) at (0, 1) {Ethernet Link (PFC)};
    \node[layer, fill=Slate!10] (roce_phy) at (0, 0) {Ethernet Physical};

    % RDMA bypass arrow
    \draw[->, thick, OrangeLine] (1.8, 4.3) -- (1.8, 0.5) node[midway, right, align=center, font=\scriptsize] {Kernel\\Bypass};
  \end{scope}

  % Connectors
  \draw[dashed, gray] (ib_verbs) -- (roce_verbs);
  \draw[dashed, gray] (ib_trans) -- (roce_trans);

\end{tikzpicture}
```
:::

RDMA over Converged Ethernet[^fn-roce] provides an alternative that leverages existing Ethernet infrastructure. RoCEv2 operates over UDP/IP, enabling RDMA semantics across routed networks. While RoCE offers lower capital costs and operational familiarity, it requires careful configuration of Priority Flow Control and Explicit Congestion Notification to prevent packet loss. In ML workloads, even small packet loss rates cause significant performance degradation because collective operations must wait for retransmissions.

[^fn-roce]: **RoCE (RDMA over Converged Ethernet)**: A protocol that implements RDMA semantics over Ethernet networks. RoCEv2 uses UDP encapsulation for routability across L3 networks. RoCE reduces infrastructure costs by using commodity Ethernet switches but requires lossless Ethernet configuration (PFC/ECN) to achieve RDMA performance. Cloud providers typically offer RoCE-based networking (AWS EFA, Azure NDR) as a lower-cost alternative to InfiniBand.

The choice between InfiniBand and RoCE involves trade-offs beyond raw performance:

$$
\text{Effective Bandwidth} = \text{Link Rate} \times (1 - \text{Loss Rate}) \times \text{Protocol Efficiency}
$$

InfiniBand achieves protocol efficiencies above 95 percent, while RoCE typically operates at 85 to 92 percent depending on network congestion and flow control configuration. For a 400 Gb/s link, this difference translates to 47.5 GB/s versus 42.5 GB/s effective throughput, a gap that compounds across thousands of collective operations per training step.

Network interface cards for ML workloads increasingly integrate compute capabilities. NVIDIA's ConnectX-7 adapters include programmable engines for in-network aggregation, enabling switch-based gradient reduction that reduces traffic volumes. These SmartNICs offload collective operations from the GPU, overlapping communication with computation more effectively than software-only approaches.

#### Network Topology Design

The physical arrangement of switches and links fundamentally constrains distributed training performance. Fat-tree topologies[^fn-fat-tree] [@leiserson1985fattrees], derived from Clos network theory, provide full bisection bandwidth[^fn-bisection-bw]: any partition of the network can communicate at full link rate with the other half. For AllReduce operations that require all-to-all communication patterns, this property ensures no bottlenecks regardless of job placement.

[^fn-fat-tree]: **Fat-tree topology**: A hierarchical network structure where bandwidth increases toward the root, resembling a tree with thicker branches near the top. Unlike traditional trees where aggregation creates bottlenecks, fat-trees use multiple uplinks per switch to maintain bandwidth at each tier. The name comes from Charles Leiserson'1985 paper that applied this concept to parallel computing.

[^fn-bisection-bw]: **Bisection bandwidth**: The minimum bandwidth available when a network is divided into two equal halves. If bisection bandwidth equals the sum of all edge bandwidths, the network is non-blocking, meaning any communication pattern can proceed at full speed. This metric predicts worst-case performance for collective operations like AllReduce that involve all-to-all communication.

A three-tier fat-tree with radix-64 switches supports 65,536 endpoints while maintaining non-blocking connectivity. The bandwidth at each tier equals

$$
B_{\text{tier}} = \frac{k}{2} \times B_{\text{link}} \times N_{\text{switches}}
$$

where $k$ represents the switch radix and $N_{\text{switches}}$ represents the number of switches at that tier. For NDR InfiniBand with 64-port switches, each spine switch contributes 1.6 TB/s of bisection bandwidth.

Rail-optimized topologies offer an alternative for workloads dominated by tensor parallelism. In these designs, GPUs at the same position across multiple nodes connect through dedicated "rails" with minimal switch hops. An 8-rail design connects GPU 0 from each of 32 nodes through a single leaf switch, enabling efficient pipeline parallelism where activations flow between corresponding GPUs across nodes. This approach sacrifices the flexibility of fat-tree networks for reduced latency on structured communication patterns.

+-----------------------+------------------+--------------------+-------------------+
| **Topology**          | **Bisection BW** | **Latency (hops)** | **Best Workload** |
+:======================+:=================+===================:+:==================+
| **Fat-tree (3-tier)** | Full             | 4-6                | General/AllReduce |
+-----------------------+------------------+--------------------+-------------------+
| **Rail-optimized**    | Rail-limited     | 2                  | Tensor parallel   |
+-----------------------+------------------+--------------------+-------------------+
| **Dragonfly**         | Variable         | 3-5                | Large scale       |
+-----------------------+------------------+--------------------+-------------------+
| **Torus (TPU)**       | Dimension-based  | O(√N)              | Structured comms  |
+-----------------------+------------------+--------------------+-------------------+

::: {#fig-network-topologies fig-env="figure" fig-pos="htb" fig-cap="**Network Topologies for ML**. Visualizing three common interconnect architectures (@fig-network-topologies). (A) Fat-Tree provides full bisection bandwidth for general-purpose communication. (B) Torus (used in TPUs) connects neighbors in a grid/mesh, optimizing for local patterns. (C) Rail-Optimized designs prioritize dedicated paths between corresponding GPUs across nodes, minimizing switch hops for tensor parallelism."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{NodeColor}{RGB}{200,200,200}
  \definecolor{SwitchColor}{RGB}{100,150,200}

  \tikzset{
    switch/.style={circle, fill=SwitchColor, draw=black!50, inner sep=2pt, minimum size=0.4cm},
    node/.style={rectangle, fill=NodeColor, draw=black!50, inner sep=2pt, minimum size=0.4cm}
  }

  % Fat Tree
  \begin{scope}
    \node[anchor=south] at (2, 3.5) {\textbf{A. Leaf-Spine (Fat-Tree)}};
    % Spine
    \foreach \x in {0.5, 1.5, 2.5, 3.5} \node[switch] (s\x) at (\x, 3) {};
    % Leaf
    \foreach \x in {0, 1, 3, 4} \node[switch] (l\x) at (\x, 1.5) {};
    % Nodes
    \foreach \x in {0, 0.5, 1, 1.5} \node[node] (n\x) at (\x-0.25, 0) {};
    \foreach \x in {3, 3.5, 4, 4.5} \node[node] (nm\x) at (\x-0.25, 0) {};

    % Connections
    \foreach \s in {0.5, 1.5, 2.5, 3.5} {
        \draw[gray, thin] (s\s) -- (l0);
        \draw[gray, thin] (s\s) -- (l1);
        \draw[gray, thin] (s\s) -- (l3);
        \draw[gray, thin] (s\s) -- (l4);
    }
    \draw[gray] (l0) -- (n0); \draw[gray] (l0) -- (n0.5);
    \draw[gray] (l1) -- (n1); \draw[gray] (l1) -- (n1.5);
  \end{scope}

  % Torus
  \begin{scope}[shift={(6,0)}]
    \node[anchor=south] at (1.5, 3.5) {\textbf{B. 2D Torus / Mesh}};
    \foreach \x in {0, 1, 2, 3} {
        \foreach \y in {0, 1, 2, 3} {
            \node[node] (t\x\y) at (\x, \y) {};
        }
    }
    % Grid links
    \foreach \x in {0, 1, 2} {
        \foreach \y in {0, 1, 2, 3} {
             \draw[thick] (t\x\y) -- (t\the\numexpr\x+1\relax\y);
        }
    }
    \foreach \x in {0, 1, 2, 3} {
        \foreach \y in {0, 1, 2} {
             \draw[thick] (t\x\y) -- (t\x\the\numexpr\y+1\relax);
        }
    }
    % Torus wrap (dashed)
    \draw[dashed] (t30) to[bend right] (t00);
    \draw[dashed] (t03) to[bend left] (t00);
  \end{scope}

  % Rail Optimized
  \begin{scope}[shift={(11,0)}]
    \node[anchor=south] at (1.5, 3.5) {\textbf{C. Rail-Optimized}};

    % Rail Switches
    \foreach \x in {0, 1, 2, 3} \node[switch, fill=orange!50] (rs\x) at (\x, 3) {R\x};

    % Nodes (with multiple GPUs)
    \node[draw, fit={(0,-0.5) (3, 0.5)}, inner sep=4pt] (host1) {};
    \node[anchor=west] at (host1.west) {Node 1};

    \node[draw, fit={(0,-2.0) (3, -1.0)}, inner sep=4pt] (host2) {};
    \node[anchor=west] at (host2.west) {Node 2};

    % GPUs
    \foreach \x in {0, 1, 2, 3} {
        \node[node, fill=violet!30] (g1\x) at (\x, 0) {};
        \node[node, fill=violet!30] (g2\x) at (\x, -1.5) {};

        \draw[thick, orange] (g1\x) -- (rs\x);
        \draw[thick, orange] (g2\x) -- (rs\x);
    }

  \end{scope}

\end{tikzpicture}
```
:::

The distinction between non-blocking and oversubscribed networks carries significant implications for ML workloads. A 2-to-1 oversubscription ratio halves the effective bisection bandwidth, potentially doubling AllReduce time for large collectives. While oversubscription reduces infrastructure costs, the impact on training throughput often negates the savings. Most production ML clusters deploy non-blocking networks for training, reserving oversubscribed designs for serving traffic where request-response patterns tolerate contention.

#### Multi-Rack and Multi-Datacenter Training

Scaling beyond a single rack introduces inter-rack connectivity as a potential bottleneck. Even with non-blocking leaf-spine architectures, cable lengths increase from meters to tens of meters, adding propagation delay. More significantly, the spine layer becomes a shared resource across all training jobs, requiring careful traffic engineering to prevent interference.

Cross-datacenter training enables access to geographically distributed GPU resources but faces fundamental latency constraints. A 100 km fiber link introduces approximately 0.5 ms round-trip latency from propagation alone, before considering switch processing or protocol overhead. For synchronous training with tight AllReduce coupling, this latency directly extends iteration time:

$$
T_{\text{iteration}} = T_{\text{compute}} + T_{\text{comm}} + T_{\text{latency}} \times N_{\text{rounds}}
$$

Asynchronous methods like Local SGD reduce communication frequency but introduce staleness that affects convergence. Practical cross-datacenter training typically employs hierarchical aggregation. Workers synchronize within each datacenter, then datacenters exchange aggregated gradients at lower frequency.

Network partitions present a more severe challenge than performance degradation. When connectivity between datacenters fails, training jobs must either pause, wasting expensive GPU time, or continue with partial gradients, risking divergence. Partition-tolerant training algorithms remain an active research area, with approaches ranging from elastic data parallelism to speculative gradient accumulation. We examine fault tolerance mechanisms in detail in @sec-fault-tolerance.

### Serving Network Requirements

Inference serving demands different network characteristics than training. Where training optimizes for bulk throughput, serving prioritizes latency consistency across diverse request patterns. A recommendation model serving millions of queries per second cannot tolerate the tail latency variance acceptable in batch training.

#### Load Balancer Architectures

ML serving deployments require load balancers that understand model-specific traffic patterns. Layer 4 load balancers operate on TCP/UDP flows, distributing connections based on IP addresses and ports. They offer high throughput with minimal latency overhead but cannot inspect request content for intelligent routing.

Layer 7 load balancers parse application protocols, enabling routing decisions based on request characteristics. For ML serving, this enables routing requests to model versions, directing traffic based on input features, or implementing request coalescing for batch inference. The cost is increased latency, typically 0.5 to 2 ms per hop for TLS termination and HTTP parsing.

Consistent hashing provides session affinity for stateful inference scenarios. When serving autoregressive language models, subsequent tokens in a generation session should route to the same replica to reuse KV cache state. The hash function maps session identifiers to replicas:

$$
\text{replica} = \text{hash}(\text{session\_id}) \mod N_{\text{replicas}}
$$

Virtual nodes improve load distribution when replicas have heterogeneous capacity. Each physical replica appears multiple times in the hash ring proportional to its capacity, naturally directing more traffic to more capable instances.

Geographic load distribution becomes essential for global ML services. DNS-based global load balancing directs users to nearby deployments, reducing round-trip latency. However, model updates must propagate across all regions consistently, requiring coordination between deployment systems and traffic management.

#### Service Mesh for ML

Service mesh architectures insert proxy sidecars alongside ML services, enabling consistent observability and traffic management without application changes. For ML deployments, sidecars capture request latencies, model versions, and input characteristics that feed monitoring and debugging systems.

Traffic routing through service mesh enables sophisticated A/B testing beyond simple traffic splitting. Requests can route based on user segments, input features, or model confidence scores. A recommendation system might route uncertain predictions to an ensemble while serving confident predictions from a faster single model.

Circuit breaker patterns prevent cascade failures when model replicas become unhealthy. When error rates exceed thresholds, the circuit opens and redirects traffic to healthy replicas or fallback models. For ML serving, circuit breakers must account for model-specific health indicators. High latency might indicate GPU memory pressure rather than failure, warranting throttling rather than failover.

### Network Performance Analysis

Quantitative understanding of network performance enables informed decisions about infrastructure investment and training configurations. The interplay between model architecture, parallelism strategy, and network capability determines overall system efficiency.

#### Bandwidth Utilization Patterns

AllReduce[^fn-allreduce] bandwidth requirements scale with model size and parallelism configuration. For data parallelism with $N$ workers and model parameters $P$, each worker must send and receive approximately $2P$ bytes per iteration (assuming ring AllReduce [@patarasuk2009bandwidth]). The required bandwidth to hide communication behind computation is:

[^fn-allreduce]: **AllReduce**: A collective communication operation that synchronizes gradients across all workers in distributed training. The Introduction established AllReduce as a key synchronization primitive; @sec-communication examines the algorithms and their efficiency characteristics in detail.

$$
B_{\text{required}} = \frac{2P}{T_{\text{compute}}}
$$

For a 175B parameter model with FP16 gradients of 350 GB, achieving 50 percent compute utilization on hardware with 1 second compute time requires 700 GB/s aggregate bandwidth, far exceeding single-link capacity and motivating sophisticated parallelism strategies.

::: {.callout-warning}
## Practical AllReduce Efficiency

Theoretical bandwidth calculations assume ideal conditions. Production AllReduce operations achieve 60 to 80 percent of theoretical bandwidth due to multiple overheads. Each ring stage incurs 5 to 20 microseconds of fixed startup latency per chunk. CPU-GPU and GPU-NIC memory transfers add latency. The NCCL and Gloo software stack processing introduces protocol overhead. Shared fabric with concurrent jobs reduces effective bandwidth through network contention.

Well-tuned systems on dedicated InfiniBand fabric achieve 75 to 85 percent efficiency. Many deployments, particularly those using RoCE or shared networks, achieve only 40 to 60 percent. Always benchmark actual collective performance rather than relying on theoretical link rates.
:::

NCCL is the standard communication library for NVIDIA GPU clusters.[^fn-nccl]

[^fn-nccl]: **NCCL (NVIDIA Collective Communications Library)**: NVIDIA's optimized library for multi-GPU and multi-node collective operations. NCCL automatically selects algorithms based on topology (ring, tree, or hybrid) and handles GPU-to-GPU communication via NVLink, PCIe, or network. It achieves 85-95% of theoretical bandwidth on well-configured systems, making it the default communication backend for PyTorch and TensorFlow distributed training.

Gradient compression techniques, including sparsification and quantization, can reduce bandwidth requirements by 10 to 100 times for some models, trading computation and potential accuracy impact for reduced network utilization. These compression algorithms are examined in detail in @sec-communication.

Pipeline parallelism communication patterns differ fundamentally from data parallelism. Rather than bulk AllReduce, pipeline stages exchange activation tensors between adjacent stages. The communication volume depends on activation size rather than parameter count, favoring models with small intermediate representations.

#### Latency Analysis

Network latency accumulates from multiple sources, each contributing to collective operation time:

+-----------------------+---------------------+-----------------------------------+
| **Source**            | **Typical Latency** | **Mitigation**                    |
+:======================+====================:+:==================================+
| **Switch hop**        | 100-400 ns          | Topology optimization             |
+-----------------------+---------------------+-----------------------------------+
| **Cable propagation** | 5 ns/m              | Compact layout                    |
+-----------------------+---------------------+-----------------------------------+
| **NIC processing**    | 1-2 μs              | Hardware offload                  |
+-----------------------+---------------------+-----------------------------------+
| **NCCL software**     | 5-20 μs             | Kernel fusion, persistent kernels |
+-----------------------+---------------------+-----------------------------------+
| **Memory copy**       | Variable            | Zero-copy RDMA                    |
+-----------------------+---------------------+-----------------------------------+

For small messages, latency dominates over bandwidth. The crossover point where bandwidth becomes the limiting factor occurs at:

$$
M_{\text{crossover}} = \text{Latency} \times \text{Bandwidth}
$$

For a system with 5 microseconds latency and 50 GB/s bandwidth, messages smaller than 250 KB are latency-bound. This motivates message aggregation in collective implementations, batching small gradient tensors to amortize latency overhead.

Congestion control algorithms significantly impact performance under contention. Traditional TCP congestion control, designed for fairness across independent flows, performs poorly for synchronized ML traffic where all flows compete simultaneously. DCQCN (Data Center Quantized Congestion Notification)[^fn-dcqcn] [@zhu2015dcqcn] for RoCE and hardware-based credit flow control for InfiniBand provide faster response to congestion, reducing tail latency. NCCL implements topology-aware algorithms that schedule transfers to minimize contention, exploiting knowledge of collective patterns that general-purpose congestion control lacks. We examine these collective operation implementations in detail in @sec-communication.

[^fn-dcqcn]: **DCQCN (Data Center Quantized Congestion Notification)**: A congestion control algorithm designed for RoCE networks. DCQCN uses ECN (Explicit Congestion Notification) marks from switches to throttle senders before packet loss occurs. Unlike TCP's reactive approach, DCQCN responds in microseconds, critical for the synchronized traffic patterns in distributed training where a single slow flow can delay all workers.

## Summary

The high-bandwidth networks examined here are the nervous system of the machine learning fleet. They determine whether thousands of isolated GPUs function as a single coherent supercomputer or as a fragmented collection of servers.

We explored how InfiniBand and RoCE fabrics enable memory-to-memory transfer at 400+ Gbps, bypassing CPU bottlenecks to keep pace with accelerator throughput. We saw how topology optimization—fat-trees for general workloads, rail-optimized networks for large models, and torus meshes for TPUs—physically shapes the communication patterns available to distributed algorithms.

::: {.callout-important title="Key Takeaways"}

* **Network as Computer**: At scale, the interconnect is not just plumbing but a primary component of the compute engine. Bandwidth and topology determine training speed as much as GPU FLOPS.
* **Latency Matters**: For inference and small-message training, tail latency and protocol overhead dominate raw bandwidth.
* **Topology Choice is Workload-Dependent**: Fat-tree provides flexibility; rail-optimized and torus topologies provide efficiency for structured patterns.

:::

With the fleet built and the engines connected, we have established the compute and communication capacity of the Machine Learning Fleet. However, these systems require a constant stream of data to remain productive.

The next chapter, @sec-storage, examines the high-performance storage architectures—parallel filesystems, tiered caches, and checkpointing pipelines—that keep our massive compute fleet fed with data.
