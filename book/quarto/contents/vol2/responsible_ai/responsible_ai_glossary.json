{
  "metadata": {
    "chapter": "responsible_ai",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.501964",
    "total_terms": 36,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.290168"
  },
  "terms": [
    {
      "term": "accountability",
      "definition": "The mechanisms by which individuals or organizations are held responsible for the outcomes of AI systems, involving traceability, documentation, auditing, and the ability to remedy harms.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial example",
      "definition": "Carefully crafted inputs that are nearly identical to legitimate inputs but cause machine learning models to make incorrect predictions with high confidence.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial training",
      "definition": "A technique for improving model robustness by training on adversarial examples, helping models learn more stable decision boundaries.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "algorithmic fairness",
      "definition": "The principle that automated systems should not disproportionately disadvantage individuals or groups based on protected attributes such as race, gender, or age.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "artificial intelligence",
      "definition": "Computer systems designed to perform tasks that typically require human intelligence, including learning, reasoning, perception, and decision-making.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "automation bias",
      "definition": "The tendency for humans to over-rely on automated system outputs even when clear errors are present, potentially compromising human oversight.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "concept bottleneck models",
      "definition": "Neural network architectures that first predict interpretable intermediate concepts before making final predictions, combining deep learning power with transparency.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "counterfactual explanations",
      "definition": "Explanations that describe how a model's output would change if specific input features were modified, particularly useful for understanding decision boundaries.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "demographic parity",
      "definition": "A fairness criterion requiring that the probability of receiving a positive prediction is independent of group membership across protected attributes.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "differential privacy",
      "definition": "A mathematical framework that provides formal guarantees about individual privacy by adding calibrated noise to computations or outputs.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distribution shift",
      "definition": "The mismatch between training data and real-world deployment conditions that can degrade model performance and reliability over time.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "equality of opportunity",
      "definition": "A fairness criterion focused on ensuring equal true positive rates across groups, guaranteeing that qualified individuals are treated equally regardless of group membership.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "equalized odds",
      "definition": "A fairness definition requiring that true positive and false positive rates are equal across different demographic groups.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "explainability",
      "definition": "The ability of stakeholders to understand how a machine learning model produces its outputs through post-hoc explanation techniques.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fairness constraints",
      "definition": "Technical and policy restrictions designed to ensure equitable treatment across demographic groups in machine learning systems.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated learning",
      "definition": "A distributed machine learning approach where models are trained across decentralized data sources without centralizing raw data.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gdpr",
      "definition": "The General Data Protection Regulation, European Union legislation that requires meaningful information about automated decision-making logic and grants individuals rights over their data.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "human oversight",
      "definition": "The principle that human judgment should supervise, correct, or halt automated decisions, maintaining meaningful human control over AI systems.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "interpretability",
      "definition": "The degree to which humans can understand the reasoning behind a machine learning model's predictions, often referring to inherently transparent models.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning lifecycle",
      "definition": "The complete process of developing, deploying, and maintaining ML systems, from data collection through model retirement.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine unlearning",
      "definition": "Techniques for removing the influence of specific data points from trained models without complete retraining, supporting data deletion rights.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "membership inference attacks",
      "definition": "Privacy attacks that attempt to determine whether a specific data point was included in a model's training set by analyzing model behavior.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model cards",
      "definition": "Documentation framework that provides structured information about machine learning models, including intended use, performance characteristics, and limitations.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multicalibration",
      "definition": "A fairness technique ensuring that model predictions remain calibrated across intersecting subgroups, addressing complex demographic interactions.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "post-hoc explanations",
      "definition": "Explanation methods applied after model training that treat the model as a black box and infer reasoning patterns from input-output behavior.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "privacy-preserving techniques",
      "definition": "Methods designed to protect individual privacy in machine learning, including differential privacy, federated learning, and local processing.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "responsible ai",
      "definition": "The development and deployment of machine learning systems that explicitly uphold ethical principles, minimize harm, and promote socially beneficial outcomes.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "reward hacking",
      "definition": "The phenomenon where AI systems exploit unintended aspects of reward functions to maximize scores while violating the intended objectives.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "robustness",
      "definition": "A model's ability to maintain stable and consistent performance under input variations, environmental changes, or adversarial conditions.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "specification gaming",
      "definition": "When AI systems find unexpected ways to achieve high rewards that technically satisfy the objective function but violate the intended purpose.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transparency",
      "definition": "Openness about how AI systems are built, trained, validated, and deployed, including disclosure of data sources, design assumptions, and limitations.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "value alignment",
      "definition": "The principle that AI systems should pursue goals consistent with human intent and ethical norms, addressing the challenge of encoding human values in machine objectives.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "value-sensitive design",
      "definition": "A methodology for incorporating human values into technology design through systematic stakeholder engagement and ethical consideration of system impacts.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bias detection",
      "definition": "Systematic methods for identifying unfair discrimination or disparate treatment across different demographic groups in machine learning system outputs.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bias mitigation",
      "definition": "Techniques and interventions designed to reduce unfair discrimination in machine learning systems, applied during data collection, model training, or post-processing stages.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "governance frameworks",
      "definition": "Structured approaches for managing responsible AI development including policies, procedures, oversight mechanisms, and accountability structures.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    }
  ]
}
