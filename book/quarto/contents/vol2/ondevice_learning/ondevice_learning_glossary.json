{
  "metadata": {
    "chapter": "ondevice_learning",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.499787",
    "total_terms": 35,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.288695"
  },
  "terms": [
    {
      "term": "adapter modules",
      "definition": "Small trainable neural network components inserted between frozen layers of a pretrained model to enable lightweight adaptation without modifying the base architecture.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "backpropagation",
      "definition": "The algorithm for computing gradients in neural networks by propagating error signals backward through layers, essential for training but computationally expensive on resource-constrained devices.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bias-only adaptation",
      "definition": "A lightweight training strategy that freezes all model weights and updates only scalar bias terms, drastically reducing memory requirements and computational overhead for on-device learning.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "catastrophic forgetting",
      "definition": "The phenomenon where neural networks lose previously learned knowledge when adapting to new tasks, a critical challenge in continual on-device learning scenarios.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "client scheduling",
      "definition": "The process of selecting which devices participate in federated learning rounds based on availability, data quality, and resource constraints to ensure representative model updates.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "continual learning",
      "definition": "The ability of machine learning models to learn new tasks sequentially without forgetting previously acquired knowledge, essential for adaptive on-device systems.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data compression",
      "definition": "Techniques for reducing the size and complexity of training data through encoding, quantization, or feature extraction to enable efficient storage and processing on memory-constrained devices.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "depthwise separable convolutions",
      "definition": "A computational technique that decomposes standard convolutions into depthwise and pointwise operations, reducing parameters and computation by 8-9x for mobile-optimized architectures.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "differential privacy",
      "definition": "A mathematical framework for quantifying and limiting privacy leakage by adding calibrated noise to data or model updates, commonly used in federated learning.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge computing",
      "definition": "A distributed computing paradigm that brings computation and data storage closer to data sources, enabling real-time processing with reduced latency and bandwidth requirements.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "experience replay",
      "definition": "A memory-based technique that stores past training examples in a buffer to prevent catastrophic forgetting and stabilize learning in streaming or continual adaptation scenarios.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated averaging",
      "definition": "The standard algorithm for federated learning where client model updates are aggregated using weighted averaging based on local dataset sizes to produce a global model.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated learning",
      "definition": "A distributed machine learning approach where models are trained across multiple devices using local data, with only model updates shared rather than raw data to preserve privacy.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "few-shot learning",
      "definition": "A machine learning paradigm that enables models to adapt to new tasks using only a small number of labeled examples, critical for data-sparse on-device scenarios.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gdpr",
      "definition": "The General Data Protection Regulation, a European Union law that imposes strict requirements on personal data processing and significantly influences privacy-preserving machine learning design.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient descent",
      "definition": "An optimization algorithm that iteratively updates model parameters by moving in the direction opposite to the gradient of the loss function, fundamental to neural network training.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "knowledge distillation",
      "definition": "A technique for transferring knowledge from a larger teacher model to a smaller student model, enabling deployment of compact models on resource-constrained devices.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "low-rank adaptation",
      "definition": "A parameter-efficient fine-tuning method that approximates weight updates using low-rank matrices, reducing trainable parameters while maintaining adaptation capability.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "meta-learning",
      "definition": "The process of learning how to learn, where models are trained to quickly adapt to new tasks with minimal data, particularly useful for personalization in on-device systems.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobilenet",
      "definition": "A family of efficient neural network architectures designed for mobile devices using depthwise separable convolutions to achieve significant reductions in model size and computation.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model compression",
      "definition": "Techniques for reducing model size and computational requirements through methods like quantization, pruning, and knowledge distillation to enable deployment on edge devices.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural engine",
      "definition": "Specialized hardware accelerators designed for machine learning inference and training, such as Apple's Neural Engine or Google's Edge TPU, optimized for on-device AI workloads.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "non-iid data",
      "definition": "Non-independent and identically distributed data where samples are not uniformly distributed across devices or time, creating challenges for federated learning convergence and generalization.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "on-device learning",
      "definition": "The local adaptation or training of machine learning models directly on deployed hardware devices without reliance on continuous connectivity to centralized servers.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "personalization layers",
      "definition": "Model components, typically the final classification layers, that are adapted locally to user-specific data while keeping shared backbone layers frozen.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pruning",
      "definition": "A model compression technique that removes unnecessary connections or neurons from neural networks to reduce model size and computational requirements without significantly impacting performance.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "quantization",
      "definition": "The process of reducing the precision of model weights and activations from floating-point to lower-bit representations to decrease memory usage and accelerate computation on edge devices.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "secure aggregation",
      "definition": "A cryptographic protocol that enables federated learning servers to compute aggregate model updates without accessing individual client contributions, enhancing privacy protection.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sparse updates",
      "definition": "A training strategy that selectively updates only a subset of model parameters based on their importance or contribution to performance, reducing computational and memory overhead.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tinyml",
      "definition": "A field focused on deploying machine learning models on microcontrollers and extremely resource-constrained devices with kilobytes of memory and milliwatts of power consumption.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transfer learning",
      "definition": "A machine learning technique that leverages knowledge from a pretrained model on one task to improve learning on a related task, enabling efficient adaptation with limited data.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight freezing",
      "definition": "A technique that fixes most model parameters during training while allowing only specific layers or components to be updated, reducing computational requirements for on-device adaptation.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge training",
      "definition": "The process of training or fine-tuning machine learning models directly on edge devices, enabling personalization and adaptation without requiring data transmission to cloud servers.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware constraint optimization",
      "definition": "Techniques for adapting ML algorithms and models to work within the memory, compute, and power limitations of mobile and embedded devices.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobile-optimized architectures",
      "definition": "Neural network designs specifically created for mobile deployment, emphasizing parameter efficiency, computational speed, and energy conservation.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    }
  ]
}
