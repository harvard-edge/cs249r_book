# Principles of Distributed Scale {.unnumbered}

These principles govern the coordination of multiple learners. They define the limits of parallelism and why simply adding more GPUs does not always yield linear speedups.

::: {.callout-note icon=false title="The Distributed Step Time Law"}
**The Law**: The time to complete one training step is the sum of computation and non-overlapped communication.
$$ T_{step}(N) = \frac{T_{compute}}{N} + T_{comm}(N) - T_{overlap} $$

**The Engineering Implication**:
Scaling is a race between parallelizable compute (which shrinks with $N$) and sequential communication overhead (which grows or stays constant). To scale efficiently, algorithms must minimize $T_{comm}$ or maximize $T_{overlap}$ (e.g., through **Gradient Accumulation**).
:::

::: {.callout-note icon=false title="Amdahl’s Law for Clusters"}
**The Law**: The maximum speedup of a distributed workload is strictly limited by the sequential fraction of the task—typically the gradient synchronization step.

**The Engineering Implication**:
Once communication dominates, adding hardware provides diminishing returns. Breaking this limit requires algorithmic changes: increasing batch size to reduce sync frequency, or using **Asynchronous SGD** to break the sequential dependency.
:::

::: {.callout-note icon=false title="The Bandwidth-Latency Trade-off ($\alpha$-$\beta$ Model)"}
**The Law**: Communication time is a function of fixed latency (\alpha) and message-dependent bandwidth (\beta).
$$ T(n) = \alpha + \frac{n}{\beta} $$

**The Engineering Implication**:
Small messages (e.g., MoE routing, pipeline bubbles) are **latency-bound**; large messages (e.g., gradients) are **bandwidth-bound**. Optimization strategies must match the regime: fuse small messages to amortize $\alpha$, compress large messages to improve $\beta$.
:::

::: {.callout-note icon=false title="The Young-Daly Checkpoint Law"}
**The Law**: The optimal checkpoint frequency balances the cost of writing the checkpoint against the expected cost of reworking lost progress.
$$ \tau_{opt} = \sqrt{2 \cdot T_{write} \cdot \text{MTBF}} $$

**The Engineering Implication**:
Checkpointing is not "free." Writing too often wastes I/O bandwidth; writing too rarely risks catastrophic loss. As cluster size grows, MTBF drops, forcing more frequent checkpoints and demanding higher-bandwidth storage systems.
:::
