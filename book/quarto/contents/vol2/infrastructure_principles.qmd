# Principles of Infrastructure {.unnumbered}

These principles define the physical constraints of the machine learning fleet. They explain why datacenter design is a first-order concern for AI scalability.

::: {.callout-note icon=false title="The Power Density Wall"}
**The Law**: Cluster scaling is constrained by thermal dissipation limits (Watts per rack) and cooling capacity, not just silicon area or floor space.

**The Engineering Implication**:
Modern AI accelerators generate heat densities that exceed air cooling capabilities. **Liquid cooling** becomes a facility requirement, not an option, for large-scale training clusters.
:::

::: {.callout-note icon=false title="The Memory Capacity Gap"}
**The Law**: Model parameter growth ($10\times$/year) consistently outpaces GPU memory capacity growth ($2\times$/year).

**The Engineering Implication**:
Models no longer fit on single devices. Architectures must embrace **3D Parallelism** (splitting the model itself via Tensor and Pipeline Parallelism) as the default state, breaking the abstraction of the "single device."
:::

::: {.callout-note icon=false title="The CAP Theorem for Scheduling"}
**The Law**: A distributed scheduler cannot simultaneously guarantee strict **Consistency** (fairness), high **Availability** (throughput), and **Partition Tolerance**.

**The Engineering Implication**:
Production schedulers must make explicit trade-offs. **Kubernetes** favors availability (eventual consistency), making it robust for services. **Slurm** favors consistency (strict ordering), making it superior for tightly coupled training jobs.
:::
