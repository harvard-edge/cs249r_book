{
  "metadata": {
    "chapter": "robust_ai",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.502396",
    "total_terms": 57,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.290520"
  },
  "terms": [
    {
      "term": "adversarial attack",
      "definition": "A deliberate attempt to deceive machine learning models by crafting carefully designed inputs that cause incorrect predictions or behaviors while appearing benign to humans.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial example",
      "definition": "Maliciously crafted inputs designed to fool machine learning models into making incorrect predictions, often by adding imperceptible perturbations to legitimate data.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial training",
      "definition": "A defense technique that involves training models on adversarial examples to improve their robustness and ability to correctly classify adversarial inputs.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "anomaly detection",
      "definition": "The identification of patterns in data that do not conform to expected behavior, often used to detect outliers, faults, or malicious activities in systems.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "autoencoder",
      "definition": "A neural network architecture that learns compressed data representations by minimizing reconstruction error, commonly used for anomaly detection and dimensionality reduction.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "backdoor attack",
      "definition": "A type of data poisoning where hidden triggers are embedded in training data, causing models to behave maliciously when specific patterns are encountered during inference.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bayesian neural networks",
      "definition": "Neural networks that incorporate probability distributions over their weights, enabling uncertainty quantification in predictions and more robust decision making.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bit flip",
      "definition": "A hardware fault where a single bit in memory or a register unexpectedly changes its value from 0 to 1 or vice versa, potentially corrupting data or computations.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "built-in self-test (bist)",
      "definition": "Hardware testing mechanisms that allow components to test themselves for faults using dedicated circuitry and predefined test patterns.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "checkpoint and restart mechanisms",
      "definition": "Techniques that periodically save a program's state so it can resume from the last saved state after a failure, improving system resilience.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "combinational logic",
      "definition": "Digital logic circuits where the output depends only on the current input states, not any past states or memory elements.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "concept drift",
      "definition": "A change in the relationship between input features and target outputs over time, requiring model adaptation to maintain performance.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "continual learning",
      "definition": "The ability of machine learning models to learn continuously from new data distributions while retaining knowledge from previous distributions without catastrophic forgetting.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "covariate shift",
      "definition": "A type of distribution shift where the input distribution changes while the conditional relationship between inputs and outputs remains stable.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data poisoning",
      "definition": "An attack where training data is deliberately manipulated or corrupted to compromise model performance, behavior, or security during deployment.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data sanitization",
      "definition": "The process of deliberately and permanently removing or destroying data stored on memory devices to make it unrecoverable, ensuring data security.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "defensive distillation",
      "definition": "A technique that trains a student model to mimic a teacher model's behavior using soft labels, reducing sensitivity to adversarial perturbations.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distribution shift",
      "definition": "The phenomenon where data encountered during model deployment differs from the training distribution, potentially degrading model performance.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "double modular redundancy (dmr)",
      "definition": "A fault-tolerance technique where computations are duplicated across two independent systems to identify and correct errors through comparison.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dual-use dilemma",
      "definition": "The challenge of mitigating misuse of technology that has both positive and negative potential applications, particularly relevant in AI security.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "electromigration",
      "definition": "The movement of metal atoms in a conductor under the influence of an electric field, potentially causing permanent hardware faults over time.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ensemble methods",
      "definition": "Machine learning approaches that combine predictions from multiple models to improve accuracy, robustness, and reliability compared to individual models.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "error-correcting codes",
      "definition": "Methods used in data storage and transmission to detect and correct errors, improving system reliability and data integrity.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "f1 score",
      "definition": "A measure of model accuracy that combines precision and recall into a single metric, calculated as their harmonic mean.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fast gradient sign method (fgsm)",
      "definition": "A gradient-based adversarial attack that generates adversarial examples by adding small perturbations in the direction of the gradient.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fault tolerance",
      "definition": "The ability of a system to continue operating correctly even when some of its components fail or encounter errors.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "glitches",
      "definition": "Momentary deviations in voltage, current, or signal that can cause incorrect operation in digital systems and circuits.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware redundancy",
      "definition": "The duplication of critical hardware components to provide backup functionality and improve system reliability through voting mechanisms.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "heartbeat mechanisms",
      "definition": "Periodic signals sent between system components to monitor health and detect failures, enabling timely fault detection and recovery.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hot spares",
      "definition": "Backup components kept ready to instantaneously replace failing components without disrupting system operation, providing redundancy.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "huber loss",
      "definition": "A robust loss function used in regression that is less sensitive to outliers compared to squared error loss, improving training stability.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "intermittent faults",
      "definition": "Hardware faults that occur sporadically and unpredictably, appearing and disappearing without consistent patterns, making diagnosis challenging.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "label shift",
      "definition": "A type of distribution shift where the distribution of target labels changes while the conditional relationship between features and labels remains constant.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lookup table",
      "definition": "A data structure that replaces runtime computation with simpler array indexing operations, commonly used for performance optimization.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning operations (mlops)",
      "definition": "The practice of deploying and maintaining machine learning models in production reliably and efficiently through automated pipelines.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "minimax",
      "definition": "A decision-making strategy used in game theory that attempts to minimize the maximum possible loss in adversarial scenarios.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model uncertainty",
      "definition": "The inadequacy of a machine learning model to capture the full complexity of the underlying data-generating process, leading to prediction uncertainty.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "monte carlo dropout",
      "definition": "A technique that uses multiple forward passes with different dropout masks at inference time to estimate prediction uncertainty.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "oxide breakdown",
      "definition": "The failure of an oxide layer in transistors due to excessive electric field stress, causing permanent hardware faults.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "permanent faults",
      "definition": "Hardware defects that persist irreversibly until repair or component replacement, consistently affecting system behavior.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "principle of least privilege",
      "definition": "A security concept where users are given the minimum access levels necessary to complete their job functions, reducing security risks.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "regularization",
      "definition": "Methods used in machine learning to prevent overfitting by adding penalty terms to the loss function, constraining model complexity.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "robust ai",
      "definition": "The ability of artificial intelligence systems to maintain performance and reliability despite internal errors, external perturbations, and environmental changes.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scan chains",
      "definition": "Dedicated test paths in processors that provide access to internal registers and logic for comprehensive hardware testing and fault detection.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "silent data corruption (sdc)",
      "definition": "Undetected errors during computation or data transfer that propagate through system layers without triggering alerts, potentially compromising results.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "software fault",
      "definition": "Unintended behavior in software systems resulting from defects, bugs, or design oversights that can impair performance or compromise security.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stochastic computing",
      "definition": "Computing techniques that use random bits and probabilistic operations to perform arithmetic, potentially offering better fault tolerance than traditional methods.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stuck-at fault",
      "definition": "A permanent hardware fault where a signal line becomes fixed at a logical 0 or 1 regardless of input, causing incorrect computations.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "thermal stress",
      "definition": "Hardware degradation caused by repeated cycling through high and low temperatures, leading to material fatigue and potential failures.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transfer learning",
      "definition": "A technique that leverages knowledge gained from one domain to improve performance in another, helping models adapt to new distributions.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transient faults",
      "definition": "Temporary hardware faults that do not persist or cause permanent damage but can lead to incorrect computations if not handled properly.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "triple modular redundancy (tmr)",
      "definition": "A fault-tolerance technique where three instances of a computation are performed, with majority voting determining the correct result.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "watchdog timer",
      "definition": "A hardware component that monitors system execution and triggers recovery actions if the system becomes unresponsive or stuck.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "attack taxonomy",
      "definition": "Systematic classification of cybersecurity threats and adversarial attacks against ML systems, organizing threats by method, target, and impact to guide defense strategies.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distribution shift types",
      "definition": "Formal categorization of changes in data distributions including covariate shift, label shift, concept drift, and domain shift, each requiring specific adaptation techniques.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "robustness metrics",
      "definition": "Quantitative measures for evaluating model stability under various perturbations, including adversarial accuracy, certified robustness bounds, and performance under distribution shift.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "domain adaptation",
      "definition": "Machine learning techniques that enable models trained on one domain to perform well on a different but related domain, addressing distribution mismatch challenges.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    }
  ]
}
