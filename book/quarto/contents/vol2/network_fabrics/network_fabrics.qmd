---
engine: jupyter
---

# Network Fabrics {#sec-network-fabrics}

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_network_fabrics.png){fig-alt=""}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{100}{25}{15}{10}
\end{marginfigure}

_Why does the network connecting accelerators matter more than the accelerators themselves at scale?_

A single GPU can perform trillions of operations per second, but distributed training requires those operations to coordinate across thousands of devices. Every synchronization point, whether gradient averaging, activation exchange, or parameter update, depends on network bandwidth and latency. When network capacity cannot keep pace with accelerator throughput, GPUs sit idle waiting for data to arrive, and adding more GPUs makes the problem worse rather than better. At sufficient scale, network design dominates system performance: the topology determines which communication patterns are efficient, the bandwidth determines how large models can be partitioned, and the latency determines how tightly coupled training can be. Organizations that treat networking as an afterthought discover that their expensive accelerators deliver a fraction of theoretical performance because the network became the bottleneck nobody planned for.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Model network communication cost using the **$\alpha$-$\beta$ framework** and identify bandwidth-dominated versus latency-dominated regimes
- Compare **RDMA** transport protocols (**InfiniBand** and **RoCE**) in terms of latency, lossless guarantees, and operational complexity
- Analyze network topologies (**fat-tree**, **rail-optimized**, **dragonfly**) by computing **bisection bandwidth** and hop count for ML collective patterns
- Evaluate congestion control mechanisms (**PFC**, **DCQCN**, **HPCC**) and their impact on tail latency during distributed training
- Design network virtualization strategies for multi-tenant GPU clusters using **SR-IOV** and traffic isolation
- Diagnose network performance bottlenecks using RDMA counters, link-level telemetry, and bandwidth testing tools

:::

```{python}
#| label: network-fabrics-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ NETWORK FABRICS: CHAPTER-WIDE CONSTANTS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-network-fabrics-introduction and the bandwidth comparison
# │          table used throughout the chapter.
# │
# │ Goal: Compare IB HDR (INFINIBAND_HDR_BW) vs IB NDR (INFINIBAND_NDR_BW)
# │       vs NVLink H100 (NVLINK_H100_BW) bandwidth on a 256-GPU cluster to
# │       establish why fabric topology matters for communication-bound training;
# │       compute fat-tree host count for k=64 to anchor topology scaling.
# │ Show: "400" Gbps NDR vs "200" Gbps HDR vs "900" GB/s NVLink — inline in
# │       the fabric selection and topology trade-off paragraphs.
# │ How: All bandwidth conversions via .m_as(); fat-tree hosts via integer math.
# │
# │ Imports: mlsys.constants (INFINIBAND_NDR_BW, INFINIBAND_HDR_BW,
# │           NVLINK_H100_BW, NVLINK_A100_BW, PCIE_GEN5_BW, PCIE_GEN4_BW,
# │           H100_FLOPS_FP16_TENSOR, H100_TDP, H100_MEM_CAPACITY, H100_MEM_BW,
# │           A100_FLOPS_FP16_TENSOR, B200_FLOPS_FP16_TENSOR, B200_MEM_BW,
# │           Gbps, GB, TB, second, watt, GiB, TFLOPs, flop, byte, NS)
# │ Exports: ib_ndr_gbps, ib_hdr_gbps, ib_ndr_gbs, ib_hdr_gbs, nvlink_h100_gbs,
# │          nvlink_a100_gbs, pcie5_gbs, h100_tflops, h100_tdp, h100_mem,
# │          h100_mem_bw, a100_tflops, b200_tflops, b200_mem_bw,
# │          ib_ndr_latency_us, nvlink_to_ib_ratio, fat_tree_hosts,
# │          alpha_ib_us, beta_ib_gbs
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.constants import (
    INFINIBAND_NDR_BW, INFINIBAND_HDR_BW,
    NVLINK_H100_BW, NVLINK_A100_BW,
    PCIE_GEN5_BW, PCIE_GEN4_BW,
    H100_FLOPS_FP16_TENSOR, H100_TDP, H100_MEM_CAPACITY, H100_MEM_BW,
    A100_FLOPS_FP16_TENSOR,
    B200_FLOPS_FP16_TENSOR, B200_MEM_BW,
    Gbps, GB, TB, second, watt, GiB, TFLOPs, flop, byte, NS
)
from mlsys.formatting import fmt, sci, check, md, md_math

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class NetworkFabricsSetup:
    """
    Namespace for Network Fabrics reference parameters.
    Scenario: Mapping interconnect performance for distributed clusters.
    """

    # ┌── 1. PARAMETERS (Inputs) ───────────────────────────────────────────────
    # Interconnects
    ib_ndr_raw = INFINIBAND_NDR_BW
    ib_hdr_raw = INFINIBAND_HDR_BW
    nvlink_h100_raw = NVLINK_H100_BW
    nvlink_a100_raw = NVLINK_A100_BW
    pcie5_raw = PCIE_GEN5_BW

    # GPU specs
    h100_flops_raw = H100_FLOPS_FP16_TENSOR
    h100_mem_raw = H100_MEM_CAPACITY
    h100_mem_bw_raw = H100_MEM_BW
    h100_tdp_raw = H100_TDP

    a100_flops_raw = A100_FLOPS_FP16_TENSOR
    b200_flops_raw = B200_FLOPS_FP16_TENSOR
    b200_mem_bw_raw = B200_MEM_BW

    # Topology parameters
    fat_tree_k = 64
    ib_ndr_hop_ns = 500
    alpha_ib_us_val = 1.5

    # ┌── 2. CALCULATION (The Physics) ─────────────────────────────────────────
    ib_ndr_gbps_val = ib_ndr_raw.m_as(Gbps)
    ib_ndr_gbs_val = (ib_ndr_raw / 8).m_as(GB/second)
    nvlink_to_ib_ratio_val = nvlink_h100_raw.m_as(GB/second) / ib_ndr_gbs_val

    # Fat-tree hosts for radix k: (k^3)/4
    fat_tree_hosts_val = (fat_tree_k ** 3) // 4

    # ┌── 3. INVARIANTS (Guardrails) ───────────────────────────────────────────
    check(ib_ndr_gbps_val == 400, f"NDR InfiniBand should be 400 Gbps, got {ib_ndr_gbps_val}")
    check(nvlink_to_ib_ratio_val > 15, f"NVLink/IB ratio should be > 15x, got {nvlink_to_ib_ratio_val:.1f}")
    check(fat_tree_hosts_val == 65536, f"Fat-tree hosts for k=64 should be 65536, got {fat_tree_hosts_val}")

    # ┌── 4. OUTPUTS (Formatting) ──────────────────────────────────────────────
    ib_ndr_gbps = f"{ib_ndr_gbps_val:.0f}"
    ib_hdr_gbps = f"{ib_hdr_raw.m_as(Gbps):.0f}"
    ib_ndr_gbs = f"{ib_ndr_gbs_val:.0f}"
    ib_hdr_gbs = f"{(ib_hdr_raw / 8).m_as(GB/second):.0f}"

    nvlink_h100_gbs = f"{nvlink_h100_raw.m_as(GB/second):.0f}"
    nvlink_a100_gbs = f"{nvlink_a100_raw.m_as(GB/second):.0f}"
    pcie5_gbs = f"{pcie5_raw.m_as(GB/second):.0f}"

    h100_tflops = f"{h100_flops_raw.m_as(TFLOPs/second):.0f}"
    h100_tdp = f"{h100_tdp_raw.m_as(watt):.0f}"
    h100_mem = f"{h100_mem_raw.m_as(GiB):.0f}"
    h100_mem_bw = f"{h100_mem_bw_raw.m_as(TB/second):.2f}"
    a100_tflops = f"{a100_flops_raw.m_as(TFLOPs/second):.0f}"
    b200_tflops = f"{b200_flops_raw.m_as(TFLOPs/second):,.0f}"
    b200_mem_bw = f"{b200_mem_bw_raw.m_as(TB/second):.0f}"

    ib_ndr_latency_us = f"{ib_ndr_hop_ns / 1000:.1f}"
    nvlink_to_ib_ratio = f"{nvlink_to_ib_ratio_val:.0f}"
    fat_tree_hosts = fmt(fat_tree_hosts_val, precision=0)

    # Performance model defaults
    alpha_ib_us = f"{alpha_ib_us_val:.1f}"
    beta_ib_gbs = f"{ib_ndr_gbs_val:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
ib_ndr_gbps = NetworkFabricsSetup.ib_ndr_gbps
ib_hdr_gbps = NetworkFabricsSetup.ib_hdr_gbps
ib_ndr_gbs = NetworkFabricsSetup.ib_ndr_gbs
ib_hdr_gbs = NetworkFabricsSetup.ib_hdr_gbs
nvlink_h100_gbs = NetworkFabricsSetup.nvlink_h100_gbs
nvlink_a100_gbs = NetworkFabricsSetup.nvlink_a100_gbs
pcie5_gbs = NetworkFabricsSetup.pcie5_gbs
h100_tflops = NetworkFabricsSetup.h100_tflops
h100_tdp = NetworkFabricsSetup.h100_tdp
h100_mem = NetworkFabricsSetup.h100_mem
h100_mem_bw = NetworkFabricsSetup.h100_mem_bw
a100_tflops = NetworkFabricsSetup.a100_tflops
b200_tflops = NetworkFabricsSetup.b200_tflops
b200_mem_bw = NetworkFabricsSetup.b200_mem_bw
ib_ndr_latency_us = NetworkFabricsSetup.ib_ndr_latency_us
nvlink_to_ib_ratio = NetworkFabricsSetup.nvlink_to_ib_ratio
fat_tree_hosts = NetworkFabricsSetup.fat_tree_hosts
alpha_ib_us = NetworkFabricsSetup.alpha_ib_us
beta_ib_gbs = NetworkFabricsSetup.beta_ib_gbs

# Static FEC latency references
fec_latency_ns_low = 100
fec_latency_ns_high = 200
```

## The Synchronization Backbone {#sec-network-fabrics-introduction}

Consider the running example that threads through this volume: a 175-billion-parameter language model partitioned across 1,000 GPUs. Each training step requires an AllReduce of 350 GB of gradient data, meaning every GPU must send and receive its share before the next step can begin. If even one link in the fabric is slow, all 999 other GPUs wait. The network is not auxiliary infrastructure; it is the synchronization backbone that determines whether this cluster trains efficiently or wastes millions of dollars in idle compute.

In the **Fleet Stack** (@sec-vol2-introduction), network fabrics form the connective tissue binding the Infrastructure Layer into a coherent whole. @sec-compute-infrastructure established the building blocks: accelerators, power delivery, and cooling. Those components define what each node can compute in isolation. This chapter wires those nodes together, because at scale, communication cost dominates computation cost. The **Law of Distributed Efficiency** (@eq-distributed-efficiency) makes this explicit: the $T_{\text{sync}} / T_{\text{compute}}$ ratio in the Scaling Factor is determined almost entirely by the network fabric. The fabric constrains every layer above it in the stack: @sec-distributed-training-systems cannot overlap communication with computation unless the fabric provides sufficient bandwidth, @sec-collective-communication cannot choose optimal algorithms without knowing the topology, and @sec-fault-tolerance-reliability must account for network partitions alongside node failures.

The physical network fabric exists to carry three fundamental collective communication patterns. An AllReduce sums gradients across thousands of GPUs so that every device holds the identical average—the heartbeat of synchronous training. An AllGather collects different model portions so that every GPU can reconstruct the full model state. An AllToAll, the most demanding pattern, requires every GPU to send unique data to every other GPU, a requirement critical to expert parallelism. While @sec-collective-communication covers the *algorithms* that orchestrate these patterns, this chapter covers the *physics* of the wires and switches that carry them. Understanding the distinction matters because the fabric's physical properties—bandwidth, latency, topology—determine which patterns are efficient and which become bottlenecks.

::: {.callout-perspective title="The Network as a Gradient Bus"}

In a single machine, the memory bus moves data between the processor and memory. In a distributed training cluster, the network fabric serves the analogous role: it is the **Gradient Bus** that moves parameter updates between workers. Just as memory bus bandwidth determines single-device throughput (the Memory Wall from @sec-compute-infrastructure), network fabric bandwidth determines multi-device throughput (the Communication Wall). Every concept in this chapter, from protocols to topologies to congestion control, exists to make this Gradient Bus as fast and reliable as possible.

:::

With this framing in mind, consider the concrete bandwidth cliff that separates intra-node from inter-node communication. @sec-compute-infrastructure established that a single H100 delivers `{python} h100_tflops` TFLOPS of FP16 throughput with `{python} h100_mem_bw` TB/s of memory bandwidth. Within a node, eight such accelerators communicate through NVLink at `{python} nvlink_h100_gbs` GB/s. But the moment computation crosses a node boundary, the available bandwidth drops by a factor of `{python} nvlink_to_ib_ratio`$\times$, from `{python} nvlink_h100_gbs` GB/s (NVLink) to `{python} ib_ndr_gbs` GB/s (NDR InfiniBand per port). This cliff, the transition from intra-node to inter-node communication, is the central challenge of network fabric design. For our 175B model, moving 350 GB of gradients through `{python} ib_ndr_gbs` GB/s links means that the AllReduce alone can take seconds, during which every GPU in the cluster sits idle unless the fabric and collective algorithms can overlap that transfer with computation.

@fig-bandwidth-hierarchy makes this hierarchy concrete by plotting the bandwidth at each level across four GPU generations.

::: {#fig-bandwidth-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**The Bandwidth Hierarchy**. Bandwidth at four levels of the communication hierarchy across four GPU generations, on a logarithmic scale. While HBM and NVLink bandwidth have grown roughly 9$\\times$ and 6$\\times$ respectively from V100 to B200, InfiniBand has grown only 4$\\times$. The annotations show the NVLink-to-InfiniBand ratio for each generation, quantifying the cliff that distributed training must cross at every synchronization point." fig-alt="Grouped bar chart with log-scale y-axis showing bandwidth in GB per second at four hierarchy levels across four GPU generations from V100 to B200."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BANDWIDTH HIERARCHY (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-bandwidth-hierarchy — bandwidth cliff in "The Bandwidth Cliff"
# │
# │ Goal: Plot HBM, NVLink, InfiniBand, PCIe bandwidth across V100–B200 to
# │       quantify the NVLink-to-IB ratio that distributed training must cross.
# │ Show: Grouped bar chart (log scale), NVLink/IB ratio annotations per gen.
# │ How: Hardcoded verified data; matplotlib grouped bars; viz.setup_plot().
# │
# │ Imports: numpy (np), mlsys.viz (viz)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
from mlsys import viz

fig, ax, COLORS, plt = viz.setup_plot(figsize=(8, 5))

# --- Verified data ---
generations = ["V100\n(2017)", "A100\n(2020)", "H100\n(2022)", "B200\n(2024)"]
hbm         = [900,  2039,  3350,  8000]
nvlink      = [300,   600,   900,  1800]
ib          = [ 24,    24,    48,   100]
pcie        = [ 15.75, 32,    64,    64]

n_gen = len(generations)
n_tiers = 4
x = np.arange(n_gen)
width = 0.18

tier_data   = [hbm, nvlink, ib, pcie]
tier_labels = ["HBM", "NVLink", "InfiniBand", "PCIe"]
tier_colors = [COLORS["BlueLine"], COLORS["GreenLine"], COLORS["OrangeLine"], COLORS["VioletLine"]]

for j, (data, label, color) in enumerate(zip(tier_data, tier_labels, tier_colors)):
    offset = (j - 1.5) * width
    bars = ax.bar(x + offset, data, width, label=label, color=color, alpha=0.85, zorder=3)

# --- Annotate NVLink / IB ratio ---
ratios = [n / i for n, i in zip(nvlink, ib)]
for i_gen in range(n_gen):
    # Position annotation above the NVLink bar
    nvlink_x = x[i_gen] + (1 - 1.5) * width
    ax.annotate(f"{ratios[i_gen]:.0f}$\\times$",
                xy=(nvlink_x + width * 0.5, nvlink[i_gen]),
                xytext=(0, 8), textcoords="offset points",
                fontsize=7.5, ha="center", color=COLORS["crimson"],
                fontweight="bold",
                bbox=dict(boxstyle="round,pad=0.15", fc="white", ec=COLORS["crimson"], alpha=0.8))

ax.set_yscale("log")
ax.set_ylim(5, 20000)
ax.set_xticks(x)
ax.set_xticklabels(generations)
ax.set_ylabel("Bandwidth (GB/s, log scale)")
ax.set_xlabel("GPU Generation")
ax.legend(loc="upper left", frameon=True, fancybox=True, framealpha=0.9, ncol=2)
ax.set_title("")
plt.show()
```
:::

@fig-bandwidth-hierarchy reveals that the NVLink-to-InfiniBand ratio has remained between 12$\times$ and 25$\times$ across four GPU generations, despite absolute bandwidth improvements at every tier. This persistent cliff is not a temporary engineering shortcoming; it reflects the fundamental physics that on-package interconnects (NVLink) operate over millimeters of copper, while inter-node links (InfiniBand) span meters of cable and must traverse switches. The ratio determines which parallelism strategies are efficient: tensor parallelism, which requires continuous high-bandwidth exchange of activations, is viable within a node but impractical across nodes. Pipeline and data parallelism, which tolerate the lower inter-node bandwidth, must carry the burden of cross-node communication. Every topology and protocol decision in the remainder of this chapter is an attempt to minimize the impact of this hierarchy on collective communication performance.

To see why this ratio matters so viscerally, consider what happens during a single training step. @fig-bandwidth-cliff-gantt contrasts the timeline for an 8-GPU job communicating within a node versus a 64-GPU job communicating across nodes.

::: {#fig-bandwidth-cliff-gantt fig-env="figure" fig-pos="htb" fig-cap="**The Bandwidth Cliff in a Training Step**. Two timelines for the same model: an 8-GPU intra-node job (top) using NVLink at 900 GB/s, and a 64-GPU inter-node job (bottom) using InfiniBand at 50 GB/s. The AllReduce phase (red) grows from a thin sliver to a dominant fraction of the step. Without communication--computation overlap, GPUs sit idle during the entire AllReduce, and the utilization gap between the two scenarios exceeds 10 percentage points." fig-alt="Two horizontal Gantt bars. Top bar shows forward pass, backward pass, and thin AllReduce sliver at 99 percent utilization. Bottom bar shows same compute phases but a wide AllReduce block dropping utilization to 87 percent."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{BlueL}{HTML}{D1E6F3}
  \definecolor{GreenLine}{HTML}{008F45}
  \definecolor{GreenL}{HTML}{D4EFDF}
  \definecolor{RedLine}{HTML}{CB202D}
  \definecolor{RedL}{HTML}{F5D2D5}
  \definecolor{OrangeLine}{HTML}{E67817}
  \definecolor{BrownLine}{HTML}{78492A}

  % --- Scenario A: Intra-node (NVLink) ---
  \node[font=\small\bfseries, anchor=east] at (-0.3, 1.8) {Intra-Node};
  \node[font=\scriptsize, anchor=east, text=black!60] at (-0.3, 1.4) {8 GPUs, NVLink};

  % Forward pass
  \fill[GreenLine!80] (0, 1.2) rectangle (4, 2.0);
  \node[font=\scriptsize, text=white] at (2, 1.6) {Forward (100 ms)};

  % Backward pass
  \fill[BlueLine!80] (4, 1.2) rectangle (8, 2.0);
  \node[font=\scriptsize, text=white] at (6, 1.6) {Backward (100 ms)};

  % AllReduce (tiny)
  \fill[RedLine] (8, 1.2) rectangle (8.2, 2.0);
  \node[font=\tiny, anchor=west, text=RedLine] at (8.3, 1.6) {AR (1 ms)};

  % Utilization annotation
  \node[font=\scriptsize\bfseries, anchor=west, text=GreenLine] at (9.0, 1.6) {99.5\% util.};

  % --- Scenario B: Inter-node (InfiniBand) ---
  \node[font=\small\bfseries, anchor=east] at (-0.3, 0.2) {Inter-Node};
  \node[font=\scriptsize, anchor=east, text=black!60] at (-0.3, -0.2) {64 GPUs, IB};

  % Forward pass
  \fill[GreenLine!80] (0, -0.4) rectangle (4, 0.4);
  \node[font=\scriptsize, text=white] at (2, 0.0) {Forward (100 ms)};

  % Backward pass
  \fill[BlueLine!80] (4, -0.4) rectangle (8, 0.4);
  \node[font=\scriptsize, text=white] at (6, 0.0) {Backward (100 ms)};

  % AllReduce (large)
  \fill[RedLine] (8, -0.4) rectangle (10.4, 0.4);
  \node[font=\scriptsize, text=white] at (9.2, 0.0) {AllReduce (30 ms)};

  % Utilization annotation
  \node[font=\scriptsize\bfseries, anchor=west, text=OrangeLine] at (10.6, 0.0) {87\% util.};

  % Time axis
  \draw[->, thick, black!60] (0, -1.0) -- (11, -1.0) node[right, font=\scriptsize] {Time (ms)};
  \foreach \x/\t in {0/0, 4/100, 8/200, 10.4/230} {
    \draw[black!60] (\x, -0.9) -- (\x, -1.1);
    \node[font=\tiny, below] at (\x, -1.1) {\t};
  }

  % Idle cost annotation
  \draw[RedLine, thick, decorate, decoration={brace, amplitude=4pt}]
    (10.4, -0.5) -- (8.2, -0.5)
    node[midway, below=5pt, font=\tiny\bfseries, text=RedLine] {30 ms idle per step};

\end{tikzpicture}
```
:::

The 12 percentage-point utilization gap in @fig-bandwidth-cliff-gantt represents millions of dollars in wasted compute over a months-long training run. This is why network fabric design is not an afterthought but the central engineering challenge of distributed training.

## How ML Networking Inverts Datacenter Assumptions {#sec-network-workload-inversion}

A network architect from the world of large-scale web services would find the traffic patterns of a distributed training cluster deeply counter-intuitive. Traditional datacenter traffic is characterized by a vast number of small, independent, and asynchronous flows. Millions of users accessing a web service generate a stochastic traffic pattern that is well-served by standard TCP/IP and statistically multiplexed, oversubscribed networks.

ML training workloads are the complete opposite. They are synchronous, periodic, and dominated by a small number of massive, collective communication operations. This fundamental difference in workload characteristics inverts the core assumptions of traditional network design.

| **Workload Pattern**   | **Traditional Datacenter Assumption**  | **ML Reality**                             |
|:-----------------------|:---------------------------------------|:-------------------------------------------|
| **Traffic Pattern**    | Asynchronous, stochastic, many-to-many | Synchronous, periodic, all-to-all          |
| **Flow Type**          | Millions of small, short-lived flows   | A few massive, long-lived "elephant" flows |
| **Performance Metric** | Average throughput, per-flow fairness  | Tail latency, global synchronization time  |
| **Loss Tolerance**     | Tolerant (TCP retransmits)             | Intolerant (one dropped packet stalls all) |
| **Congestion**         | Localized, independent events          | Global, correlated (incast)                |

: **ML Networking Inverts Traditional Datacenter Assumptions**: Where web services require fairness for millions of independent flows, ML training requires a globally synchronized, lossless fabric optimized for a handful of massive collective operations. {#tbl-network-assumptions}

**The Synchronicity Inversion**: Web traffic is asynchronous; one user's slow connection does not affect another's. Distributed training is governed by the Bulk Synchronous Parallel (BSP) model. All 1,024 GPUs in a training job must complete their gradient exchange before *any* of them can proceed to the next step. This means the performance of the entire cluster is dictated by the slowest link. A single congested switch port that delays one GPU's packets by 100ms effectively wastes 100ms of compute time for all 1,024 GPUs. Tail latency is not an outlier; it is the bottleneck.

**The Flow Inversion**: Traditional networks are designed for fairness among millions of "mice flows". ML training is dominated by a few "elephant flows" corresponding to the gradient AllReduce. A single AllReduce on a 175B parameter model can involve exchanging 350GB of data. Standard flow control and routing mechanisms like ECMP, which use static hashing to balance flows, are poorly suited to this traffic pattern, as they can inadvertently map multiple elephant flows to the same link, creating massive congestion while other links sit idle.

**The Loss Inversion**: TCP/IP was designed for unreliable networks and handles packet loss gracefully through retransmission. RDMA-based protocols used in ML clusters (InfiniBand, RoCE) assume a lossless fabric. A single dropped packet forces a Go-Back-N retransmission that can stall the sender for milliseconds, creating a catastrophic straggler that delays the entire synchronous training step. This is why ML fabrics must be engineered for zero packet loss, using credit-based flow control (InfiniBand) or carefully tuned Priority Flow Control (PFC) with large buffers (Ethernet).

These inversions explain why running large-scale distributed training over a standard enterprise Ethernet network is inefficient or impossible. The network fabric for ML is not just a set of pipes; it is a distributed, high-performance "bus" for collective communication, and it must be designed from the ground up for the unique physics of synchronous, large-scale parallelism.

### The Five-Level Model {#sec-network-fabrics-five-level-model}

Network engineering is often taught as a layer cake of protocols. To make the physics of ML cluster fabrics concrete, we adapt this approach into a **Five-Level Model** specific to high-performance interconnects:

- **Level 1: Wire and Link** (@sec-network-fabrics-wire-link). The physics of signal transmission. We examine why signal integrity (PAM4, SerDes) and the speed of light in fiber impose hard constraints on latency and cluster geometry.
- **Level 2: Transport** (@sec-network-fabrics-transport). The protocols that move data reliably. We contrast InfiniBand and RoCE, and use the $\alpha$-$\beta$ model to quantify the latency-versus-bandwidth trade-off for different message sizes.
- **Level 3: Switch and Topology** (@sec-network-fabrics-topology). The shape of the network. We compare how fat-trees, rail-optimized designs, and dragonflies achieve the bisection bandwidth needed for global collectives.
- **Level 4: Fabric Behavior** (@sec-network-fabrics-behavior). The dynamics of traffic under load. We analyze congestion control (PFC, DCQCN), adaptive routing, and the incast phenomena that cause tail latency.
- **Level 5: Cluster Design** (@sec-network-fabrics-cluster). The end-to-end system. We show how these layers integrate into production supercomputers like the NVIDIA SuperPOD and Meta Grand Teton to create a unified Gradient Bus.

The remainder of this chapter ascends this stack, starting from the copper and glass at the bottom.

## Level 1: Wire and Link {#sec-network-fabrics-wire-link}

Before analyzing protocols and topologies, we must understand the physical medium. Every network design decision is ultimately constrained by *what* the wire can carry. At `{python} ib_ndr_gbps` Gbps and beyond, the physics of signal transmission imposes hard limits on cable length, power consumption, and error rates. These are not engineering inconveniences; they are fundamental constraints that shape cluster geometry.

### Signal Integrity and PAM4 {#sec-network-fabrics-pam4}

\index{PAM4}

::: {.callout-definition title="PAM4 Signaling"}

***PAM4 Signaling***\index{PAM4 Signaling!definition} is a modulation scheme that uses four voltage levels to encode two bits per symbol, doubling the data rate for a given symbol rate.

1.  **Significance (Quantitative):** It enables the high **Network Bandwidth ($BW$)** required for exa-scale fleets (e.g., 400G/800G links). However, the reduced signal-to-noise ratio necessitates **Forward Error Correction (FEC)**, which adds irreducible **Fixed Latency ($L_{lat}$)** to every packet hop.
2.  **Distinction (Durable):** Unlike **NRZ Signaling** (binary on/off), PAM4 uses multi-level amplitude to overcome the physical symbol-rate limits of copper and fiber.
3.  **Common Pitfall:** A frequent misconception is that doubling the bits per symbol is "free." In reality, it is a **Reliability-Latency Trade-off**: the processing time for error correction makes PAM4 links fundamentally higher-latency than their NRZ predecessors.

:::

To achieve `{python} ib_ndr_gbps` Gbps, we cannot simply toggle a voltage on and off faster. Signal attenuation in copper and chromatic dispersion in fiber impose a hard ceiling on the **symbol rate** (the number of signal transitions per second). Modern links overcome this by using PAM4 to pack more information into each transition.

However, the gap between adjacent voltage levels in PAM4 shrinks by a factor of three compared to NRZ, making the link highly susceptible to noise. Consequently, modern high-speed links operate near the physical limits of reliable detection and require **Forward Error Correction (FEC)** at the physical layer, typically Reed-Solomon RS(544,514) codes. This FEC processing adds significant latency—typically `{python} fec_latency_ns_low` to `{python} fec_latency_ns_high` ns per hop—to every packet. For our 175B model training, which generates 350 GB of gradient traffic per step across 1,000 GPUs, this latency floor is non-negotiable. A packet crossing three switch hops in a fat-tree incurs nearly 1 $\mu$s of irreducible latency just from FEC decoding and encoding at each port. This "physics tax" sets a hard floor on the $\alpha$ term of our performance models, limiting the speed of latency-sensitive collectives like AllReduce regardless of software optimizations.

### Reach and Medium: Copper versus Optics {#sec-network-fabrics-copper-optics}

\index{DAC}\index{AOC}\index{optical interconnect}

The choice of physical medium determines the reach and economics of each link. Three categories dominate modern ML cluster design:

- **DAC (Direct Attach Copper)**: Passive copper cables offering the lowest latency and cost (~\$50), but limited to ~3 meters. Used exclusively within a rack.
- **AOC (Active Optical Cable)**: Fiber with permanently attached transceivers, suitable for 3 to 30 meter runs (~\$500).
- **Pluggable Optics**: Separate transceivers and fiber cords, used for runs exceeding 30 meters to the network core.

::: {.callout-perspective title="The Cost of Distance"}
In an ML fleet, distance is money. A 10,000-GPU cluster requires ~20,000 optical links at the spine layer alone. At \$500 each with 10 W per link, that represents \$10 million in cabling and 200 kW of continuous power just for transceivers. This physics dictates **cluster geometry**: we pack accelerators as densely as possible (70–100 kW per rack) to maximize cheap copper and minimize expensive optics.
:::

### SerDes, Link Budget, and Power {#sec-network-fabrics-serdes}

\index{SerDes}

Every port depends on a **Serializer/Deserializer (SerDes)** circuit that converts parallel data from the switch ASIC into a serial stream. As bandwidth scales, these circuits have become the dominant power consumer in the network fabric. Consider the energy implications for our 1,000-GPU cluster: maintaining full bisection bandwidth for the 350 GB of gradient traffic requires roughly 3,000 high-speed links. If each `{python} ib_ndr_gbps` Gbps port consumes 25 W (combined SerDes and optical transceiver power), the interconnect alone draws 75 kW continuously—over 10% of the cluster's power budget dedicated merely to moving bits, not computing on them.

The reach of these links is governed by the **link budget**, a strict decibel (dB) allowance for signal attenuation. As signals traverse copper, they lose energy to skin effect and dielectric absorption. The link budget is the difference between the transmitter's output power and the receiver's sensitivity limit. For a standard 50G PAM4 lane, the budget might be 30 dB. If a 3-meter DAC cable introduces 18 dB of loss and the connectors add another 2 dB, 10 dB of margin remains. However, as frequency doubles to support 100G lanes (for 800 Gbps ports), the loss per meter increases sharply. This physics forces a brutal trade-off: to maintain signal integrity without exceeding the power budget, cable reach must decrease. 800 Gbps copper links are limited to roughly 1–2 meters, physically constraining the diameter of a compute rack and forcing the use of expensive, power-hungry optics for any connection leaving the cabinet. As link speeds approach 1.6 Tbps, SerDes and optical power will account for over 15% of total cluster energy, making power-per-bit a primary scaling constraint.

**Transition:** The wire sets hard limits on reach and speed; the transport layer must now build a reliable communication primitive on top of these physical links.

## Level 2: Transport and the Performance Model {#sec-network-fabrics-transport}

Large-scale training requires sustained, synchronized bulk transfers. A single AllReduce operation across 1,024 accelerators may move terabytes of gradient data. This pattern demands networks optimized for **Remote Direct Memory Access (RDMA)** to eliminate CPU overhead and **lossless delivery** to ensure predictable performance.

### RDMA and GPUDirect {#sec-network-fabrics-rdma}

\index{RDMA}\index{GPUDirect RDMA}

::: {.callout-definition title="Remote Direct Memory Access (RDMA)"}

***Remote Direct Memory Access (RDMA)***\index{Remote Direct Memory Access!definition} is a networking technology that allows one computer to access the memory of another directly without involving the operating system or CPU of either machine.

1.  **Significance (Quantitative):** it provides **Kernel Bypass** and **Zero-Copy** capabilities, reducing the **Communication Overhead ($L_{lat}$)** by eliminating context switches and intermediate memory copies. It is essential for saturating high-bandwidth links ($BW$) during gradient synchronization.
2.  **Distinction (Durable):** Unlike **Traditional TCP/IP Networking**, where the CPU must process every packet through a complex software stack, RDMA offloads the entire transport protocol to the network hardware (HCA).
3.  **Common Pitfall:** A frequent misconception is that RDMA is just "fast Ethernet." In reality, it requires specialized **Lossless Fabric** (like InfiniBand or RoCE with PFC) because it lacks the robust congestion control mechanisms of TCP; a single lost packet can cause massive performance collapses.

:::

Standard TCP/IP is architecturally unfit for the `{python} ib_ndr_gbps` Gbps era. The protocol stack was designed when network speeds were orders of magnitude slower than CPU memory bandwidth; today, that relationship has inverted. Processing a `{python} ib_ndr_gbps` Gbps stream through the Linux kernel imposes a crushing interrupt tax: simply copying payload data between user space and kernel buffers can consume the entire memory bandwidth of a dual-socket server, requiring tens of CPU cores merely to keep the pipe full. This creates a CPU wall where the host processor becomes the bottleneck for network traffic, starving the application logic it is meant to serve.

RDMA bypasses this entire layer. By offloading the transport logic to the NIC hardware, it allows applications to read and write directly to remote memory. For ML, **GPUDirect RDMA** extends this zero-copy principle to the accelerators themselves. Without it, a gradient update follows a tortuous path: GPU memory $\rightarrow$ CPU system RAM $\rightarrow$ kernel buffer $\rightarrow$ NIC. GPUDirect short-circuits this to a single PCIe transaction: GPU $\rightarrow$ NIC. For our 175B model's 350 GB gradient exchange, this optimization eliminates 700 GB of redundant memory copies across the cluster per step. This not only reduces latency but crucially frees the CPU to orchestrate complex pipelining logic rather than acting as a data mover.

### InfiniBand and RoCE {#sec-network-fabrics-ib-roce}

\index{InfiniBand}\index{RoCE}

Two transport protocols dominate modern GPU clusters:

- **InfiniBand (IB)**: A purpose-built HPC architecture designed for point-to-point, switched fabrics. It uses hardware-level **credit-based flow control**—a sender must hold a credit (representing available buffer space at the receiver) before transmitting. This makes IB inherently **lossless** at the link layer. It includes a **Subnet Manager (SM)** for global routing and **Virtual Lanes (VLs)** for traffic isolation.
- **RoCE (RDMA over Converged Ethernet)**: Implements RDMA semantics over standard Ethernet. RoCEv2 encapsulates IB transport headers in UDP/IP packets. While it allows using multi-vendor Ethernet switches, it requires sophisticated congestion control (PFC and ECN) to approximate the losslessness that IB provides natively.

@fig-ib-roce-stack compares the two stacks, showing how both expose the same **Verbs API** to applications despite their different transport foundations.

::: {#fig-ib-roce-stack fig-env="figure" fig-pos="htb" fig-cap="**High-Performance Networking Stacks**. Comparison of InfiniBand and RoCE protocol stacks. Both expose the same Verbs API, but RoCE relies on Priority Flow Control (PFC) in the Ethernet layer to approximate InfiniBand's native lossless guarantees." fig-alt="Side-by-side diagram comparing InfiniBand and RoCE protocol stacks from application layer down to physical layer, showing Verbs API at top and transport differences."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{OrchidL}{HTML}{F4E7F8}
  \definecolor{SlateL}{HTML}{E5E7E9}
  \definecolor{OrangeLine}{HTML}{CC5500}

  \tikzset{
    layer/.style={draw=black!70, fill=white, minimum width=3.2cm, minimum height=0.7cm, font=\sffamily\footnotesize},
    header/.style={font=\bfseries\sffamily, align=center, crimson}
  }

  % InfiniBand Stack
  \begin{scope}[local bounding box=IB]
    \node[header] at (0, 5) {InfiniBand};
    \node[layer, fill=OrchidL] (ib_verbs) at (0, 4) {Verbs API};
    \node[layer, fill=OrchidL!50] (ib_trans) at (0, 3) {IB Transport};
    \node[layer, fill=OrchidL!50] (ib_net) at (0, 2) {IB Network};
    \node[layer, fill=OrchidL!50] (ib_link) at (0, 1) {IB Link};
    \node[layer, fill=OrchidL!50] (ib_phy) at (0, 0) {IB Physical};

    % RDMA bypass arrow
    \draw[->, thick, OrangeLine] (-1.9, 4.3) -- (-1.9, 0.5) node[midway, left, align=center, font=\scriptsize\bfseries] {Kernel\\Bypass};
  \end{scope}

  % RoCE Stack
  \begin{scope}[shift={(5.5,0)}, local bounding box=RoCE]
    \node[header] at (0, 5) {RoCEv2};
    \node[layer, fill=OrchidL] (roce_verbs) at (0, 4) {Verbs API};
    \node[layer, fill=OrchidL!50] (roce_trans) at (0, 3) {IB Transport};
    \node[layer, fill=SlateL] (roce_udp) at (0, 2) {UDP / IP};
    \node[layer, fill=SlateL] (roce_eth) at (0, 1) {Ethernet Link (PFC)};
    \node[layer, fill=SlateL] (roce_phy) at (0, 0) {Ethernet Physical};

    % RDMA bypass arrow
    \draw[->, thick, OrangeLine] (1.9, 4.3) -- (1.9, 0.5) node[midway, right, align=center, font=\scriptsize\bfseries] {Kernel\\Bypass};
  \end{scope}

  % Connectors
  \draw[dashed, gray] (ib_verbs) -- (roce_verbs);
  \draw[dashed, gray] (ib_trans) -- (roce_trans);

\end{tikzpicture}
```
:::

### Losslessness and the Go-Back-N Problem {#sec-network-fabrics-losslessness}

ML collectives assume in-order, lossless delivery, but the hardware implementation of this reliability introduces a critical fragility. While TCP handles packet loss gracefully via Selective Acknowledgement (retransmitting only the specific missing segment), RDMA protocols like RoCEv2 typically rely on the simpler **Go-Back-N** mechanism. This design choice is driven by the physical constraints of the NIC: implementing complex reassembly logic for out-of-order packets requires substantial on-chip SRAM, which consumes precious die area needed for SerDes blocks and packet processing engines. The NIC hardware is optimized for throughput, not state management.

The trade-off is a severe penalty upon failure. If a network switch drops a single packet 900 MB into a 1 GB gradient transfer, the receiver discards all subsequent packets, forcing the sender to retransmit the entire tail of the message—potentially 100 MB of data for a single missed frame. At `{python} ib_ndr_gbps` Gbps, this retransmission triggers a latency spike orders of magnitude larger than the wire delay. In a synchronous training loop where thousands of GPUs wait for the slowest member, this straggler effect means a single dropped packet idles the entire cluster. The network fabric must therefore behave as a lossless medium, pushing the complexity of flow control into the switches via Priority Flow Control (PFC) to ensure buffers never overflow.

::: {.callout-checkpoint title="Protocol Selection"}

Consider a 2,048-GPU training cluster that will run both large language model training (gradient messages of several gigabytes) and reinforcement learning (frequent small control messages).

1. Which protocol would you recommend and why? Consider that large messages are bandwidth-dominated while small messages are latency-dominated.
2. If you chose RoCE, what additional infrastructure would be needed compared to InfiniBand?
3. How would your answer change if the cluster also needed to serve inference traffic to external users over the same physical network?

:::

### The $\alpha$-$\beta$ Performance Model {#sec-network-fabrics-performance-model}

\index{alpha-beta model}

::: {.callout-definition title="The alpha-beta Model"}

***α-β Model***\index{Alpha-Beta Model!definition} is a linear cost model for network communication where the time to transfer a message of $n$ bytes is $T(n) = \alpha + n/\beta$.

1.  **Significance (Quantitative):** It maps directly to the **Iron Law**, where **α** (alpha) represents the fixed **Startup Latency ($L_{lat}$)** and **β** (beta) represents the **Network Bandwidth ($BW$)**. It separates the **Latency-Bound Regime** (small messages) from the **Bandwidth-Bound Regime** (large messages).
2.  **Distinction (Durable):** Unlike **Idealized Throughput Models**, the α-β model captures the **Fixed Penalty** of communication, explaining why many small messages are $100\times$ more expensive than one large message of the same total size.
3.  **Common Pitfall:** A frequent misconception is that $\alpha$ is just "network delay." In reality, $\alpha$ includes **Software Overhead** (kernel context switches, stack traversal, and library synchronization) that can often exceed the physical wire delay.

:::

The model reveals two regimes:
1. **Latency-dominated ($n < \alpha\beta$):** Transfer time is dominated by the startup cost $\alpha$. Small control messages and pipeline bubbles fall here.
2. **Bandwidth-dominated ($n > \alpha\beta$):** Transfer time scales with $n/\beta$. Gradient AllReduce typically falls here.

For NDR InfiniBand with $\alpha \approx 1.5 \mu\text{s}$ and $\beta \approx 50 \text{GB/s}$, the crossover point $n^* = \alpha\beta \approx 75$ KB. Messages smaller than this gain little from more bandwidth; messages larger than this gain little from lower latency.

To see why this distinction matters in practice, consider two messages that our 175B model training job sends every iteration. The first is a 4 KB pipeline-scheduling control message that coordinates microbatch handoffs between pipeline stages. Applying the model: $T(4\text{ KB}) = 1.5\;\mu\text{s} + 4\text{ KB} / 50\text{ GB/s} \approx 1.5\;\mu\text{s} + 0.08\;\mu\text{s} = 1.58\;\mu\text{s}$. The bandwidth term contributes only 5% of the total. Doubling the link speed to 100 GB/s would save a negligible 0.04 $\mu$s. For this message, the only way to reduce transfer time is to reduce the hop count (which lowers $\alpha$), not to buy faster links.

The second message is a 350 MB gradient shard for one layer's AllReduce. Now: $T(350\text{ MB}) = 1.5\;\mu\text{s} + 350\text{ MB} / 50\text{ GB/s} \approx 0.0015\text{ ms} + 7.0\text{ ms} = 7.0\text{ ms}$. The latency term is invisible. Doubling bandwidth to 100 GB/s would halve this to 3.5 ms, a direct and proportional gain. These two cases illustrate why network design must address both $\alpha$ and $\beta$ simultaneously: topology and hop count control the latency-dominated regime, while link speed and path diversity control the bandwidth-dominated regime.

@fig-alpha-beta-crossover makes these two regimes visible across the full range of message sizes. For InfiniBand, the crossover occurs at approximately 50 KB: messages smaller than this are latency-dominated (the flat region on the left), while larger messages are bandwidth-dominated (the linear region on the right). The 5$\times$ latency gap between InfiniBand and Ethernet RoCE is clearly visible for small messages but becomes irrelevant for the multi-megabyte gradient transfers that dominate training communication.

::: {#fig-alpha-beta-crossover fig-env="figure" fig-pos="htb" fig-cap="**The Alpha-Beta Crossover**. Transfer time as a function of message size for InfiniBand NDR and Ethernet RoCE. Small messages are latency-dominated (flat region), while large messages are bandwidth-dominated (linear region). The crossover point marks where investing in bandwidth begins to pay off more than reducing latency." fig-alt="Log-log plot showing transfer time versus message size for InfiniBand and Ethernet, with latency-dominated and bandwidth-dominated regions labeled."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ALPHA-BETA CROSSOVER (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-alpha-beta-crossover — latency vs bandwidth regime in fabric
# │          selection
# │
# │ Goal: Visualize transfer time vs message size for InfiniBand NDR vs Ethernet
# │       RoCE; show latency-dominated (flat) vs bandwidth-dominated (linear).
# │ Show: Log-log plot with crossover points (~50 KB for IB); alpha/beta model.
# │ How: T = alpha + n/beta; plot for both fabrics; annotate crossover.
# │
# │ Imports: matplotlib.pyplot (plt), numpy (np), matplotlib.ticker
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.ticker as ticker

alpha_ib = 1e-6
beta_ib = 50e9
alpha_eth = 5e-6
beta_eth = 25e9

n = np.logspace(0, 10, 1000)
t_ib = alpha_ib + n / beta_ib
t_eth = alpha_eth + n / beta_eth

n_cross_ib = alpha_ib * beta_ib
t_cross_ib = 2 * alpha_ib
n_cross_eth = alpha_eth * beta_eth
t_cross_eth = 2 * alpha_eth

fig, ax = plt.subplots(figsize=(9, 5))

ax.plot(n, t_ib, label=r'InfiniBand NDR (1$\mu$s, 50 GB/s)', color='#006395', linewidth=2.5)
ax.plot(n, t_eth, label=r'Ethernet RoCE (5$\mu$s, 25 GB/s)', color='#CB202D', linewidth=2.5, linestyle='--')

ax.scatter([n_cross_ib], [t_cross_ib], color='#006395', s=80, zorder=5, edgecolors='white')
ax.scatter([n_cross_eth], [t_cross_eth], color='#CB202D', s=80, zorder=5, edgecolors='white')

ax.annotate(f'Crossover\n~{n_cross_ib/1e3:.0f} KB',
            xy=(n_cross_ib, t_cross_ib),
            xytext=(n_cross_ib/20, t_cross_ib*3),
            arrowprops=dict(arrowstyle='->', color='#006395'),
            color='#006395', fontsize=9, ha='center')

ax.annotate(f'Crossover\n~{n_cross_eth/1e3:.0f} KB',
            xy=(n_cross_eth, t_cross_eth),
            xytext=(n_cross_eth*20, t_cross_eth/3),
            arrowprops=dict(arrowstyle='->', color='#CB202D'),
            color='#CB202D', fontsize=9, ha='center')

split_point = n_cross_ib
ax.axvspan(1, split_point, color='gray', alpha=0.1)

ax.text(3e1, 2e-4, "Latency\nDominated", fontsize=12, color='#444', ha='center', va='center', fontweight='bold')
ax.text(1e8, 2e-4, "Bandwidth\nDominated", fontsize=12, color='#444', ha='center', va='center', fontweight='bold')

ax.set_xscale('log')
ax.set_yscale('log')
ax.set_xlim(1, 1e10)
ax.set_ylim(5e-7, 1)
ax.grid(True, which="major", ls="-", alpha=0.6)
ax.grid(True, which="minor", ls=":", alpha=0.3)
ax.set_xlabel('Message Size (Bytes)', fontweight='bold')
ax.set_ylabel('Transfer Time (Seconds)', fontweight='bold')
ax.legend(loc='upper left', frameon=True, framealpha=0.9, fancybox=True)
ax.xaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=12))
plt.tight_layout()
plt.show()
```
:::

::: {#nb-allreduce-bottleneck .callout-notebook title="When Does AllReduce Become the Bottleneck?"}

```{python}
#| echo: false
#| label: allreduce-bottleneck
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ALLREDUCE BOTTLENECK ANALYSIS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Network Performance Modeling - Alpha-Beta Model.
# │
# │ Goal: Identify when communication intensity overwhelms computation.
# │ Show: Growth of comm overhead from 1B to 70B parameter models.
# │ How: Calculate T_compute vs T_ring_allreduce using Alpha-Beta parameters.
# │
# │ Imports: mlsys.constants, NetworkFabricsSetup.*
# │ Exports: t_comp_ms, t_comm_1b_ms, t_comm_70b_ms, comm_frac_1b
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class AllReduceBottleneck:
    """
    Scenario: Cluster of 1024 H100 GPUs training models of varying scale.
    Alpha-Beta parameters for NDR InfiniBand.
    """

    # ┌── 1. PARAMETERS (Inputs) ───────────────────────────────────────────────
    num_gpus = 1024
    peak_tflops = float(NetworkFabricsSetup.h100_tflops)
    gpu_util = 0.50
    flops_per_sample_base = 2e18 # Rough estimate for 1B model iteration

    alpha_s = float(NetworkFabricsSetup.alpha_ib_us) * 1e-6
    beta_gbs = float(NetworkFabricsSetup.beta_ib_gbs)

    params_small = 1e9
    params_large = 70e9

    # ┌── 2. CALCULATION (The Physics) ─────────────────────────────────────────
    # Compute time
    t_comp_s = flops_per_sample_base / (peak_tflops * 1e12 * gpu_util)

    # AllReduce time: 2(p-1)alpha + 2(p-1)/p * m/beta
    # Using a temp variable for the latency part of the calculation
    _latency_term = 2 * (num_gpus - 1) * alpha_s

    _bandwidth_term_small = (2 * (num_gpus - 1) / num_gpus) * ((params_small * 4) / (beta_gbs * 1e9))
    t_comm_small_s = _latency_term + _bandwidth_term_small

    _bandwidth_term_large = (2 * (num_gpus - 1) / num_gpus) * ((params_large * 4) / (beta_gbs * 1e9))
    t_comm_large_s = _latency_term + _bandwidth_term_large

    comm_frac_small = t_comm_small_s / (t_comp_s + t_comm_small_s)

    # ┌── 3. INVARIANTS (Guardrails) ───────────────────────────────────────────
    check(t_comp_s > 0.1, f"Compute time should be significant, got {t_comp_s:.2f}s")
    check(t_comm_large_s > t_comm_small_s * 50, "70B comm should be >50x 1B comm")

    # ┌── 4. OUTPUTS (Formatting) ──────────────────────────────────────────────
    t_comp_ms = f"{t_comp_s * 1000:.0f}"
    t_comm_1b_ms = f"{t_comm_small_s * 1000:.0f}"
    t_comm_70b_ms = f"{t_comm_large_s * 1000:.0f}"
    comm_frac_1b_pct = f"{comm_frac_small * 100:.1f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
t_comp_ms = AllReduceBottleneck.t_comp_ms
t_comm_1b_ms = AllReduceBottleneck.t_comm_1b_ms
t_comm_70b_ms = AllReduceBottleneck.t_comm_70b_ms
comm_frac_1b = AllReduceBottleneck.comm_frac_1b_pct

# Math built in Python (vol1 pattern: no {python} inside math)
alpha_math = md_math(f"\\alpha = {alpha_ib_us} \\;\\mu\\text{{s}}")
beta_math = md_math(f"\\beta = {beta_ib_gbs} \\;\\text{{GB/s}}")
t_comp_display_math = md(f"$$ T_{{\\text{{compute}}}} = {t_comp_ms} \\text{{ ms}} $$")
t_ring_1b_math = md_math(f"T_{{\\text{{ring}}}} \\approx {t_comm_1b_ms} \\text{{ ms}}")
comm_frac_display_math = md(f"$$ \\text{{Comm. fraction}} = {comm_frac_1b} \\% $$")
t_ring_70b_display_math = md(f"$$ T_{{\\text{{ring}}}} \\approx {t_comm_70b_ms} \\text{{ ms}} $$")
```

**Setup**: A cluster of 1,024 H100 GPUs training a model with 1 billion parameters (4 GB of FP32 gradients). Each GPU computes at `{python} h100_tflops` TFLOPS. The network uses NDR InfiniBand (`{python} alpha_math` and `{python} beta_math` per link).

**Step 1: Compute time per iteration.**

Assume each GPU processes a microbatch requiring $2 \times 10^{18}$ FLOPs (a rough estimate for a forward plus backward pass on a large batch). At `{python} h100_tflops` TFLOPS with 50% utilization:

`{python} t_comp_display_math`

**Step 2: Communication time (Ring AllReduce).**

With $p = 1{,}024$ and $m = 4 \times 10^9$ bytes:

- Total Communication: `{python} t_ring_1b_math`

**Step 3: Communication fraction.**

`{python} comm_frac_display_math`

With overlap between communication and computation (possible because the backward pass produces gradients layer by layer), the effective overhead can be reduced further. The network is not the bottleneck for this configuration.

**When does it become the bottleneck?** If we scale to 70 billion parameters (280 GB of gradients) and the per-GPU computation stays similar (through batch size scaling), the bandwidth term alone becomes:

`{python} t_ring_70b_display_math`

Now communication dominates computation by nearly 3$\times$. Our 175B model, at 2.5$\times$ this scale, would be even more severely bottlenecked by pure data parallelism. This is precisely why models beyond a few billion parameters require tensor and pipeline parallelism to partition the model, rather than relying solely on data parallelism, which must AllReduce the full gradient vector.

:::

**Transition:** The $\alpha$-$\beta$ model quantifies the speed of a single link, but our 175B-parameter model requires 1,000 GPUs to work in concert. To scale these transport primitives from a pair of nodes to a warehouse-scale supercomputer, we need a **Topology**—a specific pattern of connections that maximizes bisection bandwidth while minimizing the hop count and cabling cost for global collective operations.

## Level 3: Switch and Topology {#sec-network-fabrics-topology}

The physical arrangement of switches determines the **bisection bandwidth** and whether the fabric is **non-blocking**. These two properties govern how well the network supports the global communication patterns that dominate distributed training.

::: {.callout-definition title="Bisection Bandwidth"}

***Bisection Bandwidth***\index{Bisection Bandwidth!definition} is the minimum total bandwidth across any cut that divides the network into two equal halves.

1.  **Significance (Quantitative):** It represents the **Worst-Case Throughput** for global communication patterns (e.g., AllReduce). Within the **Iron Law**, bisection bandwidth defines the **Aggregate Bandwidth ($BW_{bisect}$)** available when the entire fleet must synchronize its state simultaneously.
2.  **Distinction (Durable):** Unlike **Aggregate Bandwidth** (the sum of all link speeds), Bisection Bandwidth measures the **Global Connectivity** of the fabric, identifying the narrowest bottleneck in the cluster.
3.  **Common Pitfall:** A frequent misconception is that adding more switches always increases bisection bandwidth. In reality, it is a **Topology Property**: if the upper tiers of the network are oversubscribed, the bisection bandwidth will be significantly lower than the sum of the edge links.

:::

For ML training, where all-to-all communication is standard, full bisection bandwidth is the foundational requirement. A fabric that falls short forces every synchronization step to bottleneck at the narrowest cross-section, and the entire cluster idles while gradients trickle through.

::: {.callout-definition title="Non-blocking Fabric"}

***Non-blocking Fabric***\index{Non-blocking Fabric!definition} is a network topology in which any permutation of input-output port pairs can communicate simultaneously at full line rate without internal contention.

1.  **Significance (Quantitative):** In ML fleets, it ensures that synchronization traffic from any subset of accelerators does not compete for shared links, maximizing the **System Throughput ($\eta$)** by eliminating network-induced stalls.
2.  **Distinction (Durable):** Unlike **Oversubscribed Fabrics** (common in web datacenters), where upper-tier links are shared among many lower-tier nodes, a Non-blocking Fabric provides **Dedicated Path Capacity** for every possible pairing.
3.  **Common Pitfall:** A frequent misconception is that non-blocking means "zero congestion." In reality, **Endpoint Congestion** (Incast) can still occur if multiple senders target the same receiver simultaneously, regardless of the topology's internal capacity.

:::

These definitions shape every topology choice that follows: fat-trees achieve non-blocking behavior through redundant switch tiers, rail-optimized networks exploit workload structure to approximate it at lower cost, and dragonflies trade bisection bandwidth for reduced cabling.

::: {#fig-topology-bisection fig-env="figure" fig-pos="htb" fig-cap="Topology Bisection Bandwidth Comparison. Fat-Tree architectures provide full bisection bandwidth, while other topologies trade this off for cost or latency." fig-alt="Bar chart or comparison diagram showing bisection bandwidth across Fat-Tree, Torus, and other network topologies."}
```{=latex}
\begin{tikzpicture}
    \usefont{T1}{phv}{m}{n}
    \definecolor{BlueL}{HTML}{006395}
    \definecolor{GreenL}{HTML}{008F45}
    \definecolor{RedL}{HTML}{CB202D}
    \definecolor{OrangeL}{HTML}{CC5500}
    \draw[thick, ->] (0,0) -- (10,0) node[anchor=north west] {Topology};
    \draw[thick, ->] (0,0) -- (0,5.5) node[anchor=south east] {Bisection \%};
    \foreach \y in {1.25, 2.5, 3.75, 5}
        \draw[gray!20, thin] (0,\y) -- (10,\y);
    \draw[dashed, thick, gray!80] (0,5) -- (10,5) node[anchor=south east] {\small Full Bisection};
    \node[anchor=east] at (0,5) {100\%};
    \node[anchor=east] at (0,2.5) {50\%};
    \node[anchor=east] at (0,0) {0\%};
    \fill[BlueL] (1,0) rectangle (2.5,5);
    \node[anchor=south] at (1.75,5) {\textbf{100\%}};
    \node[anchor=north, align=center, font=\small] at (1.75,0) {Fat-Tree};
    \fill[GreenL] (3,0) rectangle (4.5,2.5);
    \node[anchor=south] at (3.75,2.5) {\textbf{50\%}};
    \node[anchor=north, align=center, font=\small] at (3.75,0) {Rail-Opt};
    \fill[OrangeL] (5,0) rectangle (6.5,3.75);
    \node[anchor=south] at (5.75,3.75) {\textbf{75\%}};
    \node[anchor=north, align=center, font=\small] at (5.75,0) {Dragonfly};
    \fill[RedL] (7,0) rectangle (8.5,1.25);
    \node[anchor=south] at (7.75,1.25) {\textbf{25\%}};
    \node[anchor=north, align=center, font=\small] at (7.75,0) {Torus};
\end{tikzpicture}
```
:::

### Top-of-Rack (ToR) and the Failure Domain {#sec-network-fabrics-tor}

\index{ToR}

The **Top-of-Rack (ToR)** switch serves as the fundamental physical aggregation point, defining both bandwidth limits and the minimum **failure domain** for the cluster. In a high-density AI configuration using standard DGX nodes, a single rack typically houses 4 nodes, each containing 8 GPUs, for a total of 32 accelerators. The ToR switch unites these devices but also creates a critical vulnerability: if the ToR fails, it instantly partitions 32 GPUs from the training job, forcing the global scheduler to halt and recover from the last checkpoint.

To mitigate congestion at this edge, network architects maximize the switch **radix**—the number of ports available. A modern switch with a radix of 64 allocates 32 ports downlink to the servers (ensuring full bandwidth for the 32 GPUs) and 32 ports uplink to the spine. This 1:1 subscription ratio guarantees non-blocking performance at the rack level. For our 1,000-GPU cluster, the physical topology comprises approximately 32 such racks. The job scheduler must be topology-aware, spreading replicas across these failure domains to ensure that a single lost rack does not take down the entire training run (@sec-fleet-orchestration).

### Fat-Tree (Clos) Networks {#sec-network-fabrics-fat-tree}

\index{fat-tree}\index{Clos network}

::: {.callout-definition title="Fat-Tree"}

***Fat-Tree***\index{Fat-Tree!definition} is a hierarchical network topology where the aggregate bandwidth increases toward the root to provide multiple equal-cost paths between any two nodes.

1.  **Significance (Quantitative):** It is the standard for providing **Full Bisection Bandwidth ($BW_{bisect}$)**. It ensures that the fabric can sustain simultaneous full-rate communication from all endpoints, which is a non-negotiable requirement for the AllReduce collectives that dominate training.
2.  **Distinction (Durable):** Unlike a **Standard Tree** (where links near the root become bottlenecks), a Fat-Tree uses redundant switches and links to ensure the network is **Non-blocking**.
3.  **Common Pitfall:** A frequent misconception is that Fat-Trees are the most "efficient" topology. In reality, they are the most **Reliable but Expensive**: they require a massive number of switches ($O(N \log N)$) and complex cabling compared to more specialized topologies like Dragonflies or Torus networks.

:::

The fat-tree is the industry standard for ML clusters because it strictly guarantees full bisection bandwidth—a non-negotiable requirement for the AllReduce collective, which demands simultaneous, all-to-all communication. The network is constructed in hierarchical tiers: **Leaf** switches (ToR) connect directly to servers, **Spine** switches interconnect all leaves within a locality domain known as a **pod**, and **Core** switches bind multiple pods together.

A fat-tree built from switches with radix $k$ supports $N = k^3/4$ hosts. In a two-tier network using radix-64 switches, a single pod can support up to 1,024 GPUs—perfectly accommodating our 1,000-GPU reference cluster with only leaf and spine layers. Scaling beyond this requires a three-tier architecture with core switches, enabling the fabric to reach `{python} fat_tree_hosts` hosts. The infrastructure cost is substantial: a non-blocking $k=64$ tree requires 4,096 switches. At \$10,000 to \$50,000 per switch, the switching layer alone represents \$40 to \$200 million.

However, scale imposes a latency tax. Typical **hop counts** are 2 for intra-pod (leaf-spine-leaf) and 4 for inter-pod (leaf-spine-core-spine-leaf). Each additional hop introduces serialization delay and switch processing time, adding microseconds to the $\alpha$ latency term that accumulates during the thousands of synchronization steps in a training run.

### Rail-Optimized Topology {#sec-network-fabrics-rail-optimized}

\index{rail-optimized topology}

In distributed training, **tensor parallelism** creates a deterministic and highly stratified communication pattern that standard topologies fail to exploit. Because large models partition individual matrix multiplications across GPUs, GPU 0 on one node must frequently exchange activations with GPU 0 on other nodes, but rarely with GPU 1. A **rail-optimized topology** physically hardwires this logic by isolating these communication paths into dedicated networks. Instead of connecting all 8 GPUs in a node to a single ToR switch, the network connects all GPU 0s across the entire cluster to a dedicated "Rail 0" switch fabric, all GPU 1s to "Rail 1," and so on.

For our 1,000-GPU cluster spanning 125 nodes, this creates 8 parallel networks of 125 GPUs each, allowing tensor-parallel traffic to traverse a single switch hop rather than the multi-hop leaf-spine-leaf path required by a standard fat-tree. The latency benefit is significant for the frequent, small activation exchanges that tensor parallelism demands. However, this architecture introduces a sharp trade-off: data parallelism requires global synchronization across *all* GPUs, crossing all rails. Modern clusters therefore employ a hybrid approach, using rail-optimized leaves for tensor-parallel traffic while bridging the rails with a full fat-tree spine to support the global AllReduce patterns of data parallelism.

### Dragonfly and Torus Alternatives {#sec-network-fabrics-dragonfly}

\index{dragonfly topology}\index{torus}

**Dragonfly** topologies organize switches into high-radix groups, where each group functions as a fully connected island. These groups are then connected to one another via global optical links. This hierarchical structure minimizes the number of expensive long-distance cables required to scale, significantly reducing cost compared to a fat-tree. However, this efficiency comes at the price of oversubscription: while bandwidth within a group is non-blocking (100%), the bandwidth between groups is typically designed to be only 25–50% of the aggregate injection rate. For ML workloads, this creates a binary performance cliff based on job placement. A training job that fits entirely within a single dragonfly group sees full line-rate performance; a job that spans multiple groups is throttled by the limited global bandwidth, potentially suffering a 2–4$\times$ slowdown. Consequently, dragonfly fabrics require topology-aware schedulers that rigidly pack jobs into groups to avoid crossing the oversubscribed global links.

**Torus** topologies connect each node directly to its neighbors in a multidimensional grid, most commonly a 3D torus where every node links to its six adjacent peers (up/down, left/right, front/back). This design offers exceptionally high local bandwidth with minimal switching hardware, as connections travel only 1–2 hops to reach neighbors. However, global communication requires traversing the diameter of the mesh ($O(N^{1/3})$ hops), making latency scale poorly with cluster size. Google adopted this architecture for its TPU pods because Transformer training is dominated by data parallelism and pipeline parallelism, both of which use nearest-neighbor communication patterns that map naturally onto the physical grid. The limitation becomes apparent with architectures like Mixture-of-Experts (MoE), which rely on AllToAll communication patterns; on a torus, these random permutations congest the limited bisection bandwidth of the mesh, causing performance to degrade significantly compared to a switch-based fat-tree.

The trade-offs between these topologies become stark when quantified for a large-scale deployment. Consider a 4,096-GPU cluster. A non-blocking fat-tree requires approximately 2,048 switches and 40,000 optical cables to deliver 100% bisection bandwidth, enabling any GPU to communicate with any other at full speed. A 3D torus connecting the same nodes might use zero external switches (relying on direct host-to-host links) and only short copper cables, but offers only a fraction of the bisection bandwidth (scaling with $N^{2/3}$). This explains the divergence in industry architectures: Google's TPU Pods employ torus topologies because their workloads—primarily Transformer training—are remarkably predictable, dominated by nearest-neighbor communication that maps perfectly to the 3D grid. GPU clusters overwhelmingly favor fat-trees because their workloads are more diverse, ranging from recommendation systems to graph neural networks, and rely heavily on global AllReduce patterns that require the full bisection bandwidth only a tree can provide. A torus saves millions in switch costs but rigidly constrains the software; a fat-tree costs more but provides the universality needed for general-purpose AI research.

@fig-network-topologies illustrates the structural differences between these common families.

::: {#fig-network-topologies fig-env="figure" fig-pos="htb" fig-cap="**Network Topologies for ML**. (A) Fat-Tree provides full bisection bandwidth through hierarchical switch layers. (B) Torus connects neighbors, optimizing for local communication patterns such as those in TPU pods. (C) Rail-Optimized designs dedicate switch infrastructure to corresponding accelerator positions across nodes, minimizing hop count for tensor parallelism." fig-alt="Three-panel diagram: Fat-Tree with hierarchical switches, Torus with neighbor connections, Rail-Optimized with dedicated switch rails."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{NodeColor}{HTML}{D1E6F3}
  \definecolor{SwitchColor}{HTML}{006395}
  \definecolor{RailColor}{HTML}{CC5500}
  \definecolor{GPUColor}{HTML}{F5D2D5}

  \tikzset{
    switch/.style={circle, fill=SwitchColor, draw=black!70, inner sep=2pt, minimum size=0.45cm, text=white, font=\tiny\bfseries},
    node/.style={rectangle, fill=NodeColor, draw=black!70, inner sep=2pt, minimum size=0.4cm}
  }

  % Fat Tree
  \begin{scope}
    \node[anchor=south, crimson] at (2, 3.5) {\textbf{A. Leaf-Spine (Fat-Tree)}};
    % Spine (use letter suffixes to avoid decimal-point node name ambiguity)
    \node[switch] (sA) at (0.5, 3) {};
    \node[switch] (sB) at (1.5, 3) {};
    \node[switch] (sC) at (2.5, 3) {};
    \node[switch] (sD) at (3.5, 3) {};
    % Leaf
    \foreach \x in {0, 1, 3, 4} \node[switch] (l\x) at (\x, 1.5) {};
    % Nodes
    \node[node] (nA) at (-0.25, 0) {};
    \node[node] (nB) at (0.25, 0) {};
    \node[node] (nC) at (0.75, 0) {};
    \node[node] (nD) at (1.25, 0) {};
    \node[node] (nmA) at (2.75, 0) {};
    \node[node] (nmB) at (3.25, 0) {};
    \node[node] (nmC) at (3.75, 0) {};
    \node[node] (nmD) at (4.25, 0) {};

    % Connections
    \foreach \s in {sA, sB, sC, sD} {
        \draw[black!40, thin] (\s) -- (l0);
        \draw[black!40, thin] (\s) -- (l1);
        \draw[black!40, thin] (\s) -- (l3);
        \draw[black!40, thin] (\s) -- (l4);
    }
    \draw[black!60] (l0) -- (nA); \draw[black!60] (l0) -- (nB);
    \draw[black!60] (l1) -- (nC); \draw[black!60] (l1) -- (nD);
  \end{scope}

  % Rail Optimized
  \begin{scope}[shift={(6,0)}]
    \node[anchor=south, crimson] at (1.5, 3.5) {\textbf{C. Rail-Optimized}};

    % Rail Switches
    \foreach \x in {0, 1, 2, 3} \node[switch, fill=RailColor] (rs\x) at (\x, 3) {R\x};

    % Nodes
    \node[draw=black!50, rounded corners=2pt, fit={(0,-0.5) (3, 0.5)}, inner sep=4pt, fill=black!5] (host1) {};
    \node[anchor=west, font=\tiny\bfseries] at (host1.west) {Node 1};

    \node[draw=black!50, rounded corners=2pt, fit={(0,-2.0) (3, -1.0)}, inner sep=4pt, fill=black!5] (host2) {};
    \node[anchor=west, font=\tiny\bfseries] at (host2.west) {Node 2};

    % GPUs
    \foreach \x in {0, 1, 2, 3} {
        \node[node, fill=GPUColor] (g1\x) at (\x, 0) {};
        \node[node, fill=GPUColor] (g2\x) at (\x, -1.5) {};

        \draw[thick, RailColor] (g1\x) -- (rs\x);
        \draw[thick, RailColor] (g2\x) -- (rs\x);
    }
  \end{scope}
\end{tikzpicture}
```
:::

::: {.callout-checkpoint}
## Topology Selection
Your choice of network topology dictates the upper bound of training efficiency. For a standard data-parallel job (1), bandwidth is dominated by AllReduce; a non-blocking Fat Tree (Clos) is ideal to maximize bisection bandwidth. For the Mixture-of-Experts model (2), the workload is AllToAll-dominant with bursty, sparse communication; a Dragonfly or high-radix topology might be preferred to minimize hop count, though congestion control becomes critical. For the 2D mesh computation (3), likely a physics simulation or a specific parallel strategy, a Torus topology offers the best locality, mapping the physical neighbor links directly to the hardware and avoiding the core switch bottlenecks entirely.
:::

::: {#nb-bisection-bottleneck .callout-notebook title="The Bisection Bottleneck"}

```{python}
#| echo: false
#| label: bisection-bottleneck
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BISECTION BOTTLENECK ANALYSIS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Network Topology - Bisection Bandwidth impact.
# │
# │ Goal: Quantify the slowdown from network oversubscription.
# │ Show: That cost-optimized (oversubscribed) networks throttle training.
# │ How: Calculate AllReduce time for 1:1 vs 4:1 oversubscription.
# │
# │ Imports: mlsys.constants, NetworkFabricsSetup.ib_ndr_gbs_val
# │ Exports: bisec_time_a_ms, bisec_time_b_ms, waste_millions
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class BisectionBottleneck:
    """
    Scenario: 1024 accelerators (128 nodes) running AllReduce.
    Comparison of non-blocking (1:1) vs oversubscribed (4:1) fabric.
    """

    # ┌── 1. PARAMETERS (Inputs) ───────────────────────────────────────────────
    num_nodes = 128
    injection_bw_gbs = NetworkFabricsSetup.ib_ndr_gbs_val
    gradient_size_gb = 100
    oversub_ratio_b = 4
    cluster_cost_m = 300
    comm_fraction = 0.30

    # ┌── 2. CALCULATION (The Physics) ─────────────────────────────────────────
    # Bisection bandwidth (aggregate across the cut)
    total_bisec_bw_a = num_nodes * injection_bw_gbs
    total_bisec_bw_b = total_bisec_bw_a / oversub_ratio_b

    # Time = Data / Bandwidth
    time_a_s = gradient_size_gb / total_bisec_bw_a
    time_b_s = gradient_size_gb / total_bisec_bw_b

    # Economic impact
    # Relative throughput = 1 / ((1 - comm_frac) + (comm_frac * oversub_ratio))
    rel_throughput = 1 / ((1 - comm_fraction) + (comm_fraction * oversub_ratio_b))
    waste_val = (1 - rel_throughput) * cluster_cost_m

    # ┌── 3. INVARIANTS (Guardrails) ───────────────────────────────────────────
    check(time_b_s == time_a_s * 4, "Scenario B must be 4x slower than Scenario A")
    check(waste_val > 10, f"Waste should be significant, got ${waste_val:.1f}M")

    # ┌── 4. OUTPUTS (Formatting) ──────────────────────────────────────────────
    time_a_ms = f"{time_a_s * 1000:.0f}"
    time_b_ms = f"{time_b_s * 1000:.0f}"
    waste_m = f"{waste_val:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
bisec_time_a_ms = BisectionBottleneck.time_a_ms
bisec_time_b_ms = BisectionBottleneck.time_b_ms
waste_millions = BisectionBottleneck.waste_m
```

**Problem**: You have 1,024 accelerators across 128 nodes. Each node has `{python} ib_ndr_gbps` Gb/s (`{python} ib_ndr_gbs` GB/s) injection bandwidth. You run an AllReduce job that requires full bisection bandwidth.

**Scenario A (non-blocking fat-tree)**: 1:1 oversubscription ratio. Bisection bandwidth = $128 \times 50 = 6{,}400$ GB/s = 6.4 TB/s.

**Scenario B (cost-optimized)**: 4:1 oversubscription at the spine layer. Bisection bandwidth = $6{,}400 / 4 = 1{,}600$ GB/s = 1.6 TB/s.

**The calculation**: Suppose the AllReduce must exchange 100 GB of gradient data (approximately 25 billion FP32 parameters).

- Time on Scenario A: `{python} bisec_time_a_ms` ms
- Time on Scenario B: `{python} bisec_time_b_ms` ms

**The systems conclusion**: Saving money on spine switches (Scenario B) slows every synchronization step of the entire cluster by approximately 4$\times$. If AllReduce accounts for 30% of each training iteration, the cluster's effective throughput drops significantly. For a \$300 million supercomputer, this amounts to wasting approximately **\$`{python} waste_millions` million** worth of accelerator time waiting for the network. Network oversubscription is false economy for training workloads.

:::

The preceding analysis assumed a single workload type. In practice, clusters serve multiple workloads with different communication patterns, and topology selection must balance their competing demands.

::: {.callout-checkpoint title="Topology Selection for Your Workload"}

You are designing the network for a new ML cluster that will run two primary workloads: (1) training a 175B-parameter language model using 3D parallelism (tensor, pipeline, and data parallelism), and (2) serving a Mixture-of-Experts model that relies heavily on AllToAll communication to route tokens to the correct experts.

1. The training workload's tensor parallelism operates within groups of 8 GPUs, while data parallelism operates across all nodes. Which topology (fat-tree, rail-optimized, or hybrid) best matches this communication pattern, and why?
2. The MoE serving workload generates unpredictable AllToAll traffic where any GPU may need to communicate with any other. How does this change your topology choice compared to the training workload?
3. If budget constraints force you to accept 2:1 oversubscription at the spine layer, which workload suffers more, and what scheduling strategy could mitigate the impact?

:::

**Transition:** Topology provides the structural capacity for our 1,000-GPU cluster to communicate, but it does not guarantee performance. When all 1,000 GPUs simultaneously inject 350 GB of gradient traffic into the fabric, that theoretical capacity collides with the reality of **Fabric Behavior**. We must now examine how congestion control and routing dynamics determine whether this traffic flows smoothly or gridlocks under the strict synchronization of the BSP model.

## Level 4: Fabric Behavior (Congestion, Routing) {#sec-network-fabrics-behavior}

Real fabrics deviate from theoretical full-bisection bandwidth due to **congestion**, but the impact of this congestion is qualitatively different in ML clusters than in general-purpose networks. Web traffic is stochastic and asynchronous; if one user's request is delayed by 50 ms, it does not penalize the thousands of other users on the network. ML training, by contrast, is governed by the Bulk Synchronous Parallel (BSP) execution model, where every GPU must complete its communication before *any* GPU can proceed to the next compute step. This creates a weakest-link dynamic: the iteration time for the entire cluster is dictated by the slowest flow in the fabric. If a single link out of 10,000 becomes congested and doubles its latency, the effective throughput of the entire supercomputer drops for that step. In this synchronized regime, tail latency is not an outlier metric—it is the dominant performance constraint.

::: {.callout-definition title="Bulk Synchronous Parallel (BSP)"}

***Bulk Synchronous Parallel (BSP)***\index{Bulk Synchronous Parallel!definition} is a parallel computing model where execution proceeds in discrete **Supersteps**, each consisting of local computation, global communication, and a barrier synchronization.

1.  **Significance (Quantitative):** It is the standard execution model for synchronous distributed training. Within the **Iron Law**, BSP dictates that the total time $T$ is governed by the **Slowest Worker** (Straggler), making the **System Efficiency ($\eta$)** highly sensitive to network jitter and hardware variation.
2.  **Distinction (Durable):** Unlike **Asynchronous Parallelism**, which allows workers to use stale weights, BSP ensures **Mathematical Equivalence** to single-device training by enforcing a global state update at every step.
3.  **Common Pitfall:** A frequent misconception is that BSP is "inefficient" compared to async models. In reality, while it has higher **Synchronization Overhead ($L_{lat}$)**, it provides superior **Convergence Stability**, often leading to lower total operations ($O$) to reach a target loss.

:::

Because BSP enforces a global barrier at each superstep, the entire cluster moves at the speed of the slowest packet. A single congested switch port can stall 10,000 GPUs, making **tail latency (P99)** more critical than average throughput.

### Priority Flow Control (PFC) {#sec-network-fabrics-pfc}

\index{Priority Flow Control}

::: {.callout-definition title="Priority Flow Control"}

***Priority Flow Control (PFC)***\index{Priority Flow Control!definition} is a link-level mechanism that prevents buffer overflow by sending PAUSE frames to the upstream sender on a per-priority basis.

1.  **Significance (Quantitative):** It is the foundation for **Lossless Ethernet** (RoCE), ensuring that RDMA traffic is not dropped due to buffer exhaustion. This maintains the high **Effective Bandwidth ($BW$)** required for gradient synchronization.
2.  **Distinction (Durable):** Unlike **Standard Flow Control** (which pauses all traffic on a link), PFC allows latency-sensitive control traffic to proceed while only pausing the congested data-intensive queues.
3.  **Common Pitfall:** A frequent misconception is that PFC "solves" congestion. In reality, it transforms packet loss into **Congestion Spreading**: a single slow receiver can trigger a backpressure cascade that freezes unrelated flows across the entire fabric, significantly increasing the **Tail Latency ($L_{lat}$)** of the cluster.

:::

The danger of PFC lies in its cascading nature. When a switch port's buffer fills, it sends a PAUSE frame upstream, which causes *that* switch's buffers to fill, which triggers *another* PAUSE frame further upstream. In theory, this backpressure should throttle the source. In practice, a single slow receiver can propagate pauses across the entire fabric in milliseconds, freezing links that have no direct relationship to the original congestion point. This cascading behavior, known as **congestion spreading** or **victim flows**, is the primary operational risk of PFC-based lossless Ethernet.

::: {#fig-congestion-cascade fig-env="figure" fig-pos="htb" fig-cap="PFC Pause Frame Propagation. Congestion at a tail switch (S4) triggers a backpressure cascade, freezing the entire communication path upstream." fig-alt="Diagram showing PFC pause frame propagation from congested switch S4 upstream through the network path."}
```{=latex}
\begin{tikzpicture}
    \usefont{T1}{phv}{m}{n}
    \definecolor{BlueL}{HTML}{006395}
    \definecolor{RedL}{HTML}{CB202D}
    \definecolor{GreenL}{HTML}{008F45}
    \foreach \x/\name/\t in {0/S1/t=3ms, 3/S2/t=2ms, 6/S3/t=1ms, 9/S4/t=0} {
        \node[draw, thick, fill=gray!10, minimum width=1.5cm, minimum height=1cm, rounded corners] (sw\name) at (\x, 2) {\textbf{\name}};
        \node[font=\footnotesize, below] at (\x, 1.4) {\t};
        \node[draw, circle, fill=black!10, minimum size=0.5cm] (gpu\name) at (\x, 0) {G};
        \draw[thick] (sw\name) -- (gpu\name);
    }
    \draw[->, thick, GreenL, line width=1mm] (-1.5, 2.3) -- (swS1.west);
    \draw[->, thick, GreenL, line width=1mm] (swS1.east) -- (swS2.west);
    \draw[->, thick, GreenL, line width=1mm] (swS2.east) -- (swS3.west);
    \draw[->, thick, GreenL, line width=1mm] (swS3.east) -- (swS4.west);
    \node[GreenL, font=\small] at (4.5, 2.6) {Data Flow};
    \draw[->, thick, RedL, dashed, line width=0.8mm] (swS4.south west) to[bend left=20] node[midway, below, font=\scriptsize] {PAUSE} (swS3.south east);
    \draw[->, thick, RedL, dashed, line width=0.8mm] (swS3.south west) to[bend left=20] node[midway, below, font=\scriptsize] {PAUSE} (swS2.south east);
    \draw[->, thick, RedL, dashed, line width=0.8mm] (swS2.south west) to[bend left=20] node[midway, below, font=\scriptsize] {PAUSE} (swS1.south east);
    \node[star, star points=10, fill=RedL, text=white, minimum size=1cm] at (9, 3) {\textbf{!}};
    \node[RedL, font=\bfseries] at (9, 3.8) {CONGESTION};
    \node[anchor=north, RedL, font=\bfseries] at (0, -0.5) {FROZEN};
    \node[anchor=north, RedL, font=\bfseries] at (3, -0.5) {PAUSED};
    \node[anchor=north, RedL, font=\bfseries] at (6, -0.5) {PAUSED};
    \node[anchor=north, RedL, font=\bfseries] at (9, -0.5) {SOURCE};
\end{tikzpicture}
```
:::

::: {.callout-perspective title="War Story: The PFC Storm That Froze a Cluster"}

In 2022, a large RoCE-based training cluster experienced a complete fabric freeze that lasted over 40 minutes. The root cause was a single malfunctioning transceiver on one leaf switch that intermittently dropped its link speed from 400 Gbps to 100 Gbps. The reduced bandwidth caused the switch port's buffer to fill during a routine AllReduce burst, triggering PFC PAUSE frames. Because the training job spanned the entire cluster, the pauses propagated through the spine layer and back down to every leaf switch in the fabric. Within 200 milliseconds, every port in the cluster was paused. No data moved. All 4,096 GPUs sat idle.

The insidious aspect was that no alarms fired. PFC pauses are a *normal* flow-control mechanism, so monitoring systems treated them as routine. The team only noticed when training throughput dropped to zero. Diagnosis took 30 minutes because standard tools (`ping`, `iperf`) showed the network was "up" but simply not moving data. The fix was to identify and replace the faulty transceiver, but the investigation revealed a deeper problem: the cluster lacked PFC watchdog timers that would have detected the storm and disabled PFC on the offending port within seconds. After the incident, the team deployed per-port PFC pause counters with alerting thresholds and configured hardware watchdog timers to automatically disable PFC on any port sustaining pauses for more than 50 milliseconds.

This incident illustrates why InfiniBand's credit-based flow control is fundamentally more robust: credits are consumed and replenished per-hop, so a slow receiver cannot propagate backpressure beyond its immediate neighbor. PFC, by contrast, operates as a blunt per-priority mechanism that has no concept of individual flow isolation.

:::

### Proactive Congestion Control: DCQCN and HPCC {#sec-network-fabrics-congestion-control}

\index{ECN}\index{DCQCN}\index{HPCC}

To avoid the blunt instrument of PFC pauses, modern fabrics rely on proactive congestion control to modulate injection rates *before* buffers overflow. In BSP workloads, this is not merely a throughput optimization but a latency requirement: if a single packet is delayed by a congested switch queue, the entire cluster must wait for that straggler to complete the synchronization step. The tail latency of the network effectively becomes the average step time of the training job.

**DCQCN**[^fn-dcqcn] operates as a reactive feedback loop using Explicit Congestion Notification (ECN). When a switch's queue depth exceeds a configured threshold, it marks the ECN bit in the packet header. The receiver echoes this mark back to the sender via a Congestion Notification Packet (CNP), prompting the sender to reduce its injection rate using a multiplicative-decrease algorithm. While widely supported, DCQCN suffers from a "blind driver" problem: the sender receives a binary signal indicating congestion exists but lacks precise data on the severity. Consequently, senders must oscillate their rates to find the safe operating point, typically capping stable link utilization at 85–90%.

**HPCC**[^fn-hpcc] addresses this opacity by leveraging In-Network Telemetry (INT). Instead of a simple bit mark, switches append precise metadata—current queue depth, link utilization, and timestamps—to every packet header. This provides the sender with a full dashboard of the network state, allowing it to calculate the exact allowable transmission rate within a single round-trip time. By reacting to precise telemetry rather than binary signals, HPCC virtually eliminates queue buildup and stabilizes link utilization above 95%, significantly reducing the tail latency variance for bursty AllToAll traffic. The primary trade-off is hardware support: while DCQCN functions on standard ECN-capable Ethernet switches, HPCC requires programmable switches capable of pushing INT metadata at line rate.

[^fn-dcqcn]: **DCQCN (Data Center Quantized Congestion Notification)**: A congestion control protocol developed by Microsoft that combines ECN marking with rate-based flow control, designed specifically for RDMA over Converged Ethernet (RoCE) fabrics.

[^fn-hpcc]: **HPCC (High Precision Congestion Control)**: A protocol developed by Alibaba that uses in-network telemetry (INT) to achieve near-zero queue buildup, enabling higher link utilization than ECN-based schemes.

### Adaptive Routing {#sec-network-fabrics-adaptive-routing}

\index{adaptive routing}

Static routing protocols like ECMP (Equal-Cost Multi-Path) distribute traffic by hashing flow headers to fixed paths. While statistically sound for millions of small web requests, this approach fails for the elephant flows typical of ML training. Consider a scenario with 4 equal-cost paths and 8 large gradient flows. A perfect distribution would place 2 flows on each link. However, static hashing frequently results in collisions where one link carries 4 flows while another sits idle. In a synchronized training step, the completion time is dictated by the overloaded link, effectively halving the network's useful bandwidth.

**Adaptive Routing** mitigates this by allowing switches to dynamically select the output port based on real-time queue depth rather than a static hash. The implementation is protocol-dependent: InfiniBand fabrics perform *packet-level* adaptive routing, spraying individual packets across all available lanes because the hardware transport guarantees in-order delivery at the destination. Ethernet fabrics typically employ *flowlet switching*, rerouting bursts of packets only when a sufficient time gap is detected, to avoid the performance penalties associated with packet reordering.

This mechanism becomes essential for Mixture-of-Experts (MoE) models. Unlike the predictable Ring AllReduce pattern, MoE models utilize **AllToAll** communication, where every GPU sends data to every other GPU containing active experts. For a 175B-parameter model using expert parallelism with 64 experts, this generates a dense $64 \times 64$ traffic matrix—4,096 simultaneous flows. Under static ECMP, hash collisions are statistically guaranteed to create stragglers; adaptive routing is required to distribute this traffic evenly across the fabric's bisection bandwidth.

### The Incast Problem in ML {#sec-network-fabrics-incast}

\index{incast}

::: {.callout-definition title="Incast"}

***Incast***\index{Incast!definition} is a many-to-one traffic pattern where multiple senders simultaneously transmit data to a single receiver, overwhelming the receiver's switch port buffer.

1.  **Significance (Quantitative):** In ML fleets, incast occurs during the **Reduce Phase** of collective operations (e.g., AllReduce). It causes momentary buffer exhaustion, triggering packet drops or flow-control pauses that increase the **Tail Latency ($L_{lat}$)** of the entire cluster.
2.  **Distinction (Durable):** Unlike **General Congestion** (which occurs on shared core links), Incast is an **Endpoint Bottleneck**: it occurs even if the internal network fabric has infinite capacity.
3.  **Common Pitfall:** A frequent misconception is that incast is "rare." In reality, because ML training is **Synchronously Burstive**, incast is a routine event at the end of every layer's gradient computation, requiring proactive mitigation through jitter-aware scheduling or large switch buffers.

:::

ML training is particularly susceptible to incast because of its synchronized communication patterns. When a layer finishes backward computation, thousands of nodes simultaneously initiate AllReduce, targeting the same switch ports. In our 175B model training across 1,000 GPUs, each AllReduce involves every node injecting data simultaneously, creating a burst that can momentarily exceed the fabric's capacity at specific switch ports. Production clusters mitigate this by:
1. **Layer-staggering**: Starting AllReduce for early layers while later layers are still computing.
2. **Algorithm selection**: Using tree-based AllReduce instead of ring-based ones to reduce concurrent flow counts.
3. **QoS Priority**: Tagging gradient traffic with highest priority to ensure it isn't delayed by background storage or management traffic.

**Transition:** Real fabrics have congestion and tail latency; **Level 5** shows how we build end-to-end clusters anyway.

## Level 5: Cluster Design and Case Studies {#sec-network-fabrics-cluster}

The final level of the stack is the **Cluster**, where wires, transport, topology, and congestion control combine into a coherent system. The goal of cluster design is to provide an end-to-end **Gradient Bus** that makes thousands of distributed GPUs feel like a single machine. This is where the abstraction layers collapse into concrete engineering decisions: which cables to buy, how to wire the racks, which protocol to deploy, and how to validate that the resulting fabric actually delivers the bandwidth the training job expects. Two dominant architectures have emerged in production—one built on InfiniBand and one on Ethernet—and their contrasting design philosophies illuminate the trade-offs that define modern ML infrastructure.

### The GPU-to-GPU "Gradient Bus" {#sec-network-fabrics-gradient-bus}

\index{Gradient Bus}

In a well-designed cluster, the network fabric acts not just as a pipe but as a scheduler-aware extension of the system bus. The intra-node (NVLink) bandwidth is ~`{python} nvlink_to_ib_ratio`$\times$ higher than inter-node (InfiniBand/RoCE) bandwidth, and the primary mechanism for hiding this cliff is **communication-computation overlap**. During the backward pass, gradients for the final layers are computed first. Instead of waiting for the entire backward pass to finish, the system triggers an asynchronous AllReduce for these gradients immediately while the GPUs continue computing gradients for earlier layers. If the backward pass requires 500 ms of computation and the AllReduce takes 300 ms, perfect overlap masks the entire communication cost behind computation, reducing the effective overhead to $\max(0, 300 - 500) = 0$. In practice, dependency chains and resource contention limit this efficiency to 60–80%, leaving a **last-mile problem**: the gradients for the very first layer (computed last) have no subsequent computation to hide behind, exposing their raw transfer time to the critical path.

The bandwidth staircase shown in @fig-hierarchical-staircase dictates the parallelism strategy to mitigate this cliff. **Tensor Parallelism**, requiring massive bandwidth for frequent activation exchanges, is confined to the NVLink domain within a node. **Pipeline Parallelism**, involving point-to-point transfers of activations between pipeline stages, spans the InfiniBand links between nodes. **Data Parallelism**, tolerant of lower bandwidth through gradient accumulation and overlap, stretches across the full fabric.

::: {#fig-hierarchical-staircase fig-env="figure" fig-pos="htb" fig-cap="The Hierarchical Bandwidth Staircase. Communication bandwidth drops by orders of magnitude as the distance from the chip increases, dictating parallelism strategies." fig-alt="Staircase or stepped diagram showing bandwidth decreasing from on-chip through NVLink, PCIe, and network levels."}
```{=latex}
\begin{tikzpicture}
    \usefont{T1}{phv}{m}{n}
    \definecolor{BlueL}{HTML}{006395}
    \definecolor{GreenL}{HTML}{008F45}
    \definecolor{OrangeL}{HTML}{CC5500}
    \definecolor{RedL}{HTML}{CB202D}
    \fill[BlueL!20] (0, 0) rectangle (9, 1);
    \draw[BlueL, thick] (0, 0) rectangle (9, 1);
    \node[anchor=west, BlueL, font=\bfseries] at (0.2, 0.5) {NVLink (Intra-Node)};
    \node[anchor=east, black, font=\bfseries] at (8.8, 0.5) {900 GB/s};
    \node[anchor=west, font=\small\itshape] at (9.2, 0.5) {$\leftarrow$ Tensor Parallelism};
    \fill[GreenL!20] (0, -1.2) rectangle (5, -0.2);
    \draw[GreenL, thick] (0, -1.2) rectangle (5, -0.2);
    \node[anchor=west, GreenL, font=\bfseries] at (0.2, -0.7) {PCIe (Chip-to-Host)};
    \node[anchor=east, black, font=\bfseries] at (4.8, -0.7) {64 GB/s};
    \fill[OrangeL!20] (0, -2.4) rectangle (4, -1.4);
    \draw[OrangeL, thick] (0, -2.4) rectangle (4, -1.4);
    \node[anchor=west, OrangeL, font=\bfseries] at (0.2, -1.9) {InfiniBand (Inter-Node)};
    \node[anchor=east, black, font=\bfseries] at (3.8, -1.9) {50 GB/s};
    \node[anchor=west, font=\small\itshape] at (5.2, -1.9) {$\leftarrow$ Pipeline Parallelism};
    \fill[RedL!20] (0, -3.6) rectangle (2.5, -2.6);
    \draw[RedL, thick] (0, -3.6) rectangle (2.5, -2.6);
    \node[anchor=west, RedL, font=\bfseries] at (0.2, -3.1) {Ethernet (Data Center)};
    \node[anchor=east, black, font=\bfseries] at (2.3, -3.1) {25 GB/s};
    \node[anchor=west, font=\small\itshape] at (3.2, -3.1) {$\leftarrow$ Data Parallelism};
    \draw[dashed, gray] (9, 0) -- (9, -3.6);
    \draw[->, thick] (-0.5, 0.5) -- (-0.5, -3.6) node[midway, left, rotate=90] {Increasing Distance / Latency};
\end{tikzpicture}
```
:::

### Case Study: NVIDIA DGX SuperPOD {#sec-network-fabrics-dgx-superpod}

The NVIDIA DGX SuperPOD architecture connects DGX H100 nodes using a two-tier InfiniBand network, serving as the reference implementation for the Gradient Bus concept. Each node acts as a dense compute island, with eight H100 GPUs connected via NVSwitch to provide `{python} nvlink_h100_gbs` GB/s of internal bandwidth. Externally, each GPU pairs with a ConnectX-7 NIC delivering `{python} ib_ndr_gbps` Gb/s of injection bandwidth. Across a standard scalable unit of 32 nodes (256 GPUs), this yields an aggregate injection bandwidth of 12.8 TB/s ($256 \times 50$ GB/s), ensuring the fabric can ingest gradients as fast as the accelerators produce them.

This architecture explicitly instantiates the five-level model constructed in this chapter. At Level 1, it minimizes latency by using passive copper (DAC) within the rack and active optics only for spine connections. At Level 2, it relies on InfiniBand's native credit-based flow control to guarantee a lossless medium without the fragility of Ethernet PFC. Level 3 implements a rail-optimized fat-tree: all GPU 0s across the cluster connect to the same leaf switch group, ensuring that latency-sensitive tensor parallelism operations traverse only a single switch hop. At Level 4, hardware-based adaptive routing sprays packets across all spine links to maximize bisection bandwidth for data parallelism. At Level 5, the design is modular: multiple SuperPODs connect via a core switching layer to form massive clusters. For our 175B-parameter model, the physical infrastructure would consist of approximately four SuperPOD units wired together, allowing 1,024 GPUs to function as a single synchronous instrument.

### Case Study: Meta Grand Teton {#sec-network-fabrics-grand-teton}

Meta's Grand Teton training cluster operates at a vastly different scale and philosophy, connecting over 16,000 H100 GPUs (2,000 nodes) into a single production fabric. Rather than InfiniBand, Meta employs RoCEv2 over a standard 400 GbE Ethernet fabric. The primary motivation is operational scale and supply chain resilience: by using Ethernet, Meta can source switches from multiple vendors and utilize the same optical infrastructure and management tooling shared by their front-end serving fleet, avoiding the operational silo of a dedicated InfiniBand island.

Making Ethernet perform like a dedicated HPC fabric at this scale requires significant engineering at Level 4. To prevent the PFC deadlocks described in @sec-network-fabrics-pfc, the fabric utilizes aggressive watchdog timers that disable flow control on stalled ports before congestion spreads. To handle the massive incast of 16,000-way AllReduce, Meta tunes DCQCN parameters (specifically the $K_{\min}$ and $K_{\max}$ thresholds) on a per-workload basis, ensuring that congestion notification packets are sent early enough to throttle senders before switch buffers overflow. For load balancing, the cluster moves beyond standard ECMP to use flowlet switching, where the switch hardware dynamically re-hashes flows to less-utilized paths during gaps in the packet stream. While this architecture achieves over 95% of line-rate bandwidth for large gradient transfers, it accepts a trade-off: small-message latency remains 3–5$\times$ higher than InfiniBand due to the deeper buffers and complex FEC required by Ethernet switches. For giant models where bandwidth dominates, this is an acceptable exchange; for latency-sensitive MoE routing, the penalty requires careful algorithmic compensation.

::: {.callout-perspective title="InfiniBand versus RoCE: The Industry Verdict"}

The coexistence of InfiniBand (NVIDIA DGX SuperPOD) and RoCE (Meta Grand Teton, Google) in production reflects a genuine trade-off rather than a clear winner. InfiniBand provides 30 to 50% lower tail latency and simpler lossless configuration. RoCE provides 20 to 40% lower switch costs and multi-vendor flexibility. For training runs where iteration time is measured in seconds, the latency difference is often absorbed into the noise. For inference serving with tight SLOs, the latency difference may matter.

The trend is toward convergence. NVIDIA's Spectrum-4 Ethernet switches incorporate InfiniBand-inspired adaptive routing and congestion control. Broadcom's Memory DCS chips add hardware support for RDMA-optimized switching. The distinction between the two ecosystems is narrowing, though it has not disappeared.

:::

### Looking Ahead: Next-Generation Interconnects {#sec-network-fabrics-next-gen}

\index{Ultra Ethernet}\index{co-packaged optics}\index{CXL}

As cluster sizes push past 100,000 accelerators, the physics tax of current interconnects—power, latency, and reliability—becomes the primary scaling bottleneck. Four emerging technologies promise to reshape the fabric.

The **Ultra Ethernet Consortium (UEC)** represents a clean-slate overhaul of Ethernet specifically for AI and HPC. Recognizing that TCP/IP is too heavy and RoCEv2's reliance on PFC is too fragile, UEC integrates HPC-native features directly into the standard: native **packet spraying** to utilize all paths without ECMP hashing collisions, hardware-enforced **in-order delivery** to simplify NIC design, and a new credit-based congestion control mechanism that eliminates the need for Priority Flow Control entirely. The goal is to provide the losslessness of InfiniBand with the ubiquity and multi-vendor economics of Ethernet.

The power consumption of pluggable transceivers is becoming unsustainable at scale. **Co-Packaged Optics (CPO)** addresses this by moving the optical engine from the faceplate directly into the switch ASIC package. By driving optical signals almost from the silicon itself, CPO eliminates the power-hungry electrical traces across the PCB, reducing power per port by 30–50%. For a 10,000-GPU cluster, eliminating pluggable modules could save over 100 kW of power—energy that can be redirected to computation. CPO also removes the transceiver as a discrete field-replaceable unit, eliminating a common point of mechanical failure.

The bandwidth march continues to **800G and 1.6T links** (XDR InfiniBand and 800GbE/1.6TbE). A single 1.6 Tbps port delivers the bandwidth of four 400G lanes, allowing next-generation switches to provide 51.2 Tbps of aggregate throughput. This density allows architects to flatten the topology: a cluster that previously required three tiers of switches might now be serviceable with two, halving the transceiver count and reducing tail latency by removing an entire hop of switching and FEC overhead.

Finally, **CXL (Compute Express Link)** is blurring the boundary between network and memory bus. While currently an intra-rack technology, CXL 3.0 enables **memory pooling**, allowing a GPU in one chassis to access host memory in another with load/store semantics rather than RDMA message passing. This capability suggests a future where the rigid distinction between intra-node NVLink domains and inter-node InfiniBand domains dissolves into a unified memory fabric, allowing the scheduler to compose virtual nodes of arbitrary size dynamically.

**Transition:** With the physical fabric established and production architectures demonstrated, a practical question remains: how do we share this expensive infrastructure across multiple teams and workloads without sacrificing the predictable performance that training demands?

## Network Virtualization {#sec-network-fabrics-virtualization}

\index{network virtualization}\index{SR-IOV}

Production ML clusters are rarely dedicated to a single training job, creating a massive economic imperative for efficient multi-tenancy. A \$300 million supercomputer that sits 30% idle because it cannot securely isolate concurrent workloads represents a \$90 million waste of capital. To recover this utility, the network must support virtualization along three orthogonal dimensions: **bandwidth partitioning** (guaranteeing minimum throughput), **latency determinism** (preventing head-of-line blocking), and **security isolation** (preventing memory snooping between tenants). Just as hypervisors decoupled the operating system from the CPU, technologies like SR-IOV and virtual lanes decouple the training job from the physical wire. For our 175B model, this means the training job can reliably consume 80% of the cluster's bisection bandwidth while a high-priority inference service and a background data preprocessing job share the remaining 20%, with the fabric enforcing hard boundaries that prevent the preprocessor's bursty traffic from stalling gradient updates.

### SR-IOV: Hardware NIC Virtualization {#sec-network-fabrics-sriov}

\index{SR-IOV}

How do cloud providers deliver bare-metal RDMA performance to virtualized GPU instances? The answer lies in **Single Root I/O Virtualization (SR-IOV)**, a standard that allows a physical NIC to present itself as multiple independent **Virtual Functions (VFs)**. Each VF has its own hardware queues, doorbell registers, and DMA mappings. By assigning a dedicated VF to each VM or container, the hardware creates a direct path for DMA operations that bypasses the host kernel and hypervisor completely. This passthrough architecture is critical for ML training because it reduces virtualization overhead to negligible levels—typically adding less than 2% latency compared to bare metal. In a Kubernetes environment, the RDMA device plugin manages this resource allocation, exposing available VFs to the scheduler so that pods can request guaranteed network access just as they request GPU cores.

However, this hardware-level isolation comes with a strict physical constraint: bandwidth partitioning. SR-IOV slices the NIC's total capacity, meaning that eight VFs sharing a `{python} ib_ndr_gbps` Gbps NIC will each be capped at 50 Gbps. For our 175B-parameter model training on a multi-tenant cluster, this partitioning is a double-edged sword. While it guarantees that a neighboring tenant cannot steal bandwidth, it also enforces a hard ceiling on peak throughput. The training job must be architected to operate within this slice, as no amount of software optimization can burst beyond the hardware-enforced limit of the VF.

### Traffic Isolation and Quality of Service {#sec-network-fabrics-qos}

\index{quality of service}

Consider a worst-case contention scenario on a shared cluster: our 175B model is midway through a latency-sensitive AllReduce operation when a neighboring job initiates a massive checkpoint save. Without strict isolation, this bursty 100 GB write—which takes just 2 seconds at full `{python} ib_ndr_gbps` Gbps line rate—could saturate the shared spine links, introducing queuing delays that increase the AllReduce time by 50% or more. To prevent this noisy-neighbor effect, modern fabrics rely on **Quality of Service (QoS)** mechanisms that enforce fairness at the packet level.

The primary tool is the **Virtual Lane (VL)** in InfiniBand (or **Traffic Class** in RoCE), which provides up to 16 independent logical channels on a single physical link. By mapping different traffic types to separate VLs, the network ensures that a saturation event in one lane does not block progress in another. Each VL maintains its own independent credit-based flow control: if the storage traffic for the checkpoint fills up its buffer, the switch pauses only that specific lane. Gradient updates, tagged with a high-priority service level, continue to flow through their reserved lane unimpeded. On the Ethernet side, **Enhanced Transmission Selection (ETS)** provides analogous bandwidth guarantees per traffic class, while advanced switch ASICs can partition their forwarding tables and buffer pools into isolated **network slices**, ensuring that congestion in one tenant's slice cannot trigger PFC pauses in another's.

Virtualization solves the sharing problem but makes performance diagnosis harder. When a training job slows down on a multi-tenant cluster, the cause could be a physical link degradation, a noisy neighbor exceeding its bandwidth allocation, or a misconfigured QoS policy. Systematic monitoring is essential to distinguish these cases.

## Monitoring and Debugging {#sec-network-fabrics-monitoring}

\index{network monitoring}

Network performance problems in ML clusters are insidious because they manifest as **silent waste** rather than explicit failures. A degraded transceiver causing a 10% reduction in effective bandwidth might slow each training iteration by only 2–3%—a drift easily masked by the natural variance of checkpointing or data loading. Over a 30-day training run on 1,000 GPUs, this invisible drag accumulates to over 72,000 wasted GPU-hours, burning nearly \$200,000 in cloud costs without triggering a single alarm. Traditional IT monitoring tools like SNMP or `ping` are insufficient here; they measure connectivity, not the sustained throughput required by RDMA. Effective observability requires a three-layer approach: **physical monitoring** (FEC errors, signal attenuation), **transport monitoring** (PFC pause frames, retransmission rates), and **application monitoring** (NCCL algorithmic bandwidth). Only by correlating signals across these layers can operators detect that a "slow training run" is actually caused by a single degraded cable in one rack.

### Link-Level Telemetry {#sec-network-fabrics-link-telemetry}

The physical layer of a high-performance network is surprisingly fragile, making hardware counters the ground truth for fabric reliability. Every RDMA NIC and switch maintains these counters, accessible via the `perfquery` utility. **PortXmitData** and **PortRcvData** provide the raw byte counts used to compute real-time link utilization, while **PortXmitDiscards** tracks packets dropped by the switch—in a properly configured lossless fabric, discards should be exactly zero. Physical signal integrity is tracked via the **SymbolErrorCounter**, which increments on bit-level errors caused by loose cables or dirty optics, and the **LinkDownedCounter**, which logs link flaps often triggered by intermittent hardware faults or overheating transceivers.

These metrics are typically polled by a centralized monitoring system (e.g., Prometheus with Grafana dashboards) at 10–30 second intervals to catch **silent degradation**. A common failure mode involves a QSFP56 cable negotiating at HDR speed (`{python} ib_hdr_gbps` Gbps) instead of NDR (`{python} ib_ndr_gbps` Gbps), or silently dropping from 4 lanes to 2 lanes due to a single bad connector pin. The link remains "up" and functional, but its bandwidth is halved. In a 1,000-GPU cluster with over 3,000 links, even a 0.1% component failure rate guarantees that approximately 3 links are degraded at any given moment. Because distributed training algorithms like Ring AllReduce are synchronous, a single degraded link throttles the entire job to the speed of that straggler, transforming a minor hardware fault into a massive waste of idle compute cycles.

### Bandwidth and Latency Validation {#sec-network-fabrics-bw-testing}

While counters track errors, achievable performance must be empirically validated using the `perftest` suite. A healthy NDR InfiniBand link typically delivers 48 to 49 GB/s of useful payload (out of the raw `{python} ib_ndr_gbs` GB/s rate) after encoding overhead. Operators rely on periodic health checks, often running `ib_write_bw` between all node pairs to generate an **all-pairs bandwidth matrix**. This heatmap immediately visualizes cold spots in the fabric where specific spine switches or cable bundles are underperforming, allowing for targeted maintenance before jobs are scheduled.

For latency-sensitive synchronization, `ib_write_lat` measures round-trip times for small RDMA writes. Baseline NDR latency should remain below 2 $\mu$s for directly connected nodes. Latencies exceeding 5 $\mu$s suggest switch-buffer congestion or routing imbalances, while values spiking above 100 $\mu$s usually indicate the RDMA path has fallen back to TCP/IP emulation—a catastrophic configuration error. Before launching a massive training job, standard procedure involves running application-level validation via `nccl-tests`, which verifies that the fabric can sustain the expected AllReduce bandwidth across the specific collective topology (ring or tree) used by the workload. This ensures that the physical network reality matches the theoretical design before expensive compute resources are allocated.

### Systematic Debugging Workflow {#sec-network-fabrics-debug-workflow}

When a training job reports lower-than-expected throughput, the following diagnostic sequence isolates the cause:

1. **Check GPU utilization**: Rule out compute bottlenecks using `dcgmi` or `nvidia-smi`. If SM utilization is 100%, the network is not the bottleneck.
2. **Inspect NCCL logs**: Set `NCCL_DEBUG=INFO` to reveal which network transport was selected, the detected bandwidth between nodes, and any fallbacks to slower protocols.
3. **Run point-to-point tests**: Use `ib_write_bw` between specific nodes in the job. A single degraded link can bottleneck the entire ring in a Ring AllReduce.
4. **Check PFC/ECN counters**: Inspect switch counters along the path. Sustained PFC activity indicates persistent congestion that should be investigated at the scheduler or routing level.
5. **Validate Physical Layer**: Check symbol errors and CRC counts to identify failing transceivers or cables.

This workflow progresses from application-level symptoms to physical-layer causes, preventing weeks of guesswork when cluster performance drifts.

::: {.callout-checkpoint title="Diagnosing a Training Slowdown"}

**Scenario:** Your 175B model training job has been running for 3 days on 512 GPUs. You notice that the iteration time has gradually increased from 4.2 seconds to 4.8 seconds (a 14% slowdown). The GPU utilization reported by `nvidia-smi` has dropped from 92% to 85%.

1. What is the first diagnostic step you would take, and why? The drop in GPU utilization combined with increased iteration time strongly suggests a communication bottleneck rather than a compute problem. If the GPUs were throttling due to heat or experiencing ECC errors, utilization would likely remain high or the job would crash. What tool would you use to confirm?
2. You run `ib_write_bw` between all node pairs and find that one pair achieves only 24 GB/s instead of the expected 48 GB/s. What physical-layer causes could explain a 50% bandwidth drop? How would you verify using switch counters?
3. If the degraded link is in the Ring AllReduce path, estimate the impact on overall AllReduce time. Would switching to a Tree AllReduce algorithm help, and why or why not?

:::

Monitoring reveals problems, but many of the most costly mistakes stem not from hardware failures but from flawed assumptions about how networks behave at scale. The next section catalogs the most common of these misconceptions.

## Fallacies and Pitfalls {#sec-network-fabrics-fallacies}

Designing and operating high-performance fabrics for ML requires unlearning assumptions from traditional datacenter networking. The following fallacies and pitfalls capture the most common errors that stall training and degrade cluster productivity.

**Fallacy:** *More bandwidth always means faster training.*

Engineers assume upgrading from HDR (`{python} ib_hdr_gbps` Gbps) to NDR (`{python} ib_ndr_gbps` Gbps) will yield proportional gains, but the $\alpha$-$\beta$ model (@eq-alpha-beta) reveals this is only true in the bandwidth-dominated regime. For small models or the small-message phases of pipeline parallelism, the latency term $\alpha$—dominated by switch hops and FEC (~1 $\mu$s)—dictates performance. If a workload is latency-bound, a 2$\times$ bandwidth increase might improve end-to-end throughput by less than 5%, while adding 40% to the power and transceiver cost.

```{python}
#| echo: false
#| label: bandwidth-fallacy
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BANDWIDTH FALLACY
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Fallacy "More bandwidth always means faster training"
# │
# │ Goal: Show diminishing returns of BW in the latency-dominated regime.
# │ Show: That a 2x BW increase yields minimal gain for small messages.
# │ How: Calculate T(n) for HDR vs NDR for 10KB message.
# │
# │ Imports: mlsys.formatting (fmt)
# │ Exports: hdr_bw_gbs, ndr_bw_gbs, t_hdr_us, t_ndr_us, speedup_pct
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.formatting import fmt, check

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class BandwidthFallacy:
    """
    Scenario: Latency comparison for small messages across HDR vs NDR.
    Illustrates why doubling bandwidth doesn't halve latency for small packets.
    """

    # ┌── 1. PARAMETERS (Inputs) ───────────────────────────────────────────────
    alpha_us = 1.5
    hdr_gbps = 200
    ndr_gbps = 400
    msg_size_kb = 10

    # ┌── 2. CALCULATION (The Physics) ─────────────────────────────────────────
    # Link rates in GB/s
    hdr_gbs = hdr_gbps / 8
    ndr_gbs = ndr_gbps / 8

    # T = alpha + m/beta
    t_hdr = alpha_us + (msg_size_kb / (hdr_gbs * 1024 * 1024 / 1000)) # Simplified us
    # Actually m/beta in us: (size_bytes) / (GB/s)
    t_hdr = alpha_us + (msg_size_kb * 1024) / (hdr_gbs * 1e3)
    t_ndr = alpha_us + (msg_size_kb * 1024) / (ndr_gbs * 1e3)

    speedup_pct = (1 - (t_ndr / t_hdr)) * 100

    # ┌── 3. INVARIANTS (Guardrails) ───────────────────────────────────────────
    check(speedup_pct < 25, f"Speedup ({speedup_pct:.1f}%) too high; fallacy argument weakened.")

    # ┌── 4. OUTPUTS (Formatting) ──────────────────────────────────────────────
    hdr_bw_str = f"{hdr_gbps}"
    ndr_bw_str = f"{ndr_gbps}"
    t_hdr_str = f"{t_hdr:.2f}"
    t_ndr_str = f"{t_ndr:.2f}"
    speed_pct_str = f"{speedup_pct:.1f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
hdr_bw_gbs = BandwidthFallacy.hdr_bw_str
ndr_bw_gbs = BandwidthFallacy.ndr_bw_str
t_hdr_us = BandwidthFallacy.t_hdr_str
t_ndr_us = BandwidthFallacy.t_ndr_str
speedup_pct = BandwidthFallacy.speed_pct_str
```

Consider a 10 KB message (typical for control synchronization). On `{python} hdr_bw_gbs` Gbps InfiniBand, it takes `{python} t_hdr_us` $\mu$s. Upgrading to `{python} ndr_bw_gbs` Gbps reduces this to `{python} t_ndr_us` $\mu$s—a measly `{python} speedup_pct`\% improvement despite doubling the link rate.

**Pitfall:** *Assuming lossless Ethernet is as reliable as InfiniBand.*

RoCE over Ethernet can achieve throughput comparable to InfiniBand for large transfers, but the "lossless" property is an approximation maintained by PFC (@sec-network-fabrics-pfc). A misconfigured switch or a firmware bug can trigger a **PFC storm**, where PAUSE frames propagate in a loop, freezing the entire fabric. InfiniBand's credit-based flow control is inherently immune to such cascades because it operates on per-hop buffer availability rather than reactive signals. Teams deploying RoCE must invest significantly more in fabric testing and monitoring to avoid multi-day outages caused by "lossless" deadlocks.

**Fallacy:** *Network oversubscription is acceptable if "most" traffic is local.*

In general-purpose clouds, 4:1 or 8:1 oversubscription at the spine is common because traffic is stochastic. However, ML training is **Bulk Synchronous**. When an AllReduce starts, *every* node attempts to inject full bandwidth simultaneously. As shown in the Bisection Bottleneck analysis (@nb-bisection-bottleneck), even a 4:1 oversubscription slows the entire synchronization by 4$\times$. For a \$300M cluster where sync accounts for 30% of time, this "cost optimization" wastes over **\$`{python} waste_millions` million** in idle GPU cycles.

**Pitfall:** *Testing network performance with `iperf` instead of RDMA tools.*

Standard benchmarks like `iperf` measure kernel-based TCP/IP performance. Because TCP requires CPU-managed buffer copies and context switches, it often caps at 20–40 Gbps regardless of the wire speed. ML training uses **RDMA**, which bypasses the kernel entirely. A link that appears "broken" at 30 Gbps in `iperf` might be perfectly healthy and deliver 390 Gbps in `ib_write_bw`. Validation protocols must use RDMA-specific tools from the `perftest` suite to match the workload's data path.

**Fallacy:** *Adaptive routing eliminates the need for topology-aware placement.*

Adaptive routing distributes traffic across available paths, but it cannot create bandwidth that doesn't exist. If a scheduler places a 1,024-GPU job across two oversubscribed spine groups, adaptive routing will balance the traffic, but it will still be throttled by the group-to-group links. Topology-aware placement (@sec-fleet-orchestration) and adaptive routing are complementary: the former ensures bandwidth exists, while the latter ensures it is utilized efficiently.

**Pitfall:** *Neglecting PFC and ECN counter monitoring in production.*

Network performance issues in ML clusters manifest as subtle training slowdowns rather than errors. A 10% reduction in throughput due to intermittent congestion can waste thousands of GPU-hours before being noticed. Operators must alert on **PortXmitDiscards** and **PFC Pause** frame rates. A gradual increase in these counters is often the "canary in the coal mine" for a failing transceiver or a routing imbalance that will eventually lead to a job failure.

## Summary {#sec-network-fabrics-summary}

We have structured our analysis of high-bandwidth fabrics around a **Five-Level Model**, ascending from the physics of signal transmission to the architecture of warehouse-scale clusters. This framework reveals that network performance is not merely about link speed; it is the product of interactions between physical reach, transport protocols, topology, and congestion control. We began with Level 1 (Wire), establishing that PAM4 encoding and FEC impose irreducible latency floors that constrain cluster diameter. At Level 2 (Transport), we contrasted InfiniBand's native credit-based flow control with RoCE's reliance on PFC, using the $\alpha$-$\beta$ model to quantify the bandwidth-latency trade-offs inherent in distributed collectives.

Level 3 (Topology) demonstrated how non-blocking fat-trees and rail-optimized designs provide the structural bisection bandwidth required by global AllReduce patterns. However, Level 4 (Behavior) showed that structure alone is insufficient: in the synchronous world of BSP training, tail latency is the dominant constraint, necessitating proactive congestion control mechanisms like DCQCN and HPCC to prevent incast-induced stalling. Level 5 (Cluster Design) integrated these layers into production architectures like the NVIDIA SuperPOD and Meta Grand Teton, illustrating how virtualization and multi-tenancy allow these massive instruments to be shared safely.

Looking forward, the physical limits of copper and the operational complexity of Ethernet are driving a new generation of interconnects. Technologies like the Ultra Ethernet Consortium (UEC) standards, Co-Packaged Optics (CPO), and CXL memory pooling promise to flatten topologies and reduce the power tax of moving data. Yet, as our monitoring workflow discussion emphasized, no technology eliminates the need for rigorous observability. Whether debugging a single degraded transceiver or optimizing a multi-tenant scheduler, the ability to correlate physical counters with application-level throughput remains the ultimate safeguard against silent waste in the machine learning fleet.

::: {.callout-takeaways title="The Network Is the Bottleneck"}

* **Network as Computer**: At scale, the interconnect is a primary determinant of system performance. Bandwidth and topology constrain training speed as fundamentally as accelerator FLOPS.
* **$\alpha$-$\beta$ Framework**: Guides topology selection and collective algorithm design by separating latency-dominated and bandwidth-dominated regimes.
* **Lossless is Non-Negotiable**: RDMA requires a lossless fabric. InfiniBand provides this natively; Ethernet must approximate it via PFC/ECN, adding operational complexity and tail-latency risk.
* **Topology Choice is Workload-Dependent**: Fat-trees provide flexibility; rail-optimized networks minimize latency for tensor parallelism; dragonflies reduce cabling costs for massive scale.
* **Monitor or Waste**: Systematic telemetry (PFC counters, bandwidth baselines) is the only way to detect subtle network-induced training slowdowns before they waste significant GPU-hours.

:::

The practical value of this layered understanding is diagnostic precision. When a distributed training job underperforms, the complaint is invariably "the network is slow," but slowness has many causes: a failing transceiver degrading a single link, a PFC storm cascading across a subnet, a topology bottleneck starving one communication pattern while adequately serving another. Engineers who understand the Five-Level Model can isolate the layer at fault, correlate physical counters with transport behavior, and distinguish a topology limitation from a congestion control misconfiguration. This capacity to reason across abstraction layers, from SerDes signal integrity to cluster-wide bisection bandwidth, is what separates routine troubleshooting from genuine systems engineering.

Equally important, the $\alpha$-$\beta$ cost model provides a quantitative vocabulary for making architectural decisions before hardware is purchased and racks are wired. Choosing between InfiniBand and RoCE, between fat-tree and rail-optimized topologies, between 400G and 800G link speeds are all decisions with multi-million-dollar consequences that hinge on the interaction between message size distributions, collective algorithms, and physical link characteristics. The framework developed in this chapter equips practitioners to evaluate these trade-offs with analytical rigor rather than vendor benchmarks alone.

::: {.callout-chapter-connection title="From Wires to Data Pipelines"}

We have now built the network fabric that binds compute nodes into a fleet. Every byte of gradient data and activation tensor flows through this fabric. But the fleet also needs to move massive datasets and enormous checkpoints. @sec-data-storage examines the parallel storage systems and data-loading architectures that keep the fleet supplied with data.

:::
