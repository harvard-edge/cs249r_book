---
engine: jupyter
---

# Network Fabrics {#sec-network-fabrics}

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_network_fabrics.png){fig-alt="Network interconnects and fabric topology linking compute nodes in a datacenter." width=100%}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{100}{40}{15}{10}
\end{marginfigure}

_Why does the network connecting accelerators matter more than the accelerators themselves at scale?_

A single GPU can perform trillions of operations per second, but distributed training requires those operations to coordinate across thousands of devices. Every synchronization point, whether gradient averaging, activation exchange, or parameter update, depends on network bandwidth and latency. When network capacity cannot keep pace with accelerator throughput, GPUs sit idle waiting for data to arrive, and adding more GPUs makes the problem worse rather than better. At sufficient scale, network design dominates system performance: the topology determines which communication patterns are efficient, the bandwidth determines how large models can be partitioned, and the latency determines how tightly coupled training can be. Organizations that treat networking as an afterthought discover that their expensive accelerators deliver a fraction of theoretical performance because the network became the bottleneck nobody planned for.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Model network communication cost using the **$\alpha$-$\beta$ framework** and identify bandwidth-dominated versus latency-dominated regimes
- Compare **RDMA** transport protocols (**InfiniBand** and **RoCE**) in terms of latency, lossless guarantees, and operational complexity
- Analyze network topologies (**fat-tree**, **rail-optimized**, **dragonfly**) by computing **bisection bandwidth** and hop count for ML collective patterns
- Evaluate congestion control mechanisms (**PFC**, **DCQCN**, **HPCC**) and their impact on tail latency during distributed training
- Design network virtualization strategies for multi-tenant GPU clusters using **SR-IOV** and traffic isolation
- Diagnose network performance bottlenecks using RDMA counters, link-level telemetry, and bandwidth testing tools

:::

```{python}
#| label: network-fabrics-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ NETWORK FABRICS: CHAPTER-WIDE CONSTANTS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-network-fabrics-introduction and the bandwidth comparison
# │          table used throughout the chapter.
# │
# │ Goal: Compare IB HDR (INFINIBAND_HDR_BW) vs IB NDR (INFINIBAND_NDR_BW)
# │       vs NVLink H100 (NVLINK_H100_BW) bandwidth on a 256-GPU cluster to
# │       establish why fabric topology matters for communication-bound training;
# │       compute fat-tree host count for k=64 to anchor topology scaling.
# │ Show: "400" Gbps NDR vs "200" Gbps HDR vs "900" GB/s NVLink — inline in
# │       the fabric selection and topology trade-off paragraphs.
# │ How: All bandwidth conversions via .m_as(); fat-tree hosts via integer math.
# │
# │ Imports: mlsys.constants (INFINIBAND_NDR_BW, INFINIBAND_HDR_BW,
# │           NVLINK_H100_BW, NVLINK_A100_BW, PCIE_GEN5_BW, PCIE_GEN4_BW,
# │           H100_FLOPS_FP16_TENSOR, H100_TDP, H100_MEM_CAPACITY, H100_MEM_BW,
# │           A100_FLOPS_FP16_TENSOR, B200_FLOPS_FP16_TENSOR, B200_MEM_BW,
# │           Gbps, GB, TB, second, watt, GB, TFLOPs, flop, byte, NS)
# │ Exports: ib_ndr_gbps, ib_hdr_gbps, ib_ndr_gbs, ib_hdr_gbs, nvlink_h100_gbs,
# │          nvlink_a100_gbs, pcie5_gbs, h100_tflops, h100_tdp, h100_mem,
# │          h100_mem_bw, a100_tflops, b200_tflops, b200_mem_bw,
# │          ib_ndr_latency_us, nvlink_to_ib_ratio, fat_tree_hosts,
# │          alpha_ib_us, beta_ib_gbs
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.constants import (
    INFINIBAND_NDR_BW, INFINIBAND_HDR_BW,
    NVLINK_H100_BW, NVLINK_A100_BW,
    PCIE_GEN5_BW, PCIE_GEN4_BW,
    H100_FLOPS_FP16_TENSOR, H100_TDP, H100_MEM_CAPACITY, H100_MEM_BW,
    A100_FLOPS_FP16_TENSOR,
    B200_FLOPS_FP16_TENSOR, B200_MEM_BW,
    Gbps, GB, TB, second, watt, GB, TFLOPs, flop, byte, NS
)
from mlsys.formatting import fmt, sci, check, md, md_math

# Static FEC latency references
fec_latency_ns_low = 100
fec_latency_ns_high = 200
```

## The Synchronization Backbone {#sec-network-fabrics-introduction}

Consider the running example that threads through this volume: a 175-billion-parameter language model partitioned across 1,000 GPUs. Each training step requires an AllReduce of 350 GB of gradient data, meaning every GPU must send and receive its share before the next step can begin. If even one link in the fabric is slow, all 999 other GPUs wait. The network is not auxiliary infrastructure; it is the synchronization backbone that determines whether this cluster trains efficiently or wastes millions of dollars in idle compute.

In the **Fleet Stack** (@sec-vol2-introduction), network fabrics form the connective tissue binding the Infrastructure Layer into a coherent whole. @sec-compute-infrastructure established the building blocks: accelerators, power delivery, and cooling. Those components define what each node can compute in isolation. This chapter wires those nodes together, because at scale, communication cost dominates computation cost. The **Law of Distributed Efficiency** (@eq-distributed-efficiency) makes this explicit: the $T_{\text{sync}} / T_{\text{compute}}$ ratio in the Scaling Factor is determined almost entirely by the network fabric. The fabric constrains every layer above it in the stack: @sec-distributed-training-systems cannot overlap communication with computation unless the fabric provides sufficient bandwidth, @sec-collective-communication cannot choose optimal algorithms without knowing the topology, and @sec-fault-tolerance-reliability must account for network partitions alongside node failures.

The physical network fabric exists to carry three fundamental collective communication patterns. An **AllReduce**[^fn-allreduce-forward] sums gradients across thousands of GPUs so that every device holds the identical average—the heartbeat of synchronous training. An **AllGather**[^fn-collectives-forward] collects different model portions so that every GPU can reconstruct the full model state. An AllToAll, the most demanding pattern, requires every GPU to send unique data to every other GPU, a requirement critical to **expert parallelism**[^fn-moe-forward]. While @sec-collective-communication covers the *algorithms* that orchestrate these patterns, this chapter covers the *physics* of the wires and switches that carry them. Understanding the distinction matters because the fabric's physical properties—bandwidth, latency, topology—determine which patterns are efficient and which become bottlenecks.

::: {.callout-perspective title="The Network as a Gradient Bus"}

In a single machine, the memory bus moves data between the processor and memory. In a distributed training cluster, the network fabric serves the analogous role: it is the **Gradient Bus** that moves parameter updates between workers. Just as memory bus bandwidth determines single-device throughput (the Memory Wall from @sec-compute-infrastructure), network fabric bandwidth determines multi-device throughput (the Communication Wall). Every concept in this chapter, from protocols to topologies to congestion control, exists to make this Gradient Bus as fast and reliable as possible.

:::

```{python}
#| label: network-fabrics-context-scenario
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ NETWORK FABRICS CONTEXT SPECS (LEGO)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-network-fabrics-physical-layer
# │
# │ Goal: Provide initial networking and hardware specs for fabric discussion.
# │ Show: ~400 Gbps NDR; ~900 GB/s NVLink; ~1,979 TFLOPS FP16.
# │ How: pulling constants from mlsys.constants.
# │
# │ Imports: mlsys.constants (*), mlsys.formatting (fmt, check)
# │ Exports: ib_ndr_gbps, ib_hdr_gbps, ib_ndr_gbs, ib_hdr_gbs,
# │          nvlink_h100_gbs, nvlink_a100_gbs, pcie5_gbs, h100_tflops,
# │          h100_tdp, h100_mem, h100_mem_bw, a100_tflops, b200_tflops,
# │          b200_mem_bw, ib_ndr_latency_us, nvlink_to_ib_ratio,
# │          alpha_ib_us, beta_ib_gbs
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.constants import *
from mlsys.formatting import fmt, check

class NetworkFabricsContextScenario:
    """Namespace for Network Fabrics reference parameters."""

    # ┌── 1. LOAD (Constants) ───────────────────────────────────────────────
    ib_ndr_raw = INFINIBAND_NDR_BW
    ib_hdr_raw = INFINIBAND_HDR_BW
    nvlink_h100_raw = NVLINK_H100_BW
    nvlink_a100_raw = NVLINK_A100_BW
    pcie5_raw = PCIE_GEN5_BW
    h100_flops_raw = H100_FLOPS_FP16_TENSOR
    h100_mem_raw = H100_MEM_CAPACITY
    h100_mem_bw_raw = H100_MEM_BW
    h100_tdp_raw = H100_TDP
    a100_flops_raw = A100_FLOPS_FP16_TENSOR
    b200_flops_raw = B200_FLOPS_FP16_TENSOR
    b200_mem_bw_raw = B200_MEM_BW
    fat_tree_k = 64
    ib_ndr_hop_ns = 500
    alpha_ib_us_val = 1.5

    # ┌── 2. EXECUTE (The Compute) ─────────────────────────────────────────
    ib_ndr_gbps_val = ib_ndr_raw.m_as(Gbps)
    ib_ndr_gbs_val = ib_ndr_raw.m_as(GB/second)
    nvlink_to_ib_ratio_val = nvlink_h100_raw.m_as(GB/second) / ib_ndr_gbs_val

    # Fat-tree hosts for radix k: (k^3)/4
    fat_tree_hosts_val = (fat_tree_k ** 3) // 4

    # ┌── 4. OUTPUT (Formatting) ──────────────────────────────────────────────
    ib_ndr_gbps = f"{ib_ndr_gbps_val:.0f}"
    ib_hdr_gbps = f"{ib_hdr_raw.m_as(Gbps):.0f}"
    ib_ndr_gbs = f"{ib_ndr_gbs_val:.0f}"
    ib_hdr_gbs = f"{ib_hdr_raw.m_as(GB/second):.0f}"
    nvlink_h100_gbs = f"{nvlink_h100_raw.m_as(GB/second):.0f}"
    nvlink_a100_gbs = f"{nvlink_a100_raw.m_as(GB/second):.0f}"
    pcie5_gbs = f"{pcie5_raw.m_as(GB/second):.0f}"
    h100_tflops = f"{h100_flops_raw.m_as(TFLOPs/second):.0f}"
    h100_tdp = f"{h100_tdp_raw.m_as(watt):.0f}"
    h100_mem = f"{h100_mem_raw.m_as(GB):.0f}"
    h100_mem_bw = f"{h100_mem_bw_raw.m_as(TB/second):.2f}"
    a100_tflops = f"{a100_flops_raw.m_as(TFLOPs/second):.0f}"
    b200_tflops = f"{b200_flops_raw.m_as(TFLOPs/second):,.0f}"
    b200_mem_bw = f"{b200_mem_bw_raw.m_as(TB/second):.0f}"
    ib_ndr_latency_us = f"{ib_ndr_hop_ns / 1000:.1f}"
    nvlink_to_ib_ratio = f"{nvlink_to_ib_ratio_val:.0f}"
    fat_tree_hosts = fmt(fat_tree_hosts_val, precision=0)
    alpha_ib_us = f"{alpha_ib_us_val:.1f}"
    beta_ib_gbs = f"{ib_ndr_gbs_val:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
ib_ndr_gbps = NetworkFabricsContextScenario.ib_ndr_gbps
ib_hdr_gbps = NetworkFabricsContextScenario.ib_hdr_gbps
ib_ndr_gbs = NetworkFabricsContextScenario.ib_ndr_gbs
ib_hdr_gbs = NetworkFabricsContextScenario.ib_hdr_gbs
nvlink_h100_gbs = NetworkFabricsContextScenario.nvlink_h100_gbs
nvlink_a100_gbs = NetworkFabricsContextScenario.nvlink_a100_gbs
pcie5_gbs = NetworkFabricsContextScenario.pcie5_gbs
h100_tflops = NetworkFabricsContextScenario.h100_tflops
h100_tdp = NetworkFabricsContextScenario.h100_tdp
h100_mem = NetworkFabricsContextScenario.h100_mem
h100_mem_bw = NetworkFabricsContextScenario.h100_mem_bw
a100_tflops = NetworkFabricsContextScenario.a100_tflops
b200_tflops = NetworkFabricsContextScenario.b200_tflops
b200_mem_bw = NetworkFabricsContextScenario.b200_mem_bw
ib_ndr_latency_us = NetworkFabricsContextScenario.ib_ndr_latency_us
nvlink_to_ib_ratio = NetworkFabricsContextScenario.nvlink_to_ib_ratio
fat_tree_hosts = NetworkFabricsContextScenario.fat_tree_hosts
alpha_ib_us = NetworkFabricsContextScenario.alpha_ib_us
beta_ib_gbs = NetworkFabricsContextScenario.beta_ib_gbs
```

The concrete bandwidth cliff that separates intra-node from inter-node communication makes this Gradient Bus analogy precise.
 @sec-compute-infrastructure established that a single H100 delivers `{python} h100_tflops` TFLOPS of FP16 throughput with `{python} h100_mem_bw` TB/s of memory bandwidth. Within a node, eight such accelerators communicate through NVLink at `{python} nvlink_h100_gbs` GB/s. The moment computation crosses a node boundary, however, the available bandwidth drops by a factor of `{python} nvlink_to_ib_ratio`$\times$, from `{python} nvlink_h100_gbs` GB/s (NVLink) to `{python} ib_ndr_gbs` GB/s (NDR InfiniBand per port). This cliff, the transition from intra-node to inter-node communication, is the central challenge of network fabric design. For our 175B model, moving 350 GB of gradients through `{python} ib_ndr_gbs` GB/s links means that the AllReduce alone can take seconds, during which every GPU in the cluster sits idle unless the fabric and collective algorithms can overlap that transfer with computation.

@fig-bandwidth-hierarchy makes this hierarchy concrete by plotting the bandwidth at each level across four GPU generations.

::: {#fig-bandwidth-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**The Bandwidth Hierarchy**. Bandwidth at four levels of the communication hierarchy across four GPU generations, on a logarithmic scale. While HBM and NVLink bandwidth have grown roughly 9$\\times$ and 6$\\times$ respectively from V100 to B200, InfiniBand has grown only 4$\\times$. The annotations show the NVLink-to-InfiniBand ratio for each generation, quantifying the cliff that distributed training must cross at every synchronization point." fig-alt="Grouped bar chart with log-scale y-axis showing bandwidth in GB per second at four hierarchy levels across four GPU generations from V100 to B200."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BANDWIDTH HIERARCHY (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-bandwidth-hierarchy — bandwidth cliff in "The Bandwidth Cliff"
# │
# │ Goal: Plot HBM, NVLink, InfiniBand, PCIe bandwidth across V100–B200 to
# │       quantify the NVLink-to-IB ratio that distributed training must cross.
# │ Show: Grouped bar chart (log scale), NVLink/IB ratio annotations per gen.
# │ How: Hardcoded verified data; matplotlib grouped bars; viz.setup_plot().
# │
# │ Imports: numpy (np), mlsys.viz (viz)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
from mlsys import viz

fig, ax, COLORS, plt = viz.setup_plot(figsize=(8, 5))

# --- Verified data ---
generations = ["V100\n(2017)", "A100\n(2020)", "H100\n(2022)", "B200\n(2024)"]
hbm         = [900,  2039,  3350,  8000]
nvlink      = [300,   600,   900,  1800]
ib          = [ 24,    24,    48,   100]
pcie        = [ 15.75, 32,    64,    64]

n_gen = len(generations)
n_tiers = 4
x = np.arange(n_gen)
width = 0.18

tier_data   = [hbm, nvlink, ib, pcie]
tier_labels = ["HBM", "NVLink", "InfiniBand", "PCIe"]
tier_colors = [COLORS["BlueLine"], COLORS["GreenLine"], COLORS["OrangeLine"], COLORS["VioletLine"]]

for j, (data, label, color) in enumerate(zip(tier_data, tier_labels, tier_colors)):
    offset = (j - 1.5) * width
    bars = ax.bar(x + offset, data, width, label=label, color=color, alpha=0.85, zorder=3)

# --- Annotate NVLink / IB ratio ---
ratios = [n / i for n, i in zip(nvlink, ib)]
for i_gen in range(n_gen):
    # Position annotation above the NVLink bar
    nvlink_x = x[i_gen] + (1 - 1.5) * width
    ax.annotate(f"{ratios[i_gen]:.0f}$\\times$",
                xy=(nvlink_x + width * 0.5, nvlink[i_gen]),
                xytext=(0, 8), textcoords="offset points",
                fontsize=7.5, ha="center", color=COLORS["RedLine"],
                fontweight="bold",
                bbox=dict(boxstyle="round,pad=0.15", fc="white", ec=COLORS["RedLine"], alpha=0.8))

ax.set_yscale("log")
ax.set_ylim(5, 20000)
ax.set_xticks(x)
ax.set_xticklabels(generations)
ax.set_ylabel("Bandwidth (GB/s, log scale)")
ax.set_xlabel("GPU Generation")
ax.legend(loc="upper left", frameon=True, fancybox=True, framealpha=0.9, ncol=2)
ax.set_title("")
plt.show()
```
:::

@fig-bandwidth-hierarchy reveals that the NVLink-to-InfiniBand ratio has remained between 12$\times$ and 25$\times$ across four GPU generations, despite absolute bandwidth improvements at every tier. This persistent cliff is not a temporary engineering shortcoming; it reflects the fundamental physics that on-package interconnects (NVLink) operate over millimeters of copper, while inter-node links (InfiniBand) span meters of cable and must traverse switches. The ratio determines which parallelism strategies are efficient: tensor parallelism, which requires continuous high-bandwidth exchange of activations, is viable within a node but impractical across nodes. Pipeline and data parallelism, which tolerate the lower inter-node bandwidth, must carry the burden of cross-node communication. Every topology and protocol decision in the remainder of this chapter is an attempt to minimize the impact of this hierarchy on collective communication performance.

To see why this ratio matters so viscerally, consider what happens during a single training step. @fig-bandwidth-cliff-gantt contrasts the timeline for an 8-GPU job communicating within a node versus a 64-GPU job communicating across nodes.

::: {#fig-bandwidth-cliff-gantt fig-env="figure" fig-pos="htb" fig-cap="**The Bandwidth Cliff in a Training Step**. Two timelines for the same model: an 8-GPU intra-node job (top) using NVLink at 900 GB/s, and a 64-GPU inter-node job (bottom) using InfiniBand at 50 GB/s. The AllReduce phase (red) grows from a thin sliver to a dominant fraction of the step. Without communication--computation overlap, GPUs sit idle during the entire AllReduce, and the utilization gap between the two scenarios exceeds 10 percentage points." fig-alt="Two horizontal Gantt bars. Top bar shows forward pass, backward pass, and thin AllReduce sliver at 99 percent utilization. Bottom bar shows same compute phases but a wide AllReduce block dropping utilization to 87 percent."}
```{.tikz}
\usetikzlibrary{positioning, decorations.pathreplacing}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{BlueL}{HTML}{D1E6F3}
  \definecolor{GreenLine}{HTML}{008F45}
  \definecolor{GreenL}{HTML}{D4EFDF}
  \definecolor{RedLine}{HTML}{CB202D}
  \definecolor{RedL}{HTML}{F5D2D5}
  \definecolor{OrangeLine}{HTML}{E67817}
  \definecolor{BrownLine}{HTML}{78492A}

  \tikzset{
    ganttbar/.style={draw=none, text=white, font=\scriptsize, minimum height=0.8cm, inner sep=0pt, anchor=west},
    fwd/.style={ganttbar, fill=GreenLine!80},
    bwd/.style={ganttbar, fill=BlueLine!80},
    ar/.style={ganttbar, fill=RedLine},
    label/.style={font=\small\bfseries, anchor=east},
    sublabel/.style={font=\scriptsize, anchor=east, text=black!60},
    util/.style={font=\scriptsize\bfseries, anchor=west}
  }

  % --- Scenario A: Intra-node (NVLink) ---
  \begin{scope}[local bounding box=intra]
    \node[fwd, minimum width=4cm] (fwd1) at (0, 1.6) {Forward (100 ms)};
    \node[bwd, minimum width=4cm, right=0pt of fwd1] (bwd1) {Backward (100 ms)};
    \node[ar, minimum width=0.2cm, right=0pt of bwd1] (ar1) {};
    \node[font=\tiny, anchor=west, text=RedLine, right=2pt of ar1] (ar1_label) {AR (1 ms)};
    \node[util, text=GreenLine, right=10pt of ar1_label] {99.5\% util.};

    \node[label, left=0.3cm of fwd1.west, yshift=0.2cm] {Intra-Node};
    \node[sublabel, left=0.3cm of fwd1.west, yshift=-0.2cm] {8 GPUs, NVLink};
  \end{scope}

  % --- Scenario B: Inter-node (InfiniBand) ---
  \begin{scope}[local bounding box=inter, yshift=-1.6cm]
    \node[fwd, minimum width=4cm] (fwd2) at (0, 1.6) {Forward (100 ms)};
    \node[bwd, minimum width=4cm, right=0pt of fwd2] (bwd2) {Backward (100 ms)};
    \node[ar, minimum width=2.4cm, right=0pt of bwd2] (ar2) {AllReduce (30 ms)};
    \node[util, text=OrangeLine, right=0.2cm of ar2] {87\% util.};

    \node[label, left=0.3cm of fwd2.west, yshift=0.2cm] {Inter-Node};
    \node[sublabel, left=0.3cm of fwd2.west, yshift=-0.2cm] {64 GPUs, IB};
  \end{scope}

  % Time axis
  \begin{scope}[yshift=-1.0cm]
    \draw[->, line width=1.0pt, black!60] (0, 0) -- (11, 0) node[right, font=\scriptsize] {Time (ms)};
    \foreach \x/\t in {0/0, 4/100, 8/200, 10.4/230} {
      \draw[black!60, line width=1.0pt] (\x, 0.1) -- (\x, -0.1);
      \node[font=\tiny, below] at (\x, -0.1) {\t};
    }
  \end{scope}

  % Idle cost annotation
  \draw[RedLine, line width=1.2pt, decorate, decoration={brace, amplitude=4pt}]
    (10.4, -0.5) -- (8.2, -0.5)
    node[midway, below=5pt, font=\tiny\bfseries, text=RedLine] {30 ms idle per step};

\end{tikzpicture}
```
:::

The 12 percentage-point utilization gap in @fig-bandwidth-cliff-gantt represents millions of dollars in wasted compute over a months-long training run. This is why network fabric design is not an afterthought but the central engineering challenge of distributed training.

## How ML Networking Inverts Datacenter Assumptions {#sec-network-workload-inversion}

A network architect from the world of large-scale web services would find the traffic patterns of a distributed training cluster counter-intuitive. Traditional datacenter traffic is characterized by a vast number of small, independent, and asynchronous flows. Millions of users accessing a web service generate a stochastic traffic pattern that is well-served by standard TCP/IP and statistically multiplexed, oversubscribed networks.

ML training workloads are the complete opposite. They are synchronous, periodic, and dominated by a small number of massive, collective communication operations. This fundamental difference in workload characteristics inverts the core assumptions of traditional network design.

| **Workload Pattern**   | **Traditional Datacenter Assumption**  | **ML Reality**                             |
|:-----------------------|:---------------------------------------|:-------------------------------------------|
| **Traffic Pattern**    | Asynchronous, stochastic, many-to-many | Synchronous, periodic, all-to-all          |
| **Flow Type**          | Millions of small, short-lived flows   | A few massive, long-lived "elephant" flows |
| **Performance Metric** | Average throughput, per-flow fairness  | Tail latency, global synchronization time  |
| **Loss Tolerance**     | Tolerant (TCP retransmits)             | Intolerant (one dropped packet stalls all) |
| **Congestion**         | Localized, independent events          | Global, correlated (incast)                |

: **ML Networking Inverts Traditional Datacenter Assumptions**: Where web services require fairness for millions of independent flows, ML training requires a globally synchronized, lossless fabric optimized for a handful of massive collective operations. {#tbl-network-assumptions}

The synchronicity inversion is the most fundamental. Web traffic is asynchronous; one user's slow connection does not affect another's. The Bulk Synchronous Parallel (BSP)[^fn-bsp-valiant] model governs distributed training: all 1,024 GPUs in a training job must complete their gradient exchange before *any* of them can proceed to the next step. The slowest link therefore dictates the performance of the entire cluster. A single congested switch port that delays one GPU's packets by 100 ms effectively wastes 100 ms of compute time for all 1,024 GPUs. Tail latency is not an outlier; it is the bottleneck.

[^fn-bsp-valiant]: **BSP (Bulk Synchronous Parallel)**: Proposed by Leslie Valiant in 1990 as a "bridging model" between parallel hardware and software, analogous to von Neumann's model for sequential machines. ML training adopted BSP because it guarantees mathematical equivalence to single-device training at the cost of a global barrier per step, making the network fabric's tail latency the binding constraint on cluster throughput. \index{BSP!Valiant}

The flow inversion compounds this problem. Traditional networks are designed for fairness among millions of "mice flows"[^fn-elephant-mice-flows], yet ML training is dominated by a few "elephant flows" corresponding to the gradient AllReduce. A single AllReduce on a 175B parameter model can involve exchanging 350 GB of data. Standard flow control and routing mechanisms like ECMP, which use static hashing to balance flows, are poorly suited to this traffic pattern, as they can inadvertently map multiple elephant flows to the same link, creating massive congestion while other links sit idle.

[^fn-elephant-mice-flows]: **Elephant and Mice Flows**: From network measurement literature, where "mice" denote the millions of short-lived flows (web requests, DNS lookups) that traditional switches are optimized to handle fairly, and "elephants" denote the rare massive flows that dominate aggregate bandwidth. In ML clusters, a single AllReduce elephant flow can carry 350 GB---more data than millions of mice combined---and ECMP's static hash cannot subdivide it, turning one unlucky hash collision into a cluster-wide straggler. \index{Elephant Flows!network traffic}

The loss inversion completes the picture. TCP/IP was designed for unreliable networks and handles packet loss gracefully through retransmission. RDMA-based protocols used in ML clusters (InfiniBand, RoCE) assume a lossless fabric. A single dropped packet forces a Go-Back-N retransmission that can stall the sender for milliseconds, creating a catastrophic straggler that delays the entire synchronous training step. ML fabrics must therefore be engineered for zero packet loss, using credit-based flow control (InfiniBand) or carefully tuned Priority Flow Control (PFC) with large buffers (Ethernet).

These inversions explain why running large-scale distributed training over a standard enterprise Ethernet network is inefficient or impossible. The network fabric for ML is not just a set of pipes; it is a distributed, high-performance "bus" for collective communication, and it must be designed from the ground up for the unique physics of synchronous, large-scale parallelism.

### The Five-Level Model {#sec-network-fabrics-five-level-model}

Network engineering is often taught as a layer cake of protocols. To make the physics of ML cluster fabrics concrete, we adapt this approach into a **Five-Level Model** specific to high-performance interconnects:

- **Level 1: Wire and Link** (@sec-network-fabrics-wire-link). The physics of signal transmission. We examine why signal integrity (PAM4, SerDes) and the speed of light in fiber impose hard constraints on latency and cluster geometry.
- **Level 2: Transport** (@sec-network-fabrics-transport). The protocols that move data reliably. We contrast InfiniBand and RoCE, and use the $\alpha$-$\beta$ model to quantify the latency-versus-bandwidth trade-off for different message sizes.
- **Level 3: Switch and Topology** (@sec-network-fabrics-topology). The shape of the network. We compare how fat-trees, rail-optimized designs, and dragonflies achieve the bisection bandwidth needed for global collectives.
- **Level 4: Fabric Behavior** (@sec-network-fabrics-behavior). The dynamics of traffic under load. We analyze congestion control (PFC, DCQCN), adaptive routing, and the incast phenomena that cause tail latency.
- **Level 5: Cluster Design** (@sec-network-fabrics-cluster). The end-to-end system. We show how these layers integrate into production supercomputers like the NVIDIA SuperPOD and Meta Grand Teton to create a unified Gradient Bus.

The remainder of this chapter ascends this stack, starting from the copper and glass at the bottom.

## Level 1: Wire and Link {#sec-network-fabrics-wire-link}

Before analyzing protocols and topologies, we must understand the physical medium. Every network design decision is ultimately constrained by *what* the wire can carry. At `{python} ib_ndr_gbps` Gbps and beyond, the physics of signal transmission imposes hard limits on cable length, power consumption, and error rates. These are not engineering inconveniences; they are fundamental constraints that shape cluster geometry.

### Signal Integrity and PAM4 {#sec-network-fabrics-pam4}

\index{PAM4}

::: {.callout-definition title="PAM4 Signaling"}

***PAM4 Signaling***\index{PAM4 Signaling!definition} is a modulation scheme that uses four voltage levels to encode two bits per symbol, doubling the data rate for a given symbol rate.

1.  **Significance (Quantitative):** It enables the high **Network Bandwidth ($\text{BW}$)** required for exa-scale fleets (e.g., 400G/800G links). However, the reduced signal-to-noise ratio necessitates **Forward Error Correction (FEC)**, which adds irreducible **Fixed Latency ($L_{\text{lat}}$)** to every packet hop.
2.  **Distinction (Durable):** Unlike **NRZ Signaling** (binary on/off), PAM4 uses multi-level amplitude to overcome the physical symbol-rate limits of copper and fiber.
3.  **Common Pitfall:** A frequent misconception is that doubling the bits per symbol is "free." In reality, it is a **Reliability-Latency Trade-off**: the processing time for error correction makes PAM4 links fundamentally higher-latency than their NRZ predecessors.

:::

To achieve `{python} ib_ndr_gbps` Gbps, we cannot simply toggle a voltage on and off faster. Signal attenuation in copper and chromatic dispersion in fiber impose a hard ceiling on the **symbol rate** (the number of signal transitions per second). Modern links overcome this by using PAM4 to pack more information into each transition.

However, the gap between adjacent voltage levels in PAM4 shrinks by a factor of three compared to NRZ, making the link highly susceptible to noise. Consequently, modern high-speed links operate near the physical limits of reliable detection and require **Forward Error Correction (FEC)** at the physical layer, typically Reed-Solomon RS(544,514) codes. This FEC processing adds significant latency—typically `{python} fec_latency_ns_low` to `{python} fec_latency_ns_high` ns per hop—to every packet. For our 175B model training, which generates 350 GB of gradient traffic per step across 1,000 GPUs, this latency floor is non-negotiable. A packet crossing three switch hops in a fat-tree incurs nearly 1 $\mu$s of irreducible latency just from FEC decoding and encoding at each port. This "physics tax" sets a hard floor on the $\alpha$ term of our performance models, limiting the speed of latency-sensitive collectives like AllReduce regardless of software optimizations.

### Reach and Medium: Copper versus Optics {#sec-network-fabrics-copper-optics}

\index{DAC}\index{AOC}\index{optical interconnect}

The choice of physical medium determines the reach and economics of each link. Three categories dominate modern ML cluster design:

- **DAC (Direct Attach Copper)**: Passive copper cables offering the lowest latency and cost (~\$50), but limited to ~3 meters. Used exclusively within a rack.
- **AOC (Active Optical Cable)**: Uses light to carry signals over 10--100 meters. The conversion between electrical and optical signals, combined with **Forward Error Correction (FEC)**[^fn-fec-latency], adds hundreds of nanoseconds to every link.

[^fn-fec-latency]: **FEC (Forward Error Correction)**: At 400G and 800G speeds, signal integrity is so fragile that the hardware must use FEC to reconstruct corrupted bits. This adds 100--200 ns per hop---a "latency tax" that is physically unavoidable. In a three-tier fat-tree, FEC alone can contribute over 1 $\mu$s to the round-trip time, making it a significant component of the $\alpha$ latency term. \index{FEC!latency tax}
- **AOC (Active Optical Cable)**: Fiber with permanently attached transceivers, suitable for 3 to 30 meter runs (~\$500).
- **Pluggable Optics**: Separate transceivers and fiber cords, used for runs exceeding 30 meters to the network core.

::: {.callout-perspective title="The Cost of Distance"}
In an ML fleet, distance is money. A 10,000-GPU cluster requires ~20,000 optical links at the spine layer alone. At \$500 each with 10 W per link, that represents \$10 million in cabling and 200 kW of continuous power just for transceivers. This physics dictates **cluster geometry**: we pack accelerators as densely as possible (70–100 kW per rack) to maximize cheap copper and minimize expensive optics.
:::

### SerDes, Link Budget, and Power {#sec-network-fabrics-serdes}

\index{SerDes}

Every port depends on a **Serializer/Deserializer (SerDes)** circuit that converts parallel data from the switch ASIC into a serial stream. As bandwidth scales, these circuits have become the dominant power consumer in the network fabric. Consider the energy implications for our 1,000-GPU cluster: maintaining full bisection bandwidth for the 350 GB of gradient traffic requires roughly 3,000 high-speed links. If each `{python} ib_ndr_gbps` Gbps port consumes 25 W (combined SerDes and optical transceiver power), the interconnect alone draws 75 kW continuously—over 10% of the cluster's power budget dedicated merely to moving bits, not computing on them.

The **link budget**, a strict decibel (dB) allowance for signal attenuation, governs the reach of these links. As signals traverse copper, they lose energy to skin effect and dielectric absorption. The link budget is the difference between the transmitter's output power and the receiver's sensitivity limit. For a standard 50G PAM4 lane, the budget might be 30 dB. If a 3-meter DAC cable introduces 18 dB of loss and the connectors add another 2 dB, 10 dB of margin remains. However, as frequency doubles to support 100G lanes (for 800 Gbps ports), the loss per meter increases sharply. This physics forces a brutal trade-off: to maintain signal integrity without exceeding the power budget, cable reach must decrease. 800 Gbps copper links are limited to roughly 1–2 meters, physically constraining the diameter of a compute rack and forcing the use of expensive, power-hungry optics for any connection leaving the cabinet. As link speeds approach 1.6 Tbps, SerDes and optical power will account for over 15% of total cluster energy, making power-per-bit a primary scaling constraint.

The wire sets hard limits on reach and speed; the transport layer must build a reliable communication primitive on top of these physical links.

## Level 2: Transport and the Performance Model {#sec-network-fabrics-transport}

Large-scale training requires sustained, synchronized bulk transfers. A single AllReduce operation across 1,024 accelerators may move terabytes of gradient data. This pattern demands networks optimized for **Remote Direct Memory Access (RDMA)** to eliminate CPU overhead and **lossless delivery** to ensure predictable performance.

### RDMA and GPUDirect {#sec-network-fabrics-rdma}

\index{RDMA}\index{GPUDirect RDMA}

::: {.callout-definition title="Remote Direct Memory Access (RDMA)"}

***Remote Direct Memory Access (RDMA)***\index{Remote Direct Memory Access!definition} is a networking technology that allows one computer to access the memory of another directly without involving the operating system or CPU of either machine.

1.  **Significance (Quantitative):** it provides **Kernel Bypass** and **Zero-Copy** capabilities, reducing the **Communication Overhead ($L_{\text{lat}}$)** by eliminating context switches and intermediate memory copies. It is essential for saturating high-bandwidth links ($BW$) during gradient synchronization.
2.  **Distinction (Durable):** Unlike **Traditional TCP/IP Networking**, where the CPU must process every packet through a complex software stack, RDMA offloads the entire transport protocol to the network hardware (HCA).
3.  **Common Pitfall:** A frequent misconception is that RDMA is just "fast Ethernet." In reality, it requires specialized **Lossless Fabric** (like InfiniBand or RoCE with PFC) because it lacks the robust congestion control mechanisms of TCP; a single lost packet can cause massive performance collapses.

:::

Standard TCP/IP is architecturally unfit for the `{python} ib_ndr_gbps` Gbps era. The protocol stack was designed when network speeds were orders of magnitude slower than CPU memory bandwidth; today, that relationship has inverted. Processing a `{python} ib_ndr_gbps` Gbps stream through the Linux kernel imposes a crushing interrupt tax: simply copying payload data between user space and kernel buffers can consume the entire memory bandwidth of a dual-socket server, requiring tens of CPU cores merely to keep the pipe full. This creates a CPU wall where the host processor becomes the bottleneck for network traffic, starving the application logic it is meant to serve.

RDMA bypasses this entire layer. By offloading the transport logic to the NIC hardware, it allows applications to read and write directly to remote memory. For ML, **GPUDirect RDMA**[^fn-gpudirect-rdma] extends this zero-copy principle to the accelerators themselves. Without it, a gradient update follows a tortuous path: GPU memory $\rightarrow$ CPU system RAM $\rightarrow$ kernel buffer $\rightarrow$ NIC. GPUDirect short-circuits this to a single PCIe transaction: GPU $\rightarrow$ NIC. For our 175B model's 350 GB gradient exchange, this optimization eliminates 700 GB of redundant memory copies across the cluster per step. This not only reduces latency but crucially frees the CPU to orchestrate complex pipelining logic rather than acting as a data mover.

[^fn-gpudirect-rdma]: **GPUDirect RDMA**: Introduced by NVIDIA in 2013 with the Kepler architecture, GPUDirect RDMA enables the NIC to read and write GPU memory directly over PCIe without staging through host RAM. Before GPUDirect, every gradient transfer incurred two extra memory copies (GPU-to-host, host-to-NIC), adding 10--20 $\mu$s per message and consuming CPU memory bandwidth that competes with data loading. Eliminating this bounce path is what makes overlapping communication with backward-pass computation feasible at scale. \index{GPUDirect RDMA!zero-copy}

### InfiniBand and RoCE {#sec-network-fabrics-ib-roce}

\index{InfiniBand}\index{RoCE}

Two transport protocols dominate modern GPU clusters:

- **InfiniBand (IB)**[^fn-infiniband-origin]: A purpose-built HPC architecture designed for point-to-point, switched fabrics. It uses hardware-level **credit-based flow control**—a sender must hold a credit (representing available buffer space at the receiver) before transmitting. This makes IB inherently **lossless** at the link layer. It includes a **Subnet Manager (SM)** for global routing and **Virtual Lanes (VLs)** for traffic isolation.
- **RoCE (RDMA over Converged Ethernet)**: Implements RDMA semantics over standard Ethernet. RoCEv2 encapsulates IB transport headers in UDP/IP packets. While it allows using multi-vendor Ethernet switches, it requires sophisticated congestion control (PFC and ECN) to approximate the losslessness that IB provides natively.

[^fn-infiniband-origin]: **InfiniBand**: Formed in 1999 from the merger of two competing server I/O standards (Future I/O and NGIO), InfiniBand was originally designed to replace PCI as a general-purpose system interconnect. Its pivot to HPC networking preserved the credit-based, hardware-managed flow control that server I/O demanded---and this heritage is precisely why InfiniBand provides native losslessness without the PFC fragility that plagues Ethernet-based RDMA fabrics. \index{InfiniBand!origin}

@fig-ib-roce-stack compares the two stacks, showing how both expose the same **Verbs API** to applications despite their different transport foundations.

::: {#fig-ib-roce-stack fig-env="figure" fig-pos="htb" fig-cap="**High-Performance Networking Stacks**. Comparison of InfiniBand and RoCE protocol stacks. Both expose the same Verbs API, but RoCE relies on Priority Flow Control (PFC) in the Ethernet layer to approximate InfiniBand's native lossless guarantees." fig-alt="Side-by-side diagram comparing InfiniBand and RoCE protocol stacks from application layer down to physical layer, showing Verbs API at top and transport differences."}
```{.tikz}
\usetikzlibrary{positioning}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{OrchidL}{HTML}{E6D4E5}
  \definecolor{SlateL}{HTML}{EBEBEB}
  \definecolor{OrangeLine}{HTML}{E67817}
  \definecolor{RedLine}{HTML}{CB202D}

  \tikzset{
    layer/.style={draw=black!70, line width=0.75pt, fill=white, minimum width=3.2cm, minimum height=0.7cm, font=\sffamily\footnotesize, anchor=north},
    header/.style={font=\bfseries\sffamily, align=center, text=RedLine, anchor=south}
  }

  % InfiniBand Stack
  \begin{scope}[local bounding box=IB]
    \node[header] (ib_head) at (0, 5) {InfiniBand};
    \node[layer, fill=OrchidL, below=0.2cm of ib_head] (ib_verbs) {Verbs API};
    \node[layer, fill=OrchidL!50, below=0pt of ib_verbs] (ib_trans) {IB Transport};
    \node[layer, fill=OrchidL!50, below=0pt of ib_trans] (ib_net) {IB Network};
    \node[layer, fill=OrchidL!50, below=0pt of ib_net] (ib_link) {IB Link};
    \node[layer, fill=OrchidL!50, below=0pt of ib_link] (ib_phy) {IB Physical};

    % RDMA bypass arrow
    \draw[->, line width=1.2pt, OrangeLine] ([xshift=-0.3cm]ib_verbs.west) -- ([xshift=-0.3cm]ib_phy.west) node[midway, left, align=center, font=\scriptsize\bfseries] {Kernel\\Bypass};
  \end{scope}

  % RoCE Stack
  \begin{scope}[shift={(5.5,0)}, local bounding box=RoCE]
    \node[header] (roce_head) at (0, 5) {RoCEv2};
    \node[layer, fill=OrchidL, below=0.2cm of roce_head] (roce_verbs) {Verbs API};
    \node[layer, fill=OrchidL!50, below=0pt of roce_verbs] (roce_trans) {IB Transport};
    \node[layer, fill=SlateL, below=0pt of roce_trans] (roce_udp) {UDP / IP};
    \node[layer, fill=SlateL, below=0pt of roce_udp] (roce_eth) {Ethernet Link (PFC)};
    \node[layer, fill=SlateL, below=0pt of roce_eth] (roce_phy) {Ethernet Physical};

    % RDMA bypass arrow
    \draw[->, line width=1.2pt, OrangeLine] ([xshift=0.3cm]roce_verbs.east) -- ([xshift=0.3cm]roce_phy.east) node[midway, right, align=center, font=\scriptsize\bfseries] {Kernel\\Bypass};
  \end{scope}

  % Connectors
  \draw[dashed, line width=1.0pt, black!50] (ib_verbs.east) -- (roce_verbs.west);
  \draw[dashed, line width=1.0pt, black!50] (ib_trans.east) -- (roce_trans.west);

\end{tikzpicture}
```
:::

### Losslessness and the Go-Back-N Problem {#sec-network-fabrics-losslessness}

ML collectives assume in-order, lossless delivery, but the hardware implementation of this reliability introduces a critical fragility. While TCP handles packet loss gracefully via Selective Acknowledgement (retransmitting only the specific missing segment), RDMA protocols like RoCEv2 typically rely on the simpler **Go-Back-N** mechanism. This design choice is driven by the physical constraints of the NIC: implementing complex reassembly logic for out-of-order packets requires substantial on-chip SRAM, which consumes precious die area needed for SerDes blocks and packet processing engines. The NIC hardware is optimized for throughput, not state management.

The trade-off is a severe penalty upon failure. If a network switch drops a single packet 900 MB into a 1 GB gradient transfer, the receiver discards all subsequent packets, forcing the sender to retransmit the entire tail of the message—potentially 100 MB of data for a single missed frame. At `{python} ib_ndr_gbps` Gbps, this retransmission triggers a latency spike orders of magnitude larger than the wire delay. In a synchronous training loop where thousands of GPUs wait for the slowest member, this straggler effect means a single dropped packet idles the entire cluster. The network fabric must therefore behave as a lossless medium, pushing the complexity of flow control into the switches via Priority Flow Control (PFC) to ensure buffers never overflow.

::: {.callout-checkpoint title="Protocol Selection"}

Consider a 2,048-GPU training cluster that will run both large language model training (gradient messages of several gigabytes) and reinforcement learning (frequent small control messages).

1. Which protocol would you recommend and why? Consider that large messages are bandwidth-dominated while small messages are latency-dominated.
2. If you chose RoCE, what additional infrastructure would be needed compared to InfiniBand?
3. How would your answer change if the cluster also needed to serve inference traffic to external users over the same physical network?

:::

### The $\alpha$-$\beta$ Performance Model {#sec-network-fabrics-performance-model}

\index{alpha-beta model}

::: {.callout-definition title="The alpha-beta Model"}

***α-β Model***\index{Alpha-Beta Model!definition} is a linear cost model for network communication where the time to transfer a message of $n$ bytes is $T(n) = \alpha + n/\beta$.

1.  **Significance (Quantitative):** It maps directly to the **Iron Law**, where **α** (alpha) represents the fixed **Startup Latency ($L_{\text{lat}}$)** and **β** (beta) represents the **Network Bandwidth ($\text{BW}$)**. It separates the **Latency-Bound Regime** (small messages) from the **Bandwidth-Bound Regime** (large messages).
2.  **Distinction (Durable):** Unlike **Idealized Throughput Models**, the α-β model captures the **Fixed Penalty** of communication, explaining why many small messages are 100$\times$ more expensive than one large message of the same total size.
3.  **Common Pitfall:** A frequent misconception is that $\alpha$ is just "network delay." In reality, $\alpha$ includes **Software Overhead** (kernel context switches, stack traversal, and library synchronization) that can often exceed the physical wire delay.

:::

```{python}
#| echo: false
#| label: network-alpha-beta-refactor
# ┌─────────────────────────────────────────────────────────────────────────────
# │ NETWORK ALPHA-BETA MODEL (LEGO)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-network-fabrics-performance-model
# │
# │ Goal: Quantify the latency-bandwidth crossover for InfiniBand vs Ethernet.
# │ Show: Crossover points (~75 KB for IB); impact on small vs large messages.
# │ How: T = alpha + n/beta. Small msg: 4 KB; Large msg: 350 MB.
# │
# │ Imports: mlsys.constants (INFINIBAND_NDR_BW, Gbps, MILLION, TRILLION)
# │ Exports: n_cross_ib_kb_str, t_small_ib_us_str, small_bw_pct_str,
# │          t_large_ib_ms_str, ib_alpha_us_str, ib_beta_gbs_str
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.constants import INFINIBAND_NDR_BW, Gbps, MILLION, TRILLION, MICROSECOND
from mlsys.formatting import fmt, check

class NetworkAlphaBeta:
    """Namespace for the alpha-beta communication model."""
    # ┌── 1. LOAD (Constants) ───────────────────────────────────────────────
    # Level 1: Hardware Specs
    alpha_ib = 1.5e-6 # 1.5 us (typical for IB NDR)
    beta_ib = INFINIBAND_NDR_BW.m_as('byte/second') # ~50 GB/s

    # Level 2: Messages
    n_small = 4000 # 4 KB
    n_large = 350 * MILLION # 350 MB

    # ┌── 2. EXECUTE (The Compute) ─────────────────────────────────────────
    n_cross_ib = alpha_ib * beta_ib

    t_small_ib = alpha_ib + (n_small / beta_ib)
    small_bw_term_pct = ((n_small / beta_ib) / t_small_ib) * 100

    t_large_ib = alpha_ib + (n_large / beta_ib)

    # ┌── 3. GUARD (Invariants) ───────────────────────────────────────────
    check(n_cross_ib < 100000, f"Crossover ({n_cross_ib:.0f}) should be < 100 KB for IB")
    check(small_bw_term_pct < 10, f"Small msg should be latency-bound, got {small_bw_term_pct:.1f}% BW share")

    # ┌── 4. OUTPUT (Formatting) ──────────────────────────────────────────────
    n_cross_ib_kb_str = f"{n_cross_ib/1000:.0f}"
    t_small_ib_us_str = f"{t_small_ib * 1e6:.2f}"
    small_bw_pct_str = f"{small_bw_term_pct:.0f}"
    t_large_ib_ms_str = f"{t_large_ib * 1e3:.1f}"
    ib_alpha_us_str = f"{alpha_ib * 1e6:.1f}"
    ib_beta_gbs_str = f"{beta_ib / 1e9:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
n_cross_ib_kb_str = NetworkAlphaBeta.n_cross_ib_kb_str
t_small_ib_us_str = NetworkAlphaBeta.t_small_ib_us_str
small_bw_pct_str = NetworkAlphaBeta.small_bw_pct_str
t_large_ib_ms_str = NetworkAlphaBeta.t_large_ib_ms_str
ib_alpha_us_str = NetworkAlphaBeta.ib_alpha_us_str
ib_beta_gbs_str = NetworkAlphaBeta.ib_beta_gbs_str
```

The model reveals two regimes:
1. **Latency-dominated ($n < \alpha\beta$):** Transfer time is dominated by the startup cost $\alpha$. Small control messages and pipeline bubbles fall here.
2. **Bandwidth-dominated ($n > \alpha\beta$):** Transfer time scales with $n/\beta$. Gradient AllReduce typically falls here.

For NDR InfiniBand with $\alpha \approx `{python} ib_alpha_us_str` \mu\text{s}$ and $\beta \approx `{python} ib_beta_gbs_str` \text{GB/s}$, the crossover point $n = \alpha\beta \approx `{python} n_cross_ib_kb_str` \text{ KB}$. Messages smaller than this gain little from more bandwidth; messages larger than this gain little from lower latency.

To see why this distinction matters in practice, consider two messages that our 175B model training job sends every iteration. The first is a 4 KB pipeline-scheduling control message that coordinates microbatch handoffs between pipeline stages. Applying the model: $T(4\text{ KB}) \approx \mathbf{`{python} t_small_ib_us_str` \mu\text{s}}$. The bandwidth term contributes only `{python} small_bw_pct_str`% of the total. Doubling the link speed would save a negligible fraction of a microsecond. For this message, the only way to reduce transfer time is to reduce the hop count (which lowers $\alpha$), not to buy faster links.

The second message is a 350 MB gradient shard for one layer's AllReduce. Now: $T(350\text{ MB}) \approx \mathbf{`{python} t_large_ib_ms_str` \text{ ms}}$. The latency term is invisible. Doubling bandwidth to 100 GB/s would halve this transfer time, a direct and proportional gain. These two cases illustrate why network design must address both $\alpha$ and $\beta$ simultaneously: topology and hop count control the latency-dominated regime, while link speed and path diversity control the bandwidth-dominated regime.

@fig-alpha-beta-crossover makes these two regimes visible across the full range of message sizes. For InfiniBand, the crossover occurs at approximately `{python} n_cross_ib_kb_str` KB: messages smaller than this are latency-dominated (the flat region on the left), while larger messages are bandwidth-dominated (the linear region on the right). The 5$\times$ latency gap between InfiniBand and Ethernet RoCE dominates for small messages but becomes irrelevant for the multi-megabyte gradient transfers that dominate training communication.

::: {#fig-alpha-beta-crossover fig-env="figure" fig-pos="htb" fig-cap="**The Alpha-Beta Crossover**. Transfer time as a function of message size for InfiniBand NDR and Ethernet RoCE. Small messages are latency-dominated (flat region), while large messages are bandwidth-dominated (linear region). The crossover point marks where investing in bandwidth begins to pay off more than reducing latency." fig-alt="Log-log plot showing transfer time versus message size for InfiniBand and Ethernet, with latency-dominated and bandwidth-dominated regions labeled."}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ALPHA-BETA CROSSOVER (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-alpha-beta-crossover — latency vs bandwidth regime in fabric
# │          selection
# │
# │ Goal: Visualize transfer time vs message size for InfiniBand NDR vs Ethernet
# │       RoCE; show latency-dominated (flat) vs bandwidth-dominated (linear).
# │ Show: Log-log plot with crossover points (~75 KB for IB); alpha/beta model.
# │ How: T = alpha + n/beta; plot for both fabrics; annotate crossover.
# │
# │ Imports: matplotlib.pyplot (plt), numpy (np), mlsys.constants
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
from mlsys.constants import INFINIBAND_NDR_BW, NETWORK_100G_BW

alpha_ib = 1.5e-6
beta_ib = INFINIBAND_NDR_BW.m_as('byte/second')
alpha_eth = 5.0e-6
beta_eth = NETWORK_100G_BW.m_as('byte/second')

n = np.logspace(0, 10, 1000)
t_ib = alpha_ib + n / beta_ib
t_eth = alpha_eth + n / beta_eth

n_cross_ib = alpha_ib * beta_ib
t_cross_ib = 2 * alpha_ib
n_cross_eth = alpha_eth * beta_eth
t_cross_eth = 2 * alpha_eth

fig, ax = plt.subplots(figsize=(9, 5))

ax.plot(n, t_ib, label=f'InfiniBand NDR ({alpha_ib*1e6:.1f}us, {beta_ib/1e9:.0f} GB/s)', color='#006395', linewidth=2.5)
ax.plot(n, t_eth, label=f'Ethernet RoCE ({alpha_eth*1e6:.1f}us, {beta_eth/1e9:.0f} GB/s)', color='#CB202D', linewidth=2.5, linestyle='--')

ax.scatter([n_cross_ib], [t_cross_ib], color='#006395', s=80, zorder=5, edgecolors='white')
ax.scatter([n_cross_eth], [t_cross_eth], color='#CB202D', s=80, zorder=5, edgecolors='white')

ax.annotate(f'Crossover\n~{n_cross_ib/1e3:.0f} KB',
            xy=(n_cross_ib, t_cross_ib),
            xytext=(n_cross_ib/20, t_cross_ib*3),
            arrowprops=dict(arrowstyle='->', color='#006395'),
            color='#006395', fontsize=9, ha='center')

ax.annotate(f'Crossover\n~{n_cross_eth/1e3:.0f} KB',
            xy=(n_cross_eth, t_cross_eth),
            xytext=(n_cross_eth*20, t_cross_eth/3),
            arrowprops=dict(arrowstyle='->', color='#CB202D'),
            color='#CB202D', fontsize=9, ha='center')

split_point = n_cross_ib
ax.axvspan(1, split_point, color='gray', alpha=0.1)

ax.text(3e1, 2e-4, "Latency\nDominated", fontsize=12, color='#444', ha='center', va='center', fontweight='bold')
ax.text(1e8, 2e-4, "Bandwidth\nDominated", fontsize=12, color='#444', ha='center', va='center', fontweight='bold')

ax.set_xscale('log')
ax.set_yscale('log')
ax.set_xlim(1, 1e10)
ax.set_ylim(5e-7, 1)
ax.grid(True, which="major", ls="-", alpha=0.6)
ax.grid(True, which="minor", ls=":", alpha=0.3)
ax.set_xlabel('Message Size (Bytes)', fontweight='bold')
ax.set_ylabel('Transfer Time (Seconds)', fontweight='bold')
ax.legend(loc='upper left', frameon=True, framealpha=0.9, fancybox=True)
ax.xaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=12))
plt.tight_layout()
plt.show()
```
:::

::: {#nb-allreduce-bottleneck .callout-notebook title="When Does AllReduce Become the Bottleneck?"}

```{python}
#| echo: false
#| label: allreduce-bottleneck
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ALLREDUCE BOTTLENECK ANALYSIS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Network Performance Modeling - Alpha-Beta Model.
# │
# │ Goal: Identify when communication intensity overwhelms computation.
# │ Show: Growth of comm overhead from 1B to 70B parameter models.
# │ How: Calculate T_compute vs T_ring_allreduce using Alpha-Beta parameters.
# │
# │ Imports: mlsys.constants
# │ Exports: t_comp_ms, t_comm_1b_ms, t_comm_70b_ms, comm_frac_1b
# └─────────────────────────────────────────────────────────────────────────────

# ┌── LEGO ───────────────────────────────────────────────
from mlsys.constants import H100_FLOPS_FP16_TENSOR, INFINIBAND_NDR_BW, TFLOPs, second, GB
class AllReduceBottleneck:
    """
    Scenario: Cluster of 1024 H100 GPUs training models of varying scale.
    Alpha-Beta parameters for NDR InfiniBand.
    """

    # ┌── 1. LOAD (Constants) ───────────────────────────────────────────────
    num_gpus = 1024
    peak_tflops = H100_FLOPS_FP16_TENSOR.m_as(TFLOPs / second)
    gpu_util = 0.50
    flops_per_sample_base = 2e18 # Rough estimate for 1B model iteration

    alpha_s = 1.5e-6 # NDR InfiniBand hop latency
    beta_gbs = (INFINIBAND_NDR_BW / 8).m_as(GB / second)

    params_small = 1e9
    params_large = 70e9

    # ┌── 2. EXECUTE (The Compute) ─────────────────────────────────────────
    # Step 1: Compute time
    t_comp_s = flops_per_sample_base / (peak_tflops * 1e12 * gpu_util)

    # AllReduce time: 2(p-1)alpha + 2(p-1)/p * m/beta
    # Step 2: Using a temp variable for the latency part of the calculation
    _latency_term = 2 * (num_gpus - 1) * alpha_s

    _bandwidth_term_small = (2 * (num_gpus - 1) / num_gpus) * ((params_small * 4) / (beta_gbs * 1e9))
    t_comm_small_s = _latency_term + _bandwidth_term_small

    _bandwidth_term_large = (2 * (num_gpus - 1) / num_gpus) * ((params_large * 4) / (beta_gbs * 1e9))
    t_comm_large_s = _latency_term + _bandwidth_term_large

    comm_frac_small = t_comm_small_s / (t_comp_s + t_comm_small_s)

    # ┌── 3. GUARD (Invariants) ───────────────────────────────────────────
    check(t_comp_s > 0.1, f"Compute time should be significant, got {t_comp_s:.2f}s")
    check(t_comm_large_s > t_comm_small_s * 50, "70B comm should be >50x 1B comm")

    # ┌── 4. OUTPUT (Formatting) ──────────────────────────────────────────────
    t_comp_ms = f"{t_comp_s * 1000:.0f}"
    t_comm_1b_ms = f"{t_comm_small_s * 1000:.0f}"
    t_comm_70b_ms = f"{t_comm_large_s * 1000:.0f}"
    comm_frac_1b_pct = f"{comm_frac_small * 100:.1f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
t_comp_ms = AllReduceBottleneck.t_comp_ms
t_comm_1b_ms = AllReduceBottleneck.t_comm_1b_ms
t_comm_70b_ms = AllReduceBottleneck.t_comm_70b_ms
comm_frac_1b = AllReduceBottleneck.comm_frac_1b_pct

# Math built in Python (vol1 pattern: no {python} inside math)
alpha_math = md_math(f"\\alpha = {AllReduceBottleneck.alpha_s * 1e6:.1f} \\;\\mu\\text{{s}}")
beta_math = md_math(f"\\beta = {AllReduceBottleneck.beta_gbs:.0f} \\;\\text{{GB/s}}")
t_comp_display_math = md(f"$$ T_{{\\text{{compute}}}} = {t_comp_ms} \\text{{ ms}} $$")
t_ring_1b_math = md_math(f"T_{{\\text{{ring}}}} \\approx {t_comm_1b_ms} \\text{{ ms}}")
comm_frac_display_math = md(f"$$ \\text{{Comm. fraction}} = {comm_frac_1b} \\% $$")
t_ring_70b_display_math = md(f"$$ T_{{\\text{{ring}}}} \\approx {t_comm_70b_ms} \\text{{ ms}} $$")
```

**Setup**: A cluster of 1,024 H100 GPUs training a model with 1 billion parameters (4 GB of FP32 gradients). Each GPU computes at `{python} h100_tflops` TFLOPS. The network uses NDR InfiniBand (`{python} alpha_math` and `{python} beta_math` per link).

**Step 1: Compute time per iteration.**

Assume each GPU processes a microbatch requiring $2 \times 10^{18}$ FLOPs (a rough estimate for a forward plus backward pass on a large batch). At `{python} h100_tflops` TFLOPS with 50% utilization:

`{python} t_comp_display_math`

**Step 2: Communication time (Ring AllReduce).**

With $p = 1{,}024$ and $m = 4 \times 10^9$ bytes:

- Total Communication: `{python} t_ring_1b_math`

**Step 3: Communication fraction.**

`{python} comm_frac_display_math`

With overlap between communication and computation (possible because the backward pass produces gradients layer by layer), the effective overhead can be reduced further. The network is not the bottleneck for this configuration.

**When does it become the bottleneck?** If we scale to 70 billion parameters (280 GB of gradients) and the per-GPU computation stays similar (through batch size scaling), the bandwidth term alone becomes:

`{python} t_ring_70b_display_math`

Now communication dominates computation by nearly 3$\times$. Our 175B model, at 2.5$\times$ this scale, would be even more severely bottlenecked by pure data parallelism. This is precisely why models beyond a few billion parameters require tensor and pipeline parallelism to partition the model, rather than relying solely on data parallelism, which must AllReduce the full gradient vector.

:::

The $\alpha$-$\beta$ model quantifies the speed of a single link, but our 175B-parameter model requires 1,000 GPUs to work in concert. Scaling these transport primitives from a pair of nodes to a warehouse-scale supercomputer demands a **Topology**—a specific pattern of connections that maximizes bisection bandwidth while minimizing the hop count and cabling cost for global collective operations.

## Level 3: Switch and Topology {#sec-network-fabrics-topology}

The physical arrangement of switches determines the **bisection bandwidth** and whether the fabric is **non-blocking**. These two properties govern how well the network supports the global communication patterns that dominate distributed training.

::: {.callout-definition title="Bisection Bandwidth"}

***Bisection Bandwidth***\index{Bisection Bandwidth!definition} is the minimum total bandwidth across any cut that divides the network into two equal halves.

1.  **Significance (Quantitative):** It represents the **Worst-Case Throughput** for global communication patterns (e.g., AllReduce). Within the **Iron Law**, bisection bandwidth defines the **Aggregate Bandwidth ($\text{BW}_{\text{bisect}}$)** available when the entire fleet must synchronize its state simultaneously.
2.  **Distinction (Durable):** Unlike **Aggregate Bandwidth** (the sum of all link speeds), Bisection Bandwidth measures the **Global Connectivity** of the fabric, identifying the narrowest bottleneck in the cluster.
3.  **Common Pitfall:** A frequent misconception is that adding more switches always increases bisection bandwidth. In reality, it is a **Topology Property**: if the upper tiers of the network are oversubscribed, the bisection bandwidth will be significantly lower than the sum of the edge links.

:::

For ML training, where all-to-all communication is standard, full bisection bandwidth is the foundational requirement. A fabric that falls short forces every synchronization step to bottleneck at the narrowest cross-section, and the entire cluster idles while gradients trickle through.

::: {.callout-definition title="Non-blocking Fabric"}

***Non-blocking Fabric***\index{Non-blocking Fabric!definition} is a network topology in which any permutation of input-output port pairs can communicate simultaneously at full line rate without internal contention.

1.  **Significance (Quantitative):** In ML fleets, it ensures that synchronization traffic from any subset of accelerators does not compete for shared links, maximizing the **System Throughput ($\eta$)** by eliminating network-induced stalls.
2.  **Distinction (Durable):** Unlike **Oversubscribed Fabrics** (common in web datacenters), where upper-tier links are shared among many lower-tier nodes, a Non-blocking Fabric provides **Dedicated Path Capacity** for every possible pairing.
3.  **Common Pitfall:** A frequent misconception is that non-blocking means "zero congestion." In reality, **Endpoint Congestion** (Incast) can still occur if multiple senders target the same receiver simultaneously, regardless of the topology's internal capacity.

:::

These definitions shape every topology choice that follows: fat-trees achieve non-blocking behavior through redundant switch tiers, rail-optimized networks exploit workload structure to approximate it at lower cost, and dragonflies trade bisection bandwidth for reduced cabling.

::: {#fig-topology-bisection fig-env="figure" fig-pos="htb" fig-cap="**Topology Bisection Bandwidth Comparison**. Fat-Tree architectures provide full bisection bandwidth, while other topologies trade this off for cost or latency." fig-alt="Bar chart or comparison diagram showing bisection bandwidth across Fat-Tree, Torus, and other network topologies."}
```{=latex}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \usetikzlibrary{positioning}
  \tikzset{
    bar/.style 2 args={
      draw=#1, fill=#2, line width=0.75pt,
      minimum width=1.5cm, inner sep=0pt,
      anchor=south
    },
    axis/.style={line width=1.0pt, ->},
    gridline/.style={gray!20, line width=0.5pt},
    label/.style={anchor=north, align=center, font=\small},
    value/.style={anchor=south, font=\bfseries}
  }

  % Axes
  \draw[axis] (0,0) -- (10,0) node[anchor=north west] {Topology};
  \draw[axis] (0,0) -- (0,5.5) node[anchor=south east] {Bisection \%};

  % Grid lines
  \foreach \y in {1.25, 2.5, 3.75, 5}
      \draw[gridline] (0,\y) -- (10,\y);

  \draw[dashed, line width=1.0pt, gray!80] (0,5) -- (10,5) node[anchor=south east, text=black] {\small Full Bisection};

  % Y-axis labels
  \node[anchor=east] at (0,5) {100\%};
  \node[anchor=east] at (0,2.5) {50\%};
  \node[anchor=east] at (0,0) {0\%};

  % Bars
  \node[bar={BlueLine}{BlueL}, minimum height=5cm] (b1) at (1.75,0) {};
  \node[value] at (b1.north) {100\%};
  \node[label] at (b1.south) {Fat-Tree};

  \node[bar={GreenLine}{GreenL}, minimum height=2.5cm] (b2) at (3.75,0) {};
  \node[value] at (b2.north) {50\%};
  \node[label] at (b2.south) {Rail-Opt};

  \node[bar={OrangeLine}{OrangeL}, minimum height=3.75cm] (b3) at (5.75,0) {};
  \node[value] at (b3.north) {75\%};
  \node[label] at (b3.south) {Dragonfly};

  \node[bar={RedLine}{RedL}, minimum height=1.25cm] (b4) at (7.75,0) {};
  \node[value] at (b4.north) {25\%};
  \node[label] at (b4.south) {Torus};

\end{tikzpicture}
```
:::

### Top-of-Rack (ToR) and the Failure Domain {#sec-network-fabrics-tor}

\index{ToR}

The **Top-of-Rack (ToR)** switch serves as the fundamental physical aggregation point, defining both bandwidth limits and the minimum **failure domain** for the cluster. In a high-density AI configuration using standard DGX nodes, a single rack typically houses 4 nodes, each containing 8 GPUs, for a total of 32 accelerators. The ToR switch unites these devices but also creates a critical vulnerability: if the ToR fails, it instantly partitions 32 GPUs from the training job, forcing the global scheduler to halt and recover from the last checkpoint.

To mitigate congestion at this edge, network architects maximize the switch **radix**—the number of ports available. A modern switch with a radix of 64 allocates 32 ports downlink to the servers (ensuring full bandwidth for the 32 GPUs) and 32 ports uplink to the spine. This 1:1 subscription ratio guarantees non-blocking performance at the rack level. For our 1,000-GPU cluster, the physical topology comprises approximately 32 such racks. The job scheduler must be topology-aware, spreading replicas across these failure domains to ensure that a single lost rack does not take down the entire training run (@sec-fleet-orchestration).

### Fat-Tree (Clos) Networks {#sec-network-fabrics-fat-tree}

\index{fat-tree}\index{Clos network}

::: {.callout-definition title="Fat-Tree"}

***Fat-Tree***\index{Fat-Tree!definition} is a hierarchical network topology where the aggregate bandwidth increases toward the root to provide multiple equal-cost paths between any two nodes.

1.  **Significance (Quantitative):** It is the standard for providing **Full Bisection Bandwidth ($\text{BW}_{\text{bisect}}$)**. It ensures that the fabric can sustain simultaneous full-rate communication from all endpoints, which is a non-negotiable requirement for the AllReduce collectives that dominate training.
2.  **Distinction (Durable):** Unlike a **Standard Tree** (where links near the root become bottlenecks), a Fat-Tree uses redundant switches and links to ensure the network is **Non-blocking**.
3.  **Common Pitfall:** A frequent misconception is that Fat-Trees are the most "efficient" topology. In reality, they are the most **Reliable but Expensive**: they require a massive number of switches ($O(N \log N)$) and complex cabling compared to more specialized topologies like Dragonflies or Torus networks.

:::

The fat-tree[^fn-fat-tree-clos] is the industry standard for ML clusters because it strictly guarantees full bisection bandwidth—a non-negotiable requirement for the AllReduce collective, which demands simultaneous, all-to-all communication. The network is constructed in hierarchical tiers: **Leaf** switches (ToR) connect directly to servers, **Spine** switches interconnect all leaves within a locality domain known as a **pod**, and **Core** switches bind multiple pods together.

[^fn-fat-tree-clos]: **Fat-Tree (Clos Network)**: The underlying multi-stage switching theory was invented by Charles Clos at Bell Labs in 1953 to minimize the number of electromechanical crosspoints in telephone exchanges. Charles Leiserson at MIT generalized the concept in 1985 as the "fat-tree," where the tree is "fat" because link bandwidth increases toward the root, proving it could emulate any network of equal hardware volume. The same cost-minimization logic that drove 1950s telephony now drives ML cluster design: minimize switch count while guaranteeing non-blocking connectivity for global AllReduce. \index{Fat-Tree!Clos origin}

A fat-tree built from switches with **radix**[^fn-switch-radix-density] $k$ supports $N = k^3/4$ hosts. In a two-tier network using radix-64 switches, a single pod can support up to 1,024 GPUs—perfectly accommodating our 1,000-GPU reference cluster with only leaf and spine layers.

[^fn-switch-radix-density]: **Switch Radix**: Modern high-end switches (e.g., NVIDIA Quantum-2) feature a radix of 64 ports, each at 400 Gbps. This density allows a two-tier fat-tree to support up to 2,048 GPUs with only two switch hops. As model scale pushes toward 100,000 GPUs, increasing switch radix (to 128 or 256) is the only way to avoid adding more tiers and the resulting latency/cost explosion. \index{Switch Radix!port density}
 Scaling beyond this requires a three-tier architecture with core switches, enabling the fabric to reach `{python} fat_tree_hosts` hosts. The infrastructure cost is substantial: a non-blocking $k=64$ tree requires 4,096 switches. At \$10,000 to \$50,000 per switch, the switching layer alone represents \$40 to \$200 million.

However, scale imposes a latency tax. Typical **hop counts** are 2 for intra-pod (leaf-spine-leaf) and 4 for inter-pod (leaf-spine-core-spine-leaf). Each additional hop introduces serialization delay and switch processing time, adding microseconds to the $\alpha$ latency term that accumulates during the thousands of synchronization steps in a training run.

### Rail-Optimized Topology {#sec-network-fabrics-rail-optimized}

::: {#fig-rail-optimized-cabling fig-env="figure" fig-pos="htb" fig-cap="**Rail-Optimized Physical Wiring**. In modern ML nodes (e.g., DGX H100), GPUs are partitioned into 'Rails' corresponding to their PCIe slot position. GPU 0 in every node connects to Switch Rail 0, GPU 1 to Switch Rail 1, and so on. This architecture ensures that **Tensor Parallelism**—which synchronizes between corresponding GPUs—never competes for bisection bandwidth with other parallel groups, minimizing hop count and congestion for the most latency-sensitive traffic." fig-alt="Physical cabling diagram showing two server nodes, each with 4 GPUs. Cables connect GPU 0s to Rail Switch 0, GPU 1s to Rail Switch 1, etc."}
```{.tikz}
\usetikzlibrary{positioning}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{NodeColor}{HTML}{F5F5F5}
  \definecolor{GPUColor}{HTML}{D1E6F3}
  \definecolor{SwitchColor}{HTML}{006395}
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{OrangeLine}{HTML}{E67817}

  \tikzset{
    nodebox/.style={draw=black!60, line width=0.75pt, fill=NodeColor, rounded corners=2pt, minimum width=5cm, minimum height=1.5cm, anchor=center},
    gpu/.style={draw=black!40, line width=0.75pt, fill=GPUColor, minimum width=0.8cm, minimum height=0.6cm, font=\tiny\bfseries, anchor=center},
    switch/.style={circle, draw=black!60, line width=0.75pt, fill=SwitchColor, text=white, inner sep=2pt, font=\tiny\bfseries, anchor=center},
    nodelabel/.style={font=\tiny\bfseries, anchor=north west}
  }

  % Nodes
  \node[nodebox] (N1) at (0, 0) {};
  \node[nodelabel] at (N1.north west) {Node 1};

  \node[nodebox, below=1.5cm of N1] (N2) {};
  \node[nodelabel] at (N2.north west) {Node 2};

  % GPUs in Node 1
  \node[gpu] (G1_0) at ([xshift=-1.8cm]N1.center) {GPU 0};
  \node[gpu, right=0.4cm of G1_0] (G1_1) {GPU 1};
  \node[gpu, right=0.4cm of G1_1] (G1_2) {GPU 2};
  \node[gpu, right=0.4cm of G1_2] (G1_3) {GPU 3};

  % GPUs in Node 2
  \node[gpu] (G2_0) at ([xshift=-1.8cm]N2.center) {GPU 0};
  \node[gpu, right=0.4cm of G2_0] (G2_1) {GPU 1};
  \node[gpu, right=0.4cm of G2_1] (G2_2) {GPU 2};
  \node[gpu, right=0.4cm of G2_2] (G2_3) {GPU 3};

  % Rail Switches
  \node[switch, above=1.5cm of G1_0] (S0) {R0};
  \node[switch, above=1.5cm of G1_1] (S1) {R1};
  \node[switch, above=1.5cm of G1_2] (S2) {R2};
  \node[switch, above=1.5cm of G1_3] (S3) {R3};

  % Cabling
  \foreach \i in {0,1,2,3} {
    \draw[line width=1.2pt, BlueLine] (G1_\i) -- (S\i);
    \draw[line width=1.2pt, BlueLine] (G2_\i) -- (S\i);
  }

  % Annotation
  \draw[latex-latex, line width=1.0pt, OrangeLine] (G1_0) -- (G1_1) node[midway, below, font=\tiny, text=black] {NVLink};
  \node[right=0.2cm of S3, align=left, font=\tiny] {\textbf{Rail Switches}\\(InfiniBand)};

\end{tikzpicture}
```
:::

\index{rail-optimized topology}

In distributed training, **tensor parallelism** creates a deterministic and highly stratified communication pattern that standard topologies fail to exploit. Because large models partition individual matrix multiplications across GPUs, GPU 0 on one node must frequently exchange activations with GPU 0 on other nodes, but rarely with GPU 1. A **rail-optimized topology** physically hardwires this logic by isolating these communication paths into dedicated networks. Instead of connecting all 8 GPUs in a node to a single ToR switch, the network connects all GPU 0s across the entire cluster to a dedicated "Rail 0" switch fabric, all GPU 1s to "Rail 1," and so on.

::: {.callout-lighthouse title="Archetype A (GPT-4 / Llama-3): The Rail-Optimized Fleet"}
**Archetype A (GPT-4)** is the primary driver for rail-optimized fabrics. Because it uses **3D Parallelism**, it generates two distinct traffic patterns: (1) high-frequency, latency-sensitive activation exchanges for tensor parallelism, and (2) massive, bandwidth-hungry gradient averaging for data parallelism. The rail-optimized design ensures that tensor-parallel traffic traverses only a single switch hop between nodes, minimizing the latency that would otherwise stall the synchronous training loop.
:::

For our 1,000-GPU cluster spanning 125 nodes, this creates 8 parallel networks of 125 GPUs each, allowing tensor-parallel traffic to traverse a single switch hop rather than the multi-hop leaf-spine-leaf path required by a standard fat-tree.
 The latency benefit is significant for the frequent, small activation exchanges that tensor parallelism demands. However, this architecture introduces a sharp trade-off: data parallelism requires global synchronization across *all* GPUs, crossing all rails. Modern clusters therefore employ a hybrid approach, using rail-optimized leaves for tensor-parallel traffic while bridging the rails with a full fat-tree spine to support the global AllReduce patterns of data parallelism.

### Dragonfly and Torus Alternatives {#sec-network-fabrics-dragonfly}

\index{dragonfly topology}\index{torus}

A **Dragonfly**[^fn-dragonfly-isca] topology organizes switches into high-radix groups, where each group functions as a fully connected island. These groups are then connected to one another via global optical links. This hierarchical structure minimizes the number of expensive long-distance cables required to scale, reducing cabling cost by roughly 50% compared to a fat-tree. However, this efficiency comes at the price of oversubscription: while bandwidth within a group is non-blocking (100%), the bandwidth between groups is typically designed to be only 25–50% of the aggregate injection rate. For ML workloads, this creates a binary performance cliff based on job placement. A training job that fits entirely within a single dragonfly group sees full line-rate performance; a job that spans multiple groups is throttled by the limited global bandwidth, potentially suffering a 2--4$\times$ slowdown. Consequently, dragonfly fabrics require topology-aware schedulers that rigidly pack jobs into groups to avoid crossing the oversubscribed global links.

[^fn-dragonfly-isca]: **Dragonfly Topology**: Introduced by John Kim and William Dally at ISCA 2008, the dragonfly uses high-radix routers grouped into fully connected "super-nodes" to reduce global cabling by 52% compared to a folded Clos of equivalent scale. The cost savings are real but create a rigid scheduling constraint: any training job that crosses group boundaries hits the oversubscribed global links, making topology-aware placement mandatory rather than optional. \index{Dragonfly!topology origin}

A **Torus** topology connects each node directly to its neighbors in a multidimensional grid, most commonly a 3D torus where every node links to its six adjacent peers (up/down, left/right, front/back). This design offers full local bandwidth with minimal switching hardware, as connections travel only 1–2 hops to reach neighbors. However, global communication requires traversing the diameter of the mesh ($O(N^{1/3})$ hops), making latency scale poorly with cluster size. Google adopted this architecture for its TPU pods because Transformer training is dominated by data parallelism and pipeline parallelism, both of which use nearest-neighbor communication patterns that map naturally onto the physical grid. The limitation becomes apparent with architectures like Mixture-of-Experts (MoE), which rely on AllToAll communication patterns; on a torus, these random permutations congest the limited bisection bandwidth of the mesh, causing performance to degrade by 2--4$\times$ compared to a switch-based fat-tree.

The trade-offs between these topologies become stark when quantified for a large-scale deployment. Consider a 4,096-GPU cluster. A non-blocking fat-tree requires approximately 2,048 switches and 40,000 optical cables to deliver 100% bisection bandwidth, enabling any GPU to communicate with any other at full speed. A 3D torus connecting the same nodes might use zero external switches (relying on direct host-to-host links) and only short copper cables, but offers only a fraction of the bisection bandwidth (scaling with $N^{2/3}$). This explains the divergence in industry architectures: Google's TPU Pods employ torus topologies because their workloads—primarily Transformer training—are predictable, dominated by nearest-neighbor communication that maps perfectly to the 3D grid. GPU clusters overwhelmingly favor fat-trees because their workloads are more diverse, ranging from recommendation systems to graph neural networks, and rely heavily on global AllReduce patterns that require the full bisection bandwidth only a tree can provide. A torus saves millions in switch costs but rigidly constrains the software; a fat-tree costs more but provides the universality needed for general-purpose AI research.

@fig-network-topologies illustrates the structural differences between these common families.

::: {#fig-network-topologies fig-env="figure" fig-pos="htb" fig-cap="**Network Topologies for ML**. (A) Fat-Tree provides full bisection bandwidth through hierarchical switch layers. (B) Torus connects neighbors, optimizing for local communication patterns such as those in TPU pods. (C) Rail-Optimized designs dedicate switch infrastructure to corresponding accelerator positions across nodes, minimizing hop count for tensor parallelism." fig-alt="Three-panel diagram: Fat-Tree with hierarchical switches, Torus with neighbor connections, Rail-Optimized with dedicated switch rails."}
```{.tikz}
\usetikzlibrary{positioning}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{NodeColor}{HTML}{D1E6F3}
  \definecolor{SwitchColor}{HTML}{006395}
  \definecolor{RailColor}{HTML}{E67817}
  \definecolor{GPUColor}{HTML}{F5D2D5}
  \definecolor{RedLine}{HTML}{CB202D}

  \tikzset{
    switch/.style={circle, fill=SwitchColor, draw=black!70, line width=0.75pt, inner sep=2pt, minimum size=0.45cm, text=white, font=\tiny\bfseries, anchor=center},
    nodebox/.style={rectangle, fill=NodeColor, draw=black!70, line width=0.75pt, inner sep=2pt, minimum size=0.4cm, anchor=center},
    hostbox/.style={draw=black!50, line width=0.75pt, rounded corners=2pt, fill=black!5, inner sep=4pt}
  }

  % Fat Tree
  \begin{scope}[local bounding box=fattree]
    \node[anchor=south, text=RedLine] (titleA) at (2, 3.5) {\textbf{A. Leaf-Spine (Fat-Tree)}};

    % Spine
    \node[switch] (sA) at (0.5, 3) {};
    \node[switch, right=0.55cm of sA] (sB) {};
    \node[switch, right=0.55cm of sB] (sC) {};
    \node[switch, right=0.55cm of sC] (sD) {};

    % Leaf
    \node[switch] (l0) at (0, 1.5) {};
    \node[switch, right=0.55cm of l0] (l1) {};
    \node[switch, right=1.55cm of l1] (l3) {};
    \node[switch, right=0.55cm of l3] (l4) {};

    % Nodes
    \node[nodebox] (nA) at (-0.25, 0) {};
    \node[nodebox, right=0.1cm of nA] (nB) {};
    \node[nodebox, right=0.1cm of nB] (nC) {};
    \node[nodebox, right=0.1cm of nC] (nD) {};

    \node[nodebox] (nmA) at (2.75, 0) {};
    \node[nodebox, right=0.1cm of nmA] (nmB) {};
    \node[nodebox, right=0.1cm of nmB] (nmC) {};
    \node[nodebox, right=0.1cm of nmC] (nmD) {};

    % Connections
    \foreach \s in {sA, sB, sC, sD} {
        \draw[black!40, line width=1.0pt] (\s) -- (l0);
        \draw[black!40, line width=1.0pt] (\s) -- (l1);
        \draw[black!40, line width=1.0pt] (\s) -- (l3);
        \draw[black!40, line width=1.0pt] (\s) -- (l4);
    }
    \draw[black!60, line width=1.0pt] (l0) -- (nA); \draw[black!60, line width=1.0pt] (l0) -- (nB);
    \draw[black!60, line width=1.0pt] (l1) -- (nC); \draw[black!60, line width=1.0pt] (l1) -- (nD);
    \draw[black!60, line width=1.0pt] (l3) -- (nmA); \draw[black!60, line width=1.0pt] (l3) -- (nmB);
    \draw[black!60, line width=1.0pt] (l4) -- (nmC); \draw[black!60, line width=1.0pt] (l4) -- (nmD);
  \end{scope}

  % Rail Optimized
  \begin{scope}[shift={(6,0)}, local bounding box=rail]
    \node[anchor=south, text=RedLine] (titleC) at (1.5, 3.5) {\textbf{C. Rail-Optimized}};

    % Rail Switches
    \node[switch, fill=RailColor] (rs0) at (0, 3) {R0};
    \node[switch, fill=RailColor, right=0.55cm of rs0] (rs1) {R1};
    \node[switch, fill=RailColor, right=0.55cm of rs1] (rs2) {R2};
    \node[switch, fill=RailColor, right=0.55cm of rs2] (rs3) {R3};

    % Nodes
    \node[hostbox, minimum width=3.8cm, minimum height=1.0cm] (host1) at (1.5, 0) {};
    \node[anchor=west, font=\tiny\bfseries] at (host1.west) {Node 1};

    \node[hostbox, minimum width=3.8cm, minimum height=1.0cm, below=0.5cm of host1] (host2) {};
    \node[anchor=west, font=\tiny\bfseries] at (host2.west) {Node 2};

    % GPUs
    \node[nodebox, fill=GPUColor] (g10) at (0, 0) {};
    \node[nodebox, fill=GPUColor, right=0.6cm of g10] (g11) {};
    \node[nodebox, fill=GPUColor, right=0.6cm of g11] (g12) {};
    \node[nodebox, fill=GPUColor, right=0.6cm of g12] (g13) {};

    \node[nodebox, fill=GPUColor] (g20) at (0, -1.5) {};
    \node[nodebox, fill=GPUColor, right=0.6cm of g20] (g21) {};
    \node[nodebox, fill=GPUColor, right=0.6cm of g21] (g22) {};
    \node[nodebox, fill=GPUColor, right=0.6cm of g22] (g23) {};

    % Draw cables
    \foreach \x in {0, 1, 2, 3} {
        \draw[line width=1.2pt, RailColor] (g1\x) -- (rs\x);
        \draw[line width=1.2pt, RailColor] (g2\x) -- (rs\x);
    }
  \end{scope}
\end{tikzpicture}
```
:::

::: {.callout-checkpoint}
## Topology Selection
Your choice of network topology dictates the upper bound of training efficiency. For a standard data-parallel job (1), bandwidth is dominated by AllReduce; a non-blocking Fat Tree (Clos) is ideal to maximize bisection bandwidth. For the Mixture-of-Experts model (2), the workload is AllToAll-dominant with bursty, sparse communication; a Dragonfly or high-radix topology might be preferred to minimize hop count, though congestion control becomes critical. For the 2D mesh computation (3), likely a physics simulation or a specific parallel strategy, a Torus topology offers the best locality, mapping the physical neighbor links directly to the hardware and avoiding the core switch bottlenecks entirely.
:::

::: {#nb-bisection-bottleneck .callout-notebook title="The Bisection Bottleneck"}

```{python}
#| echo: false
#| label: bisection-bottleneck
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BISECTION BOTTLENECK ANALYSIS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Network Topology - Bisection Bandwidth impact.
# │
# │ Goal: Quantify the slowdown from network oversubscription.
# │ Show: That cost-optimized (oversubscribed) networks throttle training.
# │ How: Calculate AllReduce time for 1:1 vs 4:1 oversubscription.
# │
# │ Imports: mlsys.constants
# │ Exports: bisec_time_a_ms, bisec_time_b_ms, waste_millions
# └─────────────────────────────────────────────────────────────────────────────

# ┌── LEGO ───────────────────────────────────────────────
from mlsys.constants import INFINIBAND_NDR_BW, GB, second
class BisectionBottleneck:
    """
    Scenario: 1024 accelerators (128 nodes) running AllReduce.
    Comparison of non-blocking (1:1) vs oversubscribed (4:1) fabric.
    """

    # ┌── 1. LOAD (Constants) ───────────────────────────────────────────────
    num_nodes = 128
    injection_bw_gbs = (INFINIBAND_NDR_BW / 8).m_as(GB / second)
    gradient_size_gb = 100
    oversub_ratio_b = 4
    cluster_cost_m = 300
    comm_fraction = 0.30

    # ┌── 2. EXECUTE (The Compute) ─────────────────────────────────────────
    # Step 1: Bisection bandwidth (aggregate across the cut)
    total_bisec_bw_a = num_nodes * injection_bw_gbs
    total_bisec_bw_b = total_bisec_bw_a / oversub_ratio_b

    # Step 2: Time = Data / Bandwidth
    time_a_s = gradient_size_gb / total_bisec_bw_a
    time_b_s = gradient_size_gb / total_bisec_bw_b

    # Step 3: Economic impact
    # Step 4: Relative throughput = 1 / ((1 - comm_frac) + (comm_frac * oversub_ratio))
    rel_throughput = 1 / ((1 - comm_fraction) + (comm_fraction * oversub_ratio_b))
    waste_val = (1 - rel_throughput) * cluster_cost_m

    # ┌── 3. GUARD (Invariants) ───────────────────────────────────────────
    check(time_b_s == time_a_s * 4, "Scenario B must be 4x slower than Scenario A")
    check(waste_val > 10, f"Waste should be significant, got ${waste_val:.1f}M")

    # ┌── 4. OUTPUT (Formatting) ──────────────────────────────────────────────
    time_a_ms = f"{time_a_s * 1000:.0f}"
    time_b_ms = f"{time_b_s * 1000:.0f}"
    waste_m = f"{waste_val:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
bisec_time_a_ms = BisectionBottleneck.time_a_ms
bisec_time_b_ms = BisectionBottleneck.time_b_ms
waste_millions = BisectionBottleneck.waste_m
```

**Problem**: You have 1,024 accelerators across 128 nodes. Each node has `{python} ib_ndr_gbps` Gb/s (`{python} ib_ndr_gbs` GB/s) injection bandwidth. You run an AllReduce job that requires full bisection bandwidth.

**Scenario A (non-blocking fat-tree)**: 1:1 oversubscription ratio. Bisection bandwidth = $128 \times 50 = 6{,}400$ GB/s = 6.4 TB/s.

**Scenario B (cost-optimized)**: 4:1 oversubscription at the spine layer. Bisection bandwidth = $6{,}400 / 4 = 1{,}600$ GB/s = 1.6 TB/s.

**The calculation**: Suppose the AllReduce must exchange 100 GB of gradient data (approximately 25 billion FP32 parameters).

- Time on Scenario A: `{python} bisec_time_a_ms` ms
- Time on Scenario B: `{python} bisec_time_b_ms` ms

**The systems conclusion**: Saving money on spine switches (Scenario B) slows every synchronization step of the entire cluster by approximately 4$\times$. If AllReduce accounts for 30% of each training iteration, the cluster's effective throughput drops significantly. For a \$300 million supercomputer, this amounts to wasting approximately **\$`{python} waste_millions` million** worth of accelerator time waiting for the network. Network oversubscription is false economy for training workloads.

:::

The preceding analysis assumed a single workload type. In practice, clusters serve multiple workloads with different communication patterns, and topology selection must balance their competing demands.

::: {.callout-checkpoint title="Topology Selection for Your Workload"}

You are designing the network for a new ML cluster that will run two primary workloads: (1) training a 175B-parameter language model using 3D parallelism (tensor, pipeline, and data parallelism), and (2) serving a Mixture-of-Experts model that relies heavily on AllToAll communication to route tokens to the correct experts.

1. The training workload's tensor parallelism operates within groups of 8 GPUs, while data parallelism operates across all nodes. Which topology (fat-tree, rail-optimized, or hybrid) best matches this communication pattern, and why?
2. The MoE serving workload generates unpredictable AllToAll traffic where any GPU may need to communicate with any other. How does this change your topology choice compared to the training workload?
3. If budget constraints force you to accept 2:1 oversubscription at the spine layer, which workload suffers more, and what scheduling strategy could mitigate the impact?

:::

Topology provides the structural capacity for our 1,000-GPU cluster to communicate, but structure alone does not guarantee performance. When all 1,000 GPUs simultaneously inject 350 GB of gradient traffic into the fabric, that theoretical capacity collides with the reality of **Fabric Behavior**: congestion control and routing dynamics determine whether this traffic flows smoothly or gridlocks under the strict synchronization of the BSP model.

## Level 4: Fabric Behavior (Congestion, Routing) {#sec-network-fabrics-behavior}

Real fabrics deviate from theoretical full-bisection bandwidth due to **congestion**, but the impact of this congestion is qualitatively different in ML clusters than in general-purpose networks. Web traffic is stochastic and asynchronous; if one user's request is delayed by 50 ms, it does not penalize the thousands of other users on the network. The Bulk Synchronous Parallel (BSP) execution model governs ML training, by contrast: every GPU must complete its communication before *any* GPU can proceed to the next compute step. This creates a weakest-link dynamic where the slowest flow in the fabric dictates the iteration time for the entire cluster. If a single link out of 10,000 becomes congested and doubles its latency, the effective throughput of the entire supercomputer drops for that step. In this synchronized regime, tail latency is not an outlier metric—it is the dominant performance constraint.

::: {.callout-definition title="Bulk Synchronous Parallel (BSP)"}

***Bulk Synchronous Parallel (BSP)***\index{Bulk Synchronous Parallel!definition} is a parallel computing model where execution proceeds in discrete **Supersteps**, each consisting of local computation, global communication, and a barrier synchronization.

1.  **Significance (Quantitative):** It is the standard execution model for synchronous distributed training. Within the **Iron Law**, BSP dictates that the total time $T$ is governed by the **Slowest Worker** (Straggler), making the **System Efficiency ($\eta$)** highly sensitive to network jitter and hardware variation.
2.  **Distinction (Durable):** Unlike **Asynchronous Parallelism**, which allows workers to use stale weights, BSP ensures **Mathematical Equivalence** to single-device training by enforcing a global state update at every step.
3.  **Common Pitfall:** A frequent misconception is that BSP is "inefficient" compared to async models. In reality, while it has higher **Synchronization Overhead ($L_{\text{lat}}$)**, it provides superior **Convergence Stability**, often leading to lower total operations ($O$) to reach a target loss.

:::

Because BSP enforces a global barrier at each superstep, the entire cluster moves at the speed of the slowest packet. A single congested switch port can stall 10,000 GPUs, making **tail latency (P99)** more critical than average throughput.

### Priority Flow Control (PFC) {#sec-network-fabrics-pfc}

\index{Priority Flow Control}

::: {.callout-definition title="Priority Flow Control"}

***Priority Flow Control (PFC)***\index{Priority Flow Control!definition} is a link-level mechanism that prevents buffer overflow by sending PAUSE frames to the upstream sender on a per-priority basis.

1.  **Significance (Quantitative):** It is the foundation for **Lossless Ethernet** (RoCE), ensuring that RDMA traffic is not dropped due to buffer exhaustion. This maintains the high **Effective Bandwidth ($\text{BW}$)** required for gradient synchronization.
2.  **Distinction (Durable):** Unlike **Standard Flow Control** (which pauses all traffic on a link), PFC allows latency-sensitive control traffic to proceed while only pausing the congested data-intensive queues.
3.  **Common Pitfall:** A frequent misconception is that PFC "solves" congestion. In reality, it transforms packet loss into **Congestion Spreading**: a single slow receiver can trigger a backpressure cascade that freezes unrelated flows across the entire fabric, significantly increasing the **Tail Latency ($L_{\text{lat}}$)** of the cluster.

:::

The danger of PFC lies in its cascading nature. When a switch port's buffer fills, it sends a PAUSE frame upstream, which causes *that* switch's buffers to fill, which triggers *another* PAUSE frame further upstream. In theory, this backpressure should throttle the source. In practice, a single slow receiver can propagate pauses across the entire fabric in milliseconds, freezing links that have no direct relationship to the original congestion point. This cascading behavior, known as **congestion spreading** or **victim flows**, is the primary operational risk of PFC-based lossless Ethernet.

::: {#fig-congestion-cascade fig-env="figure" fig-pos="htb" fig-cap="**PFC Pause Frame Propagation**. Congestion at a tail switch (S4) triggers a backpressure cascade, freezing the entire communication path upstream." fig-alt="Diagram showing PFC pause frame propagation from congested switch S4 upstream through the network path."}
```{=latex}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, >=stealth]
  \usetikzlibrary{positioning, shapes.geometric}
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{BlueL}{HTML}{D1E6F3}
  \definecolor{RedLine}{HTML}{CB202D}
  \definecolor{RedL}{HTML}{F5D2D5}
  \definecolor{GreenLine}{HTML}{008F45}
  \definecolor{GreenL}{HTML}{D4EFDF}
  
  \tikzset{
    switch/.style={draw=black!70, fill=gray!10, line width=0.75pt, rounded corners=2pt, minimum width=1.5cm, minimum height=1cm, font=\bfseries, align=center},
    gpu/.style={draw=black!70, circle, fill=black!10, line width=0.75pt, minimum size=0.6cm, inner sep=0pt, font=\bfseries},
    dataflow/.style={->, GreenLine, line width=1.2pt},
    pauseflow/.style={->, RedLine, dashed, line width=1.0pt},
    status/.style={font=\bfseries, RedLine, anchor=north},
    time/.style={font=\footnotesize, black!70}
  }

  % Switches
  \node[switch] (swS1) {S1};
  \node[switch, right=1.5cm of swS1] (swS2) {S2};
  \node[switch, right=1.5cm of swS2] (swS3) {S3};
  \node[switch, right=1.5cm of swS3] (swS4) {S4};

  % GPUs
  \node[gpu, below=1.2cm of swS1] (gpuS1) {G};
  \node[gpu, below=1.2cm of swS2] (gpuS2) {G};
  \node[gpu, below=1.2cm of swS3] (gpuS3) {G};
  \node[gpu, below=1.2cm of swS4] (gpuS4) {G};

  % Vertical links
  \draw[line width=1.0pt, black!60] (swS1) -- (gpuS1);
  \draw[line width=1.0pt, black!60] (swS2) -- (gpuS2);
  \draw[line width=1.0pt, black!60] (swS3) -- (gpuS3);
  \draw[line width=1.0pt, black!60] (swS4) -- (gpuS4);

  % Time labels
  \node[time, below=0.1cm of swS1] {t=3 ms};
  \node[time, below=0.1cm of swS2] {t=2 ms};
  \node[time, below=0.1cm of swS3] {t=1 ms};
  \node[time, below=0.1cm of swS4] {t=0};

  % Data Flow
  \draw[dataflow] ([xshift=-1.5cm]swS1.west) -- (swS1.west);
  \draw[dataflow] (swS1.east) -- (swS2.west);
  \draw[dataflow] (swS2.east) -- (swS3.west);
  \draw[dataflow] (swS3.east) -- (swS4.west);
  \node[GreenLine, font=\small\bfseries, above=0.2cm of swS2.east] {Data Flow};

  % Pause Flow
  \draw[pauseflow] (swS4.south west) to[bend left=20] node[midway, below, font=\scriptsize] {PAUSE} (swS3.south east);
  \draw[pauseflow] (swS3.south west) to[bend left=20] node[midway, below, font=\scriptsize] {PAUSE} (swS2.south east);
  \draw[pauseflow] (swS2.south west) to[bend left=20] node[midway, below, font=\scriptsize] {PAUSE} (swS1.south east);

  % Congestion indicator
  \node[star, star points=10, fill=RedL, draw=RedLine, line width=0.75pt, text=black, minimum size=0.8cm, inner sep=0pt, above=0.2cm of swS4] (bang) {\textbf{!}};
  \node[RedLine, font=\bfseries, above=0.1cm of bang] {CONGESTION};

  % Status labels
  \node[status, below=0.2cm of gpuS1] {FROZEN};
  \node[status, below=0.2cm of gpuS2] {PAUSED};
  \node[status, below=0.2cm of gpuS3] {PAUSED};
  \node[status, below=0.2cm of gpuS4] {SOURCE};

\end{tikzpicture}
```
:::

::: {.callout-perspective title="War Story: The PFC Storm That Froze a Cluster"}

In 2022, a large RoCE-based training cluster experienced a complete fabric freeze that lasted over 40 minutes. The root cause was a single malfunctioning transceiver on one leaf switch that intermittently dropped its link speed from 400 Gbps to 100 Gbps. The reduced bandwidth caused the switch port's buffer to fill during a routine AllReduce burst, triggering PFC PAUSE frames. Because the training job spanned the entire cluster, the pauses propagated through the spine layer and back down to every leaf switch in the fabric. Within 200 milliseconds, every port in the cluster was paused. No data moved. All 4,096 GPUs sat idle.

The insidious aspect was that no alarms fired. PFC pauses are a *normal* flow-control mechanism, so monitoring systems treated them as routine. The team only noticed when training throughput dropped to zero. Diagnosis took 30 minutes because standard tools (`ping`, `iperf`) showed the network was "up" but simply not moving data. The fix was to identify and replace the faulty transceiver, but the investigation revealed a deeper problem: the cluster lacked PFC watchdog timers that would have detected the storm and disabled PFC on the offending port within seconds. After the incident, the team deployed per-port PFC pause counters with alerting thresholds and configured hardware watchdog timers to automatically disable PFC on any port sustaining pauses for more than 50 milliseconds.

This incident illustrates why InfiniBand's credit-based flow control is fundamentally more robust: credits are consumed and replenished per-hop, so a slow receiver cannot propagate backpressure beyond its immediate neighbor. PFC, by contrast, operates as a blunt per-priority mechanism that has no concept of individual flow isolation.

:::

### Proactive Congestion Control: DCQCN and HPCC {#sec-network-fabrics-congestion-control}

\index{ECN}\index{DCQCN}\index{HPCC}

To avoid the blunt instrument of PFC pauses, modern fabrics rely on proactive congestion control to modulate injection rates *before* buffers overflow. In BSP workloads, this is not merely a throughput optimization but a latency requirement: if a single packet is delayed by a congested switch queue, the entire cluster must wait for that straggler to complete the synchronization step. The tail latency of the network effectively becomes the average step time of the training job.

The first widely deployed solution, **DCQCN**[^fn-dcqcn-congestion], operates as a reactive feedback loop using Explicit Congestion Notification (ECN). When a switch's queue depth exceeds a configured threshold, it marks the ECN bit in the packet header. The receiver echoes this mark back to the sender via a Congestion Notification Packet (CNP), prompting the sender to reduce its injection rate using a multiplicative-decrease algorithm. While widely supported, DCQCN suffers from a "blind driver" problem: the sender receives a binary signal indicating congestion exists but lacks precise data on the severity. Consequently, senders must oscillate their rates to find the safe operating point, typically capping stable link utilization at 85–90%.

A more precise alternative, **HPCC**[^fn-hpcc-telemetry], addresses this opacity by using In-Network Telemetry (INT). Instead of a simple bit mark, switches append precise metadata—current queue depth, link utilization, and timestamps—to every packet header. This provides the sender with a full dashboard of the network state, allowing it to calculate the exact allowable transmission rate within a single round-trip time. By reacting to precise telemetry rather than binary signals, HPCC virtually eliminates queue buildup and stabilizes link utilization above 95%, reducing tail latency variance for bursty AllToAll traffic by up to 95% compared to DCQCN. The primary trade-off is hardware support: while DCQCN functions on standard ECN-capable Ethernet switches, HPCC requires programmable switches capable of pushing INT metadata at line rate.

[^fn-dcqcn-congestion]: **DCQCN (Data Center Quantized Congestion Notification)**: Introduced at SIGCOMM 2015 by Microsoft and Mellanox, DCQCN was the first congestion control protocol designed specifically for large-scale RDMA fabrics. Its binary ECN feedback limits stable link utilization to 85--90%, meaning a 400 Gbps link effectively delivers only 340--360 Gbps of useful gradient bandwidth under contention. \index{DCQCN!congestion control}

[^fn-hpcc-telemetry]: **HPCC (High Precision Congestion Control)**: Developed by Alibaba and presented at SIGCOMM 2019, HPCC replaced binary ECN with per-packet in-network telemetry, reducing flow completion times by up to 95% compared to DCQCN in incast-heavy workloads. The trade-off is hardware dependency: HPCC requires programmable switch ASICs capable of appending telemetry metadata at line rate, limiting deployment to clusters with newer switching silicon. \index{HPCC!congestion control}

### Adaptive Routing {#sec-network-fabrics-adaptive-routing}

\index{adaptive routing}

Static routing protocols like ECMP[^fn-ecmp-hashing] (Equal-Cost Multi-Path) distribute traffic by hashing flow headers to fixed paths. While statistically sound for millions of small web requests, this approach fails for the elephant flows typical of ML training. Consider a scenario with 4 equal-cost paths and 8 large gradient flows. A perfect distribution would place 2 flows on each link. However, static hashing frequently results in collisions where one link carries 4 flows while another sits idle. In a synchronized training step, the completion time is dictated by the overloaded link, effectively halving the network's useful bandwidth.

[^fn-ecmp-hashing]: **ECMP (Equal-Cost Multi-Path)**: ECMP selects a path by hashing on the 5-tuple (source/destination IP, source/destination port, protocol), which means the same flow always takes the same path. This determinism is a feature for packet ordering but a liability for ML: because a single AllReduce ring produces a small number of persistent flows, hash collisions are not transient statistical events but permanent hot spots that persist for the entire training run. \index{ECMP!hash collision}

**Adaptive routing** mitigates this problem by allowing switches to dynamically select the output port based on real-time queue depth rather than a static hash. The implementation is protocol-dependent: InfiniBand fabrics perform *packet-level* adaptive routing, **spraying**[^fn-packet-spraying] individual packets across all available lanes because the hardware transport guarantees in-order delivery at the destination.

[^fn-packet-spraying]: **Packet Spraying**: Unlike standard Ethernet (which hashes flows to single paths to avoid reordering), packet spraying sends individual packets of a single flow across all available paths. This maximizes bisection bandwidth for AllReduce elephant flows. InfiniBand performs this natively in hardware; the next-generation Ultra Ethernet standard (UEC) is adding this to Ethernet to eliminate the ECMP collision problem. \index{Packet Spraying!load balancing}
 Ethernet fabrics typically employ *flowlet switching*, rerouting bursts of packets only when a sufficient time gap is detected, to avoid the performance penalties associated with packet reordering.

This mechanism becomes essential for Mixture-of-Experts (MoE) models. Unlike the predictable Ring AllReduce pattern, MoE models use **AllToAll** communication, where every GPU sends data to every other GPU containing active experts. For a 175B-parameter model using expert parallelism with 64 experts, this generates a dense $64 \times 64$ traffic matrix—4,096 simultaneous flows. Under static ECMP, hash collisions are statistically guaranteed to create stragglers; adaptive routing is required to distribute this traffic evenly across the fabric's bisection bandwidth.

### The Incast Problem in ML {#sec-network-fabrics-incast}

\index{incast}

::: {.callout-definition title="Incast"}

***Incast***\index{Incast!definition} is a many-to-one traffic pattern where multiple senders simultaneously transmit data to a single receiver, overwhelming the receiver's switch port buffer.

1.  **Significance (Quantitative):** In ML fleets, incast occurs during the **Reduce Phase** of collective operations (e.g., AllReduce). It causes momentary buffer exhaustion, triggering packet drops or flow-control pauses that increase the **Tail Latency ($L_{\text{lat}}$)** of the entire cluster.
2.  **Distinction (Durable):** Unlike **General Congestion** (which occurs on shared core links), Incast is an **Endpoint Bottleneck**: it occurs even if the internal network fabric has infinite capacity.
3.  **Common Pitfall:** A frequent misconception is that incast is "rare." In reality, because ML training is **Synchronously Burstive**, incast is a routine event at the end of every layer's gradient computation, requiring proactive mitigation through jitter-aware scheduling or large switch buffers.

:::

ML training is structurally susceptible to incast because of its synchronized communication patterns. When a layer finishes backward computation, thousands of nodes simultaneously initiate AllReduce, targeting the same switch ports. In our 175B model training across 1,000 GPUs, each AllReduce involves every node injecting data simultaneously, creating a burst that can momentarily exceed the fabric's capacity at specific switch ports. Production clusters mitigate this by:
1. **Layer-staggering**: Starting AllReduce for early layers while later layers are still computing.
2. **Algorithm selection**: Using tree-based AllReduce instead of ring-based ones to reduce concurrent flow counts.
3. **QoS Priority**: Tagging gradient traffic with highest priority to ensure it is not delayed by background storage or management traffic.

Real fabrics have congestion and tail latency; the question becomes how to build end-to-end clusters that perform despite these realities.

## Level 5: Cluster Design and Case Studies {#sec-network-fabrics-cluster}

The final level of the stack is the **Cluster**, where wires, transport, topology, and congestion control combine into a coherent system. The goal of cluster design is to provide an end-to-end **Gradient Bus** that makes thousands of distributed GPUs feel like a single machine. This is where the abstraction layers collapse into concrete engineering decisions: which cables to buy, how to wire the racks, which protocol to deploy, and how to validate that the resulting fabric actually delivers the bandwidth the training job expects. Two dominant architectures have emerged in production—one built on InfiniBand and one on Ethernet—and their contrasting design philosophies illuminate the trade-offs that define modern ML infrastructure.

### The GPU-to-GPU "Gradient Bus" {#sec-network-fabrics-gradient-bus}

\index{Gradient Bus}

In a well-designed cluster, the network fabric acts not just as a pipe but as a scheduler-aware extension of the system bus. The intra-node (NVLink) bandwidth is ~`{python} nvlink_to_ib_ratio`$\times$ higher than inter-node (InfiniBand/RoCE) bandwidth, and the primary mechanism for hiding this cliff is **communication-computation overlap**. During the backward pass, gradients for the final layers are computed first. Instead of waiting for the entire backward pass to finish, the system triggers an asynchronous AllReduce for these gradients immediately while the GPUs continue computing gradients for earlier layers. If the backward pass requires 500 ms of computation and the AllReduce takes 300 ms, perfect overlap masks the entire communication cost behind computation, reducing the effective overhead to $\max(0, 300 - 500) = 0$. In practice, dependency chains and resource contention limit this efficiency to 60–80%, leaving a **last-mile problem**: the gradients for the very first layer (computed last) have no subsequent computation to hide behind, exposing their raw transfer time to the critical path.

The bandwidth staircase shown in @fig-hierarchical-staircase dictates the parallelism strategy to mitigate this cliff. **Tensor Parallelism**, requiring massive bandwidth for frequent activation exchanges, is confined to the NVLink domain within a node. **Pipeline Parallelism**, involving point-to-point transfers of activations between pipeline stages, spans the InfiniBand links between nodes. **Data Parallelism**, tolerant of lower bandwidth through gradient accumulation and overlap, stretches across the full fabric.

::: {#fig-hierarchical-staircase fig-env="figure" fig-pos="htb" fig-cap="**The Hierarchical Bandwidth Staircase**. Communication bandwidth drops by orders of magnitude as the distance from the chip increases, dictating parallelism strategies." fig-alt="Staircase or stepped diagram showing bandwidth decreasing from on-chip through NVLink, PCIe, and network levels."}
```{=latex}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \usetikzlibrary{positioning, fit}
  \tikzset{
    bar/.style 2 args={
      draw=#1, fill=#2, line width=0.75pt,
      minimum height=1cm, inner sep=0pt,
      anchor=north west
    },
    label/.style={anchor=west, font=\bfseries},
    value/.style={anchor=east, font=\bfseries},
    annot/.style={anchor=west, font=\small\itshape}
  }

  % NVLink
  \node[bar={BlueLine}{BlueL}] (nvlink) at (0, 0) [minimum width=9cm] {};
  \node[label, text=BlueLine] at ([xshift=0.2cm]nvlink.west) {NVLink (Intra-Node)};
  \node[value] at ([xshift=-0.2cm]nvlink.east) {900 GB/s};
  \node[annot] at ([xshift=0.2cm]nvlink.east) {$\leftarrow$ Tensor Parallelism};

  % PCIe
  \node[bar={GreenLine}{GreenL}, below=0.2cm of nvlink.south west] (pcie) [minimum width=5cm] {};
  \node[label, text=GreenLine] at ([xshift=0.2cm]pcie.west) {PCIe (Chip-to-Host)};
  \node[value] at ([xshift=-0.2cm]pcie.east) {64 GB/s};

  % InfiniBand
  \node[bar={OrangeLine}{OrangeL}, below=0.2cm of pcie.south west] (ib) [minimum width=4cm] {};
  \node[label, text=OrangeLine] at ([xshift=0.2cm]ib.west) {InfiniBand (Inter-Node)};
  \node[value] at ([xshift=-0.2cm]ib.east) {50 GB/s};
  \node[annot] at ([xshift=1.2cm]ib.east) {$\leftarrow$ Pipeline Parallelism};

  % Ethernet
  \node[bar={RedLine}{RedL}, below=0.2cm of ib.south west] (eth) [minimum width=2.5cm] {};
  \node[label, text=RedLine] at ([xshift=0.2cm]eth.west) {Ethernet (Data Center)};
  \node[value] at ([xshift=-0.2cm]eth.east) {25 GB/s};
  \node[annot] at ([xshift=0.7cm]eth.east) {$\leftarrow$ Data Parallelism};

  % Dashed line
  \draw[dashed, black!50, line width=1.0pt] (nvlink.south east) -- (nvlink.south east |- eth.south);

  % Distance arrow
  \draw[->, line width=1.2pt, black!70] ([xshift=-0.5cm]nvlink.west) -- ([xshift=-0.5cm]eth.west)
    node[midway, left, rotate=90, font=\small] {Increasing Distance / Latency};

\end{tikzpicture}
```
:::

### Case Study: NVIDIA DGX SuperPOD {#sec-network-fabrics-dgx-superpod}

The NVIDIA DGX SuperPOD architecture connects DGX H100 nodes using a two-tier InfiniBand network, serving as the reference implementation for the Gradient Bus concept. Each node acts as a dense compute island, with eight H100 GPUs connected via NVSwitch to provide `{python} nvlink_h100_gbs` GB/s of internal bandwidth. Externally, each GPU pairs with a ConnectX-7 NIC delivering `{python} ib_ndr_gbps` Gb/s of injection bandwidth. Across a standard scalable unit of 32 nodes (256 GPUs), this yields an aggregate injection bandwidth of 12.8 TB/s ($256 \times 50$ GB/s), ensuring the fabric can ingest gradients as fast as the accelerators produce them.

This architecture explicitly instantiates the five-level model constructed in this chapter. At Level 1, it minimizes latency by using passive copper (DAC) within the rack and active optics only for spine connections. At Level 2, it relies on InfiniBand's native credit-based flow control to guarantee a lossless medium without the fragility of Ethernet PFC. Level 3 implements a rail-optimized fat-tree: all GPU 0s across the cluster connect to the same leaf switch group, ensuring that latency-sensitive tensor parallelism operations traverse only a single switch hop. At Level 4, hardware-based adaptive routing sprays packets across all spine links to maximize bisection bandwidth for data parallelism. At Level 5, the design is modular: multiple SuperPODs connect via a core switching layer to form massive clusters. For our 175B-parameter model, the physical infrastructure would consist of approximately four SuperPOD units wired together, allowing 1,024 GPUs to function as a single synchronous instrument.

### Case Study: Meta Grand Teton {#sec-network-fabrics-grand-teton}

Meta's Grand Teton training cluster operates at a vastly different scale and philosophy, connecting over 16,000 H100 GPUs (2,000 nodes) into a single production fabric. Rather than InfiniBand, Meta employs RoCEv2 over a standard 400 GbE Ethernet fabric. The primary motivation is operational scale and supply chain resilience: by using Ethernet, Meta can source switches from multiple vendors and use the same optical infrastructure and management tooling shared by their front-end serving fleet, avoiding the operational silo of a dedicated InfiniBand island.

Making Ethernet perform like a dedicated HPC fabric at this scale requires significant engineering at Level 4. To prevent the PFC deadlocks described in @sec-network-fabrics-pfc, the fabric uses aggressive watchdog timers that disable flow control on stalled ports before congestion spreads. To handle the massive incast of 16,000-way AllReduce, Meta tunes DCQCN parameters (specifically the $K_{\min}$ and $K_{\max}$ thresholds) on a per-workload basis, ensuring that congestion notification packets are sent early enough to throttle senders before switch buffers overflow. For load balancing, the cluster moves beyond standard ECMP to use flowlet switching, where the switch hardware dynamically re-hashes flows to less-used paths during gaps in the packet stream. While this architecture achieves over 95% of line-rate bandwidth for large gradient transfers, it accepts a trade-off: small-message latency remains 3--5$\times$ higher than InfiniBand due to the deeper buffers and complex FEC required by Ethernet switches. For giant models where bandwidth dominates, this is an acceptable exchange; for latency-sensitive MoE routing, the penalty requires careful algorithmic compensation.

::: {.callout-perspective title="InfiniBand versus RoCE: The Industry Verdict"}

The coexistence of InfiniBand (NVIDIA DGX SuperPOD) and RoCE (Meta Grand Teton, Google) in production reflects a genuine trade-off rather than a clear winner. InfiniBand provides 30 to 50% lower tail latency and simpler lossless configuration. RoCE provides 20 to 40% lower switch costs and multi-vendor flexibility. For training runs where iteration time is measured in seconds, the latency difference is often absorbed into the noise. For inference serving with tight SLOs, the latency difference may matter.

The trend is toward convergence. NVIDIA's Spectrum-4 Ethernet switches incorporate InfiniBand-inspired adaptive routing and congestion control. Broadcom's Memory DCS chips add hardware support for RDMA-optimized switching. The distinction between the two ecosystems is narrowing, though it has not disappeared.

:::

### Looking Ahead: Next-Generation Interconnects {#sec-network-fabrics-next-gen}

\index{Ultra Ethernet}\index{co-packaged optics}\index{CXL}

As cluster sizes push past 100,000 accelerators, the physics tax of current interconnects—power, latency, and reliability—becomes the primary scaling bottleneck. Four emerging technologies promise to reshape the fabric.

The **Ultra Ethernet Consortium (UEC)** represents a clean-slate overhaul of Ethernet specifically for AI and HPC. Recognizing that TCP/IP is too heavy and RoCEv2's reliance on PFC is too fragile, UEC integrates HPC-native features directly into the standard: native **packet spraying** to use all paths without ECMP hashing collisions, hardware-enforced **in-order delivery** to simplify NIC design, and a new credit-based congestion control mechanism that eliminates the need for Priority Flow Control entirely. The goal is to provide the losslessness of InfiniBand with the ubiquity and multi-vendor economics of Ethernet.

The power consumption of pluggable transceivers is becoming unsustainable at scale. **Co-Packaged Optics (CPO)** addresses this by moving the optical engine from the faceplate directly into the switch ASIC package. By driving optical signals almost from the silicon itself, CPO eliminates the power-hungry electrical traces across the PCB, reducing power per port by 30–50%. For a 10,000-GPU cluster, eliminating pluggable modules could save over 100 kW of power—energy that can be redirected to computation. CPO also removes the transceiver as a discrete field-replaceable unit, eliminating a common point of mechanical failure.

The bandwidth march continues to **800G and 1.6T links** (XDR InfiniBand and 800GbE/1.6TbE). A single 1.6 Tbps port delivers the bandwidth of four 400G lanes, allowing next-generation switches to provide 51.2 Tbps of aggregate throughput. This density allows architects to flatten the topology: a cluster that previously required three tiers of switches might now be serviceable with two, halving the transceiver count and reducing tail latency by removing an entire hop of switching and FEC overhead.

Finally, **CXL (Compute Express Link)** is blurring the boundary between network and memory bus. While currently an intra-rack technology, CXL 3.0 enables **memory pooling**, allowing a GPU in one chassis to access host memory in another with load/store semantics rather than RDMA message passing. This capability suggests a future where the rigid distinction between intra-node NVLink domains and inter-node InfiniBand domains dissolves into a unified memory fabric, allowing the scheduler to compose virtual nodes of arbitrary size dynamically.

With the physical fabric established and production architectures demonstrated, a practical question remains: how do we share this expensive infrastructure across multiple teams and workloads without sacrificing the predictable performance that training demands?

## Network Virtualization {#sec-network-fabrics-virtualization}

\index{network virtualization}\index{SR-IOV}

Production ML clusters are rarely dedicated to a single training job, creating a massive economic imperative for efficient multi-tenancy. A \$300 million supercomputer that sits 30% idle because it cannot securely isolate concurrent workloads represents a \$90 million waste of capital. To recover this utility, the network must support virtualization along three orthogonal dimensions: **bandwidth partitioning** (guaranteeing minimum throughput), **latency determinism** (preventing head-of-line blocking), and **security isolation** (preventing memory snooping between tenants). Just as hypervisors decoupled the operating system from the CPU, technologies like SR-IOV and virtual lanes decouple the training job from the physical wire. For our 175B model, this means the training job can reliably consume 80% of the cluster's bisection bandwidth while a high-priority inference service and a background data preprocessing job share the remaining 20%, with the fabric enforcing hard boundaries that prevent the preprocessor's bursty traffic from stalling gradient updates.

### SR-IOV: Hardware NIC Virtualization {#sec-network-fabrics-sriov}

\index{SR-IOV}

How do cloud providers deliver bare-metal RDMA performance to virtualized GPU instances? The answer lies in **Single Root I/O Virtualization (SR-IOV)**, a standard that allows a physical NIC to present itself as multiple independent **Virtual Functions (VFs)**. Each VF has its own hardware queues, doorbell registers, and DMA mappings. By assigning a dedicated VF to each VM or container, the hardware creates a direct path for DMA operations that bypasses the host kernel and hypervisor completely. This passthrough architecture is critical for ML training because it reduces virtualization overhead to negligible levels—typically adding less than 2% latency compared to bare metal. In a Kubernetes environment, the RDMA device plugin manages this resource allocation, exposing available VFs to the scheduler so that pods can request guaranteed network access just as they request GPU cores.

However, this hardware-level isolation comes with a strict physical constraint: bandwidth partitioning. SR-IOV slices the NIC's total capacity, meaning that eight VFs sharing a `{python} ib_ndr_gbps` Gbps NIC will each be capped at 50 Gbps. For our 175B-parameter model training on a multi-tenant cluster, this partitioning is a double-edged sword. While it guarantees that a neighboring tenant cannot steal bandwidth, it also enforces a hard ceiling on peak throughput. The training job must be architected to operate within this slice, as no amount of software optimization can burst beyond the hardware-enforced limit of the VF.

### Traffic Isolation and Quality of Service {#sec-network-fabrics-qos}

\index{quality of service}

Consider a worst-case contention scenario on a shared cluster: our 175B model is midway through a latency-sensitive AllReduce operation when a neighboring job initiates a massive checkpoint save. Without strict isolation, this bursty 100 GB write—which takes just 2 seconds at full `{python} ib_ndr_gbps` Gbps line rate—could saturate the shared spine links, introducing queuing delays that increase the AllReduce time by 50% or more. To prevent this noisy-neighbor effect, modern fabrics rely on **Quality of Service (QoS)** mechanisms that enforce fairness at the packet level.

The primary tool is the **Virtual Lane (VL)** in InfiniBand (or **Traffic Class** in RoCE), which provides up to 16 independent logical channels on a single physical link. By mapping different traffic types to separate VLs, the network ensures that a saturation event in one lane does not block progress in another. Each VL maintains its own independent credit-based flow control: if the storage traffic for the checkpoint fills up its buffer, the switch pauses only that specific lane. Gradient updates, tagged with a high-priority service level, continue to flow through their reserved lane unimpeded. On the Ethernet side, **Enhanced Transmission Selection (ETS)** provides analogous bandwidth guarantees per traffic class, while advanced switch ASICs can partition their forwarding tables and buffer pools into isolated **network slices**, ensuring that congestion in one tenant's slice cannot trigger PFC pauses in another's.

Virtualization solves the sharing problem but makes performance diagnosis harder. When a training job slows down on a multi-tenant cluster, the cause could be a physical link degradation, a noisy neighbor exceeding its bandwidth allocation, or a misconfigured QoS policy. Systematic monitoring is essential to distinguish these cases.

## Monitoring and Debugging {#sec-network-fabrics-monitoring}

\index{network monitoring}

Network performance problems in ML clusters are insidious because they manifest as **silent waste** rather than explicit failures. A degraded transceiver causing a 10% reduction in effective bandwidth might slow each training iteration by only 2–3%—a drift easily masked by the natural variance of checkpointing or data loading. Over a 30-day training run on 1,000 GPUs, this invisible drag accumulates to over 72,000 wasted GPU-hours, burning nearly \$200,000 in cloud costs without triggering a single alarm. Traditional IT monitoring tools like SNMP or `ping` are insufficient here; they measure connectivity, not the sustained throughput required by RDMA. Effective observability requires a three-layer approach: **physical monitoring** (FEC errors, signal attenuation), **transport monitoring** (PFC pause frames, retransmission rates), and **application monitoring** (NCCL algorithmic bandwidth). Only by correlating signals across these layers can operators detect that a "slow training run" is actually caused by a single degraded cable in one rack.

### Link-Level Telemetry {#sec-network-fabrics-link-telemetry}

The physical layer of a high-performance network is surprisingly fragile, making hardware counters the ground truth for fabric reliability. Every RDMA NIC and switch maintains these counters, accessible via the `perfquery` utility. **PortXmitData** and **PortRcvData** provide the raw byte counts used to compute real-time link utilization, while **PortXmitDiscards** tracks packets dropped by the switch—in a properly configured lossless fabric, discards should be exactly zero. Physical signal integrity is tracked via the **SymbolErrorCounter**, which increments on bit-level errors caused by loose cables or dirty optics, and the **LinkDownedCounter**, which logs link flaps often triggered by intermittent hardware faults or overheating transceivers.

These metrics are typically polled by a centralized monitoring system (e.g., Prometheus with Grafana dashboards) at 10–30 second intervals to catch **silent degradation**. A common failure mode involves a QSFP56 cable negotiating at HDR speed (`{python} ib_hdr_gbps` Gbps) instead of NDR (`{python} ib_ndr_gbps` Gbps), or silently dropping from 4 lanes to 2 lanes due to a single bad connector pin. The link remains "up" and functional, but its bandwidth is halved. In a 1,000-GPU cluster with over 3,000 links, even a 0.1% component failure rate guarantees that approximately 3 links are degraded at any given moment. Because distributed training algorithms like Ring AllReduce are synchronous, a single degraded link throttles the entire job to the speed of that straggler, transforming a minor hardware fault into a massive waste of idle compute cycles.

### Bandwidth and Latency Validation {#sec-network-fabrics-bw-testing}

While counters track errors, achievable performance must be empirically validated using the `perftest` suite. A healthy NDR InfiniBand link typically delivers 48 to 49 GB/s of useful payload (out of the raw `{python} ib_ndr_gbs` GB/s rate) after encoding overhead. Operators rely on periodic health checks, often running `ib_write_bw` between all node pairs to generate an **all-pairs bandwidth matrix**. This heatmap immediately visualizes cold spots in the fabric where specific spine switches or cable bundles are underperforming, allowing for targeted maintenance before jobs are scheduled.

For latency-sensitive synchronization, `ib_write_lat` measures round-trip times for small RDMA writes. Baseline NDR latency should remain below 2 $\mu$s for directly connected nodes. Latencies exceeding 5 $\mu$s suggest switch-buffer congestion or routing imbalances, while values spiking above 100 $\mu$s usually indicate the RDMA path has fallen back to TCP/IP emulation—a catastrophic configuration error. Before launching a massive training job, standard procedure involves running application-level validation via `nccl-tests`, which verifies that the fabric can sustain the expected AllReduce bandwidth across the specific collective topology (ring or tree) used by the workload. This ensures that the physical network reality matches the theoretical design before expensive compute resources are allocated.

### Systematic Debugging Workflow {#sec-network-fabrics-debug-workflow}

When a training job reports lower-than-expected throughput, the following diagnostic sequence isolates the cause:

1. **Check GPU utilization**: Rule out compute bottlenecks using `dcgmi` or `nvidia-smi`. If SM utilization is 100%, the network is not the bottleneck.
2. **Inspect NCCL logs**: Set `NCCL_DEBUG=INFO` to reveal which network transport was selected, the detected bandwidth between nodes, and any fallbacks to slower protocols.
3. **Run point-to-point tests**: Use `ib_write_bw` between specific nodes in the job. A single degraded link can bottleneck the entire ring in a Ring AllReduce.
4. **Check PFC/ECN counters**: Inspect switch counters along the path. Sustained PFC activity indicates persistent congestion that should be investigated at the scheduler or routing level.
5. **Validate Physical Layer**: Check symbol errors and CRC counts to identify failing transceivers or cables.

This workflow progresses from application-level symptoms to physical-layer causes, preventing weeks of guesswork when cluster performance drifts.

::: {.callout-checkpoint title="Diagnosing a Training Slowdown"}

**Scenario:** Your 175B model training job has been running for 3 days on 512 GPUs. You notice that the iteration time has gradually increased from 4.2 seconds to 4.8 seconds (a 14% slowdown). The GPU utilization reported by `nvidia-smi` has dropped from 92% to 85%.

1. What is the first diagnostic step you would take, and why? The drop in GPU utilization combined with increased iteration time strongly suggests a communication bottleneck rather than a compute problem. If the GPUs were throttling due to heat or experiencing ECC errors, utilization would likely remain high or the job would crash. What tool would you use to confirm?
2. You run `ib_write_bw` between all node pairs and find that one pair achieves only 24 GB/s instead of the expected 48 GB/s. What physical-layer causes could explain a 50% bandwidth drop? How would you verify using switch counters?
3. If the degraded link is in the Ring AllReduce path, estimate the impact on overall AllReduce time. Would switching to a Tree AllReduce algorithm help, and why or why not?

:::

Monitoring reveals problems, but many of the most costly mistakes stem not from hardware failures but from flawed assumptions about how networks behave at scale. The next section catalogs the most common of these misconceptions.

## Fallacies and Pitfalls {#sec-network-fabrics-fallacies}

Designing and operating high-performance fabrics for ML requires unlearning assumptions from traditional datacenter networking. The following fallacies and pitfalls capture the most common errors that stall training and degrade cluster productivity.

**Fallacy:** *More bandwidth always means faster training.*

Engineers assume upgrading from HDR (`{python} ib_hdr_gbps` Gbps) to NDR (`{python} ib_ndr_gbps` Gbps) will yield proportional gains, but the $\alpha$-$\beta$ model (@eq-alpha-beta) reveals this is only true in the bandwidth-dominated regime. For small models or the small-message phases of pipeline parallelism, the latency term $\alpha$—dominated by switch hops and FEC (~1 $\mu$s)—dictates performance. If a workload is latency-bound, a 2$\times$ bandwidth increase might improve end-to-end throughput by less than 5%, while adding 40% to the power and transceiver cost.

```{python}
#| echo: false
#| label: bandwidth-fallacy
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BANDWIDTH FALLACY
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Fallacy "More bandwidth always means faster training"
# │
# │ Goal: Show diminishing returns of BW in the latency-dominated regime.
# │ Show: That a 2x BW increase yields minimal gain for small messages.
# │ How: Calculate T(n) for HDR vs NDR for 10 KB message.
# │
# │ Imports: mlsys.formatting (fmt)
# │ Exports: hdr_bw_gbs, ndr_bw_gbs, t_hdr_us, t_ndr_us, speedup_pct
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.formatting import fmt, check

# ┌── LEGO ───────────────────────────────────────────────
class BandwidthFallacy:
    """
    Scenario: Latency comparison for small messages across HDR vs NDR.
    Illustrates why doubling bandwidth does not halve latency for small packets.
    """

    # ┌── 1. LOAD (Constants) ───────────────────────────────────────────────
    alpha_us = 1.5
    hdr_gbps = 200
    ndr_gbps = 400
    msg_size_kb = 10

    # ┌── 2. EXECUTE (The Compute) ─────────────────────────────────────────
    # Step 1: Link rates in GB/s
    hdr_gbs = hdr_gbps / 8
    ndr_gbs = ndr_gbps / 8

    # Step 2: T = alpha + m/beta
    t_hdr = alpha_us + (msg_size_kb / (hdr_gbs * 1024 * 1024 / 1000)) # Simplified us
    # Step 3: Actually m/beta in us: (size_bytes) / (GB/s)
    t_hdr = alpha_us + (msg_size_kb * 1024) / (hdr_gbs * 1e3)
    t_ndr = alpha_us + (msg_size_kb * 1024) / (ndr_gbs * 1e3)

    speedup_pct = (1 - (t_ndr / t_hdr)) * 100

    # ┌── 3. GUARD (Invariants) ───────────────────────────────────────────
    check(speedup_pct < 25, f"Speedup ({speedup_pct:.1f}%) too high; fallacy argument weakened.")

    # ┌── 4. OUTPUT (Formatting) ──────────────────────────────────────────────
    hdr_bw_str = f"{hdr_gbps}"
    ndr_bw_str = f"{ndr_gbps}"
    t_hdr_str = f"{t_hdr:.2f}"
    t_ndr_str = f"{t_ndr:.2f}"
    speed_pct_str = f"{speedup_pct:.1f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
hdr_bw_gbs = BandwidthFallacy.hdr_bw_str
ndr_bw_gbs = BandwidthFallacy.ndr_bw_str
t_hdr_us = BandwidthFallacy.t_hdr_str
t_ndr_us = BandwidthFallacy.t_ndr_str
speedup_pct = BandwidthFallacy.speed_pct_str
```

Consider a 10 KB message (typical for control synchronization). On `{python} hdr_bw_gbs` Gbps InfiniBand, it takes `{python} t_hdr_us` $\mu$s. Upgrading to `{python} ndr_bw_gbs` Gbps reduces this to `{python} t_ndr_us` $\mu$s—a measly `{python} speedup_pct`\% improvement despite doubling the link rate.

**Pitfall:** *Assuming lossless Ethernet is as reliable as InfiniBand.*

RoCE over Ethernet can achieve throughput comparable to InfiniBand for large transfers, but the "lossless" property is an approximation maintained by PFC (@sec-network-fabrics-pfc). A misconfigured switch or a firmware bug can trigger a **PFC storm**, where PAUSE frames propagate in a loop, freezing the entire fabric. InfiniBand's credit-based flow control is inherently immune to such cascades because it operates on per-hop buffer availability rather than reactive signals. Teams deploying RoCE must invest substantially more engineering effort in fabric testing and monitoring to avoid multi-day outages caused by "lossless" deadlocks.

**Fallacy:** *Network oversubscription is acceptable if "most" traffic is local.*

In general-purpose clouds, 4:1 or 8:1 oversubscription at the spine is common because traffic is stochastic. However, ML training is **Bulk Synchronous**. When an AllReduce starts, *every* node attempts to inject full bandwidth simultaneously. As shown in the Bisection Bottleneck analysis (@nb-bisection-bottleneck), even a 4:1 oversubscription slows the entire synchronization by 4$\times$. For a \$300M cluster where sync accounts for 30% of time, this "cost optimization" wastes over **\$`{python} waste_millions` million** in idle GPU cycles.

**Pitfall:** *Testing network performance with `iperf` instead of RDMA tools.*

Standard benchmarks like `iperf` measure kernel-based TCP/IP performance. Because TCP requires CPU-managed buffer copies and context switches, it often caps at 20–40 Gbps regardless of the wire speed. ML training uses **RDMA**, which bypasses the kernel entirely. A link that appears "broken" at 30 Gbps in `iperf` might be perfectly healthy and deliver 390 Gbps in `ib_write_bw`. Validation protocols must use RDMA-specific tools from the `perftest` suite to match the workload's data path.

**Fallacy:** *Adaptive routing eliminates the need for topology-aware placement.*

Adaptive routing distributes traffic across available paths, but it cannot create bandwidth that does not exist. If a scheduler places a 1,024-GPU job across two oversubscribed spine groups, adaptive routing will balance the traffic, but it will still be throttled by the group-to-group links. Topology-aware placement (@sec-fleet-orchestration) and adaptive routing are complementary: the former ensures bandwidth exists, while the latter ensures it is used efficiently.

**Pitfall:** *Neglecting PFC and ECN counter monitoring in production.*

Network performance issues in ML clusters manifest as subtle training slowdowns rather than errors. A 10% reduction in throughput due to intermittent congestion can waste thousands of GPU-hours before being noticed. Operators must alert on **PortXmitDiscards** and **PFC Pause** frame rates. A gradual increase in these counters is often the "canary in the coal mine" for a failing transceiver or a routing imbalance that will eventually lead to a job failure.

## Summary {#sec-network-fabrics-summary}

We have structured our analysis of high-bandwidth fabrics around a **Five-Level Model**, ascending from the physics of signal transmission to the architecture of warehouse-scale clusters. This framework reveals that network performance is not merely about link speed; it is the product of interactions between physical reach, transport protocols, topology, and congestion control. We began with Level 1 (Wire), establishing that PAM4 encoding and FEC impose irreducible latency floors that constrain cluster diameter. At Level 2 (Transport), we contrasted InfiniBand's native credit-based flow control with RoCE's reliance on PFC, using the $\alpha$-$\beta$ model to quantify the bandwidth-latency trade-offs inherent in distributed collectives.

Level 3 (Topology) demonstrated how non-blocking fat-trees and rail-optimized designs provide the structural bisection bandwidth required by global AllReduce patterns. However, Level 4 (Behavior) showed that structure alone is insufficient: in the synchronous world of BSP training, tail latency is the dominant constraint, necessitating proactive congestion control mechanisms like DCQCN and HPCC to prevent incast-induced stalling. Level 5 (Cluster Design) integrated these layers into production architectures like the NVIDIA SuperPOD and Meta Grand Teton, illustrating how virtualization and multi-tenancy allow these massive instruments to be shared safely.

Looking forward, the physical limits of copper and the operational complexity of Ethernet are driving a new generation of interconnects. Technologies like the Ultra Ethernet Consortium (UEC) standards, Co-Packaged Optics (CPO), and CXL memory pooling promise to flatten topologies and reduce the power tax of moving data. Yet, as our monitoring workflow discussion emphasized, no technology eliminates the need for rigorous observability. Whether debugging a single degraded transceiver or optimizing a multi-tenant scheduler, the ability to correlate physical counters with application-level throughput remains the ultimate safeguard against silent waste in the machine learning fleet.

::: {.callout-takeaways title="The Network Is the Bottleneck"}

* **Network as Computer**: At scale, the interconnect is a primary determinant of system performance. Bandwidth and topology constrain training speed as fundamentally as accelerator FLOPS.
* **$\alpha$-$\beta$ Framework**: Guides topology selection and collective algorithm design by separating latency-dominated and bandwidth-dominated regimes.
* **Lossless is Non-Negotiable**: RDMA requires a lossless fabric. InfiniBand provides this natively; Ethernet must approximate it via PFC/ECN, adding operational complexity and tail-latency risk.
* **Topology Choice is Workload-Dependent**: Fat-trees provide flexibility; rail-optimized networks minimize latency for tensor parallelism; dragonflies reduce cabling costs for massive scale.
* **Monitor or Waste**: Systematic telemetry (PFC counters, bandwidth baselines) is the only way to detect subtle network-induced training slowdowns before they waste significant GPU-hours.

:::

The practical value of this layered understanding is diagnostic precision. When a distributed training job underperforms, the complaint is invariably "the network is slow," but slowness has many causes: a failing transceiver degrading a single link, a PFC storm cascading across a subnet, a topology bottleneck starving one communication pattern while adequately serving another. Engineers who understand the Five-Level Model can isolate the layer at fault, correlate physical counters with transport behavior, and distinguish a topology limitation from a congestion control misconfiguration. This capacity to reason across abstraction layers, from SerDes signal integrity to cluster-wide bisection bandwidth, is what separates routine troubleshooting from genuine systems engineering.

Equally important, the $\alpha$-$\beta$ cost model provides a quantitative vocabulary for making architectural decisions before hardware is purchased and racks are wired. Choosing between InfiniBand and RoCE, between fat-tree and rail-optimized topologies, between 400G and 800G link speeds are all decisions with multi-million-dollar consequences that hinge on the interaction between message size distributions, collective algorithms, and physical link characteristics. The framework developed in this chapter equips practitioners to evaluate these trade-offs with analytical rigor rather than vendor benchmarks alone.

::: {.callout-chapter-connection title="From Wires to Data Pipelines"}

The network fabric now binds compute nodes into a fleet, and every byte of gradient data and activation tensor flows through it. The fleet, however, also needs to move massive datasets and enormous checkpoints. @sec-data-storage examines the parallel storage systems and data-loading architectures that keep the fleet supplied with data.

:::

[^fn-allreduce-forward]: **AllReduce**: A collective operation that sums data from all nodes and distributes the result back to all nodes. @sec-collective-communication develops the mathematical cost models for ring and tree-based AllReduce implementations. \index{AllReduce!forward reference}

[^fn-collectives-forward]: **Collective Primitives**: Higher-level communication patterns involving groups of nodes. While @sec-collective-communication derives the algorithms for **AllGather**, **ReduceScatter**, and **AllToAll**, this chapter addresses the physical fabric requirements (bisection bandwidth, switch radix) that enable them at scale. \index{Collective Primitives!forward reference}

[^fn-moe-forward]: **Mixture-of-Experts (MoE)**: An architecture that activates only a subset of model "experts" per input, necessitating AllToAll communication. @sec-inference-scale and @sec-performance-engineering examine how MoE decouples model capacity from per-token compute cost ($O$). \index{MoE!forward reference}

::: { .quiz-end }
:::
