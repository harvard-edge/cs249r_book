---
bibliography: hw_acceleration.bib
---

# Scaling AI Hardware {#sec-scaling-ai-hardware}

## Purpose {.unnumbered}

_Why does scaling AI hardware beyond single accelerators require fundamentally different architectural approaches, and what engineering principles determine success at datacenter scale?_

Modern AI workloads routinely exceed what any single accelerator can deliver. Training GPT-3 on a single H100 would require over 10 years of continuous computation, consuming 314 sextillion floating-point operations. Real-time inference serving for global applications demands throughput beyond any individual chip's capacity. These computational requirements necessitate a fundamental shift from single-chip optimization to distributed acceleration strategies, where the engineering challenges transform from maximizing local resource utilization to orchestrating computation across thousands of interconnected processors.

This chapter examines the hardware architectures that enable AI systems to scale from individual accelerators to warehouse-scale computing. We analyze how multi-chip systems introduce qualitatively different constraints around communication overhead, memory coherence, and fault tolerance that require new abstractions and optimization techniques beyond those developed for single-accelerator deployments. The progression from chiplet-based integration through multi-GPU systems to TPU Pods and wafer-scale computing reveals fundamental trade-offs between computational density, interconnect bandwidth, and system complexity that determine the practical limits of AI acceleration at scale.

::: {.callout-tip title="Learning Objectives"}

- Analyze communication overhead constraints in multi-chip AI systems using Amdahl's Law to predict scaling efficiency limits

- Compare interconnect technologies (NVLink, NVSwitch, optical interconnects) and evaluate their suitability for different distributed AI workloads

- Design workload partitioning strategies for multi-GPU and TPU Pod architectures that balance computation distribution against communication costs

- Evaluate trade-offs between chiplet-based integration, multi-GPU systems, TPU Pods, and wafer-scale computing for different AI deployment scenarios

- Apply distributed memory allocation and execution scheduling techniques to minimize inter-chip communication overhead

- Assess fault tolerance requirements and redundancy mechanisms for large-scale AI acceleration systems

:::

## Multi-Chip AI Acceleration {#sec-scaling-ai-hardware-multichip-acceleration}

The transition from single-chip to multi-chip architectures represents more than simple replication. It requires rethinking how computations distribute across processors, how data flows between chips, and how systems maintain coherence at scale. Where single-chip optimization focuses on maximizing utilization within fixed resources, multi-chip systems must balance computational distribution against communication overhead, memory coherence costs, and synchronization complexity. These challenges fundamentally transform the optimization landscape, requiring new abstractions and techniques beyond those developed for individual accelerators.

Modern AI workloads increasingly demand computational resources that exceed the capabilities of single-chip accelerators. This section examines how AI systems scale from individual processors to multi-chip architectures, analyzing the motivation behind different scaling approaches and their impact on system design. These scaling considerations connect directly to the distributed training strategies covered in @sec-distributed-training and build upon the single-accelerator foundations established in earlier chapters on hardware acceleration.

The scaling of AI systems follows a natural progression: integration within a single package through chiplet architectures, multi-GPU configurations within a server, distributed accelerator pods, and wafer-scale integration. Each approach presents unique trade-offs between computational density, communication overhead, and system complexity. Chiplet architectures maintain high-speed interconnects within a package, while distributed systems sacrifice communication latency for massive parallelism.

Understanding these scaling strategies is essential for several reasons. First, it provides insight into how different hardware architectures address the growing computational demands of AI workloads. Second, it reveals the fundamental challenges that arise when extending beyond single-chip execution, such as managing inter-chip communication and coordinating distributed computation. Finally, it establishes the foundation for subsequent discussions on how mapping strategies, compilation techniques, and runtime systems evolve to support efficient execution at scale.

### Chiplet-Based Architectures {#sec-scaling-ai-hardware-chiplet-architectures}

Chiplet architectures achieve scaling by partitioning large designs into smaller, modular dies that are interconnected within a single package. Small, specialized semiconductor dies connect together within a single package to create larger, more complex processors. AMD's EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies, with compute chiplets in 7 nm paired with I/O chiplets in 14 nm, optimizing each function independently.

Modern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning. AMD's Instinct MI300 exemplifies this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.

However, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence, and thermal management become critical factors as more chiplets are integrated. Unlike traditional multi-chip systems, chiplet-based designs must carefully balance latency-sensitive workloads across multiple dies without introducing excessive bottlenecks. Memory coherence presents particular complexity: ensuring all processors in a system see the same consistent view of shared memory when multiple cores or chips access the same data. Traditional cache coherence protocols like MESI add 10-50 ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive, so most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.

### Multi-GPU Systems {#sec-scaling-ai-hardware-multi-gpu-systems}

Beyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.

A common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network). These parallelization strategies are explored in depth in @sec-distributed-training.

NVSwitch interconnects enable high-speed communication between GPUs, reducing bottlenecks in distributed training. However, scaling up the number of GPUs introduces fundamental distributed coordination challenges that become the dominant performance constraint. The arithmetic intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient synchronization across GPUs, where AllReduce operations must aggregate 175 billion parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth, but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds available capacity.

The coordination complexity compounds exponentially. While two GPUs require a single communication channel, eight GPUs need 28 interconnect paths, and fault tolerance requirements mandate redundant communication patterns. Memory consistency protocols further complicate coordination as different GPUs may observe weight updates at different times, requiring sophisticated synchronization primitives that can add 10-50 microseconds latency per training step. These seemingly small delays aggregate to hours of training time across million-iteration runs.

#### Communication Overhead and Amdahl's Law {#sec-scaling-ai-hardware-amdahl-analysis}

The fundamental limitation of distributed AI training stems from Amdahl's Law, which quantifies how communication overhead constrains parallel speedup regardless of available compute power. For distributed neural network training, communication overhead during gradient synchronization creates a sequential bottleneck that limits scalability even with infinite parallelism.

The maximum speedup achievable with distributed training is bound by Amdahl's Law:
$$
\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}
$$
where $P$ is the fraction of work that can be parallelized and $N$ is the number of processors. However, for AI training, communication overhead introduces additional sequential time:
$$
\text{Speedup}_{\text{AI}} = \frac{1}{(1-P) + \frac{P}{N} + \frac{C}{N}}
$$
where $C$ represents the communication overhead fraction.

Consider training a 175 B parameter model with 1000 H100 GPUs as a concrete example:

- **Computation time per iteration**: 100 ms of forward/backward passes
- **Communication time**: AllReduce of 175 B parameters (700 GB in FP32) across 1000 GPUs
- **Available bandwidth**: 600 GB/s per NVSwitch link
- **Communication overhead**: $\frac{700\text{GB}}{600\text{GB/s}} \times \log_2(1000) \approx 11.6\text{ms}$

Even if only 5% of training requires communication (P = 0.95), the maximum speedup is:
$$
\text{Speedup} = \frac{1}{0.05 + \frac{0.95}{1000} + \frac{0.116}{100}} \approx 8.3\text{x}
$$

This demonstrates why adding more GPUs beyond approximately 100 provides diminishing returns for large model training.

Communication requirements scale superlinearly with model size and linearly with the number of parameters. Modern transformer models require gradient synchronization across all parameters during each training step:

- **GPT-3 (175 B parameters)**: 700 GB gradient exchange per step
- **GPT-4 (estimated 1.8 T parameters)**: approximately 7 TB gradient exchange per step
- **Future 10 T parameter models**: approximately 40 TB gradient exchange per step

Even with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization for 10 T parameter models would require 22+ seconds per training step, making distributed training impractical without algorithmic innovations like gradient compression or asynchronous updates.

Multi-GPU systems face additional bottlenecks from memory bandwidth competition. When 8 H100 GPUs simultaneously access HBM during gradient computation, the effective memory bandwidth per GPU drops from 3.35 TB/s to approximately 2.1 TB/s due to memory controller contention and NUMA effects. This 37% reduction in memory performance compounds communication overhead, further limiting scalability.

Understanding Amdahl's Law guides optimization strategies:

1. **Gradient Compression**: Reduce communication volume by 10-100x through sparsification and quantization
2. **Pipeline Parallelism**: Overlap communication with computation to hide gradient synchronization latency
3. **Model Parallelism**: Partition models across devices to reduce gradient synchronization requirements
4. **Asynchronous Updates**: Relax consistency requirements to eliminate synchronization barriers

These techniques modify the effective value of $P$ and $C$ in Amdahl's equation, enabling better scaling behavior at the cost of algorithmic complexity.

### TPU Pods {#sec-scaling-ai-hardware-tpu-pods}

As models and datasets continue to expand, training and inference workloads must extend beyond single-server configurations. This scaling requirement has led to the development of sophisticated distributed systems where multiple accelerators communicate across networks. Google's TPU Pods represent a pioneering approach to this challenge, interconnecting hundreds of TPUs to function as a unified system.

The architectural design of TPU Pods differs fundamentally from traditional multi-GPU systems. While multi-GPU configurations typically rely on NVLink or PCIe connections within a single machine, TPU Pods employ high-bandwidth optical links to interconnect accelerators at data center scale. This design implements a 2D torus interconnect topology, enabling efficient data exchange between accelerators while minimizing communication bottlenecks as workloads scale across nodes.

The effectiveness of this architecture is demonstrated in its performance scaling capabilities. TPU Pod performance exhibits near-linear scaling when running ResNet-50, from quarter-pod to full-pod configurations. The system achieves a remarkable 33.0x speedup when scaled to 1024 chips compared to a 16-TPU baseline. This scaling efficiency is particularly noteworthy in larger configurations, where performance continues to scale strongly even as the system expands from 128 to 1024 chips.

However, distributing AI workloads across an entire data center introduces distributed coordination challenges that fundamentally differ from single-node systems. The 2D torus interconnect, while providing high bisection bandwidth, creates communication bottlenecks when training large transformer models that require AllReduce operations across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through the torus network, with worst-case communication requiring 32 hops between distant TPUs, creating latency penalties that compound with model size.

The distributed memory architecture exacerbates coordination complexity. Unlike multi-GPU systems with shared host memory, each TPU node maintains independent memory spaces, forcing explicit data marshaling and synchronization protocols. Network partition tolerance becomes critical as optical link failures can split the pod into disconnected islands, requiring sophisticated consensus algorithms to maintain training consistency.

The energy cost of coordination also scales dramatically: moving data across the pod's optical interconnect consumes 1000x more energy than on-chip communication within individual TPUs, transforming distributed training into a careful balance between computation parallelism and communication efficiency where AllReduce bandwidth, not compute capacity, determines overall training throughput.

### Wafer-Scale AI {#sec-scaling-ai-hardware-wafer-scale}

At the frontier of AI scaling, wafer-scale integration represents a paradigm shift that abandons traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication.

Wafer-scale integration uses an entire 300 mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores, 125x more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23 kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.

Cerebras' Wafer-Scale Engine (WSE) processors break away from the historical transistor scaling trends of CPUs, GPUs, and TPUs. While these architectures have steadily increased transistor counts along an exponential trajectory, WSE introduces an entirely new scaling paradigm, integrating trillions of transistors onto a single wafer, far surpassing even the most advanced GPUs and TPUs.

The fundamental advantage of wafer-scale AI is its ultra-fast on-die communication. Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries between separate devices, wafer-scale AI enables near-instantaneous data transfer across its vast compute array. This architecture drastically reduces communication latency, achieving performance levels that remain out of reach for conventional multi-chip systems.

Achieving this level of integration introduces substantial engineering challenges. Thermal dissipation, fault tolerance, and manufacturing yield become major constraints when fabricating a processor of this scale. Energy consumption and resource utilization represent significant sustainability considerations for these large-scale systems. Unlike distributed TPU systems, which mitigate failures by dynamically re-routing workloads, wafer-scale AI must incorporate built-in redundancy mechanisms to tolerate localized defects in the silicon. Successfully addressing these challenges is essential to realizing the full potential of wafer-scale computing as the next frontier in AI acceleration.

### Scaling Trajectory and Trade-offs {#sec-scaling-ai-hardware-trajectory}

The progressive scaling of AI acceleration introduces new challenges at each step, from single-chip processors to increasingly complex architectures. These challenges relate to data movement, memory access, interconnect efficiency, and workload distribution. @tbl-scaling-trajectory summarizes these trade-offs across different scaling approaches.

+----------------------+-------------------------------------+-----------------------------------------------------+
| **Scaling Approach** | **Key Feature**                     | **Challenges**                                      |
+:=====================+:====================================+:====================================================+
| **Chiplets**         | Modular scaling within a package    | Inter-chiplet latency, memory coherence             |
+----------------------+-------------------------------------+-----------------------------------------------------+
| **Multi-GPU**        | External GPU interconnects (NVLink) | Synchronization overhead, communication bottlenecks |
+----------------------+-------------------------------------+-----------------------------------------------------+
| **TPU Pods**         | Distributed accelerator clusters    | Interconnect congestion, workload partitioning      |
+----------------------+-------------------------------------+-----------------------------------------------------+
| **Wafer-Scale AI**   | Entire wafer as a single processor  | Thermal dissipation, fault tolerance                |
+----------------------+-------------------------------------+-----------------------------------------------------+

: **AI Acceleration Scaling Trade-offs**: Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution. {#tbl-scaling-trajectory}

While chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.

## Multi-Chip Execution Strategies {#sec-scaling-ai-hardware-execution-strategies}

As AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. In a single accelerator, execution is primarily optimized for locality, ensuring that computations are mapped efficiently to available processing elements while minimizing memory access latency. However, as AI systems extend beyond a single chip, the scope of these optimizations expands significantly. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.

### Multi-chip Execution Mapping {#sec-scaling-ai-hardware-execution-mapping}

In single-chip AI accelerators, computation placement is concerned with mapping workloads to processing elements, vector units, and tensor cores. Mapping strategies aim to maximize data locality, ensuring that computations access nearby memory to reduce costly data movement.

As AI systems scale to multi-chip execution, computation placement must consider several critical factors. Workloads need to be partitioned across multiple accelerators, which requires explicit coordination of execution order and dependencies. This division is essential due to the inherent latency associated with cross-chip communication, which contrasts sharply with single-chip systems that benefit from shared on-chip memory. Accordingly, computation scheduling must be interconnect-aware to manage these delays effectively. Additionally, achieving load balancing across accelerators is vital; an uneven distribution of tasks can result in some accelerators remaining underutilized while others operate at full capacity, ultimately hindering overall system performance.

For example, in multi-GPU training, computation mapping must ensure that each GPU has a balanced portion of the workload while minimizing expensive cross-GPU communication. Similarly, in TPU Pods, mapping strategies must align with the torus interconnect topology, ensuring that computation is placed to minimize long-distance data transfers.

Thus, while computation placement in single-chip systems is a local optimization problem, in multi-chip architectures, it becomes a global optimization challenge where execution efficiency depends on minimizing inter-chip communication and balancing workload distribution.

### Distributed Memory Allocation {#sec-scaling-ai-hardware-distributed-memory}

Memory allocation strategies in single-chip AI accelerators are designed to minimize off-chip memory accesses by using on-chip caches, SRAM, and HBM. Techniques such as tiling, data reuse, and kernel fusion ensure that computations make efficient use of fast local memory.

In multi-chip AI systems, each accelerator manages its own local memory, which necessitates the explicit allocation of model parameters, activations, and intermediate data across the devices. Unlike single-chip execution where data is fetched once and reused, multi-chip setups require deliberate strategies to minimize redundant data transfers, as data must be communicated between accelerators. Additionally, when overlapping data is processed by multiple accelerators, the synchronization of shared data can introduce significant overhead that must be carefully managed to ensure efficient execution.

For instance, in multi-GPU deep learning, gradient synchronization across GPUs is a memory-intensive operation that must be optimized to avoid network congestion. In wafer-scale AI, memory allocation must account for fault tolerance and redundancy mechanisms, ensuring that defective regions of the wafer do not disrupt execution.

Thus, while memory allocation in single-chip accelerators focuses on local cache efficiency, in multi-chip architectures, it must be explicitly coordinated across accelerators to balance memory bandwidth, minimize redundant transfers, and reduce synchronization overhead.

### Data Movement Optimization {#sec-scaling-ai-hardware-data-movement}

In single-chip AI accelerators, data movement optimization focuses on reducing DRAM accesses by leveraging caching, prefetching, and tiling. However, in multi-chip AI systems, data must frequently move between accelerators, making inter-chip data transfer the primary bottleneck rather than memory hierarchy latency.

Techniques for optimizing multi-chip data movement include overlapping computation and communication so that accelerators process data while simultaneously sending and receiving, reducing idle time. Locality-aware scheduling ensures that computations are placed on accelerators that already hold the required data, avoiding unnecessary transfers. Additionally, efficient gradient aggregation techniques, such as ring AllReduce and hierarchical AllReduce, minimize communication overhead during distributed training.

For example, in TPU Pods, systolic execution models ensure that data moves in structured patterns, reducing unnecessary off-chip transfers. In multi-GPU inference, techniques like asynchronous data fetching and overlapping computation with communication help mitigate inter-chip latency.

Thus, while data movement optimization in single-chip systems focuses on cache locality and tiling, in multi-chip architectures, the primary challenge is reducing inter-chip communication overhead to maximize efficiency.

### Compiler and Runtime Adaptations {#sec-scaling-ai-hardware-compiler-runtime}

As AI acceleration extends beyond a single chip, compilers and runtimes must adapt to manage computation placement, memory organization, and execution scheduling across multiple accelerators. The fundamental principles of locality, parallelism, and efficient scheduling remain essential, but their implementation requires new strategies for distributed execution.

Interconnect-aware workload partitioning enables compilers to distribute computations strategically across accelerators based on communication cost. For instance, in multi-GPU training, compilers optimize placement to minimize NVLink or PCIe traffic, whereas TPU Pods leverage the torus interconnect topology to enhance data exchanges.

Execution engines are redesigned for distributed coordination. Multi-chip runtimes extend single-chip execution models to handle dynamic workload distribution across accelerators, ensuring efficient utilization even as system load varies. Scheduling frameworks implement distributed execution orchestration strategies that coordinate both computation and memory access seamlessly across multiple accelerators, enabling efficient utilization of hardware resources and minimizing bottlenecks associated with data transfers.

Adaptive execution models play an important role in contemporary multi-chip architectures. These models dynamically adjust execution plans based on current hardware availability and communication constraints, ensuring that the system can respond to changing conditions and optimize performance in real time. @tbl-scaling-adaptations summarizes how compilers and runtimes adapt across these dimensions.

+---------------------------+---------------------------------------+-----------------------------------------------------------------+
| **Aspect**                | **Single-Chip AI Accelerator**        | **Multi-Chip AI System Adaptations**                            |
+:==========================+:======================================+:================================================================+
| **Computation Placement** | Local PE, vector unit, tensor core    | Interconnect-aware partitioning, distributed load balancing     |
+---------------------------+---------------------------------------+-----------------------------------------------------------------+
| **Memory Management**     | Caching, HBM reuse, local tiling      | Distributed allocation, prefetching, cross-device caching       |
+---------------------------+---------------------------------------+-----------------------------------------------------------------+
| **Scheduling**            | Fixed execution order                 | Dynamic load-aware scheduling, congestion-adaptive execution    |
+---------------------------+---------------------------------------+-----------------------------------------------------------------+
| **Optimization Scope**    | Local resource utilization            | Global system efficiency across all accelerators                |
+---------------------------+---------------------------------------+-----------------------------------------------------------------+

: **Multi-Chip Adaptations**: Compilers and runtimes extend their capabilities to dynamically adapt to system state and network congestion, enabling scalable and performant multi-chip AI systems. {#tbl-scaling-adaptations}

For example, in Google's TPU Pods, the TPU runtime is responsible for scheduling computations across multiple TPU cores, ensuring that workloads are executed in a way that minimizes communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution must synchronize operations across GPUs, ensuring that data is transferred efficiently while maintaining execution order.

Thus, while single-chip runtimes focus on optimizing execution within a single processor, multi-chip runtimes must handle system-wide execution, balancing computation, memory, and interconnect performance.

## Scaling Challenges and Future Directions {#sec-scaling-ai-hardware-challenges}

The evolution of AI hardware highlights the increasing complexity of efficiently executing large-scale machine learning workloads, progressing from single-chip accelerators to multi-chip systems and wafer-scale integration. Scaling AI systems introduces new challenges in computation placement, memory management, and data movement. While the fundamental principles of AI acceleration remain consistent, their implementation must adapt to the constraints of distributed execution, interconnect bandwidth limitations, and synchronization overhead.

Multi-chip AI architectures represent a significant step forward in addressing the computational demands of modern machine learning models. By distributing workloads across multiple accelerators, these systems offer increased performance, memory capacity, and scalability. However, realizing these benefits requires careful consideration of how computations are mapped to hardware, how memory is partitioned and accessed, and how execution is scheduled across a distributed system.

As AI models continue to grow in size and complexity, new architectural innovations, mapping strategies, and runtime optimizations will be needed to sustain efficient execution. The ongoing development of AI hardware and software reflects a broader trend in computing, where specialization and domain-specific architectures are becoming increasingly important for addressing the unique demands of emerging workloads.

Understanding the principles and trade-offs involved in multi-chip AI acceleration enables machine learning engineers and system designers to make informed decisions about how to best deploy and optimize their models. Whether training large language models on TPU pods or deploying computer vision applications on multi-GPU systems, the ability to efficiently map computations to hardware will continue to be a critical factor in realizing the full potential of AI.

## Summary {#sec-scaling-ai-hardware-summary}

Scaling AI hardware beyond single accelerators introduces fundamentally different engineering challenges that transform hardware acceleration from a local optimization problem into a distributed systems challenge. The progression from chiplet-based integration through multi-GPU systems to TPU Pods and wafer-scale computing reveals how communication overhead, memory coherence, and fault tolerance become the dominant constraints at scale, often limiting performance more than raw computational capability.

Amdahl's Law provides the theoretical framework for understanding scaling limits: communication overhead during gradient synchronization creates sequential bottlenecks that constrain parallel speedup regardless of available compute power. For large model training, these constraints explain why adding GPUs beyond approximately 100 provides diminishing returns without algorithmic innovations like gradient compression, pipeline parallelism, or asynchronous updates.

::: {.callout-important title="Key Takeaways"}
* Multi-chip AI systems require fundamentally different optimization strategies focused on minimizing inter-chip communication rather than maximizing local compute utilization
* Amdahl's Law quantifies scaling limits: communication overhead during gradient synchronization constrains speedup regardless of available compute power
* Different scaling approaches (chiplets, multi-GPU, TPU Pods, wafer-scale) present distinct trade-offs between integration density, interconnect bandwidth, and system complexity
* Distributed memory allocation and execution scheduling become global optimization problems requiring interconnect-aware strategies
* Fault tolerance and redundancy mechanisms are essential for maintaining training consistency across large-scale distributed systems
:::

The architectural choices examined in this chapter each represent different points on the trade-off curve between computational density, communication efficiency, and system reliability, from NVSwitch interconnect topologies to TPU Pod torus networks to wafer-scale integration. As AI models continue to scale, understanding these trade-offs becomes essential for designing systems that can efficiently leverage distributed acceleration while managing the inherent complexity of coordinating computation across thousands of interconnected processors.
