---
title: "Communication and Collective Operations"
bibliography: communication.bib
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR COMMUNICATION
================================================================================

CORE PRINCIPLE: Communication patterns differ fundamentally by model type.
Dense gradient sync (transformers) vs sparse updates (recommendation) vs
irregular patterns (GNNs) require different optimizations.

MODEL-SPECIFIC COMMUNICATION PATTERNS:

| Model Type      | Primary Collective | Gradient Type | Compression Benefit |
|-----------------|-------------------|---------------|---------------------|
| LLMs            | AllReduce         | Dense         | Moderate            |
| Recommendation  | AlltoAll          | Sparse        | High (embeddings)   |
| Vision (CNN)    | AllReduce         | Dense         | Moderate            |
| GNN             | Neighbor exchange | Irregular     | Low (sparse)        |
| MoE             | AlltoAll          | Selective     | Model-dependent     |

REQUIRED COVERAGE FOR THIS CHAPTER:

COLLECTIVE OPERATIONS:

- AllReduce: Dense gradient aggregation (vision, transformers, most models)
- AlltoAll: Embedding exchange (recommendation, MoE routing)
- AllGather: Model state collection (pipeline parallelism)
- ReduceScatter: Sharded gradient accumulation (ZeRO, FSDP)
- Include: When each collective is appropriate for different model types

COMMUNICATION ALGORITHMS:

- Ring AllReduce: Bandwidth-optimal for dense gradients
- Tree AllReduce: Latency-optimal for small messages
- Hierarchical: Hybrid for large clusters
- Include: Why recommendation systems often prefer different algorithms

GRADIENT COMPRESSION:

- Dense quantization: Works well for vision/NLP
- Sparse gradients: Natural for recommendation (embedding updates)
- Top-k sparsification: Benefits vary by model type
- Include: Why compression ROI differs between model architectures

NETWORK TOPOLOGY CONSIDERATIONS:

- Fat-tree: Good for AllReduce-heavy workloads
- Rail-optimized: Better for tensor parallelism
- Include: Different topologies suit different model types

CASE STUDIES TO INCLUDE:

- NCCL optimization for transformer training
- HugeCTR communication patterns for recommendation
- Graph neural network message passing at scale
- Mixture of Experts routing communication

QUANTITATIVE ANALYSIS:

- Communication/computation overlap by model type
- Bandwidth utilization for different collectives
- Latency breakdown: network vs software overhead
- Include: Same algorithm, different efficiency for different models

ANTI-PATTERNS TO AVOID:

- Assuming all distributed training uses AllReduce
- Ignoring AlltoAll importance for embeddings/MoE
- Treating gradient compression as universally beneficial
- Only optimizing for transformer communication patterns

================================================================================
-->

# Communication and Collective Operations {#sec-communication}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: A technical visualization of collective communication patterns in distributed computing. The scene shows multiple compute nodes arranged in various topologies: a ring formation demonstrating ring allreduce with data flowing clockwise, a star pattern showing parameter server architecture with a central aggregator, and a mesh topology with all-to-all connections. Each node is depicted as a stylized GPU with data packets traveling along luminous pathways between them. Visual elements include bandwidth indicators showing throughput on each link, latency clocks measuring communication time, and gradient tensors being reduced and broadcast. The composition uses a dark background with nodes in metallic silver and communication paths in vibrant colors: green for scatter operations, blue for gather, orange for reduce, and purple for broadcast. Technical diagram style with clear labeling, suitable for a networking and systems textbook._
:::

\noindent
![](images/png/cover_communication.png)

:::

## Purpose {.unnumbered}

_Why does communication between machines become the fundamental constraint that governs large-scale machine learning systems?_

Large-scale machine learning systems spread computation across many machines to overcome resource limitations, but this distribution introduces a new bottleneck: coordinated communication between independent machines. When thousands of devices must synchronize training progress, share model updates, or coordinate inference decisions, the network becomes the limiting factor that determines system efficiency. Communication overhead can dominate training time, turning what appears to be a computation problem into a network problem that requires fundamentally different engineering approaches. Physical constraints including bandwidth limitations, latency across geographic distances, and energy costs of data movement are as immutable as computational limits, yet communication systems are far less intuitive to reason about than processing power. Mastering communication patterns and collective operations transforms systems engineers from passive consumers of network infrastructure into active designers who can orchestrate distributed computation to leverage network capabilities rather than struggle against network constraints.

## Coming 2026

This chapter will cover AllReduce, parameter servers, network topology, and collective primitives.

```{=latex}
\part{key:vol2_distributed}
```
