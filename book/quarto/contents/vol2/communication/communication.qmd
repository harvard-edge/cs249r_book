---
title: "Communication and Collective Operations"
---

# Communication and Collective Operations {#sec-communication}

## Purpose {.unnumbered}

_Why does communication between machines become the fundamental constraint that governs large-scale machine learning systems?_

Distributed machine learning systems distribute computation to overcome resource limitations, but this distribution introduces a new bottleneck: coordinated communication between independent machines. When thousands of devices must synchronize training progress, share model updates, or coordinate inference decisions, the network becomes the limiting factor that determines system efficiency. Communication overhead can dominate training time, turning what appears to be a computation problem into a network problem that requires fundamentally different engineering approaches. Physical constraints including bandwidth limitations, latency across geographic distances, and energy costs of data movement are as immutable as computational limits, yet communication systems are far less intuitive to reason about than processing power. Mastering communication patterns and collective operations transforms systems engineers from passive consumers of network infrastructure into active designers who can orchestrate distributed computation to work with rather than against network reality.

## Coming 2026

This chapter will cover AllReduce, parameter servers, network topology, and collective primitives.

```{=latex}
\part{key:vol2_distributed}
```
