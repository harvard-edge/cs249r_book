% Bibliography for Communication and Collective Operations chapter

% Foundational Communication Models
@inproceedings{culler1993logp,
  title={LogP: Towards a realistic model of parallel computation},
  author={Culler, David and Karp, Richard and Patterson, David and Sahay, Abhijit and Schauser, Klaus Erik and Santos, Eunice and Subramonian, Ramesh and von Eicken, Thorsten},
  booktitle={ACM SIGPLAN Notices},
  volume={28},
  number={7},
  pages={1--12},
  year={1993},
  organization={ACM}
}

@article{alexandrov1995loggp,
  title={LogGP: Incorporating long messages into the LogP modelâ€”one step closer towards a realistic model for parallel computation},
  author={Alexandrov, Albert and Ionescu, Mihai F and Schauser, Klaus E and Scheiman, Chris},
  journal={ACM SIGPLAN Notices},
  volume={30},
  number={7},
  pages={95--105},
  year={1995}
}

% Ring AllReduce and Distributed Deep Learning
@misc{gibiansky2017baidu,
  title={Bringing HPC Techniques to Deep Learning},
  author={Gibiansky, Andrew},
  year={2017},
  howpublished={Baidu Research Blog},
  url={http://research.baidu.com/bringing-hpc-techniques-deep-learning/}
}

% NVIDIA NCCL
@misc{jeaugey2017nccl,
  title={NCCL 2.0},
  author={Jeaugey, Sylvain},
  year={2017},
  howpublished={NVIDIA Developer Blog},
  url={https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2/}
}

% MPI Standard
@techreport{mpi2021standard,
  title={MPI: A Message-Passing Interface Standard Version 4.0},
  author={{MPI Forum}},
  year={2021},
  institution={Message Passing Interface Forum},
  url={https://www.mpi-forum.org/docs/}
}

% Gradient Compression
@inproceedings{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{lin2018deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{aji2017sparse,
  title={Sparse communication for distributed gradient descent},
  author={Aji, Alham Fikri and Heafield, Kenneth},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={440--445},
  year={2017}
}

% Recommendation Systems
@article{naumov2019dlrm,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

% Large Language Models
@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

% Distributed Training Frameworks
@inproceedings{shoeybi2019megatron,
  title={Megatron-LM: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{narayanan2021megatron,
  title={Efficient large-scale language model training on GPU clusters using Megatron-LM},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kasber, Prethvi and Petrini, Fabrizio and Sang, Eric and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@inproceedings{rajbhandari2020zero,
  title={ZeRO: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{zhao2023fsdp,
  title={PyTorch FSDP: Experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Patel, Tejash and Lian, Wei and Yang, Yongli and others},
  journal={Proceedings of the VLDB Endowment},
  volume={16},
  number={12},
  pages={3848--3860},
  year={2023}
}

@article{li2020pytorch,
  title={PyTorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={12},
  pages={3005--3018},
  year={2020}
}

@article{sergeev2018horovod,
  title={Horovod: Fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

% Mixture of Experts
@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@inproceedings{lepikhin2021gshard,
  title={GShard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

% Gradient Compression
@inproceedings{vogels2020powersgd,
  title={PowerSGD: Practical low-rank gradient compression for distributed optimization},
  author={Vogels, Thijs and Karimireddy, Sai Praneeth and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% Network Topologies
@inproceedings{alfares2008scalable,
  title={A scalable, commodity data center network architecture},
  author={Al-Fares, Mohammad and Loukissas, Alexander and Vahdat, Amin},
  booktitle={ACM SIGCOMM Computer Communication Review},
  volume={38},
  number={4},
  pages={63--74},
  year={2008}
}

@inproceedings{kim2008dragonfly,
  title={Technology-driven, highly-scalable dragonfly topology},
  author={Kim, John and Dally, William J and Scott, Steve and Abts, Dennis},
  booktitle={Proceedings of the 35th Annual International Symposium on Computer Architecture},
  pages={77--88},
  year={2008}
}

% TPU and Hardware
@inproceedings{jouppi2023tpuv4,
  title={TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings},
  author={Jouppi, Norman P and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--14},
  year={2023}
}

% In-Network Computing
@inproceedings{graham2020sharp,
  title={Scalable hierarchical aggregation and reduction protocol (SHARP) streaming-aggregation hardware design and evaluation},
  author={Graham, Richard L and Bureddy, Devendar and Lui, Pak and Rober, Hal and Bloch, Gilad and Shainer, Gilad and Poole, Jeff and Tygert, Mark and Nguyen, Phil and Srivastava, Anurag and others},
  booktitle={International Conference on High Performance Computing},
  pages={41--59},
  year={2020},
  organization={Springer}
}

% Graph Neural Networks
@inproceedings{hamilton2017graphsage,
  title={Inductive representation learning on large graphs},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{karypis1998metis,
  title={A fast and high quality multilevel scheme for partitioning irregular graphs},
  author={Karypis, George and Kumar, Vipin},
  journal={SIAM Journal on Scientific Computing},
  volume={20},
  number={1},
  pages={359--392},
  year={1998}
}
