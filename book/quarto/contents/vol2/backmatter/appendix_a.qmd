# Appendix A: System Architectures & Foundations {#sec-appendix-systems}

This appendix provides a reference for the architectural paradigms and foundational concepts that underpin the distributed machine learning systems discussed in Volume II. It contrasts the emerging "ML Fleet" architecture with traditional High-Performance Computing (HPC) and Warehouse-Scale Computing (WSC) models, and recaps core ML systems concepts from Volume I.

## The Three System Paradigms {#sec-appendix-systems-three-paradigms}

Machine Learning Systems at scale represent a convergence of two distinct computing lineages: the raw computational power of HPC and the fault-tolerant scalability of WSC. Understanding how ML systems differ from their predecessors is crucial for designing infrastructure that meets the unique demands of large-scale training and inference.

### 1. High-Performance Computing (HPC) {#sec-appendix-systems-hpc}
*The lineage of Supercomputers.*

*   **Philosophy**: "Maximize FLOPs per second."
*   **Workload**: Tightly coupled simulations (e.g., weather, nuclear physics).
*   **Hardware**: Specialized interconnects (InfiniBand), low-latency fabrics, homogeneous nodes.
*   **Fault Tolerance**: **Checkpoint/Restart**. If one node fails, the entire job stops, rolls back to the last checkpoint, and restarts.
*   **Scheduling**: Batch scheduling (Slurm). Jobs request rigid resource shapes (e.g., "512 nodes for 24 hours").
*   **The "Pets" Model**: Nodes are individually important. A specific node failure is a critical event.

### 2. Warehouse-Scale Computing (WSC) {#sec-appendix-systems-wsc}
*The lineage of the Web.*

*   **Philosophy**: "Maximize Requests per second (QPS)."
*   **Workload**: Loosely coupled services (e.g., Search, Gmail, Social Media).
*   **Hardware**: Commodity Ethernet, varying generations of hardware, heterogeneous nodes.
*   **Fault Tolerance**: **Redundancy**. If one node fails, the load balancer reroutes traffic to another replica. The user never notices.
*   **Scheduling**: Service orchestration (Kubernetes, Borg). Jobs are dynamic, elastic, and bin-packed.
*   **The "Cattle" Model**: Nodes are interchangeable and expendable. Failures are routine background noise.

### 3. The Machine Learning Fleet {#sec-appendix-systems-ml-fleet}
*The Hybrid Architecture.*

ML Systems require the **throughput** of HPC (to train massive models) but must operate on the **scale** and **unreliability** of WSC.

*   **Philosophy**: "Maximize Model Quality per Dollar/Watt."
*   **Workload**:
    *   *Training*: Synchronous, bandwidth-heavy (like HPC) but long-running and resilient (like WSC).
    *   *Inference*: Latency-sensitive (like WSC) but computationally heavy (like HPC).
*   **Hardware**: A fusion. TCP/IP for control planes, InfiniBand/NVLink for data planes. GPU-centric nodes.
*   **Fault Tolerance**: **Elasticity**. Training jobs can shrink/expand or pause/resume without full restarts. Inference serves degrade gracefully.
*   **Scheduling**: Gang scheduling (all-or-nothing allocation) combined with dynamic preemption and replacement.

| Feature | HPC (Supercomputer) | WSC (Web Cloud) | ML Fleet (AI Cluster) |
| :--- | :--- | :--- | :--- |
| **Coupling** | Tight (MPI) | Loose (RPC/HTTP) | Hybrid (NCCL + RPC) |
| **State** | Stateful (RAM) | Stateless (DB-backed) | Semi-Stateful (Checkpoints + KV Cache) |
| **Network** | Latency-optimized | Bandwidth-optimized | Bisection-Bandwidth critical |
| **Bottleneck** | Compute (FLOPs) | I/O (Disk/Net) | Memory Bandwidth (HBM) |

## Foundations of ML Systems (Volume I Recap) {#sec-appendix-systems-vol1-recap}

This section provides a condensed reference of the foundational concepts covered in Volume I. These fundamentals—spanning the ML lifecycle, hardware foundations, and optimization techniques—form the building blocks upon which the scale-focused topics in Volume II are constructed.

### The Machine Learning Lifecycle {#sec-appendix-systems-lifecycle}
1.  **Data Engineering**: ETL pipelines, Feature Stores, Validation.

2.  **Model Development**: Architecture search, Hyperparameter tuning.

3.  **Deployment**: Serving, Monitoring, Incident Response.

4.  **Operations (MLOps)**: CI/CD for data/models, Drift detection.

### Hardware Foundations {#sec-appendix-systems-hardware}
*   **CPUs**: Sequential logic, preprocessing, orchestration.
*   **GPUs**: Parallel tensor arithmetic (Matrix Multiply), HBM.
*   **TPUs/NPUs**: Systolic arrays for dense matrix ops, low power.
*   **Memory Hierarchy**: HBM (Fast/Small) $\to$ SRAM (Faster/Tiny) $\to$ NVMe (Slow/Huge).

### Optimization Fundamentals {#sec-appendix-systems-optimization}
*   **Precision**: FP32 (Training) vs. BF16 (Mixed) vs. INT8 (Inference).
*   **Sparsity**: Structured (N:M) vs. Unstructured pruning.
*   **Distillation**: Teacher $\to$ Student knowledge transfer.
*   **The Iron Law**: $L = \frac{D}{B} + \frac{Ops}{P} + L_{lat}$. Performance is bounded by Data movement or Compute.