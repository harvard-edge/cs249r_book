# Appendix A: Foundations of ML Systems {#sec-appendix-foundations}

This appendix provides a primer on the single-node machine learning systems concepts that form the building blocks for the distributed systems discussed in Volume II. For a comprehensive treatment of these foundations, readers should refer to *Volume I: Build, Optimize, Deploy*.

## The Machine Learning Lifecycle

Modern machine learning follows a four-stage engineering lifecycle:

1.  **Data Engineering**: The process of transforming raw inputs into high-quality features. This involves collection, cleaning, validation, and feature store management.
2.  **Model Development**: The iterative search for the best model architecture and parameters. This includes selecting loss functions, optimizers (e.g., SGD, Adam), and hyperparameter tuning.
3.  **Deployment**: Transitioning a model from a research environment to a production serving system. Key concerns include latency, throughput, and cold-start handling.
4.  **Operations (MLOps)**: The continuous monitoring and maintenance of models in production. This includes tracking performance drift, detecting anomalies, and triggering automated retraining.

## Hardware Foundations

### The Unit of Compute
Machine learning systems rely on a heterogeneous compute stack:
*   **CPUs**: Highly flexible, optimized for sequential logic and complex branching. They handle data preprocessing and orchestration.
*   **GPUs**: Massively parallel processors optimized for the tensor arithmetic (matrix multiplication) that defines deep learning. They utilize thousands of simple cores and High Bandwidth Memory (HBM).
*   **TPUs/NPUs**: Custom ASICs (Application-Specific Integrated Circuits) designed specifically for neural network operations, often utilizing systolic arrays to minimize data movement.

### The Memory Hierarchy
Data movement is the primary performance bottleneck in ML systems. The memory hierarchy ranges from fast, expensive, and small registers to slow, cheap, and large persistent storage:
*   **Registers/L1 Cache**: Single-cycle access, directly adjacent to compute.
*   **SRAM (L2/L3)**: High speed, low capacity, resides on the processor die.
*   **DRAM (HBM/DDR)**: The primary residence for model weights and activations.
*   **Flash/Disk**: Persistent storage for datasets and model checkpoints.

## Optimization Fundamentals

### The Latency-Throughput Tradeoff
ML systems must balance two competing objectives:
*   **Latency**: The time taken to process a single request (critical for real-time apps).
*   **Throughput**: The total number of requests processed per unit time (critical for cost efficiency).
Increasing **Batch Size** typically improves throughput by maximizing parallel utilization but increases latency.

### Numerical Precision
Modern hardware supports various bit-widths for floating-point and integer numbers:
*   **FP32 (Standard)**: High precision, high memory/energy cost.
*   **FP16/BF16 (Mixed Precision)**: Reduces memory usage by 2x and accelerates training on modern tensor cores with negligible accuracy loss.
*   **INT8 (Quantization)**: Reduces memory by 4x and is common for edge inference.

### Model Compression
Techniques to reduce model size and compute requirements:
*   **Pruning**: Removing non-essential weights or neurons from the network.
*   **Distillation**: Training a smaller "student" model to mimic a larger "teacher" model.
*   **Quantization**: Lowering the precision of weights and activations.
