---
engine: jupyter
---

# Fleet Foundations {#sec-fleet-foundations}

## Purpose {.unnumbered}

_What reference numbers and physical laws should every fleet-scale ML engineer carry into distributed system design decisions?_

Designing a system for a single machine requires knowledge of memory hierarchy latencies, roofline ridge points, and precision trade-offs. But the moment a training job spans two machines, a new set of numbers takes over. Network latency replaces cache latency as the dominant concern. Component failure rates compound from negligible to inevitable. Communication overhead erodes the scaling efficiency that justifies buying more accelerators in the first place.

This appendix collects the reference numbers and compact models for fleet-scale reasoning. It begins with a comparison of the three system paradigms that underpin this book, then provides the "numbers every fleet engineer should know"---organized around the three axes of Communication, Computation, and Coordination (C$^3$). It concludes with the scaling physics and thermal constraints that govern cluster design.

```{python}
#| label: appendix-fleet-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET FOUNDATIONS — MASTER COMPUTATION (LEGO)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: PERSISTENT — All values used throughout the Fleet Foundations
# │          appendix: hardware reference table, MTBF tables, checkpoint sizing,
# │          effective FLOPS, comm-compute ratio, and all prose inline values.
# │
# │ Goal: Provide all quantitative fleet engineering constants in one place.
# │ How: pint Quantities from mlsys.constants; fleet formulas from formulas.py.
# │
# │ Imports: mlsys.constants (*), mlsys.formulas (calc_*), mlsys.formatting (*)
# │ Exports: FF = FleetFoundations (accessed as FF.attribute in downstream cells)
# └─────────────────────────────────────────────────────────────────────────────

import math
from mlsys.constants import *
from mlsys.formatting import fmt, fmt_percent, check, md, md_math, sci_latex
from mlsys.formulas import (
    calc_mtbf_cluster, calc_mtbf_node,
    calc_failure_probability, calc_effective_flops,
    calc_amdahls_speedup, calc_ring_allreduce_time,
    calc_young_daly_interval, calc_checkpoint_size
)

# ┌── LEGO ───────────────────────────────────────────────
class FleetFoundations:
    """Namespace for fleet-scale reference calculations."""

    # ┌── 1. LOAD (Constants) ──────────────────────────────────────────────
    # Level 1: Interconnect & Memory
    h100_qty = H100_FLOPS_FP16_TENSOR
    h100_bw_qty = H100_MEM_BW
    nvlink_h100_qty = NVLINK_H100_BW
    pcie5_qty = PCIE_GEN5_BW
    ib_ndr_bw = INFINIBAND_NDR_BW_GBS
    ib_ndr_lat = IB_NDR_LATENCY_US

    # Level 2: Node
    gpus_per_node = GPUS_PER_HOST
    node_mtbf_params = {
        "gpu_mttf": GPU_MTTF_HOURS,
        "nic_mttf": NIC_MTTF_HOURS,
        "psu_mttf": PSU_MTTF_HOURS,
    }

    # Level 3: Cluster
    cluster_sizes = [CLUSTER_SMALL_GPUS, CLUSTER_MEDIUM_GPUS, CLUSTER_LARGE_GPUS, CLUSTER_MEGA_GPUS]
    cl_small = CLUSTER_SMALL_GPUS
    cl_medium = CLUSTER_MEDIUM_GPUS
    cl_large = CLUSTER_LARGE_GPUS
    cl_mega = CLUSTER_MEGA_GPUS  # for prose: "At FF.cl_mega GPUs"

    # Efficiency & Budgets
    mfu_range = (MFU_TRAINING_LOW, MFU_TRAINING_HIGH)
    oh_pipeline = OVERHEAD_PIPELINE_BUBBLE
    oh_checkpoint = OVERHEAD_CHECKPOINT
    oh_failure = OVERHEAD_FAILURE_RECOVERY
    oh_maintenance = OVERHEAD_MAINTENANCE

    # Scaling efficiency percentages (for prose: ~FF.eff_1024%)
    eff_32 = int(SCALING_EFF_32GPU * 100)
    eff_256 = int(SCALING_EFF_256GPU * 100)
    eff_1024 = int(SCALING_EFF_1024GPU * 100)
    eff_8192 = int(SCALING_EFF_8192GPU * 100)

    # ┌── 2. EXECUTE (The Compute) ────────────────────────────────────────
    # Step 1: Level 4: Ratios & Rework
    nvlink_h100_bw_val = int(nvlink_h100_qty.m_as(GB / second))
    nvlink_to_ib = int(nvlink_h100_bw_val / ib_ndr_bw)

    node_mtbf_h = calc_mtbf_node(
        node_mtbf_params["gpu_mttf"], 8,
        node_mtbf_params["nic_mttf"], 8,
        node_mtbf_params["psu_mttf"], 4
    )

    mtbf_256_h = calc_mtbf_cluster(GPU_MTTF_HOURS, 256)
    mtbf_2048_h = calc_mtbf_cluster(GPU_MTTF_HOURS, 2048)
    mtbf_8192_h = calc_mtbf_cluster(GPU_MTTF_HOURS, 8192)
    mtbf_100k_h = calc_mtbf_cluster(GPU_MTTF_HOURS, CLUSTER_MEGA_GPUS)
    mtbf_256_min = mtbf_256_h.m_as(ureg.minute)
    mtbf_2048_min = mtbf_2048_h.m_as(ureg.minute)
    mtbf_8192_min = mtbf_8192_h.m_as(ureg.minute)
    mtbf_100k_min = mtbf_100k_h.m_as(ureg.minute)
    pfail_256_24h = calc_failure_probability(mtbf_256_h, 24 * ureg.hour)
    pfail_2048_24h = calc_failure_probability(mtbf_2048_h, 24 * ureg.hour)
    pfail_8192_24h = calc_failure_probability(mtbf_8192_h, 24 * ureg.hour)
    pfail_100k_24h = calc_failure_probability(mtbf_100k_h, 24 * ureg.hour)

    # Step 2: Effective FLOPS (1024-GPU cluster)
    _peak_1024_qty = 1024 * h100_qty
    goodput_ratio = 1.0 - (oh_pipeline + oh_checkpoint + oh_failure + oh_maintenance)
    _eff_flops_1024_qty = calc_effective_flops(
        _peak_1024_qty,
        mfu_range[1],
        SCALING_EFF_1024GPU,
        goodput_ratio
    )
    # Store as plain float so prose/formatters never see a Quantity (avoids display bugs)
    eff_fraction = float((_eff_flops_1024_qty / _peak_1024_qty).m_as(''))

    # ┌── 3. GUARD (Invariants) ──────────────────────────────────────────
    check(nvlink_to_ib > 10, "NVLink must be >10x IB for hierarchy to hold")
    check(mtbf_100k_min < 60, "100K GPU cluster MTBF must be <1 hour")
    check(eff_fraction < 0.25, "Effective fraction must be <25% at 1024 GPUs")

    # ┌── 4. OUTPUT (Formatting) ─────────────────────────────────────────────
    # (Attributes kept for downstream cell access)
    peak_1024 = _peak_1024_qty.m_as(TFLOPs / second)
    eff_flops_1024 = _eff_flops_1024_qty.m_as(TFLOPs / second)

    # Pre-computed formatted strings
    mfu_low = int(MFU_TRAINING_LOW * 100)
    mfu_high = int(MFU_TRAINING_HIGH * 100)
    oh_pipeline_pct = int(oh_pipeline * 100)
    oh_checkpoint_pct = int(oh_checkpoint * 100)
    oh_failure_pct = int(oh_failure * 100)
    oh_maintenance_pct = int(oh_maintenance * 100)

    # Interconnect scalars (for tables/prose)
    nvlink_h100_bw = int(nvlink_h100_qty.m_as(GB / second))
    pcie5_bw = int(pcie5_qty.m_as(GB / second))
    ib_hdr_bw = INFINIBAND_HDR_BW_GBS
    ib_xdr_bw = INFINIBAND_XDR_BW_GBS
    roce_bw = ROCE_100G_BW_GBS
    eth_400g_bw = ETHERNET_400G_BW_GBS
    tpuv5_ici = int(TPUV5P_ICI_BW.m_as(GB / second))
    ib_hdr_lat = IB_HDR_LATENCY_US
    roce_lat = ROCE_LATENCY_US
    tcp_lat = TCP_LATENCY_US
    ib_to_tcp_lat = int(TCP_LATENCY_US / IB_NDR_LATENCY_US)

    # Checkpoint sizes (GB or TB for display)
    ckpt_7b_gb = calc_checkpoint_size(7e9, 16).m_as(GB)
    ckpt_70b_gb = calc_checkpoint_size(70e9, 16).m_as(GB)
    ckpt_175b_gb = calc_checkpoint_size(175e9, 16).m_as(GB)
    ckpt_1t_tb = calc_checkpoint_size(1e12, 16).m_as(TB)

    # Rack & PUE
    rack_trad = RACK_POWER_TRADITIONAL_KW
    rack_ai = RACK_POWER_AI_TYPICAL_KW
    rack_ai_high = RACK_POWER_AI_HIGH_KW
    air_limit = AIR_COOLING_LIMIT_KW
    pue_liquid = PUE_LIQUID_COOLED
    pue_air = PUE_BEST_AIR
    pue_typical = PUE_TYPICAL
    pue_legacy = PUE_LEGACY
    rack_ratio = rack_ai / rack_trad

    # Accelerator table (scalars for fmt)
    h100_flops = int(H100_FLOPS_FP16_TENSOR.m_as(TFLOPs / second))
    h100_bw_tbs = fmt(H100_MEM_BW.m_as(TB / second), precision=2, commas=False)
    h100_cap = int(H100_MEM_CAPACITY.m_as(GiB))
    h100_tdp = int(H100_TDP.m_as(watt))
    b200_flops = int(B200_FLOPS_FP16_TENSOR.m_as(TFLOPs / second))
    b200_bw_tbs = fmt(B200_MEM_BW.m_as(TB / second), precision=2, commas=False)
    b200_cap = int(B200_MEM_CAPACITY.m_as(GiB))
    b200_tdp = int(B200_TDP.m_as(watt))
    tpuv5_flops = int(TPUV5P_FLOPS_BF16.m_as(TFLOPs / second))
    tpuv5_bw_tbs = fmt(TPUV5P_MEM_BW.m_as(TB / second), precision=2, commas=False)
    tpuv5_cap = int(TPUV5P_MEM_CAPACITY.m_as(GiB))

# ── EXPORTS (Bridge to Text) ────────────────────────────────────────────────
FF = FleetFoundations

# ── Invariant Checks ─────────────────────────────────────────────────────
check(FF.nvlink_to_ib > 10, "NVLink must be >10x IB for hierarchy to hold")
check(FF.mtbf_100k_min < 60, "100K GPU cluster MTBF must be <1 hour")
check(FF.pfail_8192_24h > 0.9, "8K GPU cluster must have >90% failure prob in 24h")
check(FF.eff_fraction < 0.25, "Effective fraction must be <25% at 1024 GPUs")
```

## How to Use This Appendix {.unnumbered}

This appendix is designed as a reference. When you encounter a fleet-scale design question, use it to turn a vague symptom into a specific constraint and then choose the lever that can actually move.

- **"How fast can I communicate between nodes?"**: Start with the Communication Numbers in @sec-fleet-foundations-numbers-to-know for the bandwidth and latency hierarchy.
- **"How many GPUs do I actually need?"**: Use the Scaling Physics in @sec-fleet-foundations-scaling-physics to understand why doubling GPUs does not halve training time.
- **"How often will my cluster fail?"**: Check the Coordination Numbers for MTBF tables and failure probability calculations.
- **"Is my cluster power-limited or compute-limited?"**: See Thermal and Power Physics in @sec-fleet-foundations-thermal-power for power density and cooling constraints.
- **"What does a typical overhead budget look like?"**: The Coordination Numbers include the four overhead categories that erode goodput.
- **Cross-reference for depth**: When you want the full narrative, jump to @sec-compute-infrastructure, @sec-network-fabrics, @sec-distributed-training-systems, @sec-collective-communication, or @sec-fault-tolerance-reliability.

## The Three System Paradigms {#sec-fleet-foundations-system-paradigms}

Machine learning infrastructure at scale inherits design DNA from two distinct computing lineages---and then breaks the assumptions of both. Understanding where ML systems borrow from High-Performance Computing (HPC) and where they borrow from Warehouse-Scale Computing (WSC) is essential for choosing the right design trade-offs. A system architect who treats ML training as "just HPC" will build infrastructure that cannot tolerate failures. One who treats it as "just web services" will build infrastructure that cannot sustain the tight coupling that synchronous training demands.

### High-Performance Computing {.unnumbered}

HPC systems descend from the supercomputer tradition. Their design philosophy is to maximize FLOPs per second on tightly coupled simulations---weather modeling, molecular dynamics, nuclear physics. Every node matters: they use specialized interconnects (InfiniBand), low-latency fabrics, and homogeneous hardware. Fault tolerance follows the **checkpoint/restart** model: if one node fails, the entire job stops, rolls back to the last checkpoint, and restarts from scratch. Scheduling is batch-oriented (Slurm), with jobs requesting rigid resource shapes ("512 nodes for 24 hours"). Nodes are *pets*---individually important, individually tracked.

### Warehouse-Scale Computing {.unnumbered}

WSC systems descend from the web services tradition. Their design philosophy is to maximize queries per second across loosely coupled services---search, email, social media. Hardware is commodity Ethernet, varying generations coexist, and nodes are heterogeneous. Fault tolerance follows the **redundancy** model: if one node fails, the load balancer reroutes traffic to another replica. The user never notices. Scheduling is dynamic (Kubernetes, Borg), with elastic bin-packing. Nodes are *cattle*---interchangeable and expendable.

### The ML Fleet: A Hybrid Architecture {.unnumbered}

ML systems require the computational throughput of HPC (to train massive models with synchronous gradient updates) but must operate at the scale and unreliability of WSC (thousands of accelerators running for weeks). This creates a hybrid that borrows selectively from both traditions.

Training workloads are synchronous and bandwidth-hungry like HPC, but long-running and failure-tolerant like WSC. Inference workloads are latency-sensitive like WSC, but computationally heavy like HPC. The network is a fusion: TCP/IP for control planes, InfiniBand or NVLink for data planes. Fault tolerance uses **elastic** strategies---training jobs can shrink, expand, pause, or resume without full restarts. Scheduling combines gang allocation (all-or-nothing for training) with dynamic preemption and replacement.

@tbl-fleet-paradigm-comparison summarizes the design trade-offs across the three paradigms. The key insight is that ML fleets cannot simply adopt either HPC or WSC patterns wholesale; they must selectively combine elements from each based on the workload phase.

| **Dimension**   | **HPC (Supercomputer)** | **WSC (Web Cloud)**   | **ML Fleet (AI Cluster)**              |
|:----------------|:------------------------|:----------------------|:---------------------------------------|
| **Philosophy**  | Maximize FLOPs/s        | Maximize QPS          | Maximize Model Quality per Dollar/Watt |
| **Coupling**    | Tight (MPI)             | Loose (RPC/HTTP)      | Hybrid (NCCL + RPC)                    |
| **State**       | Stateful (RAM)          | Stateless (DB-backed) | Semi-Stateful (Checkpoints + KV Cache) |
| **Network**     | Latency-optimized       | Bandwidth-optimized   | Bisection-bandwidth critical           |
| **Bottleneck**  | Compute (FLOPs)         | I/O (Disk/Net)        | Memory bandwidth (HBM)                 |
| **Fault model** | Checkpoint/Restart      | Redundancy/Replicas   | Elastic shrink/expand                  |
| **Scheduling**  | Batch (Slurm)           | Orchestration (K8s)   | Gang + preemption                      |
| **Node model**  | Pets (tracked)          | Cattle (expendable)   | Pets during job, cattle between jobs   |

: **Three System Paradigms**: ML fleets inherit design DNA from both HPC and WSC but break assumptions of each. Training resembles HPC (tight coupling), inference resembles WSC (elastic serving), and fault tolerance is a hybrid of both. {#tbl-fleet-paradigm-comparison}

### Foundations Recap {.unnumbered}

The following provides a compact reference for the key foundational ideas that reappear throughout the distributed systems chapters.

- **The Iron Law** ($T \approx D_{\text{vol}}/BW + O/(R_{\text{peak}} \cdot \eta) + L_{\text{lat}}$): Performance is bounded by data movement or compute. At fleet scale, the data movement term expands to include inter-node communication, not just memory bandwidth.
- **Roofline Model**: Distinguishes compute-bound from memory-bound workloads using arithmetic intensity. At fleet scale, a third ceiling appears: **network-bound** workloads whose performance is limited by inter-node bandwidth.
- **Amdahl's Law**: Caps strong-scaling speedup at $1/s$. At fleet scale, the "serial fraction" includes not just sequential code but also synchronization barriers, collective communication, and pipeline bubbles.
- **Training memory rule**: 16 bytes per parameter for mixed-precision Adam training. At fleet scale, this determines how model state is partitioned across nodes (ZeRO, tensor parallelism, pipeline parallelism).
- **Little's Law** ($L = \lambda W$): Sizes inference infrastructure. At fleet scale, it determines how many serving replicas are needed behind a load balancer.

The transition from single-machine to fleet-scale reasoning requires extending these models with new dimensions: network topology, failure probability, and coordination overhead. The numbers in the next section provide the quantitative foundation for that extension.

---

## Numbers Every Fleet Engineer Should Know {#sec-fleet-foundations-numbers-to-know}

Just as single-machine analysis depends on a set of core numbers, fleet-scale engineering is governed by a set of predictable ratios and scaling behaviors. The single-machine numbers still apply within each node, but a new set of numbers governs the spaces *between* nodes. While absolute values evolve with hardware generations, the *ratios* between communication tiers and the *scaling behavior* of failure rates remain remarkably stable. **Memorize the ratios and scaling trends; use the specific numbers as sanity checks.**

::: {.callout-note title="Node-Level Numbers for Fleet Reasoning"}
Fleet reasoning depends on a few node-level numbers that directly affect fleet design: **16 bytes per parameter** (checkpoint sizing, ZeRO partitioning, memory budgets); **NVLink vs. HBM bandwidth** (intra-node parallelism placement); **peak FLOPS and HBM capacity** (MFU and effective FLOPS, batch and model sharding). The hardware table below and the communication numbers in the preceding section give inter-node and current-generation values; the memory hierarchy and roofline ridge points within a node provide the necessary baseline.
:::

::: {.callout-takeaways title="Three Fleet Numbers That Matter Most"}
If you memorize nothing else from this section, memorize these:

1. **18$\times$ gap**: NVLink bandwidth within a node is ~`{python} FF.nvlink_to_ib`$\times$ faster than InfiniBand between nodes. This ratio determines where you place parallelism boundaries---model parallelism within a node, data parallelism across nodes.

2. **MTBF scales as $1/N$**: A cluster's mean time between failures is the single-component MTBF divided by the number of components. At `{python} fmt(FF.cl_mega, precision=0)` GPUs, expect a failure every `{python} fmt(FF.mtbf_100k_min, precision=0)` minutes.

3. **~`{python} fmt_percent(FF.eff_fraction)`% effective utilization**: After MFU, scaling efficiency, and overhead losses compound, a 1,024-GPU cluster delivers roughly `{python} fmt_percent(FF.eff_fraction)`% of its peak FLOPS as useful training work.
:::

Quick reference --- @tbl-fleet-numbers-quick-ref condenses the numbers below into one place. Use it for back-of-envelope checks; use the detailed tables in each subsection when designing or debugging.

```{python}
#| label: fleet-quick-ref
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET QUICK REFERENCE TABLE
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @tbl-fleet-numbers-quick-ref — one-page fleet number summary
# │
# │ Goal: Format all quick-reference values from FF for the summary table.
# │ Show: Bandwidth ratios, MTBF, MFU, PUE, rack power in table cells.
# │ How: fmt() and fmt_percent() on FF (FleetFoundations) attributes.
# │
# │ Imports: mlsys.formatting (fmt, fmt_percent)
# │ Exports: FleetQuickRef.nvlink_ib, FleetQuickRef.ib_ndr_gbs, etc.
# └─────────────────────────────────────────────────────────────────────────────

class FleetQuickRef:
    nvlink_ib      = fmt(FF.nvlink_to_ib, precision=0, commas=False)
    ib_ndr_gbs     = fmt(FF.ib_ndr_bw, precision=0, commas=False)
    ib_ndr_us      = fmt(FF.ib_ndr_lat, precision=0, commas=False)
    eff_1024       = fmt(FF.eff_1024, precision=0, commas=False)
    eff_8192       = fmt(FF.eff_8192, precision=0, commas=False)
    mtbf_8k_min    = fmt(FF.mtbf_8192_min, precision=0, commas=False)
    mtbf_100k_min  = fmt(FF.mtbf_100k_min, precision=0, commas=False)
    ckpt_175       = fmt(FF.ckpt_175b_gb, precision=0, commas=False)
    goodput        = fmt_percent(FF.goodput_ratio, precision=0, commas=False)
    mfu_lo         = fmt(FF.mfu_low, precision=0, commas=False)
    mfu_hi         = fmt(FF.mfu_high, precision=0, commas=False)
    rack_ai        = fmt(FF.rack_ai, precision=0, commas=False)
    air_limit      = fmt(FF.air_limit, precision=0, commas=False)
    pue_liq        = fmt(FF.pue_liquid, precision=2, commas=False)
    pue_typ        = fmt(FF.pue_typical, precision=2, commas=False)
    h100_tdp       = fmt(FF.h100_tdp, precision=0, commas=False)
```

| **Category**               | **Number**                                                                                                                                                   | **Use**                                                             |
|:---------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|
| **Communication**          | NVLink ~`{python} FleetQuickRef.nvlink_ib`× IB NDR                                                                                                           | Parallelism boundary (in-node vs cross-node)                        |
| **Communication**          | IB NDR ~`{python} FleetQuickRef.ib_ndr_gbs` GB/s, ~`{python} FleetQuickRef.ib_ndr_us` μs one-way                                                             | Inter-node bandwidth and latency                                    |
| **Computation**            | MFU `{python} FleetQuickRef.mfu_lo`--`{python} FleetQuickRef.mfu_hi`%, η ~`{python} FleetQuickRef.eff_1024`% @ 1K, ~`{python} FleetQuickRef.eff_8192`% @ 8K  | Effective FLOPS and scaling sanity checks                           |
| **Coordination**           | MTBF 8K: ~`{python} FleetQuickRef.mtbf_8k_min` min; 100K: ~`{python} FleetQuickRef.mtbf_100k_min` min                                                        | Failure expectation and checkpoint cadence                          |
| **Coordination**           | 175B checkpoint ~`{python} FleetQuickRef.ckpt_175` GB (16 B/param)                                                                                           | Recovery and storage sizing                                         |
| **Coordination**           | Goodput ~`{python} FleetQuickRef.goodput`% after overheads                                                                                                   | Wall-clock utilization                                              |
| **Power & sustainability** | AI rack ~`{python} FleetQuickRef.rack_ai` kW; air limit ~`{python} FleetQuickRef.air_limit` kW                                                               | Cooling feasibility (liquid required above air limit)               |
| **Power & sustainability** | PUE liquid ~`{python} FleetQuickRef.pue_liq`, typical ~`{python} FleetQuickRef.pue_typ`; H100 `{python} FleetQuickRef.h100_tdp` W → 10K GPUs ≈ 7 MW IT × PUE | Facility load and carbon (see @sec-fleet-foundations-thermal-power) |

: **Numbers every fleet engineer should know (quick reference)**: One-page summary of the fleet-scale reference numbers in this section. See the detailed Communication, Computation, and Coordination tables below for full context. {#tbl-fleet-numbers-quick-ref}

### The Invariants: Ratios That Will Not Change {.unnumbered}

These relationships are governed by physics or architecture---they will still be true in 2035.

#### Network Hierarchy Ratio {.unnumbered}

The bandwidth gap between intra-node and inter-node communication is an architectural invariant. Chip-to-chip links (NVLink, ICI) connect through short, wide, dedicated paths on a shared substrate. Inter-node links (InfiniBand, Ethernet) must traverse cables, switches, and protocol stacks. This structural difference guarantees that intra-node bandwidth will always be an order of magnitude higher than inter-node bandwidth.

Currently, NVLink 4.0 provides `{python} FF.nvlink_to_ib`$\times$ more bandwidth than InfiniBand NDR. Even as both technologies improve, the ratio persists because both are constrained by the same physics: signaling rates, lane counts, and connector density. This ratio is the single most important number for parallelism strategy: any operation requiring more bandwidth than the inter-node link can provide must be confined within a single node.

#### Failure Scaling Law {.unnumbered}

For $N$ independent components, each with mean time to failure $\text{MTTF}$, the cluster MTBF is:

$$ \text{MTBF}_{\text{cluster}} = \frac{\text{MTTF}_{\text{component}}}{N} $$ {#eq-mtbf-cluster}

This is pure arithmetic, not an approximation. Doubling the cluster size halves the time between failures. At `{python} fmt(FF.cl_mega, precision=0)` GPUs with a `{python} fmt(GPU_MTTF_HOURS, precision=0)`-hour per-GPU MTTF, the cluster experiences a GPU failure every `{python} fmt(FF.mtbf_100k_min, precision=0)` minutes. No fault tolerance strategy can avoid this---the question is how quickly the system recovers.

#### AllReduce Overhead Scaling {.unnumbered}

The bandwidth-optimal Ring AllReduce algorithm transfers $2(N-1)/N \times M$ bytes, where $M$ is the message size and $N$ is the number of participants. As $N$ grows large, this approaches $2M$---the total bytes transferred is nearly independent of the number of GPUs. This is why Ring AllReduce scales well in the bandwidth term. The latency term, however, grows as $2(N-1) \times \alpha$, making latency the bottleneck for small messages on large rings. This tradeoff motivates hierarchical AllReduce strategies that use Ring AllReduce within nodes and Tree AllReduce across nodes.

### Communication Numbers {.unnumbered}

Communication defines the boundaries of parallelism. These tables quantify the bandwidth and latency at each tier of the network hierarchy, from the fastest intra-node links to the slowest wide-area connections. The key question for any distributed ML operation is: *which tier of the network hierarchy does this communication cross?*

@tbl-fleet-bandwidth-hierarchy shows the bandwidth available at each tier. Note the order-of-magnitude drops as communication crosses node boundaries.

```{python}
#| label: fleet-comm-numbers
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET COMMUNICATION BANDWIDTH AND LATENCY
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @tbl-fleet-bandwidth-hierarchy, @tbl-fleet-latency-hierarchy
# │
# │ Goal: Format communication bandwidth and latency strings for hierarchy tables.
# │ Show: FleetCommNumbers.nvlink_bw, FleetCommNumbers.ib_ndr_bw, etc. in table cells.
# │ How: fmt() on FF (FleetFoundations) bandwidth/latency values.
# │
# │ Imports: mlsys.formatting (fmt)
# │ Exports: FleetCommNumbers.nvlink_bw, FleetCommNumbers.pcie5_bw, FleetCommNumbers.ib_*,
# │          FleetCommNumbers.tpuv5_ici, FleetCommNumbers.nvlink_to_ib, FleetCommNumbers.*_lat
# └─────────────────────────────────────────────────────────────────────────────

class FleetCommNumbers:
    # ── Bandwidth strings ───────────────────────────────────────────────────
    nvlink_bw    = fmt(FF.nvlink_h100_bw, precision=0)
    pcie5_bw     = fmt(FF.pcie5_bw, precision=0)
    ib_ndr_bw    = fmt(FF.ib_ndr_bw, precision=0)
    ib_hdr_bw    = fmt(FF.ib_hdr_bw, precision=0)
    ib_xdr_bw    = fmt(FF.ib_xdr_bw, precision=0)
    roce_bw      = fmt(FF.roce_bw, precision=1, commas=False)
    eth_400g_bw  = fmt(FF.eth_400g_bw, precision=0)
    tpuv5_ici    = fmt(FF.tpuv5_ici, precision=0)

    # ── Latency strings ─────────────────────────────────────────────────────
    ib_ndr_lat   = fmt(FF.ib_ndr_lat, precision=0, commas=False)
    ib_hdr_lat   = fmt(FF.ib_hdr_lat, precision=0, commas=False)
    roce_lat     = fmt(FF.roce_lat, precision=0, commas=False)
    tcp_lat      = fmt(FF.tcp_lat, precision=0, commas=False)

    nvlink_to_ib = fmt(FF.nvlink_to_ib, precision=0, commas=False)
```

| **Interconnect**      | **Bandwidth**                              | **Typical Role**                            |
|:----------------------|:-------------------------------------------|:--------------------------------------------|
| **NVLink 4.0 (H100)** | `{python} FleetCommNumbers.nvlink_bw` GB/s | Tensor/pipeline parallelism within a node   |
| **TPU v5p ICI**       | `{python} FleetCommNumbers.tpuv5_ici` GB/s | Intra-pod model parallelism (Google)        |
| **PCIe Gen5 x16**     | `{python} FleetCommNumbers.pcie5_bw` GB/s  | CPU-GPU data transfer, NIC attachment       |
| **IB XDR (800 Gbps)** | `{python} FleetCommNumbers.ib_xdr_bw` GB/s | Next-gen inter-node (2025+)                 |
| **IB NDR (400 Gbps)** | `{python} FleetCommNumbers.ib_ndr_bw` GB/s | Current inter-node standard for AI clusters |
| **IB HDR (200 Gbps)** | `{python} FleetCommNumbers.ib_hdr_bw` GB/s | Previous-gen inter-node                     |
| **RoCE v2 (100 GbE)** | `{python} FleetCommNumbers.roce_bw` GB/s   | Budget clusters, inference fleets           |

: **Communication Bandwidth Hierarchy**: Bandwidth drops by roughly 18$\times$ crossing from intra-node (NVLink) to inter-node (InfiniBand). This ratio determines parallelism placement. {#tbl-fleet-bandwidth-hierarchy}

@tbl-fleet-latency-hierarchy shows the one-way latency at each tier. For collective operations on small messages, latency---not bandwidth---is the bottleneck.

| **Interconnect**      | **One-Way Latency**                            | **Implication**                       |
|:----------------------|:-----------------------------------------------|:--------------------------------------|
| **InfiniBand NDR**    | ~`{python} FleetCommNumbers.ib_ndr_lat` $\mu$s | Low enough for synchronous AllReduce  |
| **InfiniBand HDR**    | ~`{python} FleetCommNumbers.ib_hdr_lat` $\mu$s | Adequate for most training topologies |
| **RoCE v2**           | ~`{python} FleetCommNumbers.roce_lat` $\mu$s   | Acceptable for data parallelism       |
| **TCP/IP (Ethernet)** | ~`{python} FleetCommNumbers.tcp_lat` $\mu$s    | Too slow for synchronous training     |
| **Cross-datacenter**  | ~40,000 $\mu$s (40 ms)                         | Physics floor; async training only    |

: **Communication Latency Hierarchy**: Latency determines whether synchronous training is feasible. TCP/IP is roughly 10$\times$ slower than InfiniBand NDR, making it unsuitable for gradient synchronization in large clusters. {#tbl-fleet-latency-hierarchy}

### Computation Numbers {.unnumbered}

Raw peak FLOPS are a necessary but misleading metric for fleet capacity planning. Two multiplicative losses---Model FLOPS Utilization (MFU) and scaling efficiency---reduce effective throughput dramatically. Understanding these losses transforms fleet sizing from guesswork into engineering.

Model FLOPS Utilization (MFU) measures what fraction of peak FLOPS a training workload actually achieves. Well-optimized large-model training on current hardware achieves `{python} FF.mfu_low`--`{python} FF.mfu_high`% MFU. The gap comes from memory stalls, kernel launch overhead, pipeline bubbles, and suboptimal operator fusion. MFU below `{python} FF.mfu_low`% signals optimization opportunities; MFU above `{python} FF.mfu_high`% indicates excellent hardware utilization.

Scaling efficiency ($\eta$) measures how much useful computation survives as you add more accelerators. @tbl-fleet-scaling-efficiency shows the empirical ranges for well-optimized distributed training.

| **Cluster Size** | **Scaling Efficiency ($\eta$)** | **Implication**                                  |
|:-----------------|:--------------------------------|:-------------------------------------------------|
| **32 GPUs**      | ~`{python} FF.eff_32`%          | Near-linear scaling; communication is negligible |
| **256 GPUs**     | ~`{python} FF.eff_256`%         | Communication starts to erode throughput         |
| **1,024 GPUs**   | ~`{python} FF.eff_1024`%        | Significant overhead; optimization critical      |
| **8,192 GPUs**   | ~`{python} FF.eff_8192`%        | Fleet-scale regime; 65% of compute is overhead   |

: **Scaling Efficiency by Cluster Size**: Efficiency degrades roughly as the logarithm of cluster size. These ranges assume well-optimized data parallelism with gradient compression. Poorly optimized systems can lose 2--3$\times$ more. {#tbl-fleet-scaling-efficiency}

### Coordination Numbers {.unnumbered}

At fleet scale, coordination---failure recovery, checkpointing, and maintenance---consumes a measurable fraction of wall-clock time. These numbers quantify the costs of keeping a large cluster running.

#### Failure Rates by Cluster Size {.unnumbered}

@tbl-fleet-mtbf shows how cluster MTBF shrinks with scale, using a per-GPU MTTF of `{python} fmt(GPU_MTTF_HOURS, precision=0)` hours (~5.7 years). The failure probability column shows the likelihood of at least one GPU failure during a 24-hour training window.

```{python}
#| label: fleet-mtbf-table
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET MTBF TABLE FORMATTING
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @tbl-fleet-mtbf — cluster MTBF by GPU count
# │
# │ Goal: Format MTBF hours, minutes, and P(failure) percentages for table.
# │ Show: FleetMtbfTable.mtbf_256, FleetMtbfTable.mtbf_8192_min, etc. in table cells.
# │ How: fmt() on FF cluster MTBF values; derive minutes and P(fail) strings.
# │
# │ Imports: mlsys.formatting (fmt)
# │ Exports: FleetMtbfTable.mtbf_256, FleetMtbfTable.mtbf_2048, FleetMtbfTable.mtbf_8192,
# │          FleetMtbfTable.mtbf_100k, FleetMtbfTable.mtbf_*_min, FleetMtbfTable.pfail_*
# └─────────────────────────────────────────────────────────────────────────────

class FleetMtbfTable:
    mtbf_256       = fmt(FF.mtbf_256_h, precision=1, commas=False)
    mtbf_2048      = fmt(FF.mtbf_2048_h, precision=1, commas=False)
    mtbf_8192      = fmt(FF.mtbf_8192_h, precision=1, commas=False)
    mtbf_100k      = fmt(FF.mtbf_100k_h, precision=2, commas=False)

    mtbf_256_min   = fmt(FF.mtbf_256_min, precision=0)
    mtbf_2048_min  = fmt(FF.mtbf_2048_min, precision=0)
    mtbf_8192_min  = fmt(FF.mtbf_8192_min, precision=0)
    mtbf_100k_min  = fmt(FF.mtbf_100k_min, precision=0)

    pfail_256      = fmt(FF.pfail_256_24h * 100, precision=0, commas=False)
    pfail_2048     = fmt(FF.pfail_2048_24h * 100, precision=0, commas=False)
    pfail_8192     = fmt(FF.pfail_8192_24h * 100, precision=0, commas=False)
    pfail_100k     = fmt(FF.pfail_100k_24h * 100, precision=0, commas=False)
```

| **Cluster Size** | **MTBF (GPU-only)**                       | **Minutes**                             | **P(failure) in 24 hours**            |
|:-----------------|:------------------------------------------|:----------------------------------------|:--------------------------------------|
| **256 GPUs**     | `{python} FleetMtbfTable.mtbf_256` hours  | `{python} FleetMtbfTable.mtbf_256_min`  | `{python} FleetMtbfTable.pfail_256`%  |
| **2,048 GPUs**   | `{python} FleetMtbfTable.mtbf_2048` hours | `{python} FleetMtbfTable.mtbf_2048_min` | `{python} FleetMtbfTable.pfail_2048`% |
| **8,192 GPUs**   | `{python} FleetMtbfTable.mtbf_8192` hours | `{python} FleetMtbfTable.mtbf_8192_min` | `{python} FleetMtbfTable.pfail_8192`% |
| **100,000 GPUs** | `{python} FleetMtbfTable.mtbf_100k` hours | `{python} FleetMtbfTable.mtbf_100k_min` | `{python} FleetMtbfTable.pfail_100k`% |

: **MTBF and Failure Probability by Cluster Size**: GPU-only failure model with per-GPU MTTF of 50,000 hours. Real clusters also include NIC, PSU, cable, and switch failures, making these estimates conservative. The probability column uses $P(\geq 1) = 1 - e^{-T/\text{MTBF}}$ for $T = 24$ hours. {#tbl-fleet-mtbf}

The key takeaway: at 8,192 GPUs and above, failure is not a risk---it is a certainty within any training run longer than a few hours. Fault tolerance is not optional at fleet scale; it is a prerequisite for completing any training job. @sec-fault-tolerance-reliability covers the mechanisms in detail.

#### Checkpoint Sizes {.unnumbered}

Checkpointing is the primary recovery mechanism, and its cost depends on the model size. @tbl-fleet-checkpoint-sizes shows checkpoint sizes for mixed-precision Adam training (16 bytes per parameter: 2B for BF16 weights, 2B for gradients, 12B for FP32 master weights + momentum + variance).

```{python}
#| label: fleet-checkpoint-sizes
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET CHECKPOINT SIZES
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @tbl-fleet-checkpoint-sizes — checkpoint sizes by model scale
# │
# │ Goal: Format checkpoint sizes in GB/TB for table.
# │ Show: FleetCheckpointSizes.ckpt_7b, FleetCheckpointSizes.ckpt_175b, etc. in table cells.
# │ How: fmt() on FF checkpoint size values.
# │
# │ Imports: mlsys.formatting (fmt)
# │ Exports: FleetCheckpointSizes.ckpt_7b, FleetCheckpointSizes.ckpt_70b,
# │          FleetCheckpointSizes.ckpt_175b, FleetCheckpointSizes.ckpt_1t
# └─────────────────────────────────────────────────────────────────────────────

class FleetCheckpointSizes:
    ckpt_7b   = fmt(FF.ckpt_7b_gb, precision=0)
    ckpt_70b  = fmt(FF.ckpt_70b_gb, precision=0)
    ckpt_175b = fmt(FF.ckpt_175b_gb, precision=0)
    ckpt_1t   = fmt(FF.ckpt_1t_tb, precision=0)
```

| **Model Size**      | **Checkpoint Size**                          | **Write Time @ 100 GB/s** |
|:--------------------|:---------------------------------------------|:--------------------------|
| **7B parameters**   | `{python} FleetCheckpointSizes.ckpt_7b` GB   | ~1 second                 |
| **70B parameters**  | `{python} FleetCheckpointSizes.ckpt_70b` GB  | ~11 seconds               |
| **175B parameters** | `{python} FleetCheckpointSizes.ckpt_175b` GB | ~28 seconds               |
| **1T parameters**   | `{python} FleetCheckpointSizes.ckpt_1t` TB   | ~2.7 minutes              |

: **Checkpoint Sizes by Model Scale**: Uses 16 bytes/parameter (mixed-precision Adam). Write time assumes 100 GB/s aggregate storage bandwidth. Asynchronous checkpointing (@sec-fault-tolerance-reliability) can overlap writes with training, reducing the visible overhead. {#tbl-fleet-checkpoint-sizes}

#### Overhead Budgets {.unnumbered}

At fleet scale, four categories of overhead consume wall-clock time that is not spent on useful training:

| **Overhead Category**   | **Typical Budget**             | **Lever**                                |
|:------------------------|:-------------------------------|:-----------------------------------------|
| **Pipeline bubbles**    | ~`{python} FF.oh_pipeline`%    | Increase microbatches per pipeline stage |
| **Checkpointing**       | ~`{python} FF.oh_checkpoint`%  | Async checkpointing, faster storage      |
| **Failure recovery**    | ~`{python} FF.oh_failure`%     | Faster detection, elastic rescheduling   |
| **Maintenance windows** | ~`{python} FF.oh_maintenance`% | Rolling upgrades, live migration         |

: **Overhead Budgets for Fleet-Scale Training**: These are fractions of wall-clock time. At 10,000+ GPUs, failure recovery dominates. The compound effect is multiplicative: total goodput ratio $\approx (1 - 0.05)(1 - 0.03)(1 - 0.10)(1 - 0.05) \approx 77$%. {#tbl-fleet-overhead-budgets}

#### Power and Sustainability Numbers {.unnumbered}

Fleet-scale capacity planning and sustainability reporting require a few power numbers that every fleet engineer should know. At fleet scale, the critical numbers are **rack power density**, **PUE**, and the **air-cooling limit**---they determine where you can build and what your facility load will be.

| **Quantity**               | **Typical value**                                          | **Use**                                          |
|:---------------------------|:-----------------------------------------------------------|:-------------------------------------------------|
| **Traditional rack**       | `{python} FF.rack_trad` kW                                 | Baseline for non-AI datacenters                  |
| **AI rack (current gen)**  | `{python} FF.rack_ai` kW                                   | Liquid cooling required                          |
| **AI rack (high-density)** | `{python} FF.rack_ai_high` kW                              | Direct-to-chip liquid                            |
| **Air cooling limit**      | ~`{python} FF.air_limit` kW per rack                       | Physics ceiling; above this, liquid is mandatory |
| **PUE (liquid-cooled AI)** | ~`{python} fmt(FF.pue_liquid, precision=2, commas=False)`  | Best case: facility load ≈ IT load               |
| **PUE (best air-cooled)**  | ~`{python} fmt(FF.pue_air, precision=2, commas=False)`     | Hyperscale best practice                         |
| **PUE (industry average)** | ~`{python} fmt(FF.pue_typical, precision=2, commas=False)` | Sanity check for cost/carbon                     |
| **H100 TDP**               | `{python} FF.h100_tdp` W per GPU                           | IT load: 10K GPUs × 700 W = 7 MW                 |

Rule of thumb: **IT load (MW) = (number of GPUs × TDP per GPU) / 10⁶**; **facility load = IT load × PUE**. A 10,000-GPU H100 cluster at 700 W each is 7 MW IT; at PUE 1.4 that is 9.8 MW facility draw. For carbon and cost, see @sec-fleet-foundations-thermal-power and @sec-sustainable-ai.

### Current Hardware Reference (c. 2024--2025) {.unnumbered}

These numbers reflect the current generation of fleet-scale hardware. Use them for back-of-envelope calculations, but expect them to improve ~2$\times$ every 2--3 years.

```{python}
#| label: fleet-hardware-ref
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET HARDWARE REFERENCE TABLE
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @tbl-fleet-hardware-ref — H100, B200, TPU v5p specs
# │
# │ Goal: Format accelerator specs for hardware reference table.
# │ Show: FleetHardwareRef.h100_flops, FleetHardwareRef.b200_cap, etc. in table cells.
# │ How: fmt() on FF hardware values; use FF string attributes where preformatted.
# │
# │ Imports: mlsys.formatting (fmt)
# │ Exports: FleetHardwareRef.h100_flops, FleetHardwareRef.h100_cap, FleetHardwareRef.h100_tdp,
# │          FleetHardwareRef.b200_flops, FleetHardwareRef.b200_cap, FleetHardwareRef.b200_tdp,
# │          FleetHardwareRef.tpuv5_flops, FleetHardwareRef.tpuv5_cap, FleetHardwareRef.tpuv5_ici
# └─────────────────────────────────────────────────────────────────────────────

class FleetHardwareRef:
    h100_flops  = fmt(FF.h100_flops, precision=0)
    h100_cap    = fmt(FF.h100_cap, precision=0)
    h100_tdp    = fmt(FF.h100_tdp, precision=0)

    b200_flops  = fmt(FF.b200_flops, precision=0)
    b200_cap    = fmt(FF.b200_cap, precision=0)
    b200_tdp    = fmt(FF.b200_tdp, precision=0)

    tpuv5_flops = fmt(FF.tpuv5_flops, precision=0)
    tpuv5_cap   = fmt(FF.tpuv5_cap, precision=0)
    tpuv5_ici   = fmt(FF.tpuv5_ici, precision=0)
```

| **Spec**             | **NVIDIA H100 (SXM)**                         | **NVIDIA B200**                               | **Google TPU v5p**                               |
|:---------------------|:----------------------------------------------|:----------------------------------------------|:-------------------------------------------------|
| **BF16/FP16 Peak**   | `{python} FleetHardwareRef.h100_flops` TFLOPS | `{python} FleetHardwareRef.b200_flops` TFLOPS | `{python} FleetHardwareRef.tpuv5_flops` TFLOPS   |
| **Memory Bandwidth** | `{python} FF.h100_bw_tbs` TB/s                | `{python} FF.b200_bw_tbs` TB/s                | `{python} FF.tpuv5_bw_tbs` TB/s                  |
| **HBM Capacity**     | `{python} FleetHardwareRef.h100_cap` GB       | `{python} FleetHardwareRef.b200_cap` GB       | `{python} FleetHardwareRef.tpuv5_cap` GB         |
| **Intra-Node Link**  | 900 GB/s (NVLink 4.0)                         | 1,800 GB/s (NVLink 5.0)                       | `{python} FleetHardwareRef.tpuv5_ici` GB/s (ICI) |
| **TDP**              | `{python} FleetHardwareRef.h100_tdp` W        | `{python} FleetHardwareRef.b200_tdp` W        | ~400 W (estimated)                               |

: **Fleet-Scale Hardware Reference (c. 2024--2025)**: Per-accelerator specifications for the current generation. The B200 represents a ~4.5$\times$ compute uplift over the H100, with proportional memory bandwidth and capacity increases. {#tbl-fleet-hardware-ref}

Cluster landmarks A DGX H100 SuperPOD contains 32 DGX H100 nodes (256 H100 GPUs). Meta's Grand Teton cluster deployed 16,384 H100 GPUs. Google's TPU v5p pods scale to 8,960 chips. The largest announced clusters (as of 2025) exceed 100,000 accelerators.

---

## Scaling Physics {#sec-fleet-foundations-scaling-physics}

The numbers in the previous section describe *what* the hardware can do. Scaling physics describes *what happens* when you try to use more of it. Three models govern fleet-scale reasoning: Amdahl's Law extended to fleet overhead, the communication-computation ratio, and weak scaling behavior.

::: {.callout-tip title="Why This Matters"}
You have provisioned 4,096 GPUs for a training job, but your throughput is only 2.5$\times$ better than a 1,024-GPU run. Is this expected? Scaling physics gives you the diagnostic tools to determine whether your system is performing as physics allows or whether there is an engineering problem to fix.
:::

### Amdahl's Law at Fleet Scale {#sec-fleet-foundations-amdahls-fleet}

Amdahl's Law establishes that the maximum speedup of a parallel system is limited by its serial fraction $s$. At fleet scale, the "serial fraction" is not just sequential code---it includes every operation that forces all $N$ GPUs to wait:

- **AllReduce synchronization**: All GPUs must complete their gradient computation before any can proceed.
- **Pipeline bubbles**: The warmup and cooldown phases of pipeline parallelism leave stages idle.
- **Checkpoint writes**: Even asynchronous checkpoints contend for storage bandwidth.
- **Python-level overhead**: Single-threaded operations in the training loop (data loading, metric logging).

```{python}
#| label: fleet-amdahl-example
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET AMDAHL EXAMPLE
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-fleet-foundations-amdahls-fleet worked example
# │
# │ Goal: Compute Amdahl's Law speedup at 32/256/1024/8192 GPUs for 10% serial fraction.
# │ Show: Speedup values and the Amdahl ceiling for inline prose.
# │ How: calc_amdahls_speedup() from formulas.py; check() for invariants.
# │
# │ Imports: mlsys.formulas (calc_amdahls_speedup), mlsys.formatting (fmt, check)
# │ Exports: s_fleet_pct_str, max_speedup_str, su_32_str, su_256_str, su_1024_str, su_8192_str
# └─────────────────────────────────────────────────────────────────────────────

class FleetAmdahlExample:
    """Amdahl's Law speedup at 32/256/1024/8192 GPUs for 10% serial fraction."""

    # ┌── 1. LOAD (Constants) ──────────────────────────────────────────────
    s_fleet  = 0.10
    n_values = [32, 256, 1024, 8192]

    # ┌── 2. EXECUTE (The Compute) ────────────────────────────────────────
    speedups = {}
    for _n in n_values:
        speedups[_n] = calc_amdahls_speedup(1 - s_fleet, _n)

    max_speedup = 1 / s_fleet

    # ┌── 3. GUARD (Invariants) ──────────────────────────────────────────
    check(speedups[8192] < max_speedup, "Speedup must be below Amdahl limit")
    check(max_speedup == 10.0, "Max speedup for 10% serial fraction must be 10x")

    # ┌── 4. OUTPUT (Formatting) ─────────────────────────────────────────────
    s_fleet_pct_str = "10"
    max_speedup_str = fmt(max_speedup, precision=0, commas=False)
    su_32_str       = fmt(speedups[32],   precision=1, commas=False)
    su_256_str      = fmt(speedups[256],  precision=1, commas=False)
    su_1024_str     = fmt(speedups[1024], precision=1, commas=False)
    su_8192_str     = fmt(speedups[8192], precision=1, commas=False)
```

To see the fleet-scale implications, consider a training workload where `{python} FleetAmdahlExample.s_fleet_pct_str`% of wall-clock time is spent in synchronization, communication, and other serial overhead. Amdahl's Law gives the following speedups:

- With 32 GPUs: `{python} FleetAmdahlExample.su_32_str`$\times$ speedup (good efficiency)
- With 256 GPUs: `{python} FleetAmdahlExample.su_256_str`$\times$ speedup (diminishing returns begin)
- With 1,024 GPUs: `{python} FleetAmdahlExample.su_1024_str`$\times$ speedup (approaching the ceiling)
- With 8,192 GPUs: `{python} FleetAmdahlExample.su_8192_str`$\times$ speedup (nearly at the Amdahl limit)
- With $N \to \infty$: capped at `{python} FleetAmdahlExample.max_speedup_str`$\times$

With just `{python} FleetAmdahlExample.s_fleet_pct_str`% serial overhead, no amount of hardware can deliver more than `{python} FleetAmdahlExample.max_speedup_str`$\times$ speedup on this fixed workload. This is why fleet-scale training does not simply add more GPUs to the same problem---it scales the problem (weak scaling) to keep the serial fraction small relative to the total work.

::: {.callout-important title="The Compound Overhead Trap"}
The `{python} FleetAmdahlExample.s_fleet_pct_str`% serial fraction above is optimistic. In practice, fleet-scale serial overhead accumulates from multiple sources: `{python} FF.oh_pipeline`% pipeline bubbles + `{python} FF.oh_checkpoint`% checkpointing + `{python} FF.oh_failure`% failure recovery + `{python} FF.oh_maintenance`% maintenance. These are not all strictly serial in the Amdahl sense---some overlap with computation---but they illustrate how quickly small per-category overheads compound into significant throughput loss.
:::

### The Communication-Computation Ratio {#sec-fleet-foundations-comm-compute-ratio}

The fundamental question for any distributed training strategy is: *does the computation between synchronization points take long enough to hide the communication?* The **communication-computation ratio** ($\rho$) answers this directly:

$$ \rho = \frac{T_{\text{comm}}}{T_{\text{comp}}} $$ {#eq-comm-compute-ratio}

When $\rho < 1$, computation dominates and communication can be overlapped. When $\rho > 1$, the system is **communication-bound**---GPUs spend more time waiting for data than computing on it.

```{python}
#| label: fleet-comm-comp-ratio
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET COMM-COMPUTE RATIO
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-fleet-foundations-comm-compute-ratio worked example (@tbl-fleet-comm-comp)
# │
# │ Goal: Compute ρ = T_comm / T_comp for 3 scenarios: 7B DP, 350M DP, tensor-parallel.
# │ Show: AllReduce times in ms and ρ ratios for each scenario; ~0.1 for DP 7B, ~3 for DP 350M.
# │ How: calc_ring_allreduce_time() with IB NDR params; NVLink BW for tensor-parallel.
# │
# │ Imports: mlsys.constants (INFINIBAND_NDR_BW_GBS, IB_NDR_LATENCY_US, NVLINK_H100_BW, GB, second)
# │ Exports: ar_7b_ms_str, rho_7b_str, ar_350m_ms_str, rho_350m_str, rho_tp_str
# └─────────────────────────────────────────────────────────────────────────────

class FleetCommCompRatio:
    """Communication-to-computation ratio ρ for three parallelism scenarios."""

    # ── SCENARIO 1: Data parallelism, large model ──────────────────────────
    # 7B model, 256 GPUs, IB NDR
    grad_bytes_7b = 7e9 * 2           # 7B params * 2 bytes (BF16 gradients)
    allreduce_time_7b = calc_ring_allreduce_time(
        message_bytes=grad_bytes_7b,
        n_gpus=256,
        bandwidth_bytes_s=INFINIBAND_NDR_BW_GBS * 1e9,
        latency_s=IB_NDR_LATENCY_US * 1e-6
    )  # Quantity[second]

    comp_time_7b = 0.050  # 50 ms (seconds)
    rho_7b = allreduce_time_7b.m_as(ureg.second) / comp_time_7b

    # ── SCENARIO 2: Data parallelism, small model ──────────────────────────
    # 350M model, 256 GPUs, IB NDR
    grad_bytes_350m = 350e6 * 2       # 350M params * 2 bytes
    allreduce_time_350m = calc_ring_allreduce_time(
        message_bytes=grad_bytes_350m,
        n_gpus=256,
        bandwidth_bytes_s=INFINIBAND_NDR_BW_GBS * 1e9,
        latency_s=IB_NDR_LATENCY_US * 1e-6
    )  # Quantity[second]
    comp_time_350m = 0.005  # 5 ms (seconds, smaller model)
    rho_350m = allreduce_time_350m.m_as(ureg.second) / comp_time_350m

    # ── SCENARIO 3: Tensor parallelism, within node ────────────────────────
    # Activation transfer: 8 GPUs, NVLink, ~16 MB per layer
    act_bytes = 16e6  # 16 MB
    act_transfer_time = act_bytes / (NVLINK_H100_BW.m_as(GB / second) * 1e9)
    comp_time_layer = 0.001  # 1 ms per layer
    rho_tp = act_transfer_time / comp_time_layer

    # ── INVARIANTS ──────────────────────────────────────────────────────────
    check(rho_7b > 0.1, "7B comm ratio must be non-trivial")
    check(rho_350m > 0.01, "350M comm ratio must be non-trivial")

    # ── OUTPUTS ─────────────────────────────────────────────────────────────
    ar_7b_ms_str   = fmt(allreduce_time_7b.m_as(ureg.millisecond),   precision=1, commas=False)
    rho_7b_str     = fmt(rho_7b,   precision=2, commas=False)
    ar_350m_ms_str = fmt(allreduce_time_350m.m_as(ureg.millisecond), precision=1, commas=False)
    rho_350m_str   = fmt(rho_350m, precision=1, commas=False)
    rho_tp_str     = fmt(rho_tp,   precision=3, commas=False)
```

@tbl-fleet-comm-comp shows the ratio for three representative scenarios. The contrast between them reveals why parallelism strategy must match the workload.

| **Scenario**                            | **$T_{\text{comm}}$**                                                   | **$T_{\text{comp}}$** | **$\rho$**                                 |
|:----------------------------------------|:------------------------------------------------------------------------|:----------------------|:-------------------------------------------|
| **Data parallel, 7B model, 256 GPUs**   | `{python} FleetCommCompRatio.ar_7b_ms_str` ms (AllReduce over IB NDR)   | ~50 ms (fwd+bwd)      | `{python} FleetCommCompRatio.rho_7b_str`   |
| **Data parallel, 350M model, 256 GPUs** | `{python} FleetCommCompRatio.ar_350m_ms_str` ms (AllReduce over IB NDR) | ~5 ms (fwd+bwd)       | `{python} FleetCommCompRatio.rho_350m_str` |
| **Tensor parallel, 8 GPUs (NVLink)**    | ~0.02 ms (activation over NVLink)                                       | ~1 ms (one layer)     | `{python} FleetCommCompRatio.rho_tp_str`   |

: **Communication-Computation Ratio ($\rho$)**: When $\rho \ll 1$, communication can be fully overlapped with computation. When $\rho \gg 1$, the system is communication-bound and GPUs sit idle waiting for data. Tensor parallelism within a node benefits from NVLink's high bandwidth, keeping $\rho$ small. {#tbl-fleet-comm-comp}

The 7B model on 256 GPUs achieves $\rho =$ `{python} FleetCommCompRatio.rho_7b_str`---communication takes about `{python} fmt(FleetCommCompRatio.rho_7b * 100, precision=0)`% as long as computation, which can be partially overlapped. The 350M model on the same cluster has $\rho =$ `{python} FleetCommCompRatio.rho_350m_str`---communication dominates, making this configuration communication-bound. The solution is either to use fewer GPUs (reduce $N$ in the AllReduce) or to increase the computation per step (larger batch size, gradient accumulation).

Tensor parallelism within a node achieves $\rho =$ `{python} FleetCommCompRatio.rho_tp_str`, confirming that NVLink bandwidth is sufficient to keep intra-node parallelism compute-bound. This is the quantitative reason why tensor parallelism is confined within nodes while data parallelism spans across them.

### Weak Scaling at Fleet Scale {#sec-fleet-foundations-weak-scaling-fleet}

Amdahl's Law paints a pessimistic picture because it assumes a fixed problem size. In practice, fleet-scale training often follows **weak scaling**: the problem size (tokens, data, model parameters) grows proportionally with the number of GPUs. Gustafson's Law captures this more optimistic view.

The key insight for fleet-scale ML is that weak scaling is not just a mathematical convenience---it reflects reality. Engineers do not use 8,192 GPUs to train a 7B model faster; they use them to train a 70B or 700B model in reasonable time. As the model grows, the computation per step grows quadratically (more parameters, larger activations), while communication grows linearly (gradient size scales with parameter count). This means the communication-computation ratio $\rho$ actually *improves* for larger models on larger clusters---a property sometimes called **strong scaling in weak scaling's clothing**.

```{python}
#| label: fleet-effective-flops
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLEET EFFECTIVE FLOPS — 1,024-GPU COMPOUND LOSS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-fleet-foundations-weak-scaling-fleet compound-loss callout
# │
# │ Goal: Format peak and effective FLOPS; build display math (vol1 pattern:
# │       no {python} inside $...$ or $$...$$).
# │ Show: FleetEffectiveFlops.effective_eq_math display equation; peak, eff_pct etc. in prose.
# │ How: fmt() on FF values; build md() display math from formatted strings.
# │
# │ Imports: mlsys.formatting (fmt, fmt_percent, md)
# │ Exports: FleetEffectiveFlops.peak, FleetEffectiveFlops.eff_pct, FleetEffectiveFlops.goodput_pct,
# │          FleetEffectiveFlops.mfu_pct, FleetEffectiveFlops.scaling_pct, FleetEffectiveFlops.effective_eq_math
# └─────────────────────────────────────────────────────────────────────────────

class FleetEffectiveFlops:
    _peak_str     = fmt(FF.peak_1024, precision=0)
    _eff_str      = fmt(FF.eff_flops_1024, precision=0)
    _mfu_fmt      = fmt(MFU_TRAINING_HIGH, precision=2, commas=False)
    _scaling_fmt  = fmt(SCALING_EFF_1024GPU, precision=2, commas=False)
    _goodput_fmt  = fmt(FF.goodput_ratio, precision=2, commas=False)

    peak          = _peak_str
    # Effective % of peak (0.50 × 0.50 × 0.77 ≈ 19.25%); use fmt_percent to avoid display bugs
    eff_pct       = fmt_percent(FF.eff_fraction, precision=1)
    goodput_pct   = fmt_percent(FF.goodput_ratio, precision=0, commas=False)
    mfu_pct       = fmt_percent(MFU_TRAINING_HIGH, precision=0, commas=False)
    scaling_pct   = fmt_percent(SCALING_EFF_1024GPU, precision=0, commas=False)
    loss_pct      = fmt_percent(1 - FF.eff_fraction, precision=0)

    effective_eq_math = md(
        f"$$= {_peak_str} \\times {_mfu_fmt} \\times {_scaling_fmt} \\times {_goodput_fmt} "
        f"\\approx {_eff_str} \\text{{ TFLOPS}}$$"
    )
```

::: {.callout-perspective title="The Compound Loss of Fleet Utilization"}

A 1,024-GPU H100 cluster has a peak aggregate throughput of `{python} FleetEffectiveFlops.peak` TFLOPS. After the three multiplicative losses, the effective throughput is:

$$\text{Effective} = \text{Peak} \times \text{MFU} \times \eta_{\text{scaling}} \times \text{Goodput Ratio}$$

`{python} FleetEffectiveFlops.effective_eq_math`

The cluster delivers `{python} FleetEffectiveFlops.eff_pct`% of its peak FLOPS as useful training work. The remaining `{python} FleetEffectiveFlops.loss_pct`% is consumed by hardware underutilization (`{python} FleetEffectiveFlops.mfu_pct`% MFU), communication overhead (`{python} FleetEffectiveFlops.scaling_pct`% scaling efficiency), and operational losses (`{python} FleetEffectiveFlops.goodput_pct`% goodput ratio).

This is not a failure of engineering---it is the physics of fleet-scale computation. Every additional GPU adds less marginal useful work, but the total throughput still far exceeds what a smaller cluster could achieve. The goal is not to reach 100% utilization; the goal is to deliver trained models faster than any smaller configuration could.

:::

## Thermal and Power Physics {#sec-fleet-foundations-thermal-power}

Compute performance ultimately converts to heat, and heat must be removed. At fleet scale, thermal and power constraints are not secondary concerns---they determine where you can build, how densely you can pack accelerators, and what your operating costs will be. A cluster that is architecturally sound but thermally infeasible cannot be built.

::: {.callout-tip title="Why This Matters"}
Your cluster design calls for 10,000 H100 GPUs at 700 W each---7 MW of IT load. With a PUE of 1.4, the facility must deliver 9.8 MW total. This is the electrical load of a small town. Before optimizing software, you must ensure the physics of power delivery and heat removal are feasible.
:::

### Power Density Wall {#sec-fleet-foundations-power-density}

The shift from traditional datacenter workloads to AI training has created a **power density crisis**. @tbl-fleet-power-density-fleet quantifies the gap.

| **Configuration**             | **Power per Rack**            | **Cooling Implication**                   |
|:------------------------------|:------------------------------|:------------------------------------------|
| **Traditional datacenter**    | `{python} FF.rack_trad` kW    | Standard air cooling sufficient           |
| **AI cluster (current gen)**  | `{python} FF.rack_ai` kW      | Liquid cooling required                   |
| **AI cluster (high-density)** | `{python} FF.rack_ai_high` kW | Direct-to-chip liquid cooling required    |
| **Air cooling limit**         | ~`{python} FF.air_limit` kW   | Physics ceiling for forced-air convection |

: **Power Density: Traditional vs. AI Workloads**: AI racks consume roughly 5.8$\times$ the power of traditional racks. Air cooling physically cannot remove heat fast enough above ~30 kW per rack, making liquid cooling mandatory for modern AI clusters. {#tbl-fleet-power-density-fleet}

The `{python} fmt(FF.rack_ratio, precision=1, commas=False)`$\times$ increase in rack power density between traditional and AI workloads is not merely an engineering challenge---it represents a fundamental constraint on facility design. Existing datacenters built for `{python} FF.rack_trad` kW racks cannot simply be "retrofitted" with AI hardware. The electrical distribution, cooling infrastructure, and floor loading must all be redesigned. This is why new AI-focused datacenters are being built from the ground up with liquid cooling as the baseline assumption.

### The Energy Hierarchy at Scale {#sec-fleet-foundations-energy-hierarchy}

Power Usage Effectiveness (PUE) measures how efficiently a datacenter converts electrical power into useful IT computation. A PUE of 1.0 would mean every watt goes to computing; values above 1.0 represent overhead for cooling, power conversion, lighting, and other facility systems.

@tbl-fleet-pue-fleet compares PUE across datacenter generations and cooling technologies.

| **Datacenter Type**             | **PUE**                                                   | **Overhead per MW of IT Load** |
|:--------------------------------|:----------------------------------------------------------|:-------------------------------|
| **Liquid-cooled AI datacenter** | `{python} fmt(FF.pue_liquid, precision=2, commas=False)`  | 60 kW cooling + infrastructure |
| **Best-in-class air-cooled**    | `{python} fmt(FF.pue_air, precision=2, commas=False)`     | 120 kW overhead                |
| **Industry average**            | `{python} fmt(FF.pue_typical, precision=2, commas=False)` | 400 kW overhead                |
| **Legacy enterprise**           | `{python} fmt(FF.pue_legacy, precision=2, commas=False)`  | 580 kW overhead                |

: **Power Usage Effectiveness (PUE)**: Lower is better. Liquid cooling achieves PUE near 1.0 because it removes heat directly from the chip without the intermediate step of heating air. The gap between legacy (1.58) and liquid-cooled (1.06) represents a 49% reduction in total facility power. {#tbl-fleet-pue-fleet}

For fleet-scale cost calculations, PUE directly multiplies the electricity bill. A 10 MW IT load in a legacy datacenter (PUE 1.58) requires 15.8 MW total, while the same load in a liquid-cooled facility (PUE 1.06) requires only 10.6 MW---a savings of 5.2 MW. At typical commercial electricity rates, this difference translates to millions of dollars per year. @sec-sustainable-ai covers the full sustainability implications.

---

## Fallacies and Pitfalls {#sec-fleet-foundations-fallacies-pitfalls}

::: {.callout-warning title="Fallacy: Adding more GPUs always makes training faster."}
Amdahl's Law caps speedup at $1/s$ for a fixed workload. At `{python} FleetAmdahlExample.s_fleet_pct_str`% serial overhead, no fleet larger than ~`{python} FleetAmdahlExample.max_speedup_str`$\times$ the baseline delivers meaningful additional speedup. Beyond the Amdahl limit, every additional GPU contributes nearly zero marginal throughput while increasing failure probability and communication overhead. The solution is weak scaling: grow the problem to match the hardware.
:::

::: {.callout-warning title="Fallacy: InfiniBand is just fast Ethernet."}
InfiniBand and Ethernet differ architecturally, not just in speed. InfiniBand provides kernel-bypass (RDMA), hardware-managed flow control, and credit-based congestion avoidance. Ethernet relies on software-managed TCP/IP stacks with orders-of-magnitude higher latency. RoCE (RDMA over Converged Ethernet) bridges some of this gap, but requires lossless Ethernet configuration (PFC, ECN) that introduces its own failure modes. The choice between InfiniBand and Ethernet is a system architecture decision, not a bandwidth selection.
:::

::: {.callout-warning title="Pitfall: Ignoring failure probability at scale."}
A single GPU with a `{python} fmt(GPU_MTTF_HOURS, precision=0)`-hour MTTF seems extremely reliable---that is nearly 6 years. But at `{python} fmt(FF.cl_large, precision=0)` GPUs, the cluster MTBF drops to `{python} FleetMtbfTable.mtbf_8192` hours (`{python} FleetMtbfTable.mtbf_8192_min` minutes). The probability of at least one failure in a 24-hour training window is `{python} FleetMtbfTable.pfail_8192`%. Designing a training system without automated failure recovery at this scale guarantees that every long-running job will fail and require manual intervention.
:::

::: {.callout-warning title="Pitfall: Optimizing MFU without considering scaling efficiency."}
A team achieves 50% MFU on a single node---excellent performance. They scale to 1,024 GPUs and expect `{python} fmt(1024 * 0.5, precision=0)`$\times$ throughput (512 GPU-equivalents of useful work). But scaling efficiency at 1,024 GPUs is ~`{python} FF.eff_1024`%, so actual throughput is only `{python} fmt(1024 * 0.5 * SCALING_EFF_1024GPU, precision=0)` GPU-equivalents---half the expected value. MFU and scaling efficiency are independent multiplicative factors; optimizing one without measuring the other leads to incorrect capacity estimates.
:::

::: {.callout-warning title="Fallacy: Air cooling is sufficient for AI workloads."}
Air cooling physically cannot remove heat fast enough above ~`{python} FF.air_limit` kW per rack. Current-generation AI racks consume `{python} FF.rack_ai`--`{python} FF.rack_ai_high` kW, well beyond this limit. Attempting to air-cool an AI cluster leads to thermal throttling, reduced clock speeds, and ultimately component failure. Liquid cooling is not an optimization for AI clusters---it is a physical requirement.
:::

## Summary {.unnumbered}

:::: {.callout-takeaways title="Key Takeaways"}

- **Three paradigms, one hybrid**: ML fleets combine HPC's tight coupling (for training) with WSC's elastic fault tolerance (for inference), creating a new architectural paradigm that cannot adopt either predecessor's patterns wholesale.
- **The 18$\times$ bandwidth gap**: NVLink provides ~`{python} FleetCommNumbers.nvlink_to_ib`$\times$ more bandwidth than InfiniBand. This invariant ratio determines parallelism placement: tensor parallelism within nodes, data parallelism across nodes.
- **Failure is certain at scale**: Cluster MTBF scales as $1/N$. At 8,192 GPUs, expect a failure every `{python} FleetMtbfTable.mtbf_8192_min` minutes. Fault tolerance is not optional---it is a prerequisite for completing any fleet-scale training job.
- **Compound utilization loss**: After MFU (~`{python} FleetEffectiveFlops.mfu_pct`%), scaling efficiency (~`{python} FleetEffectiveFlops.scaling_pct`%), and operational overhead (~`{python} FleetEffectiveFlops.goodput_pct`% goodput) compound, a 1,024-GPU cluster delivers ~`{python} FleetEffectiveFlops.eff_pct`% of peak FLOPS as useful work.
- **Power density demands liquid cooling**: AI racks consume `{python} fmt(FF.rack_ratio, precision=1, commas=False)`$\times$ the power of traditional racks. Air cooling fails above ~`{python} FF.air_limit` kW per rack, making liquid cooling a physical requirement for modern AI clusters.
- **Communication-computation ratio ($\rho$) governs scaling strategy**: When $\rho > 1$, GPUs are idle waiting for data---reduce parallelism or increase computation per step. When $\rho \ll 1$, communication can be fully overlapped.
- **Weak scaling is the fleet-scale paradigm**: Engineers do not use more GPUs to solve the same problem faster (strong scaling); they use more GPUs to solve larger problems in reasonable time. This keeps the serial fraction small and utilization high.

::::
