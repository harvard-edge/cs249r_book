# Appendix A: Foundations Review {#sec-appendix-systems}

## Purpose {.unnumbered}

_What foundational knowledge is required to understand machine learning systems at scale?_

Large-scale ML infrastructure is a hybrid of high-performance computing (tight coupling, bandwidth-hungry collectives) and warehouse-scale computing (service reliability, tail latency, elastic fleets). This appendix provides a compact reference for these architectural paradigms and a recap of the machine learning system fundamentals --- hardware hierarchies, the Iron Law, and the D·A·M taxonomy --- that underpin the scale-focused topics in this book.

## How to Use This Appendix {.unnumbered}

Use this appendix as a **Foundational Review** when you need to quickly refresh your intuition on single-machine bottlenecks or map a distributed workload to its architectural lineage.

## The Three System Paradigms {#sec-appendix-systems-three-paradigms}

Machine Learning Systems at scale represent a convergence of two distinct computing lineages: the raw computational power of High-Performance Computing (HPC) and the fault-tolerant scalability of Warehouse-Scale Computing (WSC). 

### 1. High-Performance Computing (HPC) {#sec-appendix-systems-hpc}
*The lineage of Supercomputers.*

*   **Philosophy**: "Maximize FLOPs per second."
*   **Fault Tolerance**: **Checkpoint/Restart**. If one node fails, the entire job stops, rolls back to the last checkpoint, and restarts.
*   **Scheduling**: Batch scheduling (Slurm). Jobs request rigid resource shapes (e.g., "512 nodes for 24 hours").

### 2. Warehouse-Scale Computing (WSC) {#sec-appendix-systems-wsc}
*The lineage of the Web.*

*   **Philosophy**: "Maximize Requests per second (QPS)."
*   **Fault Tolerance**: **Redundancy**. If one node fails, the load balancer reroutes traffic to another replica. The user never notices.
*   **Scheduling**: Service orchestration (Kubernetes, Borg). Jobs are dynamic, elastic, and bin-packed.

### 3. The Machine Learning Fleet {#sec-appendix-systems-ml-fleet}
*The Hybrid Architecture.*

ML Systems require the **throughput** of HPC (to train massive models) but must operate on the **scale** and **unreliability** of WSC.

| **Feature**    | **HPC (Supercomputer)** | **WSC (Web Cloud)**   | **ML Fleet (AI Cluster)**              |
|:---------------|:------------------------|:----------------------|:---------------------------------------|
| **Coupling**   | Tight (MPI)             | Loose (RPC/HTTP)      | Hybrid (NCCL + RPC)                    |
| **State**      | Stateful (RAM)          | Stateless (DB-backed) | Semi-Stateful (Checkpoints + KV Cache) |
| **Network**    | Latency-optimized       | Bandwidth-optimized   | Bisection-Bandwidth critical           |
| **Bottleneck** | Compute (FLOPs)         | I/O (Disk/Net)        | Memory Bandwidth (HBM)                 |

## Recap of ML System Fundamentals {#sec-appendix-systems-vol1-recap}

Distributed systems reasoning depends on a precise understanding of the single-machine constraints that they attempt to overcome.

### Hardware Foundations {#sec-appendix-systems-hardware}
*   **GPUs**: Parallel tensor arithmetic engines optimized for Matrix Multiplication. They hide memory latency by maintaining thousands of threads in flight (SIMT).
*   **High Bandwidth Memory (HBM)**: 3D-stacked DRAM that provides the terabytes-per-second bandwidth required to feed parallel arithmetic units.
*   **Memory Hierarchy**: The fundamental gap in speed and capacity between **Registers** (few MB, infinite speed), **SRAM** (tens of MB, TB/s), **HBM** (tens of GB, TB/s), and **NVMe** (TBs, GB/s).

### The Iron Law of ML Systems {#sec-appendix-systems-iron-law}
The time $T$ required for an ML operation is the sum of three terms representing the system's physical limits:
$$T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak}} + L_{lat}$$
Where:
- $D_{vol}$: Volume of data moved.
- $BW$: System bandwidth.
- $O$: Total operations (FLOPs).
- $R_{peak}$: Peak throughput (FLOPS).
- $L_{lat}$: Fixed latency (speed of light, software overhead).

### Optimization Techniques {#sec-appendix-systems-optimization}
*   **Quantization**: Reducing numerical precision (FP32 $\to$ FP16 $\to$ INT8) to shrink $D_{vol}$ and increase effective $BW$.
*   **Pruning/Sparsity**: Removing weights to reduce both $D_{vol}$ and $O$.
*   **Operator Fusion**: Combining operations to keep data in fast SRAM, minimizing trips to HBM.
