# Appendix A: Review of Machine Learning Systems Foundations {#sec-appendix-systems}

## Purpose {.unnumbered}

_What foundational mechanics govern the behavior of machine learning systems on individual nodes?_

Machine learning systems at scale do not operate in a vacuum; they are an extension of the physical and algorithmic constraints that govern single-machine computation. This appendix provides a rigorous review of those foundations—spanning the memory hierarchy, the mechanics of backpropagation, and the "Iron Law" of performance. We treat these not as introductory topics, but as the first-order constraints that determine which distributed architectures are viable and which are physically impossible.

## A.1 The Three Computing Paradigms {#sec-appendix-systems-paradigms}

Large-scale ML infrastructure represents a convergence of three distinct computing lineages. Understanding where ML inherits its assumptions is critical for diagnosing system failures.

| **Feature**    | **HPC (Supercomputer)** | **WSC (Warehouse-Scale)** | **ML Fleet (AI Cluster)**              |
|:---------------|:------------------------|:--------------------------|:---------------------------------------|
| **Lineage**    | Scientific Simulation   | Web Services (Search/Ads) | **Distributed Intelligence**           |
| **Coupling**   | Tight (Synchronous)     | Loose (Asynchronous)      | **Hybrid (BSP + Elastic)**             |
| **State**      | Stateful (In-Memory)    | Stateless (Database)      | **Semi-Stateful (Weights/KV)**         |
| **Faults**     | Stop-the-world          | Fail-over/Redundancy      | **Elastic Recovery**                   |
| **Bottleneck** | Compute (FLOPS)         | I/O (Network/Disk)        | **Memory Bandwidth (HBM)**             |

## A.2 Machine Learning as a Systems Workload {#sec-appendix-systems-workload}

At the systems level, deep learning is characterized by two distinct computational phases: the **Forward Pass** (Inference) and the **Backward Pass** (Training).

### The Silicon Contract (GEMM)
The fundamental unit of ML computation is the **General Matrix Multiplication (GEMM)**. Modern accelerators are designed around the "Silicon Contract": if the software can express its logic as large, dense matrix operations, the hardware guarantees near-peak utilization through specialized systolic arrays or tensor cores. Small, irregular, or sparse operations violate this contract, falling off the "Performance Cliff" into a regime dominated by instruction overhead rather than math.

### The Memory Wave of Backpropagation
Training introduces a unique memory challenge known as the **Activation Wave**. During the forward pass, every intermediate activation must be stored in memory to be used later during the backward pass to compute gradients. This creates a memory footprint that scales linearly with model depth and batch size, often exceeding the storage available for model weights by 3--5$\times$. This "Memory Wall" is the primary driver for distributed strategies like **Activation Checkpointing** and **ZeRO**.

## A.3 The Single-Node Memory Hierarchy {#sec-appendix-systems-memory}

The most significant constraint in ML systems is the gap between logic speed and data delivery. We quantify this through the memory hierarchy.

| **Tier**       | **Technology** | **Typical Capacity** | **Typical Bandwidth** | **Latency (approx)** |
|:---------------|:---------------|:---------------------|:----------------------|:---------------------|
| **Compute**    | Registers      | ~20 MB               | ~300 TB/s             | < 1 ns               |
| **Buffer**     | L1/SRAM        | ~40 MB               | ~20 TB/s              | ~10 ns               |
| **Primary**    | HBM3/DRAM      | ~80 GB               | ~3 TB/s               | ~100 ns              |
| **Secondary**  | NVMe/Flash     | ~4 TB                | ~10 GB/s              | ~10,000 ns           |

: **The Single-Node Memory Hierarchy**. The 1,000$\times$ bandwidth gap between SRAM and HBM, and the 300$\times$ gap between HBM and NVMe, dictate the "Arithmetic Intensity" required to saturate an accelerator. {#tbl-appendix-memory-hierarchy}

## A.4 The Iron Law of ML Systems {#sec-appendix-systems-iron-law}

To reason quantitatively about any ML system, we use the **Iron Law**\index{Iron Law!definition}, which decomposes the time $T$ required for an operation into three physical terms:

$$T = \underbrace{\frac{D_{\text{vol}}}{BW}}_{\text{Data Movement}} + \underbrace{\frac{O}{R_{\text{peak}}}}_{\text{Calculation}} + \underbrace{L_{\text{lat}}}_{\text{Fixed Latency}}$$

Every optimization technique in this book targets one of these terms:
1.  **Quantization** (FP16 $\to$ INT8) reduces $D_{\text{vol}}$ to unblock the Bandwidth wall.
2.  **Operator Fusion** keeps data in SRAM to eliminate $D_{\text{vol}}$ trips to HBM.
3.  **Distributed Parallelism** divides $O$ across $N$ processors to increase effective $R_{\text{peak}}$.

## A.5 The D·A·M Taxonomy {#sec-appendix-systems-dam}

We organize all system design decisions into the **Data · Algorithm · Machine (D·A·M)** taxonomy\index{D·A·M Taxonomy}. 

*   **Data (D)**: The volume, velocity, and quality of information.
*   **Algorithm (A)**: The mathematical structure (e.g., Transformers, CNNs) and the optimization logic.
*   **Machine (M)**: The physical substrate (Accelerators, Interconnects, Storage).

The central thesis of ML systems engineering is that these three vertices are **inseparable**. A change in the Machine (e.g., switching from HBM2 to HBM3) reshapes which Algorithms are efficient, which in turn determines how much Data can be processed within a given power budget.
