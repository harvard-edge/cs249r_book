# Principles of Responsibility {.unnumbered}

These principles govern the societal impact of AI systems. They define the limits of what algorithms can solve and the necessity of human governance. We treat these not as "ethics" (optional) but as "physics" (constraints).

::: {.callout-note icon=false title="The Fairness Impossibility Law"}
**The Law**: It is mathematically impossible to simultaneously satisfy **Calibration**, **Equalized Odds**, and **Demographic Parity** when base rates differ between groups.
$$ P(Y=1|A=a) \neq P(Y=1|A=b) \implies \text{Trade-off Required} $$

**The Engineering Implication**:
Fairness is a constraint satisfaction problem with no global optimum. "Unbiased" is mathematically invalid. Engineers must treat fairness metrics like latency budgets: explicit trade-offs chosen by stakeholders, enforced by the system, and monitored for violation.
:::

::: {.callout-note icon=false title="The Sociotechnical Feedback Invariant"}
**The Law**: Deployed models shape the environment they operate in. The probability distribution of future data $P_{t+1}(X)$ is a function of the model's past decisions $f_t(X)$.
$$ P_{t+1}(X) = g(P_t(X), f_t(X)) $$

**The Engineering Implication**:
Systems require **Closed-Loop Governance**. A model that maximizes accuracy on static test data can still destroy its own ecosystem (e.g., recommender systems polarizing users, predictive policing reinforcing bias). Reliability requires modeling the *feedback loop*, not just the *feed-forward inference*.
:::

::: {.callout-note icon=false title="The Offline-First Design Invariant"}
**The Law**: In resource-constrained deployments (AI for Good), connectivity is an intermittent enhancement, not a reliable dependency.

**The Engineering Implication**:
Architectures for the "Edge" must be fully functional offline. Critical logic and inference must reside on-device; the cloud should be used only for periodic synchronization and heavy lifting updates, never for the critical path of the application.
:::