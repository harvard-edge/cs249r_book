---
---

# Optimization at Scale {#sec-optimization-at-scale}

::: {layout-narrow}
:::.column-margin
_Gemini Pro 3 Prompt: A technical visualization of system-level machine learning optimization. The scene depicts a tiered inference architecture: 'Pre-fill' nodes (dense compute) handing off state to 'Decode' nodes (high memory bandwidth). Visual elements include a speculative decoding tree expanding potential future tokens, and a shared memory pool representing PagedAttention reducing fragmentation. The style is schematic and precise, emphasizing the flow of data and state between specialized components._
:::

\noindent
![](images/png/cover_optimization.png){fig-alt=""}

:::

## Purpose {.unnumbered}

_How do we squeeze maximum throughput and minimum latency from massive models when single-device optimizations are no longer enough?_

Foundational optimization techniques like quantization and pruning are typically applied to individual models. At the scale of modern production systems—serving billions of tokens per day with models exceeding device memory—optimization shifts from a model-level concern to a system-level architecture. We are no longer just optimizing a matrix multiplication; we are optimizing the flow of petabytes of state across a distributed fleet.

This chapter examines optimizations that only emerge at scale: disaggregated serving architectures that split compute-bound and memory-bound phases, memory management techniques like PagedAttention that virtually address transformer state, and speculative execution strategies that trade abundant compute for lower latency.

::: {.callout-tip title="Learning Objectives"}
- Design disaggregated serving architectures that decouple pre-fill (compute-bound) from decode (memory-bound) phases to maximize cluster utilization.
- Implement advanced memory management techniques like PagedAttention and Prefix Caching to eliminate fragmentation and enable massive batch sizes.
- Evaluate speculative decoding strategies, calculating the optimal draft/target model ratio based on acceptance rates and hardware latencies.
- Architect synthetic data pipelines for scale-driven distillation, using teacher models to generate training curricula for efficient student deployment.
:::

## Disaggregated Serving: Splitting the Workload {#sec-disaggregated-serving}

LLM inference consists of two distinct phases with different computational characteristics, requiring different batching strategies within the same request. @tbl-prefill-decode contrasts these phases:

**Prefill phase**: Process the entire input prompt in parallel. Computation scales with prompt length. Memory access pattern is compute-bound (high arithmetic intensity[^fn-arithmetic-intensity]).

**Decode phase**: Generate output tokens one at a time. Each token requires loading entire model weights. Memory access pattern is bandwidth-bound (low arithmetic intensity).

[^fn-arithmetic-intensity]: **Arithmetic intensity**: The ratio of compute operations to memory accesses, measured in FLOPs per byte. Prefill achieves 100+ FLOPs/byte (compute-bound), while decode achieves 1-10 FLOPs/byte (memory-bound). This 10-100x difference explains why prefill and decode require fundamentally different optimization strategies.

+-------------+-------------------+-------------------+----------------+-------------------+
| **Phase**   | **Computation**   | **Memory Access** | **Bottleneck** | **Optimal Batch** |
+:============+==================:+:==================+:===============+==================:+
| **Prefill** | O(prompt_length²) | Weight loading    | Compute        | Small (1–8)       |
| **Decode**  | O(1) per token    | Weight loading    | Bandwidth      | Large (100s)      |
+-------------+-------------------+-------------------+----------------+-------------------+

: **Prefill vs Decode Characteristics**. The two phases have opposite optimization requirements. {#tbl-prefill-decode}

This dichotomy creates a scheduling challenge: prefill operations are long-running and compute-intensive, while decode operations are short and bandwidth-limited. Mixing them in the same batch can cause interference. This creates the **Static Power Waste** problem analyzed in @sec-sustainable-ai, where low-utilization decode steps dominate energy consumption.

**Chunked prefill**[^fn-chunked-prefill] addresses this by breaking long prompts into fixed-size chunks that interleave with decode operations:

[^fn-chunked-prefill]: **Chunked prefill motivation**: Without chunking, a 32K-token prompt blocks decode for 5-30 seconds while prefill completes. Chunking divides the prompt into 256–1024 token chunks processed between decode iterations, bounding decode latency at the cost of slightly longer prefill time. This trade-off favors interactive applications where decode responsiveness matters more than total completion time.

$$\text{Chunk latency} = \frac{\text{Chunk size}}{\text{Prefill throughput}}$$

With chunk size chosen to match decode iteration time, prefill and decode can share GPU resources without decode latency spikes.

**Prefill-decode disaggregation** takes this further by running prefill and decode on separate GPU pools:

- **Prefill pool**: Optimized for compute intensity (e.g., H100 nodes with high TFLOPS) using large batch sizes to maximize throughput.
- **Decode pool**: Optimized for memory bandwidth (e.g., nodes with maximum HBM capacity or specialized High Bandwidth Flash as discussed in @sec-storage) to handle thousands of concurrent autoregressive streams.

This separation enables independent scaling: prefill capacity scales with input volume while decode capacity scales with output volume. Crucially, this architecture relies on the high-speed, RDMA-enabled networking fabric established in @sec-cluster-networking. When a prefill finishes, the resulting KV cache—often megabytes of data—must be migrated to a decode node within the inter-token latency budget (typically <10 ms). InfiniBand's sub-microsecond latency and high bisection bandwidth are the physical enablers of this logical disaggregation.

::: {.callout-note title="Sarathi: Chunked Prefill Implementation"}

The Sarathi system [@agrawal2023sarathi] implements chunked prefill with the following design:

**Chunk sizing**: Chunks are sized to complete in approximately the same time as one decode iteration (typically 10–50 ms). For a prefill throughput of 10,000 tokens/second, a 20 ms chunk processes 200 tokens.

**Interleaving schedule**: Each GPU iteration processes either:

- One prefill chunk for a new request, OR
- One decode step for all active sequences

This ensures decode latency remains bounded regardless of incoming prompt lengths.

**KV cache transfer**: When prefill completes, the generated KV cache transfers to decode slots. With NVLink, this transfer adds <1 ms for typical prompt lengths.

**Performance impact**:

- Without chunking: Long prompts cause decode latency spikes of 100 ms+
- With chunking: Decode latency bounded to 30 ms P99 regardless of prompt length

:::

## KV Cache Management {#sec-optimization-kv-cache}

Load balancing distributes requests across replicas at the service level for all model types. Within each replica, we now narrow our focus to a challenge specific to autoregressive language models. While recommendation and vision models have fixed memory footprints during inference, LLMs accumulate state as they generate tokens. This accumulated state, stored in the key-value (KV) cache, represents a distinctive memory management challenge that does not exist for other model types.

Autoregressive language models maintain KV caches that store attention context from previous tokens, enabling efficient generation without recomputing attention over the entire sequence history. As context lengths grow and serving scales, KV cache management becomes a critical bottleneck at the replica level of the serving hierarchy. A 70B parameter model with 128K context can require over 100GB just for KV cache, exceeding the model weights themselves.

This section examines the memory management techniques that enable efficient LLM serving at scale: PagedAttention for fragmentation-free allocation, prefix caching for common prompt sharing, and speculative decoding for latency reduction.

### KV Cache Fundamentals {#sec-optimization-kv-cache-fundamentals}

Autoregressive generation without caching requires $O(t^2)$ computation per token because each transformer layer must recompute attention keys and values for all previous tokens.[^fn-transformer-attention] The KV cache stores these computed key and value vectors, reducing generation to $O(t)$ per token. For serving at scale, this memory savings creates a critical management challenge since KV cache memory can exceed model weights for long contexts.

[^fn-transformer-attention]: **Transformer attention mechanics**: In transformer models, the self-attention mechanism computes attention scores between all pairs of tokens using query (Q), key (K), and value (V) projections. For a sequence of length $t$, this requires computing $t \times t$ attention weights. During autoregressive generation, each new token must attend to all previous tokens. Without caching, this means recomputing all previous K and V projections for every generated token. For readers seeking deeper background, see Vaswani et al. [@vaswani2017attention] or standard deep learning references.

The cache size grows with context as calculated by @eq-kv-cache-size:

$$\text{KV cache size} = 2 \times L \times H \times S \times B \times P$$ {#eq-kv-cache-size}

where:

- $L$ = number of layers
- $H$ = hidden dimension
- $S$ = sequence length
- $B$ = batch size
- $P$ = precision (bytes per element)
- Factor of 2 accounts for both keys and values

::: {.callout-example title="Engineering Calculation: KV-Cache Capacity Estimator"}
**The Problem**: You are serving Llama-3-70B (FP16 weights $\approx 140$ GB) on an 8xH100 node (640 GB total HBM). You want to determine the maximum batch size for a context length of 128K tokens.

**Formula**:
$$ M_{KV} = 2 \times n_{layers} \times n_{heads} \times d_{head} \times P_{prec} $$
$$ \text{Total Memory} = M_{weights} + (\text{Batch} \times \text{Context} \times M_{KV}) $$

**Parameters**:
*   $n_{layers} = 80$, $n_{heads} = 64$, $d_{head} = 128$.
*   $P_{prec} = 2$ bytes (FP16).
*   Context $= 131,072$ tokens.

**Step 1: Calculate Memory Per Token**
$$ M_{KV} = 2 \times 80 \times 64 \times 128 \times 2 = 2,621,440 \text{ bytes} \approx \mathbf{2.6 \text{ MB/token}} $$

**Step 2: Calculate Cache per Request (128K context)**
$$ 131,072 \text{ tokens} \times 2.6 \text{ MB/token} \approx \mathbf{340 \text{ GB/request}} $$

**Step 3: Determine Max Batch Size**
Available Memory for KV = $640 \text{ GB (Total)} - 140 \text{ GB (Weights)} - 20 \text{ GB (System)} = 480 \text{ GB}$.
$$ \text{Max Batch} = \lfloor \frac{480}{340} \rfloor = \mathbf{1} $$

**Conclusion**: Despite having 8 H100s, you can only serve **one** concurrent 128K-context request due to the memory wall. To fix this, you *must* use **PagedAttention** (to reduce fragmentation) and **GQA** (Grouped-Query Attention) to reduce $n_{heads}$.
:::

### The Fragmentation Problem {#sec-optimization-kv-fragmentation}

Traditional memory allocation for KV cache pre-allocates contiguous memory for each sequence based on maximum expected length. This creates two forms of waste:

**Internal fragmentation**: Sequences shorter than the maximum allocation waste the unused portion. If maximum length is 4,096 but average output is 100 tokens, 97.5% of allocated memory is wasted.

**External fragmentation**: As sequences complete and new ones start, memory becomes fragmented into non-contiguous free blocks. Even with sufficient total free memory, no single block may be large enough for a new maximum-length allocation.

Consider a simplified example with 8 memory slots and maximum sequence length of 4:

```
Time 0: Allocate Seq A (slots 0-3), Seq B (slots 4-7)
        [A][A][A][A][B][B][B][B]

Time 1: Seq A completes (2 tokens), Seq B continues
        [ ][ ][A][A][B][B][B][ ]  <- A only used 2 slots

Time 2: Try to allocate Seq C (needs 4 slots)
        [ ][ ][A][A][B][B][B][ ]  <- No contiguous block of 4!

Result: 4 free slots but cannot allocate new sequence
```

Production systems report 60-80% memory waste from fragmentation under realistic workloads, severely limiting batch sizes and throughput.

### PagedAttention {#sec-optimization-paged-attention}

PagedAttention [@kwon2023vllm], introduced in vLLM, applies virtual memory[^fn-virtual-memory] concepts to KV cache management. Instead of contiguous allocation, the KV cache is divided into fixed-size pages (typically 16-256 tokens), and sequences are allocated pages on demand.

[^fn-virtual-memory]: **Virtual memory for KV cache**: Just as operating systems use virtual memory to provide processes with contiguous address spaces backed by non-contiguous physical memory, PagedAttention provides sequences with logically contiguous KV caches backed by non-contiguous GPU memory blocks. This enables dynamic allocation, sharing, and efficient memory utilization without fragmentation.

@fig-paged-attention illustrates the key concepts including page tables that map logical sequence positions to physical memory pages, block size that defines the number of tokens per page (typically 16 tokens), and physical blocks that provide fixed-size memory allocations assignable to any sequence.

::: {#fig-paged-attention fig-env="figure" fig-pos="htb" fig-cap="**PagedAttention Memory Mapping**. Decoupling the logical view of a sequence's KV cache (contiguous pages) from its physical storage (non-contiguous 16-token blocks). A block table maps logical pages to physical blocks, allowing the system to fill fragmentation gaps with small blocks from any sequence." fig-alt="Three-part diagram: left shows 4 contiguous logical pages (P0-P3), center shows block table mapping logical to physical addresses, right shows 12 scattered physical blocks in HBM with 4 highlighted as mapped."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9]
  \definecolor{BlockColor}{RGB}{200,220,255}
  \definecolor{MappedColor}{RGB}{255,220,200}

  \tikzset{
    block/.style={draw=black!70, thick, minimum width=0.8cm, minimum height=0.6cm, font=\tiny},
    arrow/.style={->, >=stealth, thick, gray!60}
  }

  % Logical View
  \node[anchor=west] at (0, 3.5) {\textbf{Logical KV Cache} (Contiguous)};
  \node[block, fill=BlockColor] (L0) at (0.5, 3) {P0};
  \node[block, fill=BlockColor] (L1) at (1.3, 3) {P1};
  \node[block, fill=BlockColor] (L2) at (2.1, 3) {P2};
  \node[block, fill=BlockColor] (L3) at (2.9, 3) {P3};

  % Page Table
  \node[draw, dashed, inner sep=4pt] at (5, 2) (table) {
    \begin{tabular}{c|c}
      \tiny Logic & \tiny Phys \\
      \hline
      \tiny P0 & \tiny B7 \\
      \tiny P1 & \tiny B2 \\
      \tiny P2 & \tiny B9 \\
      \tiny P3 & \tiny B4
    \end{tabular}
  };
  \node[above, font=\scriptsize] at (table.north) {Block Table};

  % Physical View
  \node[anchor=west] at (8, 3.5) {\textbf{Physical HBM} (Scattered)};
  \foreach \x/\y/\id/\fill in {8.5/3/B0/white, 9.3/3/B1/white, 10.1/3/B2/MappedColor, 10.9/3/B3/white,
                               8.5/2.2/B4/MappedColor, 9.3/2.2/B5/white, 10.1/2.2/B6/white, 10.9/2.2/B7/MappedColor,
                               8.5/1.4/B8/white, 9.3/1.4/B9/MappedColor, 10.1/1.4/B10/white, 10.9/1.4/B11/white} {
    \node[block, fill=\fill] (phys\id) at (\x, \y) {\id};
  }

  % Mapping arrows
  \draw[arrow] (L0) to[out=-90, in=180] (table);
  \draw[arrow] (table) to[out=0, in=180] (physB7);
  \draw[arrow] (table) to[out=0, in=180] (physB2);
  \draw[arrow] (table) to[out=0, in=180] (physB9);
  \draw[arrow] (table) to[out=0, in=180] (physB4);

\end{tikzpicture}
```
:::

PagedAttention provides several benefits. It eliminates internal fragmentation by allocating only the pages needed for actual tokens. It eliminates external fragmentation because any free page can be used by any sequence. It enables dynamic growth so sequences can grow without pre-allocation. It supports memory sharing so common prefixes can share physical pages.

::: {.callout-note title="PagedAttention Implementation Details"}

**Memory layout**:

```
Physical blocks (16 tokens × hidden_dim × 2 × precision):
Block 0:  [K₀...K₁₅, V₀...V₁₅]
Block 1:  [K₀...K₁₅, V₀...V₁₅]
...
Block N:  [K₀...K₁₅, V₀...V₁₅]
```

**Page table per sequence**:

```{.python}
class PageTable:
    def __init__(self, max_blocks):
        self.block_map = {}  # logical_block -> physical_block

    def allocate_block(self, logical_idx, physical_block):
        self.block_map[logical_idx] = physical_block

    def get_physical(self, logical_idx):
        return self.block_map[logical_idx]
```

**Attention kernel modification**:

Standard attention: `output = softmax(Q @ K.T / sqrt(d)) @ V`

PagedAttention:
```{.python}
def paged_attention(Q, page_table, physical_blocks, block_size):
    # Gather K, V from non-contiguous physical blocks
    for logical_idx in range(num_logical_blocks):
        physical_idx = page_table[logical_idx]
        K_block = physical_blocks[physical_idx].K
        V_block = physical_blocks[physical_idx].V
        # Compute attention for this block
        attention_scores = Q @ K_block.T / sqrt(d)
        output += softmax(attention_scores) @ V_block
    return output
```

**Performance impact**:

The gather operations add overhead, but it is minimal compared to the memory savings:

+--------------------+------------------------+---------------------------+
| **Approach**       | **Memory Utilization** | **Throughput (relative)** |
+:===================+=======================:+==========================:+
| **Contiguous**     | 30-40%                 | 1.0x (baseline)           |
| **PagedAttention** | 95%+                   | 2.5-4x                    |
+--------------------+------------------------+---------------------------+

The 2.5-4x throughput improvement comes from fitting more concurrent sequences in the same memory.

:::

### Prefix Caching {#sec-optimization-prefix-caching}

Many LLM workloads share common prefixes across requests. System prompts like "You are a helpful assistant..." are prepended to every request. Few-shot examples use the same examples for many queries. Document context involves multiple questions about the same document. Recomputing these shared prefixes wastes both compute (prefill) and memory (duplicate KV cache entries).

**Prefix caching** shares KV cache entries across requests with common prefixes. @fig-prefix-caching demonstrates how shared system prompts avoid redundant computation:

::: {#fig-prefix-caching fig-env="figure" fig-pos="htb" fig-cap="**Prefix Caching via Block Sharing**. PagedAttention enables efficient prefix caching by allowing multiple sequences' block tables to point to the same physical blocks for shared content. In this example, the System Prompt is stored in blocks 0-5. Request A and Request B maps their first 6 logical pages to these same physical blocks, storing only their unique suffixes in new blocks. This dramatically reduces memory usage and prefill computation for workloads with shared context." fig-alt="Block diagram showing two requests sharing physical memory. Top row: 6 green shared blocks (B0-B5) for system prompt. Request A and Request B page tables both point to shared blocks, with unique suffix blocks in blue and pink respectively."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{SharedColor}{RGB}{144,238,144}
  \definecolor{UniqueColorA}{RGB}{173,216,230}
  \definecolor{UniqueColorB}{RGB}{255,182,193}
  \definecolor{BlockColor}{RGB}{240,240,240}

  \tikzset{
    block/.style={draw=black!60, fill=BlockColor, minimum width=0.8cm, minimum height=0.6cm, font=\scriptsize},
    ptr/.style={->, >=stealth, thick}
  }

  % Physical Memory Blocks (Central)
  \node[anchor=north] at (4, 4) {\textbf{Physical Blocks (GPU)}};

  % Shared Blocks 0-5
  \foreach \x in {0,1,2,3,4,5} {
      \node[block, fill=SharedColor] (pb\x) at (\x*0.9 + 1, 3) {B\x};
  }
  \node[right, font=\scriptsize] at (6.5, 3) {(System Prompt)};

  % Unique Blocks for A
  \node[block, fill=UniqueColorA] (pb10) at (1, 2) {B10};
  \node[block, fill=UniqueColorA] (pb11) at (1.9, 2) {B11};

  % Unique Blocks for B
  \node[block, fill=UniqueColorB] (pb12) at (4, 2) {B12};
  \node[block, fill=UniqueColorB] (pb13) at (4.9, 2) {B13};
  \node[block, fill=UniqueColorB] (pb14) at (5.8, 2) {B14};


  % Logical View - Request A
  \node[anchor=east] at (-1, 1) {\textbf{Request A}};
  \node[block, fill=SharedColor] (la0) at (0, 1) {P0};
  \node[block, fill=SharedColor] (la1) at (0.9, 1) {P1};
  \node[font=\scriptsize] at (1.8, 1) {...};
  \node[block, fill=UniqueColorA] (la6) at (2.7, 1) {P6};

  \draw[ptr, SharedColor!70!black] (la0) -- (pb0);
  \draw[ptr, SharedColor!70!black] (la1) -- (pb1);
  \draw[ptr, UniqueColorA!70!black] (la6) -- (pb10);

  % Logical View - Request B
  \node[anchor=east] at (-1, 0) {\textbf{Request B}};
  \node[block, fill=SharedColor] (lb0) at (0, 0) {P0};
  \node[block, fill=SharedColor] (lb1) at (0.9, 0) {P1};
  \node[font=\scriptsize] at (1.8, 0) {...};
  \node[block, fill=UniqueColorB] (lb6) at (2.7, 0) {P6};

  \draw[ptr, SharedColor!70!black] (lb0) to[in=220, out=45] (pb0);
  \draw[ptr, SharedColor!70!black] (lb1) to[in=220, out=45] (pb1);
  \draw[ptr, UniqueColorB!70!black] (lb6) -- (pb12);

  % Annotations
  \node[align=left, font=\scriptsize, anchor=west] at (7, 1) {Both requests point\\to same physical blocks\\for blocks 0-5};

\end{tikzpicture}
```
:::

**Implementation with PagedAttention**:

Prefix caching integrates naturally with PagedAttention through copy-on-write semantics:

```
System prompt → Physical blocks [0, 1, 2, 3, 4, 5]

Request A page table: [0, 1, 2, 3, 4, 5, 10, 11]  <- shares prefix blocks
Request B page table: [0, 1, 2, 3, 4, 5, 12, 13, 14]  <- shares prefix blocks
Request C page table: [0, 1, 2, 3, 4, 5, 15]  <- shares prefix blocks
```

All three requests reference the same physical blocks for the system prompt. Only when generating unique tokens do they allocate new blocks.

::: {.callout-note title="Prefix Caching at Scale"}

Consider a chatbot service with a 2000-token system prompt and 1000 concurrent users:

**Without prefix caching**:

- KV cache per user: 2000 + 500 (avg response) = 2500 tokens
- Total KV cache: 2500 × 1000 × 2 × 80 × 8192 × 2 = 6.5 TB

**With prefix caching**:

- Shared prefix: 2000 tokens (once)
- Unique per user: 500 tokens
- Total: (2000 × 1) + (500 × 1000) = 502,000 tokens
- Memory: 502,000 × 2 × 80 × 8192 × 2 = 1.3 TB

**Savings**: 80% reduction in KV cache memory, enabling 5x more concurrent users.

**Prefix hit rate** determines effectiveness:

+----------------------------------+---------------------+--------------------+
| **Workload**                     | **Prefix Hit Rate** | **Memory Savings** |
+:=================================+====================:+===================:+
| **Chatbot (same system prompt)** | 95%+                | 70-80%             |
| **Document QA (same doc)**       | 80-90%              | 50-70%             |
| **General API (diverse)**        | 20-40%              | 10-30%             |
+----------------------------------+---------------------+--------------------+

:::

### KV Cache Compression {#sec-optimization-kv-compression}

Beyond efficient allocation, reducing the size of cached values provides additional memory savings. Several techniques compress the KV cache:

**Quantization**: Store cached keys and values at reduced precision.

$$\text{Compressed size} = \text{Original size} \times \frac{b_{compressed}}{b_{original}}$$

+---------------------+----------------------+--------------------+
| **Precision**       | **Memory per Token** | **Quality Impact** |
+====================:+=====================:+===================:+
| **FP16 (baseline)** | 2 bytes              | None               |
| **FP8**             | 1 byte               | &lt;1% degradation |
| **INT8**            | 1 byte               | 1-2% degradation   |
| **INT4**            | 0.5 bytes            | 3-5% degradation   |
+---------------------+----------------------+--------------------+

**Key observation**: KV cache values are more tolerant of quantization than model weights because they are intermediate activations, not learned parameters.

**Sliding window attention**: For very long contexts, maintain full cache only for recent tokens:

```
Full context: 100,000 tokens
Sliding window: 4,096 tokens

Cache strategy:

- Tokens 0-95,904: Discarded or compressed
- Tokens 95,904-100,000: Full precision cache

Trade-off: Cannot attend to very old tokens, but sufficient for most tasks.
```

**Grouped-query attention (GQA)**[^fn-gqa] [@ainslie2023gqa]: Architectural change that reduces KV cache by sharing key-value heads:

[^fn-gqa]: **GQA trade-off**: Grouped-query attention interpolates between multi-head attention (MHA, one KV head per query head) and multi-query attention (MQA, one KV head for all query heads). GQA with 8 KV heads for 64 query heads achieves 8x KV cache reduction versus MHA while maintaining most of MHA's model quality, unlike MQA which can degrade quality significantly.

+-------------------------+--------------+---------------------------+
| **Attention Type**      | **KV Heads** | **Cache Size (relative)** |
+:========================+=============:+==========================:+
| **Multi-head (MHA)**    | 64           | 1.0x                      |
| **Grouped-query (GQA)** | 8            | 0.125x                    |
| **Multi-query (MQA)**   | 1            | 0.016x                    |
+-------------------------+--------------+---------------------------+

Modern models like Llama 2 [@touvron2023llama2] and Mistral use GQA specifically to reduce KV cache requirements.

### Speculative Decoding {#sec-optimization-speculative-decoding}

Autoregressive generation is inherently sequential: each token depends on previous tokens. Speculative decoding[^fn-speculative] [@leviathan2023speculative; @chen2023accelerating] breaks this bottleneck by using a smaller draft model to predict multiple tokens, then verifying them in parallel with the target model.

[^fn-speculative]: **Speculative execution in LLMs**: The concept borrows from CPU speculative execution, where processors predict branch outcomes and execute instructions ahead of confirmation. Similarly, speculative decoding predicts multiple tokens and processes them speculatively, rolling back only when the target model disagrees. Unlike CPU speculation (which is invisible to software), LLM speculation requires explicit acceptance/rejection logic.

**Algorithm**:

1. Draft model generates $k$ tokens speculatively: $t_1, t_2, ..., t_k$
2. Target model verifies all $k$ tokens in a single forward pass
3. Accept prefix of correct tokens, reject from first incorrect token
4. Continue from last accepted token

**Why this works**: The draft model is much smaller (7B vs 70B) and can generate $k$ tokens in the time the target model generates 1 token. Verification is cheap because the target model can process all $k$ tokens in parallel (like prefill).

::: {.callout-note title="Speculative Decoding Example"}

**Target model**: Llama-70B (30 tokens/second)
**Draft model**: Llama-7B (300 tokens/second)
**Speculation length**: $k = 4$ tokens

Speculative decoding accelerates generation by using a small model to draft tokens which a larger model verifies in parallel.

**Scenario**: Generating "The quick brown fox jumps". @fig-speculative-decoding contrasts sequential baseline decoding with speculative verification:

::: {#fig-speculative-decoding fig-env="figure" fig-pos="htb" fig-cap="**Speculative Decoding Process**. Instead of generating tokens sequentially with the large target model (slow), a small draft model quickly proposes a sequence of $K$ tokens. The target model then verifies all $K$ tokens in a single parallel forward pass (similar to prefill). If the draft tokens match the target's output, they are accepted, effectively generating multiple tokens per target model step. If a mismatch occurs, the sequence is rolled back to the first error." fig-alt="Timeline comparison of two decoding methods. Top: standard decoding with 4 sequential generation steps. Bottom: speculative decoding with fast draft, parallel verify, then 3 accepted tokens (green) and 1 rejected (pink). Braces show time savings."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{DraftColor}{RGB}{255,228,196}
  \definecolor{VerifyColor}{RGB}{176,224,230}
  \definecolor{AcceptColor}{RGB}{144,238,144}
  \definecolor{RejectColor}{RGB}{255,182,193}

  \tikzset{
     step/.style={draw=black!50, rounded corners=2pt, minimum height=0.7cm, font=\scriptsize, align=center}
  }

  % Baseline
  \node[anchor=west] at (0, 4) {\textbf{Standard Decoding} (Sequential)};
  \draw[->] (0, 3.5) -- (8, 3.5) node[right] {Time};

  \node[step, fill=VerifyColor, minimum width=1.5cm] at (1, 3.7) {Gen T1};
  \node[step, fill=VerifyColor, minimum width=1.5cm] at (2.6, 3.7) {Gen T2};
  \node[step, fill=VerifyColor, minimum width=1.5cm] at (4.2, 3.7) {Gen T3};
  \node[step, fill=VerifyColor, minimum width=1.5cm] at (5.8, 3.7) {Gen T4};

  % Speculative
  \node[anchor=west] at (0, 2) {\textbf{Speculative Decoding}};
  \draw[->] (0, 1.5) -- (8, 1.5) node[right] {Time};

  % Draft Step
  \node[step, fill=DraftColor, minimum width=2.0cm] (draft) at (1.2, 1.0) {Draft Model\\Gen [T1..T4]};
  \node[below, font=\tiny] at (draft.south) {Fast, Low latency};

  % Verify Step
  \node[step, fill=VerifyColor, minimum width=2.0cm] (verify) at (3.5, 1.0) {Target Model\\Verify [T1..T4]};
  \node[below, font=\tiny] at (verify.south) {Parallel Forward Pass};

  % Outcome
  \node[step, fill=AcceptColor, minimum width=0.5cm] at (5.0, 1.8) {T1};
  \node[step, fill=AcceptColor, minimum width=0.5cm] at (5.6, 1.8) {T2};
  \node[step, fill=AcceptColor, minimum width=0.5cm] at (6.2, 1.8) {T3};
  \node[step, fill=RejectColor, minimum width=0.5cm] at (6.8, 1.8) {T4};

  \draw[->, thick, gray] (draft) -- (verify);
  \draw[->, thick, gray] (verify) -- (5.0, 1.5);

  \node[right, font=\scriptsize, align=left] at (7, 1.8) {Accept 3\\Reject 1};

  % Speedup annotation
  \draw[decorate, decoration={brace, amplitude=5pt}] (0.2, 3.2) -- (6.6, 3.2) node[midway, below=0.2cm, font=\scriptsize] {Standard Time};
  \draw[decorate, decoration={brace, amplitude=5pt}] (0.2, 0.5) -- (4.8, 0.5) node[midway, below=0.2cm, font=\scriptsize] {Speculative Time};

\end{tikzpicture}
```
:::

**Effective speedup**: 43/30 = 1.43x

**Factors affecting speedup**:

+---------------------+-------------------+
| **Acceptance Rate** | **Speedup**       |
+====================:+==================:+
| **90% (easy text)** | 2.5–3×            |
| **70% (typical)**   | 1.5–2×            |
| **50% (hard text)** | 1.2–1.5×          |
| **30% (very hard)** | &lt;1× (overhead) |
+---------------------+-------------------+

Speedup depends on how well the draft model predicts the target model's output. For well-aligned model pairs (same training data, similar architecture), acceptance rates of 70-80% are common.

:::

**Self-speculative decoding** uses early exit from the target model itself as the draft, avoiding the need for a separate model:

```
Target model layers: 80

Draft: Layers 1-20 → predict next token
Verify: Layers 1-80 → confirm or reject
```

This eliminates the need to load and manage a separate draft model, at the cost of lower acceptance rates than a dedicated draft model.

### KV Cache in Distributed Settings {#sec-optimization-kv-cache-distributed}

The tensor and pipeline parallelism strategies from @sec-inference-sharding introduce additional KV cache management complexity:

**Tensor parallelism**: KV cache is sharded across devices along with attention heads. Each device stores cache for its subset of heads.

```
8-way tensor parallelism:
Device 0: KV cache for heads 0-7
Device 1: KV cache for heads 8-15
...
Device 7: KV cache for heads 56-63
```

**Cross-device sharing**: Prefix caching across tensor-parallel devices requires cache to be sharded identically on all devices. This is automatic when prefixes are processed with the same tensor-parallel configuration.

**KV cache migration**: When consistent hashing routes a conversation to a different replica (due to failure or rebalancing), the KV cache must be migrated:

```
Migration options:
1. Rebuild: Re-run prefill on new replica (500 ms+ for long context)
2. Transfer: Send KV cache over network (100MB at 100Gbps = 8 ms)
3. Hybrid: Transfer if small, rebuild if large

Decision threshold:
if cache_size_bytes / network_bandwidth < prefill_time:
    transfer()
else:
    rebuild()
```

For Llama-70B with 4K context, KV cache is ~80MB per sequence. At 100 Gbps, transfer takes 6.4 ms versus ~500 ms for prefill. Transfer is clearly better.

### Memory Management Best Practices {#sec-optimization-kv-best-practices}

Effective KV cache management combines multiple techniques:

**Sizing the KV cache pool**:

```
Available GPU memory = Total - Weights - Activations - Overhead
KV pool size = 0.9 × Available  # Leave 10% headroom

Max concurrent sequences = KV pool size / (avg_seq_length × per_token_cache)
```

When cache is full, systems use several eviction policies. LRU (Least Recently Used) evicts sequences with oldest last access. Size-based eviction removes longest sequences first to free most memory. Priority-based eviction protects high-priority or paid-tier requests.

**Preemption** for continuous batching:

When a new high-priority request cannot fit, the system follows a sequence of steps. It selects victim sequences using the eviction policy. It swaps the victim's KV cache to CPU memory. It allocates GPU memory to the new request. When the victim is resumed, it swaps back from CPU.

::: {.callout-note title="KV Cache Memory Hierarchy"}

Production systems use a memory hierarchy for KV cache:

+--------------+--------------+-------------+-------------------+
| **Tier**     | **Capacity** | **Latency** | **Use Case**      |
+:=============+=============:+============:+:==================+
| **GPU HBM**  | 80GB         | 0 ms        | Active sequences  |
| **CPU DRAM** | 1TB          | 1–5 ms      | Swapped sequences |
| **NVMe SSD** | 10TB         | 10–50 ms    | Long-term cache   |
+--------------+--------------+-------------+-------------------+

**Swap implementation**:

```{.python}
async def swap_to_cpu(sequence_id):
    kv_cache = gpu_cache[sequence_id]
    cpu_cache[sequence_id] = kv_cache.cpu()  # Async transfer
    gpu_cache.free(sequence_id)


async def swap_to_gpu(sequence_id):
    cpu_kv = cpu_cache[sequence_id]
    gpu_cache[sequence_id] = cpu_kv.cuda()  # Async transfer
    cpu_cache.free(sequence_id)
```

**Observed performance**:

- GPU-only (no swapping): 50 concurrent sequences
- GPU+CPU swapping: 500 concurrent sequences (10x)
- Average swap latency: 3 ms (acceptable for non-urgent requests)

:::

## Weight Quantization for Serving {#sec-optimization-weight-quantization}

The KV cache techniques we just examined address dynamic memory allocation during inference. But model weights themselves represent static memory consumption that persists regardless of batch size or sequence length. For a 70B model, weights alone consume 140GB in FP16, leaving limited capacity for KV cache on even the largest GPUs. Quantization offers a complementary approach: reduce the memory footprint of weights themselves.

::: {.callout-note title="Assumed Prior Knowledge: Quantization Fundamentals"}
This section assumes familiarity with quantization fundamentals including **post-training quantization (PTQ)**, which quantizes a trained model without retraining, and **quantization-aware training (QAT)**, which incorporates quantization effects during training for higher accuracy. Readers should also understand precision formats (FP32, FP16, INT8) and the basic trade-off between numerical precision and memory/compute efficiency. These concepts are covered in standard deep learning optimization references and in Volume I's treatment of model compression.
:::

Quantization reduces numerical precision of model weights and activations, decreasing memory footprint by 2-4x while increasing decode throughput, which is memory-bandwidth limited rather than compute limited. While these quantization fundamentals are established techniques, serving at scale introduces distinct challenges. Models must be quantized after training without access to training data. Quantization must preserve quality across diverse inputs. Hardware deployment targets vary from datacenter GPUs to edge accelerators. This section examines quantization techniques specifically designed for production inference.

### LLM-Specific Quantization Challenges {#sec-optimization-llm-quantization}

Large language models present unique quantization challenges distinct from vision or recommendation models. The outlier activation problem occurs because certain attention heads produce activation magnitudes orders of magnitude larger than typical values. Naive quantization clips these outliers, causing significant quality degradation.

Consider a Llama-70B layer where most activations fall within [-10, 10] but specific channels reach magnitudes of 1000+. Symmetric INT8 quantization with range [-127, 127] must choose:

- **Wide range** [-1000, 1000]: Most values map to 0, losing information
- **Narrow range** [-10, 10]: Outliers clip, causing large errors

This outlier distribution motivates the specialized quantization methods that follow.

### GPTQ: Layer-by-Layer Weight Quantization {#sec-optimization-gptq}

GPTQ [@frantar2023gptq] quantizes LLM weights using Hessian-based[^fn-hessian] error compensation. Rather than quantizing all weights independently, GPTQ adjusts remaining weights to compensate for errors introduced by quantization.

[^fn-hessian]: **Hessian matrix in quantization**: The Hessian matrix $H = X^T X$ captures second-order information about how output changes with respect to weight perturbations. Weights with large Hessian diagonal entries have outsized impact on model outputs; GPTQ uses this information to prioritize preserving important weights and compensate for errors in less important ones.

The GPTQ algorithm processes the model layer by layer. For each layer, it calibrates using a small dataset (128-256 samples). It quantizes weights in order of decreasing Hessian magnitude. It adjusts remaining weights to minimize output error.

**Key insight**: The Hessian matrix $H = X^T X$ captures which weights most affect outputs. GPTQ builds on the Optimal Brain Surgeon (OBS) framework, using the inverse Hessian to guide error compensation.

**Column-wise processing algorithm**:

GPTQ processes each weight matrix column by column, using Cholesky decomposition of $H^{-1}$ for efficiency:

```
For each layer's weight matrix W:
  1. Compute Hessian: H = X^T X from calibration activations
  2. Apply Cholesky factorization to H^{-1}
  3. For each column q = 1 to d_col:
     a. Quantize column: w_q = round(W[:,q] / Δ) × Δ
     b. Compute quantization error: δ = W[:,q] - w_q
     c. Update remaining columns to compensate:
        W[:,q+1:] += δ × [H^{-1}][:,q+1:] / [H^{-1}]_{qq}
```

The compensation step propagates quantization error to unquantized columns, where the Hessian-weighted update minimizes output deviation. This achieves $O(d_{row} \cdot d_{col}^2)$ complexity versus $O(d_{row} \cdot d_{col}^3)$ for naive OBS.

**Quantization formula**:

$$w_q = \text{round}\left(\frac{w}{\Delta}\right) \cdot \Delta$$

where $\Delta$ is the quantization step size. GPTQ uses per-group quantization with group sizes of 128 weights, enabling finer-grained scaling that reduces quantization error for outlier channels.

**Performance characteristics**:

+---------------+----------+-------------------------+----------------------+-----------------------+
| **Model**     | **Bits** | **Perplexity Increase** | **Memory Reduction** | **Quantization Time** |
+==============:+=========:+========================:+=====================:+======================:+
| **Llama-7B**  | 4        | +0.3                    | 4x                   | 15 min                |
| **Llama-13B** | 4        | +0.2                    | 4x                   | 30 min                |
| **Llama-70B** | 4        | +0.15                   | 4x                   | 3 hours               |
+---------------+----------+-------------------------+----------------------+-----------------------+

GPTQ's strengths include fast quantization without retraining, minimal quality loss for 4-bit weights, and broad hardware compatibility. Its limitations include requiring calibration data, sensitivity to calibration set selection, and per-layer processing that cannot leverage cross-layer information.

### AWQ: Activation-Aware Weight Quantization {#sec-optimization-awq}

The central observation is that not all weights are equally important. AWQ [@lin2024awq] recognizes that weights connected to channels with large activation magnitudes have disproportionate impact on outputs. Rather than protecting weights based on their own magnitude, AWQ protects weights based on the magnitude of activations they produce.

**Algorithm**:

1. Run calibration samples to measure per-channel activation magnitudes
2. Identify "salient" channels with large activations
3. Scale weights for salient channels up before quantization
4. Scale outputs down correspondingly (fused into subsequent layer)

**Scaling formulation**:

For weight matrix $W$ and activation statistics $s$ (per-channel activation magnitudes):

$$W' = W \cdot \text{diag}(s^\alpha)$$

where $\alpha \in [0.5, 1.0]$ controls scaling aggressiveness. This preserves salient channels while allowing aggressive quantization of less important weights.

**Comparison with GPTQ**:

+----------------------------+---------------------------+-------------------------+
| **Aspect**                 | **GPTQ**                  | **AWQ**                 |
+:===========================+:==========================+:========================+
| **Error compensation**     | Adjusts remaining weights | Scales salient channels |
| **Calibration data**       | 128-256 samples           | 128 samples             |
| **Quality (4-bit)**        | Very good                 | Excellent               |
| **Speed**                  | Faster                    | Slightly slower         |
| **Hardware compatibility** | Broad                     | Broad                   |
+----------------------------+---------------------------+-------------------------+

AWQ typically achieves 0.5-1% lower perplexity degradation than GPTQ at the same bit-width, making it preferred for production deployments where quality is paramount.

### SmoothQuant: Migrating Quantization Difficulty {#sec-optimization-smoothquant}

SmoothQuant [@xiao2023smoothquant] addresses the activation outlier problem by migrating quantization difficulty from activations to weights. Weights have predictable distributions; activations have unpredictable outliers. SmoothQuant transfers the outlier problem to weights where it can be handled with per-channel scaling.

**Core technique**: Insert smoothing operations that divide activations by per-channel scales while multiplying weights by the same scales:

$$Y = X W = (X \cdot \text{diag}(s)^{-1}) \cdot (\text{diag}(s) \cdot W) = \hat{X} \hat{W}$$

This transformation is mathematically equivalent but produces smoother activation distributions.

**Migration strength** $\alpha$ controls the trade-off:

$$s_j = \max(|X_j|)^\alpha / \max(|W_j|)^{1-\alpha}$$

- $\alpha = 0$: No migration, activations remain difficult
- $\alpha = 1$: Full migration, weights absorb all difficulty
- $\alpha = 0.5$: Balanced (typical setting)

**W8A8 deployment**: SmoothQuant enables INT8 quantization for both weights and activations:

+--------------------------+------------+---------------------+--------------------+---------------+
| **Configuration**        | **Memory** | **Prefill Speedup** | **Decode Speedup** | **Quality**   |
+=========================:+===========:+====================:+===================:+:==============+
| **FP16 (baseline)**      | 1x         | 1x                  | 1x                 | Baseline      |
| **W8A16 (weights only)** | 2x         | 1.3x                | 1.8x               | &lt;0.5% loss |
| **W8A8 (SmoothQuant)**   | 2x         | 1.8-2x              | 1.3-1.5x           | &lt;1% loss   |
+--------------------------+------------+---------------------+--------------------+---------------+

**Critical distinction**: W8A8 provides near 2x speedup for compute-bound prefill (large batch processing initial prompt), but only 1.3-1.5x speedup for memory-bound decode (generating tokens one at a time). LLM serving is typically decode-heavy, so real-world throughput improvements from W8A8 are often 1.3-1.7x rather than the theoretical 2x compute throughput of INT8 Tensor Cores.

### KV Cache Quantization {#sec-optimization-kv-cache-quantization}

While weight quantization reduces model storage, the KV cache dominates memory consumption for long-context LLM serving. At 32K+ token context lengths, KV cache can exceed model weights in memory usage. KV cache quantization addresses this critical bottleneck.

**KV cache memory scaling**:

$$\text{KV Cache} = 2 \times \text{layers} \times \text{heads} \times d_{head} \times \text{seq\_len} \times \text{batch} \times \text{bytes}$$

For a 70B model (80 layers, 64 heads, 128 $d_{head}$) with 32K context in FP16:

$$\text{KV per sequence} = 2 \times 80 \times 64 \times 128 \times 32768 \times 2 = 85.9 \text{ GB}$$

A single long-context sequence consumes more memory than the model weights.

This reveals a critical property: KV cache values exhibit different distributions than model weights, enabling targeted quantization strategies.

- **Keys**: Relatively uniform distributions, tolerate aggressive quantization (2-4 bits)
- **Values**: More sensitive to quantization, require careful calibration (4-6 bits)

**KIVI (Key-Value cache quantization for Inference)**:

KIVI quantizes keys and values asymmetrically based on their sensitivity:

+---------------+----------------+--------------+---------------------+
| **Component** | **Precision**  | **Grouping** | **Quality Impact**  |
+:==============+===============:+:=============+:====================+
| **Keys**      | 2-bit          | Per-channel  | Minimal             |
| **Values**    | 4-bit          | Per-token    | &lt;0.5% perplexity |
| **Combined**  | ~3-bit average | Mixed        | &lt;1% perplexity   |
+---------------+----------------+--------------+---------------------+

**Memory impact of combined weight and KV quantization**:

+-------------------+-----------------+--------------------+------------------+
| **Configuration** | **Weight Size** | **KV Cache (32K)** | **Total Memory** |
+==================:+================:+===================:+=================:+
| **FP16/FP16**     | 140GB           | 86GB               | 226GB            |
| **W4A16/FP16**    | 35GB            | 86GB               | 121GB            |
| **W4A16/KV4**     | 35GB            | 21GB               | 56GB             |
+-------------------+-----------------+--------------------+------------------+

KV cache quantization enables 4x longer contexts or 4x higher batch sizes on the same hardware.

**Integration with PagedAttention**: Quantized KV cache requires quantization-aware block management:

```python
# Conceptual: vLLM with KV cache quantization
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    kv_cache_dtype="fp8",  # FP8 E4M3 for KV cache
    quantization="awq",  # 4-bit weights
)
```

With FP8 KV cache, the per-sequence memory drops from 2.5MB to 1.25MB per 1K tokens, doubling maximum concurrent sequences.

### Rotation-Based Quantization {#sec-optimization-rotation-quantization}

Traditional quantization methods (GPTQ, AWQ, SmoothQuant) address outliers through compensation or migration. **Rotation-based quantization** [@ashkboos2024quarot] takes a fundamentally different approach: mathematically transforming the weight and activation space to eliminate outliers entirely.

The crucial realization is that outliers are artifacts of the coordinate basis representation. Rotating to a different basis spreads extreme values uniformly, making all values quantization-friendly.

**QuaRot (Quantization with Rotation)**:

QuaRot applies orthogonal Hadamard transforms to weights and activations:

$$X' = X \cdot H, \quad W' = H^T \cdot W$$

where $H$ is a Hadamard matrix (orthogonal, efficiently computable without storage).

**Why Hadamard transforms work**: For a vector with one extreme outlier, the Hadamard transform distributes that outlier's magnitude across all dimensions:

```
Original:  [1000, 1, 1, 1]     # One large outlier
Hadamard:  [252, 250, 250, 250] # Uniform distribution
```

After transformation, no single value dominates, enabling uniform quantization.

**Advantages over migration-based methods**:

+-----------------------+-----------------+---------------------------+
| **Aspect**            | **SmoothQuant** | **QuaRot**                |
+:======================+:================+:==========================+
| **Calibration data**  | Required        | Not required (data-free)  |
| **Minimum precision** | W8A8            | W4A4                      |
| **Runtime overhead**  | ~0%             | ~3% (Hadamard transforms) |
| **Outlier handling**  | Migration       | Elimination               |
+-----------------------+-----------------+---------------------------+

**Performance comparison**:

+-----------------+---------------+----------------------------+-----------------+
| **Method**      | **Precision** | **LLaMA-2-70B Perplexity** | **vs Baseline** |
+:================+==============:+===========================:+================:+
| **FP16**        | W16A16        | 3.12                       | Baseline        |
| **SmoothQuant** | W8A8          | 3.18                       | +0.06           |
| **GPTQ**        | W4A16         | 3.24                       | +0.12           |
| **QuaRot**      | W4A4          | 3.31                       | +0.19           |
+-----------------+---------------+----------------------------+-----------------+

QuaRot achieves 4-bit weights AND 4-bit activations with quality competitive to GPTQ's 4-bit weights only. This enables approximately 8x memory reduction versus FP16.

**SpinQuant**: An extension of QuaRot that learns optimal rotation matrices during a short fine-tuning phase, improving quality at the cost of training compute.

### Hardware-Deployment Co-design {#sec-optimization-quant-hardware}

Quantization strategies must match target hardware capabilities. Different accelerators support different precisions with varying performance multipliers.

**NVIDIA Tensor Core Support**:

+----------------+-------------------+-------------------+---------------------+
| **Format**     | **Ampere (A100)** | **Hopper (H100)** | **Speedup vs FP16** |
+===============:+:==================+:==================+====================:+
| **FP16**       | Yes               | Yes               | 1x                  |
| **BF16**       | Yes               | Yes               | 1x                  |
| **INT8**       | Yes               | Yes               | 2x                  |
| **FP8 (E4M3)** | No                | Yes               | 2x                  |
| **INT4**       | Via CUTLASS       | Native            | 4x                  |
+----------------+-------------------+-------------------+---------------------+

**Memory bandwidth dominance**: For autoregressive LLM decode, memory bandwidth limits throughput since each token reads the entire model:

$$\text{Decode throughput} \propto \frac{\text{Memory bandwidth}}{\text{Model size in bytes}}$$

4-bit quantization delivers 4x throughput improvement for memory-bound decode, making it highly valuable despite modest compute gains.

**Deployment configurations**:

+------------------------+------------------------------------+-------------------------------+
| **Quantization**       | **Best For**                       | **Framework Support**         |
+=======================:+:===================================+:==============================+
| **W4A16 (GPTQ/AWQ)**   | Consumer GPUs, memory-constrained  | vLLM, TensorRT-LLM, llama.cpp |
| **W8A8 (SmoothQuant)** | INT8 accelerators, high throughput | TensorRT-LLM, ONNX Runtime    |
| **FP8**                | H100/H200 deployments              | TensorRT-LLM                  |
| **W4A4**               | Research, extreme compression      | Limited                       |
+------------------------+------------------------------------+-------------------------------+

### Framework Integration {#sec-optimization-quant-frameworks}

Production serving frameworks integrate quantization with their batching and memory management systems.

**vLLM quantization**:

```python
from vllm import LLM

# Load AWQ-quantized model
llm = LLM(
    model="TheBloke/Llama-2-70B-AWQ",
    quantization="awq",
    dtype="float16",  # Activations in FP16
    gpu_memory_utilization=0.9,
)
```

vLLM automatically handles quantized weight loading, kernel selection for W4A16 GEMM operations, and KV cache management.

**TensorRT-LLM quantization**:

```bash
# Quantize model with AWQ
python quantize.py --model_dir /path/to/llama-70b \
                   --output_dir /path/to/llama-70b-awq \
                   --qformat int4_awq \
                   --calib_size 512
```

TensorRT-LLM generates optimized kernels for specific GPU architectures, fuses operations to minimize memory traffic, and supports both weight-only and W8A8 quantization.

**Quantization + PagedAttention**: Quantized models combine with PagedAttention for maximum memory efficiency:

$$\text{Max batch} = \frac{\text{GPU Memory} - \text{Quantized Weights}}{\text{KV Cache per Sequence}}$$

A 70B model with 4-bit weights requires approximately 35GB, leaving 45GB on an 80GB A100 for KV cache. With FP16 KV cache at 2.5MB per 1K tokens per sequence, this supports ~18 concurrent 1K-token sequences versus ~8 with FP16 weights.

### Quantization Selection Guidelines {#sec-optimization-quant-selection}

Choosing the appropriate quantization method depends on deployment constraints:

**Decision framework**:

```
1. Is latency or throughput the primary goal?
   - Latency-sensitive: Prefer FP16/BF16 (no quantization overhead)
   - Throughput-oriented: Quantization typically beneficial

2. What hardware is available?
   - H100/H200: Consider FP8 (native support, minimal quality loss)
   - A100/A10G: W8A8 or W4A16 depending on workload
   - Consumer GPUs: W4A16 often necessary for memory

3. What quality requirements exist?
   - <0.5% degradation acceptable: AWQ 4-bit
   - <1% degradation acceptable: GPTQ 4-bit or SmoothQuant W8A8
   - No degradation acceptable: FP16/BF16 only

4. Is the model compute-bound or memory-bound?
   - Compute-bound (prefill): W8A8 provides 2x speedup
   - Memory-bound (decode): W4A16 provides 4x memory bandwidth
```

**Quantization impact on serving cost**:

+-------------------------+------------------------------------+
| **Configuration**       | **Cost per 1M tokens (estimated)** |
+========================:+===================================:+
| **FP16 on 8xA100**      | $2.40                              |
| **AWQ 4-bit on 4xA100** | $1.20                              |
| **AWQ 4-bit on 2xA100** | $0.60                              |
+-------------------------+------------------------------------+

Quantization can reduce serving costs by 2-4x while maintaining acceptable quality, making it essential for cost-effective LLM deployment.

## Summary {#sec-optimization-summary}

Optimization at scale moves beyond modifying weights to modifying system architecture. By recognizing the distinct physical constraints of different inference phases, managing memory like an operating system, and speculating on future outputs, we can achieve performance gains orders of magnitude larger than kernel-level micro-optimizations alone.

The "Memory Wall" forces a fundamental rethink of the inference stack. The physics of prefill and decode phases explains why disaggregated serving is the only way to maximize hardware efficiency for long-context models. PagedAttention functions as the virtual memory manager for the KV cache, while prefix caching and speculative decoding trade abundant compute for lower user-facing latency. Advanced quantization algorithms like AWQ and GPTQ make serving trillion-parameter models economically viable.

::: {.callout-important title="Key Takeaways"}

* **Software-Hardware Co-design**: Efficient optimization at scale requires matching model execution patterns to hardware constraints—specifically separating compute-bound prefill from memory-bound decode.
* **Memory Virtualization**: PagedAttention eliminates the fragmentation that traditionally wasted up to 80% of GPU memory, enabling the massive batch sizes required for low-cost serving.
* **Speculative Execution**: By using a small "draft" model to predict tokens, we can accelerate the slowest part of LLM generation (the serial decode) without sacrificing the accuracy of the target model.
* **Calibration-Aware Quantization**: At fleet scale, simple post-training quantization fails for LLMs. Advanced methods like AWQ protect salient weights to maintain model quality at 4-bit precision.

:::

The optimizations developed here—managing the KV cache like an OS and speculating on future tokens—break the serial bottlenecks of individual model instances.

The next chapter, @sec-edge-intelligence, examines how we extend these optimizations to the absolute limit of the network: the billions of resource-constrained devices at the edge.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
