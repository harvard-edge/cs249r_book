---
---

# Optimization at Scale {#sec-optimization-at-scale}

::: {layout-narrow}
:::.column-margin
_Gemini Pro 3 Prompt: A technical visualization of system-level machine learning optimization. The scene depicts a tiered inference architecture: 'Pre-fill' nodes (dense compute) handing off state to 'Decode' nodes (high memory bandwidth). Visual elements include a speculative decoding tree expanding potential future tokens, and a shared memory pool representing PagedAttention reducing fragmentation. The style is schematic and precise, emphasizing the flow of data and state between specialized components._
:::

\noindent
![](images/png/cover_optimization.png)

:::

## Purpose {.unnumbered}

_How do we squeeze maximum throughput and minimum latency from massive models when single-device optimizations are no longer enough?_

Foundational optimization techniques like quantization and pruning are typically applied to individual models. At the scale of modern production systems—serving billions of tokens per day with models exceeding device memory—optimization shifts from a model-level concern to a system-level architecture. We are no longer just optimizing a matrix multiplication; we are optimizing the flow of petabytes of state across a distributed fleet.

This chapter examines optimizations that only emerge at scale: disaggregated serving architectures that split compute-bound and memory-bound phases, memory management techniques like PagedAttention that virtually address transformer state, and speculative execution strategies that trade abundant compute for lower latency.

::: {.callout-tip title="Learning Objectives"}
- Design disaggregated serving architectures that decouple pre-fill (compute-bound) from decode (memory-bound) phases to maximize cluster utilization.
- Implement advanced memory management techniques like PagedAttention and Prefix Caching to eliminate fragmentation and enable massive batch sizes.
- Evaluate speculative decoding strategies, calculating the optimal draft/target model ratio based on acceptance rates and hardware latencies.
- Architect synthetic data pipelines for scale-driven distillation, using teacher models to generate training curricula for efficient student deployment.
:::

## Disaggregated Serving: Splitting the Workload {#sec-disaggregated-serving}

The fundamental physics of Large Language Model (LLM) inference reveals a dichotomy:
*   **Pre-fill Phase**: Processing the input prompt. Compute-bound (matrix-matrix multiplies). Requires high FLOPS.
*   **Decode Phase**: Generating tokens one by one. Memory-bandwidth bound (matrix-vector multiplies). Requires high HBM bandwidth.

In a monolithic server, one hardware configuration must serve both, leading to inevitable inefficiency. **Disaggregated Serving** splits these phases onto specialized hardware pools.

### The Physics of Pre-fill vs. Decode
*   **Arithmetic Intensity Analysis**: Applying the Roofline model to show why these phases sit in different hardware regimes.
*   **KV Cache Handover**: The system challenge of moving gigabytes of KV cache state from Pre-fill nodes to Decode nodes efficiently.

### Architecture Patterns
*   **Split-Serving**: Routing requests through a pre-fill pool before handing off to a decode pool.
## KV Cache Management {#sec-inference-kv-cache}

Load balancing distributes requests across replicas at the service level for all model types. Within each replica, we now narrow our focus to a challenge specific to autoregressive language models. While recommendation and vision models have fixed memory footprints during inference, LLMs accumulate state as they generate tokens. This accumulated state, stored in the key-value (KV) cache, represents a distinctive memory management challenge that does not exist for other model types.

Autoregressive language models maintain KV caches that store attention context from previous tokens, enabling efficient generation without recomputing attention over the entire sequence history. As context lengths grow and serving scales, KV cache management becomes a critical bottleneck at the replica level of the serving hierarchy. A 70B parameter model with 128K context can require over 100GB just for KV cache, exceeding the model weights themselves.

This section examines the memory management techniques that enable efficient LLM serving at scale: PagedAttention for fragmentation-free allocation, prefix caching for common prompt sharing, and speculative decoding for latency reduction.

### KV Cache Fundamentals {#sec-inference-kv-cache-fundamentals}

As established in transformer architecture fundamentals, autoregressive generation without caching requires $O(t^2)$ computation per token because each transformer layer must recompute attention keys and values for all previous tokens. The KV cache stores these computed key and value vectors, reducing generation to $O(t)$ per token. For serving at scale, this memory savings creates a critical management challenge since KV cache memory can exceed model weights for long contexts.

The cache size grows with context as calculated by @eq-kv-cache-size:

$$\text{KV cache size} = 2 \times L \times H \times S \times B \times P$$ {#eq-kv-cache-size}

where:

- $L$ = number of layers
- $H$ = hidden dimension
- $S$ = sequence length
- $B$ = batch size
- $P$ = precision (bytes per element)
- Factor of 2 accounts for both keys and values

::: {.callout-example title="Engineering Calculation: KV-Cache Capacity Estimator"}
**The Problem**: You are serving Llama-3-70B (FP16 weights $\approx 140$ GB) on an 8xH100 node (640 GB total HBM). You want to determine the maximum batch size for a context length of 128K tokens.

**Formula**:
$$ M_{KV} = 2 \times n_{layers} \times n_{heads} \times d_{head} \times P_{prec} $$
$$ \text{Total Memory} = M_{weights} + (\text{Batch} \times \text{Context} \times M_{KV}) $$

**Parameters**:
*   $n_{layers} = 80$, $n_{heads} = 64$, $d_{head} = 128$.
*   $P_{prec} = 2$ bytes (FP16).
*   Context $= 131,072$ tokens.

**Step 1: Calculate Memory Per Token**
$$ M_{KV} = 2 \times 80 \times 64 \times 128 \times 2 = 2,621,440 \text{ bytes} \approx \mathbf{2.6 \text{ MB/token}} $$

**Step 2: Calculate Cache per Request (128K context)**
$$ 131,072 \text{ tokens} \times 2.6 \text{ MB/token} \approx \mathbf{340 \text{ GB/request}} $$

**Step 3: Determine Max Batch Size**
Available Memory for KV = $640 \text{ GB (Total)} - 140 \text{ GB (Weights)} - 20 \text{ GB (System)} = 480 \text{ GB}$.
$$ \text{Max Batch} = \lfloor \frac{480}{340} \rfloor = \mathbf{1} $$

**Conclusion**: Despite having 8 H100s, you can only serve **one** concurrent 128K-context request due to the memory wall. To fix this, you *must* use **PagedAttention** (to reduce fragmentation) and **GQA** (Grouped-Query Attention) to reduce $n_{heads}$.
:::

### The Fragmentation Problem {#sec-inference-kv-fragmentation}

Traditional memory allocation for KV cache pre-allocates contiguous memory for each sequence based on maximum expected length. This creates two forms of waste:

**Internal fragmentation**: Sequences shorter than the maximum allocation waste the unused portion. If maximum length is 4,096 but average output is 100 tokens, 97.5% of allocated memory is wasted.

**External fragmentation**: As sequences complete and new ones start, memory becomes fragmented into non-contiguous free blocks. Even with sufficient total free memory, no single block may be large enough for a new maximum-length allocation.

Consider a simplified example with 8 memory slots and maximum sequence length of 4:

```
Time 0: Allocate Seq A (slots 0-3), Seq B (slots 4-7)
        [A][A][A][A][B][B][B][B]

Time 1: Seq A completes (2 tokens), Seq B continues
        [ ][ ][A][A][B][B][B][ ]  <- A only used 2 slots

Time 2: Try to allocate Seq C (needs 4 slots)
        [ ][ ][A][A][B][B][B][ ]  <- No contiguous block of 4!

Result: 4 free slots but cannot allocate new sequence
```

Production systems report 60-80% memory waste from fragmentation under realistic workloads, severely limiting batch sizes and throughput.

### PagedAttention {#sec-inference-paged-attention}

PagedAttention [@kwon2023vllm], introduced in vLLM, applies virtual memory[^fn-virtual-memory] concepts to KV cache management. Instead of contiguous allocation, the KV cache is divided into fixed-size pages (typically 16-256 tokens), and sequences are allocated pages on demand.

[^fn-virtual-memory]: **Virtual memory for KV cache**: Just as operating systems use virtual memory to provide processes with contiguous address spaces backed by non-contiguous physical memory, PagedAttention provides sequences with logically contiguous KV caches backed by non-contiguous GPU memory blocks. This enables dynamic allocation, sharing, and efficient memory utilization without fragmentation.

The key concepts include page tables that map logical sequence positions to physical memory pages, block size that defines the number of tokens per page (typically 16 tokens), and physical blocks that provide fixed-size memory allocations assignable to any sequence.

::: {.callout-note title="Figure: PagedAttention Memory Mapping" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9]
  \definecolor{BlockColor}{RGB}{200,220,255}
  \definecolor{MappedColor}{RGB}{255,220,200}

  \tikzset{
    block/.style={draw=black!70, thick, minimum width=0.8cm, minimum height=0.6cm, font=\tiny},
    arrow/.style={->, >=stealth, thick, gray!60}
  }

  % Logical View
  \node[anchor=west] at (0, 3.5) {\textbf{Logical KV Cache} (Contiguous)};
  \node[block, fill=BlockColor] (L0) at (0.5, 3) {P0};
  \node[block, fill=BlockColor] (L1) at (1.3, 3) {P1};
  \node[block, fill=BlockColor] (L2) at (2.1, 3) {P2};
  \node[block, fill=BlockColor] (L3) at (2.9, 3) {P3};

  % Page Table
  \node[draw, dashed, inner sep=4pt] at (5, 2) (table) {
    \begin{tabular}{c|c}
      \tiny Logic & \tiny Phys \\
      \hline
      \tiny P0 & \tiny B7 \\
      \tiny P1 & \tiny B2 \\
      \tiny P2 & \tiny B9 \\
      \tiny P3 & \tiny B4
    \end{tabular}
  };
  \node[above, font=\scriptsize] at (table.north) {Block Table};

  % Physical View
  \node[anchor=west] at (8, 3.5) {\textbf{Physical HBM} (Scattered)};
  \foreach \x/\y/\id/\fill in {8.5/3/B0/white, 9.3/3/B1/white, 10.1/3/B2/MappedColor, 10.9/3/B3/white,
                               8.5/2.2/B4/MappedColor, 9.3/2.2/B5/white, 10.1/2.2/B6/white, 10.9/2.2/B7/MappedColor,
                               8.5/1.4/B8/white, 9.3/1.4/B9/MappedColor, 10.1/1.4/B10/white, 10.9/1.4/B11/white} {
    \node[block, fill=\fill] (phys\id) at (\x, \y) {\id};
  }

  % Mapping arrows
  \draw[arrow] (L0) to[out=-90, in=180] (table);
  \draw[arrow] (table) to[out=0, in=180] (physB7);
  \draw[arrow] (table) to[out=0, in=180] (physB2);
  \draw[arrow] (table) to[out=0, in=180] (physB9);
  \draw[arrow] (table) to[out=0, in=180] (physB4);

\end{tikzpicture}
```
**PagedAttention Memory Mapping**. Decoupling the logical view of a sequence's KV cache (contiguous pages) from its physical storage (non-contiguous 16-token blocks). A block table maps logical pages to physical blocks, allowing the system to fill fragmentation gaps with small blocks from any sequence.
:::

PagedAttention provides several benefits. It eliminates internal fragmentation by allocating only the pages needed for actual tokens. It eliminates external fragmentation because any free page can be used by any sequence. It enables dynamic growth so sequences can grow without pre-allocation. It supports memory sharing so common prefixes can share physical pages.

::: {.callout-note title="PagedAttention Implementation Details"}

**Memory layout**:

```
Physical blocks (16 tokens × hidden_dim × 2 × precision):
Block 0:  [K₀...K₁₅, V₀...V₁₅]
Block 1:  [K₀...K₁₅, V₀...V₁₅]
...
Block N:  [K₀...K₁₅, V₀...V₁₅]
```

**Page table per sequence**:

```{.python}
class PageTable:
    def __init__(self, max_blocks):
        self.block_map = {}  # logical_block -> physical_block

    def allocate_block(self, logical_idx, physical_block):
        self.block_map[logical_idx] = physical_block

    def get_physical(self, logical_idx):
        return self.block_map[logical_idx]
```

**Attention kernel modification**:

Standard attention: `output = softmax(Q @ K.T / sqrt(d)) @ V`

PagedAttention:
```{.python}
def paged_attention(Q, page_table, physical_blocks, block_size):
    # Gather K, V from non-contiguous physical blocks
    for logical_idx in range(num_logical_blocks):
        physical_idx = page_table[logical_idx]
        K_block = physical_blocks[physical_idx].K
        V_block = physical_blocks[physical_idx].V
        # Compute attention for this block
        attention_scores = Q @ K_block.T / sqrt(d)
        output += softmax(attention_scores) @ V_block
    return output
```

**Performance impact**:

The gather operations add overhead, but it is minimal compared to the memory savings:

+--------------------+------------------------+---------------------------+
| **Approach**       | **Memory Utilization** | **Throughput (relative)** |
+:===================+=======================:+==========================:+
| **Contiguous**     | 30-40%                 | 1.0x (baseline)           |
| **PagedAttention** | 95%+                   | 2.5-4x                    |
+--------------------+------------------------+---------------------------+

The 2.5-4x throughput improvement comes from fitting more concurrent sequences in the same memory.

:::

### Prefix Caching {#sec-inference-prefix-caching}

Many LLM workloads share common prefixes across requests. System prompts like "You are a helpful assistant..." are prepended to every request. Few-shot examples use the same examples for many queries. Document context involves multiple questions about the same document. Recomputing these shared prefixes wastes both compute (prefill) and memory (duplicate KV cache entries).

**Prefix caching** shares KV cache entries across requests with common prefixes. @fig-prefix-caching demonstrates how shared system prompts avoid redundant computation:

::: {#fig-prefix-caching fig-env="figure" fig-pos="htb" fig-cap="**Prefix Caching via Block Sharing**. PagedAttention enables efficient prefix caching by allowing multiple sequences' block tables to point to the same physical blocks for shared content. In this example, the System Prompt is stored in blocks 0-5. Request A and Request B maps their first 6 logical pages to these same physical blocks, storing only their unique suffixes in new blocks. This dramatically reduces memory usage and prefill computation for workloads with shared context."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{SharedColor}{RGB}{144,238,144}
  \definecolor{UniqueColorA}{RGB}{173,216,230}
  \definecolor{UniqueColorB}{RGB}{255,182,193}
  \definecolor{BlockColor}{RGB}{240,240,240}

  \tikzset{
    block/.style={draw=black!60, fill=BlockColor, minimum width=0.8cm, minimum height=0.6cm, font=\scriptsize},
    ptr/.style={->, >=stealth, thick}
  }

  % Physical Memory Blocks (Central)
  \node[anchor=north] at (4, 4) {\textbf{Physical Blocks (GPU)}};

  % Shared Blocks 0-5
  \foreach \x in {0,1,2,3,4,5} {
      \node[block, fill=SharedColor] (pb\x) at (\x*0.9 + 1, 3) {B\x};
  }
  \node[right, font=\scriptsize] at (6.5, 3) {(System Prompt)};

  % Unique Blocks for A
  \node[block, fill=UniqueColorA] (pb10) at (1, 2) {B10};
  \node[block, fill=UniqueColorA] (pb11) at (1.9, 2) {B11};

  % Unique Blocks for B
  \node[block, fill=UniqueColorB] (pb12) at (4, 2) {B12};
  \node[block, fill=UniqueColorB] (pb13) at (4.9, 2) {B13};
  \node[block, fill=UniqueColorB] (pb14) at (5.8, 2) {B14};


  % Logical View - Request A
  \node[anchor=east] at (-1, 1) {\textbf{Request A}};
  \node[block, fill=SharedColor] (la0) at (0, 1) {P0};
  \node[block, fill=SharedColor] (la1) at (0.9, 1) {P1};
  \node[font=\scriptsize] at (1.8, 1) {...};
  \node[block, fill=UniqueColorA] (la6) at (2.7, 1) {P6};

  \draw[ptr, SharedColor!70!black] (la0) -- (pb0);
  \draw[ptr, SharedColor!70!black] (la1) -- (pb1);
  \draw[ptr, UniqueColorA!70!black] (la6) -- (pb10);

  % Logical View - Request B
  \node[anchor=east] at (-1, 0) {\textbf{Request B}};
  \node[block, fill=SharedColor] (lb0) at (0, 0) {P0};
  \node[block, fill=SharedColor] (lb1) at (0.9, 0) {P1};
  \node[font=\scriptsize] at (1.8, 0) {...};
  \node[block, fill=UniqueColorB] (lb6) at (2.7, 0) {P6};

  \draw[ptr, SharedColor!70!black] (lb0) to[in=220, out=45] (pb0);
  \draw[ptr, SharedColor!70!black] (lb1) to[in=220, out=45] (pb1);
  \draw[ptr, UniqueColorB!70!black] (lb6) -- (pb12);

  % Annotations
  \node[align=left, font=\scriptsize, anchor=west] at (7, 1) {Both requests point\\to same physical blocks\\for blocks 0-5};

\end{tikzpicture}
```
:::

**Implementation with PagedAttention**:

Prefix caching integrates naturally with PagedAttention through copy-on-write semantics:

```
System prompt → Physical blocks [0, 1, 2, 3, 4, 5]

Request A page table: [0, 1, 2, 3, 4, 5, 10, 11]  <- shares prefix blocks
Request B page table: [0, 1, 2, 3, 4, 5, 12, 13, 14]  <- shares prefix blocks
Request C page table: [0, 1, 2, 3, 4, 5, 15]  <- shares prefix blocks
```

All three requests reference the same physical blocks for the system prompt. Only when generating unique tokens do they allocate new blocks.

::: {.callout-note title="Prefix Caching at Scale"}

Consider a chatbot service with a 2000-token system prompt and 1000 concurrent users:

**Without prefix caching**:

- KV cache per user: 2000 + 500 (avg response) = 2500 tokens
- Total KV cache: 2500 × 1000 × 2 × 80 × 8192 × 2 = 6.5 TB

**With prefix caching**:

- Shared prefix: 2000 tokens (once)
- Unique per user: 500 tokens
- Total: (2000 × 1) + (500 × 1000) = 502,000 tokens
- Memory: 502,000 × 2 × 80 × 8192 × 2 = 1.3 TB

**Savings**: 80% reduction in KV cache memory, enabling 5x more concurrent users.

**Prefix hit rate** determines effectiveness:

+----------------------------------+---------------------+--------------------+
| **Workload**                     | **Prefix Hit Rate** | **Memory Savings** |
+:=================================+====================:+===================:+
| **Chatbot (same system prompt)** | 95%+                | 70-80%             |
| **Document QA (same doc)**       | 80-90%              | 50-70%             |
| **General API (diverse)**        | 20-40%              | 10-30%             |
+----------------------------------+---------------------+--------------------+

:::

### KV Cache Compression {#sec-inference-kv-compression}

Beyond efficient allocation, reducing the size of cached values provides additional memory savings. Several techniques compress the KV cache:

**Quantization**: Store cached keys and values at reduced precision.

$$\text{Compressed size} = \text{Original size} \times \frac{b_{compressed}}{b_{original}}$$

+---------------------+----------------------+--------------------+
| **Precision**       | **Memory per Token** | **Quality Impact** |
+====================:+=====================:+===================:+
| **FP16 (baseline)** | 2 bytes              | None               |
| **FP8**             | 1 byte               | &lt;1% degradation |
| **INT8**            | 1 byte               | 1-2% degradation   |
| **INT4**            | 0.5 bytes            | 3-5% degradation   |
+---------------------+----------------------+--------------------+

**Key observation**: KV cache values are more tolerant of quantization than model weights because they are intermediate activations, not learned parameters.

**Sliding window attention**: For very long contexts, maintain full cache only for recent tokens:

```
Full context: 100,000 tokens
Sliding window: 4,096 tokens

Cache strategy:

- Tokens 0-95,904: Discarded or compressed
- Tokens 95,904-100,000: Full precision cache

Trade-off: Cannot attend to very old tokens, but sufficient for most tasks.
```

**Grouped-query attention (GQA)**[^fn-gqa] [@ainslie2023gqa]: Architectural change that reduces KV cache by sharing key-value heads:

[^fn-gqa]: **GQA trade-off**: Grouped-query attention interpolates between multi-head attention (MHA, one KV head per query head) and multi-query attention (MQA, one KV head for all query heads). GQA with 8 KV heads for 64 query heads achieves 8x KV cache reduction versus MHA while maintaining most of MHA's model quality, unlike MQA which can degrade quality significantly.

+-------------------------+--------------+---------------------------+
| **Attention Type**      | **KV Heads** | **Cache Size (relative)** |
+:========================+=============:+==========================:+
| **Multi-head (MHA)**    | 64           | 1.0x                      |
| **Grouped-query (GQA)** | 8            | 0.125x                    |
| **Multi-query (MQA)**   | 1            | 0.016x                    |
+-------------------------+--------------+---------------------------+

Modern models like Llama 2 [@touvron2023llama2] and Mistral use GQA specifically to reduce KV cache requirements.

### Speculative Decoding {#sec-inference-speculative-decoding}

Autoregressive generation is inherently sequential: each token depends on previous tokens. Speculative decoding[^fn-speculative] [@leviathan2023speculative; @chen2023accelerating] breaks this bottleneck by using a smaller draft model to predict multiple tokens, then verifying them in parallel with the target model.

[^fn-speculative]: **Speculative execution in LLMs**: The concept borrows from CPU speculative execution, where processors predict branch outcomes and execute instructions ahead of confirmation. Similarly, speculative decoding predicts multiple tokens and processes them speculatively, rolling back only when the target model disagrees. Unlike CPU speculation (which is invisible to software), LLM speculation requires explicit acceptance/rejection logic.

**Algorithm**:

1. Draft model generates $k$ tokens speculatively: $t_1, t_2, ..., t_k$
2. Target model verifies all $k$ tokens in a single forward pass
3. Accept prefix of correct tokens, reject from first incorrect token
4. Continue from last accepted token

**Why this works**: The draft model is much smaller (7B vs 70B) and can generate $k$ tokens in the time the target model generates 1 token. Verification is cheap because the target model can process all $k$ tokens in parallel (like prefill).

::: {.callout-note title="Speculative Decoding Example"}

**Target model**: Llama-70B (30 tokens/second)
**Draft model**: Llama-7B (300 tokens/second)
**Speculation length**: $k = 4$ tokens

Speculative decoding accelerates generation by using a small model to draft tokens which a larger model verifies in parallel.

**Scenario**: Generating "The quick brown fox jumps". @fig-speculative-decoding contrasts sequential baseline decoding with speculative verification:

::: {#fig-speculative-decoding fig-env="figure" fig-pos="htb" fig-cap="**Speculative Decoding Process**. Instead of generating tokens sequentially with the large target model (slow), a small draft model quickly proposes a sequence of $K$ tokens. The target model then verifies all $K$ tokens in a single parallel forward pass (similar to prefill). If the draft tokens match the target's output, they are accepted, effectively generating multiple tokens per target model step. If a mismatch occurs, the sequence is rolled back to the first error."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{DraftColor}{RGB}{255,228,196}
  \definecolor{VerifyColor}{RGB}{176,224,230}
  \definecolor{AcceptColor}{RGB}{144,238,144}
  \definecolor{RejectColor}{RGB}{255,182,193}

  \tikzset{
     step/.style={draw=black!50, rounded corners=2pt, minimum height=0.7cm, font=\scriptsize, align=center}
  }

  % Baseline
  \node[anchor=west] at (0, 4) {\textbf{Standard Decoding} (Sequential)};
  \draw[->] (0, 3.5) -- (8, 3.5) node[right] {Time};

  \node[step, fill=VerifyColor, minimum width=1.5cm] at (1, 3.7) {Gen T1};
  \node[step, fill=VerifyColor, minimum width=1.5cm] at (2.6, 3.7) {Gen T2};
  \node[step, fill=VerifyColor, minimum width=1.5cm] at (4.2, 3.7) {Gen T3};
  \node[step, fill=VerifyColor, minimum width=1.5cm] at (5.8, 3.7) {Gen T4};

  % Speculative
  \node[anchor=west] at (0, 2) {\textbf{Speculative Decoding}};
  \draw[->] (0, 1.5) -- (8, 1.5) node[right] {Time};

  % Draft Step
  \node[step, fill=DraftColor, minimum width=2.0cm] (draft) at (1.2, 1.0) {Draft Model\\Gen [T1..T4]};
  \node[below, font=\tiny] at (draft.south) {Fast, Low latency};

  % Verify Step
  \node[step, fill=VerifyColor, minimum width=2.0cm] (verify) at (3.5, 1.0) {Target Model\\Verify [T1..T4]};
  \node[below, font=\tiny] at (verify.south) {Parallel Forward Pass};

  % Outcome
  \node[step, fill=AcceptColor, minimum width=0.5cm] at (5.0, 1.8) {T1};
  \node[step, fill=AcceptColor, minimum width=0.5cm] at (5.6, 1.8) {T2};
  \node[step, fill=AcceptColor, minimum width=0.5cm] at (6.2, 1.8) {T3};
  \node[step, fill=RejectColor, minimum width=0.5cm] at (6.8, 1.8) {T4};

  \draw[->, thick, gray] (draft) -- (verify);
  \draw[->, thick, gray] (verify) -- (5.0, 1.5);

  \node[right, font=\scriptsize, align=left] at (7, 1.8) {Accept 3\\Reject 1};

  % Speedup annotation
  \draw[decorate, decoration={brace, amplitude=5pt}] (0.2, 3.2) -- (6.6, 3.2) node[midway, below=0.2cm, font=\scriptsize] {Standard Time};
  \draw[decorate, decoration={brace, amplitude=5pt}] (0.2, 0.5) -- (4.8, 0.5) node[midway, below=0.2cm, font=\scriptsize] {Speculative Time};

\end{tikzpicture}
```
:::

**Effective speedup**: 43/30 = 1.43x

**Factors affecting speedup**:

+---------------------+-------------------+
| **Acceptance Rate** | **Speedup**       |
+====================:+==================:+
| **90% (easy text)** | 2.5-3x            |
| **70% (typical)**   | 1.5-2x            |
| **50% (hard text)** | 1.2-1.5x          |
| **30% (very hard)** | &lt;1x (overhead) |
+---------------------+-------------------+

Speedup depends on how well the draft model predicts the target model's output. For well-aligned model pairs (same training data, similar architecture), acceptance rates of 70-80% are common.

:::

**Self-speculative decoding** uses early exit from the target model itself as the draft, avoiding the need for a separate model:

```
Target model layers: 80

Draft: Layers 1-20 → predict next token
Verify: Layers 1-80 → confirm or reject
```

This eliminates the need to load and manage a separate draft model, at the cost of lower acceptance rates than a dedicated draft model.

### KV Cache in Distributed Settings {#sec-inference-kv-cache-distributed}

The tensor and pipeline parallelism strategies from @sec-inference-sharding introduce additional KV cache management complexity:

**Tensor parallelism**: KV cache is sharded across devices along with attention heads. Each device stores cache for its subset of heads.

```
8-way tensor parallelism:
Device 0: KV cache for heads 0-7
Device 1: KV cache for heads 8-15
...
Device 7: KV cache for heads 56-63
```

**Cross-device sharing**: Prefix caching across tensor-parallel devices requires cache to be sharded identically on all devices. This is automatic when prefixes are processed with the same tensor-parallel configuration.

**KV cache migration**: When consistent hashing routes a conversation to a different replica (due to failure or rebalancing), the KV cache must be migrated:

```
Migration options:
1. Rebuild: Re-run prefill on new replica (500ms+ for long context)
2. Transfer: Send KV cache over network (100MB at 100Gbps = 8ms)
3. Hybrid: Transfer if small, rebuild if large

Decision threshold:
if cache_size_bytes / network_bandwidth < prefill_time:
    transfer()
else:
    rebuild()
```

For Llama-70B with 4K context, KV cache is ~80MB per sequence. At 100 Gbps, transfer takes 6.4ms versus ~500ms for prefill. Transfer is clearly better.

### Memory Management Best Practices {#sec-inference-kv-best-practices}

Effective KV cache management combines multiple techniques:

**Sizing the KV cache pool**:

```
Available GPU memory = Total - Weights - Activations - Overhead
KV pool size = 0.9 × Available  # Leave 10% headroom

Max concurrent sequences = KV pool size / (avg_seq_length × per_token_cache)
```

When cache is full, systems use several eviction policies. LRU (Least Recently Used) evicts sequences with oldest last access. Size-based eviction removes longest sequences first to free most memory. Priority-based eviction protects high-priority or paid-tier requests.

**Preemption** for continuous batching:

When a new high-priority request cannot fit, the system follows a sequence of steps. It selects victim sequences using the eviction policy. It swaps the victim's KV cache to CPU memory. It allocates GPU memory to the new request. When the victim is resumed, it swaps back from CPU.

::: {.callout-note title="KV Cache Memory Hierarchy"}

Production systems use a memory hierarchy for KV cache:

+--------------+--------------+-------------+-------------------+
| **Tier**     | **Capacity** | **Latency** | **Use Case**      |
+:=============+=============:+============:+:==================+
| **GPU HBM**  | 80GB         | 0ms         | Active sequences  |
| **CPU DRAM** | 1TB          | 1-5ms       | Swapped sequences |
| **NVMe SSD** | 10TB         | 10-50ms     | Long-term cache   |
+--------------+--------------+-------------+-------------------+

**Swap implementation**:

```{.python}
async def swap_to_cpu(sequence_id):
    kv_cache = gpu_cache[sequence_id]
    cpu_cache[sequence_id] = kv_cache.cpu()  # Async transfer
    gpu_cache.free(sequence_id)


async def swap_to_gpu(sequence_id):
    cpu_kv = cpu_cache[sequence_id]
    gpu_cache[sequence_id] = cpu_kv.cuda()  # Async transfer
    cpu_cache.free(sequence_id)
```

**Observed performance**:

- GPU-only (no swapping): 50 concurrent sequences
- GPU+CPU swapping: 500 concurrent sequences (10x)
- Average swap latency: 3ms (acceptable for non-urgent requests)

:::

## Weight Quantization for Serving {#sec-inference-weight-quantization}

The KV cache techniques we just examined address dynamic memory allocation during inference. But model weights themselves represent static memory consumption that persists regardless of batch size or sequence length. For a 70B model, weights alone consume 140GB in FP16, leaving limited capacity for KV cache on even the largest GPUs. Quantization offers a complementary approach: reduce the memory footprint of weights themselves.

Quantization reduces numerical precision of model weights and activations, decreasing memory footprint by 2-4x while increasing decode throughput, which is memory-bandwidth limited rather than compute limited. While quantization fundamentals like post-training quantization (PTQ) and quantization-aware training (QAT) are established techniques, serving at scale introduces distinct challenges. Models must be quantized after training without access to training data. Quantization must preserve quality across diverse inputs. Hardware deployment targets vary from datacenter GPUs to edge accelerators. This section examines quantization techniques specifically designed for production inference.

### LLM-Specific Quantization Challenges {#sec-inference-llm-quantization}

Large language models present unique quantization challenges distinct from vision or recommendation models. The outlier activation problem occurs because certain attention heads produce activation magnitudes orders of magnitude larger than typical values. Naive quantization clips these outliers, causing significant quality degradation.

Consider a Llama-70B layer where most activations fall within [-10, 10] but specific channels reach magnitudes of 1000+. Symmetric INT8 quantization with range [-127, 127] must choose:

- **Wide range** [-1000, 1000]: Most values map to 0, losing information
- **Narrow range** [-10, 10]: Outliers clip, causing large errors

This outlier distribution motivates the specialized quantization methods that follow.

### GPTQ: Layer-by-Layer Weight Quantization {#sec-inference-gptq}

GPTQ [@frantar2023gptq] quantizes LLM weights using Hessian-based[^fn-hessian] error compensation. Rather than quantizing all weights independently, GPTQ adjusts remaining weights to compensate for errors introduced by quantization.

[^fn-hessian]: **Hessian matrix in quantization**: The Hessian matrix $H = X^T X$ captures second-order information about how output changes with respect to weight perturbations. Weights with large Hessian diagonal entries have outsized impact on model outputs; GPTQ uses this information to prioritize preserving important weights and compensate for errors in less important ones.

The GPTQ algorithm processes the model layer by layer. For each layer, it calibrates using a small dataset (128-256 samples). It quantizes weights in order of decreasing Hessian magnitude. It adjusts remaining weights to minimize output error.

**Key insight**: The Hessian matrix $H = X^T X$ captures which weights most affect outputs. GPTQ builds on the Optimal Brain Surgeon (OBS) framework, using the inverse Hessian to guide error compensation.

**Column-wise processing algorithm**:

GPTQ processes each weight matrix column by column, using Cholesky decomposition of $H^{-1}$ for efficiency:

```
For each layer's weight matrix W:
  1. Compute Hessian: H = X^T X from calibration activations
  2. Apply Cholesky factorization to H^{-1}
  3. For each column q = 1 to d_col:
     a. Quantize column: w_q = round(W[:,q] / Δ) × Δ
     b. Compute quantization error: δ = W[:,q] - w_q
     c. Update remaining columns to compensate:
        W[:,q+1:] += δ × [H^{-1}][:,q+1:] / [H^{-1}]_{qq}
```

The compensation step propagates quantization error to unquantized columns, where the Hessian-weighted update minimizes output deviation. This achieves $O(d_{row} \cdot d_{col}^2)$ complexity versus $O(d_{row} \cdot d_{col}^3)$ for naive OBS.

**Quantization formula**:

$$w_q = \text{round}\left(\frac{w}{\Delta}\right) \cdot \Delta$$

where $\Delta$ is the quantization step size. GPTQ uses per-group quantization with group sizes of 128 weights, enabling finer-grained scaling that reduces quantization error for outlier channels.

**Performance characteristics**:

+---------------+----------+-------------------------+----------------------+-----------------------+
| **Model**     | **Bits** | **Perplexity Increase** | **Memory Reduction** | **Quantization Time** |
+==============:+=========:+========================:+=====================:+======================:+
| **Llama-7B**  | 4        | +0.3                    | 4x                   | 15 min                |
| **Llama-13B** | 4        | +0.2                    | 4x                   | 30 min                |
| **Llama-70B** | 4        | +0.15                   | 4x                   | 3 hours               |
+---------------+----------+-------------------------+----------------------+-----------------------+

GPTQ's strengths include fast quantization without retraining, minimal quality loss for 4-bit weights, and broad hardware compatibility. Its limitations include requiring calibration data, sensitivity to calibration set selection, and per-layer processing that cannot leverage cross-layer information.

### AWQ: Activation-Aware Weight Quantization {#sec-inference-awq}

AWQ [@lin2024awq] observes that not all weights are equally important. Weights connected to channels with large activation magnitudes have disproportionate impact on outputs.

**Key insight**: Rather than protecting weights based on their own magnitude, protect weights based on the magnitude of activations they produce.

**Algorithm**:

1. Run calibration samples to measure per-channel activation magnitudes
2. Identify "salient" channels with large activations
3. Scale weights for salient channels up before quantization
4. Scale outputs down correspondingly (fused into subsequent layer)

**Scaling formulation**:

For weight matrix $W$ and activation statistics $s$ (per-channel activation magnitudes):

$$W' = W \cdot \text{diag}(s^\alpha)$$

where $\alpha \in [0.5, 1.0]$ controls scaling aggressiveness. This preserves salient channels while allowing aggressive quantization of less important weights.

**Comparison with GPTQ**:

+----------------------------+---------------------------+-------------------------+
| **Aspect**                 | **GPTQ**                  | **AWQ**                 |
+:===========================+:==========================+:========================+
| **Error compensation**     | Adjusts remaining weights | Scales salient channels |
| **Calibration data**       | 128-256 samples           | 128 samples             |
| **Quality (4-bit)**        | Very good                 | Excellent               |
| **Speed**                  | Faster                    | Slightly slower         |
| **Hardware compatibility** | Broad                     | Broad                   |
+----------------------------+---------------------------+-------------------------+

AWQ typically achieves 0.5-1% lower perplexity degradation than GPTQ at the same bit-width, making it preferred for production deployments where quality is paramount.

### SmoothQuant: Migrating Quantization Difficulty {#sec-inference-smoothquant}

SmoothQuant [@xiao2023smoothquant] addresses the activation outlier problem by migrating quantization difficulty from activations to weights. Weights have predictable distributions; activations have unpredictable outliers. SmoothQuant transfers the outlier problem to weights where it can be handled with per-channel scaling.

**Core technique**: Insert smoothing operations that divide activations by per-channel scales while multiplying weights by the same scales:

$$Y = X W = (X \cdot \text{diag}(s)^{-1}) \cdot (\text{diag}(s) \cdot W) = \hat{X} \hat{W}$$

This transformation is mathematically equivalent but produces smoother activation distributions.

**Migration strength** $\alpha$ controls the trade-off:

$$s_j = \max(|X_j|)^\alpha / \max(|W_j|)^{1-\alpha}$$

- $\alpha = 0$: No migration, activations remain difficult
- $\alpha = 1$: Full migration, weights absorb all difficulty
- $\alpha = 0.5$: Balanced (typical setting)

**W8A8 deployment**: SmoothQuant enables INT8 quantization for both weights and activations:

+--------------------------+------------+---------------------+--------------------+---------------+
| **Configuration**        | **Memory** | **Prefill Speedup** | **Decode Speedup** | **Quality**   |
+=========================:+===========:+====================:+===================:+:==============+
| **FP16 (baseline)**      | 1x         | 1x                  | 1x                 | Baseline      |
| **W8A16 (weights only)** | 2x         | 1.3x                | 1.8x               | &lt;0.5% loss |
| **W8A8 (SmoothQuant)**   | 2x         | 1.8-2x              | 1.3-1.5x           | &lt;1% loss   |
+--------------------------+------------+---------------------+--------------------+---------------+

**Critical distinction**: W8A8 provides near 2x speedup for compute-bound prefill (large batch processing initial prompt), but only 1.3-1.5x speedup for memory-bound decode (generating tokens one at a time). LLM serving is typically decode-heavy, so real-world throughput improvements from W8A8 are often 1.3-1.7x rather than the theoretical 2x compute throughput of INT8 Tensor Cores.

### KV Cache Quantization {#sec-inference-kv-cache-quantization}

While weight quantization reduces model storage, the KV cache dominates memory consumption for long-context LLM serving. At 32K+ token context lengths, KV cache can exceed model weights in memory usage. KV cache quantization addresses this critical bottleneck.

**KV cache memory scaling**:

$$\text{KV Cache} = 2 \times \text{layers} \times \text{heads} \times d_{head} \times \text{seq\_len} \times \text{batch} \times \text{bytes}$$

For a 70B model (80 layers, 64 heads, 128 $d_{head}$) with 32K context in FP16:

$$\text{KV per sequence} = 2 \times 80 \times 64 \times 128 \times 32768 \times 2 = 85.9 \text{ GB}$$

A single long-context sequence consumes more memory than the model weights.

**Key insight**: KV cache values exhibit different distributions than model weights, enabling targeted quantization strategies:

- **Keys**: Relatively uniform distributions, tolerate aggressive quantization (2-4 bits)
- **Values**: More sensitive to quantization, require careful calibration (4-6 bits)

**KIVI (Key-Value cache quantization for Inference)**:

KIVI quantizes keys and values asymmetrically based on their sensitivity:

+---------------+----------------+--------------+---------------------+
| **Component** | **Precision**  | **Grouping** | **Quality Impact**  |
+:==============+===============:+:=============+:====================+
| **Keys**      | 2-bit          | Per-channel  | Minimal             |
| **Values**    | 4-bit          | Per-token    | &lt;0.5% perplexity |
| **Combined**  | ~3-bit average | Mixed        | &lt;1% perplexity   |
+---------------+----------------+--------------+---------------------+

**Memory impact of combined weight and KV quantization**:

+-------------------+-----------------+--------------------+------------------+
| **Configuration** | **Weight Size** | **KV Cache (32K)** | **Total Memory** |
+==================:+================:+===================:+=================:+
| **FP16/FP16**     | 140GB           | 86GB               | 226GB            |
| **W4A16/FP16**    | 35GB            | 86GB               | 121GB            |
| **W4A16/KV4**     | 35GB            | 21GB               | 56GB             |
+-------------------+-----------------+--------------------+------------------+

KV cache quantization enables 4x longer contexts or 4x higher batch sizes on the same hardware.

**Integration with PagedAttention**: Quantized KV cache requires quantization-aware block management:

```python
# Conceptual: vLLM with KV cache quantization
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    kv_cache_dtype="fp8",  # FP8 E4M3 for KV cache
    quantization="awq",  # 4-bit weights
)
```

With FP8 KV cache, the per-sequence memory drops from 2.5MB to 1.25MB per 1K tokens, doubling maximum concurrent sequences.

### Rotation-Based Quantization {#sec-inference-rotation-quantization}

Traditional quantization methods (GPTQ, AWQ, SmoothQuant) address outliers through compensation or migration. **Rotation-based quantization** [@ashkboos2024quarot] takes a fundamentally different approach: mathematically transforming the weight and activation space to eliminate outliers entirely.

**Key insight**: Outliers are artifacts of the coordinate basis representation. Rotating to a different basis spreads extreme values uniformly, making all values quantization-friendly.

**QuaRot (Quantization with Rotation)**:

QuaRot applies orthogonal Hadamard transforms to weights and activations:

$$X' = X \cdot H, \quad W' = H^T \cdot W$$

where $H$ is a Hadamard matrix (orthogonal, efficiently computable without storage).

**Why Hadamard transforms work**: For a vector with one extreme outlier, the Hadamard transform distributes that outlier's magnitude across all dimensions:

```
Original:  [1000, 1, 1, 1]     # One large outlier
Hadamard:  [252, 250, 250, 250] # Uniform distribution
```

After transformation, no single value dominates, enabling uniform quantization.

**Advantages over migration-based methods**:

+-----------------------+-----------------+---------------------------+
| **Aspect**            | **SmoothQuant** | **QuaRot**                |
+:======================+:================+:==========================+
| **Calibration data**  | Required        | Not required (data-free)  |
| **Minimum precision** | W8A8            | W4A4                      |
| **Runtime overhead**  | ~0%             | ~3% (Hadamard transforms) |
| **Outlier handling**  | Migration       | Elimination               |
+-----------------------+-----------------+---------------------------+

**Performance comparison**:

+-----------------+---------------+----------------------------+-----------------+
| **Method**      | **Precision** | **LLaMA-2-70B Perplexity** | **vs Baseline** |
+:================+==============:+===========================:+================:+
| **FP16**        | W16A16        | 3.12                       | Baseline        |
| **SmoothQuant** | W8A8          | 3.18                       | +0.06           |
| **GPTQ**        | W4A16         | 3.24                       | +0.12           |
| **QuaRot**      | W4A4          | 3.31                       | +0.19           |
+-----------------+---------------+----------------------------+-----------------+

QuaRot achieves 4-bit weights AND 4-bit activations with quality competitive to GPTQ's 4-bit weights only. This enables approximately 8x memory reduction versus FP16.

**SpinQuant**: An extension of QuaRot that learns optimal rotation matrices during a short fine-tuning phase, improving quality at the cost of training compute.

### Hardware-Deployment Co-design {#sec-inference-quant-hardware}

Quantization strategies must match target hardware capabilities. Different accelerators support different precisions with varying performance multipliers.

**NVIDIA Tensor Core Support**:

+----------------+-------------------+-------------------+---------------------+
| **Format**     | **Ampere (A100)** | **Hopper (H100)** | **Speedup vs FP16** |
+===============:+:==================+:==================+====================:+
| **FP16**       | Yes               | Yes               | 1x                  |
| **BF16**       | Yes               | Yes               | 1x                  |
| **INT8**       | Yes               | Yes               | 2x                  |
| **FP8 (E4M3)** | No                | Yes               | 2x                  |
| **INT4**       | Via CUTLASS       | Native            | 4x                  |
+----------------+-------------------+-------------------+---------------------+

**Memory bandwidth dominance**: For autoregressive LLM decode, memory bandwidth limits throughput since each token reads the entire model:

$$\text{Decode throughput} \propto \frac{\text{Memory bandwidth}}{\text{Model size in bytes}}$$

4-bit quantization delivers 4x throughput improvement for memory-bound decode, making it highly valuable despite modest compute gains.

**Deployment configurations**:

+------------------------+------------------------------------+-------------------------------+
| **Quantization**       | **Best For**                       | **Framework Support**         |
+=======================:+:===================================+:==============================+
| **W4A16 (GPTQ/AWQ)**   | Consumer GPUs, memory-constrained  | vLLM, TensorRT-LLM, llama.cpp |
| **W8A8 (SmoothQuant)** | INT8 accelerators, high throughput | TensorRT-LLM, ONNX Runtime    |
| **FP8**                | H100/H200 deployments              | TensorRT-LLM                  |
| **W4A4**               | Research, extreme compression      | Limited                       |
+------------------------+------------------------------------+-------------------------------+

### Framework Integration {#sec-inference-quant-frameworks}

Production serving frameworks integrate quantization with their batching and memory management systems.

**vLLM quantization**:

```python
from vllm import LLM

# Load AWQ-quantized model
llm = LLM(
    model="TheBloke/Llama-2-70B-AWQ",
    quantization="awq",
    dtype="float16",  # Activations in FP16
    gpu_memory_utilization=0.9,
)
```

vLLM automatically handles quantized weight loading, kernel selection for W4A16 GEMM operations, and KV cache management.

**TensorRT-LLM quantization**:

```bash
# Quantize model with AWQ
python quantize.py --model_dir /path/to/llama-70b \
                   --output_dir /path/to/llama-70b-awq \
                   --qformat int4_awq \
                   --calib_size 512
```

TensorRT-LLM generates optimized kernels for specific GPU architectures, fuses operations to minimize memory traffic, and supports both weight-only and W8A8 quantization.

**Quantization + PagedAttention**: Quantized models combine with PagedAttention for maximum memory efficiency:

$$\text{Max batch} = \frac{\text{GPU Memory} - \text{Quantized Weights}}{\text{KV Cache per Sequence}}$$

A 70B model with 4-bit weights requires approximately 35GB, leaving 45GB on an 80GB A100 for KV cache. With FP16 KV cache at 2.5MB per 1K tokens per sequence, this supports ~18 concurrent 1K-token sequences versus ~8 with FP16 weights.

### Quantization Selection Guidelines {#sec-inference-quant-selection}

Choosing the appropriate quantization method depends on deployment constraints:

**Decision framework**:

```
1. Is latency or throughput the primary goal?
   - Latency-sensitive: Prefer FP16/BF16 (no quantization overhead)
   - Throughput-oriented: Quantization typically beneficial

2. What hardware is available?
   - H100/H200: Consider FP8 (native support, minimal quality loss)
   - A100/A10G: W8A8 or W4A16 depending on workload
   - Consumer GPUs: W4A16 often necessary for memory

3. What quality requirements exist?
   - <0.5% degradation acceptable: AWQ 4-bit
   - <1% degradation acceptable: GPTQ 4-bit or SmoothQuant W8A8
   - No degradation acceptable: FP16/BF16 only

4. Is the model compute-bound or memory-bound?
   - Compute-bound (prefill): W8A8 provides 2x speedup
   - Memory-bound (decode): W4A16 provides 4x memory bandwidth
```

**Quantization impact on serving cost**:

+-------------------------+------------------------------------+
| **Configuration**       | **Cost per 1M tokens (estimated)** |
+========================:+===================================:+
| **FP16 on 8xA100**      | $2.40                              |
| **AWQ 4-bit on 4xA100** | $1.20                              |
| **AWQ 4-bit on 2xA100** | $0.60                              |
+-------------------------+------------------------------------+

Quantization can reduce serving costs by 2-4x while maintaining acceptable quality, making it essential for cost-effective LLM deployment.

## Multi-Tenancy and Isolation {#sec-inference-multitenancy}

The optimizations we have examined so far, from batching through sharding, caching, and quantization, focus on individual model deployments. This moves us to the platform level of the serving hierarchy, where the challenge shifts from optimizing a single model to managing multiple models, customers, and workloads on shared infrastructure.

Multi-tenancy enables efficient resource utilization but introduces challenges around isolation, fairness, and quality of service guarantees. A noisy neighbor consuming excessive resources can degrade performance for all other tenants.

This section examines the techniques for sharing inference infrastructure while maintaining isolation between tenants.

### The Multi-Tenancy Challenge {#sec-inference-multitenancy-challenge}

Multi-tenancy provides significant benefits:

- **Cost efficiency**: Sharing infrastructure across tenants improves utilization
- **Operational simplicity**: Fewer clusters to manage, monitor, and upgrade
- **Statistical multiplexing**: Aggregate traffic is more predictable than per-tenant traffic

However, sharing introduces risks. @tbl-tenancy-comparison weighs utilization gains against isolation challenges:

- **Noisy neighbors**: One tenant's burst traffic impacts others
- **Resource contention**: GPU memory, network bandwidth, CPU cycles
- **Security boundaries**: Tenant data must remain isolated
- **SLO complexity**: Different tenants have different requirements

+--------------------------+------------------------+------------------------+
| **Aspect**               | **Single-Tenant**      | **Multi-Tenant**       |
+:=========================+:=======================+:=======================+
| **Resource utilization** | 30-50%                 | 70-90%                 |
| **Cost per request**     | Higher                 | Lower                  |
| **SLO guarantees**       | Simple                 | Complex                |
| **Isolation**            | Complete               | Requires engineering   |
| **Operational overhead** | Higher (many clusters) | Lower (fewer clusters) |
+--------------------------+------------------------+------------------------+

: **Single vs Multi-Tenant Tradeoffs**: Multi-tenancy reduces cost but requires careful isolation engineering. {#tbl-tenancy-comparison}

### Noisy Neighbor Problems {#sec-inference-noisy-neighbor}

The noisy neighbor problem occurs when one tenant's workload degrades performance for others sharing the same infrastructure.

**GPU memory contention**: A tenant with unexpectedly long sequences consumes KV cache memory, forcing evictions that impact other tenants.

```
Scenario: 3 tenants sharing GPU with 60GB KV cache pool

Normal state:
  Tenant A: 20GB (200 sequences)
  Tenant B: 20GB (200 sequences)
  Tenant C: 20GB (200 sequences)

Noisy neighbor (Tenant C starts long-context requests):
  Tenant C: 45GB (150 sequences, longer context)
  Tenant A: 7.5GB (evicted to 75 sequences)
  Tenant B: 7.5GB (evicted to 75 sequences)

Impact: Tenants A and B see 62% reduction in batch size
```

**Network bandwidth saturation**: A tenant streaming many large responses saturates network bandwidth, increasing latency for all tenants.

**Compute interference**: GPU time-sharing between tenants introduces context-switching overhead and unpredictable latency.

::: {.callout-note title="Quantifying Noisy Neighbor Impact"}

Consider an inference platform serving 10 tenants on shared H100 GPUs:

**Baseline (even load)**:

- Each tenant: 100 QPS, 10ms P99 latency
- GPU utilization: 70%
- All SLOs met

**Noisy neighbor scenario** (Tenant 3 bursts to 500 QPS):

+------------------+----------+-----------------+----------------+
| **Tenant**       | **QPS**  | **P99 Latency** | **SLO Status** |
+=================:+=========:+================:+:===============+
| **Tenant 1**     | 100      | 25ms            | Violated       |
| **Tenant 2**     | 100      | 28ms            | Violated       |
| **Tenant 3**     | 500      | 45ms            | Violated       |
| **Tenants 4-10** | 100 each | 22-30ms         | Violated       |
+------------------+----------+-----------------+----------------+

Without isolation, one tenant's burst causes cascade failures for all tenants.

**With isolation** (per-tenant resource quotas):

+------------------+------------------+-----------------+--------------------------+
| **Tenant**       | **QPS (actual)** | **P99 Latency** | **SLO Status**           |
+=================:+=================:+================:+:=========================+
| **Tenant 1**     | 100              | 11ms            | Met                      |
| **Tenant 2**     | 100              | 11ms            | Met                      |
| **Tenant 3**     | 120 (throttled)  | 50ms            | Violated (only for them) |
| **Tenants 4-10** | 100 each         | 11ms            | Met                      |
+------------------+------------------+-----------------+--------------------------+

Isolation contains the impact to the offending tenant.

:::

### Resource Quotas and Fair Sharing {#sec-inference-quotas}

Resource quotas limit what each tenant can consume, preventing any single tenant from monopolizing shared resources.

**Hard quotas** enforce strict limits:

```python
class TenantQuota:
    max_concurrent_requests: int  # e.g., 100
    max_kv_cache_mb: int  # e.g., 20,000
    max_qps: int  # e.g., 1,000
    max_batch_tokens: int  # e.g., 50,000


def admit_request(tenant_id, request):
    quota = get_quota(tenant_id)
    usage = get_usage(tenant_id)

    if usage.concurrent >= quota.max_concurrent:
        return RateLimitError("concurrent request limit")
    if usage.kv_cache_mb >= quota.max_kv_cache_mb:
        return RateLimitError("memory limit")
    if usage.qps >= quota.max_qps:
        return RateLimitError("rate limit")

    return admit(request)
```

**Soft quotas** with fair sharing allow exceeding limits when resources are available:

```
Tenant quota: 100 QPS (soft limit)

When cluster is underutilized (50%):
  Tenant can burst to 200 QPS (2x quota)

When cluster is saturated (90%):
  Tenant limited to 100 QPS (quota enforced)
```

This approach maximizes utilization while protecting tenants during contention.

**Max-min fairness** allocates resources to maximize the minimum allocation:

```
Total capacity: 1000 QPS
Tenants: A (demand 300), B (demand 200), C (demand 800)
Total demand: 1300 QPS (exceeds capacity)

Max-min allocation:
1. Give each tenant equal share: 333 QPS
2. A needs only 300, donate 33 to others
3. B needs only 200, donate 133 to others
4. C receives donations: 333 + 33 + 133 = 499 QPS

Final: A=300, B=200, C=500
All demands met up to fair share, C limited proportionally
```

### Priority Scheduling {#sec-inference-priority-scheduling}

When tenants have different SLO requirements, priority scheduling ensures high-priority requests receive resources first.

**Priority classes**:

+-----------------+--------------------+-------------------------+------------------------+
| **Class**       | **Use Case**       | **Preemption**          | **Resource Guarantee** |
+:================+:===================+:========================+:=======================+
| **Critical**    | Revenue-generating | Can preempt lower       | 100% reserved          |
| **Standard**    | General traffic    | Can preempt best-effort | Weighted share         |
| **Best-effort** | Background, batch  | Cannot preempt          | No guarantee           |
+-----------------+--------------------+-------------------------+------------------------+

**Priority-aware queuing**:

```
Incoming requests sorted by priority, then arrival time:

Queue state:
  [Critical-001] [Critical-002] [Standard-001] [Standard-002] [BestEffort-001]
       ↑ Process first

New Critical-003 arrives:
  [Critical-001] [Critical-002] [Critical-003] [Standard-001] [Standard-002]
       ↑ Jumps ahead of Standard requests
```

**Preemption for LLM serving**:

When a critical request arrives but all GPU slots are occupied by lower-priority requests:

1. Select victim request(s) from lowest priority class
2. Pause victim's generation (save KV cache state)
3. Allocate GPU slot to critical request
4. When critical request completes, resume victim

```
Before preemption:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]

Critical request arrives:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]
                ↓ preempt
  GPU slots: [Critical E] [Standard B] [Standard C] [Standard D]
  Paused: Best-effort A (KV cache saved to CPU)

After Critical E completes:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]
  (A resumed from saved state)
```

### Bulkhead Pattern {#sec-inference-bulkhead}

The bulkhead pattern[^fn-bulkhead] [@nygard2007releaseit] physically isolates tenant workloads, preventing failures from propagating across tenants. Named after ship compartments that contain flooding to isolated sections.

[^fn-bulkhead]: **Bulkhead pattern origins**: The Titanic's bulkheads failed because they did not extend to the top of the ship, allowing water to spill over as the ship tilted. Modern software bulkheads learn from this: effective isolation requires complete separation (dedicated resources) rather than partial isolation (shared pools with quotas), at least for critical workloads.

**Deployment-level bulkheads** dedicate replicas to specific tenants or tenant groups. @fig-bulkhead-patterns illustrates complete isolation between gold and standard tiers:

::: {#fig-bulkhead-patterns fig-env="figure" fig-pos="htb" fig-cap="**Bulkhead Isolation Patterns**. To prevent cascading failures in multi-tenant systems, bulkheads isolate resources. **Deployment-level bulkheads** (shown) assign dedicated physical replicas to high-priority tenants, ensuring complete isolation. **Request-level bulkheads** enforce strict concurrency limits within shared processes. Like ship compartments, these boundaries ensure that a failure or resource exhaustion in one segment cannot sink the entire platform."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{GoldColor}{RGB}{255,215,0}
  \definecolor{StdColor}{RGB}{220,220,220}
  \definecolor{FailColor}{RGB}{255,99,71}

  % Gold Tier System
  \node[anchor=west] at (0, 4) {\textbf{Gold Tier} (Dedicated)};
  \node[draw, fill=white, rounded corners] (gold_lb) at (1, 3) {Gold LB};

  \node[draw, fill=GoldColor!30] (r1) at (3.5, 3.5) {Replica 1};
  \node[draw, fill=GoldColor!30] (r2) at (3.5, 2.5) {Replica 2};

  \draw[->, thick] (gold_lb) -- (r1);
  \draw[->, thick] (gold_lb) -- (r2);

  % Standard Tier System
  \node[anchor=west] at (0, 1) {\textbf{Standard Tier} (Shared Pool)};
  \node[draw, fill=white, rounded corners] (std_lb) at (1, 0) {Std LB};

  \node[draw, fill=StdColor] (r3) at (3.5, 1.0) {Replica 3};
  \node[draw, fill=FailColor] (r4) at (3.5, 0.0) {Replica 4\\(Failure)};
  \node[draw, fill=StdColor] (r5) at (3.5, -1.0) {Replica 5};

  \draw[->, thick] (std_lb) -- (r3);
  \draw[->, thick] (std_lb) -- (r4);
  \draw[->, thick] (std_lb) -- (r5);

  % Isolation Barrier
  \draw[dashed, ultra thick, black!50] (-0.5, 1.8) -- (5.5, 1.8);
  \node[fill=white, inner sep=2pt, font=\scriptsize, align=center] at (2.5, 1.8) {\textbf{Complete Isolation}\\No Shared Resources};

  % Impact Annotation
  \node[align=left, font=\scriptsize, color=FailColor!80!black] at (5, 0) {Failure constrained\\to Shared Pool};
  \node[align=left, font=\scriptsize, color=GoldColor!50!black] at (5, 3) {Gold Tier\\unaffected};

\end{tikzpicture}
```
:::

Deployment-level bulkheads provide complete isolation for premium tenants, ensuring that their performance is never affected by other workloads. The tradeoff is lower overall resource utilization and increased operational overhead from managing dedicated infrastructure.

**Request-level bulkheads** limit the fraction of resources any single request can consume.

```
Per-request limits:
  max_input_tokens: 8,000
  max_output_tokens: 2,000
  max_execution_time: 30s

Prevents single request from consuming excessive resources.
```

**Failure isolation**: Errors in one tenant's requests do not affect others.

```
Tenant A sends malformed input causing model error:
  Without bulkhead: Error may crash shared inference worker
  With bulkhead: Error caught, only Tenant A's request fails
                 Other tenants continue normally
```

::: {.callout-note title="Bulkhead Configuration for API Tiers"}

Consider an LLM API with three service tiers:

**Enterprise tier**:

- Dedicated GPU pool (no sharing)
- Custom model fine-tuning
- 99.9% availability SLO
- Price: $$$

**Professional tier**:

- Shared GPU pool with guaranteed capacity
- Priority scheduling over free tier
- 99.5% availability SLO
- Price: $$

**Free tier**:

- Shared GPU pool, best-effort
- Rate limited (10 QPS)
- No SLO guarantee
- Price: Free

**Bulkhead configuration**:

```{.yaml}
tiers:
  enterprise:
    gpu_pool: "dedicated"
    isolation: "hardware"
    replicas: 8
    preemption: false

  professional:
    gpu_pool: "shared-premium"
    isolation: "resource-quota"
    quota_fraction: 0.7  # 70% of shared pool
    preemption: true

  free:
    gpu_pool: "shared-premium"
    isolation: "resource-quota"
    quota_fraction: 0.3  # 30% of shared pool
    preemption: false  # can be preempted
```

:::

### Model Isolation {#sec-inference-model-isolation}

When multiple models run on shared infrastructure, additional isolation is needed:

**Memory isolation**: Ensure one model's memory usage does not impact others.

```
GPU memory partitioning (80GB H100):

Model A: 40GB reserved (50%)
Model B: 30GB reserved (37.5%)
Shared pool: 10GB (12.5%)
```

**Compute isolation**: GPU time-sharing between models introduces latency variance. Options include:

- **MIG (Multi-Instance GPU)**[^fn-mig]: Hardware partitioning of A100/H100 into isolated GPU instances

[^fn-mig]: **Multi-Instance GPU (MIG)**: NVIDIA technology introduced with Ampere (A100) that partitions a GPU into up to 7 isolated instances, each with dedicated memory bandwidth and L2 cache. Unlike time-slicing, MIG provides hardware-level isolation with guaranteed performance, enabling secure multi-tenancy on expensive datacenter GPUs.
- **Time-slicing**: Cooperative scheduling between models (higher overhead)
- **Dedicated GPUs**: Each model gets dedicated hardware (lower utilization)

**Model loading isolation**: Loading one model should not evict another from GPU memory.

```python
class ModelManager:
    def load_model(self, model_id, priority):
        required_memory = get_model_size(model_id)
        available = get_free_gpu_memory()

        if required_memory > available:
            # Check if eviction would violate isolation
            evictable = get_evictable_memory(priority)
            if required_memory > evictable:
                raise InsufficientMemoryError(
                    "Cannot load without violating isolation constraints"
                )
            evict_lower_priority_models(priority, required_memory)

        load_to_gpu(model_id)
```

### Observability for Multi-Tenancy {#sec-inference-multitenancy-observability}

Effective multi-tenancy requires per-tenant visibility into resource consumption and performance:

**Per-tenant metrics**:

- Request count, latency distribution (P50, P95, P99)
- GPU memory usage, KV cache utilization
- Throttling events, preemption counts
- Error rates by error type

**Alerting thresholds**:

```yaml
alerts:
  - name: tenant_slo_violation
    condition: p99_latency > slo_target * 1.1
    for: 5m
    severity: warning

  - name: tenant_quota_exhaustion
    condition: usage > quota * 0.9
    for: 1m
    severity: warning

  - name: noisy_neighbor_detection
    condition: usage > fair_share * 2.0
    for: 5m
    severity: info
```

**Chargeback and attribution**: Track resource consumption for billing and capacity planning.

```
Tenant A monthly report:
  Total requests: 10,000,000
  GPU-seconds consumed: 50,000
  KV cache GB-hours: 2,500
  Network egress GB: 100

  Billed: $X based on consumption
```

## Autoscaling {#sec-inference-autoscaling}

Multi-tenancy, examined in the previous section, addresses how to share fixed infrastructure capacity across multiple workloads efficiently. Autoscaling addresses the complementary problem: how to adjust that capacity dynamically as aggregate demand changes.

Production inference systems experience traffic fluctuations that make static provisioning inefficient. Autoscaling dynamically adjusts capacity to match demand, reducing costs during low-traffic periods while maintaining SLOs during peaks. Without autoscaling, operators must choose between overprovisioning (paying for idle capacity during quiet periods) or underprovisioning (violating SLOs during traffic spikes). With effective autoscaling, infrastructure follows demand, though the cold start problem unique to GPU-based serving makes this significantly more challenging than traditional web service scaling.

This section examines autoscaling strategies for inference, with particular attention to the cold start problem and techniques for reducing its impact.

### Scaling Dimensions {#sec-inference-scaling-dimensions}

Inference systems can scale along multiple dimensions. @tbl-scaling-dimensions contrasts horizontal, vertical, and batch size scaling:

**Horizontal scaling (replicas)**: Add or remove model replicas to adjust throughput.

$$\text{Capacity} = \text{Replicas} \times \text{Per-replica throughput}$$

**Vertical scaling (GPU type)**: Use more powerful GPUs for higher per-replica throughput.

$$\text{Cost efficiency} = \frac{\text{Throughput}}{\text{GPU cost}}$$

**Batch size scaling**: Adjust batch sizes to trade latency for throughput.

$$\text{Latency} \uparrow \text{ as } \text{Batch size} \uparrow \text{, Throughput} \uparrow$$

+-------------------------------+-------------+------------+--------------------------+
| **Scaling Type**              | **Latency** | **Cost**   | **Speed**                |
+:==============================+:============+:===========+:=========================+
| **Horizontal (add replicas)** | Unchanged   | Linear     | Slow (minutes)           |
| **Batch size**                | Increases   | Unchanged  | Instant                  |
| **Vertical (better GPU)**     | Unchanged   | Non-linear | Very slow (redeployment) |
+-------------------------------+-------------+------------+--------------------------+

: **Scaling Dimension Tradeoffs**: Each scaling approach has different characteristics. {#tbl-scaling-dimensions}

### The Cold Start Problem {#sec-inference-cold-start}

Cold start latency represents a significant challenge for serverless inference, with model loading often dominating the total startup time.

Unlike stateless web services that start in seconds, inference services have significant startup latency as defined in @eq-cold-start-time:

$$T_{cold start} = T_{provision} + T_{load} + T_{warmup}$$ {#eq-cold-start-time}

**Provisioning time** ($T_{provision}$): Acquiring a GPU instance takes 30 seconds to several minutes depending on cloud provider and GPU type.[^fn-cold-start]

[^fn-cold-start]: **Cold start in ML vs. serverless**: The cold start problem for GPU inference is 10-100x worse than for serverless functions. Lambda functions cold start in 100ms-1s; GPU inference cold starts in 1-10 minutes due to GPU allocation, model loading, and CUDA initialization. This fundamental difference makes predictive scaling and warm pools essential for GPU workloads.

**Model loading time** ($T_{load}$): Loading model weights from storage to GPU memory. For large models:

+------------------+---------------------+--------------------+
| **Model Size**   | **Load Time (SSD)** | **Load Time (S3)** |
+=================:+====================:+===================:+
| **7B (14GB)**    | 5s                  | 30s                |
| **70B (140GB)**  | 45s                 | 5min               |
| **175B (350GB)** | 2min                | 12min              |
+------------------+---------------------+--------------------+

**Warmup time** ($T_{warmup}$): First inference after loading is slower due to:

- JIT compilation of kernels
- CUDA context initialization
- Memory pool allocation
- Cache population

Warmup typically requires 10-30 dummy inferences, adding 5-30 seconds.

::: {.callout-note title="Cold Start Timeline for Llama-70B"}

Bringing up a new replica for Llama-70B on H100:

+-------------------------------+------------------+----------------+
| **Phase**                     | **Duration**     | **Cumulative** |
+:==============================+=================:+===============:+
| **Cloud API request**         | 5s               | 5s             |
| **GPU instance provisioning** | 60s              | 65s            |
| **Container startup**         | 10s              | 75s            |
| **Model download (S3)**       | 180s             | 255s           |
| **Model load to GPU**         | 45s              | 300s           |
| **CUDA warmup**               | 15s              | 315s           |
| **Readiness probe pass**      | 5s               | 320s           |
| **Total cold start**          | **5 min 20 sec** |                |
+-------------------------------+------------------+----------------+

**Implication**: Scaling decisions must anticipate demand 5+ minutes in advance. Reactive scaling alone cannot handle sudden traffic spikes.

@fig-cold-start-breakdown visualizes the cumulative timeline:

:::

::: {#fig-cold-start-breakdown fig-env="figure" fig-pos="htb" fig-cap="**Anatomy of a Cold Start**. Bringing up a new GPU inference replica is a multi-step process taking minutes. While container startup is fast, provisioning the specialized instance and downloading massive model weights (100GB+) dominate the timeline. CUDA context initialization and \"warmup\" inference passes add further delay. This 5+ minute lag makes purely reactive scaling dangerous for handling sudden traffic spikes."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, xscale=0.025, yscale=0.8]
  % xscale: 1cm = 40 seconds. Total 320s = 8cm
  \definecolor{ProvColor}{RGB}{240,230,140}
  \definecolor{DownColor}{RGB}{135,206,235}
  \definecolor{LoadColor}{RGB}{144,238,144}
  \definecolor{WarmColor}{RGB}{255,160,122}

  % Axis
  \draw[->] (0, 0) -- (330, 0) node[right] {Seconds};
  \foreach \x/\label in {0/0s, 60/1m, 120/2m, 180/3m, 240/4m, 300/5m} {
      \draw (\x, 0.1) -- (\x, -0.1) node[below, font=\scriptsize] {\label};
  }

  % Bar Stack
  \draw[fill=ProvColor] (0, 1) rectangle (65, 2);
  \node[font=\tiny] at (32, 1.5) {Provision};

  \draw[fill=DownColor] (65, 1) rectangle (245, 2);
  \node[font=\tiny] at (155, 1.5) {Model Download (Big!)};

  \draw[fill=LoadColor] (245, 1) rectangle (290, 2);
  \node[font=\tiny] at (267, 1.5) {Load};

  \draw[fill=WarmColor] (290, 1) rectangle (315, 2);
  \node[font=\tiny] at (302, 1.5) {Warm};

  % Connect total
  \draw[decorate, decoration={brace, amplitude=5pt, raise=5pt}] (0, 2) -- (315, 2) node[midway, above=10pt, font=\bfseries] {Total Cold Start: > 5 Minutes};

  % Legend
  \node[anchor=west, font=\scriptsize] at (0, 3.5) {\textbf{Breakdown for 70B Model}};

\end{tikzpicture}
```
:::

### Reactive Scaling {#sec-inference-reactive-scaling}

Reactive scaling adjusts capacity based on observed metrics:

**Metric-based scaling**:

```yaml
autoscaling:
  metric: cpu_utilization  # or gpu_utilization, queue_depth
  target_value: 70%
  scale_up_threshold: 80%
  scale_down_threshold: 50%
  cooldown_period: 300s
```

**Queue-depth scaling**: Scale based on request queue length.

$$\text{Desired replicas} = \left\lceil \frac{\text{Queue depth}}{\text{Queue target}} \times \text{Current replicas} \right\rceil$$

**Latency-based scaling**: Scale to maintain latency SLO.

$$\text{Desired replicas} = \left\lceil \frac{P99_{observed}}{P99_{target}} \times \text{Current replicas} \right\rceil$$

**Reactive scaling limitations**:

1. Response delay: Cold start time prevents rapid response
2. Oscillation: Can create scale-up/scale-down cycles
3. Over-provisioning: Must provision for worst-case during cold start

::: {.callout-note title="Reactive Scaling Response Analysis"}

Consider traffic spike from 1000 to 3000 QPS:

**Current state**: 10 replicas, 100 QPS each, 70% utilization

**Target state**: 30 replicas for 3000 QPS

**Without pre-warming**:

- T=0: Spike detected, scale-up triggered
- T=0 to T=5min: Cold start for 20 new replicas
- T=0 to T=5min: Existing 10 replicas handle 3000 QPS (300 QPS each)
- Utilization: 210% (overloaded)
- P99 latency: 500ms+ (SLO violated)

**With pool of warm spares (5 replicas)**:

- T=0: Spike detected, warm spares activated immediately
- T=0: 15 replicas handle 3000 QPS (200 QPS each)
- T=0 to T=5min: Scale up 15 more replicas
- Utilization: 140% (elevated but manageable)
- P99 latency: 80ms (SLO maintained)

Warm spares provide buffer during cold start period.

:::

### Predictive Scaling {#sec-inference-predictive-scaling}

Predictive scaling anticipates demand before it occurs, initiating scaling ahead of traffic changes.

**Time-series forecasting**: Use historical patterns to predict future demand.

```python
def predict_demand(current_time, history):
    # Seasonal decomposition
    daily_pattern = extract_daily_seasonality(history)
    weekly_pattern = extract_weekly_seasonality(history)

    # Trend estimation
    trend = estimate_trend(history)

    # Forecast
    predicted = (
        daily_pattern[current_time.hour]
        * weekly_pattern[current_time.weekday()]
        * trend
    )
    return predicted
```

**Event-driven scaling**: Scale proactively for known events.

```yaml
scheduled_scaling:
  - event: "product_launch"
    time: "2024-03-15 09:00 UTC"
    target_replicas: 50  # 5x normal
    ramp_up: 30min  # Start scaling 30min before

  - event: "weekly_newsletter"
    cron: "0 10 * * 1"  # Every Monday 10am
    target_replicas: 20  # 2x normal
    duration: 2h
```

**Hybrid approach**: Combine predictive baseline with reactive adjustment.

$$\text{Target replicas} = \max(\text{Predicted}, \text{Reactive}) + \text{Buffer}$$

::: {.callout-note title="Predictive Scaling for Daily Traffic Patterns"}

A chatbot service shows predictable daily patterns:

+----------------+-----------------+---------------------+
| **Time (UTC)** | **Typical QPS** | **Replicas Needed** |
+===============:+================:+====================:+
| 00:00-06:00    | 500             | 5                   |
| 06:00-09:00    | 1500            | 15 (ramp up)        |
| 09:00-17:00    | 3000            | 30 (peak)           |
| 17:00-20:00    | 2000            | 20 (ramp down)      |
| 20:00-00:00    | 1000            | 10                  |
+----------------+-----------------+---------------------+

**Predictive schedule** (accounting for cold start):

+----------+--------------+---------------------+-----------------------+
| **Time** | **Action**   | **Replicas Active** | **Replicas Starting** |
+=========:+:=============+====================:+======================:+
| 05:30    | Scale up     | 5                   | +10 warming           |
| 06:00    | Traffic ramp | 15                  | -                     |
| 08:30    | Scale up     | 15                  | +15 warming           |
| 09:00    | Peak traffic | 30                  | -                     |
| 17:00    | Scale down   | 20                  | -10 terminating       |
| 20:00    | Scale down   | 10                  | -10 terminating       |
| 00:00    | Scale down   | 5                   | -5 terminating        |
+----------+--------------+---------------------+-----------------------+

**Cost comparison**:

- Reactive only: Must over-provision during ramp (45 replicas peak)
- Predictive: Right-sized provisioning (30 replicas peak)
- Savings: 33% GPU cost reduction

:::

::: {.callout-note title="Figure: Predictive vs. Reactive Scaling" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  % Axes
  \draw[->, thick] (0,0) -- (10,0) node[right] {Time};
  \draw[->, thick] (0,0) -- (0,5) node[above] {Load / Capacity};

  % Traffic Curve (Sine-like spike)
  \draw[gray, ultra thick, dashed, domain=0:9, samples=100] plot (\x, {1 + 3*exp(-(\x-5)^2/2)});
  \node[gray, anchor=west] at (6, 4.2) {Actual Traffic};

  % Reactive Capacity (Lagging)
  \draw[red, thick] (0,1.2) -- (5.2,1.2) -- (5.2,4.2) -- (9,4.2);
  \node[red, anchor=west] at (5.5, 1.5) {Reactive (Late)};

  % Predictive Capacity (Early)
  \draw[blue, thick] (0,1.2) -- (3.5,1.2) -- (4.5,4.2) -- (9,4.2);
  \node[blue, anchor=west] at (2, 4.5) {Predictive (On Time)};

  % Deficit Area
  \fill[red!20, opacity=0.5] (5, 1.2) -- (5.2, 1.2) -- (5.2, 4.2) -- (5, 4.2) -- cycle;
  \node[red, font=\tiny, rotate=90] at (5.1, 2.7) {SLO VIOLATION};

\end{tikzpicture}
```
**Predictive vs. Reactive Scaling**. Reactive scaling (red line) responds to traffic spikes after they occur, leading to periods of under-provisioning where SLOs are violated due to cold start latency. Predictive scaling (blue line) anticipates traffic and begins provisioning capacity *before* the spike arrives, ensuring consistent performance.
:::

### Warm Pool Management {#sec-inference-warm-pools}

Maintaining a pool of pre-warmed replicas reduces effective cold start time:

**Warm pool sizing**:

$$\text{Warm pool size} = \frac{\text{Max expected spike}}{\text{Per-replica throughput}} \times \text{Headroom factor}$$

For example, if max spike is 2x normal and headroom factor is 1.5:

$$\text{Warm pool} = 2 \times 1.5 = 3\text{x minimum pool capacity}$$

**Warm pool cost**: Maintaining warm replicas costs money even when idle.

$$\text{Warm pool cost} = \text{Pool size} \times \text{GPU cost/hour} \times \text{Idle fraction}$$

Trade-off: More warm replicas = faster response but higher cost.

**Tiered warm pools**: Different readiness levels with different costs.

+----------+-----------------------------+-------------------+---------------------+
| **Tier** | **State**                   | **Response Time** | **Cost (relative)** |
+:=========+:============================+:==================+====================:+
| **Hot**  | GPU loaded, running         | Instant           | 100%                |
| **Warm** | GPU allocated, model loaded | 30s               | 60%                 |
| **Cold** | GPU not allocated           | 5+ min            | 0%                  |
+----------+-----------------------------+-------------------+---------------------+

```
Pool configuration:
  Hot: 2 replicas (instant burst capacity)
  Warm: 5 replicas (30s activation)
  Cold: Unlimited (cloud provider)

Scaling sequence:
  1. Activate Hot replicas immediately
  2. Activate Warm replicas within 30s
  3. Cold start new replicas if demand persists
```

### Scaling Response Time Analysis {#sec-inference-scaling-response}

The total time to respond to a scaling event is given by @eq-scaling-response:

$$T_{response} = T_{detect} + T_{decide} + T_{provision} + T_{warmup}$$ {#eq-scaling-response}

+------------------+--------------+-------------------------+
| **Component**    | **Duration** | **Optimization**        |
+:=================+=============:+:========================+
| **Detection**    | 10-60s       | Reduce metrics interval |
| **Decision**     | 1-5s         | Faster autoscaler       |
| **Provisioning** | 30s-5min     | Warm pools              |
| **Warmup**       | 5-30s        | Pre-compilation         |
+------------------+--------------+-------------------------+

**Optimizing each component**:

**Detection speed**: Use high-frequency metrics (1s vs 60s intervals) for faster detection. Trade-off: More metric volume, potentially noisier signals.

**Decision speed**: Pre-compute scaling plans based on predicted scenarios. When trigger occurs, execute pre-computed plan immediately.

**Provisioning speed**: Warm pools eliminate provisioning for anticipated demand. Spot/preemptible instances can reduce provisioning time (already running, just need allocation).

**Warmup speed**: Pre-compiled TensorRT engines skip JIT compilation. Lazy loading defers some initialization to first request.

### Spot and Preemptible Instances {#sec-inference-spot-instances}

Cloud providers offer discounted GPU instances[^fn-spot] that can be reclaimed with short notice:

[^fn-spot]: **Spot instance economics**: Spot instances represent cloud providers selling unused capacity at steep discounts (60-90%). The catch is 30-second to 2-minute termination notice. For inference, this means designing for graceful degradation: in-flight requests may need to be re-routed or failed, and KV cache state is lost. The cost savings justify this complexity for non-critical or burst traffic.

+----------------------+--------------+-------------------------+-----------------+
| **Instance Type**    | **Discount** | **Interruption Notice** | **Use Case**    |
+:=====================+=============:+:========================+:================+
| **On-demand**        | 0%           | Never                   | SLO-critical    |
| **Reserved**         | 30-60%       | Never                   | Steady baseline |
| **Spot/Preemptible** | 60-90%       | 30s-2min                | Burst capacity  |
+----------------------+--------------+-------------------------+-----------------+

**Graceful handling of spot termination**:

```python
def handle_spot_termination():
    # Received 2-minute warning
    # 1. Stop accepting new requests
    stop_accepting_requests()

    # 2. Complete in-flight requests (if possible)
    await complete_inflight(timeout=90)

    # 3. Save state for resumption elsewhere
    save_kv_cache_to_storage()

    # 4. Signal load balancer to redirect traffic
    deregister_from_loadbalancer()

    # 5. Terminate gracefully
    shutdown()
```

**Spot-aware architecture**:

```
Traffic distribution:

Request arrives
    │
    ▼
Load balancer
    │
    ├── 70% → On-demand replicas (guaranteed capacity)
    │
    └── 30% → Spot replicas (cost savings, may be interrupted)
```

Best-effort requests route to spot instances; SLO-critical requests use on-demand.

## Global Inference Infrastructure {#sec-inference-global}

Production inference systems serving global user bases must operate across multiple geographic regions. A user in Tokyo expects low-latency responses regardless of where models were trained or where the company headquarters is located. This section examines the architectural patterns for multi-region inference deployment.

### Why Multi-Region Matters {#sec-inference-global-why}

Single-region deployment creates fundamental limitations:

**Latency floor**: Network round-trip time (RTT) to distant users cannot be optimized away:

+-------------------+--------------------+-------------------------+
| **User Location** | **RTT to US-East** | **RTT to Local Region** |
+:==================+===================:+========================:+
| **New York**      | 10ms               | 10ms                    |
| **London**        | 75ms               | 10ms                    |
| **Tokyo**         | 150ms              | 10ms                    |
| **Sydney**        | 200ms              | 10ms                    |
+-------------------+--------------------+-------------------------+

For interactive applications (chatbots, autocomplete), these delays compound across multiple model calls per request.

**Availability**: Single-region deployment creates a single point of failure. Cloud region outages, while rare, affect all users simultaneously.

**Regulatory compliance**: Data residency requirements (GDPR, data sovereignty laws) may require processing user data within specific geographic boundaries.

### Multi-Region Architecture Patterns {#sec-inference-global-patterns}

**Pattern 1: Global load balancing with regional replicas**

```
                    Global Load Balancer
                    (Latency-based routing)
                           │
         ┌─────────────────┼─────────────────┐
         ▼                 ▼                 ▼
    US-East           EU-West           Asia-Pacific
    ┌─────────┐       ┌─────────┐       ┌─────────┐
    │ vLLM    │       │ vLLM    │       │ vLLM    │
    │ Replicas│       │ Replicas│       │ Replicas│
    └─────────┘       └─────────┘       └─────────┘
         │                 │                 │
         └─────────────────┼─────────────────┘
                           ▼
                    Model Registry
                    (Synchronized)
```

Each region runs independent inference replicas with identical models. The global load balancer routes users to the nearest region based on latency.

**Key considerations**:

- **Model synchronization**: Model updates must propagate to all regions. Options include:
  - Push-based: Central registry pushes to all regions (simple, potential inconsistency window)
  - Pull-based: Regions poll for updates (higher latency, guaranteed consistency)
  - Hybrid: Push notification + pull verification

- **Version consistency**: During model rollouts, different regions may briefly serve different versions. For most applications this is acceptable; for applications requiring strict consistency, implement version pinning in request routing.

**Pattern 2: Edge caching with central inference**

For models too large to replicate globally, cache responses at the edge:

```
User → Edge Cache (CDN) → Regional Proxy → Central Inference
           │                    │
           └── Cache hit ───────┘
               (< 10ms)

           └── Cache miss ──────────────────→
               (Full latency, populate cache)
```

**Effectiveness depends on request repeatability**:

+---------------------+--------------------+-----------------+
| **Workload**        | **Cache Hit Rate** | **Suitability** |
+:====================+===================:+:================+
| **Autocomplete**    | 60-80%             | Excellent       |
| **FAQ chatbot**     | 40-60%             | Good            |
| **Open-ended chat** | 5-15%              | Poor            |
| **Code generation** | 20-40%             | Moderate        |
+---------------------+--------------------+-----------------+

Semantic caching[^fn-semantic-cache] (caching based on embedding similarity rather than exact match) can improve hit rates for open-ended workloads.

[^fn-semantic-cache]: **Semantic caching**: Unlike traditional exact-match caching, semantic caching returns cached responses for queries that are semantically similar to previously seen queries. This requires embedding the query and searching a vector database for similar cached queries. The trade-off is increased complexity and potential for incorrect cache hits when semantically similar queries should produce different responses.

**Pattern 3: Federated inference with model sharding**

For the largest models, shard across regions:

```
User request
    │
    ▼
Request Router
    │
    ├── Layers 1-40  → US-East GPUs
    │
    └── Layers 41-80 → EU-West GPUs

    Pipeline parallelism across regions
```

This pattern is rarely practical due to inter-region latency dominating compute time, but may apply for extremely large models where no single region has sufficient GPU capacity.

### Cross-Region Failover {#sec-inference-global-failover}

When a region becomes unavailable, traffic must reroute to healthy regions:

**Active-active failover**:

```python
# Simplified global routing logic
def route_request(user_region, request):
    primary = get_nearest_healthy_region(user_region)
    secondary = get_second_nearest_healthy_region(user_region)

    try:
        return call_region(primary, request, timeout=2.0)
    except (Timeout, RegionUnavailable):
        # Failover with increased latency
        return call_region(secondary, request, timeout=5.0)
```

**Failover considerations for stateful LLM serving**:

- **Session affinity loss**: Users mid-conversation lose KV cache state. The fallback region must regenerate context from conversation history.
- **Capacity spike**: The receiving region sees sudden traffic increase. Pre-provision headroom (typically 30-50% over steady-state) or accept degraded latency during failover.
- **Gradual recovery**: When the failed region recovers, gradually shift traffic back to avoid oscillation.

### Global Model Deployment {#sec-inference-global-deployment}

Deploying model updates across regions requires careful coordination:

**Phased rollout strategy**:

```
1. Deploy to canary region (e.g., 1% traffic in US-East)
2. Monitor metrics for 1 hour
3. If healthy, deploy to remaining US-East replicas
4. Monitor for 4 hours
5. Deploy to EU-West (different user population)
6. Monitor for 4 hours
7. Deploy to Asia-Pacific
8. Complete rollout
```

**Rollback across regions**:

If issues are detected after partial deployment:

```
Region Status:
  US-East:      v2.1 (new) ← Issue detected
  EU-West:      v2.0 (old)
  Asia-Pacific: v2.0 (old)

Action: Rollback US-East to v2.0
  - Switch traffic to v2.0 replicas
  - Maintain v2.1 replicas for debugging
  - Do not proceed with EU-West deployment
```

**Metrics for global deployment health**:

+-------------------+----------------+---------------------------+
| **Metric**        | **Per-Region** | **Global**                |
+:==================+:===============+:==========================+
| **Error rate**    | &lt; 0.1%      | &lt; 0.1%                 |
| **P99 latency**   | &lt; target    | &lt; 2x single-region     |
| **Throughput**    | Stable         | Stable                    |
| **Model quality** | Within bounds  | Consistent across regions |
+-------------------+----------------+---------------------------+

### Cost Optimization Across Regions {#sec-inference-global-cost}

GPU pricing varies by region. Optimize placement for cost while meeting latency requirements:

+-------------+---------------------+---------------+-------------------------+
| **Region**  | **H100 Spot Price** | **On-Demand** | **Latency to US Users** |
+:============+====================:+==============:+========================:+
| **US-East** | $2.50/hr            | $4.00/hr      | 10-50ms                 |
| **US-West** | $2.30/hr            | $3.80/hr      | 30-70ms                 |
| **EU-West** | $2.80/hr            | $4.20/hr      | 75-100ms                |
+-------------+---------------------+---------------+-------------------------+

**Cost-aware routing**:

For latency-tolerant workloads (batch inference, background processing), route to the cheapest available region:

```python
def route_batch_request(request):
    if request.priority == "low":
        # Route to cheapest region with capacity
        return get_cheapest_region_with_capacity()
    else:
        # Route to nearest region
        return get_nearest_region(request.user_location)
```

This can reduce costs by 20-40% for batch workloads while maintaining SLOs for interactive traffic.

## Case Studies {#sec-inference-case-studies}

The techniques presented throughout this chapter come together in production systems serving billions of requests daily. This section examines four case studies that illustrate different points in the inference design space: Meta's recommendation serving (high volume, low latency), OpenAI's API infrastructure (LLM-focused), Google's search ranking (ensemble models), and TikTok's multimodal recommendation (video understanding combined with user modeling).

Each case study demonstrates how the principles of batching, sharding, load balancing, and autoscaling combine to meet specific requirements.

### Meta Recommendation Serving {#sec-inference-case-meta}

Meta's recommendation infrastructure serves predictions for feeds, ads, and content ranking across Facebook, Instagram, WhatsApp, and Messenger. This represents one of the largest production inference deployments in the world.

**Scale and requirements**:

- Request volume: Billions of requests per day
- Latency target: <10ms P99
- Model diversity: Hundreds of model variants
- Feature cardinality: Trillions of unique entities

**Architecture overview**:

```
User request → Feature collection → Embedding lookup → Model inference → Response

                    │                     │                  │
                    ▼                     ▼                  ▼
             Feature Store        Embedding Servers      GPU Inference
             (CPU, DRAM)         (CPU + SSD, 1000s)     (GPU, 100s)
```

**Key design decisions**:

**Embedding sharding at scale**: Embedding tables total over 100TB, requiring 1000+ shards. Meta uses a hybrid sharding strategy:

- Hot embeddings (top 1%): Replicated across memory on all inference servers
- Warm embeddings (next 10%): Column-sharded with 8-way parallelism
- Cold embeddings (remaining 89%): Row-sharded with consistent hashing, SSD-backed

This reduces embedding lookup latency from 50ms (naive) to 2ms through batching and locality optimization.

**Feature-parallel batching**: Instead of batching entire requests, Meta batches at the feature level. Each inference request triggers 5,000+ embedding lookups, but these lookups are batched across requests within a 1ms window. This achieves 90%+ memory bandwidth utilization on embedding servers.

**GPU-CPU hybrid architecture**: Dense model computation (ranking towers) runs on GPUs, while sparse embedding lookups run on CPU servers with large memory and SSD storage. This matches hardware to workload characteristics:

+------------------------+--------------+-------------+----------------+
| **Component**          | **Hardware** | **Latency** | **Throughput** |
+:=======================+:=============+============:+===============:+
| **Embedding lookup**   | CPU + SSD    | 2ms         | 50M lookups/s  |
| **Feature processing** | CPU          | 1ms         | 10M ops/s      |
| **Dense ranking**      | GPU          | 1.5ms       | 100K infs/s    |
+------------------------+--------------+-------------+----------------+

**Lessons learned**:

1. Embedding lookup, not model inference, often dominates latency for recommendation systems
2. Feature-parallel batching achieves higher efficiency than request-level batching
3. Hybrid CPU-GPU architectures match hardware to workload characteristics

### OpenAI API Infrastructure {#sec-inference-case-openai}

OpenAI's API serves GPT-4, GPT-3.5-turbo, and other models to millions of developers. The infrastructure must handle highly variable request sizes (from 10 tokens to 128K tokens) while maintaining quality of service across diverse workloads.

**Scale and requirements**:

- Request volume: Millions of requests per hour
- Latency target: Time-to-first-token (TTFT) <2s, throughput varies by model
- Model sizes: 7B to 175B+ parameters
- Context lengths: Up to 128K tokens

**Architecture overview**:

```
API Gateway → Rate Limiting → Request Router → Model Cluster → Response Streaming

                                     │
                                     ▼
                              ┌─────────────┐
                              │ Model Pool  │
                              │ ┌─────────┐ │
                              │ │ GPT-4   │ │
                              │ │ 8xH100  │ │
                              │ └─────────┘ │
                              │ ┌─────────┐ │
                              │ │GPT-3.5  │ │
                              │ │ 4xA100  │ │
                              │ └─────────┘ │
                              └─────────────┘
```

**Key design decisions**:

**Continuous batching with chunked prefill**: OpenAI was an early adopter of continuous batching (Orca-style) to maintain high GPU utilization despite variable output lengths. Chunked prefill bounds decode latency by processing long prompts in chunks that interleave with ongoing generation.

+--------------------------+---------------------+------------------------+
| **Batching Strategy**    | **GPU Utilization** | **TTFT (128K prompt)** |
+:=========================+====================:+=======================:+
| **Static batching**      | 45%                 | 30s (blocked)          |
| **Continuous batching**  | 75%                 | 30s (blocked)          |
| **Continuous + chunked** | 85%                 | 3s (streamed)          |
+--------------------------+---------------------+------------------------+

**Tensor parallelism for large models**: GPT-4 class models require 8-way or greater tensor parallelism for memory capacity and latency:

- 8xH100 per GPT-4 shard group
- NVLink for intra-node communication
- Consistent hashing for session affinity (KV cache reuse)

**Multi-tier rate limiting**: OpenAI implements rate limiting at multiple levels to prevent noisy neighbors:

- Per-API-key request rate limits
- Per-API-key token-per-minute limits
- Organization-level capacity quotas
- Global model capacity limits

**Dynamic capacity allocation**: During peak demand, OpenAI shifts capacity between models based on queue depth:

```
if gpt4_queue_depth > threshold:
    # Migrate some GPT-3.5 capacity to GPT-4
    reallocate_cluster_capacity(from="gpt-3.5", to="gpt-4", fraction=0.2)
```

**Lessons learned**:

1. Continuous batching is essential for LLM serving at scale
2. Prefix caching provides 2-3x efficiency for conversational workloads
3. Multi-tier rate limiting prevents cascade failures from traffic spikes

### Google Search Ranking {#sec-inference-case-google}

Google Search uses ensemble serving to combine multiple specialized models for query understanding, document relevance, and result ranking. This represents a different inference pattern: many smaller models coordinated for each request rather than one large model.

**Scale and requirements**:

- Request volume: Billions of searches per day
- Latency target: <200ms end-to-end
- Model count: Dozens of models per query
- Result processing: Thousands of documents per query

**Architecture overview**:

::: {.callout-note title="Figure: Ranking Cascade Architecture" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{L0Color}{RGB}{240,240,240}
  \definecolor{L1Color}{RGB}{220,220,220}
  \definecolor{L2Color}{RGB}{200,200,200}
  \definecolor{L3Color}{RGB}{180,180,180}

  \tikzset{
    stage/.style={draw=black!70, thick, align=center, minimum height=0.8cm}
  }

  % The Funnel
  \node[stage, fill=L0Color, minimum width=6cm] (L0) at (0, 3) {\textbf{Retrieval}\\1,000,000 → 10,000 items\\Cheap: Embeddings, BM25};
  \node[stage, fill=L1Color, minimum width=4.5cm] (L1) at (0, 2) {\textbf{L1 Ranking}\\10,000 → 1,000 items\\Fast: Linear Models, Decision Trees};
  \node[stage, fill=L2Color, minimum width=3cm] (L2) at (0, 1) {\textbf{L2 Ranking}\\1,000 → 100 items\\Moderate: Small Neural Nets};
  \node[stage, fill=L3Color, minimum width=1.5cm] (L3) at (0, 0) {\textbf{Final Rank}\\100 → 10 items\\Expensive: Large Transformers};

  % Down arrows
  \draw[->, ultra thick, gray!60] (L0) -- (L1);
  \draw[->, ultra thick, gray!60] (L1) -- (L2);
  \draw[->, ultra thick, gray!60] (L2) -- (L3);

  \node[anchor=west, font=\scriptsize, text=red!80] at (3.2, 3) {Low Precision, High Recall};
  \node[anchor=west, font=\scriptsize, text=green!60!black] at (1.0, 0) {High Precision, Low Recall};

\end{tikzpicture}
```
**Ranking Cascade Architecture**. To optimize latency and cost, search and recommendation systems use a cascade of increasingly complex models. Early stages (Retrieval) filter millions of candidates down to thousands using fast, cheap heuristics. Later stages use expensive, high-precision models only on the most promising candidates.
:::

**Key design decisions**:

**Cascading model architecture**: Rather than running one expensive model on all candidates, Google uses a ranking cascade:

+----------------------+----------------------+--------------------+--------------------+
| **Stage**            | **Model Complexity** | **Candidates**     | **Latency Budget** |
+=====================:+:=====================+===================:+===================:+
| **L0 (Retrieval)**   | Embedding lookup     | 1,000,000 → 10,000 | 10ms               |
| **L1 (First pass)**  | Linear model         | 10,000 → 1,000     | 20ms               |
| **L2 (Second pass)** | Small transformer    | 1,000 → 100        | 50ms               |
| **L3 (Final rank)**  | Large ensemble       | 100 → 10           | 100ms              |
+----------------------+----------------------+--------------------+--------------------+

This achieves 100x cost reduction compared to running L3 on all candidates.

**Speculative execution**: Given tight latency budgets, Google uses speculative execution for model ensembles:

```
# Instead of sequential:
#   q1 = model1(query)
#   q2 = model2(query)
#   q3 = model3(query, q1, q2)

# Speculative parallel:
async_q1 = async model1(query)
async_q2 = async model2(query)
async_q3 = async model3(query, predicted_q1, predicted_q2)

# Use actual results if they arrive in time, otherwise use speculative
```

**Custom TPU infrastructure**: Google runs ranking models on TPUs[^fn-tpu-search] optimized for transformer inference. TPU pods provide:

[^fn-tpu-search]: **TPU for inference**: While TPUs are often associated with training, Google's search infrastructure leverages TPU's deterministic performance and high memory bandwidth for inference. Unlike GPUs where utilization varies with batching, TPUs provide consistent latency through their systolic array architecture, critical for meeting strict search latency SLOs.

- 2D mesh topology for efficient AllReduce
- High memory bandwidth for attention operations
- Custom quantization for serving efficiency

**Deadline-aware scheduling**: Each sub-request carries a deadline, and workers prioritize by deadline proximity:

```
Worker queue: [Doc1: 50ms left] [Doc2: 30ms left] [Doc3: 80ms left]
                                      ↑ Process first

If deadline will be missed:
  Return cached/default result rather than timing out
```

**Lessons learned**:

1. Ranking cascades provide dramatic cost reduction for large candidate sets
2. Deadline propagation and priority scheduling are essential for ensemble serving
3. Custom hardware (TPU) enables efficiency that commodity GPUs cannot match

### TikTok Multimodal Recommendation {#sec-inference-case-tiktok}

TikTok's recommendation system combines video understanding (vision) with user modeling (recommendation) for personalized content ranking. This represents a multimodal inference challenge where different model types must coordinate.

**Scale and requirements**:

- Request volume: Millions of video rankings per second
- Latency target: <50ms P99
- Content volume: Millions of new videos daily
- Modalities: Video, audio, text, user signals

**Architecture overview**:

```
User request → User embedding → Candidate videos → Video understanding → Ranking

                    │                  │                    │
                    ▼                  ▼                    ▼
             User Tower          Video Cache          Vision Models
           (Transformer)        (Pre-computed)        (On-demand)
```

**Key design decisions**:

**Two-tower architecture with caching**[^fn-two-tower]: TikTok separates user understanding (online) from content understanding (offline):

[^fn-two-tower]: **Two-tower architecture**: A design pattern where user features and item features are encoded by separate "towers" (neural networks) into embeddings, then combined for scoring. This separation enables pre-computing item embeddings offline, reducing online inference to user tower computation plus a fast dot product. The architecture trades some model expressiveness for dramatic serving efficiency.

+-----------------+----------------------+-------------+--------------+
| **Tower**       | **Update Frequency** | **Latency** | **Compute**  |
+:================+:=====================+:============+:=============+
| **User tower**  | Real-time            | 5ms         | GPU (online) |
| **Video tower** | Hourly               | N/A         | GPU (batch)  |
+-----------------+----------------------+-------------+--------------+

Video embeddings are pre-computed and cached, eliminating vision inference from the critical path for most requests. Only new videos (uploaded within the hour) require online vision inference.

**Hybrid CPU-GPU inference**: Like Meta, TikTok uses CPU for embedding operations and GPU for dense model computation:

```
User features → CPU preprocessing (1ms)
             → Embedding lookup (2ms, CPU+DRAM)
             → Dense ranking (10ms, GPU)
             → Response formatting (1ms)
```

**Priority-based video analysis**: New video content is processed with different priorities:

+----------------+---------+------------------------------+
| **Priority**   | **SLA** | **Use Case**                 |
+:===============+========:+:=============================+
| **Critical**   | 5 min   | Creator with large following |
| **Standard**   | 30 min  | Normal uploads               |
| **Background** | 2 hours | Bulk/imported content        |
+----------------+---------+------------------------------+

This ensures popular creators' content reaches recommendations quickly while managing compute costs.

**Multimodal fusion**: TikTok combines multiple understanding modalities through late fusion:

```
Video embedding (512d) ─┐
Audio embedding (256d) ─┼─ Concat → Fusion MLP → Final embedding (256d)
Text embedding (256d)  ─┘
```

This allows independent updates to each modality's model without retraining the full system.

**Lessons learned**:

1. Separating online and offline components enables aggressive caching
2. Two-tower architectures scale better than joint models for user-item systems
3. Priority-based processing balances freshness against compute cost

### Cross-Cutting Observations {#sec-inference-case-observations}

Several patterns emerge across these case studies. @tbl-case-studies-summary identifies the primary technique and key innovation from each system:

**Separation of concerns**: All systems separate embedding/retrieval from ranking/generation. This enables specialized optimization for each component.

**Hybrid architectures**: No system uses GPUs exclusively. CPU+GPU combinations match hardware to workload characteristics.

**Caching at multiple levels**: Embedding caching, result caching, and intermediate representation caching all appear. Caching reduces compute at the cost of staleness.

**Progressive refinement**: Cascades and early-exit strategies reduce average compute by quickly filtering unlikely candidates.

**Deadline awareness**: All systems propagate deadlines and make explicit tradeoffs between quality and latency when under pressure.

+------------+-----------------------+---------------------------+
| **System** | **Primary Technique** | **Key Innovation**        |
+:===========+:======================+:==========================+
| **Meta**   | Embedding sharding    | Feature-parallel batching |
| **OpenAI** | Continuous batching   | Chunked prefill           |
| **Google** | Ranking cascade       | Speculative execution     |
| **TikTok** | Two-tower caching     | Multimodal fusion         |
+------------+-----------------------+---------------------------+

: **Case Study Summary**: Each system innovates on a core technique matched to its workload characteristics. {#tbl-case-studies-summary}

## Fallacies and Pitfalls {#sec-inference-fallacies-pitfalls}

The techniques presented throughout this chapter address real engineering challenges, but misconceptions about inference at scale remain common. Recognizing these fallacies and pitfalls helps practitioners avoid costly mistakes in system design and capacity planning.

**Fallacy: Inference at scale is synonymous with LLM serving.**

This misconception, reinforced by current discourse, leads to over-focus on LLM-specific techniques while ignoring the broader inference landscape. By request volume, recommendation systems constitute 80-90% of production inference at major technology companies, with vision and other models comprising most of the remainder. LLMs currently represent 1-5% of requests, though this is growing. A practitioner who only understands continuous batching and KV cache management will be unprepared for the feature-parallel batching and embedding sharding that dominate production inference. Technique selection must match the actual workload.

**Pitfall: Using training infrastructure for production serving.**

Training and serving have fundamentally different requirements. Training optimizes for aggregate throughput over hours or days; serving optimizes for per-request latency under strict SLOs. Training tolerates batch sizes of thousands; serving often requires batch sizes in single digits. Training accepts checkpoint-based recovery; serving requires graceful failover without user impact. Teams that deploy training clusters for serving often discover unacceptable latency variance, poor resource utilization, and difficulty meeting SLOs. Purpose-built serving infrastructure with appropriate batching, load balancing, and autoscaling is essential.

**Fallacy: Continuous batching solves all LLM serving problems.**

Continuous batching dramatically improves GPU utilization for LLM serving, but it addresses only one dimension of the problem. Prefill remains a bottleneck for long contexts, as the quadratic attention computation cannot be avoided regardless of how subsequent decode iterations are batched. As discussed in @sec-inference-kv-cache, KV cache memory, not compute, often limits batch size. Network bandwidth between sharded model components can dominate latency for large models. Continuous batching is necessary but not sufficient for efficient LLM serving.

**Pitfall: Sizing capacity based on average throughput.**

Queuing theory establishes that systems provisioned for average load violate SLOs during traffic peaks. At 80% average utilization, a modest 25% traffic spike pushes utilization above 100%, causing unbounded queue growth. As discussed in @sec-inference-autoscaling, the cold start problem exacerbates this: by the time new capacity is available (5+ minutes for GPU instances), SLO violations have already occurred. Capacity planning must account for peak load plus headroom, not average load.

**Fallacy: Load balancing does not matter much for inference.**

Simple load balancing strategies like round-robin seem adequate until examined quantitatively. As shown in @sec-inference-load-balancing, random assignment produces maximum queue lengths of $O(\log n / \log \log n)$ across $n$ servers. Power-of-two-choices reduces this to $O(\log \log n)$, an exponential improvement. For a 1,000-server cluster, this translates from ~4-5 requests maximum queue to ~2 requests. At the tail latencies that determine SLO compliance, this difference is substantial. The choice of load balancing algorithm has first-order impact on system performance.

**Pitfall: Ignoring the serving tax in latency budgets.**

Distributed inference introduces overhead absent from single-machine serving: network round-trips, serialization, load balancer decisions, and coordination for sharded models. This "serving tax" often consumes 10-30% of the latency budget. A team that achieves 70ms model inference on a single GPU may be surprised when end-to-end latency reaches 100ms in production due to these overheads. Latency budgets must explicitly account for distribution overhead, not just compute time.

**Fallacy: More GPU memory always means more batch size and throughput.**

While larger GPU memory enables larger batches for models that fit in memory, the bottleneck often shifts before memory is exhausted. Memory bandwidth limits throughput for bandwidth-bound operations (LLM decode). Compute limits throughput for compute-bound operations (prefill, vision inference). A 7B LLM on an H100 with 80GB memory achieves ~2,000 tokens/sec decode throughput regardless of batch size beyond 128 requests because HBM3 bandwidth (3.35 TB/s) saturates first. Adding memory to a bandwidth-bound workload provides no benefit. Understanding whether the workload is compute-bound, memory-bandwidth-bound, or capacity-bound guides appropriate resource allocation.

**Pitfall: Neglecting multi-tenancy isolation until production.**

In development and staging, single-tenant deployments work well. In production, noisy neighbors cause sudden, unpredictable performance degradation that is difficult to diagnose and resolve. A tenant bursting to 5x normal traffic can degrade latency for all other tenants on shared infrastructure. As emphasized in @sec-inference-multitenancy, resource quotas, priority scheduling, and bulkhead isolation must be designed into the system from the start, not retrofitted after production incidents.

::: {.callout-important title="Three Things to Remember"}

1. **Serving cost dominates training cost over a model's lifetime.** For high-volume applications, serving cost exceeds training cost by 100x or more. Every percentage point of serving efficiency improvement yields ongoing cost reduction. Optimize serving ruthlessly.

2. **Different model types require fundamentally different batching strategies.** Static batching for vision, continuous batching for LLMs, feature-parallel batching for recommendation systems. There is no universal optimal strategy. Match technique to workload.

3. **Power-of-two-choices provides exponential load balancing improvement.** Maximum queue length improves from $O(\log n / \log \log n)$ to $O(\log \log n)$ with minimal overhead (two probes per request). This simple technique should be standard for any distributed inference deployment.

:::

\n## Summary\n\nOptimization at scale moves beyond modifying weights to modifying system architecture. By recognizing the distinct physical constraints of different inference phases, managing memory like an operating system, and speculating on future outputs, we can achieve performance gains orders of magnitude larger than kernel-level micro-optimizations alone.\n\nThe next chapter, @sec-edge-intelligence, examines how we extend these optimizations to the absolute limit of the network: the billions of resource-constrained devices at the edge.
