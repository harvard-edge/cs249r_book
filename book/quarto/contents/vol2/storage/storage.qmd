---
title: "Storage Systems for ML"
bibliography: storage.bib
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR STORAGE SYSTEMS
================================================================================

CORE PRINCIPLE: Storage requirements differ dramatically by ML workload type.
Feature stores are critical for RecSys but less relevant for LLMs.
Checkpoint strategies vary by model architecture.

MODEL-SPECIFIC STORAGE CHARACTERISTICS:

| Model Type      | Training Data     | Checkpoint Size | Feature Store Need |
|-----------------|-------------------|-----------------|-------------------|
| LLMs            | Text corpora (TB) | TB per ckpt     | Low               |
| Recommendation  | Logs (PB)         | Embeddings (TB) | Critical          |
| Vision          | Images (TB)       | GB per ckpt     | Low-Medium        |
| Scientific      | Simulation (PB)   | Varies          | Domain-specific   |
| Speech          | Audio (TB)        | GB per ckpt     | Low               |

REQUIRED COVERAGE FOR THIS CHAPTER:

DATA LAKES AND TRAINING DATA:

- Text corpora: Deduplication, quality filtering (LLMs)
- Image datasets: Format optimization, augmentation on read (vision)
- User logs: Privacy, retention policies, sampling (recommendation)
- Include: Different preprocessing pipelines for different modalities

FEATURE STORES:

- Critical for recommendation: Real-time feature lookup, versioning
- Less relevant for LLMs: Training data != runtime features
- Include: Why RecSys engineers care deeply about feature stores

CHECKPOINT STORAGE:

- LLMs: TB-scale, infrequent, distributed across storage nodes
- Vision: GB-scale, more frequent, simpler management
- Recommendation: Embedding tables dominate, incremental updates
- Include: Different checkpoint strategies for different model types

MODEL REGISTRIES:

- Version control for model artifacts
- Metadata management across model types
- Include: How registry needs differ (single model vs ensemble)

DATA ACCESS PATTERNS:

- Sequential scan: Training data loading
- Random access: Feature lookup, embedding retrieval
- Include: Different I/O patterns for different workloads

CASE STUDIES TO INCLUDE:

- Meta feature store for recommendation
- Google data infrastructure for LLM training
- Tesla data pipeline for vision models
- Spotify ML data platform (hybrid recommendation/audio)

QUANTITATIVE ANALYSIS:

- I/O bandwidth requirements by workload type
- Storage cost breakdown (hot/warm/cold tiers)
- Latency requirements for different access patterns

ANTI-PATTERNS TO AVOID:

- Assuming all ML needs feature stores equally
- Ignoring embedding table storage challenges
- Treating checkpoint storage as model-agnostic
- Only discussing LLM training data pipelines

================================================================================
-->

# Storage Systems for ML {#sec-storage}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A layered visualization of ML storage architecture showing data flowing from raw sources to model consumption. The scene depicts a hierarchical storage system: at the base, vast data lakes represented as expansive pools of structured and unstructured data; in the middle layer, feature stores shown as organized crystalline structures with versioned features; at the top, model registries depicted as curated libraries of trained artifacts. Data pipelines flow upward through ETL processes visualized as transformation gates. Visual elements include petabyte-scale metrics, I/O throughput gauges showing streaming rates, and version control branches for datasets and models. Distributed storage nodes span across the background connected by replication streams. The color palette uses deep ocean blues for data lakes, amber for processed features, and silver for model artifacts. Clean architectural diagram style suitable for a data systems textbook._
:::

\noindent
![](images/png/cover_storage.png)

:::

## Purpose {.unnumbered}

_How do storage system architectures shape what machine learning systems can accomplish at production scale?_

Machine learning workloads create distinctive storage demands. Training requires streaming petabytes of data through accelerators at rates that saturate the fastest interconnects, while inference demands millisecond latency access to model weights and feature data across globally distributed serving infrastructure. Storage systems adequate for traditional applications become bottlenecks when confronted with ML access patterns: massive sequential reads during training, random access during feature lookup, and the need to version datasets, models, and artifacts across experimental workflows. The gap between storage capabilities and ML requirements determines training throughput, inference latency, and the feasibility of rapid iteration on model development. Understanding how distributed storage architectures, data lakes, and feature stores address these challenges enables engineers to design systems where storage supports rather than constrains machine learning progress.

::: {.callout-tip title="Learning Objectives"}

- Analyze how the ML storage hierarchy extends the classical memory hierarchy and explain why ML workloads invert traditional storage assumptions about access patterns and working set sizes

- Calculate required storage bandwidth for distributed training using the data pipeline throughput equation and apply it to different model types and cluster configurations

- Compare distributed file system architectures (GFS/Colossus, HDFS, Lustre) and object storage systems (S3, GCS) for different ML workload characteristics

- Design checkpoint strategies using the Young-Daly formula to optimize the tradeoff between checkpoint overhead and recovery time for different model types and cluster sizes

- Evaluate feature store architectures for online and offline serving, understanding point-in-time correctness requirements and why feature stores are critical infrastructure for recommendation systems

- Implement model registry and artifact management systems that ensure reproducibility and lineage tracking across experimental workflows

:::

## Storage Fundamentals for ML {#sec-storage-fundamentals}

The infrastructure foundations established in @sec-infrastructure provide the compute fabric and networking topology for large-scale ML systems. Even the most powerful GPU clusters remain idle without data to process. Storage systems determine whether thousands of accelerators receive training data at the rates they require, whether model checkpoints can be saved before failures corrupt hours of computation, and whether features can be retrieved within the latency budgets that production inference demands. Understanding storage architecture for ML begins with recognizing how ML workloads differ from the applications that shaped traditional storage system design. This chapter examines storage systems through the lens of these ML workload requirements, following data through the ML lifecycle: from storage fundamentals that explain why ML inverts traditional storage assumptions, through training data infrastructure including distributed file systems and object storage at scale, to checkpoint storage systems that address fault tolerance for long-running training jobs. Feature stores bridge training and serving for recommendation and real-time ML systems, while model registries provide the versioning and governance layer that connects training to deployment. Case studies from Google, Meta, Tesla, and Spotify illustrate how these principles manifest in production.

Traditional enterprise storage systems evolved to serve transactional databases and file servers, workloads characterized by small random accesses, strong consistency requirements, and moderate bandwidth demands. A database server might issue thousands of 4KB reads per second to serve user queries, each read potentially touching different storage locations. The storage industry optimized for this pattern, developing sophisticated caching algorithms, RAID[^fn-raid] configurations, and file systems tuned for small-block random access with transactional guarantees.

[^fn-raid]: **RAID (Redundant Array of Independent Disks)**: A storage virtualization technology that combines multiple physical drives into a single logical unit for performance and redundancy. RAID 0 stripes data across drives for speed but no redundancy; RAID 1 mirrors data for redundancy; RAID 5/6 uses parity for fault tolerance with better capacity utilization. ML training workloads typically use RAID 0 or JBOD (Just a Bunch of Disks) for maximum bandwidth, accepting the risk of data loss since training data is immutable and backed up elsewhere.

ML workloads invert nearly every assumption that shaped these systems. Training data access is predominantly sequential, streaming through datasets that may span hundreds of terabytes. Individual accesses are large, often megabytes rather than kilobytes, as models consume batches of images, text sequences, or feature vectors. Consistency requirements are relaxed; slightly stale feature values rarely affect model quality, and training can tolerate occasional data corruption through its inherent noise tolerance. But bandwidth demands are extreme, frequently requiring sustained throughput that would overwhelm systems designed for transactional workloads.

This mismatch between traditional storage design and ML requirements creates challenges that surface at every level of the storage hierarchy. Object stores designed for web-scale applications deliver excellent scalability but introduce latencies that starve GPU pipelines. Parallel file systems engineered for scientific computing provide the bandwidth but struggle with the metadata operations that ML checkpointing generates. Local NVMe[^fn-nvme] drives offer the latency characteristics that inference demands but lack the capacity for training datasets. Effective ML storage architecture requires understanding these tradeoffs and composing storage tiers that match each phase of the ML lifecycle.

[^fn-nvme]: **NVMe (Non-Volatile Memory Express)**: A storage protocol designed specifically for solid-state drives, bypassing the legacy AHCI interface created for spinning disks. NVMe achieves 3-7 GB/s sequential throughput and sub-10 microsecond latency by using multiple command queues (65,535 queues with 65,536 commands each) rather than SATA's single queue of 32 commands. For ML inference, NVMe's low latency enables rapid model loading during cold starts; for training, its high bandwidth supports local caching of datasets fetched from distributed storage.

### The ML Storage Hierarchy {#sec-storage-hierarchy}

Computer architecture courses teach the memory hierarchy as a fundamental organizing abstraction: registers at nanosecond latencies, caches at microseconds, DRAM at hundreds of nanoseconds, and storage devices at milliseconds. This hierarchy exists because faster memory is more expensive per bit, so systems use smaller amounts of fast memory as caches for larger amounts of slower memory. The principle of locality, both temporal (recently accessed data will likely be accessed again) and spatial (nearby data will likely be accessed soon), makes caching effective for most workloads.

ML systems extend this hierarchy with two critical additions: GPU High Bandwidth Memory (HBM)[^fn-hbm] and distributed storage spanning multiple tiers. The extended hierarchy (@tbl-storage-hierarchy) reveals the extreme bandwidth disparities that ML systems must navigate.

[^fn-hbm]: **High Bandwidth Memory (HBM)**: Building on the GPU architecture discussion in @sec-infrastructure, HBM is a 3D-stacked memory technology that places DRAM dies vertically atop the processor using through-silicon vias (TSVs), achieving bandwidth impossible with traditional off-package memory. The H100 GPU uses HBM3 delivering 3.35 TB/s across a 5120-bit interface, compared to 200 GB/s for DDR5. This 17x bandwidth advantage is essential for ML workloads where model weights, activations, and gradients must flow continuously through the processor. The tradeoff is capacity: HBM costs approximately $15/GB versus $3/GB for standard DRAM, limiting GPUs to 80-192 GB while CPUs access terabytes of DDR memory.

| Storage Tier | Typical Capacity | Bandwidth | Latency | Cost ($/GB) |
|--------------|------------------|-----------|---------|-------------|
| GPU HBM | 80 GB | 3.35 TB/s | ~10 ns | ~15.00 |
| Host DRAM | 512 GB - 2 TB | 200 GB/s | ~100 ns | ~3.00 |
| Local NVMe SSD | 4-30 TB | 7-25 GB/s | ~10 μs | ~0.10 |
| Parallel File System | 100+ PB | 1+ TB/s aggregate | ~1 ms | ~0.03 |
| Object Storage | Unlimited | 100 GB/s aggregate | ~50 ms | ~0.02 |
| Archive/Cold Storage | Unlimited | 1 GB/s | Minutes to hours | ~0.004 |

: Extended memory hierarchy for ML systems. Bandwidth figures represent practical throughput; latency represents typical access times. {#tbl-storage-hierarchy}

The bandwidth column in @tbl-storage-hierarchy deserves particular attention. GPU HBM delivers 3.35 TB/s, roughly 17x faster than host DRAM and 130x faster than the fastest local NVMe drives. This disparity creates the central challenge of ML storage systems: keeping accelerators fed with data at rates that prevent them from idling.

::: {.callout-note title="Figure Placeholder: Storage Hierarchy" collapse="true"}
```{.tikz}
% TODO: Pyramid diagram showing HBM -> DRAM -> NVMe -> DFS -> Object Store
\node[draw, align=center] {Storage Hierarchy\nBandwidth vs Capacity};
```
**ML Storage Hierarchy**. A pyramid visualization extending the classic memory hierarchy. The peak shows GPU HBM (3+ TB/s, small capacity), descending through Host DRAM, Local NVMe, and Distributed File Systems, to the base of Object Storage (100+ GB/s aggregate, unlimited capacity). The bandwidth gap between HBM and storage tiers drives the need for sophisticated caching and prefetching.
:::

These bandwidth specifications become concrete when we trace data flow through a single training iteration. Consider what happens when an H100 GPU processes a training batch. At 1,979 TFLOPS[^fn-tflops] of FP16 compute capability, the GPU can perform approximately 2 quadrillion floating-point operations per second. A typical transformer forward pass requires roughly 6 FLOPs per parameter per token. For a 7 billion parameter model processing 2048-token sequences with a batch size of 32, each forward pass involves:

[^fn-tflops]: **TFLOPS (Tera Floating-Point Operations Per Second)**: A measure of computational throughput representing one trillion ($10^{12}$) floating-point operations per second. The H100's 1,979 TFLOPS uses FP16 (16-bit floating-point) precision with sparsity optimization; dense FP16 achieves 990 TFLOPS, and FP32 (32-bit) achieves 67 TFLOPS. This 15x difference between FP16 and FP32 explains why ML training increasingly uses mixed-precision techniques, performing bulk computation in FP16 while maintaining FP32 precision for numerically sensitive operations like loss accumulation.

$$\text{FLOPs per batch} = 6 \times 7 \times 10^9 \times 2048 \times 32 \approx 2.75 \times 10^{15}$$

At 1,979 TFLOPS, this computation completes in approximately 1.4 seconds, during which the next batch must be ready in GPU memory. If data arrives even slightly slower than the GPU consumes it, expensive accelerator time is wasted waiting for storage.

### How ML Workloads Invert Traditional Assumptions {#sec-storage-inverted-assumptions}

Traditional storage system design optimizes for workloads with specific characteristics: random access patterns, working sets that fit in cache, and write-heavy transactional loads. ML workloads systematically violate each of these assumptions (@tbl-storage-assumptions), requiring fundamentally different storage architectures.

**Sequential streaming dominates.** Database workloads exhibit random access patterns as queries retrieve specific records from large tables. ML training, by contrast, performs massive sequential scans through datasets. A training epoch reads every sample once, in whatever order the shuffling algorithm produces, before repeating. This access pattern resembles video streaming more than database queries. Storage systems optimized for random IOPS[^fn-iops] (input/output operations per second) waste their capabilities on ML workloads, while systems optimized for sequential throughput excel.

[^fn-iops]: **IOPS (Input/Output Operations Per Second)**: A storage performance metric counting discrete read or write operations regardless of size. Enterprise SSDs achieve 100,000-1,000,000 IOPS for small (4KB) random accesses but only 10,000-50,000 IOPS for large (1MB) sequential accesses. The distinction matters for ML: training data pipelines care about throughput (GB/s), not IOPS, because they read large batches sequentially. Feature stores serving inference require high IOPS for millions of small random lookups. Storage vendors often advertise IOPS prominently, but ML engineers should evaluate throughput for training and latency for inference.

**Working sets exceed any cache level.** Traditional applications exhibit locality: a web server repeatedly accesses the same popular pages, a database repeatedly queries hot rows. Caching exploits this locality, keeping frequently accessed data in fast memory. ML training datasets are accessed uniformly: each sample is read once per epoch, with no sample more likely to be accessed than any other during training. A 10 TB image dataset cannot be cached in DRAM; each sample is effectively cold when accessed. This lack of locality renders traditional caching strategies ineffective for training data.

**Write patterns are bursty rather than continuous.** Transactional systems generate continuous streams of small writes as users update records. ML systems generate occasional massive writes when saving checkpoints. A 175 billion parameter model checkpoint [@brown2020gpt3] occupies approximately 700 GB; saving it every 10 minutes generates 70 GB/minute average throughput but concentrated into bursts that saturate storage bandwidth for 1-2 minutes followed by idle periods. This bursty pattern requires storage systems that can absorb high-bandwidth writes without blocking ongoing reads.

**Read/write ratios vary dramatically by phase.** Training reads vastly exceed writes: a typical training run reads the dataset dozens of times (one per epoch) while writing only periodic checkpoints. The read-to-write ratio can exceed 100:1. Inference, conversely, is almost entirely read-only, loading model weights once and then serving requests without writes. Feature stores for recommendation systems present yet another pattern: continuous reads for serving interleaved with batch writes from offline feature computation. No single storage configuration optimizes all three patterns.

| Workload Pattern | Traditional Assumption | ML Reality |
|------------------|------------------------|------------|
| Access pattern | Random access | Sequential streaming |
| Working set | Fits in cache | Exceeds all cache levels |
| Write pattern | Continuous small writes | Bursty large writes |
| Read/write ratio | Balanced | Phase-dependent (100:1 to 1:0) |
| Locality | Strong temporal locality | No locality (uniform sampling) |

: Contrast between traditional storage assumptions and ML workload characteristics. {#tbl-storage-assumptions}

### Access Pattern Analysis {#sec-storage-access-patterns}

Understanding access patterns (@tbl-access-patterns) quantitatively enables storage system selection and capacity planning. ML workloads exhibit distinct patterns across different phases and model types.

::: {.callout-note title="Figure Placeholder: Access Patterns" collapse="true"}
```{.tikz}
% TODO: Plots of I/O over time for Training (streaming), Checkpoint (burst), Serving (random)
\node[draw, align=center] {Access Patterns\nStreaming vs Bursty vs Random};
```
**Workload Access Patterns**. Visual comparison of I/O behavior across ML phases. Training data access appears as massive sequential streams with global shuffling. Checkpoint access shows infrequent, intense write bursts. Feature store access demonstrates continuous, high-frequency random reads. Understanding these distinct signatures is essential for selecting appropriate storage backends.
:::

**Training data access** follows a streaming sequential pattern with random shuffling. Each epoch reads the entire dataset once, but the order is randomized to prevent the model from learning ordering artifacts. This creates a pattern that is globally sequential (every sample accessed once) but locally random (no predictable next sample). Storage systems must support high sequential bandwidth while handling the pseudo-random access order that shuffling creates.

For distributed training, access patterns multiply: $N$ workers each read $1/N$ of the dataset per step, but the global shuffle means no locality between workers. If workers access a shared storage system, the aggregate access pattern appears random even though each worker performs sequential reads of its partition.

**Checkpoint access** exhibits extreme write bursts followed by long read intervals. During normal training, checkpoints represent nearly all write traffic. The access pattern is:

- Write: Save complete model state every $T_{checkpoint}$ minutes
- Read: Load most recent checkpoint on training restart
- Delete: Remove old checkpoints after new ones are verified

The checkpoint write burst must complete before the next training step can safely proceed (for synchronous checkpointing) or within a bounded delay (for asynchronous approaches). Checkpoint reads are infrequent but critical: when failures occur, recovery time depends on checkpoint load bandwidth.

**Feature store access** patterns differ fundamentally from training data. Online serving requires point lookups: given a user ID, retrieve that user's features. This is the random access pattern that traditional storage optimizes for, with latency requirements in single-digit milliseconds. Offline feature computation generates batch writes as feature pipelines process new data. The pattern resembles:

- Online reads: Millions of random point lookups per second, < 10ms latency requirement
- Offline writes: Batch updates every minutes to hours, throughput-optimized

**Model serving access** loads model weights at startup, then serves inference requests using weights cached in GPU memory. The access pattern is write-once-read-many with extreme read amplification: a 7 billion parameter model loaded once serves millions of inference requests. Storage bandwidth matters only during model loading; once loaded, storage is unused until model updates or server restarts.

| Model Type | Training Data Pattern | Checkpoint Pattern | Feature Pattern | Serving Pattern |
|------------|----------------------|-------------------|-----------------|-----------------|
| LLM | Sequential streaming, TB-scale | Infrequent, 100GB-1TB bursts | Minimal | Load once, cache in GPU |
| RecSys | Log streaming, continuous | Incremental embedding updates | Continuous random lookups | Hot embeddings in memory |
| Vision | Sequential with augmentation | Regular, 1-10GB | Minimal | Load once per model |
| Scientific | Irregular, domain-specific | Regular or continuous | Domain-specific | Varies by application |

: Access patterns vary significantly by model type and system phase. {#tbl-access-patterns}

### Data Pipeline Throughput Requirements {#sec-storage-throughput-requirements}

The central quantitative question for ML storage is: what bandwidth does training require? The answer depends on cluster size, accelerator utilization targets, sample sizes, and iteration speed.

::: {.callout-definition title="Data Pipeline Throughput Equation"}

The required storage bandwidth to sustain distributed training is:

$$B_{required} = N_{GPUs} \times U_{target} \times \frac{S_{batch}}{T_{iteration}}$$

where $N_{GPUs}$ is the number of accelerators, $U_{target}$ is the target utilization (typically 0.8-0.95), $S_{batch}$ is the batch size in bytes per GPU, and $T_{iteration}$ is the iteration time in seconds.

:::

This equation reveals that storage bandwidth requirements grow linearly with cluster size. Doubling the number of GPUs doubles the required storage bandwidth, assuming iteration time and batch size remain constant.

::: {.callout-example title="ImageNet Training Bandwidth Requirements"}

Consider training a ResNet-50 [@he2016resnet] model on ImageNet [@deng2009imagenet] using 256 H100 GPUs. The calculation proceeds as follows:

**Given values:**

- $N_{GPUs} = 256$ H100 GPUs
- $U_{target} = 0.80$ (80% utilization)
- Images are 224×224 RGB after resize, but stored as JPEG at ~150 KB average
- Batch size per GPU: 256 images
- Target iteration time: 200 ms (achievable with optimized training)

**Batch size calculation:**

$$S_{batch} = 256 \text{ images} \times 150 \text{ KB/image} = 38.4 \text{ MB}$$

**Required bandwidth:**

$$B_{required} = 256 \times 0.80 \times \frac{38.4 \text{ MB}}{0.2 \text{ s}} = 39.3 \text{ GB/s}$$

This 39.3 GB/s requirement exceeds single-node NVMe capabilities (typically 7-25 GB/s) and requires either distributed data loading across multiple nodes or a parallel file system delivering aggregate bandwidth at this scale.

:::

The bandwidth requirement varies dramatically by model type due to differences in sample size and iteration time.

::: {.callout-example title="LLM Training Bandwidth Requirements"}

For LLM training, samples are tokenized text sequences rather than images, and batch sizes are constrained by GPU memory for activation storage [@brown2020gpt3].

**GPT-3 scale training scenario:**

- $N_{GPUs} = 1024$ A100 GPUs (128 nodes × 8 GPUs)
- $U_{target} = 0.85$
- Sequence length: 2048 tokens
- Batch size per GPU: 8 sequences (memory limited)
- Token storage: 2 bytes per token (int16)
- Target iteration time: 1.5 seconds

**Batch size calculation:**

$$S_{batch} = 8 \text{ sequences} \times 2048 \text{ tokens} \times 2 \text{ bytes} = 32.8 \text{ KB}$$

**Required bandwidth:**

$$B_{required} = 1024 \times 0.85 \times \frac{32.8 \text{ KB}}{1.5 \text{ s}} = 19.0 \text{ MB/s}$$

This dramatically lower bandwidth requirement (19 MB/s vs. 39 GB/s for ImageNet) explains why LLM training is typically compute-bound rather than I/O-bound. The bottleneck shifts to checkpoint I/O rather than training data streaming.

:::

::: {.callout-example title="Recommendation System Training Bandwidth"}

Recommendation systems present unique challenges with massive embedding tables and sparse feature access [@naumov2019dlrm].

**Large-scale RecSys scenario (similar to Meta DLRM):**

- $N_{GPUs} = 512$ GPUs
- $U_{target} = 0.75$ (lower due to embedding communication)
- Samples: User interaction logs with ~100 features each
- Feature encoding: 8 bytes per feature (int64 IDs)
- Batch size per GPU: 65,536 samples (large batches for sparse models)
- Target iteration time: 100 ms

**Batch size calculation:**

$$S_{batch} = 65536 \text{ samples} \times 100 \text{ features} \times 8 \text{ bytes} = 52.4 \text{ MB}$$

**Required bandwidth:**

$$B_{required} = 512 \times 0.75 \times \frac{52.4 \text{ MB}}{0.1 \text{ s}} = 201.3 \text{ GB/s}$$

This 201 GB/s requirement exceeds even large parallel file systems. Recommendation systems typically address this through data locality: each worker processes a partition of the data stored on local SSDs, eliminating cross-node data transfer. The embedding table accesses, which are random lookups into trillion-parameter tables, become the true storage bottleneck.

:::

### Consistency Models for ML Storage {#sec-storage-consistency}

The CAP theorem, which @sec-vol2-introduction introduced in the context of distributed system tradeoffs, directly shapes storage architecture decisions. As established there, during network partitions a distributed system must choose between consistency (all readers see the same data) and availability (requests succeed even during failures). The formal statement[^fn-cap-theorem] [@brewer2000towards; @gilbert2002brewer] is that distributed systems can provide at most two of these three guarantees: consistency, availability, and partition tolerance.

[^fn-cap-theorem]: **CAP Theorem**: Proven by Eric Brewer in 2000 and formalized by Gilbert and Lynch in 2002, the CAP theorem states that during a network partition, a distributed system must choose between consistency (all nodes see the same data) and availability (every request receives a response). Since partitions are inevitable in distributed systems, the practical choice is between CP (consistent but may reject requests during partitions) and AP (available but may return stale data). For ML storage, training data and checkpoints favor CP (correctness over availability), while feature stores often accept AP (fresh-enough features are better than failed requests).

Understanding consistency models enables correct storage system selection for different ML components.

::: {.callout-definition title="Consistency Model Definitions"}

**Linearizability (Strong Consistency)**: Every operation appears to take effect instantaneously at some point between its invocation and response. All clients observe operations in the same order, and that order respects real-time ordering. Checkpoint storage requires linearizability because:

1. After checkpoint write completes, any reader must see complete checkpoint
2. No reader should ever see a partial or corrupted state
3. Recovery process must read exactly what was written

Linearizable systems are expensive: they require coordination (consensus protocols like Paxos or Raft) on every write. This is acceptable for checkpoints (written every 10-30 minutes) but unacceptable for training data reads (millions per second).

**Sequential Consistency**: Operations appear in the same order to all clients, but that order may not respect real-time. Sufficient for training data where "everyone sees the same dataset version" is required but "immediately after upload" is not.

**Read-Your-Writes Consistency**: After a client writes data, subsequent reads by that same client will see the new value. Required for online feature stores where user actions update features that affect subsequent recommendations.

**Eventual Consistency**: After a period of no writes, all replicas converge to the same value. Acceptable for training data that is uploaded once and read many times, or for offline feature computation where batch updates propagate over minutes.

The key insight: stronger consistency requires more coordination, reducing throughput and increasing latency. Match consistency level to workload requirements.

:::

For ML systems, the appropriate consistency model depends on the storage tier and access pattern.

**Training data storage** can sacrifice strong consistency for availability. Training samples are immutable once created: the same image or text sequence is read identically by all workers. Eventual consistency suffices because training order does not affect final model quality (samples are shuffled anyway), stale reads of a dataset do not corrupt training (reading an older version of a sample causes no harm), and dataset updates are infrequent (new data is added between training runs, not during).

Object storage systems like S3 and GCS are suitable for training data. Since December 2020, AWS S3 provides strong read-after-write consistency for all operations [@aws2020s3], eliminating previous eventual consistency concerns. GCS has always provided strong consistency. Both systems prioritize availability and can serve training data reliably at scale.

**Checkpoint storage** requires strong consistency for correctness. A checkpoint must be complete and consistent before training can safely continue: a partially written checkpoint that appears complete causes catastrophic failure on recovery. The checkpoint must be either fully written or not visible at all (atomic writes), immediately readable after completion (read-after-write consistency), and must survive storage system failures once acknowledged (durable writes). These requirements favor strongly consistent storage systems or careful application-level protocols that implement atomic writes atop eventually consistent storage.

**Feature store storage** requires consistency guarantees that vary by access pattern. Offline features can tolerate eventual consistency: batch computations produce new feature versions that propagate to serving over minutes to hours. Online features for serving require read-your-writes consistency at minimum: after a user action updates their features, subsequent requests must see those updated features. Strong consistency may be necessary for use cases like fraud detection where stale features could enable fraudulent transactions.

| Storage Tier | Consistency Requirement | Rationale | Suitable Systems |
|--------------|------------------------|-----------|------------------|
| Training data | Strong consistency (now standard) | Immutable data, modern object stores provide strong consistency | S3, GCS, HDFS |
| Checkpoints | Strong consistency | Partial checkpoints cause catastrophic failure | Parallel FS with atomic writes |
| Offline features | Eventual consistency | Batch updates, staleness acceptable | Data warehouses, object storage |
| Online features | Read-your-writes or stronger | User experience requires fresh features | Redis, Bigtable, DynamoDB |
| Model weights | Read-after-write | Model updates must be immediately visible | Consistent object storage |

: Consistency requirements vary by storage tier and access pattern. {#tbl-consistency-requirements}

::: {.callout-note title="CAP Theorem Implications for ML Storage"}

The CAP theorem's implications for ML storage differ from traditional applications. As Stoica et al. observe [@stoica2017berkeley], training storage can sacrifice availability for consistency (a brief storage outage during checkpoint writes is acceptable if it ensures checkpoint correctness), while serving storage might sacrifice consistency for availability (serving stale features is preferable to failing requests entirely). Understanding these tradeoffs enables storage architecture decisions that match the actual requirements of each ML system component.

:::

Consistency guarantees determine what data readers see. Equally important is when they see it. This brings us to latency, and specifically the challenge of tail latency in distributed systems.

### Tail Latency in Distributed Storage {#sec-tail-latency}

At scale, tail latency[^fn-tail-latency] dominates storage system behavior. The median access latency tells an incomplete story; what matters for ML systems is the 99th or 99.9th percentile latency (P99, P999). Dean and Barroso's seminal paper "The Tail at Scale" [@dean2013tail] formalized why: when a system makes parallel requests to many storage nodes, the overall latency is determined by the slowest response.

[^fn-tail-latency]: **Tail Latency**: The latency experienced by the slowest requests, typically measured at the 99th (P99) or 99.9th (P999) percentile. In distributed systems, tail latencies are disproportionately important because a single slow component delays the entire operation. If a training step requires data from 100 storage nodes and each has a 1% chance of being slow, 63% of steps will experience at least one slow access. Google's production systems target P99 latency rather than median, recognizing that users experience the tail while operators often only monitor the median.

**Why tail latency matters for ML:**

Consider a distributed training step that reads data from 100 storage nodes simultaneously. If each node's latency distribution has a 1% chance of being "slow" (say, 100ms instead of the typical 10ms), then:

$$P(\text{at least one slow}) = 1 - (0.99)^{100} = 0.634$$

More than 63% of training steps will experience at least one slow storage access, making the tail latency the effective latency for the system.

**Tail latency sources** include garbage collection pauses (JVM-based storage systems like HDFS NameNode can stall for seconds during GC), disk queue depth (when too many concurrent requests hit one disk, queue wait time dominates), network congestion (shared network fabric experiences transient congestion), background maintenance (compaction, replication, and verification compete with foreground requests), and resource contention (multiple tenants sharing storage create interference).

**Mitigation strategies** include hedged requests (issue redundant reads, take first response; tradeoff: 2x read amplification), backup requests (issue second request if first is slow; tradeoff: lower amplification, higher complexity), selective replica choice (route to least-loaded replica; tradeoff: requires load monitoring), request cancellation (cancel in-flight requests when first completes; reduces wasted work), and deadline propagation (drop requests exceeding deadline; prioritizes freshness).

**Production example:**

Google's storage systems implement hedged requests: after waiting for the P50 latency (e.g., 10ms), issue a backup request to a different replica. Take whichever response arrives first. This reduces P99 latency dramatically at the cost of increased read traffic:

$$\text{P99 with hedging} \approx P50 + P50 \times \epsilon$$

where $\epsilon$ accounts for the time to detect slowness and issue the backup.

For ML training, tail latency tolerance is higher (seconds of delay are acceptable occasionally) than for inference (where P99 latency directly impacts user experience). Checkpoint writes, being infrequent, can tolerate higher tail latency than feature store lookups.

### Storage System Selection Framework {#sec-storage-selection}

Given the diversity of ML storage requirements, practitioners need a systematic framework for selecting appropriate storage systems. The decision depends on access pattern, scale, latency requirements, and cost constraints.

For training data exceeding local storage, systems under 10 TB can use local NVMe RAID or network-attached storage; systems between 10 TB and 1 PB benefit from parallel file systems (Lustre, GPFS) or high-performance object storage; systems exceeding 1 PB typically require object storage (S3, GCS) with intelligent caching. For checkpoint storage, single nodes can use local NVMe with backup to durable storage; multi-node deployments with models under 100 GB need parallel file systems with atomic write support; models exceeding 100 GB require distributed checkpointing across multiple storage targets. For feature stores, offline-only workloads can use data warehouses (Snowflake, BigQuery) or data lakes; online serving with sub-10ms requirements needs in-memory cache (Redis) backed by persistent storage; online serving at scale requires purpose-built feature stores (Feast, Tecton, Vertex Feature Store).

The selection framework above maps requirements to storage tiers. We now examine each tier in depth, following data through the ML lifecycle: first the training data that feeds models, then the checkpoints that preserve training progress, then the features required for serving, and finally the model artifacts themselves.

## Training Data Infrastructure {#sec-training-data-infrastructure}

Training datasets for production ML systems range from terabytes to petabytes, far exceeding single-machine storage capacity. Storing and serving this data requires distributed storage systems designed for high throughput sequential access. This section examines the distributed file systems and object storage architectures that power large-scale ML training, along with the data formats and pipeline architectures that efficiently deliver data to accelerators.

### Distributed File Systems {#sec-distributed-file-systems}

Distributed file systems provide a POSIX[^fn-posix]-compatible interface to storage spread across hundreds or thousands of machines. This familiar file system interface (open, read, write, close) enables existing software to access distributed storage without modification, a significant advantage for ML frameworks designed around local file access patterns.

[^fn-posix]: **POSIX (Portable Operating System Interface)**: A family of standards specifying the API between applications and operating systems, including file operations (open, read, write, close, seek). POSIX file semantics guarantee strong consistency: reads return the most recently written data, and writes to the same region are serialized. Distributed file systems that provide POSIX semantics (HDFS, Lustre) are easier to adopt but incur coordination overhead to maintain consistency. Object stores like S3 deliberately abandon POSIX semantics, offering simpler APIs with weaker guarantees but better scalability.

#### Google File System and Colossus {#sec-gfs-colossus}

The Google File System (GFS) [@ghemawat2003google], published in 2003, established the architectural template for large-scale distributed storage. Its design decisions, optimized for Google's web crawling and indexing workloads, proved remarkably well suited for ML training data.

GFS makes several unconventional design choices:

**Large block sizes.** Where traditional file systems use 4 KB blocks, GFS uses 64 MB chunks. This reduces metadata overhead: a 1 PB dataset contains only 16 million chunks rather than 256 billion 4 KB blocks. For ML training data, which is read sequentially in large batches, large blocks eliminate seek overhead and maximize throughput.

**Single master, multiple chunkservers.** A single master maintains all file system metadata (namespace, chunk locations, access permissions) while data flows directly between clients and chunkservers. This architecture simplifies consistency guarantees: the master is the single source of truth for metadata. For training data, which is written once and read many times, this works well.

**Relaxed consistency model.** GFS provides weak consistency guarantees: concurrent writers may produce undefined file regions, and readers may see stale data during failures. These limitations are acceptable for training data (immutable after creation) but problematic for checkpoints (requiring atomicity).

**Colossus**, Google's successor to GFS deployed around 2010, addresses GFS limitations while maintaining its core design. Key improvements include:

- Distributed metadata: The single master bottleneck is eliminated by sharding metadata across multiple servers
- Smaller block sizes: 1 MB blocks improve space efficiency for smaller files
- Erasure coding[^fn-erasure-coding]: Reduces storage overhead from 3x replication to ~1.5x while maintaining durability
- Reed-Solomon encoding [@reed1960polynomial]: Enables efficient reconstruction of failed blocks

[^fn-erasure-coding]: **Erasure Coding**: A data protection technique that breaks data into fragments, adds redundant parity fragments, and distributes them across storage nodes such that the original data can be reconstructed from any subset of fragments. A common configuration is RS(6,3): 6 data fragments plus 3 parity fragments, tolerating any 3 failures with only 1.5x storage overhead (versus 3x for triple replication). The trade-off is computational cost: reconstruction requires Galois field arithmetic that consumes CPU cycles. For ML training data that is read frequently but rarely reconstructed, erasure coding dramatically reduces storage costs at petabyte scale.

For ML workloads, Colossus delivers aggregate read bandwidth exceeding 1 TB/s across Google's infrastructure, sufficient to feed thousands of TPU chips simultaneously.

#### Hadoop Distributed File System {#sec-hdfs}

HDFS[^fn-hdfs] [@shvachko2010hadoop], the open-source implementation inspired by GFS, became the storage foundation for the Hadoop ecosystem and remains widely deployed for ML training data. Its architecture mirrors GFS:

- **NameNode**: Single master maintaining file system namespace and block locations
- **DataNodes**: Workers storing actual data blocks
- **Block replication**: Default 3x replication for durability

[^fn-hdfs]: **Hadoop Distributed File System (HDFS)**: Developed at Yahoo in 2006 as an open-source GFS clone, HDFS became the dominant distributed file system for big data workloads. Its design assumes commodity hardware failures are common and optimizes for batch processing with large files. HDFS stores files as sequences of 128 MB blocks (configurable), each replicated across three DataNodes. While effective for MapReduce workloads, HDFS's single-NameNode architecture limits metadata operations to roughly 10,000 per second, which becomes a bottleneck when ML training involves millions of small files or frequent checkpoint metadata updates.

HDFS optimizes for the same access patterns as GFS: large sequential reads and writes, with files written once and read many times. A typical HDFS deployment achieves 100-200 MB/s per DataNode, scaling to aggregate cluster throughput of 10-100 GB/s depending on cluster size.

**HDFS limitations for modern ML** include the single NameNode bottleneck (file system operations serialize through one server, limiting metadata operations to roughly 10,000 per second), JVM overhead (Java implementation adds latency and memory overhead compared to native implementations), and the small file problem (each file consumes NameNode memory regardless of size, so millions of small files exhaust metadata capacity).

#### The Small File Problem: IOPS vs Bandwidth

A critical limitation of HDFS (and similar distributed file systems) is the separation of metadata and data. While aggregate **bandwidth** scales linearly with the number of DataNodes, metadata throughput (**IOPS**) is often bottlenecked by the single NameNode.

Consider a 1 TB dataset:
*   **Scenario A**: Stored as 1,000 files of 1 GB each. Reading the dataset requires 1,000 metadata RPCs to the NameNode.
*   **Scenario B**: Stored as 100 million files of 10 KB each (e.g., individual small images). Reading the dataset requires 100 million metadata RPCs.

In Scenario B, the NameNode will be overwhelmed by metadata requests long before the DataNodes saturate their network links. The training job will spend most of its time waiting for `open()` calls to complete rather than reading data. This structural bottleneck drives the necessity for aggregation formats like **TFRecord** or **Parquet**, which bundle thousands of small samples into a single large file, amortizing one metadata operation over many data reads.

These limitations become acute for ML workloads with many small files (e.g., individual images) or high-frequency metadata operations (e.g., checkpoint writes). HDFS works well for datasets stored as large sequential files (TFRecord, Parquet) but struggles with directory structures containing millions of individual samples.

#### Lustre and Parallel File Systems {#sec-lustre}

Lustre[^fn-lustre] [@schwan2003lustre], developed for high-performance computing (HPC), takes a different architectural approach optimized for parallel access from thousands of compute nodes simultaneously. Where GFS and HDFS prioritize simplicity and fault tolerance, Lustre prioritizes raw throughput.

[^fn-lustre]: **Lustre**: A parallel distributed file system developed by Cluster File Systems starting in 1999 and now maintained as open source. The name combines "Linux" and "cluster." Lustre powers seven of the top ten supercomputers globally and is the dominant storage solution for scientific computing. Its design prioritizes aggregate bandwidth over fault tolerance: Lustre achieves 1+ TB/s throughput by striping files across hundreds of storage servers, achieving performance characteristics summarized in @tbl-lustre-performance, but requires careful administration and can be less forgiving of hardware failures than cloud-native solutions. For ML training on HPC clusters, Lustre's bandwidth often exceeds cloud object storage by 10-100x.

**Lustre architecture** separates metadata from data more aggressively. Metadata Servers (MDS) handle file system namespace operations (create, open, stat), Object Storage Servers (OSS) store actual file data, and Object Storage Targets (OST) represent individual storage devices attached to OSSs.

Files are striped across multiple OSTs, enabling parallel access that aggregates bandwidth from many storage devices. A large file might be striped across 100 OSTs; reading the file in parallel achieves 100x the bandwidth of a single OST.

**Lustre performance characteristics**:

| Configuration | Aggregate Bandwidth | Typical Use |
|---------------|---------------------|-------------|
| Small cluster (10 OSTs) | 10-20 GB/s | Research lab ML training |
| Medium cluster (100 OSTs) | 100-200 GB/s | Production ML training |
| Large cluster (1000+ OSTs) | 1+ TB/s | Exascale HPC, large LLM training |

: Lustre scales linearly with storage server count. {#tbl-lustre-performance}

For ML training, Lustre excels when training data is stored as large files that can be striped across many OSTs, multiple training jobs read the same data concurrently (shared datasets), or checkpoint writes require high bandwidth to minimize training interruption.

Lustre's disadvantage is operational complexity: tuning stripe sizes, managing quota, and handling metadata server failures requires specialized expertise that cloud-native teams may lack.

#### Comparative Analysis {#sec-dfs-comparison}

The choice between distributed file systems depends on workload characteristics, operational expertise, and existing infrastructure as compared in @tbl-dfs-comparison.

| System | Strengths | Weaknesses | Best For |
|--------|-----------|------------|----------|
| GFS/Colossus | Massive scale, Google ecosystem integration | Proprietary to Google | Google Cloud ML |
| HDFS | Open source, Hadoop ecosystem | Single NameNode, JVM overhead | Spark-based data processing |
| Lustre | Raw throughput, HPC optimized | Operational complexity | On-premise HPC clusters |
| GPFS/Spectrum Scale [@schmuck2002gpfs] | Enterprise features, mixed workloads | Cost, complexity | Large enterprise ML |
| BeeGFS | Ease of deployment, good performance | Smaller community | Academic/research clusters |

: Distributed file system comparison for ML workloads. {#tbl-dfs-comparison}

The architectural principles pioneered by GFS found their way into the broader ecosystem through open-source implementations like HDFS and parallel file systems like Lustre. Each system makes different tradeoffs between throughput, operational complexity, and integration with existing infrastructure. For many organizations, however, the operational burden of running distributed file systems has driven adoption of a simpler alternative: object storage.

### Object Storage at Scale {#sec-object-storage}

Object storage provides a simpler abstraction than file systems: objects are stored and retrieved by key, without directories, hierarchies, or POSIX semantics. This simplicity enables massive scale, high durability, and low cost.

#### Object Storage Architecture {#sec-object-storage-architecture}

Object stores organize data as flat namespaces of key-value pairs. An object has:

- **Key**: A unique identifier (often resembling a file path: `training-data/imagenet/images/n01440764/n01440764_10026.JPEG`)
- **Value**: The object data (arbitrary bytes, typically KB to GB in size)
- **Metadata**: User-defined key-value pairs (content type, creation time, checksums)

This design eliminates the metadata bottlenecks that limit distributed file systems. Without directories to traverse or hierarchies to maintain, object stores scale to billions of objects without architectural changes.

**Durability through redundancy.** Object stores achieve extreme durability[^fn-eleven-nines] (S3 advertises 99.999999999% or "eleven nines") through erasure coding (data is split into fragments with redundant parity fragments, enabling reconstruction if any fragment is lost), geographic distribution (fragments are spread across multiple availability zones or regions), and continuous verification (background processes detect and repair bit rot).

[^fn-eleven-nines]: **Eleven Nines Durability (99.999999999%)**: This seemingly abstract number has concrete meaning: for one million objects stored for 10 million years, you would statistically expect to lose one object. Achieving this requires geographic replication, erasure coding, continuous integrity verification, and automated repair of detected corruption. For ML training data representing months of curation effort, this level of durability justifies the higher latency of object storage compared to local disks. The durability guarantee applies to stored data, not availability: objects are durably stored but may be temporarily inaccessible during availability zone outages.

Durability guarantees that data survives once written. But when does written data become visible to readers? This is the domain of consistency guarantees, which have evolved significantly in recent years.

**Consistency model evolution.** Historically, object stores provided eventual consistency: after writing an object, some readers might not immediately see the new version. This changed significantly in December 2020 when AWS S3 upgraded to strong read-after-write consistency for all operations at no additional cost [@aws2020s3]. GCS has always provided strong consistency. This evolution simplifies ML storage architecture: checkpoints can now rely on object stores providing immediate visibility after successful writes, though application-level verification remains prudent for critical data.

#### Amazon S3 and Google Cloud Storage {#sec-s3-gcs}

S3, launched in 2006, pioneered the object storage model and remains the dominant cloud storage service. GCS provides similar capabilities with Google Cloud integration.

**S3 performance characteristics** include single object throughput of 100 MB/s per connection, aggregate throughput that scales with parallel connections (hundreds of GB/s possible), latency of 50-100 ms for first byte, and request rate of 5,500 GET/s and 3,500 PUT/s per prefix.

The request rate limit is significant for ML: a dataset organized as `s3://bucket/class_name/image.jpg` limits each class to 5,500 reads per second. Sharding data across random prefixes (`s3://bucket/shard_id/sample.dat`) avoids this bottleneck.

**GCS performance** is similar, providing single object throughput of 100+ MB/s per connection, strong consistency (both GCS and S3 since December 2020 provide read-after-write consistency), and composite objects (multiple objects can be composed into one without re-upload).

Practical throughput depends heavily on access patterns as detailed in @tbl-object-storage-throughput:

| Access Pattern | Typical Throughput | Optimization |
|----------------|-------------------|--------------|
| Sequential single object | 100 MB/s | Use large objects (100+ MB) |
| Parallel multiple objects | 10-100 GB/s | Use multiple connections, random prefixes |
| Small object reads | Limited by request rate | Batch into larger objects |
| Listing operations | 1000 objects/s | Use flat namespaces, avoid deep hierarchies |

: Object storage throughput varies dramatically by access pattern. {#tbl-object-storage-throughput}

#### Object Storage for ML Training {#sec-object-storage-ml}

Using object storage for training data requires adapting to its characteristics:

**Large file aggregation.** Rather than storing individual images as objects, aggregate samples into large files (TFRecord, WebDataset, Parquet). This converts thousands of small object reads into a few large sequential reads, dramatically improving throughput.

**Parallel data loading.** Training frameworks should open multiple parallel connections to object storage. PyTorch DataLoader with `num_workers > 1` or TensorFlow's `tf.data` with `num_parallel_calls` enables this parallelism.

**Prefetching and buffering.** Object storage latency (50-100 ms) would be catastrophic if each batch waited for storage. Data pipelines must prefetch many batches ahead, overlapping storage access with GPU computation.

**Local caching.** For datasets accessed repeatedly across training runs, caching on local NVMe reduces object storage costs and improves performance. The first training run populates the cache; subsequent runs read locally.

#### Data Egress and Cost

In cloud environments, physical distance equals money. Cloud providers typically charge for data crossing availability zone (AZ) or regional boundaries.

*   **Intra-Zone**: Free and high bandwidth.
*   **Inter-Zone**: Charged (e.g., $0.01/GB) and higher latency.
*   **Inter-Region**: Expensive (e.g., $0.02-$0.05/GB) and high latency.

A common anti-pattern is storing a petabyte dataset in a regional bucket and training on a cluster pinned to a specific zone. Every training epoch reads the full petabyte across the zonal boundary, incurring massive "cross-zone networking" fees that can exceed the cost of the GPUs themselves. **Strict data locality**—ensuring the training bucket and compute cluster reside in the exact same zone—is an essential cost control mechanism.

### Data Format Selection {#sec-data-formats}

The choice of data format significantly impacts training throughput, storage cost, and pipeline complexity. Modern ML workloads have converged on a few formats optimized for different access patterns.

#### TFRecord {#sec-tfrecord}

TFRecord, TensorFlow's native format, stores data as sequential records in binary files. Each record contains a serialized protocol buffer with typed fields.

**Structure:**

```text
[length][crc32 of length][data][crc32 of data]
```

**Advantages** include sequential read optimization (records are stored contiguously, enabling high-throughput streaming), schema flexibility (protocol buffers support arbitrary nested structures), compression (GZIP or ZSTD compression can be applied transparently), and splitting (large datasets can be sharded across multiple TFRecord files).

**Limitations** include random access (accessing a specific record requires scanning from the beginning), TensorFlow coupling (while readable from other frameworks, optimized for TensorFlow), and no indexing (cannot query or filter without reading entire file).

For large-scale training that processes entire datasets sequentially, TFRecord achieves near-optimal throughput. A well-configured TFRecord pipeline can saturate 10+ GB/s NVMe storage.

#### Apache Parquet {#sec-parquet}

Parquet[^fn-parquet], developed for the Hadoop ecosystem and inspired by Google's Dremel [@melnik2010dremel], uses columnar storage that stores all values of a column together rather than all columns of a row together.

[^fn-parquet]: **Apache Parquet**: An open-source columnar storage format created in 2013 by Twitter and Cloudera. The name references the geometric floor pattern where pieces fit together efficiently. Parquet stores data column-by-column rather than row-by-row, enabling efficient compression (similar values compress well) and selective column reads (skip columns not needed for analysis). For ML feature datasets with hundreds of columns where training uses only a subset, Parquet can reduce I/O by 10x compared to row-oriented formats. The trade-off is write amplification: updating a single value requires rewriting an entire row group (typically 128 MB).

**Columnar advantages for ML** include column pruning (read only the columns needed, such as skipping metadata and reading only pixels), compression efficiency (similar values in a column compress better than mixed values in a row), and predicate pushdown (filter data without reading irrelevant rows).

**Parquet structure:**

```text
Row Group 1
  Column A chunk (values for rows 0-N)
  Column B chunk
  ...
Row Group 2
  ...
Footer (schema, statistics, locations)
```

The footer contains statistics (min/max values) for each column chunk, enabling queries to skip row groups that cannot match filter predicates.

**Parquet for ML** works well when datasets have many columns but training uses few (feature selection), filtering is needed (such as training only on samples meeting criteria), or data is shared with analytics tools (Spark, pandas, DuckDB).

**Limitations** include write amplification (updating a single value requires rewriting the row group) and lack of optimization for image or binary data (columnar layout provides little benefit).

#### WebDataset {#sec-webdataset}

WebDataset [@aizman2019webdataset] stores samples as TAR archives, with each sample's components (image, label, metadata) as separate files within the archive.

**Structure:**

```text
sample0001.jpg
sample0001.cls
sample0001.json
sample0002.jpg
sample0002.cls
...
```

**WebDataset advantages** include HTTP compatibility (TAR files can be streamed directly from web servers or object storage), simple format (standard UNIX tools can inspect and manipulate archives), shuffling (shuffle buffers can be applied during streaming), and no framework dependency (works with PyTorch, TensorFlow, JAX).

**For distributed training**, WebDataset enables efficient data loading from object storage. Each worker is assigned different TAR shards, workers stream shards in parallel with no coordination needed, and local shuffle buffers randomize sample order within each worker.

**Throughput:** WebDataset achieves 1-5 GB/s per worker from object storage, scaling linearly with worker count.

#### Format Selection Guidelines {#sec-format-selection}

| Model Type | Recommended Format | Rationale |
|------------|-------------------|-----------|
| LLM | TFRecord or custom binary | Token sequences are fixed-length arrays, columnar offers no benefit |
| Vision | WebDataset or TFRecord | Large binary blobs (images), sequential access pattern |
| RecSys | Parquet | Many sparse features, column pruning valuable |
| Scientific | HDF5 or domain-specific | Multi-dimensional arrays, random access sometimes needed |
| Multimodal | WebDataset | Different modalities (image, text, audio) naturally grouped |

: Format selection guidelines matched to model type and access characteristics. {#tbl-format-selection}

### Data Loading Pipelines {#sec-data-loading-pipelines}

The data loading pipeline connects storage to accelerators, transforming raw data into training batches. Pipeline design determines whether storage bandwidth is fully utilized and whether GPUs remain fed during training.

#### Pipeline Stages {#sec-pipeline-stages}

A typical data loading pipeline includes data reading (fetch bytes from storage such as disk, network, or object store), decompression (decompress compressed formats like GZIP, ZSTD, or JPEG), deserialization (parse structured data such as protobuf or JSON), transformation (apply augmentations like resize, crop, or normalize), batching (collate samples into batches), and transfer (move batches to accelerator memory).

Each stage has different compute and bandwidth characteristics as summarized in @tbl-pipeline-stages:

| Stage | Bound By | Parallelizable | Typical Duration |
|-------|----------|----------------|------------------|
| Reading | Storage bandwidth | Yes (sharded data) | 1-100 ms |
| Decompression | CPU compute | Yes (per sample) | 0.1-10 ms |
| Deserialization | CPU compute | Yes (per sample) | 0.01-1 ms |
| Transformation | CPU compute | Yes (per sample) | 0.1-50 ms |
| Batching | Memory bandwidth | Limited | 0.1-1 ms |
| Transfer | PCIe bandwidth | Limited (few DMA channels) | 0.1-10 ms |

: Pipeline stages have different performance characteristics. {#tbl-pipeline-stages}

#### Prefetching and Pipelining {#sec-prefetching}

The key to hiding latency is pipelining: while the GPU processes batch $N$, the CPU prepares batch $N+1$, and storage fetches data for batch $N+2$. This requires maintaining multiple batches in flight simultaneously.

The prefetch buffer size determines how much latency can be hidden:

$$T_{hidden} = N_{prefetch} \times T_{batch}$$

where $N_{prefetch}$ is the number of batches buffered and $T_{batch}$ is the GPU batch processing time.

For a 200 ms batch time with 100 ms storage latency, prefetching just 1 batch hides the storage latency. However, variance in storage latency requires larger buffers: if storage latency varies from 50-500 ms, prefetching 3-5 batches ensures GPUs never wait.

#### Caching Strategies {#sec-caching-strategies}

Caching can dramatically improve data loading performance when datasets are accessed repeatedly.

**Local disk caching** fetches from remote storage on first access and writes to local NVMe. Subsequent accesses read from local disk at 7-25 GB/s rather than network speeds. This approach is effective when the dataset fits on local storage, multiple epochs are trained, or network bandwidth is limited.

**Memory caching:** Entire dataset or frequently accessed portions are loaded into DRAM. Achieves 100+ GB/s bandwidth but limited by DRAM capacity (typically 512 GB - 2 TB per node).

**Distributed caching:** Services like Alluxio[^fn-alluxio] [@li2014tachyon] provide a distributed cache layer between compute and storage. Multiple nodes contribute memory to a shared cache, enabling datasets larger than single-node memory to be cached.

[^fn-alluxio]: **Alluxio (formerly Tachyon)**: An open-source virtual distributed storage system created at UC Berkeley AMPLab in 2013. Alluxio provides a unified namespace across multiple storage systems (S3, HDFS, Azure Blob) with memory-speed caching. For ML workloads, Alluxio can cache hot training data across cluster nodes, reducing repeated fetches from cold object storage. Meta uses a similar internal system to accelerate training data access, achieving 10x throughput improvement for frequently accessed datasets.

**Cache invalidation** for ML is straightforward: training datasets are immutable. The cache can use simple LRU eviction without concern for consistency.

#### Shuffling in Distributed Training {#sec-shuffling}

Shuffling is essential for training: without shuffling, the model sees samples in the same order every epoch, potentially learning spurious ordering correlations. Distributed shuffling is challenging because each worker must see different samples, samples should be randomly ordered within and across workers, and communication for global shuffle is expensive.

**Common approaches:**

**File-level shuffle:** Assign each worker a random subset of data files. Workers read their files sequentially. Low communication cost but limited randomness: samples within a file are always adjacent.

**Shuffle buffer:** Each worker maintains a buffer of $B$ samples. New samples are randomly exchanged with buffer contents. Provides good local randomness without communication.

**Reservoir sampling:** When the dataset is too large to shuffle in memory, reservoir sampling maintains a random subset. Each new sample has probability $k/n$ of entering the reservoir, where $k$ is reservoir size and $n$ is samples seen.

**Epoch boundary shuffle:** At epoch boundaries, workers exchange file assignments. This adds randomness across epochs without per-batch communication.

::: {.callout-example title="Distributed Data Loading for 1024-GPU Training"}

Consider loading ImageNet data for 1024-GPU training with optimal throughput.

**Configuration:**

- 1024 GPUs across 128 nodes (8 GPUs/node)
- Target: 80% GPU utilization
- Batch size per GPU: 256 images (150 KB average)
- Target iteration time: 200 ms
- Required bandwidth: 157 GB/s (from earlier calculation)

**Solution architecture** stores 1.4 TB ImageNet as 1000 WebDataset TAR shards in object storage, assigns each node roughly 8 non-overlapping shards, caches shards to local NVMe (10 TB per node available) on first epoch, uses 16 worker processes per node (2 per GPU), and maintains a prefetch buffer of 4 batches per GPU.

**Bandwidth calculation:**

- Per-node requirement: 157 GB/s / 128 nodes = 1.23 GB/s
- Local NVMe provides: 7+ GB/s (ample margin)
- First epoch from object storage: Each node needs 11 GB, ~90 seconds to cache

After the first epoch, training is entirely from local cache, eliminating network bottlenecks.

:::

### Data Locality Principles {#sec-data-locality}

The cost of moving data between nodes dominates computation time for many ML workloads. Data locality places computation where data already resides, a fundamental principle underlying both Spark [@zaharia2016apache] and Ray [@moritz2018ray].

**Locality Hierarchy:**

| Locality Level | Description | Typical Latency | Bandwidth |
|----------------|-------------|-----------------|-----------|
| GPU_LOCAL | Data in GPU HBM | ~10 ns | 3+ TB/s |
| CPU_LOCAL | Data in same-node CPU memory | ~100 ns | 200 GB/s |
| RACK_LOCAL | Data on another node in same rack | ~100 μs | 25 GB/s |
| ANY | Data on any node in cluster | ~1 ms | 10 GB/s |

The 100x bandwidth difference between CPU_LOCAL (200 GB/s) and ANY (10 GB/s via network) makes locality-aware scheduling essential for bandwidth-intensive ML workloads.

**Scheduling for Locality:**

Ray's scheduler exemplifies locality-aware scheduling:

1. When a task requires a data object, the scheduler identifies nodes holding that object
2. Tasks are preferentially scheduled to nodes with data (if capacity available)
3. If no capacity, the task is scheduled elsewhere and data is transferred
4. Scheduling decisions consider: data size, transfer time, node load, task urgency

For training data loading:

- Each worker is assigned shards stored on its local NVMe
- Workers read local shards (7+ GB/s) rather than remote (1 GB/s network)
- Shuffle operations respect locality: map outputs written locally, reducers fetch as needed

**When Locality Fails:**

Locality optimization breaks down when hot data (popular embeddings accessed by many workers) cannot all be local, feature store lookups are inherently non-local (random access), or small data causes scheduling overhead to exceed data transfer time.

For these cases, replication and caching substitute for locality.

**Quantitative Example:**

ImageNet training with 1000 WebDataset shards across 100 nodes:

- Local access: Each node stores 10 shards, reads at 10 GB/s = 100 MB/shard for 10 shards = 1 GB per node delivered in 0.1 seconds
- Remote access: If shards were centralized, 100 nodes competing for 10 GB/s network = 0.1 GB/s per node

Locality provides 100x throughput improvement for this workload.

### Model-Type Diversity in Training Data {#sec-training-data-diversity}

Training data requirements vary dramatically across model types as shown in @tbl-training-data-diversity, affecting storage architecture, data format selection, and pipeline design.

| Model Type | Data Format | Typical Volume | Key Storage Challenge |
|------------|------------|----------------|----------------------|
| LLM | Tokenized text | 10-100 TB | Deduplication at scale, quality filtering |
| Vision | Images/Video | 100 TB - 10 PB | Augmentation pipeline throughput |
| RecSys | User interaction logs | 1+ PB | Privacy compliance, real-time freshness |
| Scientific | Simulations, sensor data | 10+ PB | Irregular structure, domain-specific formats |
| Speech/Audio | Waveforms, spectrograms | 10-100 TB | Variable length sequences |

: Training data characteristics vary significantly by model type. {#tbl-training-data-diversity}

**LLM training data** presents unique challenges at the preprocessing stage rather than during training. Raw web crawls contain duplicate content, low-quality text, and potentially harmful material. Deduplication alone can reduce dataset size by 30-50% [@wenzek2020ccnet]. The storage challenge is running these preprocessing pipelines at scale: processing Common Crawl (petabytes of raw HTML) requires distributed processing frameworks like Spark or Dataflow, with intermediate results stored in distributed file systems.

**Vision training data** is bandwidth-intensive during training due to real-time augmentation. Each image must be decoded, randomly augmented (crop, flip, color jitter), and normalized before being batched. The augmentation pipeline runs on CPU and can become the bottleneck for large vision models. Storing pre-augmented images is impractical (each image would need hundreds of augmented variants), so augmentation must happen during training.

**Recommendation system data** involves continuous streams of user interactions rather than static datasets. Each user action (click, purchase, view) becomes a training sample. The storage challenge is maintaining freshness: recommendation models trained on yesterday's data may miss today's trends. This requires streaming data architectures (Kafka, Pub/Sub) feeding into training pipelines, with careful attention to data retention policies for privacy compliance.

**Scientific ML data** often involves domain-specific formats (HDF5, NetCDF, FITS) with multi-dimensional arrays representing simulation outputs or sensor measurements. These formats support chunked access for random access patterns that sequential formats like TFRecord do not handle well. Storage systems must support both high-throughput sequential access for training and random access for data exploration.

## Checkpoint Storage Systems {#sec-checkpoint-storage}

Training large models requires days to months of continuous computation. During this time, hardware failures, software bugs, and preemption events can terminate training at any point. Without checkpoints, a failure after two weeks of training would require restarting from the beginning, wasting millions of dollars in compute. Checkpointing saves model state periodically, enabling recovery from the most recent checkpoint rather than from scratch. The engineering challenge is minimizing checkpoint overhead while ensuring reliable recovery.

### Checkpoint Architecture Fundamentals {#sec-checkpoint-architecture}

With the cost of lost computation established, we can examine precisely what state must be preserved to enable recovery. A checkpoint captures the complete state needed to resume training exactly where it left off. This includes model parameters (the current values of all trainable weights; for a 175 billion parameter model in FP16 [@brown2020gpt3], this is 350 GB of dense data), optimizer state (optimizers like Adam[^fn-adam-optimizer] [@kingma2015adam] maintain per-parameter statistics such as first and second moments, with Adam's state being 2x the size of model parameters, adding 700 GB for the 175B parameter case), learning rate scheduler state (the current position in the learning rate schedule including step count and warmup progress), random number generator state (to ensure reproducibility, the RNG state for each worker must be saved), and data loader state (which samples have been seen in the current epoch, enabling resumption without repeating or skipping samples).

[^fn-adam-optimizer]: **Adam (Adaptive Moment Estimation)**: The most widely used optimizer for deep learning, introduced in 2014. Adam maintains two exponential moving averages per parameter: the first moment (mean of gradients, for momentum) and second moment (mean of squared gradients, for adaptive learning rates). This 2x memory overhead becomes significant at scale: a 175B parameter model requires 350 GB for weights and 700 GB for Adam state, totaling over 1 TB per checkpoint. Memory-efficient optimizers like Adafactor reduce this overhead by factorizing the second moment matrix, trading some convergence speed for 4-8x memory savings.

The total checkpoint size for a large model can reach several terabytes:

$$S_{checkpoint} = S_{params} + S_{optimizer} + S_{auxiliary}$$

For a 175B parameter model with Adam optimizer:

$$S_{checkpoint} = 350 \text{ GB} + 700 \text{ GB} + \text{~1 GB} \approx 1.05 \text{ TB}$$

### Synchronous vs Asynchronous Checkpointing {#sec-sync-async-checkpoint-storage}

Checkpointing can be performed synchronously (training stops during checkpoint) or asynchronously (checkpoint happens in background while training continues).

**Synchronous checkpointing** is simpler but introduces overhead: training pauses, all workers save their state to storage, the coordinator confirms checkpoint complete, and training resumes.

The overhead depends on checkpoint size and storage bandwidth:

$$T_{checkpoint} = \frac{S_{checkpoint}}{B_{storage}}$$

For a 1 TB checkpoint writing to a parallel file system at 50 GB/s aggregate bandwidth, checkpoint time is 20 seconds. If checkpoints occur every 10 minutes, the overhead is:

$$\text{Overhead} = \frac{T_{checkpoint}}{T_{interval}} = \frac{20 \text{ s}}{600 \text{ s}} = 3.3\%$$

**Asynchronous checkpointing** overlaps checkpoint I/O with training computation. Training continues normally, a background thread copies model state to CPU memory, the background thread writes to storage while training proceeds, and the next checkpoint begins only after the previous completes.

Asynchronous checkpointing hides I/O latency but introduces complexity: memory overhead (must maintain a copy of model state for checkpointing while training modifies the live copy), consistency (must snapshot state at a consistent point between optimizer steps), and backpressure (if checkpoint I/O is slower than training, memory fills with pending checkpoints).

For most training runs, asynchronous checkpointing reduces overhead to near zero. The memory overhead (one additional copy of model state) is typically acceptable on systems with sufficient host memory.

### Distributed Checkpointing for Sharded Models {#sec-distributed-checkpointing-storage}

When models are sharded across multiple devices using parallelism strategies (examined in @sec-distributed-training), each device holds only a portion of the model state. Checkpointing must coordinate across all devices to produce a consistent, complete checkpoint at the same logical training point.

**Coordination challenge.** The storage system must ensure all devices write their shards simultaneously, capturing a consistent snapshot of distributed model state. If devices checkpoint at different training steps, recovery would load inconsistent weights, corrupting the model. Barrier synchronization across all devices precedes checkpoint writes, ensuring global consistency.

**Bandwidth challenge.** With 128 devices each writing simultaneously, the storage system must sustain 128x single-device write rates during checkpoint windows. For terabyte-scale checkpoints, this aggregate bandwidth requirement often exceeds what a single parallel file system can provide, necessitating tiered storage architectures that use local NVMe for immediate writes with background migration to durable distributed storage.

**Data parallel optimization.** When training uses data parallelism (where multiple devices hold identical model replicas), checkpointing can save just one replica's state. Since all replicas are identical, saving one replica reduces storage by the data parallelism degree. This optimization is transparent to the storage system but significantly reduces aggregate checkpoint volume.

**Hybrid parallel checkpointing** combines coordination across multiple parallelism dimensions. For the hybrid approaches developed in @sec-distributed-training, storage systems must handle coordinated writes from many devices with optimizations like saving only one data-parallel replica (since replicas are identical) to reduce aggregate checkpoint volume.

**Checkpoint aggregation** reduces storage overhead by having ranks aggregate their state before writing. Rather than 128 small writes, a few ranks collect and write large aggregated files. This improves storage efficiency (fewer small files) at the cost of additional memory and communication.

### Checkpoint Failure Handling {#sec-checkpoint-failures}

Checkpoint storage must handle failures gracefully. Understanding failure modes and their mitigations is essential for reliable training at scale.

**Partial Write Failure:**

Storage fails after some but not all checkpoint shards are written. Without protection, recovery might load inconsistent state: some shards from the new checkpoint, some from the old.

*Solution: Atomic commit protocol.* All shards are first written to temporary locations with unique names (e.g., `checkpoint_step_10000.tmp.worker_0`). After all workers confirm write completion, a single atomic operation (rename or commit record) marks the checkpoint complete. Recovery reads only committed checkpoints, identified by the presence of the commit marker.

```text
1. Workers write: checkpoint_step_10000.tmp.worker_{0..N}
2. Coordinator verifies all workers complete
3. Coordinator writes: checkpoint_step_10000.COMPLETE
4. On recovery: only load checkpoints with .COMPLETE marker
```

**Storage Node Failure During Training:**

A storage server fails while training continues, potentially corrupting a checkpoint in progress.

*Solution: Replication.* With 3x replication, losing one storage node does not lose data. The storage system automatically redirects writes to surviving replicas. Checkpoint writes should require quorum acknowledgment (2 of 3 replicas) before confirming success.

**Checkpoint Corruption Detection:**

Silent data corruption (bit flips, media errors) can render checkpoints unusable without obvious failure signals.

*Solution: End-to-end checksums.* Each checkpoint shard includes a cryptographic hash of its contents. On recovery:

1. Load checkpoint shard
2. Compute checksum
3. Verify against stored checksum
4. If mismatch: fall back to previous checkpoint, alert operators

**Example failure scenario and recovery cost:**

1. Training at step 10,000, checkpoint initiated
2. Worker 0 writes shard successfully
3. Network partition: Workers 1-3 cannot reach storage
4. Coordinator timeout (30 seconds): checkpoint marked failed
5. Training continues from last good checkpoint (step 9,000)
6. Work from steps 9,000-10,000 must be repeated

*Cost calculation:*

$$\text{Wasted compute} = 1000 \text{ steps} \times 1.5 \text{ s/step} \times 1024 \text{ GPUs} \times \$0.001\text{/GPU-s} = \$1,536$$

This quantifies why checkpoint reliability matters: a $1,536 loss from one failed checkpoint motivates engineering investment in checkpoint robustness.

**Mitigating Checkpoint Storms**

When 4,000 GPUs attempt to write checkpoint data simultaneously at the end of an epoch, the aggregate instantaneous bandwidth demand can exceed the storage system's ingress capacity by orders of magnitude. This phenomenon, known as a **checkpoint storm**, creates a "thundering herd" effect that can trigger timeout errors or packet drops in the storage network.

The solution is **staggered checkpointing**. Rather than having all ranks write at $T=0$, the system introduces a deterministic delay based on rank:
$$
T_{write}(rank) = T_{start} + (rank \times \delta)
$$
where $\delta$ is a small delay (e.g., 10-100ms). This smoothes the write burst from a sharp spike into a plateau that matches the storage system's sustained throughput capability. While this slightly extends the total checkpoint time, it drastically reduces failure rates and tail latency.

**Straggler Mitigation:**

With 1000+ workers, one slow worker can delay checkpoint completion by minutes, blocking all training.

*Strategies:*

1. **Timeout with fallback**: If worker does not reach barrier in T seconds, assume failed, abort checkpoint, continue with previous
2. **Async checkpointing**: Workers checkpoint when ready; coordinator tracks which steps have full coverage
3. **Backup workers**: Redundant workers ensure N-k completion suffices (similar to gradient synchronization strategies)

### Optimal Checkpoint Interval: Young-Daly Formula {#sec-checkpoint-interval}

Checkpointing too frequently wastes time on I/O overhead. Checkpointing too infrequently risks losing large amounts of work to failures. The optimal interval balances these concerns.

The **Young-Daly formula**[^fn-young-daly] [@young1974first; @daly2006higher] provides the optimal checkpoint interval:

[^fn-young-daly]: **Young-Daly Formula**: Named after John Young (1974) who derived the first approximation and John Daly (2006) who provided the higher-order correction. The formula emerges from minimizing expected wasted work: checkpoint too rarely and failures lose significant progress; checkpoint too often and overhead accumulates. The square root relationship ($T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$) means that doubling checkpoint write speed allows 41% less frequent checkpoints, not 50%. This counterintuitive result has significant cost implications for storage architecture decisions.

::: {.callout-definition title="Young-Daly Checkpoint Interval"}

$$T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$$

where $T_{save}$ is the time to save a checkpoint and $MTBF$ is the mean time between failures.

:::

This formula minimizes expected wasted work, accounting for both checkpoint overhead and work lost to failures. The MTBF[^fn-mtbf] for GPU clusters decreases inversely with cluster size, making optimal checkpoint intervals surprisingly short for large-scale training.

[^fn-mtbf]: **MTBF (Mean Time Between Failures)**: A reliability metric representing the average time a system operates before experiencing a failure. For a single GPU, MTBF might be 30,000 hours (about 3.5 years). For a cluster, MTBF decreases inversely with component count: 1,024 GPUs with individual MTBF of 30,000 hours yield a cluster MTBF of roughly 29 hours, meaning failures occur almost daily. This inverse scaling explains why large-scale ML training is fundamentally a distributed systems problem: at 10,000 GPUs, MTBF drops to about 3 hours, making failure handling the dominant engineering challenge rather than an edge case.

::: {.callout-example title="Checkpoint Interval for Large-Scale Training"}

Consider training GPT-3 scale model on 1024 GPUs:

**Given:**
- Checkpoint size: 1 TB
- Storage bandwidth: 100 GB/s aggregate
- Checkpoint time: $T_{save} = 10$ seconds
- GPU MTBF: 30,000 hours per GPU
- Cluster MTBF: $30000 / 1024 \approx 29$ hours

**Optimal interval calculation:**

$$T_{opt} = \sqrt{2 \times 10 \text{ s} \times 29 \times 3600 \text{ s}} = \sqrt{2,088,000} \approx 1445 \text{ s} \approx 24 \text{ min}$$

**Interpretation:** Checkpoint every 24 minutes. This balances the 10-second checkpoint overhead against the 29-hour MTBF.

**Efficiency calculation:**

Expected work lost per failure: $T_{opt}/2 = 12$ minutes

Checkpoint overhead: $T_{save}/T_{opt} = 10/1445 = 0.7\%$

Total overhead: Checkpoint overhead + (expected loss rate × loss per failure)

With one failure per 29 hours and 12 minutes lost per failure:

$$\text{Loss rate} = \frac{12 \text{ min}}{29 \times 60 \text{ min}} = 0.7\%$$

Total overhead: 0.7% (checkpointing) + 0.7% (failures) = 1.4%

:::

### Incremental and Delta Checkpointing {#sec-incremental-checkpointing}

Full checkpoints save the complete model state regardless of how much has changed. For models where only a portion of parameters change significantly between checkpoints, incremental approaches reduce storage and I/O costs.

**Incremental checkpointing** saves only parameters that have changed since the last checkpoint. This requires tracking which parameters have been modified, maintaining a base checkpoint plus deltas, and periodically consolidating into a new full checkpoint.

For dense models where all parameters update every step, incremental checkpointing provides little benefit. For sparse models or fine-tuning scenarios where many parameters are frozen, the savings can be substantial.

**Delta compression** compresses the difference between consecutive checkpoints rather than the absolute values. If parameters change by small amounts each step, the delta is highly compressible. Techniques include XOR encoding (store the XOR of current and previous parameter values), floating-point prediction (predict next value from previous and store residual), and lossy compression (accept small errors in checkpoint for large compression ratios).

Practical compression ratios depend on model type and training dynamics as shown in @tbl-delta-checkpoint:

| Model Type | Full Checkpoint | Delta Size | Compression Ratio |
|------------|-----------------|------------|-------------------|
| LLM (dense updates) | 1 TB | 50-100 GB | 10-20x |
| RecSys (sparse embedding updates) | 10 TB | 100-500 GB | 20-100x |
| Vision (fine-tuning) | 10 GB | 100 MB | 100x |

: Delta checkpointing effectiveness varies by model type and training phase. {#tbl-delta-checkpoint}

### Checkpoint Storage Architecture {#sec-checkpoint-storage-architecture-storage}

The storage system for checkpoints must satisfy several requirements: high bandwidth (minimize checkpoint time), strong consistency (partial checkpoints must not appear complete), durability (checkpoints must survive storage failures), and low latency for reads (fast recovery after failures).

**Parallel file systems** (Lustre, GPFS) provide the highest bandwidth through striping and parallel I/O. A large Lustre deployment can deliver 100+ GB/s aggregate bandwidth, enabling TB-scale checkpoints in seconds. The tradeoff is operational complexity and cost.

**Object storage** (S3, GCS) provides durability and low cost but higher latency and lower bandwidth than parallel file systems. Object storage works well for infrequent checkpoints of moderate size (< 100 GB) but becomes bottleneck for TB-scale checkpoints every few minutes.

**Tiered storage** combines the benefits of both by writing checkpoints to fast parallel file system, asynchronously copying to durable object storage, deleting from fast tier after copy completes, and recovering from fast tier if available or otherwise from object storage.

This architecture provides both performance (fast writes) and durability (object storage backup) while managing cost (limited fast storage).

### Model-Type Checkpoint Considerations {#sec-checkpoint-model-types}

Checkpoint strategies vary significantly by model architecture as summarized in @tbl-checkpoint-strategies:

| Model Type | Checkpoint Size | Typical Frequency | Recommended Strategy |
|------------|-----------------|-------------------|---------------------|
| LLM (175B) | 700 GB - 1 TB | Every 10-30 min | Distributed async, tiered storage |
| LLM (7B) | 14-28 GB | Every 5-15 min | Single-node, parallel FS |
| RecSys (10TB embeddings) | 10+ TB | Incremental every hour | Delta compression, streaming |
| Vision (ResNet-50) | 200 MB | Every epoch | Simple sync, local + remote copy |
| Vision (ViT-22B) | 88-175 GB | Every 15-30 min | Distributed, parallel FS |

: Checkpoint strategies should be tailored to model size and training dynamics. {#tbl-checkpoint-strategies}

**Large Language Models** checkpoint infrequently (relative to iteration count) because checkpoint size dominates. A 1 TB checkpoint at 100 GB/s still takes 10 seconds, during which thousands of dollars of GPU time is consumed. Asynchronous checkpointing and high-bandwidth storage are essential.

**Recommendation Systems** present unique challenges due to massive embedding tables. A DLRM-style model[^fn-dlrm] [@naumov2019dlrm] might have 10 TB of embedding parameters but only 1 GB of dense MLP parameters. Incremental checkpointing of modified embeddings (which may be a small fraction of the table) provides order-of-magnitude savings over full checkpoints.

[^fn-dlrm]: **DLRM (Deep Learning Recommendation Model)**: An open-source recommendation model architecture published by Meta in 2019 that became the industry reference implementation. DLRM processes sparse categorical features (user ID, item ID) through embedding tables and dense numerical features through MLPs, combining them via feature interactions. The architecture's defining characteristic is embedding table dominance: production DLRM variants at Meta exceed 10 TB of embeddings with only gigabytes of dense parameters. This asymmetry drives unique storage requirements where embedding access patterns determine system performance.

**Vision Models** at typical scales (< 1 billion parameters) checkpoint easily. The entire checkpoint fits in a few GB, which can be written in under a second even to modest storage. The challenge is ensuring checkpoints are copied to durable storage before being deleted locally.

**Fine-tuning runs** of any model type can use delta checkpointing efficiently. Only the fine-tuned parameters (often < 1% of total for LoRA[^fn-lora]-style methods) need to be saved, reducing checkpoint size by 100x or more.

[^fn-lora]: **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning technique introduced by Microsoft in 2021 that freezes the original model weights and trains small low-rank matrices that modify layer outputs. A typical LoRA configuration adds only 0.1-1% additional parameters while achieving performance comparable to full fine-tuning. For storage, LoRA dramatically reduces checkpoint sizes: a 7B parameter model fine-tuned with LoRA requires only 10-100 MB of adapter weights rather than 14 GB of full weights. This enables storing hundreds of fine-tuned variants without the storage cost of full model copies.

## Feature Stores {#sec-feature-stores}

Checkpoint storage ensures that training can recover from failures, preserving weeks of computational progress in durable, recoverable form. But training is only half the ML lifecycle: once models are trained, serving them at production scale requires different storage infrastructure entirely. Where checkpoints optimize for write bursts and strong consistency, serving infrastructure must optimize for continuous low-latency reads across millions of concurrent requests.

Feature stores address this serving-time challenge, transforming raw data into ML-ready signals with strict latency guarantees. For recommendation systems, the dominant ML workload in production, feature stores are essential infrastructure: every user request triggers hundreds of feature lookups that must complete in milliseconds. Production systems at companies like Meta, Google, and Netflix depend critically on feature stores to bridge the gap between offline training pipelines and real-time serving requirements.

This section develops the architecture and design principles of feature stores, with particular attention to why they matter enormously for recommendation systems while playing a smaller role for LLMs and vision models.

### Why Feature Stores Exist {#sec-feature-store-motivation}

The core problem feature stores solve is the training-serving gap: features computed during training must be reproducible during serving, but the contexts differ dramatically.

**During training**, features are computed in batch over historical data. There is no latency constraint: a training pipeline can spend hours computing features over millions of training examples. The priority is correctness and coverage.

**During serving**, features must be available within milliseconds for real-time inference. A recommendation system making personalized content suggestions has perhaps 50ms total latency budget; feature retrieval might consume 5-10ms of that budget. The priority is latency and availability.

Without a feature store, teams face painful tradeoffs: duplicate implementation (engineers write feature computation logic twice, once in batch Python or Spark for training and once in low-latency Java or C++ for serving, creating maintenance burden and inconsistency risk), point-in-time bugs (training features are computed with access to future data that would not be available at serving time, causing training-serving skew), and freshness problems (features computed in batch become stale and serving uses outdated information).

Feature stores solve these problems by providing unified feature computation (write feature logic once, execute in both batch and streaming contexts), point-in-time correctness (retrieve features as they were at a specific historical moment), low-latency serving (pre-computed features available with single-digit millisecond latency), and feature reuse (features computed once can be shared across many models).

### Feature Store Architecture {#sec-feature-store-architecture}

A feature store has two primary components: the offline store for training and the online store for serving.

::: {.callout-note title="Figure Placeholder: Feature Store Architecture" collapse="true"}
```{.tikz}
% TODO: Diagram showing Offline Store (Training) and Online Store (Serving) paths
\node[draw, align=center] {Feature Store\nOffline vs Online Paths};
```
**Feature Store System Design**. The dual-path architecture solving the training-serving skew problem. The offline path (top) processes batch data into a data warehouse for training. The online path (bottom) ingests streaming events into a low-latency key-value store for serving. A unified API ensures that features retrieved for training (historical) match those retrieved for inference (current), guaranteeing point-in-time correctness.
:::

#### Offline Store {#sec-offline-store}

The offline store contains the complete historical record of feature values, enabling training on any time window. It is optimized for throughput rather than latency.

**Storage technologies**: Data lakes (S3 + Parquet), data warehouses (BigQuery, Snowflake, Redshift), or specialized time-series databases.

**Data organization**: Features are stored with timestamps, enabling point-in-time queries:

```text
| user_id | feature_name      | value | timestamp           |
|---------|-------------------|-------|---------------------|
| 12345   | purchase_count_7d | 3     | 2024-01-15 00:00:00 |
| 12345   | purchase_count_7d | 5     | 2024-01-16 00:00:00 |
| 12345   | avg_session_length| 4.2   | 2024-01-15 00:00:00 |
```

**Point-in-time joins**: Training requires joining features to labels at the exact time the label event occurred. If a user clicked an ad at 2024-01-15 14:23:00, training needs the features that were available at 14:22:59, not the features computed later that day.

::: {.callout-warning title="Point-in-Time Correctness"}

Point-in-time correctness is the single most important property of feature stores. Using future information during training (a form of data leakage) produces models that perform well in offline evaluation but fail in production. This bug is insidious because metrics look great until deployment.

Example: A fraud detection model trained with features including "user_reported_fraud" appears to achieve 99% accuracy. But this feature is only populated after fraud is reported, which happens after the transaction being scored. The model learns to recognize already-reported fraud, not to predict future fraud.

:::

**Query patterns**: Offline queries retrieve features for millions of training examples in batch:

```sql
SELECT
    labels.user_id,
    labels.item_id,
    labels.clicked,
    features.purchase_count_7d,
    features.avg_session_length
FROM training_labels labels
LEFT JOIN features
    ON labels.user_id = features.user_id
    -- Point-in-time correctness: only use features available before the event
    AND features.timestamp <= labels.event_time
    -- Limit to most recent feature version within window
    AND features.timestamp > labels.event_time - INTERVAL 1 DAY
```

#### Online Store {#sec-online-store}

The online store provides low-latency access to the most recent feature values for serving. It trades historical depth for speed.

**Storage technologies**: Key-value stores optimized for point lookups (Redis[^fn-redis], DynamoDB, Bigtable [@chang2008bigtable], Cassandra).

[^fn-redis]: **Redis (Remote Dictionary Server)**: An in-memory data structure store created by Salvatore Sanfilippo in 2009, now the most widely used in-memory database. Redis achieves sub-millisecond latency for key-value lookups by keeping all data in RAM, with optional persistence to disk. For feature stores, Redis serves as the online store providing 100,000+ lookups per second per instance with P99 latency under 1ms. The trade-off is cost: at approximately $10-15/GB/month for managed Redis versus $0.02/GB/month for object storage, organizations carefully select which features justify in-memory storage.

**Data organization**: Features are stored by entity key with only the most recent value:

```json
Key: user:12345
Value: {
    "purchase_count_7d": 5,
    "avg_session_length": 4.2,
    "last_updated": "2024-01-16T14:30:00Z"
}
```

**Access patterns**: Online queries retrieve features for a single entity or small batch:

```python
features = online_store.get_features(
    entity_id="user:12345",
    feature_names=["purchase_count_7d", "avg_session_length"],
)
# Returns in < 5ms
```

**Consistency with offline**: The online store must reflect the same feature values that the offline store would return for "now". Feature computation pipelines update both stores, with the offline store receiving the historical record and the online store receiving the current value.

#### Feature Computation Pipelines {#sec-feature-computation}

Features are computed by pipelines that transform raw data into feature values. These pipelines can operate in batch or streaming mode.

**Batch pipelines** compute features over historical data, typically running daily or hourly. They are simpler to implement and debug but produce features that are always somewhat stale.

```python
# Batch feature: user's purchase count in last 7 days
@feature(schedule="daily")
def purchase_count_7d(user_id: str, date: datetime) -> int:
    return db.query(
        f"""
        SELECT COUNT(*) FROM purchases
        WHERE user_id = '{user_id}'
        AND purchase_date > '{date - timedelta(days=7)}'
    """
    )
```

**Streaming pipelines** compute features from real-time event streams, providing fresh feature values within seconds of underlying events. They are more complex but essential for use cases where freshness matters (e.g., fraud detection, real-time recommendations).

```python
# Streaming feature: user's purchases in current session
@streaming_feature(source="purchase_events")
def session_purchase_count(event: PurchaseEvent, state: State) -> int:
    # Detect session boundary and reset counter
    if event.session_id != state.current_session:
        state.reset()  # Clear accumulated state from previous session
        state.current_session = event.session_id
    state.count += 1  # Increment within current session
    return (
        state.count
    )  # Updated within seconds of each purchase event
```

**Lambda architecture**[^fn-lambda-architecture] combines both: batch pipelines provide accurate but stale features; streaming pipelines provide fresh but potentially approximate features. The serving layer merges results, preferring fresh streaming values when available.

[^fn-lambda-architecture]: **Lambda Architecture**: A data processing architecture proposed by Nathan Marz in 2011, combining batch processing (accurate but slow) with stream processing (fast but approximate). The name references functional programming's lambda calculus. For feature stores, lambda architecture maintains two pipelines: batch pipelines compute historically accurate features daily, while streaming pipelines update features in real-time from event streams. The serving layer merges both views, using streaming data for recent windows and batch data for longer historical windows. The alternative Kappa architecture uses only streaming, simplifying operations but requiring careful handling of reprocessing scenarios.

### Feature Lookup Latency Budget {#sec-feature-latency}

Online serving imposes strict latency constraints. The feature lookup must fit within the overall inference latency budget.

::: {.callout-definition title="Feature Lookup Latency Budget"}

$$L_{feature} < L_{SLO} - L_{model} - L_{network} - L_{margin}$$

where:

- $L_{SLO}$ is the end-to-end latency SLO (e.g., 100ms)
- $L_{model}$ is model inference time (e.g., 20ms)
- $L_{network}$ is network round-trip time (e.g., 10ms)
- $L_{margin}$ is safety margin for variance (e.g., 20ms)

:::

::: {.callout-example title="Feature Latency Budget for Recommendation Serving"}

A recommendation system has 100ms end-to-end SLO. Breaking down the budget:

- Model inference: 30ms (two-tower retrieval + ranking)
- Network overhead: 15ms (client-server round trips)
- Business logic: 10ms (filtering, deduplication)
- Safety margin: 15ms (P99 variance)
- **Available for features: 30ms**

With 30ms budget and 200 features to retrieve, each feature lookup must complete in 0.15ms average. This is achievable only with:

1. Batch lookups (one round-trip for all 200 features)
2. Features co-located in same key-value store
3. In-memory storage (Redis, not disk-backed)
4. Same-region deployment (< 1ms network)

:::

### Embedding Table Storage {#sec-embedding-tables}

Recommendation systems present a unique storage challenge: embedding tables containing vectors for millions or billions of entities. These tables can reach terabytes in size as shown in @tbl-embedding-sizes while requiring single-digit millisecond lookup latency.

**Scale of embedding tables:**

| Application | Entities | Embedding Dim | Total Size |
|-------------|----------|---------------|------------|
| User embeddings | 1 billion | 128 | 512 GB |
| Product embeddings | 100 million | 256 | 100 GB |
| Ad embeddings | 10 million | 512 | 20 GB |
| Sparse ID embeddings | 100 billion | 64 | 25 TB |

: Embedding table sizes for production recommendation systems. {#tbl-embedding-sizes}

Tables of this scale cannot fit in memory on a single machine, requiring distributed storage strategies that balance latency, cost, and capacity.

**Storage strategies:**

**In-memory storage** (Redis, Memcached) provides the lowest latency (< 1ms) but highest cost. A 500 GB embedding table requires expensive high-memory instances. Suitable for frequently accessed embeddings with strict latency requirements.

**SSD-backed key-value stores** (RocksDB, Cassandra) provide 1-10ms latency at lower cost. The embedding is loaded from SSD on cache miss. Suitable for infrequently accessed embeddings or when cost constraints preclude in-memory storage.

**Tiered storage** keeps hot embeddings in memory and cold embeddings on SSD. Access patterns in recommendation systems are highly skewed: a small fraction of users and items account for most traffic. Keeping the top 10% of embeddings in memory while the remaining 90% are on SSD can provide good latency at reasonable cost.

**Embedding sharding** distributes large embedding tables across multiple servers:

$$\text{Shard ID} = \text{hash}(\text{entity\_id}) \mod N_{shards}$$

Each shard stores 1/N of the embeddings. Lookup requires determining the correct shard and querying that server. With consistent hashing, adding or removing shards rebalances minimal data.

### Model-Type Feature Store Requirements {#sec-feature-store-model-types}

Feature store importance varies dramatically by model type as shown in @tbl-feature-store-criticality:

| Model Type | Feature Store Criticality | Rationale |
|------------|---------------------------|-----------|
| RecSys | **Critical** | Millions of lookups/second, user/item features essential |
| Fraud Detection | **Critical** | Real-time features detect fraud patterns |
| Ad Ranking | **Critical** | User context, ad features, bid signals |
| Search Ranking | High | Query understanding, user history |
| LLMs | Low | Minimal runtime features, prompt is the input |
| Vision | Low-Medium | Optional context features, mainly model input |
| Speech | Low | Audio input, minimal runtime features |

: Feature store criticality varies by ML application. {#tbl-feature-store-criticality}

**Why feature stores are critical for RecSys but not LLMs:**

Recommendation systems make predictions about user-item interactions, requiring features about both the user and items. At serving time, the system must retrieve:

- User features: Demographics, historical behavior, session context
- Item features: Category, popularity, freshness
- Context features: Time of day, device, location
- Cross features: User-item affinity scores, collaborative filtering signals

These features cannot be derived from the input alone (unlike LLM prompts, which contain all necessary information). The feature store is the only way to provide this information to the model at serving time.

LLMs, by contrast, receive their input as a prompt. The prompt contains all information the model needs to generate a response. There are no external features to look up. The "feature store" for an LLM is simply the tokenizer and any prompt templates, not a database of entity features.

### Feature Store Platforms {#sec-feature-store-platforms}

Several platforms provide feature store functionality, each with different tradeoffs as summarized in @tbl-feature-store-buildbuy:

**Open source:**

- **Feast**: Most popular open-source feature store. Supports multiple backends (Redis, DynamoDB, BigQuery). Provides point-in-time joins, feature versioning.
- **Hopsworks**: Feature store with MLOps integration. Strong support for feature pipelines and versioning.

**Cloud-managed:**

- **Vertex AI Feature Store (GCP)**: Managed service with BigQuery integration. Auto-scaling online serving.
- **SageMaker Feature Store (AWS)**: Integrated with SageMaker ML workflow. S3 offline store, DynamoDB-backed online store.
- **Azure ML Feature Store**: Part of Azure ML ecosystem. Supports feature materialization and serving.

**Enterprise:**

- **Tecton**: Enterprise feature platform built by Feast creators. Sophisticated streaming feature support.
- **Databricks Feature Store**: Integrated with Databricks lakehouse. Unity Catalog integration for governance.

**Build vs buy considerations:**

| Factor | Build In-House | Use Platform |
|--------|---------------|--------------|
| Customization | Full control | Limited by platform |
| Development cost | High initial investment | Lower initial cost |
| Operations | Requires dedicated team | Managed by vendor |
| Scale | Requires expertise | Built-in scaling |
| Integration | Custom to stack | May require adaptation |

: Feature store build vs buy tradeoffs. {#tbl-feature-store-buildbuy}

For most organizations, starting with an open-source solution (Feast) or cloud-managed service provides faster time-to-value than building custom infrastructure. Custom feature stores become worthwhile at massive scale (billions of features, millions of QPS) where platform limitations become constraints.

## Model Registries and Artifact Management {#sec-model-registries}

The preceding sections addressed storage for different phases of the ML lifecycle: training data infrastructure provides the raw material, checkpoint storage preserves model state during training for fault tolerance, and feature stores deliver runtime data for serving. But what about the trained models themselves? Training produces model weights, but deploying those weights to production requires its own infrastructure: storage, versioning, and governance for the trained artifacts.

Model registries provide the storage, versioning, and governance layer between training completion and production deployment. They answer questions that become critical at scale: Which model version is currently in production? What training data and hyperparameters produced this model? Who approved this model for deployment?

### Model Registry Architecture {#sec-registry-architecture}

A model registry stores and organizes model artifacts with associated metadata, enabling teams to manage the lifecycle of models from experimentation through deployment to retirement.

**Core components:**

**Artifact storage** holds the actual model files: weights, configurations, preprocessing artifacts, and any ancillary files needed for inference. Storage backends range from simple object storage (S3, GCS) to specialized artifact stores (Artifactory, Nexus).

**Metadata store** maintains information about each model version: training parameters, performance metrics, data lineage, and deployment status. This is typically a database (PostgreSQL, MySQL) or document store (MongoDB).

**Versioning system** tracks model versions with semantic versioning or auto-incrementing identifiers. Each version is immutable: once registered, a model version cannot be modified.

**Access control** governs who can register, read, and promote models. Different teams may have different permissions (data scientists can register, MLOps can promote to production, only approved models can be deployed).

### Model Versioning {#sec-model-versioning}

Model versioning differs from code versioning in important ways:

**Artifacts are large and binary.** Git handles text diffs efficiently but struggles with multi-GB model files. Model registries use content-addressable storage, storing each unique artifact once regardless of how many versions reference it.

**Versions may not be sequential.** Teams often run multiple experiments in parallel, producing model versions that branch from different starting points. The registry must handle non-linear version histories.

**Metadata is as important as artifacts.** Knowing what hyperparameters, training data, and code version produced a model is essential for debugging and reproducibility.

A typical model version record includes:

```yaml
model_name: "product_recommender"
version: "v2.3.1"
status: "production"
registered_at: "2024-01-15T10:23:45Z"
registered_by: "ml-team-ci"

artifacts:
  model_weights: "s3://models/product_recommender/v2.3.1/model.pt"
  config: "s3://models/product_recommender/v2.3.1/config.yaml"
  tokenizer: "s3://models/product_recommender/v2.3.1/tokenizer/"

training:
  framework: "pytorch"
  framework_version: "2.1.0"
  training_data: "s3://datasets/product_interactions/2024-01-01/"
  training_data_hash: "sha256:a3b4c5..."
  hyperparameters:
    learning_rate: 0.001
    batch_size: 256
    epochs: 50
  training_job_id: "train-20240115-001"
  training_duration_hours: 12.5

metrics:
  validation_accuracy: 0.847
  validation_loss: 0.312
  auc: 0.923

lineage:
  parent_model: "product_recommender:v2.2.0"
  code_commit: "git:abc123"
  experiment_id: "exp-2024-01-15-hyperopt"
```

### Reproducibility and Lineage Tracking {#sec-reproducibility}

Reproducibility in ML requires tracking not just the model weights but the entire provenance chain: what data, code, and environment produced this model?

**Data lineage** tracks which datasets were used for training and validation. This includes:

- Dataset versions or snapshot identifiers
- Data preprocessing pipeline versions
- Any filtering or sampling applied
- Hash of the actual data used

**Code lineage** links models to the code that produced them:

- Git commit hash of training code
- Container image digest for training environment
- Framework versions and dependencies

**Environment lineage** captures the computational environment:

- Hardware (GPU type, count)
- Software (CUDA version, Python version, package versions)
- Random seeds used

**Full lineage enables:**

1. **Debugging**: When a model behaves unexpectedly, trace back to exact training conditions
2. **Auditing**: Demonstrate to regulators exactly how a model was trained
3. **Reproduction**: Train identical model from lineage information
4. **Comparison**: Understand why two model versions differ

::: {.callout-note title="Reproducibility in Practice"}

Perfect reproducibility in ML is difficult due to non-determinism in GPU operations, floating-point associativity, and framework internals. Lineage tracking enables approximate reproduction: training with the same data, code, and hyperparameters typically produces a model with similar (but not bit-identical) performance.

:::

### Model Lifecycle Stages {#sec-model-lifecycle}

Models progress through lifecycle stages, with the registry tracking current stage and stage transitions:

**Development**: Experimental models under active iteration. Many versions may be created and discarded. No quality guarantees.

**Staging**: Candidate models undergoing evaluation. Limited access, subjected to validation tests. Models that pass move to production; those that fail return to development.

**Production**: Approved models serving live traffic. Strict change control, monitoring requirements. Only promoted models reach production.

**Archived**: Retired models no longer serving traffic but retained for reference, auditing, or rollback. May be moved to cold storage.

**Deprecated**: Models scheduled for removal. Alerts generated if still accessed. Fully deleted after retention period.

```text
Development --> Staging --> Production --> Archived
     ^              |             |
     |              v             v
     +---- (failed) +-- Deprecated -> Deleted
```

### Artifact Storage Considerations {#sec-artifact-storage}

Model artifacts range from megabytes (small classifiers) to terabytes (large foundation models), requiring different storage strategies.

**Small models (< 1 GB)**: Object storage (S3, GCS) with standard redundancy. Download to inference servers is fast; model can be fetched on each server start.

**Medium models (1-100 GB)**: Object storage with regional caching. Pre-deploy models to inference servers to avoid startup latency. Consider compression for network transfer.

**Large models (> 100 GB)**: Distributed storage with parallel download. Sharded across multiple files for parallel access. May require local NVMe for serving latency requirements.

**Storage cost optimization strategies for model artifacts are detailed in @tbl-artifact-storage:**

| Strategy | Benefit | Tradeoff |
|----------|---------|----------|
| Compression | 2-5x size reduction | CPU overhead on load |
| Deduplication | Shared layers stored once | Complexity in artifact management |
| Tiered storage | Cold storage for old versions | Retrieval latency |
| Differential storage | Store only changed weights | Requires base model + diffs |

: Artifact storage optimization strategies. {#tbl-artifact-storage}

### Model Registry Platforms {#sec-registry-platforms}

Several platforms provide model registry functionality as compared in @tbl-registry-platforms:

**MLflow Model Registry**[^fn-mlflow] [@zaharia2018mlflow]: Open source, widely adopted. Integrates with MLflow tracking. Supports model stages, versioning, and deployment integration. Backed by file system or database storage.

[^fn-mlflow]: **MLflow**: An open-source MLOps platform created by Databricks in 2018, now the most widely adopted experiment tracking and model management tool. MLflow provides four components: Tracking (logging experiments), Projects (packaging code), Models (model packaging format), and Registry (model versioning and deployment). Its Python-first API integrates with all major ML frameworks. The model registry uses a database backend (SQLite for development, PostgreSQL or MySQL for production) for metadata and object storage for artifacts. Over 10 million monthly downloads make MLflow the de facto standard for experiment tracking in non-enterprise ML teams.

**Weights & Biases Model Registry**: Part of W&B platform. Strong experiment tracking integration. Artifact versioning with lineage.

**Vertex AI Model Registry (GCP)**: Managed service with Vertex AI integration. Model versioning, deployment to endpoints.

**SageMaker Model Registry (AWS)**: Part of SageMaker MLOps. Model groups, versions, approval workflows.

**Azure ML Model Registry**: Part of Azure ML. Model versioning, deployment, monitoring integration.

**Comparison:**

| Platform | Open Source | Deployment Integration | Lineage | Approval Workflows |
|----------|-------------|------------------------|---------|-------------------|
| MLflow | Yes | Via plugins | Basic | Basic |
| W&B | No | Limited | Strong | Limited |
| Vertex AI | No | Native GCP | Good | Yes |
| SageMaker | No | Native AWS | Good | Yes |
| Azure ML | No | Native Azure | Good | Yes |

: Model registry platform comparison. {#tbl-registry-platforms}

### Registry Design Patterns {#sec-registry-patterns}

Effective model registry usage follows established patterns:

**Pattern: Immutable versions.** Once registered, a model version cannot be modified. Updates create new versions. This ensures reproducibility and enables safe rollback.

**Pattern: Promotion gates.** Models must pass automated tests before promotion to production: validation metrics above threshold, bias tests passed, latency requirements met. Human approval may be required for certain model types.

**Pattern: Canary metadata.** Model versions include canary configuration: what percentage of traffic to receive initially, what metrics to monitor, automatic rollback conditions.

**Pattern: Model cards.** Each production model has a model card documenting intended use, limitations, performance characteristics, and ethical considerations. Required for governance and user understanding.

**Pattern: Retention policies.** Old model versions are automatically archived or deleted based on policy: keep last N versions, keep all versions newer than date, never delete production versions.

## Case Studies {#sec-storage-case-studies}

The storage system principles developed throughout this chapter manifest differently across organizations depending on their dominant workloads, scale, and infrastructure maturity. Four organizations illustrate these variations: Google for LLM training (sequential access, checkpoint-intensive), Meta for recommendation serving (random access, latency-critical), Tesla for vision data pipelines (bandwidth-intensive, quality-focused), and Spotify for hybrid workloads that balance multiple access patterns. As you read each case, watch for recurring themes: how scale forces architectural redesign of systems that worked at smaller sizes, how workload characteristics drive storage tier selection and optimization, and why data quality infrastructure proves as important as raw storage capacity.

This section examines these four case studies to illustrate how leading ML organizations have designed storage systems for their specific requirements.

### Google Colossus: Storage for LLM Training {#sec-case-study-google}

Google's Colossus file system, the successor to the original Google File System (GFS), demonstrates storage architecture optimized for massive-scale sequential workloads including LLM training.

**Scale and requirements:**

Google trains models including PaLM [@chowdhery2022palm] (540 billion parameters) and Gemini [@gemini2023] across thousands of TPUs. Training data for these models spans tens of terabytes of tokenized text, requiring sustained read throughput of hundreds of GB/s across the training cluster. Checkpoints for the largest models exceed 1 TB and must be saved within minutes to minimize training interruption.

**Architecture decisions:**

**Distributed metadata.** Unlike GFS's single master, Colossus distributes file system metadata across multiple servers (Curators). This eliminates the metadata bottleneck that would otherwise limit operations to ~10,000/second, enabling the high-frequency checkpoint writes and metadata operations that large-scale training requires.

**Erasure coding.** Colossus uses Reed-Solomon erasure coding [@reed1960polynomial] rather than simple replication. A typical configuration stores 9 data fragments plus 3 parity fragments, achieving durability equivalent to 3x replication while using only 1.33x storage. For petabyte-scale training data, this reduces storage costs by billions of dollars.

**D (Disk) servers.** Colossus separates the storage layer into D servers that manage physical disks. This abstraction enables flexible placement of data across disk types (HDD for cold data, SSD for hot data) without changing the file system interface.

**Integration with TPU architecture:**

Colossus integrates deeply with TPU training infrastructure:

- Data is striped to enable parallel reads from many D servers simultaneously
- TPU hosts run Colossus clients that prefetch training data during computation
- Checkpoint writes use dedicated bandwidth allocation to prevent interference with training data reads

**Lessons** include that at Google scale, the file system must be redesigned rather than just scaled (GFS concepts like large blocks, append-optimized, and single namespace remain valid, but implementation must be distributed), that erasure coding is essential for cost-effective storage at petabyte scale, and that tight integration between storage and compute systems enables efficiencies impossible with generic storage.

### Meta Feature Store: Recommendation at Scale {#sec-case-study-meta}

Meta's recommendation systems serve billions of users across Facebook, Instagram, and WhatsApp, requiring feature store infrastructure that handles trillions of feature lookups daily while maintaining single-digit millisecond latency.

**Scale and requirements:**

- Billions of users, each with hundreds of features
- Trillions of feature lookups per day
- P99 latency requirement: < 10ms
- Features updated continuously from user activity streams
- Embedding tables exceeding 10 TB

**Architecture decisions:**

**Hybrid online store.** Meta's feature store uses a tiered architecture:

- **L1 cache**: Process-local cache on serving machines. Sub-millisecond access for hot features.
- **L2 cache**: Distributed cache (Memcached) for warm features. 1-2ms access.
- **Persistent store**: Distributed key-value store (similar to RocksDB) for cold features. 5-10ms access.

This tiering exploits the power-law distribution of feature access: the most active 1% of users account for a disproportionate share of requests.

**Streaming feature computation.** Features are computed by streaming pipelines (similar to Flink) processing user activity in real-time. A user's "items_viewed_last_hour" feature updates within seconds of each view event, enabling recommendations that reflect immediate interests.

**Embedding table sharding.** User embeddings are sharded across thousands of servers using consistent hashing. Each server holds a fraction of the embedding table in memory. Lookup involves hashing the user ID to determine the shard, then a single network hop to retrieve the embedding.

**Point-in-time correctness.** The offline feature store maintains timestamped feature values, enabling training data generation with correct historical features. Training pipelines join labels (user interactions) with features as they existed at interaction time, preventing data leakage.

**Lessons** include that feature store design is dominated by the access pattern (billions of point lookups per second require in-memory storage for hot data), that streaming feature computation is essential for recommendation freshness, that tiered caching exploits access pattern skew to provide good latency at reasonable cost, and that point-in-time correctness is a non-negotiable requirement that is extremely difficult to retrofit.

### Tesla: Video Data Pipeline for Vision Training {#sec-case-study-tesla}

Tesla's Autopilot and Full Self-Driving systems are trained on video data collected from millions of vehicles, presenting unique storage challenges for vision model training at scale.

**Scale and requirements:**

- Fleet of millions of vehicles continuously collecting video
- Petabytes of video ingested daily (before filtering)
- Training datasets of selected clips reaching hundreds of TB
- Video must be decoded, augmented, and streamed to training GPUs
- Data selection is as important as data quantity

**Architecture decisions:**

**Hierarchical data selection** recognizes that not all collected video is valuable for training. Tesla's pipeline implements progressive filtering through on-vehicle filtering (neural networks on vehicle hardware identify interesting scenarios like edge cases or novel situations), upload filtering (only flagged clips are uploaded over cellular or WiFi), offline filtering (more sophisticated models further filter uploaded data), and labeling queue (high-value clips are prioritized for human labeling).

This filtering reduces storage requirements by orders of magnitude while focusing training on the most valuable data.

**Object storage with intelligent tiering** stores raw video in object storage with automatic tiering. Recent uploads reside in hot storage for immediate processing, processed clips move to warm storage, and archived raw footage resides in cold storage for potential re-processing.

**Custom data format** was developed by Tesla specifically for ML training. The format uses temporal compression aware of training access patterns (random frame access), provides multiple resolution variants for different training stages, and embeds sensor metadata (GPS, IMU, camera calibration) in the format.

**Distributed video decoding.** Video decoding is CPU-intensive. Tesla's data pipeline distributes decoding across many CPU workers, with decoded frames streamed to GPUs. This decouples decode throughput from GPU count.

**Lessons** include that for video data, the storage problem is inseparable from the data selection problem (storing everything is infeasible and intelligent filtering is essential), that custom data formats can provide significant efficiency gains when standard formats impose unacceptable overhead, and that decoding and augmentation pipelines require dedicated compute and cannot be an afterthought.

### Spotify: Hybrid ML Platform Storage {#sec-case-study-spotify}

Spotify combines recommendation systems (user-music matching) with audio understanding (content analysis), requiring storage infrastructure that serves both workload types efficiently.

**Scale and requirements:**

- Hundreds of millions of users with listening history
- Tens of millions of tracks, podcasts, and audiobooks
- Recommendation serving at millions of QPS
- Audio analysis models processing newly uploaded content
- Features spanning user behavior and audio content

**Architecture decisions:**

**Unified feature platform.** Spotify's feature platform serves both recommendation and audio ML:

- User features: Listening history, preferences, demographics
- Content features: Audio embeddings, genre classification, tempo
- Contextual features: Time of day, device, location

A single feature store serves all models, enabling feature reuse across teams.

**Content embedding pipeline.** New audio content flows through embedding pipelines:

1. Upload to object storage (GCS)
2. Audio analysis models extract embeddings
3. Embeddings written to feature store
4. Content available for recommendation within hours of upload

**Batch and streaming feature computation.** Some features are batch-computed (user's "top genres last month"), while others are streaming ("songs played this session"). Both types flow into the same feature store with appropriate freshness guarantees.

**GCP-native storage stack.** Spotify runs primarily on Google Cloud:

- BigQuery for offline feature store and analytics
- Bigtable for online feature serving
- GCS for raw data and model artifacts
- Dataflow for feature computation pipelines

This cloud-native approach reduces operational burden while providing the scale needed for Spotify's workloads.

**Model versioning for A/B testing.** Spotify runs continuous A/B tests with many model variants serving traffic simultaneously. The model registry tracks which variants are in each test, enabling analysis of model performance in production.

**Lessons** include that different ML workloads (recommendation and content understanding) can share storage infrastructure when designed with flexibility, that cloud-managed services provide operational simplicity at the cost of some customization, and that feature platforms serving multiple teams create significant organizational value through feature reuse.

### Cross-Cutting Themes {#sec-case-studies-themes}

Several themes emerge across these case studies:

**Scale requires architectural adaptation.** Solutions that work at small scale fail at large scale. Each organization redesigned storage architecture as scale increased, rather than simply adding capacity.

**Workload characteristics drive design.** LLM training (sequential, checkpoint-heavy) requires different storage than recommendation (random access, latency-critical). Organizations must understand their workload mix.

**Data quality infrastructure is as important as data quantity.** Tesla and Google invest heavily in data selection and quality, recognizing that more data is not always better data.

**Feature stores are production-critical for recommendation.** Meta and Spotify treat feature stores as tier-1 infrastructure with the same reliability requirements as serving systems.

**Cloud vs on-premise tradeoffs remain.** Google builds custom infrastructure; Spotify uses cloud services. Both approaches work; the choice depends on scale, expertise, and strategic priorities.

## Fallacies and Pitfalls {#sec-storage-fallacies-pitfalls}

Even organizations operating at the scales described above encounter recurring misconceptions. The following fallacies and pitfalls represent hard-won lessons from production ML systems, where assumptions from traditional computing fail to transfer to ML workloads.

**Fallacy: Object storage latency is acceptable for training data.**

Object stores like S3 and GCS offer seemingly infinite capacity at low cost, but their latency characteristics poorly match ML training patterns. First-byte latency of 50-200ms for object storage versus 1-10ms for file systems means that small-batch access patterns incur unacceptable overhead. A training pipeline reading 1000 small files sequentially from S3 spends more time waiting for first bytes than transferring data.

The solution is not to avoid object storage (it remains the most cost-effective option for large datasets) but to design pipelines around its characteristics: large sequential reads, extensive prefetching, and local caching. The common antipattern of treating object storage as a drop-in replacement for NFS leads to training throughput 3-5x lower than achievable.

**Pitfall: Sizing checkpoint storage by capacity rather than bandwidth.**

Organizations provision checkpoint storage based on model size: "Our model is 500GB, so we need 5TB for 10 checkpoints." This ignores the critical constraint: bandwidth during checkpoint writes.

A 500GB checkpoint written to NFS at 1 GB/s takes 8+ minutes. For a 1000-GPU training job with MTBF of 4 hours, the Young-Daly optimal interval is approximately 23 minutes. Spending 8 minutes (35% of the interval) on checkpoint writes is unacceptable overhead. The same checkpoint written to parallel file system at 100 GB/s takes 5 seconds (0.4% overhead).

Capacity planning must start with bandwidth: "We need to write 500GB in under 30 seconds" leads to very different architecture than "we need 5TB of capacity."

**Fallacy: Checkpoint storage is a solved problem.**

Modern frameworks checkpoint transparently, creating the illusion that checkpointing "just works." This masks the coordination overhead that dominates checkpoint time at scale.

With 1000 GPUs, each writing 500MB, the aggregate checkpoint is 500GB. But achieving this requires:

1. All ranks reaching checkpoint barrier synchronously
2. Coordinated access to storage (avoiding hotspots)
3. Verification that all shards completed successfully
4. Atomic update of "latest checkpoint" pointer

The actual wall-clock time often exceeds the theoretical write time by 2-3x due to stragglers, coordination, and verification. Organizations that benchmark single-node checkpointing and extrapolate are consistently surprised by distributed checkpoint overhead.

**Pitfall: Assuming training data locality exists.**

Traditional storage optimization assumes data locality: frequently accessed data should be near compute. For training data, this assumption fails fundamentally. Each training sample is accessed once per epoch, and randomized shuffling ensures no sample is accessed more frequently than others.

Caching strategies designed for hot data provide zero benefit: the working set equals the dataset. Prefetching strategies that predict future access based on past access fail: shuffling makes access unpredictable. Storage systems designed for "hot tier" and "cold tier" find all training data equally tepid.

Effective training storage optimizes for sequential bandwidth and prefetch depth rather than cache hit rates.

**Fallacy: Feature store latency does not matter because serving latency is dominated by model inference.**

For LLMs, this is true: model inference of 50-500ms dwarfs any feature lookup. For recommendation systems, it is catastrophically false.

A recommendation model inference might take 2ms. With 100 features requiring 50th-percentile lookup of 1ms each, feature retrieval would dominate latency. Production feature stores must achieve sub-millisecond P50 and single-digit-millisecond P99 lookups. Organizations that treat feature stores as "just another cache" discover too late that they are on the critical path for serving latency.

**Pitfall: Underestimating point-in-time correctness requirements.**

Training a recommendation model on features that include the label (e.g., using "user clicked on item X" as a feature when predicting "will user click on item X?") creates models that work perfectly during training and fail catastrophically in production.

Point-in-time correctness requires that features used during training reflect only information available before the prediction event. This constraint is easy to state and deceptively difficult to enforce. Feature pipelines that aggregate over time windows, join multiple data sources, or depend on asynchronous updates can all violate point-in-time correctness in subtle ways.

The failure mode is insidious: models train well, validate well, and then underperform in production without clear explanation. Engineering teams often chase phantom bugs for weeks before identifying temporal leakage.

## Summary {#sec-storage-summary}

Storage systems form the foundation upon which large-scale ML training and serving are built. This chapter developed the principles and architectures that enable storage systems to meet the distinctive requirements of machine learning workloads.

We began by examining how ML workloads systematically invert traditional storage assumptions: sequential streaming replaces random access, working sets exceed cache capacity, write patterns are bursty rather than continuous, and read/write ratios vary dramatically by phase. The data pipeline throughput equation ($B_{required} = N_{GPUs} \times U_{target} \times S_{batch}/T_{iteration}$) provides quantitative guidance for storage capacity planning.

Training data infrastructure comprises distributed file systems (GFS/Colossus, HDFS, Lustre) and object storage (S3, GCS), each optimized for different access patterns and scales. Data format selection (TFRecord, Parquet, WebDataset) significantly impacts pipeline throughput, with different formats suited to different model types. Data loading pipelines must prefetch and buffer data to hide storage latency and keep accelerators fully utilized.

Checkpoint storage enables fault tolerance for long-running training jobs. The Young-Daly formula ($T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$) provides the optimal checkpoint interval balancing overhead against recovery time. Distributed and incremental checkpointing techniques address the challenges of TB-scale model state.

Feature stores bridge training and serving for models requiring runtime features. They are critical infrastructure for recommendation systems (trillions of lookups daily) while playing minimal roles for LLMs (prompt contains all information). Point-in-time correctness prevents training-serving skew that causes models to fail silently in production.

Model registries provide versioning, lineage tracking, and governance for model artifacts. They enable reproducibility by connecting trained models to the data, code, and environment that produced them.

::: {.callout-important title="Key Takeaways"}
* Storage bandwidth, not capacity, typically limits ML training throughput: systems must be designed to saturate accelerator memory bandwidth with training data, requiring careful attention to prefetching, data format optimization, and parallel I/O
* Storage requirements differ dramatically by model type: LLMs need massive text corpora and terabyte-scale checkpoints, recommendation systems require real-time feature stores with sub-millisecond latency, and vision models demand efficient image pipeline formats
* Feature stores are critical infrastructure for recommendation systems where feature lookup latency directly impacts serving time, but less relevant for LLMs where training data pipelines dominate
* Checkpoint storage strategy must balance recovery granularity against overhead: frequent checkpoints minimize lost work but consume I/O bandwidth, requiring tiered approaches with local NVMe for speed and distributed storage for durability
:::

The storage architectures developed in this chapter enable the distributed training systems examined in @sec-distributed-training. Understanding checkpoint storage is prerequisite to understanding how distributed training maintains progress across failures. The Young-Daly checkpoint interval formula established here will inform the fault tolerance strategies developed in @sec-fault-tolerance, where we shift focus from storage optimization to recovery reliability. Training data pipelines must integrate with parallelism strategies: data parallel training requires different data sharding than model parallel training. Feature stores connect to the inference systems covered in @sec-inference-at-scale, where serving latency budgets constrain feature lookup time. Model registries interface with deployment pipelines and the operational concerns addressed in @sec-ops-scale.

```{=latex}
\part{key:vol2_distributed_training}
```
