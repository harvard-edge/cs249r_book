---
title: "Storage Systems for ML"
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR STORAGE SYSTEMS
================================================================================

CORE PRINCIPLE: Storage requirements differ dramatically by ML workload type.
Feature stores are critical for RecSys but less relevant for LLMs.
Checkpoint strategies vary by model architecture.

MODEL-SPECIFIC STORAGE CHARACTERISTICS:

| Model Type      | Training Data     | Checkpoint Size | Feature Store Need |
|-----------------|-------------------|-----------------|-------------------|
| LLMs            | Text corpora (TB) | TB per ckpt     | Low               |
| Recommendation  | Logs (PB)         | Embeddings (TB) | Critical          |
| Vision          | Images (TB)       | GB per ckpt     | Low-Medium        |
| Scientific      | Simulation (PB)   | Varies          | Domain-specific   |
| Speech          | Audio (TB)        | GB per ckpt     | Low               |

REQUIRED COVERAGE FOR THIS CHAPTER:

DATA LAKES AND TRAINING DATA:

- Text corpora: Deduplication, quality filtering (LLMs)
- Image datasets: Format optimization, augmentation on read (vision)
- User logs: Privacy, retention policies, sampling (recommendation)
- Include: Different preprocessing pipelines for different modalities

FEATURE STORES:

- Critical for recommendation: Real-time feature lookup, versioning
- Less relevant for LLMs: Training data != runtime features
- Include: Why RecSys engineers care deeply about feature stores

CHECKPOINT STORAGE:

- LLMs: TB-scale, infrequent, distributed across storage nodes
- Vision: GB-scale, more frequent, simpler management
- Recommendation: Embedding tables dominate, incremental updates
- Include: Different checkpoint strategies for different model types

MODEL REGISTRIES:

- Version control for model artifacts
- Metadata management across model types
- Include: How registry needs differ (single model vs ensemble)

DATA ACCESS PATTERNS:

- Sequential scan: Training data loading
- Random access: Feature lookup, embedding retrieval
- Include: Different I/O patterns for different workloads

CASE STUDIES TO INCLUDE:

- Meta feature store for recommendation
- Google data infrastructure for LLM training
- Tesla data pipeline for vision models
- Spotify ML data platform (hybrid recommendation/audio)

QUANTITATIVE ANALYSIS:

- I/O bandwidth requirements by workload type
- Storage cost breakdown (hot/warm/cold tiers)
- Latency requirements for different access patterns

ANTI-PATTERNS TO AVOID:

- Assuming all ML needs feature stores equally
- Ignoring embedding table storage challenges
- Treating checkpoint storage as model-agnostic
- Only discussing LLM training data pipelines

================================================================================
-->

# Storage Systems for ML {#sec-storage}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: A layered visualization of ML storage architecture showing data flowing from raw sources to model consumption. The scene depicts a hierarchical storage system: at the base, vast data lakes represented as expansive pools of structured and unstructured data; in the middle layer, feature stores shown as organized crystalline structures with versioned features; at the top, model registries depicted as curated libraries of trained artifacts. Data pipelines flow upward through ETL processes visualized as transformation gates. Visual elements include petabyte-scale metrics, I/O throughput gauges showing streaming rates, and version control branches for datasets and models. Distributed storage nodes span across the background connected by replication streams. The color palette uses deep ocean blues for data lakes, amber for processed features, and silver for model artifacts. Clean architectural diagram style suitable for a data systems textbook._
:::

\noindent
![](images/png/cover_storage.png)

:::

## Purpose {.unnumbered}

_How do storage system architectures shape what machine learning systems can accomplish at production scale?_

Machine learning workloads create distinctive storage demands. Training requires streaming petabytes of data through accelerators at rates that saturate the fastest interconnects, while inference demands millisecond latency access to model weights and feature data across globally distributed serving infrastructure. Storage systems adequate for traditional applications become bottlenecks when confronted with ML access patterns: massive sequential reads during training, random access during feature lookup, and the need to version datasets, models, and artifacts across experimental workflows. The gap between storage capabilities and ML requirements determines training throughput, inference latency, and the feasibility of rapid iteration on model development. Understanding how distributed storage architectures, data lakes, and feature stores address these challenges enables engineers to design systems where storage supports rather than constrains machine learning progress.

## Coming 2026

This chapter will cover distributed storage, data lakes, and feature stores at scale.
