---
title: "Storage Systems for ML"
---

# Storage Systems for ML {#sec-storage}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: A layered visualization of ML storage architecture showing data flowing from raw sources to model consumption. The scene depicts a hierarchical storage system: at the base, vast data lakes represented as expansive pools of structured and unstructured data; in the middle layer, feature stores shown as organized crystalline structures with versioned features; at the top, model registries depicted as curated libraries of trained artifacts. Data pipelines flow upward through ETL processes visualized as transformation gates. Visual elements include petabyte-scale metrics, I/O throughput gauges showing streaming rates, and version control branches for datasets and models. Distributed storage nodes span across the background connected by replication streams. The color palette uses deep ocean blues for data lakes, amber for processed features, and silver for model artifacts. Clean architectural diagram style suitable for a data systems textbook._
:::

\noindent
![](images/png/cover_storage.png)

:::

## Purpose {.unnumbered}

_How do storage system architectures shape what machine learning systems can accomplish at production scale?_

Machine learning workloads create distinctive storage demands. Training requires streaming petabytes of data through accelerators at rates that saturate the fastest interconnects, while inference demands millisecond latency access to model weights and feature data across globally distributed serving infrastructure. Storage systems adequate for traditional applications become bottlenecks when confronted with ML access patterns: massive sequential reads during training, random access during feature lookup, and the need to version datasets, models, and artifacts across experimental workflows. The gap between storage capabilities and ML requirements determines training throughput, inference latency, and the feasibility of rapid iteration on model development. Understanding how distributed storage architectures, data lakes, and feature stores address these challenges enables engineers to design systems where storage supports rather than constrains machine learning progress.

## Coming 2026

This chapter will cover distributed storage, data lakes, and feature stores at scale.
