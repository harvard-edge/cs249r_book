---
title: "Storage Systems for ML"
---

# Storage Systems for ML {#sec-storage}

## Purpose {.unnumbered}

_Why does the architecture of storage systems fundamentally constrain what machine learning systems can accomplish at production scale?_

Machine learning workloads place demands on storage systems unlike any other computing domain: training requires streaming petabytes of data through accelerators at rates that saturate the fastest interconnects, while inference demands millisecond-latency access to model weights and feature data across globally distributed serving infrastructure. Storage systems that perform adequately for traditional applications become crippling bottlenecks when confronted with ML access patterns characterized by massive sequential reads during training, random access during feature lookup, and the need to version datasets, models, and artifacts across complex experimental workflows. The gap between storage system capabilities and ML requirements determines training throughput, inference latency, and the feasibility of iterating rapidly on model development. Understanding how distributed storage architectures, data lakes, and feature stores address these challenges enables engineers to design systems where storage accelerates rather than impedes machine learning progress.

## Coming 2026

This chapter will cover distributed storage, data lakes, and feature stores at scale.
