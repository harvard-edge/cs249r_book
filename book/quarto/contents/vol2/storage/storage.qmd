---
engine: jupyter
---

```{python}
#| echo: false
#| label: chapter-start
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER START
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter initialization and global imports
# │
# │ Why: Registers this chapter with the mlsys registry and provides shared
# │      imports for all subsequent calculation cells.
# │
# │ Imports: mlsys.registry, mlsys.constants, mlsys.formatting
# │ Exports: (none)
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.registry import start_chapter
from mlsys.constants import (
    GB, TB, PB, Gbps, byte, second,
    BILLION, TRILLION, SEC_PER_HOUR, SEC_PER_DAY, BITS_PER_BYTE, KIB_TO_BYTES
)
from mlsys.formatting import fmt, sci, check

start_chapter("vol2:storage")
```

# Storage {#sec-storage}

::: {layout-narrow}
::: {.column-margin}
_Gemini Pro 3 Prompt: A layered visualization of ML storage architecture showing data flowing from raw sources to model consumption. The scene depicts a hierarchical storage system: at the base, vast data lakes represented as expansive pools of structured and unstructured data; in the middle layer, feature stores shown as organized crystalline structures with versioned features; at the top, model registries depicted as curated libraries of trained artifacts. Data pipelines flow upward through ETL processes visualized as transformation gates. Visual elements include petabyte-scale metrics, I/O throughput gauges showing streaming rates, and version control branches for datasets and models. Distributed storage nodes span across the background connected by replication streams. The color palette uses deep ocean blues for data lakes, amber for processed features, and silver for model artifacts. Clean architectural diagram style suitable for a data systems textbook. Rendered in the style of Nanobanana._
:::

\noindent
![](images/png/cover_storage.png)

:::

## Purpose {.unnumbered}

_Why does storage become the invisible bottleneck that prevents accelerators from reaching their potential?_

Accelerators can compute faster than storage can feed them. A modern GPU processes data at terabytes per second internally, but even the fastest NVMe drives deliver single-digit gigabytes per second, and distributed storage systems add latency that compounds into idle accelerators waiting for data to arrive. This mismatch is invisible in benchmarks that measure accelerator performance in isolation but dominates real workloads where training data must stream continuously, checkpoints must be saved reliably, and features must be retrieved with millisecond latency. The gap between what accelerators can consume and what storage can deliver shapes system architecture at every level: it forces careful attention to data formats, caching strategies, and pipeline design that would be unnecessary if storage kept pace with compute. Organizations that optimize accelerator utilization without addressing storage discover that their expensive hardware runs at a fraction of capacity because nobody planned for the data path.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Analyze how ML workloads invert traditional storage assumptions and apply the data pipeline throughput equation to calculate required bandwidth for different training scenarios.
- Compare distributed file system architectures and object storage systems for ML training workloads based on throughput, latency, and consistency requirements.
- Apply the Young-Daly formula to determine optimal checkpoint intervals for different cluster sizes and model types.
- Evaluate feature store architectures for online and offline serving, explaining point-in-time correctness requirements and latency constraints.
- Design data format strategies using TFRecord, Parquet, or WebDataset based on model type and access patterns.
- Analyze model registry architectures for versioning, lineage tracking, and reproducibility across experimental workflows.

:::

```{python}
#| label: storage-setup
#| echo: false

from mlsys.constants import *
from mlsys.formatting import fmt, sci

# GPU specs
a100_mem = f"{A100_MEM_CAPACITY.to(GiB).magnitude:.0f}"
h100_mem = f"{H100_MEM_CAPACITY.to(GiB).magnitude:.0f}"
h100_bw_tbs = f"{H100_MEM_BW.to(TB/second).magnitude:.2f}"
h100_fp8_tflops = f"{H100_FLOPS_FP8_TENSOR.to(TFLOPs/second).magnitude:,.0f}"
h100_fp16_tflops = f"{H100_FLOPS_FP16_TENSOR.to(TFLOPs/second).magnitude:,.0f}"

# Model specs
gpt3_params_b = f"{GPT3_PARAMS.to(Mparam).magnitude/1000:.0f}"
resnet_params_m = f"{RESNET50_PARAMS.to(Mparam).magnitude:.1f}"

# Storage
nvme_bw = f"{NVME_SEQUENTIAL_BW.to(GB/second).magnitude:.1f}"
```

## Storage Fundamentals for ML {#sec-storage-storage-fundamentals-ml-48a9}

In the **Systems Sandwich** (@sec-vol2-introduction), Storage is the **Fuel Line** of the Machine Learning Fleet. While compute nodes (@sec-compute) provide the engine and networks (@sec-communication-collective-operations) provide the transmission, storage provides the fuel (data). If the **Iron Law** states that $L = (\text{Data} + \text{Compute}) / \text{Bandwidth}$, this chapter is about ensuring that the storage bandwidth term ($\text{Bandwidth}_{disk}$) never becomes the bottleneck.

The infrastructure foundations established in @sec-compute provide the compute fabric...

Traditional enterprise storage systems evolved to serve transactional databases and file servers, workloads characterized by small random accesses, strong consistency requirements, and moderate bandwidth demands. A database server might issue thousands of 4KB reads per second to serve user queries, each read potentially touching different storage locations. The storage industry optimized for this pattern, developing sophisticated caching algorithms, RAID[^fn-raid] configurations, and file systems tuned for small-block random access with transactional guarantees.

[^fn-raid]: **RAID (Redundant Array of Independent Disks)**: A storage virtualization technology that combines multiple physical drives into a single logical unit for performance and redundancy. RAID 0 stripes data across drives for speed but no redundancy; RAID 1 mirrors data for redundancy; RAID 5/6 uses parity for fault tolerance with better capacity utilization. ML training workloads typically use RAID 0 or JBOD (Just a Bunch of Disks) for maximum bandwidth, accepting the risk of data loss since training data is immutable and backed up elsewhere.

ML workloads invert nearly every assumption that shaped these systems. Training data access is predominantly sequential, streaming through datasets that may span hundreds of terabytes. Individual accesses are large, often megabytes rather than kilobytes, as models consume batches of images, text sequences, or feature vectors. Consistency requirements are relaxed; slightly stale feature values rarely affect model quality, and training can tolerate occasional data corruption through its inherent noise tolerance. But bandwidth demands are extreme, frequently requiring sustained throughput that would overwhelm systems designed for transactional workloads.

This mismatch between traditional storage design and ML requirements creates challenges that surface at every level of the storage hierarchy. Object stores designed for web-scale applications deliver excellent scalability but introduce latencies that starve GPU pipelines. Parallel file systems engineered for scientific computing provide the bandwidth but struggle with the metadata operations that ML checkpointing generates. Local NVMe[^fn-nvme] drives offer the latency characteristics that inference demands but lack the capacity for training datasets. Effective ML storage architecture requires understanding these tradeoffs and composing storage tiers that match each phase of the ML lifecycle.

[^fn-nvme]: **NVMe (Non-Volatile Memory Express)**: A storage protocol designed specifically for solid-state drives, bypassing the legacy AHCI interface created for spinning disks. NVMe achieves 3-7 GB/s sequential throughput and sub-10 microsecond latency by using multiple command queues (65,535 queues with 65,536 commands each) rather than SATA's single queue of 32 commands. For ML inference, NVMe's low latency enables rapid model loading during cold starts; for training, its high bandwidth supports local caching of datasets fetched from distributed storage.

### The ML Storage Hierarchy {#sec-storage-ml-storage-hierarchy-0942}

Computer architecture courses teach the memory hierarchy as a fundamental organizing abstraction: registers at nanosecond latencies, caches at microseconds, DRAM at hundreds of nanoseconds, and storage devices at milliseconds. This hierarchy exists because faster memory is more expensive per bit, so systems use smaller amounts of fast memory as caches for larger amounts of slower memory. The principle of locality, both temporal (recently accessed data will likely be accessed again) and spatial (nearby data will likely be accessed soon), makes caching effective for most workloads.

ML systems extend this hierarchy with two critical additions: GPU High Bandwidth Memory (HBM)[^fn-hbm] and distributed storage spanning multiple tiers. @tbl-storage-hierarchy reveals the extreme bandwidth disparities that ML systems must navigate.

[^fn-hbm]: **High Bandwidth Memory (HBM)**: As established in @sec-compute, HBM is a 3D-stacked memory technology that places DRAM dies vertically atop the processor. In the storage hierarchy, HBM serves as the 'Level 0' tier where weights and activations must reside for active computation. While delivering TB/s of bandwidth, its high cost ($15/GB) limits capacity, necessitating the multi-tier storage architecture examined in this chapter.

| **Storage Tier**         | **Typical Capacity** |               **Bandwidth** |      **Latency** | **Cost ($/GB)** |
|:-----------------------|:-------------------|--------------------------:|---------------:|--------------:|
| **GPU HBM**              | 80 GB                | `{python} h100_bw_tbs` TB/s |           ~10 ns |          ~15.00 |
| **Host DRAM**            | 512 GB - 2 TB        |                    200 GB/s |          ~100 ns |           ~3.00 |
| **Local NVMe SSD**       | 4-30 TB              |                   7-25 GB/s |           ~10 μs |           ~0.10 |
| **Parallel File System** | 100+ PB              |           1+ TB/s aggregate |            ~1 ms |           ~0.03 |
| **Object Storage**       | Unlimited            |          100 GB/s aggregate |           ~50 ms |           ~0.02 |
| **Archive/Cold Storage** | Unlimited            |                      1 GB/s | Minutes to hours |          ~0.004 |

: **Extended Memory Hierarchy for ML Systems**: GPU HBM delivers 3.35 TB/s at ~10 ns latency but costs $15/GB, while object storage offers unlimited capacity at $0.02/GB with 50 ms latency. This 300,000× bandwidth gap between HBM and storage drives the need for sophisticated prefetching and caching strategies. {#tbl-storage-hierarchy}

Looking at the bandwidth column, GPU HBM delivers `{python} h100_bw_tbs` TB/s, roughly 17x faster than host DRAM and 130x faster than the fastest local NVMe drives. This disparity creates the central challenge of ML storage systems: keeping accelerators fed with data at rates that prevent them from idling.

### High Bandwidth Flash (HBF): Blurring Storage and Memory {#sec-storage-high-bandwidth-flash-hbf-blurring-storage-memory-9bea}

Recent research [@ma2024challenges] challenges the rigid separation between storage and memory for inference workloads. As models grow to trillions of parameters, storing all weights in HBM becomes prohibitively expensive ($100/GB vs $0.10/GB for Flash). **High Bandwidth Flash (HBF)** proposes using specialized flash controllers to achieve 50-100 GB/s read throughput directly into accelerator memory.

This architecture treats Flash not as persistent storage, but as a "cold memory" tier. @fig-storage-hierarchy visualizes this expanded hierarchy where Flash occupies a middle ground between traditional DRAM and distributed storage. For autoregressive generation where the *entire* model is read once per token but arithmetic intensity is low, HBF allows serving massive models from a single node, replacing racks of GPUs with a single flash-augmented server. While latency is higher than HBM, cost-per-token drops by orders of magnitude for batch-insensitive workloads.

::: {#fig-storage-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**ML Storage Hierarchy**. A pyramid visualization extending the classic memory hierarchy. The peak shows GPU HBM (3+ TB/s, small capacity), descending through Host DRAM, Local NVMe, and Distributed File Systems, to the base of Object Storage (100+ GB/s aggregate, unlimited capacity). The bandwidth gap between HBM and storage tiers drives the need for sophisticated caching and prefetching." fig-alt="Five-tier pyramid: GPU HBM, Host DRAM, Local NVMe, Parallel File System, Object Storage from top to base. Left axis shows bandwidth 3.3 TB/s to 100 GB/s. Right axis shows capacity 80 GB to exabytes."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{L1Color}{RGB}{255,100,100}
  \definecolor{L2Color}{RGB}{255,180,100}
  \definecolor{L3Color}{RGB}{255,255,150}
  \definecolor{L4Color}{RGB}{150,255,150}
  \definecolor{L5Color}{RGB}{150,200,255}

  % Pyramid Layers
  \coordinate (T) at (0, 5);
  \coordinate (BL) at (-4, 0);
  \coordinate (BR) at (4, 0);

  % HBM
  \filldraw[fill=L1Color, draw=black] (0,5) -- (-0.8,4) -- (0.8,4) -- cycle;
  \node at (0, 4.3) {\textbf{GPU HBM}};

  % DRAM
  \filldraw[fill=L2Color, draw=black] (-0.8,4) -- (-1.6,3) -- (1.6,3) -- (0.8,4) -- cycle;
  \node at (0, 3.5) {Host DRAM};

  % NVMe
  \filldraw[fill=L3Color, draw=black] (-1.6,3) -- (-2.4,2) -- (2.4,2) -- (1.6,3) -- cycle;
  \node at (0, 2.5) {Local NVMe SSD};

  % DFS
  \filldraw[fill=L4Color, draw=black] (-2.4,2) -- (-3.2,1) -- (3.2,1) -- (2.4,2) -- cycle;
  \node at (0, 1.5) {Parallel File System (Lustre/GPFS)};

  % Object
  \filldraw[fill=L5Color, draw=black] (-3.2,1) -- (-4,0) -- (4,0) -- (3.2,1) -- cycle;
  \node at (0, 0.5) {Object Storage (S3/GCS)};

  % Annotations Left (Bandwidth)
  \draw[->, ultra thick] (-4.5, 0.5) -- (-4.5, 4.5) node[midway, left, rotate=90] {Bandwidth (Log Scale)};
  \node[left, font=\footnotesize] at (-4.5, 4.5) {3.3 TB/s};
  \node[left, font=\footnotesize] at (-4.5, 0.5) {100 GB/s};

  % Annotations Right (Capacity)
  \draw[->, ultra thick] (4.5, 4.5) -- (4.5, 0.5) node[midway, right, rotate=90] {Capacity (Log Scale)};
  \node[right, font=\footnotesize] at (4.5, 4.5) {80 GB};
  \node[right, font=\footnotesize] at (4.5, 0.5) {Exabytes};

\end{tikzpicture}
```
:::

These bandwidth specifications become concrete when we trace data flow through a single training iteration. Consider what happens when an H100 GPU processes a training batch. At `{python} h100_fp8_tflops` TFLOPS[^fn-tflops] of FP16 compute capability, the GPU can perform approximately 2 quadrillion floating-point operations per second. A typical transformer forward pass requires roughly 6 FLOPs per parameter per token. For a 7 billion parameter model processing 2048-token sequences with a batch size of 32, each forward pass involves:

[^fn-tflops]: **TFLOPS (Tera Floating-Point Operations Per Second)**: A measure of computational throughput representing one trillion ($10^{12}$) floating-point operations per second. The H100's `{python} h100_fp8_tflops` TFLOPS uses FP16 (16-bit floating-point) precision with sparsity optimization; dense FP16 achieves `{python} h100_fp16_tflops` TFLOPS, and FP32 (32-bit) achieves 67 TFLOPS. This 15x difference between FP16 and FP32 explains why ML training increasingly uses mixed-precision techniques, performing bulk computation in FP16 while maintaining FP32 precision for numerically sensitive operations like loss accumulation.

$$\text{FLOPs per batch} = 6 \times 7 \times 10^9 \times 2048 \times 32 \approx 2.75 \times 10^{15}$$

At `{python} h100_fp8_tflops` TFLOPS, this computation completes in approximately 1.4 seconds, during which the next batch must be ready in GPU memory. If data arrives even slightly slower than the GPU consumes it, expensive accelerator time is wasted waiting for storage.

### How ML Workloads Invert Traditional Assumptions {#sec-storage-ml-workloads-invert-traditional-assumptions-fef8}

Traditional storage system design optimizes for workloads with specific characteristics: random access patterns, working sets that fit in cache, and write-heavy transactional loads. ML workloads systematically violate each of these assumptions, requiring fundamentally different storage architectures.

**Sequential streaming dominates.** Database workloads exhibit random access patterns as queries retrieve specific records from large tables. ML training, by contrast, performs massive sequential scans through datasets. A training epoch reads every sample once, in whatever order the shuffling algorithm produces, before repeating. This access pattern resembles video streaming more than database queries. Storage systems optimized for random IOPS[^fn-iops] (input/output operations per second) waste their capabilities on ML workloads, while systems optimized for sequential throughput excel.

[^fn-iops]: **IOPS (Input/Output Operations Per Second)**: A storage performance metric counting discrete read or write operations regardless of size. Enterprise SSDs achieve 100,000-1,000,000 IOPS for small (4KB) random accesses but only 10,000-50,000 IOPS for large (1MB) sequential accesses. The distinction matters for ML: training data pipelines care about throughput (GB/s), not IOPS, because they read large batches sequentially. Feature stores serving inference require high IOPS for millions of small random lookups. Storage vendors often advertise IOPS prominently, but ML engineers should evaluate throughput for training and latency for inference.

**Working sets exceed any cache level.** Traditional applications exhibit locality: a web server repeatedly accesses the same popular pages, a database repeatedly queries hot rows. Caching exploits this locality, keeping frequently accessed data in fast memory. ML training datasets are accessed uniformly: each sample is read once per epoch, with no sample more likely to be accessed than any other during training. A 10 TB image dataset cannot be cached in DRAM; each sample is effectively cold when accessed. This lack of locality renders traditional caching strategies ineffective for training data.

**Write patterns are bursty rather than continuous.** Transactional systems generate continuous streams of small writes as users update records. ML systems generate occasional massive writes when saving checkpoints. A `{python} gpt3_params_b` billion parameter model checkpoint [@brown2020gpt3] occupies approximately 700 GB; saving it every 10 minutes generates 70 GB/minute average throughput but concentrated into bursts that saturate storage bandwidth for 1-2 minutes followed by idle periods. This bursty pattern requires storage systems that can absorb high-bandwidth writes without blocking ongoing reads.

**Read/write ratios vary dramatically by phase.** Training reads vastly exceed writes: a typical training run reads the dataset dozens of times (one per epoch) while writing only periodic checkpoints. The read-to-write ratio can exceed 100:1. Inference, conversely, is almost entirely read-only, loading model weights once and then serving requests without writes. Feature stores for recommendation systems present yet another pattern: continuous reads for serving interleaved with batch writes from offline feature computation. No single storage configuration optimizes all three patterns. @tbl-storage-assumptions captures these fundamental mismatches, revealing why enterprise storage systems designed for transactional workloads underperform for ML.

| **Workload Pattern** | **Traditional Assumption** | **ML Reality**                 |
|:-------------------|:-------------------------|:-----------------------------|
| **Access pattern**   | Random access              | Sequential streaming           |
| **Working set**      | Fits in cache              | Exceeds all cache levels       |
| **Write pattern**    | Continuous small writes    | Bursty large writes            |
| **Read/write ratio** | Balanced                   | Phase-dependent (100:1 to 1:0) |
| **Locality**         | Strong temporal locality   | No locality (uniform sampling) |

: **ML Workloads Invert Traditional Storage Assumptions**: Where databases optimize for random IOPS with cacheable working sets, ML training streams sequentially through datasets that exceed all cache levels. This fundamental mismatch explains why enterprise storage systems underperform for ML by 10x or more. {#tbl-storage-assumptions}

### Access Pattern Analysis {#sec-storage-access-pattern-analysis-3683}

Understanding access patterns quantitatively enables storage system selection and capacity planning. ML workloads exhibit distinct patterns across different phases and model types. @fig-access-patterns compares throughput for sequential versus random access, showing how storage performance degrades dramatically when workloads deviate from sequential streaming. @tbl-access-patterns breaks down these patterns by model architecture.

| **Model Type** | **Training Data Pattern**      | **Checkpoint Pattern**        | **Feature Pattern**       | **Serving Pattern**      |
|:-------------|:-----------------------------|:----------------------------|:------------------------|:-----------------------|
| **LLM**        | Sequential streaming, TB-scale | Infrequent, 100GB-1TB bursts  | Minimal                   | Load once, cache in GPU  |
| **RecSys**     | Log streaming, continuous      | Incremental embedding updates | Continuous random lookups | Hot embeddings in memory |
| **Vision**     | Sequential with augmentation   | Regular, 1-10GB               | Minimal                   | Load once per model      |
| **Scientific** | Irregular, domain-specific     | Regular or continuous         | Domain-specific           | Varies by application    |

: **Access Patterns by Model Type and Phase**: LLMs stream TB-scale data with infrequent 100GB-1TB checkpoint bursts; RecSys requires continuous random embedding lookups from multi-TB tables. These divergent patterns require fundamentally different storage architectures. {#tbl-access-patterns}

::: {#fig-access-patterns fig-env="figure" fig-pos="htb" fig-cap="**Access Pattern Throughput Comparison**. High-performance storage delivers dramatically different throughput depending on access patterns. Sequential reads (blue) saturate bandwidth quickly as request size increases. Random reads (red), typical of databases or poorly shuffled datasets, suffer from seek latency and protocol overhead, delivering a fraction of the hardware's potential performance." fig-alt="Line graph comparing sequential vs random read throughput across I/O request sizes from 4KB to 1MB. Sequential reads reach 10.8 GB/s, while random reads plateau at 4.5 GB/s. Arrow marks the performance gap as the I/O wall."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \begin{axis}[
    width=10cm, height=6cm,
    xlabel={I/O Request Size},
    ylabel={Throughput (GB/s)},
    xmode=log,
    log basis x=2,
    xtick={4, 16, 64, 256, 1024},
    xticklabels={4KB, 16KB, 64KB, 256KB, 1MB},
    ymin=0, ymax=12,
    grid=major,
    legend pos=north west,
    legend style={draw=none, fill=none}
  ]
    % Sequential (High throughput)
    \addplot[color=blue, ultra thick, mark=square] coordinates {
      (4, 2)
      (16, 6)
      (64, 10)
      (256, 10.5)
      (1024, 10.8)
    };
    \addlegendentry{Sequential Read (ML Training)}

    % Random (Low throughput)
    \addplot[color=red, ultra thick, mark=triangle] coordinates {
      (4, 0.2)
      (16, 0.5)
      (64, 1.2)
      (256, 2.8)
      (1024, 4.5)
    };
    \addlegendentry{Random Read (Database)}

    \node[anchor=west] at (axis cs: 64, 6) {The "I/O Wall"};
    \draw[->, thick] (axis cs: 64, 5.8) -- (axis cs: 64, 1.5);
  \end{axis}

  \node[align=center, font=\footnotesize, anchor=north] at (5, -1.2) {Larger block sizes amortize overhead, favored by ML sequential scans.};
\end{tikzpicture}
```
:::

**Training data access** follows a streaming sequential pattern with random shuffling. Each epoch reads the entire dataset once, but the order is randomized to prevent the model from learning ordering artifacts. This creates a pattern that is globally sequential (every sample accessed once) but locally random (no predictable next sample). Storage systems must support high sequential bandwidth while handling the pseudo-random access order that shuffling creates.

For distributed training, access patterns multiply: $N$ workers each read $1/N$ of the dataset per step, but the global shuffle means no locality between workers. If workers access a shared storage system, the aggregate access pattern appears random even though each worker performs sequential reads of its partition.

**Checkpoint access** exhibits extreme write bursts followed by long read intervals. During normal training, checkpoints represent nearly all write traffic. The access pattern is:

- Write: Save complete model state every $T_{checkpoint}$ minutes
- Read: Load most recent checkpoint on training restart
- Delete: Remove old checkpoints after new ones are verified

The checkpoint write burst must complete before the next training step can safely proceed (for synchronous checkpointing) or within a bounded delay (for asynchronous approaches). Checkpoint reads are infrequent but critical: when failures occur, recovery time depends on checkpoint load bandwidth.

**Feature store access** patterns differ fundamentally from training data. Online serving requires point lookups: given a user ID, retrieve that user's features. This is the random access pattern that traditional storage optimizes for, with latency requirements in single-digit milliseconds. Offline feature computation generates batch writes as feature pipelines process new data. The pattern resembles:

- Online reads: Millions of random point lookups per second, < 10ms latency requirement
- Offline writes: Batch updates every minutes to hours, throughput-optimized

**Model serving access** loads model weights at startup, then serves inference requests using weights cached in GPU memory. The access pattern is write-once-read-many with extreme read amplification: a 7 billion parameter model loaded once serves millions of inference requests. Storage bandwidth matters only during model loading; once loaded, storage is unused until model updates or server restarts.

### Data Pipeline Throughput Requirements {#sec-storage-data-pipeline-throughput-requirements-50aa}

The central quantitative question for ML storage is: what bandwidth does training require? The answer depends on cluster size, accelerator utilization targets, sample sizes, and iteration speed. We capture these dependencies in a single *data pipeline throughput equation*.

::: {.callout-definition title="Data Pipeline Throughput Equation"}

The required storage bandwidth to sustain distributed training is:

$$B_{required} = N_{GPUs} \times U_{target} \times \frac{S_{batch}}{T_{iteration}}$$

where $N_{GPUs}$ is the number of accelerators, $U_{target}$ is the target utilization (typically 0.8-0.95), $S_{batch}$ is the batch size in bytes per GPU, and $T_{iteration}$ is the iteration time in seconds.

:::

This equation reveals that storage bandwidth requirements grow linearly with cluster size. Doubling the number of GPUs doubles the required storage bandwidth, assuming iteration time and batch size remain constant.

::: {.callout-example title="ImageNet Training Bandwidth Requirements"}

Consider training a ResNet-50 [@he2016resnet] model on ImageNet [@deng2009imagenet] using 256 H100 GPUs. The calculation proceeds as follows:

**Given values:**

- $N_{GPUs} = 256$ H100 GPUs
- $U_{target} = 0.80$ (80% utilization)
- Images are 224×224 RGB after resize, but stored as JPEG at ~150 KB average
- Batch size per GPU: 256 images
- Target iteration time: 200 ms (achievable with optimized training)

**Batch size calculation:**

$$S_{batch} = 256 \text{ images} \times 150 \text{ KB/image} = 38.4 \text{ MB}$$

**Required bandwidth:**

$$B_{required} = 256 \times 0.80 \times \frac{38.4 \text{ MB}}{0.2 \text{ s}} = 39.3 \text{ GB/s}$$

This 39.3 GB/s requirement exceeds single-node NVMe capabilities (typically 7-25 GB/s) and requires either distributed data loading across multiple nodes or a parallel file system delivering aggregate bandwidth at this scale.

:::

The bandwidth requirement varies dramatically by model type due to differences in sample size and iteration time.

::: {.callout-example title="LLM Training Bandwidth (GPT-3)"}

For LLM training, samples are tokenized text sequences rather than images, and batch sizes are constrained by GPU memory for activation storage [@brown2020gpt3].

**GPT-3 scale training scenario:**

- $N_{GPUs} = 1024$ A100 GPUs (128 nodes × 8 GPUs)
- $U_{target} = 0.85$
- Sequence length: 2048 tokens
- Batch size per GPU: 8 sequences (memory limited)
- Token storage: 2 bytes per token (int16)
- Target iteration time: 1.5 seconds

**Batch size calculation:**

$$S_{batch} = 8 \text{ sequences} \times 2048 \text{ tokens} \times 2 \text{ bytes} = 32.8 \text{ KB}$$

**Required bandwidth:**

$$B_{required} = 1024 \times 0.85 \times \frac{32.8 \text{ KB}}{1.5 \text{ s}} = 19.0 \text{ MB/s}$$

This dramatically lower bandwidth requirement (19 MB/s vs. 39 GB/s for ImageNet) explains why LLM training is typically compute-bound rather than I/O-bound. The bottleneck shifts to checkpoint I/O rather than training data streaming.

:::

While LLM training proved compute-bound rather than I/O-bound, recommendation systems tell a different story. Their sparse, random-access patterns create far higher storage demands.

::: {.callout-example title="RecSys Training Bandwidth (DLRM)"}

Recommendation systems present unique challenges with massive embedding tables and sparse feature access [@naumov2019dlrm].

**Large-scale RecSys scenario (similar to Meta DLRM):**

- $N_{GPUs} = 512$ GPUs
- $U_{target} = 0.75$ (lower due to embedding communication)
- Samples: User interaction logs with ~100 features each
- Feature encoding: 8 bytes per feature (int64 IDs)
- Batch size per GPU: 65,536 samples (large batches for sparse models)
- Target iteration time: 100 ms

**Batch size calculation:**

$$S_{batch} = 65536 \text{ samples} \times 100 \text{ features} \times 8 \text{ bytes} = 52.4 \text{ MB}$$

**Required bandwidth:**

$$B_{required} = 512 \times 0.75 \times \frac{52.4 \text{ MB}}{0.1 \text{ s}} = 201.3 \text{ GB/s}$$

This 201 GB/s requirement exceeds even large parallel file systems. Recommendation systems typically address this through data locality: each worker processes a partition of the data stored on local SSDs, eliminating cross-node data transfer. The embedding table accesses, which are random lookups into trillion-parameter tables, become the true storage bottleneck.

:::

### Consistency Models for ML Storage {#sec-storage-consistency-models-ml-storage-5609-models-ml-storage-5609}

The CAP theorem, introduced in @sec-vol2-introduction as the fundamental constraint governing distributed system tradeoffs, directly shapes storage architecture decisions. As established there, during network partitions a distributed system must choose between consistency (all readers see the same data) and availability (requests succeed even during failures). The formal statement[^fn-cap-theorem] [@brewer2000towards; @gilbert2002brewer] is that distributed systems can provide at most two of these three guarantees: consistency, availability, and partition tolerance.

[^fn-cap-theorem]: **CAP Theorem**: Proven by Eric Brewer in 2000 and formalized by Gilbert and Lynch in 2002, the CAP theorem states that during a network partition, a distributed system must choose between consistency (all nodes see the same data) and availability (every request receives a response). Since partitions are inevitable in distributed systems, the practical choice is between CP (consistent but may reject requests during partitions) and AP (available but may return stale data). For ML storage, training data and checkpoints favor CP (correctness over availability), while feature stores often accept AP (fresh-enough features are better than failed requests).

Understanding consistency models enables correct storage system selection for different ML components. Four levels of consistency form a spectrum from strongest to weakest.

::: {.callout-definition title="Consistency Model Definitions"}

**Linearizability (Strong Consistency)**: Every operation appears to take effect instantaneously at some point between its invocation and response. All clients observe operations in the same order, and that order respects real-time ordering. Checkpoint storage requires linearizability because:

1. After checkpoint write completes, any reader must see complete checkpoint
2. No reader should ever see a partial or corrupted state
3. Recovery process must read exactly what was written

Linearizable systems are expensive: they require coordination (consensus protocols like Paxos or Raft) on every write. This is acceptable for checkpoints (written every 10-30 minutes) but unacceptable for training data reads (millions per second).

**Sequential Consistency**: Operations appear in the same order to all clients, but that order may not respect real-time. Sufficient for training data where "everyone sees the same dataset version" is required but "immediately after upload" is not.

**Read-Your-Writes Consistency**: After a client writes data, subsequent reads by that same client will see the new value. Required for online feature stores where user actions update features that affect subsequent recommendations.

**Eventual Consistency**: After a period of no writes, all replicas converge to the same value. Acceptable for training data that is uploaded once and read many times, or for offline feature computation where batch updates propagate over minutes.

The key insight: stronger consistency requires more coordination, reducing throughput and increasing latency. Match consistency level to workload requirements.

:::

For ML systems, the appropriate consistency model depends on the storage tier and access pattern.

**Training data storage** can sacrifice strong consistency for availability. Training samples are immutable once created: the same image or text sequence is read identically by all workers. Eventual consistency suffices because training order does not affect final model quality (samples are shuffled anyway), stale reads of a dataset do not corrupt training (reading an older version of a sample causes no harm), and dataset updates are infrequent (new data is added between training runs, not during).

Object storage systems like S3 and GCS are suitable for training data. Since December 2020, AWS S3 provides strong read-after-write consistency for all operations [@aws2020s3], eliminating previous eventual consistency concerns. GCS has always provided strong consistency. Both systems prioritize availability and can serve training data reliably at scale.

**Checkpoint storage** requires strong consistency for correctness. A checkpoint must be complete and consistent before training can safely continue: a partially written checkpoint that appears complete causes catastrophic failure on recovery. The checkpoint must be either fully written or not visible at all (atomic writes), immediately readable after completion (read-after-write consistency), and must survive storage system failures once acknowledged (durable writes). These requirements favor strongly consistent storage systems or careful application-level protocols that implement atomic writes atop eventually consistent storage.

**Feature store storage** requires consistency guarantees that vary by access pattern. Offline features can tolerate eventual consistency: batch computations produce new feature versions that propagate to serving over minutes to hours. Online features for serving require read-your-writes consistency at minimum: after a user action updates their features, subsequent requests must see those updated features. Strong consistency may be necessary for use cases like fraud detection where stale features could enable fraudulent transactions.

@tbl-consistency-requirements maps each storage tier to its appropriate consistency model: training data tolerates eventual consistency because samples are immutable and shuffled anyway, while checkpoints demand linearizability because partial writes cause catastrophic recovery failures. These tier-specific consistency choices have direct *CAP theorem implications* for ML storage architecture.

| **Storage Tier**     | **Consistency Requirement**       | **Rationale**                                                   | **Suitable Systems**            |
|:-------------------|:--------------------------------|:--------------------------------------------------------------|:------------------------------|
| **Training data**    | Strong consistency (now standard) | Immutable data, modern object stores provide strong consistency | S3, GCS, HDFS                   |
| **Checkpoints**      | Strong consistency                | Partial checkpoints cause catastrophic failure                  | Parallel FS with atomic writes  |
| **Offline features** | Eventual consistency              | Batch updates, staleness acceptable                             | Data warehouses, object storage |
| **Online features**  | Read-your-writes or stronger      | User experience requires fresh features                         | Redis, Bigtable, DynamoDB       |
| **Model weights**    | Read-after-write                  | Model updates must be immediately visible                       | Consistent object storage       |

: **Consistency Requirements by Storage Tier**: Training data tolerates eventual consistency because samples are immutable and shuffled; checkpoints demand strong consistency because partial writes corrupt model state. This consistency spectrum enables cost-effective architecture by avoiding over-specification. {#tbl-consistency-requirements}

::: {.callout-perspective title="CAP Theorem Implications"}

The CAP theorem's implications for ML storage differ from traditional applications. As Stoica et al. observe [@stoica2017berkeley], training storage can sacrifice availability for consistency (a brief storage outage during checkpoint writes is acceptable if it ensures checkpoint correctness), while serving storage might sacrifice consistency for availability (serving stale features is preferable to failing requests entirely). Understanding these tradeoffs enables storage architecture decisions that match the actual requirements of each ML system component.

:::

Consistency guarantees determine what data readers see. Equally important is when they see it. This brings us to latency, and specifically the challenge of tail latency in distributed systems.

### Tail Latency in Distributed Storage {#sec-storage-tail-latency-distributed-storage-ae2f}

At scale, tail latency[^fn-tail-latency] dominates storage system behavior. The median access latency tells an incomplete story; what matters for ML systems is the 99th or 99.9th percentile latency (P99, P999). Dean and Barroso's seminal paper "The Tail at Scale" [@dean2013tail] formalized why: when a system makes parallel requests to many storage nodes, the overall latency is determined by the slowest response.

[^fn-tail-latency]: **Tail Latency**: The latency experienced by the slowest requests, typically measured at the 99th (P99) or 99.9th (P999) percentile. In distributed systems, tail latencies are disproportionately important because a single slow component delays the entire operation. If a training step requires data from 100 storage nodes and each has a 1% chance of being slow, 63% of steps will experience at least one slow access. Google's production systems target P99 latency rather than median, recognizing that users experience the tail while operators often only monitor the median.

**Why tail latency matters for ML:**

Consider a distributed training step that reads data from 100 storage nodes simultaneously. If each node's latency distribution has a 1% chance of being "slow" (say, 100ms instead of the typical 10ms), then:

$$P(\text{at least one slow}) = 1 - (0.99)^{100} = 0.634$$

More than 63% of training steps will experience at least one slow storage access, making the tail latency the effective latency for the system.

**Tail latency sources** include garbage collection pauses (JVM-based storage systems like HDFS NameNode can stall for seconds during GC), disk queue depth (when too many concurrent requests hit one disk, queue wait time dominates), network congestion (shared network fabric experiences transient congestion), background maintenance (compaction, replication, and verification compete with foreground requests), and resource contention (multiple tenants sharing storage create interference).

**Mitigation strategies** include hedged requests (issue redundant reads, take first response; tradeoff: 2x read amplification), backup requests (issue second request if first is slow; tradeoff: lower amplification, higher complexity), selective replica choice (route to least-loaded replica; tradeoff: requires load monitoring), request cancellation (cancel in-flight requests when first completes; reduces wasted work), and deadline propagation (drop requests exceeding deadline; prioritizes freshness).

**Production example:**

Google's storage systems implement hedged requests: after waiting for the P50 latency (e.g., 10ms), issue a backup request to a different replica. Take whichever response arrives first. This reduces P99 latency dramatically at the cost of increased read traffic:

$$\text{P99 with hedging} \approx P50 + P50 \times \epsilon$$

where $\epsilon$ accounts for the time to detect slowness and issue the backup.

For ML training, tail latency tolerance is higher (seconds of delay are acceptable occasionally) than for inference (where P99 latency directly impacts user experience). Checkpoint writes, being infrequent, can tolerate higher tail latency than feature store lookups.

### Storage System Selection Framework {#sec-storage-storage-system-selection-framework-2f13}

Given the diversity of ML storage requirements, practitioners need a systematic framework for selecting appropriate storage systems. The decision depends on access pattern, scale, latency requirements, and cost constraints.

For training data exceeding local storage, systems under 10 TB can use local NVMe RAID or network-attached storage; systems between 10 TB and 1 PB benefit from parallel file systems (Lustre, GPFS) or high-performance object storage; systems exceeding 1 PB typically require object storage (S3, GCS) with intelligent caching. For checkpoint storage, single nodes can use local NVMe with backup to durable storage; multi-node deployments with models under 100 GB need parallel file systems with atomic write support; models exceeding 100 GB require distributed checkpointing across multiple storage targets. For feature stores, offline-only workloads can use data warehouses (Snowflake, BigQuery) or data lakes; online serving with sub-10ms requirements needs in-memory cache (Redis) backed by persistent storage; online serving at scale requires purpose-built feature stores (Feast, Tecton, Vertex Feature Store).

The selection framework above maps requirements to storage tiers. We now examine each tier in depth, following data through the ML lifecycle: first the training data that feeds models, then the checkpoints that preserve training progress, then the features required for serving, and finally the model artifacts themselves.

## Training Data Infrastructure {#sec-storage-training-data-infrastructure-205b}

Training datasets for production ML systems range from terabytes to petabytes, far exceeding single-machine storage capacity. Storing and serving this data requires distributed storage systems designed for high throughput sequential access. This section examines the distributed file systems and object storage architectures that power large-scale ML training, along with the data formats and pipeline architectures that efficiently deliver data to accelerators.

### Distributed File Systems {#sec-storage-distributed-file-systems-15f4}

Distributed file systems provide a POSIX[^fn-posix]-compatible interface to storage spread across hundreds or thousands of machines. This familiar file system interface (open, read, write, close) enables existing software to access distributed storage without modification, a significant advantage for ML frameworks designed around local file access patterns.

[^fn-posix]: **POSIX (Portable Operating System Interface)**: A family of standards specifying the API between applications and operating systems, including file operations (open, read, write, close, seek). POSIX file semantics guarantee strong consistency: reads return the most recently written data, and writes to the same region are serialized. Distributed file systems that provide POSIX semantics (HDFS, Lustre) are easier to adopt but incur coordination overhead to maintain consistency. Object stores like S3 deliberately abandon POSIX semantics, offering simpler APIs with weaker guarantees but better scalability.

#### Google File System and Colossus {#sec-storage-google-file-system-colossus-40ea}

The Google File System (GFS) [@ghemawat2003google], published in 2003, established the architectural template for large-scale distributed storage. Its design decisions, optimized for Google's web crawling and indexing workloads, proved remarkably well suited for ML training data.

GFS makes several unconventional design choices:

**Large block sizes.** Where traditional file systems use 4 KB blocks, GFS uses 64 MB chunks. This reduces metadata overhead: a 1 PB dataset contains only 16 million chunks rather than 256 billion 4 KB blocks. For ML training data, which is read sequentially in large batches, large blocks eliminate seek overhead and maximize throughput.

**Single master, multiple chunkservers.** A single master maintains all file system metadata (namespace, chunk locations, access permissions) while data flows directly between clients and chunkservers. This architecture simplifies consistency guarantees: the master is the single source of truth for metadata. For training data, which is written once and read many times, this works well.

**Relaxed consistency model.** GFS provides weak consistency guarantees: concurrent writers may produce undefined file regions, and readers may see stale data during failures. These limitations are acceptable for training data (immutable after creation) but problematic for checkpoints (requiring atomicity).

**Colossus**, Google's successor to GFS deployed around 2010, addresses GFS limitations while maintaining its core design. Key improvements include:

- Distributed metadata: The single master bottleneck is eliminated by sharding metadata across multiple servers
- Smaller block sizes: 1 MB blocks improve space efficiency for smaller files
- Erasure coding[^fn-erasure-coding]: Reduces storage overhead from 3x replication to ~1.5x while maintaining durability
- Reed-Solomon encoding [@reed1960polynomial]: Enables efficient reconstruction of failed blocks

[^fn-erasure-coding]: **Erasure Coding**: A data protection technique that breaks data into fragments, adds redundant parity fragments, and distributes them across storage nodes such that the original data can be reconstructed from any subset of fragments. A common configuration is RS(6,3): 6 data fragments plus 3 parity fragments, tolerating any 3 failures with only 1.5x storage overhead (versus 3x for triple replication). The trade-off is computational cost: reconstruction requires Galois field arithmetic that consumes CPU cycles. For ML training data that is read frequently but rarely reconstructed, erasure coding dramatically reduces storage costs at petabyte scale.

For ML workloads, Colossus delivers aggregate read bandwidth exceeding 1 TB/s across Google's infrastructure, sufficient to feed thousands of TPU chips simultaneously.

#### Hadoop Distributed File System {#sec-storage-hadoop-distributed-file-system-0aac}

HDFS[^fn-hdfs] [@shvachko2010hadoop], the open-source implementation inspired by GFS, became the storage foundation for the Hadoop ecosystem and remains widely deployed for ML training data. Its architecture mirrors GFS:

- **NameNode**: Single master maintaining file system namespace and block locations
- **DataNodes**: Workers storing actual data blocks
- **Block replication**: Default 3x replication for durability

[^fn-hdfs]: **Hadoop Distributed File System (HDFS)**: Developed at Yahoo in 2006 as an open-source GFS clone, HDFS became the dominant distributed file system for big data workloads. Its design assumes commodity hardware failures are common and optimizes for batch processing with large files. HDFS stores files as sequences of 128 MB blocks (configurable), each replicated across three DataNodes. While effective for MapReduce workloads, HDFS's single-NameNode architecture limits metadata operations to roughly 10,000 per second, which becomes a bottleneck when ML training involves millions of small files or frequent checkpoint metadata updates.

HDFS optimizes for the same access patterns as GFS: large sequential reads and writes, with files written once and read many times. A typical HDFS deployment achieves 100-200 MB/s per DataNode, scaling to aggregate cluster throughput of 10-100 GB/s depending on cluster size.

**HDFS limitations for modern ML** include the single NameNode bottleneck (file system operations serialize through one server, limiting metadata operations to roughly 10,000 per second), JVM overhead (Java implementation adds latency and memory overhead compared to native implementations), and the small file problem (each file consumes NameNode memory regardless of size, so millions of small files exhaust metadata capacity).

#### The Small File Problem: IOPS vs Bandwidth {#sec-storage-small-file-problem-iops-vs-bandwidth-6819}

A critical limitation of HDFS (and similar distributed file systems) is the separation of metadata and data. While aggregate **bandwidth** scales linearly with the number of DataNodes, metadata throughput (**IOPS**) is often bottlenecked by the single NameNode.

Consider a 1 TB dataset:

*   **Scenario A**: Stored as 1,000 files of 1 GB each. Reading the dataset requires 1,000 metadata RPCs to the NameNode.
*   **Scenario B**: Stored as 100 million files of 10 KB each (e.g., individual small images). Reading the dataset requires 100 million metadata RPCs.

In Scenario B, the NameNode will be overwhelmed by metadata requests long before the DataNodes saturate their network links. The training job will spend most of its time waiting for `open()` calls to complete rather than reading data. This structural bottleneck drives the necessity for aggregation formats like **TFRecord** or **Parquet**, which bundle thousands of small samples into a single large file, amortizing one metadata operation over many data reads.

These limitations become acute for ML workloads with many small files (e.g., individual images) or high-frequency metadata operations (e.g., checkpoint writes). HDFS works well for datasets stored as large sequential files (TFRecord, Parquet) but struggles with directory structures containing millions of individual samples.

#### Lustre and Parallel File Systems {#sec-storage-lustre-parallel-file-systems-9af6}

Lustre[^fn-lustre] [@schwan2003lustre], developed for high-performance computing (HPC), takes a different architectural approach optimized for parallel access from thousands of compute nodes simultaneously. Where GFS and HDFS prioritize simplicity and fault tolerance, Lustre prioritizes raw throughput.

[^fn-lustre]: **Lustre**: A parallel distributed file system developed by Cluster File Systems starting in 1999 and now maintained as open source. The name combines "Linux" and "cluster." Lustre powers seven of the top ten supercomputers globally and is the dominant storage solution for scientific computing. Its design prioritizes aggregate bandwidth over fault tolerance: as @tbl-lustre-performance quantifies, Lustre achieves 1+ TB/s throughput by striping files across hundreds of storage servers, but requires careful administration and can be less forgiving of hardware failures than cloud-native solutions. For ML training on HPC clusters, Lustre's bandwidth often exceeds cloud object storage by 10-100x.

**Lustre architecture** separates metadata from data more aggressively. Metadata Servers (MDS) handle file system namespace operations (create, open, stat), Object Storage Servers (OSS) store actual file data, and Object Storage Targets (OST) represent individual storage devices attached to OSSs.

Files are striped across multiple OSTs, enabling parallel access that aggregates bandwidth from many storage devices. A large file might be striped across 100 OSTs; reading the file in parallel achieves 100x the bandwidth of a single OST.

**Lustre performance characteristics**:

| **Configuration**              | **Aggregate Bandwidth** | **Typical Use**                  |
|:-----------------------------|----------------------:|:-------------------------------|
| **Small cluster (10 OSTs)**    |              10-20 GB/s | Research lab ML training         |
| **Medium cluster (100 OSTs)**  |            100-200 GB/s | Production ML training           |
| **Large cluster (1000+ OSTs)** |                 1+ TB/s | Exascale HPC, large LLM training |

: **Lustre Performance Scaling**: Aggregate bandwidth scales linearly with Object Storage Target (OST) count, from 10-20 GB/s with 10 OSTs to 1+ TB/s with 1000+ OSTs. This linear scaling enables Lustre to feed the largest GPU clusters for LLM training. {#tbl-lustre-performance}

For ML training, Lustre excels when training data is stored as large files that can be striped across many OSTs, multiple training jobs read the same data concurrently (shared datasets), or checkpoint writes require high bandwidth to minimize training interruption.

Lustre's disadvantage is operational complexity: tuning stripe sizes, managing quota, and handling metadata server failures requires specialized expertise that cloud-native teams may lack.

#### Comparative Analysis {#sec-storage-comparative-analysis-648f}

The choice between distributed file systems depends on workload characteristics, operational expertise, and existing infrastructure. @tbl-dfs-comparison guides selection based on each system's strengths and ideal deployment contexts.

| **System**                                 | **Strengths**                               | **Weaknesses**                | **Best For**                |
|:-----------------------------------------|:------------------------------------------|:----------------------------|:--------------------------|
| **GFS/Colossus**                           | Massive scale, Google ecosystem integration | Proprietary to Google         | Google Cloud ML             |
| **HDFS**                                   | Open source, Hadoop ecosystem               | Single NameNode, JVM overhead | Spark-based data processing |
| **Lustre**                                 | Raw throughput, HPC optimized               | Operational complexity        | On-premise HPC clusters     |
| **GPFS/Spectrum Scale [@schmuck2002gpfs]** | Enterprise features, mixed workloads        | Cost, complexity              | Large enterprise ML         |
| **BeeGFS**                                 | Ease of deployment, good performance        | Smaller community             | Academic/research clusters  |

: **Distributed File System Selection Guide**: HDFS excels for Spark-based data processing despite single-NameNode limitations; Lustre delivers raw throughput for HPC clusters but demands operational expertise. The choice hinges on workload characteristics and team capabilities. {#tbl-dfs-comparison}

The architectural principles pioneered by GFS found their way into the broader ecosystem through open-source implementations like HDFS and parallel file systems like Lustre. Each system makes different tradeoffs between throughput, operational complexity, and integration with existing infrastructure. For many organizations, however, the operational burden of running distributed file systems has driven adoption of a simpler alternative: object storage.

### Object Storage at Scale {#sec-storage-object-storage-scale-188b}

Object storage provides a simpler abstraction than file systems: objects are stored and retrieved by key, without directories, hierarchies, or POSIX semantics. This simplicity enables massive scale, high durability, and low cost.

#### Object Storage Architecture {#sec-storage-object-storage-architecture-7226}

Object stores organize data as flat namespaces of key-value pairs. An object has:

- **Key**: A unique identifier (often resembling a file path: `training-data/imagenet/images/n01440764/n01440764_10026.JPEG`)
- **Value**: The object data (arbitrary bytes, typically KB to GB in size)
- **Metadata**: User-defined key-value pairs (content type, creation time, checksums)

This design eliminates the metadata bottlenecks that limit distributed file systems. Without directories to traverse or hierarchies to maintain, object stores scale to billions of objects without architectural changes.

**Durability through redundancy.** Object stores achieve extreme durability[^fn-eleven-nines] (S3 advertises 99.999999999% or "eleven nines") through erasure coding (data is split into fragments with redundant parity fragments, enabling reconstruction if any fragment is lost), geographic distribution (fragments are spread across multiple availability zones or regions), and continuous verification (background processes detect and repair bit rot).

[^fn-eleven-nines]: **Eleven Nines Durability (99.999999999%)**: This seemingly abstract number has concrete meaning: for one million objects stored for 10 million years, you would statistically expect to lose one object. Achieving this requires geographic replication, erasure coding, continuous integrity verification, and automated repair of detected corruption. For ML training data representing months of curation effort, this level of durability justifies the higher latency of object storage compared to local disks. The durability guarantee applies to stored data, not availability: objects are durably stored but may be temporarily inaccessible during availability zone outages.

Durability guarantees that data survives once written. But when does written data become visible to readers? This is the domain of consistency guarantees, which have evolved significantly in recent years.

**Consistency model evolution.** Historically, object stores provided eventual consistency: after writing an object, some readers might not immediately see the new version. This changed significantly in December 2020 when AWS S3 upgraded to strong read-after-write consistency for all operations at no additional cost [@aws2020s3]. GCS has always provided strong consistency. This evolution simplifies ML storage architecture: checkpoints can now rely on object stores providing immediate visibility after successful writes, though application-level verification remains prudent for critical data.

#### Amazon S3 and Google Cloud Storage {#sec-storage-amazon-s3-google-cloud-storage-46c7}

S3, launched in 2006, pioneered the object storage model and remains the dominant cloud storage service. GCS provides similar capabilities with Google Cloud integration.

**S3 performance characteristics** include single object throughput of 100 MB/s per connection, aggregate throughput that scales with parallel connections (hundreds of GB/s possible), latency of 50-100 ms for first byte, and request rate of 5,500 GET/s and 3,500 PUT/s per prefix.

The request rate limit is significant for ML: a dataset organized as `s3://bucket/class_name/image.jpg` limits each class to 5,500 reads per second. Sharding data across random prefixes (`s3://bucket/shard_id/sample.dat`) avoids this bottleneck.

**GCS performance** is similar, providing single object throughput of 100+ MB/s per connection, strong consistency (both GCS and S3 since December 2020 provide read-after-write consistency), and composite objects (multiple objects can be composed into one without re-upload).

Practical throughput depends heavily on access patterns. @tbl-object-storage-throughput reveals how sequential large-object reads can achieve 100x the throughput of small random lookups:

| **Access Pattern**            |  **Typical Throughput** | **Optimization**                            |
|:----------------------------|----------------------:|:------------------------------------------|
| **Sequential single object**  |                100 MB/s | Use large objects (100+ MB)                 |
| **Parallel multiple objects** |             10-100 GB/s | Use multiple connections, random prefixes   |
| **Small object reads**        | Limited by request rate | Batch into larger objects                   |
| **Listing operations**        |          1000 objects/s | Use flat namespaces, avoid deep hierarchies |

: **Object Storage Throughput by Access Pattern**: Sequential multi-object reads with parallel connections achieve 10-100 GB/s, while small object reads are limited by request rate (5,500 GET/s per prefix). Batching small samples into large files converts the latter into the former. {#tbl-object-storage-throughput}

#### Object Storage for ML Training {#sec-storage-object-storage-ml-training-f3f9}

Using object storage for training data requires adapting to its characteristics:

**Large file aggregation.** Rather than storing individual images as objects, aggregate samples into large files (TFRecord, WebDataset, Parquet). This converts thousands of small object reads into a few large sequential reads, dramatically improving throughput.

**Parallel data loading.** Training frameworks should open multiple parallel connections to object storage. PyTorch DataLoader with `num_workers > 1` or TensorFlow's `tf.data` with `num_parallel_calls` enables this parallelism.

**Prefetching and buffering.** Object storage latency (50-100 ms) would be catastrophic if each batch waited for storage. Data pipelines must prefetch many batches ahead, overlapping storage access with GPU computation.

**Local caching.** For datasets accessed repeatedly across training runs, caching on local NVMe reduces object storage costs and improves performance. The first training run populates the cache; subsequent runs read locally.

#### Data Egress and Cost {#sec-storage-data-egress-cost-505b}

In cloud environments, physical distance equals money. Cloud providers typically charge for data crossing availability zone (AZ) or regional boundaries.

*   **Intra-Zone**: Free and high bandwidth.
*   **Inter-Zone**: Charged (e.g., $0.01/GB) and higher latency.
*   **Inter-Region**: Expensive (e.g., $0.02-$0.05/GB) and high latency.

A common anti-pattern is storing a petabyte dataset in a regional bucket and training on a cluster pinned to a specific zone. Every training epoch reads the full petabyte across the zonal boundary, incurring massive "cross-zone networking" fees that can exceed the cost of the GPUs themselves. **Strict data locality** (ensuring the training bucket and compute cluster reside in the exact same zone) is an essential cost control mechanism.

### Data Format Selection {#sec-storage-data-format-selection-00b9}

The choice of data format significantly impacts training throughput, storage cost, and pipeline complexity. Modern ML workloads have converged on a few formats optimized for different access patterns.

#### TFRecord {#sec-storage-tfrecord-3b76}

TFRecord, TensorFlow's native format, stores data as sequential records in binary files. Each record contains a serialized protocol buffer with typed fields.

**Structure:**

```text
[length][crc32 of length][data][crc32 of data]
```

**Advantages** include sequential read optimization (records are stored contiguously, enabling high-throughput streaming), schema flexibility (protocol buffers support arbitrary nested structures), compression (GZIP or ZSTD compression can be applied transparently), and splitting (large datasets can be sharded across multiple TFRecord files).

**Limitations** include random access (accessing a specific record requires scanning from the beginning), TensorFlow coupling (while readable from other frameworks, optimized for TensorFlow), and no indexing (cannot query or filter without reading entire file).

For large-scale training that processes entire datasets sequentially, TFRecord achieves near-optimal throughput. A well-configured TFRecord pipeline can saturate 10+ GB/s NVMe storage.

#### Apache Parquet {#sec-storage-apache-parquet-ec9a}

Parquet[^fn-parquet], developed for the Hadoop ecosystem and inspired by Google's Dremel [@melnik2010dremel], uses columnar storage that stores all values of a column together rather than all columns of a row together.

[^fn-parquet]: **Apache Parquet**: An open-source columnar storage format created in 2013 by Twitter and Cloudera. The name references the geometric floor pattern where pieces fit together efficiently. Parquet stores data column-by-column rather than row-by-row, enabling efficient compression (similar values compress well) and selective column reads (skip columns not needed for analysis). For ML feature datasets with hundreds of columns where training uses only a subset, Parquet can reduce I/O by 10x compared to row-oriented formats. The trade-off is write amplification: updating a single value requires rewriting an entire row group (typically 128 MB).

**Columnar advantages for ML** include column pruning (read only the columns needed, such as skipping metadata and reading only pixels), compression efficiency (similar values in a column compress better than mixed values in a row), and predicate pushdown (filter data without reading irrelevant rows).

**Parquet structure:**

```text
Row Group 1
  Column A chunk (values for rows 0-N)
  Column B chunk
  ...
Row Group 2
  ...
Footer (schema, statistics, locations)
```

The footer contains statistics (min/max values) for each column chunk, enabling queries to skip row groups that cannot match filter predicates.

**Parquet for ML** works well when datasets have many columns but training uses few (feature selection), filtering is needed (such as training only on samples meeting criteria), or data is shared with analytics tools (Spark, pandas, DuckDB).

**Limitations** include write amplification (updating a single value requires rewriting the row group) and lack of optimization for image or binary data (columnar layout provides little benefit).

#### WebDataset {#sec-storage-webdataset-86b3}

WebDataset [@aizman2019webdataset] stores samples as TAR archives, with each sample's components (image, label, metadata) as separate files within the archive.

**Structure:**

```text
sample0001.jpg
sample0001.cls
sample0001.json
sample0002.jpg
sample0002.cls
...
```

**WebDataset advantages** include HTTP compatibility (TAR files can be streamed directly from web servers or object storage), simple format (standard UNIX tools can inspect and manipulate archives), shuffling (shuffle buffers can be applied during streaming), and no framework dependency (works with PyTorch, TensorFlow, JAX).

**For distributed training**, WebDataset enables efficient data loading from object storage. Each worker is assigned different TAR shards, workers stream shards in parallel with no coordination needed, and local shuffle buffers randomize sample order within each worker.

**Throughput:** WebDataset achieves 1-5 GB/s per worker from object storage, scaling linearly with worker count.

#### Format Selection Guidelines {#sec-storage-format-selection-guidelines-78a8}

@tbl-format-selection guides the matching of storage formats to model types, which requires considering access patterns and data characteristics. For LLMs processing tokenized sequences, TFRecord or custom binary formats minimize overhead; for vision models with images, WebDataset efficiently packages JPEG files; for structured features, Parquet's columnar layout enables selective reading.

| **Model Type** | **Recommended Format**    | **Rationale**                                                       |
|:-------------|:------------------------|:------------------------------------------------------------------|
| **LLM**        | TFRecord or custom binary | Token sequences are fixed-length arrays, columnar offers no benefit |
| **Vision**     | WebDataset or TFRecord    | Large binary blobs (images), sequential access pattern              |
| **RecSys**     | Parquet                   | Many sparse features, column pruning valuable                       |
| **Scientific** | HDF5 or domain-specific   | Multi-dimensional arrays, random access sometimes needed            |
| **Multimodal** | WebDataset                | Different modalities (image, text, audio) naturally grouped         |

: **Data Format Selection by Model Type**: LLMs benefit from TFRecord's sequential streaming; RecSys exploits Parquet's column pruning for sparse features; vision models use WebDataset for efficient image packaging. Format selection impacts throughput by 2-10x. {#tbl-format-selection}

### Data Loading Pipelines {#sec-storage-data-loading-pipelines-b193}

The data loading pipeline connects storage to accelerators, transforming raw data into training batches. Pipeline design determines whether storage bandwidth is fully utilized and whether GPUs remain fed during training.

#### Pipeline Stages {#sec-storage-pipeline-stages-0bb0}

A typical data loading pipeline includes data reading (fetch bytes from storage such as disk, network, or object store), decompression (decompress compressed formats like GZIP, ZSTD, or JPEG), deserialization (parse structured data such as protobuf or JSON), transformation (apply augmentations like resize, crop, or normalize), batching (collate samples into batches), and transfer (move batches to accelerator memory).

Each stage has different compute and bandwidth characteristics. @tbl-pipeline-stages identifies which operations can be parallelized and where bottlenecks typically emerge:

| **Stage**           | **Bound By**      | **Parallelizable**         | **Typical Duration** |
|:------------------|:----------------|:-------------------------|-------------------:|
| **Reading**         | Storage bandwidth | Yes (sharded data)         |             1-100 ms |
| **Decompression**   | CPU compute       | Yes (per sample)           |            0.1-10 ms |
| **Deserialization** | CPU compute       | Yes (per sample)           |            0.01-1 ms |
| **Transformation**  | CPU compute       | Yes (per sample)           |            0.1-50 ms |
| **Batching**        | Memory bandwidth  | Limited                    |             0.1-1 ms |
| **Transfer**        | PCIe bandwidth    | Limited (few DMA channels) |            0.1-10 ms |

: **Data Loading Pipeline Stages**: Reading and transformation dominate latency (1-100ms and 0.1-50ms respectively), while batching and transfer are fast (< 10ms). Parallelizing reading and transformation through worker processes is essential to saturate storage bandwidth. {#tbl-pipeline-stages}

#### Prefetching and Pipelining {#sec-storage-prefetching-pipelining-285e}

The key to hiding latency is pipelining: while the GPU processes batch $N$, the CPU prepares batch $N+1$, and storage fetches data for batch $N+2$. This requires maintaining multiple batches in flight simultaneously. When pipelining fails to fully hide I/O latency, the resulting idle time is captured by the *data stall ratio*.

::: {.callout-example title="Engineering Metric: Data Stall Ratio"}
**The Problem**: Your GPU utilization is stuck at 80%. Is it a kernel issue or a storage issue?

**Formula**:
$$ T_{step} = \max(T_{compute}, T_{io}) $$
$$ \text{Data Stall \%} = \frac{T_{step} - T_{compute}}{T_{step}} \times 100 $$

**Scenario**: ResNet-50 training.

*   **GPU Compute Time ($T_{compute}$)**: 200 ms per batch.
*   **I/O Load Time ($T_{io}$)**: 250 ms (due to slow HDD or network).

**Calculation**:
$$ T_{step} = \max(200, 250) = 250 \text{ ms} $$
$$ \text{Data Stall \%} = \frac{250 - 200}{250} = \mathbf{20\%} $$

**Conclusion**: Your expensive GPU is idle 20% of the time waiting for data. **Solution**: Increase prefetch buffer depth (`num_workers`), switch to local NVMe caching, or use a compressed data format (WebDataset) to reduce $T_{io}$.
:::

The prefetch buffer size determines how much latency can be hidden:

$$T_{hidden} = N_{prefetch} \times T_{batch}$$

where $N_{prefetch}$ is the number of batches buffered and $T_{batch}$ is the GPU batch processing time.

For a 200 ms batch time with 100 ms storage latency, prefetching just 1 batch hides the storage latency. However, variance in storage latency requires larger buffers: if storage latency varies from 50-500 ms, prefetching 3-5 batches ensures GPUs never wait. @fig-data-pipeline-vol2 visualizes this coordination between storage, CPU preprocessing, and GPU training:

::: {#fig-data-pipeline-vol2 fig-env="figure" fig-pos="htb" fig-cap="**Hiding Storage Latency with Prefetching**. Without prefetching (top), the GPU sits idle while the CPU loads and transforms the next batch. With pipelining (bottom), the CPU prepares Batch $N+1$ while the GPU processes Batch $N$, ensuring the GPU is never starved of data. The prefetch buffer smooths out I/O latency jitter." fig-alt="Two timelines comparing data loading strategies. Top: sequential loading with GPU idle during CPU load phases. Bottom: pipelined approach where CPU loads batch N+1 while GPU trains batch N, with dashed arrows showing data flow."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9]
  \definecolor{CpuColor}{RGB}{200,220,255}
  \definecolor{GpuColor}{RGB}{150,255,150}
  \definecolor{WaitColor}{RGB}{255,200,200}

  \tikzset{
    block/.style={draw=black!60, thick, minimum height=0.6cm, align=center, font=\footnotesize},
    cpu_block/.style={block, fill=CpuColor},
    gpu_block/.style={block, fill=GpuColor},
    wait_block/.style={block, fill=WaitColor, pattern=north east lines, pattern color=red}
  }

  % Timeline 1: No Pipelining
  \node[anchor=west, font=\bfseries] at (0, 3.5) {Sequential (No Prefetch)};
  % Batch 1
  \node[cpu_block, minimum width=2cm] (L1) at (1, 2.5) {Load B1};
  \node[gpu_block, minimum width=1.5cm, right=0cm of L1] (P1) {Train B1};
  % Batch 2
  \node[cpu_block, minimum width=2cm, right=0cm of P1] (L2) {Load B2};
  \node[gpu_block, minimum width=1.5cm, right=0cm of L2] (P2) {Train B2};

  \node[red, font=\footnotesize, above=0.1cm of P1] {GPU Active};
  \node[red, font=\footnotesize, above=0.1cm of L2] {GPU Idle};

  % Timeline 2: Pipelined
  \node[anchor=west, font=\bfseries] at (0, 1) {Pipelined (Prefetch N=2)};

  % CPU Row
  \node[anchor=east] at (0, 0.5) {CPU (Load)};
  \node[cpu_block, minimum width=2cm] (PL1) at (1, 0.5) {Load B1};
  \node[cpu_block, minimum width=2cm, right=0cm of PL1] (PL2) {Load B2};
  \node[cpu_block, minimum width=2cm, right=0cm of PL2] (PL3) {Load B3};
  \node[cpu_block, minimum width=2cm, right=0cm of PL3] (PL4) {Load B4};

  % GPU Row
  \node[anchor=east] at (0, -0.5) {GPU (Train)};
  \node[gpu_block, minimum width=1.5cm, below=0.4cm of PL2] (PP1) {Train B1};
  \node[gpu_block, minimum width=1.5cm, right=0cm of PP1] (PP2) {Train B2};
  \node[gpu_block, minimum width=1.5cm, right=0cm of PP2] (PP3) {Train B3};

  % Arrows
  \draw[->, dashed] (PL1.east) -- (PP1.west);
  \draw[->, dashed] (PL2.east) -- (PP2.west);

  % Label
  \node[align=left, font=\footnotesize, anchor=west] at (8, 0) {IO hidden\\behind compute};

\end{tikzpicture}
```
:::

#### Caching Strategies {#sec-storage-caching-strategies-c709}

Caching can dramatically improve data loading performance when datasets are accessed repeatedly.

**Local disk caching** fetches from remote storage on first access and writes to local NVMe. Subsequent accesses read from local disk at 7-25 GB/s rather than network speeds. This approach is effective when the dataset fits on local storage, multiple epochs are trained, or network bandwidth is limited.

**Memory caching:** Entire dataset or frequently accessed portions are loaded into DRAM. Achieves 100+ GB/s bandwidth but limited by DRAM capacity (typically 512 GB - 2 TB per node).

**Distributed caching:** Services like Alluxio[^fn-alluxio] [@li2014tachyon] provide a distributed cache layer between compute and storage. Multiple nodes contribute memory to a shared cache, enabling datasets larger than single-node memory to be cached.

[^fn-alluxio]: **Alluxio (formerly Tachyon)**: An open-source virtual distributed storage system created at UC Berkeley AMPLab in 2013. Alluxio provides a unified namespace across multiple storage systems (S3, HDFS, Azure Blob) with memory-speed caching. For ML workloads, Alluxio can cache hot training data across cluster nodes, reducing repeated fetches from cold object storage. Meta uses a similar internal system to accelerate training data access, achieving 10x throughput improvement for frequently accessed datasets.

**Cache invalidation** for ML is straightforward: training datasets are immutable. The cache can use simple LRU eviction without concern for consistency.

#### Shuffling in Distributed Training {#sec-storage-shuffling-distributed-training-38ae}

Shuffling is essential for training: without shuffling, the model sees samples in the same order every epoch, potentially learning spurious ordering correlations. Distributed shuffling is challenging because each worker must see different samples, samples should be randomly ordered within and across workers, and communication for global shuffle is expensive.

**Common approaches:**

**File-level shuffle:** Assign each worker a random subset of data files. Workers read their files sequentially. Low communication cost but limited randomness: samples within a file are always adjacent.

**Shuffle buffer:** Each worker maintains a buffer of $B$ samples. New samples are randomly exchanged with buffer contents. Provides good local randomness without communication.

**Reservoir sampling:** When the dataset is too large to shuffle in memory, reservoir sampling maintains a random subset. Each new sample has probability $k/n$ of entering the reservoir, where $k$ is reservoir size and $n$ is samples seen.

**Epoch boundary shuffle:** At epoch boundaries, workers exchange file assignments. This adds randomness across epochs without per-batch communication.

Combining these techniques into a complete data loading architecture requires careful coordination of sharding, caching, and prefetching across all nodes.

::: {.callout-example title="Data Loading for 1024-GPU Training"}

Consider loading ImageNet data for 1024-GPU training with optimal throughput.

**Configuration:**

- 1024 GPUs across 128 nodes (8 GPUs/node)
- Target: 80% GPU utilization
- Batch size per GPU: 256 images (150 KB average)
- Target iteration time: 200 ms
- Required bandwidth: 157 GB/s (from earlier calculation)

**Solution architecture** stores 1.4 TB ImageNet as 1000 WebDataset TAR shards in object storage, assigns each node roughly 8 non-overlapping shards, caches shards to local NVMe (10 TB per node available) on first epoch, uses 16 worker processes per node (2 per GPU), and maintains a prefetch buffer of 4 batches per GPU.

**Bandwidth calculation:**

- Per-node requirement: 157 GB/s / 128 nodes = 1.23 GB/s
- Local NVMe provides: 7+ GB/s (ample margin)
- First epoch from object storage: Each node needs 11 GB, ~90 seconds to cache

After the first epoch, training is entirely from local cache, eliminating network bottlenecks.

:::

### Data Locality Principles {#sec-storage-data-locality-principles-7419}

The cost of moving data between nodes dominates computation time for many ML workloads. Data locality places computation where data already resides, a fundamental principle underlying both Spark [@zaharia2016apache] and Ray [@moritz2018ray].

**Locality Hierarchy:**

| **Locality Level** | **Description**                   | **Typical Latency** | **Bandwidth** |
|:-----------------|:--------------------------------|------------------:|------------:|
| **GPU_LOCAL**      | Data in GPU HBM                   |              ~10 ns |       3+ TB/s |
| **CPU_LOCAL**      | Data in same-node CPU memory      |             ~100 ns |      200 GB/s |
| **RACK_LOCAL**     | Data on another node in same rack |             ~100 μs |       25 GB/s |
| **ANY**            | Data on any node in cluster       |               ~1 ms |       10 GB/s |

The 100x bandwidth difference between CPU_LOCAL (200 GB/s) and ANY (10 GB/s via network) makes locality-aware scheduling essential for bandwidth-intensive ML workloads.

**Scheduling for Locality:**

Ray's scheduler exemplifies locality-aware scheduling:

1. When a task requires a data object, the scheduler identifies nodes holding that object
2. Tasks are preferentially scheduled to nodes with data (if capacity available)
3. If no capacity, the task is scheduled elsewhere and data is transferred
4. Scheduling decisions consider: data size, transfer time, node load, task urgency

For training data loading:

- Each worker is assigned shards stored on its local NVMe
- Workers read local shards (7+ GB/s) rather than remote (1 GB/s network)
- Shuffle operations respect locality: map outputs written locally, reducers fetch as needed

**When Locality Fails:**

Locality optimization breaks down when hot data (popular embeddings accessed by many workers) cannot all be local, feature store lookups are inherently non-local (random access), or small data causes scheduling overhead to exceed data transfer time.

For these cases, replication and caching substitute for locality.

```{python}
#| label: sharded-io-calc
#| echo: false

# Sharded I/O locality analysis
total_shards = 1000
n_nodes_io = 100
shards_per_node = total_shards // n_nodes_io  # 10
local_read_gbs = 10  # GB/s NVMe
shard_size_mb = 100  # MB per shard (approx for ImageNet)
data_per_node_gb = shards_per_node * shard_size_mb / 1000  # 1 GB
local_read_time_s = data_per_node_gb / local_read_gbs  # 0.1s

# Remote comparison
network_bw_gbs = 10  # GB/s shared network
per_node_remote_gbs = network_bw_gbs / n_nodes_io  # 0.1 GB/s
locality_speedup = local_read_gbs / per_node_remote_gbs  # 100x

# Format
shards_per_node_str = f"{shards_per_node}"
data_per_node_gb_str = f"{data_per_node_gb:.0f}"
local_read_time_str = f"{local_read_time_s}"
per_node_remote_gbs_str = f"{per_node_remote_gbs}"
locality_speedup_str = f"{int(locality_speedup)}"
```

**Quantitative Example:**

ImageNet training with 1000 WebDataset shards across 100 nodes:

- Local access: Each node stores `{python} shards_per_node_str` shards, reads at 10 GB/s = 100 MB/shard for `{python} shards_per_node_str` shards = `{python} data_per_node_gb_str` GB per node delivered in `{python} local_read_time_str` seconds
- Remote access: If shards were centralized, 100 nodes competing for 10 GB/s network = `{python} per_node_remote_gbs_str` GB/s per node

Locality provides `{python} locality_speedup_str`x throughput improvement for this workload.

### I/O Optimization for ML Data Pipelines {#sec-storage-io-optimization-ml-data-pipelines-b1ff}

The bandwidth calculations in @sec-storage-data-pipeline-throughput-requirements-50aa establish what storage systems must deliver. This section examines how to achieve those targets through systematic I/O optimization, addressing the gap between raw storage capabilities and effective training throughput.

#### The I/O Bandwidth Requirement Equation {#sec-storage-io-bandwidth-requirement-equation-6f3b}

Training pipelines must deliver data faster than GPUs consume it. When I/O bandwidth falls below GPU consumption rate, expensive accelerators idle waiting for data. The fundamental constraint is:

::: {.callout-definition title="I/O Bandwidth Requirement"}

For I/O not to bottleneck training, storage bandwidth must satisfy:

$$B_{io} \geq B_{gpu} \times \frac{T_{preprocess}}{T_{batch}}$$

where $B_{gpu}$ is the effective data consumption rate of the accelerator, $T_{preprocess}$ is the time to preprocess one batch (decode, augment, normalize), and $T_{batch}$ is the GPU batch processing time. The ratio $T_{preprocess}/T_{batch}$ represents how many batches must be in flight to hide preprocessing latency.

:::

This equation reveals that I/O requirements depend not just on GPU speed but on preprocessing complexity. Vision models with heavy augmentation (random crops, color jitter, mixup) may require 3-5x higher I/O bandwidth than the raw data consumption rate suggests, because preprocessing takes 3-5x longer than GPU computation.

#### Sequential vs Random Access: The Performance Gap {#sec-storage-sequential-vs-random-access-performance-gap-51f6}

Storage device performance varies dramatically with access pattern. The numbers in @tbl-io-access-patterns reveal why data format and access pattern optimization matter more than raw device specifications.

| **Storage Type**     | **Sequential Read** | **Random Read (4KB)** | **Random/Sequential** |
|:-------------------|------------------:|--------------------:|--------------------:|
| **HDD (7200 RPM)**   |        150-200 MB/s | 0.5-1 MB/s (100 IOPS) |              0.3-0.5% |
| **SATA SSD**         |        500-550 MB/s |          200-400 MB/s |                40-70% |
| **NVMe SSD**         |            3-7 GB/s |   500 MB/s - 1.5 GB/s |                15-25% |
| **NVMe (4x RAID 0)** |          12-25 GB/s |              2-6 GB/s |                15-25% |

: **Storage Performance by Access Pattern**: HDDs suffer 200-300x degradation for random access due to mechanical seek time; SSDs suffer 4-7x degradation due to small transfer sizes not amortizing controller overhead. ML training pipelines must be designed for sequential access to achieve advertised throughput. {#tbl-io-access-patterns}

The performance gap between sequential and random access has profound implications for ML data pipelines:

**HDDs are unusable for random access ML workloads.** A 7200 RPM HDD delivers 150-200 MB/s sequential throughput, sufficient for a modest training pipeline. But random 4KB reads (the pattern when accessing individual small images from disk) yield only 0.5-1 MB/s due to 8-10ms seek times. A dataset of 1 million individual JPEG files on HDD would require over 10 days to read randomly, versus 2 hours sequentially.

**SSDs narrow but do not eliminate the gap.** NVMe SSDs achieve 3-7 GB/s sequential throughput, but random 4KB reads drop to 500 MB/s-1.5 GB/s. The 4-7x degradation occurs because small transfers do not amortize the fixed overhead per I/O operation (command processing, internal flash page reads). For ML, this means that storing images as individual files on NVMe still leaves significant performance on the table.

**Aggregated formats recover sequential performance.** Converting a dataset from individual files to TFRecord, WebDataset, or Parquet transforms random access into sequential streaming. A WebDataset TAR archive containing 10,000 images reads at full sequential bandwidth (7 GB/s on NVMe) rather than random access rates (1.5 GB/s). This 4-5x throughput improvement often makes the difference between I/O-bound and compute-bound training.

#### Worked Example: ImageNet Data Loading Bottleneck Analysis {#sec-storage-worked-example-imagenet-data-loading-bottleneck-analysis-8b6c}

To see how storage format and device choice interact, we walk through a complete I/O bottleneck analysis for a standard vision training workload.

::: {.callout-example title="ImageNet I/O Bottleneck Analysis"}

Consider training ResNet-50 [@he2016resnet] on ImageNet [@deng2009imagenet] with a single NVIDIA A100 GPU. We analyze whether I/O bottlenecks training and quantify the benefit of format optimization.

**Training configuration:**

- Model: ResNet-50 (`{python} resnet_params_m`M parameters)
- Dataset: ImageNet (1.28M training images, ~150 KB average JPEG size)
- Batch size: 256 images
- Target throughput: 1000 images/second (achievable A100 throughput)
- Preprocessing: Decode JPEG, random crop to 224x224, normalize, augment

#### Step 1: Calculate Raw I/O Requirement {.unnumbered}

$$B_{raw} = 1000 \text{ images/s} \times 150 \text{ KB/image} = 150 \text{ MB/s}$$

This 150 MB/s requirement seems modest. Even an HDD could sustain this rate for sequential reads.

#### Step 2: Account for Access Pattern {.unnumbered}

ImageNet stored as individual JPEG files (the common download format) creates a directory structure with 1000 class folders. Training with shuffling requires random access across the entire dataset:

- HDD random read: ~100 IOPS = 100 images/s (10x slower than target)
- NVMe random read: ~300K IOPS = 300K images/s (sufficient, with overhead)

#### Step 3: Account for Preprocessing Overhead {.unnumbered}

JPEG decoding and augmentation require CPU time. Typical preprocessing costs:

- JPEG decode: 2-5 ms per image
- Augmentation (crop, flip, normalize): 1-2 ms per image
- Total preprocessing: 3-7 ms per image

For 256-image batches at 1000 images/s:

$$T_{batch} = \frac{256}{1000} = 256 \text{ ms}$$

$$T_{preprocess} = 256 \times 5 \text{ ms} = 1280 \text{ ms (single-threaded)}$$

Single-threaded preprocessing is 5x slower than GPU consumption. This requires 5 parallel workers to keep pace.

#### Step 4: Calculate Effective I/O Requirement {.unnumbered}

With preprocessing parallelism, effective I/O bandwidth must support 5 concurrent streams reading ahead:

$$B_{effective} = 5 \times 150 \text{ MB/s} = 750 \text{ MB/s}$$

#### Step 5: Format Optimization Impact {.unnumbered}

| **Configuration**            | **Read Pattern** | **Achievable** |    **Bottleneck?** |
|:---------------------------|:---------------|-------------:|-----------------:|
| **Individual JPEGs on HDD**  | Random           |        15 MB/s | Yes (50x too slow) |
| **Individual JPEGs on NVMe** | Random           |       1.5 GB/s |                 No |
| **WebDataset on HDD**        | Sequential       |       150 MB/s |  Yes (5x too slow) |
| **WebDataset on NVMe**       | Sequential       |         7 GB/s |   No (9x headroom) |

**Conclusion:** Individual JPEG storage on HDD creates a 50x I/O bottleneck. Converting to WebDataset and using NVMe provides 9x headroom, ensuring I/O never limits training throughput.

:::

#### Prefetching Benefit Quantification {#sec-storage-prefetching-benefit-quantification-2af4}

Prefetching overlaps I/O latency with GPU computation, but its benefit depends on the relationship between I/O latency, preprocessing time, and batch time. We quantify this benefit through *prefetch efficiency*.

::: {.callout-definition title="Prefetch Efficiency"}

The **prefetch efficiency** $\eta_{prefetch}$ measures how much of I/O latency is hidden:

$$\eta_{prefetch} = \min\left(1, \frac{N_{prefetch} \times T_{batch}}{T_{io} + T_{preprocess}}\right)$$

where $N_{prefetch}$ is the number of batches prefetched, $T_{batch}$ is GPU batch processing time, $T_{io}$ is I/O latency to fetch one batch, and $T_{preprocess}$ is preprocessing time per batch.

When $\eta_{prefetch} = 1$, all I/O latency is hidden and the GPU never waits for data.

:::

**Numerical example: Prefetch buffer sizing**

Consider a training pipeline with:

- $T_{batch} = 200$ ms (GPU batch processing)
- $T_{io} = 50$ ms (NVMe read latency for one batch)
- $T_{preprocess} = 400$ ms (preprocessing with 4 workers)

The I/O + preprocessing pipeline takes 450 ms end-to-end. To hide this latency:

$$N_{prefetch} \geq \frac{T_{io} + T_{preprocess}}{T_{batch}} = \frac{450}{200} = 2.25$$

Rounding up, 3 batches must be prefetched to achieve $\eta_{prefetch} = 1$.

**Memory cost of prefetching:**

Each prefetched batch consumes memory. For ImageNet with 256-image batches at 224x224x3 float32:

$$\text{Memory per batch} = 256 \times 224 \times 224 \times 3 \times 4 = 154 \text{ MB}$$

Three prefetched batches require 462 MB of CPU memory, a negligible cost on modern systems with 256+ GB DRAM.

**Diminishing returns beyond the minimum:**

Prefetching more than the minimum provides robustness against I/O latency variance. If I/O latency varies from 20-150 ms (typical for object storage), the worst-case pipeline time becomes:

$$T_{pipeline,worst} = 150 + 400 = 550 \text{ ms}$$

Required prefetch depth: $\lceil 550/200 \rceil = 3$ batches.

Additional prefetch beyond this point provides diminishing returns: buffer 4 batches tolerates up to 400 ms latency spikes; buffer 5 tolerates up to 600 ms. The marginal benefit decreases while memory cost increases linearly.

#### Data Format Performance Comparison {#sec-storage-data-format-performance-comparison-bcb4}

Data format selection impacts both storage efficiency and read throughput. @tbl-format-performance compares common formats across dimensions relevant to ML training.

| **Format**     | **Read Speed** | **Storage Overhead** | **Random Access** | **Best For**         |
|:-------------|-------------:|-------------------:|:----------------|:-------------------|
| **Raw JPEG**   |  1x (baseline) |                 None | Yes (slow)        | Small datasets       |
| **TFRecord**   |           3-5x |      5-10% (headers) | No                | TensorFlow pipelines |
| **WebDataset** |           3-5x |   1-2% (TAR headers) | No                | PyTorch, multi-modal |
| **Parquet**    |           2-3x |   -20% (compression) | Column-level      | Tabular/sparse data  |
| **LMDB**       |           4-6x |               10-20% | Yes (fast)        | Repeated epochs      |
| **HDF5**       |           2-4x |                5-15% | Yes (chunked)     | Scientific data      |

: **Data Format Performance Comparison**: Sequential formats (TFRecord, WebDataset) achieve 3-5x read speedup over individual files by eliminating per-file metadata overhead. LMDB provides fast random access through memory-mapped B-trees, ideal for workloads requiring non-sequential access. {#tbl-format-performance}

**TFRecord** excels for TensorFlow pipelines, providing native integration with `tf.data` and supporting transparent compression. The 5-10% storage overhead from record headers is offset by optional GZIP/ZSTD compression that typically reduces total size by 10-30%.

**WebDataset** uses the ubiquitous TAR format, enabling direct streaming from HTTP/S3 without custom readers. Its minimal 1-2% overhead makes it storage-efficient, and the format's simplicity ensures long-term readability.

**Parquet** provides column pruning for tabular datasets where training uses a subset of features. A recommendation model training on 10 of 500 features reads only 2% of the data, yielding effective 50x speedup over row-oriented formats.

**LMDB** (Lightning Memory-Mapped Database) maintains a B-tree index enabling O(log n) random access while achieving near-sequential read speeds through memory mapping. This makes LMDB ideal for workloads that require random access (such as curriculum learning or hard example mining) without sacrificing throughput.

#### I/O Profiling and Diagnosis {#sec-storage-io-profiling-diagnosis-5a7f}

Diagnosing I/O bottlenecks requires measurement at multiple levels. The following metrics identify where pipeline time is spent:

**GPU utilization** below target (< 90%) indicates a bottleneck somewhere in the pipeline. However, low GPU utilization does not pinpoint whether the bottleneck is I/O, preprocessing, or data transfer.

**Data loader wait time** measures how long the training loop blocks waiting for the next batch. PyTorch's DataLoader exposes this through profiling, as shown in @lst-dataloader-profiling.

::: {#lst-dataloader-profiling lst-cap="**DataLoader Wait Time Profiling**: Using PyTorch's built-in profiler to measure how long the training loop blocks waiting for the next batch, which reveals whether the data pipeline is the bottleneck."}
```{.python}
import torch.profiler

with torch.profiler.profile(
    activities=[torch.profiler.ProfilerActivity.CPU],
    record_shapes=True,
) as prof:
    for batch in dataloader:
        # Training step
        pass

# Look for "DataLoader" entries showing wait time
print(prof.key_averages().table(sort_by="cpu_time_total"))
```
:::

**Storage bandwidth utilization** indicates whether the storage system is saturated. On Linux, `iostat -x 1` shows per-device utilization as in @lst-iostat-utilization.

::: {#lst-iostat-utilization lst-cap="**Monitoring Storage Bandwidth**: The `iostat` command reveals per-device I/O utilization, helping identify whether storage is saturated or underutilized."}
```{.bash}
$ iostat -x 1
Device    r/s    rMB/s   rrqm/s  %util
nvme0n1   45000  6800    0       98%    # Near saturation
nvme1n1   12000  1800    0       45%    # Underutilized
```
:::

If storage utilization is low but GPU utilization is also low, the bottleneck is CPU preprocessing. If storage utilization is high and GPU utilization is low, the storage system is undersized.

**I/O wait percentage** in CPU statistics (`top`, `htop`) reveals when processes block on I/O, as @lst-iowait-monitoring illustrates.

::: {#lst-iowait-monitoring lst-cap="**I/O Wait Detection**: The `top` command shows I/O wait as a percentage of CPU time, indicating when processes are blocked waiting for storage operations to complete."}
```{.bash}
$ top
%Cpu(s):  15.2 us,  3.1 sy,  0.0 ni, 45.3 id, 35.4 wa, ...
                                           ^^^^^ I/O wait
```
:::

I/O wait above 20% indicates storage is constraining the pipeline.

### Model-Type Diversity in Training Data {#sec-storage-modeltype-diversity-training-data-5bdb}

Training data requirements vary dramatically across model types, affecting storage architecture, data format selection, and pipeline design. LLMs demand massive text corpora with sophisticated deduplication; vision models require high-bandwidth image pipelines; recommendation systems process continuous streams of user logs that can reach petabyte scale.

@tbl-training-data-diversity captures how training data characteristics diverge across model types, from LLMs requiring 10-100 TB of deduplicated text to RecSys processing 1+ PB of streaming user logs. **LLM training data** presents unique challenges at the preprocessing stage rather than during training. Raw web crawls contain duplicate content, low-quality text, and potentially harmful material. Deduplication alone can reduce dataset size by 30-50% [@wenzek2020ccnet]. The storage challenge is running these preprocessing pipelines at scale: processing Common Crawl (petabytes of raw HTML) requires distributed processing frameworks like Spark or Dataflow, with intermediate results stored in distributed file systems.

| **Model Type**   | **Data Format**          | **Typical Volume** | **Key Storage Challenge**                    |
|:---------------|:-----------------------|-----------------:|:-------------------------------------------|
| **LLM**          | Tokenized text           |          10-100 TB | Deduplication at scale, quality filtering    |
| **Vision**       | Images/Video             |     100 TB - 10 PB | Augmentation pipeline throughput             |
| **RecSys**       | User interaction logs    |              1+ PB | Privacy compliance, real-time freshness      |
| **Scientific**   | Simulations, sensor data |             10+ PB | Irregular structure, domain-specific formats |
| **Speech/Audio** | Waveforms, spectrograms  |          10-100 TB | Variable length sequences                    |

: **Training Data Diversity by Model Type**: LLMs require 10-100 TB of deduplicated text; RecSys processes 1+ PB of user interaction logs with real-time freshness requirements; vision models need high-throughput augmentation pipelines for 100 TB - 10 PB image datasets. {#tbl-training-data-diversity}

**Vision training data** is bandwidth-intensive during training due to real-time augmentation. Each image must be decoded, randomly augmented (crop, flip, color jitter), and normalized before being batched. The augmentation pipeline runs on CPU and can become the bottleneck for large vision models. Storing pre-augmented images is impractical (each image would need hundreds of augmented variants), so augmentation must happen during training.

**Recommendation system data** involves continuous streams of user interactions rather than static datasets. Each user action (click, purchase, view) becomes a training sample. The storage challenge is maintaining freshness: recommendation models trained on yesterday's data may miss today's trends. This requires streaming data architectures (Kafka, Pub/Sub) feeding into training pipelines, with careful attention to data retention policies for privacy compliance.

**Scientific ML data** often involves domain-specific formats (HDF5, NetCDF, FITS) with multi-dimensional arrays representing simulation outputs or sensor measurements. These formats support chunked access for random access patterns that sequential formats like TFRecord do not handle well. Storage systems must support both high-throughput sequential access for training and random access for data exploration.

## Checkpoint Storage Systems {#sec-storage-checkpoint-storage-systems-b7c1}

Training large models requires days to months of continuous computation. During this time, hardware failures, software bugs, and preemption events can terminate training at any point. Without checkpoints, a failure after two weeks of training would require restarting from the beginning, wasting millions of dollars in compute. Checkpointing saves model state periodically, enabling recovery from the most recent checkpoint rather than from scratch. The engineering challenge is minimizing checkpoint overhead while ensuring reliable recovery.

### Checkpoint Architecture Fundamentals {#sec-storage-checkpoint-architecture-fundamentals-135d}

With the cost of lost computation established, we can examine precisely what state must be preserved to enable recovery. A checkpoint captures the complete state needed to resume training exactly where it left off. This includes model parameters (the current values of all trainable weights; for a `{python} gpt3_params_b` billion parameter model in FP16 [@brown2020gpt3], this is 350 GB of dense data), optimizer state (optimizers like Adam[^fn-adam-optimizer] [@kingma2015adam] maintain per-parameter statistics such as first and second moments, with Adam's state being 2x the size of model parameters, adding 700 GB for the `{python} gpt3_params_b`B parameter case), learning rate scheduler state (the current position in the learning rate schedule including step count and warmup progress), random number generator state (to ensure reproducibility, the RNG state for each worker must be saved), and data loader state (which samples have been seen in the current epoch, enabling resumption without repeating or skipping samples).

[^fn-adam-optimizer]: **Adam (Adaptive Moment Estimation)**: The most widely used optimizer for deep learning, introduced in 2014. Adam maintains two exponential moving averages per parameter: the first moment (mean of gradients, for momentum) and second moment (mean of squared gradients, for adaptive learning rates). This 2x memory overhead becomes significant at scale: a `{python} gpt3_params_b`B parameter model requires 350 GB for weights and 700 GB for Adam state, totaling over 1 TB per checkpoint. Memory-efficient optimizers like Adafactor reduce this overhead by factorizing the second moment matrix, trading some convergence speed for 4-8x memory savings.

The total checkpoint size for a large model can reach several terabytes:

$$S_{checkpoint} = S_{params} + S_{optimizer} + S_{auxiliary}$$

For a `{python} gpt3_params_b`B parameter model with Adam optimizer:

$$S_{checkpoint} = 350 \text{ GB} + 700 \text{ GB} + \text{~1 GB} \approx 1.05 \text{ TB}$$

### Synchronous vs Asynchronous Checkpointing {#sec-storage-synchronous-vs-asynchronous-checkpointing-c3ca}

Checkpointing can be performed synchronously (training stops during checkpoint) or asynchronously (checkpoint happens in background while training continues).

**Synchronous checkpointing** is simpler but introduces overhead: training pauses, all workers save their state to storage, the coordinator confirms checkpoint complete, and training resumes.

The overhead depends on checkpoint size and storage bandwidth:

$$T_{checkpoint} = \frac{S_{checkpoint}}{B_{storage}}$$

For a 1 TB checkpoint writing to a parallel file system at 50 GB/s aggregate bandwidth, checkpoint time is 20 seconds. If checkpoints occur every 10 minutes, the overhead is:

$$\text{Overhead} = \frac{T_{checkpoint}}{T_{interval}} = \frac{20 \text{ s}}{600 \text{ s}} = 3.3\%$$

**Asynchronous checkpointing** overlaps checkpoint I/O with training computation. Training continues normally, a background thread copies model state to CPU memory, the background thread writes to storage while training proceeds, and the next checkpoint begins only after the previous completes.

Asynchronous checkpointing hides I/O latency but introduces complexity: memory overhead (must maintain a copy of model state for checkpointing while training modifies the live copy), consistency (must snapshot state at a consistent point between optimizer steps), and backpressure (if checkpoint I/O is slower than training, memory fills with pending checkpoints).

For most training runs, asynchronous checkpointing reduces overhead to near zero. @fig-async-checkpoint contrasts synchronous checkpointing (where training stalls during writes) with asynchronous approaches that overlap I/O with computation. The memory overhead (one additional copy of model state) is typically acceptable on systems with sufficient host memory.

::: {#fig-async-checkpoint fig-env="figure" fig-pos="htb" fig-cap="**Zero-Overhead Checkpointing**. (A) Synchronous checkpointing halts training (`Stop-the-World`), wasting valuable accelerator time on I/O. (B) Asynchronous checkpointing captures a snapshot in CPU memory and writes to storage in the background while training resumes immediately. This overlaps the massive write I/O with useful computation." fig-alt="Two timelines comparing checkpoint strategies. Top: synchronous with GPU idle during 2-minute write phases. Bottom: asynchronous with brief RAM snapshots triggering background storage writes while training continues uninterrupted."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9]
  \definecolor{ComputeColor}{RGB}{200,220,255}
  \definecolor{BlockColor}{RGB}{255,100,100}
  \definecolor{AsyncColor}{RGB}{150,255,150}
  \definecolor{DiskColor}{RGB}{240,240,240}

  \tikzset{
    block/.style={draw=black!60, thick, minimum height=0.8cm, align=center, font=\footnotesize},
    compute/.style={block, fill=ComputeColor, minimum width=2.5cm},
    stall/.style={block, fill=BlockColor, minimum width=2cm},
    snap/.style={block, fill=AsyncColor, minimum width=0.5cm},
    disk/.style={block, fill=DiskColor, pattern=north east lines, pattern color=gray!50}
  }

  % Synchronous Timeline
  \node[anchor=west, font=\bfseries] at (0, 3) {Synchronous Checkpointing};
  \node[compute] (S1) at (1.5, 2) {Training};
  \node[stall, right=0cm of S1] (S2) {Write (2m)};
  \node[compute, right=0cm of S2] (S3) {Training};
  \node[stall, right=0cm of S3] (S4) {Write (2m)};

  \draw[decorate, decoration={brace, amplitude=5pt}, red, thick] (S2.south west) -- (S2.south east) node[midway, below=5pt] {GPU Idle};

  % Asynchronous Timeline
  \node[anchor=west, font=\bfseries] at (0, 0) {Asynchronous Checkpointing};
  % GPU Line
  \node[anchor=east] at (0, -1) {GPU/CPU};
  \node[compute] (A1) at (1.5, -1) {Training};
  \node[snap, right=0cm of A1] (A2) {Cp};
  \node[compute, minimum width=4cm, right=0cm of A2] (A3) {Training};
  \node[snap, right=0cm of A3] (A4) {Cp};
  \node[compute, right=0cm of A4] (A5) {Training};

  % Storage Line
  \node[anchor=east] at (0, -2) {Storage};
  \node[disk, minimum width=2cm, below=0.2cm of A2, xshift=1cm] (D1) {Background Write};
  \node[disk, minimum width=2cm, below=0.2cm of A4, xshift=1cm] (D2) {Background Write};

  \draw[->, dashed] (A2.south) -- (D1.north west);
  \draw[->, dashed] (A4.south) -- (D2.north west);

  \node[right=0.2cm of A2, font=\footnotesize, align=left] {RAM Snapshot\\(Seconds)};

\end{tikzpicture}
```
:::

### Distributed Checkpointing for Sharded Models {#sec-storage-distributed-checkpointing-sharded-models-7369}

When models are sharded across multiple devices using parallelism strategies (examined in @sec-distributed-training-systems), each device holds only a portion of the model state. Checkpointing must coordinate across all devices to produce a consistent, complete checkpoint at the same logical training point.

**Coordination challenge.** The storage system must ensure all devices write their shards simultaneously, capturing a consistent snapshot of distributed model state. If devices checkpoint at different training steps, recovery would load inconsistent weights, corrupting the model. Barrier synchronization across all devices precedes checkpoint writes, ensuring global consistency.

**Bandwidth challenge.** With 128 devices each writing simultaneously, the storage system must sustain 128x single-device write rates during checkpoint windows. For terabyte-scale checkpoints, this aggregate bandwidth requirement often exceeds what a single parallel file system can provide, necessitating tiered storage architectures that use local NVMe for immediate writes with background migration to durable distributed storage.

**Data parallel optimization.** When training uses data parallelism (where multiple devices hold identical model replicas), checkpointing can save just one replica's state. Since all replicas are identical, saving one replica reduces storage by the data parallelism degree. This optimization is transparent to the storage system but significantly reduces aggregate checkpoint volume.

**Hybrid parallel checkpointing** combines coordination across multiple parallelism dimensions. For the hybrid approaches developed in @sec-distributed-training-systems, storage systems must handle coordinated writes from many devices with optimizations like saving only one data-parallel replica (since replicas are identical) to reduce aggregate checkpoint volume.

**Checkpoint aggregation** reduces storage overhead by having ranks aggregate their state before writing. Rather than 128 small writes, a few ranks collect and write large aggregated files. This improves storage efficiency (fewer small files) at the cost of additional memory and communication.

### Checkpoint Failure Handling {#sec-storage-checkpoint-failure-handling-b721}

Checkpoint storage must handle failures gracefully. Understanding failure modes and their mitigations is essential for reliable training at scale.

**Partial Write Failure:**

Storage fails after some but not all checkpoint shards are written. Without protection, recovery might load inconsistent state: some shards from the new checkpoint, some from the old.

*Solution: Atomic commit protocol.* All shards are first written to temporary locations with unique names (e.g., `checkpoint_step_10000.tmp.worker_0`). After all workers confirm write completion, a single atomic operation (rename or commit record) marks the checkpoint complete. Recovery reads only committed checkpoints, identified by the presence of the commit marker.

```text
1. Workers write: checkpoint_step_10000.tmp.worker_{0..N}
2. Coordinator verifies all workers complete
3. Coordinator writes: checkpoint_step_10000.COMPLETE
4. On recovery: only load checkpoints with .COMPLETE marker
```

**Storage Node Failure During Training:**

A storage server fails while training continues, potentially corrupting a checkpoint in progress.

*Solution: Replication.* With 3x replication, losing one storage node does not lose data. The storage system automatically redirects writes to surviving replicas. Checkpoint writes should require quorum acknowledgment (2 of 3 replicas) before confirming success.

**Checkpoint Corruption Detection:**

Silent data corruption (bit flips, media errors) can render checkpoints unusable without obvious failure signals.

*Solution: End-to-end checksums.* Each checkpoint shard includes a cryptographic hash of its contents. On recovery:

1. Load checkpoint shard
2. Compute checksum
3. Verify against stored checksum
4. If mismatch: fall back to previous checkpoint, alert operators

**Example failure scenario and recovery cost:**

1. Training at step 10,000, checkpoint initiated
2. Worker 0 writes shard successfully
3. Network partition: Workers 1-3 cannot reach storage
4. Coordinator timeout (30 seconds): checkpoint marked failed
5. Training continues from last good checkpoint (step 9,000)
6. Work from steps 9,000-10,000 must be repeated

*Cost calculation:*

$$\text{Wasted compute} = 1000 \text{ steps} \times 1.5 \text{ s/step} \times 1024 \text{ GPUs} \times \$0.001\text{/GPU-s} = \$1,536$$

This quantifies why checkpoint reliability matters: a $1,536 loss from one failed checkpoint motivates engineering investment in checkpoint robustness.

**Mitigating Checkpoint Storms**

When 4,000 GPUs attempt to write checkpoint data simultaneously at the end of an epoch, the aggregate instantaneous bandwidth demand can exceed the storage system's ingress capacity by orders of magnitude. This phenomenon, known as a **checkpoint storm**, creates a "thundering herd" effect that can trigger timeout errors or packet drops in the storage network.

The solution is **staggered checkpointing**. Rather than having all ranks write at $T=0$, the system introduces a deterministic delay based on rank:
$$
T_{write}(rank) = T_{start} + (rank \times \delta)
$$
where $\delta$ is a small delay (e.g., 10-100ms). This smoothes the write burst from a sharp spike into a plateau that matches the storage system's sustained throughput capability. While this slightly extends the total checkpoint time, it drastically reduces failure rates and tail latency.

**Straggler Mitigation:**

With 1000+ workers, one slow worker can delay checkpoint completion by minutes, blocking all training.

*Strategies:*

1. **Timeout with fallback**: If worker does not reach barrier in T seconds, assume failed, abort checkpoint, continue with previous

2. **Async checkpointing**: Workers checkpoint when ready; coordinator tracks which steps have full coverage

3. **Backup workers**: Redundant workers ensure N-k completion suffices (similar to gradient synchronization strategies)

### Optimal Checkpoint Interval: Young-Daly Formula {#sec-storage-optimal-checkpoint-interval-youngdaly-formula-07b1}

Checkpointing too frequently wastes time on I/O overhead. Checkpointing too infrequently risks losing large amounts of work to failures. The optimal interval balances these concerns.

The **Young-Daly formula**[^fn-young-daly] [@young1974first; @daly2006higher] provides the optimal checkpoint interval:

[^fn-young-daly]: **Young-Daly Formula**: Named after John Young (1974) who derived the first approximation and John Daly (2006) who provided the higher-order correction. The formula emerges from minimizing expected wasted work: checkpoint too rarely and failures lose significant progress; checkpoint too often and overhead accumulates. The square root relationship ($T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$) means that doubling checkpoint write speed allows 41% less frequent checkpoints, not 50%. This counterintuitive result has significant cost implications for storage architecture decisions.

::: {.callout-definition title="Young-Daly Checkpoint Interval"}

$$T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$$

where $T_{save}$ is the time to save a checkpoint and $MTBF$ is the mean time between failures.

:::

The optimal interval formula above provides the theoretical answer; the following worked example applies it to a realistic large-scale scenario to reveal the practical *checkpoint tax*.

::: {.callout-notebook title="The Checkpoint Tax"}
**Problem**: You are training a model on **1,000 GPUs**. The cluster fails (on average) every **4 hours**. It takes **5 minutes** to save a checkpoint. How often should you checkpoint?

**The Math (Young-Daly)**:

1.  **Checkpoint Cost ($C$)**: 5 minutes.
2.  **MTBF ($M$)**: 4 hours = 240 minutes.
3.  **Optimal Interval**: $\tau_{opt} = \sqrt{2 \cdot C \cdot M} = \sqrt{2 \cdot 5 \cdot 240} = \sqrt{2400} \approx \mathbf{49 \text{ minutes}}$.

**The Systems Conclusion**: You should checkpoint roughly every hour.

*   **Too Frequent (every 10 mins)**: You spend 50% of your time writing to disk.
*   **Too Rare (every 4 hours)**: You lose 2 hours of work on average per failure.
*   **Optimal**: You waste only ~10% of total time on overhead + recovery.
:::

This formula minimizes expected wasted work, accounting for both checkpoint overhead and work lost to failures. @fig-young-daly-storage plots the tradeoff: frequent checkpoints waste time on I/O; infrequent checkpoints lose more work during failures. The MTBF[^fn-mtbf] for GPU clusters decreases inversely with cluster size, making optimal checkpoint intervals surprisingly short for large-scale training.

::: {#fig-young-daly-storage fig-env="figure" fig-pos="htb" fig-cap="**The Checkpoint Tradeoff**. Plotting total training time overhead against checkpoint interval. Checkpointing too frequently (left side) incurs high I/O overhead. Checkpointing too rarely (right side) risks losing hours of work when failures occur. The optimal point $T_{opt}$ minimizes the sum of these costs." fig-alt="Line graph with three curves plotting cost against checkpoint interval. Green dashed line shows decreasing checkpoint overhead. Red dashed line shows increasing rework cost. Blue solid line shows U-shaped total cost with optimal point marked."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \begin{axis}[
    width=10cm, height=6cm,
    xlabel={Checkpoint Interval $\tau$},
    ylabel={Total Wasted Time (Cost)},
    xtick=\empty, ytick=\empty,
    axis lines=left,
    ymin=0, ymax=10,
    xmin=0.5, xmax=10,
    grid=major,
    legend pos=north east,
    legend style={draw=none}
  ]
    % Overhead Cost (Writing Checkpoints): C / tau
    \addplot[domain=1:10, samples=50, green!60!black, thick, dashed] {4/x};
    \addlegendentry{Checkpoint Overhead ($1/\tau$)}

    % Recovery Cost (Rework): M * tau / 2
    \addplot[domain=1:10, samples=50, red, thick, dashed] {0.2*x};
    \addlegendentry{Rework Cost ($\propto \tau$)}

    % Total Cost
    \addplot[domain=1:10, samples=50, blue, ultra thick] {4/x + 0.2*x};
    \addlegendentry{Total Cost}

    % Optimal Point
    \draw[dashed, black] (axis cs: 4.47, 0) -- (axis cs: 4.47, 1.79);
    \node[circle, fill=blue, inner sep=2pt] at (axis cs: 4.47, 1.79) {};
    \node[anchor=south west, font=\bfseries] at (axis cs: 4.47, 1.8) {Optimal $\tau$};

  \end{axis}

  \node[align=center, font=\footnotesize] at (5, -1.2) {Total Cost $ \approx \frac{C}{\tau} + \frac{\tau}{2M} $. Minimum at $\tau = \sqrt{2CM}$.};
\end{tikzpicture}
```
:::

[^fn-mtbf]: **MTBF (Mean Time Between Failures)**: A reliability metric representing the average time a system operates before experiencing a failure. For a single GPU, MTBF might be 30,000 hours (about 3.5 years). For a cluster, MTBF decreases inversely with component count: 1,024 GPUs with individual MTBF of 30,000 hours yield a cluster MTBF of roughly 29 hours, meaning failures occur almost daily. This inverse scaling explains why large-scale ML training is fundamentally a distributed systems problem: at 10,000 GPUs, MTBF drops to about 3 hours, making failure handling the dominant engineering challenge rather than an edge case.

Applying the Young-Daly formula to a real-world large-scale training scenario illustrates how these reliability constraints translate into practical checkpoint policies. @sec-distributed-training-systems-systems quantifies this tradeoff for a 1024-GPU cluster, showing how optimal intervals minimize aggregate wasted compute.

:::

### Incremental and Delta Checkpointing {#sec-storage-incremental-delta-checkpointing-eecd}

Full checkpoints save the complete model state regardless of how much has changed. For models where only a portion of parameters change significantly between checkpoints, incremental approaches reduce storage and I/O costs.

**Incremental checkpointing** saves only parameters that have changed since the last checkpoint. This requires tracking which parameters have been modified, maintaining a base checkpoint plus deltas, and periodically consolidating into a new full checkpoint.

For dense models where all parameters update every step, incremental checkpointing provides little benefit. For sparse models or fine-tuning scenarios where many parameters are frozen, the savings can be substantial.

**Delta compression** compresses the difference between consecutive checkpoints rather than the absolute values. If parameters change by small amounts each step, the delta is highly compressible. Techniques include XOR encoding (store the XOR of current and previous parameter values), floating-point prediction (predict next value from previous and store residual), and lossy compression (accept small errors in checkpoint for large compression ratios).

Practical compression ratios depend on model type and training dynamics. Sparse embedding updates in recommendation systems achieve 20-100x compression; dense LLM training typically sees 10-20x reduction.

@tbl-delta-checkpoint quantifies delta checkpointing effectiveness across model types: sparse embedding updates in recommendation systems achieve 20-100x compression, while dense LLM training typically sees 10-20x reduction.

| **Model Type**                        | **Full Checkpoint** | **Delta Size** | **Compression Ratio** |
|:------------------------------------|------------------:|-------------:|--------------------:|
| **LLM (dense updates)**               |                1 TB |      50-100 GB |                10-20x |
| **RecSys (sparse embedding updates)** |               10 TB |     100-500 GB |               20-100x |
| **Vision (fine-tuning)**              |               10 GB |         100 MB |                  100x |

: **Delta Checkpoint Compression by Model Type**: Sparse RecSys embedding updates achieve 20-100x compression; dense LLM updates achieve 10-20x; fine-tuning frozen models achieves 100x by saving only adapter weights. {#tbl-delta-checkpoint}

### Checkpoint Storage Architecture {#sec-storage-checkpoint-storage-architecture-0a1d}

The storage system for checkpoints must satisfy several requirements: high bandwidth (minimize checkpoint time), strong consistency (partial checkpoints must not appear complete), durability (checkpoints must survive storage failures), and low latency for reads (fast recovery after failures).

**Parallel file systems** (Lustre, GPFS) provide the highest bandwidth through striping and parallel I/O. A large Lustre deployment can deliver 100+ GB/s aggregate bandwidth, enabling TB-scale checkpoints in seconds. The tradeoff is operational complexity and cost.

**Object storage** (S3, GCS) provides durability and low cost but higher latency and lower bandwidth than parallel file systems. Object storage works well for infrequent checkpoints of moderate size (< 100 GB) but becomes bottleneck for TB-scale checkpoints every few minutes.

**Tiered storage** combines the benefits of both by writing checkpoints to fast parallel file system, asynchronously copying to durable object storage, deleting from fast tier after copy completes, and recovering from fast tier if available or otherwise from object storage.

This architecture provides both performance (fast writes) and durability (object storage backup) while managing cost (limited fast storage).

### Model-Type Checkpoint Considerations {#sec-storage-modeltype-checkpoint-considerations-d23e}

Checkpoint strategies vary significantly by model architecture. LLMs demand high-bandwidth distributed checkpointing for multi-hundred-GB models; vision models with smaller parameter counts can checkpoint to local storage; recommendation systems benefit from incremental updates that save only modified embeddings.

@tbl-checkpoint-strategies demonstrates how checkpoint strategies should be tailored to model size and training dynamics. **Large Language Models** checkpoint infrequently (relative to iteration count) because checkpoint size dominates. A 1 TB checkpoint at 100 GB/s still takes 10 seconds, during which thousands of dollars of GPU time is consumed. Asynchronous checkpointing and high-bandwidth storage are essential.

| **Model Type**                      | **Checkpoint Size** | **Typical Frequency**  | **Recommended Strategy**          |
|:----------------------------------|------------------:|:---------------------|:--------------------------------|
| **LLM (`{python} gpt3_params_b`B)** |       700 GB - 1 TB | Every 10-30 min        | Distributed async, tiered storage |
| **LLM (7B)**                        |            14-28 GB | Every 5-15 min         | Single-node, parallel FS          |
| **RecSys (10TB embeddings)**        |              10+ TB | Incremental every hour | Delta compression, streaming      |
| **Vision (ResNet-50)**              |              200 MB | Every epoch            | Simple sync, local + remote copy  |
| **Vision (ViT-22B)**                |           88-175 GB | Every 15-30 min        | Distributed, parallel FS          |

: **Checkpoint Strategies by Model Type**: 175B-parameter LLMs require distributed async checkpointing with tiered storage for 700 GB-1 TB checkpoints; RecSys with 10 TB embedding tables benefit from incremental delta compression; small vision models can use simple synchronous checkpointing. {#tbl-checkpoint-strategies}

**Recommendation Systems** present unique challenges due to massive embedding tables. A DLRM-style model[^fn-dlrm] [@naumov2019dlrm] might have 10 TB of embedding parameters but only 1 GB of dense MLP parameters. Incremental checkpointing of modified embeddings (which may be a small fraction of the table) provides order-of-magnitude savings over full checkpoints.

[^fn-dlrm]: **DLRM (Deep Learning Recommendation Model)**: An open-source recommendation model architecture published by Meta in 2019 that became the industry reference implementation. DLRM processes sparse categorical features (user ID, item ID) through embedding tables and dense numerical features through MLPs, combining them via feature interactions. The architecture's defining characteristic is embedding table dominance: production DLRM variants at Meta exceed 10 TB of embeddings with only gigabytes of dense parameters. This asymmetry drives unique storage requirements where embedding access patterns determine system performance.

**Vision Models** at typical scales (< 1 billion parameters) checkpoint easily. The entire checkpoint fits in a few GB, which can be written in under a second even to modest storage. The challenge is ensuring checkpoints are copied to durable storage before being deleted locally.

**Fine-tuning runs** of any model type can use delta checkpointing efficiently. Only the fine-tuned parameters (often < 1% of total for LoRA[^fn-lora]-style methods) need to be saved, reducing checkpoint size by 100x or more.

[^fn-lora]: **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning technique introduced by Microsoft in 2021 that freezes the original model weights and trains small low-rank matrices that modify layer outputs. A typical LoRA configuration adds only 0.1-1% additional parameters while achieving performance comparable to full fine-tuning. For storage, LoRA dramatically reduces checkpoint sizes: a 7B parameter model fine-tuned with LoRA requires only 10-100 MB of adapter weights rather than 14 GB of full weights. This enables storing hundreds of fine-tuned variants without the storage cost of full model copies.

## Feature Stores {#sec-storage-feature-stores-a514}

Checkpoint storage ensures that training can recover from failures, preserving weeks of computational progress in durable, recoverable form. But training is only half the ML lifecycle: once models are trained, serving them at production scale requires different storage infrastructure entirely. Where checkpoints optimize for write bursts and strong consistency, serving infrastructure must optimize for continuous low-latency reads across millions of concurrent requests.

Feature stores address this serving-time challenge, transforming raw data into ML-ready signals with strict latency guarantees. For recommendation systems, the dominant ML workload in production, feature stores are essential infrastructure: every user request triggers hundreds of feature lookups that must complete in milliseconds.

::: {.callout-definition title="Feature Store"}
***Feature Store*** is a centralized system that manages the transformation, storage, and serving of machine learning features. It unifies the offline training pipeline (batch features) with the online inference service (real-time features) to ensure **point-in-time correctness** and consistency between training and production environments.
:::

Production systems at companies like Meta, Google, and Netflix depend critically on feature stores to bridge the gap between offline training pipelines and real-time serving requirements.

This section develops the architecture and design principles of feature stores, with particular attention to why they matter enormously for recommendation systems while playing a smaller role for LLMs and vision models.

### Why Feature Stores Exist {#sec-storage-feature-stores-exist-daca}

The core problem feature stores solve is the training-serving gap: features computed during training must be reproducible during serving, but the contexts differ dramatically.

**During training**, features are computed in batch over historical data. There is no latency constraint: a training pipeline can spend hours computing features over millions of training examples. The priority is correctness and coverage.

**During serving**, features must be available within milliseconds for real-time inference. A recommendation system making personalized content suggestions has perhaps 50ms total latency budget; feature retrieval might consume 5-10ms of that budget. The priority is latency and availability.

Without a feature store, teams face painful tradeoffs: duplicate implementation (engineers write feature computation logic twice, once in batch Python or Spark for training and once in low-latency Java or C++ for serving, creating maintenance burden and inconsistency risk), point-in-time bugs (training features are computed with access to future data that would not be available at serving time, causing training-serving skew), and freshness problems (features computed in batch become stale and serving uses outdated information).

Feature stores solve these problems by providing unified feature computation (write feature logic once, execute in both batch and streaming contexts), point-in-time correctness (retrieve features as they were at a specific historical moment), low-latency serving (pre-computed features available with single-digit millisecond latency), and feature reuse (features computed once can be shared across many models).

### Feature Store Architecture {#sec-storage-feature-store-architecture-5328}

A feature store has two primary components: the offline store for training and the online store for serving. @fig-feature-store-architecture illustrates how features flow through batch pipelines for historical training data and through streaming pipelines for real-time inference.

::: {#fig-feature-store-architecture fig-env="figure" fig-pos="htb" fig-cap="**Feature Store System Design**. The dual-path architecture solving the training-serving skew problem. The offline path (top) processes batch data into a data warehouse for training. The online path (bottom) ingests streaming events into a low-latency key-value store for serving. A unified API ensures that features retrieved for training (historical) match those retrieved for inference (current), guaranteeing point-in-time correctness." fig-alt="Dual-path architecture diagram. Top path: batch sources to feature transform to offline store (data warehouse) to model training. Bottom path: stream sources to stream process to online store (Redis) to model serving. Dashed lines show shared logic."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm]
  \definecolor{OfflineColor}{RGB}{200,220,255}
  \definecolor{OnlineColor}{RGB}{255,220,200}
  \definecolor{StoreColor}{RGB}{230,230,230}

  \tikzset{
    proc/.style={draw, rectangle, rounded corners, minimum height=0.8cm, align=center, fill=white},
    store/.style={draw, cylinder, shape border rotate=90, aspect=0.25, minimum height=1cm, minimum width=1.5cm, align=center, fill=StoreColor},
    arrow/.style={->, >=stealth, thick}
  }

  % Offline Path (Top)
  \node[anchor=west] at (-1, 3) {\textbf{Offline Path (Training)}};
  \node[proc, fill=OfflineColor] (BatchSource) at (0, 2) {Batch\\Sources};
  \node[proc] (Transform) at (2.5, 2) {Feature\\Transform};
  \node[store, fill=OfflineColor!50] (OfflineStore) at (5.5, 2) {Offline Store\\(Data Warehouse)};
  \node[proc] (Training) at (8.5, 2) {Model\\Training};

  % Online Path (Bottom)
  \node[anchor=west] at (-1, -1) {\textbf{Online Path (Inference)}};
  \node[proc, fill=OnlineColor] (StreamSource) at (0, 0) {Stream\\Sources};
  \node[proc] (StreamProc) at (2.5, 0) {Stream\\Process};
  \node[store, fill=OnlineColor!50] (OnlineStore) at (5.5, 0) {Online Store\\(Redis/KV)};
  \node[proc] (Serving) at (8.5, 0) {Model\\Serving};

  % Edges
  \draw[arrow] (BatchSource) -- (Transform);
  \draw[arrow] (Transform) -- (OfflineStore);
  \draw[arrow] (OfflineStore) -- node[above, font=\scriptsize, align=center] {Historical\\Features} (Training);

  \draw[arrow] (StreamSource) -- (StreamProc);
  \draw[arrow] (StreamProc) -- (OnlineStore);
  \draw[arrow] (OnlineStore) -- node[above, font=\scriptsize, align=center] {Low Latency\\Features} (Serving);

  % Unified Logic Sync
  \draw[<->, dashed, thick, blue] (Transform) -- node[right, font=\scriptsize, align=center] {Shared Feature\\Logic} (StreamProc);
  \draw[->, dashed, thick, gray] (Transform) to[bend right=20] node[left, font=\tiny] {Sync} (OnlineStore);

\end{tikzpicture}
```
:::

#### Offline Store {#sec-storage-offline-store-1a3b}

The offline store contains the complete historical record of feature values, enabling training on any time window. It is optimized for throughput rather than latency.

**Storage technologies**: Data lakes (S3 + Parquet), data warehouses (BigQuery, Snowflake, Redshift), or specialized time-series databases.

**Data organization**: Features are stored with timestamps, enabling point-in-time queries:

```text
| **user_id** | **feature_name** | **value** | **timestamp** |
|:---|:---|---:|---:|
| 12345 | purchase_count_7d | 3 | 2024-01-15 00:00:00 |
| 12345 | purchase_count_7d | 5 | 2024-01-16 00:00:00 |
| 12345 | avg_session_length | 4.2 | 2024-01-15 00:00:00 |
```

**Point-in-time joins**: Training requires joining features to labels at the exact time the label event occurred. If a user clicked an ad at 2024-01-15 14:23:00, training needs the features that were available at 14:22:59, not the features computed later that day.

::: {.callout-warning title="Point-in-Time Correctness"}

Point-in-time correctness is the single most important property of feature stores. Using future information during training (a form of data leakage) produces models that perform well in offline evaluation but fail in production. This bug is insidious because metrics look great until deployment.

Example: A fraud detection model trained with features including "user_reported_fraud" appears to achieve 99% accuracy. But this feature is only populated after fraud is reported, which happens after the transaction being scored. The model learns to recognize already-reported fraud, not to predict future fraud.

:::

**Query patterns**: Offline queries retrieve features for millions of training examples in batch, using point-in-time correct joins as shown in @lst-pit-feature-query.

::: {#lst-pit-feature-query lst-cap="**Point-in-Time Correct Feature Join**: The temporal join condition ensures that only features available before the training event are used, preventing data leakage from future information."}
```{.sql}
SELECT
    labels.user_id,
    labels.item_id,
    labels.clicked,
    features.purchase_count_7d,
    features.avg_session_length
FROM training_labels labels
LEFT JOIN features
    ON labels.user_id = features.user_id
    -- Point-in-time correctness: only use features available before the event
    AND features.timestamp <= labels.event_time
    -- Limit to most recent feature version within window
    AND features.timestamp > labels.event_time - INTERVAL 1 DAY
```
:::

#### Online Store {#sec-storage-online-store-b9da}

The online store provides low-latency access to the most recent feature values for serving. It trades historical depth for speed.

**Storage technologies**: Key-value stores optimized for point lookups (Redis[^fn-redis], DynamoDB, Bigtable [@chang2008bigtable], Cassandra).

[^fn-redis]: **Redis (Remote Dictionary Server)**: An in-memory data structure store created by Salvatore Sanfilippo in 2009, now the most widely used in-memory database. Redis achieves sub-millisecond latency for key-value lookups by keeping all data in RAM, with optional persistence to disk. For feature stores, Redis serves as the online store providing 100,000+ lookups per second per instance with P99 latency under 1ms. The trade-off is cost: at approximately $10-15/GB/month for managed Redis versus $0.02/GB/month for object storage, organizations carefully select which features justify in-memory storage.

**Data organization**: Features are stored by entity key with only the most recent value, following the schema in @lst-online-store-schema.

::: {#lst-online-store-schema lst-cap="**Online Feature Store Key-Value Schema**: Each entity key maps to a flat dictionary of its most recent feature values, optimized for single-key point lookups at serving time."}
```{.json}
Key: user:12345
Value: {
    "purchase_count_7d": 5,
    "avg_session_length": 4.2,
    "last_updated": "2024-01-16T14:30:00Z"
}
```
:::

**Access patterns**: Online queries retrieve features for a single entity or small batch, as demonstrated in @lst-online-feature-lookup.

::: {#lst-online-feature-lookup lst-cap="**Low-Latency Online Feature Retrieval**: A point lookup retrieves the most recent feature values for a single entity, typically completing in under 5 milliseconds."}
```{.python}
features = online_store.get_features(
    entity_id="user:12345",
    feature_names=["purchase_count_7d", "avg_session_length"],
)
# Returns in < 5ms
```
:::

**Consistency with offline**: The online store must reflect the same feature values that the offline store would return for "now". Feature computation pipelines update both stores, with the offline store receiving the historical record and the online store receiving the current value.

#### Feature Computation Pipelines {#sec-storage-feature-computation-pipelines-2522}

Features are computed by pipelines that transform raw data into feature values. These pipelines can operate in batch or streaming mode.

**Batch pipelines** compute features over historical data, typically running daily or hourly. They are simpler to implement and debug but produce features that are always somewhat stale, as @lst-batch-feature-pipeline illustrates.

::: {#lst-batch-feature-pipeline lst-cap="**Batch Feature Computation**: A scheduled pipeline that computes aggregate features over historical data, trading freshness for simplicity and debuggability."}
```{.python}
# Batch feature: user's purchase count in last 7 days
@feature(schedule="daily")
def purchase_count_7d(user_id: str, date: datetime) -> int:
    return db.query(f"""
        SELECT COUNT(*) FROM purchases
        WHERE user_id = '{user_id}'
        AND purchase_date > '{date - timedelta(days=7)}'
    """)
```
:::

**Streaming pipelines** compute features from real-time event streams, providing fresh feature values within seconds of underlying events. They are more complex but essential for use cases where freshness matters (e.g., fraud detection, real-time recommendations), as shown in @lst-streaming-feature-pipeline.

::: {#lst-streaming-feature-pipeline lst-cap="**Streaming Feature Computation**: A real-time pipeline that updates feature values within seconds of each event, maintaining session-level state for low-latency serving."}
```{.python}
# Streaming feature: user's purchases in current session
@streaming_feature(source="purchase_events")
def session_purchase_count(event: PurchaseEvent, state: State) -> int:
    # Detect session boundary and reset counter
    if event.session_id != state.current_session:
        state.reset()  # Clear accumulated state from previous session
        state.current_session = event.session_id
    state.count += 1  # Increment within current session
    return (
        state.count
    )  # Updated within seconds of each purchase event
```
:::

**Lambda architecture**[^fn-lambda-architecture] combines both: batch pipelines provide accurate but stale features; streaming pipelines provide fresh but potentially approximate features. The serving layer merges results, preferring fresh streaming values when available.

[^fn-lambda-architecture]: **Lambda Architecture**: A data processing architecture proposed by Nathan Marz in 2011, combining batch processing (accurate but slow) with stream processing (fast but approximate). The name references functional programming's lambda calculus. For feature stores, lambda architecture maintains two pipelines: batch pipelines compute historically accurate features daily, while streaming pipelines update features in real-time from event streams. The serving layer merges both views, using streaming data for recent windows and batch data for longer historical windows. The alternative Kappa architecture uses only streaming, simplifying operations but requiring careful handling of reprocessing scenarios.

### Feature Lookup Latency Budget {#sec-storage-feature-lookup-latency-budget-2e90}

Online serving imposes strict latency constraints. The feature lookup must fit within the overall inference latency budget, expressed formally as a *feature lookup latency budget*.

::: {.callout-definition title="Feature Lookup Latency Budget"}

$$L_{feature} < L_{SLO} - L_{model} - L_{network} - L_{margin}$$

where:

- $L_{SLO}$ is the end-to-end latency SLO (e.g., 100ms)
- $L_{model}$ is model inference time (e.g., 20ms)
- $L_{network}$ is network round-trip time (e.g., 10ms)
- $L_{margin}$ is safety margin for variance (e.g., 20ms)

:::

Applying this formula to a concrete recommendation serving scenario reveals how tight the feature budget becomes once other components claim their share.

::: {.callout-example title="Feature Latency Budget for RecSys"}

A recommendation system has 100ms end-to-end SLO. Breaking down the budget:

- Model inference: 30ms (two-tower retrieval + ranking)
- Network overhead: 15ms (client-server round trips)
- Business logic: 10ms (filtering, deduplication)
- Safety margin: 15ms (P99 variance)
- **Available for features: 30ms**

With 30ms budget and 200 features to retrieve, each feature lookup must complete in 0.15ms average. This is achievable only with:

1. Batch lookups (one round-trip for all 200 features)
2. Features co-located in same key-value store
3. In-memory storage (Redis, not disk-backed)
4. Same-region deployment (< 1ms network)

:::

### Embedding Table Storage {#sec-storage-embedding-table-storage-a0fd}

Recommendation systems present a unique storage challenge: embedding tables containing vectors for millions or billions of entities. These tables can reach terabytes in size while requiring single-digit millisecond lookup latency. User embeddings for a billion users at 128 dimensions consume 512 GB; sparse ID features for 100 billion entities can reach 25 TB.

**Scale of embedding tables:**

@tbl-embedding-sizes reveals the scale challenge: user embeddings for a billion users at 128 dimensions consume 512 GB, while sparse ID features for 100 billion entities can reach 25 TB. These embedding tables cannot fit in memory on a single machine, requiring distributed storage strategies that balance latency, cost, and capacity.

| **Application**          | **Entities** | **Embedding Dim** | **Total Size** |
|:-----------------------|-----------:|----------------:|-------------:|
| **User embeddings**      |    1 billion |               128 |         512 GB |
| **Product embeddings**   |  100 million |               256 |         100 GB |
| **Ad embeddings**        |   10 million |               512 |          20 GB |
| **Sparse ID embeddings** |  100 billion |                64 |          25 TB |

: **Embedding Table Sizes at Production Scale**: User embeddings for 1 billion users at 128 dimensions consume 512 GB; sparse ID embeddings for 100 billion entities reach 25 TB. These scales exceed single-machine memory, requiring distributed sharding strategies. {#tbl-embedding-sizes}

**Storage strategies:**

**In-memory storage** (Redis, Memcached) provides the lowest latency (< 1ms) but highest cost. A 500 GB embedding table requires expensive high-memory instances. Suitable for frequently accessed embeddings with strict latency requirements.

**SSD-backed key-value stores** (RocksDB, Cassandra) provide 1-10ms latency at lower cost. The embedding is loaded from SSD on cache miss. Suitable for infrequently accessed embeddings or when cost constraints preclude in-memory storage.

**Tiered storage** keeps hot embeddings in memory and cold embeddings on SSD. Access patterns in recommendation systems are highly skewed: a small fraction of users and items account for most traffic. Keeping the top 10% of embeddings in memory while the remaining 90% are on SSD can provide good latency at reasonable cost.

**Embedding sharding** distributes large embedding tables across multiple servers:

$$\text{Shard ID} = \text{hash}(\text{entity\_id}) \mod N_{shards}$$

Each shard stores 1/N of the embeddings. Lookup requires determining the correct shard and querying that server. With consistent hashing, adding or removing shards rebalances minimal data.

### Model-Type Feature Store Requirements {#sec-storage-modeltype-feature-store-requirements-42ef}

Feature store importance varies dramatically by model type. Recommendation systems, fraud detection, and ad ranking require feature stores as critical infrastructure (millions of lookups per second with user, item, and contextual features). LLMs and vision models need them far less, primarily consuming prompts or images as direct inputs.

@tbl-feature-store-criticality reveals why feature stores matter enormously for recommendation systems while playing minimal roles for LLMs. **Why feature stores are critical for RecSys but not LLMs:**

| **Model Type**      | **Feature Store Criticality** | **Rationale**                                            |
|:------------------|:----------------------------|:-------------------------------------------------------|
| **RecSys**          | **Critical**                  | Millions of lookups/second, user/item features essential |
| **Fraud Detection** | **Critical**                  | Real-time features detect fraud patterns                 |
| **Ad Ranking**      | **Critical**                  | User context, ad features, bid signals                   |
| **Search Ranking**  | High                          | Query understanding, user history                        |
| **LLMs**            | Low                           | Minimal runtime features, prompt is the input            |
| **Vision**          | Low-Medium                    | Optional context features, mainly model input            |
| **Speech**          | Low                           | Audio input, minimal runtime features                    |

: **Feature Store Criticality by Application**: RecSys, fraud detection, and ad ranking require feature stores as critical infrastructure (millions of lookups/second); LLMs and vision models need them minimally since prompts and images serve as direct inputs. {#tbl-feature-store-criticality}

Recommendation systems make predictions about user-item interactions, requiring features about both the user and items. At serving time, the system must retrieve:

- User features: Demographics, historical behavior, session context
- Item features: Category, popularity, freshness
- Context features: Time of day, device, location
- Cross features: User-item affinity scores, collaborative filtering signals

These features cannot be derived from the input alone (unlike LLM prompts, which contain all necessary information). The feature store is the only way to provide this information to the model at serving time.

LLMs, by contrast, receive their input as a prompt. The prompt contains all information the model needs to generate a response. There are no external features to look up. The "feature store" for an LLM is simply the tokenizer and any prompt templates, not a database of entity features.

### Feature Store Platforms {#sec-storage-feature-store-platforms-b0db}

Several platforms provide feature store functionality, each with different tradeoffs:

**Open source:**

- **Feast**: Most popular open-source feature store. Supports multiple backends (Redis, DynamoDB, BigQuery). Provides point-in-time joins, feature versioning.
- **Hopsworks**: Feature store with MLOps integration. Strong support for feature pipelines and versioning.

**Cloud-managed:**

- **Vertex AI Feature Store (GCP)**: Managed service with BigQuery integration. Auto-scaling online serving.
- **SageMaker Feature Store (AWS)**: Integrated with SageMaker ML workflow. S3 offline store, DynamoDB-backed online store.
- **Azure ML Feature Store**: Part of Azure ML ecosystem. Supports feature materialization and serving.

**Enterprise:**

- **Tecton**: Enterprise feature platform built by Feast creators. Sophisticated streaming feature support.
- **Databricks Feature Store**: Integrated with Databricks lakehouse. Unity Catalog integration for governance.

**Build vs buy considerations:**

@tbl-feature-store-buildbuy weighs the costs and benefits of building versus adopting an existing platform. For most organizations, starting with an open-source solution (Feast) or cloud-managed service provides faster time-to-value than building custom infrastructure. Custom feature stores become worthwhile at massive scale (billions of features, millions of QPS) where platform limitations become constraints.

| **Factor**           | **Build In-House**      | **Use Platform**       |
|:-------------------|:----------------------|:---------------------|
| **Customization**    | Full control            | Limited by platform    |
| **Development cost** | High initial investment | Lower initial cost     |
| **Operations**       | Requires dedicated team | Managed by vendor      |
| **Scale**            | Requires expertise      | Built-in scaling       |
| **Integration**      | Custom to stack         | May require adaptation |

: **Feature Store Build vs. Buy Tradeoffs**: Building in-house provides full customization at high development cost; platforms offer lower initial cost with managed operations but limited flexibility. Most organizations start with open-source (Feast) or cloud-managed services. {#tbl-feature-store-buildbuy}

The feature store provides a consistent view of *structured* data. However, modern AI systems, particularly Large Language Models, require efficient access to *unstructured* data represented as dense vectors. This requirement has given rise to a new storage primitive.

## Vector Databases and Retrieval Systems {#sec-storage-vector-databases-retrieval-systems-12ab}

While Feature Stores manage the "structured context" (user history, item counts), Vector Databases manage the "semantic context" (document meaning, image content). They serve as the **Long-Term Memory** for AI systems, enabling Retrieval-Augmented Generation (RAG) and semantic search.

::: {.callout-definition title="Vector Database"}
**Vector Database**: A specialized database optimized for storing and querying high-dimensional vectors. Unlike relational databases (exact match) or search engines (keyword match), vector databases perform **Approximate Nearest Neighbor (ANN)** search to find vectors semantically similar to a query vector.
:::

### The System Challenge: ANN at Scale {#sec-storage-ann-at-scale-34cd}

The core engineering challenge of a Vector DB is the trade-off between **Recall**, **Latency**, and **Memory**. An exact nearest neighbor search (k-NN) requires comparing the query against every vector in the database ($O(N)$), which is prohibitively slow for billion-scale datasets.

To achieve sub-10ms latency, systems use Approximate Nearest Neighbor (ANN) algorithms that build index structures to prune the search space:

*   **IVF (Inverted File Index)**: Partitions the vector space into Voronoi cells. Search is restricted to the cells closest to the query.
*   **HNSW (Hierarchical Navigable Small World)**: Builds a multi-layer graph. Search starts at the top layer (coarse navigation) and descends to finer layers for precision. HNSW offers superior latency/recall trade-offs but consumes significantly more memory for the graph structure.
*   **DiskANN**: Offloads the raw vectors to SSDs while keeping a compressed index in RAM, enabling datasets larger than system memory.

### Distributed Consistency and Freshness {#sec-storage-vector-consistency-56ef}

Unlike static training datasets, vector databases in production are often mutable. New documents are ingested, embedded, and indexed continuously.

*   **Real-time Indexing**: Building an HNSW graph is computationally expensive. Systems often use a **Log-Structured Merge (LSM)** tree approach: new vectors go into a small, fresh in-memory index, while background processes merge them into the large, immutable on-disk index.
*   **Consistency**: For RAG systems, "read-your-writes" consistency is critical. If a user uploads a document, they expect the AI to be able to answer questions about it immediately. This requires tight coupling between the embedding pipeline and the serving index.

## Model Registries and Artifact Management {#sec-storage-model-registries-artifact-management-c193}

The preceding sections addressed storage for different phases of the ML lifecycle: training data infrastructure provides the raw material, checkpoint storage preserves model state during training for fault tolerance, and feature stores deliver runtime data for serving. But what about the trained models themselves? Training produces model weights, but deploying those weights to production requires its own infrastructure: storage, versioning, and governance for the trained artifacts.

Model registries provide the storage, versioning, and governance layer between training completion and production deployment. They answer questions that become critical at scale: Which model version is currently in production? What training data and hyperparameters produced this model? Who approved this model for deployment?

### Model Registry Architecture {#sec-storage-model-registry-architecture-4cd6}

A model registry stores and organizes model artifacts with associated metadata, enabling teams to manage the lifecycle of models from experimentation through deployment to retirement.

**Core components:**

**Artifact storage** holds the actual model files: weights, configurations, preprocessing artifacts, and any ancillary files needed for inference. Storage backends range from simple object storage (S3, GCS) to specialized artifact stores (Artifactory, Nexus).

**Metadata store** maintains information about each model version: training parameters, performance metrics, data lineage, and deployment status. This is typically a database (PostgreSQL, MySQL) or document store (MongoDB).

**Versioning system** tracks model versions with semantic versioning or auto-incrementing identifiers. Each version is immutable: once registered, a model version cannot be modified.

**Access control** governs who can register, read, and promote models. Different teams may have different permissions (data scientists can register, MLOps can promote to production, only approved models can be deployed).

### Model Versioning {#sec-storage-model-versioning-2d7c}

Model versioning differs from code versioning in important ways:

**Artifacts are large and binary.** Git handles text diffs efficiently but struggles with multi-GB model files. Model registries use content-addressable storage, storing each unique artifact once regardless of how many versions reference it.

**Versions may not be sequential.** Teams often run multiple experiments in parallel, producing model versions that branch from different starting points. The registry must handle non-linear version histories.

**Metadata is as important as artifacts.** Knowing what hyperparameters, training data, and code version produced a model is essential for debugging and reproducibility.

A typical model version record includes the metadata shown in @lst-model-registry-record.

::: {#lst-model-registry-record lst-cap="**Model Version Provenance Metadata**: A registry record capturing the full provenance chain for a model version, including artifact locations, training configuration, evaluation metrics, and lineage links to parent models and experiments."}
```{.yaml}
model_name: "product_recommender"
version: "v2.3.1"
status: "production"
registered_at: "2024-01-15T10:23:45Z"
registered_by: "ml-team-ci"

artifacts:
  model_weights: "s3://models/product_recommender/v2.3.1/model.pt"
  config: "s3://models/product_recommender/v2.3.1/config.yaml"
  tokenizer: "s3://models/product_recommender/v2.3.1/tokenizer/"

training:
  framework: "pytorch"
  framework_version: "2.1.0"
  training_data: "s3://datasets/product_interactions/2024-01-01/"
  training_data_hash: "sha256:a3b4c5..."
  hyperparameters:
    learning_rate: 0.001
    batch_size: 256
    epochs: 50
  training_job_id: "train-20240115-001"
  training_duration_hours: 12.5

metrics:
  validation_accuracy: 0.847
  validation_loss: 0.312
  auc: 0.923

lineage:
  parent_model: "product_recommender:v2.2.0"
  code_commit: "git:abc123"
  experiment_id: "exp-2024-01-15-hyperopt"
```
:::

### Reproducibility and Lineage Tracking {#sec-storage-reproducibility-lineage-tracking-6178}

Reproducibility in ML requires tracking not just the model weights but the entire provenance chain: what data, code, and environment produced this model?

**Data lineage** tracks which datasets were used for training and validation. This includes:

- Dataset versions or snapshot identifiers
- Data preprocessing pipeline versions
- Any filtering or sampling applied
- Hash of the actual data used

**Code lineage** links models to the code that produced them:

- Git commit hash of training code
- Container image digest for training environment
- Framework versions and dependencies

**Environment lineage** captures the computational environment:

- Hardware (GPU type, count)
- Software (CUDA version, Python version, package versions)
- Random seeds used

**Full lineage enables:**

1. **Debugging**: When a model behaves unexpectedly, trace back to exact training conditions

2. **Auditing**: Demonstrate to regulators exactly how a model was trained

3. **Reproduction**: Train identical model from lineage information

4. **Comparison**: Understand why two model versions differ

These capabilities depend on the assumption that lineage information suffices for exact reproduction, but hardware non-determinism complicates *reproducibility in practice*.

::: {.callout-note title="Reproducibility in Practice"}

Perfect reproducibility in ML is difficult due to non-determinism in GPU operations, floating-point associativity, and framework internals. Lineage tracking enables approximate reproduction: training with the same data, code, and hyperparameters typically produces a model with similar (but not bit-identical) performance.

:::

### Model Lifecycle Stages {#sec-storage-model-lifecycle-stages-1b55}

Models progress through lifecycle stages, with the registry tracking current stage and stage transitions:

**Development**: Experimental models under active iteration. Many versions may be created and discarded. No quality guarantees.

**Staging**: Candidate models undergoing evaluation. Limited access, subjected to validation tests. Models that pass move to production; those that fail return to development.

**Production**: Approved models serving live traffic. Strict change control, monitoring requirements. Only promoted models reach production.

**Archived**: Retired models no longer serving traffic but retained for reference, auditing, or rollback. May be moved to cold storage.

**Deprecated**: Models scheduled for removal. Alerts generated if still accessed. Fully deleted after retention period.

```text
Development --> Staging --> Production --> Archived
     ^              |             |
     |              v             v
     +---- (failed) +-- Deprecated -> Deleted
```

### Artifact Storage Considerations {#sec-storage-artifact-storage-considerations-2c32}

Model artifacts range from megabytes (small classifiers) to terabytes (large foundation models), requiring different storage strategies.

**Small models (< 1 GB)**: Object storage (S3, GCS) with standard redundancy. Download to inference servers is fast; model can be fetched on each server start.

**Medium models (1-100 GB)**: Object storage with regional caching. Pre-deploy models to inference servers to avoid startup latency. Consider compression for network transfer.

**Large models (> 100 GB)**: Distributed storage with parallel download. Sharded across multiple files for parallel access. May require local NVMe for serving latency requirements.

Storage cost optimization strategies for model artifacts balance size reduction against access complexity. Compression achieves 2-5x size reduction at the cost of CPU overhead during loading; tiered storage moves old versions to cold storage but increases retrieval latency:

@tbl-artifact-storage summarizes these optimization strategies, from compression achieving 2-5x size reduction to tiered storage that moves old versions to cold storage at the cost of retrieval latency.

| **Strategy**             | **Benefit**                   | **Tradeoff**                      |
|:-----------------------|:----------------------------|:--------------------------------|
| **Compression**          | 2-5x size reduction           | CPU overhead on load              |
| **Deduplication**        | Shared layers stored once     | Complexity in artifact management |
| **Tiered storage**       | Cold storage for old versions | Retrieval latency                 |
| **Differential storage** | Store only changed weights    | Requires base model + diffs       |

: **Model Artifact Storage Optimization**: Compression achieves 2-5x size reduction at the cost of CPU load time; deduplication stores shared layers once across model variants; tiered storage moves old versions to cold storage while maintaining retrieval capability. {#tbl-artifact-storage}

### Model Registry Platforms {#sec-storage-model-registry-platforms-8746}

Several platforms provide model registry functionality with varying levels of integration and enterprise features:

**MLflow Model Registry**[^fn-mlflow] [@zaharia2018mlflow]: Open source, widely adopted. Integrates with MLflow tracking. Supports model stages, versioning, and deployment integration. Backed by file system or database storage.

[^fn-mlflow]: **MLflow**: An open-source MLOps platform created by Databricks in 2018, now the most widely adopted experiment tracking and model management tool. MLflow provides four components: Tracking (logging experiments), Projects (packaging code), Models (model packaging format), and Registry (model versioning and deployment). Its Python-first API integrates with all major ML frameworks. The model registry uses a database backend (SQLite for development, PostgreSQL or MySQL for production) for metadata and object storage for artifacts. Widely adopted with millions of downloads, MLflow has become the de facto standard for experiment tracking in non-enterprise ML teams.

**Weights & Biases Model Registry**: Part of W&B platform. Strong experiment tracking integration. Artifact versioning with lineage.

**Vertex AI Model Registry (GCP)**: Managed service with Vertex AI integration. Model versioning, deployment to endpoints.

**SageMaker Model Registry (AWS)**: Part of SageMaker MLOps. Model groups, versions, approval workflows.

**Azure ML Model Registry**: Part of Azure ML. Model versioning, deployment, monitoring integration.

@tbl-registry-platforms compares platforms across key dimensions. MLflow offers the widest adoption and open-source flexibility; cloud-managed options provide tighter integration with their respective ecosystems:

| **Platform**  | **Open Source** | **Deployment Integration** | **Lineage** | **Approval Workflows** |
|:------------|:--------------|:-------------------------|:----------|:---------------------|
| **MLflow**    | Yes             | Via plugins                | Basic       | Basic                  |
| **W&B**       | No              | Limited                    | Strong      | Limited                |
| **Vertex AI** | No              | Native GCP                 | Good        | Yes                    |
| **SageMaker** | No              | Native AWS                 | Good        | Yes                    |
| **Azure ML**  | No              | Native Azure               | Good        | Yes                    |

: **Model Registry Platform Comparison**: MLflow offers open-source flexibility with the widest adoption; cloud-managed options (Vertex AI, SageMaker, Azure ML) provide native deployment integration and approval workflows but limit portability. {#tbl-registry-platforms}

### Registry Design Patterns {#sec-storage-registry-design-patterns-154f}

Effective model registry usage follows established patterns:

**Pattern: Immutable versions.** Once registered, a model version cannot be modified. Updates create new versions. This ensures reproducibility and enables safe rollback.

**Pattern: Promotion gates.** Models must pass automated tests before promotion to production: validation metrics above threshold, bias tests passed, latency requirements met. Human approval may be required for certain model types.

**Pattern: Canary metadata.** Model versions include canary configuration: what percentage of traffic to receive initially, what metrics to monitor, automatic rollback conditions.

**Pattern: Model cards.** Each production model has a model card documenting intended use, limitations, performance characteristics, and ethical considerations. Required for governance and user understanding.

**Pattern: Retention policies.** Old model versions are automatically archived or deleted based on policy: keep last N versions, keep all versions newer than date, never delete production versions.

## Case Studies {#sec-storage-case-studies-dcd1-dcd1}

The storage system principles developed throughout this chapter manifest differently across organizations depending on their dominant workloads, scale, and infrastructure maturity. Four organizations illustrate these variations: Google for LLM training (sequential access, checkpoint-intensive), Meta for recommendation serving (random access, latency-critical), Tesla for vision data pipelines (bandwidth-intensive, quality-focused), and Spotify for hybrid workloads that balance multiple access patterns. As you read each case, watch for recurring themes: how scale forces architectural redesign of systems that worked at smaller sizes, how workload characteristics drive storage tier selection and optimization, and why data quality infrastructure proves as important as raw storage capacity.

This section examines these four case studies to illustrate how leading ML organizations have designed storage systems for their specific requirements.

### Google Colossus: Storage for LLM Training {#sec-storage-google-colossus-storage-llm-training-ed0e}

Google's Colossus file system, the successor to the original Google File System (GFS), demonstrates storage architecture optimized for massive-scale sequential workloads including LLM training.

**Scale and requirements:**

Google trains models including PaLM [@chowdhery2022palm] (540 billion parameters) and Gemini [@gemini2023] across thousands of TPUs. Training data for these models spans tens of terabytes of tokenized text, requiring sustained read throughput of hundreds of GB/s across the training cluster. Checkpoints for the largest models exceed 1 TB and must be saved within minutes to minimize training interruption.

**Architecture decisions:**

**Distributed metadata.** Unlike GFS's single master, Colossus distributes file system metadata across multiple servers (Curators). This eliminates the metadata bottleneck that would otherwise limit operations to ~10,000/second, enabling the high-frequency checkpoint writes and metadata operations that large-scale training requires.

```{python}
#| label: erasure-coding-calc
#| echo: false

# Reed-Solomon erasure coding overhead
data_fragments = 9
parity_fragments = 3
total_fragments = data_fragments + parity_fragments
storage_overhead = total_fragments / data_fragments
storage_overhead_str = f"{storage_overhead:.2f}"
```

**Erasure coding.** Colossus uses Reed-Solomon erasure coding [@reed1960polynomial] rather than simple replication. A typical configuration stores `{python} data_fragments` data fragments plus `{python} parity_fragments` parity fragments, achieving durability equivalent to 3x replication while using only `{python} storage_overhead_str`x storage. For petabyte-scale training data, this dramatically reduces storage costs.

**D (Disk) servers.** Colossus separates the storage layer into D servers that manage physical disks. This abstraction enables flexible placement of data across disk types (HDD for cold data, SSD for hot data) without changing the file system interface.

**Integration with TPU architecture:**

Colossus integrates deeply with TPU training infrastructure:

- Data is striped to enable parallel reads from many D servers simultaneously
- TPU hosts run Colossus clients that prefetch training data during computation
- Checkpoint writes use dedicated bandwidth allocation to prevent interference with training data reads

**Lessons** include that at Google scale, the file system must be redesigned rather than just scaled (GFS concepts like large blocks, append-optimized, and single namespace remain valid, but implementation must be distributed), that erasure coding is essential for cost-effective storage at petabyte scale, and that tight integration between storage and compute systems enables efficiencies impossible with generic storage.

### Meta Feature Store: Recommendation at Scale {#sec-storage-meta-feature-store-recommendation-scale-34b3}

Meta's recommendation systems serve billions of users across Facebook, Instagram, and WhatsApp, requiring feature store infrastructure that handles trillions of feature lookups daily while maintaining single-digit millisecond latency.

**Scale and requirements:**

- Billions of users, each with hundreds of features
- Trillions of feature lookups per day
- P99 latency requirement: < 10ms
- Features updated continuously from user activity streams
- Embedding tables exceeding 10 TB

**Architecture decisions:**

**Hybrid online store.** Meta's feature store uses a tiered architecture:

- **L1 cache**: Process-local cache on serving machines. Sub-millisecond access for hot features.
- **L2 cache**: Distributed cache (Memcached) for warm features. 1-2ms access.
- **Persistent store**: Distributed key-value store (similar to RocksDB) for cold features. 5-10ms access.

This tiering exploits the power-law distribution of feature access: the most active 1% of users account for a disproportionate share of requests.

**Streaming feature computation.** Features are computed by streaming pipelines (similar to Flink) processing user activity in real-time. A user's "items_viewed_last_hour" feature updates within seconds of each view event, enabling recommendations that reflect immediate interests.

**Embedding table sharding.** User embeddings are sharded across thousands of servers using consistent hashing. Each server holds a fraction of the embedding table in memory. Lookup involves hashing the user ID to determine the shard, then a single network hop to retrieve the embedding.

**Point-in-time correctness.** The offline feature store maintains timestamped feature values, enabling training data generation with correct historical features. Training pipelines join labels (user interactions) with features as they existed at interaction time, preventing data leakage.

**Lessons** include that feature store design is dominated by the access pattern (billions of point lookups per second require in-memory storage for hot data), that streaming feature computation is essential for recommendation freshness, that tiered caching exploits access pattern skew to provide good latency at reasonable cost, and that point-in-time correctness is a non-negotiable requirement that is extremely difficult to retrofit.

### Tesla: Video Data Pipeline for Vision Training {#sec-storage-tesla-video-data-pipeline-vision-training-a7ca}

Tesla's Autopilot and Full Self-Driving systems are trained on video data collected from millions of vehicles, presenting unique storage challenges for vision model training at scale.

**Scale and requirements:**

- Fleet of millions of vehicles continuously collecting video
- Petabytes of video ingested daily (before filtering)
- Training datasets of selected clips reaching hundreds of TB
- Video must be decoded, augmented, and streamed to training GPUs
- Data selection is as important as data quantity

**Architecture decisions:**

**Hierarchical data selection** recognizes that not all collected video is valuable for training. Tesla's pipeline implements progressive filtering through on-vehicle filtering (neural networks on vehicle hardware identify interesting scenarios like edge cases or novel situations), upload filtering (only flagged clips are uploaded over cellular or WiFi), offline filtering (more sophisticated models further filter uploaded data), and labeling queue (high-value clips are prioritized for human labeling).

This filtering reduces storage requirements by orders of magnitude while focusing training on the most valuable data.

**Object storage with intelligent tiering** stores raw video in object storage with automatic tiering. Recent uploads reside in hot storage for immediate processing, processed clips move to warm storage, and archived raw footage resides in cold storage for potential re-processing.

**Custom data format** was developed by Tesla specifically for ML training. The format uses temporal compression aware of training access patterns (random frame access), provides multiple resolution variants for different training stages, and embeds sensor metadata (GPS, IMU, camera calibration) in the format.

**Distributed video decoding.** Video decoding is CPU-intensive. Tesla's data pipeline distributes decoding across many CPU workers, with decoded frames streamed to GPUs. This decouples decode throughput from GPU count.

**Lessons** include that for video data, the storage problem is inseparable from the data selection problem (storing everything is infeasible and intelligent filtering is essential), that custom data formats can provide significant efficiency gains when standard formats impose unacceptable overhead, and that decoding and augmentation pipelines require dedicated compute and cannot be an afterthought.

### Spotify: Hybrid ML Platform Storage {#sec-storage-spotify-hybrid-ml-platform-storage-31df}

Spotify combines recommendation systems (user-music matching) with audio understanding (content analysis), requiring storage infrastructure that serves both workload types efficiently.

**Scale and requirements:**

- Hundreds of millions of users with listening history
- Tens of millions of tracks, podcasts, and audiobooks
- Recommendation serving at millions of QPS
- Audio analysis models processing newly uploaded content
- Features spanning user behavior and audio content

**Architecture decisions:**

**Unified feature platform.** Spotify's feature platform serves both recommendation and audio ML:

- User features: Listening history, preferences, demographics
- Content features: Audio embeddings, genre classification, tempo
- Contextual features: Time of day, device, location

A single feature store serves all models, enabling feature reuse across teams.

**Content embedding pipeline.** New audio content flows through embedding pipelines:

1. Upload to object storage (GCS)
2. Audio analysis models extract embeddings
3. Embeddings written to feature store
4. Content available for recommendation within hours of upload

**Batch and streaming feature computation.** Some features are batch-computed (user's "top genres last month"), while others are streaming ("songs played this session"). Both types flow into the same feature store with appropriate freshness guarantees.

**GCP-native storage stack.** Spotify runs primarily on Google Cloud:

- BigQuery for offline feature store and analytics
- Bigtable for online feature serving
- GCS for raw data and model artifacts
- Dataflow for feature computation pipelines

This cloud-native approach reduces operational burden while providing the scale needed for Spotify's workloads.

**Model versioning for A/B testing.** Spotify runs continuous A/B tests with many model variants serving traffic simultaneously. The model registry tracks which variants are in each test, enabling analysis of model performance in production.

**Lessons** include that different ML workloads (recommendation and content understanding) can share storage infrastructure when designed with flexibility, that cloud-managed services provide operational simplicity at the cost of some customization, and that feature platforms serving multiple teams create significant organizational value through feature reuse.

### Cross-Cutting Themes {#sec-storage-crosscutting-themes-93c2}

Several themes emerge across these case studies:

**Scale requires architectural adaptation.** Solutions that work at small scale fail at large scale. Each organization redesigned storage architecture as scale increased, rather than simply adding capacity.

**Workload characteristics drive design.** LLM training (sequential, checkpoint-heavy) requires different storage than recommendation (random access, latency-critical). Organizations must understand their workload mix.

**Data quality infrastructure is as important as data quantity.** Tesla and Google invest heavily in data selection and quality, recognizing that more data is not always better data.

**Feature stores are production-critical for recommendation.** Meta and Spotify treat feature stores as tier-1 infrastructure with the same reliability requirements as serving systems.

**Cloud vs on-premise tradeoffs remain.** Google builds custom infrastructure; Spotify uses cloud services. Both approaches work; the choice depends on scale, expertise, and strategic priorities.

## Fallacies and Pitfalls {#sec-storage-fallacies-pitfalls-a786-a786}

Even organizations operating at the scales described above encounter recurring misconceptions. The following fallacies and pitfalls represent hard-won lessons from production ML systems, where assumptions from traditional computing fail to transfer to ML workloads.

**Fallacy:** ***Object storage latency is acceptable for training data.***

Object stores like S3 and GCS offer seemingly infinite capacity at low cost, leading teams to treat them as drop-in replacements for file systems. However, first-byte latency of 50-200ms for object storage versus 1-10ms for distributed file systems means that small-batch access patterns incur unacceptable overhead. A training pipeline reading 1000 small files sequentially from S3 spends more time waiting for first bytes than transferring data. As @sec-storage-object-storage-scale-188b demonstrates, object storage remains the most cost-effective option for large datasets when pipelines are designed around its characteristics: large sequential reads (multi-MB), extensive prefetching, and local caching. Treating object storage as a transparent NFS replacement yields training throughput 3-5x lower than achievable with proper access pattern optimization.

**Pitfall:** ***Sizing checkpoint storage by capacity rather than bandwidth.***

Organizations provision checkpoint storage based on model size: "Our model is 500GB, so we need 5TB for 10 checkpoints." This ignores the critical constraint identified in @sec-storage-optimal-checkpoint-interval-youngdaly-formula-07b1: bandwidth during checkpoint writes. A 500GB checkpoint written to NFS at 1 GB/s takes 8+ minutes. For a 1000-GPU training job with MTBF of 4 hours, the Young-Daly optimal interval is approximately 23 minutes: $T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$. Spending 8 minutes (35% of the interval) on checkpoint writes is unacceptable overhead. The same checkpoint written to parallel file system at 100 GB/s takes 5 seconds (0.4% overhead). Capacity planning must start with bandwidth requirements: "We need to write 500GB in under 30 seconds" leads to radically different architecture than "we need 5TB of capacity."

**Fallacy:** ***Checkpoint storage is a solved problem.***

Modern frameworks checkpoint transparently, creating the illusion that checkpointing "just works." This masks the coordination overhead that dominates checkpoint time at scale. With 1000 GPUs each writing 500MB, the aggregate checkpoint is 500GB. As @sec-storage-distributed-checkpointing-sharded-models-7369 explains, achieving this requires all ranks reaching the checkpoint barrier synchronously, coordinated storage access avoiding hotspots, verification that all shards completed successfully, and atomic update of the "latest checkpoint" pointer. The actual wall-clock time often exceeds theoretical write time by 2-3x due to stragglers, coordination, and verification. Organizations that benchmark single-node checkpointing (5 seconds) and linearly extrapolate to distributed settings (expecting 5 seconds) are consistently surprised when distributed checkpoint overhead yields 12-15 second wall-clock times.

**Pitfall:** ***Assuming training data locality exists.***

Traditional storage optimization assumes data locality: frequently accessed data should be cached near compute. For training data, this assumption fails fundamentally. As @sec-storage-data-locality-principles-7419 demonstrates, each training sample is accessed once per epoch, and randomized shuffling ensures no sample is accessed more frequently than others. Caching strategies designed for hot data provide zero benefit because the working set equals the entire dataset. Prefetching strategies that predict future access based on past access fail because shuffling makes access unpredictable. Storage systems designed with "hot tier" (SSD) and "cold tier" (HDD) architectures find all training data equally tepid: every sample has identical access frequency. Effective training storage must optimize for sequential bandwidth (200+ GB/s aggregate) and prefetch depth (64+ batches ahead) rather than cache hit rates.

**Fallacy:** ***Feature store latency does not matter because serving latency is dominated by model inference.***

Teams assume that feature lookup latency is negligible compared to model inference time. For LLMs, this is true: model inference of 50-500ms dwarfs any feature lookup. For recommendation systems, this is catastrophically false. As @sec-storage-feature-lookup-latency-budget-2e90 explains, a recommendation model inference might take 2ms. With 100 features requiring 50th-percentile lookup of 1ms each, feature retrieval consumes 100ms, dominating the serving budget by 50x. Production feature stores for recommendation systems must achieve sub-millisecond P50 and single-digit-millisecond P99 lookups. Organizations that treat feature stores as "just another cache" with relaxed latency targets (10-50ms typical for caches) discover too late that feature lookup sits directly on the critical path for serving latency.

**Pitfall:** ***Underestimating point-in-time correctness requirements.***

Training a recommendation model on features that include the label (using "user clicked on item X" as a feature when predicting "will user click on item X?") creates models that work perfectly during training and fail catastrophically in production. Point-in-time correctness requires that features used during training reflect only information available before the prediction event. This constraint is easy to state and deceptively difficult to enforce in @sec-storage-feature-store-architecture-5328 implementations. Feature pipelines that aggregate over time windows, join multiple data sources, or depend on asynchronous updates can all violate point-in-time correctness in subtle ways. As @fig-pit-correctness demonstrates, joining training labels with features requires careful timestamp filtering to prevent future information from contaminating training data. The failure mode is insidious: models train well (95% offline accuracy), validate well (94% holdout accuracy), and then underperform in production (78% online CTR) without clear explanation. Engineering teams often chase phantom bugs for weeks before identifying temporal leakage as the root cause.

::: {#fig-pit-correctness fig-env="figure" fig-pos="htb" fig-cap="**Point-in-Time Correctness**. To prevent data leakage, features for a training example must be joined based on the timestamp of the event. If an ad impression occurred at $T_{event}$, the model must be trained using features as they existed at $T < T_{event}$. Using features from $T > T_{event}$ (e.g., \"user clicked\") to predict the click introduces future information, rendering the model useless in production." fig-alt="Timeline showing feature versions V1, V2, V3 at timestamps 10:00, 10:05, 10:10. Training event at 10:08 marked in red. Green bracket shows correct join using V2. Red bracket marks V3 as data leakage since it occurs after the training event."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  % Timeline
  \draw[->, thick] (0, 0) -- (10, 0) node[right] {Time};

  % Feature Updates
  \foreach \x/\t/\v in {1/10:00/V1, 4/10:05/V2, 7/10:10/V3} {
    \draw[blue, thick] (\x, 0.2) -- (\x, -0.2);
    \node[blue, above] at (\x, 0.2) {Feature $\v$};
    \node[gray, below, font=\scriptsize] at (\x, -0.2) {\t};
  }

  % Event (Label)
  \draw[red, ultra thick] (6, 0.5) -- (6, -0.5);
  \node[red, above, font=\bfseries] at (6, 0.5) {Training Event};
  \node[red, below, font=\scriptsize] at (6, -0.5) {Click @ 10:08};

  % Correct Join Interval
  \draw[decorate, decoration={brace, amplitude=5pt}, green!60!black, thick] (4.1, -1) -- (6, -1);
  \node[green!60!black, below=6pt, align=center, font=\footnotesize] at (5.05, -1) {\textbf{Correct Join}\\(Latest $T < T_{event}$)\\Value: V2};

  % Leakage Interval
  \draw[decorate, decoration={brace, amplitude=5pt, mirror}, red, thick] (6.1, -1) -- (7, -1);
  \node[red, below=6pt, align=center, font=\footnotesize] at (8.5, -1) {\textbf{Data Leakage}\\(Future Info)\\Value: V3};

\end{tikzpicture}
```
:::

**Fallacy:** ***SSDs eliminate I/O bottlenecks for ML training.***

The transition from HDDs to SSDs represents a transformative improvement in storage performance: 100x better random access latency, 10-30x higher IOPS, and 10-30x greater sequential bandwidth. This leads teams to assume that NVMe storage eliminates I/O as a bottleneck. As @sec-storage-io-optimization-ml-data-pipelines-b1ff quantifies, this assumption fails for three reasons.

First, the GPU-storage bandwidth gap remains vast. An H100 GPU consumes data at `{python} h100_bw_tbs` TB/s from HBM. Even four NVMe drives in RAID 0 deliver only 25 GB/s, a 130x deficit. SSDs close the gap relative to HDDs but remain fundamentally slower than GPU memory bandwidth.

Second, preprocessing often dominates over I/O. Vision model training requires JPEG decoding (2-5 ms per image), random augmentation (1-2 ms), and normalization. At 1000 images/second, single-threaded preprocessing takes 7 seconds per second of training. The bottleneck shifts from storage to CPU, requiring 7+ parallel preprocessing workers. SSDs with their abundant bandwidth sit idle while CPUs struggle to decode images.

Third, access pattern matters more than raw speed. As @tbl-io-access-patterns demonstrates, NVMe SSDs achieve 7 GB/s sequential but only 1.5 GB/s for random 4KB reads. A dataset of 1 million individual JPEG files accessed randomly achieves only 20% of the drive's potential. Converting to WebDataset or TFRecord recovers the full 7 GB/s. The format optimization provides 5x speedup independent of storage medium.

The quantitative reality: a team upgrading from HDD to NVMe may see 3-5x improvement rather than the expected 30x, because preprocessing or access pattern inefficiencies were the true bottleneck. Effective I/O optimization requires profiling to identify the actual bottleneck, then addressing it systematically: storage upgrades help when storage is saturated, format optimization helps when access patterns are inefficient, and worker parallelism helps when preprocessing is the constraint.

## Summary {#sec-storage-summary-7933-7933}

Storage systems form the foundation upon which large-scale ML training and serving are built. This chapter developed the principles and architectures that enable storage systems to meet the distinctive requirements of machine learning workloads.

We began by examining how ML workloads systematically invert traditional storage assumptions: sequential streaming replaces random access, working sets exceed cache capacity, write patterns are bursty rather than continuous, and read/write ratios vary dramatically by phase. The data pipeline throughput equation ($B_{required} = N_{GPUs} \times U_{target} \times S_{batch}/T_{iteration}$) provides quantitative guidance for storage capacity planning.

Training data infrastructure comprises distributed file systems (GFS/Colossus, HDFS, Lustre) and object storage (S3, GCS), each optimized for different access patterns and scales. Data format selection (TFRecord, Parquet, WebDataset) significantly impacts pipeline throughput, with different formats suited to different model types. Data loading pipelines must prefetch and buffer data to hide storage latency and keep accelerators fully utilized.

Checkpoint storage enables fault tolerance for long-running training jobs. The Young-Daly formula ($T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$) provides the optimal checkpoint interval balancing overhead against recovery time. Distributed and incremental checkpointing techniques address the challenges of TB-scale model state.

Feature stores bridge training and serving for models requiring runtime features. They are critical infrastructure for recommendation systems (trillions of lookups daily) while playing minimal roles for LLMs (prompt contains all information). Point-in-time correctness prevents training-serving skew that causes models to fail silently in production.

Model registries provide versioning, lineage tracking, and governance for model artifacts. They enable reproducibility by connecting trained models to the data, code, and environment that produced them.

The following points summarize the essential principles that should guide storage system design for machine learning workloads.

::: {.callout-takeaways}

* Storage bandwidth, not capacity, typically limits ML training throughput: systems must be designed to saturate accelerator memory bandwidth with training data, requiring careful attention to prefetching, data format optimization, and parallel I/O
* Storage requirements differ dramatically by model type: LLMs need massive text corpora and terabyte-scale checkpoints, recommendation systems require real-time feature stores with sub-millisecond latency, and vision models demand efficient image pipeline formats
* Feature stores are critical infrastructure for recommendation systems where feature lookup latency directly impacts serving time, but less relevant for LLMs where training data pipelines dominate
* Checkpoint storage strategy must balance recovery granularity against overhead: frequent checkpoints minimize lost work but consume I/O bandwidth, requiring tiered approaches with local NVMe for speed and distributed storage for durability
:::

The storage architectures examined here ensure that data is available when needed, whether for training throughput or serving latency. However, a massive compute fleet with fast networking and storage is useless if it cannot be managed efficiently.

The next chapter, @sec-orchestration-resource-management, examines the software "brain" of the fleet: the orchestration and resource management systems (Slurm, Kubernetes) that coordinate thousands of jobs across these resources to ensure fairness and maximize utilization.
