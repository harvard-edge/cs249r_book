---
---

# Orchestration and Resource Management {#sec-orchestration}

::: {layout-narrow}
:::.column-margin
_Gemini Pro 3 Prompt: A visualization of a cluster scheduler as a massive "Tetris" game. The scene shows a 3D bin-packing process where job blocks of varying shapes (representing GPU, CPU, and Memory requirements) are being fitted into available node slots. The scheduler 'brain' is depicted as a glowing control tower making real-time placement decisions. Visual elements include priority queues, preemption events displacing lower-priority blocks, and gang scheduling groups arriving as locked-together units. The style is technical and schematic.
:::

\noindent
![Futuristic datacenter cityscape with glowing server towers, interconnected hexagonal nodes, and network pathways representing cluster job scheduling and resource allocation.](images/png/cover_orchestration.png){fig-alt="Stylized datacenter visualization with blue glowing server towers, hexagonal node clusters connected by bright network pathways, and circuit board patterns below."}

:::

## Purpose {.unnumbered}

_Why does resource allocation become the primary bottleneck when hardware is plentiful?_

A thousand GPUs sitting idle waiting for scheduling decisions cost the same as a thousand GPUs computing useful work. At scale, the limiting factor shifts from having enough hardware to using it efficiently: jobs waiting in queues while resources sit idle, fragmentation leaving gaps too small for any pending job, deadlocks where multiple jobs each hold partial resources while waiting for more. Orchestration is the discipline of extracting useful work from shared infrastructure—deciding which jobs run on which nodes, when preemption serves the greater good, how to balance fairness across teams against raw utilization, and how to prevent the coordination mechanisms themselves from becoming bottlenecks. Poor orchestration transforms expensive hardware into expensive waste; effective orchestration transforms a collection of machines into a coherent computing resource where capacity translates reliably into completed work.

::: {.callout-tip title="Learning Objectives"}
- Implement gang scheduling policies to prevent resource fragmentation and deadlocks in distributed training jobs.
- Design multi-tenant quota systems using hierarchical fair-share and borrowing to maximize cluster utilization.
- Differentiate between HPC (Slurm) and Cloud Native (Kubernetes) scheduling paradigms and their trade-offs for ML workloads.
- Architect auto-scaling policies for inference workloads based on custom metrics like queue depth and GPU memory pressure.
:::

## Introduction {#sec-orch-introduction}

::: {.callout-note title="Connection: The Systems Sandwich"}
In the **Systems Sandwich** model of Volume II (introduced in @sec-vol2-introduction), Orchestration operates at the **Operational Layer (Distribution)**, but its decisions are fundamentally constrained by the **Physical Layer (Infrastructure)**. The scheduler must reason about physical realities: GPU heterogeneity, NVLink topology, rack power limits, and network bisection bandwidth. When scheduling goes wrong, debugging requires examining all three layers to identify whether the root cause is physical (hardware constraints), logical (algorithm limitations), or operational (policy misconfiguration). The Systems Sandwich provides a systematic framework for this diagnosis.
:::

Training GPT-3 required 1024 A100 GPUs running continuously for 34 days [@brown2020language]. Now consider a research organization operating a 10,000 GPU cluster with 100 such training jobs queued, alongside thousands of smaller experiments and inference workloads competing for the same resources. How should the system decide which jobs run, when they run, and where they run? This scheduling problem sits at the heart of large-scale ML infrastructure.

The economic stakes are substantial. Consider the cost of poor scheduling decisions: a 10,000 GPU cluster at $2 per GPU-hour costs $480,000 per day to operate. If scheduling inefficiencies leave 30 percent of GPUs idle, that translates to $144,000 per day in wasted capacity, or over $52 million annually. Conversely, improving utilization from 50 percent to 80 percent effectively adds 6,000 GPUs worth of productive capacity without purchasing additional hardware.

ML workloads present scheduling challenges that distinguish them from traditional high-performance computing. Gang scheduling represents the most fundamental difference: a distributed training job requiring 1024 GPUs cannot make progress with only 512. Traditional HPC jobs often scale flexibly, but synchronous data parallelism demands all-or-nothing allocation. A scheduler that partially allocates resources creates deadlocks where multiple jobs each hold some GPUs while waiting for more, with none able to proceed.

GPU heterogeneity adds another dimension. Modern clusters contain mixtures of A100, H100, and V100 accelerators with different memory capacities, compute throughput, and prices. An H100 provides roughly twice the training throughput of an A100 but costs proportionally more. The scheduler must match workloads to appropriate hardware while maintaining high utilization across heterogeneous pools.

Job duration unpredictability further complicates scheduling. A training run may converge early and complete in days rather than weeks. Hardware failures may abort jobs unexpectedly. Hyperparameter searches may reveal early that certain configurations will not succeed. Traditional scientific computing workloads, such as weather simulations or molecular dynamics, have more predictable runtimes that enable better scheduling decisions.

This chapter teaches the algorithms and trade-offs for ML cluster orchestration. We examine three key topics: gang scheduling and bin packing algorithms that prevent resource fragmentation and deadlocks, fair-share quota systems that balance multi-tenant access with cluster utilization, and the architectural differences between HPC schedulers like Slurm and cloud-native systems like Kubernetes. By the end, you will understand how to design scheduling policies that maximize both efficiency and fairness for ML workloads at scale.

## The Scheduler's Challenge {#sec-scheduler-challenge}

### Bin Packing at Scale
*   **The Problem**: Fitting diverse jobs (1-GPU inference, 1024-GPU training) onto fixed node sizes.
*   **Constraints**: Locality (NVLink), Topology (Switch hops), Power (Rack limits).

### Gang Scheduling
*   **Definition**: Ensuring all $N$ workers for a distributed job start simultaneously.
*   **The "Hold and Wait" Problem**: Why standard schedulers deadlock without gang logic.

## Orchestration Paradigms {#sec-orchestration-paradigms}

### Slurm: The HPC Heritage
*   **Philosophy**: Batch queuing, strict fairness, static partitions.
*   **Pros/Cons**: Unbeatable for massive static training jobs; rigid for microservices.

### Kubernetes: The Cloud Native Standard
*   **Philosophy**: Reconciling state, eventual consistency, services.
*   **Adaptations for ML**: Volcano scheduler, Kueue, NVIDIA Device Plugin.

## Resource Efficiency {#sec-resource-efficiency}

### Multi-Tenancy and Quotas
*   **Hierarchical Fair Share**: Sharing resources between Dept A and Dept B.
*   **Over-subscription**: Selling more capacity than exists to account for idle time.

### Preemption and Priority
*   **Spot Instances**: Utilizing idle capacity for fault-tolerant jobs.
*   **Priority Classes**: Ensuring inference SLAs kick training jobs off the cluster.

## Resource Management and Scheduling

The datacenter infrastructure and high-speed networks discussed in the previous chapters provide the physical foundation for large-scale ML. Translating these resources into productive workloads requires sophisticated scheduling systems that balance utilization, fairness, and job completion time. This section examines the scheduling challenges unique to ML workloads and the systems designed to address them.

::: {#fig-cluster-scheduling fig-env="figure" fig-pos="htb" fig-cap="**Cluster Resource Management Architecture**. A high-level view of a distributed scheduler. Jobs enter a prioritized queue. The scheduler matches resource requests (see @fig-cluster-scheduling) (GPUs, Memory) against available nodes, enforcing fairness and locality constraints. Node agents (like Kubelet or Slurmd) launch containers and monitor health, reporting status back to the control plane." fig-alt="Flowchart showing job flow: User submits to Job Queue, Scheduler Core performs bin packing, Resource Manager tracks state and launches jobs on GPU Node Agents 1 through N. Agents report status back to Resource Manager."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm]
  \definecolor{ProcColor}{RGB}{255,250,240}
  \definecolor{DataColor}{RGB}{240,248,255}

  \tikzset{
    process/.style={draw=black, thick, fill=ProcColor, rounded corners=2pt, minimum width=2.5cm, minimum height=1cm, align=center},
    database/.style={draw=black, cylinder, shape border rotate=90, aspect=0.25, fill=DataColor, minimum width=2cm, minimum height=1.5cm, align=center},
    arrow/.style={->, >=stealth, thick}
  }

  % Components
  \node[process] (user) {User / Client\\(Job Submission)};
  \node[database, right=of user] (queue) {Job Queue\\(Priority/FIFO)};
  \node[process, right=of queue, fill=orange!10] (scheduler) {Scheduler Core\\(Bin Packing)};
  \node[process, below=of scheduler] (resman) {Resource Manager\\(State Tracking)};

  \node[process, below=of user, xshift=-1cm] (node1) {Agent 1\\(GPU Node)};
  \node[process, right=0.5cm of node1] (node2) {Agent 2\\(GPU Node)};
  \node[process, right=0.5cm of node2] (node3) {Agent N\\(GPU Node)};

  % Flows
  \draw[arrow] (user) -- (queue);
  \draw[arrow] (queue) -- (scheduler);
  \draw[arrow] (scheduler) -- node[right, font=\footnotesize] {Allocation} (resman);
  \draw[arrow] (resman) -- node[right, font=\footnotesize] {Launch} (node2);
  \draw[arrow] (node1) -- (resman);
  \draw[arrow] (node2) -- (resman);
  \draw[arrow] (node3) -- (resman);

  % Policies
  \node[above=0.2cm of scheduler, font=\footnotesize\bfseries] {Policies: Fair Share, Gang};

\end{tikzpicture}
```
:::

ML workloads present scheduling challenges distinct from traditional computing. Training jobs require coordinated access to multiple GPUs, often spanning nodes connected via the InfiniBand fabric discussed in the previous section. Inference workloads demand consistent latency while handling unpredictable traffic patterns. Both compete for the same accelerator resources, creating tension between throughput-oriented batch processing and latency-sensitive serving.

### Why Distributed Scheduling is Hard

Before examining specific schedulers, understanding why cluster scheduling differs fundamentally from single-machine scheduling clarifies the design constraints these systems face.

**Distributed Systems Challenges.** Cluster scheduling is not merely "putting jobs on machines" at larger scale. Several fundamental distributed systems problems make it intrinsically harder.

Partial failures pose the first challenge. A node can fail between allocation and job start. The scheduler may successfully allocate 32 GPUs across 4 nodes, only to have one node fail before the job launches. The remaining 24 GPUs sit idle while the scheduler detects the failure and re-allocates.

Network partitions create a second problem. The scheduler may lose connectivity to a subset of nodes while those nodes continue operating. From the scheduler's perspective, the nodes appear failed. From the nodes' perspective, jobs may still be running.

State inconsistency emerges as a third challenge. Resource state may differ between the scheduler's view and reality on individual nodes. A GPU the scheduler believes is free may still be cleaning up from a previous job.

Ordering without global time presents the fourth fundamental issue. Without a global clock, determining whether allocation A happened before allocation B across different nodes requires careful protocol design. Two jobs may both believe they "own" the same GPU if the system is not carefully designed.

**CAP Theorem Implications.** The CAP theorem[^fn-cap] applies directly to cluster scheduling: a scheduler cannot simultaneously provide perfect consistency (every allocation reflects true cluster state), availability (every request gets a response), and partition tolerance (system operates despite network failures).

[^fn-cap]: **CAP theorem**: As introduced in @sec-vol2-introduction, a fundamental distributed systems result proving that no system can simultaneously guarantee Consistency (all nodes see the same data), Availability (every request receives a response), and Partition tolerance (the system operates despite network failures). First conjectured by Eric Brewer in 2000 and formally proven by Gilbert and Lynch in 2002 [@gilbert2002brewer], CAP forces designers to choose which property to sacrifice when partitions occur. Most modern systems choose availability over strict consistency, using eventual consistency models.

Production schedulers make different trade-offs. Slurm prioritizes consistency, blocking allocations during uncertainty. Kubernetes prioritizes availability, using eventual consistency with reconciliation loops. Custom ML schedulers often accept bounded inconsistency for performance.

**Failure Rates at Scale.** At scale, failure is normal operation, not exceptional. With 99.9% annual GPU reliability (typical for datacenter hardware), a 4096-GPU cluster experiences:

$$ \text{Expected failures per day} = 4096 \times \frac{0.001}{365} \approx 0.01 \text{ GPU failures/day} $$

More realistically, including software failures, driver issues, and thermal events, production clusters see 1 to 4 failures per day per 1000 GPUs. A multi-week training run on 4096 GPUs will experience multiple failures. Infrastructure and scheduling systems must anticipate this reality.

### Batch Scheduling for Training

Training large models requires allocating substantial GPU resources for extended periods. A 175B parameter model may require 1024 GPUs for weeks, while smaller experiments need only a few GPUs for hours. Scheduling systems must efficiently pack these diverse workloads while respecting resource constraints and fairness policies.

#### Slurm for HPC-Style ML

Slurm[^fn-slurm] (Simple Linux Utility for Resource Management) [@yoo2003slurm] dominates HPC environments and extends naturally to GPU-intensive ML training. Its partition-based architecture maps well to heterogeneous accelerator pools.

[^fn-slurm]: **Slurm**: An open-source job scheduler originating from Lawrence Livermore National Laboratory (2002) that manages compute resources across thousands of nodes. Slurm's design prioritizes predictability and fairness for long-running scientific workloads. Unlike Kubernetes' declarative model, Slurm uses imperative job submission with explicit resource requests, making it easier to reason about allocation guarantees but less flexible for dynamic workloads.

A typical ML cluster configuration defines partitions by accelerator type and interconnect:

+---------------+---------------+------------------+--------------------+
| **Partition** | **GPUs/Node** | **Interconnect** | **Typical Use**    |
+:==============+==============:+:=================+:===================+
| **dgx-a100**  | 8 x A100      | NVLink + IB NDR  | Large LLM training |
+---------------+---------------+------------------+--------------------+
| **a100-pcie** | 4 x A100      | PCIe + IB HDR    | Medium training    |
+---------------+---------------+------------------+--------------------+
| **inference** | 2 x A10G      | Ethernet         | Model serving      |
+---------------+---------------+------------------+--------------------+
| **debug**     | 1 x V100      | Ethernet         | Development        |
+---------------+---------------+------------------+--------------------+

GPU allocation strategies significantly impact utilization. The `--gres=gpu:N`[^fn-gres] flag requests N GPUs, but naive allocation can fragment nodes. Consider a 64-node cluster with 8 GPUs each, totaling 512 GPUs. If jobs request 6 GPUs, each job wastes 2 GPUs per node, reducing effective capacity to 75 percent. Slurm's `SelectTypeParameters=CR_GPU` enables GPU-level scheduling, while `--gpus-per-node` ensures jobs receive full nodes when beneficial for NVLink communication.

[^fn-gres]: **GRES (Generic Resource Scheduling)**: Slurm's mechanism for scheduling non-CPU resources like GPUs, FPGAs, or specialized accelerators. GRES tracks resource availability per node and enforces exclusive allocation. The gres.conf file defines available resources, while jobs request them via `--gres=resource_type:count`. This abstraction allows Slurm to manage diverse accelerator types without code changes.

Fair-share scheduling prevents any single user or project from monopolizing resources. The classic fair-share formula computes effective priority as:

$$P_{effective} = P_{base} \times \frac{F_{target}}{F_{actual} + \epsilon}$$

where $F_{target}$ represents the user's allocated share and $F_{actual}$ their recent usage. This naturally deprioritizes heavy users while allowing burst access when resources are idle.

Preemption policies enable high-priority jobs to reclaim resources from running workloads. For ML training, this requires checkpoint-aware preemption. Jobs receive SIGTERM with configurable grace periods, typically 60 to 300 seconds, to save checkpoints before SIGKILL. The `PreemptMode=REQUEUE` setting automatically restarts preempted jobs, while `GraceTime` controls the checkpoint window.

#### Kubernetes for ML Workloads

Kubernetes has become the standard platform for ML infrastructure, particularly for organizations requiring unified management of training and serving workloads. Native Kubernetes lacks ML-aware scheduling, but extensions address this gap.

GPU scheduling relies on device plugins that expose accelerators as extended resources. The NVIDIA device plugin registers GPUs with the kubelet, enabling pod specifications as shown in @lst-k8s-gpu-alloc.

::: {#lst-k8s-gpu-alloc lst-cap="**Kubernetes GPU Allocation**: Pod resource specification requesting GPUs through the NVIDIA device plugin. The `nvidia.com/gpu` resource name follows Kubernetes extended resource conventions, where the domain prefix identifies the device plugin vendor. This declarative syntax enables portable GPU workload definitions across any Kubernetes cluster with the NVIDIA device plugin installed."
}
```{.yaml}
# Kubernetes pod resource specification for GPU allocation
resources:
  limits:
    nvidia.com/gpu: 4  # Request exactly 4 GPUs for this pod
```
:::

This binary allocation model wastes resources when workloads need less than a full GPU. Multi-Instance GPU[^fn-mig] technology addresses this by partitioning A100 and H100 GPUs into isolated instances. An A100-80GB can be divided into configurations ranging from 7 small instances of 10GB each to 2 large instances of 40GB each. The device plugin exposes MIG instances as separate resources:

+-----------------+----------------+--------------+----------------------+
| **MIG Profile** | **GPU Memory** | **SM Count** | **Typical Workload** |
+=================+===============:+=============:+:=====================+
| **1g.10gb**     | 10 GB          | 14 SMs       | Small inference      |
+-----------------+----------------+--------------+----------------------+
| **2g.20gb**     | 20 GB          | 28 SMs       | Medium inference     |
+-----------------+----------------+--------------+----------------------+
| **3g.40gb**     | 40 GB          | 42 SMs       | Large inference      |
+-----------------+----------------+--------------+----------------------+
| **7g.80gb**     | 80 GB          | 98 SMs       | Training             |
+-----------------+----------------+--------------+----------------------+

[^fn-mig]: **Multi-Instance GPU (MIG)**: NVIDIA hardware feature (A100 and later) that partitions a single GPU into up to 7 isolated instances, each with dedicated memory, cache, and compute resources. Unlike software-based GPU sharing, MIG provides hardware isolation that prevents memory access between instances. This enables secure multi-tenant GPU sharing but requires workloads to fit within instance memory limits.

Gang scheduling[^fn-gang] ensures distributed training jobs receive all requested resources simultaneously. Without gang scheduling, a job requesting 32 GPUs might receive 24 immediately while waiting indefinitely for the remaining 8, wasting the already-allocated resources. The Volcano batch scheduler and scheduler plugins like Coscheduling implement gang semantics through PodGroup abstractions. Jobs specify minimum member counts, and the scheduler delays placement until all pods can be scheduled together.

[^fn-gang]: **Gang scheduling**: A scheduling policy that allocates resources for multi-component jobs atomically, ensuring all components start simultaneously or none do. First developed for parallel computing in the 1980s, gang scheduling prevents deadlock scenarios where jobs partially acquire resources and block each other. For ML training, gang scheduling ensures all workers are ready before training begins, avoiding wasted GPU cycles.

Priority classes control preemption behavior. A typical hierarchy assigns training workloads medium priority, inference high priority, and development jobs low priority. The `preemptionPolicy: PreemptLowerPriority` setting enables inference pods to reclaim resources from training when latency SLOs are threatened, though organizations must balance this against training job completion times.

#### Custom ML Schedulers

Research schedulers have demonstrated significant improvements over general-purpose systems by exploiting ML-specific characteristics.

**Tiresias** [@gu2019tiresias] observes that ML training jobs have predictable resource requirements after initial epochs. Rather than requiring users to estimate job duration, often inaccurate by 2 to 5 times, Tiresias uses a two-dimensional attained service[^fn-attained-service] scheduler. Jobs accumulate "service" based on GPU-time consumed, with priority decreasing as service increases. A discretized version groups jobs into service bins, promoting short jobs without requiring duration estimates. Experiments show 40 to 60 percent reduction in average job completion time compared to FIFO scheduling.

[^fn-attained-service]: **Attained service scheduling**: A scheduling discipline where job priority decreases with cumulative resource usage. Originally developed for processor sharing, attained service naturally prioritizes short jobs without requiring duration estimates. For ML workloads where job length correlates with model complexity, this approach provides near-optimal average completion times while remaining robust to inaccurate user estimates.

**Gandiva** [@xiao2018gandiva] exploits the iterative nature of deep learning. Training alternates between GPU-intensive forward and backward passes and CPU-intensive data loading. Gandiva time-slices GPU access at iteration boundaries, enabling higher utilization through oversubscription. A cluster with 100 GPUs might support 120 concurrent jobs if each spends 20 percent of time waiting for data. Gandiva also implements grow-shrink elasticity, automatically adjusting data parallelism degree based on resource availability.

**Themis** [@mahajan2020themis] addresses fairness in long-running ML workloads. Traditional fair-share treats all GPU-seconds equally, but ML jobs have diminishing returns as training progresses. Themis defines a finish-time fairness metric, allocating resources to minimize the maximum slowdown any job experiences relative to exclusive access. This approach benefits shorter jobs without excessive penalty to longer ones.

Locality-aware scheduling recognizes that communication topology matters for distributed training. A 64-GPU job performs better on 8 nodes of 8 GPUs each than 16 nodes of 4 GPUs, due to higher NVLink bandwidth within nodes. Advanced schedulers consider the fat-tree topology discussed in @sec-networking-ml, preferring allocations that share fewer switch hops. Experiments show 15 to 30 percent training throughput improvement from topology-aware placement.

::: {.callout-example title="Systems Sandwich: Debugging Low GPU Utilization"}

**The Problem**: A 1,000-GPU cluster reports 60% average utilization despite a full job queue with over 50 pending jobs. Engineering leadership expects greater than 85% utilization given the capital investment. The standard monitoring dashboards show plenty of idle GPUs, yet users complain about long queue times. Where is the problem, and how do we systematically diagnose it?

The **Systems Sandwich** framework (introduced in @sec-vol2-introduction) provides a structured approach: analyze the Physical Layer first to understand hardware constraints, then the Operational Layer to understand scheduling logic, and finally the interaction between layers to identify the root cause.

**Physical Layer Analysis**: The cluster contains heterogeneous hardware acquired over three procurement cycles:

+------------------+------------+------------------+--------------------+-------------------+
| **GPU Type**     | **Count**  | **Memory**       | **Interconnect**   | **Nodes**         |
+:=================+===========:+:=================+:===================+:==================+
| **A100-80GB**    | 400        | 80 GB HBM2e      | NVLink (600 GB/s)  | 50 nodes x 8 GPUs |
+------------------+------------+------------------+--------------------+-------------------+
| **A100-40GB**    | 400        | 40 GB HBM2e      | NVLink (600 GB/s)  | 50 nodes x 8 GPUs |
+------------------+------------+------------------+--------------------+-------------------+
| **V100-32GB**    | 200        | 32 GB HBM2       | PCIe Gen3 (32 GB/s)| 50 nodes x 4 GPUs |
+------------------+------------+------------------+--------------------+-------------------+

The physical topology creates three distinct resource pools with different capabilities. The A100 nodes support efficient tensor parallelism via NVLink, while V100 nodes are limited to data parallelism due to PCIe bandwidth constraints.

**Operational Layer Analysis**: The Slurm scheduler implements gang scheduling with strict resource type matching. Examining the job specifications reveals the demand pattern:

- 15 large training jobs requesting 64+ A100-80GB GPUs (total demand: 1,200 A100-80GB GPUs)
- 8 medium jobs requesting 32 A100-40GB GPUs (total demand: 256 A100-40GB)
- Zero jobs explicitly requesting V100 resources

The scheduler's allocation log shows A100-80GB GPUs at 95% allocation (380/400), but with only 5 jobs actually running because gang scheduling holds resources for jobs that cannot yet be fully satisfied. The remaining 10 large jobs each hold partial allocations (64-96 GPUs reserved) while waiting for additional A100-80GB resources that never become available, creating a **hold-and-wait deadlock pattern**.

**Diagnosis**: Multiple pathologies compound to create low utilization:

1. **Over-specification**: Model memory analysis reveals that 12 of the 15 large jobs actually require only 45 GB peak memory per GPU, well within A100-40GB capacity. Users copied job templates specifying A100-80GB without recalculating requirements.

2. **Pool fragmentation**: The strict homogeneity requirement means a 64-GPU job requesting "A100-80GB" cannot use any A100-40GB GPUs, even when 300 A100-40GB GPUs sit idle.

3. **Stranded resources**: No jobs target V100 hardware because researchers perceive it as "legacy." The 200 V100 GPUs contribute zero productive work despite consuming power and cooling.

4. **Gang scheduling deadlock**: Without a timeout or preemption policy, partially-satisfied jobs hold resources indefinitely, blocking other work.

**Root Cause (Policy-Infrastructure Mismatch)**: Job templates and organizational practices evolved for a homogeneous A100-80GB cluster. When infrastructure expanded with heterogeneous hardware, the Operational Layer policies were never updated. The scheduler correctly implements its configured policy; the policy itself creates the utilization gap.

**Solution**: Implement tiered resource matching with topology awareness:

1. **Capability-based scheduling**: Replace exact GPU type requests with capability requirements. Instead of `--gres=gpu:a100-80g:64`, jobs specify `--constraint="gpu_mem>=40GB"` and let the scheduler select appropriate hardware.

2. **Topology-aware placement**: For jobs requiring tensor parallelism, add constraint `--constraint="nvlink"` to ensure placement on NVLink-connected nodes. Data-parallel jobs can omit this constraint to enable V100 scheduling.

3. **Gang scheduling with timeout**: Configure `SchedulerParameters=bf_continue,bf_window=7200` to enable backfill scheduling and prevent indefinite resource holding. Jobs waiting more than 2 hours for gang completion release partial allocations.

4. **Workload-hardware matching guidance**: Provide researchers with a decision matrix mapping model sizes to minimum GPU memory requirements, reducing over-specification.

**Quantified Impact**: After implementing tiered matching and backfill scheduling over a two-week validation period:

+------------------+------------------+------------------+------------------+
| **Pool**         | **Before**       | **After**        | **Change**       |
+:=================+=================:+=================:+=================:+
| A100-80GB        | 95% allocated,   | 89% allocated,   | +5% effective    |
|                  | 72% effective    | 94% effective    |                  |
+------------------+------------------+------------------+------------------+
| A100-40GB        | 64% allocated    | 88% allocated    | +24%             |
+------------------+------------------+------------------+------------------+
| V100             | 0% allocated     | 71% allocated    | +71%             |
+------------------+------------------+------------------+------------------+
| **Cluster-wide** | **60%**          | **84%**          | **+40%**         |
+------------------+------------------+------------------+------------------+

The distinction between "allocated" and "effective" utilization captures the gang scheduling deadlock: before the fix, A100-80GB GPUs showed high allocation in Slurm but low actual compute utilization because allocated jobs could not start.

This 40% improvement in effective cluster capacity equals approximately 400 additional GPUs worth of productive work. At $2 per GPU-hour, this represents $7 million in annual recovered value, achieved through policy changes requiring zero additional hardware investment.

**The Systems Sandwich Lesson**: Surface-level diagnosis suggested a scheduling algorithm problem (Operational Layer), perhaps requiring a more sophisticated scheduler. Deeper analysis revealed the root cause as a **mismatch between Physical Layer heterogeneity and Operational Layer policies designed for homogeneous infrastructure**. The scheduler worked correctly; it implemented the wrong policy. Effective debugging required examining both layers and their interaction, recognizing that infrastructure evolution had invalidated assumptions embedded in job templates and scheduling configuration. The fix addressed the policy layer, not the algorithm layer.
:::

### Online Serving Resource Management

Inference workloads require different scheduling strategies than training. Latency matters more than throughput, traffic fluctuates unpredictably, and resource requirements vary by model size and request characteristics.

#### Autoscaling for Inference

Horizontal Pod Autoscaling (HPA) adjusts replica counts based on metrics. Default CPU utilization targets, often 50 to 70 percent, poorly reflect GPU inference workloads. Effective ML autoscaling uses custom metrics:

+-------------------------+------------------+----------------------------------+
| **Metric**              | **Target Range** | **Considerations**               |
+:========================+=================:+:=================================+
| **GPU utilization**     | 60-80%           | Varies by model batch efficiency |
+-------------------------+------------------+----------------------------------+
| **Request queue depth** | 10-50 requests   | Prevents latency spikes          |
+-------------------------+------------------+----------------------------------+
| **P99 latency**         | &lt; SLO target  | Reactive, lags demand changes    |
+-------------------------+------------------+----------------------------------+
| **Pending tokens**      | Model-specific   | LLM-specific, accounts for KV    |
+-------------------------+------------------+----------------------------------+

Vertical Pod Autoscaling (VPA) adjusts resource requests and limits for individual pods. For inference, VPA can right-size memory allocations based on observed usage. GPU resources cannot be vertically scaled without pod restart, limiting VPA's utility for accelerated workloads.

LLM inference requires specialized scaling due to the key-value cache[^fn-kv-cache]. A 70B parameter model serving long-context requests may require 80GB+ of GPU memory for KV cache alone, even with PagedAttention optimizations (detailed in @sec-optimization-kv-cache). Scaling decisions must account for both request rate and context length distribution.

[^fn-kv-cache]: **Key-Value (KV) cache**: Memory that stores computed attention key and value tensors from previous tokens during autoregressive generation. Without caching, each new token would require recomputing attention over the entire sequence. KV cache grows linearly with sequence length and batch size; for a 70B model with 128K context, the cache can exceed model weights in memory usage. PagedAttention (vLLM) manages this memory more efficiently through virtual memory techniques.

#### Resource Isolation

Noisy neighbor problems occur when colocated workloads interfere with each other. On GPUs, interference manifests through shared memory bandwidth, L2 cache contention, and PCIe bottlenecks. MIG provides hardware isolation but at the cost of flexibility. Software approaches include careful placement policies and time-based resource contracts.

GPU memory isolation prevents one model from consuming memory needed by another. Without explicit limits, a memory leak or unexpectedly large batch can crash colocated workloads. Container runtimes can enforce memory limits through CUDA Multi-Process Service[^fn-mps], though this adds latency overhead of approximately 5 to 10 microseconds per kernel launch.

[^fn-mps]: **CUDA MPS (Multi-Process Service)**: A CUDA feature that enables multiple processes to share a GPU with reduced context switching overhead. Unlike time-slicing where only one process accesses the GPU at a time, MPS allows concurrent kernel execution from different processes. MPS improves utilization for small workloads but provides limited isolation compared to MIG; a memory-intensive process can still impact colocated workloads through shared cache pressure.

CPU pinning assigns specific cores to inference pods, preventing scheduler migration that causes cache invalidation. For latency-sensitive workloads, isolating cores using `isolcpus` kernel parameters and `taskset` affinity removes OS scheduling jitter. Combined with NUMA-aware[^fn-numa] placement, this reduces P99 latency by 10 to 30 percent for sub-millisecond inference tasks.

[^fn-numa]: **NUMA (Non-Uniform Memory Access)**: A memory architecture where access time depends on memory location relative to the processor. Modern multi-socket servers have distinct memory controllers per CPU socket; accessing local memory takes approximately 100ns, while accessing remote memory via the interconnect takes 150-200ns. For ML inference, placing GPU workloads on CPU cores closest to the GPU's PCIe connection minimizes data transfer latency.

### Multi-Tenancy Considerations

Production ML platforms serve multiple teams with competing priorities. Quota systems balance guaranteed access against overall utilization, while security isolation protects sensitive models and data.

#### Quota Management

GPU quota allocation typically operates at the namespace or project level. A simple approach allocates fixed GPU counts, but this leads to underutilization when teams have variable workloads. Hierarchical quotas enable departmental limits with sub-team flexibility:

$$Q_{effective} = \min(Q_{team}, Q_{department} - \sum_{other\ teams} U_{allocated})$$

Fair-share across teams extends the single-user formula to organizational hierarchies. When aggregate demand exceeds capacity, each team receives resources proportional to their share allocation. Unused capacity borrows down the hierarchy, maximizing utilization while respecting priorities.

Burst capacity handling enables teams to temporarily exceed quotas when resources are available. Overcommitment ratios of 1.2 to 1.5 times are common, with admission controllers tracking actual versus requested resources. When contention occurs, jobs using burst capacity face preemption first.

#### Security Isolation

Namespace separation provides the fundamental isolation boundary in Kubernetes. Each team operates within dedicated namespaces with role-based access control (RBAC) limiting visibility to their own resources. Network policies extend isolation to the network layer, preventing cross-namespace communication except through explicitly permitted services.

Network policies for ML workloads must balance isolation with distributed training requirements. A policy might allow all-to-all communication within a namespace (for ring-AllReduce) while blocking ingress from other namespaces. Egress policies prevent training jobs from accessing external networks, reducing data exfiltration risk.

GPU virtualization options range from time-slicing with low isolation and high flexibility, to MIG with hardware isolation and fixed partitions, to full device passthrough with complete isolation and lowest utilization. The choice depends on workload sensitivity and trust boundaries. Multi-tenant inference platforms typically use MIG for isolation between tenants, while single-tenant training clusters favor device passthrough for maximum performance.

The scheduling and resource management infrastructure discussed here enables efficient use of the datacenter resources and networks from previous sections. Effective schedulers achieve 70 to 85 percent GPU utilization in production clusters, compared to 30 to 50 percent with naive approaches. This efficiency translates directly to cost. A 1000-GPU cluster at 80 percent utilization delivers the equivalent capacity of 1600 GPUs at 50 percent utilization. As organizations scale ML infrastructure, scheduling sophistication becomes a primary determinant of both cost efficiency and researcher productivity. These resource management capabilities also provide the foundation for the fault-tolerant systems discussed in @sec-fault-tolerance and the operational practices covered in @sec-ops-scale.

## Summary

Orchestration and resource management transform datacenter hardware into productive ML infrastructure. The schedulers examined here determine whether thousands of isolated GPUs function as a single coherent supercomputer or as a fragmented collection of servers.

We explored the fundamental distributed systems challenges that make cluster scheduling intrinsically hard: partial failures, network partitions, state inconsistency, and the CAP theorem trade-offs. Production schedulers make different design choices, with Slurm prioritizing consistency and Kubernetes prioritizing availability through eventual consistency.

We examined batch scheduling systems for training workloads. Slurm provides partition-based architecture with fair-share policies and checkpoint-aware preemption. Kubernetes extensions like Volcano and device plugins enable ML-aware scheduling with gang semantics and MIG support. Research schedulers like Tiresias, Gandiva, and Themis demonstrate how exploiting ML-specific characteristics improves job completion times and fairness.

For inference workloads, we analyzed autoscaling based on custom metrics, resource isolation through MIG and CPU pinning, and multi-tenancy with hierarchical quotas. These specialized policies improve utilization from the 30-50% typical of naive clusters to the 70-85% required for economic viability.

::: {.callout-important title="Key Takeaways"}

* **Distributed Scheduling is Hard**: Cluster scheduling faces fundamental challenges (partial failures, network partitions, state inconsistency) that single-machine schedulers never encounter. The CAP theorem forces trade-offs between consistency and availability.
* **Gang Scheduling Prevents Deadlock**: Distributed training requires atomic resource allocation. Without gang semantics, jobs partially acquire resources and block each other, wasting already-allocated GPUs.
* **ML-Specific Policies Matter**: Exploiting ML workload characteristics (predictable resource needs after initial epochs, iterative computation, diminishing returns) enables specialized schedulers to outperform general-purpose systems by 40-60% in job completion time.
* **Utilization is King**: The high cost of ML infrastructure makes scheduling efficiency a first-order economic driver. Improving utilization from 40% to 80% effectively halves hardware costs.

:::

With the fleet built—compute nodes defined, networks connected, storage configured, and schedulers running—the machine is ready. But a training cluster is only a means to an end: producing models that serve users.

**Part III: Deployment at Scale** begins with @sec-inference, where we examine the challenges of serving these models to millions of users with low latency and high throughput.

```{=latex}
\part{key:vol2_deployment}
```
