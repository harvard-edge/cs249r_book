---
title: "Fault Tolerance and Reliability"
bibliography: fault_tolerance.bib
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR FAULT TOLERANCE
================================================================================

CORE PRINCIPLE: Fault tolerance requirements differ by workload type.
Training fault tolerance (checkpoint/restart) differs from serving
fault tolerance (redundancy/failover). Different models have different needs.

MODEL-SPECIFIC FAULT TOLERANCE CONSIDERATIONS:

| Model Type      | Training Recovery    | Serving Redundancy  | Critical State      |
|-----------------|---------------------|---------------------|---------------------|
| LLMs            | Checkpoint (TB)     | Model replicas      | KV cache            |
| Recommendation  | Incremental ckpt    | Feature store HA    | Embedding tables    |
| Vision          | Standard ckpt       | Load balancing      | Minimal state       |
| Real-time       | Fast recovery       | Hot standby         | Session state       |

REQUIRED COVERAGE FOR THIS CHAPTER:

TRAINING FAULT TOLERANCE:

- Checkpoint frequency: Depends on model size, iteration time
- Checkpoint storage: Distributed for large models, local for small
- Recovery granularity: Full restart vs elastic recovery
- Include: Different strategies for different model sizes

SERVING FAULT TOLERANCE:

- Stateless serving: Vision, many NLP models (simpler)
- Stateful serving: LLM with KV cache, session-based (complex)
- Feature store HA: Critical for recommendation systems
- Include: Why RecSys fault tolerance focuses on feature stores

GRACEFUL DEGRADATION:

- Model fallback: Use simpler model when primary fails
- Feature fallback: Default features when lookup fails (RecSys)
- Quality degradation: Reduce batch size, accept higher latency
- Include: Different degradation strategies by model type

DISTRIBUTED TRAINING FAILURES:

- Worker failure: Replace vs redistribute
- Network partition: Synchronous vs asynchronous handling
- Straggler mitigation: Backup workers, bounded staleness
- Include: How failure handling differs by parallelism strategy

CASE STUDIES TO INCLUDE:

- Meta recommendation serving resilience
- OpenAI training fault tolerance at scale
- Google TPU pod failure handling
- Netflix chaos engineering for ML

QUANTITATIVE ANALYSIS:

- Mean time between failures at different scales
- Recovery time objectives by workload type
- Cost of downtime for different applications
- Checkpoint overhead as percentage of training time

ANTI-PATTERNS TO AVOID:

- Treating all fault tolerance as checkpoint/restart
- Ignoring feature store reliability for RecSys
- Assuming stateless serving (LLMs have KV cache state)
- One-size-fits-all recovery strategies

================================================================================
-->

# Fault Tolerance and Reliability {#sec-fault-tolerance}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: A dramatic visualization of fault tolerance mechanisms protecting a distributed ML system. The scene shows a cluster of compute nodes with several nodes experiencing failures depicted as red warning indicators and disconnected links. Protective mechanisms spring into action: checkpoint systems shown as periodic snapshots being saved to persistent storage, redundant replicas activating to replace failed nodes, and graceful degradation paths routing around damaged components. A central reliability monitor displays system health metrics with some indicators in warning states but the overall system remaining operational. Visual elements include recovery timelines, failover arrows, and heartbeat signals between nodes. The composition contrasts the chaos of failures with the order of recovery mechanisms. Color scheme uses stable greens and blues for healthy components, red and orange for failures, and gold for active recovery processes. Technical yet dramatic style suitable for a reliability engineering textbook._
:::

\noindent
![](images/png/cover_fault_tolerance.png)

:::

## Purpose {.unnumbered}

_Why must machine learning systems continue delivering predictions reliably despite inevitable hardware failures, network outages, and unexpected operational disruptions?_

Machine learning systems deployed in production environments operate in a world of uncertainty: infrastructure fails, networks disconnect, data becomes corrupted, and computational resources become unavailable without warning. Unlike experimental systems that can simply restart when encountering failures, production ML systems serve millions of users and support critical decision-making, where reliability determines whether organizations can trust these systems with consequential outcomes. Traditional software failures manifest visibly through errors and exceptions, but ML system degradation under fault conditions often remains silent and undetected until catastrophic loss of service occurs. A recommendation system silently serving stale predictions erodes user trust over weeks before anyone notices. A healthcare diagnostic system becoming unavailable during peak hours jeopardizes patient care. An autonomous vehicle losing sensor redundancy becomes dangerous without warning. Mastering fault tolerance transforms ML systems from fragile prototypes dependent on perfect conditions into resilient production systems capable of sustained operation despite the failures that inevitably occur at scale.

## Coming 2026

This chapter will cover checkpointing, recovery, and graceful degradation in ML systems.

```{=latex}
\part{key:vol2_production}
```
