---
engine: jupyter
---

# Fault Tolerance and Reliability {#sec-fault-tolerance-reliability-reliability}

::: {layout-narrow}
::: {.column-margin}
_Gemini Pro 3 Prompt: A dramatic visualization of fault tolerance mechanisms protecting a distributed ML system. The scene shows a cluster of compute nodes with several nodes experiencing failures depicted as red warning indicators and disconnected links. Protective mechanisms spring into action: checkpoint systems shown as periodic snapshots being saved to persistent storage, redundant replicas activating to replace failed nodes, and graceful degradation paths routing around damaged components. A central reliability monitor displays system health metrics with some indicators in warning states but the overall system remaining operational. Visual elements include recovery timelines, failover arrows, and heartbeat signals between nodes. The composition contrasts the chaos of failures with the order of recovery mechanisms. Color scheme uses stable greens and blues for healthy components, red and orange for failures, and gold for active recovery processes. Technical yet dramatic style suitable for a reliability engineering textbook. Rendered in the style of Nanobanana._
:::

\noindent
![](images/png/cover_fault_tolerance.png)

:::

## Purpose {.unnumbered}

_Why does scale transform hardware failure from rare exception to routine condition that systems must absorb continuously?_

A single GPU fails perhaps once per year. A thousand GPUs experience failures daily. A ten-thousand GPU cluster sees failures hourly. This arithmetic is inescapable: individual component reliability does not change, but aggregate system reliability degrades multiplicatively as components are added. At frontier scale, the question is not whether failures will occur during a training run but how many, and systems that cannot absorb failures without losing progress cannot operate at all. The same logic applies to serving: a globally distributed inference system experiences regional outages, network partitions, and capacity fluctuations as continuous background conditions rather than exceptional events. Fault tolerance at scale is not about preventing failures—that is impossible—but about designing systems where failures are expected, detected, isolated, and recovered from automatically, allowing useful work to continue despite the constant churn of components entering and leaving operational status.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Calculate system-level MTBF from component failure rates and derive optimal checkpoint intervals using the Young-Daly formula to minimize wasted work
- Design distributed checkpoint and recovery strategies that coordinate state across thousands of GPUs while managing multi-terabyte checkpoint sizes
- Implement serving redundancy, replication, and failover mechanisms that maintain availability within millisecond latency requirements under partial failures
- Apply graceful degradation strategies including model fallback, feature fallback, and load shedding to maintain partial service when full capability is unavailable
- Construct observability infrastructure using metrics, logs, and distributed traces to diagnose non-deterministic failures and silent accuracy degradation
- Evaluate fault tolerance trade-offs across training and serving workloads by analyzing their distinct latency tolerances, state requirements, and recovery strategies

:::

```{python}
#| label: fault-tolerance-setup
#| echo: false

from mlsys.constants import *
from mlsys.formatting import fmt, sci

# GPT-3 model parameters
gpt3_params_b = f"{GPT3_PARAMS.to(param).magnitude / BILLION:.0f}"

# GPT-3 checkpoint size: weights (2 bytes FP16) + Adam m (4 bytes) + Adam v (4 bytes) = 12 bytes/param
gpt3_ckpt_bytes = GPT3_PARAMS.magnitude * 12 * byte
gpt3_ckpt_tb = f"{gpt3_ckpt_bytes.to(TB).magnitude:.1f}"

# GPT-3 Adam optimizer state: m + v = 8 bytes/param
gpt3_adam_bytes = GPT3_PARAMS.magnitude * 8 * byte
gpt3_adam_tb = f"{gpt3_adam_bytes.to(TB).magnitude:.1f}"

# Per-worker shard for 1000 workers
gpt3_shard_gb = f"{gpt3_ckpt_bytes.to(GB).magnitude / 1000:.1f}"
```

## Failure Analysis at Scale {#sec-fault-tolerance-reliability-reliability-failure-analysis-scale-6b4b}

In the **Systems Sandwich** (@sec-vol2-introduction), Fault Tolerance acts as the **Immune System** of the Physical Layer. While the algorithmic layer assumes a perfect execution engine, the physical layer is built from chaotic, fallible matter. Thermodynamics ensures that hardware will fail; Fault Tolerance ensures the system survives.

The distributed training systems examined in @sec-distributed-training-systems achieve massive throughput by coordinating thousands of devices. However, this coordination creates fragility: a failure in any single device can stall the entire synchronous fleet. This chapter builds the resilience layer necessary to keep that fleet running.

The transition from small-scale experimentation to large-scale production changes the relationship between systems and failures. A researcher training a model on a single GPU might experience hardware failure once per year. That same researcher scaling to a 1,000 GPU cluster will experience failures multiple times per day. This shift from rare exception to routine occurrence demands different engineering approaches. The mathematical analysis that follows makes this transition precise and quantitative.

Understanding failure at scale requires abandoning the mindset that treats failures as bugs to be fixed. Individual component failures cannot be eliminated; they can only be managed. Memory errors, network partitions, storage corruption, and software crashes will occur with statistical regularity that increases predictably with system size. The engineering challenge is not to prevent these failures but to build systems that continue making progress despite them.

This perspective shift has profound implications for system design. Fault-tolerant systems cannot assume that any operation will succeed; they must verify completion and handle failure as a normal code path. Recovery mechanisms cannot be afterthoughts tested occasionally; they must be exercised continuously to ensure they work when needed. And coordination protocols must account for partial failures where some components succeed while others fail, leaving the system in states that naive error handling would not anticipate.

The techniques developed in this chapter draw from decades of distributed systems research but apply that research to the specific characteristics of ML workloads. ML training exhibits properties that enable fault tolerance strategies unavailable to general distributed systems: the mathematical properties of stochastic gradient descent tolerate certain types of errors that would corrupt other computations, checkpoint sizes are large but predictable, and recovery targets need only be approximate rather than exact. Exploiting these properties enables fault tolerance mechanisms specifically optimized for ML that achieve better efficiency than general-purpose approaches.

### The Mathematics of Inevitable Failure {#sec-fault-tolerance-reliability-reliability-mathematics-inevitable-failure-93ef}

System reliability engineering provides the foundational framework for understanding failure at scale [@birolini2017reliability]. Individual components exhibit failure rates characterized by the failure rate parameter $\lambda$,[^fn-failure-rate] measured in failures per unit time. For a single component with constant failure rate $\lambda$, the probability of surviving without failure until time $t$ follows an exponential distribution:

[^fn-failure-rate]: **Failure Rate ($\lambda$)**: The instantaneous probability of failure per unit time, assuming the component has survived until that point. For electronic components, $\lambda$ is typically expressed in FITs (Failures In Time), where 1 FIT equals one failure per billion device-hours. A GPU with 50,000-hour MTBF has $\lambda = 20$ FITs, meaning 20 failures per billion device-hours of operation.

$$ R_{single}(t) = e^{-\lambda t} $$ {#eq-single-component-reliability}

The mean time between failures (MTBF) for this component equals $1/\lambda$. Modern GPUs in datacenter environments exhibit MTBF values ranging from 40,000 to 100,000 hours depending on operating conditions, cooling effectiveness, and manufacturing variation.[^fn-gpu-mtbf]

[^fn-gpu-mtbf]: **GPU MTBF Variation**: Published MTBF figures for datacenter GPUs vary significantly based on measurement methodology and operating conditions. NVIDIA reports A100 MTBF of approximately 50,000 hours under specified conditions, but actual field experience shows substantial variation. Higher ambient temperatures, power supply fluctuations, and memory-intensive workloads all reduce effective MTBF. Google's published TPU failure data and Meta's GPU fleet telemetry suggest actual failure rates 20–50% higher than manufacturer specifications.

When multiple independent components operate in a system where any single failure causes system failure, @eq-system-reliability-product formalizes how system reliability becomes the product of individual component reliabilities:

$$ R_{system}(t) = \prod_{i=1}^{N} R_i(t) = \prod_{i=1}^{N} e^{-\lambda_i t} $$ {#eq-system-reliability-product}

For $N$ identical components with individual failure rate $\lambda$, this simplifies to:

$$ R_{system}(t) = e^{-N\lambda t} $$ {#eq-system-reliability-n-components}

The system failure rate becomes $N\lambda$, and @eq-system-mtbf expresses how the system MTBF scales inversely with component count. This inverse scaling reveals the counterintuitive reality of *the 9s of reliability* at cluster scale:

$$ MTBF_{system} = \frac{1}{N\lambda} = \frac{MTBF_{component}}{N} $$ {#eq-system-mtbf}

::: {.callout-notebook title="The 9s of Reliability"}
**Problem**: You have a cluster of **10,000 GPUs**. Each GPU is incredibly reliable, with **99.99%** availability (only 52 minutes of downtime per year). What is the probability that the **entire cluster** is up for **1 hour**?

**The Math**:

1.  **Single GPU Success Probability ($P_{1h}$)**: A GPU failing once per year (8760 hours) has hourly survival prob $\approx 1 - (1/8760) = 0.99988$.
2.  **Cluster Success Probability ($P_{cluster}$)**: $P_{cluster} = (P_{1h})^{10,000}$.
3.  **Calculation**: $0.99988^{10000} \approx \mathbf{0.30}$.

**The Systems Conclusion**: Even with 99.99% reliable hardware, a 10k GPU cluster has only a **30% chance** of surviving a single hour without a failure. Relying on hardware reliability is mathematically impossible at scale. You *must* handle failure in software.
:::

This linear relationship between component count and failure rate has a profound implication: *scale transforms failure* from a rare event into a continuous condition.

::: {.callout-perspective title="Scale Transforms Failure"}
A single GPU with MTBF of 50,000 hours (5.7 years) fails rarely enough that manual intervention suffices. A 10,000 GPU cluster with the same per-GPU reliability has system MTBF of 5 hours. Failures occur continuously, multiple times per day. Systems must be designed expecting failure, not hoping to avoid it.
:::

### Quantitative Reliability Analysis {#sec-fault-tolerance-reliability-quantitative-analysis}

Building quantitative understanding of system reliability requires moving beyond qualitative descriptions to precise mathematical frameworks. Reliability engineering provides formal methods for calculating failure rates, predicting system availability, and designing fault tolerance strategies based on measurable metrics.

#### MTBF and FIT Rate Calculations {#sec-fault-tolerance-reliability-mtbf-fit}

Mean Time Between Failures (MTBF) quantifies the expected operational time between successive failures. Complementing MTBF, the Failures In Time (FIT) rate provides a normalized measure, defined as the number of failures per billion ($10^9$) device-hours of operation:

$$\text{FIT} = \frac{10^9}{\text{MTBF}}$$

A GPU with MTBF of 50,000 hours has a FIT rate of $10^9 / 50,000 = 20,000$ FIT.

#### System Reliability Composition {#sec-fault-tolerance-reliability-composition}

Complex ML systems comprise multiple components whose individual failure rates combine to determine overall system reliability.

For **series systems** (e.g., a node where all 8 GPUs must work), failure of any component causes system failure. The system reliability $R_{\text{sys}}$ is the product of individual component reliabilities:
$$R_{\text{sys}} = \prod_{i=1}^{n} R_i$$
The system MTBF for a series configuration is $\text{MTBF}_{\text{sys}} = 1 / \sum \lambda_i$. This reveals the scaling challenge: adding components reduces system MTBF linearly.

For **parallel systems** providing redundancy (e.g., active-active model replicas), failure occurs only when all redundant components fail simultaneously:
$$R_{\text{sys}} = 1 - \prod_{i=1}^{n} (1 - R_i)$$

#### Worked Example: GPU Cluster Reliability {#sec-fault-tolerance-reliability-worked-example-cluster}

Consider training a **Llama-3 (Lighthouse)** 70B model on 1,024 A100 GPUs.
*   **GPU Logic**: 4,000 FIT per GPU $\times$ 1,024 = 4,096,000 FIT $\rightarrow$ **MTBF $\approx$ 244 hours**.
*   **HBM2 Memory**: 250 FIT per Mb. Total Mb = $1,024 \times 80 \text{ GB} \times 8 \times 1,024 \approx 671 \text{M Mb}$. Total FIT $\approx$ 167B FIT $\rightarrow$ **MTBF $\approx$ 21 seconds**.

Without ECC memory protection, large-scale training is impossible as corruption would occur every 21 seconds. With ECC providing a 100$\times$ reduction in soft error rates, the memory MTBF improves to 36 minutes, and the combined system MTBF becomes approximately **35 minutes**. This quantitative analysis justifies checkpoint frequencies of 15-30 minutes for large-scale training.

### Worked Example: Cluster MTBF Calculation {#sec-fault-tolerance-reliability-reliability-worked-example-cluster-mtbf-calculation-9255}

Consider a training cluster designed for large language model development with the following specifications:

- 10,000 NVIDIA H100 GPUs
- Individual GPU MTBF: 50,000 hours
- Each GPU connected to host via PCIe (MTBF: 200,000 hours)
- Each node contains 8 GPUs with shared power supply (MTBF: 100,000 hours)
- Network infrastructure per node (NIC, cables): MTBF 150,000 hours

#### Step 1: Calculate Failure Rate per GPU Subsystem {.unnumbered}

Each GPU operates within a failure domain that includes the GPU itself, its PCIe connection, and proportional shares of the power supply and network infrastructure.

$$ \lambda_{GPU} = \frac{1}{50,000} = 2.0 \times 10^{-5} \text{ failures/hour} $$

$$ \lambda_{PCIe} = \frac{1}{200,000} = 0.5 \times 10^{-5} \text{ failures/hour} $$

$$ \lambda_{power/GPU} = \frac{1}{8} \times \frac{1}{100,000} = 0.125 \times 10^{-5} \text{ failures/hour} $$

$$ \lambda_{network/GPU} = \frac{1}{8} \times \frac{1}{150,000} = 0.083 \times 10^{-5} \text{ failures/hour} $$

#### Step 2: Calculate Total Per-GPU Failure Rate {.unnumbered}

$$ \lambda_{total/GPU} = (2.0 + 0.5 + 0.125 + 0.083) \times 10^{-5} = 2.708 \times 10^{-5} \text{ failures/hour} $$

#### Step 3: Calculate Cluster Failure Rate and MTBF {.unnumbered}

$$ \lambda_{cluster} = 10,000 \times 2.708 \times 10^{-5} = 0.2708 \text{ failures/hour} $$

$$ MTBF_{cluster} = \frac{1}{0.2708} = 3.69 \text{ hours} $$

**Interpretation**: This cluster experiences a failure approximately every 3.7 hours on average. Over a 24-hour period, the expected number of failures is 6.5. A training run lasting one week will experience approximately 45 failures. Any training system operating at this scale must treat failure as a continuous condition, not an exceptional event.

@tbl-cluster-mtbf-scaling generalizes this analysis across cluster sizes, demonstrating the mathematical inevitability of frequent failures at scale.

| **Cluster Size (GPUs)** | **Individual GPU MTBF** |     **Cluster MTBF** | **Expected Failures per Day** |
|:----------------------|----------------------:|-------------------:|----------------------------:|
| 8                       |              50,000 hrs | 6,250 hrs (260 days) |                         0.004 |
| 64                      |              50,000 hrs |    781 hrs (32 days) |                          0.03 |
| 512                     |              50,000 hrs |      98 hrs (4 days) |                          0.24 |
| 1,000                   |              50,000 hrs |      50 hrs (2 days) |                          0.48 |
| 4,000                   |              50,000 hrs |             12.5 hrs |                           1.9 |
| 10,000                  |              50,000 hrs |                5 hrs |                           4.8 |
| 25,000                  |              50,000 hrs |                2 hrs |                          12.0 |

: **Cluster MTBF Scaling**: System-level mean time between failures decreases linearly with cluster size, transforming failures from rare events to continuous operating conditions at scale. A training cluster sized for modern LLM development (10,000+ GPUs) experiences multiple failures daily. {#tbl-cluster-mtbf-scaling}

### Failure Taxonomy {#sec-fault-tolerance-reliability-reliability-failure-taxonomy-e78c}

The MTBF calculations above tell us HOW OFTEN failures occur, which is critical for setting checkpoint intervals and sizing recovery infrastructure. But designing effective fault tolerance also requires understanding WHAT KIND of failures occur. A network partition that resolves in seconds demands different handling than a permanent GPU failure. A silent memory corruption that produces incorrect gradients requires different detection mechanisms than a node crash that stops responding entirely. Not all failures are equivalent, and understanding failure characteristics guides the selection of appropriate recovery mechanisms. The taxonomy presented here classifies failures along two primary dimensions: temporal behavior (transient versus persistent) and failure manifestation (fail-stop versus Byzantine).

#### Transient Failures {#sec-fault-tolerance-reliability-reliability-transient-failures-57b4}

Transient failures occur temporarily and resolve without intervention. Examples include:

- **Network packet loss**: Momentary congestion causes dropped packets, but retransmission succeeds
- **Memory bit flips**: Cosmic ray induced single-event upsets[^fn-seu] corrupt individual bits

[^fn-seu]: **Single-Event Upsets (SEUs)**: Bit flips caused by high-energy particles (cosmic rays at sea level, alpha particles from packaging materials) striking memory cells. At datacenter scale, SEU rates become significant: approximately one bit flip per gigabyte of RAM per month. ECC memory detects and corrects single-bit errors but cannot correct multi-bit errors in the same word, making silent corruption possible even with error correction.
- **Thermal throttling**: Temporary performance reduction due to temperature spikes
- **Software timeouts**: Temporary resource contention causes operation delays

Transient failures are particularly insidious in ML training because they may not trigger explicit errors. A transient memory bit flip during gradient computation produces incorrect gradients that propagate through subsequent training steps. The model continues training but produces subtly degraded results. Studies of large-scale training runs have documented cases where transient hardware errors caused training to diverge after hundreds of hours, wasting substantial compute resources [@dixit2021silent].[^fn-silent-corruption]

[^fn-silent-corruption]: **Silent Data Corruption in Training**: Meta's published analysis of large-scale training [@dixit2021silent] identified silent data corruption as a significant concern, with approximately 0.1% of training runs exhibiting anomalous loss trajectories traceable to hardware errors that did not trigger explicit failures. Detection required monitoring training loss statistics and comparing against expected convergence curves.

The appropriate response to transient failures depends on detection capability. Errors that trigger explicit exceptions can be handled through retry logic. Silent corruption requires validation mechanisms such as gradient checksums, periodic model evaluation, and statistical monitoring of training dynamics.

#### Fail-Stop Failures {#sec-fault-tolerance-reliability-reliability-failstop-failures-5cdd}

Fail-stop failures cause components to cease operation entirely and detectably. The failed component stops responding to requests and can be identified through timeout mechanisms. Examples include:

- **GPU hardware failure**: Memory errors cause device to become unresponsive
- **Node crash**: Operating system failure terminates all processes
- **Network partition**: Physical or logical disconnection isolates node from cluster
- **Storage failure**: Disk failure prevents checkpoint read/write operations

Fail-stop failures are the easiest class to handle because detection is straightforward: the component stops responding. Recovery involves replacing the failed component and restoring state from the most recent checkpoint. The primary challenge is minimizing detection time and recovery latency.

Detection time $T_{detect}$ typically involves heartbeat mechanisms where each worker periodically signals liveness to a coordinator. If no heartbeat arrives within timeout period $T_{timeout}$, the coordinator declares failure. Setting $T_{timeout}$ requires balancing false positive rate against detection latency. False positives declare healthy workers failed due to transient delays, while slow detection wastes compute during the detection window.

For a heartbeat interval of $H$ seconds and expected network delay variance $\sigma_d$, @eq-timeout-calculation defines the timeout heuristic:

$$ T_{timeout} = H + k\sigma_d $$ {#eq-timeout-calculation}

where $k$ typically ranges from 3 to 5 to achieve low false positive rates while maintaining reasonable detection speed.

#### Byzantine Failures {#sec-fault-tolerance-reliability-reliability-byzantine-failures-8a24}

While fail-stop failures are relatively straightforward to handle because the component stops responding, we detect the absence, and we replace it, a far more insidious class exists. What happens when a component continues operating but produces incorrect results? A GPU that returns wrong gradients without throwing errors, a network that delivers corrupted packets that pass CRC checks, or a worker that computes different results for identical inputs? These Byzantine failures represent the most challenging class.

**The Physics of Silent Corruption**

At the nanometer scale of modern transistors, hardware is not deterministic; it is probabilistic. Two primary mechanisms drive **Silent Data Corruption (SDC)**:

1.  **Single-Event Upsets (SEUs)**: High-energy particles (cosmic rays at sea level, alpha particles from packaging materials) strike memory cells or logic gates, flipping a bit from 0 to 1. At 10,000+ GPUs, this is a statistical certainty.
2.  **Manufacturing Variances**: "Marginal" chips that pass initial QA may exhibit bit flips only under specific voltage/temperature conditions (e.g., during the intense $di/dt$ swings of a backward pass).

**Case Study: Facebook's Compression SDC**
Facebook documented a pervasive SDC issue where a hardware fault caused a valid file to be reported as "size zero" during decompression [@dixit2021silent]. As @fig-sdc-example illustrates, the system "worked" (no crash), but data was silently deleted. In ML, this manifests as valid-looking but mathematically garbage gradients.

::: {#fig-sdc-example fig-env="figure" fig-pos="htb" fig-cap="**Silent Data Corruption Propagation**. Unexpected faults can return incorrect file sizes, leading to data loss during decompression and propagating errors through distributed querying systems. This example from Facebook emphasizes how silent errors bypass standard exception handlers. Source: Facebook (2021)." fig-alt="System diagram showing data flow from compressed storage through defective CPU to database. Arrows indicate processing stages where file size calculation returns zero, causing missing rows in output."}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\footnotesize]
\tikzset{%
helvetica/.style={align=flush center,font=\small\usefont{T1}{phv}{m}{n}},
Line/.style={line width=1.0pt,black!50,text=black},
cube/.style={cylinder, draw,shape border rotate=90, aspect=1.8,inner ysep=0pt,
    minimum height=34mm,minimum width=25mm, cylinder uses custom fill,
    cylinder body fill=black!07,cylinder end fill=black!25},
Box/.style={,
    inner xsep=2pt,
    node distance=1.1,
    draw=GreenLine,
    line width=0.75pt,
    font=\usefont{T1}{phv}{m}{n}\small,
    align=flush center,
    fill=GreenL,
    text width=29mm,
    minimum width=29mm, minimum height=10mm
  },
Box2/.style={helvetica,
    inner xsep=2pt,
    node distance=0.8,
    draw=VioletLine,
    line width=0.75pt,
    font=\usefont{T1}{phv}{m}{n}\small,
     align=flush center,
    fill=VioletL2,
    text width=32mm,
    minimum width=32mm, minimum height=8mm
  },
}
\definecolor{CPU}{RGB}{0,120,176}
%%%
\node[Box](B2){Scale math.pow()};
\node[Box,above=of B2](B1){Decompress file size calculation};

\begin{scope}[local bounding box = CPU,shift={($(B2)+(0,-2.6)$)},
                          scale=0.7, every node/.append style={transform shape}]
\node[fill=CPU,minimum width=56, minimum height=56,
            rounded corners=8,outer sep=2pt] (C1) {};
\node[fill=white,minimum width=44, minimum height=44] (C2) {};
\node[fill=CPU!40,minimum width=39, minimum height=39,
            align=center,inner sep=0pt,font=\usefont{T1}{phv}{m}{n}
            \fontsize{8pt}{9}\selectfont] (C3) {Defective\\CPU};

\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=3, minimum height=12,
           inner sep=0pt,anchor=south](GO\y)at($(C1.north west)!\x!(C1.north east)$){};
}
\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=3, minimum height=12,
           inner sep=0pt,anchor=north](DO\y)at($(C1.south west)!\x!(C1.south east)$){};
}
\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=12, minimum height=3,
           inner sep=0pt,anchor=east](LE\y)at($(C1.north west)!\x!(C1.south west)$){};
}
\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=12, minimum height=3,
           inner sep=0pt,anchor=west](DE\y)at($(C1.north east)!\x!(C1.south east)$){};
}
\end{scope}
%%
\begin{scope}[local bounding box = CY1,shift={($(B2)+(5,-0.1)$)}]
\node (CA1) [cube] {};
\node (CA2) [cube,minimum height=10pt, fill=CPU!60]at($(CA1.bottom)!0.1!(CA1.top)$) {};
\node (CA3) [cube,minimum height=10pt,fill=red!80]at($(CA2.bottom)+(0,2.6mm)$){};
\node (CA4) [cube,minimum height=10pt,fill=red!80]at($(CA3.bottom)+(0,2.6mm)$){};
\node (CA5) [cube,minimum height=10pt, fill=CPU!60]at($(CA1.bottom)!0.65!(CA1.top)$) {};
\node[align=center]at (CA1){Spark shuffle and\\ merge database};
\end{scope}
%%
\begin{scope}[local bounding box = CY2,shift={($(B2)+(-5,-0.1)$)}]
\node (LCA1) [cube] {};
\node[align=center]at (LCA1){Spark pre-shuffle \\ data store\\(compressed)};
\end{scope}
\node[single arrow, draw=black,thick, fill=VioletL,
      minimum width = 15pt, single arrow head extend=3pt,rotate=270,
      minimum height=7mm]at($(B2)!0.52!(B1)$) {};
\node[single arrow, draw=black,thick, fill=VioletL,
      minimum width = 15pt, single arrow head extend=3pt,rotate=270,
      minimum height=7mm]at($(B2)!0.39!(CPU)$) {};
%
\coordinate(DES)at($(DE1)!0.5!(DE6)$);
\coordinate(LEV)at($(LE1)!0.5!(LE6)$);
\node[single arrow, draw=black,thick, fill=VioletL, inner sep=1pt,
      minimum width = 14pt, single arrow head extend=2pt,anchor=east,
      minimum height=18mm](LS)at($(LEV)+(-0.5,0)$) {};
\node[single arrow, draw=black,thick, fill=VioletL, inner sep=1pt,
      minimum width = 14pt, single arrow head extend=2pt,anchor=west,
      minimum height=18mm](DS)at($(DES)+(0.5,0)$) {};
%
%fitting
\scoped[on background layer]
\node[draw=violet,inner xsep=6.5mm,inner ysep=6.5mm,outer sep=0pt,
yshift=2mm,fill=none,fit=(CPU)(B1),line width=2.5pt](BB1){};
\node[below=3pt of  BB1.north,anchor=north,helvetica]{Shuffle and merge};
%%%
\node[Box2,below left=0.5 of LS](N2){\textbf{2.} Compute (1.1)\textsuperscript{53}};
\node[Box2,below right=0.5 of DS,fill=BlueL,draw=BlueLine](R3){\textbf{3.} Result = 0};
\node[Box2,below right=0.3 and -2.5 of R3,text width=43mm](N3){\textbf{3.} Expected Result = 156.24};
%
\node[Box2,above= of CY2](N1){\textbf{1.} Compute file size for decompression};
\node[Box2,above= of CY1](N4){\textbf{4.} Write file to database if size $>$ 0};
\node[Box2,below right= 0.2 and -1.15of CY1](N5){\textbf{5.} Missing rows in DB};
%
\draw[Line,-latex](N5)|-(CA3.before bottom);
\draw[Line,-latex](N5.50)|-(CA4.6);
\draw[Line](N3.20)|-(R3);
\draw[Line,-latex](LCA1.top)|-(B1);
\draw[Line,latex-](CA1.top)|-(B1);
\end{tikzpicture}
```
:::

Real-world evidence of SDC in production systems confirms these risks. @fig-sdc-jeffdean shows corrupted data blocks accumulating in a shuffle and merge database at Google, where even a small fraction of corrupted blocks can cascade into significant data quality degradation.

![**Silent Data Corruption in Spark**. Modern AI systems, particularly those employing large-scale data processing like Spark, are vulnerable to silent data corruption (SDC) accumulating during data transfer and storage. SDC manifests in a shuffle and merge database, highlighting corrupted data blocks (red) amidst healthy data (blue/gray). Source: Jeff Dean at MLSys 2024, Keynote.](./images/jpg/sdc-google-jeff-dean.jpeg){#fig-sdc-jeffdean fig-alt="Database visualization with grid of data blocks. Red blocks indicate corrupted entries scattered among blue and gray healthy blocks in a shuffle and merge database structure."}

**Case Study: Google's Gradient Spikes**
Google reported that SDC in TPU pods often manifests as sudden, inexplicable spikes in gradient norm (@fig-sdc-training-fault). A single bit flip in an exponent can turn a $10^{-5}$ gradient into $10^{20}$, destroying weeks of training.

::: {#fig-sdc-training-fault fig-env="figure" fig-pos="htb" fig-cap="**Gradient Norm Deviation**. Transient hardware faults, such as silent data corruption (SDC), disrupt optimization by causing abrupt changes in gradient norms. Real-world data from Google's production fleet confirms that SDC anomalies manifest as visible spikes in gradient norm, indicating a disruption to the expected parameter update process. Source: Jeff Dean, MLSys 2024 Keynote." fig-alt="Line graph of gradient norm over training time showing smooth baseline with sudden spike anomaly caused by silent data corruption event."}
![](images/jpg/google_sdc_jeff_dean_anomaly.jpg)
:::

**Mitigation: Hot Spares and Checksums**
Google addresses this by maintaining "Hot Spares"—running the same computation on two distinct chips or having a standby ready to take over. If a "Sanity Checker" (monitoring loss/gradients) detects an anomaly, the workload is instantly migrated to the hot spare, and the suspect chip is drained for diagnostics (@fig-sdc-controller). This moves reliability from the *component* (which we cannot trust) to the *system* (which verifies the result).

::: {#fig-sdc-controller fig-env="figure" fig-pos="htb" fig-cap="**Hot Spare Redundancy**. Google's data centers utilize hot spare cores to maintain uninterrupted ML training despite hardware failures, seamlessly transitioning workloads from defective machines to backup resources. This approach contrasts with parallel redundancy techniques like DMR/TMR by providing a reactive fault tolerance mechanism that minimizes downtime and preserves data integrity during ML training. Source: Jeff Dean, MLSys 2024 Keynote." fig-alt="Four-panel sequence: normal training grid, defective machine marked red, SDC checker detecting fault, workload transferred to hot spare while defective unit sent for repair."}
```{.tikz}
\begin{tikzpicture}[line width=0.75pt,font=\small\usefont{T1}{phv}{m}{n}]
\definecolor{Green}{RGB}{84,180,53}
\definecolor{Red}{RGB}{249,56,39}
\definecolor{Blue}{RGB}{0,97,168}
%
\tikzset{%
helvetica/.style={align=flush center,font=\small\usefont{T1}{phv}{m}{n}},
 Line/.style={line width=2.0pt,black!50,rounded corners=7,-latex},
main/.style={circle, minimum size=5mm, line width=0.7mm,draw=red,keep name},
keep name/.style={prefix after command={\pgfextra{\let\fixname\tikzlastnode}}},
    red box/.style={
      append after command={
        node [rotate=-50,
          fit=(\fixname) ,
          fill=red,
          text width=1.3mm,
          inner sep=-\pgflinewidth,
          rectangle
        ] {}
      }
    }
}
\tikzset{
  Box/.style={helvetica,
    inner xsep=2pt,
    node distance=0.7,
    draw=Green,
    rounded corners,
    fill=Green,
    minimum width=11mm, minimum height=6mm
  },
 Text/.style={%
    inner sep=2pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor,
    helvetica,
    align=flush center,
    minimum width=10mm, minimum height=6mm
  },
}
\begin{scope}[local bounding box=M1,shift={(0,0)}]
\foreach \x in {1,2,3}{
    \foreach \y in {1,2,3}{
    \def\br{M1}
        \node[Box](R\y\x\br) at (1.3*\x,-0.8*\y) {};
    }
}
\node[Box,draw=Blue,fill=Blue]at(R32M1){};
\node[Box,draw=Siva,fill=Siva]at(R33M1){};
\node[below=0.2 of R32M1]{Normal training state};
\end{scope}

\begin{scope}[local bounding box=M1,shift={(4.5,0)}]
\foreach \x in {1,2,3}{
    \foreach \y in {1,2,3}{
    \def\br{M2}
        \node[Box](R\y\x\br) at (1.3*\x,-0.8*\y) {};
    }
}
\node[Box,draw=Blue,fill=Blue]at(R32M2){};
\node[Box,draw=Siva,fill=Siva]at(R33M2){};
\node[below=0.2 of R32M2,align=center,
            red](DM){Defective machine\\ causes SDC};
\node [main,red box] (c) at (R23M2){};
\draw[Line,red](R23M2)--++(0:1)|-(DM);
\end{scope}

\begin{scope}[local bounding box=M1,shift={(9.0,0)}]
\foreach \x in {1,2,3}{
    \foreach \y in {1,2,3}{
    \def\br{M3}
        \node[Box](R\y\x\br) at (1.3*\x,-0.8*\y) {};
    }
}
\node[Box,draw=Blue,fill=Blue]at(R32M3){};
\node[Box,draw=Blue,fill=none,line width=2pt]at(R23M3){};
\node[Box,draw=Siva,fill=Siva]at(R33M3){};
\node[below=0.2 of R32M3,align=center,
            Blue](SD){SDC checker\\ automatically\\ identifies SDC};
\node [main,red box] (c) at (R23M3){};
\draw[Line,Blue](R23M3)--++(0:1)|-(SD);
\end{scope}

\begin{scope}[local bounding box=M1,shift={(13.5,0)}]
\foreach \x in {1,2,3}{
    \foreach \y in {1,2,3}{
    \def\br{M4}
        \node[Box](R\y\x\br) at (1.3*\x,-0.8*\y) {};
    }
}
\node[Box,draw=Blue,fill=Blue]at(R32M4){};
\node[Box,draw=red,fill=white,line width=2pt]at(R23M4){};
\node[Box,draw=Blue,fill=Green,line width=2pt]at(R33M4){};
\node[below=0.2 of R32M4,align=center,
            Blue](SD1){SDC checker moves\\ training to hot spare\\
            and sends defective\\ machine for repair};
\node [main,red box] (c) at (R23M4){};
\draw[Line,Blue](R33M4)--++(0:1)|-(SD1);
\end{scope}

\begin{scope}[local bounding box=LE,shift={(3.5,0.4)}]
\node[Box,draw=Green,fill=Green](ZE){};
\node[right=2pt of ZE,font=\small\usefont{T1}{phv}{m}{n}
             \footnotesize](L1){Synchronous Training Worker};
\node[Box,draw=Blue,fill=Blue,right=of L1](PL){};
\node[right=2pt of PL,font=\small\usefont{T1}{phv}{m}{n}
             \footnotesize](L2){SDC checker};
%
\node[Box,draw=Siva,fill=Siva,right=of L2](SI){};
\node[right=2pt of SI,font=\small\usefont{T1}{phv}{m}{n}
             \footnotesize](L3){Hot spare};
\scoped[on background layer]
\node[draw=BackLine,inner xsep=10,inner ysep=6,yshift=0mm,
           fill=BackColor!60,fit=(ZE)(L3),line width=0.75pt](BB1){};
\end{scope}
\end{tikzpicture}
```
:::

Byzantine failures include:

- **Silent data corruption**: Memory or computation errors produce wrong values without triggering errors
- **Numerical instability**: Floating-point edge cases cause gradients to become NaN or infinity
- **Determinism violations**: Race conditions cause different workers to compute different results for identical inputs
- **Adversarial corruption**: Malicious actors intentionally inject incorrect gradients

Byzantine failures are particularly dangerous in distributed training because the standard assumption that workers compute identical gradients for identical data no longer holds. A single Byzantine worker can corrupt the averaged gradient, potentially causing training to diverge or converge to a poor solution. @fig-failure-types contrasts the straightforward detection of fail-stop failures with the insidious nature of Byzantine corruption.

::: {#fig-failure-types fig-env="figure" fig-pos="htb" fig-cap="**Fail-Stop vs. Byzantine Failures**. In the fail-stop model (left), a failed worker simply ceases to send messages, which is easily detected by timeouts. In the Byzantine model (right), a failed worker continues to participate but sends incorrect data (e.g., corrupted gradients reported as valid), which can poison the global model state if not detected by validational redundancy." fig-alt="Side-by-side diagrams. Left: fail-stop failure with worker W2 silent, dashed timeout arrow to coordinator. Right: Byzantine failure with W2 sending incorrect gradient 9.9 while W1 sends valid 0.5, resulting in poisoned update warning."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{GoodColor}{RGB}{200,255,200}
  \definecolor{BadColor}{RGB}{255,200,200}

  \tikzset{
    node_style/.style={circle, draw, minimum size=1cm, top color=white, bottom color=gray!10},
    msg/.style={draw, rectangle, rounded corners, font=\scriptsize, fill=white}
  }

  % Fail-Stop
  \node[anchor=south] at (2, 2.5) {\textbf{Fail-Stop Failure}};
  \node[node_style, fill=GoodColor] (C1) at (0, 0) {Coord};
  \node[node_style, fill=GoodColor] (W1) at (2, 1.5) {W1};
  \node[node_style, fill=BadColor, label={right:Silent}] (W2) at (4, 0) {W2 (X)};

  \draw[->, thick, green!60!black] (W1) -- node[msg, above, sloped] {Grad} (C1);
  \draw[->, thick, dashed, red] (W2) -- node[msg, below, sloped] {Timeout} (C1);

  % Byzantine
  \begin{scope}[xshift=7cm]
    \node[anchor=south] at (2, 2.5) {\textbf{Byzantine Failure}};
    \node[node_style, fill=GoodColor] (C2) at (0, 0) {Coord};
    \node[node_style, fill=GoodColor] (W3) at (2, 1.5) {W1};
    \node[node_style, fill=BadColor, label={right:Malicious}] (W4) at (4, 0) {W2 (?!)};

    \draw[->, thick, green!60!black] (W3) -- node[msg, above, sloped] {Grad: 0.5} (C2);
    \draw[->, thick, red] (W4) -- node[msg, below, sloped] {Grad: 9.9} (C2);

    \node[red, font=\bfseries] at (2, -1) {Poisoned Update!};
  \end{scope}
\end{tikzpicture}
```
:::

Detection of Byzantine failures requires redundant computation. Multiple workers computing gradients for the same data enable comparison of results. Statistical outlier detection can identify workers consistently producing anomalous gradients. These detection mechanisms add computational overhead and may not catch subtle corruption.

Byzantine-resilient distributed training algorithms exist but impose significant overhead. Algorithms such as Krum [@blanchard2017machine] and coordinate-wise trimmed mean [@yin2018byzantine] compute aggregates that are robust to a bounded number of Byzantine workers, but they require more communication and computation than simple averaging. @sec-robust-ai examines hardware faults and Byzantine failures in greater depth, including detection mechanisms and algorithmic resilience strategies.[^fn-byzantine-ml]

[^fn-byzantine-ml]: **Byzantine-Resilient ML**: Research on Byzantine-resilient distributed learning has produced algorithms like Krum [@blanchard2017machine] (selecting the gradient most similar to others), trimmed mean [@yin2018byzantine] (discarding extreme values before averaging), and signSGD [@bernstein2018signsgd] (using only gradient signs). However, these methods typically assume a bounded fraction of Byzantine workers (often < 50%) and may not fully protect against sophisticated attacks.

#### Correlated Failures {#sec-fault-tolerance-reliability-reliability-correlated-failures-95e8}

The reliability calculations in @sec-fault-tolerance-reliability-mathematics-inevitable-failure-93ef assume independent failures. Real systems exhibit correlated failures where multiple components fail simultaneously due to shared dependencies:

- **Power supply failure**: All GPUs in a node lose power simultaneously
- **Network switch failure**: All nodes connected to the switch become unreachable
- **Cooling system failure**: Thermal shutdown affects multiple racks
- **Software bugs**: A bug in the CUDA driver crashes all processes using that driver version
- **Operator error**: Misconfiguration affects entire cluster

Correlated failures violate the independence assumption underlying @eq-system-reliability-n-components. When failures are correlated, the actual system reliability is lower than the formula predicts. More importantly, correlated failures can defeat redundancy strategies. Three replicas of a model provide no availability benefit if all three run on the same power domain and a power failure takes out all three simultaneously.

Defending against correlated failures requires understanding failure domains and ensuring redundancy spans independent failure domains. @tbl-failure-domains catalogs common failure domains in ML infrastructure, from single GPUs to entire datacenter regions, each requiring distinct mitigation strategies. @fig-failure-domains illustrates how these domains nest hierarchically, with failures propagating downward through the containment structure.

| **Failure Domain**    | **Impact Scope**         | **Typical Recovery Time** | **Mitigation Strategy**      |
|:--------------------|:-----------------------|:------------------------|:---------------------------|
| **Single GPU**        | 1 GPU                    | Seconds (spare)           | Hot spare, elastic training  |
| **Node (power/OS)**   | 8 GPUs                   | Minutes                   | Checkpoint, node replacement |
| **Rack (ToR switch)** | 32–64 GPUs               | Minutes to hours          | Cross-rack redundancy        |
| **Power domain**      | 100–500 GPUs             | Hours                     | Multiple power feeds         |
| **Datacenter region** | All GPUs                 | Hours to days             | Geographic distribution      |
| **Software version**  | All GPUs running version | Minutes (rollback)        | Staged rollouts              |

: **Failure Domains in ML Infrastructure**: Understanding failure domain boundaries enables placement of redundant components across independent domains, preventing correlated failures from defeating redundancy strategies. {#tbl-failure-domains}

::: {#fig-failure-domains fig-env="figure" fig-pos="htb" fig-cap="**Hierarchy of Failure Domains**. Failure domains are often nested or overlapping. A GPU failure affects one device. A node failure affects 8 GPUs. A rack switch failure affects 32-64 GPUs. A power distribution unit (PDU) failure may affect multiple racks. Effective fault tolerance requires placing replicas across independent domains (e.g., different racks or rows) to survive correlated failures." fig-alt="Nested rectangles showing failure domain hierarchy. Region contains Zone A and Zone B. Each zone contains a rack with switch and PDU. Each rack contains a node with OS and PCIe. Each node contains multiple GPUs. Annotation explains containment."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{RegionColor}{RGB}{240,240,250}
  \definecolor{ZoneColor}{RGB}{230,230,255}
  \definecolor{RackColor}{RGB}{220,240,220}
  \definecolor{NodeColor}{RGB}{250,230,230}

  % Region
  \draw[fill=RegionColor, thick] (0,0) rectangle (10,6);
  \node[anchor=north west, font=\bfseries] at (0,6) {Failure Domain: Region (e.g., us-east-1)};

  % Zone A
  \draw[fill=ZoneColor, thick] (0.5, 0.5) rectangle (4.5, 5);
  \node[anchor=north west, font=\bfseries] at (0.5, 5) {Zone A};

  % Rack 1 (Zone A)
  \draw[fill=RackColor, thick] (1, 1) rectangle (4, 4);
  \node[anchor=north west, font=\bfseries] at (1, 4) {Rack (Switch/PDU)};

  % Node (Rack 1)
  \draw[fill=NodeColor, thick] (1.5, 1.5) rectangle (3.5, 3);
  \node[anchor=north west, font=\bfseries] at (1.5, 3) {Node (OS/PCIe)};

  % GPU
  \node[draw, fill=white, minimum width=0.5cm, minimum height=0.5cm] at (2, 2) {GPU};
  \node[draw, fill=white, minimum width=0.5cm, minimum height=0.5cm] at (3, 2) {GPU};

  % Zone B
  \draw[fill=ZoneColor, thick] (5.5, 0.5) rectangle (9.5, 5);
  \node[anchor=north west, font=\bfseries] at (5.5, 5) {Zone B};

  % Rack 2 (Zone B)
  \draw[fill=RackColor, thick] (6, 1) rectangle (9, 4);
  \node[anchor=north west, font=\bfseries] at (6, 4) {Rack};

   \node[align=left, font=\footnotesize] at (5, -1) {Hierarchical containment means failures propagate downwards.\\A Zone failure implies Rack, Node, and GPU failure.};

\end{tikzpicture}
```
:::

### The Bathtub Curve and Hardware Lifecycle {#sec-fault-tolerance-reliability-reliability-bathtub-curve-hardware-lifecycle-7d8a}

The failure taxonomy above classifies failure types and domains, answering WHAT KIND of failures occur. Equally important for designing fault tolerance is understanding WHEN in a component's lifetime failures are most likely to occur. Hardware failure rates are not constant over component lifetime. @fig-bathtub-curve illustrates the bathtub curve, a well-established model in reliability engineering that describes how failure rates vary across three distinct phases:

**Infant Mortality Phase**: New components exhibit elevated failure rates due to manufacturing defects, improper installation, and early-life wear-out of marginal components. This phase typically lasts days to weeks for electronic components. Burn-in testing[^fn-burn-in] operates components under stress conditions before deployment to precipitate infant mortality failures before production use.

[^fn-burn-in]: **Burn-in Testing**: A reliability screening process where components are operated at elevated temperature (typically 85–125°C) and voltage for 24–168 hours before deployment. The goal is to accelerate infant mortality failures, catching defective components before they reach production. Major cloud providers burn-in GPUs before deployment, which is why newly-received cloud instances typically exhibit lower failure rates than bare-metal deployments of new hardware.

**Useful Life Phase**: After surviving infant mortality, components exhibit relatively constant failure rates [@klutke2003critical]. This phase represents the longest portion of component lifetime and is the period where the exponential reliability model in @eq-single-component-reliability applies most accurately. For datacenter GPUs, the useful life phase typically spans 3–5 years.

**Wear-Out Phase**: As components age, failure rates increase due to accumulated wear. For GPUs, wear mechanisms include electromigration[^fn-electromigration] in circuits, thermal cycling stress on solder joints, and degradation of thermal interface materials. The onset of wear-out depends heavily on operating conditions; components operated at high temperatures or with frequent thermal cycling enter wear-out earlier.

[^fn-electromigration]: **Electromigration**: The gradual movement of metal atoms in conductors due to momentum transfer from electrons. At high current densities (common in modern chips with nanometer-scale wires), electromigration can create voids that increase resistance or opens that break circuits entirely. Operating at higher temperatures accelerates electromigration exponentially, which is why GPU thermal management directly impacts component lifespan.

The practical implication for ML systems is that fleet-wide failure rates depend on age distribution. A cluster populated entirely with new GPUs will experience elevated failure rates during the first few weeks, followed by a stable period, then increasing failures as the fleet ages. Mixed-age fleets exhibit more consistent aggregate failure rates because different cohorts are in different lifecycle phases.

::: {#fig-bathtub-curve fig-env="figure" fig-pos="htb" fig-cap="**The Bathtub Curve**. Hardware failure rates $\lambda(t)$ vary over time. (1) **Infant Mortality**: High failure rate initially due to manufacturing defects. (2) **Useful Life**: Constant, low failure rate where random failures dominate. (3) **Wear-Out**: Increasing failure rate as components age. Burn-in testing aims to filter out infant mortality failures before deployment." fig-alt="Line graph of failure rate versus component age showing bathtub shape. Three phases: infant mortality with high decreasing rate, useful life with constant low rate, and wear-out with increasing rate. Vertical dashed line marks burn-in period."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \begin{axis}[
    width=10cm, height=6cm,
    xlabel={Time (Component Age)},
    ylabel={Failure Rate $\lambda(t)$},
    xtick=\empty, ytick=\empty,
    axis lines=left,
    ymin=0, ymax=1.2,
    xmin=0, xmax=10,
    grid=none
  ]
    % Infant Mortality
    \addplot[domain=0.5:2, samples=50, red, ultra thick] {1/(x*2) + 0.2};
    \node[red, align=center, font=\footnotesize] at (axis cs: 1.2, 0.9) {Infant Mortality\\(Defects)};

    % Useful Life
    \addplot[domain=2:7, samples=2, blue, ultra thick] {0.2};
    \node[blue, align=center, font=\footnotesize] at (axis cs: 4.5, 0.35) {Useful Life\\(Random Failures)};

    % Wear Out
    \addplot[domain=7:9.5, samples=50, orange, ultra thick] {0.2 + 0.05*exp(x-7)};
    \node[orange, align=center, font=\footnotesize] at (axis cs: 8.5, 0.9) {Wear-Out\\(Aging)};

    % Burn-in Line
    \draw[dashed, thick, black!60] (axis cs: 2, 0) -- (axis cs: 2, 1.2);
    \node[anchor=north west, font=\scriptsize] at (axis cs: 2, 1.2) {Burn-in Period};
  \end{axis}

  \node[align=center, font=\footnotesize] at (5, -1.2) {Burn-in testing filters infant mortality.\\Proactive replacement preempts wear-out.};
\end{tikzpicture}
```
:::

Proactive maintenance strategies aim to replace components approaching wear-out before they fail in production. Predictive analytics using GPU telemetry can identify components likely to fail soon. Temperature trends, error counts, and performance degradation enable scheduled replacement during maintenance windows rather than unplanned outages during training runs.

### Model-Type Diversity in Failure Impact {#sec-fault-tolerance-reliability-reliability-modeltype-diversity-failure-impact-8ec0}

While the mathematics of failure rates apply universally, the cost of failure differs dramatically across model types. The impact of losing an hour of training depends on what that training costs, how much state must be recovered, and how long recovery takes. @tbl-failure-impact-by-model quantifies these factors across model architectures, revealing orders-of-magnitude variation from LLMs incurring millions of dollars in wasted compute to vision models losing modest amounts of progress.

| **Model Type**                   | **Typical Training Duration** | **Checkpoint Size** | **State Sensitivity**           | **Failure Cost**            |
|:-------------------------------|:----------------------------|------------------:|:------------------------------|:--------------------------|
| **GPT-4 / Llama-3 (Lighthouse)** | 2–4 weeks                     |          350–700 GB | High (position in curriculum)   | $2–5M compute per 24hr loss |
| **Vision (ViT-Large)**           | 1–3 days                      |              1–2 GB | Medium (augmentation state)     | $10–50K per day loss        |
| **DLRM (Lighthouse)**            | Continuous                    | 2–4 TB (embeddings) | Very High (embedding freshness) | Revenue impact per hour     |
| **Speech (Whisper-scale)**       | 3–7 days                      |             5–10 GB | Medium                          | $50–200K per day loss       |
| **Scientific (AlphaFold)**       | Days to weeks                 |            10–50 GB | High (exploration state)        | Research delay              |

: **Failure Impact by Model Type**: The cost of training failures varies dramatically by model type, driven by training duration, checkpoint overhead, and the value of accumulated training state. These differences demand model-specific fault tolerance strategies. {#tbl-failure-impact-by-model}

**Large Language Models** experience the highest absolute failure costs due to their extended training durations and the computational expense of each training hour. A GPT-4 scale training run consuming 25,000 GPUs at approximately $2 per GPU-hour incurs $1.2M in compute costs per day. A failure that loses 24 hours of training progress costs $1.2M in wasted compute plus schedule delay. The checkpoint overhead for models with hundreds of billions of parameters can itself become significant, with 700GB checkpoints requiring several minutes to write even with fast distributed storage.

**Recommendation Systems** present unique challenges because their training is often continuous rather than episodic. The value of a RecSys model derives partly from its freshness. Embeddings that capture recent user behavior outperform stale embeddings. A failure that loses hours of embedding updates may degrade recommendation quality in ways that directly impact revenue. Meta has documented that recommendation model freshness directly correlates with engagement metrics, making recovery time a business-critical metric.[^fn-recsys-freshness]

[^fn-recsys-freshness]: **RecSys Freshness Impact**: Meta's published research on deep learning recommendation models [@naumov2019deep] indicates that embedding staleness measured in hours can produce measurable degradation in recommendation relevance. Their continuous training infrastructure prioritizes minimal interruption over checkpoint optimization, using incremental checkpointing and elastic training to maintain freshness.

**Vision Models** occupy a middle ground with moderate training durations and manageable checkpoint sizes. The relatively small checkpoints enable frequent checkpointing with minimal overhead. ViT-Large checkpoints of 1–2 GB impose little overhead. Data augmentation state represents the primary state beyond model weights that must be preserved for reproducible recovery. Current augmentation parameters and data shuffling seed must be captured.

**Scientific Models** such as those used in protein structure prediction or climate simulation often have unique state requirements beyond model parameters. AlphaFold-style training may maintain exploration state tracking which protein families have been sampled, preventing repetition during recovery. Drug discovery models may track which molecular configurations have been evaluated. This domain-specific state complicates checkpoint and recovery design.

### Economic Framework for Fault Tolerance Investment {#sec-fault-tolerance-reliability-reliability-economic-framework-fault-tolerance-investment-e032}

Fault tolerance mechanisms consume resources: storage for checkpoints, bandwidth for checkpoint writes, compute cycles for redundant calculations, and engineering time for implementation and maintenance. Rational investment in fault tolerance requires quantifying both the cost of failures and the cost of prevention.

Failure costs include wasted compute, schedule delay, opportunity cost, and engineering time. Wasted compute measures GPU-hours expended on training steps that must be repeated. Schedule delay captures how extended time to a trained model impacts business timelines. Opportunity cost recognizes that compute consumed by recovery cannot be used for other training. Engineering cost accounts for time spent debugging failures and manually recovering.

Prevention costs include storage, throughput overhead, recovery infrastructure, and complexity. Storage cost scales with model size and checkpoint frequency. Checkpoint writes consume memory bandwidth and may stall training. Recovery infrastructure requires spare capacity and automated recovery systems. Fault tolerant systems are harder to develop and debug.

Optimal investment in fault tolerance balances these costs. For small-scale training on a few GPUs where failures are rare, minimal fault tolerance may be cost-optimal. Infrequent checkpoints and manual recovery suffice. For large-scale training on thousands of GPUs where failures occur multiple times daily, extensive fault tolerance provides positive return on investment. Frequent checkpoints, automatic recovery, and elastic training become essential. @fig-young-daly-storage visualizes how the trade-off between checkpoint overhead and recovery cost reaches an optimum that depends on both system MTBF and checkpoint write time.

@eq-ft-total-cost presents a simplified economic model for expected cost per training run:

$$ C_{total} = C_{compute} + E[N_{failures}] \times C_{per\_failure} + C_{ft} $$ {#eq-ft-total-cost}

where $C_{compute}$ is the base compute cost, $E[N_{failures}]$ is the expected number of failures during training, $C_{per\_failure}$ is the cost per failure, and $C_{ft}$ is the cost of fault tolerance mechanisms. The cost per failure includes wasted compute plus overhead.

@eq-ft-investment-criterion formalizes when fault tolerance investment is justified:

$$ \frac{\partial C_{ft}}{\partial x} < \frac{\partial (E[N_{failures}] \times C_{per\_failure})}{\partial x} $$ {#eq-ft-investment-criterion}

where $x$ represents investment in fault tolerance mechanisms. In practice, this means investing in fault tolerance until the marginal cost of additional protection exceeds the marginal reduction in failure costs.

Before diving into specific mechanisms, three foundational principles should guide every design decision.

::: {.callout-note title="Three Rules of Failure at Scale"}

1. **At scale, failures are continuous, not exceptional.** A 10,000-GPU cluster experiences failures every few hours. Systems must be designed expecting failure as normal operation.
2. **The optimal checkpoint interval is $\sqrt{2 \times T_{save} \times MTBF}$.** The Young-Daly formula provides quantitative guidance for checkpoint frequency. We derive this formula in @sec-fault-tolerance-reliability-checkpoint-restart-fundamentals-0ac2.
3. **Training and serving have fundamentally different fault tolerance requirements.** Training tolerates minutes of recovery time; serving requires milliseconds. This difference demands entirely different approaches.

:::

## Training Fault Tolerance {#sec-fault-tolerance-reliability-reliability-training-fault-tolerance-7f22}

The failure analysis in @sec-fault-tolerance-reliability-failure-analysis-scale-6b4b established that large-scale training systems will experience failures frequently. The checkpoint storage infrastructure developed in @sec-storage provides the foundation for training fault tolerance. This section develops the recovery mechanisms that use those storage capabilities to enable training to continue despite failures. Checkpoint and restart captures training state periodically so that failures lose only the work since the last checkpoint rather than all work since training began. However, the apparent simplicity of "save state, restore on failure" conceals substantial engineering complexity when applied to distributed systems training models with hundreds of billions of parameters.

### Checkpoint and Restart Fundamentals {#sec-fault-tolerance-reliability-reliability-checkpoint-restart-fundamentals-0ac2}

Checkpointing captures sufficient state to resume training from a recorded point: model parameters, optimizer state, training progress indicators, random state for reproducibility, and learning rate schedule position. The checkpoint sizes established in @sec-storage, reaching `{python} gpt3_ckpt_tb` TB for `{python} gpt3_params_b`B parameter models with Adam optimizer state,[^fn-adam-state] create both storage demands and recovery time constraints that fault tolerance mechanisms must address.

[^fn-adam-state]: **Adam Optimizer State**: Adam [@kingma2014adam] tracks first-moment ($m$, exponentially weighted gradient average) and second-moment ($v$, exponentially weighted squared gradient average) estimates for each parameter. This enables adaptive learning rates per parameter but triples memory requirements compared to vanilla SGD. For a `{python} gpt3_params_b`B parameter model, this means `{python} gpt3_adam_tb` TB of optimizer state alone, which dominates checkpoint size and recovery time.

#### Checkpoint Interval from Failure Analysis {#sec-fault-tolerance-reliability-reliability-checkpoint-interval-failure-analysis-c28d}

Checkpointing involves a critical trade-off: frequent checkpoints minimize lost work when failures occur but consume time and resources, while infrequent checkpoints minimize overhead but risk losing substantial work to failures.

The Young-Daly formula developed in @sec-storage provides the optimal checkpoint interval: $T_{opt} = \sqrt{2 \times T_{save} \times MTBF}$. The failure analysis in this chapter allows us to calculate the MTBF term this formula requires.

::: {.callout-example title="Optimal Checkpoint Interval"}
Let's apply the Young-Daly formula to a real-world scenario.

**Scenario**: You are training Llama-3 on a cluster of 16,000 GPUs.

*   **Checkpoint Cost ($C$)**: It takes **2 minutes** to save the model state to shared storage.
*   **Mean Time Between Failures ($M$)**: At this scale, you experience a silent data corruption or node failure every **3 hours** (180 minutes).

**Objective**: Find the optimal checkpoint interval $\tau$ that minimizes wasted time.

**Calculation**:
$$
\tau_{opt} = \sqrt{2 \cdot C \cdot M}
$$
$$
\tau_{opt} = \sqrt{2 \cdot (2 \text{ min}) \cdot (180 \text{ min})} = \sqrt{720} \approx \mathbf{26.8 \text{ minutes}}
$$

**Result**: You should checkpoint roughly every **27 minutes**.

*   **If you checkpoint every 10 mins**: You waste too much time writing to disk ($17\%$ overhead).
*   **If you checkpoint every 60 mins**: You lose too much work when the inevitable crash happens ($15\%$ overhead).
*   **At 27 mins**: You minimize the combined cost of overhead and recovery ($7\%$ total overhead).
:::

The U-shaped cost curve in @fig-young-daly-storage visualizes this trade-off: checkpoint overhead decreases as intervals lengthen, while rework cost from failures increases linearly. The optimal point minimizes their sum.

For our 10,000 GPU cluster with calculated system MTBF of 3.69 hours (from @sec-fault-tolerance-reliability-worked-example-cluster-mtbf-calculation-9255) and checkpoint time of 21 seconds (from `{python} gpt3_ckpt_tb` TB checkpoint at 100 GB/s), @eq-young-daly-applied computes the optimal interval:

$$ T_{opt} = \sqrt{2 \times 21s \times 3.69hr \times 3600s/hr} \approx 12.4 \text{ minutes} $$ {#eq-young-daly-applied}

This result demonstrates why failure analysis matters: without knowing the system MTBF, we cannot set checkpoint intervals rationally. With this interval, checkpoint overhead consumes approximately 2.8% of training time. However, practitioners should be aware of the *Young-Daly formula assumptions* that underpin this estimate.

::: {.callout-warning title="Young-Daly Formula Assumptions"}
The Young-Daly formula provides valuable intuition but rests on assumptions that may not hold in practice:

1. **Exponentially distributed failures**: Assumes constant failure rate. Real systems exhibit "bathtub curve" behavior with higher rates during burn-in and wear-out phases.
2. **Deterministic checkpoint time**: Assumes $T_{save}$ is constant. In practice, checkpoint time varies 2–3$\times$ due to storage contention, network congestion, and memory pressure.
3. **Recovery time equals checkpoint time**: Assumes recovery reads same data written during checkpoint. Often recovery takes 3–5$\times$ longer due to job scheduling delays, topology reconstruction, and warmup.
4. **Single failure mode**: Assumes one failure at a time. Correlated failures (power, cooling, shared switch) violate this assumption.
5. **Infinite timeline**: Optimal for long training runs. Short runs (where total time is comparable to MTBF) require different analysis.

When assumptions are violated, the optimal interval may shift significantly. As a rule of thumb, if restart overhead significantly exceeds checkpoint time, use $\sqrt{2 \times (T_{save} + T_{restart}) \times MTBF}$ instead.
:::

#### Checkpoint Overhead Analysis {#sec-fault-tolerance-reliability-reliability-checkpoint-overhead-analysis-f5df}

Beyond the time consumed by checkpoint writes, checkpointing imposes additional overhead through memory consumption and training disruption.

Checkpoint serialization requires memory buffers for gathering distributed state and preparing data for write. For synchronous checkpointing, all workers must hold their checkpoint data in memory until the checkpoint completes. This potentially requires significant additional memory allocation.

Synchronous checkpointing pauses training while the checkpoint writes. Even with fast storage, the pause disrupts the training pipeline and may cause GPU idle time. Data loading and forward passes cannot proceed during checkpoint operations.

@eq-checkpoint-overhead quantifies the wasted time due to checkpointing:

$$ O_{ckpt} = \frac{T_{save}}{T_{interval}} + \frac{T_{pause}}{T_{interval}} $$ {#eq-checkpoint-overhead}

where $T_{pause}$ represents any training pause beyond the checkpoint write time. This includes memory allocation, coordination, and serialization.

**The "Stop-the-World" Cost**

The financial impact of synchronous checkpointing at scale is severe. Consider a 10,000 GPU cluster training a frontier model.
*   **Idle Resource**: 10,000 H100 GPUs.
*   **Pause Duration**: 2 minutes (typical for multi-TB checkpoints on shared storage).
*   **Wasted Compute**: $10,000 \times \frac{2}{60} \approx 333$ GPU-hours.
*   **Financial Loss**: At ~$3/GPU-hour, a single checkpoint costs **$1,000** in wasted idle time.

If checkpoints occur hourly, the system wastes **$24,000 per day** simply waiting for storage I/O. This economic reality drives the aggressive adoption of asynchronous checkpointing strategies that move data movement off the critical path.

@tbl-checkpoint-overhead-by-model reveals how checkpoint characteristics vary dramatically by model architecture: larger models require longer write times but also benefit from correspondingly longer optimal intervals.

| **Model Type**         | **Checkpoint Size** | **Write Time (100 GB/s)** | **Optimal Interval (5hr MTBF)** | **Overhead** |
|:---------------------|------------------:|------------------------:|------------------------------:|-----------:|
| **GPT-3 (Lighthouse)** |              2.1 TB |                      21 s |                        14.5 min |         2.4% |
| **GPT-3.5 (20B)**      |              240 GB |                     2.4 s |                         4.6 min |         0.9% |
| **BERT-Large**         |              1.3 GB |                   0.013 s |                            22 s |        0.06% |
| **DLRM (Lighthouse)**  |                4 TB |                      40 s |                          20 min |         3.3% |
| **ResNet-50**          |              100 MB |                   0.001 s |                             6 s |        0.02% |
| **ViT-Large**          |              1.2 GB |                   0.012 s |                            21 s |        0.06% |

: **Checkpoint Overhead by Model Type**: Larger models with correspondingly larger checkpoints require longer save times but also benefit from longer optimal checkpoint intervals. The percentage overhead remains manageable (under 5%) for most configurations. {#tbl-checkpoint-overhead-by-model}

While @tbl-checkpoint-overhead-by-model suggests modest overhead percentages, real deployments often encounter checkpoint times far exceeding these theoretical estimates. Diagnosing such discrepancies requires examining the full system stack.

```{python}
#| label: checkpoint-debug-calc
#| echo: false

# 70B model checkpoint sizing
model_params_b = 70  # billions
bytes_per_param = 2  # BF16
weights_gb = model_params_b * bytes_per_param  # 140 GB
optimizer_gb = weights_gb * 2  # Adam first + second moments
total_ckpt_gb = weights_gb + optimizer_gb  # 420 GB

# Storage constraints
nfs_gbps = 10  # Gbps network
nfs_gbs = nfs_gbps / 8  # 1.25 GB/s
min_write_s = total_ckpt_gb / nfs_gbs  # seconds
min_write_min = min_write_s / 60  # minutes

# Contention analysis
n_nodes = 64
per_node_gbs = nfs_gbs / n_nodes  # GB/s per node
per_node_mbs = per_node_gbs * 1000  # MB/s per node
serialized_min = (total_ckpt_gb / per_node_gbs) / 60

# Training extension
overhead_pct = 30
base_weeks = 2
extended_weeks = base_weeks * (1 + overhead_pct / 100)
extra_cost_k = 500  # $K

# Format strings
weights_gb_str = f"{weights_gb:.0f}"
optimizer_gb_str = f"{optimizer_gb:.0f}"
total_ckpt_gb_str = f"{total_ckpt_gb:.0f}"
nfs_gbs_str = f"{nfs_gbs}"
min_write_s_str = f"{min_write_s:.0f}"
min_write_min_str = f"{min_write_min:.1f}"
per_node_mbs_str = f"{per_node_mbs:.0f}"
serialized_min_str = f"{serialized_min:.0f}"
extended_weeks_str = f"{extended_weeks:.1f}"
extra_cost_k_str = f"{extra_cost_k}"
```

::: {.callout-example title="Debugging Checkpoint Overhead"}

A team training a 70B parameter model observes that checkpointing takes 10 minutes per checkpoint, far exceeding their expected 2-minute target. Training throughput has dropped 30% because the cluster sits idle during checkpoints. How do we diagnose and resolve this?

**The Systems Sandwich Diagnosis**

The Systems Sandwich framework (@sec-vol2-introduction) structures our investigation across three layers:

**Physical Layer Analysis (Infrastructure)**

We begin at the bottom of the sandwich, examining the hardware constraints:

- **Model state size**: 70B parameters $\times$ 2 bytes (BF16) = `{python} weights_gb_str` GB for weights, plus `{python} optimizer_gb_str` GB for Adam optimizer states (first and second moments), totaling approximately `{python} total_ckpt_gb_str` GB per checkpoint
- **Storage system**: Shared NFS filer with 10 Gbps network attachment
- **Theoretical bandwidth**: 10 Gbps = `{python} nfs_gbs_str` GB/s maximum throughput
- **Minimum write time**: `{python} total_ckpt_gb_str` GB / `{python} nfs_gbs_str` GB/s = `{python} min_write_s_str` seconds (`{python} min_write_min_str` minutes)

The physical layer reveals our first insight: even with perfect efficiency, the NFS bandwidth cannot achieve a 2-minute checkpoint for this model size.

**Operational Layer Analysis (Distribution Logic)**

Moving to the middle layer, we examine the checkpoint coordination:

- **Checkpoint mode**: Synchronous, all 512 GPUs pause and wait
- **Write pattern**: All 64 nodes writing simultaneously to shared NFS
- **Observed behavior**: 10-minute writes instead of the `{python} min_write_min_str`-minute theoretical minimum

The operational layer reveals contention: 64 nodes competing for 10 Gbps creates severe congestion. Each node effectively receives only `{python} per_node_mbs_str` MB/s (10 Gbps / 64 nodes), extending checkpoint time to `{python} total_ckpt_gb_str` GB / `{python} per_node_gbs` GB/s ≈ `{python} serialized_min_str` minutes per node if writes were serialized. The observed 10 minutes reflects partial parallelism with significant queuing delays.

**Societal Layer Analysis (Control Plane)**

At the top layer, we consider organizational constraints:

- **SLA requirement**: Training must complete within 2 weeks
- **Current trajectory**: 30% overhead extends training to `{python} extended_weeks_str` weeks
- **Budget impact**: Extended training incurs additional \$`{python} extra_cost_k_str`K in compute costs

**The Solution: Layer-Aware Fix**

Understanding all three layers enables a targeted solution:

1. **Physical Layer**: Install local NVMe staging drives (3.5 GB/s per node)
2. **Operational Layer**: Implement asynchronous checkpointing:
   - Phase 1: GPU $\rightarrow$ CPU memory copy (fast, 32 GB/s PCIe)
   - Phase 2: CPU $\rightarrow$ local NVMe (3.5 GB/s, training resumes)
   - Phase 3: Background NVMe $\rightarrow$ NFS copy (overlapped with training)
3. **Validation**: New checkpoint overhead drops to 12 seconds (GPU-to-CPU copy), reducing training impact from 30% to under 1%

This example illustrates the Systems Sandwich principle: diagnosing distributed systems failures requires examining all layers. A purely algorithmic fix (Operational Layer) would fail because the physical bandwidth was insufficient. A purely hardware fix (Physical Layer) would be wasteful without understanding the coordination pattern. The solution required changes at multiple layers working in concert.
:::

#### Synchronous vs Asynchronous Checkpointing {#sec-fault-tolerance-reliability-reliability-synchronous-vs-asynchronous-checkpointing-ecc1}

The synchronous and asynchronous checkpointing approaches examined in @sec-storage create different failure recovery trade-offs. **Synchronous checkpointing** guarantees a globally consistent state, with all workers at the same training step, simplifying recovery logic. All workers coordinate to reach a consistent state, write their portions, and resume training only after all writes complete.

**Asynchronous checkpointing** reduces training disruption but requires tracking which workers have completed which checkpoints, adding complexity to recovery coordination. Workers snapshot their state to CPU memory or staging storage, then continue training while a background process writes the snapshot to persistent storage.[^fn-async-checkpoint]

[^fn-async-checkpoint]: **Asynchronous Checkpoint Implementation**: DeepSpeed [@rasley2020deepspeed] implements asynchronous checkpointing by maintaining a checkpoint staging buffer in CPU memory. When a checkpoint triggers, GPU state is copied to CPU asynchronously using CUDA streams, allowing GPU computation to continue. A separate thread writes staged data to storage while training proceeds. This achieves near-zero checkpoint overhead for models where staging memory is available.

#### Checkpoint Storage and Recovery {#sec-fault-tolerance-reliability-reliability-checkpoint-storage-recovery-6634}

The tiered checkpoint storage architecture developed in @sec-storage, with local NVMe for speed, distributed filesystem for durability, and object storage for long-term retention, provides the storage foundation on which recovery mechanisms operate. This section focuses on how recovery mechanisms leverage that infrastructure rather than the storage design itself.

For fault tolerance, the critical concern is not where checkpoints are stored but how quickly they can be read during recovery. Recovery time depends on storage tier bandwidth: local NVMe enables fastest recovery (5–10 GB/s per node), distributed filesystems provide moderate speed with durability (50–200 GB/s aggregate), while object storage offers slowest recovery but highest durability for disaster recovery scenarios.

### Distributed Checkpointing {#sec-fault-tolerance-reliability-reliability-distributed-checkpointing-f670}

Building on the distributed checkpoint architectures for sharded models established in @sec-storage, recovery requires understanding the coordination protocols that ensure checkpoint consistency. When training spans multiple workers, two primary approaches exist: centralized checkpointing where a coordinator gathers all state and writes a single checkpoint, and distributed checkpointing where each worker writes its own portion of the checkpoint.

#### Centralized Checkpointing {#sec-fault-tolerance-reliability-reliability-centralized-checkpointing-83a7}

In centralized checkpointing, workers send their state to a coordinator process that assembles and writes the complete checkpoint. This approach simplifies checkpoint management and produces self-contained checkpoint files but creates scalability bottlenecks.

All state flows through the coordinator, creating a network bottleneck. The coordinator must have memory for the entire checkpoint, creating a memory bottleneck. Coordinator failure loses the checkpoint operation, creating a single point of failure.

Centralized checkpointing works acceptably for small-scale distributed training but becomes impractical at large scale. Tens of workers can be supported, but not hundreds or thousands.

#### Distributed Checkpointing {#sec-fault-tolerance-reliability-reliability-distributed-checkpointing-5dcc}

In distributed checkpointing, each worker writes its portion of the checkpoint to a shared filesystem or object storage. A coordinator signals when to checkpoint and confirms completion, but state flows directly from workers to storage without aggregation.

**Coordination protocol**:

1. Coordinator broadcasts checkpoint request with checkpoint ID
2. Each worker reaches a consistent state (barrier synchronization)
3. Each worker writes its shard to `checkpoint_<id>/worker_<rank>.pt`
4. Each worker confirms write completion to coordinator
5. Coordinator writes checkpoint metadata after all confirmations
6. Coordinator broadcasts checkpoint complete, training resumes

This protocol ensures that either all workers complete their writes or the checkpoint is incomplete. Valid checkpoints have complete metadata. Incomplete checkpoints have missing metadata and can be detected. Partial checkpoints can be garbage collected. In practice, the strictness of this coordination defines distinct *checkpoint consistency models*.

::: {.callout-note title="Checkpoint Consistency Models"}
The idealized protocol above assumes step 2 completes quickly. At scale, this barrier synchronization becomes the dominant checkpoint cost because there is almost always at least one slow worker in a 10,000+ GPU cluster.

**Strict Synchronous**: All workers checkpoint at exactly the same training step. Provides strongest consistency but highest overhead from barrier synchronization.

**Bounded Asynchronous**: Workers may be within $k$ steps of each other (typically $k=1–3$). The checkpoint manager tracks the "checkpoint wavefront" across workers. Recovery uses the earliest consistent cut across all shards. This trades perfect consistency for dramatically reduced synchronization overhead and is what production systems actually use.

**Eventual Consistency**: Workers checkpoint when convenient, reconcile during recovery. Lowest overhead but requires complex recovery logic to reconstruct consistent state.
:::

The basic protocol has a subtle correctness bug. If the coordinator crashes after some workers confirm but before writing metadata, those workers believe the checkpoint succeeded while the system has no valid checkpoint.

Production systems use two-phase commit[^fn-two-phase-commit] to ensure correctness. Two-phase commit is a classic distributed systems protocol [@gray1978notes]:

[^fn-two-phase-commit]: **Two-Phase Commit (2PC)**: A distributed algorithm ensuring all participants either commit or abort a transaction together. Phase 1 (prepare): coordinator asks all participants to prepare and vote. Phase 2 (commit/abort): if all vote yes, coordinator broadcasts commit; otherwise abort. 2PC guarantees atomicity but can block indefinitely if the coordinator fails after prepare. For checkpointing, this blocking is acceptable since a stuck checkpoint can be timed out and retried.

In the prepare phase, workers write to staging location and report success to coordinator. In the commit phase, coordinator atomically renames or commits all shards together. For example, the coordinator moves files from `staging/` to `checkpoints/`. If the coordinator fails between phases, workers detect during next heartbeat and rollback staged writes.

This ensures atomic checkpoint commits. Either all shards are committed together, or none are.

**Consistency considerations**: In distributed training with synchronous gradient updates, workers naturally reach consistent states at step boundaries. Checkpointing at step boundaries ensures all workers have applied the same updates. With asynchronous training or pipeline parallelism, defining and reaching consistent states requires more careful coordination.

#### Sharded Checkpointing {#sec-fault-tolerance-reliability-reliability-sharded-checkpointing-58d0}

Modern distributed training frameworks partition model state across workers using techniques like ZeRO (Zero Redundancy Optimizer) and FSDP (Fully Sharded Data Parallel). In these configurations, no single worker holds complete model state. Each worker holds only its assigned parameter shard plus corresponding optimizer state.

**Sharded checkpointing**[^fn-sharded-ckpt] [@rajbhandari2020zero] leverages this distribution: each worker writes only its shard, dramatically reducing per-worker write volume. Recovery loads shards and redistributes state to workers based on the recovery configuration.

[^fn-sharded-ckpt]: **Sharded Checkpointing**: A checkpoint strategy where each worker saves only its local partition of the model and optimizer state, rather than gathering everything to a single writer. For ZeRO-3 or FSDP with 1000 workers training a `{python} gpt3_params_b`B model, each worker writes approximately `{python} gpt3_shard_gb` GB instead of one worker writing `{python} gpt3_ckpt_tb` TB. This parallelizes I/O across all workers, achieving aggregate write bandwidth of 100+ GB/s on modern distributed filesystems.

This approach enables efficient checkpointing even for massive models. A `{python} gpt3_params_b`B parameter model with `{python} gpt3_ckpt_tb` TB checkpoint distributed across 1,000 workers requires each worker to write only `{python} gpt3_shard_gb` GB, achievable in seconds with local NVMe storage.

When recovering with a different number of workers than the checkpoint, shard redistribution must remap state to the new worker configuration. This occurs due to elastic scaling or hardware changes. Modern frameworks support flexible resharding, enabling recovery even when worker count changes.

### Failure Detection and Recovery {#sec-fault-tolerance-reliability-reliability-failure-detection-recovery-0f54}

Checkpoints enable recovery, but the recovery process itself requires careful engineering. Recovery time directly impacts training efficiency. Faster recovery means less wasted compute waiting for the system to resume.

@eq-recovery-time decomposes recovery time into four primary components:

$$ T_{recovery} = T_{detect} + T_{restart} + T_{load} + T_{warmup} $$ {#eq-recovery-time}

where:

- $T_{detect}$: Time to detect the failure
- $T_{restart}$: Time to restart failed workers or reconfigure the job
- $T_{load}$: Time to load checkpoint state
- $T_{warmup}$: Time for training to reach steady-state performance (data pipeline fill, GPU warmup)

#### Failure Detection Mechanisms {#sec-fault-tolerance-reliability-reliability-failure-detection-mechanisms-efed}

**Heartbeat monitoring**: Each worker periodically sends heartbeat messages to a coordinator or monitoring service. Missing heartbeats trigger failure detection. Heartbeat interval and timeout parameters control the trade-off between detection speed and false positive rate.

During distributed training, collective operations hang if any participant fails. AllReduce and broadcast operations suffer from this issue. Framework-level timeouts on collective operations detect failures that would otherwise cause indefinite hangs. NCCL[^fn-nccl-timeout] provides configurable timeout parameters for this purpose. The `NCCL_TIMEOUT` parameter controls detection speed.

[^fn-nccl-timeout]: **NCCL Timeout Configuration**: NVIDIA's NCCL library provides `NCCL_TIMEOUT` (default 30 minutes in recent versions) and `NCCL_BLOCKING_WAIT` environment variables for failure detection. Setting aggressive timeouts (e.g., 5 minutes) enables faster failure detection but may cause false positives during legitimate long operations. Production systems typically set 10–30 minute timeouts and use separate heartbeat mechanisms for faster detection.

Container orchestration systems and cluster managers provide health check mechanisms that detect unresponsive workers. Kubernetes and SLURM offer these capabilities. Liveness probes verify that processes are running. Readiness probes verify that processes are ready to handle requests.

**Loss Spike Detection**

Silent hardware errors often fail to crash the process but corrupt the mathematical result. Bit flips in ALU logic exemplify this failure mode. In training, this manifests as a sudden, catastrophic spike in the loss function. Loss jumps 10x-100x or becomes NaN or infinity instantly. This is rarely an algorithmic issue if hyperparameters were stable. It is a signature of hardware failure. Systems should automatically pause training, identify which rank contributed the corrupted gradient, and drain that node before restarting from the last good checkpoint. Checksums or replay can identify the culprit rank.

**Training dynamics monitoring**: Monitoring loss values, gradient norms, and other training statistics can detect Byzantine failures that do not cause explicit errors. Sudden loss spikes or gradient explosions may indicate silent corruption.

In practice, the gap between theoretical heartbeat timeouts and actual detection latencies is substantial, because distinguishing genuine failures from temporary stragglers remains an inherently difficult problem.

::: {.callout-warning title="Realistic Failure Detection Latencies"}
Production experience shows that failure detection takes significantly longer than theoretical heartbeat timeouts suggest. The core challenge is distinguishing failures from stragglers:

| **Failure Type**           | **Typical Detection Time** | **Why**                                        |
|:-------------------------|-------------------------:|:---------------------------------------------|
| **Process crash**          |               5–30 seconds | Heartbeat timeout + verification retries       |
| **GPU hang**               |             30–120 seconds | Must distinguish from legitimately slow kernel |
| **Network partition**      |             60–180 seconds | Must distinguish from temporary congestion     |
| **Silent data corruption** |           Minutes to hours | Requires statistical anomaly detection         |

These latencies exist because aggressive timeouts cause false positives (killing healthy-but-slow workers), while conservative timeouts delay real failure detection. Production systems typically use multi-stage detection: fast initial timeout triggers investigation, slower confirmation timeout triggers recovery.
:::

#### Recovery Procedures {#sec-fault-tolerance-reliability-reliability-recovery-procedures-002d}

Upon failure detection, the recovery procedure executes:

1. **Job termination**: Stop all remaining workers to prevent inconsistent state

2. **Resource reclamation**: Release failed nodes, request replacement resources

3. **Job restart**: Launch new worker processes with recovered configuration

4. **Checkpoint loading**: Each worker loads its state shard from the latest valid checkpoint

5. **State synchronization**: Workers synchronize to ensure consistent starting state

6. **Training resumption**: Resume training from the checkpoint step

**Automatic recovery** systems perform these steps without human intervention. Modern training frameworks integrate with cluster managers to automate recovery. DeepSpeed's `deepspeed.launch` can be configured for automatic restart on failure. PyTorch's `torchrun` (elastic launch) provides similar capabilities.

**Recovery validation**: After loading a checkpoint, validation steps confirm successful recovery:

- Verify model parameters match expected shapes and dtypes
- Run a few training steps and verify loss is consistent with pre-failure values
- Confirm gradient computations produce expected statistics

#### Distinguishing Stragglers from Failures {#sec-fault-tolerance-reliability-reliability-distinguishing-stragglers-failures-285d}

A straggler is a worker that remains operational but performs slower than peers, causing other workers to wait during synchronization points. Stragglers arise from multiple causes. Hardware degradation causes thermal throttling or memory errors. Resource contention occurs when other processes consume CPU, memory, or network bandwidth. Data loading delays stem from slow storage access or network filesystem issues. Software inefficiency manifests when different workers hit different code paths.

The challenge is distinguishing stragglers from failures. Stragglers should trigger mitigation. Failures should trigger recovery. Aggressive timeouts treat stragglers as failures, causing unnecessary job restarts. Conservative timeouts waste compute waiting for stragglers.

**Straggler mitigation strategies**:

- **Backup workers**: Replicate work assigned to slow workers, use first result
- **Bounded staleness**: Allow training to proceed with stale gradients from slow workers
- **Dynamic load balancing**: Redistribute work away from slow workers
- **Proactive replacement**: Detect degrading workers and replace before they fail

### Elastic Training {#sec-fault-tolerance-reliability-reliability-elastic-training-4f87}

The checkpoint and restart mechanisms developed above share a common assumption: training operates with a fixed number of workers. When failures occur, we stop, recover state from checkpoint, and restart, ideally with replacement resources. But what if the system could adapt to failures dynamically, continuing training with fewer workers rather than stopping entirely? This alternative approach, **elastic training** (@fig-elastic-flow), enables dynamic scaling by adding or removing workers without stopping training.

::: {#fig-elastic-flow fig-env="figure" fig-pos="htb" fig-cap="**Elastic Training Recovery**. Unlike static training which aborts on failure, elastic training adapts. When a worker fails, the job pauses, redistributes the dataset and model shards across the remaining $N-1$ workers, and resumes training from the last consistent state. This capability transforms hard failures into temporary throughput degradations." fig-alt="Flowchart with 5 steps and decision diamond. Training on N GPUs flows to monitor alert diamond. On failure: pause training, rescale batch and learning rate to N-1 GPUs, resume training. Loop returns to monitoring. Annotation highlights key step."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm]
  \tikzset{
    proc/.style={draw, rectangle, rounded corners, minimum width=2.5cm, minimum height=1cm, align=center, fill=blue!10},
    decision/.style={draw, diamond, aspect=2, minimum width=2.5cm, align=center, fill=yellow!10},
    arrow/.style={->, >=stealth, thick}
  }

  \node[proc] (Start) {Training (N GPUs)};
  \node[decision, below=of Start] (Fail) {Monitor Alert:\\Node Failure?};
  \node[proc, below=of Fail, fill=red!10] (Pause) {Pause Training};
  \node[proc, below=of Pause] (Rescale) {Rescale Batch/LR\\to N-1 GPUs};
  \node[proc, below=of Rescale, fill=green!10] (Resume) {Resume Training};

  \draw[arrow] (Start) -- (Fail);
  \draw[arrow] (Fail) -- node[right] {Yes} (Pause);
  \draw[arrow] (Fail.east) -- ++(1,0) |- node[right, near start] {No} (Start);
  \draw[arrow] (Pause) -- (Rescale);
  \draw[arrow] (Rescale) -- (Resume);
  \draw[arrow] (Resume.east) -- ++(1.5,0) |- (Start);

  \node[right=0.5cm of Rescale, align=left, font=\footnotesize] {Key Step:\\Adjust Global Batch\\or Gradient Accum};
\end{tikzpicture}
```
:::

Elastic training provides several advantages. For fault tolerance, failures reduce worker count rather than stopping training. For resource efficiency, training can use variable resource allocations. For preemption handling, systems gracefully handle preemption in shared clusters. For cost optimization, systems scale based on spot instance availability.

#### Elastic Training Mechanisms {#sec-fault-tolerance-reliability-reliability-elastic-training-mechanisms-49b1}

Implementing elastic training requires adapting several training components:

**Batch size adjustment**: With fewer workers, each worker must process more samples to maintain the global batch size, or the global batch size must be reduced. Reducing global batch size may require learning rate adjustment.

**Learning rate scaling**: The relationship between batch size and optimal learning rate has been studied extensively. Goyal et al. [@goyal2017accurate] demonstrated that a linear scaling rule works well in practice: when scaling the batch size by factor $k$, scale the learning rate also by factor $k$. @eq-lr-scaling expresses an alternative square root scaling law that provides more conservative adjustment:

$$ \eta_{new} = \eta_{base} \times \sqrt{\frac{N_{new}}{N_{base}}} $$ {#eq-lr-scaling}

where $N$ represents the number of workers (and thus the global batch size). The linear rule [@goyal2017accurate] is often preferred for large batch training with warmup, while square root scaling provides a more conservative alternative when stability is a concern.

**Gradient accumulation**: To maintain effective batch size with fewer workers, each worker can accumulate gradients over multiple micro-batches before synchronization. If worker count drops by half, doubling the accumulation steps maintains the same effective batch size.

**Data loader redistribution**: When workers change, the data loader must redistribute data assignments. This requires coordination to ensure all data is processed and no data is duplicated or dropped.

**State resharding**: If using sharded model parallelism (ZeRO/FSDP), changing worker count requires redistributing model shards. This can be done online (migrating shards between workers) or through checkpoint reload (resharding during recovery).

#### Spot Instance Arbitrage {#sec-fault-tolerance-reliability-reliability-spot-instance-arbitrage-68ad}

Elastic training is not just a reliability mechanism; it is a powerful economic lever. Cloud providers offer preemptible ("spot") instances at 60–90% discounts compared to on-demand pricing, with the caveat that they can be reclaimed with little warning.

Traditional static jobs cannot use spot instances effectively because a single preemption kills the entire run. Elastic training transforms preemption from a fatal error into a recoverable resizing event. When a spot node is reclaimed:

1.  The job detects the node loss.
2.  It pauses briefly to redistribute the workload among surviving nodes (or waits for a replacement).
3.  Training resumes.

This capability allows organizations to train on massive clusters of cheap, unreliable hardware. The cost savings (e.g., $0.60/hour vs $3.00/hour) far outweigh the efficiency loss from occasional resizing pauses, often reducing total training bills by >50%.

#### Framework Support for Elastic Training {#sec-fault-tolerance-reliability-reliability-framework-support-elastic-training-8b75}

Modern frameworks provide varying levels of elastic training support:

**PyTorch Elastic (TorchElastic)** [@li2020pytorch]: Provides elastic launch capabilities through `torchrun`. Supports membership changes through a rendezvous mechanism.[^fn-rendezvous] Workers can join or leave, and the training process adapts. Integrates with Kubernetes for automatic scaling.

[^fn-rendezvous]: **Rendezvous Mechanism**: A distributed coordination protocol where workers discover each other and agree on training configuration before starting. TorchElastic implements rendezvous using etcd or a built-in C10d store, handling dynamic membership by re-running rendezvous when workers join or leave. The rendezvous assigns ranks, establishes the process group, and ensures all workers agree on world size before training proceeds.

**DeepSpeed** [@rasley2020deepspeed]: Supports elastic training through integration with Azure ML and automatic checkpoint/restart mechanisms. The ZeRO optimizer can reshard checkpoints for different worker counts.

**Ray Train**: Built on Ray's actor model, provides native elasticity. Workers are Ray actors that can be dynamically added or removed. Ray's distributed object store facilitates efficient state redistribution.

**Horovod Elastic** [@sergeev2018horovod]: Extends Horovod's data parallel training with elastic capabilities. Workers can join or leave during training, with automatic rank reassignment and gradient accumulation adjustment.

@tbl-elastic-training-comparison compares how modern frameworks vary significantly in their elastic training support, with differences in automatic recovery mechanisms and state management approaches.

| **Framework**       | **Elastic Support** | **Automatic Recovery** | **State Resharding** | **Cluster Integration** |
|:------------------|:------------------|:---------------------|:-------------------|:----------------------|
| **PyTorch Elastic** | Yes                 | Yes                    | Manual               | Kubernetes              |
| **DeepSpeed**       | Yes                 | Yes                    | Automatic            | Azure ML, SLURM         |
| **Ray Train**       | Yes                 | Yes                    | Automatic            | Ray Cluster             |
| **Horovod Elastic** | Yes                 | Yes                    | Manual               | SLURM, Kubernetes       |

: **Elastic Training Framework Comparison**: Modern frameworks provide varying levels of support for elastic training, with different approaches to automatic recovery and state management. {#tbl-elastic-training-comparison}

### Model-Specific Training Fault Tolerance {#sec-fault-tolerance-reliability-reliability-modelspecific-training-fault-tolerance-4b70}

The checkpoint and recovery strategies developed above require adaptation for different model types due to their distinct characteristics.

#### Large Language Models {#sec-fault-tolerance-reliability-reliability-large-language-models-5637}

LLM training presents the most demanding fault tolerance requirements due to massive checkpoint sizes and extended training durations.

**Checkpoint optimization for LLMs**:

- Use mixed-precision checkpoints (FP16/BF16 for weights, FP32 for optimizer critical state)
- Leverage ZeRO/FSDP sharding to distribute checkpoint writes
- Implement asynchronous checkpointing to minimize training disruption
- Use incremental checkpoints that store only changed state

**Curriculum and position tracking**: LLM training often uses curriculum learning (training on different data distributions over time) and position tracking (which documents have been processed). Checkpoint state must include curriculum position to ensure correct data presentation after recovery.

**Long-context considerations**: Models trained with long contexts (32K, 128K tokens) have larger activation memory and correspondingly larger per-step state. Checkpoint frequency may need adjustment to balance recovery granularity against checkpoint overhead.

#### Recommendation Systems {#sec-fault-tolerance-reliability-reliability-recommendation-systems-41d6}

Recommendation models with trillion-parameter embedding tables present unique checkpoint challenges.

**Embedding table checkpointing**: Embedding tables can be multiple terabytes. Full checkpointing at high frequency is impractical. Strategies include:

- **Incremental checkpointing**: Only save embeddings that changed since last checkpoint
- **Tiered checkpointing**: Frequent checkpoints for model parameters, infrequent for embeddings
- **Embedding versioning**: Maintain embedding versions with efficient delta storage

**Continuous training considerations**: RecSys models often train continuously on streaming data. The concept of "training completion" does not apply. Fault tolerance focuses on minimizing data loss and maintaining embedding freshness rather than protecting a single long training run.

**Feature store coordination**: RecSys training often depends on feature stores for user and item features. Checkpoint state must include feature versions to ensure consistency between model state and features used for training.

#### Vision Models {#sec-fault-tolerance-reliability-reliability-vision-models-b394}

Vision models have moderate checkpoint sizes but present unique considerations:

**Data augmentation state**: Reproducible training requires capturing augmentation state (random seeds, augmentation parameters). Recovery should produce identical training trajectories to minimize variance.

**Batch normalization synchronization**: Vision models using batch normalization require care during recovery. Running statistics must be consistent across workers. Synchronized batch norm requires explicit coordination of statistics during recovery.

**Multi-scale training**: Some vision training uses progressive resizing or multi-scale inputs. Checkpoint must capture current scale configuration and schedule position.

#### Scientific and Specialized Models {#sec-fault-tolerance-reliability-reliability-scientific-specialized-models-40e4}

Scientific models (protein structure prediction, molecular dynamics, climate simulation) often have domain-specific state:

**Exploration state**: Models exploring large search spaces (protein conformations, molecular configurations) must track what has been explored. Losing this state causes redundant exploration.

**Simulation state**: Models coupled with simulations must checkpoint both ML model state and simulation state for consistent recovery.

**Reproducibility requirements**: Scientific applications often require exact reproducibility for validation. Checkpoint and recovery must preserve complete determinism, requiring careful handling of all random state.

## Serving Fault Tolerance {#sec-fault-tolerance-reliability-reliability-serving-fault-tolerance-5375}

With training fault tolerance mechanisms established, we now turn to a fundamentally different challenge. Training fault tolerance accepts minutes of recovery time in exchange for protecting hours or days of training progress, a reasonable trade when each training hour on a large cluster costs thousands of dollars. Serving fault tolerance operates under constraints that make this trade impossible: millisecond latency requirements mean users cannot wait for recovery, and continuous availability expectations mean every minute of downtime directly impacts users and revenue. A training system that takes five minutes to recover loses five minutes of training time. A serving system that takes five minutes to recover may lose millions of requests and substantial revenue. This fundamental difference, minutes versus milliseconds, batch versus real-time, demands entirely different approaches.

This section develops fault tolerance mechanisms appropriate for serving's stringent requirements. While @sec-inference-scale examines distributed serving architectures comprehensively, this section focuses specifically on fault tolerance and reliability aspects.

### Stateless vs Stateful Serving {#sec-fault-tolerance-reliability-reliability-stateless-vs-stateful-serving-0a60}

The complexity of serving fault tolerance depends critically on whether the serving system maintains state across requests.

#### Stateless Serving {#sec-fault-tolerance-reliability-reliability-stateless-serving-5842}

In stateless serving, each request is independent. The serving system maintains no per-session state; all information needed to process a request is contained in the request itself plus the static model weights.

Examples of stateless serving:

- **Image classification**: Each image is classified independently
- **Object detection**: Each frame is processed independently
- **Single-turn text classification**: Each text snippet is classified without context
- **Embedding generation**: Each input is embedded independently

Fault tolerance for stateless serving is straightforward:

- **Redundant replicas**: Multiple copies of the model serve requests in parallel
- **Load balancing**: Requests are distributed across healthy replicas
- **Health checks**: Failed replicas are removed from the load balancer
- **Automatic replacement**: Failed replicas are restarted or replaced

When a replica fails, in-flight requests to that replica fail but can be retried on another replica. No state is lost. Recovery requires only starting a new replica and loading model weights, typically completing in seconds to minutes depending on model size.

#### Stateful Serving {#sec-fault-tolerance-reliability-reliability-stateful-serving-5761}

The simplicity of stateless serving makes it the preferred architecture whenever possible. However, many ML applications inherently require state across requests. Consider a chatbot: a single-turn question-answering system can operate statelessly, processing each question independently. But a conversational assistant that remembers previous exchanges must maintain conversation history, transforming fault tolerance from simple retry to state preservation.

Stateful serving maintains state across requests within a session. Subsequent requests depend on state accumulated from previous requests.

Stateful serving appears in multiple applications. LLM conversations accumulate KV cache across turns. Streaming speech recognition maintains context from previous audio. Recommendation sessions accumulate user context. Interactive editing maintains document state across edits.

Stateful serving complicates fault tolerance because state loss degrades service. KV cache loss requires reprocessing all previous turns. Session context loss forces users to repeat previous interactions. Accumulated state loss degrades quality when context is unavailable.

Fault tolerance for stateful serving requires multiple mechanisms. Session affinity routes requests within a session to the same replica. State checkpointing periodically saves session state for recovery. State replication maintains copies for high availability. Graceful degradation allows service to continue with reduced quality if state is lost.

@tbl-stateless-stateful-comparison contrasts how the fundamental difference between stateless and stateful serving manifests in every aspect of fault tolerance design, from request routing to recovery complexity.

| **Aspect**              | **Stateless Serving**        | **Stateful Serving**               |
|:----------------------|:---------------------------|:---------------------------------|
| **Request routing**     | Any replica                  | Session-affine replica             |
| **Failure impact**      | Retry on another replica     | Potential state loss               |
| **Recovery complexity** | Restart and load weights     | Reload state + reconstruct context |
| **Redundancy approach** | Active-active replicas       | Replicated state + standby         |
| **Failover latency**    | Milliseconds (load balancer) | Seconds (state transfer)           |

: **Stateless vs Stateful Serving Fault Tolerance**: Stateful serving introduces significant complexity in fault tolerance due to the need to preserve accumulated session state. {#tbl-stateless-stateful-comparison}

### Redundancy and Replication {#sec-fault-tolerance-reliability-reliability-redundancy-replication-2239}

Redundancy is the foundation of serving fault tolerance. By maintaining multiple copies of serving capability, the system can continue operating when individual copies fail.

#### Availability Calculations {#sec-fault-tolerance-reliability-reliability-availability-calculations-ef0a}

For a single replica with availability $A_{single}$ (probability of being operational at any given time), @eq-availability-redundancy quantifies how multiple independent replicas achieve higher system availability:

$$ A_{system} = 1 - (1 - A_{single})^R $$ {#eq-availability-redundancy}

where $R$ is the number of replicas.

**Worked Example**:

Single replica availability: $A_{single} = 99\%$ (3.65 days of downtime per year)

With two replicas: $A = 1 - (0.01)^2 = 99.99\%$ (52.6 minutes downtime per year)

With three replicas: $A = 1 - (0.01)^3 = 99.9999\%$ (31.5 seconds downtime per year)

This calculation assumes independent failures. Correlated failures reduce actual availability below these theoretical values. Shared power, shared network, and software bugs create correlation.

#### Replication Strategies {#sec-fault-tolerance-reliability-reliability-replication-strategies-aaa2}

**Active-active replication** (@fig-serving-redundancy, left): All replicas actively serve requests.

::: {#fig-serving-redundancy fig-env="figure" fig-pos="htb" fig-cap="**Serving Redundancy Strategies**. Comparison of Active-Active vs. Active-Passive replication. Active-Active (left) distributes load across all replicas, maximizing utilization but requiring capacity headroom to absorb failures. Active-Passive (right) keeps a standby replica idle and synchronized via heartbeat, simplifying failover logic at the cost of idle resource utilization." fig-alt="Two diagrams comparing replication strategies. Left: active-active with load balancer sending 50% to each of two green replicas. Right: active-passive with load balancer sending 100% to primary while dashed standby receives heartbeat sync."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{ActiveColor}{RGB}{200,255,200}
  \definecolor{PassiveColor}{RGB}{240,240,240}

  % Active-Active
  \node[anchor=south] at (2.5, 3.5) {\textbf{Active-Active}};
  \node[draw, fill=gray!10] (LB1) at (2.5, 3) {Load Balancer};
  \node[draw, fill=ActiveColor] (R1) at (1, 1) {Replica 1};
  \node[draw, fill=ActiveColor] (R2) at (4, 1) {Replica 2};

  \draw[->, thick] (LB1) -- node[left, font=\scriptsize] {50\%} (R1);
  \draw[->, thick] (LB1) -- node[right, font=\scriptsize] {50\%} (R2);

  % Active-Passive
  \begin{scope}[xshift=7cm]
    \node[anchor=south] at (2.5, 3.5) {\textbf{Active-Passive}};
    \node[draw, fill=gray!10] (LB2) at (2.5, 3) {Load Balancer};
    \node[draw, fill=ActiveColor] (R3) at (1, 1) {Primary};
    \node[draw, fill=PassiveColor, dashed] (R4) at (4, 1) {Standby};

    \draw[->, thick] (LB2) -- node[left, font=\scriptsize] {100\%} (R3);
    \draw[->, thick, dashed, gray] (LB2) -- (R4);

    \draw[<->, dashed, red] (R3) -- node[below, font=\scriptsize] {Heartbeat / Sync} (R4);
  \end{scope}
\end{tikzpicture}
```
:::

Load is distributed across replicas. Failure of one replica increases load on remaining replicas. This approach maximizes resource utilization but requires sufficient capacity in remaining replicas to handle increased load.

**Active-passive replication** (@fig-serving-redundancy, right): Primary replicas serve requests while standby replicas remain idle but ready. Failure of a primary triggers failover to standby. This approach provides simpler failover but wastes standby resources during normal operation.

**Geographic replication**: Replicas are distributed across geographic regions. This protects against regional failures (datacenter outage, regional network issues) but introduces latency for requests routed to distant regions.

**Multi-tier replication**: Different replication strategies operate at different levels. Edge caches are replicated for latency optimization. Regional serving clusters provide geographic coverage. Global primary ensures consistency and freshness.

#### Replica Placement and Failure Domains {#sec-fault-tolerance-reliability-reliability-replica-placement-failure-domains-73a7}

Effective redundancy requires placing replicas in independent failure domains. Different machines tolerate individual machine failures. Different racks tolerate rack-level failures from power and ToR switch issues. Different availability zones tolerate datacenter section failures. Different regions tolerate entire datacenter failures.

The level of independence should match the availability requirements and cost constraints. Regional replication is expensive but necessary for the highest availability requirements. It requires duplicate compute and network costs.

### Failover Mechanisms {#sec-fault-tolerance-reliability-reliability-failover-mechanisms-5ddd}

When a replica fails, traffic must be redirected to healthy replicas. The speed and reliability of this failover determines the impact of failures on users.

#### Health Checking {#sec-fault-tolerance-reliability-reliability-health-checking-a604}

Health checks verify that replicas are operational and ready to serve requests:

**Liveness checks**: Verify that the process is running and responsive. A simple HTTP endpoint that returns 200 indicates liveness. Failure to respond triggers process restart.

**Readiness checks**: Verify that the replica is ready to serve requests. For ML serving, readiness requires model weights loaded, GPU initialized and responsive, warmup complete, and dependencies available. The first inference is often slower, so warmup must complete. Feature stores and caches must be available as dependencies.

**Inference health checks**: Verify that inference produces correct results. Run a known input through the model and verify the output matches expected results. This catches silent failures where the model produces incorrect results without errors.

Health check parameters require tuning. Check interval determines how often to check. For example, every 5 seconds. Timeout determines how long to wait for response. For example, 2 seconds. Failure threshold determines how many failures before marking unhealthy. For example, 3 failures. Success threshold determines how many successes before marking healthy. For example, 2 successes.

#### Load Balancer Integration {#sec-fault-tolerance-reliability-reliability-load-balancer-integration-2acd}

Load balancers route requests to healthy replicas and remove unhealthy replicas from rotation. L4 load balancing routes based on IP and port, offering simple and fast operation. L7 load balancing routes based on HTTP and gRPC content, enabling sophisticated routing. Service mesh provides advanced traffic management, observability, and security.

Load balancer failover latency depends on health check frequency and failure detection logic. Aggressive settings enable fast failover but increase false positives. This marks healthy replicas as unhealthy during transient issues.

#### Session Affinity and Stateful Failover {#sec-fault-tolerance-reliability-reliability-session-affinity-stateful-failover-27b2}

For stateful serving, session affinity routes all requests within a session to the same replica:

Load balancers maintain session-to-replica mapping through sticky sessions. Implementation uses cookies, headers, or IP hashing.

State failover offers multiple options. State loss accepts degraded quality on failover by regenerating state from scratch. State checkpointing periodically saves session state for recovery. State replication copies state to standby replica. Distributed state stores session state in external stores like Redis or Memcached.

The choice depends on state size, update frequency, and quality impact of state loss.

| **Approach**                | **Recovery Latency** | **Consistency** | **Operational Complexity** |
|:--------------------------|:-------------------|:--------------|:-------------------------|
| **State loss**              | Fast                 | None            | Low                        |
| **Checkpointing**           | Medium               | Eventual        | Medium                     |
| **Synchronous replication** | Fast                 | Strong          | High                       |
| **Distributed state**       | Fast                 | Configurable    | Medium                     |

### Model-Specific Serving Fault Tolerance {#sec-fault-tolerance-reliability-reliability-modelspecific-serving-fault-tolerance-3245}

Different model types have distinct serving fault tolerance requirements based on their state characteristics and latency constraints.

#### LLM Serving Fault Tolerance {#sec-fault-tolerance-reliability-reliability-llm-serving-fault-tolerance-462b}

LLM serving with conversational context presents significant fault tolerance challenges:

**KV cache state**: The KV cache[^fn-kv-cache-ft] can be substantial (gigabytes for long contexts across attention layers). Losing the KV cache requires regenerating all previous turns, which can take seconds to minutes.

[^fn-kv-cache-ft]: **KV Cache Size**: For transformer models, the KV cache stores key and value projections for all previous tokens across all attention layers. Size scales as $2 \times L \times H \times S \times D$ where L is layers, H is heads, S is sequence length, and D is head dimension. For a 70B model with 80 layers, 64 heads, 128K context, and 128-dimension heads, this reaches approximately 160 GB per conversation, making KV cache state a dominant factor in LLM serving fault tolerance.

LLM serving fault tolerance takes multiple approaches. Accepting regeneration cost means regenerating KV cache from conversation history on failure. This approach is simple but can significantly increase latency for long conversations. KV cache checkpointing periodically saves KV cache state. This enables partial recovery but introduces storage overhead and latency for checkpointing. KV cache replication duplicates KV cache to standby. This provides fast failover but doubles memory requirements. Prefix caching stores common prefixes separately. System prompts and shared context are cached. On failure, common prefixes restore quickly. Only session-specific state requires regeneration.

**Prompt caching services** like those offered by cloud providers store and reuse KV cache for common prefixes, reducing both cost and recovery time for failures.

#### Recommendation Serving Fault Tolerance {#sec-fault-tolerance-reliability-reliability-recommendation-serving-fault-tolerance-1a91}

Recommendation systems have unique fault tolerance requirements centered on feature stores and real-time updates:

**Feature store availability**: Recommendations depend on user and item features from feature stores. Feature store unavailability degrades recommendation quality or blocks recommendations entirely.

Feature store fault tolerance employs multiple strategies. Replicated feature stores span availability zones. Local caching stores frequently accessed features. Fallback to stale features accepts quality degradation. Default features activate when feature lookup fails.

Some features update in real-time. Recent user actions exemplify real-time features. Failure of real-time feature pipelines causes recommendations to use stale data. Monitoring feature freshness and alerting on staleness is essential.

**Embedding service availability**: Large embedding tables may be served from dedicated embedding services. These services require their own fault tolerance through replication and failover.

#### Vision Serving Fault Tolerance {#sec-fault-tolerance-reliability-reliability-vision-serving-fault-tolerance-56b5}

Vision model serving is typically stateless, simplifying fault tolerance:

**Simple redundancy**: Multiple model replicas behind load balancer. Failure of one replica routes requests to others.

**GPU health monitoring**: Vision inference is GPU-intensive. GPU failures (thermal issues, memory errors) should trigger replica restart. NVIDIA's DCGM provides GPU health monitoring.

Vision models depend on preprocessing. Resize and normalize operations must execute correctly. Preprocessing failures should be detected and handled gracefully. Returning errors is preferable to incorrect predictions.

Vision models deployed on edge devices face different fault tolerance challenges. Device failures, network disconnection, and local storage limitations create unique problems. Edge fault tolerance often involves graceful degradation when cloud connectivity is lost.

## Graceful Degradation {#sec-fault-tolerance-reliability-reliability-graceful-degradation-9122}

The serving fault tolerance mechanisms developed above, redundancy, replication, and health monitoring, aim to maintain full service despite failures. But what happens when failures accumulate faster than redundancy can absorb them, or when resources become so constrained that full-quality service is impossible? Rather than failing completely, well-designed systems degrade gracefully: maintaining partial service at reduced quality rather than total failure. Users experience degraded quality but still receive value from the system.

### Degradation Dimensions {#sec-fault-tolerance-reliability-reliability-degradation-dimensions-4b25}

Service can degrade along multiple dimensions:

**Quality degradation**: Serve predictions from simpler, faster models with lower accuracy. A recommendation system might fall back from a sophisticated multi-tower model to a simpler collaborative filtering model.

**Latency degradation**: Accept longer response times to maintain quality. Under high load, batching more requests together increases latency but maintains throughput.

**Coverage degradation**: Serve partial results rather than complete results. A search system might return top-10 results instead of top-100 when compute is constrained.

**Freshness degradation**: Serve cached or stale results rather than real-time computation. News recommendations might serve hour-old recommendations when the recommendation service is unavailable.

**Feature degradation**: Use fewer features when feature retrieval fails. A recommendation system might use only content features when user history is unavailable.

### Graceful Degradation Strategies {#sec-fault-tolerance-reliability-reliability-graceful-degradation-strategies-e85e}

#### Model Fallback {#sec-fault-tolerance-reliability-reliability-model-fallback-bc6e}

Maintain multiple model versions with different resource requirements:

- **Primary model**: Full capability, highest resource requirements
- **Secondary model**: Reduced capability, lower resource requirements
- **Tertiary model**: Minimal capability, minimal resources
- **Static fallback**: Precomputed defaults, no inference required

When primary model is unavailable or overloaded, fall back to secondary. Continue falling back as necessary.

**Example cascade for image classification**:

1. **Primary**: ViT-Large (307M params, 88% ImageNet top-1)

2. **Secondary**: EfficientNet-B4 (19M params, 83% ImageNet top-1)

3. **Tertiary**: MobileNet-V3-Large (5.4M params, 75% ImageNet top-1)

4. **Fallback**: Return "classification unavailable" or cached results

Model fallback requires:

- Multiple models deployed and ready to serve
- Routing logic to select appropriate model
- Monitoring to track fallback frequency
- Quality metrics to measure degradation impact

#### Feature Fallback {#sec-fault-tolerance-reliability-reliability-feature-fallback-0730}

When feature retrieval fails, use default or computed fallback values:

**Default features**: Precomputed population-level defaults substitute for missing user or item features. For a recommendation system:

- Missing user embedding: Use average embedding across all users
- Missing item features: Use genre/category-level defaults
- Missing real-time features: Use most recent cached value

**Feature computation fallback**: Compute approximate features from available data:

- Missing user history: Use demographic similarity
- Missing item attributes: Use text embedding from title/description
- Missing contextual features: Use time-based defaults

**Feature importance tiers**: Prioritize features by importance to prediction quality:

| **Tier**      | **Example Features**          | **Missing Action** |   **Quality Impact** |
|:------------|:----------------------------|:-----------------|-------------------:|
| **Critical**  | User ID, Item ID              | Block request      |         Cannot serve |
| **Important** | User history, Item attributes | Use defaults       |   5–10% quality loss |
| **Useful**    | Real-time context             | Use cached         |    2–5% quality loss |
| **Optional**  | Secondary signals             | Omit               | &lt; 2% quality loss |

#### Load Shedding {#sec-fault-tolerance-reliability-reliability-load-shedding-8476}

When system capacity is insufficient for incoming load, deliberately drop requests to protect system stability:

**Random shedding**: Randomly drop a fraction of incoming requests. Simple but does not prioritize valuable requests.

**Priority-based shedding**: Classify requests by priority and drop low-priority requests first:

- Premium users serviced before free users
- Revenue-generating requests prioritized over analytics
- Interactive requests prioritized over background batch

**Admission control**: Rate limit at system entry points. Reject requests that would exceed capacity rather than accepting and degrading all requests.

Circuit breakers[^fn-circuit-breaker] prevent resource exhaustion by failing fast when dependent services are unhealthy.

[^fn-circuit-breaker]: **Circuit Breaker Pattern**: Borrowed from electrical engineering, where circuit breakers prevent electrical fires by cutting power during overload. In software, the pattern (popularized by Michael Nygard's "Release It!") wraps calls to external services and "trips open" after a threshold of failures, failing fast without attempting the call. This prevents resource exhaustion from waiting on failing services and gives downstream services time to recover.

Circuit breakers operate in three states: closed (normal operation), open (failing fast to prevent resource exhaustion), and half-open (probing for recovery). Examine @fig-circuit-breaker to understand the state transitions that enable this protective mechanism to both shield the system from cascading failures and automatically recover when conditions improve.

::: {#fig-circuit-breaker fig-env="figure" fig-pos="htb" fig-cap="**Circuit Breaker States**. The circuit breaker protects the system from cascading failure. **Closed**: Normal operation. **Open**: Error threshold exceeded; all requests fail fast to prevent resource exhaustion. **Half-Open**: After a timeout, a limited number of requests are allowed through to probe the dependency's health. Success resets to Closed; failure returns to Open." fig-alt="State diagram with three circular nodes. Green Closed state transitions to red Open state on errors exceeding threshold. Open transitions to yellow Half-Open on timeout. Half-Open returns to Closed on success or back to Open on failure."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=2.5cm]
  \tikzset{
    state/.style={circle, draw, minimum size=2cm, align=center, thick},
    arrow/.style={->, >=stealth, thick, bend left=45}
  }

  % States
  \node[state, fill=green!20] (Closed) {CLOSED\\(Normal)};
  \node[state, fill=red!20, right=of Closed] (Open) {OPEN\\(Fail Fast)};
  \node[state, fill=yellow!20, below=of Open] (Half) {HALF-OPEN\\(Probing)};

  % Transitions
  \draw[arrow] (Closed) edge node[above] {Errors > Threshold} (Open);
  \draw[arrow] (Open) edge node[right] {Timeout Expiry} (Half);
  \draw[arrow] (Half) edge node[below left] {Success} (Closed);
  \draw[arrow] (Half) edge node[left] {Failure} (Open);

\end{tikzpicture}
```
:::

#### Graceful Degradation Implementation {#sec-fault-tolerance-reliability-reliability-graceful-degradation-implementation-1aa5}

Implementing graceful degradation requires continuous monitoring of system health. Track request latency percentiles at p50, p95, and p99. Monitor error rates by error type. Measure resource utilization for CPU, GPU, and memory. Watch queue depths and wait times.

Degradation triggers define conditions that activate degradation. @lst-degradation-triggers illustrates three common trigger conditions that progressively activate fallback mechanisms based on latency, error rate, and feature store health.

::: {#lst-degradation-triggers lst-cap="**Progressive Degradation Triggers**: Conditions that activate graceful degradation based on tail latency, error rate, and feature store health. Each trigger activates a different fallback mechanism appropriate to the detected problem."}
```{.python}
# Monitor tail latency for user-facing impact
if p99_latency > threshold:
    activate_model_fallback()  # Switch to faster, simpler model

# Track error rates to detect downstream failures
if error_rate > threshold:
    activate_circuit_breaker()  # Fail fast, prevent cascade

# Feature store slowdowns degrade recommendation quality
if feature_store_latency > threshold:
    activate_feature_fallback()  # Use cached/default features
```
:::

Progressively increase degradation as conditions worsen rather than binary switching. Increase fallback percentage gradually as load increases.

Automatically recover from degraded state when conditions improve. Use hysteresis to prevent oscillation. Require sustained improvement before recovering.

### Degradation Monitoring and Alerting {#sec-fault-tolerance-reliability-reliability-degradation-monitoring-alerting-87c2}

Graceful degradation enables continued operation but at reduced quality. Monitoring ensures degradation is detected, measured, and addressed.

Track degradation metrics including percentage of requests served by fallback models, percentage of features using defaults, request drop rate from load shedding, and quality metric differences between primary and fallback.

Set alerting thresholds appropriately. Degradation activated triggers informational alert. Sustained degradation beyond 5 minutes triggers warning alert. Severe degradation affecting over 50% of requests triggers critical alert. Extended degradation beyond 1 hour triggers escalation.

After degradation events, conduct post-incident analysis. Analyze root cause of degradation, effectiveness of fallback mechanisms, user impact and business impact, and improvements to prevent future degradation.

## Distributed Debugging and Observability {#sec-fault-tolerance-reliability-reliability-distributed-debugging-observability-1a7f}

The graceful degradation strategies developed above assume we understand what is failing and why. Deciding whether to shed load, fall back to simpler models, or serve cached results requires knowing which components are degraded and how severely. But diagnosing failures in distributed ML systems presents unique challenges that demand specialized observability infrastructure. The non-determinism inherent in distributed computation, combined with the complexity of ML inference pipelines, makes failures difficult to reproduce and diagnose. This section develops the observability infrastructure necessary for effective debugging and informed degradation decisions.

### Why Distributed ML Systems Are Hard to Debug {#sec-fault-tolerance-reliability-reliability-distributed-ml-systems-hard-debug-8d8a}

Several factors combine to make distributed ML debugging exceptionally challenging.

Distributed systems exhibit non-deterministic behavior[^fn-heisenbug] from multiple sources. Network timing variations change execution order. Thread scheduling differences alter race conditions. GPU kernel execution order varies across runs. Floating-point operation ordering changes results. A bug that manifests on one execution may not reproduce on subsequent executions. These "Heisenbugs" appear to disappear when observed.

[^fn-heisenbug]: **Heisenbugs in Distributed Systems**: Named after the Heisenberg uncertainty principle, Heisenbugs are bugs that seem to disappear or change behavior when you try to observe them. In distributed ML systems, adding logging or debugging instrumentation changes timing, which can mask race conditions. Deterministic replay systems like rr (for single-node) or Dapper-style distributed tracing help by recording execution without altering timing.

**Partial failures**: Unlike single-machine systems where failures are typically total, distributed systems experience partial failures. Some components fail while others continue. The interaction between working and failed components produces complex failure modes.

**Scale**: With thousands of components, manual inspection is impossible. Automated tools must filter relevant information from massive telemetry streams.

**Emergent behavior**: System behavior emerges from interactions between components. Individual components may appear healthy while system-level behavior is incorrect.

ML systems face additional debugging challenges. Silent accuracy degradation produces wrong results without errors. Numerical issues like NaN and infinity propagate through computation. Data-dependent bugs manifest only for specific inputs. Distinguishing intentional model behavior changes from bugs becomes difficult. Learning causes expected behavior changes that resemble bugs.

### Observability Pillars {#sec-fault-tolerance-reliability-reliability-observability-pillars-0e45}

Effective distributed debugging requires three observability pillars: metrics, logs, and traces.

#### Metrics {#sec-fault-tolerance-reliability-reliability-metrics-a1e0}

Metrics are numerical measurements collected over time.

Infrastructure metrics include CPU and GPU utilization, memory usage and allocation, network bandwidth and latency, and storage I/O and latency.

Application metrics include request rate and latency percentiles, error counts by error type, queue depths and wait times, and cache hit rates.

ML-specific metrics include inference latency by model, batch utilization, feature retrieval latency, and model prediction distributions for drift detection.

Metrics enable real-time dashboards showing system health, alerting when metrics exceed thresholds, capacity planning based on utilization trends, and performance analysis and optimization.

#### Logs {#sec-fault-tolerance-reliability-reliability-logs-9906}

Logs capture discrete events with context:

**Structured logging**: Use structured formats (JSON) with consistent fields. @lst-structured-log-entry shows a typical error entry that captures both the failure context and the trace identifiers needed to correlate events across services.

::: {#lst-structured-log-entry lst-cap="**Structured JSON Log Entry**: A structured log entry for a GPU memory allocation failure, including trace and span identifiers for correlation with distributed traces and resource metrics for diagnosis."}
```{.json}
{
  "timestamp": "2024-01-15T10:23:45.123Z",
  "level": "ERROR",
  "service": "inference-server",
  "trace_id": "abc123",
  "span_id": "def456",
  "message": "GPU memory allocation failed",
  "gpu_id": 3,
  "requested_bytes": 4294967296,
  "available_bytes": 2147483648
}
```
:::

**Log levels**: Use consistent log levels:

- DEBUG: Detailed diagnostic information
- INFO: General operational information
- WARN: Potential problems that don't prevent operation
- ERROR: Problems that prevent specific operations
- FATAL: System-wide failures requiring immediate attention

**Log aggregation**: Centralize logs from all components for search and analysis. Tools like Elasticsearch, Loki, or cloud logging services enable searching across distributed logs.

#### Traces {#sec-fault-tolerance-reliability-reliability-traces-ffc5}

Traces track requests across distributed components:

**Distributed tracing concepts**:

- **Trace**: End-to-end journey of a request
- **Span**: Single operation within a trace
- **Context propagation**: Passing trace context between services

**Trace example for ML inference**:

```text
Trace: user-request-12345
├── Span: api-gateway (5ms)
│   └── Span: auth-service (2ms)
├── Span: feature-service (15ms)
│   ├── Span: user-feature-lookup (8ms)
│   └── Span: item-feature-lookup (12ms)
├── Span: inference-service (45ms)
│   ├── Span: preprocessing (3ms)
│   ├── Span: model-inference (40ms)
│   └── Span: postprocessing (2ms)
└── Span: response-formatting (1ms)
Total: 68ms
```

**Tracing tools**: OpenTelemetry[^fn-opentelemetry] provides a standard API for distributed tracing. Backend systems like Jaeger, Zipkin, or cloud tracing services store and visualize traces.

[^fn-opentelemetry]: **OpenTelemetry**: A CNCF project that merged OpenTracing and OpenCensus to provide a vendor-neutral standard for telemetry (traces, metrics, logs). OpenTelemetry defines APIs, SDKs, and a wire protocol (OTLP) for collecting and exporting observability data. Adopting OpenTelemetry avoids vendor lock-in and enables switching between backends (Jaeger, Datadog, Honeycomb, etc.) without code changes.

### ML-Specific Debugging {#sec-fault-tolerance-reliability-reliability-mlspecific-debugging-27d5}

Beyond general distributed debugging, ML systems require specialized debugging capabilities:

#### Numerical Debugging {#sec-fault-tolerance-reliability-reliability-numerical-debugging-ee2a}

ML computations are prone to numerical issues:

**NaN detection**:[^fn-nan-propagation] Because NaN values propagate silently through all downstream computations, early detection is essential. @lst-nan-detection shows a minimal check that catches corruption before it reaches users.

[^fn-nan-propagation]: **NaN Propagation**: In floating-point arithmetic, NaN (Not a Number) values propagate through computations: any operation involving NaN produces NaN. This makes a single NaN in gradient computation corrupt all downstream parameters. Common causes include division by zero (e.g., in normalization layers with zero variance), log of zero or negative numbers, and overflow to infinity followed by subtraction. Early NaN detection prevents wasted training iterations.

::: {#lst-nan-detection lst-cap="**NaN Detection**: Checking model outputs for NaN values prevents silent corruption from propagating to downstream consumers. Logging the input hash enables reproducibility during diagnosis."}
```{.python}
# Check tensor for NaN values
# (propagate from any corrupted computation)
if torch.isnan(output).any():
    log.error("NaN detected in output", input_hash=hash(input))
    # Log input hash for reproducibility, then fallback or fail fast
```
:::

During training, monitor gradient statistics. Gradient norm detects explosion or vanishing. Gradient distribution detects anomalies. Layer-wise gradients identify problematic layers.

Mixed-precision training[^fn-mixed-precision-issues] can introduce numerical issues. Monitor for loss scale adjustments indicating underflow, gradient overflow exceeding FP16 range, and inconsistency between FP16 and FP32 results.

[^fn-mixed-precision-issues]: **Mixed-Precision Numerical Issues**: FP16 has limited dynamic range (approximately 6x10^-8 to 65504), causing underflow for small gradients and overflow for large values. Loss scaling multiplies loss by a large factor (e.g., 1024) before backpropagation to prevent gradient underflow, then divides gradients by the same factor. When overflow occurs, the loss scaler automatically reduces the scaling factor and skips the corrupted step.

#### Data Debugging {#sec-fault-tolerance-reliability-reliability-data-debugging-9dfd}

ML bugs often originate in data. Validate inputs match expected format. Shape must match expected dimensions. Values must be in expected ranges. Required fields must be present. Encoding must match expected format.

Track feature distributions over time. Detect distribution shift when feature values change. Detect missing features when null rates increase. Detect outliers when extreme values appear.

Trace data transformations. Log intermediate data shapes and statistics. Validate transforms produce expected results. Compare pipeline outputs against known-good data.

#### Straggler Detection and Analysis {#sec-fault-tolerance-reliability-reliability-straggler-detection-analysis-6925}

Identify and diagnose slow components:

**Timing instrumentation**: Measuring time for each operation enables latency attribution across pipeline stages. @lst-timing-instrumentation wraps operations in context managers that emit per-span timing metrics, making it straightforward to compare performance across replicas.

::: {#lst-timing-instrumentation lst-cap="**Operation-Level Timing**: Context manager instrumentation attributes latency to individual pipeline stages, enabling straggler detection by comparing the same span across replicas."}
```{.python}
# Wrap operations in timing context managers for latency attribution
with timer("feature_lookup"):
    features = feature_store.lookup(
        ids
    )  # Often the latency bottleneck
with timer("model_inference"):
    predictions = model(features)  # GPU time, compare across replicas
```
:::

Compare component timing across replicas using percentile analysis. p50 shows typical performance. p99 shows tail latency. Comparing p99 across replicas identifies stragglers.

Stragglers arise from multiple root causes. Hardware issues include thermal throttling and memory errors. Data skew means some inputs are slower to process. Resource contention occurs when other processes consume resources. Network issues create slow connection to data stores.

### Common Failure Patterns {#sec-fault-tolerance-reliability-reliability-common-failure-patterns-3306}

The observability pillars above enable detection of recurring failure patterns that experience with large-scale ML systems has identified. The following patterns illustrate how metrics, logs, and traces work together in diagnosis.

#### Training Failures {#sec-fault-tolerance-reliability-reliability-training-failures-bc3a}

**Loss spike then recovery**: Transient data issue or numerical instability caused temporary spike. Usually self-correcting but should be investigated.

**Loss spike then plateau**: Learning rate too high, corrupted checkpoint, or data bug. Requires investigation and potentially rollback.

**Gradual divergence**: Silent data corruption, hardware error, or distributed training desynchronization. Difficult to detect, requires monitoring.

**Hang without error**: Deadlock in collective communication, crashed worker blocking synchronization. Requires timeout detection.

#### Serving Failures {#sec-fault-tolerance-reliability-reliability-serving-failures-3322}

**Latency spike**: Resource contention, garbage collection, cold cache, or model reload. Usually transient but repeated spikes indicate capacity issues.

**Error rate increase**: Dependency failure, data format change, or model bug. Requires immediate investigation.

**Silent quality degradation**: Model drift, feature degradation, or data pipeline issues. Requires quality monitoring to detect.

**Cascade failure**: One failing component causes others to fail through timeout exhaustion, resource depletion, or error propagation. Requires circuit breakers and isolation.

Cascade failures are particularly insidious because the root cause is obscured by the symptoms it generates. The following example illustrates how the three observability pillars work together to trace a cascade back to its origin.

::: {.callout-tip title="Diagnosing a Cascade Failure"}
Consider a recommendation system experiencing sudden latency spikes. The three observability pillars work together to diagnose the root cause:

**Metrics** reveal the symptom: p99 latency jumped from 45ms to 800ms at 14:32, with error rate increasing from 0.1% to 15%.

**Traces** isolate the bottleneck: traces show the feature-service span consuming 700ms instead of the normal 12ms. The model-inference span remains normal at 40ms.

**Logs** identify the root cause: feature-service logs show repeated connection timeouts to the user embedding cache at 14:31, followed by cache miss storms as requests bypass the failed cache and hit the embedding database directly.

The diagnosis: cache node failure caused cache miss avalanche, overwhelming the embedding database and propagating latency to all requests. The fix: circuit breaker on cache access, falling back to default embeddings when cache is unavailable.
:::

## Case Studies {#sec-fault-tolerance-reliability-reliability-case-studies-927f}

This section examines how leading organizations implement fault tolerance in production ML systems, illustrating the principles developed throughout this chapter.

### Large-Scale LLM Training Fault Tolerance {#sec-fault-tolerance-reliability-reliability-largescale-llm-training-fault-tolerance-f4aa}

Training models at GPT-4 scale or larger requires fault tolerance capable of handling hundreds of failures during multi-week training runs. Recent publications from organizations training frontier models provide detailed insights into these challenges [@jiang2024megascale; @dubey2024llama].[^fn-openai-scale]

[^fn-openai-scale]: **Large-Scale Training**: MegaScale [@jiang2024megascale] documented training at 12,288 GPU scale with over 100 automated recoveries. Meta's Llama 3 training [@dubey2024llama] experienced 466 job interruptions over 54 days on 16,384 GPUs, with 78% attributed to hardware issues. At this scale, the infrastructure experiences multiple failures per hour, requiring sophisticated fault tolerance to complete training.

**Infrastructure configuration** (based on published data from MegaScale and Llama 3):

- Cluster scale: 10,000–25,000+ GPUs
- Expected failure rate: Multiple failures per hour (one every 3 hours for 16,384 GPUs)
- Training duration: Weeks to months
- Checkpoint size: Multiple terabytes

Multiple fault tolerance mechanisms work together. Hierarchical checkpointing uses frequent lightweight checkpoints for fast recovery from common failures, plus periodic full checkpoints for major milestones. Elastic training continues with reduced worker count during failures, with workers rejoining as they recover. Training dynamics monitoring continuously tracks loss curves, gradient statistics, and activation distributions to detect silent corruption. Redundant infrastructure maintains multiple power feeds, network paths, and storage systems to minimize correlated failures.

Lessons learned from large-scale training are instructive. At sufficient scale, deterministic replay becomes impossible. Probabilistic equivalence is the target. Investment in debugging infrastructure pays off quickly through faster failure diagnosis. Proactive hardware replacement based on telemetry prevents many failures.

### Google: TPU Pod Failure Handling {#sec-fault-tolerance-reliability-reliability-google-tpu-pod-failure-handling-ace7}

Google's TPU pods present unique fault tolerance challenges due to their tightly-coupled architecture [@jouppi2023tpu].[^fn-tpu-pods]

[^fn-tpu-pods]: **TPU Pod Failure Rates**: Google reports that in large-scale accelerator clusters, hardware failures are constant. On average, each day 0.08% of TPU machines, 0.005% of Inter-Chip Interconnect (ICI) cables, and 0.04% of optical circuit switches experience failures [@jouppi2023tpu]. These statistics imply that a pod of 4,096 chips will experience multiple system-level interruptions daily, making automated recovery essential.

TPU pods face specific challenges. Chips are interconnected, so failures require routing around failed units. Firmware and software must coordinate for consistent state during recovery.

Google's fault tolerance approach uses multiple techniques. Pods include spare chips that can replace failed chips through reconfiguration. TPU state checkpoints to Google Cloud Storage for durability. Major failures restart the entire pod from checkpoint rather than partial recovery. Telemetry identifies chips likely to fail, enabling proactive replacement.

**Training efficiency impact**:

Google reports achieving 90%+ training efficiency (actual training time / ideal training time) on large TPU pods despite regular failures, through careful checkpoint interval optimization and fast recovery mechanisms.

### Meta: Recommendation Serving Resilience {#sec-fault-tolerance-reliability-reliability-meta-recommendation-serving-resilience-8daf}

Meta's recommendation systems serve billions of users with strict availability requirements.[^fn-meta-recsys]

[^fn-meta-recsys]: **Meta Recommendation Scale**: Meta processes hundreds of millions of inference requests/second across Feed, Reels, Stories, and Ads. DLRM (Deep Learning Recommendation Model) requires terabytes of embedding tables distributed across thousands of servers. Systems maintain 99.99%+ availability through redundancy, graceful degradation, and real-time model updates adapting to shifting user behavior.

**System requirements**:

- Availability target: 99.99%+ (< 52 minutes downtime per year)
- Latency target: p99 < 50ms
- Scale: Hundreds of millions QPS across products
- Continuous model updates for freshness

The fault tolerance architecture employs multiple layers. Multi-tier caching places cached recommendations at edge to reduce dependence on backend availability. Feature stores replicate across datacenters with automatic failover. Multiple model versions deploy with fallback to previous version on issues. Quality-tiered fallbacks progress from personalized to popular to cached.

Key innovations improve resilience. Predictive prefetching loads likely-needed features during low load to reduce real-time dependency. Embedding compression enables faster recovery and replication. A/B testing integration triggers automatic rollback when quality metrics degrade.

### Netflix: Chaos Engineering for ML {#sec-fault-tolerance-reliability-reliability-netflix-chaos-engineering-ml-64a0}

Netflix pioneered chaos engineering [@basiri2016chaos] and applies these principles to ML systems.[^fn-netflix-chaos]

[^fn-netflix-chaos]: **Netflix Chaos Engineering**: Disciplined approach to building confidence in system resilience by intentionally injecting failures [@basiri2016chaos]. Chaos Monkey randomly terminates instances; Chaos Kong simulates region failures; Latency Monkey injects delays. Netflix runs these continuously in production, discovering weaknesses before real failures occur. Now an industry standard adopted by Amazon, Google, and Microsoft.

Chaos engineering principles guide ML experimentation. Inject failures in production because real fault tolerance requires testing with real production conditions. Automate experiments so continuous chaos experiments verify ongoing resilience. Minimize blast radius by starting with small-scope failures and expanding. Have a hypothesis so each experiment tests a specific fault tolerance mechanism.

ML-specific chaos experiments target key failure modes. Model server termination verifies load balancer routes around failed servers. Feature store latency injection verifies fallback features activate appropriately. GPU memory pressure verifies graceful degradation under resource constraints. Model file corruption verifies integrity checks detect corruption.

Results and learnings show the value of chaos engineering. Chaos experiments regularly discover latent issues before they cause outages. Testing recovery mechanisms is as important as building them. Developer confidence in fault tolerance enables faster deployment.

### Synthesis: Common Themes Across Case Studies {#sec-fault-tolerance-reliability-reliability-synthesis-common-themes-across-case-studies-14f5}

These case studies demonstrate the principles developed throughout this chapter operating at production scale. The mathematical frameworks, from the exponential reliability model that predicts failure frequency to the Young-Daly formula that optimizes checkpoint intervals, inform real design decisions at Meta, Google, and Netflix. Several themes emerge across these diverse implementations:

1. **Proactive over reactive**: Leading organizations invest in predicting and preventing failures rather than just recovering from them.

2. **Observability is essential**: All organizations emphasize comprehensive monitoring as the foundation for fault tolerance.

3. **Graceful degradation by design**: Systems are designed with explicit degradation paths rather than adding them after failures occur.

4. **Testing fault tolerance**: Chaos engineering and similar practices verify that fault tolerance mechanisms actually work.

5. **Investment scales with value**: Organizations invest in fault tolerance proportional to the cost of failures for their specific systems.

## Fallacies and Pitfalls {#sec-fault-tolerance-reliability-reliability-fallacies-pitfalls-74b8}

Fault tolerance for distributed ML systems involves counterintuitive mathematics and subtle trade-offs. The scale of modern clusters guarantees frequent failures, yet engineers accustomed to smaller systems often underestimate failure rates and misapply intuition when setting checkpoint intervals or planning recovery strategies. As @sec-fault-tolerance-reliability-mathematics-inevitable-failure-93ef demonstrates, a 10,000-GPU cluster with per-GPU MTBF of 10,000 hours will experience failures every few hours. These fallacies and pitfalls capture common errors that lead to wasted compute, extended training times, and production incidents when fault tolerance mechanisms fail under real conditions.

**Fallacy: Hardware failures are the main concern.**

This intuition comes from traditional systems where disk failures, power outages, and network partitions dominate. In ML systems, software failures and configuration errors cause the majority of incidents.

Industry experience from large-scale ML systems suggests the following breakdown:

- Hardware failures: 15–25%
- Software bugs: 30–40%
- Configuration errors: 20–30%
- Resource exhaustion: 15–20%
- Unknown/other: 5–10%

Investing heavily in hardware redundancy while neglecting software robustness (input validation, gradual rollouts, configuration management) leaves the majority of failure modes unaddressed. The most reliable ML systems treat software bugs as inevitable and design defensively.

**Pitfall: Setting checkpoint interval by intuition.**

Organizations commonly set checkpoint intervals based on "feels right": "every hour seems reasonable" or "every 1000 steps." As @sec-fault-tolerance-reliability-checkpoint-restart-fundamentals-0ac2 explains, the Young-Daly formula reveals these intuitions are often wrong. For a 1000-GPU cluster with MTBF of 4 hours and checkpoint time of 5 minutes:

$$T_{opt} = \sqrt{2 \times 5 \times 240} = \sqrt{2400} \approx 49 \text{ minutes}$$

The intuitive "every hour" is close but suboptimal. More critically, if checkpoint time increases to 15 minutes (larger model, slower storage), the optimal interval becomes 85 minutes, not the "every 15 minutes" that some teams adopt to "stay safe." Too-frequent checkpointing wastes more compute than it saves. The quantitative approach reveals that intuition-based intervals often deviate 2–3$\times$ from optimal in either direction.

**Fallacy: MTBF assumes independent failures.**

The reliability equation $MTBF_{system} = MTBF_{component}/N$ assumes component failures are statistically independent. In production, failures correlate:

- **Shared power domain**: UPS failure takes down entire rack
- **Shared switch**: Top-of-rack switch failure partitions all connected GPUs
- **Shared software**: Bug triggered by specific input fails all replicas simultaneously
- **Thermal correlation**: Cooling failure causes clustered GPU throttling

Correlated failures reduce effective MTBF below the independent-failure calculation, sometimes dramatically. A cluster with 1000 "independent" GPUs each with 10,000-hour MTBF should have system MTBF of 10 hours. If failures correlate with factor 10 (10 GPUs fail together on average), effective MTBF drops to 1 hour.

Reliability engineering must identify and mitigate correlation through diversity: different power feeds, different network paths, different software versions in canary deployments.

**Pitfall: Ignoring restart overhead in checkpoint planning.**

While the Young-Daly formula accounts for checkpoint save time, practitioners often forget restart overhead:

1. **Job scheduling delay**: Acquiring replacement GPUs takes minutes in shared clusters

2. **Checkpoint loading**: Reading distributed checkpoint from storage

3. **Warmup time**: Learning rate warmup, batch normalization statistics recalculation

4. **Communication re-establishment**: NCCL ring topology reconstruction

Total restart time can be 3–5$\times$ checkpoint save time. A 5-minute checkpoint save followed by 20-minute restart means effective failure cost is 25 minutes, not 5 minutes. The modified Young-Daly formula should use $T_{save} + T_{restart}$ for the overhead term, significantly increasing optimal checkpoint intervals.

**Pitfall: Treating all failures as "restarts".**

Engineers often treat all failures as "node crashed, restart from checkpoint," but different failure modes require different responses. Transient failures (network congestion, thermal throttling) should trigger retry/pause, not restart, since state remains in memory. Permanent failures (GPU death, node crash) require checkpoint-restart with migration to new hardware. Silent corruption (bit flips, ECC errors) demands rollback to a previous checkpoint, not just the latest one, requiring checkpoint history retention. Resource exhaustion (OOM, memory fragmentation) needs reconfiguration before restart; otherwise the job crashes again immediately. As @sec-fault-tolerance-reliability-failure-taxonomy-e78c details, misdiagnosing failure type wastes compute: treating transient network blips as permanent failures wastes hours re-initializing, while ignoring silent corruption poisons model weights undetected.

**Fallacy: Checkpoints are automatically consistent.**

Modern frameworks checkpoint transparently, creating the illusion of automatic consistency. In practice, distributed checkpoints require coordination that can fail subtly:

1. **Rank desynchronization**: If rank 0 checkpoints iteration 1000 while rank 1 checkpoints iteration 1001, the checkpoint is inconsistent

2. **Partial writes**: Storage failure mid-checkpoint leaves incomplete shards

3. **Optimizer state lag**: Sharded optimizer state may not match model weights if captured at different times

4. **In-flight gradients**: AllReduce in progress during checkpoint may or may not be included

Production systems must implement checkpoint validation: verify all shards exist, verify iteration numbers match, verify optimizer state matches model state. Organizations that discover corrupted checkpoints during recovery from a failure have no recourse except restarting from an earlier (potentially much earlier) checkpoint.

**Pitfall: Testing fault tolerance only during failures.**

Fault tolerance mechanisms are code paths that execute rarely in normal operation. Like backup systems never tested until disaster strikes, fault tolerance code paths accumulate bugs:

- Checkpoint restoration logic untested because training never crashed
- Fallback model never loaded because primary never failed
- Circuit breaker thresholds tuned for old traffic patterns

Chaos engineering (intentionally injecting failures) transforms fault tolerance from "we think it works" to "we know it works." Organizations that regularly kill random GPUs during training, inject network partitions, and fail primary models discover bugs before they matter.

The cost of regular fault injection (some failed experiments, some minor outages) is far less than the cost of discovering broken fault tolerance during an actual failure.

**Fallacy: Elastic training eliminates the need for checkpointing.**

Elastic training (@sec-fault-tolerance-reliability-elastic-training-4f87) adjusts parallelism degree when workers fail, continuing with reduced capacity, which appears to eliminate checkpoint-restart overhead. However, state consistency challenges remain: reducing from N to N-1 workers requires redistributing model shards, optimizer states, and data assignments consistently. Below some minimum viable size, training becomes infeasible (model does not fit, batch size too small), requiring checkpoint-restart regardless. Each removed worker reduces throughput; accumulated failures progressively degrade training speed until checkpoint-restart becomes preferable to continued degradation. If a failure was caused by a software bug triggered by specific data, the bug persists in remaining workers. Elastic training is complementary to checkpointing, not a replacement; the reduced checkpoint frequency still requires occasional checkpoints for catastrophic failures and training completion.

**Fallacy: Silent data corruption is rare in modern hardware.**

Modern GPUs and memory systems include extensive error correction (ECC, CRC, parity), creating the intuition that silent data corruption is negligible. Large-scale studies reveal otherwise [@dixit2021silent; @sridharan2015memory]: 0.01-0.1% of GPUs per year exhibit silent corruption not caught by ECC, and cosmic ray-induced bit flips occur at approximately 1 per GB-month. For a 10,000-GPU cluster, this means 1-10 silent GPU corruption events per year and hundreds of memory bit flips monthly. Silent corruption causes mysterious training anomalies: loss spikes attributed to "bad batches" may be hardware errors, gradient NaNs blamed on learning rates may be bit flips, and models failing to converge despite correct hyperparameters may have corrupted weights. Detection strategies include redundant computation (computing batches on multiple workers and comparing), gradient checksums (verifying AllReduce consistency), and statistical monitoring of gradient/activation distributions. Unlike detectable failures, silent corruption does not trigger errors; training "succeeds" but produces subtly broken models, requiring detection mechanisms as discussed in @sec-fault-tolerance-reliability-mlspecific-debugging-27d5.

## Summary {#sec-fault-tolerance-reliability-reliability-summary-48b0}

Fault tolerance transforms the statistical certainty of hardware failure from a project-ending catastrophe into a manageable operational routine. We explored how checkpointing mechanisms (synchronous and asynchronous) preserve months of work by periodically capturing the state of massive models. We analyzed how elastic training and redundancy allow systems to continue operating even as individual nodes disappear from the cluster.

::: {.callout-takeaways}

* **Scale Guarantees Failure**: A 10,000-GPU cluster will encounter hardware failures every few hours; software must treat failure as a normal state.
* **Checkpointing is the Baseline**: Synchronous checkpointing is simple but incurs high overhead; asynchronous approaches hide I/O latency at the cost of consistency complexity.
* **Elasticity enables Persistence**: Designing training jobs to be "elastic" (reconfiguring around lost or added nodes as shown in @fig-elastic-flow) is superior to simple restart-from-scratch strategies.
* **Stateful Serving is the Frontier**: For LLMs, the KV cache represents a massive serving state that must be replicated or migrated to prevent high-latency session restarts, with strategies illustrated in @fig-serving-redundancy.
:::

With the algorithmic foundations of distributed training established—parallelism strategies, communication patterns, and fault tolerance mechanisms—we have defined the *workload* of modern AI. But algorithms cannot run on air. They require a physical machine capable of sustaining these massive throughputs and communication patterns.

We now turn from the logical to the physical. **Part II: Building the Machine Learning Fleet** begins with **Compute Infrastructure** (@sec-compute), where we examine the silicon, power, and cooling systems required to build the supercomputers that power these algorithms. We will see how the logical AllReduce rings we designed in Part I physically materialize as kilometers of fiber optic cabling and megawatts of power delivery.
