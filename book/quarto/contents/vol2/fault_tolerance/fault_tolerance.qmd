---
title: "Fault Tolerance and Reliability"
bibliography: fault_tolerance.bib
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR FAULT TOLERANCE
================================================================================

CORE PRINCIPLE: Fault tolerance requirements differ by workload type.
Training fault tolerance (checkpoint/restart) differs from serving
fault tolerance (redundancy/failover). Different models have different needs.

MODEL-SPECIFIC FAULT TOLERANCE CONSIDERATIONS:

| Model Type      | Training Recovery    | Serving Redundancy  | Critical State      |
|-----------------|---------------------|---------------------|---------------------|
| LLMs            | Checkpoint (TB)     | Model replicas      | KV cache            |
| Recommendation  | Incremental ckpt    | Feature store HA    | Embedding tables    |
| Vision          | Standard ckpt       | Load balancing      | Minimal state       |
| Real-time       | Fast recovery       | Hot standby         | Session state       |

REQUIRED COVERAGE FOR THIS CHAPTER:

TRAINING FAULT TOLERANCE:

- Checkpoint frequency: Depends on model size, iteration time
- Checkpoint storage: Distributed for large models, local for small
- Recovery granularity: Full restart vs elastic recovery
- Include: Different strategies for different model sizes

SERVING FAULT TOLERANCE:

- Stateless serving: Vision, many NLP models (simpler)
- Stateful serving: LLM with KV cache, session-based (complex)
- Feature store HA: Critical for recommendation systems
- Include: Why RecSys fault tolerance focuses on feature stores

GRACEFUL DEGRADATION:

- Model fallback: Use simpler model when primary fails
- Feature fallback: Default features when lookup fails (RecSys)
- Quality degradation: Reduce batch size, accept higher latency
- Include: Different degradation strategies by model type

DISTRIBUTED TRAINING FAILURES:

- Worker failure: Replace vs redistribute
- Network partition: Synchronous vs asynchronous handling
- Straggler mitigation: Backup workers, bounded staleness
- Include: How failure handling differs by parallelism strategy

CASE STUDIES TO INCLUDE:

- Meta recommendation serving resilience
- OpenAI training fault tolerance at scale
- Google TPU pod failure handling
- Netflix chaos engineering for ML

QUANTITATIVE ANALYSIS:

- Mean time between failures at different scales
- Recovery time objectives by workload type
- Cost of downtime for different applications
- Checkpoint overhead as percentage of training time

ANTI-PATTERNS TO AVOID:

- Treating all fault tolerance as checkpoint/restart
- Ignoring feature store reliability for RecSys
- Assuming stateless serving (LLMs have KV cache state)
- One-size-fits-all recovery strategies

================================================================================
-->

# Fault Tolerance and Reliability {#sec-fault-tolerance}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: A dramatic visualization of fault tolerance mechanisms protecting a distributed ML system. The scene shows a cluster of compute nodes with several nodes experiencing failures depicted as red warning indicators and disconnected links. Protective mechanisms spring into action: checkpoint systems shown as periodic snapshots being saved to persistent storage, redundant replicas activating to replace failed nodes, and graceful degradation paths routing around damaged components. A central reliability monitor displays system health metrics with some indicators in warning states but the overall system remaining operational. Visual elements include recovery timelines, failover arrows, and heartbeat signals between nodes. The composition contrasts the chaos of failures with the order of recovery mechanisms. Color scheme uses stable greens and blues for healthy components, red and orange for failures, and gold for active recovery processes. Technical yet dramatic style suitable for a reliability engineering textbook._
:::

\noindent
![](images/png/cover_fault_tolerance.png)

:::

## Purpose {.unnumbered}

_Why must machine learning systems continue delivering predictions reliably despite inevitable hardware failures, network outages, and unexpected operational disruptions?_

Machine learning systems deployed in production environments operate in a world of uncertainty: infrastructure fails, networks disconnect, data becomes corrupted, and computational resources become unavailable without warning. Unlike experimental systems that can simply restart when encountering failures, production ML systems serve millions of users and support critical decision-making, where reliability determines whether organizations can trust these systems with consequential outcomes. Traditional software failures manifest visibly through errors and exceptions, but ML system degradation under fault conditions often remains silent and undetected until catastrophic loss of service occurs. A recommendation system silently serving stale predictions erodes user trust over weeks before anyone notices. A healthcare diagnostic system becoming unavailable during peak hours jeopardizes patient care. An autonomous vehicle losing sensor redundancy becomes dangerous without warning. Mastering fault tolerance transforms ML systems from fragile prototypes dependent on perfect conditions into resilient production systems capable of sustained operation despite the failures that inevitably occur at scale.

::: {.callout-tip title="Learning Objectives"}

- Calculate system-level failure rates from component reliability metrics using exponential reliability models, and derive checkpoint frequency requirements that minimize expected wasted work

- Design checkpoint and recovery strategies for distributed training systems by selecting appropriate intervals using the Young-Daly formula and implementing distributed checkpoint coordination

- Implement serving fault tolerance mechanisms including redundancy, replication, and failover strategies that meet latency requirements orders of magnitude stricter than training systems

- Apply graceful degradation patterns including model fallback, feature fallback, and load shedding to maintain service availability under partial system failures

- Construct distributed debugging and observability infrastructure using tracing, logging, and monitoring to diagnose failures in non-deterministic distributed ML systems

- Evaluate fault tolerance trade-offs across different model types (LLMs, recommendation systems, vision models) by analyzing their distinct state management requirements and recovery strategies

:::

## Failure Analysis at Scale {#sec-failure-analysis-scale}

The distributed training systems examined in @sec-distributed-training and the communication infrastructure developed in @sec-communication enable training across thousands of devices. These chapters established what distributed systems can accomplish when everything works correctly. This chapter addresses the uncomfortable reality: at scale, everything does not work correctly. Failures are not exceptional events to be handled by error messages and restarts. They are continuous, expected operating conditions that systems must accommodate as naturally as they accommodate normal computation.

The transition from small-scale experimentation to large-scale production fundamentally changes the relationship between systems and failures. A researcher training a model on a single GPU might experience hardware failure once per year. That same researcher scaling to a 1,000 GPU cluster will experience failures multiple times per day. This shift from rare exception to routine occurrence demands entirely different engineering approaches. The mathematical analysis that follows makes this transition precise and quantitative.

Understanding failure at scale requires abandoning the mindset that treats failures as bugs to be fixed. Individual component failures cannot be eliminated; they can only be managed. Memory errors, network partitions, storage corruption, and software crashes will occur with statistical regularity that increases predictably with system size. The engineering challenge is not to prevent these failures but to build systems that continue making progress despite them.

This perspective shift has profound implications for system design. Fault-tolerant systems cannot assume that any operation will succeed; they must verify completion and handle failure as a normal code path. Recovery mechanisms cannot be afterthoughts tested occasionally; they must be exercised continuously to ensure they work when needed. And coordination protocols must account for partial failures where some components succeed while others fail, leaving the system in states that naive error handling would not anticipate.

The techniques developed in this chapter draw from decades of distributed systems research but apply that research to the specific characteristics of ML workloads. ML training exhibits properties that enable fault tolerance strategies unavailable to general distributed systems: the mathematical properties of stochastic gradient descent tolerate certain types of errors that would corrupt other computations, checkpoint sizes are large but predictable, and recovery targets need only be approximate rather than exact. Exploiting these properties enables fault tolerance mechanisms specifically optimized for ML that achieve better efficiency than general-purpose approaches.

### The Mathematics of Inevitable Failure {#sec-mathematics-inevitable-failure}

System reliability engineering provides the foundational framework for understanding failure at scale. Individual components exhibit failure rates characterized by the failure rate parameter $\lambda$, measured in failures per unit time. For a single component with constant failure rate $\lambda$, the probability of surviving without failure until time $t$ follows an exponential distribution:

$$
R_{single}(t) = e^{-\lambda t}
$$ {#eq-single-component-reliability}

The mean time between failures (MTBF) for this component equals $1/\lambda$. Modern GPUs in datacenter environments exhibit MTBF values ranging from 40,000 to 100,000 hours depending on operating conditions, cooling effectiveness, and manufacturing variation.[^fn-gpu-mtbf]

[^fn-gpu-mtbf]: **GPU MTBF Variation**: Published MTBF figures for datacenter GPUs vary significantly based on measurement methodology and operating conditions. NVIDIA reports A100 MTBF of approximately 50,000 hours under specified conditions, but actual field experience shows substantial variation. Higher ambient temperatures, power supply fluctuations, and memory-intensive workloads all reduce effective MTBF. Google's published TPU failure data and Meta's GPU fleet telemetry suggest actual failure rates 20-50% higher than manufacturer specifications.

When multiple independent components operate in a system where any single failure causes system failure, the system reliability becomes the product of individual component reliabilities:

$$
R_{system}(t) = \prod_{i=1}^{N} R_i(t) = \prod_{i=1}^{N} e^{-\lambda_i t}
$$ {#eq-system-reliability-product}

For $N$ identical components with individual failure rate $\lambda$, this simplifies to:

$$
R_{system}(t) = e^{-N\lambda t}
$$ {#eq-system-reliability-n-components}

The system failure rate becomes $N\lambda$, and the system MTBF becomes:

$$
MTBF_{system} = \frac{1}{N\lambda} = \frac{MTBF_{component}}{N}
$$ {#eq-system-mtbf}

This linear relationship between component count and system failure rate has profound implications. Doubling the number of GPUs halves the expected time until failure. This mathematical reality explains why fault tolerance transitions from optional enhancement to essential requirement as systems scale.

::: {.callout-important title="Key Insight: Scale Transforms Failure from Exception to Expectation"}
A single GPU with MTBF of 50,000 hours (5.7 years) fails rarely enough that manual intervention suffices. A 10,000 GPU cluster with the same per-GPU reliability has system MTBF of 5 hours. Failures occur continuously, multiple times per day. Systems must be designed expecting failure, not hoping to avoid it.
:::

### Worked Example: Cluster MTBF Calculation {#sec-cluster-mtbf-calculation}

Consider a training cluster designed for large language model development with the following specifications:

- 10,000 NVIDIA H100 GPUs
- Individual GPU MTBF: 50,000 hours
- Each GPU connected to host via PCIe (MTBF: 200,000 hours)
- Each node contains 8 GPUs with shared power supply (MTBF: 100,000 hours)
- Network infrastructure per node (NIC, cables): MTBF 150,000 hours

**Step 1: Calculate failure rate per GPU subsystem**

Each GPU operates within a failure domain that includes the GPU itself, its PCIe connection, and proportional shares of the power supply and network infrastructure:

$$
\lambda_{GPU} = \frac{1}{50,000} = 2.0 \times 10^{-5} \text{ failures/hour}
$$

$$
\lambda_{PCIe} = \frac{1}{200,000} = 0.5 \times 10^{-5} \text{ failures/hour}
$$

$$
\lambda_{power/GPU} = \frac{1}{8} \times \frac{1}{100,000} = 0.125 \times 10^{-5} \text{ failures/hour}
$$

$$
\lambda_{network/GPU} = \frac{1}{8} \times \frac{1}{150,000} = 0.083 \times 10^{-5} \text{ failures/hour}
$$

**Step 2: Calculate total per-GPU failure rate**

$$
\lambda_{total/GPU} = (2.0 + 0.5 + 0.125 + 0.083) \times 10^{-5} = 2.708 \times 10^{-5} \text{ failures/hour}
$$

**Step 3: Calculate cluster failure rate and MTBF**

$$
\lambda_{cluster} = 10,000 \times 2.708 \times 10^{-5} = 0.2708 \text{ failures/hour}
$$

$$
MTBF_{cluster} = \frac{1}{0.2708} = 3.69 \text{ hours}
$$

**Interpretation**: This cluster experiences a failure approximately every 3.7 hours on average. Over a 24-hour period, the expected number of failures is 6.5. A training run lasting one week (168 hours) will experience approximately 45 failures. Any training system operating at this scale must treat failure as a continuous condition, not an exceptional event.

@tbl-cluster-mtbf-scaling shows how cluster MTBF scales with system size, demonstrating the mathematical inevitability of frequent failures at scale.

| Cluster Size (GPUs) | Individual GPU MTBF | Cluster MTBF | Expected Failures per Day |
|:-------------------:|:------------------:|:------------:|:------------------------:|
| 8 | 50,000 hrs | 6,250 hrs (260 days) | 0.004 |
| 64 | 50,000 hrs | 781 hrs (32 days) | 0.03 |
| 512 | 50,000 hrs | 98 hrs (4 days) | 0.24 |
| 1,000 | 50,000 hrs | 50 hrs (2 days) | 0.48 |
| 4,000 | 50,000 hrs | 12.5 hrs | 1.9 |
| 10,000 | 50,000 hrs | 5 hrs | 4.8 |
| 25,000 | 50,000 hrs | 2 hrs | 12.0 |

: **Cluster MTBF Scaling**: System-level mean time between failures decreases linearly with cluster size, transforming failures from rare events to continuous operating conditions at scale. A training cluster sized for modern LLM development (10,000+ GPUs) experiences multiple failures daily. {#tbl-cluster-mtbf-scaling}

### Failure Taxonomy {#sec-failure-taxonomy}

Not all failures are equivalent. Understanding failure characteristics guides the selection of appropriate recovery mechanisms. The taxonomy presented here classifies failures along two primary dimensions: temporal behavior (transient versus persistent) and failure manifestation (fail-stop versus Byzantine).

#### Transient Failures {#sec-transient-failures}

Transient failures occur temporarily and resolve without intervention. Examples include:

- **Network packet loss**: Momentary congestion causes dropped packets, but retransmission succeeds
- **Memory bit flips**: Cosmic ray induced single-event upsets corrupt individual bits
- **Thermal throttling**: Temporary performance reduction due to temperature spikes
- **Software timeouts**: Temporary resource contention causes operation delays

Transient failures are particularly insidious in ML training because they may not trigger explicit errors. A transient memory bit flip during gradient computation produces incorrect gradients that propagate through subsequent training steps. The model continues training but produces subtly degraded results. Studies of large-scale training runs have documented cases where transient hardware errors caused training to diverge after hundreds of hours, wasting substantial compute resources.[^fn-silent-corruption]

[^fn-silent-corruption]: **Silent Data Corruption in Training**: Meta's published analysis of large-scale training identified silent data corruption as a significant concern, with approximately 0.1% of training runs exhibiting anomalous loss trajectories traceable to hardware errors that did not trigger explicit failures. Detection required monitoring training loss statistics and comparing against expected convergence curves.

The appropriate response to transient failures depends on detection capability. Errors that trigger explicit exceptions can be handled through retry logic. Silent corruption requires validation mechanisms such as gradient checksums, periodic model evaluation, and statistical monitoring of training dynamics.

#### Fail-Stop Failures {#sec-failstop-failures}

Fail-stop failures cause components to cease operation entirely and detectably. The failed component stops responding to requests and can be identified through timeout mechanisms. Examples include:

- **GPU hardware failure**: Memory errors cause device to become unresponsive
- **Node crash**: Operating system failure terminates all processes
- **Network partition**: Physical or logical disconnection isolates node from cluster
- **Storage failure**: Disk failure prevents checkpoint read/write operations

Fail-stop failures are the easiest class to handle because detection is straightforward: the component stops responding. Recovery involves replacing the failed component and restoring state from the most recent checkpoint. The primary challenge is minimizing detection time and recovery latency.

Detection time $T_{detect}$ typically involves heartbeat mechanisms where each worker periodically signals liveness to a coordinator. If no heartbeat arrives within timeout period $T_{timeout}$, the coordinator declares failure. Setting $T_{timeout}$ requires balancing false positive rate (declaring healthy workers failed due to transient delays) against detection latency (slow detection wastes compute during the detection window).

For a heartbeat interval of $H$ seconds and expected network delay variance $\sigma_d$, a common heuristic sets:

$$
T_{timeout} = H + k\sigma_d
$$ {#eq-timeout-calculation}

where $k$ typically ranges from 3 to 5 to achieve low false positive rates while maintaining reasonable detection speed.

#### Byzantine Failures {#sec-byzantine-failures}

Byzantine failures represent the most challenging class: components continue operating but produce incorrect results. Named after the Byzantine Generals Problem in distributed systems theory, these failures include:

- **Silent data corruption**: Memory or computation errors produce wrong values without triggering errors
- **Numerical instability**: Floating-point edge cases cause gradients to become NaN or infinity
- **Determinism violations**: Race conditions cause different workers to compute different results for identical inputs
- **Adversarial corruption**: Malicious actors intentionally inject incorrect gradients

Byzantine failures are particularly dangerous in distributed training because the standard assumption that workers compute identical gradients for identical data no longer holds. A single Byzantine worker can corrupt the averaged gradient, potentially causing training to diverge or converge to a poor solution.

Detection of Byzantine failures requires redundant computation. If multiple workers compute gradients for the same data, their results can be compared. Statistical outlier detection can identify workers consistently producing anomalous gradients. However, these detection mechanisms add computational overhead and may not catch subtle corruption.

Byzantine-resilient distributed training algorithms exist but impose significant overhead. Algorithms such as Krum and coordinate-wise trimmed mean compute aggregates that are robust to a bounded number of Byzantine workers, but they require more communication and computation than simple averaging.[^fn-byzantine-ml]

[^fn-byzantine-ml]: **Byzantine-Resilient ML**: Research on Byzantine-resilient distributed learning has produced algorithms like Krum (selecting the gradient most similar to others), trimmed mean (discarding extreme values before averaging), and SIGNSGD (using only gradient signs). However, these methods typically assume a bounded fraction of Byzantine workers (often < 50%) and may not fully protect against sophisticated attacks.

#### Correlated Failures {#sec-correlated-failures}

The reliability calculations in @sec-mathematics-inevitable-failure assume independent failures. Real systems exhibit correlated failures where multiple components fail simultaneously due to shared dependencies:

- **Power supply failure**: All GPUs in a node lose power simultaneously
- **Network switch failure**: All nodes connected to the switch become unreachable
- **Cooling system failure**: Thermal shutdown affects multiple racks
- **Software bugs**: A bug in the CUDA driver crashes all processes using that driver version
- **Operator error**: Misconfiguration affects entire cluster

Correlated failures violate the independence assumption underlying @eq-system-reliability-n-components. When failures are correlated, the actual system reliability is lower than the formula predicts. More importantly, correlated failures can defeat redundancy strategies. Three replicas of a model provide no availability benefit if all three run on the same power domain and a power failure takes out all three simultaneously.

Defending against correlated failures requires understanding failure domains and ensuring redundancy spans independent failure domains. @tbl-failure-domains lists common failure domains in ML infrastructure and their typical impact scope.

| Failure Domain | Impact Scope | Typical Recovery Time | Mitigation Strategy |
|:--------------|:-------------|:---------------------|:-------------------|
| Single GPU | 1 GPU | Seconds (spare) | Hot spare, elastic training |
| Node (power/OS) | 8 GPUs | Minutes | Checkpoint, node replacement |
| Rack (ToR switch) | 32-64 GPUs | Minutes to hours | Cross-rack redundancy |
| Power domain | 100-500 GPUs | Hours | Multiple power feeds |
| Datacenter region | All GPUs | Hours to days | Geographic distribution |
| Software version | All GPUs running version | Minutes (rollback) | Staged rollouts |

: **Failure Domains in ML Infrastructure**: Understanding failure domain boundaries enables placement of redundant components across independent domains, preventing correlated failures from defeating redundancy strategies. {#tbl-failure-domains}

### The Bathtub Curve and Hardware Lifecycle {#sec-bathtub-curve}

Hardware failure rates are not constant over component lifetime. The bathtub curve, a well-established model in reliability engineering, describes how failure rates vary across three distinct phases:

**Infant Mortality Phase**: New components exhibit elevated failure rates due to manufacturing defects, improper installation, and early-life wear-out of marginal components. This phase typically lasts days to weeks for electronic components. Burn-in testing, where components are operated under stress conditions before deployment, aims to precipitate infant mortality failures before production use.

**Useful Life Phase**: After surviving infant mortality, components exhibit relatively constant failure rates. This phase represents the longest portion of component lifetime and is the period where the exponential reliability model (@eq-single-component-reliability) applies most accurately. For datacenter GPUs, the useful life phase typically spans 3-5 years.

**Wear-Out Phase**: As components age, failure rates increase due to accumulated wear. For GPUs, wear mechanisms include electromigration in circuits, thermal cycling stress on solder joints, and degradation of thermal interface materials. The onset of wear-out depends heavily on operating conditions; components operated at high temperatures or with frequent thermal cycling enter wear-out earlier.

The practical implication for ML systems is that fleet-wide failure rates depend on age distribution. A cluster populated entirely with new GPUs will experience elevated failure rates during the first few weeks (infant mortality), followed by a stable period, then increasing failures as the fleet ages. Mixed-age fleets exhibit more consistent aggregate failure rates because different cohorts are in different lifecycle phases.

Proactive maintenance strategies aim to replace components approaching wear-out before they fail in production. Predictive analytics using GPU telemetry (temperature trends, error counts, performance degradation) can identify components likely to fail soon, enabling scheduled replacement during maintenance windows rather than unplanned outages during training runs.

### Model-Type Diversity in Failure Impact {#sec-failure-impact-diversity}

The cost of failure differs dramatically across model types due to differences in training duration, checkpoint size, and state that must be preserved. @tbl-failure-impact-by-model illustrates these differences.

| Model Type | Typical Training Duration | Checkpoint Size | State Sensitivity | Failure Cost |
|:-----------|:-------------------------|:----------------|:------------------|:-------------|
| LLM (175B params) | 2-4 weeks | 350-700 GB | High (position in curriculum) | $2-5M compute per 24hr loss |
| Vision (ViT-Large) | 1-3 days | 1-2 GB | Medium (augmentation state) | $10-50K per day loss |
| RecSys (1T embeddings) | Continuous | 2-4 TB (embeddings) | Very High (embedding freshness) | Revenue impact per hour |
| Speech (Whisper-scale) | 3-7 days | 5-10 GB | Medium | $50-200K per day loss |
| Scientific (AlphaFold) | Days to weeks | 10-50 GB | High (exploration state) | Research delay |

: **Failure Impact by Model Type**: The cost of training failures varies dramatically by model type, driven by training duration, checkpoint overhead, and the value of accumulated training state. These differences demand model-specific fault tolerance strategies. {#tbl-failure-impact-by-model}

**Large Language Models** experience the highest absolute failure costs due to their extended training durations and the computational expense of each training hour. A GPT-4 scale training run consuming 25,000 GPUs at approximately $2 per GPU-hour incurs $1.2M in compute costs per day. A failure that loses 24 hours of training progress costs $1.2M in wasted compute plus schedule delay. The checkpoint overhead for models with hundreds of billions of parameters can itself become significant, with 700GB checkpoints requiring several minutes to write even with fast distributed storage.

**Recommendation Systems** present unique challenges because their training is often continuous rather than episodic. The value of a RecSys model derives partly from its freshness: embeddings that capture recent user behavior outperform stale embeddings. A failure that loses hours of embedding updates may degrade recommendation quality in ways that directly impact revenue. Meta has documented that recommendation model freshness directly correlates with engagement metrics, making recovery time a business-critical metric.[^fn-recsys-freshness]

[^fn-recsys-freshness]: **RecSys Freshness Impact**: Meta's published research on deep learning recommendation models indicates that embedding staleness measured in hours can produce measurable degradation in recommendation relevance. Their continuous training infrastructure prioritizes minimal interruption over checkpoint optimization, using incremental checkpointing and elastic training to maintain freshness.

**Vision Models** occupy a middle ground with moderate training durations and manageable checkpoint sizes. The relatively small checkpoints (1-2 GB for ViT-Large) enable frequent checkpointing with minimal overhead. Data augmentation state (current augmentation parameters, data shuffling seed) represents the primary state beyond model weights that must be preserved for reproducible recovery.

**Scientific Models** such as those used in protein structure prediction or climate simulation often have unique state requirements beyond model parameters. AlphaFold-style training may maintain exploration state tracking which protein families have been sampled, preventing repetition during recovery. Drug discovery models may track which molecular configurations have been evaluated. This domain-specific state complicates checkpoint and recovery design.

### Economic Framework for Fault Tolerance Investment {#sec-economic-framework}

Fault tolerance mechanisms consume resources: storage for checkpoints, bandwidth for checkpoint writes, compute cycles for redundant calculations, and engineering time for implementation and maintenance. Rational investment in fault tolerance requires quantifying both the cost of failures and the cost of prevention.

**Cost of Failure** includes:

- Wasted compute: GPU-hours expended on training steps that must be repeated
- Schedule delay: Extended time to trained model impacts business timelines
- Opportunity cost: Compute consumed by recovery cannot be used for other training
- Engineering cost: Time spent debugging failures and manually recovering

**Cost of Prevention** includes:

- Storage cost: Checkpoint storage scales with model size and checkpoint frequency
- Throughput overhead: Checkpoint writes consume memory bandwidth and may stall training
- Recovery infrastructure: Spare capacity, automated recovery systems
- Complexity cost: Fault tolerant systems are harder to develop and debug

The optimal investment in fault tolerance balances these costs. For small-scale training on a few GPUs where failures are rare, minimal fault tolerance (infrequent checkpoints, manual recovery) may be cost-optimal. For large-scale training on thousands of GPUs where failures occur multiple times daily, extensive fault tolerance (frequent checkpoints, automatic recovery, elastic training) provides positive return on investment.

A simplified economic model considers expected cost per training run:

$$
C_{total} = C_{compute} + E[N_{failures}] \times C_{per\_failure} + C_{ft}
$$ {#eq-total-cost}

where $C_{compute}$ is the base compute cost, $E[N_{failures}]$ is the expected number of failures during training, $C_{per\_failure}$ is the cost per failure (wasted compute plus overhead), and $C_{ft}$ is the cost of fault tolerance mechanisms.

Fault tolerance investment is justified when:

$$
\frac{\partial C_{ft}}{\partial x} < \frac{\partial (E[N_{failures}] \times C_{per\_failure})}{\partial x}
$$ {#eq-ft-investment-criterion}

where $x$ represents investment in fault tolerance mechanisms. In practice, this means investing in fault tolerance until the marginal cost of additional protection exceeds the marginal reduction in failure costs.

::: {.callout-note title="The 3 Things Students Must Remember About Failure at Scale"}

1. **At scale, failures are continuous, not exceptional.** A 10,000-GPU cluster experiences failures every few hours. Systems must be designed expecting failure as normal operation.

2. **The optimal checkpoint interval is $\sqrt{2 \times T_{save} \times MTBF}$.** The Young-Daly formula (derived in @sec-checkpoint-fundamentals) provides quantitative guidance for checkpoint frequency.

3. **Training and serving have fundamentally different fault tolerance requirements.** Training tolerates minutes of recovery time; serving requires milliseconds. This fundamental difference demands entirely different approaches.

:::

## Training Fault Tolerance {#sec-training-fault-tolerance}

The failure analysis in @sec-failure-analysis-scale established that large-scale training systems will experience failures frequently. This section develops the mechanisms that enable training to continue despite these failures. The fundamental approach, checkpoint and restart, captures training state periodically so that failures lose only the work since the last checkpoint rather than all work since training began. However, the apparent simplicity of "save state, restore on failure" conceals substantial engineering complexity when applied to distributed systems training models with hundreds of billions of parameters.

### Checkpoint and Restart Fundamentals {#sec-checkpoint-fundamentals}

Checkpointing captures sufficient state to resume training from a recorded point. The minimal checkpoint must include:

- **Model parameters**: All learnable weights and biases
- **Optimizer state**: Momentum buffers, adaptive learning rate accumulators (Adam's $m$ and $v$)
- **Training progress**: Current epoch, batch index, global step count
- **Random state**: RNG seeds for reproducible data shuffling and augmentation
- **Learning rate schedule state**: Current position in learning rate schedule

For large models, optimizer state often exceeds model size. Adam maintains two state tensors ($m$ and $v$) per parameter, so optimizer state equals twice the model parameter count. For a 175B parameter model at FP32 precision:

$$
\text{Model size} = 175 \times 10^9 \times 4 \text{ bytes} = 700 \text{ GB}
$$

$$
\text{Optimizer state (Adam)} = 2 \times 700 \text{ GB} = 1,400 \text{ GB}
$$

$$
\text{Total checkpoint size} = 700 + 1,400 = 2,100 \text{ GB}
$$

With mixed-precision training storing model parameters and optimizer state at different precisions, checkpoint sizes can be reduced but remain substantial.

#### The Optimal Checkpoint Interval {#sec-optimal-checkpoint-interval}

Checkpointing involves a fundamental trade-off. Frequent checkpoints minimize lost work when failures occur but consume time and resources for checkpoint operations. Infrequent checkpoints minimize checkpoint overhead but risk losing substantial work to failures.

The Young-Daly formula provides the optimal checkpoint interval that minimizes expected total time including both checkpointing overhead and failure recovery:[^fn-young-daly]

$$
T_{opt} = \sqrt{2 \times T_{save} \times MTBF}
$$ {#eq-young-daly}

[^fn-young-daly]: **Young-Daly Formula Derivation**: Originally derived by John Young (1974) for general checkpointing systems and refined by William Daly (2004) for high-performance computing. The formula assumes exponentially distributed failure times and derives the interval that minimizes expected time to completion including checkpoint overhead and failure recovery.

where $T_{save}$ is the time to write a checkpoint and $MTBF$ is the mean time between failures.

::: {.callout-warning title="Young-Daly Formula Assumptions"}
The Young-Daly formula provides valuable intuition but rests on assumptions that may not hold in practice:

1. **Exponentially distributed failures**: Assumes constant failure rate. Real systems exhibit "bathtub curve" behavior with higher rates during burn-in and wear-out phases.

2. **Deterministic checkpoint time**: Assumes $T_{save}$ is constant. In practice, checkpoint time varies 2-3x due to storage contention, network congestion, and memory pressure.

3. **Recovery time equals checkpoint time**: Assumes recovery reads same data written during checkpoint. Often recovery takes 3-5x longer due to job scheduling delays, topology reconstruction, and warmup.

4. **Single failure mode**: Assumes one failure at a time. Correlated failures (power, cooling, shared switch) violate this assumption.

5. **Infinite timeline**: Optimal for long training runs. Short runs (where total time is comparable to MTBF) require different analysis.

When assumptions are violated, the optimal interval may shift significantly. As a rule of thumb, if restart overhead significantly exceeds checkpoint time, use $\sqrt{2 \times (T_{save} + T_{restart}) \times MTBF}$ instead.
:::

**Derivation sketch**: Let $T_c$ be the checkpoint interval. Expected wasted work per failure is $T_c/2$ (on average, failure occurs halfway through the interval) plus recovery time. Checkpoint overhead per interval is $T_{save}/T_c$. Minimizing total expected time yields the square root relationship.

::: {.callout-tip title="Worked Example: Optimal Checkpoint Interval for GPT-175B"}

Consider training GPT-175B on a 10,000 GPU cluster:

**Given:**

- Checkpoint size: 2.1 TB (model + optimizer state)
- Distributed storage write bandwidth: 100 GB/s aggregate
- Cluster MTBF: 5 hours (from @sec-cluster-mtbf-calculation)

**Step 1: Calculate checkpoint save time**

$$
T_{save} = \frac{2,100 \text{ GB}}{100 \text{ GB/s}} = 21 \text{ seconds}
$$

**Step 2: Apply Young-Daly formula**

$$
T_{opt} = \sqrt{2 \times 21 \text{ s} \times 5 \text{ hr}} = \sqrt{2 \times 21 \times 18,000 \text{ s}^2}
$$

$$
T_{opt} = \sqrt{756,000} \approx 870 \text{ seconds} \approx 14.5 \text{ minutes}
$$

**Interpretation**: Optimal checkpoint interval is approximately 14.5 minutes. With this interval, checkpoint overhead consumes $21/870 = 2.4\%$ of training time.

**Verification**: Expected wasted work per failure is $870/2 = 435$ seconds. Expected number of failures per hour is $1/5 = 0.2$. Expected wasted work per hour is $0.2 \times 435 = 87$ seconds. Checkpoint overhead per hour is $(3600/870) \times 21 = 87$ seconds. The optimal interval balances these equal contributions to overhead, confirming the formula's validity.

:::

#### Checkpoint Overhead Analysis {#sec-checkpoint-overhead}

Beyond the time consumed by checkpoint writes, checkpointing imposes additional overhead through memory consumption and training disruption.

**Memory overhead**: Checkpoint serialization requires memory buffers for gathering distributed state and preparing data for write. For synchronous checkpointing, all workers must hold their checkpoint data in memory until the checkpoint completes, potentially requiring significant additional memory allocation.

**Training disruption**: Synchronous checkpointing pauses training while the checkpoint writes. Even with fast storage, the pause disrupts the training pipeline, potentially causing GPU idle time as data loading and forward passes cannot proceed during checkpoint operations.

**Checkpoint overhead** can be expressed as:

$$
O_{ckpt} = \frac{T_{save}}{T_{interval}} + \frac{T_{pause}}{T_{interval}}
$$ {#eq-checkpoint-overhead}

where $T_{pause}$ represents any training pause beyond the checkpoint write time (memory allocation, coordination, serialization).

@tbl-checkpoint-overhead-by-model shows checkpoint characteristics for different model types, illustrating how checkpoint overhead varies with model architecture.

| Model Type | Checkpoint Size | Write Time (100 GB/s) | Optimal Interval (5hr MTBF) | Overhead |
|:-----------|:----------------|:----------------------|:---------------------------|:---------|
| GPT-175B | 2.1 TB | 21 s | 14.5 min | 2.4% |
| GPT-3.5 (20B) | 240 GB | 2.4 s | 4.6 min | 0.9% |
| BERT-Large | 1.3 GB | 0.013 s | 22 s | 0.06% |
| DLRM-1T embeddings | 4 TB | 40 s | 20 min | 3.3% |
| ResNet-50 | 100 MB | 0.001 s | 6 s | 0.02% |
| ViT-Large | 1.2 GB | 0.012 s | 21 s | 0.06% |

: **Checkpoint Overhead by Model Type**: Larger models with correspondingly larger checkpoints require longer save times but also benefit from longer optimal checkpoint intervals. The percentage overhead remains manageable (under 5%) for most configurations. {#tbl-checkpoint-overhead-by-model}

#### Synchronous vs Asynchronous Checkpointing {#sec-sync-async-checkpoint}

**Synchronous checkpointing** pauses training while the checkpoint writes. All workers coordinate to reach a consistent state, write their portions of the checkpoint, and resume training only after all writes complete. This approach guarantees checkpoint consistency but introduces training pauses.

**Asynchronous checkpointing** continues training while the checkpoint writes in the background. Workers snapshot their state to CPU memory or staging storage, then continue training while a background process writes the snapshot to persistent storage. This approach minimizes training disruption but introduces complexity:

- **Memory pressure**: Snapshot data consumes additional memory until the write completes
- **Consistency challenges**: Care must be taken to ensure the snapshot represents a consistent point in training
- **I/O contention**: Background writes compete for storage bandwidth with data loading

Modern frameworks like PyTorch's FSDP provide asynchronous checkpointing support through memory-efficient state snapshotting and background writes. DeepSpeed's checkpointing similarly supports overlap between checkpoint writes and continued training.[^fn-async-checkpoint]

[^fn-async-checkpoint]: **Asynchronous Checkpoint Implementation**: DeepSpeed implements asynchronous checkpointing by maintaining a checkpoint staging buffer in CPU memory. When a checkpoint triggers, GPU state is copied to CPU asynchronously using CUDA streams, allowing GPU computation to continue. A separate thread writes staged data to storage while training proceeds. This achieves near-zero checkpoint overhead for models where staging memory is available.

#### Checkpoint Storage Architecture {#sec-checkpoint-storage-architecture}

Where checkpoints are stored is as critical as how often they are written. Production systems use hierarchical storage with different tiers for different purposes:

| Tier | Technology | Write Bandwidth | Durability | Use Case |
|------|------------|-----------------|------------|----------|
| Tier 1 | Local NVMe | 5-10 GB/s per node | Not durable | Async staging buffer |
| Tier 2 | Distributed FS (Lustre, GPFS) | 50-200 GB/s aggregate | Durable within datacenter | Primary checkpoint storage |
| Tier 3 | Object storage (S3, GCS) | 10-50 GB/s aggregate | Cross-region durable | Disaster recovery, long-term retention |

**Hierarchical checkpoint flow**:

1. GPU state snapshots to local NVMe (fastest, enables async checkpoint)
2. Background promotion to distributed filesystem (durable, fast random access for recovery)
3. Periodic archival to object storage (cheapest per GB, disaster recovery)

**Storage cost analysis**: Checkpoints can consume significant storage budget at scale. A 175B model training with 14-minute checkpoint intervals for 30 days generates:

$$\frac{30 \times 24 \times 60}{14} \times 2.1\text{ TB} = 6,480\text{ TB}$$

At $0.02/GB/month for object storage, this costs $130,000/month just for checkpoint storage. Production systems implement checkpoint versioning and garbage collection, retaining only recent checkpoints plus periodic snapshots for long-term recovery.

### Distributed Checkpointing {#sec-distributed-checkpointing}

When training spans multiple workers, checkpoint coordination becomes essential. Two primary approaches exist: centralized checkpointing where a coordinator gathers all state and writes a single checkpoint, and distributed checkpointing where each worker writes its own portion of the checkpoint.

#### Centralized Checkpointing {#sec-centralized-checkpointing}

In centralized checkpointing, workers send their state to a coordinator process that assembles and writes the complete checkpoint. This approach simplifies checkpoint management and produces self-contained checkpoint files but creates scalability bottlenecks:

- **Network bottleneck**: All state flows through the coordinator
- **Memory bottleneck**: Coordinator must have memory for entire checkpoint
- **Single point of failure**: Coordinator failure loses the checkpoint operation

Centralized checkpointing works acceptably for small-scale distributed training (tens of workers) but becomes impractical at large scale.

#### Distributed Checkpointing {#sec-distributed-checkpointing-impl}

In distributed checkpointing, each worker writes its portion of the checkpoint to a shared filesystem or object storage. A coordinator signals when to checkpoint and confirms completion, but state flows directly from workers to storage without aggregation.

**Coordination protocol**:

1. Coordinator broadcasts checkpoint request with checkpoint ID
2. Each worker reaches a consistent state (barrier synchronization)
3. Each worker writes its shard to `checkpoint_<id>/worker_<rank>.pt`
4. Each worker confirms write completion to coordinator
5. Coordinator writes checkpoint metadata after all confirmations
6. Coordinator broadcasts checkpoint complete, training resumes

This protocol ensures that either all workers complete their writes (valid checkpoint) or the checkpoint is incomplete (detectable through missing metadata). Partial checkpoints can be garbage collected.

::: {.callout-note title="Checkpoint Consistency Models"}
The idealized protocol above assumes step 2 completes quickly. At scale, this barrier synchronization becomes the dominant checkpoint cost because there is almost always at least one slow worker in a 10,000+ GPU cluster.

**Strict Synchronous**: All workers checkpoint at exactly the same training step. Provides strongest consistency but highest overhead from barrier synchronization.

**Bounded Asynchronous**: Workers may be within $k$ steps of each other (typically $k=1-3$). The checkpoint manager tracks the "checkpoint wavefront" across workers. Recovery uses the earliest consistent cut across all shards. This trades perfect consistency for dramatically reduced synchronization overhead and is what production systems actually use.

**Eventual Consistency**: Workers checkpoint when convenient, reconcile during recovery. Lowest overhead but requires complex recovery logic to reconstruct consistent state.
:::

**Two-Phase Checkpoint Commit**: The basic protocol has a subtle correctness bug: if the coordinator crashes after some workers confirm but before writing metadata, those workers believe the checkpoint succeeded while the system has no valid checkpoint.

Production systems use two-phase commit:

1. **Prepare phase**: Workers write to staging location, report success to coordinator
2. **Commit phase**: Coordinator atomically renames/commits all shards together (e.g., moving from `staging/` to `checkpoints/`)
3. **Recovery**: If coordinator fails between phases, workers detect during next heartbeat and rollback staged writes

This ensures atomic checkpoint commits: either all shards are committed together, or none are.

**Consistency considerations**: In distributed training with synchronous gradient updates, workers naturally reach consistent states at step boundaries. Checkpointing at step boundaries ensures all workers have applied the same updates. With asynchronous training or pipeline parallelism, defining and reaching consistent states requires more careful coordination.

#### Sharded Checkpointing {#sec-sharded-checkpointing}

Modern distributed training frameworks partition model state across workers using techniques like ZeRO (Zero Redundancy Optimizer) and FSDP (Fully Sharded Data Parallel). In these configurations, no single worker holds complete model state. Each worker holds only its assigned parameter shard plus corresponding optimizer state.

**Sharded checkpointing** leverages this distribution: each worker writes only its shard, dramatically reducing per-worker write volume. Recovery loads shards and redistributes state to workers based on the recovery configuration.

This approach enables efficient checkpointing even for massive models. A 175B parameter model with 2.1 TB checkpoint distributed across 1,000 workers requires each worker to write only 2.1 GB, achievable in seconds with local NVMe storage.

**Shard redistribution**: When recovering with a different number of workers than the checkpoint (due to elastic scaling or hardware changes), shard redistribution must remap state to the new worker configuration. Modern frameworks support flexible resharding, enabling recovery even when worker count changes.

### Failure Detection and Recovery {#sec-failure-detection-recovery}

Checkpoints enable recovery, but the recovery process itself requires careful engineering. Recovery time directly impacts training efficiency: faster recovery means less wasted compute waiting for the system to resume.

Recovery time decomposes into:

$$
T_{recovery} = T_{detect} + T_{restart} + T_{load} + T_{warmup}
$$ {#eq-recovery-time}

where:

- $T_{detect}$: Time to detect the failure
- $T_{restart}$: Time to restart failed workers or reconfigure the job
- $T_{load}$: Time to load checkpoint state
- $T_{warmup}$: Time for training to reach steady-state performance (data pipeline fill, GPU warmup)

#### Failure Detection Mechanisms {#sec-failure-detection}

**Heartbeat monitoring**: Each worker periodically sends heartbeat messages to a coordinator or monitoring service. Missing heartbeats trigger failure detection. Heartbeat interval and timeout parameters control the trade-off between detection speed and false positive rate.

**Collective operation timeouts**: During distributed training, collective operations (AllReduce, broadcast) hang if any participant fails. Framework-level timeouts on collective operations detect failures that would otherwise cause indefinite hangs. NCCL provides configurable timeout parameters (`NCCL_TIMEOUT`) for this purpose.

**Health check probes**: Container orchestration systems (Kubernetes) and cluster managers (SLURM) provide health check mechanisms that detect unresponsive workers. Liveness probes verify that processes are running; readiness probes verify that processes are ready to handle requests.

**Training dynamics monitoring**: Monitoring loss values, gradient norms, and other training statistics can detect Byzantine failures that do not cause explicit errors. Sudden loss spikes or gradient explosions may indicate silent corruption.

::: {.callout-warning title="Realistic Failure Detection Latencies"}
Production experience shows that failure detection takes significantly longer than theoretical heartbeat timeouts suggest. The fundamental challenge is distinguishing failures from stragglers:

| Failure Type | Typical Detection Time | Why |
|-------------|----------------------|-----|
| Process crash | 5-30 seconds | Heartbeat timeout + verification retries |
| GPU hang | 30-120 seconds | Must distinguish from legitimately slow kernel |
| Network partition | 60-180 seconds | Must distinguish from temporary congestion |
| Silent data corruption | Minutes to hours | Requires statistical anomaly detection |

These latencies exist because aggressive timeouts cause false positives (killing healthy-but-slow workers), while conservative timeouts delay real failure detection. Production systems typically use multi-stage detection: fast initial timeout triggers investigation, slower confirmation timeout triggers recovery.
:::

#### Recovery Procedures {#sec-recovery-procedures}

Upon failure detection, the recovery procedure executes:

1. **Job termination**: Stop all remaining workers to prevent inconsistent state
2. **Resource reclamation**: Release failed nodes, request replacement resources
3. **Job restart**: Launch new worker processes with recovered configuration
4. **Checkpoint loading**: Each worker loads its state shard from the latest valid checkpoint
5. **State synchronization**: Workers synchronize to ensure consistent starting state
6. **Training resumption**: Resume training from the checkpoint step

**Automatic recovery** systems perform these steps without human intervention. Modern training frameworks integrate with cluster managers to automate recovery. DeepSpeed's `deepspeed.launch` can be configured for automatic restart on failure. PyTorch's `torchrun` (elastic launch) provides similar capabilities.

**Recovery validation**: After loading a checkpoint, validation steps confirm successful recovery:

- Verify model parameters match expected shapes and dtypes
- Run a few training steps and verify loss is consistent with pre-failure values
- Confirm gradient computations produce expected statistics

#### Distinguishing Stragglers from Failures {#sec-straggler-detection}

A straggler is a worker that remains operational but performs slower than peers, causing other workers to wait during synchronization points. Stragglers can arise from:

- **Hardware degradation**: GPU thermal throttling, memory errors causing retries
- **Resource contention**: Other processes consuming CPU, memory, or network bandwidth
- **Data loading delays**: Slow storage access, network filesystem issues
- **Software inefficiency**: Different workers hitting different code paths

The challenge is distinguishing stragglers (which should trigger mitigation) from failures (which should trigger recovery). Aggressive timeouts treat stragglers as failures, causing unnecessary job restarts. Conservative timeouts waste compute waiting for stragglers.

**Straggler mitigation strategies**:

- **Backup workers**: Replicate work assigned to slow workers, use first result
- **Bounded staleness**: Allow training to proceed with stale gradients from slow workers
- **Dynamic load balancing**: Redistribute work away from slow workers
- **Proactive replacement**: Detect degrading workers and replace before they fail

### Elastic Training {#sec-elastic-training}

Traditional distributed training operates with a fixed number of workers. Failures require either waiting for replacement resources or restarting with fewer workers (requiring checkpoint resharding). **Elastic training** enables dynamic scaling: adding or removing workers without stopping training.

Elastic training provides several advantages:

- **Fault tolerance**: Failures reduce worker count rather than stopping training
- **Resource efficiency**: Training can use variable resource allocations
- **Preemption handling**: Gracefully handle preemption in shared clusters
- **Cost optimization**: Scale based on spot instance availability

#### Elastic Training Mechanisms {#sec-elastic-training-mechanisms}

Implementing elastic training requires adapting several training components:

**Batch size adjustment**: With fewer workers, each worker must process more samples to maintain the global batch size, or the global batch size must be reduced. Reducing global batch size may require learning rate adjustment.

**Learning rate scaling**: The relationship between batch size and optimal learning rate follows (approximately) a square root scaling law:

$$
\eta_{new} = \eta_{base} \times \sqrt{\frac{N_{new}}{N_{base}}}
$$ {#eq-lr-scaling}

where $N$ represents the number of workers (and thus the global batch size). This scaling maintains similar training dynamics when worker count changes.

**Gradient accumulation**: To maintain effective batch size with fewer workers, each worker can accumulate gradients over multiple micro-batches before synchronization. If worker count drops by half, doubling the accumulation steps maintains the same effective batch size.

**Data loader redistribution**: When workers change, the data loader must redistribute data assignments. This requires coordination to ensure all data is processed and no data is duplicated or dropped.

**State resharding**: If using sharded model parallelism (ZeRO/FSDP), changing worker count requires redistributing model shards. This can be done online (migrating shards between workers) or through checkpoint reload (resharding during recovery).

#### Framework Support for Elastic Training {#sec-elastic-framework-support}

Modern frameworks provide varying levels of elastic training support:

**PyTorch Elastic (TorchElastic)**: Provides elastic launch capabilities through `torchrun`. Supports membership changes through a rendezvous mechanism. Workers can join or leave, and the training process adapts. Integrates with Kubernetes for automatic scaling.

**DeepSpeed**: Supports elastic training through integration with Azure ML and automatic checkpoint/restart mechanisms. The ZeRO optimizer can reshard checkpoints for different worker counts.

**Ray Train**: Built on Ray's actor model, provides native elasticity. Workers are Ray actors that can be dynamically added or removed. Ray's distributed object store facilitates efficient state redistribution.

**Horovod Elastic**: Extends Horovod's data parallel training with elastic capabilities. Workers can join or leave during training, with automatic rank reassignment and gradient accumulation adjustment.

@tbl-elastic-training-comparison compares elastic training capabilities across frameworks.

| Framework | Elastic Support | Automatic Recovery | State Resharding | Cluster Integration |
|:----------|:---------------|:-------------------|:-----------------|:-------------------|
| PyTorch Elastic | Yes | Yes | Manual | Kubernetes |
| DeepSpeed | Yes | Yes | Automatic | Azure ML, SLURM |
| Ray Train | Yes | Yes | Automatic | Ray Cluster |
| Horovod Elastic | Yes | Yes | Manual | SLURM, Kubernetes |

: **Elastic Training Framework Comparison**: Modern frameworks provide varying levels of support for elastic training, with different approaches to automatic recovery and state management. {#tbl-elastic-training-comparison}

### Model-Specific Training Fault Tolerance {#sec-model-specific-training-ft}

The checkpoint and recovery strategies developed above require adaptation for different model types due to their distinct characteristics.

#### Large Language Models {#sec-llm-training-ft}

LLM training presents the most demanding fault tolerance requirements due to massive checkpoint sizes and extended training durations.

**Checkpoint optimization for LLMs**:

- Use mixed-precision checkpoints (FP16/BF16 for weights, FP32 for optimizer critical state)
- Leverage ZeRO/FSDP sharding to distribute checkpoint writes
- Implement asynchronous checkpointing to minimize training disruption
- Use incremental checkpoints that store only changed state

**Curriculum and position tracking**: LLM training often uses curriculum learning (training on different data distributions over time) and position tracking (which documents have been processed). Checkpoint state must include curriculum position to ensure correct data presentation after recovery.

**Long-context considerations**: Models trained with long contexts (32K, 128K tokens) have larger activation memory and correspondingly larger per-step state. Checkpoint frequency may need adjustment to balance recovery granularity against checkpoint overhead.

#### Recommendation Systems {#sec-recsys-training-ft}

Recommendation models with trillion-parameter embedding tables present unique checkpoint challenges.

**Embedding table checkpointing**: Embedding tables can be multiple terabytes. Full checkpointing at high frequency is impractical. Strategies include:

- **Incremental checkpointing**: Only save embeddings that changed since last checkpoint
- **Tiered checkpointing**: Frequent checkpoints for model parameters, infrequent for embeddings
- **Embedding versioning**: Maintain embedding versions with efficient delta storage

**Continuous training considerations**: RecSys models often train continuously on streaming data. The concept of "training completion" does not apply. Fault tolerance focuses on minimizing data loss and maintaining embedding freshness rather than protecting a single long training run.

**Feature store coordination**: RecSys training often depends on feature stores for user and item features. Checkpoint state must include feature versions to ensure consistency between model state and features used for training.

#### Vision Models {#sec-vision-training-ft}

Vision models have moderate checkpoint sizes but present unique considerations:

**Data augmentation state**: Reproducible training requires capturing augmentation state (random seeds, augmentation parameters). Recovery should produce identical training trajectories to minimize variance.

**Batch normalization synchronization**: Vision models using batch normalization require care during recovery. Running statistics must be consistent across workers. Synchronized batch norm requires explicit coordination of statistics during recovery.

**Multi-scale training**: Some vision training uses progressive resizing or multi-scale inputs. Checkpoint must capture current scale configuration and schedule position.

#### Scientific and Specialized Models {#sec-scientific-training-ft}

Scientific models (protein structure prediction, molecular dynamics, climate simulation) often have domain-specific state:

**Exploration state**: Models exploring large search spaces (protein conformations, molecular configurations) must track what has been explored. Losing this state causes redundant exploration.

**Simulation state**: Models coupled with simulations must checkpoint both ML model state and simulation state for consistent recovery.

**Reproducibility requirements**: Scientific applications often require exact reproducibility for validation. Checkpoint and recovery must preserve complete determinism, requiring careful handling of all random state.

## Serving Fault Tolerance {#sec-serving-fault-tolerance}

Training fault tolerance accepts minutes of recovery time in exchange for protecting hours or days of training progress. Serving fault tolerance operates under fundamentally different constraints: millisecond latency requirements and continuous availability expectations. A training system that takes five minutes to recover loses five minutes of training time. A serving system that takes five minutes to recover may lose millions of requests and substantial revenue.

This section develops fault tolerance mechanisms appropriate for serving's stringent requirements, building on the serving fundamentals introduced in Volume I while addressing the scale and complexity of distributed serving systems.

### Stateless vs Stateful Serving {#sec-stateless-stateful-serving}

The complexity of serving fault tolerance depends critically on whether the serving system maintains state across requests.

#### Stateless Serving {#sec-stateless-serving}

In stateless serving, each request is independent. The serving system maintains no per-session state; all information needed to process a request is contained in the request itself plus the static model weights.

Examples of stateless serving:

- **Image classification**: Each image is classified independently
- **Object detection**: Each frame is processed independently
- **Single-turn text classification**: Each text snippet is classified without context
- **Embedding generation**: Each input is embedded independently

Fault tolerance for stateless serving is straightforward:

- **Redundant replicas**: Multiple copies of the model serve requests in parallel
- **Load balancing**: Requests are distributed across healthy replicas
- **Health checks**: Failed replicas are removed from the load balancer
- **Automatic replacement**: Failed replicas are restarted or replaced

When a replica fails, in-flight requests to that replica fail but can be retried on another replica. No state is lost. Recovery requires only starting a new replica and loading model weights, typically completing in seconds to minutes depending on model size.

#### Stateful Serving {#sec-stateful-serving}

Stateful serving maintains state across requests within a session. Subsequent requests depend on state accumulated from previous requests.

Examples of stateful serving:

- **LLM conversations**: KV cache accumulates attention state across turns
- **Streaming speech recognition**: Model maintains context from previous audio
- **Recommendation sessions**: User context accumulates within a session
- **Interactive editing**: Model maintains document state across edits

Stateful serving complicates fault tolerance because state loss degrades service:

- **KV cache loss**: Regenerating KV cache requires reprocessing all previous turns
- **Session context loss**: User must repeat previous interactions
- **Accumulated state loss**: Quality degrades when context is unavailable

Fault tolerance for stateful serving requires:

- **Session affinity**: Requests within a session route to the same replica
- **State checkpointing**: Session state is periodically saved for recovery
- **State replication**: Session state is replicated for high availability
- **Graceful degradation**: Service continues with reduced quality if state is lost

@tbl-stateless-stateful-comparison contrasts fault tolerance approaches for stateless and stateful serving.

| Aspect | Stateless Serving | Stateful Serving |
|:-------|:-----------------|:-----------------|
| Request routing | Any replica | Session-affine replica |
| Failure impact | Retry on another replica | Potential state loss |
| Recovery complexity | Restart and load weights | Reload state + reconstruct context |
| Redundancy approach | Active-active replicas | Replicated state + standby |
| Failover latency | Milliseconds (load balancer) | Seconds (state transfer) |

: **Stateless vs Stateful Serving Fault Tolerance**: Stateful serving introduces significant complexity in fault tolerance due to the need to preserve accumulated session state. {#tbl-stateless-stateful-comparison}

### Redundancy and Replication {#sec-redundancy-replication}

Redundancy is the foundation of serving fault tolerance. By maintaining multiple copies of serving capability, the system can continue operating when individual copies fail.

#### Availability Calculations {#sec-availability-calculations}

For a single replica with availability $A_{single}$ (probability of being operational at any given time), multiple independent replicas achieve higher system availability:

$$
A_{system} = 1 - (1 - A_{single})^R
$$ {#eq-availability-redundancy}

where $R$ is the number of replicas.

**Worked Example**:

Single replica availability: $A_{single} = 99\%$ (3.65 days of downtime per year)

With two replicas: $A = 1 - (0.01)^2 = 99.99\%$ (52.6 minutes downtime per year)

With three replicas: $A = 1 - (0.01)^3 = 99.9999\%$ (31.5 seconds downtime per year)

This calculation assumes independent failures. Correlated failures (shared power, shared network, software bugs) reduce actual availability below these theoretical values.

#### Replication Strategies {#sec-replication-strategies}

**Active-active replication**: All replicas actively serve requests. Load is distributed across replicas. Failure of one replica increases load on remaining replicas. This approach maximizes resource utilization but requires sufficient capacity in remaining replicas to handle increased load.

**Active-passive replication**: Primary replicas serve requests while standby replicas remain idle but ready. Failure of a primary triggers failover to standby. This approach provides simpler failover but wastes standby resources during normal operation.

**Geographic replication**: Replicas are distributed across geographic regions. This protects against regional failures (datacenter outage, regional network issues) but introduces latency for requests routed to distant regions.

**Multi-tier replication**: Different replication strategies at different levels:

- Edge caches replicated for latency optimization
- Regional serving clusters for geographic coverage
- Global primary for consistency and freshness

#### Replica Placement and Failure Domains {#sec-replica-placement}

Effective redundancy requires placing replicas in independent failure domains:

- **Different machines**: Tolerates individual machine failures
- **Different racks**: Tolerates rack-level failures (power, ToR switch)
- **Different availability zones**: Tolerates datacenter section failures
- **Different regions**: Tolerates entire datacenter failures

The level of independence should match the availability requirements and cost constraints. Regional replication is expensive (duplicate compute, network costs) but necessary for the highest availability requirements.

### Failover Mechanisms {#sec-failover-mechanisms}

When a replica fails, traffic must be redirected to healthy replicas. The speed and reliability of this failover determines the impact of failures on users.

#### Health Checking {#sec-health-checking}

Health checks verify that replicas are operational and ready to serve requests:

**Liveness checks**: Verify that the process is running and responsive. A simple HTTP endpoint that returns 200 indicates liveness. Failure to respond triggers process restart.

**Readiness checks**: Verify that the replica is ready to serve requests. For ML serving, readiness includes:

- Model weights loaded
- GPU initialized and responsive
- Warmup complete (first inference often slower)
- Dependencies available (feature stores, caches)

**Inference health checks**: Verify that inference produces correct results. Run a known input through the model and verify the output matches expected results. This catches silent failures where the model produces incorrect results without errors.

Health check parameters require tuning:

- **Check interval**: How often to check (e.g., every 5 seconds)
- **Timeout**: How long to wait for response (e.g., 2 seconds)
- **Failure threshold**: How many failures before marking unhealthy (e.g., 3)
- **Success threshold**: How many successes before marking healthy (e.g., 2)

#### Load Balancer Integration {#sec-load-balancer-integration}

Load balancers route requests to healthy replicas and remove unhealthy replicas from rotation:

- **L4 load balancing**: Routes based on IP/port, simple and fast
- **L7 load balancing**: Routes based on HTTP/gRPC content, enables sophisticated routing
- **Service mesh**: Provides advanced traffic management, observability, security

Load balancer failover latency depends on health check frequency and failure detection logic. Aggressive settings enable fast failover but increase false positives (marking healthy replicas as unhealthy during transient issues).

#### Session Affinity and Stateful Failover {#sec-session-affinity}

For stateful serving, session affinity routes all requests within a session to the same replica:

**Sticky sessions**: Load balancer maintains session-to-replica mapping. Implemented via cookies, headers, or IP hashing.

**State failover options**:

1. **State loss**: Accept degraded quality on failover (regenerate state from scratch)
2. **State checkpointing**: Periodically save session state for recovery
3. **State replication**: Replicate state to standby replica
4. **Distributed state**: Store session state in external store (Redis, Memcached)

The choice depends on state size, update frequency, and quality impact of state loss:

| Approach | Recovery Latency | Consistency | Operational Complexity |
|:---------|:----------------|:------------|:----------------------|
| State loss | Fast | None | Low |
| Checkpointing | Medium | Eventual | Medium |
| Synchronous replication | Fast | Strong | High |
| Distributed state | Fast | Configurable | Medium |

### Model-Specific Serving Fault Tolerance {#sec-model-specific-serving-ft}

Different model types have distinct serving fault tolerance requirements based on their state characteristics and latency constraints.

#### LLM Serving Fault Tolerance {#sec-llm-serving-ft}

LLM serving with conversational context presents significant fault tolerance challenges:

**KV cache state**: The KV cache can be substantial (gigabytes for long contexts across attention layers). Losing the KV cache requires regenerating all previous turns, which can take seconds to minutes.

**Fault tolerance approaches for LLM serving**:

1. **Accept regeneration cost**: On failure, regenerate KV cache from conversation history. Simple but can significantly increase latency for long conversations.

2. **KV cache checkpointing**: Periodically save KV cache state. Enables partial recovery but introduces storage overhead and latency for checkpointing.

3. **KV cache replication**: Replicate KV cache to standby. Provides fast failover but doubles memory requirements.

4. **Prefix caching**: Cache common prefixes (system prompts, shared context) separately. On failure, restore common prefixes quickly, regenerate only session-specific state.

**Prompt caching services** like those offered by cloud providers store and reuse KV cache for common prefixes, reducing both cost and recovery time for failures.

#### Recommendation Serving Fault Tolerance {#sec-recsys-serving-ft}

Recommendation systems have unique fault tolerance requirements centered on feature stores and real-time updates:

**Feature store availability**: Recommendations depend on user and item features from feature stores. Feature store unavailability degrades recommendation quality or blocks recommendations entirely.

**Feature store fault tolerance**:

- Replicated feature stores across availability zones
- Local caching of frequently accessed features
- Fallback to stale features with quality degradation
- Default features when feature lookup fails

**Real-time feature updates**: Some features (recent user actions) update in real-time. Failure of real-time feature pipelines causes recommendations to use stale data. Monitoring feature freshness and alerting on staleness is essential.

**Embedding service availability**: Large embedding tables may be served from dedicated embedding services. These services require their own fault tolerance through replication and failover.

#### Vision Serving Fault Tolerance {#sec-vision-serving-ft}

Vision model serving is typically stateless, simplifying fault tolerance:

**Simple redundancy**: Multiple model replicas behind load balancer. Failure of one replica routes requests to others.

**GPU health monitoring**: Vision inference is GPU-intensive. GPU failures (thermal issues, memory errors) should trigger replica restart. NVIDIA's DCGM provides GPU health monitoring.

**Preprocessing pipeline**: Vision models depend on preprocessing (resize, normalize). Preprocessing failures should be detected and handled gracefully, potentially returning errors rather than incorrect predictions.

**Edge deployment considerations**: Vision models deployed on edge devices face different fault tolerance challenges: device failures, network disconnection, local storage limitations. Edge fault tolerance often involves graceful degradation when cloud connectivity is lost.

## Graceful Degradation {#sec-graceful-degradation}

Fault tolerance aims to maintain service despite failures. When maintaining full service quality is impossible, graceful degradation maintains partial service at reduced quality rather than complete failure. Users experience degraded quality but still receive value from the system.

### Degradation Dimensions {#sec-degradation-dimensions}

Service can degrade along multiple dimensions:

**Quality degradation**: Serve predictions from simpler, faster models with lower accuracy. A recommendation system might fall back from a sophisticated multi-tower model to a simpler collaborative filtering model.

**Latency degradation**: Accept longer response times to maintain quality. Under high load, batching more requests together increases latency but maintains throughput.

**Coverage degradation**: Serve partial results rather than complete results. A search system might return top-10 results instead of top-100 when compute is constrained.

**Freshness degradation**: Serve cached or stale results rather than real-time computation. News recommendations might serve hour-old recommendations when the recommendation service is unavailable.

**Feature degradation**: Use fewer features when feature retrieval fails. A recommendation system might use only content features when user history is unavailable.

### Graceful Degradation Strategies {#sec-degradation-strategies}

#### Model Fallback {#sec-model-fallback}

Maintain multiple model versions with different resource requirements:

- **Primary model**: Full capability, highest resource requirements
- **Secondary model**: Reduced capability, lower resource requirements
- **Tertiary model**: Minimal capability, minimal resources
- **Static fallback**: Precomputed defaults, no inference required

When primary model is unavailable or overloaded, fall back to secondary. Continue falling back as necessary.

**Example cascade for image classification**:

1. **Primary**: ViT-Large (307M params, 99.2% accuracy)
2. **Secondary**: EfficientNet-B4 (19M params, 97.1% accuracy)
3. **Tertiary**: MobileNet-V3 (5.4M params, 93.2% accuracy)
4. **Fallback**: Return "classification unavailable" or cached results

Model fallback requires:

- Multiple models deployed and ready to serve
- Routing logic to select appropriate model
- Monitoring to track fallback frequency
- Quality metrics to measure degradation impact

#### Feature Fallback {#sec-feature-fallback}

When feature retrieval fails, use default or computed fallback values:

**Default features**: Precomputed population-level defaults substitute for missing user or item features. For a recommendation system:

- Missing user embedding: Use average embedding across all users
- Missing item features: Use genre/category-level defaults
- Missing real-time features: Use most recent cached value

**Feature computation fallback**: Compute approximate features from available data:

- Missing user history: Use demographic similarity
- Missing item attributes: Use text embedding from title/description
- Missing contextual features: Use time-based defaults

**Feature importance tiers**: Prioritize features by importance to prediction quality:

| Tier | Example Features | Missing Action | Quality Impact |
|:-----|:-----------------|:---------------|:---------------|
| Critical | User ID, Item ID | Block request | Cannot serve |
| Important | User history, Item attributes | Use defaults | 5-10% quality loss |
| Useful | Real-time context | Use cached | 2-5% quality loss |
| Optional | Secondary signals | Omit | < 2% quality loss |

#### Load Shedding {#sec-load-shedding}

When system capacity is insufficient for incoming load, deliberately drop requests to protect system stability:

**Random shedding**: Randomly drop a fraction of incoming requests. Simple but does not prioritize valuable requests.

**Priority-based shedding**: Classify requests by priority and drop low-priority requests first:

- Premium users serviced before free users
- Revenue-generating requests prioritized over analytics
- Interactive requests prioritized over background batch

**Admission control**: Rate limit at system entry points. Reject requests that would exceed capacity rather than accepting and degrading all requests.

**Circuit breakers**: When downstream services fail, stop sending requests rather than accumulating failures. Circuit breakers prevent cascade failures where one failing service causes callers to fail due to timeouts and resource exhaustion.

Circuit breaker states:

1. **Closed**: Normal operation, requests pass through
2. **Open**: Downstream is failing, requests fail immediately without attempting downstream call
3. **Half-open**: Testing recovery, allow limited requests through to test if downstream recovered

#### Graceful Degradation Implementation {#sec-degradation-implementation}

Implementing graceful degradation requires:

**Health metrics**: Continuous monitoring of system health:

- Request latency percentiles (p50, p95, p99)
- Error rates by error type
- Resource utilization (CPU, GPU, memory)
- Queue depths and wait times

**Degradation triggers**: Conditions that activate degradation:

```
if p99_latency > threshold:
    activate_model_fallback()

if error_rate > threshold:
    activate_circuit_breaker()

if feature_store_latency > threshold:
    activate_feature_fallback()
```

**Gradual degradation**: Progressively increase degradation as conditions worsen rather than binary switching. Increase fallback percentage gradually as load increases.

**Recovery**: Automatically recover from degraded state when conditions improve. Use hysteresis to prevent oscillation (require sustained improvement before recovering).

### Degradation Monitoring and Alerting {#sec-degradation-monitoring}

Graceful degradation enables continued operation but at reduced quality. Monitoring ensures degradation is detected, measured, and addressed:

**Degradation metrics**:

- Percentage of requests served by fallback models
- Percentage of features using defaults
- Request drop rate from load shedding
- Quality metric differences between primary and fallback

**Alerting thresholds**:

- Degradation activated: Informational alert
- Sustained degradation (> 5 minutes): Warning alert
- Severe degradation (> 50% requests affected): Critical alert
- Extended degradation (> 1 hour): Escalation

**Post-incident analysis**: After degradation events, analyze:

- Root cause of degradation
- Effectiveness of fallback mechanisms
- User impact and business impact
- Improvements to prevent future degradation

## Distributed Debugging and Observability {#sec-distributed-debugging}

Diagnosing failures in distributed ML systems presents unique challenges. The non-determinism inherent in distributed computation, combined with the complexity of ML inference pipelines, makes failures difficult to reproduce and diagnose. This section develops the observability infrastructure necessary for effective debugging.

### Why Distributed ML Systems Are Hard to Debug {#sec-debugging-challenges}

Several factors combine to make distributed ML debugging exceptionally challenging:

**Non-determinism**: Distributed systems exhibit non-deterministic behavior due to:

- Network timing variations
- Thread scheduling differences
- GPU kernel execution order
- Floating-point operation ordering

A bug that manifests on one execution may not reproduce on subsequent executions, creating "Heisenbugs" that appear to disappear when observed.

**Partial failures**: Unlike single-machine systems where failures are typically total, distributed systems experience partial failures. Some components fail while others continue. The interaction between working and failed components produces complex failure modes.

**Scale**: With thousands of components, manual inspection is impossible. Automated tools must filter relevant information from massive telemetry streams.

**Emergent behavior**: System behavior emerges from interactions between components. Individual components may appear healthy while system-level behavior is incorrect.

**ML-specific challenges**:

- Silent accuracy degradation (wrong results without errors)
- Numerical issues (NaN, infinity) that propagate through computation
- Data-dependent bugs that manifest only for specific inputs
- Model behavior changes that are intentional (learning) vs. bugs

### Observability Pillars {#sec-observability-pillars}

Effective distributed debugging requires three observability pillars: metrics, logs, and traces.

#### Metrics {#sec-metrics}

Metrics are numerical measurements collected over time:

**Infrastructure metrics**:

- CPU/GPU utilization
- Memory usage and allocation
- Network bandwidth and latency
- Storage I/O and latency

**Application metrics**:

- Request rate and latency percentiles
- Error counts by error type
- Queue depths and wait times
- Cache hit rates

**ML-specific metrics**:

- Inference latency by model
- Batch utilization
- Feature retrieval latency
- Model prediction distributions (for drift detection)

Metrics enable:

- Real-time dashboards showing system health
- Alerting when metrics exceed thresholds
- Capacity planning based on utilization trends
- Performance analysis and optimization

#### Logs {#sec-logs}

Logs capture discrete events with context:

**Structured logging**: Use structured formats (JSON) with consistent fields:

```json
{
  "timestamp": "2024-01-15T10:23:45.123Z",
  "level": "ERROR",
  "service": "inference-server",
  "trace_id": "abc123",
  "span_id": "def456",
  "message": "GPU memory allocation failed",
  "gpu_id": 3,
  "requested_bytes": 4294967296,
  "available_bytes": 2147483648
}
```

**Log levels**: Use consistent log levels:

- DEBUG: Detailed diagnostic information
- INFO: General operational information
- WARN: Potential problems that don't prevent operation
- ERROR: Problems that prevent specific operations
- FATAL: System-wide failures requiring immediate attention

**Log aggregation**: Centralize logs from all components for search and analysis. Tools like Elasticsearch, Loki, or cloud logging services enable searching across distributed logs.

#### Traces {#sec-traces}

Traces track requests across distributed components:

**Distributed tracing concepts**:

- **Trace**: End-to-end journey of a request
- **Span**: Single operation within a trace
- **Context propagation**: Passing trace context between services

**Trace example for ML inference**:

```
Trace: user-request-12345
âââ Span: api-gateway (5ms)
â   âââ Span: auth-service (2ms)
âââ Span: feature-service (15ms)
â   âââ Span: user-feature-lookup (8ms)
â   âââ Span: item-feature-lookup (12ms)
âââ Span: inference-service (45ms)
â   âââ Span: preprocessing (3ms)
â   âââ Span: model-inference (40ms)
â   âââ Span: postprocessing (2ms)
âââ Span: response-formatting (1ms)
Total: 68ms
```

**Tracing tools**: OpenTelemetry provides a standard API for distributed tracing. Backend systems like Jaeger, Zipkin, or cloud tracing services store and visualize traces.

### ML-Specific Debugging {#sec-ml-specific-debugging}

Beyond general distributed debugging, ML systems require specialized debugging capabilities:

#### Numerical Debugging {#sec-numerical-debugging}

ML computations are prone to numerical issues:

**NaN detection**: Check for NaN values in model outputs:

```python
if torch.isnan(output).any():
    log.error("NaN detected in output", input_hash=hash(input))
    # Return fallback or raise error
```

**Gradient monitoring**: During training, monitor gradient statistics:

- Gradient norm (detect explosion or vanishing)
- Gradient distribution (detect anomalies)
- Layer-wise gradients (identify problematic layers)

**Precision issues**: Mixed-precision training can introduce numerical issues. Monitor for:

- Loss scale adjustments (indicates underflow)
- Gradient overflow (values exceed FP16 range)
- Inconsistency between FP16 and FP32 results

#### Data Debugging {#sec-data-debugging}

ML bugs often originate in data:

**Input validation**: Validate inputs match expected format:

- Shape matches expected dimensions
- Values in expected ranges
- Required fields present
- Encoding matches expected format

**Feature distribution monitoring**: Track feature distributions over time:

- Detect distribution shift (feature values changing)
- Detect missing features (null rates increasing)
- Detect outliers (extreme values appearing)

**Data pipeline debugging**: Trace data transformations:

- Log intermediate data shapes and statistics
- Validate transforms produce expected results
- Compare pipeline outputs against known-good data

#### Straggler Detection and Analysis {#sec-straggler-analysis}

Identify and diagnose slow components:

**Timing instrumentation**: Measure time for each operation:

```python
with timer("feature_lookup"):
    features = feature_store.lookup(ids)
with timer("model_inference"):
    predictions = model(features)
```

**Percentile analysis**: Compare component timing across replicas:

- p50 shows typical performance
- p99 shows tail latency
- Comparing p99 across replicas identifies stragglers

**Straggler root causes**:

- Hardware issues (thermal throttling, memory errors)
- Data skew (some inputs slower to process)
- Resource contention (other processes consuming resources)
- Network issues (slow connection to data stores)

### Common Failure Patterns {#sec-common-failure-patterns}

Experience with large-scale ML systems reveals recurring failure patterns:

#### Training Failures {#sec-training-failure-patterns}

**Loss spike then recovery**: Transient data issue or numerical instability caused temporary spike. Usually self-correcting but should be investigated.

**Loss spike then plateau**: Learning rate too high, corrupted checkpoint, or data bug. Requires investigation and potentially rollback.

**Gradual divergence**: Silent data corruption, hardware error, or distributed training desynchronization. Difficult to detect, requires monitoring.

**Hang without error**: Deadlock in collective communication, crashed worker blocking synchronization. Requires timeout detection.

#### Serving Failures {#sec-serving-failure-patterns}

**Latency spike**: Resource contention, garbage collection, cold cache, or model reload. Usually transient but repeated spikes indicate capacity issues.

**Error rate increase**: Dependency failure, data format change, or model bug. Requires immediate investigation.

**Silent quality degradation**: Model drift, feature degradation, or data pipeline issues. Requires quality monitoring to detect.

**Cascade failure**: One failing component causes others to fail through timeout exhaustion, resource depletion, or error propagation. Requires circuit breakers and isolation.

## Case Studies {#sec-case-studies}

This section examines how leading organizations implement fault tolerance in production ML systems, illustrating the principles developed throughout this chapter.

### OpenAI: Training Fault Tolerance at GPT Scale {#sec-openai-case-study}

Training GPT-4 scale models requires fault tolerance capable of handling hundreds of failures during multi-week training runs.[^fn-openai-scale]

[^fn-openai-scale]: **GPT-4 Training Scale**: While OpenAI has not published complete details, industry estimates based on available information suggest GPT-4 training consumed approximately 25,000 A100 GPUs over 90-100 days. At this scale, the infrastructure would experience multiple failures per hour, requiring sophisticated fault tolerance to complete training.

**Infrastructure configuration**:

- Cluster scale: Estimated 25,000+ GPUs
- Expected failure rate: Multiple failures per hour
- Training duration: 3+ months
- Checkpoint size: Multiple terabytes

**Fault tolerance mechanisms**:

1. **Hierarchical checkpointing**: Frequent lightweight checkpoints for fast recovery from common failures, plus periodic full checkpoints for major milestones.

2. **Elastic training**: Training continues with reduced worker count during failures, with workers rejoining as they recover.

3. **Training dynamics monitoring**: Continuous monitoring of loss curves, gradient statistics, and activation distributions to detect silent corruption.

4. **Redundant infrastructure**: Multiple power feeds, network paths, and storage systems to minimize correlated failures.

**Lessons learned**:

- At sufficient scale, deterministic replay becomes impossible; probabilistic equivalence is the target
- Investment in debugging infrastructure pays off quickly through faster failure diagnosis
- Proactive hardware replacement based on telemetry prevents many failures

### Google: TPU Pod Failure Handling {#sec-google-tpu-case-study}

Google's TPU pods present unique fault tolerance challenges due to their tightly-coupled architecture.[^fn-tpu-pods]

[^fn-tpu-pods]: **TPU Pod Architecture**: TPU v4 pods contain up to 4,096 TPU chips connected via high-bandwidth custom interconnects. The tightly-coupled design enables efficient collective operations but means that chip failures affect the entire pod rather than individual nodes.

**TPU-specific challenges**:

- Chips are interconnected in 3D torus; failures require routing around failed chips
- Custom interconnects make hot-swap replacement difficult
- Firmware and software must coordinate for consistent state

**Fault tolerance approach**:

1. **Spare chips**: Pods include spare chips that can replace failed chips through reconfiguration
2. **Checkpoint to distributed storage**: TPU state checkpoints to Google Cloud Storage for durability
3. **Pod-level restart**: Major failures restart the entire pod from checkpoint rather than partial recovery
4. **Predictive replacement**: Telemetry identifies chips likely to fail, enabling proactive replacement

**Training efficiency impact**:

Google reports achieving 90%+ training efficiency (actual training time / ideal training time) on large TPU pods despite regular failures, through careful checkpoint interval optimization and fast recovery mechanisms.

### Meta: Recommendation Serving Resilience {#sec-meta-case-study}

Meta's recommendation systems serve billions of users with strict availability requirements.[^fn-meta-recsys]

[^fn-meta-recsys]: **Meta Recommendation Scale**: Meta's recommendation systems process hundreds of millions of inference requests per second across multiple products (Feed, Reels, Ads). The systems must maintain 99.99%+ availability while continuously adapting to user behavior.

**System requirements**:

- Availability target: 99.99%+ (< 52 minutes downtime per year)
- Latency target: p99 < 50ms
- Scale: Hundreds of millions QPS across products
- Continuous model updates for freshness

**Fault tolerance architecture**:

1. **Multi-tier caching**: Cached recommendations at edge reduce dependence on backend availability
2. **Feature store replication**: Feature stores replicated across datacenters with automatic failover
3. **Model redundancy**: Multiple model versions deployed; fallback to previous version on issues
4. **Graceful degradation**: Quality-tiered fallbacks from personalized to popular to cached

**Key innovations**:

- **Predictive prefetching**: Prefetch likely-needed features during low load to reduce real-time dependency
- **Embedding compression**: Compressed embeddings enable faster recovery and replication
- **A/B testing integration**: Automatic rollback when quality metrics degrade

### Netflix: Chaos Engineering for ML {#sec-netflix-case-study}

Netflix pioneered chaos engineering and applies these principles to ML systems.[^fn-netflix-chaos]

[^fn-netflix-chaos]: **Netflix Chaos Engineering**: Netflix's Chaos Monkey and related tools (Chaos Kong, Latency Monkey) intentionally inject failures into production systems to verify fault tolerance. This approach has become an industry standard for building reliable distributed systems.

**Chaos engineering principles for ML**:

1. **Inject failures in production**: Real fault tolerance requires testing with real production conditions
2. **Automate experiments**: Continuous chaos experiments verify ongoing resilience
3. **Minimize blast radius**: Start with small-scope failures and expand
4. **Have a hypothesis**: Each experiment tests a specific fault tolerance mechanism

**ML-specific chaos experiments**:

- **Model server termination**: Verify load balancer routes around failed servers
- **Feature store latency injection**: Verify fallback features activate appropriately
- **GPU memory pressure**: Verify graceful degradation under resource constraints
- **Model file corruption**: Verify integrity checks detect corruption

**Results and learnings**:

- Chaos experiments regularly discover latent issues before they cause outages
- Testing recovery mechanisms is as important as building them
- Developer confidence in fault tolerance enables faster deployment

### Synthesis: Common Themes Across Case Studies {#sec-case-study-synthesis}

Several themes emerge across these diverse implementations:

1. **Proactive over reactive**: Leading organizations invest in predicting and preventing failures rather than just recovering from them.

2. **Observability is essential**: All organizations emphasize comprehensive monitoring as the foundation for fault tolerance.

3. **Graceful degradation by design**: Systems are designed with explicit degradation paths rather than adding them after failures occur.

4. **Testing fault tolerance**: Chaos engineering and similar practices verify that fault tolerance mechanisms actually work.

5. **Investment scales with value**: Organizations invest in fault tolerance proportional to the cost of failures for their specific systems.

## Chapter Summary {#sec-chapter-summary}

This chapter developed a comprehensive framework for understanding and implementing fault tolerance in machine learning systems at scale. The key insights can be organized around three themes: mathematical foundations, practical mechanisms, and operational considerations.

**Mathematical Foundations**: Reliability engineering provides the quantitative tools for reasoning about failure. The exponential reliability model (@eq-system-reliability-n-components) shows that system failure rate grows linearly with component count, making failures inevitable at scale. The Young-Daly formula (@eq-young-daly) provides optimal checkpoint intervals that balance checkpoint overhead against expected lost work. These mathematical tools transform fault tolerance from intuition-based design to quantitative engineering.

**Practical Mechanisms**: Training and serving require fundamentally different fault tolerance approaches due to their different latency tolerances. Training fault tolerance centers on checkpoint and restart, with distributed checkpointing, elastic training, and failure detection enabling continued training despite failures. Serving fault tolerance emphasizes redundancy, replication, and graceful degradation, enabling continued service with minimal user impact. The distinction between stateless and stateful serving significantly affects fault tolerance complexity.

**Operational Considerations**: Production fault tolerance requires comprehensive observability through metrics, logs, and traces. ML-specific debugging addresses numerical issues, data problems, and the unique challenge of silent quality degradation. Graceful degradation strategies including model fallback, feature fallback, and load shedding maintain partial service when full capability is unavailable.

The case studies demonstrate that fault tolerance at scale requires systematic engineering investment. Organizations successfully operating large-scale ML systems treat fault tolerance as a first-class concern, investing in proactive failure prevention, comprehensive monitoring, and regular testing through chaos engineering.

::: {.callout-important title="The 3 Things to Remember"}

1. **At scale, failures are continuous, not exceptional.** A 10,000-GPU cluster experiences failures every few hours. Systems must be designed expecting failure as a normal operating condition, not an exceptional event to be avoided.

2. **The optimal checkpoint interval is $\sqrt{2 \times T_{save} \times MTBF}$.** The Young-Daly formula provides quantitative guidance for checkpoint frequency that minimizes total overhead from both checkpointing and failure recovery.

3. **Training and serving have fundamentally different fault tolerance requirements.** Training tolerates minutes of recovery time to protect days of compute investment. Serving requires millisecond-scale failover to maintain user experience. These different requirements demand entirely different architectural approaches.

:::

## Fallacies and Pitfalls {#sec-fault-tolerance-fallacies-pitfalls}

Fault tolerance involves subtle trade-offs that generate persistent misconceptions. These fallacies and pitfalls capture errors that compromise system reliability.

**Fallacy: Hardware failures are the main concern.**

This intuition comes from traditional systems where disk failures, power outages, and network partitions dominate. In ML systems, software failures and configuration errors cause the majority of incidents.

A survey of large-scale training failures found:

- Hardware failures: 15-25%
- Software bugs: 30-40%
- Configuration errors: 20-30%
- Resource exhaustion: 15-20%
- Unknown/other: 5-10%

Investing heavily in hardware redundancy while neglecting software robustness (input validation, gradual rollouts, configuration management) leaves the majority of failure modes unaddressed. The most reliable ML systems treat software bugs as inevitable and design defensively.

**Pitfall: Setting checkpoint interval by intuition.**

Organizations commonly set checkpoint intervals based on "feels right": "every hour seems reasonable" or "every 1000 steps." The Young-Daly formula reveals these intuitions are often wrong.

For a 1000-GPU cluster with MTBF of 4 hours and checkpoint time of 5 minutes:

$$T_{opt} = \sqrt{2 \times 5 \times 240} = \sqrt{2400} \approx 49 \text{ minutes}$$

The intuitive "every hour" is close but suboptimal. More critically, if checkpoint time increases to 15 minutes (larger model, slower storage), the optimal interval becomes 85 minutes, not the "every 15 minutes" that some teams adopt to "stay safe." Too-frequent checkpointing wastes more compute than it saves.

The quantitative approach reveals that intuition-based intervals often deviate 2-3x from optimal in either direction.

**Fallacy: MTBF assumes independent failures.**

The reliability equation $MTBF_{system} = MTBF_{component}/N$ assumes component failures are statistically independent. In production, failures correlate:

- **Shared power domain**: UPS failure takes down entire rack
- **Shared switch**: Top-of-rack switch failure partitions all connected GPUs
- **Shared software**: Bug triggered by specific input fails all replicas simultaneously
- **Thermal correlation**: Cooling failure causes clustered GPU throttling

Correlated failures reduce effective MTBF below the independent-failure calculation, sometimes dramatically. A cluster with 1000 "independent" GPUs each with 10,000-hour MTBF should have system MTBF of 10 hours. If failures correlate with factor 10 (10 GPUs fail together on average), effective MTBF drops to 1 hour.

Reliability engineering must identify and mitigate correlation through diversity: different power feeds, different network paths, different software versions in canary deployments.

**Pitfall: Ignoring restart overhead in checkpoint planning.**

The Young-Daly formula accounts for checkpoint save time but practitioners often forget restart overhead:

1. **Job scheduling delay**: Acquiring replacement GPUs takes minutes in shared clusters
2. **Checkpoint loading**: Reading distributed checkpoint from storage
3. **Warmup time**: Learning rate warmup, batch normalization statistics recalculation
4. **Communication re-establishment**: NCCL ring topology reconstruction

Total restart time can be 3-5x checkpoint save time. A 5-minute checkpoint save followed by 20-minute restart means effective failure cost is 25 minutes, not 5 minutes. The modified Young-Daly formula should use $T_{save} + T_{restart}$ for the overhead term, significantly increasing optimal checkpoint intervals.

**Fallacy: Checkpoints are automatically consistent.**

Modern frameworks checkpoint transparently, creating the illusion of automatic consistency. In practice, distributed checkpoints require coordination that can fail subtly:

1. **Rank desynchronization**: If rank 0 checkpoints iteration 1000 while rank 1 checkpoints iteration 1001, the checkpoint is inconsistent
2. **Partial writes**: Storage failure mid-checkpoint leaves incomplete shards
3. **Optimizer state lag**: Sharded optimizer state may not match model weights if captured at different times
4. **In-flight gradients**: AllReduce in progress during checkpoint may or may not be included

Production systems must implement checkpoint validation: verify all shards exist, verify iteration numbers match, verify optimizer state matches model state. Organizations that discover corrupted checkpoints during recovery from a failure have no recourse except restarting from an earlier (potentially much earlier) checkpoint.

**Pitfall: Testing fault tolerance only during failures.**

Fault tolerance mechanisms are code paths that execute rarely in normal operation. Like backup systems never tested until disaster strikes, fault tolerance code paths accumulate bugs:

- Checkpoint restoration logic untested because training never crashed
- Fallback model never loaded because primary never failed
- Circuit breaker thresholds tuned for old traffic patterns

Chaos engineering (intentionally injecting failures) transforms fault tolerance from "we think it works" to "we know it works." Organizations that regularly kill random GPUs during training, inject network partitions, and fail primary models discover bugs before they matter.

The cost of regular fault injection (some failed experiments, some minor outages) is far less than the cost of discovering broken fault tolerance during an actual failure.

**Fallacy: Elastic training eliminates the need for checkpointing.**

Elastic training adjusts parallelism degree when workers fail, continuing training with reduced capacity. This appears to eliminate checkpoint-restart overhead. However:

1. **State consistency**: Reducing from N to N-1 workers requires redistributing model shards, optimizer states, and data assignments. This redistribution must be consistent.

2. **Minimum viable size**: Below some threshold, training is infeasible (model does not fit, batch size too small). Failures beyond this threshold still require checkpoint-restart.

3. **Efficiency degradation**: Each removed worker reduces throughput. Accumulated failures progressively degrade training speed until checkpoint-restart becomes preferable.

4. **Bug propagation**: If a failure was caused by a software bug triggered by specific data, the bug persists in the remaining workers and may strike again.

Elastic training is complementary to checkpointing, not a replacement. The reduced checkpoint frequency enabled by elasticity still requires occasional checkpoints for catastrophic failures and training completion.

**Fallacy: Silent data corruption is rare in modern hardware.**

Modern GPUs, interconnects, and memory systems include extensive error correction: ECC memory, CRC verification, and parity checks. This creates an intuition that silent data corruption (undetected bit flips affecting computation) is negligible. Studies from large-scale deployments reveal otherwise.

At planetary scale, silent data corruption rates are measurable:

- **GPU SRAM corruption**: 0.01-0.1% of GPUs per year exhibit silent corruption not caught by ECC
- **Memory corruption**: Cosmic ray-induced bit flips occur at approximately 1 bit flip per GB per month
- **Interconnect corruption**: Rare CRC collisions produce undetected errors at approximately 1 per 10^18 bits

For a 10,000-GPU cluster running continuously:

- Expect 1-10 silent GPU corruption events per year
- Expect hundreds of memory bit flips per month

**Training impact**: Silent corruption causes mysterious training anomalies. Loss spikes attributed to "bad batches" may be corruption. Gradient NaN events blamed on learning rates may be hardware. Models that fail to converge despite correct hyperparameters may have corrupted weights.

**Detection strategies**:

1. **Redundant computation**: Periodically compute a batch on multiple workers and compare results
2. **Gradient checksums**: Verify AllReduce produces identical gradients across replicas
3. **Statistical monitoring**: Track gradient and activation statistics for anomalies
4. **Hardware rotation**: Cycle through GPUs to identify misbehaving units

Silent data corruption is particularly insidious because it does not trigger errors. The training "succeeds" but produces a subtly broken model. Robust fault tolerance must include detection mechanisms alongside recovery mechanisms.

The principles developed in this chapter provide the foundation for the operational practices examined in @sec-ops-scale. Fault tolerance mechanisms must be integrated into broader operational workflows including deployment, monitoring, and incident response to achieve reliable ML systems in production.

::: {.callout-important title="Key Takeaways"}
* At distributed scale, component failures are routine rather than exceptional: a 10,000-GPU cluster with 10,000-hour MTBF per GPU will average one GPU failure per hour, requiring automated recovery without human intervention
* The Young-Daly formula provides the optimal checkpoint interval $\tau_{opt} = \sqrt{2 \cdot \delta \cdot M}$ where $\delta$ is checkpoint duration and $M$ is mean time between failures, balancing checkpoint overhead against lost work from failures
* Fault tolerance requirements differ by workload: training systems need checkpoint and restart mechanisms, while serving systems require redundancy, failover, and graceful degradation strategies
* Silent data corruption is more common than intuition suggests at scale and requires active detection through redundant computation, gradient checksums, and statistical anomaly monitoring rather than relying solely on hardware error correction
:::

```{=latex}
\part{key:vol2_production}
```
