---
title: "Inference at Scale"
bibliography: inference.bib
---

# Inference at Scale {#sec-inference-at-scale}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A dynamic illustration of a global inference serving system handling millions of requests. The scene shows a central model representation being replicated across multiple serving nodes arranged in a globe-spanning network. Incoming request streams appear as flowing arrows from users worldwide, processed through load balancers depicted as traffic controllers, then distributed to model replicas. Visual elements include batching queues shown as organized request groups, latency meters displaying millisecond readings, and auto-scaling indicators showing nodes spawning and despawning. The composition emphasizes real-time responsiveness with clock symbols and speed indicators. Color palette features cool silvers and whites for infrastructure, warm oranges for active requests, and green for successful responses. Technical yet accessible style suitable for a production systems textbook._
:::

\noindent
![](images/png/cover_inference_at_scale.png)

:::

## Purpose {.unnumbered}

_Why does serving machine learning predictions at scale require fundamentally different engineering approaches than training the models that generate them?_

Training optimizes for throughput over extended periods, tolerating latency variations that would be unacceptable when users await responses in real time. Inference at scale inverts these priorities: serving systems must deliver predictions within strict latency bounds while handling request volumes that fluctuate unpredictably across hours, days, and seasons. The engineering challenges of inference extend beyond raw performance to encompass resource efficiency when serving costs dwarf training costs over a model's lifetime, availability when users expect continuous service regardless of infrastructure conditions, and consistency when the same input must produce the same output across globally distributed serving infrastructure. Understanding how serving systems, batching strategies, model sharding, and load balancing address these challenges determines whether sophisticated models deliver value to users or remain impressive but impractical demonstrations. Mastering inference at scale transforms model capabilities into reliable services that operate continuously at the speed and scale users demand.

::: {.callout-tip title="Learning Objectives"}

- Quantify why serving cost dominates training cost over a model's lifetime using the total cost of serving equation

- Design batching strategies matched to model architectures, distinguishing static batching for vision models from continuous batching for LLMs and feature-parallel batching for recommendation systems

- Apply the serving hierarchy framework (request, replica, service, platform) to systematically identify and resolve inference bottlenecks at each level

- Compare model sharding strategies (tensor parallelism, pipeline parallelism, expert parallelism, embedding sharding) by their communication patterns and latency-memory tradeoffs

- Analyze load balancing algorithms using queuing theory to explain why power-of-two-choices achieves exponentially better performance than random assignment

- Evaluate KV cache management techniques (PagedAttention, prefix caching, speculative decoding) for optimizing LLM serving throughput and memory efficiency

- Design autoscaling policies that account for GPU cold start latency and balance cost optimization with SLO guarantees

:::

## Scaling Inference Beyond Single Machines {#sec-inference-scaling-beyond}

Single-machine model serving establishes foundational principles that this chapter extends to distributed systems: the inversion from training's throughput focus to serving's latency imperative, queuing theory fundamentals that govern system behavior under load, and the latency budget framework that reveals where time actually goes within a request. Those foundations assume a single server handling requests, which suffices for many deployments. This chapter addresses what happens when single-machine serving proves insufficient and examines the engineering principles that govern inference at scale across multiple machines.

The transition from single-machine to distributed inference parallels the transition from single-machine to distributed training, but with fundamentally different constraints. Training optimizes throughput over extended periods and tolerates latency variations measured in minutes or hours. Inference at scale must maintain strict latency bounds measured in milliseconds while handling request volumes that fluctuate unpredictably. This inversion of priorities transforms every design decision, from how requests are batched to how failures are handled.

Distributed inference systems must solve problems that simply do not exist at single-machine scale. Load balancing[^fn-load-balancing] becomes critical when requests must be distributed across hundreds of GPU instances while maintaining latency guarantees. Request routing must account for model-specific characteristics. A recommendation system with trillion-parameter embedding tables requires different placement strategies than a large language model that generates responses token by token. Autoscaling must anticipate demand fluctuations that can change request volume by orders of magnitude within minutes while maintaining latency bounds that users expect.

[^fn-load-balancing]: **Load balancing** in inference systems differs fundamentally from traditional web load balancing. Web servers typically complete requests in milliseconds with uniform processing time; GPU inference workers handle requests lasting from 10ms to 30+ seconds with high variance. This variance makes techniques like least-connections and queue-depth-aware routing essential, whereas simple round-robin suffices for stateless web workloads.

The economics of inference at scale also differ fundamentally from training economics. Training costs are dominated by compute time and can be amortized over the lifetime of the resulting model. Inference costs, by contrast, are directly tied to user traffic and revenue. An e-commerce recommendation system might serve millions of requests per second during peak shopping periods, with each request contributing directly to potential revenue. The cost of overprovisioning during quiet periods or underprovisioning during peaks translates immediately to business impact, making inference efficiency a first-order concern in ways that training efficiency rarely achieves.

This chapter develops the principles and techniques for building inference systems that scale to meet these demands. We examine when distributed inference becomes necessary, how to architect systems that maintain latency bounds under varying load, and how to optimize the economics of inference through efficient resource utilization. The goal is not merely to make inference work at scale, but to make it work efficiently, reliably, and economically.

### When Single-Machine Serving Is Insufficient {#sec-inference-when-insufficient}

Three distinct signals indicate when distributed inference becomes necessary rather than merely optional (@tbl-distribution-triggers).

**Memory exhaustion** occurs when model parameters, key-value caches, or embedding tables exceed single-device capacity. A single NVIDIA H100 GPU provides 80GB of HBM3[^fn-hbm3] memory. GPT-4 class models with hundreds of billions of parameters require 200-400GB just for weights in FP16 precision, forcing distribution across multiple GPUs regardless of throughput requirements. Recommendation systems with trillion-parameter embedding tables face similar constraints: Meta's DLRM[^fn-dlrm] model stores embedding tables that require multiple terabytes of memory.

[^fn-hbm3]: **High Bandwidth Memory 3 (HBM3)**: A 3D-stacked DRAM technology providing 3.35 TB/s bandwidth on the H100, compared to 1 TB/s for HBM2e on the A100. This 3x bandwidth improvement directly enables larger batch sizes and faster token generation for memory-bandwidth-bound LLM decode operations.

[^fn-dlrm]: **Deep Learning Recommendation Model (DLRM)**: Meta's reference architecture (2019) for large-scale recommendation, processing hundreds of billions of inferences daily. DLRM separates dense features (MLP processing) from sparse features (embedding lookups), enabling hybrid CPU-GPU architectures. Embedding tables can exceed 100TB, requiring distributed storage and caching strategies for serving.

**Throughput limitations** emerge when request volume exceeds single-machine capacity even with optimal batching. Consider a recommendation system serving 100,000 queries per second with a 10ms latency budget. If single-machine throughput peaks at 10,000 QPS, no amount of optimization on that machine can satisfy demand. Horizontal scaling across multiple replicas becomes mandatory.

**Latency requirements** drive distribution when model execution time exceeds latency budgets even at batch size one. Large language models generating responses token by token face this constraint acutely: a 70-billion parameter model requires approximately 140GB of memory and achieves roughly 30 tokens per second on a single GPU due to memory bandwidth limitations. Sharding the model across multiple GPUs enables parallel computation that reduces time-to-first-token below acceptable thresholds.

+----------------+--------------------------+----------------------+-----------------------------+
| **Constraint** | **Single-Machine Limit** | **Example Workload** | **Distribution Strategy**   |
+:===============+:=========================+=====================:+:============================+
| **Memory**     | 80GB (H100)              | GPT-4 (400GB+)       | Tensor/pipeline parallelism |
| **Throughput** | ~10K QPS (vision)        | 100K QPS RecSys      | Horizontal replication      |
| **Latency**    | Model execution time     | 500ms LLM TTFT       | Model sharding              |
+----------------+--------------------------+----------------------+-----------------------------+

: **Triggers for Distributed Inference**: Each constraint type indicates different distribution strategies. Memory constraints require model sharding; throughput constraints require replication; latency constraints may require either depending on whether the bottleneck is compute or memory bandwidth. {#tbl-distribution-triggers}

### The Fundamental Inversion: Training vs Inference {#sec-inference-inversion}

The contrast between training and inference optimization extends beyond the basic throughput versus latency distinction. Training optimizes for samples processed per hour and tolerates latency variations. Inference optimizes for response time and must meet strict latency bounds. At scale, this inversion manifests in system architecture, resource allocation, and operational priorities (@tbl-training-inference-inversion).

+-------------------------+-------------------------------+------------------------------+
| **Aspect**              | **Distributed Training**      | **Distributed Inference**    |
+:========================+:==============================+:=============================+
| **Primary metric**      | Throughput (samples/hour)     | Latency (P99 ms)             |
| **Acceptable variance** | Hours                         | Milliseconds                 |
| **State management**    | Checkpoints (periodic)        | Session state (continuous)   |
| **Batch formation**     | Large, controlled             | Request-driven, variable     |
| **Failure tolerance**   | Restart from checkpoint       | Redirect without user impact |
| **Cost structure**      | Fixed duration, variable rate | Variable duration, fixed SLO |
+-------------------------+-------------------------------+------------------------------+

: **Training vs Inference System Requirements**: The fundamental inversion from throughput to latency optimization ripples through every aspect of system design. {#tbl-training-inference-inversion}

Training tolerates substantial latency variance because the optimization target is aggregate progress over hours or days. A training iteration that takes 2 seconds instead of the usual 1 second represents acceptable variation. An inference request that takes 2 seconds instead of 100 milliseconds represents catastrophic failure, potentially causing user abandonment or cascading timeouts in dependent services.

State management differs fundamentally. Training maintains model state (parameters, optimizer states) that evolves gradually and can be captured in periodic checkpoints. Inference often maintains session state (conversation history, key-value caches, user context) that must be preserved across requests and cannot tolerate the staleness that checkpoint-based recovery would introduce.

Failure handling diverges correspondingly. Training failures trigger checkpoint restoration and continuation, with minutes of lost progress being acceptable. Inference failures must be invisible to users. Requests redirect to healthy replicas, degraded results substitute for unavailable models, and SLOs must be maintained despite infrastructure instability.

### The Serving Tax: Overhead of Distribution {#sec-inference-serving-tax}

Distributing inference across multiple machines introduces overhead absent from single-machine serving. This "serving tax" must be understood and budgeted within latency constraints.

**Network communication** adds latency for every cross-machine interaction. Within a datacenter, network round-trip times range from 50-500 microseconds depending on topology and congestion. For model sharding that requires synchronization between GPUs on different machines, each synchronization point adds this overhead. A model sharded across 8 machines with 4 synchronization points per inference adds 200 microseconds to 2 milliseconds of network latency.

**Serialization overhead** converts in-memory tensors to network-transmittable formats. While modern serialization libraries like FlatBuffers and Cap'n Proto[^fn-serialization] minimize this overhead, large activation tensors still require meaningful time to serialize and deserialize. A 1GB activation tensor takes approximately 100 milliseconds to serialize, even with optimized libraries.

[^fn-serialization]: **Zero-copy serialization**: FlatBuffers (Google) and Cap'n Proto enable reading serialized data directly without parsing or unpacking, eliminating the CPU overhead of traditional formats like Protocol Buffers or JSON. For distributed inference, this reduces per-request serialization overhead from milliseconds to microseconds.

**Load balancer latency** adds another layer. Requests must be routed to appropriate replicas, which requires examining request metadata, consulting routing tables, and forwarding to selected backends. Well-optimized load balancers add 100-500 microseconds; poorly configured ones can add milliseconds.

**Coordination overhead** emerges when requests require fan-out to multiple services. A recommendation system that queries a user model, item model, and ranking model in parallel must coordinate these queries and aggregate results. The coordination logic itself consumes CPU cycles and introduces latency variation.

The total serving tax often consumes 10-30% of the latency budget in distributed systems (@eq-serving-tax):

$$L_{total} = L_{compute} + L_{network} + L_{serialization} + L_{coordination} + L_{queuing}$$ {#eq-serving-tax}

Minimizing this tax requires co-locating communicating components, using high-bandwidth interconnects, and designing communication patterns that minimize round trips.

### Serving Cost Dominates Training Cost {#sec-inference-cost-dominance}

A critical insight for infrastructure planning is that serving cost typically dominates training cost over a model's operational lifetime. This reversal from the training-centric view of model development has profound implications for where optimization effort should focus.

The total cost of operating a model comprises training cost (a one-time expense) and serving cost (an ongoing expense) (@eq-total-cost):

$$C_{total} = C_{training} + C_{serving} \times T_{deployment} \times Q_{rate}$$ {#eq-total-cost}

where $C_{training}$ is the one-time cost to train the model, $C_{serving}$ is the cost per query served, $T_{deployment}$ is the deployment duration in appropriate time units, and $Q_{rate}$ is the query rate.

::: {.callout-note title="Worked Example: Cost Dominance Analysis"}

Consider a recommendation model with the following characteristics:

**Training costs**:

- 1,000 GPU-hours on H100 GPUs at $3/GPU-hour = $3,000
- Data preparation and experimentation overhead (3x): $9,000
- **Total training cost**: $12,000

**Serving costs**:

- Deployment duration: 2 years
- Query rate: 10,000 QPS average
- Cost per query: $0.00001 (on optimized infrastructure)

**Total queries over lifetime**:
$$Q_{total} = 10,000 \text{ QPS} \times 86,400 \text{ s/day} \times 730 \text{ days} = 631 \text{ billion queries}$$

**Total serving cost**:
$$C_{serving} = 631 \times 10^9 \times \$0.00001 = \$6,310,000$$

**Ratio**: Serving cost is **526x** the training cost.

Even reducing serving cost by 10% saves $631,000, far exceeding the entire training budget. This analysis explains why production ML teams often dedicate more engineering resources to serving optimization than training optimization.

:::

The cost dominance ratio varies by application (@tbl-cost-ratios):

+-------------------------------+-------------------+-------------------------+-----------+
| **Application**               | **Training Cost** | **Annual Serving Cost** | **Ratio** |
+:==============================+==================:+========================:+==========:+
| **Recommendation (high QPS)** | $10K-100K         | $1M-10M                 | 100-1000x |
| **Search ranking**            | $100K-1M          | $10M-100M               | 100-1000x |
| **LLM API**                   | $1M-100M          | $10M-1B                 | 10-100x   |
| **Internal analytics**        | $1K-10K           | $10K-100K               | 10-100x   |
+-------------------------------+-------------------+-------------------------+-----------+

: **Training vs Serving Cost Ratios**: High-QPS applications like recommendation systems show the most extreme cost dominance of serving over training. {#tbl-cost-ratios}

This cost structure motivates the optimization techniques throughout this chapter. Every percentage point of serving efficiency improvement yields ongoing cost reduction over the model's operational lifetime.

### The Inference Landscape: Beyond LLMs {#sec-inference-landscape}

A critical misconception in current discourse frames inference at scale as synonymous with LLM serving. While large language models present distinctive challenges and attract significant attention, they represent a small fraction of production inference volume. The full inference landscape (@tbl-inference-landscape) is essential for appropriate technique selection.

::: {.callout-important title="Production Inference by Request Volume"}

By request count, production ML inference at major technology companies breaks down approximately as:

- **Recommendation and ranking**: 80-90% of requests
- **Vision and image processing**: 5-10% of requests
- **NLP/LLM**: 1-5% of requests (but growing rapidly)
- **Other (fraud detection, ads, etc.)**: 2-5% of requests

Source: Industry reports from Meta, Google, and Netflix infrastructure teams.

:::

Recommendation systems dominate because they serve predictions for every user interaction: every page load, scroll, or click triggers inference. A user browsing an e-commerce site might generate 100 recommendation requests in a single session. In contrast, LLM queries typically require explicit user action and occur less frequently.

This distribution has important implications. Recommendation systems have driven most production inference innovation. Dynamic batching, embedding sharding, feature store architectures, and low-latency serving were developed primarily for recommendation workloads. LLM-specific techniques like continuous batching and KV cache management are important but address a narrower slice of production inference.

+--------------------+--------------------+--------------------+-------------------+
| **Model Type**     | **Request Volume** | **Latency Target** | **Key Challenge** |
+:===================+:===================+:===================+:==================+
| **Recommendation** | Very high (80-90%) | &lt;10ms P99       | Embedding lookup  |
| **Vision (CNN)**   | Moderate (5-10%)   | 20-100ms           | Batch efficiency  |
| **LLM**            | Lower (1-5%)       | 100ms-10s          | Memory bandwidth  |
| **Speech/Audio**   | Lower              | Real-time          | Sequential decode |
| **Multimodal**     | Growing            | Varies             | Cross-modal sync  |
+--------------------+--------------------+--------------------+-------------------+

: **Production Inference Landscape**: Different model types have different volume, latency requirements, and optimization challenges. Technique selection must match the specific workload. {#tbl-inference-landscape}

### The Serving Hierarchy {#sec-inference-serving-hierarchy}

To organize the optimization techniques in this chapter, we introduce the serving hierarchy as a conceptual framework. Like the memory hierarchy in computer architecture, the serving hierarchy identifies distinct levels at which optimization occurs, each with different targets and techniques.

**Request level**: Optimizations that affect individual request processing. Batching strategies, caching, and preprocessing optimizations operate at this level. The target metric is per-request latency.

**Replica level**: Optimizations within a single model instance. GPU utilization, memory management, and model optimization operate here. The target metric is single-replica throughput.

**Service level**: Optimizations across multiple replicas of the same model. Load balancing, request routing, and replica management operate at this level. The target metric is aggregate service throughput while meeting latency SLOs.

**Platform level**: Optimizations across multiple services and tenants. Resource allocation, multi-tenancy, scheduling, and cluster management operate here. The target metric is overall resource efficiency while meeting diverse SLO requirements.

::: {.callout-note title="Figure Placeholder: The Serving Hierarchy" collapse="true"}
```{.tikz}
% TODO: Pyramid diagram showing the four levels of the serving hierarchy
\node[draw, align=center] {Platform Level (Multi-tenancy)\\Service Level (Load Balancing)\\Replica Level (GPU Utilization)\\Request Level (Batching)};
```
**The Serving Hierarchy**. Optimization occurs at four distinct levels, each with different objectives and techniques. The Request Level focuses on minimizing per-request latency via batching and caching. The Replica Level maximizes single-instance throughput through kernel optimization and memory management. The Service Level manages distribution across multiple replicas to meet aggregate demand. The Platform Level handles efficient resource sharing across multiple services and tenants.
:::

Each level has distinct optimization levers (@tbl-serving-hierarchy):

+--------------+-------------------------+----------------------------------------+
| **Level**    | **Optimization Target** | **Key Techniques**                     |
+:=============+:========================+:=======================================+
| **Request**  | Per-request latency     | Dynamic batching, caching, prefetching |
| **Replica**  | Throughput, utilization | Memory optimization, kernel fusion     |
| **Service**  | Aggregate capacity      | Load balancing, routing, autoscaling   |
| **Platform** | Resource efficiency     | Multi-tenancy, scheduling, placement   |
+--------------+-------------------------+----------------------------------------+

: **Serving Hierarchy Optimization Targets**: Each level of the hierarchy addresses different metrics with different techniques. {#tbl-serving-hierarchy}

The remainder of this chapter progresses through these levels: batching and caching (request level), model sharding (replica level), load balancing and autoscaling (service level), and multi-tenancy (platform level).

### Chapter Roadmap {#sec-inference-roadmap}

This chapter develops the techniques for inference at scale through the lens of the serving hierarchy:

**Batching Strategies at Scale** (@sec-inference-batching) examines how different model types require fundamentally different batching approaches. We contrast static batching for vision models, continuous batching for LLMs, and feature-parallel batching for recommendation systems, providing quantitative analysis of throughput-latency tradeoffs.

**Model Sharding for Inference** (@sec-inference-sharding) addresses when and how to distribute model computation across multiple devices. We examine tensor parallelism, pipeline parallelism, expert parallelism, and embedding sharding, with emphasis on communication patterns and overhead.

**Load Balancing and Request Routing** (@sec-inference-load-balancing) develops the theory and practice of distributing requests across replicas. We derive why power-of-two-choices achieves exponentially better load distribution than random assignment and examine routing strategies for stateful workloads.

**KV Cache Management** (@sec-inference-kv-cache) focuses on the memory management challenges specific to autoregressive language models, including PagedAttention, prefix caching, and speculative decoding.

**Multi-Tenancy and Isolation** (@sec-inference-multitenancy) examines platform-level concerns: sharing infrastructure across multiple models and users while maintaining isolation and fairness.

**Autoscaling** (@sec-inference-autoscaling) addresses dynamic capacity management, including the cold start problem unique to GPU-based serving and predictive scaling strategies.

**Case Studies** (@sec-inference-case-studies) grounds these principles in production systems at Meta, OpenAI, Google, and TikTok, demonstrating how the techniques combine in real deployments.

Throughout, we maintain the model-type diversity essential for practitioners: every major concept is illustrated across LLMs, recommendation systems, vision models, and other production workloads.

## Serving Framework Selection {#sec-inference-frameworks}

The batching, sharding, and load balancing techniques described in the chapter roadmap do not exist in isolation; they are implemented within serving frameworks that constrain and enable different optimizations. Before examining these techniques in depth, practitioners face an immediate practical decision: which serving infrastructure to build upon?

The choice of serving framework determines which optimizations are available, how models are deployed, and what performance characteristics are achievable. Understanding this landscape first provides essential context: when we discuss continuous batching in @sec-inference-continuous-batching, we will see how vLLM and TensorRT-LLM implement it differently. This section provides a systematic framework for selecting among the major options.

### Framework Categories {#sec-inference-framework-categories}

Serving frameworks fall into distinct categories based on their design philosophy and target workloads:

**General-purpose inference servers** provide broad model support with configurable optimization. Triton Inference Server (NVIDIA) offers multi-framework support (PyTorch, TensorFlow, ONNX, TensorRT), dynamic batching, model ensemble orchestration, and concurrent model execution. TensorFlow Serving provides native TensorFlow support, gRPC/REST APIs, model versioning, and batching scheduler. TorchServe (PyTorch) delivers native PyTorch support, model archiving, metrics, and multi-model serving.

**LLM-specialized servers** optimize specifically for autoregressive generation[^fn-autoregressive]. vLLM provides PagedAttention, continuous batching, tensor parallelism, and OpenAI-compatible API. TensorRT-LLM delivers NVIDIA-optimized kernels, in-flight batching, quantization, and multi-GPU support. Text Generation Inference (TGI) offers Hugging Face integration, flash attention, tensor parallelism, and watermarking.

[^fn-autoregressive]: **Autoregressive generation**: The process of generating output tokens one at a time, where each new token depends on all previously generated tokens. This sequential dependency creates the memory-bandwidth bottleneck that dominates LLM serving costs, as the entire model must be read from memory for each generated token.

**Optimization-focused runtimes** maximize inference speed through compilation. TensorRT provides graph optimization, kernel fusion, precision calibration, and NVIDIA GPU specific optimizations. ONNX Runtime offers cross-platform optimization and execution providers for different hardware. OpenVINO delivers Intel hardware optimization, model compression, and heterogeneous execution.

### Framework Selection Criteria {#sec-inference-framework-criteria}

Selection depends on model type, deployment constraints, and organizational factors:

**Model architecture determines primary candidates**:

+-------------------------+---------------------------+------------------------------------------+
| **Model Type**          | **Primary Options**       | **Key Consideration**                    |
+:========================+:==========================+:=========================================+
| **LLM (&gt;7B params)** | vLLM, TensorRT-LLM, TGI   | KV cache management, continuous batching |
| **LLM (&lt;7B params)** | vLLM, TGI, Triton         | Simpler deployment, less memory pressure |
| **Vision (CNN/ViT)**    | Triton, TensorRT, ONNX RT | Static batching, throughput optimization |
| **Recommendation**      | Triton, custom            | Feature preprocessing, embedding lookup  |
| **Multi-modal**         | Triton, custom            | Pipeline orchestration                   |
+-------------------------+---------------------------+------------------------------------------+

**Hardware constraints narrow options**:

+---------------------------+----------------------------------+
| **Hardware**              | **Supported Frameworks**         |
+:==========================+:=================================+
| **NVIDIA datacenter GPU** | All options                      |
| **NVIDIA consumer GPU**   | vLLM, TGI (limited TensorRT-LLM) |
| **AMD GPU**               | vLLM (ROCm), ONNX RT             |
| **Intel CPU/GPU**         | OpenVINO, ONNX RT                |
| **Apple Silicon**         | MLX, Core ML, ONNX RT            |
| **AWS Inferentia**        | Neuron SDK                       |
+---------------------------+----------------------------------+

**Operational requirements influence choice**:

- **Multi-model serving**: Triton excels with concurrent model execution
- **Rapid iteration**: TorchServe, TGI offer simpler deployment cycles
- **Maximum throughput**: TensorRT-LLM, vLLM with optimized kernels
- **Cross-platform**: ONNX Runtime provides broadest hardware support

### vLLM Architecture {#sec-inference-vllm}

vLLM[^fn-vllm-name] [@kwon2023vllm] has emerged as the leading open-source LLM serving framework due to its PagedAttention innovation. Understanding its architecture illustrates key LLM serving principles.

[^fn-vllm-name]: **vLLM**: The name stands for "virtual LLM," reflecting its core innovation of applying virtual memory concepts to KV cache management. Developed at UC Berkeley and released in 2023, vLLM achieved 24x throughput improvement over HuggingFace Transformers in initial benchmarks.

The core innovations in vLLM include PagedAttention (virtual memory for KV cache, covered in @sec-inference-paged-attention), continuous batching (add/remove requests mid-generation), optimized attention kernels (FlashAttention [@dao2022flashattention] integration), and tensor parallelism (automatic model sharding across GPUs).

**Architecture overview**:

```
┌─────────────────────────────────────────────────────┐
│                   vLLM Engine                        │
├─────────────────────────────────────────────────────┤
│  Scheduler          │  Block Manager                │
│  - Request queue    │  - Physical blocks            │
│  - Preemption       │  - Block tables               │
│  - Priority         │  - Copy-on-write              │
├─────────────────────┼───────────────────────────────┤
│  Model Executor     │  Cache Engine                 │
│  - Attention        │  - GPU cache                  │
│  - Sampling         │  - CPU swap space             │
│  - Tensor parallel  │  - Prefix caching             │
└─────────────────────┴───────────────────────────────┘
```

**Deployment example**:

```python
from vllm import LLM, SamplingParams

# Initialize with automatic GPU detection
llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    tensor_parallel_size=4,  # Shard across 4 GPUs
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)

# Efficient batch inference
prompts = ["Explain quantum computing", "Write a poem about AI"]
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)
outputs = llm.generate(prompts, sampling_params)
```

**Performance characteristics**:

+-----------------------------+-------------------+-----------------------------------------+
| **Metric**                  | **Typical Value** | **Notes**                               |
+:============================+==================:+:========================================+
| **Throughput vs baseline**  | 2-4x              | Compared to naive HF generation         |
| **Memory efficiency**       | 90%+ utilization  | PagedAttention eliminates fragmentation |
| **Latency overhead**        | &lt;5ms           | Scheduling and batching overhead        |
| **Max concurrent requests** | 100s-1000s        | Depends on model size and GPU memory    |
+-----------------------------+-------------------+-----------------------------------------+

### TensorRT-LLM Architecture {#sec-inference-tensorrt-llm}

TensorRT-LLM provides NVIDIA-optimized LLM inference with deep hardware integration.

TensorRT-LLM provides several core capabilities. Optimized kernels include custom CUDA kernels for attention, GEMM, and layer norms. In-flight batching implements NVIDIA's continuous batching. Quantization supports INT8, INT4, FP8 with minimal accuracy loss. Multi-GPU configurations enable tensor and pipeline parallelism with NVLink optimization.

**Build and deployment workflow**:

```bash
# Step 1: Convert model to TensorRT-LLM format
python convert_checkpoint.py \
    --model_dir /models/llama-70b \
    --output_dir /models/llama-70b-trt \
    --dtype float16 \
    --tp_size 4

# Step 2: Build optimized engine
trtllm-build \
    --checkpoint_dir /models/llama-70b-trt \
    --output_dir /engines/llama-70b \
    --gemm_plugin float16 \
    --max_batch_size 64 \
    --max_input_len 2048 \
    --max_output_len 512

# Step 3: Deploy with Triton
# (Configuration in model_repository/)
```

**Performance comparison with vLLM**:

+---------------------------+------------------+----------+--------------+
| **Scenario**              | **TensorRT-LLM** | **vLLM** | **Winner**   |
+:==========================+:=================+:=========+:=============+
| **A100 throughput**       | Higher           | Good     | TensorRT-LLM |
| **H100 throughput**       | Highest          | High     | TensorRT-LLM |
| **Deployment simplicity** | Complex          | Simple   | vLLM         |
| **Model support**         | NVIDIA curated   | Broad HF | vLLM         |
| **Quantization options**  | Extensive        | Good     | TensorRT-LLM |
+---------------------------+------------------+----------+--------------+

TensorRT-LLM typically achieves 20-50% higher throughput than vLLM on NVIDIA hardware but requires more complex deployment pipelines.

### Triton Inference Server {#sec-inference-triton}

Triton provides enterprise-grade multi-model serving with sophisticated orchestration capabilities.

Triton provides several key features for production. Multi-framework support allows a single server to host PyTorch, TensorFlow, TensorRT, and ONNX models. Dynamic batching offers configurable batching with latency targets. Model ensembles enable chaining models in inference pipelines. Concurrent execution allows multiple models to share GPU resources. Metrics and monitoring integrate with Prometheus and provide detailed latency breakdown.

**Model repository structure**:

```
model_repository/
├── text_encoder/
│   ├── config.pbtxt
│   └── 1/
│       └── model.onnx
├── image_classifier/
│   ├── config.pbtxt
│   └── 1/
│       └── model.plan  # TensorRT engine
└── ensemble_pipeline/
    ├── config.pbtxt    # Orchestrates above models
    └── 1/
```

**Dynamic batching configuration**:

```protobuf
# config.pbtxt
dynamic_batching {
    preferred_batch_size: [4, 8, 16, 32]
    max_queue_delay_microseconds: 100000  # 100ms max wait
}
instance_group [
    {
        count: 2
        kind: KIND_GPU
        gpus: [0, 1]
    }
]
```

**Use cases where Triton excels**:

- Multi-model pipelines (e.g., detection → classification → ranking)
- Mixed workloads on shared GPU clusters
- Organizations with diverse model frameworks
- Production systems requiring detailed observability

### Framework Selection Decision Tree {#sec-inference-framework-decision}

```
Start
  │
  ├─ Is this an LLM (autoregressive generation)?
  │   ├─ Yes → Is maximum throughput critical?
  │   │         ├─ Yes, NVIDIA hardware → TensorRT-LLM
  │   │         └─ No, or mixed hardware → vLLM
  │   │
  │   └─ No → Is this multi-model serving?
  │           ├─ Yes → Triton Inference Server
  │           └─ No → What's the deployment target?
  │                   ├─ NVIDIA GPU → TensorRT + Triton
  │                   ├─ Intel → OpenVINO
  │                   ├─ Cross-platform → ONNX Runtime
  │                   └─ Edge/Mobile → Platform-specific (Core ML, TFLite)
```

**Common deployment patterns**:

+-------------------------+---------------------------+-----------------------------------------+
| **Pattern**             | **Frameworks**            | **Use Case**                            |
+:========================+:==========================+:========================================+
| **LLM API service**     | vLLM + nginx              | ChatGPT-like applications               |
| **High-throughput LLM** | TensorRT-LLM + Triton     | Batch processing, enterprise            |
| **Vision pipeline**     | TensorRT + Triton         | Object detection, classification        |
| **Recommendation**      | Triton + custom embedding | E-commerce, content platforms           |
| **Multi-modal**         | Triton ensemble           | Vision-language, document understanding |
+-------------------------+---------------------------+-----------------------------------------+

### Framework Performance Benchmarking {#sec-inference-framework-benchmarks}

When evaluating frameworks, benchmark on representative workloads:

**LLM benchmark methodology**:

```python
# Standard benchmark parameters
benchmark_config = {
    "input_lengths": [128, 512, 2048],
    "output_lengths": [64, 256, 512],
    "batch_sizes": [1, 8, 32, 64],
    "concurrent_requests": [1, 10, 50, 100],
    "metrics": ["ttft", "tpot", "throughput", "gpu_util"],
}

# Time to First Token (TTFT): Latency until first token generated
# Time Per Output Token (TPOT): Average latency per subsequent token
# Throughput: Total tokens/second across all requests
# GPU utilization: Compute and memory utilization
```

**Representative benchmark results** (Llama-2-70B on 4xA100-80GB):

+---------------------+---------------+---------------+------------------------+
| **Framework**       | **TTFT (ms)** | **TPOT (ms)** | **Throughput (tok/s)** |
+:====================+==============:+==============:+=======================:+
| **TensorRT-LLM**    | 180           | 28            | 2,400                  |
| **vLLM**            | 220           | 32            | 1,900                  |
| **TGI**             | 250           | 35            | 1,700                  |
| **HF Transformers** | 400           | 85            | 600                    |
+---------------------+---------------+---------------+------------------------+

Note: Results vary significantly with configuration, input/output lengths, and batch sizes. Always benchmark on your specific workload.

### Migration and Integration Considerations {#sec-inference-framework-migration}

**Migrating between frameworks**:

- **Model compatibility**: Most frameworks support standard formats (HF, ONNX)
- **API differences**: vLLM uses OpenAI-compatible API; Triton uses gRPC/HTTP
- **Configuration translation**: Batching, parallelism settings differ by framework

**Integration with ML infrastructure**:

+--------------------+------------------------------------------------+
| **Component**      | **Integration Pattern**                        |
+:===================+:===============================================+
| **Model registry** | Pull models on startup, version management     |
| **Feature store**  | Triton ensemble preprocessing, custom backends |
| **Monitoring**     | Prometheus metrics, distributed tracing        |
| **Load balancer**  | Health checks, request routing                 |
| **Autoscaler**     | Custom metrics (queue depth, GPU utilization)  |
+--------------------+------------------------------------------------+

The framework selection made here influences all subsequent serving optimizations. The techniques in following sections (batching, sharding, caching) are implemented differently across frameworks but follow the same underlying principles.

### Orchestration Platforms for Production Serving {#sec-inference-orchestration}

Individual inference runtimes (vLLM, TensorRT-LLM, Triton) handle the mechanics of efficient inference on a single node or small cluster. Production systems require an orchestration layer that manages service composition, autoscaling, traffic management, fault tolerance, and multi-tenancy. Choosing an inference runtime without considering the orchestration layer is like choosing a database engine without considering connection pooling, replication, and query routing.

The orchestration layer handles several key responsibilities. Service composition combines multiple models and components into request pipelines. Autoscaling adjusts replicas based on traffic patterns and resource utilization. Traffic management encompasses load balancing, canary deployments, and A/B testing. Fault tolerance provides replica health monitoring and automatic recovery. Multi-tenancy isolates workloads and manages resource allocation across teams.

**Major orchestration platforms**:

+------------------------------------+--------------------------------------------------------+----------------------------------+
| **Platform**                       | **Key Capability**                                     | **Production Users**             |
+:===================================+:=======================================================+:=================================+
| **Ray Serve** [@moritz2018ray]     | Scalable Python-native serving, composable deployments | OpenAI, Uber, Instacart          |
| **KServe**                         | Kubernetes-native serving, serverless inference        | Bloomberg, Zillow, enterprises   |
| **BentoML**                        | ML model packaging and unified serving API             | Various production deployments   |
| **Seldon Core**                    | Kubernetes deployment, A/B testing, canary releases    | Financial services, retail       |
+------------------------------------+--------------------------------------------------------+----------------------------------+

**Ray Serve architecture**:

```
┌─────────────────────────────────────────────────────────────┐
│                    Ray Serve Controller                      │
├─────────────────────────────────────────────────────────────┤
│  HTTP Proxy         │  Autoscaler           │  Router        │
│  - Request routing  │  - Replica management │  - Load balance│
│  - Request batching │  - Scale up/down      │  - Affinity    │
├─────────────────────┼───────────────────────┴────────────────┤
│                    Ray Actor Pool                            │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐        │
│  │ Replica 1│ │ Replica 2│ │ Replica 3│ │ Replica N│        │
│  │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │        │
│  └──────────┘ └──────────┘ └──────────┘ └──────────┘        │
└─────────────────────────────────────────────────────────────┘
```

**Production-ready vLLM deployment with Ray Serve**:

```python
from ray import serve
from vllm import LLM, SamplingParams


@serve.deployment(
    num_replicas=4,
    ray_actor_options={"num_gpus": 4},
    autoscaling_config={
        "min_replicas": 2,
        "max_replicas": 16,
        "target_num_ongoing_requests_per_replica": 10,
    },
)
class LLMDeployment:
    def __init__(self):
        self.llm = LLM(
            model="meta-llama/Llama-2-70b-hf", tensor_parallel_size=4
        )
        self.params = SamplingParams(temperature=0.7, max_tokens=256)

    async def __call__(self, request):
        return self.llm.generate([request.prompt], self.params)[0]


# Deploy with automatic scaling
app = LLMDeployment.bind()
serve.run(app)
```

This pattern provides automatic scaling from 2 to 16 replicas based on load. It includes request batching at the serve layer, fault tolerance with automatic replica restart, and zero-downtime deployments.

**Stateless vs stateful serving**: The orchestration layer must account for whether inference is stateless or stateful.

+------------------+-----------------------+---------------------+---------------------------+
| **Serving Type** | **State Location**    | **Scaling Model**   | **Failure Recovery**      |
+:=================+:======================+:====================+:==========================+
| **Stateless**    | None or external      | Horizontal, trivial | Redirect to any replica   |
| **Stateful**     | In-process (KV cache) | Complex, sticky     | Session loss or migration |
+------------------+-----------------------+---------------------+---------------------------+

Vision models and embedding lookups are typically stateless: any replica can serve any request. LLM serving with KV cache is stateful: the cache accumulated during conversation creates replica-specific state.

Stateful LLM serving has several implications. Sticky routing is required because subsequent requests in a conversation must reach the same replica holding the KV cache. Failure recovery becomes complex because when a stateful replica fails, cached state is lost. Options include regenerating cache from history (high latency), replicating cache to backups (high bandwidth), or accepting session restart (poor experience). Autoscaling must account for sessions because scaling down stateful replicas requires draining active sessions, which can take minutes for long conversations. Memory sizing determines capacity because each active session consumes KV cache memory, limiting concurrent sessions per replica regardless of compute capacity.

**Consistency during model updates**:

When updating models across distributed replicas, requests may hit different model versions during the rollout. Different deployment strategies provide different consistency guarantees:

+-------------------------+--------------------------+-------------------+-----------------------+
| **Deployment Strategy** | **Consistency**          | **Rollout Speed** | **Risk**              |
+:========================+:=========================+:==================+:======================+
| **Blue-green**          | Strong (atomic switch)   | Instant           | High (all-or-nothing) |
| **Canary**              | Eventual (gradual shift) | Slow (hours)      | Low (progressive)     |
| **Rolling update**      | Weak (mixed versions)    | Medium            | Medium                |
+-------------------------+--------------------------+-------------------+-----------------------+

Applications requiring deterministic outputs (compliance, audit trails, reproducible debugging) should implement several practices. Pin model version by including version identifier in request routing to ensure same model handles related requests. Use temperature=0 to eliminate sampling variance, though beam search still has implementation-dependent tiebreaking. Implement version-aware caching by caching responses with model version tags and invalidating on version change.

**Failure scenario: Stateful replica crash**

When a stateful LLM replica crashes mid-conversation:

```
1. Load balancer detects health check failure (1-10 seconds)
2. New requests route to healthy replicas
3. In-flight requests fail; clients must retry
4. Session state (KV cache) is lost
5. Recovery options:
   a. Regenerate: Client resends conversation history (high latency)
   b. Redirect: Route to replica with replicated state (if available)
   c. Restart: Begin new session (poor user experience)
```

Production systems often accept option (a) with optimizations: the regenerated prefill can process the full conversation history in a single batch, taking seconds rather than the minutes the original conversation took.

**Build vs buy: Managed serving services**:

Before selecting frameworks, teams must decide whether to self-host or use managed services:

+-------------------------+--------------+--------------------------------------------------+--------------------------------------+
| **Service**             | **Provider** | **Key Features**                                 | **Trade-offs**                       |
+:========================+:=============+:=================================================+:=====================================+
| **SageMaker Endpoints** | AWS          | Managed hosting, autoscaling, A/B testing        | Lock-in, cost, limited customization |
| **Vertex AI Endpoints** | GCP          | TPU support, traffic splitting, model monitoring | GCP ecosystem dependency             |
| **Azure ML Endpoints**  | Azure        | Enterprise integration, ONNX optimization        | Azure ecosystem dependency           |
| **Anyscale Endpoints**  | Anyscale     | Ray-native, fine-grained autoscaling             | Emerging platform                    |
+-------------------------+--------------+--------------------------------------------------+--------------------------------------+

+--------------------------------+--------------------------------------------+-----------------------------------------------+
| **Approach**                   | **Advantages**                             | **Disadvantages**                             |
+:===============================+:===========================================+:==============================================+
| **Self-hosted (vLLM/Triton)**  | Full control, cost optimization at scale   | Operational burden, expertise required        |
| **Managed (SageMaker/Vertex)** | Operational simplicity, integrated tooling | Lock-in, cost at scale, limited customization |
| **Hybrid (Ray Serve + cloud)** | Flexibility, gradual migration             | Complexity in managing both                   |
+--------------------------------+--------------------------------------------+-----------------------------------------------+

**Decision factors**:

- **Team size**: Teams with fewer than 5 ML engineers often benefit from managed services
- **Scale**: More than 1M daily requests typically makes self-hosting cost-effective
- **Customization needs**: Novel architectures require self-hosting
- **Latency requirements**: Self-hosting enables co-location and deeper optimization

**Enhanced framework selection with orchestration**:

```
Start
  │
  ├─ What scale?
  │   ├─ <100 QPS → Simple deployment (managed services or single instance)
  │   ├─ 100-10K QPS → Need autoscaling
  │   │   ├─ Managed acceptable → SageMaker/Vertex
  │   │   └─ Self-hosted required → Ray Serve + vLLM/TensorRT-LLM
  │   └─ >10K QPS → Need distributed orchestration
  │       ├─ LLM → Ray Serve + vLLM with sharding
  │       ├─ RecSys → Custom or Triton with embedding sharding
  │       └─ Vision → Triton with dynamic batching
  │
  ├─ How many model types?
  │   ├─ Single model → Direct runtime deployment
  │   └─ Multiple models/pipelines → Triton or Ray Serve composition
  │
  └─ Stateful or stateless?
      ├─ Stateless → Any load balancer, simple scaling
      └─ Stateful (LLM with cache) → Sticky routing, session management
```

**Case study: Startup serving Llama-70B for customer support**

*Scenario*: A startup is launching an LLM-powered customer support chatbot using a fine-tuned Llama-70B model.

+-------------------------+----------------------------------+
| **Constraint**          | **Value**                        |
+:========================+=================================:+
| **Expected traffic**    | 100 QPS average, 500 QPS peak    |
| **Latency requirement** | &lt;2s time to first token       |
| **Budget**              | 4 H100 GPUs (leased)             |
| **Team**                | 2 ML engineers, no dedicated SRE |
| **Conversation length** | Average 8 turns, max 32K context |
+-------------------------+----------------------------------+

*Analysis*:

1. **Memory**: Llama-70B requires ~140GB in FP16. With 4-bit quantization (AWQ), this drops to ~35GB, fitting on a single H100 (80GB) with room for KV cache.

2. **Throughput**: At 500 QPS peak with average 100 output tokens, the system must sustain 50,000 tokens/second. A single quantized Llama-70B on H100 achieves ~1,000 tokens/second with continuous batching, so 4 GPUs provide headroom.

3. **Tensor parallelism vs replicas**: Two options exist:
   - 2 replicas × 2-way TP: Higher availability, lower per-request latency
   - 4 replicas × 1-way TP: Maximum throughput, simpler scaling

   With AWQ fitting on single GPU, option (b) is preferred for this traffic level.

4. **Team size**: 2 ML engineers without SRE experience suggests managed services or simple orchestration.

*Decision*:

+------------+-------------------------+--------------------+-------------------------------+
| **Option** | **Architecture**        | **Pros**           | **Cons**                      |
+:===========+:========================+:===================+:==============================+
| **A**      | SageMaker + HF TGI      | Minimal ops burden | Cost, limited optimization    |
| **B**      | vLLM + Ray Serve on EC2 | Good balance       | Some ops required             |
| **C**      | TensorRT-LLM + Triton   | Maximum throughput | Complex, overkill for 500 QPS |
+------------+-------------------------+--------------------+-------------------------------+

**Recommendation**: Option B (vLLM + Ray Serve) provides the best balance. At 500 QPS, the 30% throughput advantage of TensorRT-LLM does not justify the deployment complexity. Start with vLLM; migrate to TensorRT-LLM only if traffic grows beyond 2,000 QPS.

```python
# Recommended production configuration
@serve.deployment(
    num_replicas=4,
    ray_actor_options={"num_gpus": 1},
    autoscaling_config={
        "min_replicas": 2,
        "max_replicas": 8,
        "target_num_ongoing_requests_per_replica": 20,
    },
)
class CustomerSupportLLM:
    def __init__(self):
        self.llm = LLM(
            model="your-finetuned-llama-70b-awq",
            quantization="awq",
            max_model_len=32768,
        )
```

This configuration handles the 500 QPS peak with 4 replicas, can scale to 8 during unexpected spikes, and scales down to 2 during low-traffic periods to reduce cost.

## Batching Strategies at Scale {#sec-inference-batching}

Having selected a serving framework, we must now configure how it processes requests. The framework determines which batching implementations are available: vLLM provides continuous batching with PagedAttention, TensorRT-LLM offers in-flight batching with optimized kernels, and Triton supports custom batching policies. Understanding the underlying principles of batching enables effective configuration regardless of framework choice, because where frameworks differ in *how* they batch, the principles we examine here govern *what* batching strategy suits each workload.

The core insight of batching is that processing multiple requests together amortizes fixed costs, including model loading, kernel launch overhead, and memory transfer latency, across more work. This trades higher per-request latency for dramatically improved throughput. Single-machine serving applies this through dynamic batching, which collects requests within a time window before processing them together.

At scale, batching becomes more complex because different model architectures have fundamentally different batching requirements. A strategy optimal for vision models may be catastrophic for LLMs, and techniques developed for recommendation systems may not apply to either.

**Production reality**: Recommendation systems constitute 80-90% of inference requests at major technology companies, with vision models handling most of the remainder, and LLMs currently representing 1-5% of request volume (though growing rapidly). Despite this distribution, we present batching strategies in order of conceptual complexity: vision (straightforward batching), LLMs (continuous batching with KV cache), and recommendation (feature-parallel batching with distributed embedding). This pedagogical ordering builds understanding progressively, even though practitioners will most frequently encounter recommendation workloads first.

This section develops a taxonomy of batching strategies matched to model characteristics, providing quantitative analysis of when each approach applies and what performance to expect.

### Why Batching Differs Across Model Types {#sec-inference-batching-differences}

The core insight is that batching efficiency depends on how computation scales with batch size relative to how memory and communication scale. Different model architectures exhibit different scaling relationships, requiring different batching strategies (@tbl-batching-by-model).

::: {.callout-note title="Figure Placeholder: Batching Strategies" collapse="true"}
```{.tikz}
% TODO: Visual comparison of Static, Continuous, and Feature-Parallel batching
\node[draw, align=center] {Batching Strategies\nStatic vs Continuous vs Feature-Parallel};
```
**Batching Strategies Compared**. Visual comparison of Static Batching (Vision), Continuous Batching (LLM), and Feature-Parallel Batching (RecSys). Static batching waits for a full batch, creating latency bubbles. Continuous batching dynamically inserts requests into running batches, maximizing GPU utilization. Feature-parallel batching shards requests by feature type rather than sample count, optimizing for embedding lookup latency.
:::

For **vision models** (CNNs, ViTs processing fixed-size images), computation scales linearly with batch size while memory scales sub-linearly due to weight sharing. Larger batches improve GPU utilization with minimal overhead, making static or dynamic batching with large batch sizes optimal.

For **LLMs in the decode phase**, computation per token is small relative to memory bandwidth requirements for loading model weights. The bottleneck is memory bandwidth, not compute. Larger batches amortize weight loading across more tokens, dramatically improving throughput but with diminishing returns as batch size grows.

For **recommendation systems**, the bottleneck is often embedding lookup rather than dense computation. Batching strategies must optimize for parallel embedding access patterns rather than matrix multiplication throughput.

+-------------------+-----------------------+------------------------+--------------------+------------------------+
| **Model Type**    | **Batching Strategy** | **Typical Batch Size** | **Key Constraint** | **Throughput Scaling** |
+:==================+:======================+=======================:+:===================+:=======================+
| **Vision (CNN)**  | Static/Dynamic        | 32-256                 | GPU compute        | Near-linear to 64+     |
| **LLM (prefill)** | Dynamic               | 1-64                   | Memory capacity    | Sub-linear             |
| **LLM (decode)**  | Continuous            | 100-1000s              | Memory bandwidth   | Log-linear             |
| **RecSys**        | Feature-parallel      | 1000-10000s            | Embedding lookup   | Depends on sharding    |
| **Speech**        | Streaming             | 1                      | Real-time          | N/A (latency-bound)    |
+-------------------+-----------------------+------------------------+--------------------+------------------------+

: **Batching Strategy by Model Type**: Each model type has characteristic batching behavior determined by its computational bottleneck. {#tbl-batching-by-model}

### Static and Dynamic Batching for Vision Models {#sec-inference-static-dynamic-batching}

Vision models represent the simplest batching case because inputs have uniform size (after preprocessing) and computation follows a predictable pattern. Single-machine batching principles apply directly, with scale introducing considerations of batch formation across multiple replicas.

Static batching collects exactly $B$ requests before processing. This maximizes GPU utilization when request arrival is predictable but causes unbounded latency during low-traffic periods.

Dynamic batching collects requests for a maximum time window $T_{window}$ or until reaching maximum batch size $B_{max}$, whichever occurs first. The expected latency under Poisson arrivals with rate $\lambda$ follows @eq-dynamic-batch-latency:

$$E[L_{total}] = E[L_{queue}] + L_{batch} + L_{inference}(B)$$ {#eq-dynamic-batch-latency}

where $E[L_{queue}]$ is the expected queuing delay, $L_{batch}$ is the batch formation delay (up to $T_{window}$), and $L_{inference}(B)$ is the inference time for batch size $B$.

::: {.callout-note title="Worked Example: Dynamic Batching for ResNet-50 at Scale"}

Consider a vision classification service with the following requirements:

- **Arrival rate**: 5,000 QPS
- **Latency SLO**: 50ms P99
- **Per-image inference time**: 5ms at batch=1, 25ms at batch=32
- **Number of replicas**: 10 (each handling 500 QPS)

For a single replica with Poisson arrivals at $\lambda = 500$ QPS:

**Option A: No batching (batch=1)**

- Service time: 5ms per request
- Utilization: $\rho = \lambda \times S = 500 \times 0.005 = 2.5$ (impossible, system is overloaded)

This configuration cannot meet demand. Batching is required.

**Option B: Dynamic batching with $B_{max}=16$, $T_{window}=10ms$**

Expected requests per window: $E[B] = \lambda \times T_{window} = 500 \times 0.01 = 5$

With 5 requests per batch:

- Inference time: approximately 8ms (interpolating between batch=1 and batch=32)
- Per-request compute: 8ms / 5 = 1.6ms
- Maximum batch delay: 10ms
- Expected total latency: ~15ms mean, ~30ms P99

Utilization: $\rho = 500 \times 0.0016 = 0.8$ (sustainable)

**Option C: Dynamic batching with $B_{max}=32$, $T_{window}=20ms$**

Expected requests per window: $E[B] = 500 \times 0.02 = 10$

With 10 requests per batch:

- Inference time: approximately 12ms
- Per-request compute: 12ms / 10 = 1.2ms
- Maximum batch delay: 20ms
- Expected total latency: ~22ms mean, ~42ms P99

Utilization: $\rho = 500 \times 0.0012 = 0.6$ (comfortable)

**Tradeoff**: Option C achieves 25% better throughput (lower utilization) at the cost of higher average latency (22ms vs 15ms). Both meet the 50ms P99 SLO.

:::

At scale with multiple replicas, batch formation can occur either at individual replicas or at a centralized batching layer. Replica-local batching has each replica independently form batches from its assigned traffic. This approach is simpler to implement but may result in uneven batch sizes across replicas when load is imbalanced. Centralized batching uses a batching service to collect requests and dispatch formed batches to replicas. This achieves more uniform batch sizes but adds a centralization bottleneck and additional network hop.

Production systems typically use replica-local batching with load balancing that ensures roughly equal traffic distribution, achieving the benefits of centralized batching without the complexity.

### Continuous Batching for LLM Inference {#sec-inference-continuous-batching}

Autoregressive language models present a unique batching challenge that static and dynamic approaches handle poorly. The key insight comes from the Orca system[^fn-orca] [@yu2022orca]: traditional batching forces all sequences in a batch to complete before any new sequences can join, wasting compute when sequences finish at different times.

[^fn-orca]: **Orca**: Named after the killer whale known for highly coordinated group hunting, Orca pioneered iteration-level scheduling for LLM serving at Microsoft in 2022. The system's insight that sequences could enter and exit batches at each decode step, rather than waiting for entire batches to complete, transformed LLM serving economics.

Consider a batch of 8 sequences. If one sequence completes after 10 tokens while others require 100 tokens, the completed sequence's GPU resources sit idle for 90 iterations. With traditional batching:

$$\text{Wasted compute} = \frac{(100 - 10) \times 1}{100 \times 8} = 11.25\%$$

For realistic output length distributions with high variance, wasted compute can exceed 50%.

Continuous batching (also called iteration-level batching) decouples batch membership from iteration boundaries. At each decode iteration, the system checks for completed sequences, removes completed sequences from the batch immediately, inserts waiting sequences into freed slots, and processes the reorganized batch for the next iteration.

This dynamic batch management maintains high GPU utilization regardless of sequence length variance.

The throughput improvement from continuous batching depends on sequence length distribution. For a distribution with coefficient of variation $CV = \sigma / \mu$, the gain is approximately @eq-continuous-batching-gain:

$$\text{Throughput gain} \approx 1 + \frac{CV^2}{2}$$ {#eq-continuous-batching-gain}

With typical LLM output lengths having $CV \approx 1.0$, continuous batching achieves approximately 1.5x throughput improvement. For highly variable outputs (conversational vs. code generation), gains can reach 2-4x.

::: {.callout-note title="Implementation: Continuous Batching in vLLM"}

vLLM implements continuous batching with several key mechanisms. Iteration-level scheduling evaluates at each decode step which sequences have generated end-of-sequence tokens (remove from batch), which waiting sequences can fit in available KV cache slots (add to batch), and which sequences should be preempted if memory pressure exists (swap to CPU). Memory management uses PagedAttention (see @sec-inference-kv-cache), which enables dynamic allocation without fragmentation. When a sequence completes, its KV cache pages are immediately available for new sequences. The batched decode kernel processes all active sequences in a single batched operation despite dynamic batch composition. Sequences at different generation lengths are padded to a common shape within the kernel.

#### Preemption and Swapping

A critical challenge in continuous batching is memory contention. As sequences grow during generation, they consume more KV cache pages. If the GPU memory fills up, the system cannot simply crash; it must preempt running requests.

vLLM implements a virtual memory mechanism similar to an operating system's swap. When memory is exhausted, the scheduler identifies low-priority requests (e.g., those most recently started) and **swaps** their KV cache blocks from GPU HBM to CPU DRAM. These requests are paused until memory becomes available, at which point they are swapped back in and resumed. This mechanism ensures system stability under heavy load at the cost of increased latency for preempted requests.

**Typical performance (Llama-2 70B on 8xA100)**:

+----------------------------+---------------------------+---------------------+
| **Batching Strategy**      | **Throughput (tokens/s)** | **GPU Utilization** |
+:===========================+==========================:+====================:+
| **Static (batch=8)**       | 400                       | 45%                 |
| **Dynamic (timeout=50ms)** | 580                       | 65%                 |
| **Continuous**             | 1,200                     | 92%                 |
+----------------------------+---------------------------+---------------------+

The 3x throughput improvement from continuous batching comes from eliminating idle GPU cycles during sequence length variation.

:::

### Prefill vs Decode: The Two-Phase Challenge {#sec-inference-prefill-decode}

LLM inference consists of two distinct phases with different computational characteristics, requiring different batching strategies within the same request (@tbl-prefill-decode):

**Prefill phase**: Process the entire input prompt in parallel. Computation scales with prompt length. Memory access pattern is compute-bound (high arithmetic intensity[^fn-arithmetic-intensity]).

**Decode phase**: Generate output tokens one at a time. Each token requires loading entire model weights. Memory access pattern is bandwidth-bound (low arithmetic intensity).

[^fn-arithmetic-intensity]: **Arithmetic intensity**: The ratio of compute operations to memory accesses, measured in FLOPs per byte. Prefill achieves 100+ FLOPs/byte (compute-bound), while decode achieves 1-10 FLOPs/byte (memory-bound). This 10-100x difference explains why prefill and decode require fundamentally different optimization strategies.

+-------------+-------------------+-------------------+----------------+-------------------+
| **Phase**   | **Computation**   | **Memory Access** | **Bottleneck** | **Optimal Batch** |
+:============+==================:+:==================+:===============+==================:+
| **Prefill** | O(prompt_length²) | Weight loading    | Compute        | Small (1-8)       |
| **Decode**  | O(1) per token    | Weight loading    | Bandwidth      | Large (100s)      |
+-------------+-------------------+-------------------+----------------+-------------------+

: **Prefill vs Decode Characteristics**: The two phases have opposite optimization requirements. {#tbl-prefill-decode}

This dichotomy creates a scheduling challenge: prefill operations are long-running and compute-intensive, while decode operations are short and bandwidth-limited. Mixing them in the same batch can cause interference.

**Chunked prefill**[^fn-chunked-prefill] addresses this by breaking long prompts into fixed-size chunks that interleave with decode operations:

[^fn-chunked-prefill]: **Chunked prefill motivation**: Without chunking, a 32K-token prompt blocks decode for 5-30 seconds while prefill completes. Chunking divides the prompt into 256-1024 token chunks processed between decode iterations, bounding decode latency at the cost of slightly longer prefill time. This trade-off favors interactive applications where decode responsiveness matters more than total completion time.

$$\text{Chunk latency} = \frac{\text{Chunk size}}{\text{Prefill throughput}}$$

With chunk size chosen to match decode iteration time, prefill and decode can share GPU resources without decode latency spikes.

**Prefill-decode disaggregation** takes this further by running prefill and decode on separate GPU pools:

- Prefill pool: Optimized for compute (larger batch sizes, no KV cache persistence)
- Decode pool: Optimized for bandwidth (small batches, maximum KV cache capacity)

This separation enables independent scaling: prefill capacity scales with input volume while decode capacity scales with output volume.

::: {.callout-note title="Sarathi: Chunked Prefill Implementation"}

The Sarathi system [@agrawal2023sarathi] implements chunked prefill with the following design:

**Chunk sizing**: Chunks are sized to complete in approximately the same time as one decode iteration (typically 10-50ms). For a prefill throughput of 10,000 tokens/second, a 20ms chunk processes 200 tokens.

**Interleaving schedule**: Each GPU iteration processes either:

- One prefill chunk for a new request, OR
- One decode step for all active sequences

This ensures decode latency remains bounded regardless of incoming prompt lengths.

**KV cache transfer**: When prefill completes, the generated KV cache transfers to decode slots. With NVLink, this transfer adds <1ms for typical prompt lengths.

**Performance impact**:

- Without chunking: Long prompts cause decode latency spikes of 100ms+
- With chunking: Decode latency bounded to 30ms P99 regardless of prompt length

:::

### Feature-Parallel Batching for Recommendation Systems {#sec-inference-feature-parallel-batching}

Recommendation systems have fundamentally different batching requirements than vision or language models. The computation pattern involves:

1. **Sparse feature lookup**: Retrieve embeddings for user, item, and context features
2. **Dense feature processing**: Transform and normalize dense features
3. **Feature interaction**: Compute interactions between features (often via attention or factorization)
4. **Ranking head**: Produce final scores

The sparse embedding lookup often dominates latency and determines batching strategy.

**Feature-parallel batching** processes different feature types in parallel rather than batching entire requests:

```
Request 1: [user_id_1, item_ids_1, context_1]
Request 2: [user_id_2, item_ids_2, context_2]
Request 3: [user_id_3, item_ids_3, context_3]

Feature-parallel view:
User embeddings:  [lookup(user_1), lookup(user_2), lookup(user_3)]  → parallel
Item embeddings:  [lookup(items_1), lookup(items_2), lookup(items_3)]  → parallel
Context features: [process(ctx_1), process(ctx_2), process(ctx_3)]  → parallel

Then: Combine features per request for ranking
```

This parallelization is natural when embeddings are sharded across servers: each embedding server handles lookups for its shard across all requests in the batch.

::: {.callout-note title="Worked Example: Recommendation System Batching at Meta Scale"}

Consider Meta's recommendation infrastructure serving 10 million QPS across the platform:

**Request characteristics**:

- Each request queries ~100 items (candidate ranking)
- Each item requires 50 embedding lookups (user features, item features, cross features)
- Total embedding lookups: 5,000 per request
- Embedding table size: 100TB across 1,000 shards

**Batching strategy**:

With 10M QPS and 1,000 embedding shards, each shard receives:

$$\text{Lookups per shard} = \frac{10M \times 5000}{1000} = 50 \text{ billion lookups/sec}$$

This is clearly infeasible for single-threaded processing. Instead:

**Batch accumulation window**: 1ms
**Requests per batch**: 10,000 (at 10M QPS)
**Lookups per shard per batch**: 50M

Each embedding shard processes 50M lookups in a batched operation, achieving memory bandwidth utilization of 90%+ through sequential memory access patterns.

**Latency breakdown**:

+------------------------+--------------+-----------------------------+
| **Phase**              | **Duration** | **Notes**                   |
+:=======================+=============:+:============================+
| **Request routing**    | 0.2ms        | Consistent hashing to shard |
| **Batch accumulation** | 0.5ms (avg)  | 1ms window                  |
| **Embedding lookup**   | 2ms          | Batched, SSD-backed         |
| **Feature processing** | 1ms          | Dense computation           |
| **Ranking model**      | 1.5ms        | Final scoring               |
| **Total**              | **5.2ms**    | Within 10ms SLO             |
+------------------------+--------------+-----------------------------+

:::

### Streaming Inference for Real-Time Applications {#sec-inference-streaming}

Some applications cannot tolerate batching delay of any kind. Real-time speech recognition, video analysis, and robotics require processing inputs as they arrive with minimal latency.

**Streaming inference** processes inputs incrementally without waiting for batch formation:

- **Speech**: Process audio frames (10-20ms chunks) as they arrive from the microphone
- **Video**: Process frames at capture rate (30-60 FPS) without buffering
- **Robotics**: Process sensor readings at control loop frequency (100-1000 Hz)

For streaming applications, the relevant metric is not throughput but **time to process each input**:

$$L_{streaming} = L_{capture} + L_{transfer} + L_{inference} + L_{action}$$

where all components must complete within the inter-frame interval.

::: {.callout-note title="Streaming Speech Recognition Pipeline"}

Consider a streaming speech-to-text system with 20ms audio frames:

**Latency budget**: 100ms end-to-end (5 frames of delay)

**Pipeline stages**:

+------------------------+------------------+-----------------------------+
| **Stage**              | **Duration**     | **Notes**                   |
+:=======================+=================:+:============================+
| **Audio capture**      | 0ms (continuous) | Microphone buffer           |
| **Network to server**  | 20ms             | Including jitter buffer     |
| **Feature extraction** | 5ms              | MFCC computation            |
| **Encoder inference**  | 30ms             | Streaming Conformer         |
| **Decoder step**       | 15ms             | Autoregressive CTC          |
| **Text formatting**    | 5ms              | Capitalization, punctuation |
| **Network to client**  | 15ms             | Response transmission       |
| **Total**              | **90ms**         | Within 100ms budget         |
+------------------------+------------------+-----------------------------+

**Key constraints**:

- No batching: Each frame processes individually
- Stateful model: Encoder maintains context across frames
- Pipeline parallelism: While frame N is in decoder, frame N+1 is in encoder

GPU utilization is typically 30-50% for streaming workloads, traded for latency guarantee.

:::

### Adaptive Batching Strategies {#sec-inference-adaptive-batching}

Production systems rarely use fixed batching parameters. Instead, they adapt batching behavior based on current conditions:

**Traffic-adaptive batching** adjusts batch window based on arrival rate:

$$T_{window} = \min\left(T_{max}, \frac{B_{target}}{\lambda_{current}}\right)$$

When traffic is high, the window shrinks because the target batch size fills quickly. When traffic is low, the window extends but is capped to bound maximum latency.

**SLO-adaptive batching** monitors latency percentiles and adjusts batching aggressively:

```
if P99_latency > 0.9 * SLO:
    reduce B_max by 20%
    reduce T_window by 20%
elif P99_latency < 0.5 * SLO:
    increase B_max by 10%
    increase T_window by 10%
```

This feedback loop maintains latency headroom while maximizing throughput during normal operation.

**Request-aware batching** considers request characteristics when forming batches. For LLMs:

- Group requests by expected output length (inferred from prompt type)
- Group requests by prompt length to minimize padding
- Prioritize latency-sensitive requests in smaller batches

::: {.callout-note title="Production Adaptive Batching: The NVIDIA Triton Approach"}

Triton Inference Server implements adaptive batching with three configurable parameters:

1. **max_batch_size**: Upper bound on batch size
2. **batching_timeout_ms**: Maximum time to wait for batch formation
3. **preferred_batch_size**: Target batch sizes that align with kernel efficiency

The scheduler maintains separate queues for each preferred batch size and routes requests to minimize total latency:

$$\text{Queue selection} = \arg\min_{q} \left( \text{wait}_q + \text{exec}(|q| + 1) \right)$$

This optimization considers both the current queue length and the efficiency of the resulting batch size.

**Observed behavior on ResNet-50 (V100)**:

+-------------------+--------------------+-----------------+----------------+
| **Traffic Level** | **Avg Batch Size** | **Avg Latency** | **Throughput** |
+==================:+===================:+================:+===============:+
| **100 QPS**       | 2.1                | 8ms             | 100 QPS        |
| **500 QPS**       | 6.3                | 12ms            | 500 QPS        |
| **1000 QPS**      | 12.4               | 18ms            | 1000 QPS       |
| **2000 QPS**      | 24.1               | 28ms            | 1980 QPS       |
+-------------------+--------------------+-----------------+----------------+

The system automatically increases batch size to maintain throughput as traffic grows.

:::

### Quantitative Summary: Batching Strategy Selection {#sec-inference-batching-summary}

The choice of batching strategy depends on model characteristics, traffic patterns, and latency requirements. The following decision framework guides selection:

::: {.callout-note title="Figure Placeholder: Inference Request Lifecycle" collapse="true"}
```{.tikz}
% TODO: Flowchart: Request -> LB -> Queue -> Batcher -> GPU -> Response
\node[draw, align=center] {Inference Pipeline\nRequest Lifecycle};
```
**End-to-End Inference Pipeline**. A high-level view of the request lifecycle: Client -> Load Balancer -> Request Queue -> Batch Scheduler -> Model Execution -> Response. This visualization highlights the critical "Serving Tax" components (serialization, routing, coordination) that consume latency budget outside of the actual GPU compute time.
:::

```
Is the model autoregressive (LLM, speech)?
├─ Yes → Continuous batching with prefill chunking
└─ No → Does the model have embedding lookups dominating latency?
        ├─ Yes → Feature-parallel batching (RecSys)
        └─ No → Dynamic batching with adaptive parameters
```

For each strategy, the key parameters to tune are summarized in @tbl-batching-parameters:

+----------------------+-----------------------+----------------------------------+
| **Strategy**         | **Key Parameters**    | **Tuning Goal**                  |
+:=====================+:======================+:=================================+
| **Static**           | Batch size            | Maximize throughput              |
| **Dynamic**          | Window, max batch     | Balance latency vs throughput    |
| **Continuous**       | Chunk size, max batch | Minimize decode latency variance |
| **Feature-parallel** | Accumulation window   | Match embedding shard capacity   |
| **Streaming**        | Pipeline depth        | Meet real-time deadline          |
+----------------------+-----------------------+----------------------------------+

: **Batching Strategy Parameters**: Each strategy has distinct parameters requiring tuning for the specific deployment. {#tbl-batching-parameters}

## Model Sharding for Inference {#sec-inference-sharding}

Batching strategies from the previous section address how to efficiently process requests on a given model replica. But what constitutes a "replica" when models exceed single-GPU memory? And how can we reduce latency when even batch-size-one inference is too slow? Model sharding answers both questions by distributing inference across multiple devices.

The parallelism strategies established for distributed training in @sec-distributed-training, including tensor parallelism, pipeline parallelism, and their communication patterns, apply to inference with an important inversion: training optimizes throughput over hours while inference must minimize per-request latency within milliseconds. This changes how we configure sharding strategies. Unlike training sharding where throughput is the primary concern, inference sharding must carefully balance parallelization benefits against communication overhead within strict latency budgets. This moves us from request-level optimization to replica-level concerns in the serving hierarchy.

This section examines four sharding strategies, each suited to different model architectures and deployment requirements: tensor parallelism for attention-heavy models, pipeline parallelism for sequential architectures, expert parallelism for mixture-of-experts models, and embedding sharding for recommendation systems.

### When Sharding Becomes Necessary {#sec-inference-sharding-when}

Model sharding for inference is driven by two distinct requirements (@tbl-sharding-triggers):

**Memory requirements**: A model that cannot fit in single-GPU memory must be sharded regardless of performance considerations. For a model with $P$ parameters at precision $b$ bits, the weight memory is calculated by @eq-weight-memory:

$$\text{Memory}_{weights} = P \times \frac{b}{8} \text{ bytes}$$ {#eq-weight-memory}

A 70-billion parameter model in FP16 (16 bits) requires:

$$\text{Memory} = 70 \times 10^9 \times \frac{16}{8} = 140 \text{ GB}$$

This exceeds the 80GB capacity of an H100 GPU, requiring at minimum 2-way sharding.

**Latency requirements**: Even when a model fits in memory, sharding can reduce latency by parallelizing computation. The potential speedup depends on the parallelization efficiency (@eq-parallel-time):

$$T_{parallel} = \frac{T_{sequential}}{P} + T_{communication}$$ {#eq-parallel-time}

where $P$ is the parallelism degree and $T_{communication}$ is the synchronization overhead. Sharding provides latency benefit only when the communication overhead is smaller than the time saved through parallelization.

+-------------------------+----------------------+----------------------+--------------------+
| **Sharding Trigger**    | **Model Examples**   | **Minimum Sharding** | **Strategy**       |
+:========================+=====================:+=====================:+:===================+
| **Memory (weights)**    | Llama-70B (140GB)    | 2-way                | Tensor or pipeline |
| **Memory (KV cache)**   | GPT-4 (long context) | 4-8 way              | Tensor (for cache) |
| **Memory (embeddings)** | DLRM (100TB)         | 1000+ way            | Embedding sharding |
| **Latency**             | Any large model      | Varies               | Tensor parallelism |
+-------------------------+----------------------+----------------------+--------------------+

: **Sharding Triggers**: Different constraints lead to different sharding requirements and strategies. {#tbl-sharding-triggers}

### Tensor Parallelism {#sec-inference-tensor-parallelism}

Tensor parallelism [@shoeybi2019megatron] distributes individual layers across multiple devices, enabling parallel computation within each layer. The column-row partitioning scheme introduced for training in @sec-distributed-training applies here: splitting the first linear layer by columns and the second by rows requires only one AllReduce per transformer block. For transformer models, the primary target is the attention mechanism and feed-forward layers, which contain the majority of computation.

**Attention layer parallelism**: The multi-head attention computation naturally partitions across attention heads. For a model with $H$ attention heads distributed across $P$ devices, each device computes $H/P$ heads:

$$\text{Attention}_i = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i \text{ for heads } i \in \{1, ..., H/P\}$$

After computing local attention, an AllReduce operation (covered in @sec-communication) combines results across devices, adding communication overhead proportional to the activation size divided by the interconnect bandwidth.

**Feed-forward layer parallelism**: The feed-forward layer (typically two linear transformations with activation) partitions along the hidden dimension. For the first linear layer, columns are distributed; for the second, rows are distributed. This column-row partitioning requires only one all-reduce per feed-forward block.

The communication pattern for tensor-parallel inference follows:

::: {.callout-note title="Figure Placeholder: Tensor Parallelism Inference Flow" collapse="true"}
```{.tikz}
% TODO: Diagram showing column-row partitioning and AllReduce synchronization
\node[draw, align=center] {Tensor Parallelism\\Attention Heads Split -> AllReduce\\FFN Split (Column/Row) -> AllReduce};
```
**Tensor Parallelism for Inference**. Computation is distributed across devices by splitting tensor operations. Attention heads are partitioned across GPUs, requiring an AllReduce operation to synchronize results. Feed-forward networks use a column-row splitting strategy that requires only one AllReduce synchronization per block. This approach reduces latency for large models but introduces communication overhead that demands high-bandwidth interconnects like NVLink.
:::

The inference time with tensor parallelism follows @eq-tensor-parallel-time:

$$T_{inference} = \frac{T_{compute}}{P} + 2 \times T_{allreduce}\left(\frac{A}{P}\right)$$ {#eq-tensor-parallel-time}

where $T_{compute}$ is the sequential compute time, $P$ is the parallelism degree, and $A$ is the activation size being reduced. The factor of 2 accounts for the two all-reduce operations per transformer layer (attention and feed-forward).

::: {.callout-note title="Worked Example: Tensor Parallelism for Llama-70B"}

Consider serving Llama-70B with the following configuration:

**Model specifications**:

- Parameters: 70 billion
- Hidden dimension: 8,192
- Attention heads: 64
- Layers: 80

**Memory per GPU** (weight only, FP16):

$$\text{Memory}_{70B} = 70 \times 10^9 \times 2 = 140\text{ GB}$$

**Minimum sharding**: 2-way (140GB / 80GB per H100)

**Recommended sharding**: 8-way for optimal latency

**With 8-way tensor parallelism on 8xH100 (NVLink interconnect)**:

+---------------------------+------------------------+--------------+-------------+
| **Component**             | **Sequential (1 GPU)** | **8-way TP** | **Speedup** |
+:==========================+=======================:+=============:+:============+
| **Attention compute**     | 12ms                   | 1.5ms        | 8x          |
| **AllReduce (attention)** | 0ms                    | 0.3ms        | N/A         |
| **Feed-forward compute**  | 18ms                   | 2.25ms       | 8x          |
| **AllReduce (FF)**        | 0ms                    | 0.3ms        | N/A         |
| **Total per layer**       | **30ms**               | **4.35ms**   | **6.9x**    |
+---------------------------+------------------------+--------------+-------------+

**For 80 layers**:

- Sequential: 2,400ms per token
- 8-way TP: 348ms per token

The 6.9x speedup (vs theoretical 8x) reflects communication overhead. With 600 GB/s NVLink bandwidth, each 8MB activation all-reduce takes ~0.3ms.

**Time-to-first-token** (1024-token prompt):

- Prefill compute: ~50ms (compute-bound, near-linear scaling)
- Total TTFT: ~60ms with preprocessing

:::

### Pipeline Parallelism for Inference {#sec-inference-pipeline-parallelism}

Pipeline parallelism distributes layers across devices sequentially, with each device handling a subset of layers. Unlike tensor parallelism, there is no synchronization within a layer, only between pipeline stages.

For inference, pipeline parallelism creates bubbles differently than in training:

::: {.callout-note title="Figure Placeholder: Pipeline Parallelism Bubbles" collapse="true"}
```{.tikz}
% TODO: Diagram comparing single-request latency (sequential) vs pipelined throughput
\node[draw, align=center] {Pipeline Parallelism\\Sequential processing of single request (High Latency)\\Pipelined processing of multiple requests (High Throughput)};
```
**Pipeline Parallelism Bubbles**. For a single inference request, pipeline parallelism offers no latency benefit as the request must traverse all stages sequentially (top). However, when processing multiple concurrent requests, pipeline bubble utilization improves significantly (bottom), allowing throughput to scale with the number of stages. This makes pipeline parallelism ideal for high-throughput batch processing but less suitable for latency-critical interactive serving.
:::

For a single request, pipeline parallelism provides no latency benefit: the request must traverse all stages sequentially. The pipeline fill time equals the sequential execution time.

However, pipeline parallelism enables **throughput scaling** through pipelining multiple requests:

```
Time →
Device 0: [Req1] [Req2] [Req3] [Req4] ...
Device 1:        [Req1] [Req2] [Req3] [Req4] ...
Device 2:               [Req1] [Req2] [Req3] [Req4] ...
Device 3:                      [Req1] [Req2] [Req3] [Req4] ...
```

Once the pipeline is full, throughput equals $P$ times single-stage throughput, where $P$ is the number of pipeline stages. The steady-state latency remains approximately the single-device latency (sum of all stage times), but throughput scales with parallelism.

**When to use pipeline parallelism for inference**:

- When memory constraints require sharding but latency requirements are relaxed
- When throughput is more important than individual request latency
- When network bandwidth between devices is limited (only point-to-point communication)

@tbl-pipeline-tensor-comparison summarizes the key tradeoffs between pipeline and tensor parallelism:

+----------------------------+----------------------------------+------------------------------------+
| **Aspect**                 | **Tensor Parallelism**           | **Pipeline Parallelism**           |
+:===========================+:=================================+:===================================+
| **Single-request latency** | Reduced by ~$P$x                 | No improvement                     |
| **Throughput**             | $P$x                             | $P$x (when pipelined)              |
| **Communication pattern**  | AllReduce (bandwidth-intensive)  | Point-to-point (latency-sensitive) |
| **Memory efficiency**      | Activations replicated           | Activations passed along           |
| **Complexity**             | Higher (requires custom kernels) | Lower (layer-level partitioning)   |
+----------------------------+----------------------------------+------------------------------------+

: **Pipeline vs Tensor Parallelism**: Each strategy has distinct tradeoffs in latency, throughput, and implementation complexity. {#tbl-pipeline-tensor-comparison}

### Expert Parallelism for MoE Models {#sec-inference-expert-parallelism}

Mixture-of-Experts (MoE)[^fn-moe] models present unique sharding challenges because computation is dynamically routed to different experts based on input. Popular models like Mixtral [@jiang2024mixtral] use MoE to achieve high capacity with lower inference cost.

[^fn-moe]: **Mixture-of-Experts (MoE)**: An architecture where only a subset of model parameters (the "experts") are activated for each input, selected by a learned gating network. This enables models with very high total parameter counts (for capacity) while maintaining computational cost proportional to active parameters. The trade-off is increased memory footprint and communication complexity for expert routing.

In an MoE layer, a gating network selects $k$ experts (out of $E$ total) for each token:

$$\text{Output} = \sum_{i \in \text{top-}k} g_i \cdot \text{Expert}_i(\text{input})$$

**Expert parallelism** distributes experts across devices, with each device hosting $E/P$ experts:

::: {.callout-note title="Figure Placeholder: Mixture-of-Experts Routing" collapse="true"}
```{.tikz}
% TODO: Network diagram showing token routing to experts and AllToAll communication
\node[draw, align=center] {MoE Routing\\Token -> Gating -> AllToAll Dispatch -> Experts -> AllToAll Gather};
```
**Mixture-of-Experts (MoE) Routing**. Expert parallelism distributes "experts" across different devices. For each token, a gating mechanism selects the top-k experts. An AllToAll communication step dispatches tokens to the devices hosting their selected experts (1). Experts process the tokens in parallel (2). A second AllToAll step gathers the results back to the original device (3). This pattern enables massive model capacity but introduces all-to-all communication overhead.
:::

The communication pattern differs from tensor parallelism: instead of all-reduce (same data to all devices), expert parallelism uses all-to-all (different data to different devices based on routing).

**Load balancing challenge**: If gating decisions cluster on certain experts, devices hosting popular experts become bottlenecks while others sit idle. MoE training includes auxiliary losses to encourage balanced routing, but inference still exhibits routing imbalance.

::: {.callout-note title="Expert Parallelism for Mixtral-8x7B"}

Mixtral-8x7B uses 8 experts per MoE layer with top-2 routing:

**Model characteristics**:

- Total parameters: 47B (but only ~13B active per token)
- Experts per layer: 8
- Active experts per token: 2 (top-k = 2)
- MoE layers: Every other feed-forward layer

**Sharding strategy** (4-way expert parallelism):

- Experts 0-1 on Device 0
- Experts 2-3 on Device 1
- Experts 4-5 on Device 2
- Experts 6-7 on Device 3

**Communication pattern per token**:

1. Gating: Determine which 2 experts to use (~0.1ms)
2. AllToAll dispatch: Send token to devices hosting selected experts (~0.2ms)
3. Expert compute: Process token through selected experts (~1ms each, parallel)
4. AllToAll gather: Collect results back (~0.2ms)

**Total MoE layer time**: ~1.5ms (vs ~4ms for equivalent dense layer)

**Load balancing metrics**:

+------------------------------+---------------------+-----------------------+
| **Routing Distribution**     | **GPU Utilization** | **Throughput Impact** |
+:=============================+====================:+:======================+
| **Perfectly balanced**       | 100%                | Baseline              |
| **Moderate imbalance (20%)** | 83%                 | -17%                  |
| **Severe imbalance (50%)**   | 67%                 | -33%                  |
+------------------------------+---------------------+-----------------------+

Production systems monitor routing statistics and may retrain or fine-tune gating to improve balance.

:::

### Embedding Sharding for Recommendation Systems {#sec-inference-embedding-sharding}

Recommendation systems typically contain embedding tables that dwarf model weights in size. Meta's DLRM-scale models [@naumov2019dlrm] have embedding tables exceeding 100TB, requiring aggressive sharding strategies fundamentally different from tensor or pipeline parallelism.

**Row-wise sharding** partitions embedding tables by row (entity ID):

$$\text{Shard}_i = \{e_j : \text{hash}(j) \mod P = i\}$$

Each shard contains approximately $N/P$ embeddings, where $N$ is the total number of entities and $P$ is the shard count.

**Column-wise sharding** partitions each embedding vector across devices:

$$e_j = [e_j^{(0)}, e_j^{(1)}, ..., e_j^{(P-1)}]$$

Each device stores a slice of every embedding.

**Hybrid sharding** combines both approaches: frequently accessed embeddings are column-sharded for faster access, while the long tail uses row sharding.

The choice of embedding sharding strategy depends on lookup patterns and communication overhead (@tbl-embedding-sharding).

+-----------------------+--------------------------+-------------------+-------------------------+
| **Sharding Strategy** | **Lookup Pattern**       | **Communication** | **Best For**            |
+:======================+:=========================+:==================+:========================+
| **Row-wise**          | Single device per lookup | AllToAll gather   | Uniform access patterns |
| **Column-wise**       | All devices per lookup   | AllGather         | Hot embeddings          |
| **Hybrid**            | Varies by embedding      | Mixed             | Production RecSys       |
+-----------------------+--------------------------+-------------------+-------------------------+

: **Embedding Sharding Strategies**: Different strategies trade off lookup locality against load balance. {#tbl-embedding-sharding}

::: {.callout-note title="Figure Placeholder: Embedding Sharding Strategies" collapse="true"}
```{.tikz}
% TODO: Visual comparison of Row-wise, Column-wise, and Hybrid sharding
\node[draw, align=center] {Embedding Sharding\\Row-wise: By User ID (Network Gather)\\Column-wise: By Vector Dim (AllGather)\\Hybrid: Hot/Cold Split};
```
**Embedding Sharding Strategies**. **Row-wise sharding** places complete embedding vectors on specific servers based on entity ID, requiring a network gather for lookup. **Column-wise sharding** splits each vector across all servers, allowing parallel local lookups followed by an AllGather, which is efficient for popular "hot" embeddings. **Hybrid sharding** combines these approaches, using column sharding for hot items and row sharding for the "cold" long tail to balance load and memory.
:::

::: {.callout-note title="Embedding Sharding at Scale: Meta Infrastructure"}

Meta's recommendation infrastructure demonstrates embedding sharding at extreme scale:

**Scale**:

- Embedding tables: 100+ TB total
- Unique entities: 10+ trillion
- Embedding dimension: 128-256
- Shards: 1,000+ servers

**Sharding strategy**:

- **Hot embeddings** (top 1% by access frequency): Replicated across all shards
- **Warm embeddings** (next 10%): Column-sharded with 8-way parallelism
- **Cold embeddings** (remaining 89%): Row-sharded with consistent hashing

Each inference request requires approximately 5,000 embedding lookups. Without optimization, this would require 5,000 network round trips. Instead, the system applies several optimizations. Batch accumulation collects lookups for 1ms. Lookup deduplication removes duplicate entities across requests. Shard-aware batching groups lookups by destination shard. Parallel dispatch sends batched requests to all shards simultaneously. Streaming assembly reconstructs embeddings as responses arrive.

**Performance**:

+-------------------------+--------------------------+-----------------------+
| **Metric**              | **Without Optimization** | **With Optimization** |
+:========================+=========================:+======================:+
| **Network round trips** | 5,000                    | 1 (batched)           |
| **Lookup latency**      | 50ms                     | 2ms                   |
| **Network bandwidth**   | 10 Gbps                  | 40 Gbps (burst)       |
+-------------------------+--------------------------+-----------------------+

:::

### Hybrid Sharding Strategies {#sec-inference-hybrid-sharding}

Production systems often combine multiple sharding strategies to handle different model components optimally:

**Tensor + Pipeline parallelism**: For very large models that require both memory distribution and latency reduction:

```
8 GPUs organized as 2 pipeline stages × 4 tensor parallel:

Stage 0 (Layers 1-40):  TP across GPUs 0,1,2,3
Stage 1 (Layers 41-80): TP across GPUs 4,5,6,7
```

This achieves 4x latency reduction (from TP) while handling models requiring 8-way sharding for memory.

**Expert + Tensor parallelism**: For MoE models where individual experts are large:

```
Mixtral with large experts:

- Expert parallelism: Distribute 8 experts across 8 GPU groups
- Tensor parallelism: Each expert spread across 2 GPUs
- Total GPUs: 16
```

**Embedding + Dense parallelism**: For recommendation models with both large embeddings and large dense components:

```
DLRM-scale model:

- Embedding sharding: 1,000 shards across CPU servers
- Dense model: 8-way tensor parallel across GPUs
- Communication: Embeddings gathered to GPU, processed, returned
```

### Communication Overhead Analysis {#sec-inference-sharding-communication}

The practical speedup from sharding depends critically on communication efficiency. Each sharding strategy has characteristic communication patterns with different bandwidth and latency requirements.

AllReduce (tensor parallelism) combines data from all devices, with result available on all devices (@eq-allreduce-time).

$$T_{allreduce} = 2 \times \frac{(P-1)}{P} \times \frac{M}{B}$$ {#eq-allreduce-time}

where $P$ is the number of devices, $M$ is the message size, and $B$ is the interconnect bandwidth. The factor of 2 accounts for the reduce-scatter and all-gather phases.

Point-to-point (pipeline parallelism) sends data from one device to the next (@eq-p2p-time).

$$T_{p2p} = L + \frac{M}{B}$$ {#eq-p2p-time}

where $L$ is the network latency and $M/B$ is the transfer time.

AllToAll (expert parallelism) exchanges data where each device sends different data to each other device (@eq-alltoall-time).

$$T_{alltoall} = (P-1) \times \left(L + \frac{M/P}{B}\right)$$ {#eq-alltoall-time}

::: {.callout-note title="Interconnect Technology Comparison"}

Communication overhead depends heavily on the interconnect technology:

+--------------------+----------------------+-------------+------------------------+
| **Interconnect**   | **Bandwidth**        | **Latency** | **Use Case**           |
+===================:+=====================:+============:+:=======================+
| **NVLink (H100)**  | 900 GB/s             | 1μs         | Intra-node TP          |
| **PCIe Gen5**      | 64 GB/s              | 5μs         | Intra-node (no NVLink) |
| **InfiniBand HDR** | 200 Gb/s (25 GB/s)   | 1μs         | Inter-node             |
| **Ethernet 100G**  | 100 Gb/s (12.5 GB/s) | 10μs        | Inter-node (commodity) |
+--------------------+----------------------+-------------+------------------------+

NVLink provides the highest bandwidth for intra-node communication.

**Example: 8-way tensor parallelism communication**

Activation size: 8MB per all-reduce (batch=1, hidden=8192)

+-------------------+--------------------+---------------------+
| **Interconnect**  | **AllReduce Time** | **% of 30ms Layer** |
+:==================+===================:+====================:+
| **NVLink**        | 0.02ms             | 0.07%               |
| **InfiniBand**    | 0.7ms              | 2.3%                |
| **100G Ethernet** | 1.5ms              | 5%                  |
+-------------------+--------------------+---------------------+

NVLink enables efficient tensor parallelism within a node. Cross-node tensor parallelism requires InfiniBand for acceptable overhead.

:::

NVLink bandwidth has evolved significantly over GPU generations.[^fn-nvlink]

[^fn-nvlink]: **NVLink evolution**: NVLink bandwidth has grown from 160 GB/s (Pascal, 2016) to 900 GB/s bidirectional (Hopper, 2022) to 1.8 TB/s (Blackwell, 2024). This 10x improvement enables tensor parallelism across 8+ GPUs with less than 5% communication overhead, making large model inference practical.

### Sharding Strategy Selection {#sec-inference-sharding-selection}

The choice of sharding strategy depends on model architecture and deployment priorities (@tbl-sharding-selection).

+---------------------------+---------------------+-----------------------+---------------------+---------------------+
| **Factor**                | **Tensor Parallel** | **Pipeline Parallel** | **Expert Parallel** | **Embedding Shard** |
+:==========================+:====================+:======================+:====================+:====================+
| **Latency priority**      | Best                | Worst                 | Moderate            | N/A                 |
| **Throughput priority**   | Good                | Best (pipelined)      | Good                | Best                |
| **Interconnect limited**  | Poor fit            | Good fit              | Moderate            | Good fit            |
| **Implementation effort** | High                | Low                   | Moderate            | High                |
+---------------------------+---------------------+-----------------------+---------------------+---------------------+

: **Sharding Strategy Selection Guide**: Match strategy to deployment priorities and constraints. {#tbl-sharding-selection}

## Load Balancing and Request Routing {#sec-inference-load-balancing}

With models deployed across multiple replicas, the next challenge is distributing requests effectively. This moves us from replica-level concerns (how individual replicas are configured) to service-level optimization in the serving hierarchy: coordinating across replicas to meet aggregate throughput and latency targets.

Load balancing determines which replica handles each request, directly impacting latency, throughput, and resource utilization. Seemingly simple choices, like random assignment versus informed selection, produce dramatically different performance at scale. This section develops the theory and practice of load balancing for inference, from basic algorithms through the power-of-two-choices insight that provides exponentially better performance with minimal overhead.

### Load Balancing Fundamentals {#sec-inference-lb-fundamentals}

Load balancing serves two primary goals that sometimes conflict:

**Latency minimization**: Route requests to replicas that can serve them fastest, considering current queue depth and processing time.

**Utilization maximization**: Spread load evenly to avoid both idle replicas and overloaded replicas.

The tension arises because latency-optimal routing may concentrate load on fast replicas, reducing their performance and leaving other replicas underutilized.

Load balancing evaluation uses several key metrics. Maximum queue length measures the longest queue across all replicas and determines worst-case latency. Load variance captures the standard deviation of queue lengths and measures balance. Utilization spread represents the difference between most and least utilized replicas. Decision overhead quantifies the time required to make routing decisions.

### Round-Robin and Random Assignment {#sec-inference-lb-round-robin}

The simplest load balancing strategies assign requests without considering server state:

Round-robin assigns requests in circular order. Request 1 goes to server 1, request 2 to server 2, and so on. This guarantees perfect distribution when servers are homogeneous and request processing times are identical.

Random assignment selects a server uniformly at random for each request. With large numbers of requests, this converges to even distribution but with higher variance than round-robin.

For homogeneous servers with identical service times, both achieve near-optimal load distribution. However, production systems rarely meet these assumptions. Heterogeneous hardware introduces different GPU generations and memory configurations. Variable request sizes mean some requests take 10x longer than others. Server state variations occur when some replicas are warming up while others approach memory limits.

Under these realistic conditions, uninformed strategies perform poorly. The maximum queue length under random assignment follows @eq-random-max-queue[^fn-theta-notation]:

$$E[\text{max queue}] = \Theta\left(\frac{\log n}{\log \log n}\right)$$ {#eq-random-max-queue}

[^fn-theta-notation]: **Theta Notation (Θ)**: Asymptotic tight bound indicating both upper (O) and lower (Ω) complexity bounds match. Unlike O(f(n)) which may be loose, Θ(f(n)) precisely characterizes growth rate. In queueing analysis for inference systems, tight bounds enable accurate capacity planning: Θ(log n / log log n) queue length means exactly that scaling, not just "at most".

where $n$ is the number of servers. For 1,000 servers, this is approximately 4-5 requests. This seems small, but the unlucky requests in long queues experience significantly higher latency.

### The Power of Two Choices {#sec-inference-two-choices}

A remarkable result in load balancing theory [@mitzenmacher2001power] shows that querying just two random servers before making a routing decision provides exponentially better load distribution than random assignment.[^fn-balls-bins]

[^fn-balls-bins]: **Balls-into-bins problem**: The power-of-two-choices result comes from probabilistic analysis of throwing balls into bins. With random placement, maximum load is $\Theta(\log n / \log \log n)$. With two choices and placing in the less-loaded bin, maximum load drops to $\Theta(\log \log n)$. This exponential improvement from a constant-factor change in the algorithm is a foundational result in randomized algorithms.

The power-of-two-choices algorithm operates as follows. Select two servers uniformly at random. Query both for their current queue length. Route the request to the server with the shorter queue.

This simple modification reduces maximum queue length from $O(\log n / \log \log n)$ to $O(\log \log n)$ (@eq-two-choices-max-queue):

$$E[\text{max queue}]_{\text{two choices}} = \Theta(\log \log n)$$ {#eq-two-choices-max-queue}

For 1,000 servers:

- Random assignment max queue: ~4-5 requests
- Two choices max queue: ~2 requests

The improvement is exponential: two choices with 1,000 servers achieves better balance than random with just 10 servers.

::: {.callout-important title="Exponential Improvement from a Simple Change"}

The power-of-two-choices result is one of the most impactful findings in distributed systems theory. By examining just one additional server, maximum queue length improves from $O(\log n / \log \log n)$ to $O(\log \log n)$, an exponential improvement.

This has profound practical implications:

- Near-optimal load balancing with minimal overhead (2 probes vs n probes)
- Scalable: improvement increases with system size
- Robust: works with heterogeneous servers and variable request sizes
- Simple: easy to implement in any load balancer

Production systems at Google, Meta, and AWS all use variants of power-of-two-choices.

:::

**Why does this work?** Intuitively, random assignment occasionally makes poor choices (routing to an already-busy server), and these mistakes compound. With two choices, the algorithm almost never makes the worst choice, avoiding the tail behavior that creates long queues.

Mathematically, the key insight is that with random assignment, when $d$ servers have queue length $k$, the probability of queue length $k+1$ growing is proportional to $d/n$. With two choices, this probability drops to $(d/n)^2$, creating a super-exponential decay in queue length distribution.

### Weighted and Adaptive Load Balancing {#sec-inference-weighted-lb}

When servers have different capacities, naive load balancing creates imbalance. A mix of A100 GPUs (high capacity) and T4 GPUs (lower capacity) receiving equal request rates will have T4 servers overloaded while A100 servers are underutilized.

**Weighted round-robin** assigns requests proportional to server capacity:

$$P(\text{route to server } i) = \frac{w_i}{\sum_j w_j}$$

where $w_i$ is the weight (capacity) of server $i$.

**Weighted two-choices** applies the same principle:

1. Select two servers with probability proportional to their weights
2. Query both for current load relative to their capacity
3. Route to the server with lower relative load

::: {.callout-note title="Worked Example: Heterogeneous GPU Cluster"}

Consider a cluster with mixed GPU types:

- 10 H100 GPUs (capacity: 1000 QPS each)
- 20 A100 GPUs (capacity: 600 QPS each)
- Total capacity: 10×1000 + 20×600 = 22,000 QPS

**Target traffic**: 15,000 QPS

**Weighted assignment**:

- H100 weight: 1000 / 22000 = 4.5%
- A100 weight: 600 / 22000 = 2.7%

**Expected load per server**:

- H100: 15000 × 0.045 = 682 QPS (68% utilization)
- A100: 15000 × 0.027 = 409 QPS (68% utilization)

Both server types operate at equal utilization, maximizing overall capacity while maintaining latency consistency.

**Without weighting** (equal distribution):

- Per-server load: 15000 / 30 = 500 QPS
- H100 utilization: 50% (underutilized)
- A100 utilization: 83% (overloaded, latency spikes)

:::

**Adaptive load balancing** adjusts weights dynamically based on observed performance:

```
For each server i:
    latency[i] = exponential_moving_average(observed_latency)
    weight[i] = 1 / latency[i]  # Inverse latency weighting
```

This automatically adapts to:

- Server degradation (memory pressure, thermal throttling)
- Request size variations (some traffic patterns harder to serve)
- Background tasks consuming resources

### Least-Connections Load Balancing {#sec-inference-least-connections}

An alternative to random selection is routing to the server with the fewest active connections (or shortest queue). This requires maintaining global state but provides better balance for variable-size requests.

**Least-connections algorithm**:

1. Maintain a count of active requests per server
2. Route each new request to the server with the minimum count
3. Increment count on dispatch, decrement on completion

For long-running requests (common in LLM serving), least-connections significantly outperforms round-robin because it accounts for current load rather than just historical assignments.

The challenge is maintaining accurate connection counts in a distributed system. Options include:

- **Centralized counter**: Single source of truth, potential bottleneck
- **Distributed counters with gossip**: Eventually consistent, may route to stale information
- **Sampled least-connections**: Query a subset of servers, choose minimum (combines with two-choices)

::: {.callout-note title="Least-Connections for LLM Serving"}

LLM inference has highly variable request durations based on output length:

- Short response (10 tokens): 500ms
- Long response (500 tokens): 25s
- Ratio: 50x

With round-robin at 100 QPS across 10 servers:

- Each server receives 10 requests/second
- If one server gets multiple long requests, it falls behind
- Queue builds while other servers sit idle

With least-connections:

- New requests route away from servers processing long responses
- Servers finishing short requests receive new work immediately
- Load naturally balances based on actual work remaining

**Observed improvement** (production LLM serving):

+-----------------------+-----------------+-------------------+
| **Algorithm**         | **P99 Latency** | **Load Variance** |
+:======================+================:+==================:+
| **Round-robin**       | 45s             | 3.2 requests      |
| **Least-connections** | 28s             | 0.8 requests      |
| **Two-choices + LC**  | 26s             | 0.5 requests      |
+-----------------------+-----------------+-------------------+

Least-connections reduces P99 by 38%; combining with two-choices provides additional improvement.

:::

### Consistent Hashing for Stateful Routing {#sec-inference-consistent-hashing}

Many inference workloads maintain state that benefits from routing affinity:

- **LLM conversations**: KV cache from previous turns
- **Recommendation sessions**: User context and recent interactions
- **Streaming inference**: Model state from previous frames

For these workloads, routing the same user or session to the same server improves performance by avoiding cache misses and state reconstruction.

**Consistent hashing**[^fn-consistent-hashing] [@karger1997consistent] maps requests to servers based on a hash of the routing key (user ID, session ID):

[^fn-consistent-hashing]: **Consistent hashing**: Originally developed for distributed caching (Akamai, 1997), consistent hashing arranges servers on a virtual ring and assigns requests to the nearest server clockwise. When a server fails, only requests that were assigned to that server remap to the next server, minimizing disruption. This property is essential for maintaining KV cache locality during scaling events.

$$\text{server}(request) = \arg\min_{s \in S} \text{distance}(\text{hash}(key), \text{hash}(s))$$

where servers and keys are mapped onto a ring, and each request routes to the nearest server clockwise.

Key properties:

- **Deterministic**: Same key always routes to same server
- **Minimal disruption**: Adding/removing servers only remaps $K/N$ keys on average
- **Load balancing**: With virtual nodes, load distributes evenly

::: {.callout-note title="Consistent Hashing for KV Cache Affinity"}

Consider an LLM serving system where each user's conversation maintains KV cache state:

**Without affinity**:

- User sends message, routed to Server A, KV cache built
- Next message routes to Server B (random)
- KV cache rebuilt from scratch, 500ms penalty
- Average conversation: 10 turns, 4.5s wasted on cache rebuilds

**With consistent hashing**:

- User ID hashed to Server A
- All messages from this user route to Server A
- KV cache reused across turns
- Rebuild only on server changes or cache eviction

**Implementation with virtual nodes**:

Each physical server has 100 virtual nodes on the hash ring, ensuring even distribution despite server heterogeneity.

```
Hash ring positions:
Server A: [0.01, 0.03, 0.07, 0.12, ...]  (100 positions)
Server B: [0.02, 0.05, 0.09, 0.15, ...]  (100 positions)
...

Request for user "alice":
hash("alice") = 0.0834
Nearest server clockwise: Server A (at 0.09)
```

**Handling server failures**:

When Server A fails, its 100 virtual nodes are removed from the ring. Requests that would have routed to Server A now route to the next server clockwise. Only ~$1/N$ of requests are affected, where $N$ is the number of servers.

:::

### Request Routing for Sharded Models {#sec-inference-sharded-routing}

When models are sharded across devices (see @sec-inference-sharding), routing becomes more complex. A single inference request may require computation on multiple devices, necessitating coordination.

**Routing patterns for sharded models**:

**Tensor parallelism**: Request is broadcast to all devices in the shard group. Each device processes its portion of each layer. Results are synchronized via all-reduce.

```
Request → Load Balancer → Shard Group
                              │
            ┌────────────────┼────────────────┐
            ▼                ▼                ▼
         GPU 0            GPU 1            GPU 2
      (heads 0-7)      (heads 8-15)    (heads 16-23)
            │                │                │
            └────────AllReduce────────────────┘
                              │
                              ▼
                          Response
```

**Pipeline parallelism**: Request flows through stages sequentially. Each stage forwards to the next.

```
Request → Stage 0 → Stage 1 → Stage 2 → Stage 3 → Response
         (L1-20)   (L21-40)  (L41-60)  (L61-80)
```

**Expert parallelism**: Request is dispatched to devices hosting selected experts based on gating decision.

```
Request → Gating → AllToAll dispatch → Expert compute → AllToAll gather → Response
                   (to selected experts)              (results back)
```

**Routing to shard groups**: With multiple shard groups for horizontal scaling, the load balancer routes to groups rather than individual devices:

```
                    Load Balancer
                         │
          ┌──────────────┼──────────────┐
          ▼              ▼              ▼
     Shard Group 0  Shard Group 1  Shard Group 2
      (8 GPUs)       (8 GPUs)       (8 GPUs)
```

The load balancer treats each shard group as a single logical server, applying standard algorithms (round-robin, two-choices, consistent hashing) at the group level.

### Health Checking and Failover {#sec-inference-health-checking}

Load balancers must detect unhealthy servers and route around them. Health checking mechanisms include:

**Liveness probes**: Verify the server process is running.

```
GET /health/live
Response: 200 OK (process alive) or timeout (process dead)
```

**Readiness probes**: Verify the server can handle requests (model loaded, GPU initialized).

```
GET /health/ready
Response: 200 OK (ready to serve) or 503 (not ready)
```

**Deep health checks**: Verify actual inference works by running a test request.

```
POST /health/inference
Body: {"prompt": "test"}
Response: 200 OK with valid output, or error
```

::: {.callout-note title="Health Check Configuration for GPU Inference"}

GPU inference servers have unique health check considerations:

**GPU memory pressure**: Server may be alive but unable to allocate memory for new requests.

```{.python}
def readiness_check():
    free_memory = (
        torch.cuda.memory_reserved() - torch.cuda.memory_allocated()
    )
    if free_memory < MIN_REQUEST_MEMORY:
        return {
            "status": "not_ready",
            "reason": "insufficient GPU memory",
        }
    return {"status": "ready"}
```

**Model warm-up**: First inference after load is slower. Mark ready only after warm-up.

```{.python}
async def startup():
    model = load_model()
    # Warm up with dummy requests
    for _ in range(10):
        model.generate(dummy_input)
    # Now mark as ready
    global ready
    ready = True
```

**Timeout configuration**:

+-----------------+--------------+-------------+-----------------------+
| **Check Type**  | **Interval** | **Timeout** | **Failure Threshold** |
+:================+=============:+============:+======================:+
| **Liveness**    | 10s          | 5s          | 3 failures            |
| **Readiness**   | 5s           | 3s          | 2 failures            |
| **Deep health** | 30s          | 10s         | 1 failure             |
+-----------------+--------------+-------------+-----------------------+

Deep health checks run less frequently because they consume GPU resources.

:::

### Quantitative Analysis: Load Balancing Impact {#sec-inference-lb-analysis}

The choice of load balancing algorithm has quantitative impact on system performance as compared in @tbl-lb-comparison. Consider a system with 100 servers, 10,000 QPS, and variable request sizes (CV = 0.5).

+-----------------------+---------------+-----------------+------------------+
| **Algorithm**         | **Max Queue** | **P99 Latency** | **CPU Overhead** |
+:======================+==============:+================:+:=================+
| **Random**            | 4.2 requests  | 45ms            | Minimal          |
| **Round-robin**       | 2.8 requests  | 32ms            | Minimal          |
| **Two-choices**       | 1.9 requests  | 24ms            | 2 probes/request |
| **Least-connections** | 1.4 requests  | 19ms            | Global state     |
| **Two-choices + LC**  | 1.2 requests  | 17ms            | 2 probes + state |
+-----------------------+---------------+-----------------+------------------+

: **Load Balancing Algorithm Comparison**: More sophisticated algorithms reduce queue lengths and latency at the cost of increased overhead. {#tbl-lb-comparison}

The progression shows clear tradeoffs:

- Random/round-robin: Zero overhead but higher latency variance
- Two-choices: Minimal overhead (2 probes), 47% latency improvement
- Least-connections: State maintenance overhead, 58% latency improvement
- Combined: Best performance, highest complexity

For most production systems, two-choices provides the best tradeoff between performance improvement and implementation complexity. Least-connections adds value for workloads with high request size variance (LLM serving, recommendation ranking).

### Circuit Breakers and Backpressure {#sec-inference-circuit-breakers}

When servers become overloaded, routing more requests exacerbates the problem. Circuit breakers and backpressure mechanisms protect the system from cascading failures.

**Circuit breaker pattern**[^fn-circuit-breaker] [@nygard2007releaseit]:

[^fn-circuit-breaker]: **Circuit breaker**: Borrowed from electrical engineering, where circuit breakers prevent electrical fires by cutting power when current exceeds safe levels. In distributed systems, circuit breakers prevent cascade failures by quickly failing requests to unhealthy services rather than waiting for timeouts. The pattern was popularized by Netflix's Hystrix library and is now standard in service mesh implementations like Envoy and Istio.

```
States: CLOSED → OPEN → HALF-OPEN → CLOSED

CLOSED: Normal operation, route requests
OPEN: Server unhealthy, immediately reject requests
HALF-OPEN: Allow limited requests to test recovery

Transitions:

- CLOSED → OPEN: Error rate exceeds threshold (e.g., 50%)
- OPEN → HALF-OPEN: After timeout (e.g., 30s)
- HALF-OPEN → CLOSED: Test requests succeed
- HALF-OPEN → OPEN: Test requests fail
```

**Backpressure propagation**: When servers are overloaded, they signal upstream to reduce request rate:

```
Server queue depth > threshold
    → Return 503 Service Unavailable
    → Load balancer marks server as degraded
    → Routes fewer requests to this server
    → If all servers degraded, apply admission control
```

::: {.callout-note title="Cascading Failure Prevention"}

Consider a scenario where one server becomes slow (thermal throttling):

**Without circuit breaker**:

1. Server A slows down (processing 500ms instead of 50ms)
2. Load balancer continues routing to Server A
3. Requests queue on Server A, timeouts begin
4. Retry logic sends failed requests to other servers
5. Other servers overload from retry traffic
6. System-wide failure

**With circuit breaker**:

1. Server A slows down
2. Error rate on Server A rises above 50%
3. Circuit breaker opens for Server A
4. All requests route to Servers B, C, D
5. System operates at reduced capacity but remains stable
6. After recovery, circuit breaker closes, Server A rejoins

**Configuration for GPU inference**:

+------------------------+-------------+---------------------------------------+
| **Parameter**          | **Value**   | **Rationale**                         |
+:=======================+============:+:======================================+
| **Error threshold**    | 30%         | GPU OOM failures are serious          |
| **Latency threshold**  | 2x baseline | Detect throttling early               |
| **Open duration**      | 60s         | GPU recovery takes time               |
| **Half-open requests** | 5           | Careful testing before full reopening |
+------------------------+-------------+---------------------------------------+

:::

## KV Cache Management {#sec-inference-kv-cache}

Load balancing distributes requests across replicas at the service level for all model types. Within each replica, we now narrow our focus to a challenge specific to autoregressive language models. While recommendation and vision models have fixed memory footprints during inference, LLMs accumulate state as they generate tokens. This accumulated state, stored in the key-value (KV) cache, represents a distinctive memory management challenge that does not exist for other model types.

Autoregressive language models maintain KV caches that store attention context from previous tokens, enabling efficient generation without recomputing attention over the entire sequence history. As context lengths grow and serving scales, KV cache management becomes a critical bottleneck at the replica level of the serving hierarchy. A 70B parameter model with 128K context can require over 100GB just for KV cache, exceeding the model weights themselves.

This section examines the memory management techniques that enable efficient LLM serving at scale: PagedAttention for fragmentation-free allocation, prefix caching for common prompt sharing, and speculative decoding for latency reduction.

### KV Cache Fundamentals {#sec-inference-kv-cache-fundamentals}

As established in transformer architecture fundamentals, autoregressive generation without caching requires $O(t^2)$ computation per token because each transformer layer must recompute attention keys and values for all previous tokens. The KV cache stores these computed key and value vectors, reducing generation to $O(t)$ per token. For serving at scale, this memory savings creates a critical management challenge since KV cache memory can exceed model weights for long contexts.

The cache size grows with context as calculated by @eq-kv-cache-size:

$$\text{KV cache size} = 2 \times L \times H \times S \times B \times P$$ {#eq-kv-cache-size}

where:

- $L$ = number of layers
- $H$ = hidden dimension
- $S$ = sequence length
- $B$ = batch size
- $P$ = precision (bytes per element)
- Factor of 2 accounts for both keys and values

::: {.callout-note title="Worked Example: KV Cache Memory for Llama-70B"}

For Llama-70B with typical serving configuration:

- Layers ($L$): 80
- Hidden dimension ($H$): 8,192
- Context length ($S$): 4,096 tokens
- Batch size ($B$): 32 concurrent requests
- Precision ($P$): 2 bytes (FP16)

$$\text{KV cache} = 2 \times 80 \times 8192 \times 4096 \times 32 \times 2 = 344 \text{ GB}$$

This exceeds the model weights (140GB) by 2.5x!

**Memory breakdown for single H100 (80GB)**:

+-------------------+------------+--------------------------+
| **Component**     | **Memory** | **Percentage**           |
+:==================+===========:+:=========================+
| **Model weights** | 140GB      | Cannot fit on single GPU |
+-------------------+------------+--------------------------+

**With 8-way tensor parallelism across 8 H100s (640GB total)**:

+----------------------------+--------------------+-----------+
| **Component**              | **Memory per GPU** | **Total** |
+:===========================+===================:+==========:+
| **Model weights**          | 17.5GB             | 140GB     |
| **KV cache (theoretical)** | 43GB               | 344GB     |
| **Activations**            | 4GB                | 32GB      |
| **Available for KV**       | ~15GB              | 120GB     |
+----------------------------+--------------------+-----------+

The 120GB available KV cache limits concurrent batch size to ~11 requests at 4K context, not 32.

**Implication**: KV cache capacity, not compute, limits LLM serving throughput for long contexts.

:::

### The Fragmentation Problem {#sec-inference-kv-fragmentation}

Traditional memory allocation for KV cache pre-allocates contiguous memory for each sequence based on maximum expected length. This creates two forms of waste:

**Internal fragmentation**: Sequences shorter than the maximum allocation waste the unused portion. If maximum length is 4,096 but average output is 100 tokens, 97.5% of allocated memory is wasted.

**External fragmentation**: As sequences complete and new ones start, memory becomes fragmented into non-contiguous free blocks. Even with sufficient total free memory, no single block may be large enough for a new maximum-length allocation.

Consider a simplified example with 8 memory slots and maximum sequence length of 4:

```
Time 0: Allocate Seq A (slots 0-3), Seq B (slots 4-7)
        [A][A][A][A][B][B][B][B]

Time 1: Seq A completes (2 tokens), Seq B continues
        [ ][ ][A][A][B][B][B][ ]  <- A only used 2 slots

Time 2: Try to allocate Seq C (needs 4 slots)
        [ ][ ][A][A][B][B][B][ ]  <- No contiguous block of 4!

Result: 4 free slots but cannot allocate new sequence
```

Production systems report 60-80% memory waste from fragmentation under realistic workloads, severely limiting batch sizes and throughput.

### PagedAttention {#sec-inference-paged-attention}

PagedAttention [@kwon2023vllm], introduced in vLLM, applies virtual memory[^fn-virtual-memory] concepts to KV cache management. Instead of contiguous allocation, the KV cache is divided into fixed-size pages (typically 16-256 tokens), and sequences are allocated pages on demand.

[^fn-virtual-memory]: **Virtual memory for KV cache**: Just as operating systems use virtual memory to provide processes with contiguous address spaces backed by non-contiguous physical memory, PagedAttention provides sequences with logically contiguous KV caches backed by non-contiguous GPU memory blocks. This enables dynamic allocation, sharing, and efficient memory utilization without fragmentation.

The key concepts include page tables that map logical sequence positions to physical memory pages, block size that defines the number of tokens per page (typically 16 tokens), and physical blocks that provide fixed-size memory allocations assignable to any sequence.

::: {.callout-note title="Figure Placeholder: PagedAttention Memory Mapping" collapse="true"}
```{.tikz}
% TODO: Diagram showing Logical KV Cache pages mapping to Non-continuous Physical Blocks
\node[draw, align=center] {PagedAttention\\Logical View: Contiguous Sequence\\Physical View: Scattered Blocks in HBM\\Block Table: The Mapping};
```
**PagedAttention Memory Mapping**. Conceptually similar to virtual memory in operating systems, PagedAttention decouples the logical view of a sequence's KV cache (contiguous pages) from its physical storage (non-contiguous 16-token blocks). A block table maps logical pages to physical blocks, allowing the system to fill fragmentation gaps with small blocks from any sequence. This eliminates external fragmentation and enables near-100% memory utilization.
:::

PagedAttention provides several benefits. It eliminates internal fragmentation by allocating only the pages needed for actual tokens. It eliminates external fragmentation because any free page can be used by any sequence. It enables dynamic growth so sequences can grow without pre-allocation. It supports memory sharing so common prefixes can share physical pages.

::: {.callout-note title="PagedAttention Implementation Details"}

**Memory layout**:

```
Physical blocks (16 tokens × hidden_dim × 2 × precision):
Block 0:  [K₀...K₁₅, V₀...V₁₅]
Block 1:  [K₀...K₁₅, V₀...V₁₅]
...
Block N:  [K₀...K₁₅, V₀...V₁₅]
```

**Page table per sequence**:

```{.python}
class PageTable:
    def __init__(self, max_blocks):
        self.block_map = {}  # logical_block -> physical_block

    def allocate_block(self, logical_idx, physical_block):
        self.block_map[logical_idx] = physical_block

    def get_physical(self, logical_idx):
        return self.block_map[logical_idx]
```

**Attention kernel modification**:

Standard attention: `output = softmax(Q @ K.T / sqrt(d)) @ V`

PagedAttention:
```{.python}
def paged_attention(Q, page_table, physical_blocks, block_size):
    # Gather K, V from non-contiguous physical blocks
    for logical_idx in range(num_logical_blocks):
        physical_idx = page_table[logical_idx]
        K_block = physical_blocks[physical_idx].K
        V_block = physical_blocks[physical_idx].V
        # Compute attention for this block
        attention_scores = Q @ K_block.T / sqrt(d)
        output += softmax(attention_scores) @ V_block
    return output
```

**Performance impact**:

The gather operations add overhead, but it is minimal compared to the memory savings:

+--------------------+------------------------+---------------------------+
| **Approach**       | **Memory Utilization** | **Throughput (relative)** |
+:===================+=======================:+==========================:+
| **Contiguous**     | 30-40%                 | 1.0x (baseline)           |
| **PagedAttention** | 95%+                   | 2.5-4x                    |
+--------------------+------------------------+---------------------------+

The 2.5-4x throughput improvement comes from fitting more concurrent sequences in the same memory.

:::

### Prefix Caching {#sec-inference-prefix-caching}

Many LLM workloads share common prefixes across requests. System prompts like "You are a helpful assistant..." are prepended to every request. Few-shot examples use the same examples for many queries. Document context involves multiple questions about the same document. Recomputing these shared prefixes wastes both compute (prefill) and memory (duplicate KV cache entries).

**Prefix caching** shares KV cache entries across requests with common prefixes:

::: {.callout-note title="Figure Placeholder: Prefix Caching with Shared Blocks" collapse="true"}
```{.tikz}
% TODO: Tree or Block diagram showing multiple requests pointing to the same System Prompt blocks
\node[draw, align=center] {Prefix Caching\\System Prompt Blocks [0-5] (Shared)\\Request A [6-8] (Unique)\\Request B [6-9] (Unique)};
```
**Prefix Caching via Block Sharing**. PagedAttention enables efficient prefix caching by allowing multiple sequences' block tables to point to the same physical blocks for shared content. In this example, the System Prompt is stored in blocks 0-5. Request A and Request B maps their first 6 logical pages to these same physical blocks, storing only their unique suffixes in new blocks. This dramatically reduces memory usage and prefill computation for workloads with shared context.
:::

**Implementation with PagedAttention**:

Prefix caching integrates naturally with PagedAttention through copy-on-write semantics:

```
System prompt → Physical blocks [0, 1, 2, 3, 4, 5]

Request A page table: [0, 1, 2, 3, 4, 5, 10, 11]  <- shares prefix blocks
Request B page table: [0, 1, 2, 3, 4, 5, 12, 13, 14]  <- shares prefix blocks
Request C page table: [0, 1, 2, 3, 4, 5, 15]  <- shares prefix blocks
```

All three requests reference the same physical blocks for the system prompt. Only when generating unique tokens do they allocate new blocks.

::: {.callout-note title="Prefix Caching at Scale"}

Consider a chatbot service with a 2000-token system prompt and 1000 concurrent users:

**Without prefix caching**:

- KV cache per user: 2000 + 500 (avg response) = 2500 tokens
- Total KV cache: 2500 × 1000 × 2 × 80 × 8192 × 2 = 6.5 TB

**With prefix caching**:

- Shared prefix: 2000 tokens (once)
- Unique per user: 500 tokens
- Total: (2000 × 1) + (500 × 1000) = 502,000 tokens
- Memory: 502,000 × 2 × 80 × 8192 × 2 = 1.3 TB

**Savings**: 80% reduction in KV cache memory, enabling 5x more concurrent users.

**Prefix hit rate** determines effectiveness:

+----------------------------------+---------------------+--------------------+
| **Workload**                     | **Prefix Hit Rate** | **Memory Savings** |
+:=================================+====================:+===================:+
| **Chatbot (same system prompt)** | 95%+                | 70-80%             |
| **Document QA (same doc)**       | 80-90%              | 50-70%             |
| **General API (diverse)**        | 20-40%              | 10-30%             |
+----------------------------------+---------------------+--------------------+

:::

### KV Cache Compression {#sec-inference-kv-compression}

Beyond efficient allocation, reducing the size of cached values provides additional memory savings. Several techniques compress the KV cache:

**Quantization**: Store cached keys and values at reduced precision.

$$\text{Compressed size} = \text{Original size} \times \frac{b_{compressed}}{b_{original}}$$

+---------------------+----------------------+--------------------+
| **Precision**       | **Memory per Token** | **Quality Impact** |
+====================:+=====================:+===================:+
| **FP16 (baseline)** | 2 bytes              | None               |
| **FP8**             | 1 byte               | &lt;1% degradation |
| **INT8**            | 1 byte               | 1-2% degradation   |
| **INT4**            | 0.5 bytes            | 3-5% degradation   |
+---------------------+----------------------+--------------------+

**Key observation**: KV cache values are more tolerant of quantization than model weights because they are intermediate activations, not learned parameters.

**Sliding window attention**: For very long contexts, maintain full cache only for recent tokens:

```
Full context: 100,000 tokens
Sliding window: 4,096 tokens

Cache strategy:

- Tokens 0-95,904: Discarded or compressed
- Tokens 95,904-100,000: Full precision cache

Trade-off: Cannot attend to very old tokens, but sufficient for most tasks.
```

**Grouped-query attention (GQA)**[^fn-gqa] [@ainslie2023gqa]: Architectural change that reduces KV cache by sharing key-value heads:

[^fn-gqa]: **GQA trade-off**: Grouped-query attention interpolates between multi-head attention (MHA, one KV head per query head) and multi-query attention (MQA, one KV head for all query heads). GQA with 8 KV heads for 64 query heads achieves 8x KV cache reduction versus MHA while maintaining most of MHA's model quality, unlike MQA which can degrade quality significantly.

+-------------------------+--------------+---------------------------+
| **Attention Type**      | **KV Heads** | **Cache Size (relative)** |
+:========================+=============:+==========================:+
| **Multi-head (MHA)**    | 64           | 1.0x                      |
| **Grouped-query (GQA)** | 8            | 0.125x                    |
| **Multi-query (MQA)**   | 1            | 0.016x                    |
+-------------------------+--------------+---------------------------+

Modern models like Llama 2 [@touvron2023llama2] and Mistral use GQA specifically to reduce KV cache requirements.

### Speculative Decoding {#sec-inference-speculative-decoding}

Autoregressive generation is inherently sequential: each token depends on previous tokens. Speculative decoding[^fn-speculative] [@leviathan2023speculative; @chen2023accelerating] breaks this bottleneck by using a smaller draft model to predict multiple tokens, then verifying them in parallel with the target model.

[^fn-speculative]: **Speculative execution in LLMs**: The concept borrows from CPU speculative execution, where processors predict branch outcomes and execute instructions ahead of confirmation. Similarly, speculative decoding predicts multiple tokens and processes them speculatively, rolling back only when the target model disagrees. Unlike CPU speculation (which is invisible to software), LLM speculation requires explicit acceptance/rejection logic.

**Algorithm**:

1. Draft model generates $k$ tokens speculatively: $t_1, t_2, ..., t_k$
2. Target model verifies all $k$ tokens in a single forward pass
3. Accept prefix of correct tokens, reject from first incorrect token
4. Continue from last accepted token

**Why this works**: The draft model is much smaller (7B vs 70B) and can generate $k$ tokens in the time the target model generates 1 token. Verification is cheap because the target model can process all $k$ tokens in parallel (like prefill).

::: {.callout-note title="Speculative Decoding Example"}

**Target model**: Llama-70B (30 tokens/second)
**Draft model**: Llama-7B (300 tokens/second)
**Speculation length**: $k = 4$ tokens

**Scenario**: Generating "The quick brown fox jumps"

::: {.callout-note title="Figure Placeholder: Speculative Decoding Timeline" collapse="true"}
```{.tikz}
% TODO: Sequence/Timeline diagram of Draft generation followed by Parallel Verification
\node[draw, align=center] {Speculative Decoding\\Draft Model: Gen 4 tokens (Fast)\\Target Model: Verify 4 tokens (Parallel)\\Accept/Reject: Rollback to first error};
```
**Speculative Decoding Process**. Instead of generating tokens sequentially with the large target model (slow), a small draft model quickly proposes a sequence of $K$ tokens. The target model then verifies all $K$ tokens in a single parallel forward pass (similar to prefill). If the draft tokens match the target's output, they are accepted, effectively generating multiple tokens per target model step. If a mismatch occurs, the sequence is rolled back to the first error.
:::

**Effective speedup**: 43/30 = 1.43x

**Factors affecting speedup**:

+---------------------+-------------------+
| **Acceptance Rate** | **Speedup**       |
+====================:+==================:+
| **90% (easy text)** | 2.5-3x            |
| **70% (typical)**   | 1.5-2x            |
| **50% (hard text)** | 1.2-1.5x          |
| **30% (very hard)** | &lt;1x (overhead) |
+---------------------+-------------------+

Speedup depends on how well the draft model predicts the target model's output. For well-aligned model pairs (same training data, similar architecture), acceptance rates of 70-80% are common.

:::

**Self-speculative decoding** uses early exit from the target model itself as the draft, avoiding the need for a separate model:

```
Target model layers: 80

Draft: Layers 1-20 → predict next token
Verify: Layers 1-80 → confirm or reject
```

This eliminates the need to load and manage a separate draft model, at the cost of lower acceptance rates than a dedicated draft model.

### KV Cache in Distributed Settings {#sec-inference-kv-cache-distributed}

When models are sharded across devices (see @sec-inference-sharding), KV cache management gains additional complexity:

**Tensor parallelism**: KV cache is sharded across devices along with attention heads. Each device stores cache for its subset of heads.

```
8-way tensor parallelism:
Device 0: KV cache for heads 0-7
Device 1: KV cache for heads 8-15
...
Device 7: KV cache for heads 56-63
```

**Cross-device sharing**: Prefix caching across tensor-parallel devices requires cache to be sharded identically on all devices. This is automatic when prefixes are processed with the same tensor-parallel configuration.

**KV cache migration**: When consistent hashing routes a conversation to a different replica (due to failure or rebalancing), the KV cache must be migrated:

```
Migration options:
1. Rebuild: Re-run prefill on new replica (500ms+ for long context)
2. Transfer: Send KV cache over network (100MB at 100Gbps = 8ms)
3. Hybrid: Transfer if small, rebuild if large

Decision threshold:
if cache_size_bytes / network_bandwidth < prefill_time:
    transfer()
else:
    rebuild()
```

For Llama-70B with 4K context, KV cache is ~80MB per sequence. At 100 Gbps, transfer takes 6.4ms versus ~500ms for prefill. Transfer is clearly better.

### Memory Management Best Practices {#sec-inference-kv-best-practices}

Effective KV cache management combines multiple techniques:

**Sizing the KV cache pool**:

```
Available GPU memory = Total - Weights - Activations - Overhead
KV pool size = 0.9 × Available  # Leave 10% headroom

Max concurrent sequences = KV pool size / (avg_seq_length × per_token_cache)
```

When cache is full, systems use several eviction policies. LRU (Least Recently Used) evicts sequences with oldest last access. Size-based eviction removes longest sequences first to free most memory. Priority-based eviction protects high-priority or paid-tier requests.

**Preemption** for continuous batching:

When a new high-priority request cannot fit, the system follows a sequence of steps. It selects victim sequences using the eviction policy. It swaps the victim's KV cache to CPU memory. It allocates GPU memory to the new request. When the victim is resumed, it swaps back from CPU.

::: {.callout-note title="KV Cache Memory Hierarchy"}

Production systems use a memory hierarchy for KV cache:

+--------------+--------------+-------------+-------------------+
| **Tier**     | **Capacity** | **Latency** | **Use Case**      |
+:=============+=============:+============:+:==================+
| **GPU HBM**  | 80GB         | 0ms         | Active sequences  |
| **CPU DRAM** | 1TB          | 1-5ms       | Swapped sequences |
| **NVMe SSD** | 10TB         | 10-50ms     | Long-term cache   |
+--------------+--------------+-------------+-------------------+

**Swap implementation**:

```{.python}
async def swap_to_cpu(sequence_id):
    kv_cache = gpu_cache[sequence_id]
    cpu_cache[sequence_id] = kv_cache.cpu()  # Async transfer
    gpu_cache.free(sequence_id)


async def swap_to_gpu(sequence_id):
    cpu_kv = cpu_cache[sequence_id]
    gpu_cache[sequence_id] = cpu_kv.cuda()  # Async transfer
    cpu_cache.free(sequence_id)
```

**Observed performance**:

- GPU-only (no swapping): 50 concurrent sequences
- GPU+CPU swapping: 500 concurrent sequences (10x)
- Average swap latency: 3ms (acceptable for non-urgent requests)

:::

## Weight Quantization for Serving {#sec-inference-weight-quantization}

The KV cache techniques we just examined address dynamic memory allocation during inference. But model weights themselves represent static memory consumption that persists regardless of batch size or sequence length. For a 70B model, weights alone consume 140GB in FP16, leaving limited capacity for KV cache on even the largest GPUs. Quantization offers a complementary approach: reduce the memory footprint of weights themselves.

Quantization reduces numerical precision of model weights and activations, decreasing memory footprint by 2-4x while increasing decode throughput, which is memory-bandwidth limited rather than compute limited. While quantization fundamentals like post-training quantization (PTQ) and quantization-aware training (QAT) are established techniques, serving at scale introduces distinct challenges. Models must be quantized after training without access to training data. Quantization must preserve quality across diverse inputs. Hardware deployment targets vary from datacenter GPUs to edge accelerators. This section examines quantization techniques specifically designed for production inference.

### LLM-Specific Quantization Challenges {#sec-inference-llm-quantization}

Large language models present unique quantization challenges distinct from vision or recommendation models. The outlier activation problem occurs because certain attention heads produce activation magnitudes orders of magnitude larger than typical values. Naive quantization clips these outliers, causing significant quality degradation.

Consider a Llama-70B layer where most activations fall within [-10, 10] but specific channels reach magnitudes of 1000+. Symmetric INT8 quantization with range [-127, 127] must choose:

- **Wide range** [-1000, 1000]: Most values map to 0, losing information
- **Narrow range** [-10, 10]: Outliers clip, causing large errors

This outlier distribution motivates the specialized quantization methods that follow.

### GPTQ: Layer-by-Layer Weight Quantization {#sec-inference-gptq}

GPTQ [@frantar2023gptq] quantizes LLM weights using Hessian-based[^fn-hessian] error compensation. Rather than quantizing all weights independently, GPTQ adjusts remaining weights to compensate for errors introduced by quantization.

[^fn-hessian]: **Hessian matrix in quantization**: The Hessian matrix $H = X^T X$ captures second-order information about how output changes with respect to weight perturbations. Weights with large Hessian diagonal entries have outsized impact on model outputs; GPTQ uses this information to prioritize preserving important weights and compensate for errors in less important ones.

The GPTQ algorithm processes the model layer by layer. For each layer, it calibrates using a small dataset (128-256 samples). It quantizes weights in order of decreasing Hessian magnitude. It adjusts remaining weights to minimize output error.

**Key insight**: The Hessian matrix $H = X^T X$ captures which weights most affect outputs. GPTQ builds on the Optimal Brain Surgeon (OBS) framework, using the inverse Hessian to guide error compensation.

**Column-wise processing algorithm**:

GPTQ processes each weight matrix column by column, using Cholesky decomposition of $H^{-1}$ for efficiency:

```
For each layer's weight matrix W:
  1. Compute Hessian: H = X^T X from calibration activations
  2. Apply Cholesky factorization to H^{-1}
  3. For each column q = 1 to d_col:
     a. Quantize column: w_q = round(W[:,q] / Δ) × Δ
     b. Compute quantization error: δ = W[:,q] - w_q
     c. Update remaining columns to compensate:
        W[:,q+1:] += δ × [H^{-1}][:,q+1:] / [H^{-1}]_{qq}
```

The compensation step propagates quantization error to unquantized columns, where the Hessian-weighted update minimizes output deviation. This achieves $O(d_{row} \cdot d_{col}^2)$ complexity versus $O(d_{row} \cdot d_{col}^3)$ for naive OBS.

**Quantization formula**:

$$w_q = \text{round}\left(\frac{w}{\Delta}\right) \cdot \Delta$$

where $\Delta$ is the quantization step size. GPTQ uses per-group quantization with group sizes of 128 weights, enabling finer-grained scaling that reduces quantization error for outlier channels.

**Performance characteristics**:

+---------------+----------+-------------------------+----------------------+-----------------------+
| **Model**     | **Bits** | **Perplexity Increase** | **Memory Reduction** | **Quantization Time** |
+==============:+=========:+========================:+=====================:+======================:+
| **Llama-7B**  | 4        | +0.3                    | 4x                   | 15 min                |
| **Llama-13B** | 4        | +0.2                    | 4x                   | 30 min                |
| **Llama-70B** | 4        | +0.15                   | 4x                   | 3 hours               |
+---------------+----------+-------------------------+----------------------+-----------------------+

GPTQ's strengths include fast quantization without retraining, minimal quality loss for 4-bit weights, and broad hardware compatibility. Its limitations include requiring calibration data, sensitivity to calibration set selection, and per-layer processing that cannot leverage cross-layer information.

### AWQ: Activation-Aware Weight Quantization {#sec-inference-awq}

AWQ [@lin2024awq] observes that not all weights are equally important. Weights connected to channels with large activation magnitudes have disproportionate impact on outputs.

**Key insight**: Rather than protecting weights based on their own magnitude, protect weights based on the magnitude of activations they produce.

**Algorithm**:

1. Run calibration samples to measure per-channel activation magnitudes
2. Identify "salient" channels with large activations
3. Scale weights for salient channels up before quantization
4. Scale outputs down correspondingly (fused into subsequent layer)

**Scaling formulation**:

For weight matrix $W$ and activation statistics $s$ (per-channel activation magnitudes):

$$W' = W \cdot \text{diag}(s^\alpha)$$

where $\alpha \in [0.5, 1.0]$ controls scaling aggressiveness. This preserves salient channels while allowing aggressive quantization of less important weights.

**Comparison with GPTQ**:

+----------------------------+---------------------------+-------------------------+
| **Aspect**                 | **GPTQ**                  | **AWQ**                 |
+:===========================+:==========================+:========================+
| **Error compensation**     | Adjusts remaining weights | Scales salient channels |
| **Calibration data**       | 128-256 samples           | 128 samples             |
| **Quality (4-bit)**        | Very good                 | Excellent               |
| **Speed**                  | Faster                    | Slightly slower         |
| **Hardware compatibility** | Broad                     | Broad                   |
+----------------------------+---------------------------+-------------------------+

AWQ typically achieves 0.5-1% lower perplexity degradation than GPTQ at the same bit-width, making it preferred for production deployments where quality is paramount.

### SmoothQuant: Migrating Quantization Difficulty {#sec-inference-smoothquant}

SmoothQuant [@xiao2023smoothquant] addresses the activation outlier problem by migrating quantization difficulty from activations to weights. Weights have predictable distributions; activations have unpredictable outliers. SmoothQuant transfers the outlier problem to weights where it can be handled with per-channel scaling.

**Core technique**: Insert smoothing operations that divide activations by per-channel scales while multiplying weights by the same scales:

$$Y = X W = (X \cdot \text{diag}(s)^{-1}) \cdot (\text{diag}(s) \cdot W) = \hat{X} \hat{W}$$

This transformation is mathematically equivalent but produces smoother activation distributions.

**Migration strength** $\alpha$ controls the trade-off:

$$s_j = \max(|X_j|)^\alpha / \max(|W_j|)^{1-\alpha}$$

- $\alpha = 0$: No migration, activations remain difficult
- $\alpha = 1$: Full migration, weights absorb all difficulty
- $\alpha = 0.5$: Balanced (typical setting)

**W8A8 deployment**: SmoothQuant enables INT8 quantization for both weights and activations:

+--------------------------+------------+---------------------+--------------------+---------------+
| **Configuration**        | **Memory** | **Prefill Speedup** | **Decode Speedup** | **Quality**   |
+=========================:+===========:+====================:+===================:+:==============+
| **FP16 (baseline)**      | 1x         | 1x                  | 1x                 | Baseline      |
| **W8A16 (weights only)** | 2x         | 1.3x                | 1.8x               | &lt;0.5% loss |
| **W8A8 (SmoothQuant)**   | 2x         | 1.8-2x              | 1.3-1.5x           | &lt;1% loss   |
+--------------------------+------------+---------------------+--------------------+---------------+

**Critical distinction**: W8A8 provides near 2x speedup for compute-bound prefill (large batch processing initial prompt), but only 1.3-1.5x speedup for memory-bound decode (generating tokens one at a time). LLM serving is typically decode-heavy, so real-world throughput improvements from W8A8 are often 1.3-1.7x rather than the theoretical 2x compute throughput of INT8 Tensor Cores.

### KV Cache Quantization {#sec-inference-kv-cache-quantization}

While weight quantization reduces model storage, the KV cache dominates memory consumption for long-context LLM serving. At 32K+ token context lengths, KV cache can exceed model weights in memory usage. KV cache quantization addresses this critical bottleneck.

**KV cache memory scaling**:

$$\text{KV Cache} = 2 \times \text{layers} \times \text{heads} \times d_{head} \times \text{seq\_len} \times \text{batch} \times \text{bytes}$$

For a 70B model (80 layers, 64 heads, 128 $d_{head}$) with 32K context in FP16:

$$\text{KV per sequence} = 2 \times 80 \times 64 \times 128 \times 32768 \times 2 = 85.9 \text{ GB}$$

A single long-context sequence consumes more memory than the model weights.

**Key insight**: KV cache values exhibit different distributions than model weights, enabling targeted quantization strategies:

- **Keys**: Relatively uniform distributions, tolerate aggressive quantization (2-4 bits)
- **Values**: More sensitive to quantization, require careful calibration (4-6 bits)

**KIVI (Key-Value cache quantization for Inference)**:

KIVI quantizes keys and values asymmetrically based on their sensitivity:

+---------------+----------------+--------------+---------------------+
| **Component** | **Precision**  | **Grouping** | **Quality Impact**  |
+:==============+===============:+:=============+:====================+
| **Keys**      | 2-bit          | Per-channel  | Minimal             |
| **Values**    | 4-bit          | Per-token    | &lt;0.5% perplexity |
| **Combined**  | ~3-bit average | Mixed        | &lt;1% perplexity   |
+---------------+----------------+--------------+---------------------+

**Memory impact of combined weight and KV quantization**:

+-------------------+-----------------+--------------------+------------------+
| **Configuration** | **Weight Size** | **KV Cache (32K)** | **Total Memory** |
+==================:+================:+===================:+=================:+
| **FP16/FP16**     | 140GB           | 86GB               | 226GB            |
| **W4A16/FP16**    | 35GB            | 86GB               | 121GB            |
| **W4A16/KV4**     | 35GB            | 21GB               | 56GB             |
+-------------------+-----------------+--------------------+------------------+

KV cache quantization enables 4x longer contexts or 4x higher batch sizes on the same hardware.

**Integration with PagedAttention**: Quantized KV cache requires quantization-aware block management:

```python
# Conceptual: vLLM with KV cache quantization
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    kv_cache_dtype="fp8",  # FP8 E4M3 for KV cache
    quantization="awq",  # 4-bit weights
)
```

With FP8 KV cache, the per-sequence memory drops from 2.5MB to 1.25MB per 1K tokens, doubling maximum concurrent sequences.

### Rotation-Based Quantization {#sec-inference-rotation-quantization}

Traditional quantization methods (GPTQ, AWQ, SmoothQuant) address outliers through compensation or migration. **Rotation-based quantization** [@ashkboos2024quarot] takes a fundamentally different approach: mathematically transforming the weight and activation space to eliminate outliers entirely.

**Key insight**: Outliers are artifacts of the coordinate basis representation. Rotating to a different basis spreads extreme values uniformly, making all values quantization-friendly.

**QuaRot (Quantization with Rotation)**:

QuaRot applies orthogonal Hadamard transforms to weights and activations:

$$X' = X \cdot H, \quad W' = H^T \cdot W$$

where $H$ is a Hadamard matrix (orthogonal, efficiently computable without storage).

**Why Hadamard transforms work**: For a vector with one extreme outlier, the Hadamard transform distributes that outlier's magnitude across all dimensions:

```
Original:  [1000, 1, 1, 1]     # One large outlier
Hadamard:  [252, 250, 250, 250] # Uniform distribution
```

After transformation, no single value dominates, enabling uniform quantization.

**Advantages over migration-based methods**:

+-----------------------+-----------------+---------------------------+
| **Aspect**            | **SmoothQuant** | **QuaRot**                |
+:======================+:================+:==========================+
| **Calibration data**  | Required        | Not required (data-free)  |
| **Minimum precision** | W8A8            | W4A4                      |
| **Runtime overhead**  | ~0%             | ~3% (Hadamard transforms) |
| **Outlier handling**  | Migration       | Elimination               |
+-----------------------+-----------------+---------------------------+

**Performance comparison**:

+-----------------+---------------+----------------------------+-----------------+
| **Method**      | **Precision** | **LLaMA-2-70B Perplexity** | **vs Baseline** |
+:================+==============:+===========================:+================:+
| **FP16**        | W16A16        | 3.12                       | Baseline        |
| **SmoothQuant** | W8A8          | 3.18                       | +0.06           |
| **GPTQ**        | W4A16         | 3.24                       | +0.12           |
| **QuaRot**      | W4A4          | 3.31                       | +0.19           |
+-----------------+---------------+----------------------------+-----------------+

QuaRot achieves 4-bit weights AND 4-bit activations with quality competitive to GPTQ's 4-bit weights only. This enables approximately 8x memory reduction versus FP16.

**SpinQuant**: An extension of QuaRot that learns optimal rotation matrices during a short fine-tuning phase, improving quality at the cost of training compute.

### Hardware-Deployment Co-design {#sec-inference-quant-hardware}

Quantization strategies must match target hardware capabilities. Different accelerators support different precisions with varying performance multipliers.

**NVIDIA Tensor Core Support**:

+----------------+-------------------+-------------------+---------------------+
| **Format**     | **Ampere (A100)** | **Hopper (H100)** | **Speedup vs FP16** |
+===============:+:==================+:==================+====================:+
| **FP16**       | Yes               | Yes               | 1x                  |
| **BF16**       | Yes               | Yes               | 1x                  |
| **INT8**       | Yes               | Yes               | 2x                  |
| **FP8 (E4M3)** | No                | Yes               | 2x                  |
| **INT4**       | Via CUTLASS       | Native            | 4x                  |
+----------------+-------------------+-------------------+---------------------+

**Memory bandwidth dominance**: For autoregressive LLM decode, memory bandwidth limits throughput since each token reads the entire model:

$$\text{Decode throughput} \propto \frac{\text{Memory bandwidth}}{\text{Model size in bytes}}$$

4-bit quantization delivers 4x throughput improvement for memory-bound decode, making it highly valuable despite modest compute gains.

**Deployment configurations**:

+------------------------+------------------------------------+-------------------------------+
| **Quantization**       | **Best For**                       | **Framework Support**         |
+=======================:+:===================================+:==============================+
| **W4A16 (GPTQ/AWQ)**   | Consumer GPUs, memory-constrained  | vLLM, TensorRT-LLM, llama.cpp |
| **W8A8 (SmoothQuant)** | INT8 accelerators, high throughput | TensorRT-LLM, ONNX Runtime    |
| **FP8**                | H100/H200 deployments              | TensorRT-LLM                  |
| **W4A4**               | Research, extreme compression      | Limited                       |
+------------------------+------------------------------------+-------------------------------+

### Framework Integration {#sec-inference-quant-frameworks}

Production serving frameworks integrate quantization with their batching and memory management systems.

**vLLM quantization**:

```python
from vllm import LLM

# Load AWQ-quantized model
llm = LLM(
    model="TheBloke/Llama-2-70B-AWQ",
    quantization="awq",
    dtype="float16",  # Activations in FP16
    gpu_memory_utilization=0.9,
)
```

vLLM automatically handles quantized weight loading, kernel selection for W4A16 GEMM operations, and KV cache management.

**TensorRT-LLM quantization**:

```bash
# Quantize model with AWQ
python quantize.py --model_dir /path/to/llama-70b \
                   --output_dir /path/to/llama-70b-awq \
                   --qformat int4_awq \
                   --calib_size 512
```

TensorRT-LLM generates optimized kernels for specific GPU architectures, fuses operations to minimize memory traffic, and supports both weight-only and W8A8 quantization.

**Quantization + PagedAttention**: Quantized models combine with PagedAttention for maximum memory efficiency:

$$\text{Max batch} = \frac{\text{GPU Memory} - \text{Quantized Weights}}{\text{KV Cache per Sequence}}$$

A 70B model with 4-bit weights requires approximately 35GB, leaving 45GB on an 80GB A100 for KV cache. With FP16 KV cache at 2.5MB per 1K tokens per sequence, this supports ~18 concurrent 1K-token sequences versus ~8 with FP16 weights.

### Quantization Selection Guidelines {#sec-inference-quant-selection}

Choosing the appropriate quantization method depends on deployment constraints:

**Decision framework**:

```
1. Is latency or throughput the primary goal?
   - Latency-sensitive: Prefer FP16/BF16 (no quantization overhead)
   - Throughput-oriented: Quantization typically beneficial

2. What hardware is available?
   - H100/H200: Consider FP8 (native support, minimal quality loss)
   - A100/A10G: W8A8 or W4A16 depending on workload
   - Consumer GPUs: W4A16 often necessary for memory

3. What quality requirements exist?
   - <0.5% degradation acceptable: AWQ 4-bit
   - <1% degradation acceptable: GPTQ 4-bit or SmoothQuant W8A8
   - No degradation acceptable: FP16/BF16 only

4. Is the model compute-bound or memory-bound?
   - Compute-bound (prefill): W8A8 provides 2x speedup
   - Memory-bound (decode): W4A16 provides 4x memory bandwidth
```

**Quantization impact on serving cost**:

+-------------------------+------------------------------------+
| **Configuration**       | **Cost per 1M tokens (estimated)** |
+========================:+===================================:+
| **FP16 on 8xA100**      | $2.40                              |
| **AWQ 4-bit on 4xA100** | $1.20                              |
| **AWQ 4-bit on 2xA100** | $0.60                              |
+-------------------------+------------------------------------+

Quantization can reduce serving costs by 2-4x while maintaining acceptable quality, making it essential for cost-effective LLM deployment.

## Multi-Tenancy and Isolation {#sec-inference-multitenancy}

The optimizations we have examined so far, from batching through sharding, caching, and quantization, focus on individual model deployments. This moves us to the platform level of the serving hierarchy, where the challenge shifts from optimizing a single model to managing multiple models, customers, and workloads on shared infrastructure.

Multi-tenancy enables efficient resource utilization but introduces challenges around isolation, fairness, and quality of service guarantees. A noisy neighbor consuming excessive resources can degrade performance for all other tenants.

This section examines the techniques for sharing inference infrastructure while maintaining isolation between tenants.

### The Multi-Tenancy Challenge {#sec-inference-multitenancy-challenge}

Multi-tenancy provides significant benefits:

- **Cost efficiency**: Sharing infrastructure across tenants improves utilization
- **Operational simplicity**: Fewer clusters to manage, monitor, and upgrade
- **Statistical multiplexing**: Aggregate traffic is more predictable than per-tenant traffic

However, sharing introduces risks as compared in @tbl-tenancy-comparison:

- **Noisy neighbors**: One tenant's burst traffic impacts others
- **Resource contention**: GPU memory, network bandwidth, CPU cycles
- **Security boundaries**: Tenant data must remain isolated
- **SLO complexity**: Different tenants have different requirements

+--------------------------+------------------------+------------------------+
| **Aspect**               | **Single-Tenant**      | **Multi-Tenant**       |
+:=========================+:=======================+:=======================+
| **Resource utilization** | 30-50%                 | 70-90%                 |
| **Cost per request**     | Higher                 | Lower                  |
| **SLO guarantees**       | Simple                 | Complex                |
| **Isolation**            | Complete               | Requires engineering   |
| **Operational overhead** | Higher (many clusters) | Lower (fewer clusters) |
+--------------------------+------------------------+------------------------+

: **Single vs Multi-Tenant Tradeoffs**: Multi-tenancy reduces cost but requires careful isolation engineering. {#tbl-tenancy-comparison}

### Noisy Neighbor Problems {#sec-inference-noisy-neighbor}

The noisy neighbor problem occurs when one tenant's workload degrades performance for others sharing the same infrastructure.

**GPU memory contention**: A tenant with unexpectedly long sequences consumes KV cache memory, forcing evictions that impact other tenants.

```
Scenario: 3 tenants sharing GPU with 60GB KV cache pool

Normal state:
  Tenant A: 20GB (200 sequences)
  Tenant B: 20GB (200 sequences)
  Tenant C: 20GB (200 sequences)

Noisy neighbor (Tenant C starts long-context requests):
  Tenant C: 45GB (150 sequences, longer context)
  Tenant A: 7.5GB (evicted to 75 sequences)
  Tenant B: 7.5GB (evicted to 75 sequences)

Impact: Tenants A and B see 62% reduction in batch size
```

**Network bandwidth saturation**: A tenant streaming many large responses saturates network bandwidth, increasing latency for all tenants.

**Compute interference**: GPU time-sharing between tenants introduces context-switching overhead and unpredictable latency.

::: {.callout-note title="Quantifying Noisy Neighbor Impact"}

Consider an inference platform serving 10 tenants on shared H100 GPUs:

**Baseline (even load)**:

- Each tenant: 100 QPS, 10ms P99 latency
- GPU utilization: 70%
- All SLOs met

**Noisy neighbor scenario** (Tenant 3 bursts to 500 QPS):

+------------------+----------+-----------------+----------------+
| **Tenant**       | **QPS**  | **P99 Latency** | **SLO Status** |
+=================:+=========:+================:+:===============+
| **Tenant 1**     | 100      | 25ms            | Violated       |
| **Tenant 2**     | 100      | 28ms            | Violated       |
| **Tenant 3**     | 500      | 45ms            | Violated       |
| **Tenants 4-10** | 100 each | 22-30ms         | Violated       |
+------------------+----------+-----------------+----------------+

Without isolation, one tenant's burst causes cascade failures for all tenants.

**With isolation** (per-tenant resource quotas):

+------------------+------------------+-----------------+--------------------------+
| **Tenant**       | **QPS (actual)** | **P99 Latency** | **SLO Status**           |
+=================:+=================:+================:+:=========================+
| **Tenant 1**     | 100              | 11ms            | Met                      |
| **Tenant 2**     | 100              | 11ms            | Met                      |
| **Tenant 3**     | 120 (throttled)  | 50ms            | Violated (only for them) |
| **Tenants 4-10** | 100 each         | 11ms            | Met                      |
+------------------+------------------+-----------------+--------------------------+

Isolation contains the impact to the offending tenant.

:::

### Resource Quotas and Fair Sharing {#sec-inference-quotas}

Resource quotas limit what each tenant can consume, preventing any single tenant from monopolizing shared resources.

**Hard quotas** enforce strict limits:

```python
class TenantQuota:
    max_concurrent_requests: int  # e.g., 100
    max_kv_cache_mb: int  # e.g., 20,000
    max_qps: int  # e.g., 1,000
    max_batch_tokens: int  # e.g., 50,000


def admit_request(tenant_id, request):
    quota = get_quota(tenant_id)
    usage = get_usage(tenant_id)

    if usage.concurrent >= quota.max_concurrent:
        return RateLimitError("concurrent request limit")
    if usage.kv_cache_mb >= quota.max_kv_cache_mb:
        return RateLimitError("memory limit")
    if usage.qps >= quota.max_qps:
        return RateLimitError("rate limit")

    return admit(request)
```

**Soft quotas** with fair sharing allow exceeding limits when resources are available:

```
Tenant quota: 100 QPS (soft limit)

When cluster is underutilized (50%):
  Tenant can burst to 200 QPS (2x quota)

When cluster is saturated (90%):
  Tenant limited to 100 QPS (quota enforced)
```

This approach maximizes utilization while protecting tenants during contention.

**Max-min fairness** allocates resources to maximize the minimum allocation:

```
Total capacity: 1000 QPS
Tenants: A (demand 300), B (demand 200), C (demand 800)
Total demand: 1300 QPS (exceeds capacity)

Max-min allocation:
1. Give each tenant equal share: 333 QPS
2. A needs only 300, donate 33 to others
3. B needs only 200, donate 133 to others
4. C receives donations: 333 + 33 + 133 = 499 QPS

Final: A=300, B=200, C=500
All demands met up to fair share, C limited proportionally
```

### Priority Scheduling {#sec-inference-priority-scheduling}

When tenants have different SLO requirements, priority scheduling ensures high-priority requests receive resources first.

**Priority classes**:

+-----------------+--------------------+-------------------------+------------------------+
| **Class**       | **Use Case**       | **Preemption**          | **Resource Guarantee** |
+:================+:===================+:========================+:=======================+
| **Critical**    | Revenue-generating | Can preempt lower       | 100% reserved          |
| **Standard**    | General traffic    | Can preempt best-effort | Weighted share         |
| **Best-effort** | Background, batch  | Cannot preempt          | No guarantee           |
+-----------------+--------------------+-------------------------+------------------------+

**Priority-aware queuing**:

```
Incoming requests sorted by priority, then arrival time:

Queue state:
  [Critical-001] [Critical-002] [Standard-001] [Standard-002] [BestEffort-001]
       ↑ Process first

New Critical-003 arrives:
  [Critical-001] [Critical-002] [Critical-003] [Standard-001] [Standard-002]
       ↑ Jumps ahead of Standard requests
```

**Preemption for LLM serving**:

When a critical request arrives but all GPU slots are occupied by lower-priority requests:

1. Select victim request(s) from lowest priority class
2. Pause victim's generation (save KV cache state)
3. Allocate GPU slot to critical request
4. When critical request completes, resume victim

```
Before preemption:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]

Critical request arrives:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]
                ↓ preempt
  GPU slots: [Critical E] [Standard B] [Standard C] [Standard D]
  Paused: Best-effort A (KV cache saved to CPU)

After Critical E completes:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]
  (A resumed from saved state)
```

### Bulkhead Pattern {#sec-inference-bulkhead}

The bulkhead pattern[^fn-bulkhead] [@nygard2007releaseit] physically isolates tenant workloads, preventing failures from propagating across tenants. Named after ship compartments that contain flooding to isolated sections.

[^fn-bulkhead]: **Bulkhead pattern origins**: The Titanic's bulkheads failed because they did not extend to the top of the ship, allowing water to spill over as the ship tilted. Modern software bulkheads learn from this: effective isolation requires complete separation (dedicated resources) rather than partial isolation (shared pools with quotas), at least for critical workloads.

**Deployment-level bulkheads**: Dedicate replicas to specific tenants or tenant groups.

::: {.callout-note title="Figure Placeholder: Bulkhead Isolation Patterns" collapse="true"}
```{.tikz}
% TODO: Block diagram contrasting Shared Pool vs Directed/Dedicated Pools
\node[draw, align=center] {Bulkhead Pattern\\Gold Tier -> Dedicated Replicas\\Standard Tier -> Shared Replicas w/ Quotas\\Failure in Shared Pool does not affect Gold Tier};
```
**Bulkhead Isolation Patterns**. To prevent cascading failures in multi-tenant systems, bulkheads isolate resources. **Deployment-level bulkheads** (shown) assign dedicated physical replicas to high-priority tenants, ensuring complete isolation. **Request-level bulkheads** enforce strict concurrency limits within shared processes. Like ship compartments, these boundaries ensure that a failure or resource exhaustion in one segment cannot sink the entire platform.
:::

Deployment-level bulkheads provide complete isolation for premium tenants, ensuring that their performance is never affected by other workloads. The tradeoff is lower overall resource utilization and increased operational overhead from managing dedicated infrastructure.

**Request-level bulkheads** limit the fraction of resources any single request can consume.

```
Per-request limits:
  max_input_tokens: 8,000
  max_output_tokens: 2,000
  max_execution_time: 30s

Prevents single request from consuming excessive resources.
```

**Failure isolation**: Errors in one tenant's requests do not affect others.

```
Tenant A sends malformed input causing model error:
  Without bulkhead: Error may crash shared inference worker
  With bulkhead: Error caught, only Tenant A's request fails
                 Other tenants continue normally
```

::: {.callout-note title="Bulkhead Configuration for API Tiers"}

Consider an LLM API with three service tiers:

**Enterprise tier**:

- Dedicated GPU pool (no sharing)
- Custom model fine-tuning
- 99.9% availability SLO
- Price: $$$

**Professional tier**:

- Shared GPU pool with guaranteed capacity
- Priority scheduling over free tier
- 99.5% availability SLO
- Price: $$

**Free tier**:

- Shared GPU pool, best-effort
- Rate limited (10 QPS)
- No SLO guarantee
- Price: Free

**Bulkhead configuration**:

```{.yaml}
tiers:
  enterprise:
    gpu_pool: "dedicated"
    isolation: "hardware"
    replicas: 8
    preemption: false

  professional:
    gpu_pool: "shared-premium"
    isolation: "resource-quota"
    quota_fraction: 0.7  # 70% of shared pool
    preemption: true

  free:
    gpu_pool: "shared-premium"
    isolation: "resource-quota"
    quota_fraction: 0.3  # 30% of shared pool
    preemption: false  # can be preempted
```

:::

### Model Isolation {#sec-inference-model-isolation}

When multiple models run on shared infrastructure, additional isolation is needed:

**Memory isolation**: Ensure one model's memory usage does not impact others.

```
GPU memory partitioning (80GB H100):

Model A: 40GB reserved (50%)
Model B: 30GB reserved (37.5%)
Shared pool: 10GB (12.5%)
```

**Compute isolation**: GPU time-sharing between models introduces latency variance. Options include:

- **MIG (Multi-Instance GPU)**[^fn-mig]: Hardware partitioning of A100/H100 into isolated GPU instances

[^fn-mig]: **Multi-Instance GPU (MIG)**: NVIDIA technology introduced with Ampere (A100) that partitions a GPU into up to 7 isolated instances, each with dedicated memory bandwidth and L2 cache. Unlike time-slicing, MIG provides hardware-level isolation with guaranteed performance, enabling secure multi-tenancy on expensive datacenter GPUs.
- **Time-slicing**: Cooperative scheduling between models (higher overhead)
- **Dedicated GPUs**: Each model gets dedicated hardware (lower utilization)

**Model loading isolation**: Loading one model should not evict another from GPU memory.

```python
class ModelManager:
    def load_model(self, model_id, priority):
        required_memory = get_model_size(model_id)
        available = get_free_gpu_memory()

        if required_memory > available:
            # Check if eviction would violate isolation
            evictable = get_evictable_memory(priority)
            if required_memory > evictable:
                raise InsufficientMemoryError(
                    "Cannot load without violating isolation constraints"
                )
            evict_lower_priority_models(priority, required_memory)

        load_to_gpu(model_id)
```

### Observability for Multi-Tenancy {#sec-inference-multitenancy-observability}

Effective multi-tenancy requires per-tenant visibility into resource consumption and performance:

**Per-tenant metrics**:

- Request count, latency distribution (P50, P95, P99)
- GPU memory usage, KV cache utilization
- Throttling events, preemption counts
- Error rates by error type

**Alerting thresholds**:

```yaml
alerts:
  - name: tenant_slo_violation
    condition: p99_latency > slo_target * 1.1
    for: 5m
    severity: warning

  - name: tenant_quota_exhaustion
    condition: usage > quota * 0.9
    for: 1m
    severity: warning

  - name: noisy_neighbor_detection
    condition: usage > fair_share * 2.0
    for: 5m
    severity: info
```

**Chargeback and attribution**: Track resource consumption for billing and capacity planning.

```
Tenant A monthly report:
  Total requests: 10,000,000
  GPU-seconds consumed: 50,000
  KV cache GB-hours: 2,500
  Network egress GB: 100

  Billed: $X based on consumption
```

## Autoscaling {#sec-inference-autoscaling}

Multi-tenancy, examined in the previous section, addresses how to share fixed infrastructure capacity across multiple workloads efficiently. Autoscaling addresses the complementary problem: how to adjust that capacity dynamically as aggregate demand changes.

Production inference systems experience traffic fluctuations that make static provisioning inefficient. Autoscaling dynamically adjusts capacity to match demand, reducing costs during low-traffic periods while maintaining SLOs during peaks. Without autoscaling, operators must choose between overprovisioning (paying for idle capacity during quiet periods) or underprovisioning (violating SLOs during traffic spikes). With effective autoscaling, infrastructure follows demand, though the cold start problem unique to GPU-based serving makes this significantly more challenging than traditional web service scaling.

This section examines autoscaling strategies for inference, with particular attention to the cold start problem and techniques for reducing its impact.

### Scaling Dimensions {#sec-inference-scaling-dimensions}

Inference systems can scale along multiple dimensions as compared in @tbl-scaling-dimensions:

**Horizontal scaling (replicas)**: Add or remove model replicas to adjust throughput.

$$\text{Capacity} = \text{Replicas} \times \text{Per-replica throughput}$$

**Vertical scaling (GPU type)**: Use more powerful GPUs for higher per-replica throughput.

$$\text{Cost efficiency} = \frac{\text{Throughput}}{\text{GPU cost}}$$

**Batch size scaling**: Adjust batch sizes to trade latency for throughput.

$$\text{Latency} \uparrow \text{ as } \text{Batch size} \uparrow \text{, Throughput} \uparrow$$

+-------------------------------+-------------+------------+--------------------------+
| **Scaling Type**              | **Latency** | **Cost**   | **Speed**                |
+:==============================+:============+:===========+:=========================+
| **Horizontal (add replicas)** | Unchanged   | Linear     | Slow (minutes)           |
| **Batch size**                | Increases   | Unchanged  | Instant                  |
| **Vertical (better GPU)**     | Unchanged   | Non-linear | Very slow (redeployment) |
+-------------------------------+-------------+------------+--------------------------+

: **Scaling Dimension Tradeoffs**: Each scaling approach has different characteristics. {#tbl-scaling-dimensions}

### The Cold Start Problem {#sec-inference-cold-start}

Unlike stateless web services that start in seconds, inference services have significant startup latency as defined in @eq-cold-start-time:

$$T_{cold start} = T_{provision} + T_{load} + T_{warmup}$$ {#eq-cold-start-time}

**Provisioning time** ($T_{provision}$): Acquiring a GPU instance takes 30 seconds to several minutes depending on cloud provider and GPU type.[^fn-cold-start]

[^fn-cold-start]: **Cold start in ML vs. serverless**: The cold start problem for GPU inference is 10-100x worse than for serverless functions. Lambda functions cold start in 100ms-1s; GPU inference cold starts in 1-10 minutes due to GPU allocation, model loading, and CUDA initialization. This fundamental difference makes predictive scaling and warm pools essential for GPU workloads.

**Model loading time** ($T_{load}$): Loading model weights from storage to GPU memory. For large models:

+------------------+---------------------+--------------------+
| **Model Size**   | **Load Time (SSD)** | **Load Time (S3)** |
+=================:+====================:+===================:+
| **7B (14GB)**    | 5s                  | 30s                |
| **70B (140GB)**  | 45s                 | 5min               |
| **175B (350GB)** | 2min                | 12min              |
+------------------+---------------------+--------------------+

**Warmup time** ($T_{warmup}$): First inference after loading is slower due to:

- JIT compilation of kernels
- CUDA context initialization
- Memory pool allocation
- Cache population

Warmup typically requires 10-30 dummy inferences, adding 5-30 seconds.

::: {.callout-note title="Cold Start Timeline for Llama-70B"}

Bringing up a new replica for Llama-70B on H100:

+-------------------------------+------------------+----------------+
| **Phase**                     | **Duration**     | **Cumulative** |
+:==============================+=================:+===============:+
| **Cloud API request**         | 5s               | 5s             |
| **GPU instance provisioning** | 60s              | 65s            |
| **Container startup**         | 10s              | 75s            |
| **Model download (S3)**       | 180s             | 255s           |
| **Model load to GPU**         | 45s              | 300s           |
| **CUDA warmup**               | 15s              | 315s           |
| **Readiness probe pass**      | 5s               | 320s           |
| **Total cold start**          | **5 min 20 sec** |                |
+-------------------------------+------------------+----------------+

**Implication**: Scaling decisions must anticipate demand 5+ minutes in advance. Reactive scaling alone cannot handle sudden traffic spikes.

:::

::: {.callout-note title="Figure Placeholder: Cold Start Latency Breakdown" collapse="true"}
```{.tikz}
% TODO: Horizontal stacked bar chart showing time components of cold start
\node[draw, align=center] {Cold Start Timeline\\Provisioning (60s) | Download (180s) | Load (45s) | Warmup (15s)\\Total: >5 mins};
```
**Anatomy of a Cold Start**. Bringing up a new GPU inference replica is a multi-step process taking minutes. While container startup is fast, provisioning the specialized instance and downloading massive model weights (100GB+) dominate the timeline. CUDA context initialization and "warmup" inference passes add further delay. This 5+ minute lag makes purely reactive scaling dangerous for handling sudden traffic spikes.
:::

### Reactive Scaling {#sec-inference-reactive-scaling}

Reactive scaling adjusts capacity based on observed metrics:

**Metric-based scaling**:

```yaml
autoscaling:
  metric: cpu_utilization  # or gpu_utilization, queue_depth
  target_value: 70%
  scale_up_threshold: 80%
  scale_down_threshold: 50%
  cooldown_period: 300s
```

**Queue-depth scaling**: Scale based on request queue length.

$$\text{Desired replicas} = \left\lceil \frac{\text{Queue depth}}{\text{Queue target}} \times \text{Current replicas} \right\rceil$$

**Latency-based scaling**: Scale to maintain latency SLO.

$$\text{Desired replicas} = \left\lceil \frac{P99_{observed}}{P99_{target}} \times \text{Current replicas} \right\rceil$$

**Reactive scaling limitations**:

1. Response delay: Cold start time prevents rapid response
2. Oscillation: Can create scale-up/scale-down cycles
3. Over-provisioning: Must provision for worst-case during cold start

::: {.callout-note title="Reactive Scaling Response Analysis"}

Consider traffic spike from 1000 to 3000 QPS:

**Current state**: 10 replicas, 100 QPS each, 70% utilization

**Target state**: 30 replicas for 3000 QPS

**Without pre-warming**:

- T=0: Spike detected, scale-up triggered
- T=0 to T=5min: Cold start for 20 new replicas
- T=0 to T=5min: Existing 10 replicas handle 3000 QPS (300 QPS each)
- Utilization: 210% (overloaded)
- P99 latency: 500ms+ (SLO violated)

**With pool of warm spares (5 replicas)**:

- T=0: Spike detected, warm spares activated immediately
- T=0: 15 replicas handle 3000 QPS (200 QPS each)
- T=0 to T=5min: Scale up 15 more replicas
- Utilization: 140% (elevated but manageable)
- P99 latency: 80ms (SLO maintained)

Warm spares provide buffer during cold start period.

:::

### Predictive Scaling {#sec-inference-predictive-scaling}

Predictive scaling anticipates demand before it occurs, initiating scaling ahead of traffic changes.

**Time-series forecasting**: Use historical patterns to predict future demand.

```python
def predict_demand(current_time, history):
    # Seasonal decomposition
    daily_pattern = extract_daily_seasonality(history)
    weekly_pattern = extract_weekly_seasonality(history)

    # Trend estimation
    trend = estimate_trend(history)

    # Forecast
    predicted = (
        daily_pattern[current_time.hour]
        * weekly_pattern[current_time.weekday()]
        * trend
    )
    return predicted
```

**Event-driven scaling**: Scale proactively for known events.

```yaml
scheduled_scaling:
  - event: "product_launch"
    time: "2024-03-15 09:00 UTC"
    target_replicas: 50  # 5x normal
    ramp_up: 30min  # Start scaling 30min before

  - event: "weekly_newsletter"
    cron: "0 10 * * 1"  # Every Monday 10am
    target_replicas: 20  # 2x normal
    duration: 2h
```

**Hybrid approach**: Combine predictive baseline with reactive adjustment.

$$\text{Target replicas} = \max(\text{Predicted}, \text{Reactive}) + \text{Buffer}$$

::: {.callout-note title="Predictive Scaling for Daily Traffic Patterns"}

A chatbot service shows predictable daily patterns:

+----------------+-----------------+---------------------+
| **Time (UTC)** | **Typical QPS** | **Replicas Needed** |
+===============:+================:+====================:+
| 00:00-06:00    | 500             | 5                   |
| 06:00-09:00    | 1500            | 15 (ramp up)        |
| 09:00-17:00    | 3000            | 30 (peak)           |
| 17:00-20:00    | 2000            | 20 (ramp down)      |
| 20:00-00:00    | 1000            | 10                  |
+----------------+-----------------+---------------------+

**Predictive schedule** (accounting for cold start):

+----------+--------------+---------------------+-----------------------+
| **Time** | **Action**   | **Replicas Active** | **Replicas Starting** |
+=========:+:=============+====================:+======================:+
| 05:30    | Scale up     | 5                   | +10 warming           |
| 06:00    | Traffic ramp | 15                  | -                     |
| 08:30    | Scale up     | 15                  | +15 warming           |
| 09:00    | Peak traffic | 30                  | -                     |
| 17:00    | Scale down   | 20                  | -10 terminating       |
| 20:00    | Scale down   | 10                  | -10 terminating       |
| 00:00    | Scale down   | 5                   | -5 terminating        |
+----------+--------------+---------------------+-----------------------+

**Cost comparison**:

- Reactive only: Must over-provision during ramp (45 replicas peak)
- Predictive: Right-sized provisioning (30 replicas peak)
- Savings: 33% GPU cost reduction

:::

::: {.callout-note title="Figure Placeholder: Predictive vs Reactive Scaling" collapse="true"}
```{.tikz}
% TODO: Line chart showing Traffic Curve, Reactive Capacity (lagging step function), and Predictive Capacity (smooth lead)
\node[draw, align=center] {Scaling Strategies\\Traffic Curve (Sine wave)\\Reactive: Steps up AFTER spike (Late)\\Predictive: Ramps up BEFORE spike (On time)};
```
**Predictive vs. Reactive Scaling**. Reactive scaling (dashed line) responds to traffic spikes after they occur, leading to periods of under-provisioning (red zones) where SLOs are violated due to cold start latency. Predictive scaling (solid line) anticipates known traffic patterns (like daily cycles) and begins provisioning capacity *before* the traffic arrives, eliminating the deficit and ensuring consistent performance.
:::

### Warm Pool Management {#sec-inference-warm-pools}

Maintaining a pool of pre-warmed replicas reduces effective cold start time:

**Warm pool sizing**:

$$\text{Warm pool size} = \frac{\text{Max expected spike}}{\text{Per-replica throughput}} \times \text{Headroom factor}$$

For example, if max spike is 2x normal and headroom factor is 1.5:

$$\text{Warm pool} = 2 \times 1.5 = 3\text{x minimum pool capacity}$$

**Warm pool cost**: Maintaining warm replicas costs money even when idle.

$$\text{Warm pool cost} = \text{Pool size} \times \text{GPU cost/hour} \times \text{Idle fraction}$$

Trade-off: More warm replicas = faster response but higher cost.

**Tiered warm pools**: Different readiness levels with different costs.

+----------+-----------------------------+-------------------+---------------------+
| **Tier** | **State**                   | **Response Time** | **Cost (relative)** |
+:=========+:============================+:==================+====================:+
| **Hot**  | GPU loaded, running         | Instant           | 100%                |
| **Warm** | GPU allocated, model loaded | 30s               | 60%                 |
| **Cold** | GPU not allocated           | 5+ min            | 0%                  |
+----------+-----------------------------+-------------------+---------------------+

```
Pool configuration:
  Hot: 2 replicas (instant burst capacity)
  Warm: 5 replicas (30s activation)
  Cold: Unlimited (cloud provider)

Scaling sequence:
  1. Activate Hot replicas immediately
  2. Activate Warm replicas within 30s
  3. Cold start new replicas if demand persists
```

### Scaling Response Time Analysis {#sec-inference-scaling-response}

The total time to respond to a scaling event is given by @eq-scaling-response:

$$T_{response} = T_{detect} + T_{decide} + T_{provision} + T_{warmup}$$ {#eq-scaling-response}

+------------------+--------------+-------------------------+
| **Component**    | **Duration** | **Optimization**        |
+:=================+=============:+:========================+
| **Detection**    | 10-60s       | Reduce metrics interval |
| **Decision**     | 1-5s         | Faster autoscaler       |
| **Provisioning** | 30s-5min     | Warm pools              |
| **Warmup**       | 5-30s        | Pre-compilation         |
+------------------+--------------+-------------------------+

**Optimizing each component**:

**Detection speed**: Use high-frequency metrics (1s vs 60s intervals) for faster detection. Trade-off: More metric volume, potentially noisier signals.

**Decision speed**: Pre-compute scaling plans based on predicted scenarios. When trigger occurs, execute pre-computed plan immediately.

**Provisioning speed**: Warm pools eliminate provisioning for anticipated demand. Spot/preemptible instances can reduce provisioning time (already running, just need allocation).

**Warmup speed**: Pre-compiled TensorRT engines skip JIT compilation. Lazy loading defers some initialization to first request.

### Spot and Preemptible Instances {#sec-inference-spot-instances}

Cloud providers offer discounted GPU instances[^fn-spot] that can be reclaimed with short notice:

[^fn-spot]: **Spot instance economics**: Spot instances represent cloud providers selling unused capacity at steep discounts (60-90%). The catch is 30-second to 2-minute termination notice. For inference, this means designing for graceful degradation: in-flight requests may need to be re-routed or failed, and KV cache state is lost. The cost savings justify this complexity for non-critical or burst traffic.

+----------------------+--------------+-------------------------+-----------------+
| **Instance Type**    | **Discount** | **Interruption Notice** | **Use Case**    |
+:=====================+=============:+:========================+:================+
| **On-demand**        | 0%           | Never                   | SLO-critical    |
| **Reserved**         | 30-60%       | Never                   | Steady baseline |
| **Spot/Preemptible** | 60-90%       | 30s-2min                | Burst capacity  |
+----------------------+--------------+-------------------------+-----------------+

**Graceful handling of spot termination**:

```python
def handle_spot_termination():
    # Received 2-minute warning
    # 1. Stop accepting new requests
    stop_accepting_requests()

    # 2. Complete in-flight requests (if possible)
    await complete_inflight(timeout=90)

    # 3. Save state for resumption elsewhere
    save_kv_cache_to_storage()

    # 4. Signal load balancer to redirect traffic
    deregister_from_loadbalancer()

    # 5. Terminate gracefully
    shutdown()
```

**Spot-aware architecture**:

```
Traffic distribution:

Request arrives
    │
    ▼
Load balancer
    │
    ├── 70% → On-demand replicas (guaranteed capacity)
    │
    └── 30% → Spot replicas (cost savings, may be interrupted)
```

Best-effort requests route to spot instances; SLO-critical requests use on-demand.

## Global Inference Infrastructure {#sec-inference-global}

Production inference systems serving global user bases must operate across multiple geographic regions. A user in Tokyo expects low-latency responses regardless of where models were trained or where the company headquarters is located. This section examines the architectural patterns for multi-region inference deployment.

### Why Multi-Region Matters {#sec-inference-global-why}

Single-region deployment creates fundamental limitations:

**Latency floor**: Network round-trip time (RTT) to distant users cannot be optimized away:

+-------------------+--------------------+-------------------------+
| **User Location** | **RTT to US-East** | **RTT to Local Region** |
+:==================+===================:+========================:+
| **New York**      | 10ms               | 10ms                    |
| **London**        | 75ms               | 10ms                    |
| **Tokyo**         | 150ms              | 10ms                    |
| **Sydney**        | 200ms              | 10ms                    |
+-------------------+--------------------+-------------------------+

For interactive applications (chatbots, autocomplete), these delays compound across multiple model calls per request.

**Availability**: Single-region deployment creates a single point of failure. Cloud region outages, while rare, affect all users simultaneously.

**Regulatory compliance**: Data residency requirements (GDPR, data sovereignty laws) may require processing user data within specific geographic boundaries.

### Multi-Region Architecture Patterns {#sec-inference-global-patterns}

**Pattern 1: Global load balancing with regional replicas**

```
                    Global Load Balancer
                    (Latency-based routing)
                           │
         ┌─────────────────┼─────────────────┐
         ▼                 ▼                 ▼
    US-East           EU-West           Asia-Pacific
    ┌─────────┐       ┌─────────┐       ┌─────────┐
    │ vLLM    │       │ vLLM    │       │ vLLM    │
    │ Replicas│       │ Replicas│       │ Replicas│
    └─────────┘       └─────────┘       └─────────┘
         │                 │                 │
         └─────────────────┼─────────────────┘
                           ▼
                    Model Registry
                    (Synchronized)
```

Each region runs independent inference replicas with identical models. The global load balancer routes users to the nearest region based on latency.

**Key considerations**:

- **Model synchronization**: Model updates must propagate to all regions. Options include:
  - Push-based: Central registry pushes to all regions (simple, potential inconsistency window)
  - Pull-based: Regions poll for updates (higher latency, guaranteed consistency)
  - Hybrid: Push notification + pull verification

- **Version consistency**: During model rollouts, different regions may briefly serve different versions. For most applications this is acceptable; for applications requiring strict consistency, implement version pinning in request routing.

**Pattern 2: Edge caching with central inference**

For models too large to replicate globally, cache responses at the edge:

```
User → Edge Cache (CDN) → Regional Proxy → Central Inference
           │                    │
           └── Cache hit ───────┘
               (< 10ms)

           └── Cache miss ──────────────────→
               (Full latency, populate cache)
```

**Effectiveness depends on request repeatability**:

+---------------------+--------------------+-----------------+
| **Workload**        | **Cache Hit Rate** | **Suitability** |
+:====================+===================:+:================+
| **Autocomplete**    | 60-80%             | Excellent       |
| **FAQ chatbot**     | 40-60%             | Good            |
| **Open-ended chat** | 5-15%              | Poor            |
| **Code generation** | 20-40%             | Moderate        |
+---------------------+--------------------+-----------------+

Semantic caching[^fn-semantic-cache] (caching based on embedding similarity rather than exact match) can improve hit rates for open-ended workloads.

[^fn-semantic-cache]: **Semantic caching**: Unlike traditional exact-match caching, semantic caching returns cached responses for queries that are semantically similar to previously seen queries. This requires embedding the query and searching a vector database for similar cached queries. The trade-off is increased complexity and potential for incorrect cache hits when semantically similar queries should produce different responses.

**Pattern 3: Federated inference with model sharding**

For the largest models, shard across regions:

```
User request
    │
    ▼
Request Router
    │
    ├── Layers 1-40  → US-East GPUs
    │
    └── Layers 41-80 → EU-West GPUs

    Pipeline parallelism across regions
```

This pattern is rarely practical due to inter-region latency dominating compute time, but may apply for extremely large models where no single region has sufficient GPU capacity.

### Cross-Region Failover {#sec-inference-global-failover}

When a region becomes unavailable, traffic must reroute to healthy regions:

**Active-active failover**:

```python
# Simplified global routing logic
def route_request(user_region, request):
    primary = get_nearest_healthy_region(user_region)
    secondary = get_second_nearest_healthy_region(user_region)

    try:
        return call_region(primary, request, timeout=2.0)
    except (Timeout, RegionUnavailable):
        # Failover with increased latency
        return call_region(secondary, request, timeout=5.0)
```

**Failover considerations for stateful LLM serving**:

- **Session affinity loss**: Users mid-conversation lose KV cache state. The fallback region must regenerate context from conversation history.
- **Capacity spike**: The receiving region sees sudden traffic increase. Pre-provision headroom (typically 30-50% over steady-state) or accept degraded latency during failover.
- **Gradual recovery**: When the failed region recovers, gradually shift traffic back to avoid oscillation.

### Global Model Deployment {#sec-inference-global-deployment}

Deploying model updates across regions requires careful coordination:

**Phased rollout strategy**:

```
1. Deploy to canary region (e.g., 1% traffic in US-East)
2. Monitor metrics for 1 hour
3. If healthy, deploy to remaining US-East replicas
4. Monitor for 4 hours
5. Deploy to EU-West (different user population)
6. Monitor for 4 hours
7. Deploy to Asia-Pacific
8. Complete rollout
```

**Rollback across regions**:

If issues are detected after partial deployment:

```
Region Status:
  US-East:      v2.1 (new) ← Issue detected
  EU-West:      v2.0 (old)
  Asia-Pacific: v2.0 (old)

Action: Rollback US-East to v2.0
  - Switch traffic to v2.0 replicas
  - Maintain v2.1 replicas for debugging
  - Do not proceed with EU-West deployment
```

**Metrics for global deployment health**:

+-------------------+----------------+---------------------------+
| **Metric**        | **Per-Region** | **Global**                |
+:==================+:===============+:==========================+
| **Error rate**    | &lt; 0.1%      | &lt; 0.1%                 |
| **P99 latency**   | &lt; target    | &lt; 2x single-region     |
| **Throughput**    | Stable         | Stable                    |
| **Model quality** | Within bounds  | Consistent across regions |
+-------------------+----------------+---------------------------+

### Cost Optimization Across Regions {#sec-inference-global-cost}

GPU pricing varies by region. Optimize placement for cost while meeting latency requirements:

+-------------+---------------------+---------------+-------------------------+
| **Region**  | **H100 Spot Price** | **On-Demand** | **Latency to US Users** |
+:============+====================:+==============:+========================:+
| **US-East** | $2.50/hr            | $4.00/hr      | 10-50ms                 |
| **US-West** | $2.30/hr            | $3.80/hr      | 30-70ms                 |
| **EU-West** | $2.80/hr            | $4.20/hr      | 75-100ms                |
+-------------+---------------------+---------------+-------------------------+

**Cost-aware routing**:

For latency-tolerant workloads (batch inference, background processing), route to the cheapest available region:

```python
def route_batch_request(request):
    if request.priority == "low":
        # Route to cheapest region with capacity
        return get_cheapest_region_with_capacity()
    else:
        # Route to nearest region
        return get_nearest_region(request.user_location)
```

This can reduce costs by 20-40% for batch workloads while maintaining SLOs for interactive traffic.

## Case Studies {#sec-inference-case-studies}

The techniques presented throughout this chapter come together in production systems serving billions of requests daily. This section examines four case studies that illustrate different points in the inference design space: Meta's recommendation serving (high volume, low latency), OpenAI's API infrastructure (LLM-focused), Google's search ranking (ensemble models), and TikTok's multimodal recommendation (video understanding combined with user modeling).

Each case study demonstrates how the principles of batching, sharding, load balancing, and autoscaling combine to meet specific requirements.

### Meta Recommendation Serving {#sec-inference-case-meta}

Meta's recommendation infrastructure serves predictions for feeds, ads, and content ranking across Facebook, Instagram, WhatsApp, and Messenger. This represents one of the largest production inference deployments in the world.

**Scale and requirements**:

- Request volume: Billions of requests per day
- Latency target: <10ms P99
- Model diversity: Hundreds of model variants
- Feature cardinality: Trillions of unique entities

**Architecture overview**:

```
User request → Feature collection → Embedding lookup → Model inference → Response

                    │                     │                  │
                    ▼                     ▼                  ▼
             Feature Store        Embedding Servers      GPU Inference
             (CPU, DRAM)         (CPU + SSD, 1000s)     (GPU, 100s)
```

**Key design decisions**:

**Embedding sharding at scale**: Embedding tables total over 100TB, requiring 1000+ shards. Meta uses a hybrid sharding strategy:

- Hot embeddings (top 1%): Replicated across memory on all inference servers
- Warm embeddings (next 10%): Column-sharded with 8-way parallelism
- Cold embeddings (remaining 89%): Row-sharded with consistent hashing, SSD-backed

This reduces embedding lookup latency from 50ms (naive) to 2ms through batching and locality optimization.

**Feature-parallel batching**: Instead of batching entire requests, Meta batches at the feature level. Each inference request triggers 5,000+ embedding lookups, but these lookups are batched across requests within a 1ms window. This achieves 90%+ memory bandwidth utilization on embedding servers.

**GPU-CPU hybrid architecture**: Dense model computation (ranking towers) runs on GPUs, while sparse embedding lookups run on CPU servers with large memory and SSD storage. This matches hardware to workload characteristics:

+------------------------+--------------+-------------+----------------+
| **Component**          | **Hardware** | **Latency** | **Throughput** |
+:=======================+:=============+============:+===============:+
| **Embedding lookup**   | CPU + SSD    | 2ms         | 50M lookups/s  |
| **Feature processing** | CPU          | 1ms         | 10M ops/s      |
| **Dense ranking**      | GPU          | 1.5ms       | 100K infs/s    |
+------------------------+--------------+-------------+----------------+

**Lessons learned**:

1. Embedding lookup, not model inference, often dominates latency for recommendation systems
2. Feature-parallel batching achieves higher efficiency than request-level batching
3. Hybrid CPU-GPU architectures match hardware to workload characteristics

### OpenAI API Infrastructure {#sec-inference-case-openai}

OpenAI's API serves GPT-4, GPT-3.5-turbo, and other models to millions of developers. The infrastructure must handle highly variable request sizes (from 10 tokens to 128K tokens) while maintaining quality of service across diverse workloads.

**Scale and requirements**:

- Request volume: Millions of requests per hour
- Latency target: Time-to-first-token (TTFT) <2s, throughput varies by model
- Model sizes: 7B to 175B+ parameters
- Context lengths: Up to 128K tokens

**Architecture overview**:

```
API Gateway → Rate Limiting → Request Router → Model Cluster → Response Streaming

                                     │
                                     ▼
                              ┌─────────────┐
                              │ Model Pool  │
                              │ ┌─────────┐ │
                              │ │ GPT-4   │ │
                              │ │ 8xH100  │ │
                              │ └─────────┘ │
                              │ ┌─────────┐ │
                              │ │GPT-3.5  │ │
                              │ │ 4xA100  │ │
                              │ └─────────┘ │
                              └─────────────┘
```

**Key design decisions**:

**Continuous batching with chunked prefill**: OpenAI was an early adopter of continuous batching (Orca-style) to maintain high GPU utilization despite variable output lengths. Chunked prefill bounds decode latency by processing long prompts in chunks that interleave with ongoing generation.

+--------------------------+---------------------+------------------------+
| **Batching Strategy**    | **GPU Utilization** | **TTFT (128K prompt)** |
+:=========================+====================:+=======================:+
| **Static batching**      | 45%                 | 30s (blocked)          |
| **Continuous batching**  | 75%                 | 30s (blocked)          |
| **Continuous + chunked** | 85%                 | 3s (streamed)          |
+--------------------------+---------------------+------------------------+

**Tensor parallelism for large models**: GPT-4 class models require 8-way or greater tensor parallelism for memory capacity and latency:

- 8xH100 per GPT-4 shard group
- NVLink for intra-node communication
- Consistent hashing for session affinity (KV cache reuse)

**Multi-tier rate limiting**: OpenAI implements rate limiting at multiple levels to prevent noisy neighbors:

- Per-API-key request rate limits
- Per-API-key token-per-minute limits
- Organization-level capacity quotas
- Global model capacity limits

**Dynamic capacity allocation**: During peak demand, OpenAI shifts capacity between models based on queue depth:

```
if gpt4_queue_depth > threshold:
    # Migrate some GPT-3.5 capacity to GPT-4
    reallocate_cluster_capacity(from="gpt-3.5", to="gpt-4", fraction=0.2)
```

**Lessons learned**:

1. Continuous batching is essential for LLM serving at scale
2. Prefix caching provides 2-3x efficiency for conversational workloads
3. Multi-tier rate limiting prevents cascade failures from traffic spikes

### Google Search Ranking {#sec-inference-case-google}

Google Search uses ensemble serving to combine multiple specialized models for query understanding, document relevance, and result ranking. This represents a different inference pattern: many smaller models coordinated for each request rather than one large model.

**Scale and requirements**:

- Request volume: Billions of searches per day
- Latency target: <200ms end-to-end
- Model count: Dozens of models per query
- Result processing: Thousands of documents per query

**Architecture overview**:

::: {.callout-note title="Figure Placeholder: Ranking Cascade Funnel" collapse="true"}
```{.tikz}
% TODO: Funnel diagram showing candidate count reduction and model complexity increase
\node[draw, align=center] {Ranking Cascade\\Retrieval (1M Items, Simple) ->\\L1 Ranking (10K Items, Linear) ->\\L2 Ranking (100 items, Small NN) ->\\Final Ranking (10 items, Large Ensemble)};
```
**Ranking Cascade Architecture**. To optimize latency and cost, search and recommendation systems use a cascade of increasingly complex models. Early stages (Retrieval) use cheap, fast models (embeddings, linear) to filter millions of candidates down to thousands. Later stages use expensive, high-precision models (transformers, ensembles) to rank the remaining few candidates. This funnel structure ensures that heavy computation is spent only on the most promising items.
:::

**Key design decisions**:

**Cascading model architecture**: Rather than running one expensive model on all candidates, Google uses a ranking cascade:

+----------------------+----------------------+--------------------+--------------------+
| **Stage**            | **Model Complexity** | **Candidates**     | **Latency Budget** |
+=====================:+:=====================+===================:+===================:+
| **L0 (Retrieval)**   | Embedding lookup     | 1,000,000 → 10,000 | 10ms               |
| **L1 (First pass)**  | Linear model         | 10,000 → 1,000     | 20ms               |
| **L2 (Second pass)** | Small transformer    | 1,000 → 100        | 50ms               |
| **L3 (Final rank)**  | Large ensemble       | 100 → 10           | 100ms              |
+----------------------+----------------------+--------------------+--------------------+

This achieves 100x cost reduction compared to running L3 on all candidates.

**Speculative execution**: Given tight latency budgets, Google uses speculative execution for model ensembles:

```
# Instead of sequential:
#   q1 = model1(query)
#   q2 = model2(query)
#   q3 = model3(query, q1, q2)

# Speculative parallel:
async_q1 = async model1(query)
async_q2 = async model2(query)
async_q3 = async model3(query, predicted_q1, predicted_q2)

# Use actual results if they arrive in time, otherwise use speculative
```

**Custom TPU infrastructure**: Google runs ranking models on TPUs[^fn-tpu-search] optimized for transformer inference. TPU pods provide:

[^fn-tpu-search]: **TPU for inference**: While TPUs are often associated with training, Google's search infrastructure leverages TPU's deterministic performance and high memory bandwidth for inference. Unlike GPUs where utilization varies with batching, TPUs provide consistent latency through their systolic array architecture, critical for meeting strict search latency SLOs.

- 2D mesh topology for efficient AllReduce
- High memory bandwidth for attention operations
- Custom quantization for serving efficiency

**Deadline-aware scheduling**: Each sub-request carries a deadline, and workers prioritize by deadline proximity:

```
Worker queue: [Doc1: 50ms left] [Doc2: 30ms left] [Doc3: 80ms left]
                                      ↑ Process first

If deadline will be missed:
  Return cached/default result rather than timing out
```

**Lessons learned**:

1. Ranking cascades provide dramatic cost reduction for large candidate sets
2. Deadline propagation and priority scheduling are essential for ensemble serving
3. Custom hardware (TPU) enables efficiency that commodity GPUs cannot match

### TikTok Multimodal Recommendation {#sec-inference-case-tiktok}

TikTok's recommendation system combines video understanding (vision) with user modeling (recommendation) for personalized content ranking. This represents a multimodal inference challenge where different model types must coordinate.

**Scale and requirements**:

- Request volume: Millions of video rankings per second
- Latency target: <50ms P99
- Content volume: Millions of new videos daily
- Modalities: Video, audio, text, user signals

**Architecture overview**:

```
User request → User embedding → Candidate videos → Video understanding → Ranking

                    │                  │                    │
                    ▼                  ▼                    ▼
             User Tower          Video Cache          Vision Models
           (Transformer)        (Pre-computed)        (On-demand)
```

**Key design decisions**:

**Two-tower architecture with caching**[^fn-two-tower]: TikTok separates user understanding (online) from content understanding (offline):

[^fn-two-tower]: **Two-tower architecture**: A design pattern where user features and item features are encoded by separate "towers" (neural networks) into embeddings, then combined for scoring. This separation enables pre-computing item embeddings offline, reducing online inference to user tower computation plus a fast dot product. The architecture trades some model expressiveness for dramatic serving efficiency.

+-----------------+----------------------+-------------+--------------+
| **Tower**       | **Update Frequency** | **Latency** | **Compute**  |
+:================+:=====================+:============+:=============+
| **User tower**  | Real-time            | 5ms         | GPU (online) |
| **Video tower** | Hourly               | N/A         | GPU (batch)  |
+-----------------+----------------------+-------------+--------------+

Video embeddings are pre-computed and cached, eliminating vision inference from the critical path for most requests. Only new videos (uploaded within the hour) require online vision inference.

**Hybrid CPU-GPU inference**: Like Meta, TikTok uses CPU for embedding operations and GPU for dense model computation:

```
User features → CPU preprocessing (1ms)
             → Embedding lookup (2ms, CPU+DRAM)
             → Dense ranking (10ms, GPU)
             → Response formatting (1ms)
```

**Priority-based video analysis**: New video content is processed with different priorities:

+----------------+---------+------------------------------+
| **Priority**   | **SLA** | **Use Case**                 |
+:===============+========:+:=============================+
| **Critical**   | 5 min   | Creator with large following |
| **Standard**   | 30 min  | Normal uploads               |
| **Background** | 2 hours | Bulk/imported content        |
+----------------+---------+------------------------------+

This ensures popular creators' content reaches recommendations quickly while managing compute costs.

**Multimodal fusion**: TikTok combines multiple understanding modalities through late fusion:

```
Video embedding (512d) ─┐
Audio embedding (256d) ─┼─ Concat → Fusion MLP → Final embedding (256d)
Text embedding (256d)  ─┘
```

This allows independent updates to each modality's model without retraining the full system.

**Lessons learned**:

1. Separating online and offline components enables aggressive caching
2. Two-tower architectures scale better than joint models for user-item systems
3. Priority-based processing balances freshness against compute cost

### Cross-Cutting Observations {#sec-inference-case-observations}

Several patterns emerge across these case studies as summarized in @tbl-case-studies-summary:

**Separation of concerns**: All systems separate embedding/retrieval from ranking/generation. This enables specialized optimization for each component.

**Hybrid architectures**: No system uses GPUs exclusively. CPU+GPU combinations match hardware to workload characteristics.

**Caching at multiple levels**: Embedding caching, result caching, and intermediate representation caching all appear. Caching reduces compute at the cost of staleness.

**Progressive refinement**: Cascades and early-exit strategies reduce average compute by quickly filtering unlikely candidates.

**Deadline awareness**: All systems propagate deadlines and make explicit tradeoffs between quality and latency when under pressure.

+------------+-----------------------+---------------------------+
| **System** | **Primary Technique** | **Key Innovation**        |
+:===========+:======================+:==========================+
| **Meta**   | Embedding sharding    | Feature-parallel batching |
| **OpenAI** | Continuous batching   | Chunked prefill           |
| **Google** | Ranking cascade       | Speculative execution     |
| **TikTok** | Two-tower caching     | Multimodal fusion         |
+------------+-----------------------+---------------------------+

: **Case Study Summary**: Each system innovates on a core technique matched to its workload characteristics. {#tbl-case-studies-summary}

## Fallacies and Pitfalls {#sec-inference-fallacies-pitfalls}

The techniques presented throughout this chapter address real engineering challenges, but misconceptions about inference at scale remain common. Recognizing these fallacies and pitfalls helps practitioners avoid costly mistakes in system design and capacity planning.

**Fallacy: Inference at scale is synonymous with LLM serving.**

This misconception, reinforced by current discourse, leads to over-focus on LLM-specific techniques while ignoring the broader inference landscape. By request volume, recommendation systems constitute 80-90% of production inference at major technology companies, with vision and other models comprising most of the remainder. LLMs currently represent 1-5% of requests, though this is growing. A practitioner who only understands continuous batching and KV cache management will be unprepared for the feature-parallel batching and embedding sharding that dominate production inference. Technique selection must match the actual workload.

**Pitfall: Using training infrastructure for production serving.**

Training and serving have fundamentally different requirements. Training optimizes for aggregate throughput over hours or days; serving optimizes for per-request latency under strict SLOs. Training tolerates batch sizes of thousands; serving often requires batch sizes in single digits. Training accepts checkpoint-based recovery; serving requires graceful failover without user impact. Teams that deploy training clusters for serving often discover unacceptable latency variance, poor resource utilization, and difficulty meeting SLOs. Purpose-built serving infrastructure with appropriate batching, load balancing, and autoscaling is essential.

**Fallacy: Continuous batching solves all LLM serving problems.**

Continuous batching dramatically improves GPU utilization for LLM serving, but it addresses only one dimension of the problem. Prefill remains a bottleneck for long contexts, as the quadratic attention computation cannot be avoided regardless of how subsequent decode iterations are batched. KV cache memory, not compute, often limits batch size. Network bandwidth between sharded model components can dominate latency for large models. Continuous batching is necessary but not sufficient for efficient LLM serving.

**Pitfall: Sizing capacity based on average throughput.**

The nonlinear relationship between utilization and latency (from queuing theory) means that systems provisioned for average load will violate SLOs during traffic peaks. At 80% average utilization, a modest 25% traffic spike pushes utilization above 100%, causing unbounded queue growth and latency degradation. The cold start problem exacerbates this: by the time new capacity is available (5+ minutes for GPU instances), the spike may have caused significant SLO violations. Capacity planning must account for peak load plus headroom, not average load.

**Fallacy: Load balancing does not matter much for inference.**

Simple load balancing strategies like round-robin seem adequate until examined quantitatively. Random assignment produces maximum queue lengths of $O(\log n / \log \log n)$ across $n$ servers. Power-of-two-choices reduces this to $O(\log \log n)$, an exponential improvement. For a 1,000-server cluster, this translates from ~4-5 requests maximum queue to ~2 requests. At tail latencies that matter for SLOs, this difference is substantial. For LLM workloads with highly variable request durations, least-connections further improves balance. The choice of load balancing algorithm has first-order impact on system performance.

**Pitfall: Ignoring the serving tax in latency budgets.**

Distributed inference introduces overhead absent from single-machine serving: network round-trips, serialization, load balancer decisions, and coordination for sharded models. This "serving tax" often consumes 10-30% of the latency budget. A team that achieves 70ms model inference on a single GPU may be surprised when end-to-end latency reaches 100ms in production due to these overheads. Latency budgets must explicitly account for distribution overhead, not just compute time.

**Fallacy: More GPU memory always means more batch size and throughput.**

While larger GPU memory enables larger batches for models that fit in memory, the bottleneck often shifts before memory is exhausted. Memory bandwidth limits throughput for bandwidth-bound operations (LLM decode). Compute limits throughput for compute-bound operations (prefill, vision inference). Adding memory to a bandwidth-bound workload provides no benefit. Understanding whether the workload is compute-bound, memory-bound, or capacity-bound guides appropriate resource allocation.

**Pitfall: Neglecting multi-tenancy isolation until production.**

In development and staging, single-tenant deployments work well. In production, noisy neighbors cause sudden, unpredictable performance degradation that is difficult to diagnose and resolve. A tenant bursting to 5x normal traffic can degrade latency for all other tenants on shared infrastructure. Resource quotas, priority scheduling, and bulkhead isolation must be designed into the system from the start, not retrofitted after production incidents.

::: {.callout-important title="Three Things to Remember"}

1. **Serving cost dominates training cost over a model's lifetime.** For high-volume applications, serving cost exceeds training cost by 100x or more. Every percentage point of serving efficiency improvement yields ongoing cost reduction. Optimize serving ruthlessly.

2. **Different model types require fundamentally different batching strategies.** Static batching for vision, continuous batching for LLMs, feature-parallel batching for recommendation systems. There is no universal optimal strategy. Match technique to workload.

3. **Power-of-two-choices provides exponential load balancing improvement.** Maximum queue length improves from $O(\log n / \log \log n)$ to $O(\log \log n)$ with minimal overhead (two probes per request). This simple technique should be standard for any distributed inference deployment.

:::

## Summary {#sec-inference-summary}

Inference at scale transforms single-machine serving foundations into distributed systems that handle billions of requests across global infrastructure. Throughout this chapter, we have developed the principles governing this transformation, from batching strategies matched to model architectures through load balancing algorithms that provide exponentially better performance.

The serving hierarchy provides our organizing framework: request-level optimizations (batching, caching), replica-level optimizations (GPU utilization, memory management), service-level optimizations (load balancing, routing), and platform-level optimizations (multi-tenancy, scheduling). Each level has distinct metrics and techniques, and effective inference systems optimize at all levels simultaneously.

We began by establishing when distributed inference becomes necessary: memory exhaustion for models exceeding single-GPU capacity, throughput requirements beyond single-machine limits, or latency targets that demand parallel computation. The fundamental inversion from training's throughput focus to serving's latency imperative shapes every subsequent design decision. We quantified the serving tax, the overhead of distribution that must be budgeted within latency constraints, and demonstrated that serving cost dominates training cost over a model's operational lifetime.

Batching strategies vary dramatically across model types. Vision models benefit from large static or dynamic batches that maximize GPU utilization. LLMs require continuous batching to handle variable output lengths efficiently, with chunked prefill to bound decode latency during long prompt processing. Recommendation systems use feature-parallel batching that aligns with embedding shard architectures rather than request-level batching. Streaming applications for speech and video cannot tolerate batching delay at all. Selecting the wrong batching strategy for a workload can reduce throughput by 3-4x or violate latency SLOs entirely.

Model sharding distributes computation across devices when single-GPU memory or latency constraints require it. Tensor parallelism achieves latency reduction through parallel attention and feed-forward computation, with all-reduce synchronization between layers. Pipeline parallelism distributes layers across stages for memory relief without latency benefit for single requests. Expert parallelism handles mixture-of-experts models with dynamic routing. Embedding sharding scales recommendation systems' trillion-parameter tables across thousands of servers. Each strategy has distinct communication patterns and overhead characteristics that must match deployment constraints.

Load balancing determines how requests reach replicas, with seemingly simple choices producing dramatically different performance. Random assignment yields maximum queue lengths of $O(\log n / \log \log n)$, while power-of-two-choices achieves $O(\log \log n)$, an exponential improvement from a trivial modification. Consistent hashing enables session affinity for stateful workloads like LLM conversations. Circuit breakers and backpressure mechanisms protect systems from cascading failures when replicas become overloaded.

KV cache management has emerged as a critical bottleneck for LLM serving. PagedAttention eliminates memory fragmentation through virtual memory techniques, achieving 2.5-4x throughput improvement. Prefix caching shares common prompt prefixes across requests, reducing both compute and memory for conversational workloads. Speculative decoding breaks the sequential generation bottleneck by using draft models to predict multiple tokens verified in parallel. These techniques combine to make previously impractical LLM deployments economically viable.

Multi-tenancy enables cost-effective infrastructure sharing but requires careful isolation engineering. Noisy neighbors can degrade performance for all tenants without proper resource quotas, priority scheduling, and bulkhead isolation. The tradeoff between utilization and isolation must be explicitly managed based on service tier and SLO requirements.

Autoscaling addresses traffic fluctuations but faces the cold start problem unique to GPU-based inference. Model loading times of minutes make reactive scaling insufficient for sudden traffic spikes. Predictive scaling, warm pools, and tiered readiness levels combine to provide rapid scaling response while managing cost. Spot instances offer significant savings for burst capacity when graceful handling of interruptions is implemented.

The case studies demonstrate how these principles combine in production: Meta's feature-parallel batching and embedding sharding for recommendation, OpenAI's continuous batching and tensor parallelism for LLMs, Google's ranking cascades and deadline-aware scheduling for search, and TikTok's two-tower caching for multimodal recommendation. Each system innovates on techniques matched to its specific workload characteristics, but all share common patterns of separation of concerns, hybrid architectures, and progressive refinement.

::: {.callout-important title="Key Takeaways"}

* Serving cost dominates training cost by 100x or more for high-volume applications, making serving optimization the primary driver of ML infrastructure economics

* Different model types require fundamentally different batching strategies: static/dynamic for vision, continuous for LLMs, feature-parallel for recommendation

* The serving hierarchy (request, replica, service, platform) provides a framework for decomposing optimization opportunities at each level

* Power-of-two-choices load balancing achieves exponentially better queue balance ($O(\log \log n)$ vs $O(\log n / \log \log n)$) with minimal overhead

* KV cache management through PagedAttention and prefix caching enables 2-4x LLM serving efficiency improvement

* Cold start times of 5+ minutes for GPU-based serving require predictive scaling and warm pools, not just reactive autoscaling

* Production inference is dominated by recommendation systems (80-90% of requests), not LLMs, requiring model-type diversity in technique selection

:::

The techniques in this chapter enable inference systems that scale from single machines to global deployments. The next chapter, @sec-edge-intelligence, examines the other end of the deployment spectrum: inference at the edge, where devices with limited compute, memory, and power must still deliver predictions reliably. The distributed coordination patterns established here inform edge-cloud architectures that partition computation between constrained edge devices and capable cloud infrastructure.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
