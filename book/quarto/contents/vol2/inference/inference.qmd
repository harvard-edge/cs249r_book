---
---

# Inference at Scale {#sec-inference-at-scale}

::: {layout-narrow}
::: {.column-margin}
_Gemini Pro 3 Prompt: A dynamic illustration of a global inference serving system handling millions of requests. The scene shows a central model representation being replicated across multiple serving nodes arranged in a globe-spanning network. Incoming request streams appear as flowing arrows from users worldwide, processed through load balancers depicted as traffic controllers, then distributed to model replicas. Visual elements include batching queues shown as organized request groups, latency meters displaying millisecond readings, and auto-scaling indicators showing nodes spawning and despawning. The composition emphasizes real-time responsiveness with clock symbols and speed indicators. Color palette features cool silvers and whites for infrastructure, warm oranges for active requests, and green for successful responses. Technical yet accessible style suitable for a production systems textbook. Rendered in the style of Nanobanana. High resolution, rectangular image with golden ratio dimensions._
:::

\noindent
![](images/png/cover_inference_at_scale.png)

:::

## Purpose {.unnumbered}

_Why does serving machine learning predictions at scale require fundamentally different engineering approaches than training the models that generate them?_

Training optimizes for throughput over extended periods, tolerating latency variations that would be unacceptable when users await responses in real time. Inference at scale inverts these priorities: serving systems must deliver predictions within strict latency bounds while handling request volumes that fluctuate unpredictably across hours, days, and seasons. The engineering challenges of inference extend beyond raw performance to encompass resource efficiency when serving costs dwarf training costs over a model's lifetime, availability when users expect continuous service regardless of infrastructure conditions, and consistency when the same input must produce the same output across globally distributed serving infrastructure. Understanding how serving systems, batching strategies, model sharding, and load balancing address these challenges determines whether sophisticated models deliver value to users or remain impressive but impractical demonstrations. Mastering inference at scale transforms model capabilities into reliable services that operate continuously at the speed and scale users demand.

::: {.callout-tip title="Learning Objectives"}

- Quantify why serving cost dominates training cost over a model's lifetime using the total cost of serving equation

- Design batching strategies matched to model architectures, distinguishing static batching for vision models from continuous batching for LLMs and feature-parallel batching for recommendation systems

- Apply the serving hierarchy framework (request, replica, service, platform) to systematically identify and resolve inference bottlenecks at each level

- Compare model sharding strategies (tensor parallelism, pipeline parallelism, expert parallelism, embedding sharding) by their communication patterns and latency-memory tradeoffs

- Analyze load balancing algorithms using queuing theory to explain why power-of-two-choices achieves exponentially better performance than random assignment

- Evaluate KV cache management techniques (PagedAttention, prefix caching, speculative decoding) for optimizing LLM serving throughput and memory efficiency

- Design autoscaling policies that account for GPU cold start latency and balance cost optimization with SLO guarantees

:::

## Scaling Inference Beyond Single Machines {#sec-inference-scaling-beyond}

Single-machine model serving establishes foundational principles that this chapter extends to distributed systems...

::: {.callout-note title="Connection: The Systems Sandwich"}
Inference is the **Service Layer** of the Systems Sandwich. It sits above the **Operational Layer** (distributed training) and **Physical Layer** (infrastructure). While training builds the model, inference is where the model meets the user. The latency constraints here ($<100$ms) are far stricter than the throughput constraints of training, forcing us to rethink the entire stack—from batching strategies to memory management.
:::

The transition from single-machine to distributed inference parallels the transition from single-machine to distributed training, but with fundamentally different constraints. Training optimizes throughput over extended periods and tolerates latency variations measured in minutes or hours. Inference at scale must maintain strict latency bounds measured in milliseconds while handling request volumes that fluctuate unpredictably. This inversion of priorities transforms every design decision, from how requests are batched to how failures are handled.

Distributed inference systems must solve problems that simply do not exist at single-machine scale. Load balancing[^fn-load-balancing] becomes critical when requests must be distributed across hundreds of GPU instances while maintaining latency guarantees. Request routing must account for model-specific characteristics. A recommendation system with trillion-parameter embedding tables requires different placement strategies than a large language model that generates responses token by token. Autoscaling must anticipate demand fluctuations that can change request volume by orders of magnitude within minutes while maintaining latency bounds that users expect.

[^fn-load-balancing]: **Load balancing** in inference systems differs fundamentally from traditional web load balancing. Web servers typically complete requests in milliseconds with uniform processing time; GPU inference workers handle requests lasting from 10ms to 30+ seconds with high variance. This variance makes techniques like least-connections and queue-depth-aware routing essential, whereas simple round-robin suffices for stateless web workloads.

The economics of inference at scale also differ fundamentally from training economics. Training costs are dominated by compute time and can be amortized over the lifetime of the resulting model. Inference costs, by contrast, are directly tied to user traffic and revenue. An e-commerce recommendation system might serve millions of requests per second during peak shopping periods, with each request contributing directly to potential revenue. The cost of overprovisioning during quiet periods or underprovisioning during peaks translates immediately to business impact, making inference efficiency a first-order concern in ways that training efficiency rarely achieves.

This chapter develops the principles and techniques for building inference systems that scale to meet these demands. We examine when distributed inference becomes necessary, how to architect systems that maintain latency bounds under varying load, and how to optimize the economics of inference through efficient resource utilization. The goal is not merely to make inference work at scale, but to make it work efficiently, reliably, and economically.

### When Single-Machine Serving Is Insufficient {#sec-inference-when-insufficient}

Three distinct signals indicate when distributed inference becomes necessary rather than merely optional. @tbl-distribution-triggers categorizes these triggers by constraint type and corresponding strategy.

**Memory exhaustion** occurs when model parameters, key-value caches, or embedding tables exceed single-device capacity. A single NVIDIA H100 GPU provides 80GB of HBM3[^fn-hbm3] memory. GPT-4 class models with hundreds of billions of parameters require 200-400GB just for weights in FP16 precision, forcing distribution across multiple GPUs regardless of throughput requirements. Recommendation systems with trillion-parameter embedding tables face similar constraints: Meta's DLRM[^fn-dlrm] model stores embedding tables that require multiple terabytes of memory.

[^fn-hbm3]: **High Bandwidth Memory 3 (HBM3)**: A 3D-stacked DRAM technology providing 3.35 TB/s bandwidth on the H100, compared to 1 TB/s for HBM2e on the A100. This 3x bandwidth improvement directly enables larger batch sizes and faster token generation for memory-bandwidth-bound LLM decode operations.

[^fn-dlrm]: **Deep Learning Recommendation Model (DLRM)**: Meta's reference architecture (2019) for large-scale recommendation, processing hundreds of billions of inferences daily. DLRM separates dense features (MLP processing) from sparse features (embedding lookups), enabling hybrid CPU-GPU architectures. Embedding tables can exceed 100TB, requiring distributed storage and caching strategies for serving.

**Throughput limitations** emerge when request volume exceeds single-machine capacity even with optimal batching. Consider a recommendation system serving 100,000 queries per second with a 10ms latency budget. If single-machine throughput peaks at 10,000 QPS, no amount of optimization on that machine can satisfy demand. Horizontal scaling across multiple replicas becomes mandatory.

**Latency requirements** drive distribution when model execution time exceeds latency budgets even at batch size one. Large language models generating responses token by token face this constraint acutely: a 70-billion parameter model requires approximately 140GB of memory and achieves roughly 30 tokens per second on a single GPU due to memory bandwidth limitations. Sharding the model across multiple GPUs enables parallel computation that reduces time-to-first-token below acceptable thresholds.

+----------------+--------------------------+----------------------+-----------------------------+
| **Constraint** | **Single-Machine Limit** | **Example Workload** | **Distribution Strategy**   |
+:===============+:=========================+=====================:+:============================+
| **Memory**     | 80GB (H100)              | GPT-4 (400GB+)       | Tensor/pipeline parallelism |
| **Throughput** | ~10K QPS (vision)        | 100K QPS RecSys      | Horizontal replication      |
| **Latency**    | Model execution time     | 500ms LLM TTFT       | Model sharding              |
+----------------+--------------------------+----------------------+-----------------------------+

: **Triggers for Distributed Inference**: Each constraint type indicates different distribution strategies. Memory constraints require model sharding; throughput constraints require replication; latency constraints may require either depending on whether the bottleneck is compute or memory bandwidth. {#tbl-distribution-triggers}

### The Fundamental Inversion: Training vs Inference {#sec-inference-inversion}

The contrast between training and inference optimization extends beyond the basic throughput versus latency distinction. Training optimizes for samples processed per hour and tolerates latency variations. Inference optimizes for response time and must meet strict latency bounds. At scale, this inversion manifests in system architecture, resource allocation, and operational priorities, as @tbl-training-inference-inversion details across six key system aspects.

+-------------------------+-------------------------------+------------------------------+
| **Aspect**              | **Distributed Training**      | **Distributed Inference**    |
+:========================+:==============================+:=============================+
| **Primary metric**      | Throughput (samples/hour)     | Latency (P99 ms)             |
| **Acceptable variance** | Hours                         | Milliseconds                 |
| **State management**    | Checkpoints (periodic)        | Session state (continuous)   |
| **Batch formation**     | Large, controlled             | Request-driven, variable     |
| **Failure tolerance**   | Restart from checkpoint       | Redirect without user impact |
| **Cost structure**      | Fixed duration, variable rate | Variable duration, fixed SLO |
+-------------------------+-------------------------------+------------------------------+

: **Training vs Inference System Requirements**: The fundamental inversion from throughput to latency optimization ripples through every aspect of system design. {#tbl-training-inference-inversion}

Training tolerates substantial latency variance because the optimization target is aggregate progress over hours or days. A training iteration that takes 2 seconds instead of the usual 1 second represents acceptable variation. An inference request that takes 2 seconds instead of 100 milliseconds represents catastrophic failure, potentially causing user abandonment or cascading timeouts in dependent services.

State management differs fundamentally. Training maintains model state (parameters, optimizer states) that evolves gradually and can be captured in periodic checkpoints. Inference often maintains session state (conversation history, key-value caches, user context) that must be preserved across requests and cannot tolerate the staleness that checkpoint-based recovery would introduce.

Failure handling diverges correspondingly. Training failures trigger checkpoint restoration and continuation, with minutes of lost progress being acceptable. Inference failures must be invisible to users. Requests redirect to healthy replicas, degraded results substitute for unavailable models, and SLOs must be maintained despite infrastructure instability.

### The Serving Tax: Overhead of Distribution {#sec-inference-serving-tax}

Distributing inference across multiple machines introduces overhead absent from single-machine serving. This "serving tax" must be understood and budgeted within latency constraints.

**Network communication** adds latency for every cross-machine interaction. Within a datacenter, network round-trip times range from 50-500 microseconds depending on topology and congestion. For model sharding that requires synchronization between GPUs on different machines, each synchronization point adds this overhead. A model sharded across 8 machines with 4 synchronization points per inference adds 200 microseconds to 2 milliseconds of network latency.

**Serialization overhead** converts in-memory tensors to network-transmittable formats. While modern serialization libraries like FlatBuffers and Cap'n Proto[^fn-serialization] minimize this overhead, large activation tensors still require meaningful time to serialize and deserialize. A 1GB activation tensor takes approximately 100 milliseconds to serialize, even with optimized libraries.

[^fn-serialization]: **Zero-copy serialization**: FlatBuffers (Google) and Cap'n Proto enable reading serialized data directly without parsing or unpacking, eliminating the CPU overhead of traditional formats like Protocol Buffers or JSON. For distributed inference, this reduces per-request serialization overhead from milliseconds to microseconds.

**Load balancer latency** adds another layer. Requests must be routed to appropriate replicas, which requires examining request metadata, consulting routing tables, and forwarding to selected backends. Well-optimized load balancers add 100-500 microseconds; poorly configured ones can add milliseconds.

**Coordination overhead** emerges when requests require fan-out to multiple services. A recommendation system that queries a user model, item model, and ranking model in parallel must coordinate these queries and aggregate results. The coordination logic itself consumes CPU cycles and introduces latency variation.

The total serving tax often consumes 10-30% of the latency budget in distributed systems (@eq-serving-tax):

$$L_{total} = L_{compute} + L_{network} + L_{serialization} + L_{coordination} + L_{queuing}$$ {#eq-serving-tax}

Minimizing this tax requires co-locating communicating components, using high-bandwidth interconnects, and designing communication patterns that minimize round trips.

### Serving Cost Dominates Training Cost {#sec-inference-cost-dominance}

A critical insight for infrastructure planning is that serving cost typically dominates training cost over a model's operational lifetime. This reversal from the training-centric view of model development has profound implications for where optimization effort should focus.

The total cost of operating a model comprises training cost (a one-time expense) and serving cost (an ongoing expense) (@eq-total-cost):

$$C_{total} = C_{training} + C_{serving} \times T_{deployment} \times Q_{rate}$$ {#eq-total-cost}

where $C_{training}$ is the one-time cost to train the model, $C_{serving}$ is the cost per query served, $T_{deployment}$ is the deployment duration in appropriate time units, and $Q_{rate}$ is the query rate.

::: {.callout-notebook title="Engineering Calculation: The Serving Cost Multiplier" collapse="true"}
**Scenario**: Recommend-system model (DLRM).

**Training Cost ($C_{train}$)**
- Hardware: 1,000 GPU-hours @ $3/hr = $3,000
- Engineering Overhead (Data/Exp): 3x Hardware = $9,000
- **Total Training**: $12,000

**Serving Cost ($C_{serve}$)**
- Deployment: 2 years (constant updates, amortized)
- Traffic: 10,000 Queries Per Second (QPS)
- Efficiency: $10^{-5}$ dollars per query ($10 per million)

**Lifetime Volume ($Q_{total}$)**
$$ Q_{total} = 10^4 \text{/s} \times 86,400 \text{ s/day} \times 730 \text{ days} \approx \mathbf{6.31 \times 10^{11} \text{ queries}} $$

**Total Serving Cost**
$$ C_{serve\_total} = 6.31 \times 10^{11} \times \$10^{-5} = \mathbf{\$6,310,000} $$

**Conclusion**:
$$ \text{Ratio} = \frac{\$6,310,000}{\$12,000} \approx \mathbf{526\times} $$
Serving optimization leverage is 500x higher than training optimization. A 1% serving efficiency gain saves \$63k—5x the entire training budget.
:::

The cost dominance ratio varies by application. @tbl-cost-ratios quantifies this disparity:

+-------------------------------+-------------------+-------------------------+-----------+
| **Application**               | **Training Cost** | **Annual Serving Cost** | **Ratio** |
+:==============================+==================:+========================:+==========:+
| **Recommendation (high QPS)** | $10K-100K         | $1M-10M                 | 100-1000x |
| **Search ranking**            | $100K-1M          | $10M-100M               | 100-1000x |
| **LLM API**                   | $1M-100M          | $10M-1B                 | 10-100x   |
| **Internal analytics**        | $1K-10K           | $10K-100K               | 10-100x   |
+-------------------------------+-------------------+-------------------------+-----------+

: **Training vs Serving Cost Ratios**: High-QPS applications like recommendation systems show the most extreme cost dominance of serving over training. {#tbl-cost-ratios}

This cost structure motivates the optimization techniques throughout this chapter. Every percentage point of serving efficiency improvement yields ongoing cost reduction over the model's operational lifetime.

### The Inference Landscape: Beyond LLMs {#sec-inference-landscape}

A critical misconception in current discourse frames inference at scale as synonymous with LLM serving. While large language models present distinctive challenges and attract significant attention, they represent a small fraction of production inference volume. Understanding the full inference landscape proves essential for appropriate technique selection, as @tbl-inference-landscape reveals through its breakdown of model types, request volumes, and optimization challenges.

::: {.callout-important title="Production Inference by Request Volume"}

By request count, production ML inference at major technology companies breaks down approximately as:

- **Recommendation and ranking**: 80-90% of requests
- **Vision and image processing**: 5-10% of requests
- **NLP/LLM**: 1-5% of requests (but growing rapidly)
- **Other (fraud detection, ads, etc.)**: 2-5% of requests

Source: Industry reports from Meta, Google, and Netflix infrastructure teams.

:::

Recommendation systems dominate because they serve predictions for every user interaction: every page load, scroll, or click triggers inference. A user browsing an e-commerce site might generate 100 recommendation requests in a single session. In contrast, LLM queries typically require explicit user action and occur less frequently.

This distribution has important implications. Recommendation systems have driven most production inference innovation. Dynamic batching, embedding sharding, feature store architectures, and low-latency serving were developed primarily for recommendation workloads. LLM-specific techniques like continuous batching and KV cache management are important but address a narrower slice of production inference.

+--------------------+--------------------+--------------------+-------------------+
| **Model Type**     | **Request Volume** | **Latency Target** | **Key Challenge** |
+:===================+:===================+:===================+:==================+
| **Recommendation** | Very high (80-90%) | &lt;10ms P99       | Embedding lookup  |
| **Vision (CNN)**   | Moderate (5-10%)   | 20-100ms           | Batch efficiency  |
| **LLM**            | Lower (1-5%)       | 100ms-10s          | Memory bandwidth  |
| **Speech/Audio**   | Lower              | Real-time          | Sequential decode |
| **Multimodal**     | Growing            | Varies             | Cross-modal sync  |
+--------------------+--------------------+--------------------+-------------------+

: **Production Inference Landscape**: Different model types have different volume, latency requirements, and optimization challenges. Technique selection must match the specific workload. {#tbl-inference-landscape}

### The Serving Hierarchy {#sec-inference-serving-hierarchy}

To organize the optimization techniques in this chapter, we introduce the serving hierarchy as a conceptual framework. Like the memory hierarchy in computer architecture, the serving hierarchy identifies distinct levels at which optimization occurs, each with different targets and techniques.

**Request level**: Optimizations that affect individual request processing. Batching strategies, caching, and preprocessing optimizations operate at this level. The target metric is per-request latency.

**Replica level**: Optimizations within a single model instance. GPU utilization, memory management, and model optimization operate here. The target metric is single-replica throughput.

**Service level**: Optimizations across multiple replicas of the same model. Load balancing, request routing, and replica management operate at this level. The target metric is aggregate service throughput while meeting latency SLOs.

**Platform level**: Optimizations across multiple services and tenants. Resource allocation, multi-tenancy, scheduling, and cluster management operate here. The target metric is overall resource efficiency while meeting diverse SLO requirements.

::: {.callout-note title="Figure: The Serving Hierarchy" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{PlatformColor}{RGB}{200,220,255}
  \definecolor{ServiceColor}{RGB}{220,240,255}
  \definecolor{ReplicaColor}{RGB}{240,250,255}
  \definecolor{RequestColor}{RGB}{255,255,255}

  \tikzset{
    level/.style={draw=black!70, thick, align=center, minimum width=6cm, minimum height=0.8cm}
  }

  \node[level, fill=PlatformColor] (L4) at (0, 3.2) {\textbf{Platform Level}\\Multi-tenancy, Isolation, Cluster Efficiency};
  \node[level, fill=ServiceColor, minimum width=5cm] (L3) at (0, 2.4) {\textbf{Service Level}\\Load Balancing, Autoscaling, Routing};
  \node[level, fill=ReplicaColor, minimum width=4cm] (L2) at (0, 1.6) {\textbf{Replica Level}\\GPU Memory (KV Cache), Kernel Optimization};
  \node[level, fill=RequestColor, minimum width=3cm] (L1) at (0, 0.8) {\textbf{Request Level}\\Batching, Caching, Preprocessing};

  \node[anchor=west, font=\scriptsize, text=gray] at (3.2, 3.2) {Scale: Cluster};
  \node[anchor=west, font=\scriptsize, text=gray] at (2.7, 2.4) {Scale: Service};
  \node[anchor=west, font=\scriptsize, text=gray] at (2.2, 1.6) {Scale: GPU};
  \node[anchor=west, font=\scriptsize, text=gray] at (1.7, 0.8) {Scale: Logic};

\end{tikzpicture}
```
**The Serving Hierarchy**. Optimization occurs at four distinct levels. The Request Level focuses on minimizing per-request latency. The Replica Level maximizes single-instance throughput. The Service Level manages distribution across multiple replicas. The Platform Level handles efficient resource sharing across multiple services and tenants.
:::

Each level has distinct optimization levers. @tbl-serving-hierarchy maps these levels to their primary targets and techniques:

+--------------+-------------------------+----------------------------------------+
| **Level**    | **Optimization Target** | **Key Techniques**                     |
+:=============+:========================+:=======================================+
| **Request**  | Per-request latency     | Dynamic batching, caching, prefetching |
| **Replica**  | Throughput, utilization | Memory optimization, kernel fusion     |
| **Service**  | Aggregate capacity      | Load balancing, routing, autoscaling   |
| **Platform** | Resource efficiency     | Multi-tenancy, scheduling, placement   |
+--------------+-------------------------+----------------------------------------+

: **Serving Hierarchy Optimization Targets**: Each level of the hierarchy addresses different metrics with different techniques. {#tbl-serving-hierarchy}

The remainder of this chapter progresses through these levels: batching and caching (request level), model sharding (replica level), load balancing and autoscaling (service level), and multi-tenancy (platform level).

### Chapter Roadmap {#sec-inference-roadmap}

This chapter develops the techniques for inference at scale through the lens of the serving hierarchy:

**Batching Strategies at Scale** (@sec-inference-batching) examines how different model types require fundamentally different batching approaches. We contrast static batching for vision models, continuous batching for LLMs, and feature-parallel batching for recommendation systems, providing quantitative analysis of throughput-latency tradeoffs.

**Model Sharding for Inference** (@sec-inference-sharding) addresses when and how to distribute model computation across multiple devices. We examine tensor parallelism, pipeline parallelism, expert parallelism, and embedding sharding, with emphasis on communication patterns and overhead.

**Load Balancing and Request Routing** (@sec-inference-load-balancing) develops the theory and practice of distributing requests across replicas. We derive why power-of-two-choices achieves exponentially better load distribution than random assignment and examine routing strategies for stateful workloads.

**KV Cache Management** (@sec-inference-kv-cache) focuses on the memory management challenges specific to autoregressive language models, including PagedAttention, prefix caching, and speculative decoding.

**Multi-Tenancy and Isolation** (@sec-inference-multitenancy) examines platform-level concerns: sharing infrastructure across multiple models and users while maintaining isolation and fairness.

**Autoscaling** (@sec-inference-autoscaling) addresses dynamic capacity management, including the cold start problem unique to GPU-based serving and predictive scaling strategies.

**Case Studies** (@sec-inference-case-studies) grounds these principles in production systems at Meta, OpenAI, Google, and TikTok, demonstrating how the techniques combine in real deployments.

Throughout, we maintain the model-type diversity essential for practitioners: every major concept is illustrated across LLMs, recommendation systems, vision models, and other production workloads.

## Serving Framework Selection {#sec-inference-frameworks}

The batching, sharding, and load balancing techniques described in the chapter roadmap do not exist in isolation; they are implemented within serving frameworks that constrain and enable different optimizations. Before examining these techniques in depth, practitioners face an immediate practical decision: which serving infrastructure to build upon?

The choice of serving framework determines which optimizations are available, how models are deployed, and what performance characteristics are achievable. Understanding this landscape first provides essential context: when we discuss continuous batching in @sec-inference-continuous-batching, we will see how vLLM and TensorRT-LLM implement it differently. This section provides a systematic framework for selecting among the major options.

### Framework Categories {#sec-inference-framework-categories}

Serving frameworks fall into distinct categories based on their design philosophy and target workloads:

**General-purpose inference servers** provide broad model support with configurable optimization. Triton Inference Server (NVIDIA) offers multi-framework support (PyTorch, TensorFlow, ONNX, TensorRT), dynamic batching, model ensemble orchestration, and concurrent model execution. TensorFlow Serving provides native TensorFlow support, gRPC/REST APIs, model versioning, and batching scheduler. TorchServe (PyTorch) delivers native PyTorch support, model archiving, metrics, and multi-model serving.

**LLM-specialized servers** optimize specifically for autoregressive generation[^fn-autoregressive]. vLLM provides PagedAttention, continuous batching, tensor parallelism, and OpenAI-compatible API. TensorRT-LLM delivers NVIDIA-optimized kernels, in-flight batching, quantization, and multi-GPU support. Text Generation Inference (TGI) offers Hugging Face integration, flash attention, tensor parallelism, and watermarking.

[^fn-autoregressive]: **Autoregressive generation**: The process of generating output tokens one at a time, where each new token depends on all previously generated tokens. This sequential dependency creates the memory-bandwidth bottleneck that dominates LLM serving costs, as the entire model must be read from memory for each generated token.

**Optimization-focused runtimes** maximize inference speed through compilation. TensorRT provides graph optimization, kernel fusion, precision calibration, and NVIDIA GPU specific optimizations. ONNX Runtime offers cross-platform optimization and execution providers for different hardware. OpenVINO delivers Intel hardware optimization, model compression, and heterogeneous execution.

### Framework Selection Criteria {#sec-inference-framework-criteria}

Selection depends on model type, deployment constraints, and organizational factors:

**Model architecture determines primary candidates**:

+-------------------------+---------------------------+------------------------------------------+
| **Model Type**          | **Primary Options**       | **Key Consideration**                    |
+:========================+:==========================+:=========================================+
| **LLM (&gt;7B params)** | vLLM, TensorRT-LLM, TGI   | KV cache management, continuous batching |
| **LLM (&lt;7B params)** | vLLM, TGI, Triton         | Simpler deployment, less memory pressure |
| **Vision (CNN/ViT)**    | Triton, TensorRT, ONNX RT | Static batching, throughput optimization |
| **Recommendation**      | Triton, custom            | Feature preprocessing, embedding lookup  |
| **Multi-modal**         | Triton, custom            | Pipeline orchestration                   |
+-------------------------+---------------------------+------------------------------------------+

**Hardware constraints narrow options**:

+---------------------------+----------------------------------+
| **Hardware**              | **Supported Frameworks**         |
+:==========================+:=================================+
| **NVIDIA datacenter GPU** | All options                      |
| **NVIDIA consumer GPU**   | vLLM, TGI (limited TensorRT-LLM) |
| **AMD GPU**               | vLLM (ROCm), ONNX RT             |
| **Intel CPU/GPU**         | OpenVINO, ONNX RT                |
| **Apple Silicon**         | MLX, Core ML, ONNX RT            |
| **AWS Inferentia**        | Neuron SDK                       |
+---------------------------+----------------------------------+

**Operational requirements influence choice**:

- **Multi-model serving**: Triton excels with concurrent model execution
- **Rapid iteration**: TorchServe, TGI offer simpler deployment cycles
- **Maximum throughput**: TensorRT-LLM, vLLM with optimized kernels
- **Cross-platform**: ONNX Runtime provides broadest hardware support

### vLLM Architecture {#sec-inference-vllm}

vLLM[^fn-vllm-name] [@kwon2023vllm] has emerged as the leading open-source LLM serving framework due to its PagedAttention innovation. Understanding its architecture illustrates key LLM serving principles.

[^fn-vllm-name]: **vLLM**: The name stands for "virtual LLM," reflecting its core innovation of applying virtual memory concepts to KV cache management. Developed at UC Berkeley and released in 2023, vLLM achieved 24x throughput improvement over HuggingFace Transformers in initial benchmarks.

The core innovations in vLLM include PagedAttention (virtual memory for KV cache, covered in @sec-inference-paged-attention), continuous batching (add/remove requests mid-generation), optimized attention kernels (FlashAttention [@dao2022flashattention] integration), and tensor parallelism (automatic model sharding across GPUs).

**Architecture overview**:

```
┌─────────────────────────────────────────────────────┐
│                   vLLM Engine                        │
├─────────────────────────────────────────────────────┤
│  Scheduler          │  Block Manager                │
│  - Request queue    │  - Physical blocks            │
│  - Preemption       │  - Block tables               │
│  - Priority         │  - Copy-on-write              │
├─────────────────────┼───────────────────────────────┤
│  Model Executor     │  Cache Engine                 │
│  - Attention        │  - GPU cache                  │
│  - Sampling         │  - CPU swap space             │
│  - Tensor parallel  │  - Prefix caching             │
└─────────────────────┴───────────────────────────────┘
```

**Deployment example**:

```python
from vllm import LLM, SamplingParams

# Initialize with automatic GPU detection
llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    tensor_parallel_size=4,  # Shard across 4 GPUs
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)

# Efficient batch inference
prompts = ["Explain quantum computing", "Write a poem about AI"]
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)
outputs = llm.generate(prompts, sampling_params)
```

**Performance characteristics**:

+-----------------------------+-------------------+-----------------------------------------+
| **Metric**                  | **Typical Value** | **Notes**                               |
+:============================+==================:+:========================================+
| **Throughput vs baseline**  | 2-4x              | Compared to naive HF generation         |
| **Memory efficiency**       | 90%+ utilization  | PagedAttention eliminates fragmentation |
| **Latency overhead**        | &lt;5ms           | Scheduling and batching overhead        |
| **Max concurrent requests** | 100s-1000s        | Depends on model size and GPU memory    |
+-----------------------------+-------------------+-----------------------------------------+

### TensorRT-LLM Architecture {#sec-inference-tensorrt-llm}

TensorRT-LLM provides NVIDIA-optimized LLM inference with deep hardware integration.

TensorRT-LLM provides several core capabilities. Optimized kernels include custom CUDA kernels for attention, GEMM, and layer norms. In-flight batching implements NVIDIA's continuous batching. Quantization supports INT8, INT4, FP8 with minimal accuracy loss. Multi-GPU configurations enable tensor and pipeline parallelism with NVLink optimization.

**Build and deployment workflow**:

```bash
# Step 1: Convert model to TensorRT-LLM format
python convert_checkpoint.py \
    --model_dir /models/llama-70b \
    --output_dir /models/llama-70b-trt \
    --dtype float16 \
    --tp_size 4

# Step 2: Build optimized engine
trtllm-build \
    --checkpoint_dir /models/llama-70b-trt \
    --output_dir /engines/llama-70b \
    --gemm_plugin float16 \
    --max_batch_size 64 \
    --max_input_len 2048 \
    --max_output_len 512

# Step 3: Deploy with Triton
# (Configuration in model_repository/)
```

**Performance comparison with vLLM**:

+---------------------------+------------------+----------+--------------+
| **Scenario**              | **TensorRT-LLM** | **vLLM** | **Winner**   |
+:==========================+:=================+:=========+:=============+
| **A100 throughput**       | Higher           | Good     | TensorRT-LLM |
| **H100 throughput**       | Highest          | High     | TensorRT-LLM |
| **Deployment simplicity** | Complex          | Simple   | vLLM         |
| **Model support**         | NVIDIA curated   | Broad HF | vLLM         |
| **Quantization options**  | Extensive        | Good     | TensorRT-LLM |
+---------------------------+------------------+----------+--------------+

TensorRT-LLM typically achieves 20-50% higher throughput than vLLM on NVIDIA hardware but requires more complex deployment pipelines.

### Triton Inference Server {#sec-inference-triton}

Triton provides enterprise-grade multi-model serving with sophisticated orchestration capabilities.

Triton provides several key features for production. Multi-framework support allows a single server to host PyTorch, TensorFlow, TensorRT, and ONNX models. Dynamic batching offers configurable batching with latency targets. Model ensembles enable chaining models in inference pipelines. Concurrent execution allows multiple models to share GPU resources. Metrics and monitoring integrate with Prometheus and provide detailed latency breakdown.

**Model repository structure**:

```
model_repository/
├── text_encoder/
│   ├── config.pbtxt
│   └── 1/
│       └── model.onnx
├── image_classifier/
│   ├── config.pbtxt
│   └── 1/
│       └── model.plan  # TensorRT engine
└── ensemble_pipeline/
    ├── config.pbtxt    # Orchestrates above models
    └── 1/
```

**Dynamic batching configuration**:

```protobuf
# config.pbtxt
dynamic_batching {
    preferred_batch_size: [4, 8, 16, 32]
    max_queue_delay_microseconds: 100000  # 100ms max wait
}
instance_group [
    {
        count: 2
        kind: KIND_GPU
        gpus: [0, 1]
    }
]
```

**Use cases where Triton excels**:

- Multi-model pipelines (e.g., detection → classification → ranking)
- Mixed workloads on shared GPU clusters
- Organizations with diverse model frameworks
- Production systems requiring detailed observability

### Framework Selection Decision Tree {#sec-inference-framework-decision}

```
Start
  │
  ├─ Is this an LLM (autoregressive generation)?
  │   ├─ Yes → Is maximum throughput critical?
  │   │         ├─ Yes, NVIDIA hardware → TensorRT-LLM
  │   │         └─ No, or mixed hardware → vLLM
  │   │
  │   └─ No → Is this multi-model serving?
  │           ├─ Yes → Triton Inference Server
  │           └─ No → What's the deployment target?
  │                   ├─ NVIDIA GPU → TensorRT + Triton
  │                   ├─ Intel → OpenVINO
  │                   ├─ Cross-platform → ONNX Runtime
  │                   └─ Edge/Mobile → Platform-specific (Core ML, TFLite)
```

**Common deployment patterns**:

+-------------------------+---------------------------+-----------------------------------------+
| **Pattern**             | **Frameworks**            | **Use Case**                            |
+:========================+:==========================+:========================================+
| **LLM API service**     | vLLM + nginx              | ChatGPT-like applications               |
| **High-throughput LLM** | TensorRT-LLM + Triton     | Batch processing, enterprise            |
| **Vision pipeline**     | TensorRT + Triton         | Object detection, classification        |
| **Recommendation**      | Triton + custom embedding | E-commerce, content platforms           |
| **Multi-modal**         | Triton ensemble           | Vision-language, document understanding |
+-------------------------+---------------------------+-----------------------------------------+

### Framework Performance Benchmarking {#sec-inference-framework-benchmarks}

When evaluating frameworks, benchmark on representative workloads:

**LLM benchmark methodology**:

```python
# Standard benchmark parameters
benchmark_config = {
    "input_lengths": [128, 512, 2048],
    "output_lengths": [64, 256, 512],
    "batch_sizes": [1, 8, 32, 64],
    "concurrent_requests": [1, 10, 50, 100],
    "metrics": ["ttft", "tpot", "throughput", "gpu_util"],
}

# Time to First Token (TTFT): Latency until first token generated
# Time Per Output Token (TPOT): Average latency per subsequent token
# Throughput: Total tokens/second across all requests
# GPU utilization: Compute and memory utilization
```

**Representative benchmark results** (Llama-2-70B on 4xA100-80GB):

+---------------------+---------------+---------------+------------------------+
| **Framework**       | **TTFT (ms)** | **TPOT (ms)** | **Throughput (tok/s)** |
+:====================+==============:+==============:+=======================:+
| **TensorRT-LLM**    | 180           | 28            | 2,400                  |
| **vLLM**            | 220           | 32            | 1,900                  |
| **TGI**             | 250           | 35            | 1,700                  |
| **HF Transformers** | 400           | 85            | 600                    |
+---------------------+---------------+---------------+------------------------+

Note: Results vary significantly with configuration, input/output lengths, and batch sizes. Always benchmark on your specific workload.

### Migration and Integration Considerations {#sec-inference-framework-migration}

**Migrating between frameworks**:

- **Model compatibility**: Most frameworks support standard formats (HF, ONNX)
- **API differences**: vLLM uses OpenAI-compatible API; Triton uses gRPC/HTTP
- **Configuration translation**: Batching, parallelism settings differ by framework

**Integration with ML infrastructure**:

+--------------------+------------------------------------------------+
| **Component**      | **Integration Pattern**                        |
+:===================+:===============================================+
| **Model registry** | Pull models on startup, version management     |
| **Feature store**  | Triton ensemble preprocessing, custom backends |
| **Monitoring**     | Prometheus metrics, distributed tracing        |
| **Load balancer**  | Health checks, request routing                 |
| **Autoscaler**     | Custom metrics (queue depth, GPU utilization)  |
+--------------------+------------------------------------------------+

The framework selection made here influences all subsequent serving optimizations. The techniques in following sections (batching, sharding, caching) are implemented differently across frameworks but follow the same underlying principles.

### Orchestration Platforms for Production Serving {#sec-inference-orchestration}

Individual inference runtimes (vLLM, TensorRT-LLM, Triton) handle the mechanics of efficient inference on a single node or small cluster. Production systems require an orchestration layer that manages service composition, autoscaling, traffic management, fault tolerance, and multi-tenancy. Choosing an inference runtime without considering the orchestration layer is like choosing a database engine without considering connection pooling, replication, and query routing.

The orchestration layer handles several key responsibilities. Service composition combines multiple models and components into request pipelines. Autoscaling adjusts replicas based on traffic patterns and resource utilization. Traffic management encompasses load balancing, canary deployments, and A/B testing. Fault tolerance provides replica health monitoring and automatic recovery. Multi-tenancy isolates workloads and manages resource allocation across teams.

**Major orchestration platforms**:

+--------------------------------+--------------------------------------------------------+--------------------------------+
| **Platform**                   | **Key Capability**                                     | **Production Users**           |
+:===============================+:=======================================================+:===============================+
| **Ray Serve [@moritz2018ray]** | Scalable Python-native serving, composable deployments | OpenAI, Uber, Instacart        |
| **KServe**                     | Kubernetes-native serving, serverless inference        | Bloomberg, Zillow, enterprises |
| **BentoML**                    | ML model packaging and unified serving API             | Various production deployments |
| **Seldon Core**                | Kubernetes deployment, A/B testing, canary releases    | Financial services, retail     |
+--------------------------------+--------------------------------------------------------+--------------------------------+

**Ray Serve architecture**:

```
┌─────────────────────────────────────────────────────────────┐
│                    Ray Serve Controller                      │
├─────────────────────────────────────────────────────────────┤
│  HTTP Proxy         │  Autoscaler           │  Router        │
│  - Request routing  │  - Replica management │  - Load balance│
│  - Request batching │  - Scale up/down      │  - Affinity    │
├─────────────────────┼───────────────────────┴────────────────┤
│                    Ray Actor Pool                            │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐        │
│  │ Replica 1│ │ Replica 2│ │ Replica 3│ │ Replica N│        │
│  │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │        │
│  └──────────┘ └──────────┘ └──────────┘ └──────────┘        │
└─────────────────────────────────────────────────────────────┘
```

**Production-ready vLLM deployment with Ray Serve**:

```python
from ray import serve
from vllm import LLM, SamplingParams


@serve.deployment(
    num_replicas=4,
    ray_actor_options={"num_gpus": 4},
    autoscaling_config={
        "min_replicas": 2,
        "max_replicas": 16,
        "target_num_ongoing_requests_per_replica": 10,
    },
)
class LLMDeployment:
    def __init__(self):
        self.llm = LLM(
            model="meta-llama/Llama-2-70b-hf", tensor_parallel_size=4
        )
        self.params = SamplingParams(temperature=0.7, max_tokens=256)

    async def __call__(self, request):
        return self.llm.generate([request.prompt], self.params)[0]


# Deploy with automatic scaling
app = LLMDeployment.bind()
serve.run(app)
```

This pattern provides automatic scaling from 2 to 16 replicas based on load. It includes request batching at the serve layer, fault tolerance with automatic replica restart, and zero-downtime deployments.

**Stateless vs stateful serving**: The orchestration layer must account for whether inference is stateless or stateful.

+------------------+-----------------------+---------------------+---------------------------+
| **Serving Type** | **State Location**    | **Scaling Model**   | **Failure Recovery**      |
+:=================+:======================+:====================+:==========================+
| **Stateless**    | None or external      | Horizontal, trivial | Redirect to any replica   |
| **Stateful**     | In-process (KV cache) | Complex, sticky     | Session loss or migration |
+------------------+-----------------------+---------------------+---------------------------+

Vision models and embedding lookups are typically stateless: any replica can serve any request. LLM serving with KV cache is stateful: the cache accumulated during conversation creates replica-specific state.

Stateful LLM serving has several implications. Sticky routing is required because subsequent requests in a conversation must reach the same replica holding the KV cache. Failure recovery becomes complex because when a stateful replica fails, cached state is lost. Options include regenerating cache from history (high latency), replicating cache to backups (high bandwidth), or accepting session restart (poor experience). Autoscaling must account for sessions because scaling down stateful replicas requires draining active sessions, which can take minutes for long conversations. Memory sizing determines capacity because each active session consumes KV cache memory, limiting concurrent sessions per replica regardless of compute capacity.

**Consistency during model updates**:

When updating models across distributed replicas, requests may hit different model versions during the rollout. Different deployment strategies provide different consistency guarantees:

+-------------------------+--------------------------+-------------------+-----------------------+
| **Deployment Strategy** | **Consistency**          | **Rollout Speed** | **Risk**              |
+:========================+:=========================+:==================+:======================+
| **Blue-green**          | Strong (atomic switch)   | Instant           | High (all-or-nothing) |
| **Canary**              | Eventual (gradual shift) | Slow (hours)      | Low (progressive)     |
| **Rolling update**      | Weak (mixed versions)    | Medium            | Medium                |
+-------------------------+--------------------------+-------------------+-----------------------+

Applications requiring deterministic outputs (compliance, audit trails, reproducible debugging) should implement several practices. Pin model version by including version identifier in request routing to ensure same model handles related requests. Use temperature=0 to eliminate sampling variance, though beam search still has implementation-dependent tiebreaking. Implement version-aware caching by caching responses with model version tags and invalidating on version change.

**Failure scenario: Stateful replica crash**

When a stateful LLM replica crashes mid-conversation:

```
1. Load balancer detects health check failure (1-10 seconds)
2. New requests route to healthy replicas
3. In-flight requests fail; clients must retry
4. Session state (KV cache) is lost
5. Recovery options:
   a. Regenerate: Client resends conversation history (high latency)
   b. Redirect: Route to replica with replicated state (if available)
   c. Restart: Begin new session (poor user experience)
```

Production systems often accept option (a) with optimizations: the regenerated prefill can process the full conversation history in a single batch, taking seconds rather than the minutes the original conversation took.

**Build vs buy: Managed serving services**:

Before selecting frameworks, teams must decide whether to self-host or use managed services:

+-------------------------+--------------+--------------------------------------------------+--------------------------------------+
| **Service**             | **Provider** | **Key Features**                                 | **Trade-offs**                       |
+:========================+:=============+:=================================================+:=====================================+
| **SageMaker Endpoints** | AWS          | Managed hosting, autoscaling, A/B testing        | Lock-in, cost, limited customization |
| **Vertex AI Endpoints** | GCP          | TPU support, traffic splitting, model monitoring | GCP ecosystem dependency             |
| **Azure ML Endpoints**  | Azure        | Enterprise integration, ONNX optimization        | Azure ecosystem dependency           |
| **Anyscale Endpoints**  | Anyscale     | Ray-native, fine-grained autoscaling             | Emerging platform                    |
+-------------------------+--------------+--------------------------------------------------+--------------------------------------+

+--------------------------------+--------------------------------------------+-----------------------------------------------+
| **Approach**                   | **Advantages**                             | **Disadvantages**                             |
+:===============================+:===========================================+:==============================================+
| **Self-hosted (vLLM/Triton)**  | Full control, cost optimization at scale   | Operational burden, expertise required        |
| **Managed (SageMaker/Vertex)** | Operational simplicity, integrated tooling | Lock-in, cost at scale, limited customization |
| **Hybrid (Ray Serve + cloud)** | Flexibility, gradual migration             | Complexity in managing both                   |
+--------------------------------+--------------------------------------------+-----------------------------------------------+

**Decision factors**:

- **Team size**: Teams with fewer than 5 ML engineers often benefit from managed services
- **Scale**: More than 1M daily requests typically makes self-hosting cost-effective
- **Customization needs**: Novel architectures require self-hosting
- **Latency requirements**: Self-hosting enables co-location and deeper optimization

**Enhanced framework selection with orchestration**:

```
Start
  │
  ├─ What scale?
  │   ├─ <100 QPS → Simple deployment (managed services or single instance)
  │   ├─ 100-10K QPS → Need autoscaling
  │   │   ├─ Managed acceptable → SageMaker/Vertex
  │   │   └─ Self-hosted required → Ray Serve + vLLM/TensorRT-LLM
  │   └─ >10K QPS → Need distributed orchestration
  │       ├─ LLM → Ray Serve + vLLM with sharding
  │       ├─ RecSys → Custom or Triton with embedding sharding
  │       └─ Vision → Triton with dynamic batching
  │
  ├─ How many model types?
  │   ├─ Single model → Direct runtime deployment
  │   └─ Multiple models/pipelines → Triton or Ray Serve composition
  │
  └─ Stateful or stateless?
      ├─ Stateless → Any load balancer, simple scaling
      └─ Stateful (LLM with cache) → Sticky routing, session management
```

**Case study: Startup serving Llama-70B for customer support**

*Scenario*: A startup is launching an LLM-powered customer support chatbot using a fine-tuned Llama-70B model.

+-------------------------+----------------------------------+
| **Constraint**          | **Value**                        |
+:========================+=================================:+
| **Expected traffic**    | 100 QPS average, 500 QPS peak    |
| **Latency requirement** | &lt;2s time to first token       |
| **Budget**              | 4 H100 GPUs (leased)             |
| **Team**                | 2 ML engineers, no dedicated SRE |
| **Conversation length** | Average 8 turns, max 32K context |
+-------------------------+----------------------------------+

*Analysis*:

1. **Memory**: Llama-70B requires ~140GB in FP16. With 4-bit quantization (AWQ), this drops to ~35GB, fitting on a single H100 (80GB) with room for KV cache.

2. **Throughput**: At 500 QPS peak with average 100 output tokens, the system must sustain 50,000 tokens/second. A single quantized Llama-70B on H100 achieves ~1,000 tokens/second with continuous batching, so 4 GPUs provide headroom.

3. **Tensor parallelism vs replicas**: Two options exist:
   - 2 replicas × 2-way TP: Higher availability, lower per-request latency
   - 4 replicas × 1-way TP: Maximum throughput, simpler scaling

   With AWQ fitting on single GPU, option (b) is preferred for this traffic level.

4. **Team size**: 2 ML engineers without SRE experience suggests managed services or simple orchestration.

*Decision*:

+------------+-------------------------+--------------------+-------------------------------+
| **Option** | **Architecture**        | **Pros**           | **Cons**                      |
+:===========+:========================+:===================+:==============================+
| **A**      | SageMaker + HF TGI      | Minimal ops burden | Cost, limited optimization    |
| **B**      | vLLM + Ray Serve on EC2 | Good balance       | Some ops required             |
| **C**      | TensorRT-LLM + Triton   | Maximum throughput | Complex, overkill for 500 QPS |
+------------+-------------------------+--------------------+-------------------------------+

**Recommendation**: Option B (vLLM + Ray Serve) provides the best balance. At 500 QPS, the 30% throughput advantage of TensorRT-LLM does not justify the deployment complexity. Start with vLLM; migrate to TensorRT-LLM only if traffic grows beyond 2,000 QPS.

```python
# Recommended production configuration
@serve.deployment(
    num_replicas=4,
    ray_actor_options={"num_gpus": 1},
    autoscaling_config={
        "min_replicas": 2,
        "max_replicas": 8,
        "target_num_ongoing_requests_per_replica": 20,
    },
)
class CustomerSupportLLM:
    def __init__(self):
        self.llm = LLM(
            model="your-finetuned-llama-70b-awq",
            quantization="awq",
            max_model_len=32768,
        )
```

This configuration handles the 500 QPS peak with 4 replicas, can scale to 8 during unexpected spikes, and scales down to 2 during low-traffic periods to reduce cost.

## Batching Strategies at Scale {#sec-inference-batching}

Having selected a serving framework, we must now configure how it processes requests. The framework determines which batching implementations are available: vLLM provides continuous batching with PagedAttention, TensorRT-LLM offers in-flight batching with optimized kernels, and Triton supports custom batching policies. Understanding the underlying principles of batching enables effective configuration regardless of framework choice, because where frameworks differ in *how* they batch, the principles we examine here govern *what* batching strategy suits each workload.

The core insight of batching is that processing multiple requests together amortizes fixed costs, including model loading, kernel launch overhead, and memory transfer latency, across more work. This trades higher per-request latency for dramatically improved throughput. Single-machine serving applies this through dynamic batching, which collects requests within a time window before processing them together.

At scale, batching becomes more complex because different model architectures have fundamentally different batching requirements. A strategy optimal for vision models may be catastrophic for LLMs, and techniques developed for recommendation systems may not apply to either.

**Production reality**: Recommendation systems constitute 80-90% of inference requests at major technology companies, with vision models handling most of the remainder, and LLMs currently representing 1-5% of request volume (though growing rapidly). Despite this distribution, we present batching strategies in order of conceptual complexity: vision (straightforward batching), LLMs (continuous batching with KV cache), and recommendation (feature-parallel batching with distributed embedding). This pedagogical ordering builds understanding progressively, even though practitioners will most frequently encounter recommendation workloads first.

This section develops a taxonomy of batching strategies matched to model characteristics, providing quantitative analysis of when each approach applies and what performance to expect.

### Why Batching Differs Across Model Types {#sec-inference-batching-differences}

The core insight is that batching efficiency depends on how computation scales with batch size relative to how memory and communication scale. Different model architectures exhibit different scaling relationships, requiring different batching strategies.

::: {.callout-note title="Figure: Batching Strategies Compared" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  \definecolor{StaticColor}{RGB}{200,220,255}
  \definecolor{ContColor}{RGB}{255,220,200}
  \definecolor{FeatColor}{RGB}{220,255,200}

  \tikzset{
    req/.style={draw=black!70, thick, minimum width=0.8cm, minimum height=0.4cm, font=\tiny},
    group/.style={draw=black!50, dashed, rounded corners=2pt}
  }

  % Static Batching
  \node[anchor=west] at (0, 3.5) {\textbf{A. Static Batching} (Vision)};
  \draw[group, fill=StaticColor!20] (0, 2.2) rectangle (4, 3.2);
  \node[req] at (0.6, 2.7) {R1}; \node[req] at (1.6, 2.7) {R2}; \node[req] at (2.6, 2.7) {R3}; \node[req] at (3.6, 2.7) {R4};
  \node[anchor=west, font=\scriptsize, text=gray] at (4.2, 2.7) {Wait for full batch};

  % Continuous Batching
  \node[anchor=west] at (0, 1.5) {\textbf{B. Continuous Batching} (LLM)};
  \draw[group, fill=ContColor!20] (0, 0.2) rectangle (4, 1.2);
  \node[req, fill=green!20] at (0.6, 0.7) {R1}; \node[req, fill=green!20] at (1.6, 0.7) {R2};
  \node[req, fill=blue!20] at (2.6, 0.7) {R5}; \node[req, fill=gray!20] at (3.6, 0.7) {R6};
  \node[anchor=west, font=\scriptsize, text=gray] at (4.2, 0.7) {Insert mid-generation};

  % Feature Parallel
  \node[anchor=west] at (8, 3.5) {\textbf{C. Feature-Parallel} (RecSys)};
  \draw[group, fill=FeatColor!20] (8, 2.2) rectangle (12, 3.2);
  \node[req] at (8.6, 2.9) {U1}; \node[req] at (9.6, 2.9) {U2}; \node[req] at (10.6, 2.9) {U3};
  \node[req] at (8.6, 2.5) {I1}; \node[req] at (9.6, 2.5) {I2}; \node[req] at (10.6, 2.5) {I3};
  \node[anchor=west, font=\scriptsize, text=gray] at (12.2, 2.7) {Shard by feature};

\end{tikzpicture}
```
**Batching Strategies Compared**. **Static batching** (A) waits for all slots to fill, creating latency bubbles. **Continuous batching** (B) allows requests to join and leave at each iteration, maximizing GPU utilization for LLMs. **Feature-parallel batching** (C) shards requests by feature type (e.g., User IDs vs. Item IDs), optimizing for distributed embedding lookup.
:::

For **vision models** (CNNs, ViTs processing fixed-size images), computation scales linearly with batch size while memory scales sub-linearly due to weight sharing. Larger batches improve GPU utilization with minimal overhead, making static or dynamic batching with large batch sizes optimal.

For **LLMs in the decode phase**, computation per token is small relative to memory bandwidth requirements for loading model weights. The bottleneck is memory bandwidth, not compute. Larger batches amortize weight loading across more tokens, dramatically improving throughput but with diminishing returns as batch size grows.

For **recommendation systems**, the bottleneck is often embedding lookup rather than dense computation. Batching strategies must optimize for parallel embedding access patterns rather than matrix multiplication throughput.

### The Physics of Batching: The Efficiency Curve

Batching is not merely a heuristic; it is a trade-off governed by the physics of hardware utilization. We can model the relationship between Batch Size ($B$), Latency ($L$), and Throughput ($X$) to identify the optimal operating point for any inference system.

The **Latency Equation** decomposes per-request latency into fixed overheads (kernel launch, memory loading) and variable costs (compute per sample):

$$ L(B) = T_{fixed} + B \times T_{variable} $$

*   $T_{fixed}$: Costs paid once per batch (e.g., loading weights from HBM, kernel launch latency).
*   $T_{variable}$: Marginal cost of adding one request (e.g., compute time for that sample).

The **Throughput Equation** describes the system's capacity:

$$ X(B) = \frac{B}{L(B)} = \frac{B}{T_{fixed} + B \times T_{variable}} $$

This relationship reveals the **Batching Efficiency Curve**:
1.  **Small $B$**: Throughput is dominated by $T_{fixed}$. The system is **latency-bound** (or overhead-bound). Increasing $B$ yields super-linear throughput gains.
2.  **Large $B$**: As $B \to \infty$, the $T_{fixed}$ term becomes negligible. Throughput asymptotically approaches the hardware limit $1/T_{variable}$. The system becomes **compute-bound** (or bandwidth-bound for LLMs).
3.  **The Knee**: The optimal batch size is the point where throughput gains diminish while latency continues to grow linearly.

The engineering goal is to find the maximum $B$ such that $L(B) \le \text{SLO}$. This formulation explains why vision models (high $T_{variable}$) saturate at smaller batches than LLMs (high $T_{fixed}$ due to weight loading), requiring different tuning strategies. @tbl-batching-by-model summarizes how these batching characteristics vary across model architectures.

::: {.callout-notebook title="Engineering Calculation: Little's Law for Inference" collapse="true"}
**Concept**: In any stable queuing system, the average number of requests in the system ($L$) equals the arrival rate ($\lambda$) multiplied by the average time a request spends in the system ($W$).

$$ L = \lambda \times W $$

**Application: Concurrency Planning**
- **Target Throughput ($\lambda$)**: 1,000 requests/sec
- **Latency SLO ($W$)**: 100 ms (0.1 s)

**Required Concurrency ($L$)**:
$$ L = 1000 \times 0.1 = \mathbf{100 \text{ concurrent requests}} $$

**Capacity Planning**:
If a single GPU replica handles batch size 8 with 80ms latency:
1. Replica Throughput = $8 / 0.08 = 100 \text{ req/s}$
2. Replicas Needed = $1000 / 100 = 10 \text{ replicas}$

**Verification**:
Total system concurrency = $10 \text{ replicas} \times 8 \text{ batch} = 80$.
Wait! We needed 100.
*Correction*: We must account for queue depth. With 10 replicas active processing 80 reqs, 20 reqs are in queues.
Queue wait time added to latency must not violate SLO.
:::

+-------------------+-----------------------+------------------------+--------------------+------------------------+
| **Model Type**    | **Batching Strategy** | **Typical Batch Size** | **Key Constraint** | **Throughput Scaling** |
+:==================+:======================+=======================:+:===================+:=======================+
| **Vision (CNN)**  | Static/Dynamic        | 32-256                 | GPU compute        | Near-linear to 64+     |
| **LLM (prefill)** | Dynamic               | 1-64                   | Memory capacity    | Sub-linear             |
| **LLM (decode)**  | Continuous            | 100-1000s              | Memory bandwidth   | Log-linear             |
| **RecSys**        | Feature-parallel      | 1000-10000s            | Embedding lookup   | Depends on sharding    |
| **Speech**        | Streaming             | 1                      | Real-time          | N/A (latency-bound)    |
+-------------------+-----------------------+------------------------+--------------------+------------------------+

: **Batching Strategy by Model Type**: Each model type has characteristic batching behavior determined by its computational bottleneck. {#tbl-batching-by-model}

### Static and Dynamic Batching for Vision Models {#sec-inference-static-dynamic-batching}

Vision models represent the simplest batching case because inputs have uniform size (after preprocessing) and computation follows a predictable pattern. Single-machine batching principles apply directly, with scale introducing considerations of batch formation across multiple replicas.

Static batching collects exactly $B$ requests before processing. This maximizes GPU utilization when request arrival is predictable but causes unbounded latency during low-traffic periods.

Dynamic batching collects requests for a maximum time window $T_{window}$ or until reaching maximum batch size $B_{max}$, whichever occurs first. The expected latency under Poisson arrivals with rate $\lambda$ follows @eq-dynamic-batch-latency:

$$E[L_{total}] = E[L_{queue}] + L_{batch} + L_{inference}(B)$$ {#eq-dynamic-batch-latency}

where $E[L_{queue}]$ is the expected queuing delay, $L_{batch}$ is the batch formation delay (up to $T_{window}$), and $L_{inference}(B)$ is the inference time for batch size $B$.

::: {.callout-note title="Worked Example: Dynamic Batching for ResNet-50 at Scale"}

Consider a vision classification service with the following requirements:

- **Arrival rate**: 5,000 QPS
- **Latency SLO**: 50ms P99
- **Per-image inference time**: 5ms at batch=1, 25ms at batch=32
- **Number of replicas**: 10 (each handling 500 QPS)

For a single replica with Poisson arrivals at $\lambda = 500$ QPS:

**Option A: No batching (batch=1)**

- Service time: 5ms per request
- Utilization: $\rho = \lambda \times S = 500 \times 0.005 = 2.5$ (impossible, system is overloaded)

This configuration cannot meet demand. Batching is required.

**Option B: Dynamic batching with $B_{max}=16$, $T_{window}=10ms$**

Expected requests per window: $E[B] = \lambda \times T_{window} = 500 \times 0.01 = 5$

With 5 requests per batch:

- Inference time: approximately 8ms (interpolating between batch=1 and batch=32)
- Per-request compute: 8ms / 5 = 1.6ms
- Maximum batch delay: 10ms
- Expected total latency: ~15ms mean, ~30ms P99

Utilization: $\rho = 500 \times 0.0016 = 0.8$ (sustainable)

**Option C: Dynamic batching with $B_{max}=32$, $T_{window}=20ms$**

Expected requests per window: $E[B] = 500 \times 0.02 = 10$

With 10 requests per batch:

- Inference time: approximately 12ms
- Per-request compute: 12ms / 10 = 1.2ms
- Maximum batch delay: 20ms
- Expected total latency: ~22ms mean, ~42ms P99

Utilization: $\rho = 500 \times 0.0012 = 0.6$ (comfortable)

**Tradeoff**: Option C achieves 25% better throughput (lower utilization) at the cost of higher average latency (22ms vs 15ms). Both meet the 50ms P99 SLO.

:::

At scale with multiple replicas, batch formation can occur either at individual replicas or at a centralized batching layer. Replica-local batching has each replica independently form batches from its assigned traffic. This approach is simpler to implement but may result in uneven batch sizes across replicas when load is imbalanced. Centralized batching uses a batching service to collect requests and dispatch formed batches to replicas. This achieves more uniform batch sizes but adds a centralization bottleneck and additional network hop.

Production systems typically use replica-local batching with load balancing that ensures roughly equal traffic distribution, achieving the benefits of centralized batching without the complexity.

### Continuous Batching for LLM Inference {#sec-inference-continuous-batching}

Autoregressive language models present a unique batching challenge that static and dynamic approaches handle poorly. The key insight comes from the Orca system[^fn-orca] [@yu2022orca]: traditional batching forces all sequences in a batch to complete before any new sequences can join, wasting compute when sequences finish at different times.

[^fn-orca]: **Orca**: Named after the killer whale known for highly coordinated group hunting, Orca pioneered iteration-level scheduling for LLM serving at Microsoft in 2022. The system's insight that sequences could enter and exit batches at each decode step, rather than waiting for entire batches to complete, transformed LLM serving economics.

Consider a batch of 8 sequences. If one sequence completes after 10 tokens while others require 100 tokens, the completed sequence's GPU resources sit idle for 90 iterations. With traditional batching:

$$\text{Wasted compute} = \frac{(100 - 10) \times 1}{100 \times 8} = 11.25\%$$

For realistic output length distributions with high variance, wasted compute can exceed 50%.

Continuous batching (also called iteration-level batching) decouples batch membership from iteration boundaries. At each decode iteration, the system checks for completed sequences, removes completed sequences from the batch immediately, inserts waiting sequences into freed slots, and processes the reorganized batch for the next iteration.

::: {.callout-note title="Archetype A Connection: The Throughput-Latency Decoupling"}
**Archetype A (The Trillion-Parameter LLM)** relies on continuous batching to solve its primary efficiency paradox. The decode phase is memory-bandwidth bound, meaning the GPU compute cores are idle waiting for weights to load. Continuous batching saturates this bandwidth by processing unrelated requests together. Without this technique, serving Archetype A models would be economically unviable due to low GPU utilization.
:::

### Memory Management and PagedAttention {#sec-inference-memory-management}

This dynamic batch management maintains high GPU utilization regardless of sequence length variance.

The throughput improvement from continuous batching depends on sequence length distribution. For a distribution with coefficient of variation $CV = \sigma / \mu$, the gain is approximately @eq-continuous-batching-gain:

$$\text{Throughput gain} \approx 1 + \frac{CV^2}{2}$$ {#eq-continuous-batching-gain}

With typical LLM output lengths having $CV \approx 1.0$, continuous batching achieves approximately 1.5x throughput improvement. For highly variable outputs (conversational vs. code generation), gains can reach 2-4x.

::: {.callout-note title="Implementation: Continuous Batching in vLLM"}

vLLM implements continuous batching with several key mechanisms. Iteration-level scheduling evaluates at each decode step which sequences have generated end-of-sequence tokens (remove from batch), which waiting sequences can fit in available KV cache slots (add to batch), and which sequences should be preempted if memory pressure exists (swap to CPU). Memory management uses PagedAttention (detailed in @sec-inference-kv-cache), which enables dynamic allocation without fragmentation. When a sequence completes, its KV cache pages are immediately available for new sequences. The batched decode kernel processes all active sequences in a single batched operation despite dynamic batch composition. Sequences at different generation lengths are padded to a common shape within the kernel.

#### Preemption and Swapping

A critical challenge in continuous batching is memory contention. As sequences grow during generation, they consume more KV cache pages. If the GPU memory fills up, the system cannot simply crash; it must preempt running requests.

vLLM implements a virtual memory mechanism similar to an operating system's swap. When memory is exhausted, the scheduler identifies low-priority requests (e.g., those most recently started) and **swaps** their KV cache blocks from GPU HBM to CPU DRAM. These requests are paused until memory becomes available, at which point they are swapped back in and resumed. This mechanism ensures system stability under heavy load at the cost of increased latency for preempted requests.

**Typical performance (Llama-2 70B on 8xA100)**:

+----------------------------+---------------------------+---------------------+
| **Batching Strategy**      | **Throughput (tokens/s)** | **GPU Utilization** |
+:===========================+==========================:+====================:+
| **Static (batch=8)**       | 400                       | 45%                 |
| **Dynamic (timeout=50ms)** | 580                       | 65%                 |
| **Continuous**             | 1,200                     | 92%                 |
+----------------------------+---------------------------+---------------------+

The 3x throughput improvement from continuous batching comes from eliminating idle GPU cycles during sequence length variation.

:::

### Prefill vs Decode: The Two-Phase Challenge {#sec-inference-prefill-decode}

LLM inference consists of two distinct phases with different computational characteristics, requiring different batching strategies within the same request. @tbl-prefill-decode contrasts these phases:

**Prefill phase**: Process the entire input prompt in parallel. Computation scales with prompt length. Memory access pattern is compute-bound (high arithmetic intensity[^fn-arithmetic-intensity]).

**Decode phase**: Generate output tokens one at a time. Each token requires loading entire model weights. Memory access pattern is bandwidth-bound (low arithmetic intensity).

[^fn-arithmetic-intensity]: **Arithmetic intensity**: The ratio of compute operations to memory accesses, measured in FLOPs per byte. Prefill achieves 100+ FLOPs/byte (compute-bound), while decode achieves 1-10 FLOPs/byte (memory-bound). This 10-100x difference explains why prefill and decode require fundamentally different optimization strategies.

+-------------+-------------------+-------------------+----------------+-------------------+
| **Phase**   | **Computation**   | **Memory Access** | **Bottleneck** | **Optimal Batch** |
+:============+==================:+:==================+:===============+==================:+
| **Prefill** | O(prompt_length²) | Weight loading    | Compute        | Small (1-8)       |
| **Decode**  | O(1) per token    | Weight loading    | Bandwidth      | Large (100s)      |
+-------------+-------------------+-------------------+----------------+-------------------+

: **Prefill vs Decode Characteristics**: The two phases have opposite optimization requirements. {#tbl-prefill-decode}

This dichotomy creates a scheduling challenge: prefill operations are long-running and compute-intensive, while decode operations are short and bandwidth-limited. Mixing them in the same batch can cause interference. This creates the **Static Power Waste** problem analyzed in @sec-sustainable-ai, where low-utilization decode steps dominate energy consumption.

**Chunked prefill**[^fn-chunked-prefill] addresses this by breaking long prompts into fixed-size chunks that interleave with decode operations:

[^fn-chunked-prefill]: **Chunked prefill motivation**: Without chunking, a 32K-token prompt blocks decode for 5-30 seconds while prefill completes. Chunking divides the prompt into 256-1024 token chunks processed between decode iterations, bounding decode latency at the cost of slightly longer prefill time. This trade-off favors interactive applications where decode responsiveness matters more than total completion time.

$$\text{Chunk latency} = \frac{\text{Chunk size}}{\text{Prefill throughput}}$$

With chunk size chosen to match decode iteration time, prefill and decode can share GPU resources without decode latency spikes.

**Prefill-decode disaggregation** takes this further by running prefill and decode on separate GPU pools:

- **Prefill pool**: Optimized for compute intensity (e.g., H100 nodes with high TFLOPS) using large batch sizes to maximize throughput.
- **Decode pool**: Optimized for memory bandwidth (e.g., nodes with maximum HBM capacity or specialized High Bandwidth Flash as discussed in @sec-storage) to handle thousands of concurrent autoregressive streams.

This separation enables independent scaling: prefill capacity scales with input volume while decode capacity scales with output volume. Crucially, this architecture relies on the high-speed, RDMA-enabled networking fabric established in @sec-cluster-networking. When a prefill finishes, the resulting KV cache—often megabytes of data—must be migrated to a decode node within the inter-token latency budget (typically <10ms). InfiniBand's sub-microsecond latency and high bisection bandwidth are the physical enablers of this logical disaggregation.

::: {.callout-note title="Sarathi: Chunked Prefill Implementation"}

The Sarathi system [@agrawal2023sarathi] implements chunked prefill with the following design:

**Chunk sizing**: Chunks are sized to complete in approximately the same time as one decode iteration (typically 10-50ms). For a prefill throughput of 10,000 tokens/second, a 20ms chunk processes 200 tokens.

**Interleaving schedule**: Each GPU iteration processes either:

- One prefill chunk for a new request, OR
- One decode step for all active sequences

This ensures decode latency remains bounded regardless of incoming prompt lengths.

**KV cache transfer**: When prefill completes, the generated KV cache transfers to decode slots. With NVLink, this transfer adds <1ms for typical prompt lengths.

**Performance impact**:

- Without chunking: Long prompts cause decode latency spikes of 100ms+
- With chunking: Decode latency bounded to 30ms P99 regardless of prompt length

:::

### Feature-Parallel Batching for Recommendation Systems {#sec-inference-feature-parallel-batching}

Recommendation systems have fundamentally different batching requirements than vision or language models. The computation pattern involves:

1. **Sparse feature lookup**: Retrieve embeddings for user, item, and context features
2. **Dense feature processing**: Transform and normalize dense features
3. **Feature interaction**: Compute interactions between features (often via attention or factorization)
4. **Ranking head**: Produce final scores

The sparse embedding lookup often dominates latency and determines batching strategy.

**Feature-parallel batching** processes different feature types in parallel rather than batching entire requests:

```
Request 1: [user_id_1, item_ids_1, context_1]
Request 2: [user_id_2, item_ids_2, context_2]
Request 3: [user_id_3, item_ids_3, context_3]

Feature-parallel view:
User embeddings:  [lookup(user_1), lookup(user_2), lookup(user_3)]  → parallel
Item embeddings:  [lookup(items_1), lookup(items_2), lookup(items_3)]  → parallel
Context features: [process(ctx_1), process(ctx_2), process(ctx_3)]  → parallel

Then: Combine features per request for ranking
```

This parallelization is natural when embeddings are sharded across servers: each embedding server handles lookups for its shard across all requests in the batch.

::: {.callout-note title="Worked Example: Recommendation System Batching at Meta Scale"}

Consider Meta's recommendation infrastructure serving 10 million QPS across the platform:

**Request characteristics**:

- Each request queries ~100 items (candidate ranking)
- Each item requires 50 embedding lookups (user features, item features, cross features)
- Total embedding lookups: 5,000 per request
- Embedding table size: 100TB across 1,000 shards

**Batching strategy**:

With 10M QPS and 1,000 embedding shards, each shard receives:

$$\text{Lookups per shard} = \frac{10M \times 5000}{1000} = 50 \text{ billion lookups/sec}$$

This is clearly infeasible for single-threaded processing. Instead:

**Batch accumulation window**: 1ms
**Requests per batch**: 10,000 (at 10M QPS)
**Lookups per shard per batch**: 50M

Each embedding shard processes 50M lookups in a batched operation, achieving memory bandwidth utilization of 90%+ through sequential memory access patterns.

**Latency breakdown**:

+------------------------+--------------+-----------------------------+
| **Phase**              | **Duration** | **Notes**                   |
+:=======================+=============:+:============================+
| **Request routing**    | 0.2ms        | Consistent hashing to shard |
| **Batch accumulation** | 0.5ms (avg)  | 1ms window                  |
| **Embedding lookup**   | 2ms          | Batched, SSD-backed         |
| **Feature processing** | 1ms          | Dense computation           |
| **Ranking model**      | 1.5ms        | Final scoring               |
| **Total**              | **5.2ms**    | Within 10ms SLO             |
+------------------------+--------------+-----------------------------+

:::

### Streaming Inference for Real-Time Applications {#sec-inference-streaming}

Some applications cannot tolerate batching delay of any kind. Real-time speech recognition, video analysis, and robotics require processing inputs as they arrive with minimal latency.

**Streaming inference** processes inputs incrementally without waiting for batch formation:

- **Speech**: Process audio frames (10-20ms chunks) as they arrive from the microphone
- **Video**: Process frames at capture rate (30-60 FPS) without buffering
- **Robotics**: Process sensor readings at control loop frequency (100-1000 Hz)

For streaming applications, the relevant metric is not throughput but **time to process each input**:

$$L_{streaming} = L_{capture} + L_{transfer} + L_{inference} + L_{action}$$

where all components must complete within the inter-frame interval.

::: {.callout-note title="Streaming Speech Recognition Pipeline"}

Consider a streaming speech-to-text system with 20ms audio frames:

**Latency budget**: 100ms end-to-end (5 frames of delay)

**Pipeline stages**:

+------------------------+------------------+-----------------------------+
| **Stage**              | **Duration**     | **Notes**                   |
+:=======================+=================:+:============================+
| **Audio capture**      | 0ms (continuous) | Microphone buffer           |
| **Network to server**  | 20ms             | Including jitter buffer     |
| **Feature extraction** | 5ms              | MFCC computation            |
| **Encoder inference**  | 30ms             | Streaming Conformer         |
| **Decoder step**       | 15ms             | Autoregressive CTC          |
| **Text formatting**    | 5ms              | Capitalization, punctuation |
| **Network to client**  | 15ms             | Response transmission       |
| **Total**              | **90ms**         | Within 100ms budget         |
+------------------------+------------------+-----------------------------+

**Key constraints**:

- No batching: Each frame processes individually
- Stateful model: Encoder maintains context across frames
- Pipeline parallelism: While frame N is in decoder, frame N+1 is in encoder

GPU utilization is typically 30-50% for streaming workloads, traded for latency guarantee.

:::

### Adaptive Batching Strategies {#sec-inference-adaptive-batching}

Production systems rarely use fixed batching parameters. Instead, they adapt batching behavior based on current conditions:

**Traffic-adaptive batching** adjusts batch window based on arrival rate:

$$T_{window} = \min\left(T_{max}, \frac{B_{target}}{\lambda_{current}}\right)$$

When traffic is high, the window shrinks because the target batch size fills quickly. When traffic is low, the window extends but is capped to bound maximum latency.

**SLO-adaptive batching** monitors latency percentiles and adjusts batching aggressively:

```
if P99_latency > 0.9 * SLO:
    reduce B_max by 20%
    reduce T_window by 20%
elif P99_latency < 0.5 * SLO:
    increase B_max by 10%
    increase T_window by 10%
```

This feedback loop maintains latency headroom while maximizing throughput during normal operation.

**Request-aware batching** considers request characteristics when forming batches. For LLMs:

- Group requests by expected output length (inferred from prompt type)
- Group requests by prompt length to minimize padding
- Prioritize latency-sensitive requests in smaller batches

::: {.callout-note title="Production Adaptive Batching: The NVIDIA Triton Approach"}

Triton Inference Server implements adaptive batching with three configurable parameters:

1. **max_batch_size**: Upper bound on batch size
2. **batching_timeout_ms**: Maximum time to wait for batch formation
3. **preferred_batch_size**: Target batch sizes that align with kernel efficiency

The scheduler maintains separate queues for each preferred batch size and routes requests to minimize total latency:

$$\text{Queue selection} = \arg\min_{q} \left( \text{wait}_q + \text{exec}(|q| + 1) \right)$$

This optimization considers both the current queue length and the efficiency of the resulting batch size.

**Observed behavior on ResNet-50 (V100)**:

+-------------------+--------------------+-----------------+----------------+
| **Traffic Level** | **Avg Batch Size** | **Avg Latency** | **Throughput** |
+==================:+===================:+================:+===============:+
| **100 QPS**       | 2.1                | 8ms             | 100 QPS        |
| **500 QPS**       | 6.3                | 12ms            | 500 QPS        |
| **1000 QPS**      | 12.4               | 18ms            | 1000 QPS       |
| **2000 QPS**      | 24.1               | 28ms            | 1980 QPS       |
+-------------------+--------------------+-----------------+----------------+

The system automatically increases batch size to maintain throughput as traffic grows.

:::

### Quantitative Summary: Batching Strategy Selection {#sec-inference-batching-summary}

The choice of batching strategy depends on model characteristics, traffic patterns, and latency requirements. @fig-inference-lifecycle visualizes the end-to-end request path, highlighting latency sources at each stage. The following decision framework guides selection:

::: {#fig-inference-lifecycle fig-env="figure" fig-pos="htb" fig-cap="**End-to-End Inference Pipeline**. A high-level view of the request lifecycle: Client -> Load Balancer -> Request Queue -> Batch Scheduler -> Model Execution -> Response. This visualization highlights the critical \"Serving Tax\" components (serialization, routing, coordination) that consume latency budget outside of the actual GPU compute time."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm]
  \definecolor{ProcessColor}{RGB}{240,248,255}
  \definecolor{NetworkColor}{RGB}{255,245,238}
  \definecolor{QueueColor}{RGB}{245,245,245}

  \tikzset{
    process/.style={draw=black!70, thick, fill=ProcessColor, rounded corners=2pt, minimum width=2.5cm, minimum height=1.0cm, align=center},
    queue/.style={draw=black!70, thick, fill=QueueColor, shape=cylinder, shape border rotate=90, aspect=0.25, minimum width=1.5cm, minimum height=1.2cm, align=center},
    arrow/.style={->, >=stealth, thick, color=black!80}
  }

  % Nodes
  \node[process, fill=white] (client) {Client\\Application};
  \node[process, fill=NetworkColor, right=of client] (lb) {Load\\Balancer};
  \node[queue, right=of lb] (queue) {Request\\Queue};
  \node[process, right=of queue] (batcher) {Dynamic\\Batcher};
  \node[process, right=of batcher, fill=ProcessColor!80!blue] (gpu) {Model\\Execution (GPU)};

  % Edges
  \draw[arrow] (client) -- node[above, font=\scriptsize] {Network} (lb);
  \draw[arrow] (lb) -- node[above, font=\scriptsize] {Routing} (queue);
  \draw[arrow] (queue) -- node[above, font=\scriptsize] {Wait} (batcher);
  \draw[arrow] (batcher) -- node[above, font=\scriptsize] {Bus} (gpu);

  % Return path
  \draw[arrow, dashed] (gpu.south) -- +(0,-0.5) -| node[near start, below, font=\scriptsize] {Response Path} (client.south);

  % Latency Sources
  \node[below=0.8cm of lb, font=\scriptsize, align=center, color=red!70!black] {Parsing \&\\Routing Latency};
  \node[below=0.8cm of queue, font=\scriptsize, align=center, color=red!70!black] {Queuing\\Delay};
  \node[below=0.8cm of batcher, font=\scriptsize, align=center, color=red!70!black] {Batch\\Sync Time};
  \node[below=0.8cm of gpu, font=\scriptsize, align=center, color=red!70!black] {Compute\\Latency};

\end{tikzpicture}
```
:::

```
Is the model autoregressive (LLM, speech)?
├─ Yes → Continuous batching with prefill chunking
└─ No → Does the model have embedding lookups dominating latency?
        ├─ Yes → Feature-parallel batching (RecSys)
        └─ No → Dynamic batching with adaptive parameters
```

@tbl-batching-parameters summarizes the key tuning parameters for each strategy:

+----------------------+-----------------------+----------------------------------+
| **Strategy**         | **Key Parameters**    | **Tuning Goal**                  |
+:=====================+:======================+:=================================+
| **Static**           | Batch size            | Maximize throughput              |
| **Dynamic**          | Window, max batch     | Balance latency vs throughput    |
| **Continuous**       | Chunk size, max batch | Minimize decode latency variance |
| **Feature-parallel** | Accumulation window   | Match embedding shard capacity   |
| **Streaming**        | Pipeline depth        | Meet real-time deadline          |
+----------------------+-----------------------+----------------------------------+

: **Batching Strategy Parameters**: Each strategy has distinct parameters requiring tuning for the specific deployment. {#tbl-batching-parameters}

## Model Sharding for Inference {#sec-inference-sharding}

Batching strategies from the previous section address how to efficiently process requests on a given model replica. But what constitutes a "replica" when models exceed single-GPU memory? And how can we reduce latency when even batch-size-one inference is too slow? Model sharding answers both questions by distributing inference across multiple devices.

The parallelism strategies established for distributed training in @sec-distributed-training, including tensor parallelism, pipeline parallelism, and their communication patterns, apply to inference with an important inversion: training optimizes throughput over hours while inference must minimize per-request latency within milliseconds. This changes how we configure sharding strategies. Unlike training sharding where throughput is the primary concern, inference sharding must carefully balance parallelization benefits against communication overhead within strict latency budgets. This moves us from request-level optimization to replica-level concerns in the serving hierarchy.

This section examines four sharding strategies, each suited to different model architectures and deployment requirements: tensor parallelism for attention-heavy models, pipeline parallelism for sequential architectures, expert parallelism for mixture-of-experts models, and embedding sharding for recommendation systems.

### When Sharding Becomes Necessary {#sec-inference-sharding-when}

Model sharding for inference is driven by two distinct requirements. @tbl-sharding-triggers identifies the memory and latency constraints that necessitate sharding:

**Memory requirements**: A model that cannot fit in single-GPU memory must be sharded regardless of performance considerations. For a model with $P$ parameters at precision $b$ bits, the weight memory is calculated by @eq-weight-memory:

$$\text{Memory}_{weights} = P \times \frac{b}{8} \text{ bytes}$$ {#eq-weight-memory}

A 70-billion parameter model in FP16 (16 bits) requires:

$$\text{Memory} = 70 \times 10^9 \times \frac{16}{8} = 140 \text{ GB}$$

This exceeds the 80GB capacity of an H100 GPU, requiring at minimum 2-way sharding.

**Latency requirements**: Even when a model fits in memory, sharding can reduce latency by parallelizing computation. @eq-parallel-time formalizes the potential speedup as a function of parallelization efficiency:

$$T_{parallel} = \frac{T_{sequential}}{P} + T_{communication}$$ {#eq-parallel-time}

where $P$ is the parallelism degree and $T_{communication}$ is the synchronization overhead. Sharding provides latency benefit only when the communication overhead is smaller than the time saved through parallelization.

+-------------------------+----------------------+----------------------+--------------------+
| **Sharding Trigger**    | **Model Examples**   | **Minimum Sharding** | **Strategy**       |
+:========================+=====================:+=====================:+:===================+
| **Memory (weights)**    | Llama-70B (140GB)    | 2-way                | Tensor or pipeline |
| **Memory (KV cache)**   | GPT-4 (long context) | 4-8 way              | Tensor (for cache) |
| **Memory (embeddings)** | DLRM (100TB)         | 1000+ way            | Embedding sharding |
| **Latency**             | Any large model      | Varies               | Tensor parallelism |
+-------------------------+----------------------+----------------------+--------------------+

: **Sharding Triggers**: Different constraints lead to different sharding requirements and strategies. {#tbl-sharding-triggers}

### Tensor Parallelism {#sec-inference-tensor-parallelism}

Tensor parallelism [@shoeybi2019megatron] distributes individual layers across multiple devices, enabling parallel computation within each layer. The column-row partitioning scheme introduced for training in @sec-distributed-training applies here: splitting the first linear layer by columns and the second by rows requires only one AllReduce per transformer block. For transformer models, the primary target is the attention mechanism and feed-forward layers, which contain the majority of computation.

**Attention layer parallelism**: The multi-head attention computation naturally partitions across attention heads. For a model with $H$ attention heads distributed across $P$ devices, each device computes $H/P$ heads:

$$\text{Attention}_i = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i \text{ for heads } i \in \{1, ..., H/P\}$$

After computing local attention, an AllReduce operation (covered in @sec-communication) combines results across devices, adding communication overhead proportional to the activation size divided by the interconnect bandwidth.

**Feed-forward layer parallelism**: The feed-forward layer (typically two linear transformations with activation) partitions along the hidden dimension. For the first linear layer, columns are distributed; for the second, rows are distributed. This column-row partitioning requires only one all-reduce per feed-forward block.

The communication pattern for tensor-parallel inference follows a two-phase synchronization per layer. @fig-tensor-parallel-flow illustrates this flow:

::: {#fig-tensor-parallel-flow fig-env="figure" fig-pos="htb" fig-cap="**Tensor Parallelism for Inference**. Computation is distributed across devices by splitting tensor operations. Attention heads are partitioned across GPUs, requiring an AllReduce operation to synchronize results. Feed-forward networks use a column-row splitting strategy that requires only one AllReduce synchronization per block. This approach reduces latency for large models but introduces communication overhead that demands high-bandwidth interconnects like NVLink."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{GPUColor}{RGB}{230,230,250}
  \definecolor{OpColor}{RGB}{240,248,255}
  \definecolor{SyncColor}{RGB}{255,228,225}

  % GPU Columns
  \draw[fil=GPUColor, draw=none] (-2, -0.5) rectangle (1.5, 6);
  \node[above, font=\bfseries] at (-0.25, 6) {GPU 0};

  \draw[fill=GPUColor, draw=none] (2.5, -0.5) rectangle (6, 6);
  \node[above, font=\bfseries] at (4.25, 6) {GPU 1};

  % Attention Block
  \node[anchor=center] at (2, 5.5) {\textbf{Attention Layer}};

  \node[draw, fill=OpColor, minimum width=2.5cm] (att0) at (-0.25, 4.5) {Heads 1..H/2};
  \node[draw, fill=OpColor, minimum width=2.5cm] (att1) at (4.25, 4.5) {Heads H/2..H};

  \node[draw, fill=SyncColor, rounded corners, minimum width=6cm] (allreduce1) at (2, 3.5) {\textbf{AllReduce} (Combine Heads)};

  \draw[->, thick] (att0) -- (allreduce1.north -| att0);
  \draw[->, thick] (att1) -- (allreduce1.north -| att1);

  % MLP Block
  \node[anchor=center] at (2, 2.5) {\textbf{Feed-Forward (MLP)}};

  % Col Split
  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp1a) at (-0.9, 1.5) {Col};
  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp1b) at (0.4, 1.5) {Col};
  \node at (-0.25, 1.5) {...}; % Visualizing big matrix split

  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp2a) at (3.6, 1.5) {Col};
  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp2b) at (4.9, 1.5) {Col};
  \node at (4.25, 1.5) {...};

  % Row Split
  \node[draw, fill=OpColor, minimum width=2.5cm] (mlp1_out) at (-0.25, 0.5) {Row Split};
  \node[draw, fill=OpColor, minimum width=2.5cm] (mlp2_out) at (4.25, 0.5) {Row Split};

  \draw[->, thick] (allreduce1.south -| att0) -- (mlp1a.north); % Simplified connection
  \draw[->, thick] (allreduce1.south -| att1) -- (mlp2a.north);

  \draw[->] (mlp1a.south) -- (mlp1_out.north);
  \draw[->] (mlp2a.south) -- (mlp2_out.north);

  \node[draw, fill=SyncColor, rounded corners, minimum width=6cm] (allreduce2) at (2, -0.5) {\textbf{AllReduce} (Sum Outputs)};

  \draw[->, thick] (mlp1_out.south) -- (allreduce2.north -| mlp1_out);
  \draw[->, thick] (mlp2_out.south) -- (allreduce2.north -| mlp2_out);

\end{tikzpicture}
```
:::

The inference time with tensor parallelism follows @eq-tensor-parallel-time:

$$T_{inference} = \frac{T_{compute}}{P} + 2 \times T_{allreduce}\left(\frac{A}{P}\right)$$ {#eq-tensor-parallel-time}

where $T_{compute}$ is the sequential compute time, $P$ is the parallelism degree, and $A$ is the activation size being reduced. The factor of 2 accounts for the two all-reduce operations per transformer layer (attention and feed-forward).

::: {.callout-note title="Worked Example: Tensor Parallelism for Llama-70B"}

Consider serving Llama-70B with the following configuration:

**Model specifications**:

- Parameters: 70 billion
- Hidden dimension: 8,192
- Attention heads: 64
- Layers: 80

**Memory per GPU** (weight only, FP16):

$$\text{Memory}_{70B} = 70 \times 10^9 \times 2 = 140\text{ GB}$$

**Minimum sharding**: 2-way (140GB / 80GB per H100)

**Recommended sharding**: 8-way for optimal latency

**With 8-way tensor parallelism on 8xH100 (NVLink interconnect)**:

+---------------------------+------------------------+--------------+-------------+
| **Component**             | **Sequential (1 GPU)** | **8-way TP** | **Speedup** |
+:==========================+=======================:+=============:+:============+
| **Attention compute**     | 12ms                   | 1.5ms        | 8x          |
| **AllReduce (attention)** | 0ms                    | 0.3ms        | N/A         |
| **Feed-forward compute**  | 18ms                   | 2.25ms       | 8x          |
| **AllReduce (FF)**        | 0ms                    | 0.3ms        | N/A         |
| **Total per layer**       | **30ms**               | **4.35ms**   | **6.9x**    |
+---------------------------+------------------------+--------------+-------------+

**For 80 layers**:

- Sequential: 2,400ms per token
- 8-way TP: 348ms per token

The 6.9x speedup (vs theoretical 8x) reflects communication overhead. With 600 GB/s NVLink bandwidth, each 8MB activation all-reduce takes ~0.3ms.

**Time-to-first-token** (1024-token prompt):

- Prefill compute: ~50ms (compute-bound, near-linear scaling)
- Total TTFT: ~60ms with preprocessing

:::

### Pipeline Parallelism for Inference {#sec-inference-pipeline-parallelism}

Pipeline parallelism distributes layers across devices sequentially, with each device handling a subset of layers. Unlike tensor parallelism, there is no synchronization within a layer, only between pipeline stages.

For inference, pipeline parallelism creates bubbles differently than in training. @fig-pipeline-bubbles contrasts single-request latency (bubble-dominated) with pipelined throughput (bubble-amortized):

::: {#fig-pipeline-bubbles fig-env="figure" fig-pos="htb" fig-cap="**Pipeline Parallelism Bubbles**. For a single inference request, pipeline parallelism offers no latency benefit as the request must traverse all stages sequentially (top). However, when processing multiple concurrent requests, pipeline bubble utilization improves significantly (bottom), allowing throughput to scale with the number of stages. This makes pipeline parallelism ideal for high-throughput batch processing but less suitable for latency-critical interactive serving."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{Stage1}{RGB}{173,216,230}
  \definecolor{Stage2}{RGB}{144,238,144}
  \definecolor{Stage3}{RGB}{255,255,224}
  \definecolor{Bubble}{RGB}{240,240,240}

  \tikzset{
    block/.style={draw=black!50, minimum height=0.6cm, minimum width=1.4cm, font=\scriptsize},
    label/.style={font=\footnotesize\bfseries}
  }

  % Scenario A: Single Request (Sequential)
  \node[label, anchor=west] at (0, 3.5) {A. Single Request (Latency Bound)};

  % Dev 0
  \node[anchor=east] at (0, 2.8) {GPU 0};
  \node[block, fill=Stage1] at (0.8, 2.8) {Req1};
  \node[block, fill=Bubble, minimum width=2.8cm] at (3.0, 2.8) {Idle};

  % Dev 1
  \node[anchor=east] at (0, 2.0) {GPU 1};
  \node[block, fill=Bubble, minimum width=1.4cm] at (0.8, 2.0) {Idle};
  \node[block, fill=Stage2] at (2.3, 2.0) {Req1};
  \node[block, fill=Bubble, minimum width=1.4cm] at (3.7, 2.0) {Idle};

  % Dev 2
  \node[anchor=east] at (0, 1.2) {GPU 2};
  \node[block, fill=Bubble, minimum width=2.8cm] at (1.5, 1.2) {Idle};
  \node[block, fill=Stage3] at (3.7, 1.2) {Req1};

  % Total Time Arrow
  \draw[<->] (0.1, 0.8) -- (4.4, 0.8) node[midway, below, font=\scriptsize] {Total Latency = Sum of Stages};


  % Scenario B: Pipelined (Throughput)
  \node[label, anchor=west] at (0, -0.5) {B. Pipelined Batch (Throughput)};

  % Shift x origin for alignment
  \begin{scope}[shift={(0, -3)}]
      % Dev 0
      \node[anchor=east] at (0, 2.0) {GPU 0};
      \node[block, fill=Stage1] at (0.8, 2.0) {Req1};
      \node[block, fill=Stage1] at (2.3, 2.0) {Req2};
      \node[block, fill=Stage1] at (3.8, 2.0) {Req3};

      % Dev 1
      \node[anchor=east] at (0, 1.2) {GPU 1};
      \node[block, fill=Bubble] at (0.8, 1.2) {Idle};
      \node[block, fill=Stage2] at (2.3, 1.2) {Req1};
      \node[block, fill=Stage2] at (3.8, 1.2) {Req2};
      \node[block, fill=Stage2] at (5.3, 1.2) {Req3};

      % Dev 2
      \node[anchor=east] at (0, 0.4) {GPU 2};
      \node[block, fill=Bubble, minimum width=2.8cm] at (1.5, 0.4) {Idle};
      \node[block, fill=Stage3] at (3.8, 0.4) {Req1};
      \node[block, fill=Stage3] at (5.3, 0.4) {Req2};

      % Note
      \node[anchor=west, font=\scriptsize, align=left] at (6.5, 1.2) {High Throughput\\after fill};
  \end{scope}

\end{tikzpicture}
```
:::

For a single request, pipeline parallelism provides no latency benefit: the request must traverse all stages sequentially. The pipeline fill time equals the sequential execution time.

However, pipeline parallelism enables **throughput scaling** through pipelining multiple requests:

```
Time →
Device 0: [Req1] [Req2] [Req3] [Req4] ...
Device 1:        [Req1] [Req2] [Req3] [Req4] ...
Device 2:               [Req1] [Req2] [Req3] [Req4] ...
Device 3:                      [Req1] [Req2] [Req3] [Req4] ...
```

Once the pipeline is full, throughput equals $P$ times single-stage throughput, where $P$ is the number of pipeline stages. The steady-state latency remains approximately the single-device latency (sum of all stage times), but throughput scales with parallelism.

**When to use pipeline parallelism for inference**:

- When memory constraints require sharding but latency requirements are relaxed
- When throughput is more important than individual request latency
- When network bandwidth between devices is limited (only point-to-point communication)

@tbl-pipeline-tensor-comparison captures the fundamental tradeoffs between these two sharding approaches:

+----------------------------+----------------------------------+------------------------------------+
| **Aspect**                 | **Tensor Parallelism**           | **Pipeline Parallelism**           |
+:===========================+:=================================+:===================================+
| **Single-request latency** | Reduced by ~$P$x                 | No improvement                     |
| **Throughput**             | $P$x                             | $P$x (when pipelined)              |
| **Communication pattern**  | AllReduce (bandwidth-intensive)  | Point-to-point (latency-sensitive) |
| **Memory efficiency**      | Activations replicated           | Activations passed along           |
| **Complexity**             | Higher (requires custom kernels) | Lower (layer-level partitioning)   |
+----------------------------+----------------------------------+------------------------------------+

: **Pipeline vs Tensor Parallelism**: Each strategy has distinct tradeoffs in latency, throughput, and implementation complexity. {#tbl-pipeline-tensor-comparison}

### Expert Parallelism for MoE Models {#sec-inference-expert-parallelism}

Mixture-of-Experts (MoE)[^fn-moe] models present unique sharding challenges because computation is dynamically routed to different experts based on input. Popular models like Mixtral [@jiang2024mixtral] use MoE to achieve high capacity with lower inference cost.

[^fn-moe]: **Mixture-of-Experts (MoE)**: An architecture where only a subset of model parameters (the "experts") are activated for each input, selected by a learned gating network. This enables models with very high total parameter counts (for capacity) while maintaining computational cost proportional to active parameters. The trade-off is increased memory footprint and communication complexity for expert routing.

In an MoE layer, a gating network selects $k$ experts (out of $E$ total) for each token:

$$\text{Output} = \sum_{i \in \text{top-}k} g_i \cdot \text{Expert}_i(\text{input})$$

**Expert parallelism** distributes experts across devices, with each device hosting $E/P$ experts. @fig-moe-routing traces the routing, dispatch, and gather operations:

::: {#fig-moe-routing fig-env="figure" fig-pos="htb" fig-cap="**Mixture-of-Experts (MoE) Routing**. Expert parallelism distributes \"experts\" across different devices. For each token, a gating mechanism selects the top-k experts. An AllToAll communication step dispatches tokens to the devices hosting their selected experts (1). Experts process the tokens in parallel (2). A second AllToAll step gathers the results back to the original device (3). This pattern enables massive model capacity but introduces all-to-all communication overhead."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{GateColor}{RGB}{255,215,0}
  \definecolor{ExpColor}{RGB}{144,238,144}
  \definecolor{CommColor}{RGB}{255,160,122}

  % Input Token
  \node[draw, fill=white] (token) at (0, 3) {Input Token};

  % Host Device
  \node[draw, fill=GateColor!30, minimum height=1cm, minimum width=2cm] (gating) at (0, 1.5) {Gating\\(Top-2)};
  \draw[->, thick] (token) -- (gating);

  % Interconnect
  \node[text=CommColor!80!black, font=\bfseries] at (2.5, 1.5) {All-to-All Dispatch};
  \draw[->, dashed, thick, CommColor] (gating) -- (4, 3);
  \draw[->, dashed, thick, CommColor] (gating) -- (4, 0);

  % Expert Devices
  \node[draw, fill=ExpColor, minimum width=2cm] (exp1) at (5, 3) {Device 1\\Expert A};
  \node[draw, fill=ExpColor, minimum width=2cm] (exp2) at (5, 0) {Device 2\\Expert B};
  \node[draw, fill=gray!20, minimum width=2cm] (exp3) at (5, 1.5) {Device X\\Expert C (Idle)};

  % Gather
  \node[draw, fill=white, rounded corners] (agg) at (8, 1.5) {Weighted\\Sum};

  \node[text=CommColor!80!black, font=\bfseries] at (6.5, 2.5) {All-to-All Gather};
  \draw[->, dashed, thick, CommColor] (exp1) -- (agg);
  \draw[->, dashed, thick, CommColor] (exp2) -- (agg);

  % Output
  \node[right=0.5cm of agg] {Output};
  \draw[->, thick] (agg) -- +(1,0);

\end{tikzpicture}
```
:::

The communication pattern differs from tensor parallelism: instead of all-reduce (same data to all devices), expert parallelism uses all-to-all (different data to different devices based on routing).

**Load balancing challenge**: If gating decisions cluster on certain experts, devices hosting popular experts become bottlenecks while others sit idle. MoE training includes auxiliary losses to encourage balanced routing, but inference still exhibits routing imbalance.

::: {.callout-note title="Expert Parallelism for Mixtral-8x7B"}

Mixtral-8x7B uses 8 experts per MoE layer with top-2 routing:

**Model characteristics**:

- Total parameters: 47B (but only ~13B active per token)
- Experts per layer: 8
- Active experts per token: 2 (top-k = 2)
- MoE layers: Every other feed-forward layer

**Sharding strategy** (4-way expert parallelism):

- Experts 0-1 on Device 0
- Experts 2-3 on Device 1
- Experts 4-5 on Device 2
- Experts 6-7 on Device 3

**Communication pattern per token**:

1. Gating: Determine which 2 experts to use (~0.1ms)
2. AllToAll dispatch: Send token to devices hosting selected experts (~0.2ms)
3. Expert compute: Process token through selected experts (~1ms each, parallel)
4. AllToAll gather: Collect results back (~0.2ms)

**Total MoE layer time**: ~1.5ms (vs ~4ms for equivalent dense layer)

**Load balancing metrics**:

+------------------------------+---------------------+-----------------------+
| **Routing Distribution**     | **GPU Utilization** | **Throughput Impact** |
+:=============================+====================:+:======================+
| **Perfectly balanced**       | 100%                | Baseline              |
| **Moderate imbalance (20%)** | 83%                 | -17%                  |
| **Severe imbalance (50%)**   | 67%                 | -33%                  |
+------------------------------+---------------------+-----------------------+

Production systems monitor routing statistics and may retrain or fine-tune gating to improve balance.

:::

### Embedding Sharding for Recommendation Systems {#sec-inference-embedding-sharding}

Recommendation systems typically contain embedding tables that dwarf model weights in size. Meta's DLRM-scale models [@naumov2019dlrm] have embedding tables exceeding 100TB, requiring aggressive sharding strategies fundamentally different from tensor or pipeline parallelism.

**Row-wise sharding** partitions embedding tables by row (entity ID):

$$\text{Shard}_i = \{e_j : \text{hash}(j) \mod P = i\}$$

Each shard contains approximately $N/P$ embeddings, where $N$ is the total number of entities and $P$ is the shard count.

**Column-wise sharding** partitions each embedding vector across devices:

$$e_j = [e_j^{(0)}, e_j^{(1)}, ..., e_j^{(P-1)}]$$

Each device stores a slice of every embedding.

**Hybrid sharding** combines both approaches: frequently accessed embeddings are column-sharded for faster access, while the long tail uses row sharding.

The choice of embedding sharding strategy depends on lookup patterns and communication overhead. @tbl-embedding-sharding compares row-wise, column-wise, and hybrid approaches:

+-----------------------+--------------------------+-------------------+-------------------------+
| **Sharding Strategy** | **Lookup Pattern**       | **Communication** | **Best For**            |
+:======================+:=========================+:==================+:========================+
| **Row-wise**          | Single device per lookup | AllToAll gather   | Uniform access patterns |
| **Column-wise**       | All devices per lookup   | AllGather         | Hot embeddings          |
| **Hybrid**            | Varies by embedding      | Mixed             | Production RecSys       |
+-----------------------+--------------------------+-------------------+-------------------------+

: **Embedding Sharding Strategies**: Different strategies trade off lookup locality against load balance. {#tbl-embedding-sharding}

@fig-embedding-sharding visualizes how each strategy distributes embedding lookups across devices:

::: {#fig-embedding-sharding fig-env="figure" fig-pos="htb" fig-cap="**Embedding Sharding Strategies**. **Row-wise sharding** places complete embedding vectors on specific servers based on entity ID, requiring a network gather for lookup. **Column-wise sharding** splits each vector across all servers, allowing parallel local lookups followed by an AllGather, which is efficient for popular \"hot\" embeddings. **Hybrid sharding** combines these approaches, using column sharding for hot items and row sharding for the \"cold\" long tail to balance load and memory."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{TableColor}{RGB}{240,248,255}
  \definecolor{ShardColor}{RGB}{176,196,222}

  % A. Row-Wise
  \node[anchor=west, font=\bfseries] at (0, 4.5) {A. Row-Wise Sharding};

  \draw[fill=TableColor] (0, 2.5) rectangle (1, 4);
  \node[rotate=90] at (-0.3, 3.25) {Users 0..N};

  \draw[->, thick] (1.2, 3.25) -- (2.2, 3.25);

  % Server 1
  \draw[fill=ShardColor] (2.5, 3.5) rectangle (3.5, 4.0);
  \node[right, font=\scriptsize] at (3.5, 3.75) {Svr 1: Users 0..K};

  % Server 2
  \draw[fill=ShardColor] (2.5, 2.5) rectangle (3.5, 3.0);
  \node[right, font=\scriptsize] at (3.5, 2.75) {Svr 2: Users K..N};

  \node[font=\scriptsize, align=center] at (1.7, 2.0) {Partition by ID\\(Network Gather)};


  % B. Column-Wise
  \node[anchor=west, font=\bfseries] at (6, 4.5) {B. Column-Wise Sharding};

  \draw[fill=TableColor] (6, 2.5) rectangle (7, 4);
  \node[rotate=90] at (5.7, 3.25) {Dims 0..D};

  \draw[->, thick] (7.2, 3.25) -- (8.2, 3.25);

  % Server 1 (Col split)
  \draw[fill=ShardColor] (8.5, 2.5) rectangle (9.0, 4.0);
  \node[above, font=\scriptsize] at (8.75, 4.0) {Svr 1};

  % Server 2
  \draw[fill=ShardColor] (9.1, 2.5) rectangle (9.6, 4.0);
  \node[above, font=\scriptsize] at (9.35, 4.0) {Svr 2};

  \node[font=\scriptsize, align=center] at (7.7, 2.0) {Partition Vector\\(AllGather)};


  % C. Hybrid
  \node[anchor=west, font=\bfseries] at (0, 0.5) {C. Hybrid Sharding};

  \draw[fill=red!10] (0, -1.0) rectangle (1.5, 0);
  \node[font=\scriptsize] at (0.75, -0.5) {Hot (Top 1\%)};

  \draw[fill=blue!10] (0, -2.5) rectangle (1.5, -1.1);
  \node[font=\scriptsize] at (0.75, -1.8) {Cold (Tail)};

  \draw[->, thick] (1.7, -1.25) -- (2.7, -1.25);

  \node[draw, fill=red!10, font=\scriptsize] at (4, -0.5) {Replicated / Col-wise};
  \node[draw, fill=blue!10, font=\scriptsize] at (4, -1.8) {Row-wise Sharded};

\end{tikzpicture}
```
:::

::: {.callout-note title="Embedding Sharding at Scale: Meta Infrastructure"}

Meta's recommendation infrastructure demonstrates embedding sharding at extreme scale:

**Scale**:

- Embedding tables: 100+ TB total
- Unique entities: 10+ trillion
- Embedding dimension: 128-256
- Shards: 1,000+ servers

**Sharding strategy**:

- **Hot embeddings** (top 1% by access frequency): Replicated across all shards
- **Warm embeddings** (next 10%): Column-sharded with 8-way parallelism
- **Cold embeddings** (remaining 89%): Row-sharded with consistent hashing

Each inference request requires approximately 5,000 embedding lookups. Without optimization, this would require 5,000 network round trips. Instead, the system applies several optimizations. Batch accumulation collects lookups for 1ms. Lookup deduplication removes duplicate entities across requests. Shard-aware batching groups lookups by destination shard. Parallel dispatch sends batched requests to all shards simultaneously. Streaming assembly reconstructs embeddings as responses arrive.

**Performance**:

+-------------------------+--------------------------+-----------------------+
| **Metric**              | **Without Optimization** | **With Optimization** |
+:========================+=========================:+======================:+
| **Network round trips** | 5,000                    | 1 (batched)           |
| **Lookup latency**      | 50ms                     | 2ms                   |
| **Network bandwidth**   | 10 Gbps                  | 40 Gbps (burst)       |
+-------------------------+--------------------------+-----------------------+

:::

### Hybrid Sharding Strategies {#sec-inference-hybrid-sharding}

Production systems often combine multiple sharding strategies to handle different model components optimally:

**Tensor + Pipeline parallelism**: For very large models that require both memory distribution and latency reduction:

```
8 GPUs organized as 2 pipeline stages × 4 tensor parallel:

Stage 0 (Layers 1-40):  TP across GPUs 0,1,2,3
Stage 1 (Layers 41-80): TP across GPUs 4,5,6,7
```

This achieves 4x latency reduction (from TP) while handling models requiring 8-way sharding for memory.

**Expert + Tensor parallelism**: For MoE models where individual experts are large:

```
Mixtral with large experts:

- Expert parallelism: Distribute 8 experts across 8 GPU groups
- Tensor parallelism: Each expert spread across 2 GPUs
- Total GPUs: 16
```

**Embedding + Dense parallelism**: For recommendation models with both large embeddings and large dense components:

```
DLRM-scale model:

- Embedding sharding: 1,000 shards across CPU servers
- Dense model: 8-way tensor parallel across GPUs
- Communication: Embeddings gathered to GPU, processed, returned
```

### Communication Overhead Analysis {#sec-inference-sharding-communication}

The practical speedup from sharding depends critically on communication efficiency. Each sharding strategy has characteristic communication patterns with different bandwidth and latency requirements.

@eq-allreduce-time quantifies AllReduce communication time for tensor parallelism, where data is combined from all devices with the result available on all devices.

$$T_{allreduce} = 2 \times \frac{(P-1)}{P} \times \frac{M}{B}$$ {#eq-allreduce-time}

where $P$ is the number of devices, $M$ is the message size, and $B$ is the interconnect bandwidth. The factor of 2 accounts for the reduce-scatter and all-gather phases.

@eq-p2p-time expresses the simpler point-to-point communication for pipeline parallelism, where data flows from one device to the next.

$$T_{p2p} = L + \frac{M}{B}$$ {#eq-p2p-time}

where $L$ is the network latency and $M/B$ is the transfer time.

@eq-alltoall-time captures the more complex AllToAll communication for expert parallelism, where each device exchanges distinct data with every other device.

$$T_{alltoall} = (P-1) \times \left(L + \frac{M/P}{B}\right)$$ {#eq-alltoall-time}

::: {.callout-note title="Interconnect Technology Comparison"}

Communication overhead depends heavily on the interconnect technology:

+--------------------+----------------------+-------------+------------------------+
| **Interconnect**   | **Bandwidth**        | **Latency** | **Use Case**           |
+===================:+=====================:+============:+:=======================+
| **NVLink (H100)**  | 900 GB/s             | 1μs         | Intra-node TP          |
| **PCIe Gen5**      | 64 GB/s              | 5μs         | Intra-node (no NVLink) |
| **InfiniBand HDR** | 200 Gb/s (25 GB/s)   | 1μs         | Inter-node             |
| **Ethernet 100G**  | 100 Gb/s (12.5 GB/s) | 10μs        | Inter-node (commodity) |
+--------------------+----------------------+-------------+------------------------+

NVLink provides the highest bandwidth for intra-node communication.

**Example: 8-way tensor parallelism communication**

Activation size: 8MB per all-reduce (batch=1, hidden=8192)

+-------------------+--------------------+---------------------+
| **Interconnect**  | **AllReduce Time** | **% of 30ms Layer** |
+:==================+===================:+====================:+
| **NVLink**        | 0.02ms             | 0.07%               |
| **InfiniBand**    | 0.7ms              | 2.3%                |
| **100G Ethernet** | 1.5ms              | 5%                  |
+-------------------+--------------------+---------------------+

NVLink enables efficient tensor parallelism within a node. Cross-node tensor parallelism requires InfiniBand for acceptable overhead.

:::

NVLink bandwidth has evolved significantly over GPU generations.[^fn-nvlink]

[^fn-nvlink]: **NVLink evolution**: NVLink bandwidth has grown from 160 GB/s (Pascal, 2016) to 900 GB/s bidirectional (Hopper, 2022) to 1.8 TB/s (Blackwell, 2024). This 10x improvement enables tensor parallelism across 8+ GPUs with less than 5% communication overhead, making large model inference practical.

### Sharding Strategy Selection {#sec-inference-sharding-selection}

The choice of sharding strategy depends on model architecture and deployment priorities. @tbl-sharding-selection evaluates each approach across four critical factors:

+---------------------------+---------------------+-----------------------+---------------------+---------------------+
| **Factor**                | **Tensor Parallel** | **Pipeline Parallel** | **Expert Parallel** | **Embedding Shard** |
+:==========================+:====================+:======================+:====================+:====================+
| **Latency priority**      | Best                | Worst                 | Moderate            | N/A                 |
| **Throughput priority**   | Good                | Best (pipelined)      | Good                | Best                |
| **Interconnect limited**  | Poor fit            | Good fit              | Moderate            | Good fit            |
| **Implementation effort** | High                | Low                   | Moderate            | High                |
+---------------------------+---------------------+-----------------------+---------------------+---------------------+

: **Sharding Strategy Selection Guide**: Match strategy to deployment priorities and constraints. {#tbl-sharding-selection}

## Load Balancing and Request Routing {#sec-inference-load-balancing}

With models deployed across multiple replicas, the next challenge is distributing requests effectively. This moves us from replica-level concerns (how individual replicas are configured) to service-level optimization in the serving hierarchy: coordinating across replicas to meet aggregate throughput and latency targets.

Load balancing determines which replica handles each request, directly impacting latency, throughput, and resource utilization. Seemingly simple choices, like random assignment versus informed selection, produce dramatically different performance at scale. This section develops the theory and practice of load balancing for inference, from basic algorithms through the power-of-two-choices insight that provides exponentially better performance with minimal overhead.

::: {.callout-note title="Archetype B Connection: The Tail at Scale"}
**Archetype B (The Global Real-Time Recommendation Engine)** is the canonical victim of tail latency. Processing 10 million QPS means that a 1-in-10,000 latency spike happens 1,000 times every second. For Archetype B, sophisticated load balancing (like Power-of-Two-Choices) is mandatory to suppress these outliers; simple Round-Robin would allow queues to build up, causing the 99th percentile latency to collapse into unacceptable slowness.
:::

### Load Balancing Fundamentals {#sec-inference-lb-fundamentals}

Load balancing serves two primary goals that sometimes conflict:

**Latency minimization**: Route requests to replicas that can serve them fastest, considering current queue depth and processing time.

**Utilization maximization**: Spread load evenly to avoid both idle replicas and overloaded replicas.

The tension arises because latency-optimal routing may concentrate load on fast replicas, reducing their performance and leaving other replicas underutilized.

Load balancing evaluation uses several key metrics. Maximum queue length measures the longest queue across all replicas and determines worst-case latency. Load variance captures the standard deviation of queue lengths and measures balance. Utilization spread represents the difference between most and least utilized replicas. Decision overhead quantifies the time required to make routing decisions.

### Round-Robin and Random Assignment {#sec-inference-lb-round-robin}

The simplest load balancing strategies assign requests without considering server state:

Round-robin assigns requests in circular order. Request 1 goes to server 1, request 2 to server 2, and so on. This guarantees perfect distribution when servers are homogeneous and request processing times are identical.

Random assignment selects a server uniformly at random for each request. With large numbers of requests, this converges to even distribution but with higher variance than round-robin.

For homogeneous servers with identical service times, both achieve near-optimal load distribution. However, production systems rarely meet these assumptions. Heterogeneous hardware introduces different GPU generations and memory configurations. Variable request sizes mean some requests take 10x longer than others. Server state variations occur when some replicas are warming up while others approach memory limits.

Under these realistic conditions, uninformed strategies perform poorly. The maximum queue length under random assignment follows @eq-random-max-queue[^fn-theta-notation]:

$$E[\text{max queue}] = \Theta\left(\frac{\log n}{\log \log n}\right)$$ {#eq-random-max-queue}

[^fn-theta-notation]: **Theta Notation (Θ)**: Asymptotic tight bound indicating both upper (O) and lower (Ω) complexity bounds match. Unlike O(f(n)) which may be loose, Θ(f(n)) precisely characterizes growth rate. In queueing analysis for inference systems, tight bounds enable accurate capacity planning: Θ(log n / log log n) queue length means exactly that scaling, not just "at most".

where $n$ is the number of servers. For 1,000 servers, this is approximately 4-5 requests. This seems small, but the unlucky requests in long queues experience significantly higher latency.

### The Power of Two Choices {#sec-inference-two-choices}

A remarkable result in load balancing theory [@mitzenmacher2001power] shows that querying just two random servers before making a routing decision provides exponentially better load distribution than random assignment.[^fn-balls-bins]

[^fn-balls-bins]: **Balls-into-bins problem**: The power-of-two-choices result comes from probabilistic analysis of throwing balls into bins. With random placement, maximum load is $\Theta(\log n / \log \log n)$. With two choices and placing in the less-loaded bin, maximum load drops to $\Theta(\log \log n)$. This exponential improvement from a constant-factor change in the algorithm is a foundational result in randomized algorithms.

The power-of-two-choices algorithm operates as follows. Select two servers uniformly at random. Query both for their current queue length. Route the request to the server with the shorter queue.

@eq-two-choices-max-queue formalizes this exponential improvement, reducing maximum queue length from $O(\log n / \log \log n)$ to $O(\log \log n)$:

$$E[\text{max queue}]_{\text{two choices}} = \Theta(\log \log n)$$ {#eq-two-choices-max-queue}

For 1,000 servers:

- Random assignment max queue: ~4-5 requests
- Two choices max queue: ~2 requests

The improvement is exponential: two choices with 1,000 servers achieves better balance than random with just 10 servers.

::: {.callout-important title="Exponential Improvement from a Simple Change"}

The power-of-two-choices result is one of the most impactful findings in distributed systems theory. By examining just one additional server, maximum queue length improves from $O(\log n / \log \log n)$ to $O(\log \log n)$, an exponential improvement.

This has profound practical implications:

- Near-optimal load balancing with minimal overhead (2 probes vs n probes)
- Scalable: improvement increases with system size
- Robust: works with heterogeneous servers and variable request sizes
- Simple: easy to implement in any load balancer

Production systems at Google, Meta, and AWS all use variants of power-of-two-choices.

:::

**Why does this work?** Intuitively, random assignment occasionally makes poor choices (routing to an already-busy server), and these mistakes compound. With two choices, the algorithm almost never makes the worst choice, avoiding the tail behavior that creates long queues.

Mathematically, the key insight is that with random assignment, when $d$ servers have queue length $k$, the probability of queue length $k+1$ growing is proportional to $d/n$. With two choices, this probability drops to $(d/n)^2$, creating a super-exponential decay in queue length distribution.

### Weighted and Adaptive Load Balancing {#sec-inference-weighted-lb}

When servers have different capacities, naive load balancing creates imbalance. A mix of A100 GPUs (high capacity) and T4 GPUs (lower capacity) receiving equal request rates will have T4 servers overloaded while A100 servers are underutilized.

**Weighted round-robin** assigns requests proportional to server capacity:

$$P(\text{route to server } i) = \frac{w_i}{\sum_j w_j}$$

where $w_i$ is the weight (capacity) of server $i$.

**Weighted two-choices** applies the same principle:

1. Select two servers with probability proportional to their weights
2. Query both for current load relative to their capacity
3. Route to the server with lower relative load

::: {.callout-note title="Worked Example: Heterogeneous GPU Cluster"}

Consider a cluster with mixed GPU types:

- 10 H100 GPUs (capacity: 1000 QPS each)
- 20 A100 GPUs (capacity: 600 QPS each)
- Total capacity: 10×1000 + 20×600 = 22,000 QPS

**Target traffic**: 15,000 QPS

**Weighted assignment**:

- H100 weight: 1000 / 22000 = 4.5%
- A100 weight: 600 / 22000 = 2.7%

**Expected load per server**:

- H100: 15000 × 0.045 = 682 QPS (68% utilization)
- A100: 15000 × 0.027 = 409 QPS (68% utilization)

Both server types operate at equal utilization, maximizing overall capacity while maintaining latency consistency.

**Without weighting** (equal distribution):

- Per-server load: 15000 / 30 = 500 QPS
- H100 utilization: 50% (underutilized)
- A100 utilization: 83% (overloaded, latency spikes)

:::

**Adaptive load balancing** adjusts weights dynamically based on observed performance:

```
For each server i:
    latency[i] = exponential_moving_average(observed_latency)
    weight[i] = 1 / latency[i]  # Inverse latency weighting
```

This automatically adapts to:

- Server degradation (memory pressure, thermal throttling)
- Request size variations (some traffic patterns harder to serve)
- Background tasks consuming resources

### Least-Connections Load Balancing {#sec-inference-least-connections}

An alternative to random selection is routing to the server with the fewest active connections (or shortest queue). This requires maintaining global state but provides better balance for variable-size requests.

**Least-connections algorithm**:

1. Maintain a count of active requests per server
2. Route each new request to the server with the minimum count
3. Increment count on dispatch, decrement on completion

For long-running requests (common in LLM serving), least-connections significantly outperforms round-robin because it accounts for current load rather than just historical assignments.

The challenge is maintaining accurate connection counts in a distributed system. Options include:

- **Centralized counter**: Single source of truth, potential bottleneck
- **Distributed counters with gossip**: Eventually consistent, may route to stale information
- **Sampled least-connections**: Query a subset of servers, choose minimum (combines with two-choices)

::: {.callout-note title="Least-Connections for LLM Serving"}

LLM inference has highly variable request durations based on output length:

- Short response (10 tokens): 500ms
- Long response (500 tokens): 25s
- Ratio: 50x

With round-robin at 100 QPS across 10 servers:

- Each server receives 10 requests/second
- If one server gets multiple long requests, it falls behind
- Queue builds while other servers sit idle

With least-connections:

- New requests route away from servers processing long responses
- Servers finishing short requests receive new work immediately
- Load naturally balances based on actual work remaining

**Observed improvement** (production LLM serving):

+-----------------------+-----------------+-------------------+
| **Algorithm**         | **P99 Latency** | **Load Variance** |
+:======================+================:+==================:+
| **Round-robin**       | 45s             | 3.2 requests      |
| **Least-connections** | 28s             | 0.8 requests      |
| **Two-choices + LC**  | 26s             | 0.5 requests      |
+-----------------------+-----------------+-------------------+

Least-connections reduces P99 by 38%; combining with two-choices provides additional improvement.

:::

### Consistent Hashing for Stateful Routing {#sec-inference-consistent-hashing}

Many inference workloads maintain state that benefits from routing affinity:

- **LLM conversations**: KV cache from previous turns
- **Recommendation sessions**: User context and recent interactions
- **Streaming inference**: Model state from previous frames

For these workloads, routing the same user or session to the same server improves performance by avoiding cache misses and state reconstruction.

**Consistent hashing**[^fn-consistent-hashing] [@karger1997consistent] maps requests to servers based on a hash of the routing key (user ID, session ID):

[^fn-consistent-hashing]: **Consistent hashing**: Originally developed for distributed caching (Akamai, 1997), consistent hashing arranges servers on a virtual ring and assigns requests to the nearest server clockwise. When a server fails, only requests that were assigned to that server remap to the next server, minimizing disruption. This property is essential for maintaining KV cache locality during scaling events.

$$\text{server}(request) = \arg\min_{s \in S} \text{distance}(\text{hash}(key), \text{hash}(s))$$

where servers and keys are mapped onto a ring, and each request routes to the nearest server clockwise.

Key properties:

- **Deterministic**: Same key always routes to same server
- **Minimal disruption**: Adding/removing servers only remaps $K/N$ keys on average
- **Load balancing**: With virtual nodes, load distributes evenly

::: {.callout-note title="Consistent Hashing for KV Cache Affinity"}

Consider an LLM serving system where each user's conversation maintains KV cache state:

**Without affinity**:

- User sends message, routed to Server A, KV cache built
- Next message routes to Server B (random)
- KV cache rebuilt from scratch, 500ms penalty
- Average conversation: 10 turns, 4.5s wasted on cache rebuilds

**With consistent hashing**:

- User ID hashed to Server A
- All messages from this user route to Server A
- KV cache reused across turns
- Rebuild only on server changes or cache eviction

**Implementation with virtual nodes**:

Each physical server has 100 virtual nodes on the hash ring, ensuring even distribution despite server heterogeneity.

```
Hash ring positions:
Server A: [0.01, 0.03, 0.07, 0.12, ...]  (100 positions)
Server B: [0.02, 0.05, 0.09, 0.15, ...]  (100 positions)
...

Request for user "alice":
hash("alice") = 0.0834
Nearest server clockwise: Server A (at 0.09)
```

**Handling server failures**:

When Server A fails, its 100 virtual nodes are removed from the ring. Requests that would have routed to Server A now route to the next server clockwise. Only ~$1/N$ of requests are affected, where $N$ is the number of servers.

:::

### Request Routing for Sharded Models {#sec-inference-sharded-routing}

The sharding strategies examined in @sec-inference-sharding introduce routing complexity: a single inference request may require computation on multiple devices, necessitating coordination.

**Routing patterns for sharded models**:

**Tensor parallelism**: Request is broadcast to all devices in the shard group. Each device processes its portion of each layer. Results are synchronized via all-reduce.

```
Request → Load Balancer → Shard Group
                              │
            ┌────────────────┼────────────────┐
            ▼                ▼                ▼
         GPU 0            GPU 1            GPU 2
      (heads 0-7)      (heads 8-15)    (heads 16-23)
            │                │                │
            └────────AllReduce────────────────┘
                              │
                              ▼
                          Response
```

**Pipeline parallelism**: Request flows through stages sequentially. Each stage forwards to the next.

```
Request → Stage 0 → Stage 1 → Stage 2 → Stage 3 → Response
         (L1-20)   (L21-40)  (L41-60)  (L61-80)
```

**Expert parallelism**: Request is dispatched to devices hosting selected experts based on gating decision.

```
Request → Gating → AllToAll dispatch → Expert compute → AllToAll gather → Response
                   (to selected experts)              (results back)
```

**Routing to shard groups**: With multiple shard groups for horizontal scaling, the load balancer routes to groups rather than individual devices:

```
                    Load Balancer
                         │
          ┌──────────────┼──────────────┐
          ▼              ▼              ▼
     Shard Group 0  Shard Group 1  Shard Group 2
      (8 GPUs)       (8 GPUs)       (8 GPUs)
```

The load balancer treats each shard group as a single logical server, applying standard algorithms (round-robin, two-choices, consistent hashing) at the group level.

### Health Checking and Failover {#sec-inference-health-checking}

Load balancers must detect unhealthy servers and route around them. Health checking mechanisms include:

**Liveness probes**: Verify the server process is running.

```
GET /health/live
Response: 200 OK (process alive) or timeout (process dead)
```

**Readiness probes**: Verify the server can handle requests (model loaded, GPU initialized).

```
GET /health/ready
Response: 200 OK (ready to serve) or 503 (not ready)
```

**Deep health checks**: Verify actual inference works by running a test request.

```
POST /health/inference
Body: {"prompt": "test"}
Response: 200 OK with valid output, or error
```

::: {.callout-note title="Health Check Configuration for GPU Inference"}

GPU inference servers have unique health check considerations:

**GPU memory pressure**: Server may be alive but unable to allocate memory for new requests.

```{.python}
def readiness_check():
    free_memory = (
        torch.cuda.memory_reserved() - torch.cuda.memory_allocated()
    )
    if free_memory < MIN_REQUEST_MEMORY:
        return {
            "status": "not_ready",
            "reason": "insufficient GPU memory",
        }
    return {"status": "ready"}
```

**Model warm-up**: First inference after load is slower. Mark ready only after warm-up.

```{.python}
async def startup():
    model = load_model()
    # Warm up with dummy requests
    for _ in range(10):
        model.generate(dummy_input)
    # Now mark as ready
    global ready
    ready = True
```

**Timeout configuration**:

+-----------------+--------------+-------------+-----------------------+
| **Check Type**  | **Interval** | **Timeout** | **Failure Threshold** |
+:================+=============:+============:+======================:+
| **Liveness**    | 10s          | 5s          | 3 failures            |
| **Readiness**   | 5s           | 3s          | 2 failures            |
| **Deep health** | 30s          | 10s         | 1 failure             |
+-----------------+--------------+-------------+-----------------------+

Deep health checks run less frequently because they consume GPU resources.

:::

### Quantitative Analysis: Load Balancing Impact {#sec-inference-lb-analysis}

The choice of load balancing algorithm has quantitative impact on system performance. Consider a system with 100 servers, 10,000 QPS, and variable request sizes (CV = 0.5). @tbl-lb-comparison quantifies the latency and overhead tradeoffs:

+-----------------------+---------------+-----------------+------------------+
| **Algorithm**         | **Max Queue** | **P99 Latency** | **CPU Overhead** |
+:======================+==============:+================:+:=================+
| **Random**            | 4.2 requests  | 45ms            | Minimal          |
| **Round-robin**       | 2.8 requests  | 32ms            | Minimal          |
| **Two-choices**       | 1.9 requests  | 24ms            | 2 probes/request |
| **Least-connections** | 1.4 requests  | 19ms            | Global state     |
| **Two-choices + LC**  | 1.2 requests  | 17ms            | 2 probes + state |
+-----------------------+---------------+-----------------+------------------+

: **Load Balancing Algorithm Comparison**: More sophisticated algorithms reduce queue lengths and latency at the cost of increased overhead. {#tbl-lb-comparison}

The progression shows clear tradeoffs:

- Random/round-robin: Zero overhead but higher latency variance
- Two-choices: Minimal overhead (2 probes), 47% latency improvement
- Least-connections: State maintenance overhead, 58% latency improvement
- Combined: Best performance, highest complexity

For most production systems, two-choices provides the best tradeoff between performance improvement and implementation complexity. Least-connections adds value for workloads with high request size variance (LLM serving, recommendation ranking).

### Circuit Breakers and Backpressure {#sec-inference-circuit-breakers}

When servers become overloaded, routing more requests exacerbates the problem. Circuit breakers and backpressure mechanisms protect the system from cascading failures.

**Circuit breaker pattern**[^fn-circuit-breaker] [@nygard2007releaseit]:

[^fn-circuit-breaker]: **Circuit breaker**: Borrowed from electrical engineering, where circuit breakers prevent electrical fires by cutting power when current exceeds safe levels. In distributed systems, circuit breakers prevent cascade failures by quickly failing requests to unhealthy services rather than waiting for timeouts. The pattern was popularized by Netflix's Hystrix library and is now standard in service mesh implementations like Envoy and Istio.

```
States: CLOSED → OPEN → HALF-OPEN → CLOSED

CLOSED: Normal operation, route requests
OPEN: Server unhealthy, immediately reject requests
HALF-OPEN: Allow limited requests to test recovery

Transitions:

- CLOSED → OPEN: Error rate exceeds threshold (e.g., 50%)
- OPEN → HALF-OPEN: After timeout (e.g., 30s)
- HALF-OPEN → CLOSED: Test requests succeed
- HALF-OPEN → OPEN: Test requests fail
```

**Backpressure propagation**: When servers are overloaded, they signal upstream to reduce request rate:

```
Server queue depth > threshold
    → Return 503 Service Unavailable
    → Load balancer marks server as degraded
    → Routes fewer requests to this server
    → If all servers degraded, apply admission control
```

::: {.callout-note title="Cascading Failure Prevention"}

Consider a scenario where one server becomes slow (thermal throttling):

**Without circuit breaker**:

1. Server A slows down (processing 500ms instead of 50ms)
2. Load balancer continues routing to Server A
3. Requests queue on Server A, timeouts begin
4. Retry logic sends failed requests to other servers
5. Other servers overload from retry traffic
6. System-wide failure

**With circuit breaker**:

1. Server A slows down
2. Error rate on Server A rises above 50%
3. Circuit breaker opens for Server A
4. All requests route to Servers B, C, D
5. System operates at reduced capacity but remains stable
6. After recovery, circuit breaker closes, Server A rejoins

**Configuration for GPU inference**:

+------------------------+-------------+---------------------------------------+
| **Parameter**          | **Value**   | **Rationale**                         |
+:=======================+============:+:======================================+
| **Error threshold**    | 30%         | GPU OOM failures are serious          |
| **Latency threshold**  | 2x baseline | Detect throttling early               |
| **Open duration**      | 60s         | GPU recovery takes time               |
| **Half-open requests** | 5           | Careful testing before full reopening |
+------------------------+-------------+---------------------------------------+

:::

## Summary {#sec-inference-summary}

Inference at scale is the final realization of the Machine Learning Fleet. Throughout Part I, we designed the logical algorithms for scale, and in Part II, we built the physical machines to execute them. This chapter has developed the service layer— the "interface to reality"—that transforms those massive computational resources into low-latency global predictions.

The serving hierarchy provides our organizing framework for these optimizations: from request-level batching and caching, to replica-level GPU memory management, up to service-level load balancing and platform-level multi-tenancy. We established that the fundamental inversion from training (throughput-bound) to inference (latency-bound) necessitates a different engineering mindset, where "the tail" (P99 latency) is the primary determinant of success.

We analyzed why different model architectures require distinct batching strategies: static/dynamic for vision, feature-parallel for recommendation, and continuous batching for LLMs. We explored how model sharding techniques like tensor and expert parallelism reduce latency for frontier models, provided the underlying networking fabric discussed in @sec-communication can sustain the synchronization overhead. Finally, we examined the critical memory management challenges of autoregressive generation, where techniques like PagedAttention and speculative decoding break the sequential bottlenecks of modern AI.

::: {.callout-important title="Key Takeaways"}

* **The Inference Wall**: Serving cost often dominates training cost by 100x or more over a model's lifetime, making inference efficiency the primary driver of ML economics.
* **Prefill vs. Decode**: LLM serving is split into two regimes: compute-bound prefill (input processing) and memory-bandwidth-bound decode (token generation). Optimizing for the "Decode" phase requires high-bandwidth memory (HBM3) or specialized storage-as-memory (HBF).
* **Batching is Not Universal**: There is no "optimal" batch size for all models. Vision models scale with batch size; recommendation models scale with feature-parallel shards; LLMs scale via continuous, iteration-level management.
* **Power of Two Choices**: Simple, randomized load balancing achieves exponentially better queue balance ($O(\log \log n)$) with minimal overhead, a must-have for any large-scale deployment.
* **Statefulness is the Challenge**: Unlike stateless vision models, LLM serving is inherently stateful due to the KV cache. This requires sticky routing, prefix caching, and sophisticated memory paging (PagedAttention) to prevent fragmentation.
* **Distribution is a Tax**: Every step of distribution—network round-trips, serialization, and coordination—adds a "serving tax" that must be explicitly budgeted within the milliseconds allowed for a user response.

:::

We also saw that efficient serving requires specialized hardware. The "Decode" phase of LLM inference is memory-bandwidth bound, motivating the specialized inference accelerators we examined in @sec-infrastructure.

The serving architectures examined here—load balancers, model sharding, and batching strategies—provide the backbone for production inference. However, at extreme scale, even perfectly balanced clusters hit the "Memory Wall" and the physical limits of sequential token generation.

The next chapter, @sec-optimization-at-scale, examines the advanced speed and efficiency tricks—PagedAttention, Speculative Decoding, and Prefill-Decode Disaggregation—that allow us to break through these bottlenecks.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
