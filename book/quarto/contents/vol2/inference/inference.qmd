---
---

# Inference at Scale {#sec-inference-at-scale}

::: {layout-narrow}
::: {.column-margin}
_Gemini Pro 3 Prompt: A dynamic illustration of a global inference serving system handling millions of requests. The scene shows a central model representation being replicated across multiple serving nodes arranged in a globe-spanning network. Incoming request streams appear as flowing arrows from users worldwide, processed through load balancers depicted as traffic controllers, then distributed to model replicas. Visual elements include batching queues shown as organized request groups, latency meters displaying millisecond readings, and auto-scaling indicators showing nodes spawning and despawning. The composition emphasizes real-time responsiveness with clock symbols and speed indicators. Color palette features cool silvers and whites for infrastructure, warm oranges for active requests, and green for successful responses. Technical yet accessible style suitable for a production systems textbook. Rendered in the style of Nanobanana. High resolution, rectangular image with golden ratio dimensions._
:::

\noindent
![](images/png/cover_inference_at_scale.png)

:::

## Purpose {.unnumbered}

_Why does serving machine learning predictions at scale require fundamentally different engineering approaches than training the models that generate them?_

Training optimizes for throughput over extended periods, tolerating latency variations that would be unacceptable when users await responses in real time. Inference at scale inverts these priorities: serving systems must deliver predictions within strict latency bounds while handling request volumes that fluctuate unpredictably across hours, days, and seasons. The engineering challenges of inference extend beyond raw performance to encompass resource efficiency when serving costs dwarf training costs over a model's lifetime, availability when users expect continuous service regardless of infrastructure conditions, and consistency when the same input must produce the same output across globally distributed serving infrastructure. Understanding how serving systems, batching strategies, model sharding, and load balancing address these challenges determines whether sophisticated models deliver value to users or remain impressive but impractical demonstrations. Mastering inference at scale transforms model capabilities into reliable services that operate continuously at the speed and scale users demand.

::: {.callout-tip title="Learning Objectives"}

- Quantify why serving cost dominates training cost over a model's lifetime using the total cost of serving equation

- Design batching strategies matched to model architectures, distinguishing static batching for vision models from continuous batching for LLMs and feature-parallel batching for recommendation systems

- Apply the serving hierarchy framework (request, replica, service, platform) to systematically identify and resolve inference bottlenecks at each level

- Compare model sharding strategies (tensor parallelism, pipeline parallelism, expert parallelism, embedding sharding) by their communication patterns and latency-memory tradeoffs

- Analyze load balancing algorithms using queuing theory to explain why power-of-two-choices achieves exponentially better performance than random assignment

- Recognize KV cache memory challenges in LLM serving and understand how techniques like continuous batching and memory management enable high GPU utilization

- Design autoscaling policies that account for GPU cold start latency and balance cost optimization with SLO guarantees

:::

## Scaling Inference Beyond Single Machines {#sec-inference-scaling-beyond}

Single-machine model serving establishes foundational principles that this chapter extends to distributed systems. Single-machine inference optimization, including batching, caching, model optimization, and hardware acceleration, provides the building blocks. This chapter addresses when those techniques reach their limits and distributed approaches become necessary.

::: {.callout-note title="Connection: The Systems Sandwich"}
Inference is the **Service Layer** of the Systems Sandwich. It sits above the **Operational Layer** (distributed training) and **Physical Layer** (infrastructure). While training builds the model, inference is where the model meets the user. The latency constraints here ($<100$ms) are far stricter than the throughput constraints of training, forcing us to rethink the entire stack—from batching strategies to memory management.
:::

The transition from single-machine to distributed inference parallels the transition from single-machine to distributed training, but with fundamentally different constraints. Training optimizes throughput over extended periods and tolerates latency variations measured in minutes or hours. Inference at scale must maintain strict latency bounds measured in milliseconds while handling request volumes that fluctuate unpredictably. This inversion of priorities transforms every design decision, from how requests are batched to how failures are handled.

Distributed inference systems must solve problems that do not exist at single-machine scale. Load balancing[^fn-load-balancing] becomes critical when requests must be distributed across hundreds of GPU instances while maintaining latency guarantees. Request routing must account for model-specific characteristics: recommendation systems with trillion-parameter embedding tables require different placement strategies than large language models that generate responses token by token. Autoscaling must anticipate demand fluctuations that can change request volume by orders of magnitude within minutes while maintaining latency bounds users expect.

[^fn-load-balancing]: **Load balancing** in inference systems differs fundamentally from traditional web load balancing. Web servers typically complete requests in milliseconds with uniform processing time; GPU inference workers handle requests lasting from 10ms to 30+ seconds with high variance. This variance makes techniques like least-connections and queue-depth-aware routing essential, whereas simple round-robin suffices for stateless web workloads.

The economics of inference at scale differ fundamentally from training economics. Training costs are dominated by compute time and can be amortized over the lifetime of the resulting model. Inference costs are directly tied to user traffic and revenue. An e-commerce recommendation system might serve millions of requests per second during peak shopping periods, with each request contributing directly to potential revenue. The cost of overprovisioning during quiet periods or underprovisioning during peaks translates immediately to business impact. Inference efficiency becomes a first-order concern in ways that training efficiency rarely achieves.

This chapter develops the principles and techniques for building inference systems that scale to meet these demands. We examine when distributed inference becomes necessary, how to architect systems that maintain latency bounds under varying load, and how to optimize the economics of inference through efficient resource utilization. The goal is not merely to make inference work at scale but to make it work efficiently, reliably, and economically.

### When Single-Machine Serving Is Insufficient {#sec-inference-when-insufficient}

Three distinct signals indicate when distributed inference becomes necessary rather than merely optional. @tbl-distribution-triggers categorizes these triggers by constraint type and corresponding strategy.

**Memory exhaustion** occurs when model parameters, key-value caches, or embedding tables exceed single-device capacity. A single NVIDIA H100 GPU provides 80GB of HBM3[^fn-hbm3] memory. GPT-4 class models with hundreds of billions of parameters require 200–400GB just for weights in FP16 precision, forcing distribution across multiple GPUs regardless of throughput requirements. Recommendation systems with trillion-parameter embedding tables face similar constraints: Meta's DLRM[^fn-dlrm] model stores embedding tables that require multiple terabytes of memory.

[^fn-hbm3]: **High Bandwidth Memory 3 (HBM3)**: A 3D-stacked DRAM technology providing 3.35 TB/s bandwidth on the H100, compared to 1 TB/s for HBM2e on the A100. This 3x bandwidth improvement directly enables larger batch sizes and faster token generation for memory-bandwidth-bound LLM decode operations.

[^fn-dlrm]: **Deep Learning Recommendation Model (DLRM)**: Meta's reference architecture (2019) for large-scale recommendation, processing hundreds of billions of inferences daily. DLRM separates dense features (MLP processing) from sparse features (embedding lookups), enabling hybrid CPU-GPU architectures. Embedding tables can exceed 100TB, requiring distributed storage and caching strategies for serving.

**Throughput limitations** emerge when request volume exceeds single-machine capacity even with optimal batching. Consider a recommendation system serving 100,000 queries per second with a 10ms latency budget. If single-machine throughput peaks at 10,000 QPS, no amount of optimization on that machine can satisfy demand. Horizontal scaling across multiple replicas becomes mandatory.

**Latency requirements** drive distribution when model execution time exceeds latency budgets even at batch size one. Large language models generating responses token by token face this constraint acutely. A 70-billion parameter model requires approximately 140GB of memory and achieves roughly 30 tokens per second on a single GPU due to memory bandwidth limitations. Sharding the model across multiple GPUs enables parallel computation that reduces time-to-first-token below acceptable thresholds.

+----------------+--------------------------+----------------------+-----------------------------+
| **Constraint** | **Single-Machine Limit** | **Example Workload** | **Distribution Strategy**   |
+:===============+:=========================+=====================:+:============================+
| **Memory**     | 80GB (H100)              | GPT-4 (400GB+)       | Tensor/pipeline parallelism |
| **Throughput** | ~10K QPS (vision)        | 100K QPS RecSys      | Horizontal replication      |
| **Latency**    | Model execution time     | 500ms LLM TTFT       | Model sharding              |
+----------------+--------------------------+----------------------+-----------------------------+

: **Triggers for Distributed Inference**. Each constraint type indicates different distribution strategies. Memory constraints require model sharding; throughput constraints require replication; latency constraints may require either depending on whether the bottleneck is compute or memory bandwidth. {#tbl-distribution-triggers}

### The Fundamental Inversion: Training vs Inference {#sec-inference-inversion}

The contrast between training and inference optimization extends beyond the basic throughput-versus-latency distinction. Training optimizes for samples processed per hour and tolerates latency variations. Inference optimizes for response time and must meet strict latency bounds. At scale, this inversion manifests in system architecture, resource allocation, and operational priorities. @tbl-training-inference-inversion details six key system aspects where these differences emerge.

+-------------------------+-------------------------------+------------------------------+
| **Aspect**              | **Distributed Training**      | **Distributed Inference**    |
+:========================+:==============================+:=============================+
| **Primary metric**      | Throughput (samples/hour)     | Latency (P99 ms)             |
| **Acceptable variance** | Hours                         | Milliseconds                 |
| **State management**    | Checkpoints (periodic)        | Session state (continuous)   |
| **Batch formation**     | Large, controlled             | Request-driven, variable     |
| **Failure tolerance**   | Restart from checkpoint       | Redirect without user impact |
| **Cost structure**      | Fixed duration, variable rate | Variable duration, fixed SLO |
+-------------------------+-------------------------------+------------------------------+

: **Training vs Inference System Requirements**. The fundamental inversion from throughput to latency optimization ripples through every aspect of system design. {#tbl-training-inference-inversion}

Training tolerates substantial latency variance because the optimization target is aggregate progress over hours or days. A training iteration that takes 2 seconds instead of the usual 1 second represents acceptable variation. An inference request that takes 2 seconds instead of 100 milliseconds represents catastrophic failure, potentially causing user abandonment or cascading timeouts in dependent services.

State management differs fundamentally. Training maintains model state (parameters, optimizer states) that evolves gradually and can be captured in periodic checkpoints. Inference often maintains session state (conversation history, key-value caches, user context) that must be preserved across requests and cannot tolerate the staleness that checkpoint-based recovery would introduce.

Failure handling diverges correspondingly. Training failures trigger checkpoint restoration and continuation, with minutes of lost progress being acceptable. Inference failures must be invisible to users. Requests redirect to healthy replicas, degraded results substitute for unavailable models, and SLOs must be maintained despite infrastructure instability.

### The Serving Tax: Overhead of Distribution {#sec-inference-serving-tax}

Distributing inference across multiple machines introduces overhead absent from single-machine serving. This "serving tax" must be understood and budgeted within latency constraints.

**Network communication** adds latency for every cross-machine interaction. Within a datacenter, network round-trip times range from 50–500 microseconds depending on topology and congestion. For model sharding that requires synchronization between GPUs on different machines, each synchronization point adds this overhead. A model sharded across 8 machines with 4 synchronization points per inference adds 200 microseconds to 2 milliseconds of network latency.

**Serialization overhead** converts in-memory tensors to network-transmittable formats. While modern serialization libraries like FlatBuffers and Cap'n Proto[^fn-serialization] minimize this overhead, large activation tensors still require meaningful time to serialize and deserialize. A 1GB activation tensor takes approximately 100 milliseconds to serialize, even with optimized libraries.

[^fn-serialization]: **Zero-copy serialization**: FlatBuffers (Google) and Cap'n Proto enable reading serialized data directly without parsing or unpacking, eliminating the CPU overhead of traditional formats like Protocol Buffers or JSON. For distributed inference, this reduces per-request serialization overhead from milliseconds to microseconds.

**Load balancer latency** adds another layer. Requests must be routed to appropriate replicas, which requires examining request metadata, consulting routing tables, and forwarding to selected backends. Well-optimized load balancers add 100–500 microseconds; poorly configured ones can add milliseconds.

**Coordination overhead** emerges when requests require fan-out to multiple services. A recommendation system that queries a user model, item model, and ranking model in parallel must coordinate these queries and aggregate results. The coordination logic itself consumes CPU cycles and introduces latency variation.

The total serving tax often consumes 10–30% of the latency budget in distributed systems (@eq-serving-tax):

$$L_{total} = L_{compute} + L_{network} + L_{serialization} + L_{coordination} + L_{queuing}$$ {#eq-serving-tax}

Minimizing this tax requires co-locating communicating components, using high-bandwidth interconnects, and designing communication patterns that minimize round trips.

### Serving Cost Dominates Training Cost {#sec-inference-cost-dominance}

::: {.callout-perspective title="The Economics of Inference"}
A critical insight for infrastructure planning is that serving cost typically dominates training cost over a model's operational lifetime. This reversal from the training-centric view of model development has profound implications for where optimization effort should focus. While training is a one-time capital expenditure (CapEx), serving is a continuous operational expenditure (OpEx) that scales with user growth.
:::

The total cost of operating a model comprises training cost (a one-time expense) and serving cost (an ongoing expense) (@eq-total-cost):

$$C_{total} = C_{training} + C_{serving} \times T_{deployment} \times Q_{rate}$$ {#eq-total-cost}

where $C_{training}$ is the one-time cost to train the model, $C_{serving}$ is the cost per query served, $T_{deployment}$ is the deployment duration in appropriate time units, and $Q_{rate}$ is the query rate.

::: {.callout-notebook title="Engineering Calculation: The Serving Cost Multiplier" collapse="true"}
**Scenario**: Recommend-system model (DLRM).

**Training Cost ($C_{train}$)**
- Hardware: 1,000 GPU-hours @ $3/hr = $3,000
- Engineering Overhead (Data/Exp): 3x Hardware = $9,000
- **Total Training**: $12,000

**Serving Cost ($C_{serve}$)**
- Deployment: 2 years (constant updates, amortized)
- Traffic: 10,000 Queries Per Second (QPS)
- Efficiency: $10^{-5}$ dollars per query ($10 per million)

**Lifetime Volume ($Q_{total}$)**
$$ Q_{total} = 10^4 \text{/s} \times 86,400 \text{ s/day} \times 730 \text{ days} \approx \mathbf{6.31 \times 10^{11} \text{ queries}} $$

**Total Serving Cost**
$$ C_{serve\_total} = 6.31 \times 10^{11} \times \$10^{-5} = \mathbf{\$6,310,000} $$

**Conclusion**:
$$ \text{Ratio} = \frac{\$6,310,000}{\$12,000} \approx \mathbf{526\times} $$
Serving optimization leverage is 500x higher than training optimization. A 1% serving efficiency gain saves \$63k—5x the entire training budget.
:::

The cost dominance ratio varies by application. @tbl-cost-ratios quantifies this disparity:

+-------------------------------+-------------------+-------------------------+-----------+
| **Application**               | **Training Cost** | **Annual Serving Cost** | **Ratio** |
+:==============================+==================:+========================:+==========:+
| **Recommendation (high QPS)** | $10K–100K         | $1M–10M                 | 100–1000x |
| **Search ranking**            | $100K–1M          | $10M–100M               | 100–1000x |
| **LLM API**                   | $1M–100M          | $10M–1B                 | 10–100x   |
| **Internal analytics**        | $1K–10K           | $10K–100K               | 10–100x   |
+-------------------------------+-------------------+-------------------------+-----------+

: **Training vs Serving Cost Ratios**. High-QPS applications like recommendation systems show the most extreme cost dominance of serving over training. {#tbl-cost-ratios}

This cost structure motivates the optimization techniques throughout this chapter. Every percentage point of serving efficiency improvement yields ongoing cost reduction over the model's operational lifetime.

### The Inference Landscape: Beyond LLMs {#sec-inference-landscape}

A critical misconception in current discourse frames inference at scale as synonymous with LLM serving. While large language models present distinctive challenges and attract significant attention, they represent a small fraction of production inference volume. Understanding the full inference landscape proves essential for appropriate technique selection. @tbl-inference-landscape breaks down model types, request volumes, and optimization challenges.

::: {.callout-important title="Production Inference by Request Volume"}

By request count, production ML inference at major technology companies breaks down approximately as:

- **Recommendation and ranking**: 80–90% of requests
- **Vision and image processing**: 5–10% of requests
- **NLP/LLM**: 1–5% of requests (but growing rapidly)
- **Other (fraud detection, ads, etc.)**: 2–5% of requests

Source: Industry reports from Meta, Google, and Netflix infrastructure teams.

:::

Recommendation systems dominate because they serve predictions for every user interaction. Every page load, scroll, or click triggers inference. A user browsing an e-commerce site might generate 100 recommendation requests in a single session. In contrast, LLM queries typically require explicit user action and occur less frequently.

This distribution has important implications. Recommendation systems have driven most production inference innovation. Dynamic batching, embedding sharding, feature store architectures, and low-latency serving were developed primarily for recommendation workloads. LLM-specific techniques like continuous batching and KV cache management are important but address a narrower slice of production inference.

+--------------------+--------------------+--------------------+-------------------+
| **Model Type**     | **Request Volume** | **Latency Target** | **Key Challenge** |
+:===================+:===================+:===================+:==================+
| **Recommendation** | Very high (80–90%) | &lt;10ms P99       | Embedding lookup  |
| **Vision (CNN)**   | Moderate (5–10%)   | 20–100ms           | Batch efficiency  |
| **LLM**            | Lower (1–5%)       | 100ms–10s          | Memory bandwidth  |
| **Speech/Audio**   | Lower              | Real-time          | Sequential decode |
| **Multimodal**     | Growing            | Varies             | Cross-modal sync  |
+--------------------+--------------------+--------------------+-------------------+

: **Production Inference Landscape**. Different model types have different volume, latency requirements, and optimization challenges. Technique selection must match the specific workload. {#tbl-inference-landscape}

### The Serving Hierarchy {#sec-inference-serving-hierarchy}

To organize the optimization techniques in this chapter, we introduce the serving hierarchy as a conceptual framework. Like the memory hierarchy in computer architecture, the serving hierarchy identifies distinct levels at which optimization occurs, each with different targets and techniques.

**Request level**: Optimizations that affect individual request processing. Batching strategies, caching, and preprocessing optimizations operate at this level. The target metric is per-request latency.

**Replica level**: Optimizations within a single model instance. GPU utilization, memory management, and model optimization operate here. The target metric is single-replica throughput.

**Service level**: Optimizations across multiple replicas of the same model. Load balancing, request routing, and replica management operate at this level. The target metric is aggregate service throughput while meeting latency SLOs.

**Platform level**: Optimizations across multiple services and tenants. Resource allocation, multi-tenancy, scheduling, and cluster management operate here. The target metric is overall resource efficiency while meeting diverse SLO requirements.

::: {.callout-note title="Figure: The Serving Hierarchy" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{PlatformColor}{RGB}{200,220,255}
  \definecolor{ServiceColor}{RGB}{220,240,255}
  \definecolor{ReplicaColor}{RGB}{240,250,255}
  \definecolor{RequestColor}{RGB}{255,255,255}

  \tikzset{
    level/.style={draw=black!70, thick, align=center, minimum width=6cm, minimum height=0.8cm}
  }

  \node[level, fill=PlatformColor] (L4) at (0, 3.2) {\textbf{Platform Level}\\Multi-tenancy, Isolation, Cluster Efficiency};
  \node[level, fill=ServiceColor, minimum width=5cm] (L3) at (0, 2.4) {\textbf{Service Level}\\Load Balancing, Autoscaling, Routing};
  \node[level, fill=ReplicaColor, minimum width=4cm] (L2) at (0, 1.6) {\textbf{Replica Level}\\GPU Memory (KV Cache), Kernel Optimization};
  \node[level, fill=RequestColor, minimum width=3cm] (L1) at (0, 0.8) {\textbf{Request Level}\\Batching, Caching, Preprocessing};

  \node[anchor=west, font=\scriptsize, text=gray] at (3.2, 3.2) {Scale: Cluster};
  \node[anchor=west, font=\scriptsize, text=gray] at (2.7, 2.4) {Scale: Service};
  \node[anchor=west, font=\scriptsize, text=gray] at (2.2, 1.6) {Scale: GPU};
  \node[anchor=west, font=\scriptsize, text=gray] at (1.7, 0.8) {Scale: Logic};

\end{tikzpicture}
```
**The Serving Hierarchy**. Optimization occurs at four distinct levels. The Request Level focuses on minimizing per-request latency. The Replica Level maximizes single-instance throughput. The Service Level manages distribution across multiple replicas. The Platform Level handles efficient resource sharing across multiple services and tenants.
:::

Each level has distinct optimization levers. @tbl-serving-hierarchy maps these levels to their primary targets and techniques:

+--------------+-------------------------+----------------------------------------+
| **Level**    | **Optimization Target** | **Key Techniques**                     |
+:=============+:========================+:=======================================+
| **Request**  | Per-request latency     | Dynamic batching, caching, prefetching |
| **Replica**  | Throughput, utilization | Memory optimization, kernel fusion     |
| **Service**  | Aggregate capacity      | Load balancing, routing, autoscaling   |
| **Platform** | Resource efficiency     | Multi-tenancy, scheduling, placement   |
+--------------+-------------------------+----------------------------------------+

: **Serving Hierarchy Optimization Targets**. Each level of the hierarchy addresses different metrics with different techniques. {#tbl-serving-hierarchy}

The remainder of this chapter progresses through these levels: batching and caching (request level), model sharding (replica level), load balancing and autoscaling (service level), and multi-tenancy (platform level).

### Chapter Roadmap {#sec-inference-roadmap}

This chapter develops the techniques for inference at scale through the lens of the serving hierarchy:

**Batching Strategies at Scale** (@sec-inference-batching) examines how different model types require fundamentally different batching approaches. We contrast static batching for vision models, continuous batching for LLMs, and feature-parallel batching for recommendation systems, providing quantitative analysis of throughput-latency tradeoffs.

**Model Sharding for Inference** (@sec-inference-sharding) addresses when and how to distribute model computation across multiple devices. We examine tensor parallelism, pipeline parallelism, expert parallelism, and embedding sharding, with emphasis on communication patterns and overhead.

**Load Balancing and Request Routing** (@sec-inference-load-balancing) develops the theory and practice of distributing requests across replicas. We derive why power-of-two-choices achieves exponentially better load distribution than random assignment and examine routing strategies for stateful workloads.

**Optimization Techniques** (@sec-optimization-at-scale) covers advanced efficiency methods like KV cache management, quantization, and speculative decoding.

**Multi-Tenancy and Isolation** (@sec-inference-multitenancy) examines platform-level concerns: sharing infrastructure across multiple models and users while maintaining isolation and fairness.

**Autoscaling** (@sec-inference-autoscaling) addresses dynamic capacity management, including the cold start problem unique to GPU-based serving and predictive scaling strategies.

**Global Inference Infrastructure** (@sec-inference-global) explores architectural patterns for multi-region deployment, including latency optimization, failover strategies, and cost management.

**Case Studies** (@sec-inference-case-studies) grounds these principles in production systems at Meta, OpenAI, Google, and TikTok, demonstrating how the techniques combine in real deployments.

Throughout, we maintain the model-type diversity essential for practitioners: every major concept is illustrated across LLMs, recommendation systems, vision models, and other production workloads.

## Serving Framework Selection {#sec-inference-frameworks}

The batching, sharding, and load balancing techniques described in the chapter roadmap do not exist in isolation; they are implemented within serving frameworks that constrain and enable different optimizations. Before examining these techniques in depth, practitioners face an immediate practical decision: which serving infrastructure to build upon?

The choice of serving framework determines which optimizations are available, how models are deployed, and what performance characteristics are achievable. Understanding this landscape first provides essential context: when we discuss continuous batching in @sec-inference-continuous-batching, we will see how vLLM and TensorRT-LLM implement it differently. This section provides a systematic framework for selecting among the major options.

### Framework Categories {#sec-inference-framework-categories}

Serving frameworks fall into distinct categories based on their design philosophy and target workloads:

**General-purpose inference servers** provide broad model support with configurable optimization. Triton Inference Server (NVIDIA) offers multi-framework support (PyTorch, TensorFlow, ONNX, TensorRT), dynamic batching, model ensemble orchestration, and concurrent model execution. TensorFlow Serving provides native TensorFlow support, gRPC/REST APIs, model versioning, and batching scheduler. TorchServe (PyTorch) delivers native PyTorch support, model archiving, metrics, and multi-model serving.

**LLM-specialized servers** optimize specifically for autoregressive generation[^fn-autoregressive]. vLLM provides PagedAttention, continuous batching, tensor parallelism, and OpenAI-compatible API. TensorRT-LLM delivers NVIDIA-optimized kernels, in-flight batching, quantization, and multi-GPU support. Text Generation Inference (TGI, Hugging Face) offers model integration, flash attention, tensor parallelism, and watermarking.

[^fn-autoregressive]: **Autoregressive generation**: The process of generating output tokens one at a time, where each new token depends on all previously generated tokens. This sequential dependency creates the memory-bandwidth bottleneck that dominates LLM serving costs, as the entire model must be read from memory for each generated token.

**Optimization-focused runtimes** maximize inference speed through compilation. TensorRT provides graph optimization, kernel fusion, precision calibration, and NVIDIA GPU specific optimizations. ONNX Runtime offers cross-platform optimization and execution providers for different hardware. OpenVINO delivers Intel hardware optimization, model compression, and heterogeneous execution.

### Framework Selection Criteria {#sec-inference-framework-criteria}

Selection depends on model type, deployment constraints, and organizational factors:

**Model architecture determines primary candidates**:

+-------------------------+---------------------------+------------------------------------------+
| **Model Type**          | **Primary Options**       | **Key Consideration**                    |
+:========================+:==========================+:=========================================+
| **LLM (&gt;7B params)** | vLLM, TensorRT-LLM, TGI   | KV cache management, continuous batching |
| **LLM (&lt;7B params)** | vLLM, TGI, Triton         | Simpler deployment, less memory pressure |
| **Vision (CNN/ViT)**    | Triton, TensorRT, ONNX RT | Static batching, throughput optimization |
| **Recommendation**      | Triton, custom            | Feature preprocessing, embedding lookup  |
| **Multi-modal**         | Triton, custom            | Pipeline orchestration                   |
+-------------------------+---------------------------+------------------------------------------+

**Hardware constraints narrow options**:

+---------------------------+----------------------------------+
| **Hardware**              | **Supported Frameworks**         |
+:==========================+:=================================+
| **NVIDIA datacenter GPU** | All options                      |
| **NVIDIA consumer GPU**   | vLLM, TGI (limited TensorRT-LLM) |
| **AMD GPU**               | vLLM (ROCm), ONNX RT             |
| **Intel CPU/GPU**         | OpenVINO, ONNX RT                |
| **Apple Silicon**         | MLX, Core ML, ONNX RT            |
| **AWS Inferentia**        | Neuron SDK                       |
+---------------------------+----------------------------------+

**Operational requirements influence choice**:

- **Multi-model serving**: Triton excels with concurrent model execution
- **Rapid iteration**: TorchServe, TGI offer simpler deployment cycles
- **Maximum throughput**: TensorRT-LLM, vLLM with optimized kernels
- **Cross-platform**: ONNX Runtime provides broadest hardware support

### vLLM Architecture {#sec-inference-vllm}

vLLM[^fn-vllm-name] [@kwon2023vllm] has emerged as the leading open-source LLM serving framework due to its PagedAttention innovation. Understanding its architecture illustrates key LLM serving principles.

[^fn-vllm-name]: **vLLM**: The name stands for "virtual LLM," reflecting its core innovation of applying virtual memory concepts to KV cache management. Developed at UC Berkeley and released in 2023, vLLM achieved 24x throughput improvement over HuggingFace Transformers in initial benchmarks.

The core innovations in vLLM include PagedAttention (virtual memory for KV cache, covered in @sec-optimization-paged-attention), continuous batching (add/remove requests mid-generation), optimized attention kernels (FlashAttention [@dao2022flashattention] integration), and tensor parallelism (automatic model sharding across GPUs).

**Architecture overview**:

```
┌─────────────────────────────────────────────────────┐
│                   vLLM Engine                        │
├─────────────────────────────────────────────────────┤
│  Scheduler          │  Block Manager                │
│  - Request queue    │  - Physical blocks            │
│  - Preemption       │  - Block tables               │
│  - Priority         │  - Copy-on-write              │
├─────────────────────┼───────────────────────────────┤
│  Model Executor     │  Cache Engine                 │
│  - Attention        │  - GPU cache                  │
│  - Sampling         │  - CPU swap space             │
│  - Tensor parallel  │  - Prefix caching             │
└─────────────────────┴───────────────────────────────┘
```

**Deployment example**:

```python
from vllm import LLM, SamplingParams

# Initialize with automatic GPU detection
llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    tensor_parallel_size=4,  # Shard across 4 GPUs
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)

# Efficient batch inference
prompts = ["Explain quantum computing", "Write a poem about AI"]
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)
outputs = llm.generate(prompts, sampling_params)
```

**Performance characteristics**:

+-----------------------------+-------------------+-----------------------------------------+
| **Metric**                  | **Typical Value** | **Notes**                               |
+:============================+==================:+:========================================+
| **Throughput vs baseline**  | 2–4x              | Compared to naive HF generation         |
| **Memory efficiency**       | 90%+ utilization  | PagedAttention eliminates fragmentation |
| **Latency overhead**        | &lt;5ms           | Scheduling and batching overhead        |
| **Max concurrent requests** | 100s–1000s        | Depends on model size and GPU memory    |
+-----------------------------+-------------------+-----------------------------------------+

### TensorRT-LLM Architecture {#sec-inference-tensorrt-llm}

TensorRT-LLM provides NVIDIA-optimized LLM inference with deep hardware integration.

TensorRT-LLM provides several core capabilities. Optimized kernels include custom CUDA kernels for attention, GEMM, and layer norms. In-flight batching implements NVIDIA's continuous batching. Quantization supports INT8, INT4, FP8 with minimal accuracy loss. Multi-GPU configurations enable tensor and pipeline parallelism with NVLink optimization.

**Build and deployment workflow**:

```bash
# Step 1: Convert model to TensorRT-LLM format
python convert_checkpoint.py \
    --model_dir /models/llama-70b \
    --output_dir /models/llama-70b-trt \
    --dtype float16 \
    --tp_size 4

# Step 2: Build optimized engine
trtllm-build \
    --checkpoint_dir /models/llama-70b-trt \
    --output_dir /engines/llama-70b \
    --gemm_plugin float16 \
    --max_batch_size 64 \
    --max_input_len 2048 \
    --max_output_len 512

# Step 3: Deploy with Triton
# (Configuration in model_repository/)
```

**Performance comparison with vLLM**:

+---------------------------+------------------+----------+--------------+
| **Scenario**              | **TensorRT-LLM** | **vLLM** | **Winner**   |
+:==========================+:=================+:=========+:=============+
| **A100 throughput**       | Higher           | Good     | TensorRT-LLM |
| **H100 throughput**       | Highest          | High     | TensorRT-LLM |
| **Deployment simplicity** | Complex          | Simple   | vLLM         |
| **Model support**         | NVIDIA curated   | Broad HF | vLLM         |
| **Quantization options**  | Extensive        | Good     | TensorRT-LLM |
+---------------------------+------------------+----------+--------------+

TensorRT-LLM typically achieves 20–50% higher throughput than vLLM on NVIDIA hardware but requires more complex deployment pipelines.

### Triton Inference Server {#sec-inference-triton}

Triton provides enterprise-grade multi-model serving with sophisticated orchestration capabilities and several key features for production. Multi-framework support allows a single server to host PyTorch, TensorFlow, TensorRT, and ONNX models. Dynamic batching offers configurable batching with latency targets. Model ensembles enable chaining models in inference pipelines. Concurrent execution allows multiple models to share GPU resources. Metrics and monitoring integrate with Prometheus and provide detailed latency breakdown.

**Model repository structure**:

```
model_repository/
├── text_encoder/
│   ├── config.pbtxt
│   └── 1/
│       └── model.onnx
├── image_classifier/
│   ├── config.pbtxt
│   └── 1/
│       └── model.plan  # TensorRT engine
└── ensemble_pipeline/
    ├── config.pbtxt    # Orchestrates above models
    └── 1/
```

**Dynamic batching configuration**:

```protobuf
# config.pbtxt
dynamic_batching {
    preferred_batch_size: [4, 8, 16, 32]
    max_queue_delay_microseconds: 100000  # 100ms max wait
}
instance_group [
    {
        count: 2
        kind: KIND_GPU
        gpus: [0, 1]
    }
]
```

**Use cases where Triton excels**:

- Multi-model pipelines (detection → classification → ranking)
- Mixed workloads on shared GPU clusters
- Organizations with diverse model frameworks
- Production systems requiring detailed observability

### Framework Selection Decision Tree {#sec-inference-framework-decision}

```
Start
  │
  ├─ Is this an LLM (autoregressive generation)?
  │   ├─ Yes → Is maximum throughput critical?
  │   │         ├─ Yes, NVIDIA hardware → TensorRT-LLM
  │   │         └─ No, or mixed hardware → vLLM
  │   │
  │   └─ No → Is this multi-model serving?
  │           ├─ Yes → Triton Inference Server
  │           └─ No → What's the deployment target?
  │                   ├─ NVIDIA GPU → TensorRT + Triton
  │                   ├─ Intel → OpenVINO
  │                   ├─ Cross-platform → ONNX Runtime
  │                   └─ Edge/Mobile → Platform-specific (Core ML, TFLite)
```

**Common deployment patterns**:

+-------------------------+---------------------------+-----------------------------------------+
| **Pattern**             | **Frameworks**            | **Use Case**                            |
+:========================+:==========================+:========================================+
| **LLM API service**     | vLLM + nginx              | ChatGPT-like applications               |
| **High-throughput LLM** | TensorRT-LLM + Triton     | Batch processing, enterprise            |
| **Vision pipeline**     | TensorRT + Triton         | Object detection, classification        |
| **Recommendation**      | Triton + custom embedding | E-commerce, content platforms           |
| **Multi-modal**         | Triton ensemble           | Vision-language, document understanding |
+-------------------------+---------------------------+-----------------------------------------+

### Framework Performance Benchmarking {#sec-inference-framework-benchmarks}

When evaluating frameworks, benchmark on representative workloads:

**LLM benchmark methodology**:

```python
# Standard benchmark parameters
benchmark_config = {
    "input_lengths": [128, 512, 2048],
    "output_lengths": [64, 256, 512],
    "batch_sizes": [1, 8, 32, 64],
    "concurrent_requests": [1, 10, 50, 100],
    "metrics": ["ttft", "tpot", "throughput", "gpu_util"],
}

# Time to First Token (TTFT): Latency until first token generated
# Time Per Output Token (TPOT): Average latency per subsequent token
# Throughput: Total tokens/second across all requests
# GPU utilization: Compute and memory utilization
```

**Representative benchmark results** (Llama-2-70B on 4xA100-80GB):

+---------------------+---------------+---------------+------------------------+
| **Framework**       | **TTFT (ms)** | **TPOT (ms)** | **Throughput (tok/s)** |
+:====================+==============:+==============:+=======================:+
| **TensorRT-LLM**    | 180           | 28            | 2,400                  |
| **vLLM**            | 220           | 32            | 1,900                  |
| **TGI**             | 250           | 35            | 1,700                  |
| **HF Transformers** | 400           | 85            | 600                    |
+---------------------+---------------+---------------+------------------------+

Note: Results vary significantly with configuration, input/output lengths, and batch sizes. Always benchmark on your specific workload.

### Migration and Integration Considerations {#sec-inference-framework-migration}

**Migrating between frameworks**:

- **Model compatibility**: Most frameworks support standard formats (HF, ONNX)
- **API differences**: vLLM uses OpenAI-compatible API; Triton uses gRPC/HTTP
- **Configuration translation**: Batching, parallelism settings differ by framework

**Integration with ML infrastructure**:

+--------------------+------------------------------------------------+
| **Component**      | **Integration Pattern**                        |
+:===================+:===============================================+
| **Model registry** | Pull models on startup, version management     |
| **Feature store**  | Triton ensemble preprocessing, custom backends |
| **Monitoring**     | Prometheus metrics, distributed tracing        |
| **Load balancer**  | Health checks, request routing                 |
| **Autoscaler**     | Custom metrics (queue depth, GPU utilization)  |
+--------------------+------------------------------------------------+

The framework selection made here influences all subsequent serving optimizations. The techniques in following sections (batching, sharding, caching) are implemented differently across frameworks but follow the same underlying principles.

### Orchestration Platforms for Production Serving {#sec-inference-orchestration}

Individual inference runtimes (vLLM, TensorRT-LLM, Triton) handle the mechanics of efficient inference on a single node or small cluster. Production systems require an orchestration layer that manages service composition, autoscaling, traffic management, fault tolerance, and multi-tenancy.

The orchestration layer handles several key responsibilities. Service composition combines multiple models and components into request pipelines. Autoscaling adjusts replicas based on traffic patterns and resource utilization. Traffic management encompasses load balancing, canary deployments, and A/B testing. Fault tolerance provides replica health monitoring and automatic recovery. Multi-tenancy isolates workloads and manages resource allocation across teams.

**Major orchestration platforms**:

+--------------------------------+--------------------------------------------------------+--------------------------------+
| **Platform**                   | **Key Capability**                                     | **Production Users**           |
+:===============================+:=======================================================+:===============================+
| **Ray Serve [@moritz2018ray]** | Scalable Python-native serving, composable deployments | OpenAI, Uber, Instacart        |
| **KServe**                     | Kubernetes-native serving, serverless inference        | Bloomberg, Zillow, enterprises |
| **BentoML**                    | ML model packaging and unified serving API             | Various production deployments |
| **Seldon Core**                | Kubernetes deployment, A/B testing, canary releases    | Financial services, retail     |
+--------------------------------+--------------------------------------------------------+--------------------------------+

**Ray Serve architecture**:

```
┌─────────────────────────────────────────────────────────────┐
│                    Ray Serve Controller                      │
├─────────────────────────────────────────────────────────────┤
│  HTTP Proxy         │  Autoscaler           │  Router        │
│  - Request routing  │  - Replica management │  - Load balance│
│  - Request batching │  - Scale up/down      │  - Affinity    │
├─────────────────────┼───────────────────────┴────────────────┤
│                    Ray Actor Pool                            │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐        │
│  │ Replica 1│ │ Replica 2│ │ Replica 3│ │ Replica N│        │
│  │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │        │
│  └──────────┘ └──────────┘ └──────────┘ └──────────┘        │
└─────────────────────────────────────────────────────────────┘
```

**Production-ready vLLM deployment with Ray Serve**:

```python
from ray import serve
from vllm import LLM, SamplingParams


@serve.deployment(
    num_replicas=4,
    ray_actor_options={"num_gpus": 4},
    autoscaling_config={
        "min_replicas": 2,
        "max_replicas": 16,
        "target_num_ongoing_requests_per_replica": 10,
    },
)
class LLMDeployment:
    def __init__(self):
        self.llm = LLM(
            model="meta-llama/Llama-2-70b-hf", tensor_parallel_size=4
        )
        self.params = SamplingParams(temperature=0.7, max_tokens=256)

    async def __call__(self, request):
        return self.llm.generate([request.prompt], self.params)[0]


# Deploy with automatic scaling
app = LLMDeployment.bind()
serve.run(app)
```

This pattern provides automatic scaling from 2 to 16 replicas based on load. It includes request batching at the serve layer, fault tolerance with automatic replica restart, and zero-downtime deployments.

**Stateless vs stateful serving**: The orchestration layer must account for whether inference is stateless or stateful.

+------------------+-----------------------+---------------------+---------------------------+
| **Serving Type** | **State Location**    | **Scaling Model**   | **Failure Recovery**      |
+:=================+:======================+:====================+:==========================+
| **Stateless**    | None or external      | Horizontal, trivial | Redirect to any replica   |
| **Stateful**     | In-process (KV cache) | Complex, sticky     | Session loss or migration |
+------------------+-----------------------+---------------------+---------------------------+

Vision models and embedding lookups are typically stateless: any replica can serve any request. LLM serving with KV cache is stateful: the cache accumulated during conversation creates replica-specific state.

Stateful LLM serving has several implications:

- **Sticky routing**: Subsequent requests in a conversation must reach the same replica holding the KV cache
- **Failure recovery**: When a stateful replica fails, cached state is lost, requiring either cache regeneration from history (high latency), cache replication to backups (high bandwidth), or accepting session restart (poor experience)
- **Autoscaling complexity**: Scaling down stateful replicas requires draining active sessions, which can take minutes for long conversations
- **Memory-bound capacity**: Each active session consumes KV cache memory, limiting concurrent sessions per replica regardless of compute capacity

**Consistency during model updates**:

When updating models across distributed replicas, requests may hit different model versions during the rollout. Different deployment strategies provide different consistency guarantees:

+-------------------------+--------------------------+-------------------+-----------------------+
| **Deployment Strategy** | **Consistency**          | **Rollout Speed** | **Risk**              |
+:========================+:=========================+:==================+:======================+
| **Blue-green**          | Strong (atomic switch)   | Instant           | High (all-or-nothing) |
| **Canary**              | Eventual (gradual shift) | Slow (hours)      | Low (progressive)     |
| **Rolling update**      | Weak (mixed versions)    | Medium            | Medium                |
+-------------------------+--------------------------+-------------------+-----------------------+

Applications requiring deterministic outputs (compliance, audit trails, reproducible debugging) should implement several practices:

- **Pin model version**: Include version identifier in request routing to ensure the same model handles related requests
- **Use temperature=0**: Eliminates sampling variance, though beam search still has implementation-dependent tiebreaking
- **Implement version-aware caching**: Cache responses with model version tags and invalidate on version change

**Failure scenario: Stateful replica crash**

When a stateful LLM replica crashes mid-conversation:

```
1. Load balancer detects health check failure (1–10 seconds)
2. New requests route to healthy replicas
3. In-flight requests fail; clients must retry
4. Session state (KV cache) is lost
5. Recovery options:
   a. Regenerate: Client resends conversation history (high latency)
   b. Redirect: Route to replica with replicated state (if available)
   c. Restart: Begin new session (poor user experience)
```

Production systems often accept option (a) with optimizations. The regenerated prefill can process the full conversation history in a single batch, taking seconds rather than the minutes the original conversation took.

**Build vs buy: Managed serving services**:

Before selecting frameworks, teams must decide whether to self-host or use managed services:

+-------------------------+--------------+--------------------------------------------------+--------------------------------------+
| **Service**             | **Provider** | **Key Features**                                 | **Trade-offs**                       |
+:========================+:=============+:=================================================+:=====================================+
| **SageMaker Endpoints** | AWS          | Managed hosting, autoscaling, A/B testing        | Lock-in, cost, limited customization |
| **Vertex AI Endpoints** | GCP          | TPU support, traffic splitting, model monitoring | GCP ecosystem dependency             |
| **Azure ML Endpoints**  | Azure        | Enterprise integration, ONNX optimization        | Azure ecosystem dependency           |
| **Anyscale Endpoints**  | Anyscale     | Ray-native, fine-grained autoscaling             | Emerging platform                    |
+-------------------------+--------------+--------------------------------------------------+--------------------------------------+

+--------------------------------+--------------------------------------------+-----------------------------------------------+
| **Approach**                   | **Advantages**                             | **Disadvantages**                             |
+:===============================+:===========================================+:==============================================+
| **Self-hosted (vLLM/Triton)**  | Full control, cost optimization at scale   | Operational burden, expertise required        |
| **Managed (SageMaker/Vertex)** | Operational simplicity, integrated tooling | Lock-in, cost at scale, limited customization |
| **Hybrid (Ray Serve + cloud)** | Flexibility, gradual migration             | Complexity in managing both                   |
+--------------------------------+--------------------------------------------+-----------------------------------------------+

**Decision factors**:

- **Team size**: Teams with fewer than 5 ML engineers often benefit from managed services
- **Scale**: More than 1M daily requests typically makes self-hosting cost-effective
- **Customization needs**: Novel architectures require self-hosting
- **Latency requirements**: Self-hosting enables co-location and deeper optimization

**Enhanced framework selection with orchestration**:

```
Start
  │
  ├─ What scale?
  │   ├─ <100 QPS → Simple deployment (managed services or single instance)
  │   ├─ 100-10K QPS → Need autoscaling
  │   │   ├─ Managed acceptable → SageMaker/Vertex
  │   │   └─ Self-hosted required → Ray Serve + vLLM/TensorRT-LLM
  │   └─ >10K QPS → Need distributed orchestration
  │       ├─ LLM → Ray Serve + vLLM with sharding
  │       ├─ RecSys → Custom or Triton with embedding sharding
  │       └─ Vision → Triton with dynamic batching
  │
  ├─ How many model types?
  │   ├─ Single model → Direct runtime deployment
  │   └─ Multiple models/pipelines → Triton or Ray Serve composition
  │
  └─ Stateful or stateless?
      ├─ Stateless → Any load balancer, simple scaling
      └─ Stateful (LLM with cache) → Sticky routing, session management
```

**Case study: Startup serving Llama-70B for customer support**

*Scenario*: A startup is launching an LLM-powered customer support chatbot using a fine-tuned Llama-70B model.

+-------------------------+----------------------------------+
| **Constraint**          | **Value**                        |
+:========================+=================================:+
| **Expected traffic**    | 100 QPS average, 500 QPS peak    |
| **Latency requirement** | &lt;2s time to first token       |
| **Budget**              | 4 H100 GPUs (leased)             |
| **Team**                | 2 ML engineers, no dedicated SRE |
| **Conversation length** | Average 8 turns, max 32K context |
+-------------------------+----------------------------------+

*Analysis*:

1. **Memory**: Llama-70B requires ~140GB in FP16. With 4-bit quantization (AWQ), this drops to ~35GB, fitting on a single H100 (80GB) with room for KV cache.

2. **Throughput**: At 500 QPS peak with average 100 output tokens, the system must sustain 50,000 tokens/second. A single quantized Llama-70B on H100 achieves approximately 1,000 tokens/second with continuous batching, so 4 GPUs provide headroom.

3. **Tensor parallelism vs replicas**: Two options exist:
   - 2 replicas with 2-way TP: Higher availability, lower per-request latency
   - 4 replicas with 1-way TP: Maximum throughput, simpler scaling

   With AWQ fitting on a single GPU, the second option is preferred for this traffic level.

4. **Team size**: 2 ML engineers without SRE experience suggests managed services or simple orchestration.

*Decision*:

+------------+-------------------------+--------------------+-------------------------------+
| **Option** | **Architecture**        | **Pros**           | **Cons**                      |
+:===========+:========================+:===================+:==============================+
| **A**      | SageMaker + HF TGI      | Minimal ops burden | Cost, limited optimization    |
| **B**      | vLLM + Ray Serve on EC2 | Good balance       | Some ops required             |
| **C**      | TensorRT-LLM + Triton   | Maximum throughput | Complex, overkill for 500 QPS |
+------------+-------------------------+--------------------+-------------------------------+

**Recommendation**: Option B (vLLM + Ray Serve) provides the best balance. At 500 QPS, the 30% throughput advantage of TensorRT-LLM does not justify the deployment complexity. Start with vLLM; migrate to TensorRT-LLM only if traffic grows beyond 2,000 QPS.

```python
# Recommended production configuration
@serve.deployment(
    num_replicas=4,
    ray_actor_options={"num_gpus": 1},
    autoscaling_config={
        "min_replicas": 2,
        "max_replicas": 8,
        "target_num_ongoing_requests_per_replica": 20,
    },
)
class CustomerSupportLLM:
    def __init__(self):
        self.llm = LLM(
            model="your-finetuned-llama-70b-awq",
            quantization="awq",
            max_model_len=32768,
        )
```

This configuration handles the 500 QPS peak with 4 replicas, can scale to 8 during unexpected spikes, and scales down to 2 during low-traffic periods to reduce cost.

## Batching Strategies at Scale {#sec-inference-batching}

Having selected a serving framework, we must now configure how it processes requests. The framework determines which batching implementations are available: vLLM provides continuous batching with PagedAttention, TensorRT-LLM offers in-flight batching with optimized kernels, and Triton supports custom batching policies. Understanding the underlying principles of batching enables effective configuration regardless of framework choice, because where frameworks differ in *how* they batch, the principles we examine here govern *what* batching strategy suits each workload.

The core insight of batching is that processing multiple requests together amortizes fixed costs, including model loading, kernel launch overhead, and memory transfer latency, across more work. This trades higher per-request latency for dramatically improved throughput. Single-machine serving applies this through dynamic batching, which collects requests within a time window before processing them together.

At scale, batching becomes more complex because different model architectures have distinct batching requirements. A strategy optimal for vision models may be catastrophic for LLMs, and techniques developed for recommendation systems may not apply to either.

**Production reality**: Recommendation systems constitute 80–90% of inference requests at major technology companies, with vision models handling most of the remainder, and LLMs currently representing 1–5% of request volume (though growing rapidly). Despite this distribution, we present batching strategies in order of conceptual complexity: vision (straightforward batching), LLMs (continuous batching with KV cache), and recommendation (feature-parallel batching with distributed embedding). This pedagogical ordering builds understanding progressively, even though practitioners will most frequently encounter recommendation workloads first.

This section develops a taxonomy of batching strategies matched to model characteristics, providing quantitative analysis of when each approach applies and what performance to expect.

### Why Batching Differs Across Model Types {#sec-inference-batching-differences}

The core insight is that batching efficiency depends on how computation scales with batch size relative to how memory and communication scale. Different model architectures exhibit different scaling relationships, requiring different batching strategies.

::: {.callout-note title="Figure: Batching Strategies Compared" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  \definecolor{StaticColor}{RGB}{200,220,255}
  \definecolor{ContColor}{RGB}{255,220,200}
  \definecolor{FeatColor}{RGB}{220,255,200}

  \tikzset{
    req/.style={draw=black!70, thick, minimum width=0.8cm, minimum height=0.4cm, font=\tiny},
    group/.style={draw=black!50, dashed, rounded corners=2pt}
  }

  % Static Batching
  \node[anchor=west] at (0, 3.5) {\textbf{A. Static Batching} (Vision)};
  \draw[group, fill=StaticColor!20] (0, 2.2) rectangle (4, 3.2);
  \node[req] at (0.6, 2.7) {R1}; \node[req] at (1.6, 2.7) {R2}; \node[req] at (2.6, 2.7) {R3}; \node[req] at (3.6, 2.7) {R4};
  \node[anchor=west, font=\scriptsize, text=gray] at (4.2, 2.7) {Wait for full batch};

  % Continuous Batching
  \node[anchor=west] at (0, 1.5) {\textbf{B. Continuous Batching} (LLM)};
  \draw[group, fill=ContColor!20] (0, 0.2) rectangle (4, 1.2);
  \node[req, fill=green!20] at (0.6, 0.7) {R1}; \node[req, fill=green!20] at (1.6, 0.7) {R2};
  \node[req, fill=blue!20] at (2.6, 0.7) {R5}; \node[req, fill=gray!20] at (3.6, 0.7) {R6};
  \node[anchor=west, font=\scriptsize, text=gray] at (4.2, 0.7) {Insert mid-generation};

  % Feature Parallel
  \node[anchor=west] at (8, 3.5) {\textbf{C. Feature-Parallel} (RecSys)};
  \draw[group, fill=FeatColor!20] (8, 2.2) rectangle (12, 3.2);
  \node[req] at (8.6, 2.9) {U1}; \node[req] at (9.6, 2.9) {U2}; \node[req] at (10.6, 2.9) {U3};
  \node[req] at (8.6, 2.5) {I1}; \node[req] at (9.6, 2.5) {I2}; \node[req] at (10.6, 2.5) {I3};
  \node[anchor=west, font=\scriptsize, text=gray] at (12.2, 2.7) {Shard by feature};

\end{tikzpicture}
```
**Batching Strategies Compared**. **Static batching** (A) waits for all slots to fill, creating latency bubbles. **Continuous batching** (B) allows requests to join and leave at each iteration, maximizing GPU utilization for LLMs. **Feature-parallel batching** (C) shards requests by feature type (e.g., User IDs vs. Item IDs), optimizing for distributed embedding lookup.
:::

For **vision models** (CNNs, ViTs processing fixed-size images), computation scales linearly with batch size while memory scales sub-linearly due to weight sharing. Larger batches improve GPU utilization with minimal overhead, making static or dynamic batching with large batch sizes optimal.

For **LLMs in the decode phase**, computation per token is small relative to memory bandwidth requirements for loading model weights. The bottleneck is memory bandwidth, not compute. Larger batches amortize weight loading across more tokens, dramatically improving throughput but with diminishing returns as batch size grows.

For **recommendation systems**, the bottleneck is often embedding lookup rather than dense computation. Batching strategies must optimize for parallel embedding access patterns rather than matrix multiplication throughput.

### The Physics of Batching: The Efficiency Curve

Batching is not merely a heuristic; it is a trade-off governed by the physics of hardware utilization. We can model the relationship between Batch Size ($B$), Latency ($L$), and Throughput ($X$) to identify the optimal operating point for any inference system.

The **Latency Equation** decomposes per-request latency into fixed overheads (kernel launch, memory loading) and variable costs (compute per sample):

$$ L(B) = T_{fixed} + B \times T_{variable} $$

*   $T_{fixed}$: Costs paid once per batch (e.g., loading weights from HBM, kernel launch latency).
*   $T_{variable}$: Marginal cost of adding one request (e.g., compute time for that sample).

The **Throughput Equation** describes the system's capacity:

$$ X(B) = \frac{B}{L(B)} = \frac{B}{T_{fixed} + B \times T_{variable}} $$

This relationship reveals the **Batching Efficiency Curve**:
1.  **Small $B$**: Throughput is dominated by $T_{fixed}$. The system is **latency-bound** (or overhead-bound). Increasing $B$ yields super-linear throughput gains.
2.  **Large $B$**: As $B \to \infty$, the $T_{fixed}$ term becomes negligible. Throughput asymptotically approaches the hardware limit $1/T_{variable}$. The system becomes **compute-bound** (or bandwidth-bound for LLMs).
3.  **The Knee**: The optimal batch size is the point where throughput gains diminish while latency continues to grow linearly.

The engineering goal is to find the maximum $B$ such that $L(B) \le \text{SLO}$. This formulation explains why vision models (high $T_{variable}$) saturate at smaller batches than LLMs (high $T_{fixed}$ due to weight loading), requiring different tuning strategies. @tbl-batching-by-model summarizes how these batching characteristics vary across model architectures.

::: {.callout-notebook title="Engineering Calculation: Little's Law for Inference" collapse="true"}
**Concept**: In any stable queuing system, the average number of requests in the system ($L$) equals the arrival rate ($\lambda$) multiplied by the average time a request spends in the system ($W$).

$$ L = \lambda \times W $$

**Application: Concurrency Planning**
- **Target Throughput ($\lambda$)**: 1,000 requests/sec
- **Latency SLO ($W$)**: 100 ms (0.1 s)

**Required Concurrency ($L$)**:
$$ L = 1000 \times 0.1 = \mathbf{100 \text{ concurrent requests}} $$

**Capacity Planning**:
If a single GPU replica handles batch size 8 with 80ms latency:
1. Replica Throughput = $8 / 0.08 = 100 \text{ req/s}$
2. Replicas Needed = $1000 / 100 = 10 \text{ replicas}$

**Verification**:
Total system concurrency = $10 \text{ replicas} \times 8 \text{ batch} = 80$.
Wait! We needed 100.
*Correction*: We must account for queue depth. With 10 replicas active processing 80 reqs, 20 reqs are in queues.
Queue wait time added to latency must not violate SLO.
:::

+-------------------+-----------------------+------------------------+--------------------+------------------------+
| **Model Type**    | **Batching Strategy** | **Typical Batch Size** | **Key Constraint** | **Throughput Scaling** |
+:==================+:======================+=======================:+:===================+:=======================+
| **Vision (CNN)**  | Static/Dynamic        | 32–256                 | GPU compute        | Near-linear to 64+     |
| **LLM (prefill)** | Dynamic               | 1–64                   | Memory capacity    | Sub-linear             |
| **LLM (decode)**  | Continuous            | 100–1000s              | Memory bandwidth   | Log-linear             |
| **RecSys**        | Feature-parallel      | 1000–10000s            | Embedding lookup   | Depends on sharding    |
| **Speech**        | Streaming             | 1                      | Real-time          | N/A (latency-bound)    |
+-------------------+-----------------------+------------------------+--------------------+------------------------+

: **Batching Strategy by Model Type**. Each model type has characteristic batching behavior determined by its computational bottleneck. {#tbl-batching-by-model}

### Queuing Theory for Batched Inference {#sec-inference-queuing-theory}

The batching efficiency curve provides intuition about throughput-latency tradeoffs, but production systems require rigorous analysis to determine optimal batch sizes under stochastic arrival patterns. Queuing theory provides the mathematical framework to derive these optimal operating points and to understand why certain batch sizes outperform others under specific conditions.

#### The M/G/c/K Queue Model for GPU Serving {#sec-inference-mgck-model}

GPU inference systems can be modeled as **M/G/c/K queues**, a standard notation from queueing theory that captures the essential characteristics of production serving systems:

- **M (Markov arrivals)**: Requests arrive according to a Poisson process with rate $\lambda$. This models the memoryless property of user requests, where arrival of one request does not predict the timing of the next.
- **G (General service distribution)**: Service times follow a general distribution, not restricted to exponential. GPU inference times depend on batch size and exhibit deterministic components (compute) mixed with stochastic variation (memory contention, kernel scheduling).
- **c (Number of servers)**: The system has $c$ parallel GPU workers, each capable of serving requests independently.
- **K (Queue capacity)**: The system maintains a finite queue of capacity $K$ requests. Requests arriving to a full queue are rejected (load shedding).

::: {.callout-note title="Connection: Queuing Theory and Systems Performance"}
Queuing theory, developed by Agner Krarup Erlang in 1909 for telephone network analysis, remains foundational to systems performance engineering. The same mathematical framework that sized telephone exchanges now determines GPU cluster capacity. The M/G/c/K model is standard in systems textbooks, appearing in Jain's *The Art of Computer Systems Performance Analysis* [@jain1991art] and Kleinrock's *Queueing Systems* [@kleinrock1975queueing].
:::

For a batched inference system with batch size $B$, service time becomes a function of batch size. Let $S(B)$ denote the time to process a batch of $B$ requests. From the physics of batching (@sec-inference-batching-differences), we model this as:

$$S(B) = \alpha + \beta \cdot B$$ {#eq-service-time-batch}

where $\alpha$ represents fixed overhead (kernel launch, weight loading from HBM) and $\beta$ represents marginal per-request computation time. This linear model captures the first-order behavior observed in production systems, though actual service times may exhibit slight sublinearity due to memory bandwidth saturation at large batch sizes.

The **effective service rate** for batched processing is:

$$\mu_{eff}(B) = \frac{B}{S(B)} = \frac{B}{\alpha + \beta \cdot B}$$ {#eq-effective-service-rate}

This effective rate increases with batch size, approaching the asymptotic limit $1/\beta$ as $B \to \infty$.

#### Response Time Analysis {#sec-inference-response-time}

The total response time $T$ for a request consists of three components:

$$E[T] = E[W] + E[S_{batch}] + E[S_{compute}]$$ {#eq-total-response-time}

where:

- $E[W]$ is the expected waiting time in queue before joining a batch
- $E[S_{batch}]$ is the expected time to form a complete batch (batch accumulation delay)
- $E[S_{compute}]$ is the expected inference time once the batch executes

For dynamic batching with maximum wait time $T_{max}$ and maximum batch size $B_{max}$, the batch formation process is bounded. Under Poisson arrivals with rate $\lambda$, the expected number of requests accumulated in time $T_{max}$ is $\lambda \cdot T_{max}$. The actual batch size $B$ follows:

$$E[B] = \min(B_{max}, \lambda \cdot T_{max})$$ {#eq-expected-batch-size}

The batch accumulation delay depends on whether the batch fills by reaching $B_{max}$ or by timeout:

$$E[S_{batch}] = \begin{cases}
\frac{B_{max}}{\lambda} & \text{if } \lambda \cdot T_{max} \geq B_{max} \text{ (batch fills)} \\
\frac{T_{max}}{2} & \text{if } \lambda \cdot T_{max} < B_{max} \text{ (timeout triggers)}
\end{cases}$$ {#eq-batch-accumulation}

The factor of $1/2$ in the timeout case reflects the average wait for requests arriving uniformly throughout the batch window.

#### Optimal Batch Size Derivation {#sec-inference-optimal-batch}

The optimal batch size $B^*$ minimizes expected response time subject to throughput requirements. We seek:

$$B^* = \arg\min_{B} E[T(B)] \quad \text{subject to} \quad \mu_{eff}(B) \geq \lambda$$ {#eq-optimal-batch-objective}

The constraint ensures system stability (service rate exceeds arrival rate).

Substituting the response time components:

$$E[T(B)] = E[W(B)] + \frac{B}{2\lambda} + S(B)$$ {#eq-response-time-expanded}

For an M/G/1 queue with batch arrivals (treating each batch as a single "super-request"), the Pollaczek-Khinchine formula gives the expected waiting time:

$$E[W] = \frac{\lambda \cdot E[S^2]}{2(1 - \rho)}$$ {#eq-pk-formula}

where $\rho = \lambda \cdot E[S] / B$ is the server utilization (arrival rate times service time per request). The second moment $E[S^2]$ captures service time variability.

For our linear service time model with deterministic service (variance zero within a batch):

$$E[W(B)] = \frac{\lambda \cdot S(B)^2}{2B(1 - \lambda \cdot S(B)/B)}$$ {#eq-waiting-time-batch}

Taking the derivative with respect to $B$ and setting equal to zero yields the optimal batch size. While the full derivation involves solving a cubic equation, the approximate solution for systems where $\alpha \gg \beta$ (high fixed overhead, typical of LLMs) is:

$$B^* \approx \sqrt{\frac{2\alpha\lambda}{1 - \rho_{target}}}$$ {#eq-optimal-batch-approx}

where $\rho_{target}$ is the target utilization (typically 0.7-0.8 for production systems to maintain latency headroom).

::: {.callout-important title="The Square Root Law for Batch Sizing"}
Equation @eq-optimal-batch-approx reveals a fundamental insight: **optimal batch size scales with the square root of the fixed overhead times arrival rate**. This means:

1. **Higher traffic ($\lambda$)**: Optimal batch size increases, but sublinearly
2. **Higher fixed overhead ($\alpha$)**: Optimal batch size increases (amortize more overhead)
3. **Higher target utilization ($\rho_{target}$)**: Optimal batch size increases (more aggressive batching)

This square root relationship explains why LLMs (high $\alpha$ from weight loading) benefit from larger batches than vision models (low $\alpha$), even at the same arrival rate.
:::

#### Worked Example: GPT-3 Serving at 100 QPS {#sec-inference-gpt3-example}

Consider serving a GPT-3 class model (175B parameters) with the following characteristics:

**System Parameters**:

- Arrival rate: $\lambda = 100$ requests/second
- Hardware: 8x A100 GPUs with tensor parallelism
- Weight loading overhead: $\alpha = 50$ ms (time to load attention matrices per forward pass)
- Per-token compute: $\beta = 0.5$ ms per request (amortized across batch)
- Average output length: 100 tokens per request
- Target utilization: $\rho_{target} = 0.75$

**Service Time Model**:

For the prefill phase (processing input prompt), service time follows @eq-service-time-batch:

$$S(B) = 50 + 0.5 \cdot B \text{ ms}$$

**Optimal Batch Size Calculation**:

Applying @eq-optimal-batch-approx:

$$B^* \approx \sqrt{\frac{2 \times 50 \times 100}{1 - 0.75}} = \sqrt{\frac{10000}{0.25}} = \sqrt{40000} = 200$$

However, this exceeds practical limits (memory constraints typically cap batch size around 32-64 for large LLMs). With $B_{max} = 32$:

**Performance at Different Batch Sizes**:

@tbl-batch-size-comparison quantifies the tradeoffs across batch sizes from 1 to 32:

+------------+-------------------+-------------------+-------------------+-------------------+-------------------+
| **Batch**  | **Service Time**  | **Throughput**    | **Utilization**   | **Queue Wait**    | **Total Latency** |
| **Size B** | **S(B) (ms)**     | **(req/s)**       | **$\rho$**        | **E[W] (ms)**     | **E[T] (ms)**     |
+:===========+==================:+==================:+==================:+==================:+==================:+
| 1          | 50.5              | 19.8              | 505% (unstable)   | $\infty$          | $\infty$          |
| 4          | 52.0              | 76.9              | 130% (unstable)   | $\infty$          | $\infty$          |
| 8          | 54.0              | 148.1             | 67.5%             | 42.3              | 123.2             |
| 16         | 58.0              | 275.9             | 36.2%             | 16.4              | 92.4              |
| 32         | 66.0              | 484.8             | 20.6%             | 8.9               | 91.5              |
+------------+-------------------+-------------------+-------------------+-------------------+-------------------+

: **Batch Size Impact on GPT-3 Serving Performance**. Batch sizes below 8 cannot sustain 100 QPS (utilization exceeds 100%). The optimal operating point balances throughput against latency, with B=32 achieving lowest total latency despite higher service time. {#tbl-batch-size-comparison}

**Analysis**:

1. **Batch sizes 1-4 are unstable**: Utilization exceeds 100%, meaning the system cannot keep up with arrivals. Queues grow unboundedly.

2. **Batch size 8 achieves stability**: At 67.5% utilization, the system is stable but queuing delays contribute significantly to latency.

3. **Batch size 32 minimizes total latency**: Despite longer service time (66ms vs 54ms), the dramatic reduction in queue wait time (8.9ms vs 42.3ms) yields lower total latency.

4. **Diminishing returns beyond B=32**: Further batch size increases would reduce utilization but memory constraints prevent exploration.

::: {.callout-notebook title="Engineering Calculation: Applying Little's Law to Verify" collapse="true"}
**Verification using Little's Law**: $L = \lambda \cdot W$

At B=32 with $\lambda = 100$ req/s and $E[T] = 91.5$ ms:

$$L = 100 \times 0.0915 = 9.15 \text{ requests in system}$$

With batch size 32 and utilization 20.6%:
- Expected requests in service: $32 \times 0.206 = 6.6$
- Expected requests in queue: $9.15 - 6.6 = 2.55$

This matches our queue wait calculation: approximately 2-3 requests waiting on average.
:::

#### Decision Framework: Batch Size Selection Given SLA {#sec-inference-batch-decision}

Production systems must select batch size to meet Service Level Objectives (SLOs), typically specified as latency percentiles (e.g., P99 latency < 200ms). The following framework systematizes this decision:

**Step 1: Characterize Service Time**

Measure $\alpha$ and $\beta$ empirically by profiling inference at batch sizes 1, 8, and 32. Fit the linear model $S(B) = \alpha + \beta B$.

**Step 2: Compute Stability Threshold**

Find minimum batch size $B_{min}$ such that $\mu_{eff}(B_{min}) > \lambda$:

$$B_{min} = \frac{\alpha \lambda}{1 - \beta \lambda}$$ {#eq-stability-threshold}

Any batch size below $B_{min}$ results in an unstable system.

**Step 3: Compute Latency at Candidate Batch Sizes**

For each candidate $B \in \{B_{min}, 2B_{min}, ..., B_{max}\}$, compute:

$$E[T(B)] = \frac{\lambda \cdot S(B)^2}{2B(1 - \lambda S(B)/B)} + \frac{B}{2\lambda} + S(B)$$

**Step 4: Account for Tail Latency**

For P99 SLO compliance, use the heavy-traffic approximation for tail latency:

$$T_{P99} \approx E[T] + 2.33 \cdot \sigma_T$$ {#eq-p99-approximation}

where $\sigma_T$ is the standard deviation of response time. For exponential-like service time distributions, $\sigma_T \approx E[T]$, yielding $T_{P99} \approx 3.3 \cdot E[T]$.

**Step 5: Select Optimal Batch Size**

Choose the largest $B$ such that $T_{P99}(B) \leq \text{SLO}$:

$$B^* = \max\{B : T_{P99}(B) \leq \text{SLO}\}$$ {#eq-batch-selection-rule}

@tbl-batch-decision-framework summarizes the decision process:

+-----------------------+-----------------------------------------------+-------------------------------------+
| **Condition**         | **Recommended Action**                        | **Rationale**                       |
+:======================+:==============================================+:====================================+
| $B < B_{min}$         | Increase batch size or add replicas           | System is unstable                  |
| $T_{P99} > \text{SLO}$| Reduce batch size or add replicas             | Latency exceeds target              |
| $\rho < 0.5$          | Consider reducing replicas to save cost       | System is overprovisioned           |
| $\rho > 0.85$         | Add replicas for headroom                     | Approaching instability             |
+-----------------------+-----------------------------------------------+-------------------------------------+

: **Batch Size Decision Framework**. Systematic approach to selecting batch size based on stability, latency SLO, and utilization targets. {#tbl-batch-decision-framework}

#### Trade-off Curves: Visualizing the Operating Region {#sec-inference-tradeoff-curves}

The relationship between batch size, throughput, and latency defines an **operating region** within which production systems must function. @fig-batch-tradeoff-curves illustrates this region:

::: {#fig-batch-tradeoff-curves fig-env="figure" fig-pos="htb" fig-cap="**Batch Size Trade-off Curves**. The operating region (shaded) is bounded by the stability constraint (minimum throughput) and latency SLO (maximum latency). Larger batch sizes increase throughput but also increase latency. The optimal operating point maximizes throughput while meeting the SLO." fig-alt="Graph showing throughput on y-axis and latency on x-axis. A curved line represents different batch sizes from B=1 to B=64. The operating region is shaded, bounded by a vertical line for SLO and horizontal line for minimum throughput."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9]
  % Axes
  \draw[->, thick] (0,0) -- (8,0) node[right] {Latency (ms)};
  \draw[->, thick] (0,0) -- (0,5) node[above] {Throughput (req/s)};

  % Grid
  \draw[gray!30, thin] (0,0) grid[step=1] (7.5,4.5);

  % Operating region (shaded)
  \fill[blue!10] (0.5,1.5) -- (0.5,4) -- (5,4) -- (5,1.5) -- cycle;

  % SLO boundary
  \draw[red, thick, dashed] (5,0) -- (5,4.5) node[above, font=\scriptsize] {SLO};

  % Minimum throughput
  \draw[orange, thick, dashed] (0,1.5) -- (7.5,1.5) node[right, font=\scriptsize] {Min $\lambda$};

  % Trade-off curve (batch size increasing)
  \draw[blue, very thick]
    (0.5,0.5) node[below, font=\scriptsize] {B=1}
    .. controls (1,1.2) and (1.5,2) ..
    (2,2.5) node[above left, font=\scriptsize] {B=4}
    .. controls (2.5,3) and (3,3.3) ..
    (3.5,3.5) node[above, font=\scriptsize] {B=8}
    .. controls (4,3.7) and (4.5,3.9) ..
    (5.5,4) node[above, font=\scriptsize] {B=16}
    .. controls (6,4.05) and (6.5,4.1) ..
    (7,4.1) node[right, font=\scriptsize] {B=32};

  % Optimal point
  \fill[green!70!black] (5,4) circle (3pt);
  \node[above right, font=\scriptsize, green!50!black] at (5,4) {Optimal $B^*$};

  % Labels
  \node[font=\scriptsize] at (3,2.5) {Operating};
  \node[font=\scriptsize] at (3,2.1) {Region};

  % Axis labels
  \node[font=\scriptsize] at (1,-0.5) {50};
  \node[font=\scriptsize] at (3,-0.5) {150};
  \node[font=\scriptsize] at (5,-0.5) {250};
  \node[font=\scriptsize] at (7,-0.5) {350};
  \node[font=\scriptsize] at (-0.5,1.5) {100};
  \node[font=\scriptsize] at (-0.5,3) {200};
  \node[font=\scriptsize] at (-0.5,4.5) {300};
\end{tikzpicture}
```
:::

The trade-off curve demonstrates several key insights:

1. **Pareto frontier**: The curve represents efficient operating points; any point below the curve is dominated by a point on the curve with either higher throughput or lower latency.

2. **Knee of the curve**: The optimal batch size often lies at the "knee" where throughput gains diminish while latency continues to increase linearly.

3. **SLO-constrained optimum**: When an SLO bounds maximum latency, the optimal point is where the curve intersects the SLO boundary.

4. **Diminishing returns**: Beyond the knee, doubling batch size may increase throughput by only 10-20% while doubling latency.

This queuing-theoretic framework provides the mathematical foundation for the batching strategies examined in the following sections. Where intuition might suggest "larger batches are better for throughput," the analysis reveals that stability constraints, latency targets, and diminishing returns create a well-defined optimal operating region that varies by model type and deployment requirements.

### Static and Dynamic Batching for Vision Models {#sec-inference-static-dynamic-batching}

Vision models represent the simplest batching case because inputs have uniform size (after preprocessing) and computation follows a predictable pattern. Single-machine batching principles apply directly, with scale introducing considerations of batch formation across multiple replicas.

Static batching collects exactly $B$ requests before processing. This maximizes GPU utilization when request arrival is predictable but causes unbounded latency during low-traffic periods.

Dynamic batching collects requests for a maximum time window $T_{window}$ or until reaching maximum batch size $B_{max}$, whichever occurs first. The expected latency under Poisson arrivals with rate $\lambda$ follows @eq-dynamic-batch-latency:

$$E[L_{total}] = E[L_{queue}] + L_{batch} + L_{inference}(B)$$ {#eq-dynamic-batch-latency}

where $E[L_{queue}]$ is the expected queuing delay, $L_{batch}$ is the batch formation delay (up to $T_{window}$), and $L_{inference}(B)$ is the inference time for batch size $B$.

::: {.callout-note title="Worked Example: Dynamic Batching for ResNet-50 at Scale"}

Consider a vision classification service with the following requirements:

- **Arrival rate**: 5,000 QPS
- **Latency SLO**: 50ms P99
- **Per-image inference time**: 5ms at batch=1, 25ms at batch=32
- **Number of replicas**: 10 (each handling 500 QPS)

For a single replica with Poisson arrivals at $\lambda = 500$ QPS:

**Option A: No batching (batch=1)**

- Service time: 5ms per request
- Utilization: $\rho = \lambda \times S = 500 \times 0.005 = 2.5$ (impossible, system is overloaded)

This configuration cannot meet demand. Batching is required.

**Option B: Dynamic batching with $B_{max}=16$, $T_{window}=10ms$**

Expected requests per window: $E[B] = \lambda \times T_{window} = 500 \times 0.01 = 5$

With 5 requests per batch:

- Inference time: approximately 8ms (interpolating between batch=1 and batch=32)
- Per-request compute: 8ms / 5 = 1.6ms
- Maximum batch delay: 10ms
- Expected total latency: ~15ms mean, ~30ms P99

Utilization: $\rho = 500 \times 0.0016 = 0.8$ (sustainable)

**Option C: Dynamic batching with $B_{max}=32$, $T_{window}=20ms$**

Expected requests per window: $E[B] = 500 \times 0.02 = 10$

With 10 requests per batch:

- Inference time: approximately 12ms
- Per-request compute: 12ms / 10 = 1.2ms
- Maximum batch delay: 20ms
- Expected total latency: ~22ms mean, ~42ms P99

Utilization: $\rho = 500 \times 0.0012 = 0.6$ (comfortable)

**Tradeoff**: Option C achieves 25% better throughput (lower utilization) at the cost of higher average latency (22ms vs 15ms). Both meet the 50ms P99 SLO.

:::

At scale with multiple replicas, batch formation can occur either at individual replicas or at a centralized batching layer. Replica-local batching has each replica independently form batches from its assigned traffic. This approach is simpler to implement but may result in uneven batch sizes across replicas when load is imbalanced. Centralized batching uses a batching service to collect requests and dispatch formed batches to replicas. This achieves more uniform batch sizes but adds a centralization bottleneck and additional network hop.

Production systems typically use replica-local batching with load balancing that ensures roughly equal traffic distribution, achieving the benefits of centralized batching without the complexity.

### Continuous Batching for LLM Inference {#sec-inference-continuous-batching}

Autoregressive language models present a unique batching challenge that static and dynamic approaches handle poorly. The key insight comes from the Orca system[^fn-orca] [@yu2022orca]: traditional batching forces all sequences in a batch to complete before any new sequences can join, wasting compute when sequences finish at different times.

[^fn-orca]: **Orca**: Named after the killer whale known for highly coordinated group hunting, Orca pioneered iteration-level scheduling for LLM serving at Microsoft in 2022. The system's insight that sequences could enter and exit batches at each decode step, rather than waiting for entire batches to complete, transformed LLM serving economics.

Consider a batch of 8 sequences. If one sequence completes after 10 tokens while others require 100 tokens, the completed sequence's GPU resources sit idle for 90 iterations. With traditional batching:

$$\text{Wasted compute} = \frac{(100 - 10) \times 1}{100 \times 8} = 11.25\%$$

For realistic output length distributions with high variance, wasted compute can exceed 50%.

Continuous batching (also called iteration-level batching) decouples batch membership from iteration boundaries. At each decode iteration, the system checks for completed sequences, removes completed sequences from the batch immediately, inserts waiting sequences into freed slots, and processes the reorganized batch for the next iteration.

::: {.callout-note title="Archetype A (Scaled Lighthouse): The Throughput-Latency Decoupling"}
**Archetype A (The Trillion-Parameter LLM)** relies on continuous batching to solve its primary efficiency paradox. The decode phase is memory-bandwidth bound, meaning the GPU compute cores are idle waiting for weights to load. Continuous batching saturates this bandwidth by processing unrelated requests together. Without this technique, serving Archetype A models would be economically unviable due to low GPU utilization.
:::

### Continuous Batching Throughput Analysis {#sec-inference-memory-management}

Continuous batching's dynamic batch management maintains high GPU utilization regardless of sequence length variance. The throughput improvement depends on sequence length distribution. For a distribution with coefficient of variation $CV = \sigma / \mu$, the gain is approximately @eq-continuous-batching-gain:

$$\text{Throughput gain} \approx 1 + \frac{CV^2}{2}$$ {#eq-continuous-batching-gain}

With typical LLM output lengths having $CV \approx 1.0$, continuous batching achieves approximately 1.5x throughput improvement. For highly variable outputs (conversational vs. code generation), gains can reach 2–4x.

::: {.callout-note title="Implementation: Continuous Batching in vLLM"}

vLLM implements continuous batching with several key mechanisms. Iteration-level scheduling evaluates at each decode step which sequences have generated end-of-sequence tokens (remove from batch), which waiting sequences can fit in available KV cache slots (add to batch), and which sequences should be preempted if memory pressure exists (swap to CPU). Memory management uses PagedAttention (detailed in @sec-optimization-kv-cache), which enables dynamic allocation without fragmentation. When a sequence completes, its KV cache pages are immediately available for new sequences. The batched decode kernel processes all active sequences in a single batched operation despite dynamic batch composition. Sequences at different generation lengths are padded to a common shape within the kernel.

#### Preemption and Swapping

A critical challenge in continuous batching is memory contention. As sequences grow during generation, they consume more KV cache pages. If the GPU memory fills up, the system cannot simply crash; it must preempt running requests.

vLLM implements a virtual memory mechanism similar to an operating system's swap. When memory is exhausted, the scheduler identifies low-priority requests (e.g., those most recently started) and **swaps** their KV cache blocks from GPU HBM to CPU DRAM. These requests are paused until memory becomes available, at which point they are swapped back in and resumed. This mechanism ensures system stability under heavy load at the cost of increased latency for preempted requests.

**Typical performance (Llama-2 70B on 8xA100)**:

+----------------------------+---------------------------+---------------------+
| **Batching Strategy**      | **Throughput (tokens/s)** | **GPU Utilization** |
+:===========================+==========================:+====================:+
| **Static (batch=8)**       | 400                       | 45%                 |
| **Dynamic (timeout=50ms)** | 580                       | 65%                 |
| **Continuous**             | 1,200                     | 92%                 |
+----------------------------+---------------------------+---------------------+

The 3x throughput improvement from continuous batching comes from eliminating idle GPU cycles during sequence length variation.

:::

### Quantitative Analysis: Traditional vs Continuous Batching {#sec-inference-batching-quantitative}

The performance gap between traditional and continuous batching becomes quantifiable through careful analysis of wasted compute cycles. This section develops the mathematics of batching waste and demonstrates the improvement through a worked example with realistic LLM serving parameters.

#### The Waste Function for Traditional Batching

Traditional batching (also called static batching) processes all requests in a batch through all decode iterations until the longest sequence completes. For a batch of $n$ sequences with output lengths $\{L_1, L_2, ..., L_n\}$, the total compute performed is:

$$C_{traditional} = n \times L_{max} \times c_{decode}$$ {#eq-traditional-compute}

where $L_{max} = \max_i(L_i)$ and $c_{decode}$ is the compute cost per decode iteration per sequence. However, the useful compute is only:

$$C_{useful} = \sum_{i=1}^{n} L_i \times c_{decode}$$ {#eq-useful-compute}

The **waste ratio** quantifies the inefficiency:

$$W = 1 - \frac{C_{useful}}{C_{traditional}} = 1 - \frac{\sum_{i=1}^{n} L_i}{n \times L_{max}} = 1 - \frac{\bar{L}}{L_{max}}$$ {#eq-waste-ratio}

where $\bar{L}$ is the mean output length. This reveals that waste depends entirely on the ratio of mean to maximum output length within the batch. For uniform output lengths ($\bar{L} = L_{max}$), waste is zero. For highly variable lengths, waste can exceed 50%.

#### Worked Example: LLM Serving with Variable-Length Outputs

Consider a GPT-class model serving four concurrent requests with the following generation lengths (in tokens):

**Request characteristics**:

| Request | Prompt Length | Output Length | Total Tokens |
|:--------|:-------------:|:-------------:|:------------:|
| R1      | 100           | 50            | 150          |
| R2      | 80            | 200           | 280          |
| R3      | 120           | 100           | 220          |
| R4      | 90            | 150           | 240          |

**System parameters**:

- Decode time per iteration (batch of 4): 20 ms
- Maximum output length in batch: 200 tokens (R2)
- Mean output length: (50 + 200 + 100 + 150) / 4 = 125 tokens

**Traditional Batching Analysis**

With traditional batching, all four requests must wait for R2 to complete its 200 tokens:

- Total decode iterations: 200
- Total batch time: 200 × 20 ms = 4,000 ms
- Request completion times:
  - R1 completes useful work at iteration 50, but waits until iteration 200 → latency = 4,000 ms
  - R2 completes at iteration 200 → latency = 4,000 ms
  - R3 completes useful work at iteration 100, but waits until iteration 200 → latency = 4,000 ms
  - R4 completes useful work at iteration 150, but waits until iteration 200 → latency = 4,000 ms

Waste calculation using @eq-waste-ratio:

$$W = 1 - \frac{125}{200} = 1 - 0.625 = 37.5\%$$

The GPU performs 4 × 200 = 800 "sequence-iterations" but only 500 are useful.

**Continuous Batching Analysis**

With continuous batching, sequences depart the batch upon completion, and new requests can join:

- Iteration 50: R1 completes → slot freed, new request R5 can join
- Iteration 100: R3 completes → slot freed, new request R6 can join
- Iteration 150: R4 completes → slot freed, new request R7 can join
- Iteration 200: R2 completes

Request latencies with continuous batching (assuming no queuing delay):

- R1: 50 × 20 ms = 1,000 ms (4x improvement over traditional)
- R3: 100 × 20 ms = 2,000 ms (2x improvement)
- R4: 150 × 20 ms = 3,000 ms (1.33x improvement)
- R2: 200 × 20 ms = 4,000 ms (no improvement for longest request)

**Average latency comparison**:

- Traditional: 4,000 ms (all requests)
- Continuous: (1,000 + 2,000 + 3,000 + 4,000) / 4 = 2,500 ms

This represents a **37.5% reduction in average latency**, exactly matching the waste ratio.

::: {.callout-note title="Figure: Traditional vs Continuous Batching Timeline" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.65]
  \definecolor{R1Color}{RGB}{100,180,100}
  \definecolor{R2Color}{RGB}{100,100,200}
  \definecolor{R3Color}{RGB}{200,150,100}
  \definecolor{R4Color}{RGB}{180,100,180}
  \definecolor{WasteColor}{RGB}{220,220,220}
  \definecolor{NewColor}{RGB}{255,200,100}

  % Traditional Batching (top)
  \node[anchor=west, font=\bfseries] at (0, 6.5) {Traditional Batching};

  % Time axis
  \draw[->, thick] (0, 0.5) -- (12, 0.5);
  \node[anchor=west] at (12, 0.5) {Time};
  \foreach \x/\label in {0/0, 2.5/50, 5/100, 7.5/150, 10/200} {
    \draw (\x, 0.4) -- (\x, 0.6);
    \node[below, font=\scriptsize] at (\x, 0.4) {\label};
  }

  % R1 row
  \fill[R1Color] (0, 5.5) rectangle (2.5, 6);
  \fill[WasteColor] (2.5, 5.5) rectangle (10, 6);
  \node[anchor=west, font=\scriptsize] at (0.1, 5.75) {R1: 50 tokens};
  \node[font=\scriptsize, text=gray] at (6, 5.75) {Idle (waiting)};

  % R2 row
  \fill[R2Color] (0, 4.5) rectangle (10, 5);
  \node[anchor=west, font=\scriptsize, text=white] at (0.1, 4.75) {R2: 200 tokens};

  % R3 row
  \fill[R3Color] (0, 3.5) rectangle (5, 4);
  \fill[WasteColor] (5, 3.5) rectangle (10, 4);
  \node[anchor=west, font=\scriptsize] at (0.1, 3.75) {R3: 100 tokens};
  \node[font=\scriptsize, text=gray] at (7.5, 3.75) {Idle};

  % R4 row
  \fill[R4Color] (0, 2.5) rectangle (7.5, 3);
  \fill[WasteColor] (7.5, 2.5) rectangle (10, 3);
  \node[anchor=west, font=\scriptsize, text=white] at (0.1, 2.75) {R4: 150 tokens};
  \node[font=\scriptsize, text=gray] at (8.75, 2.75) {Idle};

  % Waste annotation
  \draw[<->, red, thick] (10.3, 2.5) -- (10.3, 6);
  \node[anchor=west, font=\scriptsize, text=red] at (10.5, 4.25) {37.5\% waste};

  % Continuous Batching (bottom, shifted down)
  \node[anchor=west, font=\bfseries] at (0, -0.5) {Continuous Batching};

  % Time axis for continuous
  \draw[->, thick] (0, -6.5) -- (12, -6.5);
  \node[anchor=west] at (12, -6.5) {Time};
  \foreach \x/\label in {0/0, 2.5/50, 5/100, 7.5/150, 10/200} {
    \draw (\x, -6.6) -- (\x, -6.4);
    \node[below, font=\scriptsize] at (\x, -6.6) {\label};
  }

  % R1 row (completes early, freed)
  \fill[R1Color] (0, -1.5) rectangle (2.5, -1);
  \draw[thick, green!50!black] (2.5, -1.25) circle (0.15);
  \node[font=\tiny, green!50!black] at (2.5, -1.25) {\checkmark};
  \node[anchor=west, font=\scriptsize] at (0.1, -1.25) {R1};

  % R5 joins in R1's slot
  \fill[NewColor] (2.5, -1.5) rectangle (7, -1);
  \node[anchor=west, font=\scriptsize] at (2.6, -1.25) {R5 (new)};

  % R2 row (longest, no change)
  \fill[R2Color] (0, -2.5) rectangle (10, -2);
  \node[anchor=west, font=\scriptsize, text=white] at (0.1, -2.25) {R2: 200 tokens};

  % R3 row (completes at 100)
  \fill[R3Color] (0, -3.5) rectangle (5, -3);
  \draw[thick, green!50!black] (5, -3.25) circle (0.15);
  \node[font=\tiny, green!50!black] at (5, -3.25) {\checkmark};
  \node[anchor=west, font=\scriptsize] at (0.1, -3.25) {R3};

  % R6 joins in R3's slot
  \fill[NewColor] (5, -3.5) rectangle (9, -3);
  \node[anchor=west, font=\scriptsize] at (5.1, -3.25) {R6 (new)};

  % R4 row (completes at 150)
  \fill[R4Color] (0, -4.5) rectangle (7.5, -4);
  \draw[thick, green!50!black] (7.5, -4.25) circle (0.15);
  \node[font=\tiny, green!50!black] at (7.5, -4.25) {\checkmark};
  \node[anchor=west, font=\scriptsize, text=white] at (0.1, -4.25) {R4};

  % R7 joins in R4's slot
  \fill[NewColor] (7.5, -4.5) rectangle (10, -4);
  \node[anchor=west, font=\scriptsize] at (7.6, -4.25) {R7};

  % Benefit annotation
  \draw[<->, green!50!black, thick] (10.3, -4.5) -- (10.3, -1);
  \node[anchor=west, font=\scriptsize, text=green!50!black] at (10.5, -2.75) {0\% waste};
  \node[anchor=west, font=\scriptsize, text=green!50!black] at (10.5, -3.25) {+3 requests};

\end{tikzpicture}
```
**Traditional vs Continuous Batching**. Top: Traditional batching wastes 37.5% of GPU cycles as completed requests (R1, R3, R4) wait idle for the longest request (R2). Bottom: Continuous batching immediately frees slots upon completion, allowing new requests (R5, R6, R7) to join. This eliminates waste and increases effective throughput.
:::

#### When Continuous Batching Provides Maximum Benefit

The analysis above reveals that continuous batching's benefit scales with output length variance. We can formalize this relationship:

**Benefit formula**: Let $CV = \sigma / \mu$ be the coefficient of variation of output lengths. The throughput improvement from continuous batching over traditional batching is:

$$\text{Improvement} \approx \frac{L_{max}}{\bar{L}} = \frac{\mu + k\sigma}{\mu} = 1 + k \cdot CV$$ {#eq-continuous-benefit}

where $k$ is the number of standard deviations the maximum output exceeds the mean (typically 2-3 for realistic distributions).

@tbl-continuous-batching-benefit quantifies this relationship across different workload types:

+-------------------------+--------+---------+-------------------+--------------------------+
| **Workload Type**       | **CV** | **k**   | **Waste (Trad.)** | **Speedup (Continuous)** |
+:========================+:======:+:=======:+:=================:+:========================:+
| Code completion         | 0.3    | 2.5     | 18%               | 1.2x                     |
| Chat (short responses)  | 0.6    | 2.0     | 33%               | 1.5x                     |
| General text generation | 1.0    | 2.5     | 50%               | 2.0x                     |
| Creative writing        | 1.5    | 3.0     | 64%               | 2.8x                     |
| RAG with variable docs  | 2.0    | 2.5     | 71%               | 3.5x                     |
+-------------------------+--------+---------+-------------------+--------------------------+

: **Continuous Batching Benefit by Workload Type**. Higher output length variance (CV) yields greater improvement from continuous batching. Workloads with predictable output lengths (code completion) see modest gains, while highly variable workloads (RAG with documents of varying length) see dramatic improvement. {#tbl-continuous-batching-benefit}

**Key insight**: Continuous batching is most valuable when:

1. **Output lengths are unpredictable**: User queries that may elicit responses from 10 to 1,000 tokens
2. **Mixed workload types**: Combining summarization (short) with generation (long) on the same cluster
3. **High request volume**: More opportunities to fill vacated slots with waiting requests
4. **Tight latency SLOs**: Short requests benefit most from early completion

Conversely, continuous batching provides minimal benefit when:

1. **Output lengths are uniform**: Fixed-length tasks like classification or embedding generation
2. **Batch sizes are small**: Few opportunities for slot reuse within a batch
3. **Request volume is low**: Vacated slots sit empty waiting for new requests

#### Implementation Complexity Trade-offs

Continuous batching's performance benefits come with implementation complexity that systems engineers must weigh:

**Memory management complexity**: Traditional batching allocates a fixed KV cache region per sequence at batch formation, deallocating only when the entire batch completes. Continuous batching requires dynamic allocation as sequences grow and immediate deallocation upon completion, necessitating sophisticated memory management akin to operating system virtual memory.

**Scheduler complexity**: Traditional batching uses simple FIFO scheduling: collect requests until batch is full or timeout expires, then execute. Continuous batching requires per-iteration decision making about which sequences to admit, which to preempt if memory pressure exists, and how to handle priority classes. This increases scheduler overhead from O(1) per batch to O(n) per iteration.

**Kernel design**: Batched GPU kernels traditionally assume fixed batch composition. Continuous batching requires kernels that handle variable-length sequences efficiently, often through techniques like packing multiple short sequences into shared attention masks or using specialized memory layouts that support dynamic batch membership.

@tbl-batching-tradeoffs summarizes these trade-offs:

+---------------------------+---------------------------+----------------------------------+
| **Dimension**             | **Traditional Batching**  | **Continuous Batching**          |
+:==========================+:==========================+:=================================+
| **Implementation effort** | Low (standard frameworks) | High (custom scheduler, kernels) |
| **Memory overhead**       | Fixed allocation          | Dynamic + fragmentation mgmt     |
| **Scheduler latency**     | ~0.1 ms per batch         | ~0.5-1 ms per iteration          |
| **Debugging complexity**  | Deterministic behavior    | State-dependent, harder to trace |
| **Throughput (variable)** | Baseline                  | 1.5-3.5x improvement             |
| **Throughput (uniform)**  | Baseline                  | ~1.0x (no improvement)           |
+---------------------------+---------------------------+----------------------------------+

: **Traditional vs Continuous Batching Trade-offs**. Continuous batching provides significant throughput gains for variable-length workloads at the cost of implementation complexity. For uniform-length workloads, the complexity overhead may not justify adoption. {#tbl-batching-tradeoffs}

**Practical recommendation**: For new LLM serving deployments, continuous batching frameworks like vLLM, TensorRT-LLM, or TGI provide the implementation complexity as a solved problem. The decision becomes whether to adopt these frameworks versus building custom serving infrastructure. For organizations with existing traditional batching systems, the migration cost must be weighed against the workload's output length variance using @tbl-continuous-batching-benefit.

::: {.callout-tip title="Systems Sandwich: Debugging High P99 Latency"}

**Problem**: An LLM serving system exhibits unexpectedly high tail latency. P50 latency is 100ms and P95 is 180ms, both within SLO, but P99 spikes to 500ms against a 200ms target. GPU utilization appears healthy at 85%. Where is the bottleneck?

**Physical Layer Analysis** (Hardware Constraints):

The system runs on 4 A100-80GB GPUs connected via PCIe Gen4 (64 GB/s per GPU) rather than NVLink. The server has a dual-socket CPU with NUMA topology. Memory bandwidth per GPU is 2 TB/s (HBM2e), adequate for decode operations. However, PCIe bandwidth limits tensor parallelism communication to 64 GB/s versus NVLink's 600 GB/s. For a batch requiring 100MB activation transfers between GPUs, PCIe adds approximately 1.5ms per synchronization point versus 0.17ms for NVLink.

**Operational Layer Analysis** (Algorithmic Behavior):

Dynamic batching is configured with max batch size 32 and timeout 50ms. Examining the batch size distribution reveals the problem: 90% of batches contain 4 to 8 requests (explaining good P50/P95), but 5% of batches reach the full 32 requests. These large batches occur during traffic bursts and experience head-of-line blocking: short requests that would complete quickly must wait for long-sequence requests in the same batch to finish all their decode iterations.

With continuous batching enabled, why does this occur? Further investigation reveals the scheduler uses FIFO ordering without preemption, meaning a burst of 32 simultaneous arrivals all enter the same batch and none can exit early because they all started at the same iteration.

**Diagnosis**:

The root cause is not the hardware (Physical Layer) but the scheduling policy (Operational Layer). Head-of-line blocking occurs when the scheduler forms large batches from bursty arrivals without considering request heterogeneity. The 500ms P99 corresponds to large batches where short requests wait for long-sequence completions.

**Solution**:

Implement priority-aware scheduling with separate batch size limits by request type:

1. **Request classification**: Tag requests by expected output length (short: under 50 tokens, medium: 50 to 200 tokens, long: over 200 tokens) based on prompt patterns or user-provided hints
2. **Differentiated batching**: Limit short-request batches to 8, medium to 16, long to 32
3. **Priority preemption**: Allow short requests to preempt long-running sequences when P99 approaches SLO

After implementing these changes, P99 dropped to 185ms. The Physical Layer was adequate; the problem was entirely in the Operational Layer's scheduling logic.

**Lesson**: When tail latency exceeds expectations despite healthy utilization, examine the batching and scheduling policies. High average GPU utilization can mask head-of-line blocking that only affects a small percentage of requests but drives P99. The Systems Sandwich methodology systematically isolates whether bottlenecks arise from hardware constraints or algorithmic decisions. See @sec-vol2-introduction for the complete Systems Sandwich framework.

:::

### Prefill vs Decode Imbalance {#sec-inference-prefill-decode}

LLM inference consists of two distinct phases: prefill (compute-bound) and decode (memory-bandwidth bound). This dichotomy creates scheduling challenges when mixing requests. While simple batching can lead to interference, advanced architectures like **Disaggregated Serving** separate these phases onto specialized hardware pools. These architectural optimizations are detailed in @sec-disaggregated-serving.

While LLM batching dominates current discussions, the majority of production inference volume comes from a different model type entirely: recommendation systems.

### Feature-Parallel Batching for Recommendation Systems {#sec-inference-feature-parallel-batching}

Recommendation systems have distinct batching requirements from vision or language models. The computation pattern involves:

1. **Sparse feature lookup**: Retrieve embeddings for user, item, and context features
2. **Dense feature processing**: Transform and normalize dense features
3. **Feature interaction**: Compute interactions between features (often via attention or factorization)
4. **Ranking head**: Produce final scores

The sparse embedding lookup often dominates latency and determines batching strategy.

**Feature-parallel batching** processes different feature types in parallel rather than batching entire requests:

```
Request 1: [user_id_1, item_ids_1, context_1]
Request 2: [user_id_2, item_ids_2, context_2]
Request 3: [user_id_3, item_ids_3, context_3]

Feature-parallel view:
User embeddings:  [lookup(user_1), lookup(user_2), lookup(user_3)]  → parallel
Item embeddings:  [lookup(items_1), lookup(items_2), lookup(items_3)]  → parallel
Context features: [process(ctx_1), process(ctx_2), process(ctx_3)]  → parallel

Then: Combine features per request for ranking
```

This parallelization is natural when embeddings are sharded across servers: each embedding server handles lookups for its shard across all requests in the batch.

::: {.callout-note title="Worked Example: Recommendation System Batching at Meta Scale"}

Consider Meta's recommendation infrastructure serving 10 million QPS across the platform:

**Request characteristics**:

- Each request queries ~100 items (candidate ranking)
- Each item requires 50 embedding lookups (user features, item features, cross features)
- Total embedding lookups: 5,000 per request
- Embedding table size: 100TB across 1,000 shards

**Batching strategy**:

With 10M QPS and 1,000 embedding shards, each shard receives:

$$\text{Lookups per shard} = \frac{10M \times 5000}{1000} = 50 \text{ billion lookups/sec}$$

This is clearly infeasible for single-threaded processing. Instead:

**Batch accumulation window**: 1ms
**Requests per batch**: 10,000 (at 10M QPS)
**Lookups per shard per batch**: 50M

Each embedding shard processes 50M lookups in a batched operation, achieving memory bandwidth utilization of 90%+ through sequential memory access patterns.

**Latency breakdown**:

+------------------------+--------------+-----------------------------+
| **Phase**              | **Duration** | **Notes**                   |
+:=======================+=============:+:============================+
| **Request routing**    | 0.2ms        | Consistent hashing to shard |
| **Batch accumulation** | 0.5ms (avg)  | 1ms window                  |
| **Embedding lookup**   | 2ms          | Batched, SSD-backed         |
| **Feature processing** | 1ms          | Dense computation           |
| **Ranking model**      | 1.5ms        | Final scoring               |
| **Total**              | **5.2ms**    | Within 10ms SLO             |
+------------------------+--------------+-----------------------------+

:::

### Streaming Inference for Real-Time Applications {#sec-inference-streaming}

Some applications cannot tolerate batching delay of any kind. Real-time speech recognition, video analysis, and robotics require processing inputs as they arrive with minimal latency.

**Streaming inference** processes inputs incrementally without waiting for batch formation:

- **Speech**: Process audio frames (10-20ms chunks) as they arrive from the microphone
- **Video**: Process frames at capture rate (30-60 FPS) without buffering
- **Robotics**: Process sensor readings at control loop frequency (100-1000 Hz)

For streaming applications, the relevant metric is not throughput but **time to process each input**:

$$L_{streaming} = L_{capture} + L_{transfer} + L_{inference} + L_{action}$$

where all components must complete within the inter-frame interval.

::: {.callout-note title="Streaming Speech Recognition Pipeline"}

Consider a streaming speech-to-text system with 20ms audio frames:

**Latency budget**: 100ms end-to-end (5 frames of delay)

**Pipeline stages**:

+------------------------+------------------+-----------------------------+
| **Stage**              | **Duration**     | **Notes**                   |
+:=======================+=================:+:============================+
| **Audio capture**      | 0ms (continuous) | Microphone buffer           |
| **Network to server**  | 20ms             | Including jitter buffer     |
| **Feature extraction** | 5ms              | MFCC computation            |
| **Encoder inference**  | 30ms             | Streaming Conformer         |
| **Decoder step**       | 15ms             | Autoregressive CTC          |
| **Text formatting**    | 5ms              | Capitalization, punctuation |
| **Network to client**  | 15ms             | Response transmission       |
| **Total**              | **90ms**         | Within 100ms budget         |
+------------------------+------------------+-----------------------------+

**Key constraints**:

- No batching: Each frame processes individually
- Stateful model: Encoder maintains context across frames
- Pipeline parallelism: While frame N is in decoder, frame N+1 is in encoder

GPU utilization is typically 30-50% for streaming workloads, traded for latency guarantee.

:::

### Adaptive Batching Strategies {#sec-inference-adaptive-batching}

Production systems rarely use fixed batching parameters. Instead, they adapt batching behavior based on current conditions:

**Traffic-adaptive batching** adjusts batch window based on arrival rate:

$$T_{window} = \min\left(T_{max}, \frac{B_{target}}{\lambda_{current}}\right)$$

When traffic is high, the window shrinks because the target batch size fills quickly. When traffic is low, the window extends but is capped to bound maximum latency.

**SLO-adaptive batching** monitors latency percentiles and adjusts batching aggressively:

```
if P99_latency > 0.9 * SLO:
    reduce B_max by 20%
    reduce T_window by 20%
elif P99_latency < 0.5 * SLO:
    increase B_max by 10%
    increase T_window by 10%
```

This feedback loop maintains latency headroom while maximizing throughput during normal operation.

**Request-aware batching** considers request characteristics when forming batches. For LLMs:

- Group requests by expected output length (inferred from prompt type)
- Group requests by prompt length to minimize padding
- Prioritize latency-sensitive requests in smaller batches

::: {.callout-note title="Production Adaptive Batching: The NVIDIA Triton Approach"}

Triton Inference Server implements adaptive batching with three configurable parameters:

1. **max_batch_size**: Upper bound on batch size
2. **batching_timeout_ms**: Maximum time to wait for batch formation
3. **preferred_batch_size**: Target batch sizes that align with kernel efficiency

The scheduler maintains separate queues for each preferred batch size and routes requests to minimize total latency:

$$\text{Queue selection} = \arg\min_{q} \left( \text{wait}_q + \text{exec}(|q| + 1) \right)$$

This optimization considers both the current queue length and the efficiency of the resulting batch size.

**Observed behavior on ResNet-50 (V100)**:

+-------------------+--------------------+-----------------+----------------+
| **Traffic Level** | **Avg Batch Size** | **Avg Latency** | **Throughput** |
+==================:+===================:+================:+===============:+
| **100 QPS**       | 2.1                | 8ms             | 100 QPS        |
| **500 QPS**       | 6.3                | 12ms            | 500 QPS        |
| **1000 QPS**      | 12.4               | 18ms            | 1000 QPS       |
| **2000 QPS**      | 24.1               | 28ms            | 1980 QPS       |
+-------------------+--------------------+-----------------+----------------+

The system automatically increases batch size to maintain throughput as traffic grows.

:::

### Quantitative Summary: Batching Strategy Selection {#sec-inference-batching-summary}

The choice of batching strategy depends on model characteristics, traffic patterns, and latency requirements. @fig-inference-lifecycle visualizes the end-to-end request path, highlighting latency sources at each stage. The following decision framework guides selection:

::: {#fig-inference-lifecycle fig-env="figure" fig-pos="htb" fig-cap="**End-to-End Inference Pipeline**. A high-level view of the request lifecycle: Client -> Load Balancer -> Request Queue -> Batch Scheduler -> Model Execution -> Response. This visualization highlights the critical \"Serving Tax\" components (serialization, routing, coordination) that consume latency budget outside of the actual GPU compute time." fig-alt="Flowchart with 5 stages: Client, Load Balancer, Request Queue, Dynamic Batcher, Model Execution. Arrows show request flow with dashed return path. Red annotations below highlight latency sources at each stage."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm]
  \definecolor{ProcessColor}{RGB}{240,248,255}
  \definecolor{NetworkColor}{RGB}{255,245,238}
  \definecolor{QueueColor}{RGB}{245,245,245}

  \tikzset{
    process/.style={draw=black!70, thick, fill=ProcessColor, rounded corners=2pt, minimum width=2.5cm, minimum height=1.0cm, align=center},
    queue/.style={draw=black!70, thick, fill=QueueColor, shape=cylinder, shape border rotate=90, aspect=0.25, minimum width=1.5cm, minimum height=1.2cm, align=center},
    arrow/.style={->, >=stealth, thick, color=black!80}
  }

  % Nodes
  \node[process, fill=white] (client) {Client\\Application};
  \node[process, fill=NetworkColor, right=of client] (lb) {Load\\Balancer};
  \node[queue, right=of lb] (queue) {Request\\Queue};
  \node[process, right=of queue] (batcher) {Dynamic\\Batcher};
  \node[process, right=of batcher, fill=ProcessColor!80!blue] (gpu) {Model\\Execution (GPU)};

  % Edges
  \draw[arrow] (client) -- node[above, font=\scriptsize] {Network} (lb);
  \draw[arrow] (lb) -- node[above, font=\scriptsize] {Routing} (queue);
  \draw[arrow] (queue) -- node[above, font=\scriptsize] {Wait} (batcher);
  \draw[arrow] (batcher) -- node[above, font=\scriptsize] {Bus} (gpu);

  % Return path
  \draw[arrow, dashed] (gpu.south) -- +(0,-0.5) -| node[near start, below, font=\scriptsize] {Response Path} (client.south);

  % Latency Sources
  \node[below=0.8cm of lb, font=\scriptsize, align=center, color=red!70!black] {Parsing \&\\Routing Latency};
  \node[below=0.8cm of queue, font=\scriptsize, align=center, color=red!70!black] {Queuing\\Delay};
  \node[below=0.8cm of batcher, font=\scriptsize, align=center, color=red!70!black] {Batch\\Sync Time};
  \node[below=0.8cm of gpu, font=\scriptsize, align=center, color=red!70!black] {Compute\\Latency};

\end{tikzpicture}
```
:::

```
Is the model autoregressive (LLM, speech)?
├─ Yes → Continuous batching with prefill chunking
└─ No → Does the model have embedding lookups dominating latency?
        ├─ Yes → Feature-parallel batching (RecSys)
        └─ No → Dynamic batching with adaptive parameters
```

@tbl-batching-parameters summarizes the key tuning parameters for each strategy:

+----------------------+-----------------------+----------------------------------+
| **Strategy**         | **Key Parameters**    | **Tuning Goal**                  |
+:=====================+:======================+:=================================+
| **Static**           | Batch size            | Maximize throughput              |
| **Dynamic**          | Window, max batch     | Balance latency vs throughput    |
| **Continuous**       | Chunk size, max batch | Minimize decode latency variance |
| **Feature-parallel** | Accumulation window   | Match embedding shard capacity   |
| **Streaming**        | Pipeline depth        | Meet real-time deadline          |
+----------------------+-----------------------+----------------------------------+

: **Batching Strategy Parameters**. Each strategy has distinct parameters requiring tuning for the specific deployment. {#tbl-batching-parameters}

## Model Sharding for Inference {#sec-inference-sharding}

Batching strategies from the previous section address how to efficiently process requests on a given model replica. But what constitutes a "replica" when models exceed single-GPU memory? And how can we reduce latency when even batch-size-one inference is too slow? Model sharding answers both questions by distributing inference across multiple devices.

The parallelism strategies established for distributed training in @sec-distributed-training, including tensor parallelism, pipeline parallelism, and their communication patterns, apply to inference with an important inversion: training optimizes throughput over hours while inference must minimize per-request latency within milliseconds. This changes how we configure sharding strategies. Unlike training sharding where throughput is the primary concern, inference sharding must carefully balance parallelization benefits against communication overhead within strict latency budgets. This moves us from request-level optimization to replica-level concerns in the serving hierarchy.

This section examines four sharding strategies, each suited to different model architectures and deployment requirements: tensor parallelism for attention-heavy models, pipeline parallelism for sequential architectures, expert parallelism for mixture-of-experts models, and embedding sharding for recommendation systems.

### When Sharding Becomes Necessary {#sec-inference-sharding-when}

Model sharding for inference is driven by two distinct requirements. @tbl-sharding-triggers identifies the memory and latency constraints that necessitate sharding:

**Memory requirements**: A model that cannot fit in single-GPU memory must be sharded regardless of performance considerations. For a model with $P$ parameters at precision $b$ bits, the weight memory is calculated by @eq-weight-memory:

$$\text{Memory}_{weights} = P \times \frac{b}{8} \text{ bytes}$$ {#eq-weight-memory}

A 70-billion parameter model in FP16 (16 bits) requires:

$$\text{Memory} = 70 \times 10^9 \times \frac{16}{8} = 140 \text{ GB}$$

This exceeds the 80GB capacity of an H100 GPU, requiring at minimum 2-way sharding.

**Latency requirements**: Even when a model fits in memory, sharding can reduce latency by parallelizing computation. @eq-parallel-time formalizes the potential speedup as a function of parallelization efficiency:

$$T_{parallel} = \frac{T_{sequential}}{P} + T_{communication}$$ {#eq-parallel-time}

where $P$ is the parallelism degree and $T_{communication}$ is the synchronization overhead. Sharding provides latency benefit only when the communication overhead is smaller than the time saved through parallelization.

+-------------------------+----------------------+----------------------+--------------------+
| **Sharding Trigger**    | **Model Examples**   | **Minimum Sharding** | **Strategy**       |
+:========================+=====================:+=====================:+:===================+
| **Memory (weights)**    | Llama-70B (140GB)    | 2-way                | Tensor or pipeline |
| **Memory (KV cache)**   | GPT-4 (long context) | 4–8 way              | Tensor (for cache) |
| **Memory (embeddings)** | DLRM (100TB)         | 1000+ way            | Embedding sharding |
| **Latency**             | Any large model      | Varies               | Tensor parallelism |
+-------------------------+----------------------+----------------------+--------------------+

: **Sharding Triggers**. Different constraints lead to different sharding requirements and strategies. {#tbl-sharding-triggers}

### Tensor Parallelism {#sec-inference-tensor-parallelism}

Tensor parallelism [@shoeybi2019megatron] distributes individual layers across multiple devices, enabling parallel computation within each layer. The column-row partitioning scheme introduced for training in @sec-distributed-training applies here: splitting the first linear layer by columns and the second by rows requires only one AllReduce per transformer block. For transformer models, the primary target is the attention mechanism and feed-forward layers, which contain the majority of computation.

**Attention layer parallelism**: The multi-head attention computation naturally partitions across attention heads. For a model with $H$ attention heads distributed across $P$ devices, each device computes $H/P$ heads:

$$\text{Attention}_i = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i \text{ for heads } i \in \{1, ..., H/P\}$$

After computing local attention, an AllReduce operation (covered in @sec-communication) combines results across devices, adding communication overhead proportional to the activation size divided by the interconnect bandwidth.

**Feed-forward layer parallelism**: The feed-forward layer (typically two linear transformations with activation) partitions along the hidden dimension. For the first linear layer, columns are distributed; for the second, rows are distributed. This column-row partitioning requires only one all-reduce per feed-forward block.

The communication pattern for tensor-parallel inference follows a two-phase synchronization per layer. @fig-tensor-parallel-flow illustrates this flow:

::: {#fig-tensor-parallel-flow fig-env="figure" fig-pos="htb" fig-cap="**Tensor Parallelism for Inference**. Computation is distributed across devices by splitting tensor operations. Attention heads are partitioned across GPUs, requiring an AllReduce operation to synchronize results. Feed-forward networks use a column-row splitting strategy that requires only one AllReduce synchronization per block. This approach reduces latency for large models but introduces communication overhead that demands high-bandwidth interconnects like NVLink." fig-alt="Two-column diagram for GPU 0 and GPU 1. Top: Attention layer with heads split across GPUs feeding into AllReduce. Bottom: Feed-forward layer showing column-row split pattern with final AllReduce synchronization."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{GPUColor}{RGB}{230,230,250}
  \definecolor{OpColor}{RGB}{240,248,255}
  \definecolor{SyncColor}{RGB}{255,228,225}

  % GPU Columns
  \draw[fil=GPUColor, draw=none] (-2, -0.5) rectangle (1.5, 6);
  \node[above, font=\bfseries] at (-0.25, 6) {GPU 0};

  \draw[fill=GPUColor, draw=none] (2.5, -0.5) rectangle (6, 6);
  \node[above, font=\bfseries] at (4.25, 6) {GPU 1};

  % Attention Block
  \node[anchor=center] at (2, 5.5) {\textbf{Attention Layer}};

  \node[draw, fill=OpColor, minimum width=2.5cm] (att0) at (-0.25, 4.5) {Heads 1..H/2};
  \node[draw, fill=OpColor, minimum width=2.5cm] (att1) at (4.25, 4.5) {Heads H/2..H};

  \node[draw, fill=SyncColor, rounded corners, minimum width=6cm] (allreduce1) at (2, 3.5) {\textbf{AllReduce} (Combine Heads)};

  \draw[->, thick] (att0) -- (allreduce1.north -| att0);
  \draw[->, thick] (att1) -- (allreduce1.north -| att1);

  % MLP Block
  \node[anchor=center] at (2, 2.5) {\textbf{Feed-Forward (MLP)}};

  % Col Split
  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp1a) at (-0.9, 1.5) {Col};
  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp1b) at (0.4, 1.5) {Col};
  \node at (-0.25, 1.5) {...}; % Visualizing big matrix split

  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp2a) at (3.6, 1.5) {Col};
  \node[draw, fill=OpColor, minimum width=1.2cm] (mlp2b) at (4.9, 1.5) {Col};
  \node at (4.25, 1.5) {...};

  % Row Split
  \node[draw, fill=OpColor, minimum width=2.5cm] (mlp1_out) at (-0.25, 0.5) {Row Split};
  \node[draw, fill=OpColor, minimum width=2.5cm] (mlp2_out) at (4.25, 0.5) {Row Split};

  \draw[->, thick] (allreduce1.south -| att0) -- (mlp1a.north); % Simplified connection
  \draw[->, thick] (allreduce1.south -| att1) -- (mlp2a.north);

  \draw[->] (mlp1a.south) -- (mlp1_out.north);
  \draw[->] (mlp2a.south) -- (mlp2_out.north);

  \node[draw, fill=SyncColor, rounded corners, minimum width=6cm] (allreduce2) at (2, -0.5) {\textbf{AllReduce} (Sum Outputs)};

  \draw[->, thick] (mlp1_out.south) -- (allreduce2.north -| mlp1_out);
  \draw[->, thick] (mlp2_out.south) -- (allreduce2.north -| mlp2_out);

\end{tikzpicture}
```
:::

The inference time with tensor parallelism follows @eq-tensor-parallel-time:

$$T_{inference} = \frac{T_{compute}}{P} + 2 \times T_{allreduce}\left(\frac{A}{P}\right)$$ {#eq-tensor-parallel-time}

where $T_{compute}$ is the sequential compute time, $P$ is the parallelism degree, and $A$ is the activation size being reduced. The factor of 2 accounts for the two all-reduce operations per transformer layer (attention and feed-forward).

::: {.callout-note title="Worked Example: Tensor Parallelism for Llama-70B"}

Consider serving Llama-70B with the following configuration:

**Model specifications**:

- Parameters: 70 billion
- Hidden dimension: 8,192
- Attention heads: 64
- Layers: 80

**Memory per GPU** (weight only, FP16):

$$\text{Memory}_{70B} = 70 \times 10^9 \times 2 = 140\text{ GB}$$

**Minimum sharding**: 2-way (140GB / 80GB per H100)

**Recommended sharding**: 8-way for optimal latency

**With 8-way tensor parallelism on 8xH100 (NVLink interconnect)**:

+---------------------------+------------------------+--------------+-------------+
| **Component**             | **Sequential (1 GPU)** | **8-way TP** | **Speedup** |
+:==========================+=======================:+=============:+:============+
| **Attention compute**     | 12ms                   | 1.5ms        | 8x          |
| **AllReduce (attention)** | 0ms                    | 0.3ms        | N/A         |
| **Feed-forward compute**  | 18ms                   | 2.25ms       | 8x          |
| **AllReduce (FF)**        | 0ms                    | 0.3ms        | N/A         |
| **Total per layer**       | **30ms**               | **4.35ms**   | **6.9x**    |
+---------------------------+------------------------+--------------+-------------+

**For 80 layers**:

- Sequential: 2,400ms per token
- 8-way TP: 348ms per token

The 6.9x speedup (vs theoretical 8x) reflects communication overhead. With 600 GB/s NVLink bandwidth, each 8MB activation all-reduce takes ~0.3ms.

**Time-to-first-token** (1024-token prompt):

- Prefill compute: ~50ms (compute-bound, near-linear scaling)
- Total TTFT: ~60ms with preprocessing

:::

### Pipeline Parallelism for Inference {#sec-inference-pipeline-parallelism}

Pipeline parallelism distributes layers across devices sequentially, with each device handling a subset of layers. Unlike tensor parallelism, there is no synchronization within a layer, only between pipeline stages.

For inference, pipeline parallelism creates bubbles differently than in training. @fig-pipeline-bubbles contrasts single-request latency (bubble-dominated) with pipelined throughput (bubble-amortized):

::: {#fig-pipeline-bubbles fig-env="figure" fig-pos="htb" fig-cap="**Pipeline Parallelism Bubbles**. For a single inference request, pipeline parallelism offers no latency benefit as the request must traverse all stages sequentially (top). However, when processing multiple concurrent requests, pipeline bubble utilization improves significantly (bottom), allowing throughput to scale with the number of stages. This makes pipeline parallelism ideal for high-throughput batch processing but less suitable for latency-critical interactive serving." fig-alt="Two timeline diagrams. Top: Single request across 3 GPUs showing sequential processing with idle bubbles. Bottom: Pipelined batch with 4 requests overlapping across stages, minimizing idle time."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{Stage1}{RGB}{173,216,230}
  \definecolor{Stage2}{RGB}{144,238,144}
  \definecolor{Stage3}{RGB}{255,255,224}
  \definecolor{Bubble}{RGB}{240,240,240}

  \tikzset{
    block/.style={draw=black!50, minimum height=0.6cm, minimum width=1.4cm, font=\scriptsize},
    label/.style={font=\footnotesize\bfseries}
  }

  % Scenario A: Single Request (Sequential)
  \node[label, anchor=west] at (0, 3.5) {A. Single Request (Latency Bound)};

  % Dev 0
  \node[anchor=east] at (0, 2.8) {GPU 0};
  \node[block, fill=Stage1] at (0.8, 2.8) {Req1};
  \node[block, fill=Bubble, minimum width=2.8cm] at (3.0, 2.8) {Idle};

  % Dev 1
  \node[anchor=east] at (0, 2.0) {GPU 1};
  \node[block, fill=Bubble, minimum width=1.4cm] at (0.8, 2.0) {Idle};
  \node[block, fill=Stage2] at (2.3, 2.0) {Req1};
  \node[block, fill=Bubble, minimum width=1.4cm] at (3.7, 2.0) {Idle};

  % Dev 2
  \node[anchor=east] at (0, 1.2) {GPU 2};
  \node[block, fill=Bubble, minimum width=2.8cm] at (1.5, 1.2) {Idle};
  \node[block, fill=Stage3] at (3.7, 1.2) {Req1};

  % Total Time Arrow
  \draw[<->] (0.1, 0.8) -- (4.4, 0.8) node[midway, below, font=\scriptsize] {Total Latency = Sum of Stages};


  % Scenario B: Pipelined (Throughput)
  \node[label, anchor=west] at (0, -0.5) {B. Pipelined Batch (Throughput)};

  % Shift x origin for alignment
  \begin{scope}[shift={(0, -3)}]
      % Dev 0
      \node[anchor=east] at (0, 2.0) {GPU 0};
      \node[block, fill=Stage1] at (0.8, 2.0) {Req1};
      \node[block, fill=Stage1] at (2.3, 2.0) {Req2};
      \node[block, fill=Stage1] at (3.8, 2.0) {Req3};

      % Dev 1
      \node[anchor=east] at (0, 1.2) {GPU 1};
      \node[block, fill=Bubble] at (0.8, 1.2) {Idle};
      \node[block, fill=Stage2] at (2.3, 1.2) {Req1};
      \node[block, fill=Stage2] at (3.8, 1.2) {Req2};
      \node[block, fill=Stage2] at (5.3, 1.2) {Req3};

      % Dev 2
      \node[anchor=east] at (0, 0.4) {GPU 2};
      \node[block, fill=Bubble, minimum width=2.8cm] at (1.5, 0.4) {Idle};
      \node[block, fill=Stage3] at (3.8, 0.4) {Req1};
      \node[block, fill=Stage3] at (5.3, 0.4) {Req2};

      % Note
      \node[anchor=west, font=\scriptsize, align=left] at (6.5, 1.2) {High Throughput\\after fill};
  \end{scope}

\end{tikzpicture}
```
:::

For a single request, pipeline parallelism provides no latency benefit: the request must traverse all stages sequentially. The pipeline fill time equals the sequential execution time.

However, pipeline parallelism enables **throughput scaling** through pipelining multiple requests:

```
Time →
Device 0: [Req1] [Req2] [Req3] [Req4] ...
Device 1:        [Req1] [Req2] [Req3] [Req4] ...
Device 2:               [Req1] [Req2] [Req3] [Req4] ...
Device 3:                      [Req1] [Req2] [Req3] [Req4] ...
```

Once the pipeline is full, throughput equals $P$ times single-stage throughput, where $P$ is the number of pipeline stages. The steady-state latency remains approximately the single-device latency (sum of all stage times), but throughput scales with parallelism.

**When to use pipeline parallelism for inference**:

- When memory constraints require sharding but latency requirements are relaxed
- When throughput is more important than individual request latency
- When network bandwidth between devices is limited (only point-to-point communication)

@tbl-pipeline-tensor-comparison captures the tradeoffs between these two sharding approaches:

+----------------------------+----------------------------------+------------------------------------+
| **Aspect**                 | **Tensor Parallelism**           | **Pipeline Parallelism**           |
+:===========================+:=================================+:===================================+
| **Single-request latency** | Reduced by ~$P$x                 | No improvement                     |
| **Throughput**             | $P$x                             | $P$x (when pipelined)              |
| **Communication pattern**  | AllReduce (bandwidth-intensive)  | Point-to-point (latency-sensitive) |
| **Memory efficiency**      | Activations replicated           | Activations passed along           |
| **Complexity**             | Higher (requires custom kernels) | Lower (layer-level partitioning)   |
+----------------------------+----------------------------------+------------------------------------+

: **Pipeline vs Tensor Parallelism**. Each strategy has distinct tradeoffs in latency, throughput, and implementation complexity. {#tbl-pipeline-tensor-comparison}

### Expert Parallelism for MoE Models {#sec-inference-expert-parallelism}

Mixture-of-Experts (MoE)[^fn-moe] models present unique sharding challenges because computation is dynamically routed to different experts based on input. Popular models like Mixtral [@jiang2024mixtral] use MoE to achieve high capacity with lower inference cost.

[^fn-moe]: **Mixture-of-Experts (MoE)**: An architecture where only a subset of model parameters (the "experts") are activated for each input, selected by a learned gating network. This enables models with very high total parameter counts (for capacity) while maintaining computational cost proportional to active parameters. The trade-off is increased memory footprint and communication complexity for expert routing.

In an MoE layer, a gating network selects $k$ experts (out of $E$ total) for each token:

$$\text{Output} = \sum_{i \in \text{top-}k} g_i \cdot \text{Expert}_i(\text{input})$$

**Expert parallelism** distributes experts across devices, with each device hosting $E/P$ experts. @fig-moe-routing traces the routing, dispatch, and gather operations:

::: {#fig-moe-routing fig-env="figure" fig-pos="htb" fig-cap="**Mixture-of-Experts (MoE) Routing**. Expert parallelism distributes \"experts\" across different devices. For each token, a gating mechanism selects the top-k experts. An AllToAll communication step dispatches tokens to the devices hosting their selected experts (1). Experts process the tokens in parallel (2). A second AllToAll step gathers the results back to the original device (3). This pattern enables massive model capacity but introduces all-to-all communication overhead." fig-alt="Diagram of MoE token routing. Gating selects top-2 experts. Dashed arrows show All-to-All dispatch to Expert A and B devices. Third device idle. Results gather via All-to-All to weighted sum output."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{GateColor}{RGB}{255,215,0}
  \definecolor{ExpColor}{RGB}{144,238,144}
  \definecolor{CommColor}{RGB}{255,160,122}

  % Input Token
  \node[draw, fill=white] (token) at (0, 3) {Input Token};

  % Host Device
  \node[draw, fill=GateColor!30, minimum height=1cm, minimum width=2cm] (gating) at (0, 1.5) {Gating\\(Top-2)};
  \draw[->, thick] (token) -- (gating);

  % Interconnect
  \node[text=CommColor!80!black, font=\bfseries] at (2.5, 1.5) {All-to-All Dispatch};
  \draw[->, dashed, thick, CommColor] (gating) -- (4, 3);
  \draw[->, dashed, thick, CommColor] (gating) -- (4, 0);

  % Expert Devices
  \node[draw, fill=ExpColor, minimum width=2cm] (exp1) at (5, 3) {Device 1\\Expert A};
  \node[draw, fill=ExpColor, minimum width=2cm] (exp2) at (5, 0) {Device 2\\Expert B};
  \node[draw, fill=gray!20, minimum width=2cm] (exp3) at (5, 1.5) {Device X\\Expert C (Idle)};

  % Gather
  \node[draw, fill=white, rounded corners] (agg) at (8, 1.5) {Weighted\\Sum};

  \node[text=CommColor!80!black, font=\bfseries] at (6.5, 2.5) {All-to-All Gather};
  \draw[->, dashed, thick, CommColor] (exp1) -- (agg);
  \draw[->, dashed, thick, CommColor] (exp2) -- (agg);

  % Output
  \node[right=0.5cm of agg] {Output};
  \draw[->, thick] (agg) -- +(1,0);

\end{tikzpicture}
```
:::

The communication pattern differs from tensor parallelism: instead of all-reduce (same data to all devices), expert parallelism uses all-to-all (different data to different devices based on routing).

**Load balancing challenge**: If gating decisions cluster on certain experts, devices hosting popular experts become bottlenecks while others sit idle. MoE training includes auxiliary losses to encourage balanced routing, but inference still exhibits routing imbalance.

::: {.callout-note title="Expert Parallelism for Mixtral-8x7B"}

Mixtral-8x7B uses 8 experts per MoE layer with top-2 routing:

**Model characteristics**:

- Total parameters: 47B (but only ~13B active per token)
- Experts per layer: 8
- Active experts per token: 2 (top-k = 2)
- MoE layers: Every other feed-forward layer

**Sharding strategy** (4-way expert parallelism):

- Experts 0-1 on Device 0
- Experts 2-3 on Device 1
- Experts 4-5 on Device 2
- Experts 6-7 on Device 3

**Communication pattern per token**:

1. Gating: Determine which 2 experts to use (~0.1ms)
2. AllToAll dispatch: Send token to devices hosting selected experts (~0.2ms)
3. Expert compute: Process token through selected experts (~1ms each, parallel)
4. AllToAll gather: Collect results back (~0.2ms)

**Total MoE layer time**: ~1.5ms (vs ~4ms for equivalent dense layer)

**Load balancing metrics**:

+------------------------------+---------------------+-----------------------+
| **Routing Distribution**     | **GPU Utilization** | **Throughput Impact** |
+:=============================+====================:+:======================+
| **Perfectly balanced**       | 100%                | Baseline              |
| **Moderate imbalance (20%)** | 83%                 | -17%                  |
| **Severe imbalance (50%)**   | 67%                 | -33%                  |
+------------------------------+---------------------+-----------------------+

Production systems monitor routing statistics and may retrain or fine-tune gating to improve balance.

:::

### Embedding Sharding for Recommendation Systems {#sec-inference-embedding-sharding}

Recommendation systems typically contain embedding tables that dwarf model weights in size. Meta's DLRM-scale models [@naumov2019dlrm] have embedding tables exceeding 100TB, requiring sharding strategies distinct from tensor or pipeline parallelism.

**Row-wise sharding** partitions embedding tables by row (entity ID):

$$\text{Shard}_i = \{e_j : \text{hash}(j) \mod P = i\}$$

Each shard contains approximately $N/P$ embeddings, where $N$ is the total number of entities and $P$ is the shard count.

**Column-wise sharding** partitions each embedding vector across devices:

$$e_j = [e_j^{(0)}, e_j^{(1)}, ..., e_j^{(P-1)}]$$

Each device stores a slice of every embedding.

**Hybrid sharding** combines both approaches: frequently accessed embeddings are column-sharded for faster access, while the long tail uses row sharding.

The choice of embedding sharding strategy depends on lookup patterns and communication overhead. @tbl-embedding-sharding compares row-wise, column-wise, and hybrid approaches:

+-----------------------+--------------------------+-------------------+-------------------------+
| **Sharding Strategy** | **Lookup Pattern**       | **Communication** | **Best For**            |
+:======================+:=========================+:==================+:========================+
| **Row-wise**          | Single device per lookup | AllToAll gather   | Uniform access patterns |
| **Column-wise**       | All devices per lookup   | AllGather         | Hot embeddings          |
| **Hybrid**            | Varies by embedding      | Mixed             | Production RecSys       |
+-----------------------+--------------------------+-------------------+-------------------------+

: **Embedding Sharding Strategies**. Different strategies trade off lookup locality against load balance. {#tbl-embedding-sharding}

@fig-embedding-sharding visualizes how each strategy distributes embedding lookups across devices:

::: {#fig-embedding-sharding fig-env="figure" fig-pos="htb" fig-cap="**Embedding Sharding Strategies**. **Row-wise sharding** places complete embedding vectors on specific servers based on entity ID, requiring a network gather for lookup. **Column-wise sharding** splits each vector across all servers, allowing parallel local lookups followed by an AllGather, which is efficient for popular \"hot\" embeddings. **Hybrid sharding** combines these approaches, using column sharding for hot items and row sharding for the \"cold\" long tail to balance load and memory." fig-alt="Three-panel diagram. A: Row-wise sharding splits users across servers by ID. B: Column-wise sharding splits embedding dimensions across servers. C: Hybrid combines both, with hot items column-sharded and cold items row-sharded."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{TableColor}{RGB}{240,248,255}
  \definecolor{ShardColor}{RGB}{176,196,222}

  % A. Row-Wise
  \node[anchor=west, font=\bfseries] at (0, 4.5) {A. Row-Wise Sharding};

  \draw[fill=TableColor] (0, 2.5) rectangle (1, 4);
  \node[rotate=90] at (-0.3, 3.25) {Users 0..N};

  \draw[->, thick] (1.2, 3.25) -- (2.2, 3.25);

  % Server 1
  \draw[fill=ShardColor] (2.5, 3.5) rectangle (3.5, 4.0);
  \node[right, font=\scriptsize] at (3.5, 3.75) {Svr 1: Users 0..K};

  % Server 2
  \draw[fill=ShardColor] (2.5, 2.5) rectangle (3.5, 3.0);
  \node[right, font=\scriptsize] at (3.5, 2.75) {Svr 2: Users K..N};

  \node[font=\scriptsize, align=center] at (1.7, 2.0) {Partition by ID\\(Network Gather)};


  % B. Column-Wise
  \node[anchor=west, font=\bfseries] at (6, 4.5) {B. Column-Wise Sharding};

  \draw[fill=TableColor] (6, 2.5) rectangle (7, 4);
  \node[rotate=90] at (5.7, 3.25) {Dims 0..D};

  \draw[->, thick] (7.2, 3.25) -- (8.2, 3.25);

  % Server 1 (Col split)
  \draw[fill=ShardColor] (8.5, 2.5) rectangle (9.0, 4.0);
  \node[above, font=\scriptsize] at (8.75, 4.0) {Svr 1};

  % Server 2
  \draw[fill=ShardColor] (9.1, 2.5) rectangle (9.6, 4.0);
  \node[above, font=\scriptsize] at (9.35, 4.0) {Svr 2};

  \node[font=\scriptsize, align=center] at (7.7, 2.0) {Partition Vector\\(AllGather)};


  % C. Hybrid
  \node[anchor=west, font=\bfseries] at (0, 0.5) {C. Hybrid Sharding};

  \draw[fill=red!10] (0, -1.0) rectangle (1.5, 0);
  \node[font=\scriptsize] at (0.75, -0.5) {Hot (Top 1\%)};

  \draw[fill=blue!10] (0, -2.5) rectangle (1.5, -1.1);
  \node[font=\scriptsize] at (0.75, -1.8) {Cold (Tail)};

  \draw[->, thick] (1.7, -1.25) -- (2.7, -1.25);

  \node[draw, fill=red!10, font=\scriptsize] at (4, -0.5) {Replicated / Col-wise};
  \node[draw, fill=blue!10, font=\scriptsize] at (4, -1.8) {Row-wise Sharded};

\end{tikzpicture}
```
:::

::: {.callout-note title="Embedding Sharding at Scale: Meta Infrastructure"}

Meta's recommendation infrastructure demonstrates embedding sharding at extreme scale:

**Scale**:

- Embedding tables: 100+ TB total
- Unique entities: 10+ trillion
- Embedding dimension: 128-256
- Shards: 1,000+ servers

**Sharding strategy**:

- **Hot embeddings** (top 1% by access frequency): Replicated across all shards
- **Warm embeddings** (next 10%): Column-sharded with 8-way parallelism
- **Cold embeddings** (remaining 89%): Row-sharded with consistent hashing

Each inference request requires approximately 5,000 embedding lookups. Without optimization, this would require 5,000 network round trips. Instead, the system applies several optimizations. Batch accumulation collects lookups for 1ms. Lookup deduplication removes duplicate entities across requests. Shard-aware batching groups lookups by destination shard. Parallel dispatch sends batched requests to all shards simultaneously. Streaming assembly reconstructs embeddings as responses arrive.

**Performance**:

+-------------------------+--------------------------+-----------------------+
| **Metric**              | **Without Optimization** | **With Optimization** |
+:========================+=========================:+======================:+
| **Network round trips** | 5,000                    | 1 (batched)           |
| **Lookup latency**      | 50ms                     | 2ms                   |
| **Network bandwidth**   | 10 Gbps                  | 40 Gbps (burst)       |
+-------------------------+--------------------------+-----------------------+

:::

### Hybrid Sharding Strategies {#sec-inference-hybrid-sharding}

Production systems often combine multiple sharding strategies to handle different model components optimally:

**Tensor + Pipeline parallelism**: For very large models that require both memory distribution and latency reduction:

```
8 GPUs organized as 2 pipeline stages × 4 tensor parallel:

Stage 0 (Layers 1-40):  TP across GPUs 0,1,2,3
Stage 1 (Layers 41-80): TP across GPUs 4,5,6,7
```

This achieves 4x latency reduction (from TP) while handling models requiring 8-way sharding for memory.

**Expert + Tensor parallelism**: For MoE models where individual experts are large:

```
Mixtral with large experts:

- Expert parallelism: Distribute 8 experts across 8 GPU groups
- Tensor parallelism: Each expert spread across 2 GPUs
- Total GPUs: 16
```

**Embedding + Dense parallelism**: For recommendation models with both large embeddings and large dense components:

```
DLRM-scale model:

- Embedding sharding: 1,000 shards across CPU servers
- Dense model: 8-way tensor parallel across GPUs
- Communication: Embeddings gathered to GPU, processed, returned
```

### Communication Overhead Analysis {#sec-inference-sharding-communication}

The practical speedup from sharding depends critically on communication efficiency. Each sharding strategy has characteristic communication patterns with different bandwidth and latency requirements.

@eq-allreduce-time quantifies AllReduce communication time for tensor parallelism, where data is combined from all devices with the result available on all devices.

$$T_{allreduce} = 2 \times \frac{(P-1)}{P} \times \frac{M}{B}$$ {#eq-allreduce-time}

where $P$ is the number of devices, $M$ is the message size, and $B$ is the interconnect bandwidth. The factor of 2 accounts for the reduce-scatter and all-gather phases.

@eq-p2p-time expresses the simpler point-to-point communication for pipeline parallelism, where data flows from one device to the next.

$$T_{p2p} = L + \frac{M}{B}$$ {#eq-p2p-time}

where $L$ is the network latency and $M/B$ is the transfer time.

@eq-alltoall-time captures the more complex AllToAll communication for expert parallelism, where each device exchanges distinct data with every other device.

$$T_{alltoall} = (P-1) \times \left(L + \frac{M/P}{B}\right)$$ {#eq-alltoall-time}

::: {.callout-note title="Interconnect Technology Comparison"}

Communication overhead depends heavily on the interconnect technology:

+--------------------+----------------------+-------------+------------------------+
| **Interconnect**   | **Bandwidth**        | **Latency** | **Use Case**           |
+===================:+=====================:+============:+:=======================+
| **NVLink (H100)**  | 900 GB/s             | 1μs         | Intra-node TP          |
| **PCIe Gen5**      | 64 GB/s              | 5μs         | Intra-node (no NVLink) |
| **InfiniBand HDR** | 200 Gb/s (25 GB/s)   | 1μs         | Inter-node             |
| **Ethernet 100G**  | 100 Gb/s (12.5 GB/s) | 10μs        | Inter-node (commodity) |
+--------------------+----------------------+-------------+------------------------+

NVLink provides the highest bandwidth for intra-node communication.

**Example: 8-way tensor parallelism communication**

Activation size: 8MB per all-reduce (batch=1, hidden=8192)

+-------------------+--------------------+---------------------+
| **Interconnect**  | **AllReduce Time** | **% of 30ms Layer** |
+:==================+===================:+====================:+
| **NVLink**        | 0.02ms             | 0.07%               |
| **InfiniBand**    | 0.7ms              | 2.3%                |
| **100G Ethernet** | 1.5ms              | 5%                  |
+-------------------+--------------------+---------------------+

NVLink enables efficient tensor parallelism within a node. Cross-node tensor parallelism requires InfiniBand for acceptable overhead.

:::

NVLink bandwidth has evolved significantly over GPU generations.[^fn-nvlink]

[^fn-nvlink]: **NVLink evolution**: NVLink bandwidth has grown from 160 GB/s (Pascal, 2016) to 900 GB/s bidirectional (Hopper, 2022) to 1.8 TB/s (Blackwell, 2024). This 10x improvement enables tensor parallelism across 8+ GPUs with less than 5% communication overhead, making large model inference practical.

### Sharding Strategy Selection {#sec-inference-sharding-selection}

The choice of sharding strategy depends on model architecture and deployment priorities. @tbl-sharding-selection evaluates each approach across four critical factors:

+---------------------------+---------------------+-----------------------+---------------------+---------------------+
| **Factor**                | **Tensor Parallel** | **Pipeline Parallel** | **Expert Parallel** | **Embedding Shard** |
+:==========================+:====================+:======================+:====================+:====================+
| **Latency priority**      | Best                | Worst                 | Moderate            | N/A                 |
| **Throughput priority**   | Good                | Best (pipelined)      | Good                | Best                |
| **Interconnect limited**  | Poor fit            | Good fit              | Moderate            | Good fit            |
| **Implementation effort** | High                | Low                   | Moderate            | High                |
+---------------------------+---------------------+-----------------------+---------------------+---------------------+

: **Sharding Strategy Selection Guide**. Match strategy to deployment priorities and constraints. {#tbl-sharding-selection}

## Load Balancing and Request Routing {#sec-inference-load-balancing}

With models deployed across multiple replicas, the next challenge is distributing requests effectively. This moves us from replica-level concerns (how individual replicas are configured) to service-level optimization in the serving hierarchy: coordinating across replicas to meet aggregate throughput and latency targets.

Load balancing determines which replica handles each request, directly impacting latency, throughput, and resource utilization. Seemingly simple choices, like random assignment versus informed selection, produce dramatically different performance at scale. This section develops the theory and practice of load balancing for inference, from basic algorithms through the power-of-two-choices insight that provides exponentially better performance with minimal overhead.

::: {.callout-note title="Archetype B (Scaled Lighthouse): The Tail at Scale"}
**Archetype B (The Global Real-Time Recommendation Engine)** is the canonical victim of tail latency. Processing 10 million QPS means that a 1-in-10,000 latency spike happens 1,000 times every second. For Archetype B, sophisticated load balancing (like Power-of-Two-Choices) is mandatory to suppress these outliers; simple Round-Robin would allow queues to build up, causing the 99th percentile latency to collapse into unacceptable slowness.
:::

### Load Balancing Principles {#sec-inference-lb-fundamentals}

Load balancing serves two primary goals that sometimes conflict:

**Latency minimization**: Route requests to replicas that can serve them fastest, considering current queue depth and processing time.

**Utilization maximization**: Spread load evenly to avoid both idle replicas and overloaded replicas.

The tension arises because latency-optimal routing may concentrate load on fast replicas, reducing their performance and leaving other replicas underutilized.

Load balancing evaluation uses several key metrics. Maximum queue length measures the longest queue across all replicas and determines worst-case latency. Load variance captures the standard deviation of queue lengths and measures balance. Utilization spread represents the difference between most and least utilized replicas. Decision overhead quantifies the time required to make routing decisions.

### Round-Robin and Random Assignment {#sec-inference-lb-round-robin}

The simplest load balancing strategies assign requests without considering server state:

Round-robin assigns requests in circular order. Request 1 goes to server 1, request 2 to server 2, and so on. This guarantees perfect distribution when servers are homogeneous and request processing times are identical.

Random assignment selects a server uniformly at random for each request. With large numbers of requests, this converges to even distribution but with higher variance than round-robin.

For homogeneous servers with identical service times, both achieve near-optimal load distribution. However, production systems rarely meet these assumptions. Heterogeneous hardware introduces different GPU generations and memory configurations. Variable request sizes mean some requests take 10x longer than others. Server state variations occur when some replicas are warming up while others approach memory limits.

Under these realistic conditions, uninformed strategies perform poorly. The maximum queue length under random assignment follows @eq-random-max-queue[^fn-theta-notation]:

$$E[\text{max queue}] = \Theta\left(\frac{\log n}{\log \log n}\right)$$ {#eq-random-max-queue}

[^fn-theta-notation]: **Theta Notation (Θ)**: Asymptotic tight bound indicating both upper (O) and lower (Ω) complexity bounds match. Unlike O(f(n)) which may be loose, Θ(f(n)) precisely characterizes growth rate. In queueing analysis for inference systems, tight bounds enable accurate capacity planning: Θ(log n / log log n) queue length means exactly that scaling, not just "at most".

where $n$ is the number of servers. For 1,000 servers, this is approximately 4-5 requests. This seems small, but the unlucky requests in long queues experience significantly higher latency.

### The Power of Two Choices {#sec-inference-two-choices}

A remarkable result in load balancing theory [@mitzenmacher2001power] shows that querying just two random servers before making a routing decision provides exponentially better load distribution than random assignment.[^fn-balls-bins]

[^fn-balls-bins]: **Balls-into-bins problem**: The power-of-two-choices result comes from probabilistic analysis of throwing balls into bins. With random placement, maximum load is $\Theta(\log n / \log \log n)$. With two choices and placing in the less-loaded bin, maximum load drops to $\Theta(\log \log n)$. This exponential improvement from a constant-factor change in the algorithm is a foundational result in randomized algorithms.

The power-of-two-choices algorithm operates as follows. Select two servers uniformly at random. Query both for their current queue length. Route the request to the server with the shorter queue.

@eq-two-choices-max-queue formalizes this exponential improvement, reducing maximum queue length from $O(\log n / \log \log n)$ to $O(\log \log n)$:

$$E[\text{max queue}]_{\text{two choices}} = \Theta(\log \log n)$$ {#eq-two-choices-max-queue}

For 1,000 servers:

- Random assignment max queue: ~4-5 requests
- Two choices max queue: ~2 requests

The improvement is exponential: two choices with 1,000 servers achieves better balance than random with just 10 servers.

::: {.callout-important title="Exponential Improvement from a Simple Change"}

The power-of-two-choices result is one of the most impactful findings in distributed systems theory. By examining just one additional server, maximum queue length improves from $O(\log n / \log \log n)$ to $O(\log \log n)$, an exponential improvement.

This has profound practical implications:

- Near-optimal load balancing with minimal overhead (2 probes vs n probes)
- Scalable: improvement increases with system size
- Robust: works with heterogeneous servers and variable request sizes
- Simple: easy to implement in any load balancer

Production systems at Google, Meta, and AWS all use variants of power-of-two-choices.

:::

**Why does this work?** Intuitively, random assignment occasionally makes poor choices (routing to an already-busy server), and these mistakes compound. With two choices, the algorithm almost never makes the worst choice, avoiding the tail behavior that creates long queues.

Mathematically, the key insight is that with random assignment, when $d$ servers have queue length $k$, the probability of queue length $k+1$ growing is proportional to $d/n$. With two choices, this probability drops to $(d/n)^2$, creating a super-exponential decay in queue length distribution.

### Weighted and Adaptive Load Balancing {#sec-inference-weighted-lb}

When servers have different capacities, naive load balancing creates imbalance. A mix of A100 GPUs (high capacity) and T4 GPUs (lower capacity) receiving equal request rates will have T4 servers overloaded while A100 servers are underutilized.

**Weighted round-robin** assigns requests proportional to server capacity:

$$P(\text{route to server } i) = \frac{w_i}{\sum_j w_j}$$

where $w_i$ is the weight (capacity) of server $i$.

**Weighted two-choices** applies the same principle:

1. Select two servers with probability proportional to their weights
2. Query both for current load relative to their capacity
3. Route to the server with lower relative load

::: {.callout-note title="Worked Example: Heterogeneous GPU Cluster"}

Consider a cluster with mixed GPU types:

- 10 H100 GPUs (capacity: 1000 QPS each)
- 20 A100 GPUs (capacity: 600 QPS each)
- Total capacity: 10×1000 + 20×600 = 22,000 QPS

**Target traffic**: 15,000 QPS

**Weighted assignment**:

- H100 weight: 1000 / 22000 = 4.5%
- A100 weight: 600 / 22000 = 2.7%

**Expected load per server**:

- H100: 15000 × 0.045 = 682 QPS (68% utilization)
- A100: 15000 × 0.027 = 409 QPS (68% utilization)

Both server types operate at equal utilization, maximizing overall capacity while maintaining latency consistency.

**Without weighting** (equal distribution):

- Per-server load: 15000 / 30 = 500 QPS
- H100 utilization: 50% (underutilized)
- A100 utilization: 83% (overloaded, latency spikes)

:::

**Adaptive load balancing** adjusts weights dynamically based on observed performance:

```
For each server i:
    latency[i] = exponential_moving_average(observed_latency)
    weight[i] = 1 / latency[i]  # Inverse latency weighting
```

This automatically adapts to:

- Server degradation (memory pressure, thermal throttling)
- Request size variations (some traffic patterns harder to serve)
- Background tasks consuming resources

### Least-Connections Load Balancing {#sec-inference-least-connections}

An alternative to random selection is routing to the server with the fewest active connections (or shortest queue). This requires maintaining global state but provides better balance for variable-size requests.

**Least-connections algorithm**:

1. Maintain a count of active requests per server
2. Route each new request to the server with the minimum count
3. Increment count on dispatch, decrement on completion

For long-running requests (common in LLM serving), least-connections significantly outperforms round-robin because it accounts for current load rather than just historical assignments.

The challenge is maintaining accurate connection counts in a distributed system. Options include:

- **Centralized counter**: Single source of truth, potential bottleneck
- **Distributed counters with gossip**: Eventually consistent, may route to stale information
- **Sampled least-connections**: Query a subset of servers, choose minimum (combines with two-choices)

::: {.callout-note title="Least-Connections for LLM Serving"}

LLM inference has highly variable request durations based on output length:

- Short response (10 tokens): 500ms
- Long response (500 tokens): 25s
- Ratio: 50x

With round-robin at 100 QPS across 10 servers:

- Each server receives 10 requests/second
- If one server gets multiple long requests, it falls behind
- Queue builds while other servers sit idle

With least-connections:

- New requests route away from servers processing long responses
- Servers finishing short requests receive new work immediately
- Load naturally balances based on actual work remaining

**Observed improvement** (production LLM serving):

+-----------------------+-----------------+-------------------+
| **Algorithm**         | **P99 Latency** | **Load Variance** |
+:======================+================:+==================:+
| **Round-robin**       | 45s             | 3.2 requests      |
| **Least-connections** | 28s             | 0.8 requests      |
| **Two-choices + LC**  | 26s             | 0.5 requests      |
+-----------------------+-----------------+-------------------+

Least-connections reduces P99 by 38%; combining with two-choices provides additional improvement.

:::

### Consistent Hashing for Stateful Routing {#sec-inference-consistent-hashing}

Many inference workloads maintain state that benefits from routing affinity:

- **LLM conversations**: KV cache from previous turns
- **Recommendation sessions**: User context and recent interactions
- **Streaming inference**: Model state from previous frames

For these workloads, routing the same user or session to the same server improves performance by avoiding cache misses and state reconstruction.

**Consistent hashing**[^fn-consistent-hashing] [@karger1997consistent] maps requests to servers based on a hash of the routing key (user ID, session ID):

[^fn-consistent-hashing]: **Consistent hashing**: Originally developed for distributed caching (Akamai, 1997), consistent hashing arranges servers on a virtual ring and assigns requests to the nearest server clockwise. When a server fails, only requests that were assigned to that server remap to the next server, minimizing disruption. This property is essential for maintaining KV cache locality during scaling events.

$$\text{server}(request) = \arg\min_{s \in S} \text{distance}(\text{hash}(key), \text{hash}(s))$$

where servers and keys are mapped onto a ring, and each request routes to the nearest server clockwise.

Key properties:

- **Deterministic**: Same key always routes to same server
- **Minimal disruption**: Adding/removing servers only remaps $K/N$ keys on average
- **Load balancing**: With virtual nodes, load distributes evenly

::: {.callout-note title="Consistent Hashing for KV Cache Affinity"}

Consider an LLM serving system where each user's conversation maintains KV cache state:

**Without affinity**:

- User sends message, routed to Server A, KV cache built
- Next message routes to Server B (random)
- KV cache rebuilt from scratch, 500ms penalty
- Average conversation: 10 turns, 4.5s wasted on cache rebuilds

**With consistent hashing**:

- User ID hashed to Server A
- All messages from this user route to Server A
- KV cache reused across turns
- Rebuild only on server changes or cache eviction

**Implementation with virtual nodes**:

Each physical server has 100 virtual nodes on the hash ring, ensuring even distribution despite server heterogeneity.

```
Hash ring positions:
Server A: [0.01, 0.03, 0.07, 0.12, ...]  (100 positions)
Server B: [0.02, 0.05, 0.09, 0.15, ...]  (100 positions)
...

Request for user "alice":
hash("alice") = 0.0834
Nearest server clockwise: Server A (at 0.09)
```

**Handling server failures**:

When Server A fails, its 100 virtual nodes are removed from the ring. Requests that would have routed to Server A now route to the next server clockwise. Only ~$1/N$ of requests are affected, where $N$ is the number of servers.

:::

### Request Routing for Sharded Models {#sec-inference-sharded-routing}

The sharding strategies examined in @sec-inference-sharding introduce routing complexity: a single inference request may require computation on multiple devices, necessitating coordination.

**Routing patterns for sharded models**:

**Tensor parallelism**: Request is broadcast to all devices in the shard group. Each device processes its portion of each layer. Results are synchronized via all-reduce.

```
Request → Load Balancer → Shard Group
                              │
            ┌────────────────┼────────────────┐
            ▼                ▼                ▼
         GPU 0            GPU 1            GPU 2
      (heads 0-7)      (heads 8-15)    (heads 16-23)
            │                │                │
            └────────AllReduce────────────────┘
                              │
                              ▼
                          Response
```

**Pipeline parallelism**: Request flows through stages sequentially. Each stage forwards to the next.

```
Request → Stage 0 → Stage 1 → Stage 2 → Stage 3 → Response
         (L1-20)   (L21-40)  (L41-60)  (L61-80)
```

**Expert parallelism**: Request is dispatched to devices hosting selected experts based on gating decision.

```
Request → Gating → AllToAll dispatch → Expert compute → AllToAll gather → Response
                   (to selected experts)              (results back)
```

**Routing to shard groups**: With multiple shard groups for horizontal scaling, the load balancer routes to groups rather than individual devices:

```
                    Load Balancer
                         │
          ┌──────────────┼──────────────┐
          ▼              ▼              ▼
     Shard Group 0  Shard Group 1  Shard Group 2
      (8 GPUs)       (8 GPUs)       (8 GPUs)
```

The load balancer treats each shard group as a single logical server, applying standard algorithms (round-robin, two-choices, consistent hashing) at the group level.

### Health Checking and Failover {#sec-inference-health-checking}

Load balancers must detect unhealthy servers and route around them. Health checking mechanisms include:

**Liveness probes**: Verify the server process is running.

```
GET /health/live
Response: 200 OK (process alive) or timeout (process dead)
```

**Readiness probes**: Verify the server can handle requests (model loaded, GPU initialized).

```
GET /health/ready
Response: 200 OK (ready to serve) or 503 (not ready)
```

**Deep health checks**: Verify actual inference works by running a test request.

```
POST /health/inference
Body: {"prompt": "test"}
Response: 200 OK with valid output, or error
```

::: {.callout-note title="Health Check Configuration for GPU Inference"}

GPU inference servers have unique health check considerations:

**GPU memory pressure**: Server may be alive but unable to allocate memory for new requests.

```{.python}
def readiness_check():
    free_memory = (
        torch.cuda.memory_reserved() - torch.cuda.memory_allocated()
    )
    if free_memory < MIN_REQUEST_MEMORY:
        return {
            "status": "not_ready",
            "reason": "insufficient GPU memory",
        }
    return {"status": "ready"}
```

**Model warm-up**: First inference after load is slower. Mark ready only after warm-up.

```{.python}
async def startup():
    model = load_model()
    # Warm up with dummy requests
    for _ in range(10):
        model.generate(dummy_input)
    # Now mark as ready
    global ready
    ready = True
```

**Timeout configuration**:

+-----------------+--------------+-------------+-----------------------+
| **Check Type**  | **Interval** | **Timeout** | **Failure Threshold** |
+:================+=============:+============:+======================:+
| **Liveness**    | 10s          | 5s          | 3 failures            |
| **Readiness**   | 5s           | 3s          | 2 failures            |
| **Deep health** | 30s          | 10s         | 1 failure             |
+-----------------+--------------+-------------+-----------------------+

Deep health checks run less frequently because they consume GPU resources.

:::

### Quantitative Analysis: Load Balancing Impact {#sec-inference-lb-analysis}

The choice of load balancing algorithm has quantitative impact on system performance. Consider a system with 100 servers, 10,000 QPS, and variable request sizes (CV = 0.5). @tbl-lb-comparison quantifies the latency and overhead tradeoffs:

+-----------------------+---------------+-----------------+------------------+
| **Algorithm**         | **Max Queue** | **P99 Latency** | **CPU Overhead** |
+:======================+==============:+================:+:=================+
| **Random**            | 4.2 requests  | 45ms            | Minimal          |
| **Round-robin**       | 2.8 requests  | 32ms            | Minimal          |
| **Two-choices**       | 1.9 requests  | 24ms            | 2 probes/request |
| **Least-connections** | 1.4 requests  | 19ms            | Global state     |
| **Two-choices + LC**  | 1.2 requests  | 17ms            | 2 probes + state |
+-----------------------+---------------+-----------------+------------------+

: **Load Balancing Algorithm Comparison**. More sophisticated algorithms reduce queue lengths and latency at the cost of increased overhead. {#tbl-lb-comparison}

The progression shows clear tradeoffs:

- Random/round-robin: Zero overhead but higher latency variance
- Two-choices: Minimal overhead (2 probes), 47% latency improvement
- Least-connections: State maintenance overhead, 58% latency improvement
- Combined: Best performance, highest complexity

For most production systems, two-choices provides the best tradeoff between performance improvement and implementation complexity. Least-connections adds value for workloads with high request size variance (LLM serving, recommendation ranking).

### Circuit Breakers and Backpressure {#sec-inference-circuit-breakers}

When servers become overloaded, routing more requests exacerbates the problem. Circuit breakers and backpressure mechanisms protect the system from cascading failures.

**Circuit breaker pattern**[^fn-circuit-breaker] [@nygard2007releaseit]:

[^fn-circuit-breaker]: **Circuit breaker**: Borrowed from electrical engineering, where circuit breakers prevent electrical fires by cutting power when current exceeds safe levels. In distributed systems, circuit breakers prevent cascade failures by quickly failing requests to unhealthy services rather than waiting for timeouts. The pattern was popularized by Netflix's Hystrix library and is now standard in service mesh implementations like Envoy and Istio.

```
States: CLOSED → OPEN → HALF-OPEN → CLOSED

CLOSED: Normal operation, route requests
OPEN: Server unhealthy, immediately reject requests
HALF-OPEN: Allow limited requests to test recovery

Transitions:

- CLOSED → OPEN: Error rate exceeds threshold (e.g., 50%)
- OPEN → HALF-OPEN: After timeout (e.g., 30s)
- HALF-OPEN → CLOSED: Test requests succeed
- HALF-OPEN → OPEN: Test requests fail
```

**Backpressure propagation**: When servers are overloaded, they signal upstream to reduce request rate:

```
Server queue depth > threshold
    → Return 503 Service Unavailable
    → Load balancer marks server as degraded
    → Routes fewer requests to this server
    → If all servers degraded, apply admission control
```

::: {.callout-note title="Cascading Failure Prevention"}

Consider a scenario where one server becomes slow (thermal throttling):

**Without circuit breaker**:

1. Server A slows down (processing 500ms instead of 50ms)
2. Load balancer continues routing to Server A
3. Requests queue on Server A, timeouts begin
4. Retry logic sends failed requests to other servers
5. Other servers overload from retry traffic
6. System-wide failure

**With circuit breaker**:

1. Server A slows down
2. Error rate on Server A rises above 50%
3. Circuit breaker opens for Server A
4. All requests route to Servers B, C, D
5. System operates at reduced capacity but remains stable
6. After recovery, circuit breaker closes, Server A rejoins

**Configuration for GPU inference**:

+------------------------+-------------+---------------------------------------+
| **Parameter**          | **Value**   | **Rationale**                         |
+:=======================+============:+:======================================+
| **Error threshold**    | 30%         | GPU OOM failures are serious          |
| **Latency threshold**  | 2x baseline | Detect throttling early               |
| **Open duration**      | 60s         | GPU recovery takes time               |
| **Half-open requests** | 5           | Careful testing before full reopening |
+------------------------+-------------+---------------------------------------+

:::

## Multi-Tenancy and Isolation {#sec-inference-multitenancy}

The optimizations we have examined so far, from batching through load balancing and sharding, focus on individual model deployments. This moves us to the platform level of the serving hierarchy, where the challenge shifts from optimizing a single model to managing multiple models, customers, and workloads on shared infrastructure.

Multi-tenancy enables efficient resource utilization but introduces challenges around isolation, fairness, and quality of service guarantees. A noisy neighbor consuming excessive resources can degrade performance for all other tenants.

This section examines the techniques for sharing inference infrastructure while maintaining isolation between tenants.

### The Multi-Tenancy Challenge {#sec-inference-multitenancy-challenge}

Multi-tenancy provides significant benefits:

- **Cost efficiency**: Sharing infrastructure across tenants improves utilization
- **Operational simplicity**: Fewer clusters to manage, monitor, and upgrade
- **Statistical multiplexing**: Aggregate traffic is more predictable than per-tenant traffic

However, sharing introduces risks. @tbl-tenancy-comparison weighs utilization gains against isolation challenges:

- **Noisy neighbors**: One tenant's burst traffic impacts others
- **Resource contention**: GPU memory, network bandwidth, CPU cycles
- **Security boundaries**: Tenant data must remain isolated
- **SLO complexity**: Different tenants have different requirements

+--------------------------+------------------------+------------------------+
| **Aspect**               | **Single-Tenant**      | **Multi-Tenant**       |
+:=========================+:=======================+:=======================+
| **Resource utilization** | 30-50%                 | 70-90%                 |
| **Cost per request**     | Higher                 | Lower                  |
| **SLO guarantees**       | Simple                 | Complex                |
| **Isolation**            | Complete               | Requires engineering   |
| **Operational overhead** | Higher (many clusters) | Lower (fewer clusters) |
+--------------------------+------------------------+------------------------+

: **Single vs Multi-Tenant Tradeoffs**: Multi-tenancy reduces cost but requires careful isolation engineering. {#tbl-tenancy-comparison}

### Noisy Neighbor Problems {#sec-inference-noisy-neighbor}

The noisy neighbor problem occurs when one tenant's workload degrades performance for others sharing the same infrastructure.

**GPU memory contention**: A tenant with unexpectedly long sequences consumes KV cache memory, forcing evictions that impact other tenants.

```
Scenario: 3 tenants sharing GPU with 60GB KV cache pool

Normal state:
  Tenant A: 20GB (200 sequences)
  Tenant B: 20GB (200 sequences)
  Tenant C: 20GB (200 sequences)

Noisy neighbor (Tenant C starts long-context requests):
  Tenant C: 45GB (150 sequences, longer context)
  Tenant A: 7.5GB (evicted to 75 sequences)
  Tenant B: 7.5GB (evicted to 75 sequences)

Impact: Tenants A and B see 62% reduction in batch size
```

**Network bandwidth saturation**: A tenant streaming many large responses saturates network bandwidth, increasing latency for all tenants.

**Compute interference**: GPU time-sharing between tenants introduces context-switching overhead and unpredictable latency.

::: {.callout-note title="Quantifying Noisy Neighbor Impact"}

Consider an inference platform serving 10 tenants on shared H100 GPUs:

**Baseline (even load)**:

- Each tenant: 100 QPS, 10ms P99 latency
- GPU utilization: 70%
- All SLOs met

**Noisy neighbor scenario** (Tenant 3 bursts to 500 QPS):

+------------------+----------+-----------------+----------------+
| **Tenant**       | **QPS**  | **P99 Latency** | **SLO Status** |
+=================:+=========:+================:+:===============+
| **Tenant 1**     | 100      | 25ms            | Violated       |
| **Tenant 2**     | 100      | 28ms            | Violated       |
| **Tenant 3**     | 500      | 45ms            | Violated       |
| **Tenants 4-10** | 100 each | 22-30ms         | Violated       |
+------------------+----------+-----------------+----------------+

Without isolation, one tenant's burst causes cascade failures for all tenants.

**With isolation** (per-tenant resource quotas):

+------------------+------------------+-----------------+--------------------------+
| **Tenant**       | **QPS (actual)** | **P99 Latency** | **SLO Status**           |
+=================:+=================:+================:+:=========================+
| **Tenant 1**     | 100              | 11ms            | Met                      |
| **Tenant 2**     | 100              | 11ms            | Met                      |
| **Tenant 3**     | 120 (throttled)  | 50ms            | Violated (only for them) |
| **Tenants 4-10** | 100 each         | 11ms            | Met                      |
+------------------+------------------+-----------------+--------------------------+

Isolation contains the impact to the offending tenant.

:::

### Resource Quotas and Fair Sharing {#sec-inference-quotas}

Resource quotas limit what each tenant can consume, preventing any single tenant from monopolizing shared resources.

**Hard quotas** enforce strict limits:

```python
class TenantQuota:
    max_concurrent_requests: int  # e.g., 100
    max_kv_cache_mb: int  # e.g., 20,000
    max_qps: int  # e.g., 1,000
    max_batch_tokens: int  # e.g., 50,000


def admit_request(tenant_id, request):
    quota = get_quota(tenant_id)
    usage = get_usage(tenant_id)

    if usage.concurrent >= quota.max_concurrent:
        return RateLimitError("concurrent request limit")
    if usage.kv_cache_mb >= quota.max_kv_cache_mb:
        return RateLimitError("memory limit")
    if usage.qps >= quota.max_qps:
        return RateLimitError("rate limit")

    return admit(request)
```

**Soft quotas** with fair sharing allow exceeding limits when resources are available:

```
Tenant quota: 100 QPS (soft limit)

When cluster is underutilized (50%):
  Tenant can burst to 200 QPS (2x quota)

When cluster is saturated (90%):
  Tenant limited to 100 QPS (quota enforced)
```

This approach maximizes utilization while protecting tenants during contention.

**Max-min fairness** allocates resources to maximize the minimum allocation:

```
Total capacity: 1000 QPS
Tenants: A (demand 300), B (demand 200), C (demand 800)
Total demand: 1300 QPS (exceeds capacity)

Max-min allocation:
1. Give each tenant equal share: 333 QPS
2. A needs only 300, donate 33 to others
3. B needs only 200, donate 133 to others
4. C receives donations: 333 + 33 + 133 = 499 QPS

Final: A=300, B=200, C=500
All demands met up to fair share, C limited proportionally
```

### Priority Scheduling {#sec-inference-priority-scheduling}

When tenants have different SLO requirements, priority scheduling ensures high-priority requests receive resources first.

**Priority classes**:

+-----------------+--------------------+-------------------------+------------------------+
| **Class**       | **Use Case**       | **Preemption**          | **Resource Guarantee** |
+:================+:===================+:========================+:=======================+
| **Critical**    | Revenue-generating | Can preempt lower       | 100% reserved          |
| **Standard**    | General traffic    | Can preempt best-effort | Weighted share         |
| **Best-effort** | Background, batch  | Cannot preempt          | No guarantee           |
+-----------------+--------------------+-------------------------+------------------------+

**Priority-aware queuing**:

```
Incoming requests sorted by priority, then arrival time:

Queue state:
  [Critical-001] [Critical-002] [Standard-001] [Standard-002] [BestEffort-001]
       ↑ Process first

New Critical-003 arrives:
  [Critical-001] [Critical-002] [Critical-003] [Standard-001] [Standard-002]
       ↑ Jumps ahead of Standard requests
```

**Preemption for LLM serving**:

When a critical request arrives but all GPU slots are occupied by lower-priority requests:

1. Select victim request(s) from lowest priority class
2. Pause victim's generation (save KV cache state)
3. Allocate GPU slot to critical request
4. When critical request completes, resume victim

```
Before preemption:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]

Critical request arrives:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]
                ↓ preempt
  GPU slots: [Critical E] [Standard B] [Standard C] [Standard D]
  Paused: Best-effort A (KV cache saved to CPU)

After Critical E completes:
  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]
  (A resumed from saved state)
```

### Bulkhead Pattern {#sec-inference-bulkhead}

The bulkhead pattern[^fn-bulkhead] [@nygard2007releaseit] physically isolates tenant workloads, preventing failures from propagating across tenants. Named after ship compartments that contain flooding to isolated sections.

[^fn-bulkhead]: **Bulkhead pattern origins**: The Titanic's bulkheads failed because they did not extend to the top of the ship, allowing water to spill over as the ship tilted. Modern software bulkheads learn from this: effective isolation requires complete separation (dedicated resources) rather than partial isolation (shared pools with quotas), at least for critical workloads.

**Deployment-level bulkheads** dedicate replicas to specific tenants or tenant groups. @fig-bulkhead-patterns illustrates complete isolation between gold and standard tiers:

::: {#fig-bulkhead-patterns fig-env="figure" fig-pos="htb" fig-cap="**Bulkhead Isolation Patterns**. To prevent cascading failures in multi-tenant systems, bulkheads isolate resources. **Deployment-level bulkheads** (shown) assign dedicated physical replicas to high-priority tenants, ensuring complete isolation. **Request-level bulkheads** enforce strict concurrency limits within shared processes. Like ship compartments, these boundaries ensure that a failure or resource exhaustion in one segment cannot sink the entire platform." fig-alt="Two-tier system diagram. Gold tier at top with dedicated load balancer and 2 replicas. Standard tier below with shared load balancer and 3 replicas, one showing failure. Dashed line marks complete isolation boundary between tiers."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  \definecolor{GoldColor}{RGB}{255,215,0}
  \definecolor{StdColor}{RGB}{220,220,220}
  \definecolor{FailColor}{RGB}{255,99,71}

  % Gold Tier System
  \node[anchor=west] at (0, 4) {\textbf{Gold Tier} (Dedicated)};
  \node[draw, fill=white, rounded corners] (gold_lb) at (1, 3) {Gold LB};

  \node[draw, fill=GoldColor!30] (r1) at (3.5, 3.5) {Replica 1};
  \node[draw, fill=GoldColor!30] (r2) at (3.5, 2.5) {Replica 2};

  \draw[->, thick] (gold_lb) -- (r1);
  \draw[->, thick] (gold_lb) -- (r2);

  % Standard Tier System
  \node[anchor=west] at (0, 1) {\textbf{Standard Tier} (Shared Pool)};
  \node[draw, fill=white, rounded corners] (std_lb) at (1, 0) {Std LB};

  \node[draw, fill=StdColor] (r3) at (3.5, 1.0) {Replica 3};
  \node[draw, fill=FailColor] (r4) at (3.5, 0.0) {Replica 4\\(Failure)};
  \node[draw, fill=StdColor] (r5) at (3.5, -1.0) {Replica 5};

  \draw[->, thick] (std_lb) -- (r3);
  \draw[->, thick] (std_lb) -- (r4);
  \draw[->, thick] (std_lb) -- (r5);

  % Isolation Barrier
  \draw[dashed, ultra thick, black!50] (-0.5, 1.8) -- (5.5, 1.8);
  \node[fill=white, inner sep=2pt, font=\scriptsize, align=center] at (2.5, 1.8) {\textbf{Complete Isolation}\\No Shared Resources};

  % Impact Annotation
  \node[align=left, font=\scriptsize, color=FailColor!80!black] at (5, 0) {Failure constrained\\to Shared Pool};
  \node[align=left, font=\scriptsize, color=GoldColor!50!black] at (5, 3) {Gold Tier\\unaffected};

\end{tikzpicture}
```
:::

Deployment-level bulkheads provide complete isolation for premium tenants, ensuring that their performance is never affected by other workloads. The tradeoff is lower overall resource utilization and increased operational overhead from managing dedicated infrastructure.

**Request-level bulkheads** limit the fraction of resources any single request can consume.

```
Per-request limits:
  max_input_tokens: 8,000
  max_output_tokens: 2,000
  max_execution_time: 30s

Prevents single request from consuming excessive resources.
```

**Failure isolation**: Errors in one tenant's requests do not affect others.

```
Tenant A sends malformed input causing model error:
  Without bulkhead: Error may crash shared inference worker
  With bulkhead: Error caught, only Tenant A's request fails
                 Other tenants continue normally
```

::: {.callout-note title="Bulkhead Configuration for API Tiers"}

Consider an LLM API with three service tiers:

**Enterprise tier**:

- Dedicated GPU pool (no sharing)
- Custom model fine-tuning
- 99.9% availability SLO
- Price: $$$

**Professional tier**:

- Shared GPU pool with guaranteed capacity
- Priority scheduling over free tier
- 99.5% availability SLO
- Price: $$

**Free tier**:

- Shared GPU pool, best-effort
- Rate limited (10 QPS)
- No SLO guarantee
- Price: Free

**Bulkhead configuration**:

```{.yaml}
tiers:
  enterprise:
    gpu_pool: "dedicated"
    isolation: "hardware"
    replicas: 8
    preemption: false

  professional:
    gpu_pool: "shared-premium"
    isolation: "resource-quota"
    quota_fraction: 0.7  # 70% of shared pool
    preemption: true

  free:
    gpu_pool: "shared-premium"
    isolation: "resource-quota"
    quota_fraction: 0.3  # 30% of shared pool
    preemption: false  # can be preempted
```

:::

### Model Isolation {#sec-inference-model-isolation}

When multiple models run on shared infrastructure, additional isolation is needed:

**Memory isolation**: Ensure one model's memory usage does not impact others.

```
GPU memory partitioning (80GB H100):

Model A: 40GB reserved (50%)
Model B: 30GB reserved (37.5%)
Shared pool: 10GB (12.5%)
```

**Compute isolation**: GPU time-sharing between models introduces latency variance. Options include:

- **MIG (Multi-Instance GPU)**[^fn-mig]: Hardware partitioning of A100/H100 into isolated GPU instances

[^fn-mig]: **Multi-Instance GPU (MIG)**: NVIDIA technology introduced with Ampere (A100) that partitions a GPU into up to 7 isolated instances, each with dedicated memory bandwidth and L2 cache. Unlike time-slicing, MIG provides hardware-level isolation with guaranteed performance, enabling secure multi-tenancy on expensive datacenter GPUs.
- **Time-slicing**: Cooperative scheduling between models (higher overhead)
- **Dedicated GPUs**: Each model gets dedicated hardware (lower utilization)

**Model loading isolation**: Loading one model should not evict another from GPU memory.

```python
class ModelManager:
    def load_model(self, model_id, priority):
        required_memory = get_model_size(model_id)
        available = get_free_gpu_memory()

        if required_memory > available:
            # Check if eviction would violate isolation
            evictable = get_evictable_memory(priority)
            if required_memory > evictable:
                raise InsufficientMemoryError(
                    "Cannot load without violating isolation constraints"
                )
            evict_lower_priority_models(priority, required_memory)

        load_to_gpu(model_id)
```

### Observability for Multi-Tenancy {#sec-inference-multitenancy-observability}

Effective multi-tenancy requires per-tenant visibility into resource consumption and performance:

**Per-tenant metrics**:

- Request count, latency distribution (P50, P95, P99)
- GPU memory usage, KV cache utilization
- Throttling events, preemption counts
- Error rates by error type

**Alerting thresholds**:

```yaml
alerts:
  - name: tenant_slo_violation
    condition: p99_latency > slo_target * 1.1
    for: 5m
    severity: warning

  - name: tenant_quota_exhaustion
    condition: usage > quota * 0.9
    for: 1m
    severity: warning

  - name: noisy_neighbor_detection
    condition: usage > fair_share * 2.0
    for: 5m
    severity: info
```

**Chargeback and attribution**: Track resource consumption for billing and capacity planning.

```
Tenant A monthly report:
  Total requests: 10,000,000
  GPU-seconds consumed: 50,000
  KV cache GB-hours: 2,500
  Network egress GB: 100

  Billed: $X based on consumption
```

## Autoscaling {#sec-inference-autoscaling}

Multi-tenancy, examined in the previous section, addresses how to share fixed infrastructure capacity across multiple workloads efficiently. Autoscaling addresses the complementary problem: how to adjust that capacity dynamically as aggregate demand changes.

Production inference systems experience traffic fluctuations that make static provisioning inefficient. Autoscaling dynamically adjusts capacity to match demand, reducing costs during low-traffic periods while maintaining SLOs during peaks. Without autoscaling, operators must choose between overprovisioning (paying for idle capacity during quiet periods) or underprovisioning (violating SLOs during traffic spikes). With effective autoscaling, infrastructure follows demand, though the cold start problem unique to GPU-based serving makes this significantly more challenging than traditional web service scaling.

This section examines autoscaling strategies for inference, with particular attention to the cold start problem and techniques for reducing its impact.

### Scaling Dimensions {#sec-inference-scaling-dimensions}

Inference systems can scale along multiple dimensions. @tbl-scaling-dimensions contrasts horizontal, vertical, and batch size scaling:

**Horizontal scaling (replicas)**: Add or remove model replicas to adjust throughput.

$$\text{Capacity} = \text{Replicas} \times \text{Per-replica throughput}$$

**Vertical scaling (GPU type)**: Use more powerful GPUs for higher per-replica throughput.

$$\text{Cost efficiency} = \frac{\text{Throughput}}{\text{GPU cost}}$$

**Batch size scaling**: Adjust batch sizes to trade latency for throughput.

$$\text{Latency} \uparrow \text{ as } \text{Batch size} \uparrow \text{, Throughput} \uparrow$$

+-------------------------------+-------------+------------+--------------------------+
| **Scaling Type**              | **Latency** | **Cost**   | **Speed**                |
+:==============================+:============+:===========+:=========================+
| **Horizontal (add replicas)** | Unchanged   | Linear     | Slow (minutes)           |
| **Batch size**                | Increases   | Unchanged  | Instant                  |
| **Vertical (better GPU)**     | Unchanged   | Non-linear | Very slow (redeployment) |
+-------------------------------+-------------+------------+--------------------------+

: **Scaling Dimension Tradeoffs**: Each scaling approach has different characteristics. {#tbl-scaling-dimensions}

### The Cold Start Problem {#sec-inference-cold-start}

Cold start latency represents a significant challenge for serverless inference, with model loading often dominating the total startup time.

Unlike stateless web services that start in seconds, inference services have significant startup latency as defined in @eq-cold-start-time:

$$T_{cold start} = T_{provision} + T_{load} + T_{warmup}$$ {#eq-cold-start-time}

**Provisioning time** ($T_{provision}$): Acquiring a GPU instance takes 30 seconds to several minutes depending on cloud provider and GPU type.[^fn-cold-start]

[^fn-cold-start]: **Cold start in ML vs. serverless**: The cold start problem for GPU inference is 10-100x worse than for serverless functions. Lambda functions cold start in 100ms-1s; GPU inference cold starts in 1-10 minutes due to GPU allocation, model loading, and CUDA initialization. This fundamental difference makes predictive scaling and warm pools essential for GPU workloads.

**Model loading time** ($T_{load}$): Loading model weights from storage to GPU memory. For large models:

+------------------+---------------------+--------------------+
| **Model Size**   | **Load Time (SSD)** | **Load Time (S3)** |
+=================:+====================:+===================:+
| **7B (14GB)**    | 5s                  | 30s                |
| **70B (140GB)**  | 45s                 | 5min               |
| **175B (350GB)** | 2min                | 12min              |
+------------------+---------------------+--------------------+

**Warmup time** ($T_{warmup}$): First inference after loading is slower due to:

- JIT compilation of kernels
- CUDA context initialization
- Memory pool allocation
- Cache population

Warmup typically requires 10-30 dummy inferences, adding 5-30 seconds.

::: {.callout-note title="Cold Start Timeline for Llama-70B"}

Bringing up a new replica for Llama-70B on H100:

+-------------------------------+------------------+----------------+
| **Phase**                     | **Duration**     | **Cumulative** |
+:==============================+=================:+===============:+
| **Cloud API request**         | 5s               | 5s             |
| **GPU instance provisioning** | 60s              | 65s            |
| **Container startup**         | 10s              | 75s            |
| **Model download (S3)**       | 180s             | 255s           |
| **Model load to GPU**         | 45s              | 300s           |
| **CUDA warmup**               | 15s              | 315s           |
| **Readiness probe pass**      | 5s               | 320s           |
| **Total cold start**          | **5 min 20 sec** |                |
+-------------------------------+------------------+----------------+

**Implication**: Scaling decisions must anticipate demand 5+ minutes in advance. Reactive scaling alone cannot handle sudden traffic spikes.

@fig-cold-start-breakdown visualizes the cumulative timeline:

:::

::: {#fig-cold-start-breakdown fig-env="figure" fig-pos="htb" fig-cap="**Anatomy of a Cold Start**. Bringing up a new GPU inference replica is a multi-step process taking minutes. While container startup is fast, provisioning the specialized instance and downloading massive model weights (100GB+) dominate the timeline. CUDA context initialization and \"warmup\" inference passes add further delay. This 5+ minute lag makes purely reactive scaling dangerous for handling sudden traffic spikes." fig-alt="Horizontal stacked bar timeline from 0 to 5 minutes. Four segments: Provision (0-1m), Model Download (1-4m, largest), Load (4-4.75m), Warmup (4.75-5.25m). Brace above shows total cold start exceeds 5 minutes."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, xscale=0.025, yscale=0.8]
  % xscale: 1cm = 40 seconds. Total 320s = 8cm
  \definecolor{ProvColor}{RGB}{240,230,140}
  \definecolor{DownColor}{RGB}{135,206,235}
  \definecolor{LoadColor}{RGB}{144,238,144}
  \definecolor{WarmColor}{RGB}{255,160,122}

  % Axis
  \draw[->] (0, 0) -- (330, 0) node[right] {Seconds};
  \foreach \x/\label in {0/0s, 60/1m, 120/2m, 180/3m, 240/4m, 300/5m} {
      \draw (\x, 0.1) -- (\x, -0.1) node[below, font=\scriptsize] {\label};
  }

  % Bar Stack
  \draw[fill=ProvColor] (0, 1) rectangle (65, 2);
  \node[font=\tiny] at (32, 1.5) {Provision};

  \draw[fill=DownColor] (65, 1) rectangle (245, 2);
  \node[font=\tiny] at (155, 1.5) {Model Download (Big!)};

  \draw[fill=LoadColor] (245, 1) rectangle (290, 2);
  \node[font=\tiny] at (267, 1.5) {Load};

  \draw[fill=WarmColor] (290, 1) rectangle (315, 2);
  \node[font=\tiny] at (302, 1.5) {Warm};

  % Connect total
  \draw[decorate, decoration={brace, amplitude=5pt, raise=5pt}] (0, 2) -- (315, 2) node[midway, above=10pt, font=\bfseries] {Total Cold Start: > 5 Minutes};

  % Legend
  \node[anchor=west, font=\scriptsize] at (0, 3.5) {\textbf{Breakdown for 70B Model}};

\end{tikzpicture}
```
:::

### Reactive Scaling {#sec-inference-reactive-scaling}

Reactive scaling adjusts capacity based on observed metrics:

**Metric-based scaling**:

```yaml
autoscaling:
  metric: cpu_utilization  # or gpu_utilization, queue_depth
  target_value: 70%
  scale_up_threshold: 80%
  scale_down_threshold: 50%
  cooldown_period: 300s
```

**Queue-depth scaling**: Scale based on request queue length.

$$\text{Desired replicas} = \left\lceil \frac{\text{Queue depth}}{\text{Queue target}} \times \text{Current replicas} \right\rceil$$

**Latency-based scaling**: Scale to maintain latency SLO.

$$\text{Desired replicas} = \left\lceil \frac{P99_{observed}}{P99_{target}} \times \text{Current replicas} \right\rceil$$

**Reactive scaling limitations**:

1. Response delay: Cold start time prevents rapid response
2. Oscillation: Can create scale-up/scale-down cycles
3. Over-provisioning: Must provision for worst-case during cold start

::: {.callout-note title="Reactive Scaling Response Analysis"}

Consider traffic spike from 1000 to 3000 QPS:

**Current state**: 10 replicas, 100 QPS each, 70% utilization

**Target state**: 30 replicas for 3000 QPS

**Without pre-warming**:

- T=0: Spike detected, scale-up triggered
- T=0 to T=5min: Cold start for 20 new replicas
- T=0 to T=5min: Existing 10 replicas handle 3000 QPS (300 QPS each)
- Utilization: 210% (overloaded)
- P99 latency: 500ms+ (SLO violated)

**With pool of warm spares (5 replicas)**:

- T=0: Spike detected, warm spares activated immediately
- T=0: 15 replicas handle 3000 QPS (200 QPS each)
- T=0 to T=5min: Scale up 15 more replicas
- Utilization: 140% (elevated but manageable)
- P99 latency: 80ms (SLO maintained)

Warm spares provide buffer during cold start period.

:::

### Predictive Scaling {#sec-inference-predictive-scaling}

Predictive scaling anticipates demand before it occurs, initiating scaling ahead of traffic changes.

**Time-series forecasting**: Use historical patterns to predict future demand.

```python
def predict_demand(current_time, history):
    # Seasonal decomposition
    daily_pattern = extract_daily_seasonality(history)
    weekly_pattern = extract_weekly_seasonality(history)

    # Trend estimation
    trend = estimate_trend(history)

    # Forecast
    predicted = (
        daily_pattern[current_time.hour]
        * weekly_pattern[current_time.weekday()]
        * trend
    )
    return predicted
```

**Event-driven scaling**: Scale proactively for known events.

```yaml
scheduled_scaling:
  - event: "product_launch"
    time: "2024-03-15 09:00 UTC"
    target_replicas: 50  # 5x normal
    ramp_up: 30min  # Start scaling 30min before

  - event: "weekly_newsletter"
    cron: "0 10 * * 1"  # Every Monday 10am
    target_replicas: 20  # 2x normal
    duration: 2h
```

**Hybrid approach**: Combine predictive baseline with reactive adjustment.

$$\text{Target replicas} = \max(\text{Predicted}, \text{Reactive}) + \text{Buffer}$$

::: {.callout-note title="Predictive Scaling for Daily Traffic Patterns"}

A chatbot service shows predictable daily patterns:

+----------------+-----------------+---------------------+
| **Time (UTC)** | **Typical QPS** | **Replicas Needed** |
+===============:+================:+====================:+
| 00:00-06:00    | 500             | 5                   |
| 06:00-09:00    | 1500            | 15 (ramp up)        |
| 09:00-17:00    | 3000            | 30 (peak)           |
| 17:00-20:00    | 2000            | 20 (ramp down)      |
| 20:00-00:00    | 1000            | 10                  |
+----------------+-----------------+---------------------+

**Predictive schedule** (accounting for cold start):

+----------+--------------+---------------------+-----------------------+
| **Time** | **Action**   | **Replicas Active** | **Replicas Starting** |
+=========:+:=============+====================:+======================:+
| 05:30    | Scale up     | 5                   | +10 warming           |
| 06:00    | Traffic ramp | 15                  | -                     |
| 08:30    | Scale up     | 15                  | +15 warming           |
| 09:00    | Peak traffic | 30                  | -                     |
| 17:00    | Scale down   | 20                  | -10 terminating       |
| 20:00    | Scale down   | 10                  | -10 terminating       |
| 00:00    | Scale down   | 5                   | -5 terminating        |
+----------+--------------+---------------------+-----------------------+

**Cost comparison**:

- Reactive only: Must over-provision during ramp (45 replicas peak)
- Predictive: Right-sized provisioning (30 replicas peak)
- Savings: 33% GPU cost reduction

:::

::: {.callout-note title="Figure: Predictive vs. Reactive Scaling" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.8]
  % Axes
  \draw[->, thick] (0,0) -- (10,0) node[right] {Time};
  \draw[->, thick] (0,0) -- (0,5) node[above] {Load / Capacity};

  % Traffic Curve (Sine-like spike)
  \draw[gray, ultra thick, dashed, domain=0:9, samples=100] plot (\x, {1 + 3*exp(-(\x-5)^2/2)});
  \node[gray, anchor=west] at (6, 4.2) {Actual Traffic};

  % Reactive Capacity (Lagging)
  \draw[red, thick] (0,1.2) -- (5.2,1.2) -- (5.2,4.2) -- (9,4.2);
  \node[red, anchor=west] at (5.5, 1.5) {Reactive (Late)};

  % Predictive Capacity (Early)
  \draw[blue, thick] (0,1.2) -- (3.5,1.2) -- (4.5,4.2) -- (9,4.2);
  \node[blue, anchor=west] at (2, 4.5) {Predictive (On Time)};

  % Deficit Area
  \fill[red!20, opacity=0.5] (5, 1.2) -- (5.2, 1.2) -- (5.2, 4.2) -- (5, 4.2) -- cycle;
  \node[red, font=\tiny, rotate=90] at (5.1, 2.7) {SLO VIOLATION};

\end{tikzpicture}
```
**Predictive vs. Reactive Scaling**. Reactive scaling (red line) responds to traffic spikes after they occur, leading to periods of under-provisioning where SLOs are violated due to cold start latency. Predictive scaling (blue line) anticipates traffic and begins provisioning capacity *before* the spike arrives, ensuring consistent performance.
:::

### Warm Pool Management {#sec-inference-warm-pools}

Maintaining a pool of pre-warmed replicas reduces effective cold start time:

**Warm pool sizing**:

$$\text{Warm pool size} = \frac{\text{Max expected spike}}{\text{Per-replica throughput}} \times \text{Headroom factor}$$

For example, if max spike is 2x normal and headroom factor is 1.5:

$$\text{Warm pool} = 2 \times 1.5 = 3\text{x minimum pool capacity}$$

**Warm pool cost**: Maintaining warm replicas costs money even when idle.

$$\text{Warm pool cost} = \text{Pool size} \times \text{GPU cost/hour} \times \text{Idle fraction}$$

Trade-off: More warm replicas = faster response but higher cost.

**Tiered warm pools**: Different readiness levels with different costs.

+----------+-----------------------------+-------------------+---------------------+
| **Tier** | **State**                   | **Response Time** | **Cost (relative)** |
+:=========+:============================+:==================+====================:+
| **Hot**  | GPU loaded, running         | Instant           | 100%                |
| **Warm** | GPU allocated, model loaded | 30s               | 60%                 |
| **Cold** | GPU not allocated           | 5+ min            | 0%                  |
+----------+-----------------------------+-------------------+---------------------+

```
Pool configuration:
  Hot: 2 replicas (instant burst capacity)
  Warm: 5 replicas (30s activation)
  Cold: Unlimited (cloud provider)

Scaling sequence:
  1. Activate Hot replicas immediately
  2. Activate Warm replicas within 30s
  3. Cold start new replicas if demand persists
```

### Scaling Response Time Analysis {#sec-inference-scaling-response}

The total time to respond to a scaling event is given by @eq-scaling-response:

$$T_{response} = T_{detect} + T_{decide} + T_{provision} + T_{warmup}$$ {#eq-scaling-response}

+------------------+--------------+-------------------------+
| **Component**    | **Duration** | **Optimization**        |
+:=================+=============:+:========================+
| **Detection**    | 10-60s       | Reduce metrics interval |
| **Decision**     | 1-5s         | Faster autoscaler       |
| **Provisioning** | 30s-5min     | Warm pools              |
| **Warmup**       | 5-30s        | Pre-compilation         |
+------------------+--------------+-------------------------+

**Optimizing each component**:

**Detection speed**: Use high-frequency metrics (1s vs 60s intervals) for faster detection. Trade-off: More metric volume, potentially noisier signals.

**Decision speed**: Pre-compute scaling plans based on predicted scenarios. When trigger occurs, execute pre-computed plan immediately.

**Provisioning speed**: Warm pools eliminate provisioning for anticipated demand. Spot/preemptible instances can reduce provisioning time (already running, just need allocation).

**Warmup speed**: Pre-compiled TensorRT engines skip JIT compilation. Lazy loading defers some initialization to first request.

### Spot and Preemptible Instances {#sec-inference-spot-instances}

Cloud providers offer discounted GPU instances[^fn-spot] that can be reclaimed with short notice:

[^fn-spot]: **Spot instance economics**: Spot instances represent cloud providers selling unused capacity at steep discounts (60-90%). The catch is 30-second to 2-minute termination notice. For inference, this means designing for graceful degradation: in-flight requests may need to be re-routed or failed, and KV cache state is lost. The cost savings justify this complexity for non-critical or burst traffic.

+----------------------+--------------+-------------------------+-----------------+
| **Instance Type**    | **Discount** | **Interruption Notice** | **Use Case**    |
+:=====================+=============:+:========================+:================+
| **On-demand**        | 0%           | Never                   | SLO-critical    |
| **Reserved**         | 30-60%       | Never                   | Steady baseline |
| **Spot/Preemptible** | 60-90%       | 30s-2min                | Burst capacity  |
+----------------------+--------------+-------------------------+-----------------+

**Graceful handling of spot termination**:

```python
def handle_spot_termination():
    # Received 2-minute warning
    # 1. Stop accepting new requests
    stop_accepting_requests()

    # 2. Complete in-flight requests (if possible)
    await complete_inflight(timeout=90)

    # 3. Save state for resumption elsewhere
    save_kv_cache_to_storage()

    # 4. Signal load balancer to redirect traffic
    deregister_from_loadbalancer()

    # 5. Terminate gracefully
    shutdown()
```

**Spot-aware architecture**:

```
Traffic distribution:

Request arrives
    │
    ▼
Load balancer
    │
    ├── 70% → On-demand replicas (guaranteed capacity)
    │
    └── 30% → Spot replicas (cost savings, may be interrupted)
```

Best-effort requests route to spot instances; SLO-critical requests use on-demand.

## Global Inference Infrastructure {#sec-inference-global}

Production inference systems serving global user bases must operate across multiple geographic regions. A user in Tokyo expects low-latency responses regardless of where models were trained or where the company headquarters is located. This section examines the architectural patterns for multi-region inference deployment.

### Why Multi-Region Matters {#sec-inference-global-why}

Single-region deployment creates fundamental limitations:

**Latency floor**: Network round-trip time (RTT) to distant users cannot be optimized away:

+-------------------+--------------------+-------------------------+
| **User Location** | **RTT to US-East** | **RTT to Local Region** |
+:==================+===================:+========================:+
| **New York**      | 10ms               | 10ms                    |
| **London**        | 75ms               | 10ms                    |
| **Tokyo**         | 150ms              | 10ms                    |
| **Sydney**        | 200ms              | 10ms                    |
+-------------------+--------------------+-------------------------+

For interactive applications (chatbots, autocomplete), these delays compound across multiple model calls per request.

**Availability**: Single-region deployment creates a single point of failure. Cloud region outages, while rare, affect all users simultaneously.

**Regulatory compliance**: Data residency requirements (GDPR, data sovereignty laws) may require processing user data within specific geographic boundaries.

### Multi-Region Architecture Patterns {#sec-inference-global-patterns}

**Pattern 1: Global load balancing with regional replicas**

```
                    Global Load Balancer
                    (Latency-based routing)
                           │
         ┌─────────────────┼─────────────────┐
         ▼                 ▼                 ▼
    US-East           EU-West           Asia-Pacific
    ┌─────────┐       ┌─────────┐       ┌─────────┐
    │ vLLM    │       │ vLLM    │       │ vLLM    │
    │ Replicas│       │ Replicas│       │ Replicas│
    └─────────┘       └─────────┘       └─────────┘
         │                 │                 │
         └─────────────────┼─────────────────┘
                           ▼
                    Model Registry
                    (Synchronized)
```

Each region runs independent inference replicas with identical models. The global load balancer routes users to the nearest region based on latency.

**Key considerations**:

- **Model synchronization**: Model updates must propagate to all regions. Options include:
  - Push-based: Central registry pushes to all regions (simple, potential inconsistency window)
  - Pull-based: Regions poll for updates (higher latency, guaranteed consistency)
  - Hybrid: Push notification + pull verification

- **Version consistency**: During model rollouts, different regions may briefly serve different versions. For most applications this is acceptable; for applications requiring strict consistency, implement version pinning in request routing.

**Pattern 2: Edge caching with central inference**

For models too large to replicate globally, cache responses at the edge:

```
User → Edge Cache (CDN) → Regional Proxy → Central Inference
           │                    │
           └── Cache hit ───────┘
               (< 10ms)

           └── Cache miss ──────────────────→
               (Full latency, populate cache)
```

**Effectiveness depends on request repeatability**:

+---------------------+--------------------+-----------------+
| **Workload**        | **Cache Hit Rate** | **Suitability** |
+:====================+===================:+:================+
| **Autocomplete**    | 60-80%             | Excellent       |
| **FAQ chatbot**     | 40-60%             | Good            |
| **Open-ended chat** | 5-15%              | Poor            |
| **Code generation** | 20-40%             | Moderate        |
+---------------------+--------------------+-----------------+

Semantic caching[^fn-semantic-cache] (caching based on embedding similarity rather than exact match) can improve hit rates for open-ended workloads.

[^fn-semantic-cache]: **Semantic caching**: Unlike traditional exact-match caching, semantic caching returns cached responses for queries that are semantically similar to previously seen queries. This requires embedding the query and searching a vector database for similar cached queries. The trade-off is increased complexity and potential for incorrect cache hits when semantically similar queries should produce different responses.

**Pattern 3: Federated inference with model sharding**

For the largest models, shard across regions:

```
User request
    │
    ▼
Request Router
    │
    ├── Layers 1-40  → US-East GPUs
    │
    └── Layers 41-80 → EU-West GPUs

    Pipeline parallelism across regions
```

This pattern is rarely practical due to inter-region latency dominating compute time, but may apply for extremely large models where no single region has sufficient GPU capacity.

### Cross-Region Failover {#sec-inference-global-failover}

When a region becomes unavailable, traffic must reroute to healthy regions:

**Active-active failover**:

```python
# Simplified global routing logic
def route_request(user_region, request):
    primary = get_nearest_healthy_region(user_region)
    secondary = get_second_nearest_healthy_region(user_region)

    try:
        return call_region(primary, request, timeout=2.0)
    except (Timeout, RegionUnavailable):
        # Failover with increased latency
        return call_region(secondary, request, timeout=5.0)
```

**Failover considerations for stateful LLM serving**:

- **Session affinity loss**: Users mid-conversation lose KV cache state. The fallback region must regenerate context from conversation history.
- **Capacity spike**: The receiving region sees sudden traffic increase. Pre-provision headroom (typically 30-50% over steady-state) or accept degraded latency during failover.
- **Gradual recovery**: When the failed region recovers, gradually shift traffic back to avoid oscillation.

### Global Model Deployment {#sec-inference-global-deployment}

Deploying model updates across regions requires careful coordination:

**Phased rollout strategy**:

```
1. Deploy to canary region (e.g., 1% traffic in US-East)
2. Monitor metrics for 1 hour
3. If healthy, deploy to remaining US-East replicas
4. Monitor for 4 hours
5. Deploy to EU-West (different user population)
6. Monitor for 4 hours
7. Deploy to Asia-Pacific
8. Complete rollout
```

**Rollback across regions**:

If issues are detected after partial deployment:

```
Region Status:
  US-East:      v2.1 (new) ← Issue detected
  EU-West:      v2.0 (old)
  Asia-Pacific: v2.0 (old)

Action: Rollback US-East to v2.0
  - Switch traffic to v2.0 replicas
  - Maintain v2.1 replicas for debugging
  - Do not proceed with EU-West deployment
```

**Metrics for global deployment health**:

+-------------------+----------------+---------------------------+
| **Metric**        | **Per-Region** | **Global**                |
+:==================+:===============+:==========================+
| **Error rate**    | &lt; 0.1%      | &lt; 0.1%                 |
| **P99 latency**   | &lt; target    | &lt; 2x single-region     |
| **Throughput**    | Stable         | Stable                    |
| **Model quality** | Within bounds  | Consistent across regions |
+-------------------+----------------+---------------------------+

### Cost Optimization Across Regions {#sec-inference-global-cost}

GPU pricing varies by region. Optimize placement for cost while meeting latency requirements:

+-------------+---------------------+---------------+-------------------------+
| **Region**  | **H100 Spot Price** | **On-Demand** | **Latency to US Users** |
+:============+====================:+==============:+========================:+
| **US-East** | $2.50/hr            | $4.00/hr      | 10-50ms                 |
| **US-West** | $2.30/hr            | $3.80/hr      | 30-70ms                 |
| **EU-West** | $2.80/hr            | $4.20/hr      | 75-100ms                |
+-------------+---------------------+---------------+-------------------------+

**Cost-aware routing**:

For latency-tolerant workloads (batch inference, background processing), route to the cheapest available region:

```python
def route_batch_request(request):
    if request.priority == "low":
        # Route to cheapest region with capacity
        return get_cheapest_region_with_capacity()
    else:
        # Route to nearest region
        return get_nearest_region(request.user_location)
```

This can reduce costs by 20-40% for batch workloads while maintaining SLOs for interactive traffic.

## Case Studies {#sec-inference-case-studies}

The techniques presented throughout this chapter come together in production systems serving billions of requests daily. This section examines four case studies that illustrate different points in the inference design space: Meta's recommendation serving (high volume, low latency), OpenAI's API infrastructure (LLM-focused), Google's search ranking (ensemble models), and TikTok's multimodal recommendation (video understanding combined with user modeling).

Each case study demonstrates how the principles of batching, sharding, load balancing, and autoscaling combine to meet specific requirements.

### Meta Recommendation Serving {#sec-inference-case-meta}

Meta's recommendation infrastructure serves predictions for feeds, ads, and content ranking across Facebook, Instagram, WhatsApp, and Messenger. This represents one of the largest production inference deployments in the world.

**Scale and requirements**:

- Request volume: Billions of requests per day
- Latency target: <10ms P99
- Model diversity: Hundreds of model variants
- Feature cardinality: Trillions of unique entities

**Architecture overview**:

```
User request → Feature collection → Embedding lookup → Model inference → Response

                    │                     │                  │
                    ▼                     ▼                  ▼
             Feature Store        Embedding Servers      GPU Inference
             (CPU, DRAM)         (CPU + SSD, 1000s)     (GPU, 100s)
```

**Key design decisions**:

**Embedding sharding at scale**: Embedding tables total over 100TB, requiring 1000+ shards. Meta uses a hybrid sharding strategy:

- Hot embeddings (top 1%): Replicated across memory on all inference servers
- Warm embeddings (next 10%): Column-sharded with 8-way parallelism
- Cold embeddings (remaining 89%): Row-sharded with consistent hashing, SSD-backed

This reduces embedding lookup latency from 50ms (naive) to 2ms through batching and locality optimization.

**Feature-parallel batching**: Instead of batching entire requests, Meta batches at the feature level. Each inference request triggers 5,000+ embedding lookups, but these lookups are batched across requests within a 1ms window. This achieves 90%+ memory bandwidth utilization on embedding servers.

**GPU-CPU hybrid architecture**: Dense model computation (ranking towers) runs on GPUs, while sparse embedding lookups run on CPU servers with large memory and SSD storage. This matches hardware to workload characteristics:

+------------------------+--------------+-------------+----------------+
| **Component**          | **Hardware** | **Latency** | **Throughput** |
+:=======================+:=============+============:+===============:+
| **Embedding lookup**   | CPU + SSD    | 2ms         | 50M lookups/s  |
| **Feature processing** | CPU          | 1ms         | 10M ops/s      |
| **Dense ranking**      | GPU          | 1.5ms       | 100K infs/s    |
+------------------------+--------------+-------------+----------------+

**Lessons learned**:

1. Embedding lookup, not model inference, often dominates latency for recommendation systems
2. Feature-parallel batching achieves higher efficiency than request-level batching
3. Hybrid CPU-GPU architectures match hardware to workload characteristics

### OpenAI API Infrastructure {#sec-inference-case-openai}

OpenAI's API serves GPT-4, GPT-3.5-turbo, and other models to millions of developers. The infrastructure must handle highly variable request sizes (from 10 tokens to 128K tokens) while maintaining quality of service across diverse workloads.

**Scale and requirements**:

- Request volume: Millions of requests per hour
- Latency target: Time-to-first-token (TTFT) <2s, throughput varies by model
- Model sizes: 7B to 175B+ parameters
- Context lengths: Up to 128K tokens

**Architecture overview**:

```
API Gateway → Rate Limiting → Request Router → Model Cluster → Response Streaming

                                     │
                                     ▼
                              ┌─────────────┐
                              │ Model Pool  │
                              │ ┌─────────┐ │
                              │ │ GPT-4   │ │
                              │ │ 8xH100  │ │
                              │ └─────────┘ │
                              │ ┌─────────┐ │
                              │ │GPT-3.5  │ │
                              │ │ 4xA100  │ │
                              │ └─────────┘ │
                              └─────────────┘
```

**Key design decisions**:

**Continuous batching with chunked prefill**: OpenAI was an early adopter of continuous batching (Orca-style) to maintain high GPU utilization despite variable output lengths. Chunked prefill bounds decode latency by processing long prompts in chunks that interleave with ongoing generation.

+--------------------------+---------------------+------------------------+
| **Batching Strategy**    | **GPU Utilization** | **TTFT (128K prompt)** |
+:=========================+====================:+=======================:+
| **Static batching**      | 45%                 | 30s (blocked)          |
| **Continuous batching**  | 75%                 | 30s (blocked)          |
| **Continuous + chunked** | 85%                 | 3s (streamed)          |
+--------------------------+---------------------+------------------------+

**Tensor parallelism for large models**: GPT-4 class models require 8-way or greater tensor parallelism for memory capacity and latency:

- 8xH100 per GPT-4 shard group
- NVLink for intra-node communication
- Consistent hashing for session affinity (KV cache reuse)

**Multi-tier rate limiting**: OpenAI implements rate limiting at multiple levels to prevent noisy neighbors:

- Per-API-key request rate limits
- Per-API-key token-per-minute limits
- Organization-level capacity quotas
- Global model capacity limits

**Dynamic capacity allocation**: During peak demand, OpenAI shifts capacity between models based on queue depth:

```
if gpt4_queue_depth > threshold:
    # Migrate some GPT-3.5 capacity to GPT-4
    reallocate_cluster_capacity(from="gpt-3.5", to="gpt-4", fraction=0.2)
```

**Lessons learned**:

1. Continuous batching is essential for LLM serving at scale
2. Prefix caching provides 2-3x efficiency for conversational workloads
3. Multi-tier rate limiting prevents cascade failures from traffic spikes

### Google Search Ranking {#sec-inference-case-google}

Google Search uses ensemble serving to combine multiple specialized models for query understanding, document relevance, and result ranking. This represents a different inference pattern: many smaller models coordinated for each request rather than one large model.

**Scale and requirements**:

- Request volume: Billions of searches per day
- Latency target: <200ms end-to-end
- Model count: Dozens of models per query
- Result processing: Thousands of documents per query

**Architecture overview**:

::: {.callout-note title="Figure: Ranking Cascade Architecture" collapse="false"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
  \definecolor{L0Color}{RGB}{240,240,240}
  \definecolor{L1Color}{RGB}{220,220,220}
  \definecolor{L2Color}{RGB}{200,200,200}
  \definecolor{L3Color}{RGB}{180,180,180}

  \tikzset{
    stage/.style={draw=black!70, thick, align=center, minimum height=0.8cm}
  }

  % The Funnel
  \node[stage, fill=L0Color, minimum width=6cm] (L0) at (0, 3) {\textbf{Retrieval}\\1,000,000 → 10,000 items\\Cheap: Embeddings, BM25};
  \node[stage, fill=L1Color, minimum width=4.5cm] (L1) at (0, 2) {\textbf{L1 Ranking}\\10,000 → 1,000 items\\Fast: Linear Models, Decision Trees};
  \node[stage, fill=L2Color, minimum width=3cm] (L2) at (0, 1) {\textbf{L2 Ranking}\\1,000 → 100 items\\Moderate: Small Neural Nets};
  \node[stage, fill=L3Color, minimum width=1.5cm] (L3) at (0, 0) {\textbf{Final Rank}\\100 → 10 items\\Expensive: Large Transformers};

  % Down arrows
  \draw[->, ultra thick, gray!60] (L0) -- (L1);
  \draw[->, ultra thick, gray!60] (L1) -- (L2);
  \draw[->, ultra thick, gray!60] (L2) -- (L3);

  \node[anchor=west, font=\scriptsize, text=red!80] at (3.2, 3) {Low Precision, High Recall};
  \node[anchor=west, font=\scriptsize, text=green!60!black] at (1.0, 0) {High Precision, Low Recall};

\end{tikzpicture}
```
**Ranking Cascade Architecture**. To optimize latency and cost, search and recommendation systems use a cascade of increasingly complex models. Early stages (Retrieval) filter millions of candidates down to thousands using fast, cheap heuristics. Later stages use expensive, high-precision models only on the most promising candidates.
:::

**Key design decisions**:

**Cascading model architecture**: Rather than running one expensive model on all candidates, Google uses a ranking cascade:

+----------------------+----------------------+--------------------+--------------------+
| **Stage**            | **Model Complexity** | **Candidates**     | **Latency Budget** |
+=====================:+:=====================+===================:+===================:+
| **L0 (Retrieval)**   | Embedding lookup     | 1,000,000 → 10,000 | 10ms               |
| **L1 (First pass)**  | Linear model         | 10,000 → 1,000     | 20ms               |
| **L2 (Second pass)** | Small transformer    | 1,000 → 100        | 50ms               |
| **L3 (Final rank)**  | Large ensemble       | 100 → 10           | 100ms              |
+----------------------+----------------------+--------------------+--------------------+

This achieves 100x cost reduction compared to running L3 on all candidates.

**Speculative execution**: Given tight latency budgets, Google uses speculative execution for model ensembles:

```
# Instead of sequential:
#   q1 = model1(query)
#   q2 = model2(query)
#   q3 = model3(query, q1, q2)

# Speculative parallel:
async_q1 = async model1(query)
async_q2 = async model2(query)
async_q3 = async model3(query, predicted_q1, predicted_q2)

# Use actual results if they arrive in time, otherwise use speculative
```

**Custom TPU infrastructure**: Google runs ranking models on TPUs[^fn-tpu-search] optimized for transformer inference. TPU pods provide:

[^fn-tpu-search]: **TPU for inference**: While TPUs are often associated with training, Google's search infrastructure leverages TPU's deterministic performance and high memory bandwidth for inference. Unlike GPUs where utilization varies with batching, TPUs provide consistent latency through their systolic array architecture, critical for meeting strict search latency SLOs.

- 2D mesh topology for efficient AllReduce
- High memory bandwidth for attention operations
- Custom quantization for serving efficiency

**Deadline-aware scheduling**: Each sub-request carries a deadline, and workers prioritize by deadline proximity:

```
Worker queue: [Doc1: 50ms left] [Doc2: 30ms left] [Doc3: 80ms left]
                                      ↑ Process first

If deadline will be missed:
  Return cached/default result rather than timing out
```

**Lessons learned**:

1. Ranking cascades provide dramatic cost reduction for large candidate sets
2. Deadline propagation and priority scheduling are essential for ensemble serving
3. Custom hardware (TPU) enables efficiency that commodity GPUs cannot match

### TikTok Multimodal Recommendation {#sec-inference-case-tiktok}

TikTok's recommendation system combines video understanding (vision) with user modeling (recommendation) for personalized content ranking. This represents a multimodal inference challenge where different model types must coordinate.

**Scale and requirements**:

- Request volume: Millions of video rankings per second
- Latency target: <50ms P99
- Content volume: Millions of new videos daily
- Modalities: Video, audio, text, user signals

**Architecture overview**:

```
User request → User embedding → Candidate videos → Video understanding → Ranking

                    │                  │                    │
                    ▼                  ▼                    ▼
             User Tower          Video Cache          Vision Models
           (Transformer)        (Pre-computed)        (On-demand)
```

**Key design decisions**:

**Two-tower architecture with caching**[^fn-two-tower]: TikTok separates user understanding (online) from content understanding (offline):

[^fn-two-tower]: **Two-tower architecture**: A design pattern where user features and item features are encoded by separate "towers" (neural networks) into embeddings, then combined for scoring. This separation enables pre-computing item embeddings offline, reducing online inference to user tower computation plus a fast dot product. The architecture trades some model expressiveness for dramatic serving efficiency.

+-----------------+----------------------+-------------+--------------+
| **Tower**       | **Update Frequency** | **Latency** | **Compute**  |
+:================+:=====================+:============+:=============+
| **User tower**  | Real-time            | 5ms         | GPU (online) |
| **Video tower** | Hourly               | N/A         | GPU (batch)  |
+-----------------+----------------------+-------------+--------------+

Video embeddings are pre-computed and cached, eliminating vision inference from the critical path for most requests. Only new videos (uploaded within the hour) require online vision inference.

**Hybrid CPU-GPU inference**: Like Meta, TikTok uses CPU for embedding operations and GPU for dense model computation:

```
User features → CPU preprocessing (1ms)
             → Embedding lookup (2ms, CPU+DRAM)
             → Dense ranking (10ms, GPU)
             → Response formatting (1ms)
```

**Priority-based video analysis**: New video content is processed with different priorities:

+----------------+---------+------------------------------+
| **Priority**   | **SLA** | **Use Case**                 |
+:===============+========:+:=============================+
| **Critical**   | 5 min   | Creator with large following |
| **Standard**   | 30 min  | Normal uploads               |
| **Background** | 2 hours | Bulk/imported content        |
+----------------+---------+------------------------------+

This ensures popular creators' content reaches recommendations quickly while managing compute costs.

**Multimodal fusion**: TikTok combines multiple understanding modalities through late fusion:

```
Video embedding (512d) ─┐
Audio embedding (256d) ─┼─ Concat → Fusion MLP → Final embedding (256d)
Text embedding (256d)  ─┘
```

This allows independent updates to each modality's model without retraining the full system.

**Lessons learned**:

1. Separating online and offline components enables aggressive caching
2. Two-tower architectures scale better than joint models for user-item systems
3. Priority-based processing balances freshness against compute cost

### Cross-Cutting Observations {#sec-inference-case-observations}

Several patterns emerge across these case studies. @tbl-case-studies-summary identifies the primary technique and key innovation from each system:

**Separation of concerns**: All systems separate embedding/retrieval from ranking/generation. This enables specialized optimization for each component.

**Hybrid architectures**: No system uses GPUs exclusively. CPU+GPU combinations match hardware to workload characteristics.

**Caching at multiple levels**: Embedding caching, result caching, and intermediate representation caching all appear. Caching reduces compute at the cost of staleness.

**Progressive refinement**: Cascades and early-exit strategies reduce average compute by quickly filtering unlikely candidates.

**Deadline awareness**: All systems propagate deadlines and make explicit tradeoffs between quality and latency when under pressure.

+------------+-----------------------+---------------------------+
| **System** | **Primary Technique** | **Key Innovation**        |
+:===========+:======================+:==========================+
| **Meta**   | Embedding sharding    | Feature-parallel batching |
| **OpenAI** | Continuous batching   | Chunked prefill           |
| **Google** | Ranking cascade       | Speculative execution     |
| **TikTok** | Two-tower caching     | Multimodal fusion         |
+------------+-----------------------+---------------------------+

: **Case Study Summary**: Each system innovates on a core technique matched to its workload characteristics. {#tbl-case-studies-summary}

## Fallacies and Pitfalls {#sec-inference-fallacies-pitfalls}

The techniques presented throughout this chapter address real engineering challenges, but misconceptions about inference at scale remain common. Recognizing these fallacies and pitfalls helps practitioners avoid costly mistakes in system design and capacity planning.

**Fallacy: Inference at scale is synonymous with LLM serving.**

This misconception, reinforced by current discourse, leads to over-focus on LLM-specific techniques while ignoring the broader inference landscape. By request volume, recommendation systems constitute 80-90% of production inference at major technology companies, with vision and other models comprising most of the remainder. LLMs currently represent 1-5% of requests, though this is growing. A practitioner who only understands continuous batching and KV cache management will be unprepared for the feature-parallel batching and embedding sharding that dominate production inference. Technique selection must match the actual workload.

**Pitfall: Using training infrastructure for production serving.**

Training and serving have different requirements. Training optimizes for aggregate throughput over hours or days; serving optimizes for per-request latency under strict SLOs. Training tolerates batch sizes of thousands; serving often requires batch sizes in single digits. Training accepts checkpoint-based recovery; serving requires graceful failover without user impact. Teams that deploy training clusters for serving often discover unacceptable latency variance, poor resource utilization, and difficulty meeting SLOs. Purpose-built serving infrastructure with appropriate batching, load balancing, and autoscaling is essential.

**Fallacy: Continuous batching solves all LLM serving problems.**

Continuous batching dramatically improves GPU utilization for LLM serving, but it addresses only one dimension of the problem. Prefill remains a bottleneck for long contexts, as the quadratic attention computation cannot be avoided regardless of how subsequent decode iterations are batched. As discussed in @sec-optimization-kv-cache, KV cache memory, not compute, often limits batch size. Network bandwidth between sharded model components can dominate latency for large models. Continuous batching is necessary but not sufficient for efficient LLM serving.

**Pitfall: Sizing capacity based on average throughput.**

Queuing theory establishes that systems provisioned for average load violate SLOs during traffic peaks. At 80% average utilization, a modest 25% traffic spike pushes utilization above 100%, causing unbounded queue growth. As discussed in @sec-inference-autoscaling, the cold start problem exacerbates this: by the time new capacity is available (5+ minutes for GPU instances), SLO violations have already occurred. Capacity planning must account for peak load plus headroom, not average load.

**Fallacy: Load balancing does not matter much for inference.**

Simple load balancing strategies like round-robin seem adequate until examined quantitatively. As shown in @sec-inference-load-balancing, random assignment produces maximum queue lengths of $O(\log n / \log \log n)$ across $n$ servers. Power-of-two-choices reduces this to $O(\log \log n)$, an exponential improvement. For a 1,000-server cluster, this translates from ~4-5 requests maximum queue to ~2 requests. At the tail latencies that determine SLO compliance, this difference is substantial. The choice of load balancing algorithm has first-order impact on system performance.

**Pitfall: Ignoring the serving tax in latency budgets.**

Distributed inference introduces overhead absent from single-machine serving: network round-trips, serialization, load balancer decisions, and coordination for sharded models. This "serving tax" often consumes 10-30% of the latency budget. A team that achieves 70ms model inference on a single GPU may be surprised when end-to-end latency reaches 100ms in production due to these overheads. Latency budgets must explicitly account for distribution overhead, not just compute time.

**Fallacy: More GPU memory always means more batch size and throughput.**

While larger GPU memory enables larger batches for models that fit in memory, the bottleneck often shifts before memory is exhausted. Memory bandwidth limits throughput for bandwidth-bound operations (LLM decode). Compute limits throughput for compute-bound operations (prefill, vision inference). A 7B LLM on an H100 with 80GB memory achieves ~2,000 tokens/sec decode throughput regardless of batch size beyond 128 requests because HBM3 bandwidth (3.35 TB/s) saturates first. Adding memory to a bandwidth-bound workload provides no benefit. Understanding whether the workload is compute-bound, memory-bandwidth-bound, or capacity-bound guides appropriate resource allocation.

**Pitfall: Neglecting multi-tenancy isolation until production.**

In development and staging, single-tenant deployments work well. In production, noisy neighbors cause sudden, unpredictable performance degradation that is difficult to diagnose and resolve. A tenant bursting to 5x normal traffic can degrade latency for all other tenants on shared infrastructure. As emphasized in @sec-inference-multitenancy, resource quotas, priority scheduling, and bulkhead isolation must be designed into the system from the start, not retrofitted after production incidents.

::: {.callout-important title="Three Things to Remember"}

1. **Serving cost dominates training cost over a model's lifetime.** For high-volume applications, serving cost exceeds training cost by 100x or more. Every percentage point of serving efficiency improvement yields ongoing cost reduction. Optimize serving ruthlessly.

2. **Different model types require different batching strategies.** Static batching for vision, continuous batching for LLMs, feature-parallel batching for recommendation systems. There is no universal optimal strategy. Match technique to workload.

3. **Power-of-two-choices provides exponential load balancing improvement.** Maximum queue length improves from $O(\log n / \log \log n)$ to $O(\log \log n)$ with minimal overhead (two probes per request). This simple technique should be standard for any distributed inference deployment.

:::

## Summary {#sec-inference-summary}

Inference at scale is the final realization of the Machine Learning Fleet. Throughout Part I, we designed the logical algorithms for scale, and in Part II, we built the physical machines to execute them. This chapter has developed the service layer— the "interface to reality"—that transforms those massive computational resources into low-latency global predictions.

The serving hierarchy provides our organizing framework for these optimizations, from request-level batching and caching to replica-level GPU memory management, up to service-level load balancing and platform-level multi-tenancy. We established that the inversion from training (throughput-bound) to inference (latency-bound) necessitates a different engineering mindset, where "the tail" (P99 latency) is the primary determinant of success.

We analyzed why different model architectures require distinct batching strategies: static/dynamic for vision, feature-parallel for recommendation, and continuous batching for LLMs. We explored how model sharding techniques like tensor and expert parallelism reduce latency for frontier models, provided the underlying networking fabric discussed in @sec-communication can sustain the synchronization overhead. Finally, we examined the critical memory management challenges of autoregressive generation, where techniques like PagedAttention and speculative decoding break the sequential bottlenecks of modern AI.

::: {.callout-important title="Key Takeaways"}

* **The Inference Wall**: Serving cost often dominates training cost by 100x or more over a model's lifetime, making inference efficiency the primary driver of ML economics.
* **Prefill vs. Decode**: LLM serving is split into two regimes: compute-bound prefill (input processing) and memory-bandwidth-bound decode (token generation). Optimizing for the "Decode" phase requires high-bandwidth memory (HBM3) or specialized storage-as-memory (HBF).
* **Batching is Not Universal**: There is no "optimal" batch size for all models. Vision models scale with batch size; recommendation models scale with feature-parallel shards; LLMs scale via continuous, iteration-level management.
* **Power of Two Choices**: Simple, randomized load balancing achieves exponentially better queue balance ($O(\log \log n)$) with minimal overhead, a must-have for any large-scale deployment.
* **Statefulness is the Challenge**: Unlike stateless vision models, LLM serving is inherently stateful due to the KV cache. This requires sticky routing, prefix caching, and sophisticated memory paging (PagedAttention) to prevent fragmentation.
* **Distribution is a Tax**: Every step of distribution—network round-trips, serialization, and coordination—adds a "serving tax" that must be explicitly budgeted within the milliseconds allowed for a user response.

:::

We also saw that efficient serving requires specialized hardware. The "Decode" phase of LLM inference is memory-bandwidth bound, motivating the specialized inference accelerators we examined in @sec-infrastructure.

The serving architectures examined here—load balancers, model sharding, and batching strategies—provide the backbone for production inference. However, at extreme scale, even perfectly balanced clusters hit the "Memory Wall" and the physical limits of sequential token generation.

The next chapter, @sec-optimization-at-scale, examines the advanced speed and efficiency tricks—PagedAttention, Speculative Decoding, and Prefill-Decode Disaggregation—that allow us to break through these bottlenecks.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
