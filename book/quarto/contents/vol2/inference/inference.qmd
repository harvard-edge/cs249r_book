---
title: "Inference at Scale"
---

# Inference at Scale {#sec-inference-at-scale}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: A dynamic illustration of a global inference serving system handling millions of requests. The scene shows a central model representation being replicated across multiple serving nodes arranged in a globe-spanning network. Incoming request streams appear as flowing arrows from users worldwide, processed through load balancers depicted as traffic controllers, then distributed to model replicas. Visual elements include batching queues shown as organized request groups, latency meters displaying millisecond readings, and auto-scaling indicators showing nodes spawning and despawning. The composition emphasizes real-time responsiveness with clock symbols and speed indicators. Color palette features cool silvers and whites for infrastructure, warm oranges for active requests, and green for successful responses. Technical yet accessible style suitable for a production systems textbook._
:::

\noindent
![](images/png/cover_inference.png)

:::

## Purpose {.unnumbered}

_Why does serving machine learning predictions at scale require fundamentally different engineering approaches than training the models that generate them?_

Training optimizes for throughput over extended periods, tolerating latency variations that would be unacceptable when users await responses in real time. Inference at scale inverts these priorities: serving systems must deliver predictions within strict latency bounds while handling request volumes that fluctuate unpredictably across hours, days, and seasons. The engineering challenges of inference extend beyond raw performance to encompass resource efficiency when serving costs dwarf training costs over a model's lifetime, availability when users expect continuous service regardless of infrastructure conditions, and consistency when the same input must produce the same output across globally distributed serving infrastructure. Understanding how serving systems, batching strategies, model sharding, and load balancing address these challenges determines whether sophisticated models deliver value to users or remain impressive but impractical demonstrations. Mastering inference at scale transforms model capabilities into reliable services that operate continuously at the speed and scale users demand.

## Coming 2026

This chapter will cover serving systems, batching, model sharding, and load balancing.
