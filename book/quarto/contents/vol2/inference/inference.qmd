---
title: "Inference at Scale"
bibliography: inference.bib
---

<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR INFERENCE AT SCALE
================================================================================

EXPERT FEEDBACK FROM SERVING CHAPTER REVIEW (January 2025):
The following topics were identified by experts (Jeff Dean, Ion Stoica, Chip Huyen,
Vijay Reddi, Song Han) as important for distributed inference but appropriately
deferred from Vol I Serving chapter to this chapter:

FROM JEFF DEAN:

- Load balancing and request routing (power-of-two-choices, consistent hashing)
- M/M/c multi-server queue model and Erlang C formula for capacity planning
- Health checking and service discovery (liveness vs readiness probes)
- Timeout and deadline propagation across service layers
- Request coalescing and deduplication
- Multi-tenancy and isolation (noisy neighbor problems)

FROM ION STOICA:

- Stateless vs stateful serving (critical for scaling decisions)
- Idempotency requirements for hedged/retry strategies
- Circuit breakers for cascade failure prevention
- Bulkhead pattern for failure isolation
- Backpressure mechanisms beyond admission control

FROM CHIP HUYEN:

- Canary deployments and traffic shifting (1%→5%→25%→100%)
- Health checks and Kubernetes readiness probes
- Model versioning and rollback procedures
- Distributed tracing and observability patterns

FROM SONG HAN:

- Speculative decoding implementation details (draft models, acceptance rates)
- KV cache memory management and prefix caching
- Model sharding strategies (tensor parallelism, pipeline parallelism)

FROM VIJAY REDDI:

- MLPerf inference Server scenario implementation at scale
- Hierarchical edge-cloud serving patterns

================================================================================

CORE PRINCIPLE: Inference workloads vary DRAMATICALLY by model type.
Recommendation systems dominate production inference volume, not LLMs.

CRITICAL INSIGHT: By request volume, the ML inference landscape is:

- Recommendation/ranking: ~80-90% of inference requests at major tech companies
- Vision/image processing: ~5-10%
- NLP/LLM: ~1-5% (but growing rapidly)

MODEL-SPECIFIC INFERENCE CHARACTERISTICS:

| Model Type      | Latency Target | Batching Strategy    | Key Bottleneck       |
|-----------------|----------------|----------------------|----------------------|
| Recommendation  | <10ms p99      | Feature-parallel     | Embedding lookup     |
| Vision (CNN)    | 20-50ms        | Dynamic batching     | Compute-bound        |
| LLM             | 100ms-seconds  | Continuous batching  | Memory bandwidth     |
| Speech          | Real-time      | Streaming            | Sequential decode    |
| Multimodal      | Varies         | Request-level        | Cross-modal sync     |

REQUIRED COVERAGE FOR THIS CHAPTER:

BATCHING STRATEGIES:

- Static batching: Vision models, simpler serving
- Dynamic batching: Variable request arrival, timeout-based
- Continuous batching: LLM-specific (Orca paper), KV cache management
- Feature batching: Recommendation systems, parallel feature lookup

SERVING ARCHITECTURES:

- Single-model serving: Most vision/NLP models
- Ensemble serving: Recommendation pipelines (multiple models in sequence)
- Cascade serving: Early-exit, model routing
- Include: Why RecSys often needs 10+ models per request

MODEL SHARDING FOR INFERENCE:

- Tensor parallelism: LLM serving across GPUs
- Embedding sharding: Recommendation serving
- Include: Different sharding strategies for different model types

LOAD BALANCING:

- Request-level: Stateless models (vision, some NLP)
- Session-level: Stateful models (conversational LLM)
- Feature-level: Recommendation (route by user/item shards)

CASE STUDIES TO INCLUDE:

- Meta recommendation serving (billions of requests/day)
- Netflix ranking system architecture
- OpenAI API serving (LLM-specific challenges)
- Google Search ranking (ensemble of models)
- TikTok video recommendation (multimodal)

LATENCY ANALYSIS DIVERSITY:

- Include p50/p99/p999 for different model types
- Show where latency budget goes (network, compute, memory)
- Compare: RecSys (feature lookup dominates) vs LLM (decode dominates)

ANTI-PATTERNS TO AVOID:

- Treating inference as synonymous with "LLM serving"
- Ignoring embedding lookup latency (critical for RecSys)
- Only discussing KV cache (LLM-specific optimization)
- Forgetting that most production ML is NOT generative

================================================================================
-->

# Inference at Scale {#sec-inference-at-scale}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A dynamic illustration of a global inference serving system handling millions of requests. The scene shows a central model representation being replicated across multiple serving nodes arranged in a globe-spanning network. Incoming request streams appear as flowing arrows from users worldwide, processed through load balancers depicted as traffic controllers, then distributed to model replicas. Visual elements include batching queues shown as organized request groups, latency meters displaying millisecond readings, and auto-scaling indicators showing nodes spawning and despawning. The composition emphasizes real-time responsiveness with clock symbols and speed indicators. Color palette features cool silvers and whites for infrastructure, warm oranges for active requests, and green for successful responses. Technical yet accessible style suitable for a production systems textbook._
:::

\noindent
![](images/png/cover_inference_at_scale.png)

:::

## Purpose {.unnumbered}

_Why does serving machine learning predictions at scale require fundamentally different engineering approaches than training the models that generate them?_

Training optimizes for throughput over extended periods, tolerating latency variations that would be unacceptable when users await responses in real time. Inference at scale inverts these priorities: serving systems must deliver predictions within strict latency bounds while handling request volumes that fluctuate unpredictably across hours, days, and seasons. The engineering challenges of inference extend beyond raw performance to encompass resource efficiency when serving costs dwarf training costs over a model's lifetime, availability when users expect continuous service regardless of infrastructure conditions, and consistency when the same input must produce the same output across globally distributed serving infrastructure. Understanding how serving systems, batching strategies, model sharding, and load balancing address these challenges determines whether sophisticated models deliver value to users or remain impressive but impractical demonstrations. Mastering inference at scale transforms model capabilities into reliable services that operate continuously at the speed and scale users demand.

## Coming 2026

This chapter will cover serving systems, batching, model sharding, and load balancing.
