@article{agrawal2023sarathi,
  title = {SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills},
  author = {
    Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav
    S. and Ramjee, Ramachandran
  },
  year = {2023},
  month = aug,
  journal = {arXiv preprint arXiv:2308.16369},
  url = {http://arxiv.org/abs/2308.16369v1},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
}

@inproceedings{ainslie2023gqa,
  title = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author = {
    Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron,
    Federico and Sanghai, Sumit
  },
  year = {2023},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
  pages = {4895--4901},
  doi = {10.18653/v1/2023.emnlp-main.298},
  url = {https://doi.org/10.18653/v1/2023.emnlp-main.298},
  source = {Crossref},
}

@inproceedings{ashkboos2024quarot,
  title = {QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs},
  author = {
    Alistarh, Dan and Ashkboos, Saleh and Cameron, Pashmina and Croci, Maximilian and Hensman,
    James and Hoefler, Torsten and Jaggi, Martin and Li, Bo and Mohtashami, Amirkeivan
  },
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems 37},
  publisher = {Neural Information Processing Systems Foundation, Inc. (NeurIPS)},
  pages = {100213--100240},
  doi = {10.52202/079017-3180},
  url = {https://doi.org/10.52202/079017-3180},
  source = {Crossref},
}

@inproceedings{brown2020gpt3,
  title = {Language Models are Few-Shot Learners},
  author = {
    Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and
    Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell,
    Amanda and others
  },
  year = {2020},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {1877--1901},
}

@article{chen2023accelerating,
  title = {Accelerating Large Language Model Decoding with Speculative Sampling},
  author = {
    Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and
    Sifre, Laurent and Jumper, John
  },
  year = {2023},
  month = feb,
  journal = {arXiv preprint arXiv:2302.01318},
  url = {http://arxiv.org/abs/2302.01318v1},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
}

% ============================================================================
% Attention Mechanisms and Architectures
% ============================================================================
@inproceedings{dao2022flashattention,
  title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R\'e, Christopher},
  year = {2022},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {16344--16359},
}

% ============================================================================
% Quantization Methods
% ============================================================================
@inproceedings{frantar2023gptq,
  title = {GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  year = {2023},
  booktitle = {International Conference on Learning Representations},
}

@article{jiang2024mixtral,
  title = {Mixtral of Experts},
  author = {
    Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary,
    Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma
    Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and
    Lavaud, L\'elio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and
    Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet,
    Th\'eophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth\'ee and Sayed, William El
  },
  year = {2024},
  month = jan,
  journal = {arXiv preprint arXiv:2401.04088},
  url = {http://arxiv.org/abs/2401.04088v1},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
}

@inproceedings{karger1997consistent,
  title = {Consistent hashing and random trees},
  author = {
    Karger, David and Lehman, Eric and Leighton, Tom and Panigrahy, Rina and Levine, Matthew and
    Lewin, Daniel
  },
  year = {1997},
  booktitle = {Proceedings of the twenty-ninth annual ACM symposium on Theory of computing  - STOC '97},
  publisher = {ACM Press},
  pages = {654--663},
  doi = {10.1145/258533.258660},
  url = {https://doi.org/10.1145/258533.258660},
  source = {Crossref},
  subtitle = {distributed caching protocols for relieving hot spots on the World Wide Web},
  organization = {ACM},
}

@inproceedings{kwon2023vllm,
  title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author = {
    Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody
    Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion
  },
  year = {2023},
  month = oct,
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  publisher = {ACM},
  pages = {611--626},
  doi = {10.1145/3600006.3613165},
  url = {https://doi.org/10.1145/3600006.3613165},
  source = {Crossref},
}

% ============================================================================
% Speculative Decoding
% ============================================================================
@inproceedings{leviathan2023speculative,
  title = {Fast Inference from Transformers via Speculative Decoding},
  author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  year = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages = {19274--19286},
  organization = {PMLR},
}

@article{lin2024awq,
  title = {AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Xiao, Guangxuan and Han, Song},
  year = {2025},
  month = jan,
  journal = {GetMobile: Mobile Computing and Communications},
  booktitle = {Proceedings of Machine Learning and Systems},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {28},
  number = {4},
  pages = {12--17},
  doi = {10.1145/3714983.3714987},
  issn = {2375-0529,2375-0537},
  url = {https://doi.org/10.1145/3714983.3714987},
  source = {Crossref},
}

% ============================================================================
% Load Balancing and Distributed Systems
% ============================================================================
@article{mitzenmacher2001power,
  title = {The power of two choices in randomized load balancing},
  author = {Mitzenmacher, M.},
  year = {2001},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {12},
  number = {10},
  pages = {1094--1104},
  doi = {10.1109/71.963420},
  issn = {1045-9219},
  url = {https://doi.org/10.1109/71.963420},
  source = {Crossref},
}

% ============================================================================
% Distributed Computing Frameworks
% ============================================================================
@inproceedings{moritz2018ray,
  title = {Ray: A Distributed Framework for Emerging AI Applications},
  author = {
    Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard
    and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I.
    and Stoica, Ion
  },
  year = {2018},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {561--577},
  organization = {USENIX Association},
}

% ============================================================================
% Recommendation Systems
% ============================================================================
@article{naumov2019dlrm,
  title = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},
  author = {
    Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and
    Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean
    and Azzolini, Alisson G. and Dzhulgakov, Dmytro and Mallevich, Andrey and Cherniavskii, Ilia
    and Lu, Yinghai and Krishnamoorthi, Raghuraman and Yu, Ansha and Kondratenko, Volodymyr and
    Pereira, Stephanie and Chen, Xianjie and Chen, Wenlin and Rao, Vijay and Jia, Bill and Xiong,
    Liang and Smelyanskiy, Misha
  },
  year = {2019},
  month = may,
  journal = {arXiv preprint arXiv:1906.00091},
  url = {http://arxiv.org/abs/1906.00091v1},
  primaryclass = {cs.IR},
  archiveprefix = {arXiv},
}

% ============================================================================
% Software Engineering and Fault Tolerance
% ============================================================================
@book{nygard2007releaseit,
  title = {Release It!: Design and Deploy Production-Ready Software},
  author = {Nygard, Michael T.},
  year = {2007},
  publisher = {Pragmatic Bookshelf},
}

% ============================================================================
% Model Parallelism and Distributed Training
% ============================================================================
@article{shoeybi2019megatron,
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {
    Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared
    and Catanzaro, Bryan
  },
  year = {2019},
  month = sep,
  journal = {arXiv preprint arXiv:1909.08053},
  url = {http://arxiv.org/abs/1909.08053v4},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
}

% ============================================================================
% Large Language Models
% ============================================================================
@article{touvron2023llama2,
  title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author = {
    Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and
    Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale,
    Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and
    Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and
    Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and
    Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa,
    Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne
    and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and
    Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and
    Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan
    and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and
    Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan,
    Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang,
    Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas
  },
  year = {2023},
  month = jul,
  journal = {arXiv preprint arXiv:2307.09288},
  url = {http://arxiv.org/abs/2307.09288v2},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
}

@inproceedings{xiao2023smoothquant,
  title = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  year = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  organization = {PMLR},
}

% Bibliography for Inference at Scale chapter

% ============================================================================
% Continuous Batching and LLM Serving Systems
% ============================================================================
@inproceedings{yu2022orca,
  title = {Orca: A Distributed Serving System for Transformer-Based Generative Models},
  author = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  year = {2022},
  booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages = {521--538},
  organization = {USENIX Association},
}
