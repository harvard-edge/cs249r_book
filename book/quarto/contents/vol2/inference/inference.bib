@article{agrawal2023sarathi,
  title = {{SARATHI}: Efficient {LLM} Inference by Piggybacking Decodes with Chunked Prefills},
  author = {
    Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav
    S. and Ramjee, Ramachandran
  },
  year = {2023},
  journal = {arXiv preprint arXiv:2308.16369},
}

@inproceedings{ainslie2023gqa,
  title = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author = {
    Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n,
    Federico and Sanghai, Sumit
  },
  year = {2023},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages = {4895--4901},
}

@inproceedings{ashkboos2024quarot,
  title = {{QuaRot}: Outlier-Free 4-Bit Inference in Rotated {LLMs}},
  author = {
    Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L. and Li, Bo and Jaggi,
    Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James
  },
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems},
}

@inproceedings{brown2020gpt3,
  title = {Language Models are Few-Shot Learners},
  author = {
    Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and
    Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell,
    Amanda and others
  },
  year = {2020},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {1877--1901},
}

@article{chen2023accelerating,
  title = {Accelerating Large Language Model Decoding with Speculative Sampling},
  author = {
    Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and
    Sifre, Laurent and Jumper, John
  },
  year = {2023},
  journal = {arXiv preprint arXiv:2302.01318},
}

% ============================================================================
% Attention Mechanisms and Architectures
% ============================================================================
@inproceedings{dao2022flashattention,
  title = {{FlashAttention}: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {16344--16359},
}

% ============================================================================
% Quantization Methods
% ============================================================================
@inproceedings{frantar2023gptq,
  title = {{GPTQ}: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  year = {2023},
  booktitle = {International Conference on Learning Representations},
}

@article{jiang2024mixtral,
  title = {Mixtral of Experts},
  author = {
    Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary,
    Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma
    Bou and Bressand, Florian and others
  },
  year = {2024},
  journal = {arXiv preprint arXiv:2401.04088},
}

@inproceedings{karger1997consistent,
  title = {
    Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on
    the World Wide Web
  },
  author = {
    Karger, David and Lehman, Eric and Leighton, Tom and Panigraphy, Rina and Levine, Matthew and
    Lewin, Daniel
  },
  year = {1997},
  booktitle = {Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing},
  pages = {654--663},
  organization = {ACM},
}

@inproceedings{kwon2023vllm,
  title = {Efficient Memory Management for Large Language Model Serving with {PagedAttention}},
  author = {
    Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody
    Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion
  },
  year = {2023},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  publisher = {ACM},
  pages = {611--626},
}

% ============================================================================
% Speculative Decoding
% ============================================================================
@inproceedings{leviathan2023speculative,
  title = {Fast Inference from Transformers via Speculative Decoding},
  author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  year = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages = {19274--19286},
  organization = {PMLR},
}

@inproceedings{lin2024awq,
  title = {{AWQ}: Activation-aware Weight Quantization for {LLM} Compression and Acceleration},
  author = {
    Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang,
    Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song
  },
  year = {2024},
  booktitle = {Proceedings of Machine Learning and Systems},
}

% ============================================================================
% Load Balancing and Distributed Systems
% ============================================================================
@article{mitzenmacher2001power,
  title = {The Power of Two Choices in Randomized Load Balancing},
  author = {Mitzenmacher, Michael},
  year = {2001},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  publisher = {IEEE},
  volume = {12},
  number = {10},
  pages = {1094--1104},
}

% ============================================================================
% Distributed Computing Frameworks
% ============================================================================
@inproceedings{moritz2018ray,
  title = {Ray: A Distributed Framework for Emerging {AI} Applications},
  author = {
    Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard
    and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I.
    and Stoica, Ion
  },
  year = {2018},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {561--577},
  organization = {USENIX Association},
}

% ============================================================================
% Recommendation Systems
% ============================================================================
@article{naumov2019dlrm,
  title = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},
  author = {
    Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and
    Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean
    and Azzolini, Alisson G. and others
  },
  year = {2019},
  journal = {arXiv preprint arXiv:1906.00091},
}

% ============================================================================
% Software Engineering and Fault Tolerance
% ============================================================================
@book{nygard2007releaseit,
  title = {Release It!: Design and Deploy Production-Ready Software},
  author = {Nygard, Michael T.},
  year = {2007},
  publisher = {Pragmatic Bookshelf},
}

% ============================================================================
% Model Parallelism and Distributed Training
% ============================================================================
@article{shoeybi2019megatron,
  title = {{Megatron-LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {
    Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared
    and Catanzaro, Bryan
  },
  year = {2019},
  journal = {arXiv preprint arXiv:1909.08053},
}

% ============================================================================
% Large Language Models
% ============================================================================
@article{touvron2023llama2,
  title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author = {
    Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and
    Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale,
    Shruti and others
  },
  year = {2023},
  journal = {arXiv preprint arXiv:2307.09288},
}

@inproceedings{xiao2023smoothquant,
  title = {{SmoothQuant}: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  year = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  organization = {PMLR},
}

% Bibliography for Inference at Scale chapter

% ============================================================================
% Continuous Batching and LLM Serving Systems
% ============================================================================
@inproceedings{yu2022orca,
  title = {Orca: A Distributed Serving System for Transformer-Based Generative Models},
  author = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  year = {2022},
  booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages = {521--538},
  organization = {USENIX Association},
}
