---
engine: jupyter
---

```{python}
#| echo: false
#| label: chapter-start
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER START
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter initialization and global imports
# │
# │ Why: Registers this chapter with the mlsys registry and provides shared
# │      imports for all subsequent calculation cells.
# │
# │ Imports: mlsys.registry, mlsys.constants, mlsys.formatting
# │ Exports: (none)
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.registry import start_chapter
from mlsys.constants import (
    A100_MEM_BW, A100_FLOPS_FP16_TENSOR,
    B200_FLOPS_FP8_TENSOR, B200_MEM_BW, B200_MEM_CAPACITY,
    H100_MEM_BW, H100_FLOPS_FP16_TENSOR, H100_FLOPS_FP8_TENSOR,
    ENERGY_DRAM_PJ_PER_BYTE, ENERGY_FLOP_FP16_PJ,
    TFLOPs, second, GB, TB, byte, flop
)
from mlsys.formatting import fmt, sci, check

start_chapter("vol2:performance_engineering")
```

# Performance Engineering {#sec-performance-engineering}

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_performance.png){fig-alt="High-performance compute kernels and memory-aware optimization for ML inference." width=100%}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{30}{35}{100}{15}
\end{marginfigure}

_Why does an H100 GPU capable of nearly 2,000 teraflops often sit 95% idle during inference?_

Machine learning performance is a **negotiation between logic and physics**. Modern accelerators deliver exascale compute, but only if the software respects the physical constraints of the **Memory Wall** and the **Power Wall**. *When* a kernel is designed without awareness of the memory hierarchy, it spends more time moving data through HBM than performing arithmetic, collapsing system efficiency ($\eta$). The art of performance engineering is maximizing the **Arithmetic Intensity** of every operation—extracting the highest computational value from every byte moved. This chapter explores the techniques that bridge the gap between theoretical hardware peak ($R_{\text{peak}}$) and actual workload throughput: **Kernel Fusion** to eliminate redundant memory trips, **Quantization** to shrink the data footprint ($D_{\text{vol}}$), and **Compilation** to optimize the dataflow across the silicon. Without this mastery, the Machine Learning Fleet becomes a collection of expensive heaters, consuming megawatts of power to shuffle data while the arithmetic units wait.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Analyze memory-bound versus compute-bound workloads using the **Roofline Model** and identify the hardware ridge point
- Implement **Kernel Fusion** to minimize global memory bandwidth consumption by keeping intermediate activations in fast SRAM
- Apply **Mixed-Precision Optimization** (FP16, BF16, FP8) to increase arithmetic throughput while reducing memory pressure
- Evaluate **I/O-Aware Algorithms** (e.g., FlashAttention) that use tiling to bypass memory bandwidth bottlenecks
- Compare **Interpreter** versus **Compiler** performance (Eager vs. Graph mode) in terms of dispatch overhead and graph-level optimization
- Design **Performance-Critical Kernels** that honor the "Silicon Contract" by maximizing Tensor Core utilization

:::

```{python}
#| label: perf-eng-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ PERFORMANCE ENGINEERING: HARDWARE BOUNDS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-memory-wall and the Roofline analysis
# │          sections throughout the chapter.
# │
# │ Goal: Establish the memory wall by comparing H100/B200 peak compute vs
# │       memory bandwidth, and quantify the energy ratio between movement
# │       and calculation (DRAM vs FLOP).
# │ Show: "3.35" TB/s H100 bandwidth, "8" TB/s B200 bandwidth, "~150x"
# │       energy gap between DRAM access and FP16 math — inline in the
# │       memory wall and arithmetic intensity paragraphs.
# │ How: .m_as() for unit scaling; ratio = DRAM_energy / FLOP_energy.
# │
# │ Imports: mlsys.constants (H100_MEM_BW, H100_FLOPS_FP16_TENSOR,
# │           B200_FLOPS_FP8_TENSOR, B200_MEM_BW, B200_MEM_CAPACITY,
# │           ENERGY_DRAM_PJ_PER_BYTE, ENERGY_FLOP_FP16_PJ,
# │           TFLOPs, TB, GB, byte, flop)
# │ Exports: a100_hbm_bw_str, a100_hbm_bw_gbs_str, a100_fp16_str, a100_ridge_fp16_str,
# │          b200_fp8_str, b200_hbm_bw_str, b200_mem_str, energy_ratio_str
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.constants import (
    A100_MEM_BW, A100_FLOPS_FP16_TENSOR,
    B200_FLOPS_FP8_TENSOR, B200_MEM_BW, B200_MEM_CAPACITY,
    H100_MEM_BW, H100_FLOPS_FP16_TENSOR, H100_FLOPS_FP8_TENSOR,
    ENERGY_DRAM_PJ_PER_BYTE, ENERGY_FLOP_FP16_PJ,
    TFLOPs, second, GB, TB, byte, flop
)
from mlsys.formatting import fmt, sci, check

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class PerfEngSetup:
    """Namespace for Performance Engineering reference bounds."""

    # ┌── 1. PARAMETERS (Inputs) ───────────────────────────────────────────────
    h100_bw = H100_MEM_BW
    h100_flops = H100_FLOPS_FP16_TENSOR
    
    b200_bw = B200_MEM_BW
    b200_flops_fp8 = B200_FLOPS_FP8_TENSOR
    b200_cap = B200_MEM_CAPACITY
    
    energy_byte = ENERGY_DRAM_PJ_PER_BYTE
    energy_flop = ENERGY_FLOP_FP16_PJ

    # ┌── 2. CALCULATION (The Physics) ─────────────────────────────────────────
    h100_ridge = (h100_flops / h100_bw).to(flop/byte).magnitude
    energy_ratio = energy_byte.m_as(byte**-1) / energy_flop.m_as(flop**-1)

    # ┌── 3. INVARIANTS (Guardrails) ───────────────────────────────────────────
    check(h100_ridge > 200, f"Expected H100 ridge > 200, got {h100_ridge:.1f}")

    # ┌── 4. OUTPUTS (Formatting) ──────────────────────────────────────────────
    h100_hbm_bw_str = f"{h100_bw.m_as(TB/second):.2f}"
    h100_ridge_str = f"{h100_ridge:.0f}"
    
    b200_fp8_str = f"{b200_flops_fp8.m_as(TFLOPs/second):,.0f}"
    b200_hbm_bw_str = f"{b200_bw.m_as(TB/second):.1f}"
    b200_mem_str = f"{b200_cap.m_as(GB):.0f}"
    
    energy_ratio_str = f"{energy_ratio:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
h100_hbm_bw_str = PerfEngSetup.h100_hbm_bw_str
h100_ridge_str = PerfEngSetup.h100_ridge_str
b200_fp8_str = PerfEngSetup.b200_fp8_str
b200_hbm_bw_str = PerfEngSetup.b200_hbm_bw_str
b200_mem_str = PerfEngSetup.b200_mem_str
energy_ratio_str = PerfEngSetup.energy_ratio_str
```

## The Memory Wall and the Efficiency Frontier {#sec-performance-engineering-memory-wall}
