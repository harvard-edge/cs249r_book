---
engine: jupyter
---

```{python}
#| echo: false
#| label: chapter-start
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER START
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter initialization and global imports
# │
# │ Why: Registers this chapter with the mlsys registry and provides shared
# │      imports for all subsequent calculation cells.
# │
# │ Imports: mlsys.registry, mlsys.constants, mlsys.formatting
# │ Exports: (none)
# └─────────────────────────────────────────────────────────────────────────────
from mlsys.registry import start_chapter
from mlsys.constants import *
from mlsys.formatting import fmt, sci, check
from mlsys.formulas import calc_bottleneck, model_memory

start_chapter("vol2:performance_engineering")
```

# Performance Engineering {#sec-performance-engineering}

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_optimization.png)

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlfleetstack{30}{25}{100}{10}
\end{marginfigure}

_How do we make billion-parameter models run on millisecond timescales?_

Model compression, covered in earlier chapters, reduces the size of what we compute. Performance engineering reshapes *how* we compute to match the physics of the hardware. The distinction matters: a quantized model loaded naively into a GPU kernel that reads every weight from off-chip memory wastes the very bandwidth savings that quantization was designed to provide. Real performance comes from understanding the full path a tensor travels, from registers through SRAM to HBM and back, and then engineering each step to eliminate wasted movement. This chapter develops the system-level optimization techniques that bridge the gap between a theoretically efficient model and a production artifact that saturates hardware. We examine operator fusion and tiling strategies that keep data in fast SRAM, precision formats that double effective bandwidth, compilation frameworks that automate kernel selection, and algorithmic innovations like speculative decoding and sparse expert routing that fundamentally change the performance equation. Together, these techniques transform a model that "should" be fast into one that *is* fast, often by an order of magnitude.

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Analyze the **Memory Wall** using the **Roofline Model** and diagnose whether a given ML workload is compute-bound or memory-bound on a specific accelerator.
- Explain how **Operator Fusion** and **Tiling** (FlashAttention) overcome HBM bandwidth limitations by keeping intermediate data in on-chip SRAM.
- Implement **FP8 Training** using E4M3 and E5M2 formats and apply **Block-wise Quantization** (LLM.int8(), GPTQ, AWQ) to handle outlier features in large language models.
- Apply **Graph Compilation** frameworks (torch.compile, XLA, TensorRT) to automate operator fusion, memory planning, and kernel selection.
- Evaluate **Speculative Decoding** strategies to reduce inference latency by trading compute for latency using small draft models.
- Design **Mixture of Experts (MoE)** systems that decouple model capacity from inference cost through sparse activation and expert parallelism.
- Diagnose performance bottlenecks using **System Profiling** tools (Nsight Systems, PyTorch Profiler) and roofline plots.

:::

::: {.callout-note title="Connection: The Fleet Stack"}

Performance Engineering is the **Optimization Layer** of the Fleet Stack. While Inference at Scale (@sec-inference-scale) defines the serving architecture and scheduling policies, Performance Engineering optimizes the individual operations that execute within each serving node. In the **Fleet Stack** (@sec-vol2-introduction), this chapter sits between the Serving Layer (how work is scheduled) and the Infrastructure Layer (how hardware executes). Every technique here targets the same goal: closing the gap between theoretical hardware peak and achieved throughput, turning the **Iron Law** from a speed limit into a speedometer that reads closer to maximum.

:::

```{python}
#| echo: false
#| label: perf-eng-setup
# ┌─────────────────────────────────────────────────────────────────────────────
# │ PERFORMANCE ENGINEERING SYSTEM CONSTANTS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Chapter-wide setup for @sec-performance-engineering
# │
# │ Goal: Establish H100/A100/B200 hardware specs as shared reference for roofline,
# │       bottleneck analysis, and SRAM-vs-HBM energy ratio throughout the chapter.
# │ Show: H100 ~3.35 TB/s BW, ~989 TFLOPs FP16, ~1979 TFLOPs FP8; ridge points
# │       ~295 FP16 and ~590 FP8 FLOP/byte; energy ratio ~1280x DRAM/SRAM.
# │ How: Pull from mlsys.constants; compute ridge points as FLOPS/BW ratio
# │       (TFLOPs/TB → FLOP/byte via .m_as(flop/byte)).
# │
# │ Imports: mlsys.constants (H100_MEM_BW, H100_FLOPS_FP16_TENSOR,
# │           H100_FLOPS_FP8_TENSOR, H100_MEM_CAPACITY, H100_TDP,
# │           A100_MEM_BW, A100_FLOPS_FP16_TENSOR, A100_MEM_CAPACITY,
# │           B200_FLOPS_FP8_TENSOR, B200_MEM_BW, B200_MEM_CAPACITY,
# │           ENERGY_DRAM_ACCESS_PJ, ENERGY_SRAM_L1_PJ)
# │ Exports: h100_hbm_bw_str, h100_hbm_bw_gbs_str, h100_fp16_str, h100_fp8_str,
# │          h100_mem_str, h100_tdp_str, h100_ridge_fp16_str, h100_ridge_fp8_str,
# │          a100_hbm_bw_str, a100_hbm_bw_gbs_str, a100_fp16_str, a100_ridge_fp16_str,
# │          b200_fp8_str, b200_hbm_bw_str, b200_mem_str, energy_ratio_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class PerformanceSetup:
    """
    Namespace for Performance Engineering reference hardware specs.
    """

    # ┌── 1. PARAMETERS (Inputs) ───────────────────────────────────────────────
    # H100 specs
    h100_hbm_bw = H100_MEM_BW
    h100_fp16 = H100_FLOPS_FP16_TENSOR
    h100_fp8 = H100_FLOPS_FP8_TENSOR
    h100_mem = H100_MEM_CAPACITY
    h100_tdp = H100_TDP

    # A100 specs
    a100_hbm_bw = A100_MEM_BW
    a100_fp16 = A100_FLOPS_FP16_TENSOR
    a100_mem = A100_MEM_CAPACITY

    # B200 specs
    b200_fp8 = B200_FLOPS_FP8_TENSOR
    b200_hbm_bw = B200_MEM_BW
    b200_mem = B200_MEM_CAPACITY

    # Energy
    energy_dram_pj = ENERGY_DRAM_ACCESS_PJ
    energy_sram_pj = ENERGY_SRAM_L1_PJ

    # ┌── 2. CALCULATION (The Physics) ─────────────────────────────────────────
    # Roofline ridge points (FLOPS / BW = ops/byte at the knee)
    h100_ridge_fp16 = h100_fp16 / h100_hbm_bw
    h100_ridge_fp8 = h100_fp8 / h100_hbm_bw
    a100_ridge_fp16 = a100_fp16 / a100_hbm_bw

    # Energy ratio
    energy_ratio_val = energy_dram_pj / energy_sram_pj

    # ┌── 3. INVARIANTS (Guardrails) ───────────────────────────────────────────
    check(h100_hbm_bw.m_as(TB / second) > 3.0, "H100 HBM BW mismatch")
    check(h100_fp16.m_as(TFLOPs / second) > 900, "H100 FP16 mismatch")

    # ┌── 4. OUTPUTS (Formatting) ──────────────────────────────────────────────
    # Exports handled below...

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
# H100
h100_hbm_bw_str = fmt(PerformanceSetup.h100_hbm_bw, "TB/s", precision=2)
h100_hbm_bw_gbs_str = fmt(PerformanceSetup.h100_hbm_bw, "GB/s", precision=0)
h100_fp16_str = fmt(PerformanceSetup.h100_fp16, "TFLOPs/s", precision=0)
h100_fp8_str = fmt(PerformanceSetup.h100_fp8, "TFLOPs/s", precision=0)
h100_mem_str = fmt(PerformanceSetup.h100_mem, "GiB", precision=0)
h100_tdp_str = fmt(PerformanceSetup.h100_tdp, "W", precision=0)
h100_ridge_fp16_str = f"{PerformanceSetup.h100_ridge_fp16.m_as(flop/byte):.0f}"
h100_ridge_fp8_str = f"{PerformanceSetup.h100_ridge_fp8.m_as(flop/byte):.0f}"

# A100
a100_hbm_bw_str = fmt(PerformanceSetup.a100_hbm_bw, "TB/s", precision=0)
a100_hbm_bw_gbs_str = fmt(PerformanceSetup.a100_hbm_bw, "GB/s", precision=0)
a100_fp16_str = fmt(PerformanceSetup.a100_fp16, "TFLOPs/s", precision=0)
a100_ridge_fp16_str = f"{PerformanceSetup.a100_ridge_fp16.m_as(flop/byte):.0f}"

# B200
b200_fp8_str = fmt(PerformanceSetup.b200_fp8, "TFLOPs/s", precision=0)
b200_hbm_bw_str = fmt(PerformanceSetup.b200_hbm_bw, "TB/s", precision=0)
b200_mem_str = fmt(PerformanceSetup.b200_mem, "GiB", precision=0)

# Energy ratio
energy_ratio_str = f"{PerformanceSetup.energy_ratio_val.m_as(''):.0f}"
```

## The Memory Wall and the Efficiency Frontier {#sec-performance-engineering-memory-wall}

Why does an H100 GPU capable of 989 teraFLOPS often sit 95% idle while generating text from a large language model? The processor is starving for data. Performance engineering operates within a constrained optimization space defined by the Memory Wall, where the speed of moving bytes from memory to compute units fundamentally caps our operational throughput.

Part II established the distributed logic of the fleet: parallelism strategies (@sec-distributed-training-systems), communication patterns (@sec-collective-communication), fault recovery (@sec-fault-tolerance-reliability), and resource orchestration (@sec-fleet-orchestration). Those chapters ensured that workloads reach the right hardware and survive failures along the way. This chapter ensures that each workload *uses* that hardware efficiently, extracting maximum throughput from every accelerator cycle.

### The Iron Law of ML Performance {#sec-performance-engineering-efficiency-frontier}

Performance engineering operates within a constrained optimization space defined by the **Iron Law** of ML system performance:

$$
\text{Time} = \max\left( \frac{\text{Compute}}{\text{FLOPS}}, \; \frac{\text{Memory Access}}{\text{Bandwidth}} \right) + \text{Overhead}
$$ {#eq-iron-law-perf}

This is the performance engineer's most important equation. It appears simple, but its implications are profound because the $\max$ operator means that only one term matters at a time. Optimizing the wrong term yields zero improvement. An engineer who spends a week optimizing compute throughput for a memory-bound workload has wasted that week entirely. The equation demands diagnosis before optimization.

This equation decomposes execution time into three terms. The first fraction represents compute time: the total floating-point operations divided by the hardware's peak throughput. The second fraction represents memory time: the total bytes transferred divided by the memory bandwidth. The $\max$ operator reflects the roofline principle: the slower of the two determines performance. The overhead term captures everything else: kernel launch latency, synchronization, communication, and software stack inefficiency.

Standard model compression (pruning, quantization, distillation) reduces the numerators, performing fewer operations on smaller data. System optimization, the focus of this chapter, attacks the *structure* of the equation itself:

**Operator fusion and tiling** (FlashAttention, fused kernels) reduce the Memory Access numerator by eliminating intermediate HBM round-trips. When a sequence of operations keeps its data in SRAM, the effective Memory Access term shrinks dramatically, often by 10--30 $\times$ for attention computation.

**Precision engineering** (FP8, INT4, KV cache compression) reduces the Memory Access numerator by representing each value in fewer bytes. Halving the precision halves the bytes transferred, doubling the effective bandwidth.

**Graph compilation** (torch.compile, XLA, TensorRT) reduces the Overhead term by eliminating kernel launch gaps, fusing operations, and optimizing memory allocation.

**Communication-computation overlap** transforms the equation for distributed systems by making the communication overhead concurrent with the compute term, effectively eliminating it from the critical path when the overlap condition (@eq-overlap-condition) holds.

**Algorithmic innovations** (speculative decoding, MoE) change the Compute numerator itself by performing a fundamentally different, less expensive computation that produces equivalent results.

Each technique attacks a different term, and this taxonomy guides optimization strategy: diagnose which term dominates (using the roofline model from @sec-performance-engineering-roofline), then apply the technique targeting that term. Applying a technique that targets the non-dominant term wastes engineering effort.

@fig-iron-law-flowchart codifies this diagnostic process as a decision flowchart, mapping each bottleneck to its corresponding optimization technique.

::: {#fig-iron-law-flowchart fig-env="figure" fig-pos="htb" fig-cap="**Iron Law Diagnostic Flowchart**. The optimization process begins with profiling to determine which term in @eq-iron-law-perf dominates. If the workload is compute-bound, precision engineering or algorithmic changes reduce the numerator. If memory-bound, operator fusion and tiling reduce HBM traffic. If overhead-bound, graph compilation and communication overlap attack the residual term. Applying the wrong technique yields zero improvement." fig-alt="Flowchart starting with Profile Workload, branching to Compute Bound, Memory Bound, or Overhead Bound, each leading to specific optimization techniques."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{BlueL}{HTML}{D1E6F3}
  \definecolor{GreenLine}{HTML}{008F45}
  \definecolor{GreenL}{HTML}{D4EFDF}
  \definecolor{OrangeLine}{HTML}{E67817}
  \definecolor{OrangeL}{HTML}{FCE4CC}
  \definecolor{RedLine}{HTML}{CB202D}
  \definecolor{RedL}{HTML}{F5D2D5}
  \definecolor{VioletLine}{HTML}{7E317B}
  \definecolor{VioletL}{HTML}{E6D4E5}

  \tikzset{
    start/.style={draw=black!70, fill=black!10, rounded corners=3pt, thick,
                  minimum width=3cm, minimum height=0.8cm, align=center, font=\small\bfseries},
    diag/.style={diamond, draw=#1, fill=#1!12, thick, aspect=2.5,
                 inner sep=1pt, align=center, font=\scriptsize},
    action/.style={draw=#1, fill=#1!15, rounded corners=3pt, thick,
                   minimum width=2.6cm, minimum height=0.7cm, align=center, font=\scriptsize},
    arrow/.style={-{Triangle[width=5pt,length=4pt]}, thick, black!50}
  }

  % Start
  \node[start] (profile) at (5, 0) {Profile Workload\\(Roofline Analysis)};

  % Three branches
  \node[diag=BlueLine] (cb) at (0.5, -2.2) {Compute\\Bound?};
  \node[diag=OrangeLine] (mb) at (5, -2.2) {Memory\\Bound?};
  \node[diag=VioletLine] (ob) at (9.5, -2.2) {Overhead\\Bound?};

  \draw[arrow] (profile) -- (cb);
  \draw[arrow] (profile) -- (mb);
  \draw[arrow] (profile) -- (ob);

  % Compute-bound actions
  \node[action=BlueLine] (a1) at (-0.8, -4.2) {Precision Eng.\\(FP8, INT4)};
  \node[action=BlueLine] (a2) at (2.0, -4.2) {Algorithmic\\(MoE, Speculative)};
  \draw[arrow] (cb) -- (a1);
  \draw[arrow] (cb) -- (a2);

  % Memory-bound actions
  \node[action=OrangeLine] (a3) at (3.8, -4.2) {Operator Fusion\\(FlashAttention)};
  \node[action=OrangeLine] (a4) at (6.4, -4.2) {Tiling \&\\KV Cache Opt.};
  \draw[arrow] (mb) -- (a3);
  \draw[arrow] (mb) -- (a4);

  % Overhead-bound actions
  \node[action=VioletLine] (a5) at (8.2, -4.2) {Graph Compile\\(torch.compile)};
  \node[action=VioletLine] (a6) at (10.8, -4.2) {Comm. Overlap\\(CUDA Graphs)};
  \draw[arrow] (ob) -- (a5);
  \draw[arrow] (ob) -- (a6);

  % Re-profile loop
  \node[start, minimum width=2.2cm, font=\scriptsize\bfseries] (re) at (5, -5.8) {Re-profile};
  \draw[arrow] (a1.south) |- (re);
  \draw[arrow] (a2.south) |- (re);
  \draw[arrow] (a3.south) |- (re);
  \draw[arrow] (a4.south) |- (re);
  \draw[arrow] (a5.south) |- (re);
  \draw[arrow] (a6.south) |- (re);

  % Loop back
  \draw[arrow, dashed] (re.east) -- ++(2.5,0) |- (profile.east);
  \node[font=\tiny\itshape, text=black!50] at (9.5, -3.0) {iterate until};
  \node[font=\tiny\itshape, text=black!50] at (9.5, -3.3) {target met};

\end{tikzpicture}
```
:::

The **efficiency frontier** is the Pareto-optimal curve of model quality versus system throughput. A model on the frontier cannot improve throughput without sacrificing quality, or vice versa. The techniques in this chapter push the frontier outward by making each quality level achievable at higher throughput, or equivalently, by making each throughput level achievable at higher quality. An organization's goal is not merely to reach the frontier but to find the point on it that best matches their latency, throughput, cost, and quality requirements.

The multi-dimensional nature of this frontier makes optimization challenging. The relevant dimensions include:

- **Throughput** (tokens/second or requests/second): How much work the system completes per unit time.
- **Latency** (time-to-first-token, inter-token latency): How quickly the system responds to individual requests.
- **Cost** (dollars per million tokens): The economic efficiency of the system.
- **Quality** (perplexity, benchmark accuracy, human preference): The accuracy and usefulness of model outputs.
- **Memory** (peak GPU memory): The resource constraint that limits batch size and sequence length.

These dimensions interact in non-obvious ways. Increasing batch size improves throughput and cost efficiency but degrades latency. Reducing precision improves throughput and memory but may degrade quality. Speculative decoding improves latency but may increase per-token cost. The performance engineer's task is to navigate these trade-offs guided by the application's specific requirements.

A real-time chatbot prioritizes latency (time-to-first-token under 200 ms, inter-token latency under 50 ms) and may tolerate higher per-token cost. A batch processing pipeline for document summarization prioritizes throughput and cost, tolerating seconds of latency. A medical diagnostic system prioritizes quality above all else, accepting lower throughput and higher cost. Each application maps to a different optimal point on the efficiency frontier, and the techniques in this chapter provide the tools to reach that point.

To make this concrete, consider two deployment configurations for the same 70B LLM:

**Configuration A (Latency-optimized)**: FP16 weights, batch size 1, speculative decoding enabled. Each H100 GPU serves approximately 80 tokens/second with 20 ms inter-token latency. Cost: 8 GPUs dedicated to a single user stream, approximately \$0.12 per 1,000 output tokens.

**Configuration B (Throughput-optimized)**: INT4 weights (AWQ), batch size 64, no speculation. Each H100 serves approximately 4,000 tokens/second aggregate throughput across all batched requests, with 120 ms inter-token latency per request. Cost: 4 GPUs serving 64 concurrent users, approximately \$0.002 per 1,000 output tokens.

Configuration B achieves 60 $\times$ lower cost per token than Configuration A, but at 6 $\times$ higher latency. Neither configuration is objectively "better"; they represent different points on the efficiency frontier, optimized for different applications. The performance engineering techniques in this chapter are the tools for navigating between these points.

### The Memory Wall {#sec-performance-engineering-memory-wall-physics}

The efficiency frontier establishes what we are optimizing *toward*. The physics of memory bandwidth determines where we start. Every performance engineering problem in modern ML begins with the same observation: memory bandwidth, not compute, is the bottleneck. Consider a single autoregressive decoding step in a large language model. The model reads its full weight matrix from High Bandwidth Memory (HBM) to generate a single token, performing only one or two multiply-accumulate operations per weight loaded. An NVIDIA H100 delivers `{python} h100_fp8_str` TFLOPS of FP8 compute but only `{python} h100_hbm_bw_str` TB/s of memory bandwidth. If every byte loaded from memory does fewer than `{python} h100_ridge_fp8_str` arithmetic operations, the compute units sit idle, starved for data. This gap between compute capability and memory delivery rate is the **Memory Wall**, and it defines the landscape within which all performance engineering operates.

The memory wall is not a temporary engineering limitation; it is a consequence of physics. Moving data costs energy proportional to distance. Accessing a value from on-chip SRAM (L1 cache) costs approximately 0.5 pJ, while fetching the same value from off-chip HBM costs roughly 640 pJ, a ratio of `{python} energy_ratio_str` $\times$. Manufacturing constraints limit the amount of SRAM that can sit close to the compute units. HBM provides capacity (the H100 offers `{python} h100_mem_str` GB) but at physically greater distance, requiring the data to traverse longer wires. The fundamental tension is that models need gigabytes of parameters and state, but physics dictates that only kilobytes of data can be near the compute units at any given moment.

This tension shapes every optimization technique in this chapter. Operator fusion reduces the number of trips to HBM by combining operations so that intermediate results stay in SRAM. Precision engineering reduces the number of bytes per trip by representing values in FP8 or INT4 instead of FP16. Tiling strategies restructure algorithms to maximize data reuse within SRAM. Graph compilers automate these transformations. Each technique attacks a different term in the same fundamental equation: minimize the ratio of bytes moved to operations performed.

### The GPU Memory Hierarchy {#sec-performance-engineering-memory-hierarchy}

To understand why the memory wall exists, consider the physical structure of a modern GPU's memory system. The hierarchy spans four levels, each trading capacity for bandwidth and latency.

**Registers** are the fastest storage, located directly within each streaming multiprocessor (SM). The H100 provides 256 KB of register file per SM across its 132 SMs, totaling approximately 33 MB of register space across the entire chip. Register access is essentially free in terms of latency (one clock cycle) and energy (~0.01 pJ per access). But registers are private to individual threads and cannot be shared.

**Shared memory (SRAM)** is the next level, pooled within each SM. The H100 provides up to 228 KB of configurable shared memory per SM. This memory is shared among all threads in a thread block, enabling cooperative data reuse. Access latency is approximately 20--30 clock cycles (~20 ns), and energy cost is roughly 0.5 pJ per access. Shared memory is the critical resource for operator fusion: if intermediate results fit in shared memory, they never need to traverse the slow HBM bus.

**L2 cache** sits between the SMs and HBM, providing a 50 MB on-chip buffer on the H100. The L2 cache captures reuse patterns automatically (when the same data is accessed by multiple SMs) but cannot be explicitly managed by kernel authors. Access latency is approximately 200 clock cycles (~130 ns). The L2 cache is particularly important for multi-head attention, where multiple attention heads may access the same KV cache entries. If the KV cache for a given sequence position fits in L2, subsequent heads accessing the same position benefit from cache hits rather than paying the full HBM access cost.

**High Bandwidth Memory (HBM)** is the main off-chip memory, providing `{python} h100_mem_str` GB of capacity at `{python} h100_hbm_bw_str` TB/s bandwidth. HBM access latency is approximately 300 ns, and each access costs roughly 640 pJ of energy. Despite the "high bandwidth" designation, the bandwidth-to-capacity ratio means that reading the full `{python} h100_mem_str` GB of HBM takes approximately 24 ms, far longer than the sub-millisecond latency targets of real-time inference.

The energy cost of data movement has a direct economic consequence at datacenter scale. Consider a training cluster of 1,000 H100 GPUs, each performing approximately $10^{12}$ memory accesses per second during a memory-bound workload. If each access reads from HBM at 640 pJ, the memory subsystem alone consumes approximately 640 W per GPU, a significant fraction of the H100's 700 W TDP. If operator fusion moves half of those accesses from HBM to SRAM (at 0.5 pJ each), the per-GPU memory power drops by approximately 320 W. Across 1,000 GPUs, this saves 320 kW, equivalent to powering roughly 250 homes. This is not a secondary consideration; at cloud electricity prices, the annual cost difference is substantial, and it scales linearly with cluster size. The physics of data movement is not merely a performance constraint; it is an economic one.

The performance engineering challenge reduces to a data placement problem: keep the data that the compute units need in the fastest memory that can hold it. When a kernel reads a tensor from HBM, processes it, and writes the result back to HBM, the HBM round-trip dominates execution time for any operation with low arithmetic intensity. The techniques in this chapter all share the goal of keeping data closer to compute for longer.

### The Widening Gap {#sec-performance-engineering-widening-gap}

The memory wall is not static; it grows wider with each hardware generation. Compute throughput has scaled exponentially, roughly doubling every two years with new GPU architectures. Memory bandwidth has improved more slowly, constrained by the physics of off-chip signaling and the economics of HBM manufacturing.

| **GPU**  | **Year** | **Peak FP16 (TFLOPS)** | **HBM BW (TB/s)** | **Ridge Point (FLOP/byte)** |
|:---------|:---------|:-----------------------|:------------------|:----------------------------|
| **V100** | 2017     | 125                    | 0.9               | 139                         |
| **A100** | 2020     | 312                    | 2.0               | 156                         |
| **H100** | 2022     | 989                    | 3.35              | 295                         |
| **B200** | 2024     | 4,500                  | 8.0               | 563                         |

: **The Widening Memory Wall**. Compute throughput has increased 36 $\times$ from V100 to B200 over seven years, while memory bandwidth has increased only 8.9 $\times$. The ridge point has increased 4 $\times$, meaning more workloads fall into the memory-bound regime with each generation. {#tbl-widening-gap}

@tbl-widening-gap quantifies this trend. The ridge point increased from 139 FLOP/byte on the V100 to 563 FLOP/byte on the B200. An operation with arithmetic intensity of 200 FLOP/byte was compute-bound on the V100 and A100, memory-bound on the H100, and deeply memory-bound on the B200. This means that performance engineering techniques targeting memory efficiency, fusion, precision, and tiling, become *more* important with each hardware generation, not less. The engineering effort invested in FlashAttention and INT4 quantization today will yield even greater returns on future hardware.

### The Roofline Model {#sec-performance-engineering-roofline}

The **Roofline Model** provides a quantitative framework for diagnosing whether a workload is compute-bound or memory-bound on a specific piece of hardware. Introduced by Williams, Waterman, and Patterson (2009), the model plots achievable performance as a function of **arithmetic intensity**, defined as the ratio of floating-point operations to bytes transferred from memory.

::: {.callout-definition title="Arithmetic Intensity"}

***Arithmetic Intensity***\index{Arithmetic Intensity!definition} is the ratio of floating-point operations (FLOPs) performed to bytes of data moved from off-chip memory (typically HBM) for a given computation, measured in FLOP/byte. This ratio determines a workload's *regime of operation*: when arithmetic intensity falls below the hardware's ridge point ($P/B$, where $P$ is peak compute and $B$ is peak bandwidth), the workload is *bandwidth-bound* and performance scales with memory throughput; when arithmetic intensity exceeds the ridge point, the workload is *compute-bound* and performance scales with arithmetic throughput. Correctly classifying this regime is the first step in any optimization effort, because optimizing for the wrong bottleneck yields no improvement.

:::

For a given accelerator with peak compute $P$ (in FLOPS) and peak memory bandwidth $B$ (in bytes/second), the achievable performance of a workload with arithmetic intensity $I$ (in FLOP/byte) is:

$$
\text{Achievable FLOPS} = \min(P, \; B \times I)
$$ {#eq-roofline}

The transition point where these two limits intersect is the **ridge point**:

$$
I_{\text{ridge}} = \frac{P}{B}
$$ {#eq-ridge-point}

Workloads with $I < I_{\text{ridge}}$ are memory-bound: their performance is limited by how fast data can be loaded, not how fast it can be processed. Workloads with $I > I_{\text{ridge}}$ are compute-bound: the arithmetic units are the bottleneck. @fig-roofline-model illustrates this relationship graphically.

::: {.callout-note title="Figure: The Roofline Model" collapse="false"}

```{.tikz}
%| fig-cap: "**The Roofline Model**. Achievable performance (y-axis) as a function of arithmetic intensity (x-axis) on a log-log plot. The sloped line represents the memory bandwidth ceiling; the flat line represents the compute ceiling. Their intersection is the ridge point. Most transformer inference operations fall in the memory-bound region (left of the ridge point), while large batched GEMMs fall in the compute-bound region (right)."
%| fig-alt: "Log-log plot showing roofline model with memory bandwidth ceiling as diagonal line and compute ceiling as horizontal line, meeting at ridge point. Workload types are marked: LLM decode and element-wise ops on the left (memory-bound), large GEMM on the right (compute-bound)."
%| label: fig-roofline-model

\begin{tikzpicture}[>=stealth, scale=1.0]
  % Axes
  \draw[thick, ->] (0,0) -- (10,0) node[below] {Arithmetic Intensity (FLOP/byte)};
  \draw[thick, ->] (0,0) -- (0,7) node[above, rotate=90, anchor=south] {Achievable TFLOPS};

  % Axis labels (log scale markers)
  \node[below] at (1,0) {\small 1};
  \node[below] at (3,0) {\small 10};
  \node[below] at (5,0) {\small 100};
  \node[below] at (7,0) {\small 1000};

  \node[left] at (0,1) {\small 1};
  \node[left] at (0,3) {\small 100};
  \node[left] at (0,5) {\small 989};
  \node[left] at (0,6) {\small 1979};

  % Memory bandwidth ceiling (slope = bandwidth)
  \draw[blue, very thick] (0.5,0.5) -- (5.5,5.5);

  % Compute ceiling FP16
  \draw[red, very thick] (5.5,5.5) -- (9.5,5.5);

  % Compute ceiling FP8 (higher)
  \draw[red!50, thick, dashed] (4.8,6.2) -- (9.5,6.2);
  \draw[blue!50, thick, dashed] (0.5,0.9) -- (4.8,6.2);

  % Ridge point
  \fill[black] (5.5,5.5) circle (3pt);
  \node[above right] at (5.5,5.5) {\small Ridge Point};
  \node[below right, font=\scriptsize] at (5.5,5.2) {$\sim$295 FLOP/byte};

  % FP8 ridge point
  \fill[black!50] (4.8,6.2) circle (2pt);
  \node[above, font=\scriptsize] at (4.8,6.4) {FP8 Ridge};

  % Regions
  \node[blue, font=\small, rotate=0] at (2.5,1.5) {Memory-Bound};
  \node[red, font=\small] at (7.5,4.8) {Compute-Bound};

  % Workload markers
  \fill[orange] (1.2,1.2) circle (4pt);
  \node[right, font=\scriptsize, orange] at (1.4,1.0) {LLM Decode (B=1)};

  \fill[orange] (1.8,1.8) circle (4pt);
  \node[right, font=\scriptsize, orange] at (2.0,1.6) {Element-wise};

  \fill[green!60!black] (7.5,5.5) circle (4pt);
  \node[below, font=\scriptsize, green!60!black] at (7.5,5.3) {Large GEMM};

  \fill[purple] (3.8,3.8) circle (4pt);
  \node[right, font=\scriptsize, purple] at (4.0,3.6) {Attention};

  % Legend
  \draw[red, very thick] (0.5,6.8) -- (1.2,6.8);
  \node[right, font=\scriptsize] at (1.3,6.8) {FP16 Ceiling (989 TFLOPS)};
  \draw[red!50, thick, dashed] (0.5,6.4) -- (1.2,6.4);
  \node[right, font=\scriptsize] at (1.3,6.4) {FP8 Ceiling (1979 TFLOPS)};
  \draw[blue, very thick] (0.5,6.0) -- (1.2,6.0);
  \node[right, font=\scriptsize] at (1.3,6.0) {HBM BW (3.35 TB/s)};
\end{tikzpicture}
```

:::

The ridge point of the NVIDIA H100 at FP16 precision is:

$$
I_{\text{ridge}}^{\text{H100, FP16}} = \frac{989 \text{ TFLOPS}}{3.35 \text{ TB/s}} \approx 295 \text{ FLOP/byte}
$$

```{python}
#| echo: false
#| label: roofline-ridge-calc
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ROOFLINE RIDGE POINT COMPARISON
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-roofline — ridge point prose comparison
# │
# │ Goal: Verify and expose ridge-point values (P/B = FLOPS/BW in FLOP/byte) for
# │       A100 FP16 (~153), H100 FP16 (~295), and H100 FP8 (~591) to explain why
# │       each new generation makes more workloads memory-bound.
# │ Show: ~153 FLOP/byte (A100), ~295 (H100 FP16), ~591 (H100 FP8) — inline prose.
# │ How: Extract pre-computed ridge quantities from PerformanceSetup; convert to
# │       FLOP/byte using .m_as(flop/byte); guard ordering with check().
# │
# │ Imports: (none — values from perf-eng-setup cell), mlsys.formatting (check)
# │ Exports: a100_ridge_str, h100_fp16_ridge_str, h100_fp8_ridge_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class RooflineRidgeCalc:
    """Ridge point comparison across A100/H100 FP16/FP8."""

    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    # Already computed in setup cell

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────
    # Ridge points already computed; verify
    a100_ridge_val = PerformanceSetup.a100_ridge_fp16.m_as(flop/byte)
    h100_ridge_fp16_val = PerformanceSetup.h100_ridge_fp16.m_as(flop/byte)
    h100_ridge_fp8_val = PerformanceSetup.h100_ridge_fp8.m_as(flop/byte)

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    check(h100_ridge_fp16_val > a100_ridge_val,
          "H100 FP16 ridge should exceed A100 FP16 ridge")
    check(h100_ridge_fp8_val > h100_ridge_fp16_val,
          "H100 FP8 ridge should exceed H100 FP16 ridge")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    a100_ridge_str = f"{a100_ridge_val:.0f}"
    h100_fp16_ridge_str = f"{h100_ridge_fp16_val:.0f}"
    h100_fp8_ridge_str = f"{h100_ridge_fp8_val:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
a100_ridge_str = RooflineRidgeCalc.a100_ridge_str
h100_fp16_ridge_str = RooflineRidgeCalc.h100_fp16_ridge_str
h100_fp8_ridge_str = RooflineRidgeCalc.h100_fp8_ridge_str
```

This means that any operation performing fewer than `{python} h100_fp16_ridge_str` floating-point operations per byte loaded is memory-bound on the H100 at FP16. At FP8 precision, where compute doubles to `{python} h100_fp8_str` TFLOPS while bandwidth remains `{python} h100_hbm_bw_str` TB/s, the ridge point rises to approximately `{python} h100_fp8_ridge_str` FLOP/byte. The A100, with `{python} a100_fp16_str` TFLOPS and `{python} a100_hbm_bw_str` TB/s, has a lower ridge point of approximately `{python} a100_ridge_str` FLOP/byte at FP16. Each hardware generation increases compute faster than bandwidth, pushing the ridge point higher and making more workloads memory-bound.

@fig-shifting-roofline overlays the roofline models for four GPU generations on a single log-log plot, making the generational shift visible at a glance. The ridge point has grown from 139 FLOP/byte on the V100 to 625 FLOP/byte on the B200, a 4.5 $\times$ increase in seven years. An operation like naive self-attention, with an arithmetic intensity near 10 FLOP/byte, was memory-bound on every generation but falls progressively further below the ridge with each new chip. More critically, operations near 200 FLOP/byte, such as large matrix multiplications, transition from compute-bound on V100 to memory-bound on B200. The same kernel can change performance regime across hardware generations, a fact that demands re-profiling whenever hardware is upgraded.

::: {#fig-shifting-roofline fig-env="figure" fig-pos="htb" fig-cap="**The Shifting Roofline Across GPU Generations**. Overlaid roofline models for V100 through B200 show the ridge point growing from 139 to 625 FLOP/byte. Operations like naive attention, which were compute-bound on V100, become memory-bound on B200 as the roofline shifts. The same kernel can change performance regime across hardware generations." fig-alt="Log-log roofline plot for V100, A100, H100, B200 with ridge points and example ML operations marked"}
```{python}
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ SHIFTING ROOFLINE (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-shifting-roofline — roofline across GPU generations
# │
# │ Goal: Overlay rooflines for V100–B200; show ridge point growth 139→625;
# │       mark ops moving from compute-bound to memory-bound.
# │ Show: Log-log; four rooflines; ridge points; example ops.
# │ How: Peak TFLOPS, HBM BW; ridge = TFLOPS/BW; viz.setup_plot().
# │
# │ Imports: numpy (np), matplotlib.pyplot (plt), mlsys.viz (viz)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
import matplotlib.pyplot as plt
from mlsys import viz

fig, ax, COLORS, plt = viz.setup_plot(figsize=(9, 6))

# GPU specifications: (name, peak FP16 TFLOPS, HBM BW TB/s, color)
gpus = [
    ("V100", 125, 0.900, COLORS["VioletLine"]),
    ("A100", 312, 2.039, COLORS["BlueLine"]),
    ("H100", 989, 3.350, COLORS["GreenLine"]),
    ("B200", 5000, 8.000, COLORS["OrangeLine"]),
]

ai_range = np.logspace(0, 4, 500)  # 1 to 10000 FLOP/byte

for name, peak_tflops, bw_tbs, color in gpus:
    bw_bytes = bw_tbs * 1e12  # bytes/s
    peak_flops = peak_tflops * 1e12  # FLOPS
    ridge = peak_flops / bw_bytes  # FLOP/byte

    # Roofline: min(peak, BW * AI), expressed in TFLOPS
    attainable = np.minimum(peak_tflops, bw_tbs * 1e3 * ai_range / 1e3)
    # Simpler: attainable TFLOPS = min(peak_tflops, BW_TB/s * AI * 1e-0)
    # BW in TB/s * AI in FLOP/byte = TFLOP/s (since TB/s * FLOP/byte = 1e12 * FLOP / (1e12 * byte) * byte/s... )
    # Let's compute correctly:
    # attainable FLOPS = min(peak_flops, bw_bytes * AI)
    # attainable TFLOPS = attainable FLOPS / 1e12
    attainable_tflops = np.minimum(peak_tflops, (bw_tbs * ai_range))

    ax.plot(ai_range, attainable_tflops, color=color, linewidth=2.0, label=f"{name}")

    # Mark ridge point
    ax.plot(ridge, peak_tflops, "o", color=color, markersize=8, zorder=5)
    # Label ridge point
    offset_y = 1.25 if name != "B200" else 0.75
    ax.annotate(f"{name} ridge\n{ridge:.0f} FLOP/byte",
                xy=(ridge, peak_tflops), fontsize=7.5,
                color=color, fontweight="bold",
                ha="center", va="bottom" if name != "B200" else "top",
                xytext=(0, 8 if name != "B200" else -8),
                textcoords="offset points")

# Example ML operations as vertical dashed lines
ops = [
    ("LayerNorm\n(~5 FLOP/byte)", 5, "bottom"),
    ("Attention (naive)\n(~10 FLOP/byte)", 10, "bottom"),
    ("MatMul (large)\n(~200 FLOP/byte)", 200, "bottom"),
]

for label, ai_val, va_pos in ops:
    ax.axvline(x=ai_val, color=COLORS["primary"], linestyle=":", linewidth=1.0, alpha=0.5)
    ax.text(ai_val, 0.8, label, fontsize=7, color=COLORS["primary"],
            ha="center", va="bottom", rotation=0,
            bbox=dict(boxstyle="round,pad=0.2", facecolor="white", edgecolor="none", alpha=0.8))

# Arrow annotation for ridge shift
ax.annotate("",
            xy=(625, 4200), xytext=(139, 160),
            arrowprops=dict(arrowstyle="->", color="crimson", lw=2.0, linestyle="-"))
ax.text(90, 500, "Ridge point shifted\n4.5x in 7 years",
        fontsize=9, color="crimson", fontweight="bold",
        ha="center", va="center",
        bbox=dict(boxstyle="round,pad=0.3", facecolor=COLORS["RedL"], edgecolor="crimson", alpha=0.8))

ax.set_xscale("log")
ax.set_yscale("log")
ax.set_xlabel("Arithmetic Intensity (FLOP/byte)")
ax.set_ylabel("Attainable Performance (TFLOPS)")
ax.set_xlim(1, 10000)
ax.set_ylim(0.5, 10000)
ax.legend(loc="lower right", fontsize=9, title="GPU Generation")

plt.tight_layout()
plt.show()
```
:::

### Where ML Workloads Fall {#sec-performance-engineering-workload-placement}

Different ML operations have vastly different arithmetic intensities. Understanding where each falls on the roofline determines which optimization strategies apply.

```{python}
#| echo: false
#| label: workload-intensity-calc
# ┌─────────────────────────────────────────────────────────────────────────────
# │ WORKLOAD ARITHMETIC INTENSITY
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-workload-placement — classify ML ops
# │
# │ Goal: Compute arithmetic intensity (FLOP/byte) for a 4096×4096 FP16 GEMM,
# │       a GELU element-wise op, and LLM decode at batch=1 on a 4096-dim model
# │       to show why inference is predominantly memory-bound.
# │ Show: ~1365 FLOP/byte (GEMM, compute-bound), ~1.25 (GELU, memory-bound),
# │       ~1 (LLM decode batch=1, deeply memory-bound) — inline and in table.
# │ How: FLOP count / byte count; GEMM uses 2*M*N*K / (M*K + K*N + M*N)*bytes;
# │       element-wise uses ~5 FLOPs per element / (load + store) bytes.
# │
# │ Imports: (none — standalone calculation), mlsys.formatting (check)
# │ Exports: gemm_intensity_str, elem_intensity_str, decode_intensity_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class WorkloadIntensityCalc:
    """Arithmetic intensity for GEMM, GELU, and LLM decode operations."""

    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    # GEMM: C = A*B where A is MxK, B is KxN (FP16: 2 bytes/element)
    M, K, N = 4096, 4096, 4096
    bytes_per_elem = 2  # FP16

    # Element-wise (e.g., GELU): 1 op per element, load + store
    elem_count = M * K

    # LLM decode: batch=1, read full weight matrix
    decode_batch = 1
    decode_hidden = 4096

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────

    # GEMM: 2*M*N*K FLOPs, load A (M*K*2) + B (K*N*2) + store C (M*N*2)
    gemm_flops = 2 * M * N * K
    gemm_bytes = (M * K + K * N + M * N) * bytes_per_elem
    gemm_intensity = gemm_flops / gemm_bytes

    # Element-wise: ~5 FLOPs per element (GELU approximation), load+store
    elem_flops = 5 * elem_count
    elem_bytes = 2 * elem_count * bytes_per_elem  # load + store
    elem_intensity = elem_flops / elem_bytes

    # LLM decode step: batch=1, weight matrix is hidden*hidden
    decode_flops = 2 * decode_batch * decode_hidden * decode_hidden
    decode_weight_bytes = decode_hidden * decode_hidden * bytes_per_elem
    decode_activation_bytes = decode_batch * decode_hidden * bytes_per_elem
    decode_bytes = decode_weight_bytes + decode_activation_bytes
    decode_intensity = decode_flops / decode_bytes

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    check(gemm_intensity > 100, f"GEMM should have high AI, got {gemm_intensity}")
    check(elem_intensity < 5, f"Element-wise should have low AI, got {elem_intensity}")
    check(decode_intensity < 5, f"Decode should be memory-bound, got {decode_intensity}")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    gemm_intensity_str = f"{gemm_intensity:.0f}"
    elem_intensity_str = f"{elem_intensity:.1f}"
    decode_intensity_str = f"{decode_intensity:.1f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
gemm_intensity_str = WorkloadIntensityCalc.gemm_intensity_str
elem_intensity_str = WorkloadIntensityCalc.elem_intensity_str
decode_intensity_str = WorkloadIntensityCalc.decode_intensity_str
```

Large matrix multiplications (GEMMs) are the most compute-intensive operations in ML. A square matrix multiplication of dimension 4096 $\times$ 4096 in FP16 performs approximately 137 billion FLOPs while loading roughly 100 MB of data, yielding an arithmetic intensity of approximately `{python} gemm_intensity_str` FLOP/byte. This sits well above the H100's ridge point, making large GEMMs firmly compute-bound.

Element-wise operations tell the opposite story. A GELU activation applied to a 4096 $\times$ 4096 tensor performs roughly 5 operations per element but must load and store each element, yielding an arithmetic intensity of approximately `{python} elem_intensity_str` FLOP/byte. These operations are profoundly memory-bound, spending almost all their time waiting for data transfers rather than computing.

Autoregressive LLM decoding at batch size one represents the extreme case. Each decoding step reads the entire weight matrix (gigabytes of data) to produce a single output token. With a hidden dimension of 4096 and batch size 1, the arithmetic intensity is approximately `{python} decode_intensity_str` FLOP/byte, deep in the memory-bound regime. This explains why LLM token generation achieves a tiny fraction of peak FLOPS: the GPU spends nearly all its time reading weights, not multiplying them.

| **Operation**                      | **Arithmetic Intensity** | **H100 FP16 Regime** | **Primary Bottleneck** |
|:-----------------------------------|:-------------------------|:---------------------|:-----------------------|
| **GEMM (4096 $\times$ 4096)**      | ~1,365 FLOP/byte         | Compute-bound        | Tensor core throughput |
| **Self-Attention (seq=2048)**      | ~50--200 FLOP/byte       | Memory-bound         | HBM bandwidth          |
| **Element-wise (GELU, LayerNorm)** | ~1--3 FLOP/byte          | Memory-bound         | HBM bandwidth          |
| **LLM Decode (batch=1)**           | ~1--2 FLOP/byte          | Memory-bound         | HBM bandwidth          |

: **Arithmetic Intensity of Common ML Operations**. Most operations in transformer inference, aside from large batched GEMMs, fall below the H100's ridge point and are therefore memory-bound. Performance engineering focuses on reducing the memory traffic of these operations. {#tbl-arithmetic-intensity}

The central insight from @tbl-arithmetic-intensity is that the majority of operations in a transformer inference pipeline are memory-bound. Training workloads with large batch sizes shift more operations into the compute-bound regime because GEMM dimensions scale with batch size. But inference, especially autoregressive generation, is dominated by memory-bound operations. This is why the techniques in the rest of this chapter, fusion, tiling, reduced precision, and algorithmic shortcuts, all target the same fundamental problem: reducing bytes moved per operation.

This observation also explains a common source of confusion: why GPU benchmarks (which report peak TFLOPS) often fail to predict real inference performance. Two GPUs with different TFLOPS but identical memory bandwidth will achieve virtually identical LLM decode throughput at batch size 1, because decode is entirely memory-bound. The correct metric for comparing GPUs for LLM inference is not FLOPS but rather the combination of memory bandwidth and memory capacity. Bandwidth determines the token generation rate, and capacity determines the maximum batch size (and therefore throughput). Only at large batch sizes, where decode approaches the compute-bound regime, do the FLOPS differences between GPUs translate into throughput differences.

### Batch Size as the Universal Control Knob {#sec-performance-engineering-batch-size}

Batch size is the most powerful, and most constrained, lever for performance. Increasing the batch size transforms the arithmetic intensity of every operation. For an LLM decode step, the arithmetic intensity scales linearly with batch size:

$$
I_{\text{decode}}(\text{batch}) = \frac{2 \times \text{params} \times \text{batch}}{\text{params} \times \text{bytes\_per\_param} + \text{batch} \times d \times \text{bytes\_per\_elem}}
$$

At batch size 1, the denominator is dominated by the weight term ($\text{params} \times \text{bytes\_per\_param}$), and $I \approx 2 / \text{bytes\_per\_param} \approx 1$ FLOP/byte for FP16. At batch size 256, the input term becomes significant, and $I \approx 2 \times 256 / \text{bytes\_per\_param} \approx 256$ FLOP/byte, approaching the compute-bound regime.

This means that at large batch sizes, the GPU transitions from memory-bound to compute-bound, and utilization increases dramatically. A single H100 achieving 2% utilization at batch size 1 may achieve 60% utilization at batch size 256. The economic implication is stark: the cost per token decreases by 30 $\times$ as batch size increases from 1 to 256.

The constraint is memory: each additional request in the batch requires its own KV cache, and the total KV cache across all requests must fit in GPU memory alongside the model weights. A 70B model with 80 GB of weights in FP16 leaves almost no room for KV cache on an 80 GB GPU. This is precisely why the precision engineering techniques covered later in this chapter matter: INT4 weight quantization frees 60 GB for KV cache, enabling batch sizes that transform the economics of serving.

The continuous batching systems introduced in @sec-inference-scale manage batch size dynamically, adding and removing requests as they complete. Performance engineering's role is to maximize the effective batch size by minimizing the per-request memory footprint, primarily through KV cache compression and weight quantization.

A critical enabler for large batch sizes is **paged KV cache management**, introduced by vLLM (Kwon et al., 2023). Traditional KV cache implementations pre-allocate contiguous memory for each request's maximum possible sequence length. If the maximum is 4,096 tokens but the average is 500, approximately 88% of the allocated memory is wasted. Paged attention divides the KV cache into fixed-size blocks (pages), allocated on demand as the sequence grows. This eliminates memory fragmentation and enables near-100% utilization of the KV cache memory budget. The performance impact is indirect but substantial: by reducing memory waste, paged attention enables 2--4 $\times$ larger effective batch sizes, which in turn improve throughput and GPU utilization through the batch size mechanism described above.

The interaction between paged attention and KV cache quantization is multiplicative. Paged attention reduces memory *waste* (from fragmentation), while quantization reduces memory *usage* (from precision). Together, they can increase the effective batch size by 8--16 $\times$ compared to a baseline system with pre-allocated FP16 KV caches, fundamentally changing the economics of LLM serving.

### The Prefill-Decode Decomposition {#sec-performance-engineering-prefill-decode}

Modern LLM serving systems decompose each request into two distinct phases with fundamentally different performance characteristics, a distinction that drives system architecture and optimization strategy.

The **prefill phase** processes the entire input prompt in parallel. If the prompt contains $P$ tokens, the prefill phase executes a single forward pass over all $P$ tokens simultaneously. The GEMM operations have shape [$P$, $d$] $\times$ [$d$, $d$], making the batch dimension equal to $P$. For a prompt of 1024 tokens, this is arithmetically intensive: the arithmetic intensity is approximately $2 \times 1024 / 2 = 1024$ FLOP/byte for FP16 weights, well into the compute-bound regime. Prefill is therefore limited by Tensor Core throughput, not memory bandwidth.

The **decode phase** generates output tokens one at a time, autoregressively. Each step has a batch dimension of 1 (for a single request) or the number of concurrent requests (for batched serving). At batch size 1, decode is deeply memory-bound as analyzed in @sec-performance-engineering-workload-placement.

This decomposition has profound implications for system design. A system optimized for prefill (maximizing FLOPS utilization) would use large matrix sizes and high compute throughput. A system optimized for decode (maximizing bandwidth utilization) would use aggressive quantization and memory optimization. A real serving system must handle both phases, often simultaneously across different requests in a continuous batching framework.

**Disaggregated serving** addresses this by running prefill and decode on separate hardware pools. Prefill servers are optimized for compute (fewer, higher-FLOPS GPUs), while decode servers are optimized for memory bandwidth and capacity (more memory per GPU, aggressive quantization). The KV cache computed during prefill is transferred to a decode server, which handles the subsequent autoregressive generation. This disaggregation allows each phase to use hardware and software configurations tuned for its specific bottleneck.

The performance characteristics of each phase determine which optimization techniques apply. FlashAttention provides the largest speedup during prefill, where the quadratic attention computation dominates. KV cache quantization and speculative decoding apply exclusively to the decode phase. Precision engineering (FP8/INT4 weights) benefits both phases, but through different mechanisms: prefill benefits from doubled compute throughput (FP8 Tensor Cores), while decode benefits from doubled effective bandwidth (half the bytes per weight read).

::: {.callout-notebook title="The Roofline Diagnostic"}

**Problem**: You are deploying a 70B parameter LLM on 8 $\times$ H100 GPUs with tensor parallelism. At batch size 1, each GPU holds approximately 17.5B parameters in FP16 (35 GB of weights). Each decode step reads all weights to produce one token. What is the achieved arithmetic intensity, and what is the theoretical maximum token generation rate?

**The Math**:

*Step 1: Arithmetic Intensity.*
Each decode step per GPU: FLOPs $= 2 \times 17.5 \times 10^9 = 35 \times 10^9$ FLOP. Bytes loaded $= 17.5 \times 10^9 \times 2 = 35 \times 10^9$ bytes $= 35$ GB.

$$
I = \frac{35 \times 10^9 \text{ FLOP}}{35 \times 10^9 \text{ bytes}} = 1.0 \text{ FLOP/byte}
$$

This is far below the H100 ridge point of ~295 FLOP/byte. The operation is deeply memory-bound.

*Step 2: Token Rate.*
Since the operation is memory-bound, performance is limited by bandwidth, not compute:

$$
t_{\text{decode}} = \frac{35 \text{ GB}}{3.35 \text{ TB/s}} \approx 10.4 \text{ ms per token}
$$

This yields approximately 96 tokens/second per GPU, or about 96 tokens/second for the model (since tensor parallelism does not multiply throughput for memory-bound decode). In practice, overheads from KV cache reads and NVLink synchronization reduce this to 40--70 tokens/second.

**Takeaway**: At batch size 1, fewer than 0.4% of the H100's FP16 FLOPS are utilized. The only ways to improve are: (a) increase batch size to amortize weight reads, (b) reduce weight bytes via quantization, or (c) use speculative decoding to generate multiple tokens per weight read.

:::

The roofline model establishes the physics that constrains all subsequent optimization. With this diagnostic framework in place, we can now examine the first and most impactful strategy for breaking through the memory wall: keeping data in SRAM instead of round-tripping through HBM.

## Operator Fusion and Kernel Engineering {#sec-performance-engineering-fusion}

Consider the simple sequence of operations $Y = \text{LayerNorm}(\text{GELU}(XW + b))$. In a naive implementation, the GPU writes the output of the matrix multiply back to main memory, reads it back for the GELU, writes it out again, and reads it one final time for the LayerNorm. This redundant data movement shatters performance. Operator fusion solves this by keeping intermediate results in ultra-fast registers, executing the entire sequence in a single trip to memory.

### The Kernel Launch Problem {#sec-performance-engineering-kernel-launch}

Each GPU kernel launch involves overhead: the CPU must prepare launch parameters, dispatch to the GPU command queue, and the GPU must schedule thread blocks across its streaming multiprocessors (SMs). For a small element-wise operation on a modern GPU, this overhead can be 5--20 $\mu$s, a time during which a memory-bound kernel might have already completed its useful work. When a transformer layer comprises dozens of small operations (add, multiply, normalize, activate), the cumulative launch overhead becomes significant.

More importantly, each unfused kernel must materialize its output in HBM. Consider a sequence of three operations: $Y = \text{LayerNorm}(\text{GELU}(XW + b))$. Without fusion, this requires:

1. **GEMM kernel**: Read $X$ and $W$ from HBM, compute $XW + b$, write result $Z_1$ to HBM.
2. **GELU kernel**: Read $Z_1$ from HBM, compute GELU($Z_1$), write $Z_2$ to HBM.
3. **LayerNorm kernel**: Read $Z_2$ from HBM, compute LayerNorm($Z_2$), write $Y$ to HBM.

Intermediate tensors $Z_1$ and $Z_2$ each occupy the same memory as the output $Y$. For a hidden dimension of 4096 and batch size of 2048 in FP16, each intermediate tensor is $4096 \times 2048 \times 2 = 16$ MB. The unfused execution reads and writes 32 MB of intermediate data that a fused kernel avoids entirely by holding $Z_1$ and $Z_2$ in registers or shared memory (SRAM) within the SM.

@fig-fusion-before-after contrasts these two execution paths, making the HBM traffic savings visible.

::: {#fig-fusion-before-after fig-env="figure" fig-pos="htb" fig-cap="**Operator Fusion: Before and After**. Left: three separate kernels each read from and write to HBM, materializing intermediate tensors $Z_1$ and $Z_2$ (5 HBM transfers total). Right: a single fused kernel reads $X$ and $W$ once, performs all three operations with intermediates held in SRAM, and writes only the final output $Y$ (2 HBM transfers). The fused path eliminates 32 MB of redundant HBM traffic per layer." fig-alt="Two side-by-side dataflow diagrams. Left unfused path shows 5 arrows between operations and HBM. Right fused path shows 2 arrows with intermediates staying in SRAM."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.85, transform shape]
  \definecolor{BlueLine}{HTML}{006395}
  \definecolor{BlueL}{HTML}{D1E6F3}
  \definecolor{GreenLine}{HTML}{008F45}
  \definecolor{GreenL}{HTML}{D4EFDF}
  \definecolor{OrangeLine}{HTML}{E67817}
  \definecolor{OrangeL}{HTML}{FCE4CC}
  \definecolor{RedLine}{HTML}{CB202D}
  \definecolor{RedL}{HTML}{F5D2D5}

  \tikzset{
    op/.style={draw=BlueLine, fill=BlueL, rounded corners=2pt, thick,
               minimum width=2.0cm, minimum height=0.6cm, align=center, font=\scriptsize\bfseries},
    mem/.style={draw=RedLine, fill=RedL!40, rounded corners=2pt, thick,
                minimum width=2.0cm, minimum height=0.5cm, align=center, font=\scriptsize},
    sram/.style={draw=GreenLine, fill=GreenL, rounded corners=2pt, thick,
                 minimum width=2.0cm, minimum height=0.5cm, align=center, font=\scriptsize},
    fusedbox/.style={draw=GreenLine, fill=GreenL!20, rounded corners=6pt, thick,
                     inner sep=6pt},
    hbmio/.style={-{Triangle[width=4pt,length=3pt]}, thick, RedLine},
    sramio/.style={-{Triangle[width=4pt,length=3pt]}, thick, GreenLine}
  }

  % === LEFT: Unfused ===
  \node[font=\small\bfseries, text=RedLine] at (1.5, 5.5) {Unfused (3 Kernels)};

  % HBM at top
  \node[mem, minimum width=3.0cm] (hbm_l) at (1.5, 4.5) {HBM (Off-Chip)};

  % Operations
  \node[op] (gemm) at (1.5, 3.2) {GEMM};
  \node[op] (gelu) at (1.5, 1.8) {GELU};
  \node[op] (ln) at (1.5, 0.4) {LayerNorm};

  % HBM read/write arrows (5 total round trips)
  \draw[hbmio] (hbm_l.south) -- node[left, font=\tiny, text=RedLine] {read} (gemm.north);
  \draw[hbmio] (gemm.east) -- ++(0.6,0) |- node[right, font=\tiny, text=RedLine, pos=0.2] {$Z_1$} (hbm_l.east);
  \draw[hbmio] (hbm_l.south west) ++(0.3,0) -- ++(0,-2.1) -- (gelu.west);
  \draw[hbmio] (gelu.east) -- ++(0.6,0) |- node[right, font=\tiny, text=RedLine, pos=0.2] {$Z_2$} ++(0,2.7);
  \draw[hbmio] ([xshift=-5pt]hbm_l.south) -- ++(0,-3.5) -- (ln.west);
  \draw[hbmio] (ln.south) -- ++(0,-0.4) node[below, font=\tiny] {$Y$ to HBM};

  % Count annotation
  \node[font=\scriptsize\bfseries, text=RedLine] at (1.5, -0.8) {5 HBM transfers};
  \node[font=\tiny, text=RedLine] at (1.5, -1.2) {32 MB intermediate traffic};

  % === RIGHT: Fused ===
  \node[font=\small\bfseries, text=GreenLine] at (8.0, 5.5) {Fused (1 Kernel)};

  % HBM at top
  \node[mem, minimum width=3.0cm] (hbm_r) at (8.0, 4.5) {HBM (Off-Chip)};

  % Fused kernel box
  \node[fusedbox, minimum width=3.2cm, minimum height=3.2cm] (fbox) at (8.0, 1.8) {};

  % Operations inside fused box
  \node[op] (f_gemm) at (8.0, 3.0) {GEMM};
  \node[op] (f_gelu) at (8.0, 1.8) {GELU};
  \node[op] (f_ln) at (8.0, 0.6) {LayerNorm};

  % SRAM label
  \node[sram, minimum width=1.0cm] at (10.0, 1.8) {SRAM};

  % Internal SRAM arrows
  \draw[sramio] (f_gemm) -- node[right, font=\tiny, text=GreenLine] {$Z_1$} (f_gelu);
  \draw[sramio] (f_gelu) -- node[right, font=\tiny, text=GreenLine] {$Z_2$} (f_ln);

  % Only 2 HBM transfers
  \draw[hbmio] (hbm_r.south) -- node[left, font=\tiny, text=RedLine] {read $X$,$W$} (fbox.north);
  \draw[hbmio] (fbox.south) -- ++(0,-0.4) node[below, font=\tiny] {$Y$ to HBM};

  % Count annotation
  \node[font=\scriptsize\bfseries, text=GreenLine] at (8.0, -0.8) {2 HBM transfers};
  \node[font=\tiny, text=GreenLine] at (8.0, -1.2) {0 MB intermediate traffic};

  % Speedup annotation
  \draw[OrangeLine, ultra thick, -{Triangle[width=6pt,length=5pt]}] (4.2, 1.8) -- (5.3, 1.8)
    node[midway, above, font=\scriptsize\bfseries, text=OrangeLine] {2.5$\times$ faster};

\end{tikzpicture}
```
:::

In a naive implementation without operator fusion, executing one Transformer layer requires roughly 50 separate kernel launches. If each launch incurs a 10-microsecond overhead, the system spends 500 microseconds purely on dispatch latency. If the actual arithmetic execution of the layer takes only 2 milliseconds, the launch overhead consumes 20% of the total wall-clock time, leaving the GPU compute units idle for one-fifth of the inference cycle. This "launch-bound" regime limits the benefits of faster hardware; doubling the GPU's FLOPs does nothing to reduce the 500-microsecond fixed cost. Operator fusion addresses this by compiling these 50 discrete operations into a small handful of fused kernels—often reducing the count to 5–10 launches—thereby reclaiming the lost cycles and shifting the workload back towards a compute-bound profile.

### Fusion Categories {#sec-performance-engineering-fusion-categories}

GPU kernel fusion falls into three categories, each with different complexity and performance impact.

**Element-wise fusion** is the simplest form: consecutive element-wise operations (add, multiply, activation functions) are combined into a single kernel. Because each output element depends on exactly one input element, this fusion is always legal and straightforward to implement. Every modern deep learning framework performs element-wise fusion automatically.

**Reduction fusion** combines an element-wise operation with a subsequent reduction (such as summing elements for a loss function, or computing mean and variance for layer normalization). This is more complex because reductions require inter-thread communication within the kernel. Reductions need warp-level shuffle instructions or shared memory to aggregate partial results across threads. Despite this complexity, the memory savings are substantial: the intermediate tensor before the reduction never materializes in HBM. For layer normalization specifically, reduction fusion avoids writing the large pre-normalization tensor to HBM and reading it back for the mean/variance computation.

**Operator-specific fusion** is the most impactful and the most difficult. These are custom kernels designed for a specific sequence of operations, such as fused attention or fused GEMM-bias-activation. The kernel architect must reason about data flow, shared memory allocation, and thread scheduling simultaneously. The payoff is transformative: FlashAttention, which we examine next, reduces attention memory traffic from quadratic to linear in sequence length.

To appreciate the quantitative impact, consider each category applied to a single transformer layer with hidden dimension 4096 and batch size 2048 in FP16. Element-wise fusion of a bias-GELU-dropout chain eliminates two intermediate tensors of 16 MB each, saving 64 MB of HBM traffic (two writes plus two reads) per layer. Across 80 layers, this reclaims 5.1 GB of memory bandwidth per forward pass. Reduction fusion of LayerNorm avoids materializing the pre-normalization tensor (16 MB) and the intermediate mean/variance statistics, saving an additional 48 MB per layer. Operator-specific attention fusion (FlashAttention) provides the largest single gain: for a sequence length of 8192, it eliminates the 128 MB per-head attention score matrix, saving over 4 GB per layer across 32 heads. The cumulative effect of all three fusion categories can reduce total HBM traffic by 60--80% for a transformer forward pass, translating directly into proportional wall-clock speedup for memory-bound workloads.

### CUDA Graphs: Eliminating Launch Overhead {#sec-performance-engineering-cuda-graphs}

An orthogonal technique for reducing the overhead term in the Iron Law is **CUDA Graphs**. While operator fusion combines multiple operations into fewer kernels, CUDA Graphs eliminate the CPU overhead of launching those kernels.

In standard PyTorch execution, each kernel launch requires the CPU to push a command to the GPU's command queue. For a transformer decoder layer with 30+ kernels, this CPU-to-GPU roundtrip (typically 5--10 $\mu$s per launch) accumulates to 150--300 $\mu$s per layer. For a 70-layer model, kernel launch overhead alone contributes 10--20 ms per forward pass, a significant fraction of the total time for memory-bound inference.

CUDA Graphs address this by recording a sequence of GPU operations (kernel launches, memory copies) into a replayable graph. The recording happens once during a warmup phase. On subsequent iterations, replaying the graph requires only a single CPU-to-GPU command that dispatches the entire recorded sequence, reducing launch overhead to approximately 5--10 $\mu$s total regardless of the number of kernels.

The benefit is substantial: for a model with 30+ kernels per layer and 70+ layers, the baseline kernel launch overhead can exceed 15 ms per forward pass. CUDA Graphs reduce this to under 0.1 ms, reclaiming 15 ms that translates directly to higher token generation rates.

The constraint is that CUDA Graphs require deterministic execution: the sequence of operations, tensor shapes, and memory addresses must be identical across replays. This conflicts with dynamic inference patterns like variable-length sequences, changing batch sizes, and conditional computation (early exit, MoE routing). In practice, CUDA Graphs are most effective for the decode phase of LLM serving, where the computation pattern is repetitive (same operations per token), and less useful for the prefill phase, where input lengths vary.

The combination of operator fusion (reducing the number of kernels) and CUDA Graphs (reducing the per-kernel overhead) can together eliminate nearly all non-compute overhead from the forward pass. When profiling reveals that kernel launch gaps constitute more than 10% of execution time, CUDA Graphs should be the first intervention considered.

### FlashAttention: Tiled Attention as a System Primitive {#sec-performance-engineering-flashattention}

Standard self-attention computes $\text{Softmax}(QK^T / \sqrt{d_k})V$, where $Q$, $K$, and $V$ are matrices of shape [sequence length $\times$ head dimension]. The na\"ive implementation materializes the full $N \times N$ attention matrix $S = QK^T$ in HBM, where $N$ is the sequence length. For $N = 8192$ and FP16 precision, this matrix alone consumes $8192 \times 8192 \times 2 = 128$ MB per attention head. At 32 heads, the materialized attention matrices require 4 GB per layer, dominating memory traffic for the entire forward pass.

```{python}
#| echo: false
#| label: flash-attention-savings
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLASHATTENTION MEMORY SAVINGS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-operator-fusion FlashAttention prose
# │
# │ Goal: Quantify HBM traffic reduction from tiling attention for a 32-head,
# │       seq_len=8192, head_dim=128, FP16 configuration to make the O(N²)→O(N)
# │       memory savings concrete and motivate the algorithm.
# │ Show: ~8,590 MB naive vs ~134 MB FlashAttention → ~64x HBM traffic reduction,
# │       inline in the paragraph after FlashAttention is introduced.
# │ How: Naive: 2*N²*heads*bytes (write+read score matrix); FA: 4*N*d*heads*bytes
# │       (read Q,K,V + write O once); savings = naive / fa.
# │
# │ Imports: (none — standalone calculation), mlsys.formatting (check)
# │ Exports: naive_mb_str, flash_mb_str, savings_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class FlashAttentionSavings:
    """HBM traffic comparison: naive attention vs FlashAttention."""

    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    seq_len = 8192
    head_dim = 128
    num_heads = 32
    bytes_per_elem_fp16 = 2

    # SRAM block size (typical for H100)
    sram_block = 128  # tile size in sequence dimension

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────

    # Naive attention HBM traffic per head:
    # Write S = Q*K^T (N*N), read S for softmax, write P = softmax(S), read P for P*V
    # Approximately: 4 * N^2 * bytes (read Q, K for GEMM; write S; read S; write P; read P, V)
    # Simplified: dominant term is materializing NxN matrix
    naive_attn_bytes_per_head = 2 * seq_len * seq_len * bytes_per_elem_fp16  # write + read S
    naive_attn_bytes_total = naive_attn_bytes_per_head * num_heads

    # FlashAttention HBM traffic per head:
    # Read Q, K, V once (3 * N * d * bytes), write O once (N * d * bytes)
    # No NxN materialization
    flash_attn_bytes_per_head = (3 + 1) * seq_len * head_dim * bytes_per_elem_fp16
    flash_attn_bytes_total = flash_attn_bytes_per_head * num_heads

    # Savings ratio
    savings_ratio = naive_attn_bytes_total / flash_attn_bytes_total

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    check(savings_ratio > 10,
          f"FlashAttention should save >10x HBM traffic, got {savings_ratio:.1f}x")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    naive_mb_str = f"{naive_attn_bytes_total / 1e6:.0f}"
    flash_mb_str = f"{flash_attn_bytes_total / 1e6:.0f}"
    savings_str = f"{savings_ratio:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
naive_mb_str = FlashAttentionSavings.naive_mb_str
flash_mb_str = FlashAttentionSavings.flash_mb_str
savings_str = FlashAttentionSavings.savings_str
```

FlashAttention, introduced by Dao et al. (2022), reformulates attention as a **tiled** computation. Instead of materializing the full $N \times N$ attention matrix, it processes $Q$, $K$, and $V$ in small blocks that fit in on-chip SRAM. The algorithm loads a block of $Q$ rows and iterates over blocks of $K$ and $V$ columns, computing partial attention scores and maintaining running statistics (online softmax) to produce the exact result without ever storing the full attention matrix in HBM.

The HBM traffic reduction is dramatic. For a sequence length of 8,192, 32 heads, and head dimension 128 in FP16, the na\"ive attention reads and writes approximately `{python} naive_mb_str` MB of attention matrices through HBM. FlashAttention reads $Q$, $K$, $V$ and writes $O$ once each, totaling approximately `{python} flash_mb_str` MB. This is a `{python} savings_str` $\times$ reduction in HBM traffic, translating directly into a proportional speedup for this memory-bound operation.

The key insight behind FlashAttention is the **online softmax** trick, which makes tiling possible for an operation that appears to require global information. Standard softmax computes $\text{softmax}(s_i) = e^{s_i} / \sum_j e^{s_j}$, but for numerical stability it first subtracts the global maximum: $\text{softmax}(s_i) = e^{s_i - m} / \sum_j e^{s_j - m}$ where $m = \max_j s_j$. Finding this global maximum seems to require seeing all scores first, which would force materializing the full $N \times N$ matrix.

The online algorithm avoids this by maintaining running statistics that are updated incrementally as each tile is processed. When processing tile $t$, the algorithm:

1. Computes a local block of scores $S_t = Q_{\text{block}} K_t^T$.
2. Updates the running maximum: $m_{\text{new}} = \max(m_{\text{old}}, \max(S_t))$.
3. Rescales the previous running sum and output: multiply by $e^{m_{\text{old}} - m_{\text{new}}}$ to correct for the updated maximum.
4. Computes the local softmax contribution using $m_{\text{new}}$ and accumulates into the running output.

After processing all tiles, the running output contains the exact same result as the standard algorithm. The rescaling step (step 3) is the critical innovation: it allows the algorithm to "fix up" previous partial results when a new tile reveals a larger maximum value. This correction is exact, not approximate, so FlashAttention produces bit-identical results to standard attention for a given numerical precision.

The cost of this tiling is additional arithmetic: the rescaling operations in step 3 add FLOPs that the standard algorithm does not perform. But because the operation is profoundly memory-bound (the arithmetic intensity of standard attention is roughly 1--10 FLOP/byte for typical sequence lengths), the additional compute is "free" in the sense that the GPU's arithmetic units would otherwise be idle, waiting for HBM data transfers. Trading extra compute for fewer memory accesses is profitable whenever the operation is memory-bound, the central principle of this entire chapter.

A concrete numerical example clarifies the memory savings. Consider one attention head with sequence length $N = 8192$ and head dimension $d = 128$. The $Q$, $K$, $V$ matrices are each $8192 \times 128$ in FP16, occupying $8192 \times 128 \times 2 = 2$ MB each (6 MB total for one head). The output matrix $O$ is the same size (2 MB). The total input/output data is therefore 8 MB per head.

The na\"ive algorithm computes $S = QK^T$, a matrix of shape $8192 \times 8192$. In FP16, this score matrix occupies $8192 \times 8192 \times 2 = 128$ MB per head. After applying softmax, the result $P = \text{softmax}(S)$ is also $128$ MB. Computing $PV$ requires reading $P$ again. In total, the na\"ive algorithm reads $Q$, $K$, $V$ from HBM (6 MB), writes $S$ (128 MB), reads $S$ for softmax (128 MB), writes $P$ (128 MB), reads $P$ for the final multiply (128 MB), and writes $O$ (2 MB). The total HBM traffic is approximately 520 MB per head, dominated by the quadratic intermediates.

FlashAttention processes the computation in tiles of size $B_r \times B_c$ (typically 128 $\times$ 128 on H100). For one tile, the algorithm loads a block of $Q$ ($128 \times 128 \times 2 = 32$ KB), a block of $K$ ($128 \times 128 \times 2 = 32$ KB), and a block of $V$ (32 KB), totaling 96 KB. This fits comfortably in the H100's 228 KB of shared memory per SM. The tile score $S_{\text{tile}} = Q_{\text{tile}} K_{\text{tile}}^T$ is computed and consumed entirely within SRAM; it is never written to HBM. The algorithm iterates over $8192 / 128 = 64$ column tiles for each of $64$ row tiles, but the total HBM traffic is just the cost of reading $Q$, $K$, $V$ once (6 MB) and writing $O$ once (2 MB), totaling 8 MB per head. This is 65 $\times$ less HBM traffic than the na\"ive algorithm, and the ratio grows quadratically with sequence length.

FlashAttention-2 (Dao, 2023) further optimizes the algorithm for modern GPU architectures by restructuring the parallelism pattern. The original FlashAttention parallelizes over batch and head dimensions, meaning each thread block handles one (batch, head) pair and iterates over the full sequence. FlashAttention-2 additionally parallelizes over the sequence dimension of the query matrix, distributing work across thread blocks more efficiently and achieving better occupancy on GPUs with many streaming multiprocessors. It also reduces the number of non-GEMM FLOPs by restructuring the rescaling operations and exploiting the asymmetry between the Q loop (outer) and K/V loop (inner).

FlashAttention-3 (Dao et al., 2024) targets the H100's new hardware features: FP8 Tensor Cores and the Tensor Memory Accelerator (TMA). By computing attention in FP8 with selective FP16 accumulation, FlashAttention-3 achieves near-peak FP8 utilization for the attention operation, further closing the gap between achieved and theoretical performance.

::: {.callout-war-story title="The FlashAttention Breakthrough"}
In 2022, Tri Dao challenged the prevailing wisdom that the attention mechanism's $O(N^2)$ complexity required better matrix multiplication kernels. His insight was that the bottleneck wasn't compute, but memory hierarchy. Standard attention materialized the massive $N \times N$ attention score matrix in high-latency HBM. FlashAttention restructured the algorithm using tiling to keep running statistics in on-chip SRAM, computing the softmax without ever writing the full matrix to global memory. This reduced memory complexity to linear $O(N)$ and wall-clock time by 2-4x. Within six months, it was integrated into PyTorch, TensorFlow, and JAX, becoming the default attention implementation for the industry.
:::

@fig-flashattention-memory-savings quantifies the memory advantage across sequence lengths. Standard attention allocates the full $N \times N$ score matrix in HBM, while FlashAttention maintains only $O(N)$ running statistics. The gap widens quadratically: at a typical 8K context, FlashAttention uses 4,096$\times$ less attention memory; at 64K long-context, the savings reach 32,768$\times$.

::: {#fig-flashattention-memory-savings fig-env="figure" fig-pos="htb" fig-cap="FlashAttention Memory Savings: O(N²) → O(N). Standard attention allocates the full N × N score matrix in HBM, while FlashAttention maintains only O(N) running statistics. The shaded region represents memory freed for KV cache, activations, or larger batch sizes. At long-context lengths (64K+), the savings exceed four orders of magnitude." fig-alt="Log-log plot of attention memory versus sequence length. Standard O(N²) curve rises steeply; FlashAttention O(N) stays flat. Shaded region shows savings, with annotations at 8K and 65K."}
```{python}
# ┌─────────────────────────────────────────────────────────────────────────────
# │ FLASHATTENTION MEMORY SAVINGS (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-flashattention-memory-savings — O(N²) vs O(N) memory
# │
# │ Goal: Plot standard attention (N²) vs FlashAttention (N) memory vs seq len;
# │       show 4+ orders magnitude savings at 64K+.
# │ Show: Two curves on log scale; shaded savings region.
# │ How: N = logspace(512, 131072); bytes = 2*N^2 vs 2*N*...; matplotlib.
# │
# │ Imports: numpy (np), matplotlib.pyplot (plt)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import numpy as np
import matplotlib.pyplot as plt

plt.style.use('seaborn-v0_8-whitegrid')
plt.figure(figsize=(10, 6))

N = np.logspace(np.log2(512), np.log2(131072), num=200, base=2.0)

bytes_per_element_fp16 = 2
num_heads = 32
batch_size = 1

mem_standard = (N**2 * bytes_per_element_fp16 * num_heads * batch_size) / 1e9
mem_flash = (N * 4 * num_heads * batch_size) / 1e9

plt.plot(N, mem_standard, label='Standard Attention O(N²)', color='#CC5500', linewidth=2)
plt.plot(N, mem_flash, label='FlashAttention O(N)', color='#006395', linewidth=2)
plt.fill_between(N, mem_flash, mem_standard, color='#D1E6F3', alpha=0.6, label='Memory Savings')

plt.xscale('log')
plt.yscale('log')

plt.xlabel('Sequence Length (N)')
plt.ylabel('Attention Memory (GB)')
plt.title('FlashAttention Memory Savings: O(N²) → O(N)', fontsize=14)

plt.axvline(x=8192, color='gray', linestyle='--', linewidth=1.5)
plt.text(8192, plt.ylim()[0]*1.5, ' Typical LLM context (8K)', rotation=90,
         verticalalignment='bottom', color='black')

plt.axvline(x=65536, color='gray', linestyle='--', linewidth=1.5)
plt.text(65536, plt.ylim()[0]*1.5, ' Long context (65K)', rotation=90,
         verticalalignment='bottom', color='black')

def annotate_ratio(n_val):
    ratio = n_val / 2
    mem_val = (n_val**2 * bytes_per_element_fp16 * num_heads) / 1e9
    plt.annotate(f'{int(ratio):,}x Savings',
                 xy=(n_val, mem_val),
                 xytext=(n_val, mem_val * 0.25),
                 arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=6),
                 ha='center', va='top', fontsize=9,
                 bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="black", lw=0.5))

annotate_ratio(8192)
annotate_ratio(65536)

plt.legend()
plt.grid(True, which="both", ls="-", alpha=0.7)
plt.tight_layout()
fig = plt.gcf()
```
:::

### Ring Attention for Extreme Sequences {#sec-performance-engineering-ring-attention}

FlashAttention eliminates the memory wall within a single GPU. **Ring Attention** extends the tiling principle across multiple GPUs in a distributed system. For sequence lengths that exceed even the memory capacity of a single GPU (e.g., million-token context windows), Ring Attention distributes $K$ and $V$ blocks across GPUs in a ring topology. Each GPU holds a portion of $Q$ and iterates through the ring, receiving $K$/$V$ blocks from its neighbor while simultaneously computing attention on the current block and sending the previous block onward.

The algorithm proceeds in $P - 1$ communication rounds (where $P$ is the number of GPUs). In each round, each GPU computes attention between its local $Q$ block and the currently resident $K$/$V$ block, then sends that $K$/$V$ block to its ring neighbor and receives the next block from its other neighbor.

This overlaps communication with computation: while GPU $i$ computes attention using $K_j$/$V_j$, it simultaneously receives $K_{j+1}$/$V_{j+1}$ from the ring. If the compute time for one tile exceeds the communication time for transferring one tile over NVLink, the communication is fully hidden. On an H100 with `{python} h100_hbm_bw_str` TB/s HBM bandwidth and 900 GB/s NVLink bandwidth, this overlap is achievable for typical tile sizes.

The practical impact of Ring Attention is measured in context length. Without it, a single GPU's attention computation is limited by HBM capacity: the KV cache for a sequence of length $N$ must fit entirely in one GPU's memory. With Ring Attention across $P$ GPUs, each GPU holds $N/P$ tokens of the KV cache, enabling context lengths of $P \times N_{\text{single}}$. For 8 GPUs, this extends the context window by 8 $\times$. Combined with FlashAttention's efficient tiling within each GPU, Ring Attention enables million-token context windows that would be impossible on single-GPU or even standard tensor-parallel configurations.

The combination of FlashAttention (intra-GPU tiling) and Ring Attention (inter-GPU distribution) represents a pattern that recurs throughout performance engineering: hierarchical optimization across the memory hierarchy. Within the GPU, FlashAttention tiles across the SRAM-HBM boundary. Across GPUs, Ring Attention tiles across the HBM-NVLink boundary. Each level exploits the same principle (tiling to reduce traffic across a slow link) adapted to a different bandwidth gap.

This hierarchical view extends to the full system stack:

| **Boundary**                        | **Slow Side**       | **Fast Side**    | **Optimization Technique**   |
|:------------------------------------|:--------------------|:-----------------|:-----------------------------|
| Registers $\leftrightarrow$ SRAM    | SRAM (20 ns)        | Registers (1 ns) | Kernel register blocking     |
| SRAM $\leftrightarrow$ HBM          | HBM (300 ns)        | SRAM (20 ns)     | FlashAttention, fusion       |
| HBM $\leftrightarrow$ NVLink        | NVLink ($\mu$s)     | HBM (300 ns)     | Ring Attention, TP overlap   |
| NVLink $\leftrightarrow$ InfiniBand | IB (10--100 $\mu$s) | NVLink ($\mu$s)  | Gradient overlap, EP routing |

: **The Memory Hierarchy Optimization Stack**. Each level of the memory hierarchy presents a bandwidth gap that can be addressed by tiling, fusion, or overlap. The techniques in this chapter all target one or more of these boundaries. {#tbl-memory-hierarchy-opt}

At every level, the optimization principle is the same: keep data on the fast side of the boundary for as long as possible, and when data must cross the boundary, transfer it in large contiguous blocks to maximize effective bandwidth. The specific technique differs at each level, but the physics is universal.

::: {.callout-checkpoint title="Fusion Fundamentals" collapse="false"}

Verify your understanding of operator fusion and tiling:

- [ ] Can you explain why element-wise operations (GELU, LayerNorm) are memory-bound despite their computational simplicity?
- [ ] Can you trace the data flow of an unfused three-operation sequence and identify which HBM reads/writes fusion eliminates?
- [ ] Can you explain how the online softmax trick enables FlashAttention to compute exact attention without materializing the $N \times N$ score matrix?
- [ ] Do you understand why Ring Attention requires the overlap of communication and computation to be efficient?

:::

Operator fusion and tiling reduce the number of *trips* to HBM. The next section addresses a complementary strategy: reducing the number of *bytes per trip* by using lower-precision numerical formats.

## Precision Engineering {#sec-performance-engineering-precision}

If moving a 2-byte FP16 weight from memory to compute takes 100 nanoseconds, how do we cut that time in half? We shrink the weight to a 1-byte FP8 value. Precision engineering recognizes that machine learning algorithms are surprisingly resilient to numerical noise, allowing us to violently compress our data types to double our effective memory bandwidth and throughput.

### FP8: The Training Frontier {#sec-performance-engineering-fp8}

Traditional mixed-precision training uses FP32 master weights with FP16 forward and backward passes. The NVIDIA H100, introduced in 2022, added hardware support for 8-bit floating point (FP8), offering two formats optimized for different phases of training.

**E4M3** (4-bit exponent, 3-bit mantissa) provides a range of approximately $\pm 448$ with moderate precision. Its tighter range but better precision makes it suitable for representing **weights and activations** in the forward pass, where values cluster in predictable distributions.

**E5M2** (5-bit exponent, 2-bit mantissa) provides a much larger range of approximately $\pm 57344$ but coarser precision. This wider range accommodates **gradients**, which can span many orders of magnitude during backpropagation. Using E4M3 for gradients would cause frequent overflow and underflow, while E5M2's range captures the full gradient distribution at the cost of slightly noisier updates.

| **Format** | **Exponent** | **Mantissa** | **Range**                | **Precision** | **Use Case**               |
|:-----------|:-------------|:-------------|:-------------------------|:--------------|:---------------------------|
| **FP32**   | 8 bits       | 23 bits      | $\pm 3.4 \times 10^{38}$ | Very high     | Master weights             |
| **FP16**   | 5 bits       | 10 bits      | $\pm 65504$              | High          | Mixed-precision            |
| **BF16**   | 8 bits       | 7 bits       | $\pm 3.4 \times 10^{38}$ | Moderate      | Training                   |
| **E4M3**   | 4 bits       | 3 bits       | $\pm 448$                | Low           | FP8 forward pass           |
| **E5M2**   | 5 bits       | 2 bits       | $\pm 57344$              | Very low      | FP8 gradients              |
| **INT8**   | N/A          | 8 bits       | $-128$ to $+127$         | Uniform       | Post-training quantization |
| **INT4**   | N/A          | 4 bits       | $-8$ to $+7$             | Uniform       | KV cache, weights          |

: **Numerical Precision Formats for ML**. Each row represents a different precision format. FP8 formats (E4M3, E5M2) occupy the sweet spot between the bandwidth of INT8 and the trainability of FP16. {#tbl-precision-formats}

The critical engineering challenge in FP8 training is **dynamic scaling**. FP8's narrow dynamic range means that a fixed scale factor will cause either overflow (large values clamp to infinity) or underflow (small values round to zero). Per-tensor scaling multiplies each tensor by a scale factor before casting to FP8, then divides by that factor after the FP8 computation. The scale factor is adjusted dynamically, typically by tracking the running maximum absolute value of each tensor and choosing a scale that maps this maximum to near the FP8 maximum representable value.

The workflow for a single FP8 forward-backward step is:

1. Read FP32 master weights; quantize to E4M3 using the current weight scale factor.
2. Execute forward pass GEMM in FP8 (E4M3 $\times$ E4M3 $\rightarrow$ FP16 accumulation).
3. Compute loss in FP16 or FP32.
4. Quantize activation gradients to E5M2 using the current gradient scale factor.
5. Execute backward pass GEMMs in FP8 (E5M2 $\times$ E4M3 $\rightarrow$ FP16 accumulation).
6. Update FP32 master weights using the FP16 accumulated gradients.
7. Update scale factors based on observed value ranges.

This three-precision approach (FP32 master weights, FP8 GEMMs, FP16 accumulation) achieves near-FP16 training quality while doubling effective throughput on FP8-capable hardware.

The practical implementation of dynamic scaling introduces a subtle bootstrapping problem: the optimal scale factor for step $t$ depends on the tensor values at step $t$, which are not known until the computation executes. Most implementations use a **delayed scaling** strategy: the scale factor for step $t$ is computed from the running statistics of steps $t-1, t-2, \ldots$, using an exponential moving average of the maximum absolute value. This introduces a one-step lag, which works well when tensor statistics change slowly (as they do during stable training) but can cause transient overflow or underflow when statistics change rapidly (at the beginning of training, after learning rate warmup, or at curriculum transitions).

An alternative is **just-in-time scaling**, where the computation first scans the tensor to find its maximum absolute value, computes the scale factor, then casts and executes the FP8 GEMM. The overhead of the extra scan pass (reading the tensor twice instead of once) is typically 5--10%, which is modest compared to the 2 $\times$ throughput gain from FP8. Just-in-time scaling is more robust but slightly less efficient than delayed scaling.

The energy implications of reduced precision extend beyond throughput. The Horowitz (2014) energy estimates show that an FP32 multiply-accumulate costs approximately 3.7 pJ, while an INT8 operation costs approximately 0.2 pJ, an 18.5 $\times$ reduction. FP8 operations fall between these extremes. At datacenter scale, where thousands of GPUs run continuously, the cumulative energy savings from reduced precision are substantial. The performance benefit (higher throughput) and the energy benefit (lower cost per operation) compound: FP8 delivers twice the throughput at roughly half the energy per operation, resulting in approximately 4 $\times$ improvement in energy efficiency for compute-bound workloads.

### Block-wise Quantization {#sec-performance-engineering-block-quant}

Post-training quantization to INT8 or INT4 delivers even greater bandwidth savings for inference, but LLMs present a unique challenge: **outlier features**. Dettmers et al. (2022) discovered that large language models develop a small number of hidden dimensions (typically fewer than 1% of all dimensions) with activation magnitudes 10--100 $\times$ larger than the rest. Applying uniform per-tensor INT8 quantization clips these outliers, destroying the information they carry, or expands the quantization range to accommodate them, wasting precision on the majority of near-zero values.

::: {.callout-definition title="Block-wise Quantization"}

***Block-wise Quantization***\index{Block-wise Quantization!definition} is a precision reduction technique that partitions a weight tensor into small blocks (typically 32-128 elements) and applies independent quantization parameters (scale and zero-point) to each block. Unlike per-tensor quantization, which uses a single scale factor for the entire tensor, block-wise quantization adapts to local value distributions, preserving outlier information that per-tensor methods destroy. The quantization error for a block of size $B$ with range $[w_{min}, w_{max}]$ mapped to $b$-bit integers is bounded by:
$$\epsilon_{block} = \frac{w_{max} - w_{min}}{2^b - 1}$$
For a 4-bit scheme with $B=32$, this typically achieves less than 1% accuracy degradation on language modeling tasks compared to FP16, while reducing memory footprint by 4x and memory bandwidth demand proportionally.

:::

**LLM.int8()** solves this by decomposing each matrix multiplication into two parts: a small set of outlier dimensions processed in FP16, and the remaining dimensions processed in INT8. The system identifies outlier dimensions at runtime (those exceeding a magnitude threshold, typically 6.0), routes them to an FP16 GEMM, and routes the remaining dimensions to an INT8 GEMM. The results are combined to produce the final output. This achieves nearly lossless INT8 inference for models that would otherwise degrade substantially under uniform quantization.

**GPTQ** (Frantar et al., 2023) takes a different approach: weight-only quantization using second-order information. Instead of quantizing each weight independently, GPTQ processes weights column by column, using the Hessian of the layer's loss surface to determine which quantization errors matter most and redistributing those errors across unquantized columns. This produces INT4 weight representations with minimal accuracy loss, even for models with severe outlier features. The key insight is that quantization error in one weight can be compensated by adjusting correlated weights.

**AWQ** (Activation-Aware Weight Quantization, Lin et al., 2024) observes that not all weights are equally important: weights connected to high-activation channels contribute disproportionately to model output. AWQ identifies these salient weights by analyzing activation magnitudes across a calibration dataset, then applies per-channel scaling to protect them before uniform group quantization. This achieves INT4 weight quantization with quality comparable to GPTQ but with significantly faster quantization time, since it avoids the expensive Hessian computation.

**SmoothQuant** (Xiao et al., 2023) takes yet another approach to the outlier problem. Rather than handling outliers at runtime (LLM.int8()) or through weight optimization (GPTQ, AWQ), SmoothQuant smooths the activation distribution *before* quantization by migrating the quantization difficulty from activations to weights. The key observation is that activation outliers are channel-specific: certain hidden dimensions consistently produce large values across all tokens. SmoothQuant applies a per-channel scaling transformation that divides the activation by a smoothing factor and multiplies the corresponding weight by the same factor. This mathematically equivalent transformation reduces activation outlier magnitudes at the cost of slightly increasing weight magnitudes, making both tensors more amenable to uniform INT8 quantization. The result is efficient W8A8 (weight-8-bit, activation-8-bit) quantization that exploits INT8 Tensor Cores for both bandwidth and compute benefits.

These four approaches, LLM.int8(), GPTQ, AWQ, and SmoothQuant, represent a progression in the sophistication of quantization techniques for LLMs. LLM.int8() handles outliers at runtime with mixed-precision decomposition but limits compression to INT8. GPTQ uses second-order information for aggressive INT4 weight compression but requires hours of calibration per model. AWQ achieves similar INT4 quality with minutes of calibration by focusing on activation-aware scaling. SmoothQuant enables W8A8 quantization by preprocessing the weight-activation pairs. In practice, AWQ has become the default choice for weight-only quantization in production LLM deployment, while SmoothQuant is preferred when both weight and activation quantization are needed for compute-bound workloads.

The choice among these techniques also depends on the deployment target. For GPU inference with Tensor Core support, GPTQ and AWQ produce INT4 weight representations that are dequantized to FP16 during the GEMM computation, leveraging the GPU's FP16 Tensor Cores. For CPU inference or edge deployment, INT8 representations (LLM.int8() or static per-channel INT8 quantization) can directly exploit integer arithmetic units without dequantization overhead.

The storage cost for block-wise quantization is minimal. Storing one FP32 scale (32 bits) for every block of 128 INT8 weights (1024 bits) increases total model size by only 3%. This small overhead allows block-wise quantization to isolate the destructive impact of outliers, preserving the effective dynamic range for the 99% of normal weights, without the bandwidth penalty of higher-precision formats.

### Post-Training vs Quantization-Aware Training {#sec-performance-engineering-ptq-qat}

The trade-off between **Post-Training Quantization (PTQ)** and **Quantization-Aware Training (QAT)** centers on the balance between engineering agility and model fidelity. For a model like Llama-2-70B, PTQ is the default choice for immediate deployment. Techniques like GPTQ or AWQ process the model layer-by-layer using a small calibration dataset (typically 128--1024 samples) to minimize reconstruction error. This process is computationally cheap, requiring approximately 4--8 GPU-hours on a single H100 to quantize a 70B model to INT4. While PTQ preserves greater than 99% of accuracy at INT8, aggressive quantization to INT4 or INT3 often incurs a steep penalty: perplexity may degrade by 0.5--1.0 points, and reasoning performance on benchmarks like MMLU can drop significantly (e.g., from 69% to below 64%).

When PTQ fails to meet quality thresholds, QAT provides the remedy by integrating quantization noise directly into the training loop. By simulating low-precision rounding during the forward pass and approximating gradients during the backward pass via the straight-through estimator (STE), the network learns to adjust its weights to be robust to quantization. The cost is substantial: QAT is effectively a full fine-tuning run, often requiring hundreds of GPU-hours and a distributed training cluster. For a 70B model, this might mean a 3-day run on 8 $\times$ H100s compared to the 4-hour single-GPU job for PTQ. Emerging techniques like **QLoRA** (Quantized Low-Rank Adaptation) bridge this gap by freezing the base model in 4-bit precision and fine-tuning only a small set of high-precision adapter weights. This hybrid approach offers the quality recovery of QAT with a memory footprint small enough to run on a single consumer GPU, effectively democratizing high-fidelity quantization.

The practical workflow in most production environments follows a two-stage approach: deploy with PTQ first (because it is fast and requires no training infrastructure), then apply QAT or QLoRA only if the PTQ model fails to meet quality requirements at the target precision. This sequence minimizes engineering effort while preserving the option of higher quality when needed.

### KV Cache Compression {#sec-performance-engineering-kv-cache}

During autoregressive generation, the Key-Value (KV) cache stores the key and value projections for all previously generated tokens. This cache grows linearly with sequence length and batch size, often becoming the dominant memory consumer in LLM serving. For a 70B parameter model with 80 layers, 64 heads, head dimension 128, and sequence length 4096 in FP16, the KV cache requires:

```{python}
#| echo: false
#| label: kv-cache-calc
# ┌─────────────────────────────────────────────────────────────────────────────
# │ KV CACHE MEMORY CALCULATION
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-kv-cache KV cache compression prose
# │
# │ Goal: Compute FP16 and INT4 KV cache footprint for a 70B MHA model (80 layers,
# │       64 KV heads, head_dim=128, seq_len=4096) to quantify the memory pressure
# │       that motivates KV cache quantization and GQA.
# │ Show: ~84.9 GB FP16 vs ~10.6 GB INT4 → 8x compression, freeing ~74 GB for
# │       larger batch sizes — inline in KV Cache Compression section.
# │ How: 2 * layers * heads * head_dim * seq_len * bytes_per_elem; compression
# │       from FP16 (2 bytes) to INT4 (0.5 bytes) = 4x reduction.
# │
# │ Imports: (none — standalone calculation), mlsys.formatting (check)
# │ Exports: kv_fp16_gb_str, kv_int4_gb_str, kv_savings_gb_str, kv_compression_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class KVCacheCalc:
    """KV cache memory footprint: FP16 vs INT4 for a 70B MHA model."""

    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    kv_layers = 80
    kv_heads = 64     # total KV heads (could be GQA, but assume MHA for max)
    kv_head_dim = 128
    kv_seq_len = 4096
    kv_bytes_fp16 = 2
    kv_bytes_int4 = 0.5

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────
    # KV cache = 2 (key+value) * layers * heads * head_dim * seq_len * bytes
    kv_fp16_bytes = 2 * kv_layers * kv_heads * kv_head_dim * kv_seq_len * kv_bytes_fp16
    kv_int4_bytes = 2 * kv_layers * kv_heads * kv_head_dim * kv_seq_len * kv_bytes_int4

    kv_fp16_gb = kv_fp16_bytes / 1e9
    kv_int4_gb = kv_int4_bytes / 1e9
    kv_savings_gb = kv_fp16_gb - kv_int4_gb
    kv_compression = kv_fp16_gb / kv_int4_gb

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    check(kv_fp16_gb > 5, f"KV cache should be substantial, got {kv_fp16_gb:.1f} GB")
    check(kv_compression > 3, f"INT4 should give >3x compression, got {kv_compression:.1f}x")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    kv_fp16_gb_str = f"{kv_fp16_gb:.1f}"
    kv_int4_gb_str = f"{kv_int4_gb:.1f}"
    kv_savings_gb_str = f"{kv_savings_gb:.1f}"
    kv_compression_str = f"{kv_compression:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
kv_fp16_gb = KVCacheCalc.kv_fp16_gb
kv_fp16_gb_str = KVCacheCalc.kv_fp16_gb_str
kv_int4_gb_str = KVCacheCalc.kv_int4_gb_str
kv_savings_gb_str = KVCacheCalc.kv_savings_gb_str
kv_compression_str = KVCacheCalc.kv_compression_str
```

$$
\text{KV cache} = 2 \times 80 \times 64 \times 128 \times 4096 \times 2 \text{ bytes} \approx \text{`{python} kv_fp16_gb_str`} \text{ GB (FP16)}
$$

This is a substantial fraction of the H100's `{python} h100_mem_str` GB of HBM, consumed by a single request's KV cache alone. For a batch of 8 concurrent requests, the KV cache would require `{python} f"{kv_fp16_gb * 8:.0f}"` GB, exceeding single-GPU capacity.

Compressing the KV cache to INT4 reduces it to approximately `{python} kv_int4_gb_str` GB per request, a `{python} kv_compression_str` $\times$ reduction. The freed memory allows either longer sequences or larger batch sizes, directly improving serving throughput. Empirical studies show that KV cache quantization to INT4 with per-channel or per-group scaling factors preserves attention quality with negligible perplexity degradation, because the attention mechanism is relatively robust to small perturbations in cached key and value representations.

### Grouped Query Attention: Architectural Precision Engineering {#sec-performance-engineering-gqa}

An architectural complement to KV cache quantization is **Grouped Query Attention (GQA)**, introduced by Ainslie et al. (2023). In standard Multi-Head Attention (MHA), every attention head has its own key and value projections. In GQA, multiple query heads share a single key-value head. A model with 32 query heads and 8 KV heads uses a group size of 4: every 4 query heads share one K and one V projection.

The impact on KV cache size is proportional to the KV head reduction. Our earlier calculation assumed 64 KV heads (MHA). With GQA using 8 KV heads instead, the KV cache shrinks by 8 $\times$:

$$
\text{KV cache (GQA)} = 2 \times 80 \times 8 \times 128 \times 4096 \times 2 \approx 1.3 \text{ GB (FP16)}
$$

This is dramatically more manageable than the `{python} kv_fp16_gb_str` GB of the MHA case. Combining GQA with INT8 KV cache quantization yields sub-gigabyte per-request cache sizes, enabling batch sizes of 50 or more on a single GPU.

GQA is not a post-hoc optimization; it must be designed into the model architecture from the start (or applied through fine-tuning). But its performance implications are so significant that nearly all modern LLMs (Llama 3, Mistral, Gemma) use some form of grouped or multi-query attention. From a performance engineering perspective, GQA represents the co-design ideal: an architectural choice driven entirely by system-level performance constraints.

The multi-query extreme (MQA), where all query heads share a single KV head, minimizes KV cache size but can degrade model quality, particularly for tasks requiring fine-grained attention patterns. GQA provides a tunable compromise: the group size $g$ (number of query heads per KV head) controls the trade-off between cache size and attention capacity. Llama 3 uses 8 KV heads with 32 query heads ($g = 4$), while Mistral uses 8 KV heads with 32 query heads ($g = 4$) as well. The convergence on similar GQA configurations across independently developed models suggests that $g = 4$ represents a Pareto-optimal point for current model scales.

The progression from Multi-Head Attention (MHA) through Grouped Query Attention (GQA) to Multi-Query Attention (MQA, where all heads share a single KV pair) illustrates a broader principle: model architecture and system performance are inseparable. Choosing between MHA, GQA-8, GQA-4, and MQA is not purely a model quality decision; it is a system throughput decision. For a 70B model serving at batch size 32 with 4096-token sequences, the KV cache under MHA might consume 80% of available GPU memory, leaving insufficient room for the batch. Under GQA-8, the KV cache shrinks to 10% of MHA's size, enabling the batch to fit and increasing throughput by an order of magnitude. The quality impact of GQA is typically less than 1% on standard benchmarks, making it an overwhelmingly favorable trade-off for production systems.

### Weight-Only vs Weight-Activation Quantization {#sec-performance-engineering-quant-strategies}

An important distinction in precision engineering is between **weight-only quantization** and **weight-activation quantization**, which target different bottlenecks and have different quality implications.

**Weight-only quantization** (GPTQ, AWQ) reduces weight precision to INT4 or INT3 while keeping activations in FP16. During a GEMM, the INT4 weights are dequantized to FP16 on-the-fly, and the computation proceeds using FP16 Tensor Cores. The benefit is reduced memory for weight storage and reduced HBM bandwidth for weight reads, but the GEMM itself still operates at FP16 precision. This approach is ideal for memory-bound inference (batch size 1 decode), where the bottleneck is reading weights from HBM.

**Weight-activation quantization** (SmoothQuant, FP8 training) reduces both weights and activations to lower precision, enabling the GEMM to execute using lower-precision arithmetic (INT8 Tensor Cores, FP8 Tensor Cores). This provides both bandwidth and compute benefits but is more challenging to implement without quality degradation, because activation distributions are more dynamic and harder to quantize than weight distributions.

The choice depends on the operational regime. For memory-bound inference (small batch sizes), weight-only INT4 quantization provides the largest speedup per unit of quality degradation. For compute-bound inference (large batch sizes) or training, weight-activation FP8 quantization provides throughput gains that weight-only quantization cannot match. Many production systems use different quantization strategies for different operating points: INT4 weight-only at low batch sizes (for latency) and FP8 weight-activation at high batch sizes (for throughput).

::: {.callout-notebook title="The Precision Dividend"}

**Problem**: You serve a 70B parameter model on 4 $\times$ H100 GPUs. The model weights in FP16 consume 140 GB (35 GB per GPU). KV cache at FP16 consumes `{python} kv_fp16_gb_str` GB per request. How does quantizing weights to INT4 and KV cache to INT8 change the maximum batch size?

**Before optimization** (all FP16):

- Weights: 35 GB/GPU
- Available for KV cache: $80 - 35 = 45$ GB/GPU
- KV cache per request: `{python} kv_fp16_gb_str` GB $\div$ 4 GPUs $\approx$ `{python} f"{kv_fp16_gb / 4:.1f}"` GB/GPU
- Maximum batch size: $\lfloor 45 / {kv_fp16_gb / 4:.1f} \rfloor$ $\approx$ `{python} f"{int(45 / (kv_fp16_gb / 4))}"` requests

**After optimization** (INT4 weights, INT8 KV cache):

- Weights: 35 GB $\times$ (4/16) = 8.75 GB/GPU (INT4)
- Available for KV cache: $80 - 8.75 = 71.25$ GB/GPU
- KV cache per request (INT8): `{python} f"{kv_fp16_gb / 2:.1f}"` GB $\div$ 4 $\approx$ `{python} f"{kv_fp16_gb / 2 / 4:.1f}"` GB/GPU
- Maximum batch size: $\lfloor 71.25 / {kv_fp16_gb / 2 / 4:.1f} \rfloor$ $\approx$ `{python} f"{int(71.25 / (kv_fp16_gb / 2 / 4))}"` requests

**Takeaway**: Precision engineering does not just make individual operations faster; it fundamentally changes serving economics by enabling larger batch sizes. Larger batches amortize the fixed cost of weight loading, shifting operations from memory-bound toward compute-bound. This single optimization can increase throughput by `{python} f"{int(71.25 / (kv_fp16_gb / 2 / 4)) / int(45 / (kv_fp16_gb / 4)):.0f}"` $\times$ or more.

:::

Precision engineering reduces the bytes per memory transaction. Operator fusion reduces the number of transactions. Together, they attack the same fundamental bottleneck from complementary directions: if you must move data across a slow bus, move less of it (precision) and move it fewer times (fusion). The multiplicative interaction between these two techniques explains why modern serving systems deploy both simultaneously: FlashAttention reduces attention HBM traffic from quadratic to linear, and INT8 KV cache compression further halves the linear term. The combined effect exceeds what either technique achieves alone.

The next section examines how graph compilers automate both of these optimizations, applying them systematically across an entire model.

## Graph Compilation {#sec-performance-engineering-compilation}

Manually writing fused CUDA kernels for every possible combination of layers in a massive neural network is a Sisyphean task for human engineers. Instead of hand-crafting these optimizations one by one, what if we could write a program to analyze the model's structure and generate the optimal kernels automatically? Graph compilation does exactly this, transforming high-level PyTorch code into specialized, hardware-aware machine instructions.

### The Compilation Pipeline {#sec-performance-engineering-compilation-pipeline}

A graph compiler transforms a high-level model definition (Python code) into optimized hardware instructions through a multi-stage pipeline. To visualize this process, consider a standard transformer FFN block consisting of a projection, an activation, a second projection, and a layer normalization: `LayerNorm(Linear(GELU(Linear(x))))`. In standard PyTorch eager execution, this sequence triggers four separate kernel launches, each reading from and writing to HBM.

In the **graph capture** stage, the compiler traces the model's execution to construct a computational graph, a directed acyclic graph where nodes represent operations and edges represent tensor dependencies. For the FFN block, this results in a graph with four primary nodes plus their associated parameter tensors. Dynamic Python control flow (loops, conditionals) must be handled by either tracing through a representative execution path or by using compiler-specific annotations to mark dynamic dimensions.

During **graph-level optimization**, the compiler applies algebraic simplifications and operation rewriting. It identifies that the bias addition in the first `Linear` layer can be folded into the matrix multiplication kernel. It also recognizes that the `GELU` activation is an element-wise operation that depends only on the output of the first `Linear`. These standard compiler optimizations can reduce graph size by 10--30% before any hardware-specific work begins.

The **operator fusion** pass is the most critical for performance. It identifies sequences of operations that can be combined into single kernels to reduce memory traffic. For the FFN block, the compiler fuses the `GELU` activation into the tail of the first `Linear` kernel (if supported as an epilogue) or fuses the `GELU` and the subsequent `LayerNorm` into a single kernel. Instead of writing the intermediate result of the first `Linear` to HBM and reading it back for `GELU`, the fused kernel keeps the data in the GPU's SRAM or registers. This typically reduces the number of HBM accesses by 30--50%, directly alleviating the memory bandwidth bottleneck.

**Memory planning** determines when to allocate and free tensors. Without optimization, a 24-layer transformer might allocate separate buffers for every intermediate activation. The compiler analyzes tensor lifetimes, recognizing that the input to the first `Linear` is no longer needed after the second `Linear` computes its output, and reuses the same physical memory addresses. For a 70B model where activations can consume gigabytes per layer, this buffer reuse reduces peak memory requirements from $O(L)$ to $O(1)$, where $L$ is the number of layers. For a model where activations consume 10 GB per layer across 80 layers, this optimization reduces peak activation memory from 800 GB (impossible on any single GPU) to approximately 10--20 GB (comfortably within a single H100). Memory planning also interacts with operator fusion: fusing two operations eliminates the intermediate tensor between them, which both removes the HBM traffic and removes the memory allocation. The compiler must reason about both effects jointly to make profitable decisions.

**Kernel selection** maps each fused operation to a specific machine code implementation. For the computationally heavy linear projections, the compiler selects a vendor-optimized cuBLAS or CUTLASS GEMM kernel. For the fused `GELU-LayerNorm` sequence, it generates a custom Triton kernel that keeps data in SRAM. The result for the FFN block is a reduction from 4 separate kernels to 2 highly optimized kernels, with a corresponding reduction in global memory traffic.

### torch.compile {#sec-performance-engineering-torch-compile}

PyTorch's `torch.compile` (introduced in PyTorch 2.0) brings graph compilation to the most widely used ML framework. It operates through three components: **TorchDynamo** for graph capture, **TorchInductor** for code generation, and **AOTAutograd** for ahead-of-time backward graph construction.

TorchDynamo operates at the Python bytecode level, a design choice that distinguishes it from earlier tracing approaches. Previous tracing methods (torch.jit.trace, torch.fx) operated at the Python source or AST level, requiring users to avoid unsupported Python constructs. TorchDynamo intercepts the bytecode interpreter itself, capturing a computational graph without requiring the user to modify their model code. When TorchDynamo encounters Python constructs it cannot trace (data-dependent control flow, unsupported operations), it inserts a "graph break" that splits the trace into multiple subgraphs, each compiled independently. The goal is to capture as large a subgraph as possible while gracefully handling dynamic Python behavior.

TorchInductor generates optimized Triton kernels (for GPU) or C++/OpenMP code (for CPU) from the captured graph. Triton is a domain-specific language for writing GPU kernels in Python-like syntax, abstracting away thread block management and memory coalescing while still exposing tiling and fusion decisions. TorchInductor automatically fuses element-wise operations, reduces memory traffic by combining operations that share inputs, and selects tile sizes through autotuning.

A minimal example illustrates the usage:

```python
import torch

def transformer_block(x, w1, w2, ln_weight, ln_bias):
    """Unfused transformer FFN block."""
    h = x @ w1                    # Linear projection
    h = torch.nn.functional.gelu(h)  # Activation
    h = h @ w2                    # Output projection
    # Layer normalization
    mean = h.mean(dim=-1, keepdim=True)
    var = h.var(dim=-1, keepdim=True, unbiased=False)
    h = (h - mean) / torch.sqrt(var + 1e-5)
    h = h * ln_weight + ln_bias
    return h

# Compile the function — TorchDynamo traces, TorchInductor optimizes
compiled_block = torch.compile(transformer_block)

# First call triggers compilation; subsequent calls use compiled code
output = compiled_block(x, w1, w2, ln_weight, ln_bias)
```

In this example, `torch.compile` will fuse the GELU activation with surrounding operations, combine the layer normalization mean/variance/normalize steps into a single kernel, and potentially fuse the bias addition with the preceding GEMM. The user writes standard PyTorch code; the compiler handles the optimization.

### XLA and TPU Optimization {#sec-performance-engineering-xla}

XLA (Accelerated Linear Algebra) is Google's graph compiler, used as the backend for JAX and TensorFlow. Unlike TorchInductor, which generates Triton code targeting NVIDIA GPUs, XLA generates HLO (High-Level Operations) intermediate representation that targets multiple backends, including Google TPUs, NVIDIA GPUs, and CPUs. Its architecture is fundamentally different from `torch.compile`: while PyTorch prioritizes flexibility by allowing graph breaks for unsupported Python features, XLA enforces **whole-program compilation**, tracing the entire computation as a single static graph and enabling global optimizations that span across layers and even across the forward and backward passes.

This global view enables XLA's most powerful feature: the **GSPMD (General Partitioner for SPMD)**. In distributed training, GSPMD automatically partitions the computation graph across thousands of TPU cores based on a few high-level user annotations. While a PyTorch user must manually wrap models with `DistributedDataParallel` or `FullyShardedDataParallel`, an XLA user defines the computation for a single device and allows the compiler to infer the necessary communication primitives (AllReduce, AllGather) and insert them into the graph. This allows for complex hybrid sharding strategies that are difficult to implement manually.

For TPU hardware specifically, XLA performs layout optimizations unavailable on other platforms. It maps matrix multiplications onto the TPU's systolic array architecture, padding dimensions to align with the 128 $\times$ 128 hardware units and scheduling instructions to hide the latency of HBM fetches. The impact of these optimizations is visible in MFU metrics. On large-scale LLM training workloads, JAX/XLA on TPUv4 typically achieves 55--65% MFU, significantly outperforming the 40--55% MFU typical of PyTorch/GPU setups without extensive hand-tuning.

The trade-off for XLA's performance is compilation latency and rigidity. Because XLA must analyze the full static graph, initial compilation can take minutes for large models, compared to seconds for `torch.compile`. Any change in input shape triggers a full recompilation. This makes XLA excellent for steady-state production workloads where the graph is static and the model runs for days or weeks, but challenging for research environments involving dynamic shapes or rapid experimental iteration. The choice between `torch.compile` and XLA often depends on the deployment context: organizations using NVIDIA GPUs predominantly use PyTorch with `torch.compile`, while organizations using Google TPUs use JAX with XLA.

### TensorRT: Inference Optimization {#sec-performance-engineering-tensorrt}

NVIDIA TensorRT is a specialized inference compiler that treats the model not as a flexible program but as a rigid global optimization problem. Because inference requires no backward pass and no gradient storage, TensorRT applies aggressive transformations that would be mathematically invalid or impractically slow for training. It performs a calibration pass where it runs the model on representative data to determine the numerical range of every activation tensor.

This calibration enables **mixed-precision quantization** at a granular level. For a 70B parameter LLM, blindly quantizing all layers to INT8 often degrades perplexity. TensorRT analyzes the sensitivity of each layer individually. It might determine that the attention layers in the first 3 blocks and the final 3 blocks are highly sensitive to precision loss, keeping them in FP16, while aggressively quantizing the middle 74 layers to INT8. This automated mixed-precision strategy recovers accuracy while capturing the throughput benefits of lower precision. TensorRT also eliminates training-only operations (dropout, batch normalization running statistics updates), optimizes for static shapes by generating kernels tuned for exact dimensions, and plans memory precisely since no gradient tensors are needed.

TensorRT performs **kernel autotuning** far beyond simple heuristics. For every operation in the graph, it benchmarks dozens of candidate kernels, varying tile sizes, thread block configurations, and unrolling factors, on the actual target hardware. It selects the single fastest implementation for that specific GPU and input shape. The performance gap between TensorRT and general-purpose compilers is substantial: in head-to-head comparisons for serving a 70B LLM, TensorRT-LLM typically achieves 1.3--1.8 $\times$ higher throughput than `torch.compile` with TorchInductor.

The trade-off for TensorRT's aggressive optimization is reduced flexibility and high compilation cost. Compilation times are measured in minutes to hours (30--60 minutes for a 70B model), and the resulting engine is strictly tied to the specific GPU architecture and input shape range. Changing any of these requires recompilation. This makes TensorRT the standard for stable, high-volume production deployments, while `torch.compile` remains the preferred choice for development and lower-volume services where rapid iteration matters more than extracting the last percentage of throughput.

### Compilation Overhead and Trade-offs {#sec-performance-engineering-compilation-overhead}

Graph compilation is not free. The compilation process itself takes time, ranging from seconds for small models with `torch.compile` to minutes or hours for large models with TensorRT's full optimization pipeline. This overhead must be amortized over the number of times the compiled model executes.

For training workloads that run for hours or days, compilation overhead is negligible. For inference workloads that serve millions of requests, the one-time compilation cost is similarly amortized. The problematic case is dynamic or infrequent workloads: a model that is compiled once but serves only a few hundred requests before being replaced by a new version may not recoup the compilation cost.

**Graph breaks** are a related challenge specific to `torch.compile`. When TorchDynamo encounters Python code it cannot trace (data-dependent control flow, calls to uncompiled libraries, dynamic tensor shapes that change between iterations), it inserts a graph break. Each break produces a separate compiled subgraph with its own compilation overhead and potential optimization boundaries. A model with 50 graph breaks produces 50+ small compiled regions, each potentially too small for meaningful fusion. Reducing graph breaks requires refactoring the model code to be more "compiler-friendly," replacing Python control flow with tensor operations and ensuring static shapes where possible.

**Dynamic shapes** present a fundamental tension between compilation and flexibility. A model compiled for input shape [batch=32, seq=512] will recompile when it encounters [batch=16, seq=1024]. TorchInductor supports "dynamic shapes" by generating kernels with symbolic dimensions, but this generality comes at the cost of reduced optimization compared to kernels specialized for exact shapes. TensorRT sidesteps this by requiring the user to specify a range of input shapes at compilation time, generating kernels that handle the specified range but nothing outside it.

Despite these limitations, graph compilation represents the most accessible optimization technique: it requires no model modifications, no custom kernels, and minimal code changes. For most workloads, `torch.compile` with default settings provides 10--40% speedup with a single line of code, making it the natural first optimization to apply before considering more specialized techniques.

The accessibility of graph compilation has changed the performance engineering workflow. Before `torch.compile`, extracting the last 30% of performance required weeks of manual kernel optimization. Now, a single line of code captures a significant fraction of that improvement, freeing the engineer to focus on the algorithmic and architectural optimizations (speculative decoding, MoE, precision engineering) that compilers cannot automate. The compiler handles the routine work; the engineer handles the creative work. This division of labor is likely to deepen as compilers improve, making the higher-level system design skills in this chapter increasingly valuable relative to low-level kernel engineering.

### Compilation Modes and Backends {#sec-performance-engineering-compilation-modes}

`torch.compile` supports multiple optimization levels that trade compilation time for runtime performance:

**`default` mode** applies standard optimizations: element-wise fusion, memory planning, and kernel selection from the pre-tuned library. Compilation is fast (seconds to minutes) and suitable for development iteration.

**`reduce-overhead` mode** additionally wraps the compiled graph in CUDA Graphs (discussed in @sec-performance-engineering-cuda-graphs), eliminating kernel launch overhead. This is particularly effective for small models where launch overhead is a significant fraction of execution time.

**`max-autotune` mode** triggers extensive kernel autotuning, benchmarking multiple kernel variants (different tile sizes, thread block configurations, memory access patterns) for each operation and selecting the fastest. This produces the highest-performance code but may take 10--30 minutes of compilation time, making it suitable for production deployment but impractical during development.

The choice of backend also matters. TorchInductor (the default) generates Triton kernels for GPU and C++ for CPU. For deployment-specific optimization, the model can be exported through `torch.export` to an intermediate representation that can be consumed by TensorRT, ONNX Runtime, or other inference-specialized runtimes. Each backend applies its own optimization passes on top of the common graph-level transformations.

### The Triton Language {#sec-performance-engineering-triton}

Between hand-written CUDA and fully automated graph compilers sits **Triton**, a Python-based language for writing GPU kernels. Triton occupies a middle ground: the programmer specifies the algorithm (tiling strategy, fusion pattern) while Triton handles low-level concerns (thread block scheduling, memory coalescing, shared memory management).

A Triton kernel for fused GELU activation illustrates the programming model:

```python
import triton
import triton.language as tl

@triton.jit
def fused_gelu_kernel(
    input_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    # Each program instance handles BLOCK_SIZE elements
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    # Load input tile from HBM into registers
    x = tl.load(input_ptr + offsets, mask=mask)

    # Fused GELU computation (tanh approximation)
    # GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 *
    # x^3)))
    x_cubed = x * x * x
    inner = 0.7978845608 * (x + 0.044715 * x_cubed)
    gelu = 0.5 * x * (1.0 + tl.math.tanh(inner))

    # Store result back to HBM
    tl.store(output_ptr + offsets, gelu, mask=mask)
```

The programmer thinks in terms of tiles (`BLOCK_SIZE` elements), not individual threads. Triton compiles this to PTX/SASS instructions, handling thread-to-data mapping, memory coalescing, and register allocation automatically. This makes it feasible for ML engineers (rather than GPU specialists) to write custom fused kernels when the automatic compiler misses a fusion opportunity.

The real power of Triton emerges when fusing multiple operations. A Triton kernel that implements `y = LayerNorm(GELU(x))` loads `x` once from HBM, computes both GELU and LayerNorm in registers/shared memory, and writes `y` once to HBM. Without fusion, this sequence requires three HBM round-trips: read `x`, write `GELU(x)`, read `GELU(x)`, write `norm_input`, read `norm_input`, write `y`. The fused kernel reduces HBM traffic by 3 $\times$, and for memory-bound operations, this translates directly to a 3 $\times$ speedup.

Triton's adoption has accelerated rapidly since its integration into PyTorch's TorchInductor backend. When `torch.compile` identifies a fusion opportunity that requires a custom kernel, TorchInductor automatically generates Triton code for the fused operation. This means that many of the fusion benefits described in this section are available to users through a single `torch.compile` call, without writing any Triton code directly. For advanced use cases where the compiler's heuristics are insufficient, hand-written Triton kernels provide a middle ground between the accessibility of PyTorch and the performance of hand-tuned CUDA.

::: {.callout-notebook title="The Compilation Dividend"}

**Problem**: You deploy a 13B parameter model for inference. Without compilation, the PyTorch eager mode processes 120 tokens/second on a single H100. The Nsight Systems trace reveals that 35% of step time is spent in element-wise kernels (LayerNorm, GELU, residual additions) and 15% is kernel launch overhead. You apply `torch.compile` with the `max-autotune` backend. Estimate the new throughput.

**The Math**:

*Step 1: Identify the addressable overhead.*
Element-wise kernels (35%) and launch overhead (15%) total 50% of execution time. torch.compile fuses element-wise operations (reducing their time by approximately 70% due to eliminated HBM round-trips) and reduces kernel launches (eliminating most launch overhead).

*Step 2: Estimate post-compilation time.*
Original time per token: $1/120 = 8.33$ ms.

- GEMM time (unchanged): $8.33 \times 0.50 = 4.17$ ms
- Element-wise time (70% reduction): $8.33 \times 0.35 \times 0.30 = 0.87$ ms
- Launch overhead (80% reduction): $8.33 \times 0.15 \times 0.20 = 0.25$ ms

New time per token: $4.17 + 0.87 + 0.25 = 5.29$ ms, yielding approximately 189 tokens/second.

**Takeaway**: torch.compile delivers a 1.58 $\times$ speedup by fusing element-wise operations and reducing launch overhead, without touching the GEMM kernels. The remaining bottleneck is now the GEMM itself (79% of step time), indicating that further improvement requires either precision reduction or batching.

:::

Graph compilation automates what manual kernel engineering achieves for individual operations, applying it systematically across the entire model graph. The next section examines an orthogonal optimization that changes the fundamental algorithm: speculative decoding, which trades cheap compute for expensive latency.

## Speculative Decoding {#sec-performance-engineering-speculative}

Imagine trying to read a book by asking for one letter at a time, waiting for each letter to be delivered before asking for the next. This is exactly how autoregressive LLMs generate text, making them painfully slow and heavily memory-bound. Speculative decoding breaks this sequential bottleneck by having a small, fast model guess the next several words, allowing the massive main model to verify them all in a single parallel step.

### The Core Algorithm {#sec-performance-engineering-speculative-algorithm}

The speculative decoding algorithm proceeds in three phases:

**Draft phase**: A small, fast model (the "draft model") generates $K$ candidate tokens autoregressively. Because the draft model is much smaller (e.g., 1B parameters versus 70B for the target), each draft step is fast, even though it is still sequential. Generating $K$ draft tokens takes roughly the time of one or two target model decode steps.

**Verification phase**: The target model processes all $K$ draft tokens *in parallel* in a single forward pass. Because this is a forward pass over $K$ known tokens (not autoregressive generation), the batch dimension is $K$, and the operation is a standard GEMM with much higher arithmetic intensity than batch-1 decode. The target model produces probability distributions for positions 1 through $K+1$.

**Acceptance phase**: Starting from the first draft token, compare the draft model's probability distribution with the target model's distribution. If the draft token has sufficiently high probability under the target model's distribution, accept it. Continue accepting tokens until a draft token is rejected or all $K$ are accepted. On rejection, sample a corrected token from a modified distribution that accounts for the draft model's error. This rejection sampling scheme guarantees that the output distribution is *exactly* the same as if the target model had generated tokens autoregressively.

The mathematical guarantee is crucial: speculative decoding is not an approximation. By using rejection sampling, the accepted tokens follow exactly the target model's distribution. The draft model only determines the speed of generation, not the quality. This is a remarkable property: the system generates tokens faster without any change in output quality.

This guarantee distinguishes speculative decoding from every other optimization technique in this chapter. Operator fusion changes the order of floating-point operations, which can alter results at the level of floating-point rounding. Precision reduction explicitly sacrifices numerical precision for speed. Quantization approximates weights and activations. Graph compilation may apply algebraically equivalent but numerically different transformations. In each case, the differences are typically negligible, but they exist. Speculative decoding is provably lossless: the output distribution is mathematically identical to standard autoregressive generation, regardless of the draft model's quality. A terrible draft model simply produces no speedup (all tokens are rejected), but the output is still correct.

The rejection sampling scheme works as follows. Let $p(x)$ be the target model's probability for token $x$ and $q(x)$ be the draft model's probability. A draft token $x$ is accepted with probability $\min(1, p(x)/q(x))$. If rejected, a corrected token is sampled from the distribution $\max(0, p(x) - q(x))$ normalized to sum to 1. This ensures that the marginal distribution of the accepted token exactly matches $p(x)$, regardless of how different $q(x)$ is from $p(x)$. The closer $q$ is to $p$, the higher the acceptance rate and the greater the speedup, but even a poorly matched draft model produces correct output, just more slowly.

```{python}
#| echo: false
#| label: speculative-speedup
# ┌─────────────────────────────────────────────────────────────────────────────
# │ SPECULATIVE DECODING SPEEDUP
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-speculative-speedup speedup analysis prose
# │
# │ Goal: Compute expected latency speedup from speculative decoding at K=5 draft
# │       tokens with a 20x-faster draft model, across three acceptance rates
# │       (α=0.9 high, 0.7 medium, 0.5 low), to show the α-sensitivity of the gain.
# │ Show: ~3.5x (α=0.9), ~2.2x (α=0.7), ~1.4x (α=0.5) — inline speedup bullets
# │       in @sec-performance-engineering-speculative-speedup.
# │ How: E[accepted tokens] = Σ αⁱ for i in 0..K (geometric sum); speedup =
# │       (E[tokens] * t_target) / (t_draft_total + t_target).
# │
# │ Imports: (none — standalone calculation), mlsys.formatting (check)
# │ Exports: sp_high_str, sp_med_str, sp_low_str, et_high_str, et_med_str, et_low_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class SpeculativeSpeedup:
    """Speculative decoding latency speedup across acceptance rates."""

    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    alpha_high = 0.9    # acceptance rate for "easy" text (continuation, common patterns)
    alpha_med = 0.7     # medium-difficulty text
    alpha_low = 0.5     # "hard" text (reasoning, code)
    K = 5               # draft length

    # Target model: time for 1 forward pass
    t_target = 1.0      # normalized
    # Draft model: time for K drafts
    t_draft_per_token = 0.05  # draft is ~20x faster
    t_draft_total = t_draft_per_token * K  # 0.25

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────
    # Expected tokens per speculation round: 1 + sum(alpha^i for i=1..K)
    # = (1 - alpha^(K+1)) / (1 - alpha) for geometric distribution

    def _expected_tokens(alpha, k):
        """Expected accepted tokens per round."""
        return sum(alpha**i for i in range(k + 1))

    def _speedup(alpha, k, t_target_norm, t_draft_total_norm):
        """Speedup over standard autoregressive."""
        et = sum(alpha**i for i in range(k + 1))
        time_per_round = t_draft_total_norm + t_target_norm
        baseline_time = et * t_target_norm
        return baseline_time / time_per_round

    sp_high = _speedup(alpha_high, K, t_target, t_draft_total)
    sp_med = _speedup(alpha_med, K, t_target, t_draft_total)
    sp_low = _speedup(alpha_low, K, t_target, t_draft_total)

    et_high = _expected_tokens(alpha_high, K)
    et_med = _expected_tokens(alpha_med, K)
    et_low = _expected_tokens(alpha_low, K)

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    check(sp_high > 2.0, f"High acceptance speedup should be >2x, got {sp_high:.1f}x")
    check(sp_low > 1.0, f"Even low acceptance should give speedup, got {sp_low:.1f}x")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    sp_high_str = f"{sp_high:.1f}"
    sp_med_str = f"{sp_med:.1f}"
    sp_low_str = f"{sp_low:.1f}"
    et_high_str = f"{et_high:.1f}"
    et_med_str = f"{et_med:.1f}"
    et_low_str = f"{et_low:.1f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
sp_high_str = SpeculativeSpeedup.sp_high_str
sp_med_str = SpeculativeSpeedup.sp_med_str
sp_low_str = SpeculativeSpeedup.sp_low_str
et_high_str = SpeculativeSpeedup.et_high_str
et_med_str = SpeculativeSpeedup.et_med_str
et_low_str = SpeculativeSpeedup.et_low_str
```

### Speedup Analysis {#sec-performance-engineering-speculative-speedup}

The speedup from speculative decoding depends on two factors: the **acceptance rate** $\alpha$ (probability that the draft model's token matches the target model's distribution) and the **draft length** $K$ (number of speculative tokens per round).

The expected number of accepted tokens per round is approximately $\sum_{i=0}^{K} \alpha^i = \frac{1 - \alpha^{K+1}}{1 - \alpha}$. The time per round is the cost of $K$ draft steps plus one target verification step. Since the draft model is typically 10--20 $\times$ smaller, and verification is a single forward pass over $K$ tokens, the total round time is dominated by the target model's single forward pass.

For $K = 5$ draft tokens and a draft model that is 20 $\times$ faster than the target:

- **High acceptance** ($\alpha = 0.9$): Expected `{python} et_high_str` tokens per round, speedup of `{python} sp_high_str` $\times$.
- **Medium acceptance** ($\alpha = 0.7$): Expected `{python} et_med_str` tokens per round, speedup of `{python} sp_med_str` $\times$.
- **Low acceptance** ($\alpha = 0.5$): Expected `{python} et_low_str` tokens per round, speedup of `{python} sp_low_str` $\times$.

The acceptance rate varies dramatically by context. Predictable text (formulaic responses, copying from context, common phrases) achieves $\alpha > 0.9$. Creative text, complex reasoning, or code generation may see $\alpha < 0.5$. In practice, systems observe average acceptance rates of 0.6--0.8 across diverse workloads, yielding 1.5--3 $\times$ latency improvements.

The acceptance rate also depends on the **temperature** of the sampling distribution. At temperature 0 (greedy decoding), the acceptance criterion is exact match: the draft token is accepted if and only if it is the argmax of the target distribution. At higher temperatures, the rejection sampling scheme allows more diverse tokens to be accepted, generally increasing the acceptance rate. This means speculative decoding provides the largest absolute speedup at low temperatures (where greedy decoding produces highly predictable output) and smaller speedup at high temperatures (where the output is more stochastic and harder to predict).

While it might seem advantageous to increase the draft length $K$ indefinitely, the returns diminish rapidly due to the geometric decay of acceptance probability. The probability of accepting the $i$-th draft token is $\alpha^i$, so for $\alpha = 0.6$, the probability of accepting the 6th token is merely $0.6^6 \approx 0.047$. Increasing $K$ from 5 to 10 only increases the expected tokens from 2.4 to 2.49, yet it doubles the verification cost for the target model. In practice, $K$ is rarely tuned beyond 5--7, as the marginal gain in accepted tokens fails to amortize the linear growth in verification latency.

The selection of the draft model size introduces a critical optimization surface. A tiny draft model (e.g., 150M parameters for a 70B target) is computationally negligible but yields a low $\alpha$ (e.g., 0.4), resulting in frequent rollbacks. Conversely, a large draft model (e.g., 7B parameters) achieves high alignment ($\alpha > 0.8$) but its own latency becomes a bottleneck, consuming time that could have been spent on target model decode steps. For a 70B target, empirical results favor a draft model in the 1B--3B range. A 1.1B draft model might achieve $\alpha = 0.6$ with 2 ms latency per draft step, yielding a 2.3 $\times$ speedup. Upgrading to a 7B draft model might boost $\alpha$ to 0.75, but if the draft latency rises to 10 ms, the net wall-clock speedup drops to 1.5 $\times$. The optimal draft model size shifts with the difficulty of the prompt: code generation ($\alpha$ often above 0.8 due to structural predictability) tolerates smaller draft models better than creative writing.

::: {.callout-notebook title="The Speculation Budget"}
Is speculative decoding worth the implementation complexity? Consider a draft model that is $10\times$ cheaper per token than the target model, and a verification step that processes a candidate batch for $1.5\times$ the cost of a single target decode step. The break-even acceptance rate $\alpha$ is roughly defined by the ratio of overhead to savings.
$$ \text{Break-Even } \alpha \approx \frac{\text{Verification Cost}}{\text{Draft Savings Ratio}} \approx \frac{1.5}{10} = 15\% $$
Since modern draft models (e.g., sheared Llama-2) typically achieve acceptance rates of 60-80% on standard prose, the arbitrage is highly profitable, often yielding $2\text{-}3\times$ end-to-end latency reductions.
:::

### System Design Considerations {#sec-performance-engineering-speculative-system}

Deploying speculative decoding in production requires solving several system-level challenges.

**Draft model selection** balances speed against acceptance rate. A larger draft model achieves higher acceptance but is slower per step. Common choices include: a distilled version of the target model (sharing architecture and vocabulary), the target model with early exit (computing only the first few transformer layers), or a separate small model trained on the same data distribution. Self-speculative decoding uses the target model itself with layer skipping, avoiding the need for a separate draft model entirely. The trade-off is that the "draft" (early layers of the target model) may produce lower-quality predictions than a dedicated small model trained specifically for this purpose.

**Online adaptation** addresses a subtle problem: the draft model's acceptance rate depends on the similarity between the draft and target model distributions, which varies across contexts. An adaptive system monitors the acceptance rate in real time and adjusts the speculation depth $K$. When acceptance is high (predictable text like formulaic responses or quoting from context), $K$ is increased to generate more tokens per round. When acceptance is low (creative writing, complex reasoning, or code generation), $K$ is decreased to avoid wasting draft compute. Some implementations maintain a running exponential average of the acceptance rate and use it as the decision threshold for dynamically adjusting $K$.

**Batch-level speculation** adds complexity. In a continuous batching system serving multiple concurrent requests, some requests may be in the draft phase while others are in verification. The system must manage separate KV caches for draft and target models, schedule verification forward passes to maintain high GPU utilization, and handle the variable number of accepted tokens per request, which changes the effective batch size for subsequent iterations.

The interaction between speculative decoding and continuous batching deserves particular attention. In a standard continuous batching system, all active requests advance by exactly one token per iteration, maintaining a predictable batch size. With speculation, different requests accept different numbers of tokens per round, creating jagged advances that complicate memory management and batch scheduling. Some systems address this by running speculation only for requests that are not batch-limited (i.e., when the batch is small enough that decode is still memory-bound), falling back to standard autoregressive generation when the batch grows large enough to be compute-bound. This adaptive strategy applies speculation where it provides the greatest benefit while avoiding overhead where batching alone provides sufficient throughput.

**Tree-structured speculation** extends beyond linear draft sequences. Instead of generating a single chain of $K$ tokens, the draft model generates a tree of possible continuations at each position. For instance, after drafting token $t_1$, the draft model might explore the top-3 most likely next tokens, generating three branches. Each branch is extended further, producing a tree of potential continuations.

The target model verifies all paths in a single batched forward pass using an attention mask that encodes the tree structure. Each node in the tree attends only to its ancestors, not to tokens on sibling branches. The longest valid path from root to leaf is accepted, and the remaining branches are discarded. For a tree with branching factor $b$ and depth $d$, the total number of candidate tokens is $\sum_{i=1}^{d} b^i$, but the memory and compute cost of verification grows correspondingly. In practice, implementations use narrow trees (branching factor 2--3, depth 3--5) to balance the increased acceptance probability against the verification overhead.

**Medusa** (Cai et al., 2024) represents a particularly efficient variant of tree-structured speculation. Instead of using a separate draft model, Medusa adds lightweight prediction heads to the target model itself. Each head predicts a token at a different future position (head 1 predicts position $t+1$, head 2 predicts position $t+2$, etc.). The heads are small MLPs trained on top of the target model's final hidden states. Because the heads share the same backbone computation, their overhead is minimal. The target model then verifies the tree of candidates formed by combining the top-$k$ predictions from each head.

**Eagle** (Li et al., 2024) extends the Medusa concept by using a lightweight autoregressive draft head instead of independent prediction heads. The draft head receives the target model's hidden states from the previous position and generates candidate tokens autoregressively, but at a fraction of the target model's cost. Because the draft head sees previous hidden states (which encode the target model's "intention"), its predictions are better aligned with the target model, achieving higher acceptance rates than Medusa's independent heads. The trade-off is that Eagle's draft head is sequential (unlike Medusa's parallel heads), but its improved acceptance rate typically compensates for the additional sequential steps.

**Lookahead decoding** takes yet another approach: it uses Jacobi iteration to solve the autoregressive generation problem in parallel. Rather than generating tokens one by one, it maintains a window of $n$-grams from previous generations and attempts to place them at future positions. The target model's forward pass is modified to simultaneously verify multiple candidate positions, accepting those where the n-gram prediction matches the model's output. This approach requires no draft model at all, avoiding the complexity of model selection and deployment.

::: {.callout-notebook title="The Speculation Budget"}

**Problem**: You serve a 70B LLM that achieves 50 tokens/second at batch size 1 (memory-bound decode). You consider adding speculative decoding with a 1.5B draft model. The draft model generates 5 tokens in 8 ms. The target model's verification pass over 5 tokens takes 12 ms (slightly more than a single decode step due to KV cache setup overhead). Average acceptance rate is $\alpha = 0.75$. Is speculation profitable?

**The Math**:

*Expected tokens per round*:
$\sum_{i=0}^{5} 0.75^i = 1 + 0.75 + 0.5625 + 0.4219 + 0.3164 + 0.2373 = 3.29$ tokens

*Time per round*: $8 \text{ ms (draft)} + 12 \text{ ms (verify)} = 20$ ms

*Effective throughput*: $3.29 / 0.020 = 164.5$ tokens/second

*Baseline*: 50 tokens/second (one token per 20 ms decode step)

*Speedup*: $164.5 / 50 = 3.29\times$

**But consider the GPU cost**: The draft model consumes GPU memory and may require a dedicated GPU or share the target model's GPU. If the 1.5B draft model consumes 3 GB of HBM on the target GPU, it reduces KV cache budget. If it runs on a separate GPU, it doubles the hardware cost. The profitability of speculation depends on whether the 3.29 $\times$ latency improvement justifies the additional resource cost.

**Takeaway**: Speculative decoding provides a 3.29 $\times$ latency improvement when compute is "free" (memory-bound decode). The decision to deploy it depends on the system-level trade-off between per-request latency and per-token resource cost.

:::

### The Hardware Cost of Speculation {#sec-performance-engineering-speculative-hardware}

Speculative decoding's resource implications extend beyond the compute and latency analysis above. The draft model requires its own GPU memory allocation: weights, KV cache, and activation memory. Consider serving a 70B model on a cluster of 4 $\times$ H100-80GB GPUs (320 GB total). The FP16 weights consume 140 GB, and operational overhead (runtime, buffers) takes another 20 GB, leaving roughly 160 GB for the KV cache. This available memory determines the maximum batch size the system can support.

Introducing a 7B parameter draft model requires loading an additional 14 GB of weights plus reserving its own KV cache, consuming approximately 20--25 GB of HBM. This "speculation tax" reduces the primary KV budget by nearly 15%, effectively lowering the maximum concurrent user capacity from approximately 128 to 108 users (assuming 1.25 GB of KV cache per user at 4096-token context). If the draft model generates $K = 5$ tokens per round, it must maintain its own KV cache for the full sequence (including all previously generated tokens). For a long conversation with 4,000 tokens of context, the draft model's KV cache can consume several hundred megabytes per request. When serving many concurrent requests, each with its own draft KV cache, the aggregate memory pressure compounds the reduction in batch capacity.

This creates a fundamental tension between **latency** and **throughput**: speculative decoding improves the latency for individual users (reducing time-per-token) but degrades the aggregate throughput of the serving node by reducing the maximum batch size. In throughput-bound scenarios, such as offline batch processing or high-traffic API endpoints, operators often disable speculation to maximize parallel concurrency, reserving the technique for latency-sensitive, real-time interactive applications.

Some implementations mitigate this by sharing architectural components between the draft and target models. If the draft model is a distilled version of the target (same vocabulary, same tokenizer, smaller hidden dimension), the embedding layers can be shared. If the draft model is the first $n$ layers of the target (self-speculation), no additional weight memory is needed at all, though the KV cache for the draft "model" overlaps with the first $n$ layers of the target's KV cache. Furthermore, during the verification phase, the target model processes $K+1$ tokens in parallel; while this is efficient for the target model's arithmetic intensity, it spikes the instantaneous memory bandwidth demand. If the system is already bandwidth-bound serving a large batch, the verification step may contend with other active requests, degrading aggregate throughput.

The decision to deploy speculation therefore requires a system-level cost-benefit analysis: does the latency improvement from speculation justify the memory and throughput cost of maintaining draft model state? For latency-sensitive applications (real-time chatbots, interactive coding assistants), the answer is typically yes. For throughput-sensitive applications (batch document processing, offline summarization), the resources consumed by the draft model are better allocated to serving larger batches of the target model alone.

The transition from individual operation optimization (fusion, precision) through model-level compilation to algorithmic innovation (speculative decoding) reveals a spectrum of optimization strategies. The next section examines the most architecturally impactful innovation: Mixture of Experts, which changes the fundamental relationship between model size and inference cost.

## Mixture of Experts {#sec-performance-engineering-moe}

If we want the reasoning capabilities of a 1-trillion parameter model, but only have the latency budget and compute budget to run a 100-billion parameter model, how do we bridge the gap? We train a massive model but only activate a tiny, relevant fraction of it for any given word. The Mixture of Experts (MoE) architecture routes inputs only to specialized sub-networks, breaking the iron link between model size and compute cost.

### MoE Architecture {#sec-performance-engineering-moe-architecture}

In a standard MoE transformer layer, the feed-forward network (FFN) is replaced by multiple parallel "expert" FFNs and a lightweight **router** (also called a gating network) that selects which experts process each token.

::: {.callout-definition title="Mixture of Experts (MoE)"}

***Mixture of Experts (MoE)***\index{Mixture of Experts!definition} is an architecture that replaces a single dense feed-forward layer with $N$ parallel "expert" sub-networks and a lightweight **gating function** (router) that selects the top-$k$ experts for each input token. For a model with $N$ experts and top-$k$ routing, only $k/N$ of the parameters are activated per token, decoupling model capacity (total parameters) from computational cost (FLOPs per token). The gating function $G(x)$ produces a sparse probability distribution:
$$G(x) = \text{TopK}(\text{Softmax}(W_g \cdot x), k)$$
where $W_g$ is a learned routing matrix. A model with 8 experts and top-2 routing has 4x the parameters of a dense equivalent but uses only 25% more FLOPs per token (due to the router and load-balancing overhead). The systems challenge is **load balancing**: if the router sends disproportionate traffic to a few experts, those experts become bottlenecks while others sit idle, wasting the parallelism that MoE was designed to exploit.

:::

For a token $x$, the router produces a probability distribution over $E$ experts:

$$
g(x) = \text{Softmax}(\text{TopK}(W_g \cdot x, k))
$$ {#eq-moe-router}

where $W_g$ is a small learnable gating matrix and TopK selects the $k$ highest-scoring experts. The layer output is the weighted sum of the selected experts' outputs:

$$
y = \sum_{i \in \text{TopK}} g_i(x) \cdot \text{Expert}_i(x)
$$ {#eq-moe-output}

Typical configurations use $E = 8$ or $E = 16$ experts with $k = 2$ active per token. This means each token uses only $k/E = 25\%$ or $12.5\%$ of the expert parameters. The shared components (attention layers, embedding layers, router) are dense and always active.

The router is typically a single linear layer ($d \rightarrow E$) followed by softmax and top-k selection. Its parameter count is negligible ($d \times E$, typically 8,192 $\times$ 16 = 131,072 parameters), but its computational and system impact is outsized: the router's decisions determine which GPUs receive which tokens, driving the AllToAll communication pattern that dominates MoE system design.

In an MoE design, the dense Feed-Forward Network (FFN) layers are replaced with a bank of $N$ parallel "expert" networks. A trainable gating mechanism, typically using a Top-$k$ Softmax function, analyzes each incoming token and routes it to only the most relevant experts (usually $k=1$ or $2$). To prevent the gate from collapsing into a pattern where it routes all tokens to a single "super-expert," an auxiliary load-balancing loss is added to the objective function, penalizing uneven distribution and ensuring all experts are utilized. This conditional computation allows for massive increases in model capacity without corresponding increases in inference latency. For example, a 1.6 Trillion parameter MoE model using Top-2 routing might only activate 12 Billion parameters to process any single token, achieving the quality of a dense 1.6T model at the cost of a 12B model.

### DeepSeek-V3: A Case Study {#sec-performance-engineering-deepseek}

DeepSeek-V3, released in late 2024, exemplifies the state of the art in MoE system design. The model uses 671B total parameters with 256 routed experts per layer and 1 shared expert, activating 8 routed experts plus the shared expert for each token. This yields approximately 37B activated parameters per token, an 18 $\times$ ratio between total and active parameters.

Several architectural innovations in DeepSeek-V3 address the system challenges described above. The model uses **Multi-head Latent Attention (MLA)**, which compresses the KV cache through low-rank projections, dramatically reducing per-token memory. The routing mechanism uses an **auxiliary-loss-free** load-balancing strategy that adjusts expert selection probabilities through a learned bias term rather than an explicit auxiliary loss, avoiding the quality degradation that can accompany auxiliary loss tuning. The training was completed on 2,048 H800 GPUs in approximately 2.8 million GPU-hours, at an estimated cost of approximately \$5.6 million, a fraction of the cost reported for comparable dense models. This cost efficiency demonstrates the economic advantage of MoE architectures: by activating only 37B of 671B parameters per token, the model achieves the quality of a much larger dense model while requiring substantially less compute per training token.

```{python}
#| echo: false
#| label: moe-economics
# ┌─────────────────────────────────────────────────────────────────────────────
# │ MOE ECONOMICS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-moe MoE performance advantages prose
# │
# │ Goal: Quantify per-token bandwidth and compute savings of a DeepSeek-V3 MoE
# │       (671B total, 37B active) versus a dense 400B model in FP16, to show that
# │       sparse activation delivers dense-quality inference at 10x lower cost.
# │ Show: 800 GB/step (dense 400B) vs ~74 GB/step (37B active) → ~11x BW reduction;
# │       ~11x FLOP reduction per token — inline in MoE performance paragraph.
# │ How: Per-token bandwidth = active_params * bytes_fp16; FLOPs = 2 * active_params;
# │       ratio = dense / active for both metrics.
# │
# │ Imports: (none — standalone calculation), mlsys.formatting (check)
# │ Exports: dense_mem_str, moe_mem_str, compute_ratio_str, bw_ratio_str
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class MoEEconomics:
    """MoE vs dense model bandwidth and compute comparison."""

    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    dense_params_b = 400     # Dense model: 400B parameters
    moe_total_params_b = 671 # DeepSeek-V3 total
    moe_active_params_b = 37 # Active per token
    bytes_fp16 = 2
    bytes_int4 = 0.5

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────

    # Memory: all parameters must be in memory (even MoE stores all experts)
    dense_mem_fp16 = dense_params_b * bytes_fp16   # 800 GB
    moe_mem_fp16 = moe_total_params_b * bytes_fp16 # 1342 GB

    # Per-token compute (FLOPs ~ 2 * active_params for forward pass)
    dense_flops_per_token = 2 * dense_params_b     # 800B FLOPs
    moe_flops_per_token = 2 * moe_active_params_b  # 74B FLOPs
    compute_ratio = dense_flops_per_token / moe_flops_per_token

    # Memory bandwidth per decode step (batch=1): read active weights
    dense_decode_bytes = dense_params_b * bytes_fp16  # 800 GB
    moe_decode_bytes = moe_active_params_b * bytes_fp16  # 74 GB
    bw_ratio = dense_decode_bytes / moe_decode_bytes

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    check(compute_ratio > 5, f"MoE should give >5x compute reduction, got {compute_ratio:.1f}x")
    check(bw_ratio > 5, f"MoE should give >5x BW reduction, got {bw_ratio:.1f}x")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    dense_mem_str = f"{dense_mem_fp16:.0f}"
    moe_mem_str = f"{moe_mem_fp16:.0f}"
    compute_ratio_str = f"{compute_ratio:.0f}"
    bw_ratio_str = f"{bw_ratio:.0f}"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
dense_mem_str = MoEEconomics.dense_mem_str
moe_mem_str = MoEEconomics.moe_mem_str
compute_ratio_str = MoEEconomics.compute_ratio_str
bw_ratio_str = MoEEconomics.bw_ratio_str
```

The performance advantages are striking. During autoregressive decode at batch size 1, the dominant cost is reading weights from HBM. A dense 400B model in FP16 reads `{python} dense_mem_str` GB per step. DeepSeek-V3, despite having more total parameters, reads only the 37B active parameters per step (approximately 74 GB in FP16), a `{python} bw_ratio_str` $\times$ reduction in per-token bandwidth. The compute savings are proportional: `{python} compute_ratio_str` $\times$ fewer FLOPs per token.

The trade-off is memory capacity: all experts must reside in memory even though only a fraction are active at any time. DeepSeek-V3's full model in FP16 requires approximately `{python} moe_mem_str` GB, necessitating distribution across many GPUs. This creates a tension between the bandwidth savings from sparse activation and the memory overhead of storing all experts.

### Expert Parallelism and Load Balancing {#sec-performance-engineering-moe-parallelism}

Distributing MoE models across GPUs introduces **expert parallelism**: each GPU holds a subset of experts, and tokens are routed to the GPU holding their assigned expert. This creates a distinctive communication pattern: instead of the AllReduce used in data parallelism, MoE requires **AllToAll** communication, where each GPU sends different data to every other GPU.

The AllToAll pattern creates a critical system challenge: **load balancing**. If the router consistently sends more tokens to certain experts than others, those experts' GPUs become bottlenecks while other GPUs sit idle. In the worst case, a single "popular" expert receives a disproportionate share of tokens, serializing computation through one GPU.

Three mechanisms address load imbalance:

**Auxiliary loss**: An additional term in the training loss penalizes uneven expert utilization. The standard auxiliary loss (from Switch Transformer, Fedus et al., 2022) adds a term proportional to the product of the fraction of tokens routed to each expert and the fraction of the routing probability assigned to that expert:

$$
\mathcal{L}_{\text{aux}} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
$$

where $f_i$ is the fraction of tokens dispatched to expert $i$, $P_i$ is the fraction of routing probability assigned to expert $i$, $N$ is the number of experts, and $\alpha$ is a balancing coefficient (typically 0.01--0.1). This loss is minimized when tokens and probabilities are uniformly distributed. DeepSeek-V3 uses a refined variant that decouples the auxiliary loss from the primary model quality loss, preventing the load-balancing objective from interfering with learning.

**Capacity factor**: Each expert has a maximum number of tokens it will accept per batch, typically expressed as a multiple of the "fair share" ($\text{batch tokens} / E$). A capacity factor of 1.25 means each expert's buffer can hold 25% more tokens than the perfectly balanced share. Tokens exceeding the capacity are either dropped (reducing model quality) or rerouted to a second-choice expert. Setting the capacity factor too low causes token dropping; setting it too high wastes memory. The optimal capacity factor depends on the natural imbalance of the data distribution and typically requires tuning per dataset.

**Expert buffering**: Some tokens are buffered and processed in the next iteration if their assigned expert is currently at capacity. This smooths out load spikes but introduces latency variance. Expert buffering is more suitable for training (where throughput matters more than per-sample latency) than for inference (where buffering delays individual responses).

### Routing Failure Modes {#sec-performance-engineering-moe-routing-failures}

The router is a small learned component, but its behavior has outsized impact on system performance and model quality. Several failure modes can occur during training and serving that the performance engineer must anticipate.

**Expert collapse** occurs when the router converges to sending most tokens to a small subset of experts, leaving the remaining experts undertrained. In a 64-expert model, if the router sends 90% of tokens to just 4 experts, the effective parameter count available for any given inference step drops precipitously: the remaining 60 experts sit idle, consuming memory but contributing no intelligence. The model effectively degrades to a small dense model with a massive memory footprint. The auxiliary loss (@eq-moe-router) addresses this, but if the auxiliary loss coefficient $\alpha$ is too small, the quality-driven gradient overwhelms the load-balancing signal. If $\alpha$ is too large, the model prioritizes uniform routing over quality, degrading outputs. Finding the right balance typically requires hyperparameter sweeps, and the optimal coefficient often differs between the early and late stages of training.

**Routing instability** manifests as oscillating expert assignments during training, where the same token type is sent to different experts in consecutive training steps. This instability prevents experts from specializing, because each expert sees a random assortment of token types rather than a consistent subset. Routing instability is particularly common in the early stages of training, before the router has learned meaningful token representations. Some implementations address this by using a warm-up period where the auxiliary loss coefficient starts high (forcing uniform routing) and gradually decreases, allowing specialization to emerge gradually.

**Distribution shift** between training and serving creates a more subtle problem. A router trained primarily on clean, formal text (like Wikipedia and books) may learn to associate specific experts with formal grammar patterns. When deployed to a chat application receiving colloquial, multilingual, or messy input, the router may forcefully direct all traffic to those "grammar experts," overloading them while leaving domain-specific experts underutilized. Even if load balancing was excellent during training, the serving distribution can create hotspots that degrade both latency and quality. Monitoring expert utilization in production and fine-tuning the router on representative serving data addresses this gap.

**Expert fragmentation** is a related but distinct failure mode where experts become hyper-specialized on narrow syntactic features rather than broad semantic categories. An expert that only activates for Python `import` statements or HTML tags provides no value for the vast majority of tokens. While computationally efficient in isolation, this fragmentation makes the model brittle: slight variations in the prompt can cause the router to switch experts erratically, leading to incoherent generation as the model struggles to maintain context across expert boundaries.

**Token dropping** occurs when a capacity-limited expert receives more tokens than its buffer can hold. The excess tokens are either routed to a second-choice expert (potentially less specialized) or dropped entirely (skipping the expert computation). While a small amount of token dropping is tolerable (typical capacity factors allow 10--20% overflow), excessive dropping indicates a routing or load-balancing problem that degrades model quality.

### MoE Serving Architecture {#sec-performance-engineering-moe-serving}

Serving MoE models introduces unique system challenges beyond those of dense models. The primary distinction is that different tokens in the same batch may route to different subsets of experts, creating irregular memory access patterns and variable compute loads per GPU.

In a typical deployment, expert parallelism is combined with tensor parallelism and data parallelism in a multi-dimensional layout. For example, DeepSeek-V3 with 256 experts might be deployed across 32 GPUs:

- **Expert parallelism (EP=8)**: 256 experts distributed across 8 expert-parallel groups, with 32 experts per GPU.
- **Tensor parallelism (TP=4)**: Each expert is sharded across 4 GPUs for reduced per-GPU memory and latency.

This hierarchical layout means that token routing involves both intra-node NVLink transfers (for tensor parallelism) and inter-node InfiniBand transfers (for expert parallelism). The AllToAll communication pattern creates a challenging traffic matrix: each GPU sends different amounts of data to every other GPU depending on the router's decisions. Unlike AllReduce, where every GPU sends and receives the same volume, AllToAll traffic is inherently unbalanced. Network congestion on individual links can create hotspots that affect overall throughput.

Prefetching offers a partial solution: if the router's decisions can be predicted or computed slightly ahead of the actual expert execution, the system can pre-stage the required expert weights in SRAM or initiate data transfers before they are needed. Some implementations maintain all expert weights in HBM but cache the most frequently accessed experts in L2 or shared memory.

The economics of MoE serving create a paradox for capacity planning. The model's total parameter count determines the minimum number of GPUs (for memory), while the active parameter count determines the compute requirement per token. A 671B MoE model requires far more GPUs than a 37B dense model, even though the per-token compute is comparable. This means the cost per token is higher than for a similarly capable dense model unless the MoE model's superior quality justifies the additional infrastructure cost.

::: {.callout-notebook title="MoE Capacity Planning"}

**Problem**: You are deploying DeepSeek-V3 (671B total parameters, 37B active per token) for a chatbot application. The model uses FP8 weights (1 byte per parameter). Your cluster has 8-GPU nodes, each with 8 $\times$ H100 (80 GB HBM per GPU, 640 GB per node). How many nodes do you need, and what is the per-token decode latency?

**The Math**:

*Step 1: Memory requirements.*
Total weight memory: $671 \times 10^9 \times 1 = 671$ GB.
A single 8-GPU node has 640 GB of HBM, insufficient for the weights alone.
Two nodes provide 1,280 GB, leaving approximately 609 GB for KV caches and activations.

*Step 2: Decode latency at batch size 1.*
Each decode step reads the 37B active parameters (37 GB in FP8). With 16 GPUs across 2 nodes, the active parameters are distributed across expert-hosting GPUs. The AllToAll communication routes tokens to the correct experts.

Local weight reads per GPU: approximately $37 / 16 \approx 2.3$ GB. At `{python} h100_hbm_bw_str` TB/s HBM bandwidth, this takes:

$$
t_{\text{read}} = \frac{2.3 \text{ GB}}{3.35 \text{ TB/s}} \approx 0.69 \text{ ms per GPU}
$$

The AllToAll overhead adds approximately 0.3 ms for routing token data across NVLink (within nodes) and InfiniBand (across nodes).

Estimated decode latency: approximately 1.0 ms per token, or roughly 1,000 tokens/second at batch size 1.

**Takeaway**: MoE enables a 671B model to achieve latency comparable to a 37B dense model (both read $\sim$37 GB of active weights per step), but requires 2 $\times$ more GPUs for memory capacity. The cost per token is higher than a 37B dense model, but the quality approaches that of a 671B dense model, which would require 4 $\times$ more GPUs and 10 $\times$ more compute per token.

:::

::: {.callout-checkpoint title="MoE System Design" collapse="false"}

Test your understanding of MoE trade-offs:

- [ ] Can you explain why MoE reduces per-token compute and bandwidth but *increases* total memory requirements?
- [ ] Can you describe why AllToAll communication is required for expert parallelism, and how it differs from AllReduce?
- [ ] Can you identify why load imbalance is more dangerous in MoE than uneven batch sizes in data parallelism?
- [ ] Can you explain why expert parallelism is typically combined with data and tensor parallelism in production deployments?

:::

The co-evolution of MoE architectures and system software illustrates a broader principle in performance engineering: the boundary between "model design" and "system design" is dissolving. Decisions about expert count, routing strategy, and capacity factors are simultaneously model quality decisions and system throughput decisions. The performance engineer must understand both domains to make informed trade-offs.

MoE models demonstrate how architectural innovation creates system-level optimization opportunities. But even the most efficient model still generates communication traffic between GPUs. The next section examines how to hide that communication cost behind computation.

## Communication-Computation Overlap {#sec-performance-engineering-overlap}

Picture an assembly line where workers stop building every time a delivery truck arrives, wait for it to unload, and only then resume their tasks. In a distributed ML cluster, GPUs behave exactly this way if communication and computation are strictly sequential. By overlapping these operations—forcing the GPU to calculate the next layer's activations while simultaneously broadcasting the previous layer's gradients—we can effectively hide the network latency entirely.

### The Overlap Principle {#sec-performance-engineering-overlap-principle}

Modern GPUs have independent hardware engines for computation (streaming multiprocessors, or SMs) and communication (NVLink engines, PCIe DMA engines, network interface controllers). These engines can operate simultaneously. If we can schedule a communication operation on the NVLink engine while the SMs execute a compute kernel, the communication time is "free" from the perspective of the critical path, as long as it completes before its result is needed.

The overlap is effective when:

$$
t_{\text{communicate}}(L_i) \leq t_{\text{compute}}(L_{i+1})
$$ {#eq-overlap-condition}

where $L_i$ and $L_{i+1}$ are consecutive layers. If the communication time for layer $i$'s gradients is less than the compute time for layer $i+1$'s backward pass, the communication is completely hidden. When this condition holds for every layer, the distributed training or inference step achieves the same throughput as a hypothetical zero-latency network.

To make this concrete, consider the backward pass of a 70B parameter model distributed across H100 GPUs. For a typical transformer layer, the gradient computation takes approximately 5 ms. This is the time window available to hide the communication of gradients from the previous layer. If the system is connected via NVLink with 900 GB/s bidirectional bandwidth, an AllReduce operation for a 500 MB gradient buffer takes roughly $t_{\text{comm}} \approx 2 \times 0.5 \text{ GB} / 900 \text{ GB/s} \approx 1.1$ ms. Since 1.1 ms is well within the 5 ms compute window, the communication is entirely hidden. However, if the same system uses 100 Gbps Ethernet (12.5 GB/s effective), the transfer time balloons to approximately 80 ms. The compute finishes in 5 ms, and the GPU sits idle for 75 ms waiting for data. Even with partial overlap, the system is network-bound.

When the overlap condition fails ($t_{\text{comm}} > t_{\text{comp}}$), only a fraction of the communication is hidden. The exposed communication time is $t_{\text{exposed}} = t_{\text{comm}} - t_{\text{comp}}$, and the effective step time includes this residual. For the Ethernet example, the exposed time is $80 - 5 = 75$ ms per layer, making the interconnect the dominant bottleneck. This analysis explains why high-bandwidth interconnects like NVLink and InfiniBand are not luxuries but necessities for distributed training: they determine whether communication can be hidden behind computation or whether it serializes the training loop.

### Overlap in Distributed Training {#sec-performance-engineering-overlap-training}

In data-parallel training, each GPU computes gradients for the entire model, then synchronizes gradients across all GPUs using AllReduce (as described in @sec-collective-communication). Without overlap, the training step time is:

$$
t_{\text{step}} = t_{\text{forward}} + t_{\text{backward}} + t_{\text{AllReduce}}
$$

With gradient overlap, the backward pass is structured so that as soon as the gradients for the last layer ($L_N$) are computed, the AllReduce for $L_N$ begins while the backward pass continues computing gradients for $L_{N-1}$. This "pipelining" of communication and computation overlaps the AllReduce with the backward pass:

$$
t_{\text{step}} = t_{\text{forward}} + \max(t_{\text{backward}}, t_{\text{AllReduce}})
$$

In practice, overlapping gradient AllReduce with the backward pass is straightforward because gradients are produced in reverse layer order and consumed only after the full AllReduce completes. Most distributed training frameworks (PyTorch DDP, Horovod) implement this overlap automatically by registering backward hooks that trigger AllReduce as soon as each layer's gradients are ready.

The implementation in PyTorch DDP uses **gradient bucketing** to balance communication granularity against overlap opportunity. Rather than launching a separate AllReduce for each parameter's gradient (which would incur excessive launch overhead) or waiting for all gradients to accumulate (which would prevent overlap entirely), DDP groups gradients into fixed-size buckets (default 25 MB). As soon as all gradients in a bucket are computed during the backward pass, the bucket's AllReduce begins. The bucket size is a tunable parameter: smaller buckets enable earlier communication start (better overlap) but increase the number of AllReduce operations (more overhead); larger buckets reduce overhead but delay communication start (worse overlap). For models with many small parameters (e.g., transformers with many LayerNorm layers), the default 25 MB bucket size works well. For models with a few very large parameters (e.g., large embedding tables), increasing the bucket size avoids fragmenting those parameters across multiple AllReduce calls.

The gradient computation order matters for overlap effectiveness. The backward pass computes gradients in reverse layer order: the last layer's gradients are computed first, and the first layer's gradients are computed last. If the model's largest parameters (typically the embedding layer) are in the first layer, their gradients are computed last, giving them the least time for overlapped communication. PyTorch DDP addresses this by allowing the user to specify gradient computation order or by automatically reordering buckets based on the observed backward pass order during the first iteration.

### Zero-Bubble Pipeline Parallelism {#sec-performance-engineering-zero-bubble}

Pipeline parallelism, introduced in @sec-distributed-training-systems, divides the model into stages assigned to different GPUs. The classic problem with pipeline parallelism is the **pipeline bubble**: GPUs at the beginning of the pipeline are idle while waiting for gradients to flow back from the end, and vice versa. These bubbles represent wasted compute.

The **1F1B** (one forward, one backward) schedule reduces the bubble by interleaving forward and backward microbatches. Once the pipeline is filled, each GPU alternates between executing one forward microbatch and one backward microbatch, keeping SMs busy most of the time.

**Zero-bubble pipeline schedules** further reduce idle time by overlapping weight gradient computation with activation gradient communication. In a standard backward pass, the GPU computes $\partial L / \partial W$ (weight gradient) and $\partial L / \partial X$ (activation gradient, sent to the previous stage) together. Zero-bubble scheduling splits these into separate kernels: a **B** kernel that computes only the activation gradient $\partial L / \partial X$ and sends it to the previous stage, and a **W** kernel that computes the weight gradient $\partial L / \partial W$ locally. The B kernel must execute promptly (it is on the critical path), but the W kernel can be scheduled opportunistically to fill bubbles.

The scheduling freedom provided by this B/W split is substantial. In a 4-stage pipeline with 8 microbatches, the standard 1F1B schedule has a bubble fraction of approximately $(p-1)/(m+p-1)$ where $p$ is the number of stages and $m$ is the number of microbatches. For $p=4, m=8$, this is $3/11 \approx 27\%$ idle time. Zero-bubble scheduling can reduce this to near zero by filling the startup and teardown bubbles with W computations.

The trade-off is memory: zero-bubble scheduling requires storing intermediate activations for longer (because the W computation is deferred), increasing peak memory usage. Some implementations address this by combining zero-bubble scheduling with activation checkpointing, selectively recomputing certain activations rather than storing them. The interaction between these techniques creates a three-way trade-off among pipeline bubble size, memory consumption, and recomputation overhead.

### Overlap in Inference {#sec-performance-engineering-overlap-inference}

Communication-computation overlap in inference manifests differently than in training. In tensor-parallel inference, a single forward pass involves multiple AllReduce operations (after each attention and FFN layer). Overlapping these communications requires splitting the computation at each layer into a communication-independent portion and a communication-dependent portion.

For example, in a two-GPU tensor-parallel transformer layer:

1. Each GPU computes its shard of the attention output.
2. AllReduce synchronizes the partial attention outputs.
3. Each GPU computes its shard of the FFN output.

Between steps 1 and 2, the GPU is idle waiting for AllReduce. If the FFN for the *previous* token (in a pipelined serving system) is available, the GPU can begin computing it while the AllReduce for the current token proceeds. This requires careful token-level pipelining and separate CUDA streams for communication and computation.

The challenge is more severe for inference than for training because the compute-to-communication ratio is less favorable. In training, a large batch means each GPU performs substantial computation (large GEMMs) between communication points. In inference at small batch sizes, each GPU performs minimal computation (small, memory-bound GEMMs) between AllReduce calls. The AllReduce time may exceed the compute time, making full overlap impossible.

Consider a concrete example: a 70B model with tensor parallelism across 8 H100 GPUs. Each transformer layer requires two AllReduce operations (one after attention, one after FFN). Each AllReduce transfers approximately $2 \times d \times \text{batch} \times 2$ bytes at FP16 (where $d = 8192$ for a 70B model). At batch size 1, the data volume is $2 \times 8192 \times 1 \times 2 = 32$ KB per AllReduce. At 900 GB/s NVLink bandwidth, this transfer takes approximately 36 ns. However, the AllReduce launch overhead (approximately 5 $\mu$s) dominates the actual data transfer time by 100 $\times$. At batch size 1, the AllReduce overhead is dominated by software launch latency, not bandwidth, and overlap provides limited benefit because there is insufficient compute to hide behind.

At batch size 64, the data volume per AllReduce grows to $2 \times 8192 \times 64 \times 2 = 2$ MB, taking approximately 2.2 $\mu$s at NVLink bandwidth. The corresponding GEMM computation on each shard also takes approximately 20 $\mu$s (for a $64 \times 8192 \times 1024$ GEMM, where 1024 is the per-GPU hidden dimension). Here, the compute time exceeds the communication time by 9 $\times$, and overlap becomes highly effective. This illustrates why batch size is the universal control knob: it simultaneously improves arithmetic intensity, GPU utilization, and communication overlap effectiveness.

### Quantifying the Overlap Opportunity {#sec-performance-engineering-overlap-quantify}

The potential benefit of communication-computation overlap depends on the relative magnitudes of communication and computation time, which vary dramatically across system configurations.

```{python}
#| echo: false
#| label: overlap-calc
# ┌─────────────────────────────────────────────────────────────────────────────
# │ COMMUNICATION-COMPUTATION OVERLAP ANALYSIS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @sec-performance-engineering-comm-comp-overlap overlap analysis prose
# │
# │ Goal: Quantify training step speedup from overlapping ring-AllReduce with
# │       backward pass for a 7B model on 8 H100 GPUs over NVLink (900 GB/s),
# │       at 40% MFU, to show when overlap fully hides communication overhead.
# │ Show: ~14 GB gradients; ~31 ms backward; ~31 ms AllReduce; step collapses from
# │       ~93 ms (no overlap) to ~62 ms (with overlap) → ~1.50x speedup — inline.
# │ How: AllReduce time = 2 * gradient_bytes / nvlink_bw; backward time =
# │       backward_flops / (h100_tflops * MFU); overlap removes min(back, AR).
# │
# │ Imports: (none — uses inline constants), mlsys.formatting (check)
# │ Exports: gradient_gb_str, allreduce_ms_str, backward_ms_str, forward_ms_str,
# │          step_no_overlap_ms_str, step_overlap_ms_str, overlap_speedup_str,
# │          overlap_status
# └─────────────────────────────────────────────────────────────────────────────

# ┌── P.I.C.O. ISOLATED SCENARIO ───────────────────────────────────────────────
class OverlapCalc:
    """Communication-computation overlap speedup for 7B model on 8 H100s."""

    # ┌── 1. PARAMETERS (Inputs) ──────────────────────────────────────────────
    model_params = 7e9           # 7B parameter model
    bytes_per_param_fp16 = 2     # FP16 gradients
    num_gpus = 8
    nvlink_bw = 900e9            # H100 NVLink: 900 GB/s bidirectional
    ring_allreduce_factor = 2    # Ring AllReduce: 2*(N-1)/N ≈ 2 for large N

    # Backward pass compute: ~2 * forward FLOPs (gradient computation)
    # Forward FLOPs ≈ 2 * params * seq_len * batch_per_gpu
    # Simplified: backward ≈ 4 * params * tokens_per_gpu
    tokens_per_gpu = 2048        # seq_len * micro_batch
    forward_flops = 2 * model_params * tokens_per_gpu
    backward_flops = 2 * forward_flops  # backward is ~2x forward
    h100_fp16_tflops = 989e12    # Tensor core TFLOPS
    mfu = 0.40                   # Realistic MFU

    # ┌── 2. CALCULATION (The Physics) ────────────────────────────────────────

    # Gradient size
    gradient_bytes = model_params * bytes_per_param_fp16

    # AllReduce time (ring): 2*(N-1)/N * gradient_bytes / bandwidth
    allreduce_time_s = ring_allreduce_factor * gradient_bytes / nvlink_bw

    # Backward pass compute time
    backward_time_s = backward_flops / (h100_fp16_tflops * mfu)

    # Forward pass compute time (roughly half of backward)
    forward_time_s = forward_flops / (h100_fp16_tflops * mfu)

    # Step time without overlap
    step_no_overlap = forward_time_s + backward_time_s + allreduce_time_s

    # Step time with overlap
    step_with_overlap = forward_time_s + max(backward_time_s, allreduce_time_s)

    # Speedup
    overlap_speedup = step_no_overlap / step_with_overlap

    # Can we fully overlap?
    can_overlap = backward_time_s > allreduce_time_s

    # ┌── 3. INVARIANTS (Guardrails) ──────────────────────────────────────────
    check(overlap_speedup > 1.0,
          f"Overlap should provide speedup, got {overlap_speedup:.2f}x")

    # ┌── 4. OUTPUTS (Formatting) ─────────────────────────────────────────────
    gradient_gb_str = f"{gradient_bytes / 1e9:.0f}"
    allreduce_ms_str = f"{allreduce_time_s * 1000:.1f}"
    backward_ms_str = f"{backward_time_s * 1000:.1f}"
    forward_ms_str = f"{forward_time_s * 1000:.1f}"
    step_no_overlap_ms_str = f"{step_no_overlap * 1000:.1f}"
    step_overlap_ms_str = f"{step_with_overlap * 1000:.1f}"
    overlap_speedup_str = f"{overlap_speedup:.2f}"
    overlap_status = "fully hidden" if can_overlap else "partially hidden"

# ┌── EXPORTS (Bridge to Text) ─────────────────────────────────────────────────
gradient_gb_str = OverlapCalc.gradient_gb_str
allreduce_ms_str = OverlapCalc.allreduce_ms_str
backward_ms_str = OverlapCalc.backward_ms_str
forward_ms_str = OverlapCalc.forward_ms_str
step_no_overlap_ms_str = OverlapCalc.step_no_overlap_ms_str
step_overlap_ms_str = OverlapCalc.step_overlap_ms_str
overlap_speedup_str = OverlapCalc.overlap_speedup_str
overlap_status = OverlapCalc.overlap_status
```

Consider a 7B parameter model trained on 8 H100 GPUs within a single node connected by NVLink at 900 GB/s. The gradient tensor contains `{python} gradient_gb_str` GB of FP16 values. A ring AllReduce across 8 GPUs transfers approximately $2 \times (N-1)/N \approx 2$ times this volume, taking approximately `{python} allreduce_ms_str` ms at NVLink bandwidth. The backward pass computation, at 40% Model FLOPs Utilization, takes approximately `{python} backward_ms_str` ms.

Without overlap, the training step requires `{python} step_no_overlap_ms_str` ms (forward + backward + AllReduce). With overlap, the AllReduce is `{python} overlap_status` behind the backward pass, reducing the step time to `{python} step_overlap_ms_str` ms, a `{python} overlap_speedup_str` $\times$ improvement. This example illustrates a critical property: overlap is most effective when backward compute time exceeds AllReduce time. For smaller models or slower interconnects (e.g., PCIe at 64 GB/s instead of NVLink at 900 GB/s), the AllReduce would exceed the backward pass, and no amount of overlap can fully hide the communication.

::: {#fig-overlap-budget fig-env="figure" fig-pos="htb" fig-cap="The Overlap Budget: As cluster scale increases (8 to 1024 GPUs), the communication overhead grows, eventually exceeding the computation budget. The overlap efficiency (black line) represents the percentage of communication hidden behind computation, which degrades from ~90% to ~40% due to inter-node latency and bandwidth constraints on a 70B parameter model." fig-alt="Stacked bar chart of step time contribution versus GPU count. Overlapped communication, pure computation, and exposed communication. Black line shows overlap efficiency degrading from 90% to 40%."}
```{python}
# ┌─────────────────────────────────────────────────────────────────────────────
# │ OVERLAP BUDGET (FIGURE)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: @fig-overlap-budget — communication-computation overlap
# │
# │ Goal: Plot compute budget, comm overhead, overlap efficiency vs GPU count;
# │       show degradation from ~90% to ~40% at scale.
# │ Show: Bar/line chart; 8–1024 GPUs; overlap % curve.
# │ How: comm_scale_factor; overlap = compute/(compute+comm); matplotlib.
# │
# │ Imports: matplotlib.pyplot (plt), numpy (np)
# │ Exports: (figure only, no prose variables)
# └─────────────────────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt
import numpy as np

plt.style.use('seaborn-v0_8-whitegrid')

gpus = np.array([8, 16, 32, 64, 128, 256, 512, 1024])
x = np.arange(len(gpus))
compute_budget = 100.0

comm_scale_factor = np.logspace(0, 1.2, len(gpus))
total_comm_cost = 35.0 * comm_scale_factor

target_efficiency = np.linspace(0.92, 0.38, len(gpus))

hidden_comm = total_comm_cost * target_efficiency
hidden_comm = np.minimum(hidden_comm, compute_budget)

realized_efficiency = hidden_comm / total_comm_cost
exposed_comm = total_comm_cost - hidden_comm

comp_overlapped = hidden_comm
comp_pure = compute_budget - comp_overlapped
comm_exposed = exposed_comm

fig, ax1 = plt.subplots(figsize=(10, 6))

c_pure = '#1f77b4'
c_overlap = '#2ca02c'
c_exposed = '#d62728'

p1 = ax1.bar(x, comp_overlapped, label='Overlapped Communication', color=c_overlap, alpha=0.8, edgecolor='white')
p2 = ax1.bar(x, comp_pure, bottom=comp_overlapped, label='Pure Computation', color=c_pure, alpha=0.7, edgecolor='white')
p3 = ax1.bar(x, comm_exposed, bottom=comp_overlapped + comp_pure, label='Exposed Communication', color=c_exposed, alpha=0.8, hatch='//', edgecolor='white')

ax2 = ax1.twinx()
line = ax2.plot(x, realized_efficiency * 100, color='black', marker='o', linewidth=2, linestyle='--', label='Overlap Efficiency')
ax2.set_ylabel('Overlap Efficiency (%)', color='black', fontsize=12, labelpad=10)
ax2.set_ylim(0, 110)
ax2.grid(False)

ax1.set_xlabel('Number of GPUs', fontsize=12)
ax1.set_ylabel('Step Time Contribution (Normalized)', fontsize=12)
ax1.set_xticks(x)
ax1.set_xticklabels(gpus)
ax1.set_title('The Overlap Budget: Distributed Training Scaling', fontsize=14, pad=15)

lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines + lines2, labels + labels2, loc='upper left', frameon=True, fancybox=True, framealpha=0.9)

ax1.text(0, compute_budget + 5, 'NVLink\nIntra-node', ha='center', fontsize=9, color='#555')
ax1.text(7, compute_budget + 130, 'InfiniBand\nInter-node', ha='center', fontsize=9, color='#555')

plt.tight_layout()
fig = plt.gcf()
```
:::

### CUDA Streams and Asynchronous Execution {#sec-performance-engineering-cuda-streams}

The mechanism enabling overlap on NVIDIA GPUs is **CUDA streams**. A CUDA stream is an ordered sequence of GPU operations (kernel launches, memory copies, NCCL collectives) that execute sequentially within the stream but can execute concurrently with operations in other streams. The application maintains two distinct streams: a compute stream for matrix multiplications and element-wise kernels, and a communication stream for NCCL operations. The workflow proceeds by launching a compute kernel on the first stream and immediately triggering an asynchronous communication call on the second:

```python
compute_stream = torch.cuda.Stream()
comm_stream = torch.cuda.Stream()

with torch.cuda.stream(compute_stream):
    output = torch.matmul(A, B)  # Non-blocking compute

with torch.cuda.stream(comm_stream):
    dist.all_reduce(gradients, async_op=True)  # Non-blocking comm

torch.cuda.synchronize()  # Wait for both to complete
```

The GPU hardware scheduler interleaves execution units from both streams, running the GEMM on the SMs while the NVLink engine handles the AllReduce data transfer. However, while streams provide logical concurrency, they contend for physical resources. The SMs must manage the data movement instructions for the communication kernel. On an H100 with 132 SMs, a heavy NCCL operation might occupy 4--8 SMs solely for protocol processing and memory copying, leading to **SM partitioning**: the available compute throughput is reduced by 3--6% during communication. If the compute kernel is dense enough to saturate 100% of the SMs, enabling overlap can paradoxically slow down execution due to this resource contention, a phenomenon known as interference. In practice, the 3--6% compute throughput reduction is far smaller than the communication time that would otherwise be exposed, making the trade-off overwhelmingly favorable.

In practice, achieving effective overlap requires attention to several details. The communication operation must be launched *before* the compute kernel it should overlap with, not after, because NCCL operations have their own launch overhead. Synchronization points (where one stream waits for another) must be minimized, as each synchronization serializes execution.

PyTorch's Distributed Data Parallel (DDP) module implements gradient overlap by registering backward hooks on each parameter. When a parameter's gradient is computed during the backward pass, the hook triggers an asynchronous AllReduce on a separate NCCL stream. The optimizer step includes an implicit synchronization point that waits for all AllReduce operations to complete. This design overlaps gradient communication with gradient computation automatically, without requiring user intervention.

The techniques covered so far, fusion, precision, compilation, speculative decoding, MoE, and communication overlap, address different aspects of the performance equation. Identifying which technique to apply in a given situation requires systematic measurement. The next section examines the profiling tools that make this diagnosis possible.

## System Profiling {#sec-performance-engineering-profiling}

An engineer spends two weeks rewriting a PyTorch module into a custom CUDA kernel to make it 5x faster, only to discover the overall model latency didn't budge because the system was entirely I/O bound. Performance engineering without measurement is expensive guesswork. System profiling provides the surgical diagnostics required to identify exactly where the GPU is waiting, allowing us to apply our optimizations with precision.

### The Profiling Hierarchy {#sec-performance-engineering-profiling-hierarchy}

ML system profiling operates at four levels, each providing different granularity and answering different questions.

**Operation-level profiling** measures the execution time and resource utilization of individual GPU kernels. Tools like NVIDIA Nsight Compute provide detailed metrics for a single kernel: achieved memory bandwidth, compute utilization, occupancy (fraction of available threads active), and instruction mix. This level answers questions like: "Is this GEMM achieving peak throughput?" and "Is this kernel memory-bound or compute-bound?"

**Trace-level profiling** captures the timeline of all GPU kernels, CPU operations, and data transfers across the full execution of a training step or inference request. NVIDIA Nsight Systems and the PyTorch Profiler provide trace views showing exactly when each kernel executes, when data transfers occur, and where GPU idle gaps (bubbles) exist. This level reveals the overall structure of execution: sequential bottlenecks, launch gaps between kernels, and opportunities for overlap.

**Distributed profiling** extends the trace across multiple GPUs and nodes, showing communication patterns alongside computation. This reveals whether communication is overlapped with compute, which collective operations dominate step time, and whether load imbalance exists across GPUs.

**Application-level profiling** measures end-to-end metrics: tokens per second, time-to-first-token, P99 latency, and GPU utilization over time. This connects hardware-level observations to user-facing performance metrics and business KPIs.

Diagnosing performance requires a drill-down approach, moving from global symptoms to local causes. When profiling a 70B model serving pipeline, the application level might reveal "end-to-end latency is 150 ms/token, 3 $\times$ above the SLO." Descending to the distributed level, traces might show one GPU consistently lagging in AllReduce operations, pointing to a straggler or network congestion. Zooming into the trace level on that specific GPU reveals the timeline of kernel execution, exposing gaps where the SMs are idle due to scheduling overhead. Finally, kernel-level profiling (using Nsight Compute) inspects the specific instruction mix of a single matrix multiplication, revealing cache misses or register pressure. Skipping levels often leads to optimizing the wrong bottleneck: optimizing a kernel is futile if the GPU is spending 40% of its time waiting on the network.

### Key Performance Metrics {#sec-performance-engineering-metrics}

ML system performance is characterized by a family of metrics, each capturing a different aspect of system behavior. Understanding their relationships and trade-offs is essential for performance engineering.

**Model FLOPs Utilization (MFU)** measures what fraction of the hardware's theoretical peak is being productively used by the model's computation. MFU captures the combined effect of all inefficiencies: memory bandwidth limitations, kernel launch overhead, communication wait time, and software stack overhead. For large-scale LLM training, MFU of 40--60% is typical; values above 60% indicate excellent optimization.

**Hardware FLOPs Utilization (HFU)** is the simpler metric: total arithmetic operations executed (including overhead like activation recomputation) divided by peak FLOPS. HFU is always higher than MFU because it counts recomputed operations that do not directly advance the model's computation. The gap between HFU and MFU quantifies the "wasted" compute from techniques like activation checkpointing.

**Time-to-first-token (TTFT)** measures the latency from when a request arrives to when the first output token is generated. TTFT includes queue waiting time, prefill computation, and any initialization overhead. For interactive applications, TTFT below 500 ms is generally acceptable; below 200 ms feels responsive.

**Inter-token latency (ITL)** measures the time between consecutive output tokens during the decode phase. ITL directly determines the perceived generation speed. For a comfortable reading experience, ITL should be below 50 ms (20 tokens/second); for real-time speech synthesis, ITL below 25 ms may be required.

**Throughput** (tokens/second for the system, or tokens/second/GPU for per-device efficiency) measures aggregate production capacity. Throughput is maximized by large batch sizes and high utilization, often at the expense of increased per-request latency. The fundamental trade-off between throughput and latency is captured by queuing theory, as explored in @sec-inference-scale.

While MFU is the standard metric for training efficiency, it is often misleading for inference. Inference is predominantly memory-bound, meaning the Tensor Cores spend most cycles waiting for data. For these workloads, **Model Bandwidth Utilization (MBU)** is the more informative metric: the ratio of achieved memory bandwidth to the hardware's peak bandwidth. A decoding step on an H100 might show a dismal 2% MFU (utilizing a fraction of the compute capability) but achieve 85% MBU (saturating the HBM3 memory bandwidth). In this context, 85% MBU indicates a highly optimized system; no amount of code optimization can squeeze more tokens per second without faster memory hardware. The choice of primary metric, MFU for training and MBU for inference, reflects the fundamental bottleneck shift between these two regimes.

### Using the Roofline for Diagnosis {#sec-performance-engineering-roofline-diagnosis}

The roofline model from @sec-performance-engineering-roofline becomes a diagnostic tool when combined with profiling data. The process is:

1. **Measure** the achieved FLOPS and memory bandwidth for a kernel using Nsight Compute.
2. **Compute** the operational arithmetic intensity from the algorithm (FLOPs / bytes transferred).
3. **Plot** the measured performance against the roofline ceiling.

A kernel that falls far below both the compute ceiling and the bandwidth ceiling has an implementation problem: launch overhead, poor memory access patterns, or low occupancy. A kernel that reaches the bandwidth ceiling but falls below the compute ceiling is memory-bound, and further optimization requires reducing memory traffic (fusion, precision) rather than improving compute efficiency. A kernel at the compute ceiling is compute-bound and can only be improved by algorithmic changes (reducing FLOPs) or faster hardware.

Consider a concrete example: a LayerNorm kernel profiled on an H100 reports 15 TFLOPS of achieved compute and 2.8 TB/s of achieved memory bandwidth. Its arithmetic intensity is $15 \text{ TFLOPS} / 2.8 \text{ TB/s} \approx 5.4$ FLOP/byte. The H100's ridge point is approximately 295 FLOP/byte at FP16. Since 5.4 is far below 295, the kernel is strictly memory-bound. The fact that it achieves 2.8 TB/s (84% of the H100's 3.35 TB/s peak bandwidth) confirms it is operating near the physical limit of the memory subsystem. The diagnosis is clear: further FLOP-level optimizations will yield zero gain; performance can only be improved by reducing data movement, either through fusion (eliminating the HBM round-trip) or precision reduction (halving the bytes per element).

### PyTorch Profiler Workflow {#sec-performance-engineering-pytorch-profiler}

The PyTorch Profiler integrates with the training loop to capture detailed traces with minimal code modification:

```python
import torch
from torch.profiler import profile, schedule, tensorboard_trace_handler

# Profile 2 warmup steps + 3 active steps
with profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    schedule=schedule(wait=1, warmup=2, active=3, repeat=1),
    on_trace_ready=tensorboard_trace_handler("./profiler_logs"),
    record_shapes=True,
    profile_memory=True,
    with_stack=True,
) as prof:
    for step, batch in enumerate(dataloader):
        if step >= 6:  # 1 wait + 2 warmup + 3 active
            break
        output = model(batch)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        prof.step()
```

The `schedule` parameter is important: it defines a warmup period where the profiler runs but does not record, allowing CUDA caches and JIT compilation to stabilize before measurement begins. Without warmup, the first few iterations include one-time costs (kernel compilation, memory allocation, CUDA context initialization) that inflate the measured times and misrepresent steady-state performance.

The resulting trace, viewable in TensorBoard or Chrome's trace viewer, shows:

- **Kernel timeline**: Which CUDA kernels execute and for how long. Look for gaps (GPU idle time) and unexpectedly long kernels.
- **Memory timeline**: GPU memory allocation and deallocation over time. Spikes indicate inefficient memory management; gradual growth suggests memory leaks.
- **CPU-GPU synchronization**: Points where the CPU waits for the GPU (or vice versa). Excessive synchronization often indicates that the CPU is not launching kernels fast enough to keep the GPU busy.
- **Communication events**: NCCL collective operations and their duration relative to compute kernels. Short compute between long communications indicates poor overlap.

### Nsight Systems: Reading the Timeline {#sec-performance-engineering-nsight-systems}

NVIDIA Nsight Systems is the primary tool for trace-level profiling. Understanding how to read its timeline view is an essential skill for performance engineers. The tool captures every GPU kernel launch, CUDA memory operation, NCCL communication, and CPU thread activity onto a unified timeline.

A typical Nsight Systems workflow begins with capturing a trace of a few training or inference iterations:

```bash
nsys profile --trace=cuda,nvtx,osrt,cudnn,cublas \
    --output=llm_profile \
    --force-overwrite=true \
    python3 inference_server.py --num-steps=5
```

The `--trace` flags control which activities are recorded. The `cuda` flag captures kernel launches and memory operations. The `nvtx` flag captures user-annotated regions (PyTorch automatically annotates module boundaries with NVTX markers). The `cublas` and `cudnn` flags capture library-level operations, which helps identify whether a GEMM is using cuBLAS or a custom kernel.

The resulting `.nsys-rep` file is opened in the Nsight Systems GUI, which presents a multi-row timeline. The most important rows are:

The **CUDA HW** row shows the actual kernel execution on the GPU. Each colored bar represents a kernel, with width proportional to execution time. Gaps between bars indicate GPU idle time, which represents wasted potential. For a well-optimized inference pipeline, the CUDA HW row should show nearly continuous kernel execution with minimal gaps.

The **CUDA API** row shows CPU-side CUDA API calls (kernel launches, memory allocations, synchronization). If CUDA API bars are significantly wider than the corresponding CUDA HW bars, the CPU is the bottleneck: it cannot launch kernels fast enough to keep the GPU busy. This is the kernel launch overhead problem that CUDA Graphs and `torch.compile` address.

The **NCCL** row (for distributed workloads) shows collective communication operations. Comparing the NCCL row with the CUDA HW row reveals whether communication overlaps with computation. If NCCL bars appear during gaps in the CUDA HW row, communication is serialized. If NCCL bars overlap with CUDA HW bars, overlap is working correctly.

The **NVTX** row shows user-annotated regions, which PyTorch maps to module names (`Linear`, `LayerNorm`, `Attention`). This connects low-level kernel names (often cryptic strings like `volta_fp16_s1688gemm_fp16_256x128_ldg8_f2f_nn`) to the model-level operations that produced them.

The key patterns to look for in a Nsight Systems trace are:

1. The ratio of kernel execution to idle time, which measures GPU utilization.
2. The distribution of kernel durations, where many short kernels suggest fusion opportunities.
3. The alignment of NCCL and CUDA HW rows, which reveals overlap effectiveness.
4. Memory allocation spikes, which may indicate inefficient memory management or unnecessary tensor materializations.
5. The proportion of time in GEMM versus non-GEMM kernels, which indicates how much of the execution is doing "useful" matrix arithmetic versus overhead.

Experienced performance engineers develop pattern recognition for these traces, quickly identifying the dominant bottleneck from the visual structure of the timeline. A trace dominated by thin, closely packed kernel bars with minimal gaps indicates a well-optimized pipeline. A trace with large gaps between kernels, or with NCCL bars that do not overlap with CUDA HW bars, immediately reveals the primary optimization target.

### Common Bottleneck Patterns {#sec-performance-engineering-bottleneck-patterns}

Profiling reveals several recurring patterns in ML workloads that map directly to the optimization techniques in this chapter.

**Pattern: Small kernel gaps.** Many short GPU idle periods between small kernels. *Diagnosis*: Kernel launch overhead dominates. *Fix*: Graph compilation (`torch.compile`) to fuse small kernels.

**Pattern: Large intermediate tensors.** Memory timeline shows spikes corresponding to attention score matrices or other intermediate results. *Diagnosis*: Unfused operations materializing intermediates in HBM. *Fix*: Operator fusion (FlashAttention for attention, custom Triton kernels for other sequences).

**Pattern: Low FLOPS utilization with high memory bandwidth.** GPU achieves near-peak bandwidth but single-digit FLOPS utilization. *Diagnosis*: Memory-bound operations dominating execution time. *Fix*: Precision reduction (FP16 to FP8, INT4 KV cache), or algorithmic changes (batching, speculative decoding).

**Pattern: Communication bubbles.** GPU idle periods coinciding with NCCL AllReduce or AllToAll operations. *Diagnosis*: Communication not overlapped with compute. *Fix*: Enable gradient overlap in DDP, restructure pipeline schedule for zero-bubble execution.

**Pattern: Load imbalance.** Some GPUs finish earlier and wait for stragglers. *Diagnosis*: Uneven work distribution. *Fix*: For MoE, adjust auxiliary loss and capacity factors. For tensor parallelism, ensure even dimension splitting.

**Pattern: Memory allocation thrashing.** The memory timeline shows repeated allocations and deallocations of large tensors, with peak memory far exceeding the steady-state usage. *Diagnosis*: The framework is creating new tensors for each forward pass instead of reusing buffers. *Fix*: Enable torch.compile's memory planning, use pre-allocated buffer pools, or apply activation checkpointing to reduce peak memory.

**Pattern: CPU-bound data loading.** GPU utilization drops periodically to zero while the CPU prepares the next batch. *Diagnosis*: Data preprocessing or tokenization is not pipelined with GPU computation. *Fix*: Increase the number of data loading workers, move tokenization to a dedicated preprocessing service, or pre-tokenize the dataset. This pattern is more common in training than in inference, but can occur in inference when input preprocessing (image resizing, text tokenization) is expensive.

::: {.callout-notebook title="The Profiler Detective"}

**Problem**: You run a 7B parameter LLM on a single H100 and observe 45 tokens/second during autoregressive generation at batch size 1. The theoretical bandwidth-limited rate is approximately 96 tokens/second. Where is the remaining 53% of performance hiding?

**Investigation**:

*Step 1: Kernel-level analysis.* Run Nsight Compute on the dominant GEMM kernel. Result: achieves 2.8 TB/s effective bandwidth out of the H100's 3.35 TB/s peak. Efficiency: 84%. This kernel is performing well.

*Step 2: Trace-level analysis.* Run Nsight Systems on a full decode step. Result: 42% of step time is spent in GEMM kernels. The remaining 58% is split between:

- Attention kernels (including KV cache reads): 28%
- Layer normalization and activation kernels: 12%
- Softmax and top-k sampling: 8%
- Kernel launch gaps: 10%

*Step 3: Identify optimization targets.*

- The KV cache attention kernel achieves only 1.9 TB/s bandwidth because of irregular memory access patterns. *Fix*: Implement KV cache quantization (INT8) with better memory layout.
- Kernel launch gaps (10% of time) come from 120+ individual kernel launches per layer. *Fix*: Apply `torch.compile` to fuse element-wise operations, reducing to ~30 kernels per layer.
- Layer normalization and activation kernels are unfused. *Fix*: Fused LayerNorm-GELU kernel via Triton.

**Takeaway**: The dominant GEMM is near-optimal, but secondary operations and launch overhead consume over half the execution time. Systematic profiling reveals that the bottleneck is not where intuition suggests (the largest kernel) but in the accumulated overhead of many small operations.

:::

### The Profiling Feedback Loop {#sec-performance-engineering-profiling-loop}

Effective performance engineering follows an iterative cycle: **profile, diagnose, optimize, verify**. The verification step is critical and often skipped. After applying an optimization, reprofile to confirm that the targeted bottleneck was addressed and that no new bottleneck emerged. Performance optimization is a waterbed problem: fixing one bottleneck often exposes the next.

A common trap is optimizing based on microbenchmarks rather than end-to-end traces. A kernel that appears 2 $\times$ faster in isolation may deliver only 5% improvement in end-to-end throughput if it was not the bottleneck, or if the surrounding code cannot take advantage of its speedup due to data dependencies. Always measure impact at the application level (tokens/second, step time, P99 latency) in addition to kernel-level metrics.

Another subtlety is that profiling itself can perturb the system being measured. The PyTorch profiler adds approximately 10--20% overhead when recording full traces with memory profiling enabled. Nsight Systems adds less overhead but still affects scheduling. Profile warmup steps before active measurement, and discount the first few profiled iterations where JIT compilation or CUDA context initialization may dominate.

### Profiling at Scale {#sec-performance-engineering-profiling-scale}

Profiling a single GPU is straightforward; profiling a distributed system with hundreds or thousands of GPUs introduces unique challenges. The volume of trace data grows linearly with the number of GPUs: a 5-second Nsight Systems trace for one GPU is approximately 500 MB; the same trace for 1,000 GPUs would be 500 GB, impractical to store or analyze.

Production systems address this through **hierarchical profiling**. At the top level, application-level metrics (MFU, throughput, step time) are collected continuously from every GPU with negligible overhead. These aggregate metrics detect when performance degrades. When a degradation is detected, **targeted profiling** is triggered on a representative subset of GPUs (typically one GPU per pipeline stage, per data-parallel group) for a short window (a few training steps). The resulting traces are analyzed to identify the specific bottleneck.

Another approach is **statistical profiling**, where each GPU randomly samples a small fraction of its kernels for detailed timing. Over many training steps, the aggregated samples provide a statistically accurate picture of the kernel time distribution without the overhead of full tracing. This is analogous to the sampling profilers (like Linux `perf`) used in traditional systems engineering, adapted for the GPU context.

The most challenging profiling scenario is intermittent stragglers: GPUs that are occasionally slow due to thermal throttling, memory errors, or network congestion, but fast most of the time. These stragglers may not appear in a short profiling window but can reduce training throughput by 10--20% over hours. Detecting them requires continuous per-GPU step-time monitoring with statistical anomaly detection, a form of profiling infrastructure that operates at the monitoring layer rather than the kernel layer.

The profiling tools and techniques described in this section provide the measurement foundation for all optimization work. Without measurement, performance engineering degenerates into guesswork. With measurement, it becomes a systematic discipline guided by quantitative evidence.

## The Optimization Playbook: A 70B LLM Case Study {#sec-performance-engineering-playbook}

You are handed a raw, unoptimized 70-billion parameter PyTorch model and told it must serve 1,000 tokens per second in production by next week. Where do you start? You cannot simply throw every technique at the wall. The optimization playbook requires a systematic, prioritized attack: first unblocking the memory wall, then fusing operators, and finally applying algorithmic techniques like speculative decoding in a specific, compounding sequence.

### The Diagnostic Sequence {#sec-performance-engineering-diagnostic-sequence}

When approaching a new ML workload for optimization, follow this sequence:

::: {.callout-definition title="Model FLOPs Utilization"}

***Model FLOPs Utilization (MFU)***\index{Model FLOPs Utilization!definition} is the ratio of observed throughput (measured in FLOPs/second based on the model's theoretical FLOP count per sample) to the hardware's peak FLOP/second capacity. Unlike raw hardware utilization, which counts all arithmetic operations including rematerialization, padding, and communication overhead, MFU counts only the FLOPs that directly contribute to the model's forward and backward passes. This distinction is critical: a system can show 90% hardware utilization while wasting half its compute on algorithmic overhead, a failure mode that only MFU exposes. MFU therefore serves as the single metric that captures both hardware efficiency and software overhead.

$$
\text{MFU} = \frac{\text{Model FLOPs per sample} \times \text{Samples per second}}{\text{Peak Hardware FLOPS}}
$$

:::

**Step 1: Baseline measurement.** Measure end-to-end throughput (tokens/second for LLMs, samples/second for training) and collect a Nsight Systems trace. Compute the Model FLOPs Utilization (MFU) as defined above. An MFU below 30% indicates substantial optimization opportunity; above 50% is good; above 60% is excellent for large models.

**Step 2: Roofline classification.** For the dominant kernels, compute the arithmetic intensity and plot on the roofline. This immediately identifies whether the workload is compute-bound or memory-bound, which determines the entire optimization strategy.

**Step 3: Select the primary bottleneck.** The primary bottleneck falls into one of three categories, each with a different optimization path:

For **memory-bound workloads** (the common case for inference):

1. Apply precision reduction (FP16 $\rightarrow$ FP8 or INT4) to increase effective bandwidth.
2. Apply operator fusion (FlashAttention, fused LayerNorm/GELU) to eliminate intermediate HBM traffic.
3. Apply graph compilation (torch.compile) to catch remaining fusion opportunities.
4. Consider algorithmic changes (speculative decoding, MoE) for further improvement.

For **compute-bound workloads** (large-batch training):

1. Ensure Tensor Cores are utilized (FP16/BF16/FP8, not FP32).
2. Apply graph compilation for kernel selection and memory planning.
3. Consider reduced precision (FP8) for 2 $\times$ compute throughput.
4. Optimize communication overlap to avoid compute idle time.

For **communication-bound workloads** (distributed training at scale):

1. Enable gradient communication overlap with backward pass.
2. Apply gradient compression or reduced precision communication.
3. Restructure pipeline schedules for zero-bubble execution.
4. Consider topology-aware placement to minimize cross-node traffic.

**Step 4: Apply and verify.** Implement the highest-impact optimization, reprofile, and verify improvement. Then iterate from Step 2 with the new profile, as the bottleneck may have shifted.

### Combining Techniques {#sec-performance-engineering-combining}

The most performant production systems combine multiple techniques simultaneously. A state-of-the-art LLM serving system might employ all of the following:

- **FlashAttention-2** or **FlashAttention-3** for memory-efficient attention computation
- **INT4 weight quantization** (GPTQ or AWQ) with FP16 dequantization during GEMM
- **INT8 KV cache compression** with per-channel scaling
- **torch.compile** or **TensorRT** for element-wise fusion and kernel selection
- **Speculative decoding** for latency reduction at low batch sizes
- **Continuous batching** with dynamic sequence grouping for high throughput
- **Tensor parallelism** across GPUs with overlapped AllReduce

These techniques are not additive in their speedup; they interact in complex ways. For instance, INT4 weight quantization reduces per-token HBM traffic by 4 $\times$, which might shift the bottleneck from memory-bound to compute-bound. Once compute-bound, further bandwidth optimizations (KV cache compression) yield diminishing returns, and compute optimizations (FP8 Tensor Cores) become the priority. This is why the iterative profile-optimize-verify loop is essential: the optimal combination depends on the specific model, hardware, and workload characteristics.

The interaction between optimizations creates a dependency graph that the performance engineer must navigate. Some combinations are synergistic: FlashAttention reduces attention memory traffic, and INT8 KV cache compression reduces KV cache memory, together freeing enough memory for larger batch sizes that transform the economics of serving. Other combinations are redundant: applying both CUDA Graphs and the `reduce-overhead` mode of `torch.compile` achieves the same result, since `reduce-overhead` internally uses CUDA Graphs. Still other combinations conflict: speculative decoding benefits most at small batch sizes (where decode is memory-bound), while many throughput optimizations work by increasing batch size. At large batch sizes, speculation adds overhead without proportional benefit.

A practical heuristic for sequencing optimizations is to apply them in order of decreasing "leverage" and increasing "effort":

1. **torch.compile** (1 line of code, 10--40% speedup): Always apply first. No downsides.
2. **Weight quantization** (INT4/FP8, hours of calibration, 2--4 $\times$ throughput): Apply second. Frees memory for batching.
3. **FlashAttention** (library swap, 2--32 $\times$ for attention): Apply third. Usually a configuration flag.
4. **KV cache compression** (INT8, library support, 2 $\times$ cache reduction): Apply fourth. Enables larger batches.
5. **Speculative decoding** (requires draft model, engineering effort): Apply last, only if latency target not met. Most complex to deploy.

This ordering reflects the principle that passive optimizations (compiler, library swaps) should precede active ones (algorithmic changes, new model components). Each step is validated by reprofiling before proceeding to the next.

::: {.callout-checkpoint title="Optimization Strategy" collapse="false"}

Test your ability to design an optimization plan:

- [ ] Given an LLM serving workload at batch size 1 that achieves 25% of peak bandwidth, can you identify the three most impactful optimizations and their expected interaction?
- [ ] Can you explain why applying FP8 quantization to a workload that is already communication-bound provides no speedup?
- [ ] Can you describe the iterative profiling workflow and explain why verifying each optimization is as important as applying it?
- [ ] Can you identify a scenario where applying `torch.compile` would *degrade* performance (hint: consider compilation overhead and graph breaks)?

:::

### Case Study: Optimizing a 70B LLM Serving Pipeline {#sec-performance-engineering-case-study}

To illustrate how the diagnostic sequence and combining principles work in practice, consider the task of optimizing a 70B parameter LLM for production serving. The target is a real-time chatbot application requiring time-to-first-token (TTFT) under 500 ms, inter-token latency (ITL) under 50 ms, and throughput of at least 1,000 tokens/second across the cluster. The model is deployed on a node of 8 H100 GPUs connected by NVLink.

#### Baseline Measurement {#sec-performance-engineering-case-baseline}

The initial deployment uses FP16 weights, standard PyTorch eager execution, and tensor parallelism across 8 GPUs. The 70B model in FP16 requires 140 GB of weight storage, distributed as approximately 17.5 GB per GPU. The baseline performance metrics are:

- **TTFT**: 1,200 ms (well above the 500 ms target)
- **ITL**: 85 ms (above the 50 ms target)
- **Throughput**: 280 tokens/second (below the 1,000 token/second target)
- **Maximum batch size**: 4 (limited by KV cache memory)

A Nsight Systems trace reveals the following time breakdown for a single decode step at batch size 1:

- GEMM kernels: 38% of step time
- Attention (including KV cache reads): 24% of step time
- Element-wise operations (LayerNorm, GELU, residual): 14% of step time
- AllReduce communication (tensor parallelism): 12% of step time
- Kernel launch gaps and overhead: 12% of step time

The roofline analysis confirms that decode is deeply memory-bound, with arithmetic intensity approximately 1 FLOP/byte at batch size 1. The GPU achieves 2.7 TB/s effective bandwidth (80% of peak), indicating reasonable kernel-level efficiency but a fundamental algorithmic limitation.

#### Optimization Round 1: Precision Engineering {#sec-performance-engineering-case-round1}

The first optimization targets the largest opportunity: reducing the bytes per weight read from HBM. Applying AWQ INT4 weight quantization reduces the weight storage from 140 GB to 35 GB (8.75 GB per GPU from 17.5 GB per GPU). The effective bandwidth for weight reads doubles because the same physical bandwidth now delivers twice as many weight values per second (each value is 4 bits instead of 16 bits, with on-the-fly dequantization to FP16 for the GEMM).

Simultaneously, applying INT8 quantization to the KV cache reduces per-request cache size by 2 $\times$. The combined effect on memory budget is transformative: each GPU now has approximately 71 GB available for KV cache, up from 62 GB. At the reduced per-request KV cache size, the maximum batch size increases from 4 to approximately 32.

Post-optimization metrics at batch size 1:

- **ITL**: 48 ms (meets the 50 ms target)
- **Throughput at batch size 32**: 720 tokens/second (still below target)

The Nsight Systems trace shows that GEMM time decreased by approximately 45% due to reduced weight reads, but attention and element-wise operations remain unchanged. The bottleneck has partially shifted.

#### Optimization Round 2: Operator Fusion {#sec-performance-engineering-case-round2}

The second round targets the 14% of step time consumed by element-wise operations and the 24% consumed by attention. Applying `torch.compile` with the `max-autotune` backend fuses element-wise operations (GELU, LayerNorm, residual additions), reducing their contribution from 14% to approximately 4% of step time. Simultaneously, enabling FlashAttention-2 replaces the standard attention implementation, reducing attention HBM traffic by approximately 16 $\times$ for the prefill phase.

For the decode phase, FlashAttention's impact is more modest because decode attention is dominated by KV cache reads rather than the $N \times N$ score matrix. However, the combination of INT8 KV cache compression and FlashAttention's efficient paged attention kernel reduces attention decode time by approximately 30%.

The `reduce-overhead` mode in `torch.compile` wraps the decode step in a CUDA Graph, eliminating the 12% kernel launch overhead almost entirely.

Post-optimization metrics:

- **TTFT**: 380 ms (meets the 500 ms target)
- **ITL**: 32 ms at batch size 1 (well below the 50 ms target)
- **Throughput at batch size 32**: 1,050 tokens/second (meets the target)

#### Optimization Round 3: Speculative Decoding {#sec-performance-engineering-case-round3}

With the throughput target met, the team focuses on further reducing ITL for the best user experience. Speculative decoding with a 1.5B draft model (AWQ INT4 quantized to 0.75 GB) is deployed on the same GPUs. The draft model generates 5 candidate tokens in 4 ms (benefiting from the INT4 quantization applied in Round 1). Verification takes approximately 8 ms.

At an average acceptance rate of 0.78, the expected tokens per round is approximately 3.5. The effective ITL becomes:

$$
\text{ITL}_{\text{effective}} = \frac{4 + 8}{3.5} \approx 3.4 \text{ ms per token}
$$

This is a 9.4 $\times$ improvement over the Round 2 ITL of 32 ms, providing a remarkably responsive user experience.

However, speculative decoding interacts with batching. At batch size 32, the verification step is no longer "free" because the GPU is closer to compute saturation. The system therefore applies speculation only when the current batch size is below 16, falling back to standard autoregressive decoding at higher loads. This adaptive policy maintains both the latency benefit at low load and the throughput benefit at high load.

#### Lessons from the Case Study {#sec-performance-engineering-case-lessons}

This optimization journey illustrates several principles:

The order of optimizations matters. Precision engineering (Round 1) was applied first because it yields the largest single improvement and enables subsequent optimizations by freeing memory for larger batch sizes. Fusion (Round 2) addressed the new bottleneck exposed by precision engineering. Speculative decoding (Round 3) provided latency improvement once the throughput target was met.

Each optimization changed the bottleneck. Before Round 1, the system was purely memory-bandwidth-bound. After INT4 quantization and batching, the system was partially compute-bound at large batch sizes. After fusion, kernel launch overhead was negligible, making the remaining bottleneck the fundamental memory-bandwidth limit for decode. Each optimization was validated by reprofiling to confirm the bottleneck shift.

The final system combines five distinct techniques: INT4 weight quantization, INT8 KV cache compression, FlashAttention-2, torch.compile with CUDA Graphs, and adaptive speculative decoding. These techniques are not independent; they interact. INT4 quantization enables larger batch sizes, which changes whether speculative decoding is profitable. FlashAttention's benefit depends on sequence length, which grows during generation. The performance engineer must reason about these interactions holistically, guided by profiling data at each stage.

This case study demonstrates how these disparate optimizations compound sequentially to transform an unusable prototype into a production-grade deployment. However, the path to these massive speedups is fraught with conventional wisdom that often proves disastrous at scale, leading us to examine the common fallacies and pitfalls of performance engineering.

## Fallacies and Pitfalls {#sec-performance-engineering-fallacies}

A team upgrades their inference cluster from A100s to H100s, expecting a massive 3x latency reduction based on the new spec sheet's teraFLOPS rating, only to find their generative model barely runs 15% faster. This leads us to one of the most pervasive traps in performance engineering: assuming that raw compute capacity dictates inference speed when the workload is entirely bound by memory bandwidth.

**Fallacy:** *More FLOPS means faster inference.*

The roofline model demonstrates that most inference operations are memory-bound, not compute-bound. A GPU with 2 $\times$ the peak FLOPS but the same memory bandwidth will not generate tokens any faster for batch-1 LLM decode. The correct metric for memory-bound workloads is bandwidth, not FLOPS. This fallacy leads organizations to purchase the most expensive compute hardware when a mid-range GPU with equivalent HBM bandwidth would deliver identical inference throughput.

**Fallacy:** *FP8 always halves training time compared to FP16.*

FP8 doubles the peak TFLOPS and doubles the effective memory bandwidth, but these gains are realized only for operations that are bottlenecked by compute or bandwidth at FP16. Element-wise operations like activation functions are already limited by kernel launch overhead, not by precision. Communication-bound distributed training steps gain nothing from reduced arithmetic precision if the communication volume (gradient sizes) is not also reduced. The actual speedup depends on the fraction of execution time spent in precision-sensitive operations.

**Pitfall:** *Optimizing the largest kernel while ignoring the long tail.*

The profiling case study above illustrates this pitfall. Engineers naturally focus on the single largest kernel, which is often the GEMM in a transformer layer. But when the GEMM is already near-optimal, the remaining performance budget is distributed across dozens of smaller operations: normalization, activation, attention scoring, KV cache management, and kernel launch overhead. Collectively, these "small" operations can consume more than half of total execution time. Graph compilation and systematic fusion address this long tail more effectively than further GEMM optimization.

**Pitfall:** *Applying speculative decoding without considering batch dynamics.*

Speculative decoding excels at batch size 1, where decode is deeply memory-bound and the verification step is essentially "free" (the GPU has ample spare compute). At large batch sizes, decode approaches the compute-bound regime, and the verification step adds meaningful compute cost. Furthermore, the variable number of accepted tokens per request complicates continuous batching schedulers. In high-throughput serving scenarios with large batches, the overhead of speculation may outweigh its latency benefits.

**Pitfall:** *Treating MoE expert count as a free scaling knob.*

Increasing the number of experts in an MoE model increases total parameters (capacity) without proportionally increasing per-token compute, which seems like a free lunch. But each additional expert increases: (1) total memory requirements, requiring more GPUs; (2) AllToAll communication volume for expert routing; (3) load balancing difficulty, since the router must distribute tokens across more experts; and (4) training instability, as more experts compete for activation. Beyond approximately 64--256 experts, the system-level costs often outweigh the capacity benefits.

**Fallacy:** *Graph compilers eliminate the need for manual kernel engineering.*

Graph compilers have improved dramatically, but they remain limited by their cost models and fusion heuristics. FlashAttention required human insight to recognize that attention could be reformulated as a tiled algorithm with online softmax, an algorithmic insight beyond the scope of any current compiler's rewrite rules. Similarly, speculative decoding and MoE routing require algorithmic innovation that compilers cannot discover. Compilers automate known optimizations; human engineers discover new ones.

**Pitfall:** *Quantizing everything to the lowest supported precision.*

Aggressive quantization (INT4 weights, INT4 KV cache, FP8 activations) can degrade model quality in ways that are difficult to detect with standard benchmarks but visible to users. Perplexity on a held-out dataset may change by less than 1%, but the model may produce subtly worse responses for edge cases, rare languages, or complex reasoning tasks. The correct approach is targeted quantization: apply the most aggressive precision to the least sensitive components (KV cache, intermediate activations) and preserve higher precision for the most sensitive (first and last layers, attention logits). Calibration on a representative dataset, followed by evaluation on diverse quality benchmarks, is essential before deploying any quantized model to production.

**Pitfall:** *Measuring throughput without measuring quality.*

A model serving system that generates 200 tokens/second is not twice as good as one generating 100 tokens/second if the first system achieves that throughput by using INT4 quantization that degrades answer quality by 15%. Performance metrics must always be reported alongside quality metrics. The correct optimization target is the Pareto frontier of throughput versus quality, not throughput alone.

**Fallacy:** *A single profiling run is sufficient to characterize performance.*

ML system performance is non-stationary. GPU thermal throttling reduces clock speeds (and therefore FLOPS) after sustained workloads, sometimes by 10--15%. Memory fragmentation accumulates over hours of serving, gradually reducing effective batch size. Network congestion varies with cluster-wide traffic patterns. A profiling run during a cold start may show different bottleneck patterns than one after hours of production serving. Reliable performance characterization requires profiling under realistic, sustained conditions, ideally sampling multiple times across a production run.

**Pitfall:** *Optimizing for average case while ignoring tail latency.*

A serving system may achieve excellent average inter-token latency (30 ms) while exhibiting P99 latency of 500 ms due to garbage collection pauses in the Python runtime, CUDA memory allocation stalls, or occasional AllReduce delays from network congestion. For interactive applications, the user experience is dominated by the worst case, not the average. Performance engineering for production systems must profile and optimize tail latency specifically, often through techniques orthogonal to the throughput optimizations in this chapter: pre-allocated memory pools, CUDA graph replay (which eliminates allocation variance), and priority scheduling for latency-sensitive requests.

Recognizing these pitfalls—from ignoring tail latency to misjudging the impact of raw FLOPS—saves teams from wasting months optimizing the wrong layer of the stack. We conclude this chapter by summarizing the core principles that guide a successful performance engineering lifecycle.

## Summary {#sec-performance-engineering-summary}

Performance engineering transforms a model that should be efficient into one that is. The techniques in this chapter address the fundamental bottleneck of modern ML systems: the memory wall. @fig-optimization-hierarchy summarizes how these techniques layer from hardware-level primitives to algorithmic innovations.

::: {.callout-note title="Figure: The Optimization Hierarchy" collapse="false"}

```{.tikz}
%| fig-cap: "**The Performance Engineering Hierarchy**. Optimization techniques organized by their level of abstraction, from hardware-level precision engineering at the base to algorithmic innovations at the top. Each layer builds on and benefits from the layers below it. The annotations show the primary mechanism and typical speedup range for each technique."
%| fig-alt: "Layered hierarchy diagram showing five optimization levels from bottom to top: Precision Engineering, Operator Fusion, Graph Compilation, Communication Overlap, and Algorithmic Innovation, with arrows showing how they interact."
%| label: fig-optimization-hierarchy

\begin{tikzpicture}[
  layer/.style={draw, thick, rounded corners=3pt, minimum width=11cm, minimum height=1.2cm, font=\small},
  label/.style={font=\scriptsize, text width=4cm, align=left},
  >=stealth
]
  % Layers from bottom to top
  \node[layer, fill=blue!15] (hw) at (0,0) {\textbf{Precision Engineering} (FP8, INT4, KV Cache Compression)};
  \node[layer, fill=green!15] (fuse) at (0,1.6) {\textbf{Operator Fusion \& Tiling} (FlashAttention, Fused Kernels)};
  \node[layer, fill=orange!15] (comp) at (0,3.2) {\textbf{Graph Compilation} (torch.compile, XLA, TensorRT)};
  \node[layer, fill=purple!15] (comm) at (0,4.8) {\textbf{Communication Overlap} (Gradient Pipelining, Zero-Bubble)};
  \node[layer, fill=red!15] (algo) at (0,6.4) {\textbf{Algorithmic Innovation} (Speculative Decoding, MoE)};

  % Right-side annotations
  \node[label, right] at (6.2,0) {Mechanism: Reduce bytes/value\\Speedup: 2--4$\times$};
  \node[label, right] at (6.2,1.6) {Mechanism: Reduce HBM trips\\Speedup: 2--32$\times$};
  \node[label, right] at (6.2,3.2) {Mechanism: Automate fusion\\Speedup: 1.1--2$\times$};
  \node[label, right] at (6.2,4.8) {Mechanism: Hide latency\\Speedup: 1.1--1.5$\times$};
  \node[label, right] at (6.2,6.4) {Mechanism: Change algorithm\\Speedup: 1.5--10$\times$};

  % Arrows between layers
  \draw[->, thick, gray] (hw.north) -- (fuse.south);
  \draw[->, thick, gray] (fuse.north) -- (comp.south);
  \draw[->, thick, gray] (comp.north) -- (comm.south);
  \draw[->, thick, gray] (comm.north) -- (algo.south);

  % Left-side label
  \node[rotate=90, font=\small, anchor=south] at (-6.5,3.2) {Increasing Abstraction $\longrightarrow$};
\end{tikzpicture}
```

:::

The **Roofline Model** provides the diagnostic framework, classifying operations as compute-bound or memory-bound based on their arithmetic intensity relative to the hardware's ridge point. For the NVIDIA H100, this ridge point is approximately `{python} h100_fp16_ridge_str` FLOP/byte at FP16, meaning most transformer operations fall in the memory-bound regime.

**Operator fusion** eliminates redundant HBM round-trips by combining sequences of operations into single kernels. FlashAttention is the canonical example, reducing attention HBM traffic by `{python} savings_str` $\times$ for long sequences through tiling and online softmax. Ring Attention extends this principle across GPUs for extreme sequence lengths.

**Precision engineering** reduces bytes per transaction. FP8 formats (E4M3 for weights, E5M2 for gradients) double effective bandwidth on H100 hardware. Block-wise quantization (LLM.int8(), GPTQ, AWQ) handles the outlier features that defeat uniform quantization. KV cache compression to INT4 or INT8 directly increases serving batch size.

**Graph compilation** automates fusion, kernel selection, and memory planning. torch.compile/TorchInductor generates optimized Triton kernels from standard PyTorch code. XLA provides whole-program optimization for JAX/TPU workloads. TensorRT delivers aggressive inference-specific optimization.

**Speculative decoding** trades compute for latency by using a fast draft model to generate candidate tokens verified in parallel by the target model, with mathematical guarantees on output quality. Speedups of 1.5--3 $\times$ are typical for diverse workloads.

**Mixture of Experts** decouples model capacity from per-token inference cost through sparse activation. DeepSeek-V3 demonstrates the frontier: 671B total parameters with only 37B active per token.

**Communication-computation overlap** hides network latency by executing communication and computation concurrently on separate hardware engines. Zero-bubble pipeline schedules approach the theoretical minimum idle time.

**System profiling** provides the measurement infrastructure to identify which bottleneck dominates and which optimization to apply, closing the loop between diagnosis and treatment.

Together, these techniques compose a coherent optimization stack. Precision engineering and operator fusion attack the **Iron Law** at the hardware level, reducing the memory and compute terms. Graph compilation automates these hardware-level optimizations across the full model. Communication overlap attacks the distributed overhead term. Speculative decoding and MoE change the algorithm itself, fundamentally restructuring the terms of the equation.

The case study in @sec-performance-engineering-case-study demonstrated how these techniques compound in practice: INT4 quantization freed memory for larger batches, which changed the arithmetic intensity, which determined whether further bandwidth or compute optimizations were profitable. Each optimization shifted the bottleneck, requiring reprofiling and a new optimization decision. This iterative, measurement-driven process is the discipline of performance engineering.

The unifying principle is that performance engineering is not about making hardware faster; it is about making software match the physics of the hardware it runs on. The memory wall is a physical constraint that grows wider with each hardware generation. The techniques in this chapter are the engineer's response: not fighting the physics but working within it, keeping data close to compute, reducing precision to the minimum that preserves quality, and restructuring algorithms to avoid unnecessary work. As models grow larger and hardware grows faster but not more bandwidth-rich, these skills become not just valuable but essential.

::: {.callout-takeaways title="Match the Software to the Silicon"}

* The **Memory Wall** is the defining constraint of modern ML performance: most transformer operations are memory-bound, with arithmetic intensity far below the hardware ridge point. Performance engineering is primarily about reducing bytes moved, not operations computed.
* **Operator Fusion** and **Tiling** (FlashAttention) eliminate redundant HBM traffic by keeping intermediate results in on-chip SRAM, reducing attention memory traffic from quadratic to linear in sequence length.
* **Precision Engineering** doubles effective bandwidth by reducing numerical representation (FP8, INT4), but requires careful handling of outlier features (LLM.int8(), GPTQ, AWQ) and dynamic scaling for training stability.
* **Graph Compilation** (torch.compile, XLA, TensorRT) automates known optimizations across the entire model graph, but cannot discover algorithmic innovations like FlashAttention or speculative decoding.
* **Speculative Decoding** is the only technique that reduces *latency* for memory-bound decode without changing precision or batch size, by trading abundant compute for scarce bandwidth through parallel verification of draft tokens.
* **Mixture of Experts** enables parameter scaling without proportional compute scaling, but introduces AllToAll communication patterns and load balancing challenges that require careful system engineering.
* **Always profile before optimizing**: the dominant bottleneck is often not where intuition suggests, and applying the wrong optimization wastes engineering effort while leaving the actual bottleneck untouched.

:::

The central lesson of performance engineering is that it is an iterative, measurement-driven discipline, not a one-shot transformation. Every optimization shifts the bottleneck: fusing attention kernels may move the constraint from HBM bandwidth to compute throughput, and reducing precision may free enough memory to increase batch size, which in turn changes the arithmetic intensity and demands a different optimization entirely. The practitioner who masters this profile-optimize-reprofile loop can extract 2 to 10 times more throughput from the same hardware than one who applies optimizations blindly. Profiling is not merely the first step; it is every step.

This iterative mindset also determines which skills endure as hardware evolves. Individual techniques will change as new accelerator generations shift the ridge point of the Roofline Model and as new architectures alter the dominant computational patterns. What will not change is the fundamental discipline: measure the system, identify the binding constraint, apply the optimization that addresses that specific constraint, and then measure again. Engineers who internalize this cycle treat performance engineering as a continuous practice rather than a checklist, and that practice is what separates systems that merely run from systems that run efficiently at scale.

::: {.callout-chapter-connection title="From Optimization to Serving"}

In this chapter, we established a rigorous toolkit for performance engineering. Through operator fusion, precision reduction, graph compilation, and architectural optimizations like speculative decoding and Mixture-of-Experts, we now understand how to extract maximum computational efficiency from a single model executing on bare metal.

Yet, optimizing an isolated forward pass is merely the precursor to deployment. In production environments, models do not operate in zero-contention vacuums. They must process unpredictable bursts of concurrent requests, multiplexing across fleets of distributed accelerators while strictly adhering to unforgiving latency and throughput budgets.

In **Inference at Scale** (@sec-inference-scale), we transition from the mechanics of local execution to the architecture of robust serving systems. The low-level optimizations developed here function as the foundational primitives for high-availability infrastructure, tackling systemic challenges like continuous batching, distributed KV-cache management, and dynamic request routing.

:::
