{"entries":[{"caption":"Edge Device Vulnerabilities","key":"sec-robust-ai-edge-device-vulnerabilities-ddfe","order":{"number":5,"section":[1,2,2,0,0,0,0]}},{"caption":"Parity Bit Error Detection: This figure provides a simple error detection scheme where an extra bit (the parity bit) ensures the total number of 1s in a data sequence is either even or odd. The second sequence includes a flipped bit, triggering the parity check and indicating a data corruption event during transmission or storage. Source: computer hope.","key":"fig-parity","order":{"number":13,"section":[1,4,5,1,1,1,0]}},{"caption":"Silent Data Corruption: Unexpected Faults Can Return Incorrect File Sizes, Leading to Data Loss During Decompression and Propagating Errors Through Distributed Querying Systems Despite Apparent Operational Success. This Example From Facebook Emphasizes the Challenge of Undetected Errors, silent Data Corruption, and the Importance of Robust Error Detection Mechanisms in Large-Scale Data Processing Pipelines. Source: Facebook.","key":"fig-sdc-example","order":{"number":1,"section":[1,2,1,0,0,0,0]}},{"caption":"Fault Tolerance Overhead Analysis: Quantitative impact of different protection mechanisms on system performance, energy consumption, and hardware area requirements. These overheads must be balanced against fault rates and recovery costs to optimize system reliability per unit resource.","key":"tbl-fault-tolerance-overhead","order":{"number":1,"section":[1,4,2,2,0,0,0]}},{"caption":"Watchdog timers","key":"sec-robust-ai-watchdog-timers-9e44","order":{"number":38,"section":[1,4,5,1,1,4,0]}},{"caption":"Software Injection Limitations","key":"sec-robust-ai-software-injection-limitations-eee7","order":{"number":104,"section":[1,10,3,2,0,0,0]}},{"caption":"Hardware Behavior Modeling","key":"sec-robust-ai-hardware-behavior-modeling-9bd8","order":{"number":109,"section":[1,10,4,2,0,0,0]}},{"caption":"Data Poisoning Attack Methods","key":"sec-robust-ai-data-poisoning-attack-methods-d168","order":{"number":69,"section":[1,8,2,2,0,0,0]}},{"caption":"Data Poisoning","key":"sec-robust-ai-data-poisoning-4b55","order":{"number":67,"section":[1,8,2,0,0,0,0]}},{"caption":"Knowledge Transfer: Pre-training on large datasets enables models to learn generalizable features, which can then be fine-tuned for specific target tasks with limited labeled data. This approach mitigates data scarcity and accelerates learning in new domains by leveraging previously acquired knowledge. Source: bhavsar","key":"fig-transfer-learning","order":{"number":34,"section":[1,8,4,3,3,0,0]}},{"caption":"Transient Faults","key":"sec-robust-ai-transient-faults-1455","order":{"number":15,"section":[1,4,2,0,0,0,0]}},{"caption":"Transient Fault Mechanism: Cosmic rays and electromagnetic interference induce bit flips within hardware by altering electrical charges in memory cells and transistors, potentially corrupting data and causing system errors. Understanding these fault sources is critical for building robust ai systems that can tolerate unpredictable hardware behavior. Source: NTT.","key":"fig-transient-fault","order":{"number":7,"section":[1,4,2,4,0,0,0]}},{"caption":"Physical-world Attacks","key":"sec-robust-ai-physicalworld-attacks-97a0","order":{"number":64,"section":[1,8,1,2,4,0,0]}},{"caption":"Gradient Norm Deviation: Transient hardware faults, such as single data corruption (SDC), disrupt optimization by causing abrupt changes in gradient norms during model training, potentially leading to convergence issues or inaccurate models. Real-world data from Google’s production fleet confirms that SDC anomalies manifest as visible spikes in gradient norm over time, indicating a disruption to the expected parameter update process. Source: jeff dean, mlsys 2024 keynote (Google).","key":"fig-sdc-training-fault","order":{"number":8,"section":[1,4,2,6,0,0,0]}},{"caption":"Hardware Fault Impact on ML Systems","key":"sec-robust-ai-hardware-fault-impact-ml-systems-b5f7","order":{"number":14,"section":[1,4,1,0,0,0,0]}},{"caption":"Software Fault Effects on ML","key":"sec-robust-ai-software-fault-effects-ml-1ba2","order":{"number":95,"section":[1,9,3,0,0,0,0]}},{"caption":"Robust AI","key":"sec-robust-ai","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Understanding the Vulnerability","key":"sec-robust-ai-understanding-vulnerability-de4c","order":{"number":59,"section":[1,8,1,1,0,0,0]}},{"caption":"Detection and Mitigation Strategies","key":"sec-robust-ai-detection-mitigation-strategies-8dbe","order":{"number":50,"section":[1,5,3,0,0,0,0]}},{"caption":"Transient Fault Origins","key":"sec-robust-ai-transient-fault-origins-2226","order":{"number":19,"section":[1,4,2,4,0,0,0]}},{"caption":"Hot Spare Redundancy: Google’s data centers utilize hot spare cores to maintain uninterrupted ML training despite hardware failures, seamlessly transitioning workloads from defective machines to backup resources. This approach contrasts with parallel redundancy techniques like DMR/TMR by providing a reactive fault tolerance mechanism that minimizes downtime and preserves data integrity during ML training. Source: jeff dean, mlsys 2024 keynote (Google).","key":"fig-sdc-controller","order":{"number":15,"section":[1,4,5,1,1,3,0]}},{"caption":"Distribution Shift Mechanisms","key":"sec-robust-ai-distribution-shift-mechanisms-f5e3","order":{"number":74,"section":[1,8,3,2,0,0,0]}},{"caption":"Dual Modular Redundancy: Tesla’s full self-driving computer employs a DMR architecture, replicating critical computations across two independent system-on-chips (socs) to mitigate hardware faults and ensure continuous operation. This redundancy enables the system to mask errors: if one soc fails, the other continues functioning, maintaining safety-critical functions like perception and control. Source: Tesla","key":"fig-tesla-dmr","order":{"number":14,"section":[1,4,5,1,1,3,0]}},{"caption":"Adversarial Attacks","key":"sec-robust-ai-adversarial-attacks-bb75","order":{"number":46,"section":[1,5,1,0,0,0,0]}},{"caption":"Conceptual Foundation","key":"sec-robust-ai-conceptual-foundation-20b5","order":{"number":47,"section":[1,5,1,1,0,0,0]}},{"caption":"Anomaly Detection With SVM: Support vector machines identify deviations from normal system behavior by mapping log data into a high-dimensional space and defining boundaries around expected values, enabling the detection of potential faults. Unsupervised anomaly detection techniques, like the one shown, are particularly valuable when labeled fault data is scarce, allowing systems to learn patterns from unlabeled operational data. Source: Google","key":"fig-ad","order":{"number":17,"section":[1,4,5,1,2,1,0]}},{"caption":"Data Poisoning Attack: Adversarial manipulation of training data introduces subtle perturbations that compromise model integrity; incremental poisoning gradually shifts model behavior over time, making detection challenging in online learning systems. This attack surface differs from adversarial examples because it targets the model during training rather than at inference.","key":"fig-poisoning-attack-example","order":{"number":26,"section":[1,8,2,2,0,0,0]}},{"caption":"Detection and Mitigation","key":"sec-robust-ai-detection-mitigation-91de","order":{"number":88,"section":[1,8,4,3,1,0,0]}},{"caption":"Memory Bandwidth Protection Analysis: Impact of ECC protection on effective memory bandwidth across different memory technologies used in ML accelerators. The bandwidth overhead directly affects training throughput for memory-bound workloads.","key":"tbl-memory-bandwidth-protection","order":{"number":2,"section":[1,4,2,3,0,0,0]}},{"caption":"Silent Data Corruption: Modern AI Systems, Particularly Those Employing Large-Scale Data Processing Like Spark, Are Vulnerable to Silent Data Corruption (SDC), Subtle Errors Accumulating During Data Transfer and Storage. SDC Manifests in a Shuffle and Merge Database, Highlighting Corrupted Data Blocks (Red) Amidst Healthy Data (Blue/Gray) and Emphasizing the Challenge of Detecting These Errors in Distributed Systems Using the Figure. Source: Jeff Dean at MLSys 2024, Keynote (Google).","key":"fig-sdc-jeffdean","order":{"number":2,"section":[1,2,1,0,0,0,0]}},{"caption":"Data Poisoning Attack: Adversaries inject malicious data into the training set to manipulate model behavior, potentially causing misclassification or performance degradation during deployment. This attack emphasizes the vulnerability of machine learning systems to compromised data integrity and the need for robust data validation techniques. Source: li","key":"fig-adversarial-attack-injection","order":{"number":31,"section":[1,8,4,2,0,0,0]}},{"caption":"Cloud Infrastructure Failures","key":"sec-robust-ai-cloud-infrastructure-failures-1c8c","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Watchdog Timer Operation: Embedded systems utilize watchdog timers to detect and recover from software or hardware faults by periodically resetting a timeout counter; failure to reset within the allotted time triggers a system reset or recovery action, ensuring continued operation. Source: ablic","key":"fig-watchdog","order":{"number":16,"section":[1,4,5,1,1,4,0]}},{"caption":"Hardware redundancy and voting mechanisms","key":"sec-robust-ai-hardware-redundancy-voting-mechanisms-b837","order":{"number":37,"section":[1,4,5,1,1,3,0]}},{"caption":"Built-in self-test (BIST) Mechanisms","key":"sec-robust-ai-builtin-selftest-bist-mechanisms-ee55","order":{"number":35,"section":[1,4,5,1,1,1,0]}},{"caption":"Data Augmentation Techniques: Applying transformations like horizontal flips, rotations, and cropping expands training datasets, improving model robustness to variations in input data and reducing overfitting. These techniques generate new training examples without requiring additional labeled data, effectively increasing dataset diversity and enhancing generalization performance.","key":"fig-data-augmentation","order":{"number":33,"section":[1,8,4,2,3,0,0]}},{"caption":"Intermittent Fault Origins","key":"sec-robust-ai-intermittent-fault-origins-678d","order":{"number":29,"section":[1,4,4,2,0,0,0]}},{"caption":"Data Poisoning Impact: Subtle perturbations to training data labels can induce significant distributional shifts, leading to model inaccuracies and compromised performance in machine learning systems. These shifts exemplify how even limited adversarial control over training data can disrupt model learning and highlight the vulnerability of data-driven approaches to malicious manipulation. Source: [@shan2023prompt].","key":"fig-distribution-shift-example","order":{"number":25,"section":[1,8,2,2,0,0,0]}},{"caption":"Breed Evolution: Selective breeding over generations produces substantial shifts in phenotypic characteristics, mirroring how data distributions change in machine learning systems over time. These temporal shifts necessitate model retraining or adaptation to maintain performance, as initial training data may no longer accurately represent current input distributions.","key":"fig-temporal-evolution","order":{"number":30,"section":[1,8,3,2,0,0,0]}},{"caption":"Software-implemented fault tolerance (SIFT) techniques","key":"sec-robust-ai-softwareimplemented-fault-tolerance-sift-techniques-92d6","order":{"number":43,"section":[1,4,5,1,2,4,0]}},{"caption":"Touchdown Detection Failure: Erroneous Sensor Readings During the Mars Polar Lander Mission Triggered a Premature Engine Shutdown, Demonstrating the Critical Need for Robust Failure Modes and Rigorous Validation of Embedded Systems, particularly Those Operating in Inaccessible Environments. This Incident Underscores How Software Errors Can Lead to Catastrophic Consequences in Safety-Critical Applications and Emphasizes the Growing Importance of Reliable AI Integration in Complex Systems. Source: Slashgear.","key":"fig-nasa-example","order":{"number":4,"section":[1,2,3,0,0,0,0]}},{"caption":"Simulation Fidelity Challenges","key":"sec-robust-ai-simulation-fidelity-challenges-dd28","order":{"number":108,"section":[1,10,4,1,0,0,0]}},{"caption":"Hardware Fault Patterns: Dnns exhibit distinct error manifestations from hardware faults, categorized by their spatial distribution across feature maps and layers. These patterns—single point, same row, bullet wake, and shatter glass—represent localized versus widespread corruption, impacting model predictions and highlighting the need for fault-tolerant system design. Source: [@bolchini2022fast].","key":"fig-hardware-errors-bolchini","order":{"number":42,"section":[1,10,4,0,0,0,0]}},{"caption":"Input Attack Detection and Defense","key":"sec-robust-ai-input-attack-detection-defense-19d3","order":{"number":77,"section":[1,8,4,0,0,0,0]}},{"caption":"Transient Fault Properties","key":"sec-robust-ai-transient-fault-properties-318c","order":{"number":16,"section":[1,4,2,1,0,0,0]}},{"caption":"Distribution Shift: Small inconsistencies between training and deployment data (represented by differing distributions of spurious feature z) can significantly disrupt model performance, even without altering the true label y. This figure emphasizes how data poisoning attacks exploit distributional differences to induce model errors and emphasizes the vulnerability of machine learning systems to subtle data manipulations. Source: [@shan2023prompt].","key":"fig-distribution-shift","order":{"number":28,"section":[1,8,3,1,0,0,0]}},{"caption":"Data Poisoning Defenses","key":"sec-robust-ai-data-poisoning-defenses-d070","order":{"number":82,"section":[1,8,4,2,0,0,0]}},{"caption":"Summary","key":"sec-robust-ai-summary-a932","order":{"number":65,"section":[1,8,1,2,5,0,0]}},{"caption":"Defense Strategies","key":"sec-robust-ai-defense-strategies-cb2d","order":{"number":80,"section":[1,8,4,1,2,0,0]}},{"caption":"Memory Hierarchy and Bandwidth Impact","key":"sec-robust-ai-memory-hierarchy-bandwidth-impact-7525","order":{"number":18,"section":[1,4,2,3,0,0,0]}},{"caption":"Hardware Injection Methods","key":"sec-robust-ai-hardware-injection-methods-8cc9","order":{"number":100,"section":[1,10,2,1,0,0,0]}},{"caption":"Fault Characteristics: Transient, permanent, and intermittent faults differ by duration, persistence, and recurrence, impacting system reliability and requiring distinct mitigation strategies for robust AI deployments. Understanding these distinctions guides the design of fault-tolerant systems capable of handling diverse hardware failures during operation.","key":"tbl-fault_types","order":{"number":3,"section":[1,4,6,0,0,0,0]}},{"caption":"CI/CD Pipeline: Automated CI/CD pipelines enforce fault-aware development by integrating testing and validation directly into the software delivery process, reducing the risk of regressions and ensuring only tested code reaches production. Containerization technologies, such as Docker and Kubernetes, further enhance reliability by providing reproducible runtime environments across these pipeline stages. Source: geeksforgeeks","key":"fig-CI-CD-procedure","order":{"number":37,"section":[1,9,4,0,0,0,0]}},{"caption":"Detection Techniques","key":"sec-robust-ai-detection-techniques-0e7e","order":{"number":79,"section":[1,8,4,1,1,0,0]}},{"caption":"Gradient-based Attacks","key":"sec-robust-ai-gradientbased-attacks-7a6f","order":{"number":61,"section":[1,8,1,2,1,0,0]}},{"caption":"Data Poisoning Effects on ML","key":"sec-robust-ai-data-poisoning-effects-ml-3ce8","order":{"number":70,"section":[1,8,2,3,0,0,0]}},{"caption":"Hardware Faults","key":"sec-robust-ai-hardware-faults-cf22","order":{"number":13,"section":[1,4,0,0,0,0,0]}},{"caption":"Software-Based Fault Injection","key":"sec-robust-ai-softwarebased-fault-injection-d8a1","order":{"number":102,"section":[1,10,3,0,0,0,0]}},{"caption":"Self-Supervised Learning for Robustness","key":"sec-robust-ai-selfsupervised-learning-robustness-0c94","order":{"number":91,"section":[1,8,4,4,0,0,0]}},{"caption":"Permanent Fault Effects on ML","key":"sec-robust-ai-permanent-fault-effects-ml-b9fd","order":{"number":26,"section":[1,4,3,4,0,0,0]}},{"caption":"Real-World Robustness Failures","key":"sec-robust-ai-realworld-robustness-failures-c119","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"Poisoning Attack: An incremental process where malicious samples are introduced to gradually shift model behavior during online learning. Continuous data streams can be manipulated without immediate detection through this. Source: [@shan2023prompt].","key":"fig-poisoning","order":{"number":27,"section":[1,8,2,4,0,0,0]}},{"caption":"Fault Injection Effects: Bit-level hardware faults can induce phantom objects and misclassifications in machine learning models, potentially leading to safety-critical errors in applications like autonomous driving; the left image represents correct classification, while the right image presents a false positive detection resulting from a single bit flip injected using pytorchfi.","key":"fig-phantom-objects","order":{"number":41,"section":[1,10,3,3,0,0,0]}},{"caption":"Autopilot Perception Failure: This Crash Provides the Critical Safety Risks of Relying on Machine Learning for Perception in Autonomous Systems, Where Failures to Correctly Classify Objects Can Lead to Catastrophic Outcomes. The Incident Underscores the Need for Robust Validation, Redundancy, and Failsafe Mechanisms in Self-Driving Vehicle Designs to Mitigate the Impact of Imperfect AI Models. Source: BBC News.","key":"fig-tesla-example","order":{"number":3,"section":[1,2,2,0,0,0,0]}},{"caption":"From ML Performance to System Reliability","key":"sec-robust-ai-ml-performance-system-reliability-7d42","order":{"number":9,"section":[1,3,2,0,0,0,0]}},{"caption":"Robustness Strategy Reference: A practical mapping of fault categories to their primary detection and mitigation approaches, providing engineers with a systematic framework for implementing comprehensive robustness solutions across the three pillars of robust AI.","key":"tbl-robustness-summary","order":{"number":7,"section":[1,12,0,0,0,0,0]}},{"caption":"Optimization-based Attacks","key":"sec-robust-ai-optimizationbased-attacks-f018","order":{"number":62,"section":[1,8,1,2,2,0,0]}},{"caption":"Software Injection Trade-offs","key":"sec-robust-ai-software-injection-tradeoffs-b390","order":{"number":103,"section":[1,10,3,1,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-robust-ai-fallacies-pitfalls-087e","order":{"number":110,"section":[1,11,0,0,0,0,0]}},{"caption":"Bridging Hardware-Software Gap","key":"sec-robust-ai-bridging-hardwaresoftware-gap-d194","order":{"number":107,"section":[1,10,4,0,0,0,0]}},{"caption":"Intermittent Fault Propagation","key":"sec-robust-ai-intermittent-fault-propagation-f85c","order":{"number":30,"section":[1,4,4,3,0,0,0]}},{"caption":"Software Fault Properties","key":"sec-robust-ai-software-fault-properties-d339","order":{"number":93,"section":[1,9,1,0,0,0,0]}},{"caption":"Adversarial Perturbation: Subtle, physically realizable modifications to input data can cause machine learning models to make incorrect predictions, even when imperceptible to humans. This example shows how small stickers on a stop sign caused a traffic sign classifier to misidentify it as a 45 mph speed limit sign with over 85% accuracy, highlighting the vulnerability of ML systems to adversarial attacks. Source: eykholt","key":"fig-graffiti","order":{"number":23,"section":[1,8,1,3,0,0,0]}},{"caption":"Heartbeat and Timeout: Distributed Systems Employ Periodic Heartbeat Messages to Detect Node Failures; A Lack of Response Within a Defined Timeout Indicates a Fault, Triggering Corrective Actions Like Workload Redistribution or Failover. This Mechanism, Analogous to Watchdog Timers, Ensures System Robustness and Continuous Operation Despite Component Failures. Source: geeksforgeeks.","key":"fig-heartbeat","order":{"number":18,"section":[1,4,5,1,2,3,0]}},{"caption":"Hardware Fault Detection and Mitigation","key":"sec-robust-ai-hardware-fault-detection-mitigation-8f7f","order":{"number":32,"section":[1,4,5,0,0,0,0]}},{"caption":"ML-Specific Injection Tools","key":"sec-robust-ai-mlspecific-injection-tools-0584","order":{"number":106,"section":[1,10,3,4,0,0,0]}},{"caption":"Data Poisoning Properties","key":"sec-robust-ai-data-poisoning-properties-2258","order":{"number":68,"section":[1,8,2,1,0,0,0]}},{"caption":"Impact on ML","key":"sec-robust-ai-impact-ml-7c6f","order":{"number":66,"section":[1,8,1,3,0,0,0]}},{"caption":"Autoencoder Architecture: Autoencoders learn compressed data representations by minimizing reconstruction error, enabling anomaly detection by identifying inputs with high reconstruction loss. During training on normal data, the network learns efficient encoding and decoding, making it sensitive to deviations indicative of potential poisoning attacks. Source: dertat","key":"fig-autoencoder","order":{"number":32,"section":[1,8,4,2,1,0,0]}},{"caption":"Monitoring and Adaptation Strategies","key":"sec-robust-ai-monitoring-adaptation-strategies-f305","order":{"number":55,"section":[1,6,2,0,0,0,0]}},{"caption":"Robust Training","key":"sec-robust-ai-robust-training-37d6","order":{"number":85,"section":[1,8,4,2,3,0,0]}},{"caption":"Fault Analysis and Performance Impact","key":"sec-robust-ai-fault-analysis-performance-impact-fa37","order":{"number":17,"section":[1,4,2,2,0,0,0]}},{"caption":"Software Injection Tool Categories","key":"sec-robust-ai-software-injection-tool-categories-23e5","order":{"number":105,"section":[1,10,3,3,0,0,0]}},{"caption":"Summary","key":"sec-robust-ai-summary-a274","order":{"number":111,"section":[1,12,0,0,0,0,0]}},{"caption":"Adversarial Perturbation: Subtle, Intentionally Crafted Noise Can Cause Neural Networks to Misclassify Images With High Confidence, Exposing a Vulnerability in Model Robustness. These Perturbations, Imperceptible to Humans, Alter the Input in a Way That Maximizes Prediction Error, Highlighting the Need for Defenses Against Adversarial Attacks. Source: Sutanto (2019).","key":"fig-adversarial-attack-noise-example","order":{"number":20,"section":[1,8,1,0,0,0,0]}},{"caption":"Radiation Testing Setup: Beam testing facilities induce hardware faults by exposing semiconductor components to high-energy particles, simulating realistic radiation environments encountered in space or particle physics experiments. This controlled fault injection method provides valuable data for assessing hardware reliability and error rates under extreme conditions, though it lacks the precise targeting capabilities of FPGA-based fault injection. Source: JD instruments [HTTPS://jdinstruments.net/tester-capabilities-radiation-test/]","key":"fig-beam-testing","order":{"number":40,"section":[1,10,2,1,0,0,0]}},{"caption":"Regression Test Automation: Automated regression tests verify that new code changes do not introduce unintended errors into existing functionality, preserving system reliability throughout the development lifecycle. Continuous execution of these tests is crucial in rapidly evolving machine learning systems where even small modifications can have widespread consequences. Source: UTOR","key":"fig-regression-testing","order":{"number":36,"section":[1,9,4,0,0,0,0]}},{"caption":"Intermittent Fault Properties","key":"sec-robust-ai-intermittent-fault-properties-9373","order":{"number":28,"section":[1,4,4,1,0,0,0]}},{"caption":"Input-Level Attacks and Model Robustness","key":"sec-robust-ai-inputlevel-attacks-model-robustness-d6ea","order":{"number":57,"section":[1,8,0,0,0,0,0]}},{"caption":"Distribution Shift Effects on ML","key":"sec-robust-ai-distribution-shift-effects-ml-752d","order":{"number":75,"section":[1,8,3,3,0,0,0]}},{"caption":"Technical Mechanisms","key":"sec-robust-ai-technical-mechanisms-98ce","order":{"number":48,"section":[1,5,1,2,0,0,0]}},{"caption":"Distribution Shift Adaptation","key":"sec-robust-ai-distribution-shift-adaptation-3d6e","order":{"number":87,"section":[1,8,4,3,0,0,0]}},{"caption":"Three Pillars Framework: The three core categories of robustness challenges that AI systems must address to ensure reliable operation in real-world deployments. A robust AI system is built upon effectively handling these three challenge areas.","key":"fig-three-pillars-framework","order":{"number":5,"section":[1,3,3,0,0,0,0]}},{"caption":"Fault and Error Models","key":"sec-robust-ai-fault-error-models-c66e","order":{"number":98,"section":[1,10,1,0,0,0,0]}},{"caption":"Fault Injection Tools and Frameworks","key":"sec-robust-ai-fault-injection-tools-frameworks-fc07","order":{"number":97,"section":[1,10,0,0,0,0,0]}},{"caption":"Adversarial Training Implementation: Practical adversarial training using FGSM to generate adversarial examples during training, mixing clean and perturbed data to improve model robustness against gradient-based attacks.","key":"lst-adversarial-training","order":{"number":1,"section":[1,8,4,1,2,0,0]}},{"caption":"Fault Mitigation Strategies: Software faults in ML systems require layered detection and mitigation techniques applied throughout the development lifecycle—from initial testing to ongoing monitoring—to ensure reliability and robustness. This table categorizes these strategies by phase and objective, providing a framework for building comprehensive fault tolerance into machine learning deployments.","key":"tbl-software-faults-summary","order":{"number":6,"section":[1,9,4,0,0,0,0]}},{"caption":"Heartbeat and timeout mechanisms","key":"sec-robust-ai-heartbeat-timeout-mechanisms-0d6b","order":{"number":42,"section":[1,4,5,1,2,3,0]}},{"caption":"Software Fault Detection and Prevention","key":"sec-robust-ai-software-fault-detection-prevention-6478","order":{"number":96,"section":[1,9,4,0,0,0,0]}},{"caption":"Introduction to Robust AI Systems","key":"sec-robust-ai-introduction-robust-ai-systems-4671","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Adversarial Attack Categories: Machine learning model robustness relies on defending against attacks that intentionally perturb input data to cause misclassification; this table categorizes these attacks by their underlying mechanism, including gradient-based, optimization-based, transfer-based, and physical-world approaches, each exploiting different model vulnerabilities. Understanding these categories is crucial for developing effective defense strategies and evaluating model security.","key":"tbl-attack_types","order":{"number":4,"section":[1,8,1,2,5,0,0]}},{"caption":"Secure Data Sourcing","key":"sec-robust-ai-secure-data-sourcing-563e","order":{"number":86,"section":[1,8,4,2,4,0,0]}},{"caption":"GPU Resource Management: Inefficient memory usage or failure to release GPU resources can lead to out-of-memory errors and suboptimal performance during training.","key":"fig-gpu-out-of-memory","order":{"number":35,"section":[1,9,2,0,0,0,0]}},{"caption":"Building on Previous Concepts","key":"sec-robust-ai-building-previous-concepts-ef4a","order":{"number":8,"section":[1,3,1,0,0,0,0]}},{"caption":"Software Faults","key":"sec-robust-ai-software-faults-889e","order":{"number":92,"section":[1,9,0,0,0,0,0]}},{"caption":"Mitigation Techniques","key":"sec-robust-ai-mitigation-techniques-0163","order":{"number":90,"section":[1,8,4,3,3,0,0]}},{"caption":"Distribution Shift Detection: Core statistical methods for monitoring data distribution changes in production, combining Kolmogorov-Smirnov tests for individual features with domain classifier approaches to detect when incoming data differs significantly from training distributions.","key":"lst-distribution-shift","order":{"number":2,"section":[1,8,4,3,2,0,0]}},{"caption":"Error Masking: Microarchitectural redundancy can absorb single-bit faults before they propagate to observable system errors, highlighting a discrepancy between hardware-level and software-level fault models. This figure details how fault masking occurs within microarchitectural components, demonstrating that software-based error detection tools may underestimate the true resilience of a system to transient errors. [@ko2021characterizing]","key":"fig-error-masking","order":{"number":38,"section":[1,10,1,0,0,0,0]}},{"caption":"Distribution Shift Types: Real-world ML systems encounter various forms of distribution shift—including covariate, concept, and prior shift—that degrade performance by altering the relationship between inputs and outputs, or the prevalence of different outcomes. Understanding these shifts and implementing system-level mitigations—such as monitoring, adaptive learning, and robust training—is crucial for maintaining reliable performance in dynamic environments.","key":"tbl-distribution-shift-summary","order":{"number":5,"section":[1,8,3,3,0,0,0]}},{"caption":"Anomaly Detection Techniques","key":"sec-robust-ai-anomaly-detection-techniques-bc27","order":{"number":83,"section":[1,8,4,2,1,0,0]}},{"caption":"The Three Pillars of Robust AI","key":"sec-robust-ai-three-pillars-robust-ai-2626","order":{"number":10,"section":[1,3,3,0,0,0,0]}},{"caption":"Intermittent Fault Effects on ML","key":"sec-robust-ai-intermittent-fault-effects-ml-28e7","order":{"number":31,"section":[1,4,4,4,0,0,0]}},{"caption":"Sanitization and Preprocessing","key":"sec-robust-ai-sanitization-preprocessing-f883","order":{"number":84,"section":[1,8,4,2,2,0,0]}},{"caption":"A Unified Framework for Robust AI","key":"sec-robust-ai-unified-framework-robust-ai-b25d","order":{"number":7,"section":[1,3,0,0,0,0,0]}},{"caption":"Permanent Faults","key":"sec-robust-ai-permanent-faults-7dfb","order":{"number":22,"section":[1,4,3,0,0,0,0]}},{"caption":"DRAM Residue Fault: Intermittent failures in DRAM chips commonly arise from microscopic residue accumulation, creating unreliable electrical connections. Physical defects can induce sporadic errors, highlighting the need for fault-tolerant system design and hardware testing via this figure. Source: hynix semiconductor","key":"fig-intermittent-fault-dram","order":{"number":12,"section":[1,4,4,2,0,0,0]}},{"caption":"Permanent Fault Propagation","key":"sec-robust-ai-permanent-fault-propagation-b770","order":{"number":25,"section":[1,4,3,3,0,0,0]}},{"caption":"Hardware-Level Detection","key":"sec-robust-ai-hardwarelevel-detection-9a56","order":{"number":34,"section":[1,4,5,1,1,0,0]}},{"caption":"Temporal Drift: Shifting data distributions over time degrade model performance unless systems adapt through continuous monitoring and retraining. Concept drift manifests as changes in input patterns—such as evolving fraud schemes or seasonal trends—that require models to learn new relationships and maintain accuracy in dynamic environments.","key":"fig-drift-over-time","order":{"number":29,"section":[1,8,3,1,0,0,0]}},{"caption":"Technical Categories","key":"sec-robust-ai-technical-categories-cc06","order":{"number":54,"section":[1,6,1,2,0,0,0]}},{"caption":"Software Fault Propagation","key":"sec-robust-ai-software-fault-propagation-59e7","order":{"number":94,"section":[1,9,2,0,0,0,0]}},{"caption":"Adversarial Perturbations: Subtle, intentionally crafted noise added to an image can cause a trained deep neural network (googlenet) to misclassify it, even though the perturbed image remains visually indistinguishable to humans. This vulnerability underscores the lack of robustness in many machine learning models and motivates research into adversarial training and defense mechanisms. Source: goodfellow et al., 2014.","key":"fig-adversarial-googlenet","order":{"number":22,"section":[1,8,1,3,0,0,0]}},{"caption":"System Implications of Distribution Shifts","key":"sec-robust-ai-system-implications-distribution-shifts-9388","order":{"number":76,"section":[1,8,3,4,0,0,0]}},{"caption":"Integration Across the ML Pipeline","key":"sec-robust-ai-integration-across-ml-pipeline-8286","order":{"number":12,"section":[1,3,5,0,0,0,0]}},{"caption":"Stuck-at Fault Model: Digital circuits can experience permanent faults where a signal line becomes fixed at a logical 0 or 1, regardless of input; this figure represents a simplified depiction of a stuck-at-0 fault, where a signal is persistently low, potentially leading to incorrect computations or system failures. Source: accendo reliability","key":"fig-stuck-fault","order":{"number":10,"section":[1,4,3,3,0,0,0]}},{"caption":"Detection Techniques","key":"sec-robust-ai-detection-techniques-b5f8","order":{"number":89,"section":[1,8,4,3,2,0,0]}},{"caption":"Permanent Fault Properties","key":"sec-robust-ai-permanent-fault-properties-08c5","order":{"number":23,"section":[1,4,3,1,0,0,0]}},{"caption":"Transfer-based Attacks","key":"sec-robust-ai-transferbased-attacks-9896","order":{"number":63,"section":[1,8,1,2,3,0,0]}},{"caption":"Heartbeat Monitoring: Redundant Node Connections and Periodic Heartbeat Messages Detect and Isolate Failing Components in Distributed Systems, Ensuring Continued Operation Despite Hardware Faults. These Mechanisms Enable Fault Tolerance by Allowing Nodes to Identify Unresponsive Peers and Reroute Communication Accordingly. Source: geeksforgeeks.","key":"fig-Reed-Solomon","order":{"number":19,"section":[1,4,5,1,2,4,0]}},{"caption":"Permanent Fault Origins","key":"sec-robust-ai-permanent-fault-origins-187d","order":{"number":24,"section":[1,4,3,2,0,0,0]}},{"caption":"Embedded System Constraints","key":"sec-robust-ai-embedded-system-constraints-ec7a","order":{"number":6,"section":[1,2,3,0,0,0,0]}},{"caption":"Bit-Flip Error: Transient faults can alter individual bits in memory, corrupting data or program instructions and potentially causing system malfunctions. These single-bit errors exemplify the vulnerability of hardware to transient faults like those induced by radiation or electromagnetic interference.","key":"fig-bit-flip","order":{"number":6,"section":[1,4,2,3,0,0,0]}},{"caption":"Data Poisoning Attacks","key":"sec-robust-ai-data-poisoning-attacks-8487","order":{"number":49,"section":[1,5,2,0,0,0,0]}},{"caption":"Adversarial Attacks","key":"sec-robust-ai-adversarial-attacks-481c","order":{"number":58,"section":[1,8,1,0,0,0,0]}},{"caption":"Common Robustness Principles","key":"sec-robust-ai-common-robustness-principles-cb22","order":{"number":11,"section":[1,3,4,0,0,0,0]}},{"caption":"Software-Level Detection","key":"sec-robust-ai-softwarelevel-detection-b9b3","order":{"number":39,"section":[1,4,5,1,2,0,0]}},{"caption":"Adversarial Perturbations: Gradient-based attacks generate subtle, intentionally crafted input noise – with magnitude controlled by \\epsilon – that maximizes the loss function j(\\theta, x, y) and causes misclassification by the model. These perturbations, imperceptible to humans, exploit model vulnerabilities by moving the input x across the decision boundary. Source: ivezic","key":"fig-gradient-attack","order":{"number":21,"section":[1,8,1,2,1,0,0]}},{"caption":"Intentional Input Manipulation","key":"sec-robust-ai-intentional-input-manipulation-6b2a","order":{"number":45,"section":[1,5,0,0,0,0,0]}},{"caption":"Case Study: Art Protection via Poisoning","key":"sec-robust-ai-case-study-art-protection-via-poisoning-6106","order":{"number":71,"section":[1,8,2,4,0,0,0]}},{"caption":"Hardware-Based Fault Injection","key":"sec-robust-ai-hardwarebased-fault-injection-ae39","order":{"number":99,"section":[1,10,2,0,0,0,0]}},{"caption":"Distribution Shift and Concept Drift","key":"sec-robust-ai-distribution-shift-concept-drift-55e2","order":{"number":52,"section":[1,6,1,0,0,0,0]}},{"caption":"Hardware Fault Summary","key":"sec-robust-ai-hardware-fault-summary-b40c","order":{"number":44,"section":[1,4,6,0,0,0,0]}},{"caption":"Robustness Evaluation Tools","key":"sec-robust-ai-robustness-evaluation-tools-6b64","order":{"number":56,"section":[1,7,0,0,0,0,0]}},{"caption":"Hardware Injection Limitations","key":"sec-robust-ai-hardware-injection-limitations-ac49","order":{"number":101,"section":[1,10,2,2,0,0,0]}},{"caption":"Distribution Shifts","key":"sec-robust-ai-distribution-shifts-2474","order":{"number":72,"section":[1,8,3,0,0,0,0]}},{"caption":"Intermittent Faults","key":"sec-robust-ai-intermittent-faults-35e9","order":{"number":27,"section":[1,4,4,0,0,0,0]}},{"caption":"Transient Fault Propagation","key":"sec-robust-ai-transient-fault-propagation-4bee","order":{"number":20,"section":[1,4,2,5,0,0,0]}},{"caption":"Evaluation and Testing","key":"sec-robust-ai-evaluation-testing-4b54","order":{"number":81,"section":[1,8,4,1,3,0,0]}},{"caption":"Distribution Shift Properties","key":"sec-robust-ai-distribution-shift-properties-f87d","order":{"number":73,"section":[1,8,3,1,0,0,0]}},{"caption":"Data Poisoning Examples: Mismatched image-text pairs represent a common data poisoning attack, where manipulated training data causes models to misclassify inputs. These adversarial examples can compromise model integrity and introduce vulnerabilities in real-world applications. Source: [@shan2023prompt].","key":"fig-dirty-label-example","order":{"number":24,"section":[1,8,2,1,0,0,0]}},{"caption":"Transient Fault Effects on ML","key":"sec-robust-ai-transient-fault-effects-ml-a01d","order":{"number":21,"section":[1,4,2,6,0,0,0]}},{"caption":"Attack Categories and Mechanisms","key":"sec-robust-ai-attack-categories-mechanisms-fed5","order":{"number":60,"section":[1,8,1,2,0,0,0]}},{"caption":"Intuitive Understanding","key":"sec-robust-ai-intuitive-understanding-8a8d","order":{"number":53,"section":[1,6,1,1,0,0,0]}},{"caption":"Intermittent Fault Mechanism: Increased resistance from cracks between copper bumps and package solder represents a common source of intermittent faults, disrupting signal transmission and potentially causing unpredictable system behavior. Microscopic material defects like these highlight the vulnerability of hardware to latent failures that are difficult to detect during testing but can manifest during operation. Source: constantinescu.","key":"fig-intermittent-fault","order":{"number":11,"section":[1,4,4,0,0,0,0]}},{"caption":"Hardware Fault Detection Methods","key":"sec-robust-ai-hardware-fault-detection-methods-ea71","order":{"number":33,"section":[1,4,5,1,0,0,0]}},{"caption":"Consistency checks and data validation","key":"sec-robust-ai-consistency-checks-data-validation-1d53","order":{"number":41,"section":[1,4,5,1,2,2,0]}},{"caption":"Environmental Shifts","key":"sec-robust-ai-environmental-shifts-a2cf","order":{"number":51,"section":[1,6,0,0,0,0,0]}},{"caption":"Runtime monitoring and anomaly detection","key":"sec-robust-ai-runtime-monitoring-anomaly-detection-b39f","order":{"number":40,"section":[1,4,5,1,2,1,0]}},{"caption":"Hardware Faults: This figure enables where hardware-induced errors can occur within a DNN processing pipeline, highlighting potential points of failure such as control units and memory modules that can lead to misclassifications in real-world applications.","key":"fig-hardware-errors","order":{"number":39,"section":[1,10,2,0,0,0,0]}},{"caption":"FDIV Error Regions: The triangular areas indicate where the pentium processor’s faulty division unit produced incorrect results when calculating 4195835/3145727; ideally, all values should round to 1.3338, but the bug caused a slight inaccuracy in the fifth digit. Source: byte magazine.","key":"fig-permanent-fault","order":{"number":9,"section":[1,4,3,1,0,0,0]}},{"caption":"Adversarial Attack Defenses","key":"sec-robust-ai-adversarial-attack-defenses-1dc8","order":{"number":78,"section":[1,8,4,1,0,0,0]}},{"caption":"Error Detection Codes","key":"sec-robust-ai-error-detection-codes-2774","order":{"number":36,"section":[1,4,5,1,1,2,0]}}],"headings":["sec-robust-ai","purpose","sec-robust-ai-introduction-robust-ai-systems-4671","sec-robust-ai-realworld-robustness-failures-c119","sec-robust-ai-cloud-infrastructure-failures-1c8c","sec-robust-ai-edge-device-vulnerabilities-ddfe","sec-robust-ai-embedded-system-constraints-ec7a","sec-robust-ai-unified-framework-robust-ai-b25d","sec-robust-ai-building-previous-concepts-ef4a","sec-robust-ai-ml-performance-system-reliability-7d42","sec-robust-ai-three-pillars-robust-ai-2626","sec-robust-ai-common-robustness-principles-cb22","sec-robust-ai-integration-across-ml-pipeline-8286","sec-robust-ai-hardware-faults-cf22","sec-robust-ai-hardware-fault-impact-ml-systems-b5f7","sec-robust-ai-transient-faults-1455","sec-robust-ai-transient-fault-properties-318c","sec-robust-ai-fault-analysis-performance-impact-fa37","sec-robust-ai-memory-hierarchy-bandwidth-impact-7525","sec-robust-ai-transient-fault-origins-2226","sec-robust-ai-transient-fault-propagation-4bee","sec-robust-ai-transient-fault-effects-ml-a01d","sec-robust-ai-permanent-faults-7dfb","sec-robust-ai-permanent-fault-properties-08c5","sec-robust-ai-permanent-fault-origins-187d","sec-robust-ai-permanent-fault-propagation-b770","sec-robust-ai-permanent-fault-effects-ml-b9fd","sec-robust-ai-intermittent-faults-35e9","sec-robust-ai-intermittent-fault-properties-9373","sec-robust-ai-intermittent-fault-origins-678d","sec-robust-ai-intermittent-fault-propagation-f85c","sec-robust-ai-intermittent-fault-effects-ml-28e7","sec-robust-ai-hardware-fault-detection-mitigation-8f7f","sec-robust-ai-hardware-fault-detection-methods-ea71","sec-robust-ai-hardwarelevel-detection-9a56","sec-robust-ai-builtin-selftest-bist-mechanisms-ee55","sec-robust-ai-error-detection-codes-2774","sec-robust-ai-hardware-redundancy-voting-mechanisms-b837","sec-robust-ai-watchdog-timers-9e44","sec-robust-ai-softwarelevel-detection-b9b3","sec-robust-ai-runtime-monitoring-anomaly-detection-b39f","sec-robust-ai-consistency-checks-data-validation-1d53","sec-robust-ai-heartbeat-timeout-mechanisms-0d6b","sec-robust-ai-softwareimplemented-fault-tolerance-sift-techniques-92d6","sec-robust-ai-hardware-fault-summary-b40c","sec-robust-ai-intentional-input-manipulation-6b2a","sec-robust-ai-adversarial-attacks-bb75","sec-robust-ai-conceptual-foundation-20b5","sec-robust-ai-technical-mechanisms-98ce","sec-robust-ai-data-poisoning-attacks-8487","sec-robust-ai-detection-mitigation-strategies-8dbe","sec-robust-ai-environmental-shifts-a2cf","sec-robust-ai-distribution-shift-concept-drift-55e2","sec-robust-ai-intuitive-understanding-8a8d","sec-robust-ai-technical-categories-cc06","sec-robust-ai-monitoring-adaptation-strategies-f305","sec-robust-ai-robustness-evaluation-tools-6b64","sec-robust-ai-inputlevel-attacks-model-robustness-d6ea","sec-robust-ai-adversarial-attacks-481c","sec-robust-ai-understanding-vulnerability-de4c","sec-robust-ai-attack-categories-mechanisms-fed5","sec-robust-ai-gradientbased-attacks-7a6f","sec-robust-ai-optimizationbased-attacks-f018","sec-robust-ai-transferbased-attacks-9896","sec-robust-ai-physicalworld-attacks-97a0","sec-robust-ai-summary-a932","sec-robust-ai-impact-ml-7c6f","sec-robust-ai-data-poisoning-4b55","sec-robust-ai-data-poisoning-properties-2258","sec-robust-ai-data-poisoning-attack-methods-d168","sec-robust-ai-data-poisoning-effects-ml-3ce8","sec-robust-ai-case-study-art-protection-via-poisoning-6106","sec-robust-ai-distribution-shifts-2474","sec-robust-ai-distribution-shift-properties-f87d","sec-robust-ai-distribution-shift-mechanisms-f5e3","sec-robust-ai-distribution-shift-effects-ml-752d","sec-robust-ai-system-implications-distribution-shifts-9388","sec-robust-ai-input-attack-detection-defense-19d3","sec-robust-ai-adversarial-attack-defenses-1dc8","sec-robust-ai-detection-techniques-0e7e","sec-robust-ai-defense-strategies-cb2d","sec-robust-ai-evaluation-testing-4b54","sec-robust-ai-data-poisoning-defenses-d070","sec-robust-ai-anomaly-detection-techniques-bc27","sec-robust-ai-sanitization-preprocessing-f883","sec-robust-ai-robust-training-37d6","sec-robust-ai-secure-data-sourcing-563e","sec-robust-ai-distribution-shift-adaptation-3d6e","sec-robust-ai-detection-mitigation-91de","sec-robust-ai-detection-techniques-b5f8","sec-robust-ai-mitigation-techniques-0163","sec-robust-ai-selfsupervised-learning-robustness-0c94","sec-robust-ai-software-faults-889e","sec-robust-ai-software-fault-properties-d339","sec-robust-ai-software-fault-propagation-59e7","sec-robust-ai-software-fault-effects-ml-1ba2","sec-robust-ai-software-fault-detection-prevention-6478","sec-robust-ai-fault-injection-tools-frameworks-fc07","sec-robust-ai-fault-error-models-c66e","sec-robust-ai-hardwarebased-fault-injection-ae39","sec-robust-ai-hardware-injection-methods-8cc9","sec-robust-ai-hardware-injection-limitations-ac49","sec-robust-ai-softwarebased-fault-injection-d8a1","sec-robust-ai-software-injection-tradeoffs-b390","sec-robust-ai-software-injection-limitations-eee7","sec-robust-ai-software-injection-tool-categories-23e5","sec-robust-ai-mlspecific-injection-tools-0584","sec-robust-ai-bridging-hardwaresoftware-gap-d194","sec-robust-ai-simulation-fidelity-challenges-dd28","sec-robust-ai-hardware-behavior-modeling-9bd8","sec-robust-ai-fallacies-pitfalls-087e","sec-robust-ai-summary-a274","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}