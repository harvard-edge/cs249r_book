{"entries":[{"caption":"Hardware Lottery","key":"sec-benchmarking-ai-hardware-lottery-22ae","order":{"number":96,"section":[1,10,3,2,0,0,0]}},{"caption":"Performance Spectrum: Scientific applications and edge devices demand vastly different computational resources, spanning multiple orders of magnitude in data rates and latency requirements. Consequently, traditional benchmarks focused solely on accuracy are insufficient; specialized evaluation metrics and benchmarks like MLPerf become essential for optimizing AI systems across diverse deployment scenarios. Source: [@duarte2022fastmlsciencebenchmarksaccelerating].","key":"fig-sciml-graph","order":{"number":11,"section":[1,10,4,3,0,0,0]}},{"caption":"Training Benchmarks","key":"sec-benchmarking-ai-training-benchmarks-7533","order":{"number":31,"section":[1,7,0,0,0,0,0]}},{"caption":"Benchmarking Granularity","key":"sec-benchmarking-ai-benchmarking-granularity-771c","order":{"number":13,"section":[1,4,0,0,0,0,0]}},{"caption":"Machine Learning Benchmarking Framework","key":"sec-benchmarking-ai-machine-learning-benchmarking-framework-3968","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Scalability & Parallelism","key":"sec-benchmarking-ai-scalability-parallelism-cbc4","order":{"number":40,"section":[1,7,2,2,0,0,0]}},{"caption":"ML Measurement Challenges","key":"sec-benchmarking-ai-ml-measurement-challenges-cc7a","order":{"number":8,"section":[1,3,1,0,0,0,0]}},{"caption":"Hardware & Software Optimization","key":"sec-benchmarking-ai-hardware-software-optimization-4f19","order":{"number":34,"section":[1,7,1,2,0,0,0]}},{"caption":"MLPerf Inference Benchmarks","key":"sec-benchmarking-ai-mlperf-inference-benchmarks-65b1","order":{"number":80,"section":[1,8,4,0,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-benchmarking-ai-fallacies-pitfalls-620e","order":{"number":107,"section":[1,13,0,0,0,0,0]}},{"caption":"MLPerf as Industry Standard","key":"sec-benchmarking-ai-mlperf-industry-standard-0883","order":{"number":101,"section":[1,10,5,0,0,0,0]}},{"caption":"Linear Scaling Assumption","key":"sec-benchmarking-ai-linear-scaling-assumption-4e28","order":{"number":50,"section":[1,7,3,1,4,0,0]}},{"caption":"AI System Interdependence: Highlights the critical interplay between infrastructure, models, and data in determining overall AI system performance, emphasizing that optimization requires a holistic approach rather than isolated improvements. this figure illustrates that gains in one component cannot fully compensate for limitations in others, necessitating co-design strategies for efficient and effective AI.","key":"fig-benchmarking-trifecta","order":{"number":14,"section":[1,11,3,0,0,0,0]}},{"caption":"Statistical & Methodological Issues","key":"sec-benchmarking-ai-statistical-methodological-issues-56f4","order":{"number":92,"section":[1,10,1,0,0,0,0]}},{"caption":"System Specifications","key":"sec-benchmarking-ai-system-specifications-79a9","order":{"number":24,"section":[1,5,6,0,0,0,0]}},{"caption":"Holistic System-Model-Data Evaluation","key":"sec-benchmarking-ai-holistic-systemmodeldata-evaluation-ae59","order":{"number":105,"section":[1,11,3,0,0,0,0]}},{"caption":"Inference Performance Evaluation","key":"sec-benchmarking-ai-inference-performance-evaluation-cc51","order":{"number":68,"section":[1,8,3,0,0,0,0]}},{"caption":"Domain-Specific Benchmarks","key":"sec-benchmarking-ai-domainspecific-benchmarks-b62e","order":{"number":6,"section":[1,2,3,0,0,0,0]}},{"caption":"Benchmarking Granularity: ML system performance assessment occurs at multiple levels, from end-to-end application metrics to individual model and hardware component efficiency, enabling targeted optimization and bottleneck identification. This hierarchical approach allows practitioners to systematically analyze system performance and prioritize improvements based on specific component limitations.","key":"fig-granularity","order":{"number":3,"section":[1,4,0,0,0,0,0]}},{"caption":"Dataset Saturation: AI systems surpass human performance on benchmark datasets, indicating that continued gains may not reflect genuine improvements in intelligence but rather optimization to fixed evaluation sets. This trend underscores the need for dynamic, challenging datasets that accurately assess AI capabilities and drive meaningful progress beyond simple pattern recognition. Source: [@kiela2021dynabench].","key":"fig-dataset-saturation","order":{"number":13,"section":[1,11,2,0,0,0,0]}},{"caption":"Development Paradigms: Model-centric AI prioritizes architectural innovation with fixed datasets, while data-centric AI systematically improves dataset quality (annotations, diversity, and bias) with consistent model architectures to achieve performance gains. Modern research indicates that strategic data enhancement often yields greater improvements than solely refining model complexity.","key":"fig-model-vs-data","order":{"number":12,"section":[1,11,2,0,0,0,0]}},{"caption":"Data Benchmarking","key":"sec-benchmarking-ai-data-benchmarking-2795","order":{"number":104,"section":[1,11,2,0,0,0,0]}},{"caption":"Standardized Datasets","key":"sec-benchmarking-ai-standardized-datasets-d6e9","order":{"number":20,"section":[1,5,2,0,0,0,0]}},{"caption":"Historical Context","key":"sec-benchmarking-ai-historical-context-1c54","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"MLPerf Power Case Study","key":"sec-benchmarking-ai-mlperf-power-case-study-28ae","order":{"number":90,"section":[1,9,4,0,0,0,0]}},{"caption":"Energy Efficiency & Cost","key":"sec-benchmarking-ai-energy-efficiency-cost-c03c","order":{"number":42,"section":[1,7,2,4,0,0,0]}},{"caption":"Problem Definition","key":"sec-benchmarking-ai-problem-definition-ea4e","order":{"number":19,"section":[1,5,1,0,0,0,0]}},{"caption":"Training Benchmark Pitfalls","key":"sec-benchmarking-ai-training-benchmark-pitfalls-749a","order":{"number":46,"section":[1,7,3,1,0,0,0]}},{"caption":"Power Measurement Techniques","key":"sec-benchmarking-ai-power-measurement-techniques-ed95","order":{"number":86,"section":[1,9,0,0,0,0,0]}},{"caption":"Inference Benchmarks","key":"sec-benchmarking-ai-inference-benchmarks-433b","order":{"number":53,"section":[1,8,0,0,0,0,0]}},{"caption":"Scalability & Efficiency","key":"sec-benchmarking-ai-scalability-efficiency-ddbb","order":{"number":57,"section":[1,8,1,3,0,0,0]}},{"caption":"Ignoring Reproducibility","key":"sec-benchmarking-ai-ignoring-reproducibility-091a","order":{"number":51,"section":[1,7,3,1,5,0,0]}},{"caption":"Model and Data Benchmarking","key":"sec-benchmarking-ai-model-data-benchmarking-f058","order":{"number":102,"section":[1,11,0,0,0,0,0]}},{"caption":"Performance Benchmarks","key":"sec-benchmarking-ai-performance-benchmarks-5d0c","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Scalability & Efficiency","key":"sec-benchmarking-ai-scalability-efficiency-18ff","order":{"number":35,"section":[1,7,1,3,0,0,0]}},{"caption":"Importance of Training Benchmarks","key":"sec-benchmarking-ai-importance-training-benchmarks-5d95","order":{"number":33,"section":[1,7,1,1,0,0,0]}},{"caption":"Mobile and Edge Benchmarks","key":"sec-benchmarking-ai-mobile-edge-benchmarks-9a94","order":{"number":29,"section":[1,5,11,0,0,0,0]}},{"caption":"Isolated Single-Node Performance","key":"sec-benchmarking-ai-isolated-singlenode-performance-88b7","order":{"number":48,"section":[1,7,3,1,2,0,0]}},{"caption":"Production Environment Evaluation","key":"sec-benchmarking-ai-production-environment-evaluation-7512","order":{"number":106,"section":[1,12,0,0,0,0,0]}},{"caption":"End-to-End Benchmarks","key":"sec-benchmarking-ai-endtoend-benchmarks-1b01","order":{"number":16,"section":[1,4,3,0,0,0,0]}},{"caption":"Benchmark Harness","key":"sec-benchmarking-ai-benchmark-harness-a5eb","order":{"number":23,"section":[1,5,5,0,0,0,0]}},{"caption":"Benchmark Evolution","key":"sec-benchmarking-ai-benchmark-evolution-c9d1","order":{"number":100,"section":[1,10,4,3,0,0,0]}},{"caption":"Inference Systems Considerations","key":"sec-benchmarking-ai-inference-systems-considerations-dfc6","order":{"number":69,"section":[1,8,3,1,0,0,0]}},{"caption":"Model Benchmarking","key":"sec-benchmarking-ai-model-benchmarking-17aa","order":{"number":103,"section":[1,11,1,0,0,0,0]}},{"caption":"Importance of Inference Benchmarks","key":"sec-benchmarking-ai-importance-inference-benchmarks-2774","order":{"number":55,"section":[1,8,1,1,0,0,0]}},{"caption":"Benchmark Engineering","key":"sec-benchmarking-ai-benchmark-engineering-99d3","order":{"number":98,"section":[1,10,4,1,0,0,0]}},{"caption":"Energy Consumption & Efficiency","key":"sec-benchmarking-ai-energy-consumption-efficiency-ad66","order":{"number":67,"section":[1,8,2,7,0,0,0]}},{"caption":"Standardized Power Measurement","key":"sec-benchmarking-ai-standardized-power-measurement-adf5","order":{"number":89,"section":[1,9,3,0,0,0,0]}},{"caption":"Inference Benchmark Synthesis","key":"sec-benchmarking-ai-inference-benchmark-synthesis-36cc","order":{"number":79,"section":[1,8,3,4,0,0,0]}},{"caption":"Computational Efficiency vs. Power Consumption","key":"sec-benchmarking-ai-computational-efficiency-vs-power-consumption-714c","order":{"number":88,"section":[1,9,2,0,0,0,0]}},{"caption":"Organizational & Strategic Issues","key":"sec-benchmarking-ai-organizational-strategic-issues-9063","order":{"number":97,"section":[1,10,4,0,0,0,0]}},{"caption":"Ignoring Application Requirements","key":"sec-benchmarking-ai-ignoring-application-requirements-b36d","order":{"number":77,"section":[1,8,3,3,6,0,0]}},{"caption":"Algorithmic Benchmarks","key":"sec-benchmarking-ai-algorithmic-benchmarks-8a54","order":{"number":9,"section":[1,3,2,0,0,0,0]}},{"caption":"Fair ML Systems Comparison","key":"sec-benchmarking-ai-fair-ml-systems-comparison-bdf8","order":{"number":59,"section":[1,8,1,5,0,0,0]}},{"caption":"Training vs. Inference Evaluation","key":"sec-benchmarking-ai-training-vs-inference-evaluation-cee8","order":{"number":30,"section":[1,6,0,0,0,0,0]}},{"caption":"Resource Utilization","key":"sec-benchmarking-ai-resource-utilization-20c7","order":{"number":41,"section":[1,7,2,3,0,0,0]}},{"caption":"Environmental Conditions","key":"sec-benchmarking-ai-environmental-conditions-6a45","order":{"number":95,"section":[1,10,3,1,0,0,0]}},{"caption":"Linear Scaling Assumption","key":"sec-benchmarking-ai-linear-scaling-assumption-b625","order":{"number":76,"section":[1,8,3,3,5,0,0]}},{"caption":"Cost & Energy Factors","key":"sec-benchmarking-ai-cost-energy-factors-8e47","order":{"number":36,"section":[1,7,1,4,0,0,0]}},{"caption":"Statistical Significance & Noise","key":"sec-benchmarking-ai-statistical-significance-noise-5c2a","order":{"number":78,"section":[1,8,3,3,7,0,0]}},{"caption":"Precision & Accuracy Trade-offs","key":"sec-benchmarking-ai-precision-accuracy-tradeoffs-828e","order":{"number":63,"section":[1,8,2,3,0,0,0]}},{"caption":"Fair ML Systems Comparison","key":"sec-benchmarking-ai-fair-ml-systems-comparison-cd73","order":{"number":37,"section":[1,7,1,5,0,0,0]}},{"caption":"Micro Benchmarks","key":"sec-benchmarking-ai-micro-benchmarks-ab67","order":{"number":14,"section":[1,4,1,0,0,0,0]}},{"caption":"Power Measurement Boundaries","key":"sec-benchmarking-ai-power-measurement-boundaries-8429","order":{"number":87,"section":[1,9,1,0,0,0,0]}},{"caption":"Training Performance Evaluation","key":"sec-benchmarking-ai-training-performance-evaluation-0876","order":{"number":45,"section":[1,7,3,0,0,0,0]}},{"caption":"Benchmarking Limitations and Best Practices","key":"sec-benchmarking-ai-benchmarking-limitations-best-practices-9b2a","order":{"number":91,"section":[1,10,0,0,0,0,0]}},{"caption":"Benchmark Workflow: AI benchmarks standardize evaluation through a structured pipeline, enabling reproducible performance comparisons across different models and systems. This workflow systematically assesses AI capabilities by defining tasks, selecting datasets, training models, and rigorously evaluating results.","key":"fig-benchmark-components","order":{"number":5,"section":[1,5,0,0,0,0,0]}},{"caption":"Power Measurement Boundaries: MLPerf defines system boundaries for power measurement, ranging from single-chip devices to full data center nodes, to enable fair comparisons of energy efficiency across diverse hardware platforms. these boundaries delineate which components’ power consumption is included in reported metrics, impacting the interpretation of performance results. Source: [@tschand2024mlperf].","key":"fig-power-diagram","order":{"number":8,"section":[1,9,1,0,0,0,0]}},{"caption":"Compression Benchmarks","key":"sec-benchmarking-ai-compression-benchmarks-42c9","order":{"number":28,"section":[1,5,10,0,0,0,0]}},{"caption":"ImageNet Benchmark: Advancements in GPU technology have driven improvements in ImageNet classification accuracy since 2012, showcasing the interplay between hardware and algorithmic progress.","key":"fig-imagenet-gpus","order":{"number":2,"section":[1,3,3,0,0,0,0]}},{"caption":"Model Selection","key":"sec-benchmarking-ai-model-selection-581b","order":{"number":21,"section":[1,5,3,0,0,0,0]}},{"caption":"Power Consumption Spectrum: Machine learning deployments exhibit a wide range of power demands, from microwatt-scale TinyML devices to milliwatt-scale microcontrollers; this variability challenges the development of standardized energy efficiency benchmarks. Understanding these differences is crucial for optimizing model deployment across resource-constrained and high-performance computing environments.","key":"tbl-power","order":{"number":5,"section":[1,9,0,0,0,0,0]}},{"caption":"Example Benchmark","key":"sec-benchmarking-ai-example-benchmark-e3a1","order":{"number":27,"section":[1,5,9,0,0,0,0]}},{"caption":"Evolution and Future Directions","key":"sec-benchmarking-ai-evolution-future-directions-d2cf","order":{"number":85,"section":[1,8,4,5,0,0,0]}},{"caption":"MLPerf Tiny","key":"sec-benchmarking-ai-mlperf-tiny-ca0d","order":{"number":84,"section":[1,8,4,4,0,0,0]}},{"caption":"Latency & Tail Latency","key":"sec-benchmarking-ai-latency-tail-latency-d5dc","order":{"number":61,"section":[1,8,2,1,0,0,0]}},{"caption":"Inference Benchmark Pitfalls","key":"sec-benchmarking-ai-inference-benchmark-pitfalls-e4c8","order":{"number":71,"section":[1,8,3,3,0,0,0]}},{"caption":"Result Interpretation","key":"sec-benchmarking-ai-result-interpretation-86d1","order":{"number":26,"section":[1,5,8,0,0,0,0]}},{"caption":"Fault Tolerance & Robustness","key":"sec-benchmarking-ai-fault-tolerance-robustness-0cf1","order":{"number":43,"section":[1,7,2,5,0,0,0]}},{"caption":"Training Metrics","key":"sec-benchmarking-ai-training-metrics-dc97","order":{"number":38,"section":[1,7,2,0,0,0,0]}},{"caption":"Energy Benchmarks","key":"sec-benchmarking-ai-energy-benchmarks-1d4a","order":{"number":5,"section":[1,2,2,0,0,0,0]}},{"caption":"MLPerf Client","key":"sec-benchmarking-ai-mlperf-client-16ec","order":{"number":83,"section":[1,8,4,3,0,0,0]}},{"caption":"MLPerf Mobile","key":"sec-benchmarking-ai-mlperf-mobile-9cce","order":{"number":82,"section":[1,8,4,2,0,0,0]}},{"caption":"Macro Benchmarks","key":"sec-benchmarking-ai-macro-benchmarks-3daf","order":{"number":15,"section":[1,4,2,0,0,0,0]}},{"caption":"Data Benchmarks","key":"sec-benchmarking-ai-data-benchmarks-0a38","order":{"number":11,"section":[1,3,4,0,0,0,0]}},{"caption":"Summary","key":"sec-benchmarking-ai-summary-52a3","order":{"number":108,"section":[1,14,0,0,0,0,0]}},{"caption":"System Design Challenges","key":"sec-benchmarking-ai-system-design-challenges-7652","order":{"number":94,"section":[1,10,3,0,0,0,0]}},{"caption":"Isolated Metrics Evaluation","key":"sec-benchmarking-ai-isolated-metrics-evaluation-b192","order":{"number":75,"section":[1,8,3,3,4,0,0]}},{"caption":"Training Benchmark Dimensions: Key categories and metrics for comprehensively evaluating machine learning training systems, moving beyond simple speed to assess resource efficiency, reproducibility, and overall performance tradeoffs. understanding these dimensions enables systematic comparison of different training approaches and infrastructure configurations.","key":"tbl-training-metrics","order":{"number":2,"section":[1,7,3,0,0,0,0]}},{"caption":"Ignoring Cold-Start Performance","key":"sec-benchmarking-ai-ignoring-coldstart-performance-319c","order":{"number":74,"section":[1,8,3,3,3,0,0]}},{"caption":"Evaluation Metrics","key":"sec-benchmarking-ai-evaluation-metrics-ea0b","order":{"number":22,"section":[1,5,4,0,0,0,0]}},{"caption":"Inference Performance Metrics: Evaluating latency, throughput, and resource usage provides a quantitative basis for optimizing deployed machine learning systems and selecting appropriate hardware configurations. Understanding these metrics and the trade-offs between them is crucial for balancing speed, cost, and accuracy in real-world applications.","key":"tbl-inference-metrics","order":{"number":3,"section":[1,8,3,0,0,0,0]}},{"caption":"Granularity Trade-offs and Selection Criteria","key":"sec-benchmarking-ai-granularity-tradeoffs-selection-criteria-cee4","order":{"number":17,"section":[1,4,4,0,0,0,0]}},{"caption":"Throughput & Batch Efficiency","key":"sec-benchmarking-ai-throughput-batch-efficiency-91d2","order":{"number":62,"section":[1,8,2,2,0,0,0]}},{"caption":"Cost & Energy Factors","key":"sec-benchmarking-ai-cost-energy-factors-b86f","order":{"number":58,"section":[1,8,1,4,0,0,0]}},{"caption":"Overemphasis on Average Latency","key":"sec-benchmarking-ai-overemphasis-average-latency-232d","order":{"number":72,"section":[1,8,3,3,1,0,0]}},{"caption":"Time and Throughput","key":"sec-benchmarking-ai-time-throughput-cc05","order":{"number":39,"section":[1,7,2,1,0,0,0]}},{"caption":"Benchmarking Granularity Levels: Different benchmark scopes (micro, macro, and end-to-end) target distinct stages of ML system development and reveal unique performance bottlenecks. Micro-benchmarks isolate individual operations for low-level optimization, macro-benchmarks evaluate complete models to guide architectural choices, and end-to-end benchmarks assess full system performance in production environments.","key":"tbl-benchmark-comparison","order":{"number":1,"section":[1,4,4,0,0,0,0]}},{"caption":"Hardware & Software Optimization","key":"sec-benchmarking-ai-hardware-software-optimization-6728","order":{"number":56,"section":[1,8,1,2,0,0,0]}},{"caption":"Dynamic Workload Scaling","key":"sec-benchmarking-ai-dynamic-workload-scaling-53c9","order":{"number":66,"section":[1,8,2,6,0,0,0]}},{"caption":"Cold-Start & Model Load Time","key":"sec-benchmarking-ai-coldstart-model-load-time-ec33","order":{"number":65,"section":[1,8,2,5,0,0,0]}},{"caption":"Overemphasis on Raw Throughput","key":"sec-benchmarking-ai-overemphasis-raw-throughput-edd7","order":{"number":47,"section":[1,7,3,1,1,0,0]}},{"caption":"Inference Benchmark Motivation","key":"sec-benchmarking-ai-inference-benchmark-motivation-9d45","order":{"number":54,"section":[1,8,1,0,0,0,0]}},{"caption":"Run Rules","key":"sec-benchmarking-ai-run-rules-af5c","order":{"number":25,"section":[1,5,7,0,0,0,0]}},{"caption":"Laboratory-to-Deployment Performance Gaps","key":"sec-benchmarking-ai-laboratorytodeployment-performance-gaps-42a2","order":{"number":93,"section":[1,10,2,0,0,0,0]}},{"caption":"Performance Metric Priorities by Deployment Context: Different operational environments demand distinct optimization focuses, reflecting varying constraints and success criteria. Understanding these priorities guides both benchmark selection and result interpretation within appropriate decision frameworks.","key":"tbl-metric-priorities","order":{"number":4,"section":[1,8,3,2,0,0,0]}},{"caption":"Benchmark Granularity Trade-offs: The core trade-off in benchmarking granularity between isolation/diagnostic power and real-world representativeness. Micro-benchmarks provide high diagnostic precision but limited real-world relevance, while end-to-end benchmarks capture realistic system behavior but offer less precise component-level insights. Effective ML system evaluation requires strategic combination of all three levels.","key":"fig-benchmark-tradeoffs","order":{"number":4,"section":[1,4,4,0,0,0,0]}},{"caption":"Inference Metrics","key":"sec-benchmarking-ai-inference-metrics-34bd","order":{"number":60,"section":[1,8,2,0,0,0,0]}},{"caption":"Reproducibility & Standardization","key":"sec-benchmarking-ai-reproducibility-standardization-cbd1","order":{"number":44,"section":[1,7,2,6,0,0,0]}},{"caption":"ImageNet Challenge Progression: Neural networks have reduced error rates from 25.8% in 2010 to 3.57% by 2015, highlighting the impact of architectural advancements on classification accuracy.","key":"fig-imagenet-challenge","order":{"number":1,"section":[1,3,2,0,0,0,0]}},{"caption":"Context-Dependent Metrics","key":"sec-benchmarking-ai-contextdependent-metrics-620b","order":{"number":70,"section":[1,8,3,2,0,0,0]}},{"caption":"Memory Footprint & Model Size","key":"sec-benchmarking-ai-memory-footprint-model-size-8176","order":{"number":64,"section":[1,8,2,4,0,0,0]}},{"caption":"MLPerf Inference","key":"sec-benchmarking-ai-mlperf-inference-da8b","order":{"number":81,"section":[1,8,4,1,0,0,0]}},{"caption":"MLPerf Training Progress: Standardized benchmarks reveal that machine learning training performance consistently surpasses moore’s law, indicating substantial gains from systems-level optimizations. These trends emphasize how focused measurement and iterative improvement drive rapid advancements in ML training efficiency and scalability. Source: [@tschand2024mlperf].","key":"fig-mlperf-training-improve","order":{"number":6,"section":[1,7,1,0,0,0,0]}},{"caption":"Benchmarking AI","key":"sec-benchmarking-ai","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Training Benchmark Motivation","key":"sec-benchmarking-ai-training-benchmark-motivation-1224","order":{"number":32,"section":[1,7,1,0,0,0,0]}},{"caption":"Training Benchmark Synthesis","key":"sec-benchmarking-ai-training-benchmark-synthesis-4f09","order":{"number":52,"section":[1,7,3,2,0,0,0]}},{"caption":"Ignoring Memory & Energy Constraints","key":"sec-benchmarking-ai-ignoring-memory-energy-constraints-8878","order":{"number":73,"section":[1,8,3,3,2,0,0]}},{"caption":"Bias and Over-Optimization","key":"sec-benchmarking-ai-bias-overoptimization-2240","order":{"number":99,"section":[1,10,4,2,0,0,0]}},{"caption":"Energy Efficiency Gains: Successive MLPerf inference benchmark versions consistently improve energy efficiency (samples/watt) across diverse system scales (datacenter, edge, and tiny), reflecting ongoing advancements in both hardware and software optimization for AI workloads. Standardized measurement protocols enable meaningful comparisons of energy efficiency improvements across different AI systems and deployment scenarios, driving sector-wide progress toward sustainable AI technologies. Source: [@tschand2024mlperf].","key":"fig-power-trends","order":{"number":9,"section":[1,9,4,0,0,0,0]}},{"caption":"System Benchmarks","key":"sec-benchmarking-ai-system-benchmarks-46fa","order":{"number":10,"section":[1,3,3,0,0,0,0]}},{"caption":"Machine Learning Benchmarks","key":"sec-benchmarking-ai-machine-learning-benchmarks-6b88","order":{"number":7,"section":[1,3,0,0,0,0,0]}},{"caption":"Hardware-Dependent Accuracy: Model performance varies significantly across hardware platforms, indicating that architectural efficiency is not solely determined by design but also by hardware compatibility. Multi-hardware models exhibit comparable accuracy to mobilenetv3 large on CPU and GPU configurations, yet achieve substantial gains on EdgeTPU and DSP, emphasizing the importance of hardware-aware model optimization for specialized computing environments. Source: [@chu2021discovering].","key":"fig-hw-lottery","order":{"number":10,"section":[1,10,3,2,0,0,0]}},{"caption":"Community-Driven Standardization","key":"sec-benchmarking-ai-communitydriven-standardization-3a09","order":{"number":12,"section":[1,3,5,0,0,0,0]}},{"caption":"Energy Consumption: The figure emphasizes the significant differences in power usage across various system types, from microwatts to megawatts, emphasizing the trade-offs between latency, cost, and energy efficiency in inference benchmarks.","key":"fig-power-differentials","order":{"number":7,"section":[1,8,0,0,0,0,0]}},{"caption":"Benchmark Components","key":"sec-benchmarking-ai-benchmark-components-1bf1","order":{"number":18,"section":[1,5,0,0,0,0,0]}},{"caption":"Ignoring Failures & Interference","key":"sec-benchmarking-ai-ignoring-failures-interference-3fe3","order":{"number":49,"section":[1,7,3,1,3,0,0]}}],"headings":["sec-benchmarking-ai","purpose","sec-benchmarking-ai-machine-learning-benchmarking-framework-3968","sec-benchmarking-ai-historical-context-1c54","sec-benchmarking-ai-performance-benchmarks-5d0c","sec-benchmarking-ai-energy-benchmarks-1d4a","sec-benchmarking-ai-domainspecific-benchmarks-b62e","sec-benchmarking-ai-machine-learning-benchmarks-6b88","sec-benchmarking-ai-ml-measurement-challenges-cc7a","sec-benchmarking-ai-algorithmic-benchmarks-8a54","sec-benchmarking-ai-system-benchmarks-46fa","sec-benchmarking-ai-data-benchmarks-0a38","sec-benchmarking-ai-communitydriven-standardization-3a09","sec-benchmarking-ai-benchmarking-granularity-771c","sec-benchmarking-ai-micro-benchmarks-ab67","sec-benchmarking-ai-macro-benchmarks-3daf","sec-benchmarking-ai-endtoend-benchmarks-1b01","sec-benchmarking-ai-granularity-tradeoffs-selection-criteria-cee4","sec-benchmarking-ai-benchmark-components-1bf1","sec-benchmarking-ai-problem-definition-ea4e","sec-benchmarking-ai-standardized-datasets-d6e9","sec-benchmarking-ai-model-selection-581b","sec-benchmarking-ai-evaluation-metrics-ea0b","sec-benchmarking-ai-benchmark-harness-a5eb","sec-benchmarking-ai-system-specifications-79a9","sec-benchmarking-ai-run-rules-af5c","sec-benchmarking-ai-result-interpretation-86d1","sec-benchmarking-ai-example-benchmark-e3a1","sec-benchmarking-ai-compression-benchmarks-42c9","sec-benchmarking-ai-mobile-edge-benchmarks-9a94","sec-benchmarking-ai-training-vs-inference-evaluation-cee8","sec-benchmarking-ai-training-benchmarks-7533","sec-benchmarking-ai-training-benchmark-motivation-1224","sec-benchmarking-ai-importance-training-benchmarks-5d95","sec-benchmarking-ai-hardware-software-optimization-4f19","sec-benchmarking-ai-scalability-efficiency-18ff","sec-benchmarking-ai-cost-energy-factors-8e47","sec-benchmarking-ai-fair-ml-systems-comparison-cd73","sec-benchmarking-ai-training-metrics-dc97","sec-benchmarking-ai-time-throughput-cc05","sec-benchmarking-ai-scalability-parallelism-cbc4","sec-benchmarking-ai-resource-utilization-20c7","sec-benchmarking-ai-energy-efficiency-cost-c03c","sec-benchmarking-ai-fault-tolerance-robustness-0cf1","sec-benchmarking-ai-reproducibility-standardization-cbd1","sec-benchmarking-ai-training-performance-evaluation-0876","sec-benchmarking-ai-training-benchmark-pitfalls-749a","sec-benchmarking-ai-overemphasis-raw-throughput-edd7","sec-benchmarking-ai-isolated-singlenode-performance-88b7","sec-benchmarking-ai-ignoring-failures-interference-3fe3","sec-benchmarking-ai-linear-scaling-assumption-4e28","sec-benchmarking-ai-ignoring-reproducibility-091a","sec-benchmarking-ai-training-benchmark-synthesis-4f09","sec-benchmarking-ai-inference-benchmarks-433b","sec-benchmarking-ai-inference-benchmark-motivation-9d45","sec-benchmarking-ai-importance-inference-benchmarks-2774","sec-benchmarking-ai-hardware-software-optimization-6728","sec-benchmarking-ai-scalability-efficiency-ddbb","sec-benchmarking-ai-cost-energy-factors-b86f","sec-benchmarking-ai-fair-ml-systems-comparison-bdf8","sec-benchmarking-ai-inference-metrics-34bd","sec-benchmarking-ai-latency-tail-latency-d5dc","sec-benchmarking-ai-throughput-batch-efficiency-91d2","sec-benchmarking-ai-precision-accuracy-tradeoffs-828e","sec-benchmarking-ai-memory-footprint-model-size-8176","sec-benchmarking-ai-coldstart-model-load-time-ec33","sec-benchmarking-ai-dynamic-workload-scaling-53c9","sec-benchmarking-ai-energy-consumption-efficiency-ad66","sec-benchmarking-ai-inference-performance-evaluation-cc51","sec-benchmarking-ai-inference-systems-considerations-dfc6","sec-benchmarking-ai-contextdependent-metrics-620b","sec-benchmarking-ai-inference-benchmark-pitfalls-e4c8","sec-benchmarking-ai-overemphasis-average-latency-232d","sec-benchmarking-ai-ignoring-memory-energy-constraints-8878","sec-benchmarking-ai-ignoring-coldstart-performance-319c","sec-benchmarking-ai-isolated-metrics-evaluation-b192","sec-benchmarking-ai-linear-scaling-assumption-b625","sec-benchmarking-ai-ignoring-application-requirements-b36d","sec-benchmarking-ai-statistical-significance-noise-5c2a","sec-benchmarking-ai-inference-benchmark-synthesis-36cc","sec-benchmarking-ai-mlperf-inference-benchmarks-65b1","sec-benchmarking-ai-mlperf-inference-da8b","sec-benchmarking-ai-mlperf-mobile-9cce","sec-benchmarking-ai-mlperf-client-16ec","sec-benchmarking-ai-mlperf-tiny-ca0d","sec-benchmarking-ai-evolution-future-directions-d2cf","sec-benchmarking-ai-power-measurement-techniques-ed95","sec-benchmarking-ai-power-measurement-boundaries-8429","sec-benchmarking-ai-computational-efficiency-vs-power-consumption-714c","sec-benchmarking-ai-standardized-power-measurement-adf5","sec-benchmarking-ai-mlperf-power-case-study-28ae","sec-benchmarking-ai-benchmarking-limitations-best-practices-9b2a","sec-benchmarking-ai-statistical-methodological-issues-56f4","sec-benchmarking-ai-laboratorytodeployment-performance-gaps-42a2","sec-benchmarking-ai-system-design-challenges-7652","sec-benchmarking-ai-environmental-conditions-6a45","sec-benchmarking-ai-hardware-lottery-22ae","sec-benchmarking-ai-organizational-strategic-issues-9063","sec-benchmarking-ai-benchmark-engineering-99d3","sec-benchmarking-ai-bias-overoptimization-2240","sec-benchmarking-ai-benchmark-evolution-c9d1","sec-benchmarking-ai-mlperf-industry-standard-0883","sec-benchmarking-ai-model-data-benchmarking-f058","sec-benchmarking-ai-model-benchmarking-17aa","sec-benchmarking-ai-data-benchmarking-2795","sec-benchmarking-ai-holistic-systemmodeldata-evaluation-ae59","sec-benchmarking-ai-production-environment-evaluation-7512","sec-benchmarking-ai-fallacies-pitfalls-620e","sec-benchmarking-ai-summary-52a3","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}