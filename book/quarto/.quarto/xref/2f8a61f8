{"entries":[{"caption":"Dense Layer Definition: Defines a dense layer using a high-level API, illustrating how neural networks implement parallel transformations across input tensors.","key":"lst-dense_layer_def","order":{"number":1,"section":[1,3,0,0,0,0,0]}},{"caption":"Memory Planning","key":"sec-ai-acceleration-memory-planning-fb9f","order":{"number":109,"section":[1,7,5,0,0,0,0]}},{"caption":"Non-Linear Functions","key":"sec-ai-acceleration-nonlinear-functions-fdce","order":{"number":20,"section":[1,3,3,1,0,0,0]}},{"caption":"Hardware Acceleration: Single-cycle non-linear operations enable efficient vector processing in ML accelerators, showcasing how specialized hardware reduces computational latency.","key":"lst-sfu_vector_ops","order":{"number":14,"section":[1,3,3,3,0,0,0]}},{"caption":"Memory Allocation","key":"sec-ai-acceleration-memory-allocation-e095","order":{"number":62,"section":[1,5,2,0,0,0,0]}},{"caption":"Data Transfer Patterns","key":"sec-ai-acceleration-data-transfer-patterns-689a","order":{"number":45,"section":[1,4,4,1,0,0,0]}},{"caption":"Output Stationary Execution: Accumulates partial sums locally to reduce memory writes and enhance efficiency during matrix multiplication, making it ideal for transformer-based models.","key":"lst-output_stationary","order":{"number":20,"section":[1,6,1,1,2,0,0]}},{"caption":"AI Memory Wall: The figure emphasizes the growing disparity between model sizes and hardware memory bandwidths, illustrating the challenge in sustaining performance as models become more complex.","key":"fig-memory-wall","order":{"number":6,"section":[1,4,1,3,0,0,0]}},{"caption":"Numerics in AI Acceleration","key":"sec-ai-acceleration-numerics-ai-acceleration-f7be","order":{"number":30,"section":[1,3,4,6,0,0,0]}},{"caption":"Linear Layers: Layer transformations combine input features to produce hidden representations. Matrix operations in neural networks enable efficient feature extraction and transformation, forming the backbone of many machine learning architectures.","key":"lst-matrix_patterns","order":{"number":9,"section":[1,3,2,2,0,0,0]}},{"caption":"Data Layout Strategies: Row-major (NHWC) and channel-major (NCHW) layouts optimize memory access patterns for different hardware architectures; NHWC suits cpus and element-wise operations, while NCHW accelerates GPU and TPU-based convolution operations. Choosing the appropriate layout significantly impacts performance by maximizing cache utilization and memory bandwidth efficiency.","key":"tbl-major","order":{"number":14,"section":[1,6,1,2,3,0,0]}},{"caption":"Hardware Mapping Fundamentals for Neural Networks","key":"sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9","order":{"number":57,"section":[1,5,0,0,0,0,0]}},{"caption":"Vectorized Multiply-Accumulate Loop: This loop showcases how RISC-V vector instructions enable efficient batch processing by performing 8 multiply-add operations simultaneously, reducing computational latency in neural network training. Source: RISC-V Architecture Manual","key":"lst-riscv_vector_mac","order":{"number":7,"section":[1,3,1,3,0,0,0]}},{"caption":"Memory Bandwidth and Architectural Trade-offs","key":"sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c","order":{"number":43,"section":[1,4,3,0,0,0,0]}},{"caption":"Machine Learning Hardware Specialization","key":"sec-ai-acceleration-machine-learning-hardware-specialization-a17f","order":{"number":7,"section":[1,2,4,0,0,0,0]}},{"caption":"Effective Computation Placement","key":"sec-ai-acceleration-effective-computation-placement-099d","order":{"number":61,"section":[1,5,1,3,0,0,0]}},{"caption":"PCIe Interface","key":"sec-ai-acceleration-pcie-interface-c5b4","order":{"number":47,"section":[1,4,4,2,1,0,0]}},{"caption":"Operation Characteristics: Matrix operations excel at many-to-many transformations common in neural network layers, while vector operations efficiently handle one-to-one transformations like activation functions and normalization. Understanding these distinctions guides the selection of appropriate computational primitives for different machine learning tasks and impacts system performance.","key":"tbl-matrix","order":{"number":3,"section":[1,3,2,3,0,0,0]}},{"caption":"Multilayer Perceptrons","key":"sec-ai-acceleration-multilayer-perceptrons-0bbc","order":{"number":53,"section":[1,4,5,1,0,0,0]}},{"caption":"Memory Placement and Data Movement","key":"sec-ai-acceleration-memory-placement-data-movement-fd52","order":{"number":69,"section":[1,5,3,4,0,0,0]}},{"caption":"Hardware Specialization Trends: Successive computing eras progressively integrate specialized hardware to accelerate prevalent workloads, moving from general-purpose CPUs to domain-specific architectures and ultimately to customizable AI accelerators. This evolution reflects a fundamental principle: tailoring hardware to computational patterns improves performance and energy efficiency, driving innovation in machine learning systems.","key":"tbl-hw-evolution","order":{"number":1,"section":[1,2,4,0,0,0,0]}},{"caption":"Systolic Array Dataflow: Processing elements within the array execute matrix operations by streaming data in a pipelined manner, maximizing operand reuse and minimizing memory access compared to traditional memory-compute architectures. This spatial and temporal locality enables efficient parallel computation, as exemplified by the multiply-accumulate units in Google’s tpuv4.","key":"fig-systolic-array","order":{"number":4,"section":[1,3,4,5,0,0,0]}},{"caption":"Specialized Computing","key":"sec-ai-acceleration-specialized-computing-1a77","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Power and Thermal Management","key":"sec-ai-acceleration-power-thermal-management-6c00","order":{"number":143,"section":[1,10,3,0,0,0,0]}},{"caption":"Mapping Primitives to Execution Units","key":"sec-ai-acceleration-mapping-primitives-execution-units-ccb6","order":{"number":25,"section":[1,3,4,1,0,0,0]}},{"caption":"Kernel Fusion","key":"sec-ai-acceleration-kernel-fusion-7faf","order":{"number":81,"section":[1,6,1,3,0,0,0]}},{"caption":"Implementation in AI Compilers","key":"sec-ai-acceleration-implementation-ai-compilers-2ae0","order":{"number":110,"section":[1,7,5,1,0,0,0]}},{"caption":"AI Memory Systems","key":"sec-ai-acceleration-ai-memory-systems-0057","order":{"number":35,"section":[1,4,0,0,0,0,0]}},{"caption":"Summary","key":"sec-ai-acceleration-summary-a5f8","order":{"number":147,"section":[1,12,0,0,0,0,0]}},{"caption":"Kernel Selection","key":"sec-ai-acceleration-kernel-selection-df01","order":{"number":106,"section":[1,7,4,0,0,0,0]}},{"caption":"Automotive Heterogeneous AI Systems","key":"sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda","order":{"number":144,"section":[1,10,4,0,0,0,0]}},{"caption":"Channel-Major Layout","key":"sec-ai-acceleration-channelmajor-layout-d6a9","order":{"number":79,"section":[1,6,1,2,2,0,0]}},{"caption":"Comparing Row-Major and Channel-Major Layouts","key":"sec-ai-acceleration-comparing-rowmajor-channelmajor-layouts-e410","order":{"number":80,"section":[1,6,1,2,3,0,0]}},{"caption":"Historical Foundations of Matrix Computation","key":"sec-ai-acceleration-historical-foundations-matrix-computation-402e","order":{"number":18,"section":[1,3,2,4,0,0,0]}},{"caption":"Data Transfer Mechanisms","key":"sec-ai-acceleration-data-transfer-mechanisms-4109","order":{"number":46,"section":[1,4,4,2,0,0,0]}},{"caption":"Vector Operations: Neural network layers frequently utilize core vector operations such as reduction, gather, and scatter to accelerate computation and efficiently process data in parallel; these operations clarify how low-level hardware optimizations map to high-level machine learning algorithms. These operations enable efficient implementation of common layers like pooling, embedding lookups, and attention mechanisms within deep learning models.","key":"tbl-vector","order":{"number":2,"section":[1,3,1,3,0,0,0]}},{"caption":"Hardware Specialization Trajectory: Computing architectures progressively incorporate specialized accelerators to address emerging performance bottlenecks and workload demands, mirroring a historical pattern from floating-point units to graphics processors and, ultimately, machine learning accelerators. This evolution reflects a strategy for improving computational efficiency by tailoring hardware to specific task characteristics and advancing increasingly complex applications.","key":"fig-timeline","order":{"number":1,"section":[1,2,1,0,0,0,0]}},{"caption":"Architectural Integration","key":"sec-ai-acceleration-architectural-integration-01b6","order":{"number":33,"section":[1,3,4,7,0,0,0]}},{"caption":"High-Level Framework Operations","key":"sec-ai-acceleration-highlevel-framework-operations-9248","order":{"number":10,"section":[1,3,1,1,0,0,0]}},{"caption":"Evolution of Hardware Specialization","key":"sec-ai-acceleration-evolution-hardware-specialization-1d21","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"Intermediate Memory Write","key":"sec-ai-acceleration-intermediate-memory-write-f140","order":{"number":82,"section":[1,6,1,3,1,0,0]}},{"caption":"Linear Layer Computation: Each output neuron is computed by summing weighted inputs from all features, followed by an activation function application. Understanding this process helps in grasping the fundamental building blocks of neural networks.","key":"lst-loop_linear_layer","order":{"number":6,"section":[1,3,1,1,0,0,0]}},{"caption":"Multi-Chip Adaptations: Efficient AI execution on multiple accelerators requires coordinated adjustments to computation placement, memory management, and scheduling to balance workload distribution and minimize communication overhead. Compilers and runtimes extend their capabilities to dynamically adapt to system state and network congestion, enabling scalable and performant multi-chip AI systems.","key":"tbl-scaling-adaptations","order":{"number":21,"section":[1,9,6,4,0,0,0]}},{"caption":"Dynamic Kernel Execution","key":"sec-ai-acceleration-dynamic-kernel-execution-33fc","order":{"number":119,"section":[1,8,2,0,0,0,0]}},{"caption":"Heterogeneous SoC AI Acceleration","key":"sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb","order":{"number":140,"section":[1,10,0,0,0,0,0]}},{"caption":"ML Model Memory Access: Different machine learning models exhibit distinct memory access patterns and bottlenecks due to variations in weight size, activation reuse, and data sparsity; these characteristics significantly impact hardware accelerator design and performance optimization. Transformers demand high bandwidth and capacity due to their massive, sparsely accessed weights, while cnns benefit from spatial locality and high activation reuse, reducing memory pressure.","key":"tbl-model-mem-compare","order":{"number":10,"section":[1,4,6,0,0,0,0]}},{"caption":"Cost-Performance Analysis","key":"sec-ai-acceleration-costperformance-analysis-e925","order":{"number":34,"section":[1,3,5,0,0,0,0]}},{"caption":"Tiling Strategies: Spatial, temporal, and hybrid tiling optimize memory access patterns for improved performance; spatial tiling maximizes data reuse within fast memory, temporal tiling exploits loop structure for reduced accesses, and hybrid tiling combines both approaches to balance computational efficiency and memory bandwidth. These techniques are crucial for AI compilers and runtime systems to automatically optimize model execution on diverse hardware.","key":"tbl-tiling-strategies","order":{"number":17,"section":[1,6,1,4,4,0,0]}},{"caption":"Data Movement Constraints","key":"sec-ai-acceleration-data-movement-constraints-a823","order":{"number":132,"section":[1,9,6,3,0,0,0]}},{"caption":"Naïve Execution: Each step writes intermediate results to memory before processing the next, leading to increased bandwidth usage and reduced efficiency. Source: NVIDIA GPU Technology Conference 2017[nvidia2017gpu]","key":"lst-naive_execution","order":{"number":22,"section":[1,6,1,3,1,0,0]}},{"caption":"Host-Accelerator Data Transfer: AI workloads require frequent data movement between CPU memory and accelerators; this figure details the sequential steps of copying input data, executing computation, and transferring results, each introducing potential performance bottlenecks. Understanding this data transfer sequence is crucial for optimizing AI system performance and minimizing latency.","key":"fig-host-accelerator-data-movement","order":{"number":7,"section":[1,4,4,0,0,0,0]}},{"caption":"Compiler Optimization Priorities: Traditional and machine learning compilers diverge in their optimization targets; traditional compilers prioritize efficient execution of sequential code, while ML compilers focus on optimizing tensor operations within computation graphs for specialized hardware. This table clarifies how ML compilers incorporate domain-specific transformations—like kernel fusion and memory-aware scheduling—to achieve high performance on accelerators, unlike the instruction scheduling and register allocation techniques used in conventional software compilation.","key":"tbl-ml-vs-traditional-compilers","order":{"number":18,"section":[1,7,1,0,0,0,0]}},{"caption":"Precision Trade-offs","key":"sec-ai-acceleration-precision-tradeoffs-8fa8","order":{"number":31,"section":[1,3,4,6,1,0,0]}},{"caption":"Memory Planning Importance","key":"sec-ai-acceleration-memory-planning-importance-e987","order":{"number":111,"section":[1,7,5,2,0,0,0]}},{"caption":"ML Accelerators Implications","key":"sec-ai-acceleration-ml-accelerators-implications-c962","order":{"number":56,"section":[1,4,6,0,0,0,0]}},{"caption":"Model Memory Pressure","key":"sec-ai-acceleration-model-memory-pressure-f95e","order":{"number":52,"section":[1,4,5,0,0,0,0]}},{"caption":"Parallelization Across Processing Elements","key":"sec-ai-acceleration-parallelization-across-processing-elements-90d6","order":{"number":68,"section":[1,5,3,3,0,0,0]}},{"caption":"Software Stack Challenges","key":"sec-ai-acceleration-software-stack-challenges-255c","order":{"number":145,"section":[1,10,5,0,0,0,0]}},{"caption":"Strategies for Dynamic Workload Distribution","key":"sec-ai-acceleration-strategies-dynamic-workload-distribution-a421","order":{"number":142,"section":[1,10,2,0,0,0,0]}},{"caption":"Data Movement Patterns","key":"sec-ai-acceleration-data-movement-patterns-3b06","order":{"number":73,"section":[1,6,1,1,0,0,0]}},{"caption":"Scaling Efficiency of TPU Pods: Increasing the number of TPU chips within a pod maintains near-linear performance gains on ResNet-50, achieving a 33.0\\times speedup from 16 to 1024 chips. This efficient scaling provides the effectiveness of the 2D torus interconnect and high-bandwidth optical links in minimizing communication bottlenecks as workloads expand across multiple accelerators.","key":"fig-tpu-pod-perf","order":{"number":11,"section":[1,9,3,0,0,0,0]}},{"caption":"Multi-GPU Scaling: NVSwitch interconnects enable high-bandwidth, low-latency communication between GPUs, overcoming PCIe bottlenecks for distributed training of large models. Scaling GPU count introduces challenges in maintaining memory consistency and efficiently scheduling workloads across interconnected devices.","key":"fig-multi-gpu","order":{"number":10,"section":[1,9,2,0,0,0,0]}},{"caption":"Computation Scheduling","key":"sec-ai-acceleration-computation-scheduling-7ccd","order":{"number":112,"section":[1,7,6,0,0,0,0]}},{"caption":"Nested Loops: Computes output values through sequential matrix multiplications and bias additions, followed by activation function application to produce final outputs.","key":"lst-loop_level_dense","order":{"number":3,"section":[1,3,0,0,0,0,0]}},{"caption":"AI Memory Wall: The growing disparity between compute performance and memory bandwidth emphasizes the increasing challenge in sustaining peak computational efficiency due to memory constraints. Over the past 20 years, while computational capabilities have advanced rapidly, memory bandwidth has not kept pace, leading to potential bottlenecks in data-intensive applications.","key":"fig-compute-memory-imbalance","order":{"number":5,"section":[1,4,1,1,0,0,0]}},{"caption":"Graph Optimization","key":"sec-ai-acceleration-graph-optimization-f888","order":{"number":102,"section":[1,7,3,0,0,0,0]}},{"caption":"Memory-Efficient Tensor Layouts","key":"sec-ai-acceleration-memoryefficient-tensor-layouts-e250","order":{"number":77,"section":[1,6,1,2,0,0,0]}},{"caption":"Vector Operations","key":"sec-ai-acceleration-vector-operations-729f","order":{"number":9,"section":[1,3,1,0,0,0,0]}},{"caption":"Memory Allocation Challenges: Efficient memory management in AI accelerators balances data access speed with hardware constraints, mitigating performance bottlenecks caused by latency, bandwidth limitations, and irregular data patterns. Addressing these challenges is critical for deploying complex models, such as transformers and graphs, which have variable and demanding memory requirements.","key":"tbl-memory-allocation","order":{"number":12,"section":[1,5,2,2,0,0,0]}},{"caption":"Multi-Layer Perceptrons","key":"sec-ai-acceleration-multilayer-perceptrons-eb18","order":{"number":95,"section":[1,6,2,3,0,0,0]}},{"caption":"Computation Placement Adaptation","key":"sec-ai-acceleration-computation-placement-adaptation-9a3c","order":{"number":138,"section":[1,9,7,4,0,0,0]}},{"caption":"Naïve Matrix Multiplication: This code directly implements matrix multiplication using nested loops, showing how each element in the output matrix is computed as a sum of products from corresponding elements in the input matrices.","key":"lst-naive_matmul_repeat","order":{"number":24,"section":[1,6,1,4,1,0,0]}},{"caption":"Off-Chip Memory","key":"sec-ai-acceleration-offchip-memory-ecdb","order":{"number":42,"section":[1,4,2,2,0,0,0]}},{"caption":"Applying Mapping Strategies to Neural Networks","key":"sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110","order":{"number":92,"section":[1,6,2,0,0,0,0]}},{"caption":"Building Blocks of Mapping Strategies","key":"sec-ai-acceleration-building-blocks-mapping-strategies-4932","order":{"number":72,"section":[1,6,1,0,0,0,0]}},{"caption":"Cross-Accelerator Scheduling","key":"sec-ai-acceleration-crossaccelerator-scheduling-4ac0","order":{"number":135,"section":[1,9,7,1,0,0,0]}},{"caption":"Data Transfer Overheads","key":"sec-ai-acceleration-data-transfer-overheads-fbc9","order":{"number":51,"section":[1,4,4,3,0,0,0]}},{"caption":"Computation Placement Definition","key":"sec-ai-acceleration-computation-placement-definition-e130","order":{"number":59,"section":[1,5,1,1,0,0,0]}},{"caption":"Processing Elements","key":"sec-ai-acceleration-processing-elements-daa1","order":{"number":28,"section":[1,3,4,4,0,0,0]}},{"caption":"Special Function Units","key":"sec-ai-acceleration-special-function-units-ed00","order":{"number":19,"section":[1,3,3,0,0,0,0]}},{"caption":"Wafer-Scale Integration: Wafer-scale AI processors integrate trillions of transistors onto a single wafer, offering ultra-fast on-die communication to surpass traditional multi-chip architectures and achieve unprecedented performance levels.","key":"fig-processor-trends","order":{"number":12,"section":[1,9,4,0,0,0,0]}},{"caption":"Execution Models Adaptation","key":"sec-ai-acceleration-execution-models-adaptation-344b","order":{"number":134,"section":[1,9,7,0,0,0,0]}},{"caption":"Unified Memory","key":"sec-ai-acceleration-unified-memory-b18f","order":{"number":50,"section":[1,4,4,2,4,0,0]}},{"caption":"Intermediate Tensor Storage: Naïve execution models require substantial memory to store intermediate tensors generated by each operation; for a 1024x1024 tensor, this table shows that storing these intermediate results—even if only the final output is needed—quadruples the total memory footprint from 4 MB to 16 MB. Minimizing this intermediate data storage is crucial for improving memory efficiency and accelerating AI computations.","key":"tbl-memory-footprint","order":{"number":15,"section":[1,6,1,3,1,0,0]}},{"caption":"Sequential Scalar Execution","key":"sec-ai-acceleration-sequential-scalar-execution-d063","order":{"number":11,"section":[1,3,1,2,0,0,0]}},{"caption":"Distributed Access Memory Allocation","key":"sec-ai-acceleration-distributed-access-memory-allocation-d970","order":{"number":131,"section":[1,9,6,2,0,0,0]}},{"caption":"Tiling Methods","key":"sec-ai-acceleration-tiling-methods-6257","order":{"number":88,"section":[1,6,1,4,3,0,0]}},{"caption":"Multi-chip Execution Mapping","key":"sec-ai-acceleration-multichip-execution-mapping-8ec9","order":{"number":130,"section":[1,9,6,1,0,0,0]}},{"caption":"Computation and Memory Scaling Changes","key":"sec-ai-acceleration-computation-memory-scaling-changes-2bb1","order":{"number":129,"section":[1,9,6,0,0,0,0]}},{"caption":"AI Acceleration Trends: Scaling AI systems provides increasing challenges in data movement and memory access, driving architectural innovations from chiplets to wafer-scale integration. Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution.","key":"tbl-scaling-trajectory","order":{"number":20,"section":[1,9,5,0,0,0,0]}},{"caption":"AI Systems Scaling Trajectory","key":"sec-ai-acceleration-ai-systems-scaling-trajectory-ad73","order":{"number":128,"section":[1,9,5,0,0,0,0]}},{"caption":"Memory Access Characteristics: Traditional workloads exhibit predictable, sequential memory access, benefiting from standard caching, while machine learning workloads introduce irregular and dynamic patterns due to sparsity and data dependencies that challenge conventional memory optimization techniques. Understanding these differences is crucial for designing memory systems that efficiently support the unique demands of modern AI applications.","key":"tbl-traditional-vs-ml-mem","order":{"number":8,"section":[1,4,1,3,0,0,0]}},{"caption":"Kernel Scheduling and Utilization","key":"sec-ai-acceleration-kernel-scheduling-utilization-99d6","order":{"number":121,"section":[1,8,4,0,0,0,0]}},{"caption":"Wafer-Scale AI","key":"sec-ai-acceleration-waferscale-ai-6420","order":{"number":127,"section":[1,9,4,0,0,0,0]}},{"caption":"Dataflow Optimization Strategies","key":"sec-ai-acceleration-dataflow-optimization-strategies-ce52","order":{"number":71,"section":[1,6,0,0,0,0,0]}},{"caption":"AI Hardware Acceleration Fundamentals","key":"sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Mobile SoC Architecture Evolution","key":"sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8","order":{"number":141,"section":[1,10,1,0,0,0,0]}},{"caption":"TPU Pods","key":"sec-ai-acceleration-tpu-pods-fd33","order":{"number":126,"section":[1,9,3,0,0,0,0]}},{"caption":"Combinatorial Complexity","key":"sec-ai-acceleration-combinatorial-complexity-ea33","order":{"number":65,"section":[1,5,3,0,0,0,0]}},{"caption":"Communication Overhead and Amdahl’s Law Analysis","key":"sec-ai-acceleration-communication-overhead-amdahls-law-analysis-8eb4","order":{"number":125,"section":[1,9,2,1,0,0,0]}},{"caption":"Multi-Chip AI Acceleration","key":"sec-ai-acceleration-multichip-ai-acceleration-38d7","order":{"number":122,"section":[1,9,0,0,0,0,0]}},{"caption":"Navigating Multi-Chip AI Complexities","key":"sec-ai-acceleration-navigating-multichip-ai-complexities-83bd","order":{"number":139,"section":[1,9,8,0,0,0,0]}},{"caption":"Input Stationary: This approach keeps input activations stationary while dynamically streaming weights to maximize memory reuse and reduce energy consumption.","key":"lst-input_stationary","order":{"number":21,"section":[1,6,1,1,3,0,0]}},{"caption":"Multi-GPU Systems","key":"sec-ai-acceleration-multigpu-systems-f017","order":{"number":124,"section":[1,9,2,0,0,0,0]}},{"caption":"Hardware Implementation of Non-Linear Functions","key":"sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d","order":{"number":21,"section":[1,3,3,2,0,0,0]}},{"caption":"Chiplet-Based Architectures","key":"sec-ai-acceleration-chipletbased-architectures-a890","order":{"number":123,"section":[1,9,1,0,0,0,0]}},{"caption":"Tiling Fundamentals","key":"sec-ai-acceleration-tiling-fundamentals-e9e6","order":{"number":86,"section":[1,6,1,4,1,0,0]}},{"caption":"Compute Units and Execution Models","key":"sec-ai-acceleration-compute-units-execution-models-f406","order":{"number":24,"section":[1,3,4,0,0,0,0]}},{"caption":"Code Generation","key":"sec-ai-acceleration-code-generation-85c8","order":{"number":115,"section":[1,7,6,3,0,0,0]}},{"caption":"Kernel Fusion for Memory Efficiency","key":"sec-ai-acceleration-kernel-fusion-memory-efficiency-f227","order":{"number":83,"section":[1,6,1,3,2,0,0]}},{"caption":"Runtime Execution Models: Traditional and AI runtimes diverge in their execution approaches; traditional runtimes prioritize sequential or multi-threaded instruction processing, while AI runtimes leverage massively parallel tensor operations for accelerated computation on machine learning workloads. This distinction necessitates specialized AI runtime architectures designed for efficient parallelization and memory management of large-scale tensor data.","key":"tbl-runtime-comparison","order":{"number":19,"section":[1,8,1,0,0,0,0]}},{"caption":"Output Stationary","key":"sec-ai-acceleration-output-stationary-54e5","order":{"number":75,"section":[1,6,1,1,2,0,0]}},{"caption":"Matrix Operations: Neural networks perform transformations using matrix multiplications and biases to achieve output predictions. Training requires careful management of input batches and activation functions to optimize model performance.","key":"lst-linear_matrix_hierarchy","order":{"number":8,"section":[1,3,2,1,0,0,0]}},{"caption":"Computation Placement Challenges: Effective neural network deployment requires strategic allocation of computations to processing elements, balancing workload distribution, data movement costs, and hardware constraints to maximize execution efficiency and avoid performance bottlenecks. Understanding these challenges guides the design of mapping strategies that optimize resource utilization and minimize communication overhead.","key":"tbl-placement-challenges","order":{"number":11,"section":[1,5,1,3,0,0,0]}},{"caption":"Operation Fusion Benefits: Fused execution reduces memory usage by eliminating the need to store intermediate tensors, directly improving efficiency on memory-bound hardware like gpus and tpus. This table quantifies the memory savings, showing a reduction from 16 MB in naïve execution to 4 MB with fused operations.","key":"tbl-fusion-benefits","order":{"number":16,"section":[1,6,1,3,2,0,0]}},{"caption":"Computation Placement Strategies: Multi-chip AI systems necessitate hierarchical workload mapping to minimize communication overhead; compilers adapt single-chip optimization techniques by considering interconnect bandwidth and latency when assigning computations to accelerators. This table contrasts computation placement in single-chip systems—local to processing elements—with multi-chip systems, where placement strategies prioritize efficient data exchange across accelerators.","key":"tbl-computation-placement","order":{"number":22,"section":[1,9,7,4,0,0,0]}},{"caption":"Computation Scheduling Importance","key":"sec-ai-acceleration-computation-scheduling-importance-04a1","order":{"number":114,"section":[1,7,6,2,0,0,0]}},{"caption":"Compilation-Runtime Support","key":"sec-ai-acceleration-compilationruntime-support-0206","order":{"number":116,"section":[1,7,7,0,0,0,0]}},{"caption":"Runtime Support","key":"sec-ai-acceleration-runtime-support-f94f","order":{"number":117,"section":[1,8,0,0,0,0,0]}},{"caption":"Implementation in AI Compilers","key":"sec-ai-acceleration-implementation-ai-compilers-ff25","order":{"number":113,"section":[1,7,6,1,0,0,0]}},{"caption":"AI Compute Primitives","key":"sec-ai-acceleration-ai-compute-primitives-8471","order":{"number":8,"section":[1,3,0,0,0,0,0]}},{"caption":"Spatial Tiling","key":"sec-ai-acceleration-spatial-tiling-247e","order":{"number":89,"section":[1,6,1,4,3,1,0]}},{"caption":"Kernel Selection Importance","key":"sec-ai-acceleration-kernel-selection-importance-3c3f","order":{"number":108,"section":[1,7,4,2,0,0,0]}},{"caption":"Non-Linear Transformations: Neural networks process input data through a sequence of linear transformations followed by non-linear activations to capture complex patterns. This layer sequence enhances model expressiveness and learning capabilities.","key":"lst-nonlinear_layer","order":{"number":11,"section":[1,3,3,1,0,0,0]}},{"caption":"Weight Stationary Matrix Multiplication: Weight stationary matrix multiplication keeps weights fixed in local memory while input activations stream through, demonstrating how it maximizes weight reuse to reduce energy costs.","key":"lst-weight_stationary","order":{"number":19,"section":[1,6,1,1,1,0,0]}},{"caption":"Matrix Operations in Neural Networks","key":"sec-ai-acceleration-matrix-operations-neural-networks-527a","order":{"number":15,"section":[1,3,2,1,0,0,0]}},{"caption":"Memory-Efficient Tiling Strategies","key":"sec-ai-acceleration-memoryefficient-tiling-strategies-9fce","order":{"number":85,"section":[1,6,1,4,0,0,0]}},{"caption":"Implementation in AI Compilers","key":"sec-ai-acceleration-implementation-ai-compilers-c917","order":{"number":107,"section":[1,7,4,1,0,0,0]}},{"caption":"Graph Optimization Importance","key":"sec-ai-acceleration-graph-optimization-importance-9ccb","order":{"number":105,"section":[1,7,3,3,0,0,0]}},{"caption":"Understanding the AI Memory Wall","key":"sec-ai-acceleration-understanding-ai-memory-wall-3ea9","order":{"number":36,"section":[1,4,1,0,0,0,0]}},{"caption":"Matrix Tiling: Partitioning large matrices into smaller tiles optimizes data reuse and reduces memory access overhead during computation. This technique improves performance on AI accelerators by enabling efficient loading and processing of data in fast memory, minimizing transfers from slower main memory.","key":"fig-tiling-diagram","order":{"number":8,"section":[1,6,1,4,0,0,0]}},{"caption":"Memory Hierarchy Trade-Offs: AI accelerators leverage a multi-level memory hierarchy to balance performance and capacity, optimizing data access for computationally intensive machine learning tasks. Each level provides distinct latency, bandwidth, and capacity characteristics that dictate how neural network components—weights, activations, and intermediate results—should be strategically allocated to minimize bottlenecks and maximize throughput.","key":"tbl-memory-heirarchy","order":{"number":9,"section":[1,4,2,0,0,0,0]}},{"caption":"SFUs History","key":"sec-ai-acceleration-sfus-history-a1b6","order":{"number":23,"section":[1,3,3,4,0,0,0]}},{"caption":"Implementation in AI Compilers","key":"sec-ai-acceleration-implementation-ai-compilers-1df9","order":{"number":104,"section":[1,7,3,2,0,0,0]}},{"caption":"Hybrid Mapping Strategies","key":"sec-ai-acceleration-hybrid-mapping-strategies-3e8c","order":{"number":96,"section":[1,6,3,0,0,0,0]}},{"caption":"Transformer Architectures","key":"sec-ai-acceleration-transformer-architectures-8f25","order":{"number":94,"section":[1,6,2,2,0,0,0]}},{"caption":"Computation Graph Optimization","key":"sec-ai-acceleration-computation-graph-optimization-a028","order":{"number":103,"section":[1,7,3,1,0,0,0]}},{"caption":"Runtime Architecture Differences for ML Systems","key":"sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e","order":{"number":118,"section":[1,8,1,0,0,0,0]}},{"caption":"Temporal Tiling: Reduces redundant memory accesses by caching weights in fast memory across multiple matrix multiplications.","key":"lst-loop_blocking","order":{"number":27,"section":[1,6,1,4,3,2,0]}},{"caption":"ML Compilation Pipeline","key":"sec-ai-acceleration-ml-compilation-pipeline-7676","order":{"number":101,"section":[1,7,2,0,0,0,0]}},{"caption":"Matrix Operations","key":"sec-ai-acceleration-matrix-operations-508d","order":{"number":14,"section":[1,3,2,0,0,0,0]}},{"caption":"Memory Challenges for Different Workloads","key":"sec-ai-acceleration-memory-challenges-different-workloads-e87c","order":{"number":64,"section":[1,5,2,2,0,0,0]}},{"caption":"Compiler Design Differences for ML Workloads","key":"sec-ai-acceleration-compiler-design-differences-ml-workloads-0698","order":{"number":100,"section":[1,7,1,0,0,0,0]}},{"caption":"Cross-Accelerator Execution Management","key":"sec-ai-acceleration-crossaccelerator-execution-management-87a9","order":{"number":137,"section":[1,9,7,3,0,0,0]}},{"caption":"Accelerator Cost-Performance Comparison: Hardware costs must be evaluated against computational capabilities to determine optimal deployment strategies. While newer accelerators like H100 offer better price-performance ratios, total cost of ownership includes power consumption, cooling requirements, and infrastructure costs that significantly impact operational economics. *TPU pricing estimated from cloud rates.","key":"tbl-accelerator-economics","order":{"number":7,"section":[1,3,5,0,0,0,0]}},{"caption":"Precision Support Evolution: GPU architectures progressively expanded support for lower-precision data types, enabling performance gains and efficiency improvements in AI workloads. Early architectures primarily utilized FP32, while later generations incorporated FP16, BF16, INT8, and INT4 to accelerate both training and inference tasks.","key":"tbl-nvidia-numerics","order":{"number":5,"section":[1,3,4,6,2,0,0]}},{"caption":"Placement-Allocation Trade-Offs: AI accelerator performance depends on strategically mapping computations to hardware and allocating resources over time, balancing parallelism, memory access, and execution efficiency to avoid bottlenecks. Careful consideration of these interdependent factors is essential for maximizing throughput and minimizing energy consumption in machine learning systems.","key":"tbl-combinatorial-complexity","order":{"number":13,"section":[1,5,3,0,0,0,0]}},{"caption":"Ordering Computation and Execution","key":"sec-ai-acceleration-ordering-computation-execution-7251","order":{"number":67,"section":[1,5,3,2,0,0,0]}},{"caption":"Hardware Implementations of Hybrid Strategies","key":"sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8","order":{"number":98,"section":[1,6,4,0,0,0,0]}},{"caption":"Non-linear Transformations: Neural networks apply linear and non-linear operations to transform input data into meaningful features for learning. Machine learning models leverage these transformations to capture complex patterns in data efficiently.","key":"lst-nonlinear_math","order":{"number":12,"section":[1,3,3,1,0,0,0]}},{"caption":"Systolic Arrays","key":"sec-ai-acceleration-systolic-arrays-6fa8","order":{"number":29,"section":[1,3,4,5,0,0,0]}},{"caption":"Row-Major Layout","key":"sec-ai-acceleration-rowmajor-layout-741f","order":{"number":78,"section":[1,6,1,2,1,0,0]}},{"caption":"Layer-Specific Mapping","key":"sec-ai-acceleration-layerspecific-mapping-1102","order":{"number":97,"section":[1,6,3,1,0,0,0]}},{"caption":"Convolutional Neural Networks","key":"sec-ai-acceleration-convolutional-neural-networks-1e47","order":{"number":93,"section":[1,6,2,1,0,0,0]}},{"caption":"Cross-Accelerator Coordination","key":"sec-ai-acceleration-crossaccelerator-coordination-8aa4","order":{"number":136,"section":[1,9,7,2,0,0,0]}},{"caption":"Tiling Challenges and Trade-offs","key":"sec-ai-acceleration-tiling-challenges-tradeoffs-e9c9","order":{"number":91,"section":[1,6,1,4,4,0,0]}},{"caption":"Types of Matrix Computations in Neural Networks","key":"sec-ai-acceleration-types-matrix-computations-neural-networks-b497","order":{"number":16,"section":[1,3,2,2,0,0,0]}},{"caption":"Tiled Matrix Multiplication: This approach divides matrices into smaller blocks to optimize memory usage by reusing data within processor cache, thereby improving computational efficiency.","key":"lst-tiled_matmul","order":{"number":25,"section":[1,6,1,4,2,0,0]}},{"caption":"Irregular Memory Access","key":"sec-ai-acceleration-irregular-memory-access-c6ec","order":{"number":39,"section":[1,4,1,3,0,0,0]}},{"caption":"Temporal Tiling","key":"sec-ai-acceleration-temporal-tiling-563b","order":{"number":90,"section":[1,6,1,4,3,2,0]}},{"caption":"Performance Benefits of Tiling","key":"sec-ai-acceleration-performance-benefits-tiling-e7bd","order":{"number":87,"section":[1,6,1,4,2,0,0]}},{"caption":"Computation Placement","key":"sec-ai-acceleration-computation-placement-23d2","order":{"number":58,"section":[1,5,1,0,0,0,0]}},{"caption":"Host-Accelerator Communication","key":"sec-ai-acceleration-hostaccelerator-communication-bb7a","order":{"number":44,"section":[1,4,4,0,0,0,0]}},{"caption":"Compiler Support","key":"sec-ai-acceleration-compiler-support-172e","order":{"number":99,"section":[1,7,0,0,0,0,0]}},{"caption":"Runtime Kernel Selection","key":"sec-ai-acceleration-runtime-kernel-selection-1ffe","order":{"number":120,"section":[1,8,3,0,0,0,0]}},{"caption":"Naïve matrix multiplication without tiling","key":"lst-naive_matmul","order":{"number":23,"section":[1,6,1,4,0,0,0]}},{"caption":"Linear Layer: Neural networks transform input data into a higher-dimensional space using linear mappings to enable complex feature extraction.","key":"lst-linear_layer_highlevel","order":{"number":4,"section":[1,3,1,1,0,0,0]}},{"caption":"Performance Benefits and Constraints","key":"sec-ai-acceleration-performance-benefits-constraints-1b74","order":{"number":84,"section":[1,6,1,3,3,0,0]}},{"caption":"Anatomy of a Modern AI Accelerator: AI accelerators integrate specialized processing elements containing tensor cores, vector units, and special function units, supported by a hierarchical memory system from high-bandwidth memory down to local caches. This architecture maximizes data reuse and parallel execution while minimizing energy-intensive data movement, forming the foundation for 100-1000× performance improvements over general-purpose processors.","key":"fig-accelerator-anatomy","order":{"number":2,"section":[1,2,4,0,0,0,0]}},{"caption":"Weight Stationary","key":"sec-ai-acceleration-weight-stationary-156a","order":{"number":74,"section":[1,6,1,1,1,0,0]}},{"caption":"GPU Performance Scaling: NVIDIA GPUs experienced a 10\\times increase in integer 8-bit TOPS (tera operations per second) over a decade, driven by architectural innovations transitioning from floating-point to tensor core acceleration. This trend reflects the growing specialization of hardware for deep learning workloads and the increasing demand for efficient inference capabilities.","key":"fig-ai-performance","order":{"number":3,"section":[1,3,4,3,0,0,0]}},{"caption":"Fully Connected Layer: Each output is computed as a weighted sum of all inputs plus a bias, followed by an activation function transformation. Linear transformations enable complex model architectures in neural networks.","key":"lst-linear_math_internal","order":{"number":5,"section":[1,3,1,1,0,0,0]}},{"caption":"Emergence of Domain-Specific Architectures","key":"sec-ai-acceleration-emergence-domainspecific-architectures-e045","order":{"number":6,"section":[1,2,3,0,0,0,0]}},{"caption":"Memory Allocation Definition","key":"sec-ai-acceleration-memory-allocation-definition-e740","order":{"number":63,"section":[1,5,2,1,0,0,0]}},{"caption":"AI Acceleration","key":"sec-ai-acceleration","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Memory Hierarchy","key":"sec-ai-acceleration-memory-hierarchy-1839","order":{"number":40,"section":[1,4,2,0,0,0,0]}},{"caption":"SIMT Execution: Each thread processes a unique output element in parallel, demonstrating how SIMT enables efficient matrix multiplication on GPUs.","key":"lst-cuda_simt","order":{"number":16,"section":[1,3,4,2,0,0,0]}},{"caption":"Matrix Operations Hardware Acceleration","key":"sec-ai-acceleration-matrix-operations-hardware-acceleration-514a","order":{"number":17,"section":[1,3,2,3,0,0,0]}},{"caption":"Parallel Computing and Graphics Processing","key":"sec-ai-acceleration-parallel-computing-graphics-processing-66b1","order":{"number":5,"section":[1,2,2,0,0,0,0]}},{"caption":"Compilers and Runtimes Adaptation","key":"sec-ai-acceleration-compilers-runtimes-adaptation-0d70","order":{"number":133,"section":[1,9,6,4,0,0,0]}},{"caption":"Matrix Multiplication: Data movement bottlenecks can lead to underutilized hardware resources, illustrating the importance of efficient data flow in optimizing machine learning model performance. Via This operation","key":"lst-matmul_data_movement","order":{"number":18,"section":[1,6,1,1,0,0,0]}},{"caption":"Special Function Units: Dedicated hardware implementations of common mathematical functions—like relu, sigmoid, and reciprocal square root—accelerate machine learning computations by eliminating software overhead and enabling parallel processing of vector data. Typical latencies of 1–2 cycles per function demonstrate the performance gains achieved through specialized circuitry instead of general-purpose arithmetic.","key":"tbl-sfu","order":{"number":4,"section":[1,3,3,3,0,0,0]}},{"caption":"Vector Operation: Vector multiplication and addition operations enable efficient parallel processing in machine learning models. Source: ARM Documentation","key":"lst-arm_sve_vector","order":{"number":15,"section":[1,3,4,2,0,0,0]}},{"caption":"Layer Computation: Neural networks compute each layer’s output via weighted input summation followed by an activation function transformation.","key":"lst-dense_expansion","order":{"number":2,"section":[1,3,0,0,0,0,0]}},{"caption":"Tensor Core Operation: Matrix multiplications are performed in parallel across entire matrix blocks, optimizing computational efficiency for neural network training.","key":"lst-tensor_core_op","order":{"number":17,"section":[1,3,4,3,0,0,0]}},{"caption":"ReLU and BatchNorm Operations: Neural networks process input data through conditional operations that can disrupt instruction pipelining and multiple passes required for normalization, highlighting efficiency challenges in traditional implementations. Source: IEEE Spectrum","key":"lst-traditional_overhead","order":{"number":13,"section":[1,3,3,2,0,0,0]}},{"caption":"NVLink Interface","key":"sec-ai-acceleration-nvlink-interface-312b","order":{"number":48,"section":[1,4,4,2,2,0,0]}},{"caption":"Memory Access Patterns in ML Workloads","key":"sec-ai-acceleration-memory-access-patterns-ml-workloads-a960","order":{"number":38,"section":[1,4,1,2,0,0,0]}},{"caption":"Matrix Unit Operation: Enables efficient block-wise matrix multiplication and accumulation in hardware-accelerated systems, showcasing how specialized units streamline computational tasks essential for AI/ML operations.","key":"lst-matrix_unit","order":{"number":10,"section":[1,3,2,3,0,0,0]}},{"caption":"On-Chip Memory","key":"sec-ai-acceleration-onchip-memory-72d1","order":{"number":41,"section":[1,4,2,1,0,0,0]}},{"caption":"Evolution from SIMD to SIMT Architectures","key":"sec-ai-acceleration-evolution-simd-simt-architectures-e1fd","order":{"number":26,"section":[1,3,4,2,0,0,0]}},{"caption":"Input Stationary","key":"sec-ai-acceleration-input-stationary-6c7b","order":{"number":76,"section":[1,6,1,1,3,0,0]}},{"caption":"Chiplet Interconnect: Modern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning.","key":"fig-AMD_chiplet_based","order":{"number":9,"section":[1,9,1,0,0,0,0]}},{"caption":"Quantifying the Compute-Memory Performance Gap","key":"sec-ai-acceleration-quantifying-computememory-performance-gap-1526","order":{"number":37,"section":[1,4,1,1,0,0,0]}},{"caption":"DMA for Data Transfers","key":"sec-ai-acceleration-dma-data-transfers-a1a7","order":{"number":49,"section":[1,4,4,2,3,0,0]}},{"caption":"Tensor Cores","key":"sec-ai-acceleration-tensor-cores-771f","order":{"number":27,"section":[1,3,4,3,0,0,0]}},{"caption":"Convolutional Neural Networks","key":"sec-ai-acceleration-convolutional-neural-networks-3085","order":{"number":54,"section":[1,4,5,2,0,0,0]}},{"caption":"AI Processor Configurations: Modern AI processors prioritize different execution unit characteristics to optimize performance for specific workloads; NVIDIA A100 leverages wide SIMD and tensor cores for training, Google TPUv4 emphasizes high-throughput BF16 matrix multiplication, and Intel Sapphire Rapids focuses on INT8-optimized inference, while apple M1 prioritizes low-power FP16 execution on smaller processing elements. These variations in SIMD width, tensor core size, and processing element count reflect the growing diversity in AI hardware architectures and their targeted applications.","key":"tbl-execution-units","order":{"number":6,"section":[1,3,4,7,0,0,0]}},{"caption":"Transformer Networks","key":"sec-ai-acceleration-transformer-networks-638c","order":{"number":55,"section":[1,4,5,3,0,0,0]}},{"caption":"Spatial Tiling: Reduces redundant memory accesses by processing matrix tiles sequentially.","key":"lst-tiled_spatial","order":{"number":26,"section":[1,6,1,4,3,1,0]}},{"caption":"Computation Placement Importance","key":"sec-ai-acceleration-computation-placement-importance-e7e9","order":{"number":60,"section":[1,5,1,2,0,0,0]}},{"caption":"Exploring the Configuration Space","key":"sec-ai-acceleration-exploring-configuration-space-f010","order":{"number":66,"section":[1,5,3,1,0,0,0]}},{"caption":"Hardware Acceleration","key":"sec-ai-acceleration-hardware-acceleration-08e3","order":{"number":22,"section":[1,3,3,3,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-ai-acceleration-fallacies-pitfalls-dc1f","order":{"number":146,"section":[1,11,0,0,0,0,0]}},{"caption":"Mapping Search Space","key":"sec-ai-acceleration-mapping-search-space-e9b6","order":{"number":70,"section":[1,5,3,5,0,0,0]}},{"caption":"Parallel Vector Execution","key":"sec-ai-acceleration-parallel-vector-execution-cdaa","order":{"number":12,"section":[1,3,1,3,0,0,0]}},{"caption":"Vector Processing History","key":"sec-ai-acceleration-vector-processing-history-c631","order":{"number":13,"section":[1,3,1,4,0,0,0]}},{"caption":"Mixed-Precision Computing","key":"sec-ai-acceleration-mixedprecision-computing-656f","order":{"number":32,"section":[1,3,4,6,2,0,0]}}],"headings":["sec-ai-acceleration","purpose","sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096","sec-ai-acceleration-evolution-hardware-specialization-1d21","sec-ai-acceleration-specialized-computing-1a77","sec-ai-acceleration-parallel-computing-graphics-processing-66b1","sec-ai-acceleration-emergence-domainspecific-architectures-e045","sec-ai-acceleration-machine-learning-hardware-specialization-a17f","sec-ai-acceleration-ai-compute-primitives-8471","sec-ai-acceleration-vector-operations-729f","sec-ai-acceleration-highlevel-framework-operations-9248","sec-ai-acceleration-sequential-scalar-execution-d063","sec-ai-acceleration-parallel-vector-execution-cdaa","sec-ai-acceleration-vector-processing-history-c631","sec-ai-acceleration-matrix-operations-508d","sec-ai-acceleration-matrix-operations-neural-networks-527a","sec-ai-acceleration-types-matrix-computations-neural-networks-b497","sec-ai-acceleration-matrix-operations-hardware-acceleration-514a","sec-ai-acceleration-historical-foundations-matrix-computation-402e","sec-ai-acceleration-special-function-units-ed00","sec-ai-acceleration-nonlinear-functions-fdce","sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d","sec-ai-acceleration-hardware-acceleration-08e3","sec-ai-acceleration-sfus-history-a1b6","sec-ai-acceleration-compute-units-execution-models-f406","sec-ai-acceleration-mapping-primitives-execution-units-ccb6","sec-ai-acceleration-evolution-simd-simt-architectures-e1fd","sec-ai-acceleration-tensor-cores-771f","sec-ai-acceleration-processing-elements-daa1","sec-ai-acceleration-systolic-arrays-6fa8","sec-ai-acceleration-numerics-ai-acceleration-f7be","sec-ai-acceleration-precision-tradeoffs-8fa8","sec-ai-acceleration-mixedprecision-computing-656f","sec-ai-acceleration-architectural-integration-01b6","sec-ai-acceleration-costperformance-analysis-e925","sec-ai-acceleration-ai-memory-systems-0057","sec-ai-acceleration-understanding-ai-memory-wall-3ea9","sec-ai-acceleration-quantifying-computememory-performance-gap-1526","sec-ai-acceleration-memory-access-patterns-ml-workloads-a960","sec-ai-acceleration-irregular-memory-access-c6ec","sec-ai-acceleration-memory-hierarchy-1839","sec-ai-acceleration-onchip-memory-72d1","sec-ai-acceleration-offchip-memory-ecdb","sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c","sec-ai-acceleration-hostaccelerator-communication-bb7a","sec-ai-acceleration-data-transfer-patterns-689a","sec-ai-acceleration-data-transfer-mechanisms-4109","sec-ai-acceleration-pcie-interface-c5b4","sec-ai-acceleration-nvlink-interface-312b","sec-ai-acceleration-dma-data-transfers-a1a7","sec-ai-acceleration-unified-memory-b18f","sec-ai-acceleration-data-transfer-overheads-fbc9","sec-ai-acceleration-model-memory-pressure-f95e","sec-ai-acceleration-multilayer-perceptrons-0bbc","sec-ai-acceleration-convolutional-neural-networks-3085","sec-ai-acceleration-transformer-networks-638c","sec-ai-acceleration-ml-accelerators-implications-c962","sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9","sec-ai-acceleration-computation-placement-23d2","sec-ai-acceleration-computation-placement-definition-e130","sec-ai-acceleration-computation-placement-importance-e7e9","sec-ai-acceleration-effective-computation-placement-099d","sec-ai-acceleration-memory-allocation-e095","sec-ai-acceleration-memory-allocation-definition-e740","sec-ai-acceleration-memory-challenges-different-workloads-e87c","sec-ai-acceleration-combinatorial-complexity-ea33","sec-ai-acceleration-exploring-configuration-space-f010","sec-ai-acceleration-ordering-computation-execution-7251","sec-ai-acceleration-parallelization-across-processing-elements-90d6","sec-ai-acceleration-memory-placement-data-movement-fd52","sec-ai-acceleration-mapping-search-space-e9b6","sec-ai-acceleration-dataflow-optimization-strategies-ce52","sec-ai-acceleration-building-blocks-mapping-strategies-4932","sec-ai-acceleration-data-movement-patterns-3b06","sec-ai-acceleration-weight-stationary-156a","sec-ai-acceleration-output-stationary-54e5","sec-ai-acceleration-input-stationary-6c7b","sec-ai-acceleration-memoryefficient-tensor-layouts-e250","sec-ai-acceleration-rowmajor-layout-741f","sec-ai-acceleration-channelmajor-layout-d6a9","sec-ai-acceleration-comparing-rowmajor-channelmajor-layouts-e410","sec-ai-acceleration-kernel-fusion-7faf","sec-ai-acceleration-intermediate-memory-write-f140","sec-ai-acceleration-kernel-fusion-memory-efficiency-f227","sec-ai-acceleration-performance-benefits-constraints-1b74","sec-ai-acceleration-memoryefficient-tiling-strategies-9fce","sec-ai-acceleration-tiling-fundamentals-e9e6","sec-ai-acceleration-performance-benefits-tiling-e7bd","sec-ai-acceleration-tiling-methods-6257","sec-ai-acceleration-spatial-tiling-247e","sec-ai-acceleration-temporal-tiling-563b","sec-ai-acceleration-tiling-challenges-tradeoffs-e9c9","sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110","sec-ai-acceleration-convolutional-neural-networks-1e47","sec-ai-acceleration-transformer-architectures-8f25","sec-ai-acceleration-multilayer-perceptrons-eb18","sec-ai-acceleration-hybrid-mapping-strategies-3e8c","sec-ai-acceleration-layerspecific-mapping-1102","sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8","sec-ai-acceleration-compiler-support-172e","sec-ai-acceleration-compiler-design-differences-ml-workloads-0698","sec-ai-acceleration-ml-compilation-pipeline-7676","sec-ai-acceleration-graph-optimization-f888","sec-ai-acceleration-computation-graph-optimization-a028","sec-ai-acceleration-implementation-ai-compilers-1df9","sec-ai-acceleration-graph-optimization-importance-9ccb","sec-ai-acceleration-kernel-selection-df01","sec-ai-acceleration-implementation-ai-compilers-c917","sec-ai-acceleration-kernel-selection-importance-3c3f","sec-ai-acceleration-memory-planning-fb9f","sec-ai-acceleration-implementation-ai-compilers-2ae0","sec-ai-acceleration-memory-planning-importance-e987","sec-ai-acceleration-computation-scheduling-7ccd","sec-ai-acceleration-implementation-ai-compilers-ff25","sec-ai-acceleration-computation-scheduling-importance-04a1","sec-ai-acceleration-code-generation-85c8","sec-ai-acceleration-compilationruntime-support-0206","sec-ai-acceleration-runtime-support-f94f","sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e","sec-ai-acceleration-dynamic-kernel-execution-33fc","sec-ai-acceleration-runtime-kernel-selection-1ffe","sec-ai-acceleration-kernel-scheduling-utilization-99d6","sec-ai-acceleration-multichip-ai-acceleration-38d7","sec-ai-acceleration-chipletbased-architectures-a890","sec-ai-acceleration-multigpu-systems-f017","sec-ai-acceleration-communication-overhead-amdahls-law-analysis-8eb4","sec-ai-acceleration-tpu-pods-fd33","sec-ai-acceleration-waferscale-ai-6420","sec-ai-acceleration-ai-systems-scaling-trajectory-ad73","sec-ai-acceleration-computation-memory-scaling-changes-2bb1","sec-ai-acceleration-multichip-execution-mapping-8ec9","sec-ai-acceleration-distributed-access-memory-allocation-d970","sec-ai-acceleration-data-movement-constraints-a823","sec-ai-acceleration-compilers-runtimes-adaptation-0d70","sec-ai-acceleration-execution-models-adaptation-344b","sec-ai-acceleration-crossaccelerator-scheduling-4ac0","sec-ai-acceleration-crossaccelerator-coordination-8aa4","sec-ai-acceleration-crossaccelerator-execution-management-87a9","sec-ai-acceleration-computation-placement-adaptation-9a3c","sec-ai-acceleration-navigating-multichip-ai-complexities-83bd","sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb","sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8","sec-ai-acceleration-strategies-dynamic-workload-distribution-a421","sec-ai-acceleration-power-thermal-management-6c00","sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda","sec-ai-acceleration-software-stack-challenges-255c","sec-ai-acceleration-fallacies-pitfalls-dc1f","sec-ai-acceleration-summary-a5f8","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}