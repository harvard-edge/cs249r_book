{"entries":[{"caption":"Segmentation","key":"sec-visionlanguage-models-vlm-segmentation-e618","order":{"number":31,"section":[0,4,5,0,0,0,0]}},{"caption":"Caption","key":"sec-visionlanguage-models-vlm-caption-a1d1","order":{"number":37,"section":[0,5,1,0,0,0,0]}},{"caption":"Latency Summary","key":"sec-visionlanguage-models-vlm-latency-summary-fec8","order":{"number":49,"section":[0,6,0,0,0,0,0]}},{"caption":"Future Implications","key":"sec-visionlanguage-models-vlm-future-implications-2fe6","order":{"number":55,"section":[0,9,0,0,0,0,0]}},{"caption":"Determining the Device and Data Type","key":"sec-visionlanguage-models-vlm-determining-device-data-type-4011","order":{"number":16,"section":[0,3,2,2,0,0,0]}},{"caption":"Training Dataset (FLD-5B)","key":"sec-visionlanguage-models-vlm-training-dataset-fld5b-2779","order":{"number":6,"section":[0,2,2,0,0,0,0]}},{"caption":"Detailed Captioning","key":"sec-visionlanguage-models-vlm-detailed-captioning-7819","order":{"number":29,"section":[0,4,3,0,0,0,0]}},{"caption":"Trade-offs","key":"sec-visionlanguage-models-vlm-tradeoffs-8964","order":{"number":53,"section":[0,8,2,0,0,0,0]}},{"caption":"Defining the Prompt","key":"sec-visionlanguage-models-vlm-defining-prompt-2cd2","order":{"number":18,"section":[0,3,3,0,0,0,0]}},{"caption":"Visual Grounding","key":"sec-visionlanguage-models-vlm-visual-grounding-7e4d","order":{"number":30,"section":[0,4,4,0,0,0,0]}},{"caption":"Printing the Output","key":"sec-visionlanguage-models-vlm-printing-output-8a57","order":{"number":24,"section":[0,3,4,3,0,0,0]}},{"caption":"Zero-shot Performance","key":"sec-visionlanguage-models-vlm-zeroshot-performance-e3c6","order":{"number":8,"section":[0,2,3,1,0,0,0]}},{"caption":"Detailed Caption","key":"sec-visionlanguage-models-vlm-detailed-caption-4b44","order":{"number":38,"section":[0,5,2,0,0,0,0]}},{"caption":"Summary","key":"sec-visionlanguage-models-vlm-summary-012c","order":{"number":51,"section":[0,8,0,0,0,0,0]}},{"caption":"Open Vocabulary Detection","key":"sec-visionlanguage-models-vlm-open-vocabulary-detection-be66","order":{"number":44,"section":[0,5,8,0,0,0,0]}},{"caption":"Best Use Cases","key":"sec-visionlanguage-models-vlm-best-use-cases-e73b","order":{"number":54,"section":[0,8,3,0,0,0,0]}},{"caption":"Comparing Florence-2 with other VLMs","key":"sec-visionlanguage-models-vlm-comparing-florence2-vlms-b507","order":{"number":11,"section":[0,2,5,0,0,0,0]}},{"caption":"Setup and Installation","key":"sec-visionlanguage-models-vlm-setup-installation-78de","order":{"number":12,"section":[0,3,0,0,0,0,0]}},{"caption":"Loading the Model and Processor","key":"sec-visionlanguage-models-vlm-loading-model-processor-fb73","order":{"number":17,"section":[0,3,2,3,0,0,0]}},{"caption":"Resources","key":"sec-visionlanguage-models-vlm-resources-fb6a","order":{"number":56,"section":[0,10,0,0,0,0,0]}},{"caption":"Fine-Tuning","key":"sec-visionlanguage-models-vlm-finetunning-80d7","order":{"number":50,"section":[0,7,0,0,0,0,0]}},{"caption":"Object Detection (OD)","key":"sec-visionlanguage-models-vlm-object-detection-od-ef58","order":{"number":27,"section":[0,4,1,0,0,0,0]}},{"caption":"Downloading and Loading the Image","key":"sec-visionlanguage-models-vlm-downloading-loading-image-e518","order":{"number":19,"section":[0,3,3,1,0,0,0]}},{"caption":"Practical Applications","key":"sec-visionlanguage-models-vlm-practical-applications-4b8f","order":{"number":10,"section":[0,2,4,0,0,0,0]}},{"caption":"Generating the Output","key":"sec-visionlanguage-models-vlm-generating-output-6e10","order":{"number":21,"section":[0,3,4,0,0,0,0]}},{"caption":"OCR","key":"sec-visionlanguage-models-vlm-ocr-708e","order":{"number":48,"section":[0,5,12,0,0,0,0]}},{"caption":"Phrase Grounding for Specific Expressions","key":"sec-visionlanguage-models-vlm-phrase-grounding-specific-expressions-0d0c","order":{"number":34,"section":[0,4,8,0,0,0,0]}},{"caption":"Open Vocabulary Object Detection","key":"sec-visionlanguage-models-vlm-open-vocabulary-object-detection-3811","order":{"number":35,"section":[0,4,9,0,0,0,0]}},{"caption":"Region to Segmentation","key":"sec-visionlanguage-models-vlm-region-segmentation-1899","order":{"number":46,"section":[0,5,10,0,0,0,0]}},{"caption":"Referring expression segmentation","key":"sec-visionlanguage-models-vlm-referring-expression-segmentation-de08","order":{"number":45,"section":[0,5,9,0,0,0,0]}},{"caption":"Caption to Phrase Grounding","key":"sec-visionlanguage-models-vlm-caption-phrase-grounding-a092","order":{"number":42,"section":[0,5,6,0,0,0,0]}},{"caption":"Cascade Tasks","key":"sec-visionlanguage-models-vlm-cascade-tasks-8b5e","order":{"number":43,"section":[0,5,7,0,0,0,0]}},{"caption":"Testing the installation","key":"sec-visionlanguage-models-vlm-testing-installation-4988","order":{"number":14,"section":[0,3,2,0,0,0,0]}},{"caption":"Key Capabilities","key":"sec-visionlanguage-models-vlm-key-capabilities-2707","order":{"number":7,"section":[0,2,3,0,0,0,0]}},{"caption":"Post-processing the Generation","key":"sec-visionlanguage-models-vlm-postprocessing-generation-6592","order":{"number":23,"section":[0,3,4,2,0,0,0]}},{"caption":"Object Detection","key":"sec-visionlanguage-models-vlm-object-detection-dd2b","order":{"number":40,"section":[0,5,4,0,0,0,0]}},{"caption":"Why Florence-2 at the Edge?","key":"sec-visionlanguage-models-vlm-florence2-edge-0534","order":{"number":2,"section":[0,1,1,0,0,0,0]}},{"caption":"Florence-2 Model Architecture","key":"sec-visionlanguage-models-vlm-florence2-model-architecture-b695","order":{"number":3,"section":[0,1,2,0,0,0,0]}},{"caption":"Environment configuration","key":"sec-visionlanguage-models-vlm-environment-configuration-0062","order":{"number":13,"section":[0,3,1,0,0,0,0]}},{"caption":"Florence-2 Tasks","key":"sec-visionlanguage-models-vlm-florence2-tasks-bcbb","order":{"number":26,"section":[0,4,0,0,0,0,0]}},{"caption":"Exploring computer vision and vision-language tasks","key":"sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-5c1d","order":{"number":36,"section":[0,5,0,0,0,0,0]}},{"caption":"Region to Texts","key":"sec-visionlanguage-models-vlm-region-texts-d085","order":{"number":47,"section":[0,5,11,0,0,0,0]}},{"caption":"Dense Region Caption","key":"sec-visionlanguage-models-vlm-dense-region-caption-fd99","order":{"number":41,"section":[0,5,5,0,0,0,0]}},{"caption":"Fine-tuned Performance","key":"sec-visionlanguage-models-vlm-finetuned-performance-119c","order":{"number":9,"section":[0,2,3,2,0,0,0]}},{"caption":"Architecture","key":"sec-visionlanguage-models-vlm-architecture-9992","order":{"number":5,"section":[0,2,1,0,0,0,0]}},{"caption":"Key Advantages of Florence-2","key":"sec-visionlanguage-models-vlm-key-advantages-florence2-0a83","order":{"number":52,"section":[0,8,1,0,0,0,0]}},{"caption":"Introduction","key":"sec-visionlanguage-models-vlm-introduction-4272","order":{"number":1,"section":[0,1,0,0,0,0,0]}},{"caption":"Technical Overview","key":"sec-visionlanguage-models-vlm-technical-overview-faee","order":{"number":4,"section":[0,2,0,0,0,0,0]}},{"caption":"Processing Inputs","key":"sec-visionlanguage-models-vlm-processing-inputs-14e0","order":{"number":20,"section":[0,3,3,2,0,0,0]}},{"caption":"Importing Required Libraries","key":"sec-visionlanguage-models-vlm-importing-required-libraries-1ee5","order":{"number":15,"section":[0,3,2,1,0,0,0]}},{"caption":"Decoding the Generated Text","key":"sec-visionlanguage-models-vlm-decoding-generated-text-70e0","order":{"number":22,"section":[0,3,4,1,0,0,0]}},{"caption":"Result","key":"sec-visionlanguage-models-vlm-result-0a91","order":{"number":25,"section":[0,3,4,4,0,0,0]}},{"caption":"OCR with Region","key":"sec-visionlanguage-models-vlm-ocr-region-75f2","order":{"number":33,"section":[0,4,7,0,0,0,0]}},{"caption":"More Detailed Caption","key":"sec-visionlanguage-models-vlm-detailed-caption-f936","order":{"number":39,"section":[0,5,3,0,0,0,0]}},{"caption":"Image Captioning","key":"sec-visionlanguage-models-vlm-image-captioning-c35d","order":{"number":28,"section":[0,4,2,0,0,0,0]}},{"caption":"Dense Region Captioning","key":"sec-visionlanguage-models-vlm-dense-region-captioning-ed94","order":{"number":32,"section":[0,4,6,0,0,0,0]}}],"headings":["vision-language-models-vlm","sec-visionlanguage-models-vlm-introduction-4272","sec-visionlanguage-models-vlm-florence2-edge-0534","sec-visionlanguage-models-vlm-florence2-model-architecture-b695","sec-visionlanguage-models-vlm-technical-overview-faee","sec-visionlanguage-models-vlm-architecture-9992","sec-visionlanguage-models-vlm-training-dataset-fld5b-2779","sec-visionlanguage-models-vlm-key-capabilities-2707","sec-visionlanguage-models-vlm-zeroshot-performance-e3c6","sec-visionlanguage-models-vlm-finetuned-performance-119c","sec-visionlanguage-models-vlm-practical-applications-4b8f","sec-visionlanguage-models-vlm-comparing-florence2-vlms-b507","sec-visionlanguage-models-vlm-setup-installation-78de","sec-visionlanguage-models-vlm-environment-configuration-0062","sec-visionlanguage-models-vlm-testing-installation-4988","sec-visionlanguage-models-vlm-importing-required-libraries-1ee5","sec-visionlanguage-models-vlm-determining-device-data-type-4011","sec-visionlanguage-models-vlm-loading-model-processor-fb73","sec-visionlanguage-models-vlm-defining-prompt-2cd2","sec-visionlanguage-models-vlm-downloading-loading-image-e518","sec-visionlanguage-models-vlm-processing-inputs-14e0","sec-visionlanguage-models-vlm-generating-output-6e10","sec-visionlanguage-models-vlm-decoding-generated-text-70e0","sec-visionlanguage-models-vlm-postprocessing-generation-6592","sec-visionlanguage-models-vlm-printing-output-8a57","sec-visionlanguage-models-vlm-result-0a91","sec-visionlanguage-models-vlm-florence2-tasks-bcbb","sec-visionlanguage-models-vlm-object-detection-od-ef58","sec-visionlanguage-models-vlm-image-captioning-c35d","sec-visionlanguage-models-vlm-detailed-captioning-7819","sec-visionlanguage-models-vlm-visual-grounding-7e4d","sec-visionlanguage-models-vlm-segmentation-e618","sec-visionlanguage-models-vlm-dense-region-captioning-ed94","sec-visionlanguage-models-vlm-ocr-region-75f2","sec-visionlanguage-models-vlm-phrase-grounding-specific-expressions-0d0c","sec-visionlanguage-models-vlm-open-vocabulary-object-detection-3811","sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-5c1d","sec-visionlanguage-models-vlm-caption-a1d1","sec-visionlanguage-models-vlm-detailed-caption-4b44","sec-visionlanguage-models-vlm-detailed-caption-f936","sec-visionlanguage-models-vlm-object-detection-dd2b","sec-visionlanguage-models-vlm-dense-region-caption-fd99","sec-visionlanguage-models-vlm-caption-phrase-grounding-a092","sec-visionlanguage-models-vlm-cascade-tasks-8b5e","sec-visionlanguage-models-vlm-open-vocabulary-detection-be66","sec-visionlanguage-models-vlm-referring-expression-segmentation-de08","sec-visionlanguage-models-vlm-region-segmentation-1899","sec-visionlanguage-models-vlm-region-texts-d085","sec-visionlanguage-models-vlm-ocr-708e","sec-visionlanguage-models-vlm-latency-summary-fec8","sec-visionlanguage-models-vlm-finetunning-80d7","sec-visionlanguage-models-vlm-summary-012c","sec-visionlanguage-models-vlm-key-advantages-florence2-0a83","sec-visionlanguage-models-vlm-tradeoffs-8964","sec-visionlanguage-models-vlm-best-use-cases-e73b","sec-visionlanguage-models-vlm-future-implications-2fe6","sec-visionlanguage-models-vlm-resources-fb6a"]}