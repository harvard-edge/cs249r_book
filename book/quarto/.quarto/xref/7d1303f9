{"entries":[{"caption":"Side-Channel Attacks","key":"sec-security-privacy-sidechannel-attacks-cdfd","order":{"number":26,"section":[1,6,4,0,0,0,0]}},{"caption":"Security Defined","key":"sec-security-privacy-security-defined-1129","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Physical Unclonable Functions: Pufs generate unique hardware fingerprints from inherent manufacturing variations, enabling device authentication and secure key generation without storing secrets. Optical and electronic PUF implementations use physical phenomena—such as light speckle patterns or timing differences—to produce challenge-response pairs that are difficult to predict or replicate. Source: [@gao2020physical].","key":"fig-pfu","order":{"number":17,"section":[1,8,7,5,0,0,0]}},{"caption":"Foundational Concepts and Definitions","key":"sec-security-privacy-foundational-concepts-definitions-d529","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"Privacy-Accuracy Trade-Offs: Data privacy techniques impose varying computational costs and offer different levels of formal privacy guarantees, requiring practitioners to balance privacy strength with model utility and deployment constraints. The table summarizes key properties—privacy guarantees, computational overhead, maturity, typical use cases, and trade-offs—to guide informed decisions when designing privacy-aware machine learning systems.","key":"tbl-privacy-technique-comparison","order":{"number":7,"section":[1,8,2,4,0,0,0]}},{"caption":"Case Study: Traffic Sign Attack","key":"sec-security-privacy-case-study-traffic-sign-attack-6e93","order":{"number":21,"section":[1,5,4,0,0,0,0]}},{"caption":"Security versus Privacy","key":"sec-security-privacy-security-versus-privacy-e0b8","order":{"number":6,"section":[1,2,3,0,0,0,0]}},{"caption":"Phase 2: Privacy Controls and Model Protection","key":"sec-security-privacy-phase2-privacy-model-protection-7a8b","order":{"number":57,"section":[1,9,2,0,0,0,0]}},{"caption":"Power Profile: The device’s power consumption remains stable during authentication when the correct password is entered, setting a baseline for comparison in subsequent figures through This figure. Source: colin o’flynn.","key":"fig-encryption","order":{"number":8,"section":[1,6,4,0,0,0,0]}},{"caption":"Adversarial Stickers: Nearly imperceptible stickers can trick machine learning models into misclassifying stop signs as speed limit signs over 85% of the time. This emphasizes the vulnerability of ML systems to adversarial attacks. Source: @eykholt2018robust.","key":"fig-adversarial-stickers","order":{"number":4,"section":[1,5,4,0,0,0,0]}},{"caption":"Layered Defense Stack: Machine learning systems require multi-faceted security strategies that progress from foundational hardware protections to data-centric privacy techniques, building trust across all layers. This architecture integrates safeguards at the data, model, runtime, and infrastructure levels to mitigate threats and ensure robust deployment in production environments.","key":"fig-defense-stack","order":{"number":13,"section":[1,8,1,0,0,0,0]}},{"caption":"Output Monitoring","key":"sec-security-privacy-output-monitoring-cf37","order":{"number":45,"section":[1,8,6,2,0,0,0]}},{"caption":"Implementation Considerations","key":"sec-security-privacy-implementation-considerations-9f4e","order":{"number":59,"section":[1,9,4,0,0,0,0]}},{"caption":"Approximate Model Theft","key":"sec-security-privacy-approximate-model-theft-1155","order":{"number":17,"section":[1,5,1,2,0,0,0]}},{"caption":"Fault Injection Attack: Manipulating assembly code bypasses safety checks, forcing a neuron’s output to zero regardless of input and demonstrating a hardware vulnerability in machine learning systems. Source: [@breier2018deeplaser].","key":"fig-injection","order":{"number":7,"section":[1,6,3,0,0,0,0]}},{"caption":"Adversarial Prompt Evasion: Implicitly adversarial prompts bypass typical content filters by triggering unintended generations, revealing limitations of solely relying on pre-generation safety checks. these examples underscore the necessity of post-hoc content analysis as a complementary defense layer for robust generative AI systems. Source: [@quaye2024adversarial.].","key":"fig-adversarial-nibbler","order":{"number":14,"section":[1,8,6,2,0,0,0]}},{"caption":"Case Study: Tesla IP Theft","key":"sec-security-privacy-case-study-tesla-ip-theft-9d78","order":{"number":18,"section":[1,5,1,3,0,0,0]}},{"caption":"Hardware Security Modules","key":"sec-security-privacy-hardware-security-modules-4377","order":{"number":52,"section":[1,8,7,4,0,0,0]}},{"caption":"Privacy-Preserving Data Techniques","key":"sec-security-privacy-privacypreserving-data-techniques-64f8","order":{"number":35,"section":[1,8,2,0,0,0,0]}},{"caption":"Physical Attacks","key":"sec-security-privacy-physical-attacks-095a","order":{"number":24,"section":[1,6,2,0,0,0,0]}},{"caption":"The Layered Defense Principle","key":"sec-security-privacy-layered-defense-principle-8706","order":{"number":34,"section":[1,8,1,0,0,0,0]}},{"caption":"Side-Channel Attack Vulnerability: Power consumption patterns reveal cryptographic key information during authentication; consistent power usage indicates correct password bytes, while abrupt changes signal incorrect input and halted processing. Even without knowing the password, an attacker can infer it by analyzing the device’s power usage during authentication attempts via this figure. Source: Colin O’Flynn.","key":"fig-encryption2","order":{"number":9,"section":[1,6,4,0,0,0,0]}},{"caption":"Model-Specific Attack Vectors","key":"sec-security-privacy-modelspecific-attack-vectors-0575","order":{"number":14,"section":[1,5,0,0,0,0,0]}},{"caption":"Mechanisms Comparison","key":"sec-security-privacy-mechanisms-comparison-2dcb","order":{"number":54,"section":[1,8,7,6,0,0,0]}},{"caption":"Case Study: Deep Learning for SCA","key":"sec-security-privacy-case-study-deep-learning-sca-b0b3","order":{"number":32,"section":[1,7,1,0,0,0,0]}},{"caption":"Secure Model Design","key":"sec-security-privacy-secure-model-design-69a6","order":{"number":41,"section":[1,8,4,0,0,0,0]}},{"caption":"STM32F415 Target Board: Enables monitoring of power consumption during AES operations on the microcontroller, highlighting side-channel vulnerabilities that can be exploited by machine learning models. Source: @scaaml_2019.","key":"fig-stm32f-board","order":{"number":12,"section":[1,7,1,0,0,0,0]}},{"caption":"Supply Chain Compromise: Stuxnet","key":"sec-security-privacy-supply-chain-compromise-stuxnet-8a4b","order":{"number":9,"section":[1,3,1,0,0,0,0]}},{"caption":"Runtime System Monitoring","key":"sec-security-privacy-runtime-system-monitoring-a71c","order":{"number":43,"section":[1,8,6,0,0,0,0]}},{"caption":"Secure Model Deployment","key":"sec-security-privacy-secure-model-deployment-e08c","order":{"number":42,"section":[1,8,5,0,0,0,0]}},{"caption":"Security-Privacy Distinctions: Machine learning systems require distinct approaches to security and privacy; security mitigates adversarial threats targeting system functionality, while privacy protects sensitive information from both intentional and unintentional exposure through data leakage or re-identification. This table clarifies how differing goals and threat models shape the specific concerns and mitigation strategies for each domain.","key":"tbl-security-privacy-comparison","order":{"number":1,"section":[1,2,3,0,0,0,0]}},{"caption":"Case Study: Supermicro Controversy","key":"sec-security-privacy-case-study-supermicro-controversy-72b7","order":{"number":30,"section":[1,6,8,0,0,0,0]}},{"caption":"Offensive ML Use Cases: This table categorizes how machine learning amplifies cyberattacks by enabling automated content generation, exploiting system vulnerabilities, and increasing attack sophistication; it details the typical ML model, targeted weakness, and resulting advantage for each offensive application. Understanding these use cases is important for developing effective defenses against increasingly intelligent threats.","key":"tbl-offensive-ml-use-cases","order":{"number":6,"section":[1,7,0,0,0,0,0]}},{"caption":"Hardware Security Foundations","key":"sec-security-privacy-hardware-security-foundations-f5e8","order":{"number":48,"section":[1,8,7,0,0,0,0]}},{"caption":"Adversarial Attacks","key":"sec-security-privacy-adversarial-attacks-9f84","order":{"number":20,"section":[1,5,3,0,0,0,0]}},{"caption":"Hardware Security Mechanisms: Each primitive provides distinct defensive capabilities that work together to create comprehensive protection from hardware-level threats.","key":"tbl-hardware-security-mechanisms","order":{"number":8,"section":[1,8,7,0,0,0,0]}},{"caption":"Federated Learning","key":"sec-security-privacy-federated-learning-3834","order":{"number":37,"section":[1,8,2,2,0,0,0]}},{"caption":"Response and Rollback","key":"sec-security-privacy-response-rollback-1792","order":{"number":47,"section":[1,8,6,4,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-security-privacy-fallacies-pitfalls-0c20","order":{"number":60,"section":[1,10,0,0,0,0,0]}},{"caption":"Fault Injection Attacks","key":"sec-security-privacy-fault-injection-attacks-8c52","order":{"number":25,"section":[1,6,3,0,0,0,0]}},{"caption":"Model Stealing Costs: Attackers can extract model weights with a relatively low query cost using publicly available apis; the table quantifies this threat for OpenAI’s ada and babbage models, showing that extracting weights achieves low root mean squared error (RMSE) with fewer than (4 ^6) queries. Estimated costs for weight extraction range from $1 to $12, demonstrating the economic feasibility of model stealing attacks despite API rate limits and associated expenses. Source: @carlini2024stealing.","key":"tbl-openai-theft","order":{"number":2,"section":[1,5,1,2,0,0,0]}},{"caption":"Privacy Defined","key":"sec-security-privacy-privacy-defined-da84","order":{"number":5,"section":[1,2,2,0,0,0,0]}},{"caption":"Supply Chain Risks","key":"sec-security-privacy-supply-chain-risks-c99c","order":{"number":29,"section":[1,6,7,0,0,0,0]}},{"caption":"Case Study: GPT-3 Data Extraction Attack","key":"sec-security-privacy-case-study-gpt3-data-extraction-attack-5126","order":{"number":40,"section":[1,8,3,0,0,0,0]}},{"caption":"Trusted Execution Environments","key":"sec-security-privacy-trusted-execution-environments-80ed","order":{"number":50,"section":[1,8,7,2,0,0,0]}},{"caption":"Secure Boot","key":"sec-security-privacy-secure-boot-5242","order":{"number":51,"section":[1,8,7,3,0,0,0]}},{"caption":"Insufficient Isolation: Jeep Cherokee Hack","key":"sec-security-privacy-insufficient-isolation-jeep-cherokee-hack-6a7c","order":{"number":10,"section":[1,3,2,0,0,0,0]}},{"caption":"Integrity Checks","key":"sec-security-privacy-integrity-checks-2989","order":{"number":46,"section":[1,8,6,3,0,0,0]}},{"caption":"Systematic Threat Analysis and Risk Assessment","key":"sec-security-privacy-systematic-threat-analysis-risk-assessment-3ef1","order":{"number":12,"section":[1,4,0,0,0,0,0]}},{"caption":"Comprehensive Defense Architectures","key":"sec-security-privacy-comprehensive-defense-architectures-48ab","order":{"number":33,"section":[1,8,0,0,0,0,0]}},{"caption":"Hardware Security Mechanisms: Machine learning systems use diverse hardware defenses—trusted execution environments, secure boot, hardware security modules, and physical unclonable functions—to establish trust and protect sensitive data across the system stack. The table details how each mechanism addresses specific security challenges—from runtime isolation and integrity verification to key management and device identity—and emphasizes the associated trade-offs in performance and complexity.","key":"tbl-hw-security-comparison","order":{"number":9,"section":[1,8,7,6,0,0,0]}},{"caption":"Practical Implementation Roadmap","key":"sec-security-privacy-practical-roadmap-8f3a","order":{"number":55,"section":[1,9,0,0,0,0,0]}},{"caption":"Threat Mitigation Flow: This diagram maps common machine learning threats to corresponding defense strategies, guiding selection based on attack vector and lifecycle stage. By following this flow, practitioners can align threat models with practical mitigation techniques, such as secure model access and data sanitization, to build more robust AI systems.","key":"fig-threat-mitigation-flow","order":{"number":5,"section":[1,5,4,0,0,0,0]}},{"caption":"Differential Privacy","key":"sec-security-privacy-differential-privacy-8c2b","order":{"number":36,"section":[1,8,2,1,0,0,0]}},{"caption":"Hardware Bugs","key":"sec-security-privacy-hardware-bugs-9efc","order":{"number":23,"section":[1,6,1,0,0,0,0]}},{"caption":"Secure Boot Sequence: Embedded systems employ a layered boot process to verify firmware and software integrity, establishing a root of trust before executing machine learning workloads and protecting against pre-runtime attacks. This architecture ensures only authenticated code runs, safeguarding model data and preventing unauthorized model substitution or modification during deployment. Source: [@rashmi2018secure].","key":"fig-secure-boot","order":{"number":16,"section":[1,8,7,3,0,0,0]}},{"caption":"Secure Enclave Architecture: Hardware-isolated enclaves enhance system security by encapsulating sensitive data and cryptographic operations within a dedicated processor and memory. This design minimizes the attack surface and protects important keys even if the main application processor is compromised, providing a trusted execution environment for security-important tasks. Source: Apple.","key":"fig-enclave","order":{"number":15,"section":[1,8,7,2,0,0,0]}},{"caption":"Stuxnet: Targets PLCs by exploiting Windows and Siemens software vulnerabilities, demonstrating supply chain compromise that enabled digital malware to cause physical infrastructure damage. Modern ML systems face analogous risks through compromised training data, backdoored dependencies, and tampered model weights. Figure 1","key":"fig-stuxnet","order":{"number":1,"section":[1,3,1,0,0,0,0]}},{"caption":"Hardware-Level Security Vulnerabilities","key":"sec-security-privacy-hardwarelevel-security-vulnerabilities-1ab4","order":{"number":22,"section":[1,6,0,0,0,0,0]}},{"caption":"Power Traces: Cryptographic computations reveal subtle, data-dependent variations in power consumption that reflect internal states during specific operations.","key":"fig-side-channel-curves","order":{"number":11,"section":[1,7,1,0,0,0,0]}},{"caption":"Hardware-Software Co-Design","key":"sec-security-privacy-hardwaresoftware-codesign-bed2","order":{"number":49,"section":[1,8,7,1,0,0,0]}},{"caption":"When ML Systems Become Attack Tools","key":"sec-security-privacy-ml-systems-become-attack-tools-2f34","order":{"number":31,"section":[1,7,0,0,0,0,0]}},{"caption":"Synthetic Data Generation","key":"sec-security-privacy-synthetic-data-generation-4349","order":{"number":38,"section":[1,8,2,3,0,0,0]}},{"caption":"Summary","key":"sec-security-privacy-summary-831c","order":{"number":61,"section":[1,11,0,0,0,0,0]}},{"caption":"Hardware Threat Landscape: Machine learning systems face diverse hardware threats ranging from intrinsic design flaws to physical attacks and supply chain vulnerabilities. Understanding these threats, and their relevance to ML hardware, is essential for building secure and trustworthy AI deployments.","key":"tbl-threat_types","order":{"number":5,"section":[1,6,0,0,0,0,0]}},{"caption":"Phase 1: Foundation Security Controls","key":"sec-security-privacy-phase1-baseline-foundation-2d9c","order":{"number":56,"section":[1,9,1,0,0,0,0]}},{"caption":"Laser Fault Injection: Focused laser pulses induce bit flips within microcontroller memory, enabling attackers to manipulate model execution and compromise system integrity. Researchers utilize this technique to simulate hardware errors, revealing vulnerabilities in embedded machine learning systems and informing the development of fault-tolerant designs. Source: [@breier2018deeplaser].","key":"fig-laser-bitflip","order":{"number":6,"section":[1,6,3,0,0,0,0]}},{"caption":"Security-Privacy Interactions and Trade-offs","key":"sec-security-privacy-securityprivacy-interactions-tradeoffs-d153","order":{"number":7,"section":[1,2,4,0,0,0,0]}},{"caption":"Input Validation","key":"sec-security-privacy-input-validation-c96f","order":{"number":44,"section":[1,8,6,1,0,0,0]}},{"caption":"Learning from Security Breaches","key":"sec-security-privacy-learning-security-breaches-6719","order":{"number":8,"section":[1,3,0,0,0,0,0]}},{"caption":"Adversarial Knowledge Spectrum: Varying levels of attacker access to model details and training data define distinct threat models, influencing the feasibility and sophistication of adversarial attacks and impacting deployment security strategies. The table categorizes these models by access level, typical attack methods, and common deployment scenarios, clarifying the practical challenges of securing machine learning systems.","key":"tbl-adversary-knowledge-spectrum","order":{"number":3,"section":[1,5,3,0,0,0,0]}},{"caption":"ML Lifecycle Threats: Model theft, data poisoning, and adversarial attacks target distinct stages of the machine learning lifecycle (from data ingestion to model deployment and inference), creating unique vulnerabilities at each step. Understanding these lifecycle positions clarifies attack surfaces and guides the development of targeted defense strategies for robust AI systems.","key":"fig-ml-lifecycle-threats","order":{"number":2,"section":[1,5,0,0,0,0,0]}},{"caption":"Model Theft Strategies: Attackers can target either a model’s internal parameters or its external behavior to create a stolen copy. Direct theft extracts model weights and architecture, while approximate theft trains a surrogate model by querying the original’s input-output behavior, potentially enabling further attacks despite lacking direct access to internal components.","key":"fig-model-theft-types","order":{"number":3,"section":[1,5,1,0,0,0,0]}},{"caption":"Model Theft","key":"sec-security-privacy-model-theft-1879","order":{"number":15,"section":[1,5,1,0,0,0,0]}},{"caption":"Physical Unclonable Functions","key":"sec-security-privacy-physical-unclonable-functions-6533","order":{"number":53,"section":[1,8,7,5,0,0,0]}},{"caption":"Power Consumption Jump: The blue line’s sharp increase after processing the first byte indicates immediate authentication failure, highlighting how incorrect passwords are quickly detected through power usage. Source: Colin O’Flynn.","key":"fig-encryption3","order":{"number":10,"section":[1,6,4,0,0,0,0]}},{"caption":"Weaponized Endpoints: Mirai Botnet","key":"sec-security-privacy-weaponized-endpoints-mirai-botnet-931c","order":{"number":11,"section":[1,3,3,0,0,0,0]}},{"caption":"Data Poisoning","key":"sec-security-privacy-data-poisoning-351f","order":{"number":19,"section":[1,5,2,0,0,0,0]}},{"caption":"Security & Privacy","key":"sec-security-privacy","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Leaky Interfaces","key":"sec-security-privacy-leaky-interfaces-9206","order":{"number":27,"section":[1,6,5,0,0,0,0]}},{"caption":"Exact Model Theft","key":"sec-security-privacy-exact-model-theft-b738","order":{"number":16,"section":[1,5,1,1,0,0,0]}},{"caption":"Comparative Properties","key":"sec-security-privacy-comparative-properties-9ca5","order":{"number":39,"section":[1,8,2,4,0,0,0]}},{"caption":"Security and Privacy in ML Systems","key":"sec-security-privacy-security-privacy-ml-systems-0b1e","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Threat Prioritization Framework","key":"sec-security-privacy-threat-prioritization-framework-f2d5","order":{"number":13,"section":[1,4,1,0,0,0,0]}},{"caption":"Threat Landscape: Machine learning systems face diverse threats throughout their lifecycle, ranging from data manipulation during training to model theft post-deployment. The table categorizes these threats by lifecycle stage and attack vector, clarifying how vulnerabilities manifest and enabling targeted mitigation strategies.","key":"tbl-threats-models-summary","order":{"number":4,"section":[1,5,4,0,0,0,0]}},{"caption":"Counterfeit Hardware","key":"sec-security-privacy-counterfeit-hardware-36fd","order":{"number":28,"section":[1,6,6,0,0,0,0]}},{"caption":"Phase 3: Advanced Threat Defense","key":"sec-security-privacy-phase3-advanced-defenses-runtime-8c2d","order":{"number":58,"section":[1,9,3,0,0,0,0]}}],"headings":["sec-security-privacy","purpose","sec-security-privacy-security-privacy-ml-systems-0b1e","sec-security-privacy-foundational-concepts-definitions-d529","sec-security-privacy-security-defined-1129","sec-security-privacy-privacy-defined-da84","sec-security-privacy-security-versus-privacy-e0b8","sec-security-privacy-securityprivacy-interactions-tradeoffs-d153","sec-security-privacy-learning-security-breaches-6719","sec-security-privacy-supply-chain-compromise-stuxnet-8a4b","sec-security-privacy-insufficient-isolation-jeep-cherokee-hack-6a7c","sec-security-privacy-weaponized-endpoints-mirai-botnet-931c","sec-security-privacy-systematic-threat-analysis-risk-assessment-3ef1","sec-security-privacy-threat-prioritization-framework-f2d5","sec-security-privacy-modelspecific-attack-vectors-0575","sec-security-privacy-model-theft-1879","sec-security-privacy-exact-model-theft-b738","sec-security-privacy-approximate-model-theft-1155","sec-security-privacy-case-study-tesla-ip-theft-9d78","sec-security-privacy-data-poisoning-351f","sec-security-privacy-adversarial-attacks-9f84","sec-security-privacy-case-study-traffic-sign-attack-6e93","sec-security-privacy-hardwarelevel-security-vulnerabilities-1ab4","sec-security-privacy-hardware-bugs-9efc","sec-security-privacy-physical-attacks-095a","sec-security-privacy-fault-injection-attacks-8c52","sec-security-privacy-sidechannel-attacks-cdfd","sec-security-privacy-leaky-interfaces-9206","sec-security-privacy-counterfeit-hardware-36fd","sec-security-privacy-supply-chain-risks-c99c","sec-security-privacy-case-study-supermicro-controversy-72b7","sec-security-privacy-ml-systems-become-attack-tools-2f34","sec-security-privacy-case-study-deep-learning-sca-b0b3","sec-security-privacy-comprehensive-defense-architectures-48ab","sec-security-privacy-layered-defense-principle-8706","sec-security-privacy-privacypreserving-data-techniques-64f8","sec-security-privacy-differential-privacy-8c2b","sec-security-privacy-federated-learning-3834","sec-security-privacy-synthetic-data-generation-4349","sec-security-privacy-comparative-properties-9ca5","sec-security-privacy-case-study-gpt3-data-extraction-attack-5126","sec-security-privacy-secure-model-design-69a6","sec-security-privacy-secure-model-deployment-e08c","sec-security-privacy-runtime-system-monitoring-a71c","sec-security-privacy-input-validation-c96f","sec-security-privacy-output-monitoring-cf37","sec-security-privacy-integrity-checks-2989","sec-security-privacy-response-rollback-1792","sec-security-privacy-hardware-security-foundations-f5e8","sec-security-privacy-hardwaresoftware-codesign-bed2","sec-security-privacy-trusted-execution-environments-80ed","sec-security-privacy-secure-boot-5242","sec-security-privacy-hardware-security-modules-4377","sec-security-privacy-physical-unclonable-functions-6533","sec-security-privacy-mechanisms-comparison-2dcb","sec-security-privacy-practical-roadmap-8f3a","sec-security-privacy-phase1-baseline-foundation-2d9c","sec-security-privacy-phase2-privacy-model-protection-7a8b","sec-security-privacy-phase3-advanced-defenses-runtime-8c2d","sec-security-privacy-implementation-considerations-9f4e","sec-security-privacy-fallacies-pitfalls-0c20","sec-security-privacy-summary-831c","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}