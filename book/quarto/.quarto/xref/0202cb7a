{"entries":[{"caption":"Battery and Thermal Constraints","key":"sec-ml-systems-battery-thermal-constraints-52eb","order":{"number":14,"section":[1,5,1,0,0,0,0]}},{"caption":"Distributed Processing Architecture","key":"sec-ml-systems-distributed-processing-architecture-8d28","order":{"number":10,"section":[1,4,1,0,0,0,0]}},{"caption":"Hardware Spectrum: Machine learning system design necessitates trade-offs between computational resources, power consumption, and cost, as exemplified by the diverse hardware platforms suitable for cloud, edge, mobile, and TinyML deployments. This table quantifies those trade-offs, revealing how device capabilities, from specialized ML accelerators in cloud data centers to low-power microcontrollers in embedded systems, shape the types of models and tasks each platform can effectively support. The quantitative thresholds provide specific decision criteria to help practitioners determine the most appropriate deployment paradigm for their applications.","key":"tbl-representative-systems","order":{"number":1,"section":[1,2,1,0,0,0,0]}},{"caption":"Cloud ML Capabilities: Cloud machine learning systems address challenges related to scale, complexity, and resource management through centralized computing infrastructure and specialized hardware. This figure outlines key considerations for deploying models in the cloud, including the need for reliable infrastructure and efficient resource allocation to handle large datasets and complex computations.","key":"fig-cloud-ml","order":{"number":3,"section":[1,3,0,0,0,0,0]}},{"caption":"Distributed Intelligence Spectrum: Machine learning system design involves trade-offs between computational resources, latency, and connectivity, resulting in a spectrum of deployment options ranging from centralized cloud infrastructure to resource-constrained edge and TinyML devices. This figure maps these options, highlighting how each approach balances processing location with device capability and network dependence. Source: [@abiresearch2024tinyml].","key":"fig-cloud-edge-TinyML-comparison","order":{"number":1,"section":[1,2,0,0,0,0,0]}},{"caption":"Hierarchical Processing","key":"sec-ml-systems-hierarchical-processing-17a5","order":{"number":24,"section":[1,7,1,2,0,0,0]}},{"caption":"TinyML System Scale: These device kits exemplify the extreme miniaturization achievable with TinyML, enabling deployment of machine learning on resource-constrained devices with limited power and memory. such compact systems broaden the applicability of ML to previously inaccessible edge applications, including wearable sensors and embedded IoT devices. Source: [@warden2018speech]","key":"fig-TinyML-example","order":{"number":9,"section":[1,6,1,0,0,0,0]}},{"caption":"Cloud Infrastructure and Scale","key":"sec-ml-systems-cloud-infrastructure-scale-848e","order":{"number":6,"section":[1,3,1,0,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-ml-systems-fallacies-pitfalls-8074","order":{"number":32,"section":[1,11,0,0,0,0,0]}},{"caption":"TinyML Advantages and Operational Trade-offs","key":"sec-ml-systems-tinyml-advantages-operational-tradeoffs-db08","order":{"number":19,"section":[1,6,2,0,0,0,0]}},{"caption":"Collaborative Learning","key":"sec-ml-systems-collaborative-learning-6f7b","order":{"number":27,"section":[1,7,1,5,0,0,0]}},{"caption":"Tiny ML: Ubiquitous Sensing at Scale","key":"sec-ml-systems-tiny-ml-ubiquitous-sensing-scale-51d8","order":{"number":17,"section":[1,6,0,0,0,0,0]}},{"caption":"Comparative Analysis and Selection Framework","key":"sec-ml-systems-comparative-analysis-selection-framework-832e","order":{"number":30,"section":[1,9,0,0,0,0,0]}},{"caption":"Shared Principles Across Deployment Paradigms","key":"sec-ml-systems-shared-principles-across-deployment-paradigms-915d","order":{"number":29,"section":[1,8,0,0,0,0,0]}},{"caption":"Train-Serve Split","key":"sec-ml-systems-trainserve-split-b9a1","order":{"number":23,"section":[1,7,1,1,0,0,0]}},{"caption":"Large-Scale Training and Inference","key":"sec-ml-systems-largescale-training-inference-f7a8","order":{"number":8,"section":[1,3,3,0,0,0,0]}},{"caption":"Edge Device Deployment: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse.","key":"fig-edgeml-example","order":{"number":6,"section":[1,4,2,0,0,0,0]}},{"caption":"Edge ML Dimensions: This figure outlines key considerations for edge machine learning, contrasting challenges with benefits and providing representative examples and characteristics. Understanding these dimensions enables designing and deploying effective AI solutions on resource-constrained devices.","key":"fig-edge-ml","order":{"number":5,"section":[1,4,0,0,0,0,0]}},{"caption":"Mobile ML Benefits and Resource Constraints","key":"sec-ml-systems-mobile-ml-benefits-resource-constraints-63a1","order":{"number":15,"section":[1,5,2,0,0,0,0]}},{"caption":"Deployment Locations: Machine learning systems vary in where computation occurs, from centralized cloud servers to local edge devices and ultra-low-power TinyML chips, each impacting latency, bandwidth, and energy consumption. This table categorizes these deployments by their processing location and associated characteristics, enabling informed decisions about system architecture and resource allocation.","key":"tbl-big_vs_tiny","order":{"number":2,"section":[1,9,0,0,0,0,0]}},{"caption":"Summary","key":"sec-ml-systems-summary-473b","order":{"number":33,"section":[1,12,0,0,0,0,0]}},{"caption":"Deployment Decision Logic: This flowchart guides selection of an appropriate machine learning deployment paradigm by systematically evaluating privacy requirements and processing constraints, ultimately balancing performance, cost, and data security. Navigating the decision tree helps practitioners determine whether cloud, edge, mobile, or tiny machine learning best suits a given application.","key":"fig-mlsys-playbook-flowchart","order":{"number":13,"section":[1,10,0,0,0,0,0]}},{"caption":"Personal Assistant and Media Processing","key":"sec-ml-systems-personal-assistant-media-processing-3419","order":{"number":16,"section":[1,5,3,0,0,0,0]}},{"caption":"Decision Framework for Deployment Selection","key":"sec-ml-systems-decision-framework-deployment-selection-f748","order":{"number":31,"section":[1,10,0,0,0,0,0]}},{"caption":"Device Memory Constraints: AI model deployment spans a wide range of devices with drastically different memory capacities, from cloud servers with 16 GB to microcontroller-based systems with only 320 kb. This progression necessitates specialized optimization techniques and efficient architectures to enable on-device intelligence with limited resources. Source: [@lin2023tiny].","key":"fig-vMLsizes","order":{"number":2,"section":[1,2,1,0,0,0,0]}},{"caption":"Convergence of ML Systems: Diverse machine learning deployments (cloud, edge, mobile, and tiny) share foundational principles in data pipelines, resource management, and system architecture, enabling hybrid solutions and systematic design approaches. Understanding these shared principles allows practitioners to adapt techniques across different paradigms and build cohesive, efficient ML workflows despite varying constraints and optimization goals.","key":"fig-ml-systems-convergence","order":{"number":11,"section":[1,8,0,0,0,0,0]}},{"caption":"Hybrid System Interactions: Data flows upward from sensors through processing layers to cloud analytics for insights, while trained models deploy downward from the cloud to enable inference at the edge, mobile, and Tiny ML devices. These connection types (deploy, data/results, analyze, and sync) establish a distributed architecture where each paradigm contributes unique capabilities to the overall machine learning system.","key":"fig-hybrid","order":{"number":10,"section":[1,7,2,0,0,0,0]}},{"caption":"Progressive Deployment","key":"sec-ml-systems-progressive-deployment-c8b7","order":{"number":25,"section":[1,7,1,3,0,0,0]}},{"caption":"Real-Time Industrial and IoT Systems","key":"sec-ml-systems-realtime-industrial-iot-systems-f946","order":{"number":12,"section":[1,4,3,0,0,0,0]}},{"caption":"Federated Learning","key":"sec-ml-systems-federated-learning-9850","order":{"number":26,"section":[1,7,1,4,0,0,0]}},{"caption":"Cloud ML Trade-offs and Constraints","key":"sec-ml-systems-cloud-ml-tradeoffs-constraints-1654","order":{"number":7,"section":[1,3,2,0,0,0,0]}},{"caption":"Edge ML: Reducing Latency and Privacy Risk","key":"sec-ml-systems-edge-ml-reducing-latency-privacy-risk-31f9","order":{"number":9,"section":[1,4,0,0,0,0,0]}},{"caption":"Hybrid Architectures: Combining Paradigms","key":"sec-ml-systems-hybrid-architectures-combining-paradigms-c1f2","order":{"number":21,"section":[1,7,0,0,0,0,0]}},{"caption":"TinyML System Characteristics: Constrained devices necessitate a focus on efficiency, driving trade-offs between model complexity, accuracy, and energy consumption, while enabling localized intelligence and real-time responsiveness in embedded applications. This figure outlines key aspects of TinyML, including the challenges of resource limitations, example applications, and the benefits of on-device machine learning.","key":"fig-tiny-ml","order":{"number":8,"section":[1,6,0,0,0,0,0]}},{"caption":"Deployment Paradigm Framework","key":"sec-ml-systems-deployment-paradigm-framework-d434","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Edge ML Benefits and Deployment Challenges","key":"sec-ml-systems-edge-ml-benefits-deployment-challenges-6e28","order":{"number":11,"section":[1,4,2,0,0,0,0]}},{"caption":"ML Systems","key":"sec-ml-systems","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Cloud ML: Maximizing Computational Power","key":"sec-ml-systems-cloud-ml-maximizing-computational-power-f232","order":{"number":5,"section":[1,3,0,0,0,0,0]}},{"caption":"Mobile ML Capabilities: Mobile machine learning systems balance performance with resource constraints through on-device processing, specialized hardware acceleration, and optimized frameworks. This figure outlines key considerations for deploying ML models on mobile devices, including the trade-offs between computational efficiency, battery life, and model performance.","key":"fig-mobile-ml","order":{"number":7,"section":[1,5,0,0,0,0,0]}},{"caption":"The Deployment Spectrum","key":"sec-ml-systems-deployment-spectrum-38d0","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"Cloud Data Center Scale: Large-scale machine learning systems require centralized infrastructure with massive computational resources and storage capacity. Googleâ€™s cloud TPU data center provides this need, housing specialized AI accelerator hardware to efficiently manage the demands of training and deploying complex models. Source: [@google2024gemini].","key":"fig-cloudml-example","order":{"number":4,"section":[1,3,1,0,0,0,0]}},{"caption":"Extreme Resource Constraints","key":"sec-ml-systems-extreme-resource-constraints-b788","order":{"number":18,"section":[1,6,1,0,0,0,0]}},{"caption":"Deployment Paradigm Foundations","key":"sec-ml-systems-deployment-paradigm-foundations-0c17","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Production System Case Studies","key":"sec-ml-systems-production-system-case-studies-17a6","order":{"number":28,"section":[1,7,2,0,0,0,0]}},{"caption":"Mobile ML: Personal and Offline Intelligence","key":"sec-ml-systems-mobile-ml-personal-offline-intelligence-7905","order":{"number":13,"section":[1,5,0,0,0,0,0]}},{"caption":"Multi-Tier Integration Patterns","key":"sec-ml-systems-multitier-integration-patterns-c96b","order":{"number":22,"section":[1,7,1,0,0,0,0]}},{"caption":"Environmental and Health Monitoring","key":"sec-ml-systems-environmental-health-monitoring-c9b0","order":{"number":20,"section":[1,6,3,0,0,0,0]}},{"caption":"ML System Trade-Offs: Radar plots quantify performance and operational characteristics across cloud, edge, mobile, and Tiny ML paradigms, revealing inherent trade-offs between compute power, latency, energy consumption, and scalability. These visualizations enable informed selection of the most suitable deployment approach based on application-specific constraints and priorities.","key":"fig-op_char","order":{"number":12,"section":[1,9,0,0,0,0,0]}}],"headings":["sec-ml-systems","purpose","sec-ml-systems-deployment-paradigm-framework-d434","sec-ml-systems-deployment-spectrum-38d0","sec-ml-systems-deployment-paradigm-foundations-0c17","sec-ml-systems-cloud-ml-maximizing-computational-power-f232","sec-ml-systems-cloud-infrastructure-scale-848e","sec-ml-systems-cloud-ml-tradeoffs-constraints-1654","sec-ml-systems-largescale-training-inference-f7a8","sec-ml-systems-edge-ml-reducing-latency-privacy-risk-31f9","sec-ml-systems-distributed-processing-architecture-8d28","sec-ml-systems-edge-ml-benefits-deployment-challenges-6e28","sec-ml-systems-realtime-industrial-iot-systems-f946","sec-ml-systems-mobile-ml-personal-offline-intelligence-7905","sec-ml-systems-battery-thermal-constraints-52eb","sec-ml-systems-mobile-ml-benefits-resource-constraints-63a1","sec-ml-systems-personal-assistant-media-processing-3419","sec-ml-systems-tiny-ml-ubiquitous-sensing-scale-51d8","sec-ml-systems-extreme-resource-constraints-b788","sec-ml-systems-tinyml-advantages-operational-tradeoffs-db08","sec-ml-systems-environmental-health-monitoring-c9b0","sec-ml-systems-hybrid-architectures-combining-paradigms-c1f2","sec-ml-systems-multitier-integration-patterns-c96b","sec-ml-systems-trainserve-split-b9a1","sec-ml-systems-hierarchical-processing-17a5","sec-ml-systems-progressive-deployment-c8b7","sec-ml-systems-federated-learning-9850","sec-ml-systems-collaborative-learning-6f7b","sec-ml-systems-production-system-case-studies-17a6","sec-ml-systems-shared-principles-across-deployment-paradigms-915d","sec-ml-systems-comparative-analysis-selection-framework-832e","sec-ml-systems-decision-framework-deployment-selection-f748","sec-ml-systems-fallacies-pitfalls-8074","sec-ml-systems-summary-473b","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}