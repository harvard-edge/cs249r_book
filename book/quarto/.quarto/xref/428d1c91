{"entries":[{"caption":"Communication Primitives","key":"sec-distributed-training-communication-primitives","order":{"number":50,"section":[1,8,3,0,0,0,0]}},{"caption":"Intra-layer Parallelism","key":"sec-distributed-training-intralayer-parallelism","order":{"number":42,"section":[1,6,2,2,0,0,0]}},{"caption":"Model Parallelism Limitations","key":"sec-distributed-training-model-parallelism-limitations","order":{"number":32,"section":[1,5,4,0,0,0,0]}},{"caption":"Memory-Efficient Data Parallelism: ZeRO and FSDP","key":"sec-distributed-training-zero-fsdp","order":{"number":19,"section":[1,4,4,0,0,0,0]}},{"caption":"AI Acceleration Scaling Trade-offs: Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution.","key":"tbl-distributed-scaling-trajectory","order":{"number":2,"section":[1,9,2,0,0,0,0]}},{"caption":"Parameter Updates","key":"sec-distributed-training-parameter-updates-model","order":{"number":25,"section":[1,5,1,4,0,0,0]}},{"caption":"With model layers distributed across devices, microbatching divides each training batch into smaller segments that flow through the pipeline concurrently. This approach minimizes device idle time during both forward and backward passes by allowing multiple microbatches to be processed simultaneously at different pipeline stages. Activations flow sequentially between devices during the forward pass, while gradients propagate in reverse during backpropagation.","key":"fig-pipline-parallelism","order":{"number":5,"section":[1,5,2,2,0,0,0]}},{"caption":"Model Parallelism Implementation","key":"sec-distributed-training-model-parallelism-implementation","order":{"number":21,"section":[1,5,1,0,0,0,0]}},{"caption":"Single-Machine to Distributed Transition","key":"sec-distributed-training-singlemachine-distributed-transition","order":{"number":5,"section":[1,1,3,0,0,0,0]}},{"caption":"Model Parallel Framework Support","key":"sec-distributed-training-model-parallel-framework-support","order":{"number":49,"section":[1,8,2,0,0,0,0]}},{"caption":"Wafer-Scale AI","key":"sec-distributed-training-wafer-scale","order":{"number":57,"section":[1,9,1,5,0,0,0]}},{"caption":"Distributed Training Fundamentals","key":"sec-distributed-training-fundamentals","order":{"number":6,"section":[1,2,0,0,0,0,0]}},{"caption":"Summary","key":"sec-distributed-training-summary","order":{"number":60,"section":[1,10,0,0,0,0,0]}},{"caption":"Distributed Training Efficiency Metrics","key":"sec-distributed-training-efficiency-metrics","order":{"number":7,"section":[1,3,0,0,0,0,0]}},{"caption":"Distributed machine learning scales model training by partitioning datasets across multiple GPUs, enabling concurrent computation of gradients, and then aggregating these gradients to update shared model parameters. This approach accelerates training through parallel processing while maintaining model consistency across all devices.","key":"fig-distributed-training","order":{"number":1,"section":[1,2,0,0,0,0,0]}},{"caption":"Hybrid Parallelism Implementation","key":"sec-distributed-training-hybrid-parallelism-implementation","order":{"number":34,"section":[1,6,1,0,0,0,0]}},{"caption":"Hierarchical Parallelism","key":"sec-distributed-training-hierarchical-parallelism","order":{"number":41,"section":[1,6,2,1,0,0,0]}},{"caption":"Backward Pass and Calculation","key":"sec-distributed-training-backward-pass-calculation-model","order":{"number":24,"section":[1,5,1,3,0,0,0]}},{"caption":"Data Parallelism Advantages","key":"sec-distributed-training-data-parallelism-advantages","order":{"number":17,"section":[1,4,2,0,0,0,0]}},{"caption":"Parallelism Variations","key":"sec-distributed-training-parallelism-variations-hybrid","order":{"number":40,"section":[1,6,2,0,0,0,0]}},{"caption":"Model Parallelism Advantages","key":"sec-distributed-training-model-parallelism-advantages","order":{"number":31,"section":[1,5,3,0,0,0,0]}},{"caption":"Parallel Training Strategies: Data, model, pipeline, and hybrid parallelism each address the challenges of scaling machine learning training by distributing workload across devices, differing in how they partition data and model parameters to optimize memory usage, communication, and scalability. Understanding these trade-offs enables practitioners to select the most effective approach for their specific model and infrastructure.","key":"tbl-parallelism-compare","order":{"number":1,"section":[1,7,0,0,0,0,0]}},{"caption":"Data Parallelism Implementation","key":"sec-distributed-training-data-parallelism-implementation","order":{"number":9,"section":[1,4,1,0,0,0,0]}},{"caption":"Backward Pass and Calculation","key":"sec-distributed-training-backward-pass-calculation","order":{"number":12,"section":[1,4,1,3,0,0,0]}},{"caption":"Parameter Updates","key":"sec-distributed-training-parameter-updates-hybrid","order":{"number":38,"section":[1,6,1,4,0,0,0]}},{"caption":"Hardware Infrastructure for Scale","key":"sec-distributed-training-hardware-infrastructure","order":{"number":51,"section":[1,9,0,0,0,0,0]}},{"caption":"Pipeline Parallelism","key":"sec-distributed-training-pipeline-parallelism","order":{"number":29,"section":[1,5,2,2,0,0,0]}},{"caption":"Hybrid Parallelism Limitations","key":"sec-distributed-training-hybrid-parallelism-limitations","order":{"number":45,"section":[1,6,4,0,0,0,0]}},{"caption":"Model Forward Pass","key":"sec-distributed-training-model-forward-pass","order":{"number":23,"section":[1,5,1,2,0,0,0]}},{"caption":"Parallelism Strategy Comparison","key":"sec-distributed-training-parallelism-strategy-comparison","order":{"number":46,"section":[1,7,0,0,0,0,0]}},{"caption":"Data Parallel Framework APIs","key":"sec-distributed-training-data-parallel-framework-apis","order":{"number":48,"section":[1,8,1,0,0,0,0]}},{"caption":"Gradient Synchronization","key":"sec-distributed-training-gradient-synchronization","order":{"number":13,"section":[1,4,1,4,0,0,0]}},{"caption":"Data Parallelism Limitations","key":"sec-distributed-training-data-parallelism-limitations","order":{"number":18,"section":[1,4,3,0,0,0,0]}},{"caption":"Framework Integration","key":"sec-distributed-training-framework-integration","order":{"number":47,"section":[1,8,0,0,0,0,0]}},{"caption":"Multi-Machine Training Requirements","key":"sec-distributed-training-multimachine-training-requirements","order":{"number":3,"section":[1,1,1,0,0,0,0]}},{"caption":"Iterative Process","key":"sec-distributed-training-iterative-process","order":{"number":26,"section":[1,5,1,5,0,0,0]}},{"caption":"Multi-Machine Scaling Fundamentals","key":"sec-distributed-training-multimachine-scaling-fundamentals","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Dataset Splitting","key":"sec-distributed-training-dataset-splitting","order":{"number":10,"section":[1,4,1,1,0,0,0]}},{"caption":"Data Parallelism","key":"sec-distributed-training-data-parallelism","order":{"number":8,"section":[1,4,0,0,0,0,0]}},{"caption":"Parameter Updating","key":"sec-distributed-training-parameter-updating","order":{"number":16,"section":[1,4,1,7,0,0,0]}},{"caption":"Distributed training replicates the model across multiple devices, each processing a subset of the data before aggregating gradients to update model parameters. This accelerates the training process compared to single-device training, and contrasts with model parallelism where the model itself is partitioned across devices.","key":"fig-train-data-parallelism","order":{"number":2,"section":[1,4,1,0,0,0,0]}},{"caption":"Multi-Chip AI Acceleration","key":"sec-distributed-training-multichip-acceleration","order":{"number":52,"section":[1,9,1,0,0,0,0]}},{"caption":"Layer-Wise Model Parallelism: Distributing a transformer model across multiple gpus assigns consecutive layers to each device, enabling parallel processing of input data and accelerating training. This partitioning strategy allows each GPU to operate on a subset of the model’s layers, reducing the memory footprint and computational load per device.","key":"fig-layers-blocks","order":{"number":4,"section":[1,5,2,1,0,0,0]}},{"caption":"Backward Pass and Gradient Calculation","key":"sec-distributed-training-backward-pass-gradient-calculation-hybrid","order":{"number":37,"section":[1,6,1,3,0,0,0]}},{"caption":"Layer-wise Partitioning","key":"sec-distributed-training-layerwise-partitioning","order":{"number":28,"section":[1,5,2,1,0,0,0]}},{"caption":"Distributed training systems use data, model, or hybrid parallelism based on model size, dataset size, and scaling constraints to accelerate training and efficiently utilize resources. This flowchart guides practitioners through a decision process, recognizing that real-world deployments often require adaptation due to factors like hardware heterogeneity and workload imbalance.","key":"fig-parallelism-flowchart","order":{"number":6,"section":[1,7,0,0,0,0,0]}},{"caption":"Communication Overhead and Amdahl’s Law","key":"sec-distributed-training-amdahl-analysis","order":{"number":55,"section":[1,9,1,3,0,0,0]}},{"caption":"Forward Pass","key":"sec-distributed-training-forward-pass-hybrid","order":{"number":36,"section":[1,6,1,2,0,0,0]}},{"caption":"TPU Pods","key":"sec-distributed-training-tpu-pods","order":{"number":56,"section":[1,9,1,4,0,0,0]}},{"caption":"Model and Data Partitioning","key":"sec-distributed-training-model-data-partitioning","order":{"number":35,"section":[1,6,1,1,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-distributed-training-fallacies-pitfalls","order":{"number":61,"section":[1,11,0,0,0,0,0]}},{"caption":"Hybrid Parallelism Advantages","key":"sec-distributed-training-hybrid-parallelism-advantages","order":{"number":44,"section":[1,6,3,0,0,0,0]}},{"caption":"Hardware Scaling Trade-offs","key":"sec-distributed-training-scaling-tradeoffs","order":{"number":58,"section":[1,9,2,0,0,0,0]}},{"caption":"Distributed Training Systems","key":"sec-distributed-training","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Multi-Chip Execution Strategies","key":"sec-distributed-training-execution-strategies","order":{"number":59,"section":[1,9,3,0,0,0,0]}},{"caption":"Distributing a neural network across multiple devices enables training models larger than the memory capacity of a single device. This approach requires careful coordination of data flow and gradient computation between devices to maintain training efficiency.","key":"fig-model-parallelism","order":{"number":3,"section":[1,5,1,0,0,0,0]}},{"caption":"Barrier Semantics and Failure Modes","key":"sec-distributed-training-barrier-failures","order":{"number":15,"section":[1,4,1,6,0,0,0]}},{"caption":"Distributed Training Complexity Trade-offs","key":"sec-distributed-training-complexity-tradeoffs","order":{"number":4,"section":[1,1,2,0,0,0,0]}},{"caption":"Tensor Parallelism","key":"sec-distributed-training-tensor-parallelism","order":{"number":30,"section":[1,5,2,3,0,0,0]}},{"caption":"Model Partitioning","key":"sec-distributed-training-model-partitioning","order":{"number":22,"section":[1,5,1,1,0,0,0]}},{"caption":"Device Forward Pass","key":"sec-distributed-training-device-forward-pass","order":{"number":11,"section":[1,4,1,2,0,0,0]}},{"caption":"Hybrid Parallelism","key":"sec-distributed-training-hybrid-parallelism","order":{"number":33,"section":[1,6,0,0,0,0,0]}},{"caption":"Iterative Process","key":"sec-distributed-training-iterative-process-hybrid","order":{"number":39,"section":[1,6,1,5,0,0,0]}},{"caption":"Model Parallelism","key":"sec-distributed-training-model-parallelism","order":{"number":20,"section":[1,5,0,0,0,0,0]}},{"caption":"Parallelism Variations","key":"sec-distributed-training-parallelism-variations","order":{"number":27,"section":[1,5,2,0,0,0,0]}},{"caption":"Inter-layer Parallelism","key":"sec-distributed-training-interlayer-parallelism","order":{"number":43,"section":[1,6,2,3,0,0,0]}},{"caption":"Multi-GPU Systems","key":"sec-distributed-training-multi-gpu-systems","order":{"number":54,"section":[1,9,1,2,0,0,0]}},{"caption":"Synchronization Models","key":"sec-distributed-training-sync-models","order":{"number":14,"section":[1,4,1,5,0,0,0]}},{"caption":"Chiplet-Based Architectures","key":"sec-distributed-training-chiplet-architectures","order":{"number":53,"section":[1,9,1,1,0,0,0]}}],"headings":["sec-distributed-training","purpose","sec-distributed-training-multimachine-scaling-fundamentals","sec-distributed-training-multimachine-training-requirements","sec-distributed-training-complexity-tradeoffs","sec-distributed-training-singlemachine-distributed-transition","sec-distributed-training-fundamentals","sec-distributed-training-efficiency-metrics","sec-distributed-training-data-parallelism","sec-distributed-training-data-parallelism-implementation","sec-distributed-training-dataset-splitting","sec-distributed-training-device-forward-pass","sec-distributed-training-backward-pass-calculation","sec-distributed-training-gradient-synchronization","sec-distributed-training-sync-models","sec-distributed-training-barrier-failures","sec-distributed-training-parameter-updating","sec-distributed-training-data-parallelism-advantages","sec-distributed-training-data-parallelism-limitations","sec-distributed-training-zero-fsdp","sec-distributed-training-model-parallelism","sec-distributed-training-model-parallelism-implementation","sec-distributed-training-model-partitioning","sec-distributed-training-model-forward-pass","sec-distributed-training-backward-pass-calculation-model","sec-distributed-training-parameter-updates-model","sec-distributed-training-iterative-process","sec-distributed-training-parallelism-variations","sec-distributed-training-layerwise-partitioning","sec-distributed-training-pipeline-parallelism","sec-distributed-training-tensor-parallelism","sec-distributed-training-model-parallelism-advantages","sec-distributed-training-model-parallelism-limitations","sec-distributed-training-hybrid-parallelism","sec-distributed-training-hybrid-parallelism-implementation","sec-distributed-training-model-data-partitioning","sec-distributed-training-forward-pass-hybrid","sec-distributed-training-backward-pass-gradient-calculation-hybrid","sec-distributed-training-parameter-updates-hybrid","sec-distributed-training-iterative-process-hybrid","sec-distributed-training-parallelism-variations-hybrid","sec-distributed-training-hierarchical-parallelism","sec-distributed-training-intralayer-parallelism","sec-distributed-training-interlayer-parallelism","sec-distributed-training-hybrid-parallelism-advantages","sec-distributed-training-hybrid-parallelism-limitations","sec-distributed-training-parallelism-strategy-comparison","sec-distributed-training-framework-integration","sec-distributed-training-data-parallel-framework-apis","sec-distributed-training-model-parallel-framework-support","sec-distributed-training-communication-primitives","sec-distributed-training-hardware-infrastructure","sec-distributed-training-multichip-acceleration","sec-distributed-training-chiplet-architectures","sec-distributed-training-multi-gpu-systems","sec-distributed-training-amdahl-analysis","sec-distributed-training-tpu-pods","sec-distributed-training-wafer-scale","sec-distributed-training-scaling-tradeoffs","sec-distributed-training-execution-strategies","sec-distributed-training-summary","sec-distributed-training-fallacies-pitfalls"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}