{"entries":[{"caption":"Quantitative Architecture Comparison: Computational complexity analysis across four major neural network architectures. Parameters scale with network dimensions (N=neurons, M=inputs, K=kernel size, C=channels, D=depth, H=hidden size, T=time steps, d=model dimension). Memory requirements reflect peak activation storage during training. Parallelism indicates amenability to parallel computation. Key bottlenecks represent primary performance limiting factors in typical deployments.","key":"tbl-architecture-comparison","order":{"number":7,"section":[1,8,3,0,0,0,0]}},{"caption":"Pattern Processing Needs","key":"sec-dnn-architectures-pattern-processing-needs-c45a","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Memory Access Complexity: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where rnns offer a parameter efficiency advantage when sequence length exceeds hidden state size (n > h).","key":"tbl-arch-complexity","order":{"number":3,"section":[1,7,2,0,0,0,0]}},{"caption":"Computational Complexity Considerations","key":"sec-dnn-architectures-computational-complexity-considerations-93fb","order":{"number":58,"section":[1,8,2,0,0,0,0]}},{"caption":"Computation Needs","key":"sec-dnn-architectures-computation-needs-9cb4","order":{"number":10,"section":[1,2,4,2,0,0,0]}},{"caption":"System Implications","key":"sec-dnn-architectures-system-implications-f25d","order":{"number":17,"section":[1,3,4,0,0,0,0]}},{"caption":"Architecture Selection Framework: Systematic matching of data characteristics to neural network architectures based on computational requirements and pattern types.","key":"tbl-architecture-selection","order":{"number":5,"section":[1,8,1,0,0,0,0]}},{"caption":"Pattern Processing Needs","key":"sec-dnn-architectures-pattern-processing-needs-a4ce","order":{"number":13,"section":[1,3,1,0,0,0,0]}},{"caption":"CNNs: Spatial Pattern Processing","key":"sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff","order":{"number":12,"section":[1,3,0,0,0,0,0]}},{"caption":"Computation Needs","key":"sec-dnn-architectures-computation-needs-22a5","order":{"number":19,"section":[1,3,4,2,0,0,0]}},{"caption":"Memory Requirements","key":"sec-dnn-architectures-memory-requirements-3dc1","order":{"number":36,"section":[1,5,2,3,1,0,0]}},{"caption":"Algorithmic Structure","key":"sec-dnn-architectures-algorithmic-structure-9d4b","order":{"number":40,"section":[1,5,3,1,0,0,0]}},{"caption":"Dynamic Attention Weights: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing for handling variable-length inputs and complex dependencies. Source: transformer explainer.","key":"fig-attention-weightcalc","order":{"number":7,"section":[1,5,2,1,0,0,0]}},{"caption":"Data Movement","key":"sec-dnn-architectures-data-movement-12b1","order":{"number":38,"section":[1,5,2,3,3,0,0]}},{"caption":"Efficiency and Optimization","key":"sec-dnn-architectures-efficiency-optimization-94d7","order":{"number":41,"section":[1,5,3,2,0,0,0]}},{"caption":"Residual Connection: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance.","key":"fig-example-skip-connection","order":{"number":9,"section":[1,6,4,0,0,0,0]}},{"caption":"Attention Head: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need.","key":"fig-transformer","order":{"number":8,"section":[1,5,3,2,0,0,0]}},{"caption":"Computation Needs","key":"sec-dnn-architectures-computation-needs-29be","order":{"number":28,"section":[1,4,4,2,0,0,0]}},{"caption":"This hierarchical approach processes input data through feature extraction using a convolution operation that combines a kernel and bias before applying an activation function.","key":"lst-conv_layer_spatial","order":{"number":3,"section":[1,3,3,0,0,0,0]}},{"caption":"Architectural Principles and Engineering Trade-offs","key":"sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"System Implications","key":"sec-dnn-architectures-system-implications-76dd","order":{"number":43,"section":[1,5,3,4,0,0,0]}},{"caption":"Computational Complexity Comparison: Scaling behaviors and resource requirements for major neural network architectures. Variables: d = dimension, h = hidden size, k = kernel size, c = channels, H,W = spatial dimensions, T = time steps, n = sequence length, b = batch size.","key":"tbl-computational-complexity","order":{"number":6,"section":[1,8,2,0,0,0,0]}},{"caption":"Scalability and Production Considerations","key":"sec-dnn-architectures-scalability-production-considerations-dcb0","order":{"number":59,"section":[1,8,2,1,0,0,0]}},{"caption":"Primitive-Hardware Co-Design: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware; this table maps common primitives to specific hardware accelerations and software optimizations, highlighting key challenges in their implementation. Specialized hardware, such as tensor cores and datapaths, address the computational demands of primitives like matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance.","key":"tbl-sys-design-implications","order":{"number":4,"section":[1,7,4,0,0,0,0]}},{"caption":"RNN Layer Step: Neural networks process sequential data through transformations that integrate current inputs and past states.","key":"lst-rnn_layer_step","order":{"number":5,"section":[1,4,3,0,0,0,0]}},{"caption":"Convolution as Matrix Multiplication: Reshaping convolutional layers into matrix multiplications using the im2col technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms.","key":"fig-im2col-diagram","order":{"number":10,"section":[1,7,1,1,0,0,0]}},{"caption":"Memory Requirements","key":"sec-dnn-architectures-memory-requirements-4900","order":{"number":9,"section":[1,2,4,1,0,0,0]}},{"caption":"Computational Mapping","key":"sec-dnn-architectures-computational-mapping-fea5","order":{"number":16,"section":[1,3,3,0,0,0,0]}},{"caption":"Core Computational Primitives","key":"sec-dnn-architectures-core-computational-primitives-bd67","order":{"number":50,"section":[1,7,1,0,0,0,0]}},{"caption":"Unified Framework: Inductive Biases","key":"sec-dnn-architectures-unified-framework-inductive-biases-099d","order":{"number":63,"section":[1,9,0,0,0,0,0]}},{"caption":"Computation Needs","key":"sec-dnn-architectures-computation-needs-a41e","order":{"number":37,"section":[1,5,2,3,2,0,0]}},{"caption":"Hardware Mapping and Optimization Strategies","key":"sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66","order":{"number":60,"section":[1,8,2,2,0,0,0]}},{"caption":"Architectural Building Blocks","key":"sec-dnn-architectures-architectural-building-blocks-a575","order":{"number":44,"section":[1,6,0,0,0,0,0]}},{"caption":"Decision Framework","key":"sec-dnn-architectures-decision-framework-dbe8","order":{"number":62,"section":[1,8,4,0,0,0,0]}},{"caption":"Attention Weights: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language.","key":"fig-transformer-attention-visualized","order":{"number":5,"section":[1,5,1,0,0,0,0]}},{"caption":"Energy Consumption Analysis Across Architectures","key":"sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681","order":{"number":55,"section":[1,7,4,1,0,0,0]}},{"caption":"Architecture Selection Decision Framework: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility.","key":"fig-dnn-fm-framework","order":{"number":12,"section":[1,8,4,0,0,0,0]}},{"caption":"Recurrent Layer Computation: Computes the hidden state at each time step through sequential transformations involving previous states and current inputs.","key":"lst-rnn_layer_compute","order":{"number":6,"section":[1,4,3,0,0,0,0]}},{"caption":"Computational Mapping","key":"sec-dnn-architectures-computational-mapping-fe7e","order":{"number":7,"section":[1,2,3,0,0,0,0]}},{"caption":"Algorithmic Structure","key":"sec-dnn-architectures-algorithmic-structure-a10c","order":{"number":14,"section":[1,3,2,0,0,0,0]}},{"caption":"Algorithmic Structure","key":"sec-dnn-architectures-algorithmic-structure-279b","order":{"number":23,"section":[1,4,2,0,0,0,0]}},{"caption":"Architecture Selection Framework","key":"sec-dnn-architectures-architecture-selection-framework-7a37","order":{"number":56,"section":[1,8,0,0,0,0,0]}},{"caption":"Architectural Comparison Summary","key":"sec-dnn-architectures-architectural-comparison-summary-f918","order":{"number":61,"section":[1,8,3,0,0,0,0]}},{"caption":"Memory Requirements","key":"sec-dnn-architectures-memory-requirements-7e2b","order":{"number":18,"section":[1,3,4,1,0,0,0]}},{"caption":"Layered Transformations: Multi-Layer Perceptrons (MLPs) implement dense connectivity through sequential matrix multiplications and non-linear activations, supporting complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: [@reagen2017deep].","key":"fig-mlp","order":{"number":1,"section":[1,2,2,0,0,0,0]}},{"caption":"RNNs: Sequential Pattern Processing","key":"sec-dnn-architectures-rnns-sequential-pattern-processing-ea14","order":{"number":21,"section":[1,4,0,0,0,0,0]}},{"caption":"The convolution operation processes input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position.","key":"fig-cnn","order":{"number":3,"section":[1,3,2,1,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-dnn-architectures-fallacies-pitfalls-3e82","order":{"number":64,"section":[1,10,0,0,0,0,0]}},{"caption":"Efficiency and Optimization","key":"sec-dnn-architectures-efficiency-optimization-ce92","order":{"number":24,"section":[1,4,2,1,0,0,0]}},{"caption":"Attention Mechanisms: Dynamic Pattern Processing","key":"sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d","order":{"number":30,"section":[1,5,0,0,0,0,0]}},{"caption":"Primitive Utilization: Neural network architectures differ in their core computational and memory access patterns, impacting hardware requirements and efficiency. Transformers uniquely combine matrix multiplication with attention mechanisms, resulting in random memory access and data movement patterns distinct from sequential rnns or strided cnns.","key":"tbl-primitive-comparison","order":{"number":2,"section":[1,6,4,0,0,0,0]}},{"caption":"This implementation shows neural networks performing weighted sum and activation functions across layers using matrix operations. The code emphasizes the core computational pattern in multi-layer perceptrons.","key":"lst-mlp_layer_matrix","order":{"number":1,"section":[1,2,3,0,0,0,0]}},{"caption":"Memory Access Primitives","key":"sec-dnn-architectures-memory-access-primitives-4e2e","order":{"number":52,"section":[1,7,2,0,0,0,0]}},{"caption":"Computational Building Blocks","key":"sec-dnn-architectures-computational-building-blocks-c3c0","order":{"number":51,"section":[1,7,1,1,0,0,0]}},{"caption":"Data Movement","key":"sec-dnn-architectures-data-movement-9a0e","order":{"number":20,"section":[1,3,4,3,0,0,0]}},{"caption":"Memory Requirements","key":"sec-dnn-architectures-memory-requirements-eb37","order":{"number":27,"section":[1,4,4,1,0,0,0]}},{"caption":"System Design Impact","key":"sec-dnn-architectures-system-design-impact-cd41","order":{"number":54,"section":[1,7,4,0,0,0,0]}},{"caption":"System-Level Building Blocks","key":"sec-dnn-architectures-systemlevel-building-blocks-72f6","order":{"number":49,"section":[1,7,0,0,0,0,0]}},{"caption":"Collective Communication Patterns: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four core patterns (broadcast, scatter, gather, and reduction) that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads.","key":"fig-collective-comm","order":{"number":11,"section":[1,7,3,0,0,0,0]}},{"caption":"Summary","key":"sec-dnn-architectures-summary-c495","order":{"number":65,"section":[1,11,0,0,0,0,0]}},{"caption":"Transformers: Attention-Only Architecture","key":"sec-dnn-architectures-transformers-attentiononly-architecture-c4f0","order":{"number":39,"section":[1,5,3,0,0,0,0]}},{"caption":"Computational Mapping","key":"sec-dnn-architectures-computational-mapping-7fe9","order":{"number":42,"section":[1,5,3,3,0,0,0]}},{"caption":"Architectural Characteristics","key":"sec-dnn-architectures-architectural-characteristics-e309","order":{"number":15,"section":[1,3,2,1,0,0,0]}},{"caption":"Nested Loops: Convolutional layers process input through multiple nested loops that handle batched images, spatial dimensions, output channels, kernel windows, and input features, revealing the detailed computational structure of convolution operations.","key":"lst-conv_layer_compute","order":{"number":4,"section":[1,3,3,0,0,0,0]}},{"caption":"System Implications","key":"sec-dnn-architectures-system-implications-b5aa","order":{"number":35,"section":[1,5,2,3,0,0,0]}},{"caption":"Data Movement Primitives","key":"sec-dnn-architectures-data-movement-primitives-101a","order":{"number":53,"section":[1,7,3,0,0,0,0]}},{"caption":"Evolution from Perceptron to Multi-Layer Networks","key":"sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f","order":{"number":45,"section":[1,6,1,0,0,0,0]}},{"caption":"Modern Architectures: Synthesis and Unification","key":"sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef","order":{"number":48,"section":[1,6,4,0,0,0,0]}},{"caption":"Pattern Processing Needs","key":"sec-dnn-architectures-pattern-processing-needs-b5e0","order":{"number":31,"section":[1,5,1,0,0,0,0]}},{"caption":"Attention Mechanism: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.","key":"lst-attention_layer_compute","order":{"number":7,"section":[1,5,2,2,0,0,0]}},{"caption":"Spatial Feature Extraction: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position.","key":"fig-cnn-spatial-processing","order":{"number":2,"section":[1,3,1,0,0,0,0]}},{"caption":"Evolution of Sequence Processing","key":"sec-dnn-architectures-evolution-sequence-processing-8a78","order":{"number":47,"section":[1,6,3,0,0,0,0]}},{"caption":"Query-Key-Value Interaction: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: transformer explainer.","key":"fig-attention","order":{"number":6,"section":[1,5,2,1,0,0,0]}},{"caption":"Architectural Characteristics","key":"sec-dnn-architectures-architectural-characteristics-47b4","order":{"number":6,"section":[1,2,2,1,0,0,0]}},{"caption":"Recurrent Neural Network Unfolding: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences.","key":"fig-rnn","order":{"number":4,"section":[1,4,2,1,0,0,0]}},{"caption":"Data-to-Architecture Mapping","key":"sec-dnn-architectures-datatoarchitecture-mapping-0b9c","order":{"number":57,"section":[1,8,1,0,0,0,0]}},{"caption":"Data Movement","key":"sec-dnn-architectures-data-movement-fc16","order":{"number":11,"section":[1,2,4,3,0,0,0]}},{"caption":"Self-Attention Mechanism: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.","key":"lst-self_attention_layer","order":{"number":8,"section":[1,5,3,3,0,0,0]}},{"caption":"Evolution from Dense to Spatial Processing","key":"sec-dnn-architectures-evolution-dense-spatial-processing-1d3b","order":{"number":46,"section":[1,6,2,0,0,0,0]}},{"caption":"This implementation computes each output neuron by accumulating weighted contributions from all inputs across the batch. The detailed step-by-step process exposes how a single layer in a neural network processes data, emphasizing the role of biases and weighted sums in producing outputs.","key":"lst-mlp_layer_compute","order":{"number":2,"section":[1,2,3,0,0,0,0]}},{"caption":"Multi-Layer Perceptrons: Dense Pattern Processing","key":"sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"Deep Learning Evolution: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. This table maps architectural eras to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements.","key":"tbl-dl-evolution","order":{"number":1,"section":[1,6,0,0,0,0,0]}},{"caption":"System Implications","key":"sec-dnn-architectures-system-implications-ecf5","order":{"number":26,"section":[1,4,4,0,0,0,0]}},{"caption":"Computational Mapping","key":"sec-dnn-architectures-computational-mapping-0096","order":{"number":25,"section":[1,4,3,0,0,0,0]}},{"caption":"Basic Attention Mechanism","key":"sec-dnn-architectures-basic-attention-mechanism-9500","order":{"number":32,"section":[1,5,2,0,0,0,0]}},{"caption":"Data Movement","key":"sec-dnn-architectures-data-movement-8591","order":{"number":29,"section":[1,4,4,3,0,0,0]}},{"caption":"Algorithmic Structure","key":"sec-dnn-architectures-algorithmic-structure-1af4","order":{"number":33,"section":[1,5,2,1,0,0,0]}},{"caption":"System Implications","key":"sec-dnn-architectures-system-implications-7a8f","order":{"number":8,"section":[1,2,4,0,0,0,0]}},{"caption":"Computational Mapping","key":"sec-dnn-architectures-computational-mapping-6c7e","order":{"number":34,"section":[1,5,2,2,0,0,0]}},{"caption":"Pattern Processing Needs","key":"sec-dnn-architectures-pattern-processing-needs-c18e","order":{"number":22,"section":[1,4,1,0,0,0,0]}},{"caption":"DNN Architectures","key":"sec-dnn-architectures","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Algorithmic Structure","key":"sec-dnn-architectures-algorithmic-structure-c012","order":{"number":5,"section":[1,2,2,0,0,0,0]}}],"headings":["sec-dnn-architectures","purpose","sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de","sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f","sec-dnn-architectures-pattern-processing-needs-c45a","sec-dnn-architectures-algorithmic-structure-c012","sec-dnn-architectures-architectural-characteristics-47b4","sec-dnn-architectures-computational-mapping-fe7e","sec-dnn-architectures-system-implications-7a8f","sec-dnn-architectures-memory-requirements-4900","sec-dnn-architectures-computation-needs-9cb4","sec-dnn-architectures-data-movement-fc16","sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff","sec-dnn-architectures-pattern-processing-needs-a4ce","sec-dnn-architectures-algorithmic-structure-a10c","sec-dnn-architectures-architectural-characteristics-e309","sec-dnn-architectures-computational-mapping-fea5","sec-dnn-architectures-system-implications-f25d","sec-dnn-architectures-memory-requirements-7e2b","sec-dnn-architectures-computation-needs-22a5","sec-dnn-architectures-data-movement-9a0e","sec-dnn-architectures-rnns-sequential-pattern-processing-ea14","sec-dnn-architectures-pattern-processing-needs-c18e","sec-dnn-architectures-algorithmic-structure-279b","sec-dnn-architectures-efficiency-optimization-ce92","sec-dnn-architectures-computational-mapping-0096","sec-dnn-architectures-system-implications-ecf5","sec-dnn-architectures-memory-requirements-eb37","sec-dnn-architectures-computation-needs-29be","sec-dnn-architectures-data-movement-8591","sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d","sec-dnn-architectures-pattern-processing-needs-b5e0","sec-dnn-architectures-basic-attention-mechanism-9500","sec-dnn-architectures-algorithmic-structure-1af4","sec-dnn-architectures-computational-mapping-6c7e","sec-dnn-architectures-system-implications-b5aa","sec-dnn-architectures-memory-requirements-3dc1","sec-dnn-architectures-computation-needs-a41e","sec-dnn-architectures-data-movement-12b1","sec-dnn-architectures-transformers-attentiononly-architecture-c4f0","sec-dnn-architectures-algorithmic-structure-9d4b","sec-dnn-architectures-efficiency-optimization-94d7","sec-dnn-architectures-computational-mapping-7fe9","sec-dnn-architectures-system-implications-76dd","sec-dnn-architectures-architectural-building-blocks-a575","sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f","sec-dnn-architectures-evolution-dense-spatial-processing-1d3b","sec-dnn-architectures-evolution-sequence-processing-8a78","sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef","sec-dnn-architectures-systemlevel-building-blocks-72f6","sec-dnn-architectures-core-computational-primitives-bd67","sec-dnn-architectures-computational-building-blocks-c3c0","sec-dnn-architectures-memory-access-primitives-4e2e","sec-dnn-architectures-data-movement-primitives-101a","sec-dnn-architectures-system-design-impact-cd41","sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681","sec-dnn-architectures-architecture-selection-framework-7a37","sec-dnn-architectures-datatoarchitecture-mapping-0b9c","sec-dnn-architectures-computational-complexity-considerations-93fb","sec-dnn-architectures-scalability-production-considerations-dcb0","sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66","sec-dnn-architectures-architectural-comparison-summary-f918","sec-dnn-architectures-decision-framework-dbe8","sec-dnn-architectures-unified-framework-inductive-biases-099d","sec-dnn-architectures-fallacies-pitfalls-3e82","sec-dnn-architectures-summary-c495","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}