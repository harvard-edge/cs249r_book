{"entries":[{"caption":"Biological Neural Processing Principles","key":"sec-dl-primer-biological-neural-processing-principles-3485","order":{"number":12,"section":[1,3,1,0,0,0,0]}},{"caption":"Handwritten Digit Variability: Real-world handwritten digits exhibit significant variations in stroke width, slant, and character formation, posing challenges for automated recognition systems like those used by the USPS. These examples demonstrate the need for robust feature extraction and model generalization to achieve high accuracy in optical character recognition (OCR) tasks.","key":"fig-usps-digit-examples","order":{"number":20,"section":[1,7,1,0,0,0,0]}},{"caption":"Rule-Based Programming: Traditional programs operate on data using explicitly defined rules, forming the basis for early AI systems but lacking the adaptability of modern machine learning approaches. This approach contrasts with deep learning, where the system infers rules from examples rather than relying on pre-programmed logic.","key":"fig-traditional","order":{"number":3,"section":[1,2,1,0,0,0,0]}},{"caption":"Parameter Update Algorithms","key":"sec-dl-primer-parameter-update-algorithms-2a98","order":{"number":55,"section":[1,5,5,1,0,0,0]}},{"caption":"Biological-Computational Analogies: Artificial neurons abstract key principles of biological neural systems, mapping neuron firing to activation functions, synaptic strength to weighted connections, and signal integration to summation operations—establishing a foundation for digital neural implementation. Distributed memory and parallel processing in biological systems find computational counterparts in weight matrices and concurrent computation, respectively, highlighting both the power and limitations of this abstraction.","key":"tbl-bio2comp","order":{"number":3,"section":[1,3,4,0,0,0,0]}},{"caption":"Error Measurement Fundamentals","key":"sec-dl-primer-error-measurement-fundamentals-f5b9","order":{"number":45,"section":[1,5,3,1,0,0,0]}},{"caption":"Computational Demands: Artificial neural network design directly translates into specific system requirements; for example, efficient activation functions necessitate fast nonlinear operation units and large-scale weight storage demands high-bandwidth memory access. Understanding this mapping guides hardware and system architecture choices for effective implementation of artificial intelligence.","key":"tbl-comp2sys","order":{"number":4,"section":[1,3,5,0,0,0,0]}},{"caption":"Layered Network Architecture: Deep neural networks transform data through successive layers, enabling the extraction of increasingly complex features and patterns. each layer applies non-linear transformations to the outputs of the previous layer, ultimately mapping raw inputs to desired outputs. Source: brunellon.","key":"fig-layers","order":{"number":13,"section":[1,4,1,2,0,0,0]}},{"caption":"Derivative Calculation Process","key":"sec-dl-primer-derivative-calculation-process-263f","order":{"number":52,"section":[1,5,4,3,0,0,0]}},{"caption":"Production System Architecture","key":"sec-dl-primer-production-system-architecture-2c33","order":{"number":73,"section":[1,7,3,0,0,0,0]}},{"caption":"Sigmoid","key":"sec-dl-primer-sigmoid-07c0","order":{"number":21,"section":[1,4,1,1,1,0,0]}},{"caption":"Rule-Based System: Traditional programming relies on explicitly defined rules to map inputs to outputs, limiting adaptability to complex or uncertain environments as every possible scenario must be anticipated and coded. This approach contrasts with deep learning, where systems learn patterns from data instead of relying on pre-programmed logic.","key":"fig-breakout","order":{"number":2,"section":[1,2,1,0,0,0,0]}},{"caption":"Production Deployment and Prediction Pipeline","key":"sec-dl-primer-production-deployment-prediction-pipeline-b81e","order":{"number":60,"section":[1,6,1,0,0,0,0]}},{"caption":"Supervised Learning from Labeled Examples","key":"sec-dl-primer-supervised-learning-labeled-examples-5b5b","order":{"number":38,"section":[1,5,1,0,0,0,0]}},{"caption":"Weight and Bias Storage Organization","key":"sec-dl-primer-weight-bias-storage-organization-4cc6","order":{"number":31,"section":[1,4,2,4,0,0,0]}},{"caption":"Biological Neuron Mapping: Artificial neurons abstract key functions from their biological counterparts, receiving weighted inputs at dendrites, summing them in the cell body, and producing an output via the axon, analogous to activation functions in artificial neural networks. This abstraction enables the construction of complex artificial neural networks capable of sophisticated information processing. Source: geeksforgeeks.","key":"fig-bio_nn2ai_nn","order":{"number":7,"section":[1,3,2,0,0,0,0]}},{"caption":"Biological Neuron Structure","key":"sec-dl-primer-biological-neuron-structure-ab31","order":{"number":13,"section":[1,3,2,0,0,0,0]}},{"caption":"Operational Phase Differences","key":"sec-dl-primer-operational-phase-differences-6d02","order":{"number":61,"section":[1,6,1,1,0,0,0]}},{"caption":"Data-Driven Rule Discovery: Deep learning models learn patterns and relationships directly from data, eliminating the need for manually specified rules and enabling automated feature extraction from raw inputs. This contrasts with traditional programming, where both rules and data are required to generate outputs, and classical machine learning, where rules are inferred from labeled data.","key":"fig-deeplearning","order":{"number":6,"section":[1,2,3,0,0,0,0]}},{"caption":"Weighted Input Summation: Perceptrons compute a weighted sum of multiple inputs, representing feature values, and pass the result to an activation function to produce an output. each input x_i is multiplied by a corresponding weight w_{ij} before being aggregated, forming the basis for learning complex patterns from data. using this figure.","key":"fig-perceptron","order":{"number":10,"section":[1,4,1,1,0,0,0]}},{"caption":"HOG Method: Identifies edges in images to create a histogram of gradients, transforming pixel values into shape descriptors that are invariant to lighting changes.","key":"fig-hog","order":{"number":5,"section":[1,2,2,0,0,0,0]}},{"caption":"Evolution of ML Paradigms","key":"sec-dl-primer-evolution-ml-paradigms-e0a4","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"Convergence and Stability Considerations","key":"sec-dl-primer-convergence-stability-considerations-59cf","order":{"number":58,"section":[1,5,5,4,0,0,0]}},{"caption":"Classical Machine Learning","key":"sec-dl-primer-classical-machine-learning-dec9","order":{"number":5,"section":[1,2,2,0,0,0,0]}},{"caption":"Network Connectivity Architectures","key":"sec-dl-primer-network-connectivity-architectures-19aa","order":{"number":29,"section":[1,4,2,2,0,0,0]}},{"caption":"\\text{a)} A neural network topology for classifying MNIST digits, showing how a 28\\times 28 pixel image is processed. The image on the left shows the original digit, with dimensions labeled. The network on the right shows how each pixel connects to the hidden layers, ultimately producing 10 outputs for digit classification.  \\text{b)} Alternative visualization of the MNIST network topology, showing how the 2D image is flattened into a 784-dimensional vector before being processed by the network. This representation emphasizes how spatial data is transformed into a format suitable for neural network processing.","key":"fig-mnist-topology-1","order":{"number":15,"section":[1,4,3,1,0,0,0]}},{"caption":"Forward Propagation Process: Neural networks transform input data into predictions by sequentially applying weighted sums and activation functions across interconnected layers, enabling complex pattern recognition. This layered computation forms the basis for both making inferences and updating model parameters during training.","key":"fig-forward-propagation","order":{"number":16,"section":[1,5,2,0,0,0,0]}},{"caption":"Common Activation Functions: Neural networks rely on nonlinear activation functions to approximate complex relationships. Each function exhibits distinct characteristics: sigmoid maps inputs to (0,1) with smooth gradients, tanh provides zero-centered outputs in (-1,1), ReLU introduces sparsity by outputting zero for negative inputs, and softmax converts logits into probability distributions. These different behaviors enable networks to learn different types of patterns and relationships.","key":"fig-activation-functions","order":{"number":11,"section":[1,4,1,1,0,0,0]}},{"caption":"Computational Growth: Exponential increases in computational power—initially at a 1.4× rate from 1952–2010, then accelerating to a doubling every 3.4 months from 2012–2022—enabled the scaling of deep learning models. this trend, coupled with a 10-month doubling cycle for large-scale models after 2015, directly addresses the historical bottleneck of training complex neural networks and fueled the recent advances in the field. Source: [@epochai2023trends].","key":"fig-trends","order":{"number":8,"section":[1,3,6,0,0,0,0]}},{"caption":"Data Flow Through Network Layers","key":"sec-dl-primer-data-flow-network-layers-0c58","order":{"number":26,"section":[1,4,1,3,0,0,0]}},{"caption":"Case Study: USPS Digit Recognition","key":"sec-dl-primer-case-study-usps-digit-recognition-1574","order":{"number":70,"section":[1,7,0,0,0,0,0]}},{"caption":"Deep Learning and the AI Triangle","key":"sec-dl-primer-deep-learning-ai-triangle-df90","order":{"number":76,"section":[1,8,0,0,0,0,0]}},{"caption":"Iterative Learning Process","key":"sec-dl-primer-iterative-learning-process-8458","order":{"number":57,"section":[1,5,5,3,0,0,0]}},{"caption":"Handwritten Digit Variability: Real-world data exhibits substantial variation in style, size, and orientation, necessitating robust pre-processing techniques for reliable machine learning performance. These images exemplify the challenges of digit recognition, where even seemingly simple inputs require normalization and feature extraction before they can be effectively processed by a neural network. Source: o. augereau.","key":"fig-handwritten","order":{"number":19,"section":[1,6,2,0,0,0,0]}},{"caption":"Forward Pass Optimization: During inference, neural networks prioritize computational efficiency by retaining only current layer activations and releasing intermediate states, unlike training where complete activation history is maintained for backpropagation. This optimization streamlines output generation by focusing resources on immediate computations rather than gradient preparation.","key":"tbl-forward-pass","order":{"number":6,"section":[1,6,3,2,0,0,0]}},{"caption":"Batch Loss Calculation Methods","key":"sec-dl-primer-batch-loss-calculation-methods-4502","order":{"number":47,"section":[1,5,3,3,0,0,0]}},{"caption":"Model Loading and Setup","key":"sec-dl-primer-model-loading-setup-c623","order":{"number":65,"section":[1,6,3,1,0,0,0]}},{"caption":"Computational Infrastructure Requirements","key":"sec-dl-primer-computational-infrastructure-requirements-62fd","order":{"number":7,"section":[1,2,4,0,0,0,0]}},{"caption":"Mini-Batch Gradient Updates","key":"sec-dl-primer-minibatch-gradient-updates-bb55","order":{"number":56,"section":[1,5,5,2,0,0,0]}},{"caption":"Key Engineering Lessons and Design Principles","key":"sec-dl-primer-key-engineering-lessons-design-principles-2b6a","order":{"number":75,"section":[1,7,5,0,0,0,0]}},{"caption":"Impact on Learning Dynamics","key":"sec-dl-primer-impact-learning-dynamics-6e07","order":{"number":48,"section":[1,5,3,4,0,0,0]}},{"caption":"Evolution of Neural Network Computing","key":"sec-dl-primer-evolution-neural-network-computing-ba82","order":{"number":17,"section":[1,3,6,0,0,0,0]}},{"caption":"Backpropagation Algorithm Steps","key":"sec-dl-primer-backpropagation-algorithm-steps-b7b3","order":{"number":50,"section":[1,5,4,1,0,0,0]}},{"caption":"System Resource Evolution: Programming paradigms shift system demands from sequential computation to structured parallelism with feature engineering, and finally to massive matrix operations and complex memory hierarchies in deep learning. This table clarifies how deep learning fundamentally alters system requirements compared to traditional programming and machine learning with engineered features, impacting computation and memory access patterns.","key":"tbl-evolution","order":{"number":1,"section":[1,2,4,0,0,0,0]}},{"caption":"Engineering Process and Design Decisions","key":"sec-dl-primer-engineering-process-design-decisions-2108","order":{"number":72,"section":[1,7,2,0,0,0,0]}},{"caption":"Performance Outcomes and Operational Impact","key":"sec-dl-primer-performance-outcomes-operational-impact-aea6","order":{"number":74,"section":[1,7,4,0,0,0,0]}},{"caption":"The Mail Sorting Challenge","key":"sec-dl-primer-mail-sorting-challenge-ed9a","order":{"number":71,"section":[1,7,1,0,0,0,0]}},{"caption":"Distributed Computing Requirements","key":"sec-dl-primer-distributed-computing-requirements-f7a5","order":{"number":10,"section":[1,2,4,3,0,0,0]}},{"caption":"Output Interpretation and Decision Making","key":"sec-dl-primer-output-interpretation-decision-making-224f","order":{"number":69,"section":[1,6,4,0,0,0,0]}},{"caption":"Individual Layer Processing","key":"sec-dl-primer-individual-layer-processing-15cf","order":{"number":40,"section":[1,5,2,1,0,0,0]}},{"caption":"Inference vs. Training Flow: During inference, neural networks utilize learned weights for forward pass computation only, simplifying the data flow and reducing computational cost compared to training, which requires both forward and backward passes for weight updates. This streamlined process enables efficient deployment of trained models for real-time predictions.","key":"fig-training-vs-inference","order":{"number":17,"section":[1,6,1,1,0,0,0]}},{"caption":"AI Hierarchy: Neural networks form a core component of deep learning within machine learning and artificial intelligence by modeling patterns in large datasets. Machine learning algorithms enable systems to learn from data as a subset of the broader AI field.","key":"fig-ai-ml-dl","order":{"number":1,"section":[1,1,0,0,0,0,0]}},{"caption":"DL Primer","key":"sec-dl-primer","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Memory and Computational Resources","key":"sec-dl-primer-memory-computational-resources-7e3a","order":{"number":67,"section":[1,6,3,3,0,0,0]}},{"caption":"Inference Forward Pass Execution","key":"sec-dl-primer-inference-forward-pass-execution-c54e","order":{"number":66,"section":[1,6,3,2,0,0,0]}},{"caption":"Implementation and Optimization Considerations","key":"sec-dl-primer-implementation-optimization-considerations-6069","order":{"number":43,"section":[1,5,2,4,0,0,0]}},{"caption":"Neural Network Fundamentals","key":"sec-dl-primer-neural-network-fundamentals-68cd","order":{"number":18,"section":[1,4,0,0,0,0,0]}},{"caption":"Forward Pass Computation Pipeline","key":"sec-dl-primer-forward-pass-computation-pipeline-0f06","order":{"number":64,"section":[1,6,3,0,0,0,0]}},{"caption":"Data Preprocessing and Normalization","key":"sec-dl-primer-data-preprocessing-normalization-cc37","order":{"number":63,"section":[1,6,2,0,0,0,0]}},{"caption":"Inference Pipeline: Machine learning systems transform raw inputs into final outputs through a series of sequential stages—preprocessing, neural network computation, and post-processing—each critical for accurate prediction and deployment. This pipeline emphasizes the distinction between model architecture and the complete system required for real-world application.","key":"fig-inference-pipeline","order":{"number":18,"section":[1,6,1,2,0,0,0]}},{"caption":"","key":"fig-virtuous-cycle","order":{"number":9,"section":[1,3,6,0,0,0,0]}},{"caption":"Weight Matrices","key":"sec-dl-primer-weight-matrices-dc39","order":{"number":28,"section":[1,4,2,1,0,0,0]}},{"caption":"Training vs. Inference: Neural networks transition from a computationally intensive training phase—requiring both forward and backward passes with updated parameters—to an efficient inference phase using fixed parameters and solely forward passes. This distinction enables deployment on resource-constrained devices by minimizing memory requirements and computational load during prediction.","key":"tbl-train-vs-inference","order":{"number":5,"section":[1,6,1,1,0,0,0]}},{"caption":"Softmax","key":"sec-dl-primer-softmax-ad79","order":{"number":24,"section":[1,4,1,1,4,0,0]}},{"caption":"Performance Enhancement Techniques","key":"sec-dl-primer-performance-enhancement-techniques-92d7","order":{"number":68,"section":[1,6,3,4,0,0,0]}},{"caption":"Deep Learning: Automatic Pattern Discovery","key":"sec-dl-primer-deep-learning-automatic-pattern-discovery-a3c1","order":{"number":6,"section":[1,2,3,0,0,0,0]}},{"caption":"Architecture Design","key":"sec-dl-primer-architecture-design-b3dc","order":{"number":32,"section":[1,4,3,0,0,0,0]}},{"caption":"Parallel Matrix Operation Patterns","key":"sec-dl-primer-parallel-matrix-operation-patterns-969c","order":{"number":8,"section":[1,2,4,1,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-dl-primer-fallacies-pitfalls-4464","order":{"number":77,"section":[1,9,0,0,0,0,0]}},{"caption":"Weight Update and Optimization","key":"sec-dl-primer-weight-update-optimization-20af","order":{"number":54,"section":[1,5,5,0,0,0,0]}},{"caption":"Computational Implementation Details","key":"sec-dl-primer-computational-implementation-details-7de4","order":{"number":53,"section":[1,5,4,4,0,0,0]}},{"caption":"Summary","key":"sec-dl-primer-summary-19d0","order":{"number":78,"section":[1,10,0,0,0,0,0]}},{"caption":"Hierarchical Memory Architecture","key":"sec-dl-primer-hierarchical-memory-architecture-5ae2","order":{"number":9,"section":[1,2,4,2,0,0,0]}},{"caption":"Step-by-Step Computation Sequence","key":"sec-dl-primer-stepbystep-computation-sequence-707a","order":{"number":42,"section":[1,5,2,3,0,0,0]}},{"caption":"Model Size and Computational Complexity","key":"sec-dl-primer-model-size-computational-complexity-093f","order":{"number":36,"section":[1,4,3,4,0,0,0]}},{"caption":"Hardware and Software Requirements","key":"sec-dl-primer-hardware-software-requirements-6309","order":{"number":16,"section":[1,3,5,0,0,0,0]}},{"caption":"Gradient Computation and Backpropagation","key":"sec-dl-primer-gradient-computation-backpropagation-e26a","order":{"number":49,"section":[1,5,4,0,0,0,0]}},{"caption":"From Biology to Silicon","key":"sec-dl-primer-biology-silicon-0482","order":{"number":11,"section":[1,3,0,0,0,0,0]}},{"caption":"Matrix Multiplication Formulation","key":"sec-dl-primer-matrix-multiplication-formulation-fb15","order":{"number":41,"section":[1,5,2,2,0,0,0]}},{"caption":"Loss Functions","key":"sec-dl-primer-loss-functions-d892","order":{"number":44,"section":[1,5,3,0,0,0,0]}},{"caption":"Artificial Neural Network Design Principles","key":"sec-dl-primer-artificial-neural-network-design-principles-57f0","order":{"number":14,"section":[1,3,3,0,0,0,0]}},{"caption":"Error Signal Propagation","key":"sec-dl-primer-error-signal-propagation-7e30","order":{"number":51,"section":[1,5,4,2,0,0,0]}},{"caption":"Bias Terms","key":"sec-dl-primer-bias-terms-e14c","order":{"number":30,"section":[1,4,2,3,0,0,0]}},{"caption":"Fully-Connected Layers: Multilayer perceptrons (MLPs) utilize dense connections between layers, enabling each neuron to integrate information from all neurons in the preceding layer. The weight matrices defining these connections—\\mathbf{W}^{(1)} \\in \\mathbb{R}^{n_1 \\times n_2} and \\mathbf{W}^{(2)} \\in \\mathbb{R}^{n_2 \\times n_3}—determine the strength of these integrations and facilitate learning complex patterns from input data. Source: J. McCaffrey.","key":"fig-connections","order":{"number":14,"section":[1,4,2,2,0,0,0]}},{"caption":"End-to-End Prediction Workflow","key":"sec-dl-primer-endtoend-prediction-workflow-43eb","order":{"number":62,"section":[1,6,1,2,0,0,0]}},{"caption":"Network Architecture Fundamentals","key":"sec-dl-primer-network-architecture-fundamentals-e6a7","order":{"number":19,"section":[1,4,1,0,0,0,0]}},{"caption":"Mathematical Translation of Neural Concepts","key":"sec-dl-primer-mathematical-translation-neural-concepts-b375","order":{"number":15,"section":[1,3,4,0,0,0,0]}},{"caption":"Neuron Correspondence: Biological neurons inspire artificial neuron design through analogous components—dendrites map to inputs (receiving signals), synapses map to weights (modulating connection strength), the soma to net input, and the axon to output—establishing a foundation for computational modeling of intelligence. This table clarifies how key functions of biological neurons are abstracted and implemented in artificial neural networks, enabling learning and information processing.","key":"tbl-bio_nn2ai_nn","order":{"number":2,"section":[1,3,2,0,0,0,0]}},{"caption":"Traditional Rule-Based Programming Limitations","key":"sec-dl-primer-traditional-rulebased-programming-limitations-e82d","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Feedforward Network Architecture","key":"sec-dl-primer-feedforward-network-architecture-b7e8","order":{"number":33,"section":[1,4,3,1,0,0,0]}},{"caption":"Tanh","key":"sec-dl-primer-tanh-1dfb","order":{"number":22,"section":[1,4,1,1,2,0,0]}},{"caption":"ReLU","key":"sec-dl-primer-relu-d351","order":{"number":23,"section":[1,4,1,1,3,0,0]}},{"caption":"Non-Linear Activation: Neural networks model complex relationships by applying non-linear activation functions to weighted sums of inputs, enabling the representation of non-linear decision boundaries. These functions transform input values, creating the capacity to learn intricate patterns beyond linear combinations via the arrangement of points. Source: Medium, sachin kaushik.","key":"fig-nonlinear","order":{"number":12,"section":[1,4,1,1,4,0,0]}},{"caption":"Deep Learning Systems Engineering Foundation","key":"sec-dl-primer-deep-learning-systems-engineering-foundation-822c","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Layers and Connections","key":"sec-dl-primer-layers-connections-8eb7","order":{"number":25,"section":[1,4,1,2,0,0,0]}},{"caption":"Parameters and Connections","key":"sec-dl-primer-parameters-connections-6c54","order":{"number":27,"section":[1,4,2,0,0,0,0]}},{"caption":"Cross-Entropy and Classification Loss Functions","key":"sec-dl-primer-crossentropy-classification-loss-functions-122b","order":{"number":46,"section":[1,5,3,2,0,0,0]}},{"caption":"Forward Pass Computation","key":"sec-dl-primer-forward-pass-computation-a837","order":{"number":39,"section":[1,5,2,0,0,0,0]}},{"caption":"Inference Pipeline","key":"sec-dl-primer-inference-pipeline-022b","order":{"number":59,"section":[1,6,0,0,0,0,0]}},{"caption":"Nonlinear Activation Functions","key":"sec-dl-primer-nonlinear-activation-functions-868a","order":{"number":20,"section":[1,4,1,1,0,0,0]}},{"caption":"Rule-Based Programming: Traditional programs rely on explicitly defined rules to operate on data, forming the basis for early AI systems but lacking adaptability in complex tasks.","key":"fig-activity-rules","order":{"number":4,"section":[1,2,1,0,0,0,0]}},{"caption":"Design Trade-offs: Depth vs Width vs Performance","key":"sec-dl-primer-design-tradeoffs-depth-vs-width-vs-performance-61e1","order":{"number":34,"section":[1,4,3,2,0,0,0]}},{"caption":"Layer Connectivity Design Patterns","key":"sec-dl-primer-layer-connectivity-design-patterns-4d8e","order":{"number":35,"section":[1,4,3,3,0,0,0]}},{"caption":"Learning Process","key":"sec-dl-primer-learning-process-38a0","order":{"number":37,"section":[1,5,0,0,0,0,0]}}],"headings":["sec-dl-primer","purpose","sec-dl-primer-deep-learning-systems-engineering-foundation-822c","sec-dl-primer-evolution-ml-paradigms-e0a4","sec-dl-primer-traditional-rulebased-programming-limitations-e82d","sec-dl-primer-classical-machine-learning-dec9","sec-dl-primer-deep-learning-automatic-pattern-discovery-a3c1","sec-dl-primer-computational-infrastructure-requirements-62fd","sec-dl-primer-parallel-matrix-operation-patterns-969c","sec-dl-primer-hierarchical-memory-architecture-5ae2","sec-dl-primer-distributed-computing-requirements-f7a5","sec-dl-primer-biology-silicon-0482","sec-dl-primer-biological-neural-processing-principles-3485","sec-dl-primer-biological-neuron-structure-ab31","sec-dl-primer-artificial-neural-network-design-principles-57f0","sec-dl-primer-mathematical-translation-neural-concepts-b375","sec-dl-primer-hardware-software-requirements-6309","sec-dl-primer-evolution-neural-network-computing-ba82","sec-dl-primer-neural-network-fundamentals-68cd","sec-dl-primer-network-architecture-fundamentals-e6a7","sec-dl-primer-nonlinear-activation-functions-868a","sec-dl-primer-sigmoid-07c0","sec-dl-primer-tanh-1dfb","sec-dl-primer-relu-d351","sec-dl-primer-softmax-ad79","sec-dl-primer-layers-connections-8eb7","sec-dl-primer-data-flow-network-layers-0c58","sec-dl-primer-parameters-connections-6c54","sec-dl-primer-weight-matrices-dc39","sec-dl-primer-network-connectivity-architectures-19aa","sec-dl-primer-bias-terms-e14c","sec-dl-primer-weight-bias-storage-organization-4cc6","sec-dl-primer-architecture-design-b3dc","sec-dl-primer-feedforward-network-architecture-b7e8","sec-dl-primer-design-tradeoffs-depth-vs-width-vs-performance-61e1","sec-dl-primer-layer-connectivity-design-patterns-4d8e","sec-dl-primer-model-size-computational-complexity-093f","sec-dl-primer-learning-process-38a0","sec-dl-primer-supervised-learning-labeled-examples-5b5b","sec-dl-primer-forward-pass-computation-a837","sec-dl-primer-individual-layer-processing-15cf","sec-dl-primer-matrix-multiplication-formulation-fb15","sec-dl-primer-stepbystep-computation-sequence-707a","sec-dl-primer-implementation-optimization-considerations-6069","sec-dl-primer-loss-functions-d892","sec-dl-primer-error-measurement-fundamentals-f5b9","sec-dl-primer-crossentropy-classification-loss-functions-122b","sec-dl-primer-batch-loss-calculation-methods-4502","sec-dl-primer-impact-learning-dynamics-6e07","sec-dl-primer-gradient-computation-backpropagation-e26a","sec-dl-primer-backpropagation-algorithm-steps-b7b3","sec-dl-primer-error-signal-propagation-7e30","sec-dl-primer-derivative-calculation-process-263f","sec-dl-primer-computational-implementation-details-7de4","sec-dl-primer-weight-update-optimization-20af","sec-dl-primer-parameter-update-algorithms-2a98","sec-dl-primer-minibatch-gradient-updates-bb55","sec-dl-primer-iterative-learning-process-8458","sec-dl-primer-convergence-stability-considerations-59cf","sec-dl-primer-inference-pipeline-022b","sec-dl-primer-production-deployment-prediction-pipeline-b81e","sec-dl-primer-operational-phase-differences-6d02","sec-dl-primer-endtoend-prediction-workflow-43eb","sec-dl-primer-data-preprocessing-normalization-cc37","sec-dl-primer-forward-pass-computation-pipeline-0f06","sec-dl-primer-model-loading-setup-c623","sec-dl-primer-inference-forward-pass-execution-c54e","sec-dl-primer-memory-computational-resources-7e3a","sec-dl-primer-performance-enhancement-techniques-92d7","sec-dl-primer-output-interpretation-decision-making-224f","sec-dl-primer-case-study-usps-digit-recognition-1574","sec-dl-primer-mail-sorting-challenge-ed9a","sec-dl-primer-engineering-process-design-decisions-2108","sec-dl-primer-production-system-architecture-2c33","sec-dl-primer-performance-outcomes-operational-impact-aea6","sec-dl-primer-key-engineering-lessons-design-principles-2b6a","sec-dl-primer-deep-learning-ai-triangle-df90","sec-dl-primer-fallacies-pitfalls-4464","sec-dl-primer-summary-19d0","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}