{"entries":[{"caption":"Quantization-Aware Training: Prepares a model to be trained in lower-precision formats, ensuring that quantization errors are accounted for during training.","key":"lst-qat_example","order":{"number":3,"section":[1,11,1,0,0,0,0]}},{"caption":"Training Mathematics","key":"sec-model-optimizations-training-mathematics-8359","order":{"number":81,"section":[1,7,5,2,1,0,0]}},{"caption":"Efficient Design Principles","key":"sec-model-optimizations-efficient-design-principles-9e60","order":{"number":90,"section":[1,8,1,1,0,0,0]}},{"caption":"Tensor Decomposition Applications","key":"sec-model-optimizations-tensor-decomposition-applications-a0ac","order":{"number":44,"section":[1,6,3,2,2,0,0]}},{"caption":"Dynamic Schemes","key":"sec-model-optimizations-dynamic-schemes-460f","order":{"number":95,"section":[1,8,2,1,0,0,0]}},{"caption":"Training Mathematics","key":"sec-model-optimizations-training-mathematics-f502","order":{"number":43,"section":[1,6,3,2,1,0,0]}},{"caption":"Model Efficiency Encoding","key":"sec-model-optimizations-model-efficiency-encoding-b2a6","order":{"number":48,"section":[1,6,4,1,0,0,0]}},{"caption":"Low-Rank Factorization","key":"sec-model-optimizations-lowrank-factorization-f5c5","order":{"number":39,"section":[1,6,3,1,0,0,0]}},{"caption":"TD Trade-offs and Challenges","key":"sec-model-optimizations-td-tradeoffs-challenges-8c9b","order":{"number":45,"section":[1,6,3,2,3,0,0]}},{"caption":"Sparsity Types","key":"sec-model-optimizations-sparsity-types-c61a","order":{"number":107,"section":[1,8,3,1,0,0,0]}},{"caption":"Quantization Impact: Moving from FP32 to INT8 reduces inference time by up to 4 times while decreasing model size by a factor of 4, making models more efficient for resource-constrained environments.","key":"fig-quantization_impact","order":{"number":14,"section":[1,7,1,2,0,0,0]}},{"caption":"LRMF vs. TD","key":"sec-model-optimizations-lrmf-vs-td-2d74","order":{"number":46,"section":[1,6,3,2,4,0,0]}},{"caption":"Numerical Precision","key":"sec-model-optimizations-numerical-precision-a93d","order":{"number":12,"section":[1,5,2,0,0,0,0]}},{"caption":"Sparsity and Pruning","key":"sec-model-optimizations-sparsity-pruning-15e1","order":{"number":118,"section":[1,8,3,6,1,0,0]}},{"caption":"Efficiency Gains","key":"sec-model-optimizations-efficiency-gains-d6b7","order":{"number":33,"section":[1,6,2,4,0,0,0]}},{"caption":"Sparsity Optimization Challenges: Unstructured sparsity, while reducing model size, hinders hardware acceleration due to irregular memory access patterns, limiting potential computational savings and requiring specialized hardware or software to realize efficiency gains. This table summarizes key challenges in effectively deploying sparse neural networks.","key":"tbl-sparsity-optimization","order":{"number":11,"section":[1,8,3,5,0,0,0]}},{"caption":"Pruning Strategies: Channel pruning adjusts filter sizes within layers, while layer pruning removes entire layers and necessitates reconnection of remaining network components. These approaches reduce model size and computational cost, but require fine-tuning to mitigate performance loss due to reduced model capacity.","key":"fig-channel-layer-pruning","order":{"number":3,"section":[1,6,1,3,0,0,0]}},{"caption":"Channelwise Quantization","key":"sec-model-optimizations-channelwise-quantization-9b31","order":{"number":74,"section":[1,7,5,1,3,3,0]}},{"caption":"Precision and Energy","key":"sec-model-optimizations-precision-energy-2b5a","order":{"number":59,"section":[1,7,1,0,0,0,0]}},{"caption":"Sparsity Exploitation","key":"sec-model-optimizations-sparsity-exploitation-d3d7","order":{"number":106,"section":[1,8,3,0,0,0,0]}},{"caption":"Structured Approximations","key":"sec-model-optimizations-structured-approximations-83c1","order":{"number":38,"section":[1,6,3,0,0,0,0]}},{"caption":"Implementation Strategy and Evaluation","key":"sec-model-optimizations-implementation-strategy-evaluation-a052","order":{"number":122,"section":[1,9,0,0,0,0,0]}},{"caption":"Hardware-Aware Design","key":"sec-model-optimizations-hardwareaware-design-c30a","order":{"number":89,"section":[1,8,1,0,0,0,0]}},{"caption":"Weights vs. Activations","key":"sec-model-optimizations-weights-vs-activations-f6e1","order":{"number":76,"section":[1,7,5,1,4,0,0]}},{"caption":"Deployment and System Considerations","key":"sec-model-optimizations-deployment-system-considerations-7353","order":{"number":36,"section":[1,6,2,4,3,0,0]}},{"caption":"Structured Patterns","key":"sec-model-optimizations-structured-patterns-8324","order":{"number":110,"section":[1,8,3,4,0,0,0]}},{"caption":"Precision Reduction Trade-offs","key":"sec-model-optimizations-precision-reduction-tradeoffs-dcd9","order":{"number":64,"section":[1,7,4,0,0,0,0]}},{"caption":"Hybrid Quantization Approach: Post-training quantization (PTQ) generates an initial quantized model that serves as a warm start for quantization-aware training (QAT), accelerating convergence and mitigating accuracy loss compared to quantizing a randomly initialized network. This two-stage process leverages the efficiency of PTQ while refining the model with training data to optimize performance under low-precision constraints.","key":"fig-ptq-qat","order":{"number":24,"section":[1,7,5,2,0,0,0]}},{"caption":"Computation and Inference Speed","key":"sec-model-optimizations-computation-inference-speed-f4f5","order":{"number":35,"section":[1,6,2,4,2,0,0]}},{"caption":"Pruning Example","key":"sec-model-optimizations-pruning-example-bb9f","order":{"number":17,"section":[1,6,1,1,0,0,0]}},{"caption":"Hardware Execution Inefficiencies","key":"sec-model-optimizations-hardware-execution-inefficiencies-2415","order":{"number":103,"section":[1,8,2,2,3,0,0]}},{"caption":"NAS in Practice","key":"sec-model-optimizations-nas-practice-e61f","order":{"number":55,"section":[1,6,4,8,0,0,0]}},{"caption":"Search Strategies","key":"sec-model-optimizations-search-strategies-e9a9","order":{"number":54,"section":[1,6,4,7,0,0,0]}},{"caption":"Knowledge Distillation: A student model learns from the softened probability distributions generated by a pre-trained teacher model, transferring knowledge beyond hard labels. This process enables the student to achieve comparable performance to the teacher with fewer parameters by using the teacher’s generalization capabilities and inter-class relationships.","key":"fig-kd-overview","order":{"number":8,"section":[1,6,2,0,0,0,0]}},{"caption":"Quantization-Aware Training","key":"sec-model-optimizations-quantizationaware-training-7148","order":{"number":80,"section":[1,7,5,2,0,0,0]}},{"caption":"Adaptive Inference","key":"sec-model-optimizations-adaptive-inference-808b","order":{"number":99,"section":[1,8,2,1,4,0,0]}},{"caption":"Sparse Matrix Multiplication: Block sparsity optimizes matrix operations by storing only non-zero elements and using structured indexing, enabling efficient GPU acceleration for neural network computations. this technique maintains compatibility with dense matrix operations while reducing memory access and computational cost, particularly beneficial for large-scale models. source: PyTorch blog.","key":"fig-2-4-gemm","order":{"number":29,"section":[1,8,3,4,0,0,0]}},{"caption":"Compression Trade-Offs: Combining pruning and quantization achieves superior compression ratios with minimal accuracy loss compared to quantization or singular value decomposition (SVD) alone, demonstrating the impact of different numerical precision optimization techniques on model size and performance. Architectural and numerical optimizations can complement each other to efficiently deploy machine learning models via this figure. Source: [@han2015deep].","key":"fig-compression-methods","order":{"number":25,"section":[1,7,7,0,0,0,0]}},{"caption":"Extreme Quantization","key":"sec-model-optimizations-extreme-quantization-a22e","order":{"number":85,"section":[1,7,6,0,0,0,0]}},{"caption":"Sub-channelwise Quantization","key":"sec-model-optimizations-subchannelwise-quantization-680c","order":{"number":75,"section":[1,7,5,1,3,4,0]}},{"caption":"Memory Optimization","key":"sec-model-optimizations-memory-optimization-20b5","order":{"number":93,"section":[1,8,1,4,0,0,0]}},{"caption":"Calibration Range Selection: Symmetric calibration uses a fixed range around zero, while asymmetric calibration adapts the range to the data distribution, potentially minimizing quantization error and preserving model accuracy. Choosing an appropriate calibration strategy balances precision with the risk of saturation for outlier values.","key":"fig-calibration-ranges","order":{"number":20,"section":[1,7,5,1,2,2,0]}},{"caption":"Optimization Process Visualization","key":"sec-model-optimizations-optimization-process-visualization-c381","order":{"number":133,"section":[1,11,3,0,0,0,0]}},{"caption":"Search Space Exploration","key":"sec-model-optimizations-search-space-exploration-5a0c","order":{"number":50,"section":[1,6,4,3,0,0,0]}},{"caption":"Architectural Efficiency Techniques","key":"sec-model-optimizations-architectural-efficiency-techniques-ba84","order":{"number":88,"section":[1,8,0,0,0,0,0]}},{"caption":"Architecture Examples","key":"sec-model-optimizations-architecture-examples-ebb3","order":{"number":57,"section":[1,6,4,10,0,0,0]}},{"caption":"Quantization-Aware Training: Retraining a pre-trained model with simulated low-precision arithmetic adapts weights to mitigate accuracy loss during deployment with reduced numerical precision, enabling efficient inference on resource-constrained devices. This process refines the model to become robust to the effects of quantization, maintaining performance despite lower precision representations.","key":"fig-qat","order":{"number":23,"section":[1,7,5,2,0,0,0]}},{"caption":"Mathematical Formulation","key":"sec-model-optimizations-mathematical-formulation-dade","order":{"number":18,"section":[1,6,1,2,0,0,0]}},{"caption":"Training Mathematics","key":"sec-model-optimizations-training-mathematics-0adf","order":{"number":40,"section":[1,6,3,1,1,0,0]}},{"caption":"Visualizing Quantization Effects","key":"sec-model-optimizations-visualizing-quantization-effects-3373","order":{"number":134,"section":[1,11,3,1,0,0,0]}},{"caption":"Model Optimizations","key":"sec-model-optimizations","order":{"number":1,"section":[1,0,0,0,0,0,0]}},{"caption":"Hardware-Aware Design Principles: Categorizing model design choices by their impact on computational cost, memory usage, and inference latency enables structured optimization for diverse hardware platforms and deployment scenarios. The table outlines key principles—such as minimizing data movement and exploiting parallelism—along with representative network architectures that embody these concepts.","key":"tbl-hardware-efficient-design","order":{"number":10,"section":[1,8,1,1,0,0,0]}},{"caption":"Calibration Methods","key":"sec-model-optimizations-calibration-methods-9cce","order":{"number":69,"section":[1,7,5,1,2,1,0]}},{"caption":"Sparsity and Optimization Challenges","key":"sec-model-optimizations-sparsity-optimization-challenges-47a0","order":{"number":121,"section":[1,8,3,6,4,0,0]}},{"caption":"Floating-Point Precision: Reduced-precision formats like FP16 and bfloat16 trade off numerical range for computational efficiency and memory savings. Bfloat16 maintains the exponent size of FP32, preserving its dynamic range and suitability for training, while FP16’s smaller exponent limits its use to inference or carefully scaled training scenarios.","key":"fig-3float","order":{"number":16,"section":[1,7,3,0,0,0,0]}},{"caption":"Sparsity Distribution: Pruned neural networks exhibit varying degrees of weight removal across layers; darker shades indicate higher sparsity, revealing which parts of the model were most affected by the pruning process. Analyzing this distribution helps practitioners understand and refine sparsity-aware optimization strategies for model compression and efficiency. Source: numenta","key":"fig-sprase-heat-map","order":{"number":32,"section":[1,11,3,2,0,0,0]}},{"caption":"Conditional Computation: Switch transformers enhance efficiency by dynamically routing tokens to specialized expert subnetworks, enabling parallel processing and reducing the computational load per input. this architecture implements a form of mixture of experts where a gating network selects which experts process each token, allowing for increased model capacity without a proportional increase in computation. source [@fedus2021switch].","key":"fig-switch-transformer","order":{"number":27,"section":[1,8,2,1,3,0,0]}},{"caption":"PTQ vs. QAT","key":"sec-model-optimizations-ptq-vs-qat-8429","order":{"number":84,"section":[1,7,5,2,4,0,0]}},{"caption":"Challenges and Limitations","key":"sec-model-optimizations-challenges-limitations-3760","order":{"number":116,"section":[1,8,3,5,0,0,0]}},{"caption":"Factorization Efficiency and Challenges","key":"sec-model-optimizations-factorization-efficiency-challenges-0a6a","order":{"number":41,"section":[1,6,3,1,2,0,0]}},{"caption":"Practical Deployment","key":"sec-model-optimizations-practical-deployment-6148","order":{"number":5,"section":[1,3,1,0,0,0,0]}},{"caption":"Post-Training Quantization","key":"sec-model-optimizations-posttraining-quantization-e865","order":{"number":66,"section":[1,7,5,1,0,0,0]}},{"caption":"AutoML Optimizations","key":"sec-model-optimizations-automl-optimizations-dbbf","order":{"number":127,"section":[1,10,1,0,0,0,0]}},{"caption":"Quantization Range Variation: Different convolutional filters exhibit unique activation ranges, necessitating per-filter quantization to minimize accuracy loss during quantization. Adjusting the granularity of clipping ranges—as shown by the differing scales for each filter—optimizes the trade-off between model size and performance. Source: [@gholami2021survey].","key":"fig-quantization-granularity","order":{"number":21,"section":[1,7,5,1,3,0,0]}},{"caption":"QAT Challenges and Trade-offs","key":"sec-model-optimizations-qat-challenges-tradeoffs-1246","order":{"number":83,"section":[1,7,5,2,3,0,0]}},{"caption":"Pruning Strategies: Unstructured, structured, and dynamic pruning each modify model weights differently, impacting both model size and computational efficiency; unstructured pruning offers the greatest compression but requires specialized hardware, while dynamic pruning adapts to input data for a balance between accuracy and resource usage.","key":"tbl-pruning","order":{"number":2,"section":[1,6,1,7,0,0,0]}},{"caption":"Multi-Technique Integration Strategies","key":"sec-model-optimizations-multitechnique-integration-strategies-70dc","order":{"number":125,"section":[1,9,3,0,0,0,0]}},{"caption":"Adaptive Computation Methods","key":"sec-model-optimizations-adaptive-computation-methods-4513","order":{"number":94,"section":[1,8,2,0,0,0,0]}},{"caption":"Quantization Complexity Roadmap: Three progressive tiers of quantization techniques, from foundational approaches suitable for quick deployment to research frontier methods for extreme resource constraints, reflecting increasing implementation effort, resource requirements, and potential accuracy trade-offs.","key":"fig-quantization-roadmap","order":{"number":17,"section":[1,7,5,0,0,0,0]}},{"caption":"Iterative Pruning","key":"sec-model-optimizations-iterative-pruning-5773","order":{"number":25,"section":[1,6,1,8,1,0,0]}},{"caption":"Multi-Technique Optimization Strategies","key":"sec-model-optimizations-multitechnique-optimization-strategies-8263","order":{"number":87,"section":[1,7,7,0,0,0,0]}},{"caption":"Optimization Dimensions: System constraints drive optimization along three core dimensions—model representation, numerical precision, and architectural efficiency—each addressing different resource limitations and performance goals. The table maps computational cost to precision and efficiency, memory/storage to representation and precision, and latency/throughput to representation and efficiency, guiding the selection of appropriate optimization techniques.","key":"tbl-constraint-opt-mapping","order":{"number":1,"section":[1,4,1,0,0,0,0]}},{"caption":"Quantization Trade-Offs: Post-training quantization, quantization-aware training, and dynamic quantization represent distinct approaches to model compression, each balancing accuracy, computational cost, and implementation complexity for machine learning systems. Understanding these trade-offs is important for selecting the optimal quantization strategy based on application requirements and resource constraints.","key":"tbl-quantization_methods","order":{"number":8,"section":[1,7,5,1,5,0,0]}},{"caption":"When to Use NAS","key":"sec-model-optimizations-use-nas-8419","order":{"number":56,"section":[1,6,4,9,0,0,0]}},{"caption":"Low-Rank Factorization: Decomposing a matrix into lower-rank approximations reduces the number of parameters needed for storage and computation, enabling efficient model representation. By expressing a matrix a as the product of two smaller matrices, u and v, we transition from storing m \\times n parameters to m \\times k + k \\times n parameters, with k representing the reduced rank. Source: The Clever Machine.","key":"fig-matrix-factorization","order":{"number":10,"section":[1,6,3,1,1,0,0]}},{"caption":"Measuring Optimization Effectiveness","key":"sec-model-optimizations-measuring-optimization-effectiveness-63fb","order":{"number":124,"section":[1,9,2,0,0,0,0]}},{"caption":"Architectural Efficiency","key":"sec-model-optimizations-architectural-efficiency-d507","order":{"number":13,"section":[1,5,3,0,0,0,0]}},{"caption":"Combined Optimizations","key":"sec-model-optimizations-combined-optimizations-1b0f","order":{"number":117,"section":[1,8,3,6,0,0,0]}},{"caption":"Model Representation","key":"sec-model-optimizations-model-representation-051a","order":{"number":11,"section":[1,5,1,0,0,0,0]}},{"caption":"Model Optimization Fundamentals","key":"sec-model-optimizations-model-optimization-fundamentals-064e","order":{"number":2,"section":[1,1,0,0,0,0,0]}},{"caption":"Implementation Tools and Software Frameworks","key":"sec-model-optimizations-implementation-tools-software-frameworks-5681","order":{"number":130,"section":[1,11,0,0,0,0,0]}},{"caption":"Unstructured Pruning","key":"sec-model-optimizations-unstructured-pruning-55ff","order":{"number":20,"section":[1,6,1,4,0,0,0]}},{"caption":"Trade-offs","key":"sec-model-optimizations-tradeoffs-a033","order":{"number":37,"section":[1,6,2,5,0,0,0]}},{"caption":"Calibration","key":"sec-model-optimizations-calibration-90d4","order":{"number":68,"section":[1,7,5,1,2,0,0]}},{"caption":"Summary","key":"sec-model-optimizations-summary-98df","order":{"number":138,"section":[1,14,0,0,0,0,0]}},{"caption":"Fallacies and Pitfalls","key":"sec-model-optimizations-fallacies-pitfalls-97f2","order":{"number":137,"section":[1,13,0,0,0,0,0]}},{"caption":"Dynamic Pruning","key":"sec-model-optimizations-dynamic-pruning-ada9","order":{"number":22,"section":[1,6,1,6,0,0,0]}},{"caption":"Candidate Architecture Evaluation","key":"sec-model-optimizations-candidate-architecture-evaluation-ee24","order":{"number":51,"section":[1,6,4,4,0,0,0]}},{"caption":"Technique Comparison","key":"sec-model-optimizations-technique-comparison-5bec","order":{"number":136,"section":[1,12,0,0,0,0,0]}},{"caption":"Visualizing Sparsity Patterns","key":"sec-model-optimizations-visualizing-sparsity-patterns-bf17","order":{"number":135,"section":[1,11,3,2,0,0,0]}},{"caption":"Convolutional Kernel Weights: Color mapping exposes patterns within learned convolutional filters, indicating feature detectors for edges, textures, or specific shapes within input images. Analyzing these weight distributions helps practitioners understand what features a neural network prioritizes and diagnose potential issues like dead or saturated filters—important for model calibration and performance optimization. Source: [@alexnet2012].","key":"fig-color-mapping","order":{"number":31,"section":[1,11,3,1,0,0,0]}},{"caption":"Target Structures","key":"sec-model-optimizations-target-structures-82e7","order":{"number":19,"section":[1,6,1,3,0,0,0]}},{"caption":"Precision Reduction Strategies","key":"sec-model-optimizations-precision-reduction-strategies-09f1","order":{"number":65,"section":[1,7,5,0,0,0,0]}},{"caption":"Challenges and Limitations","key":"sec-model-optimizations-challenges-limitations-7323","order":{"number":86,"section":[1,7,6,0,1,0,0]}},{"caption":"Model Compression Trade-Offs: Knowledge distillation and pruning represent distinct approaches to reducing model size and improving efficiency, each with unique strengths and weaknesses regarding accuracy, computational cost, and implementation complexity. Distillation prioritizes preserving accuracy through knowledge transfer, while pruning directly reduces computational demands by eliminating redundant parameters, making their combined use a common strategy for optimal performance.","key":"tbl-kd-pruning","order":{"number":4,"section":[1,6,2,5,0,0,0]}},{"caption":"PyTorch Pruning APIs: Applies unstructured and structured pruning techniques to reduce model complexity while maintaining performance. Source: PyTorch Documentation","key":"lst-pytorch_pruning","order":{"number":4,"section":[1,11,1,0,0,0,0]}},{"caption":"Model Optimization APIs and Tools","key":"sec-model-optimizations-model-optimization-apis-tools-5a85","order":{"number":131,"section":[1,11,1,0,0,0,0]}},{"caption":"Calibration Ranges","key":"sec-model-optimizations-calibration-ranges-4508","order":{"number":70,"section":[1,7,5,1,2,2,0]}},{"caption":"The NAS Optimization Problem","key":"sec-model-optimizations-nas-optimization-problem-9ba2","order":{"number":52,"section":[1,6,4,5,0,0,0]}},{"caption":"AutoML Optimization Challenges","key":"sec-model-optimizations-automl-optimization-challenges-be63","order":{"number":129,"section":[1,10,3,0,0,0,0]}},{"caption":"Optimization Strategies","key":"sec-model-optimizations-optimization-strategies-c725","order":{"number":128,"section":[1,10,2,0,0,0,0]}},{"caption":"Optimization Dimensions","key":"sec-model-optimizations-optimization-dimensions-e571","order":{"number":10,"section":[1,5,0,0,0,0,0]}},{"caption":"Quantization Trade-Offs: Quantization-aware training (QAT) minimizes accuracy loss from reduced numerical precision by incorporating quantization into the training process, while post-training quantization (PTQ) offers faster deployment but may require calibration to mitigate accuracy degradation. QAT’s retraining requirement increases training complexity compared to the simplicity of applying PTQ to a pre-trained model.","key":"tbl-qat","order":{"number":9,"section":[1,7,5,2,3,0,0]}},{"caption":"Structured Pruning","key":"sec-model-optimizations-structured-pruning-fa2a","order":{"number":21,"section":[1,6,1,5,0,0,0]}},{"caption":"Pruning Practice","key":"sec-model-optimizations-pruning-practice-1814","order":{"number":28,"section":[1,6,1,10,0,0,0]}},{"caption":"Distillation Intuition","key":"sec-model-optimizations-distillation-intuition-bde8","order":{"number":32,"section":[1,6,2,3,0,0,0]}},{"caption":"Layerwise Quantization","key":"sec-model-optimizations-layerwise-quantization-65f9","order":{"number":72,"section":[1,7,5,1,3,1,0]}},{"caption":"AutoML and Automated Optimization Strategies","key":"sec-model-optimizations-automl-automated-optimization-strategies-329f","order":{"number":126,"section":[1,10,0,0,0,0,0]}},{"caption":"Profiling and Opportunity Analysis","key":"sec-model-optimizations-profiling-opportunity-analysis-206b","order":{"number":123,"section":[1,9,1,0,0,0,0]}},{"caption":"Structural Model Optimization Methods","key":"sec-model-optimizations-structural-model-optimization-methods-ca9e","order":{"number":15,"section":[1,6,0,0,0,0,0]}},{"caption":"Static vs. Dynamic Quantization","key":"sec-model-optimizations-static-vs-dynamic-quantization-fed7","order":{"number":77,"section":[1,7,5,1,5,0,0]}},{"caption":"Dimensionality & Factorization: Low-rank matrix factorization (LRMF) and tensor decomposition reduce model storage requirements by representing data with fewer parameters, but introduce computational trade-offs during inference; LRMF applies to two-dimensional matrices, while tensor decomposition extends this approach to multi-dimensional tensors for greater compression potential.","key":"tbl-lrmf-tensor","order":{"number":5,"section":[1,6,3,2,4,0,0]}},{"caption":"Quantization and Precision Optimization","key":"sec-model-optimizations-quantization-precision-optimization-e90a","order":{"number":58,"section":[1,7,0,0,0,0,0]}},{"caption":"Iterative Pruning Performance: Gradual channel removal with interleaved fine-tuning maintains high accuracy while reducing model size; this figure provides a 0.4% accuracy drop with a 27% reduction in channels, showcasing the benefits of distributing structural modifications across multiple iterations. This approach contrasts with one-shot pruning, which often leads to significant performance degradation.","key":"fig-iterative-pruning","order":{"number":5,"section":[1,6,1,8,1,0,0]}},{"caption":"Knowledge Distillation","key":"sec-model-optimizations-knowledge-distillation-72e7","order":{"number":29,"section":[1,6,2,0,0,0,0]}},{"caption":"AutoML Workflow: Automated machine learning (automl) streamlines model development by structurally automating data preprocessing, model selection, and hyperparameter tuning, contrasting with traditional workflows requiring extensive manual effort for each stage. This automation enables practitioners to define high-level objectives and constraints, allowing automl systems to efficiently explore a vast design space and identify optimal model configurations.","key":"fig-automl-comparison","order":{"number":30,"section":[1,10,0,0,0,0,0]}},{"caption":"Sparsity and Quantization","key":"sec-model-optimizations-sparsity-quantization-d451","order":{"number":119,"section":[1,8,3,6,2,0,0]}},{"caption":"Future: Hardware and Sparse Networks","key":"sec-model-optimizations-future-hardware-sparse-networks-c0f6","order":{"number":115,"section":[1,8,3,4,5,0,0]}},{"caption":"Optimization Stack: Model optimization progresses through three layers (efficient model representation, efficient numerics representation, and efficient hardware implementation), each addressing distinct aspects of system performance and resource utilization. These layers allow structured trade-offs between model accuracy, computational cost, and memory footprint to meet the demands of different deployment environments.","key":"fig-3-sections","order":{"number":1,"section":[1,2,0,0,0,0,0]}},{"caption":"Scaling Optimization","key":"sec-model-optimizations-scaling-optimization-a193","order":{"number":91,"section":[1,8,1,2,0,0,0]}},{"caption":"Block Sparse Representation: NVIDIA’s cusparse library efficiently stores block sparse matrices by exploiting dense submatrix structures, enabling accelerated matrix operations while maintaining compatibility with dense matrix computations through block indexing. this approach reduces memory footprint and arithmetic complexity for sparse linear algebra, important for scaling machine learning models. source: NVIDIA..","key":"fig-block-sparse-gemm","order":{"number":28,"section":[1,8,3,4,0,0,0]}},{"caption":"Pruning Trade-Offs: Different pruning strategies balance memory efficiency, computational speed, accuracy retention, and hardware compatibility, impacting practical model deployment choices and system performance. Unstructured pruning offers high memory savings but requires specialized hardware, while structured pruning prioritizes computational efficiency at the cost of reduced accuracy.","key":"tbl-pruning-tradeoffs","order":{"number":3,"section":[1,6,1,10,0,0,0]}},{"caption":"Memory and Energy Optimization","key":"sec-model-optimizations-memory-energy-optimization-d8dc","order":{"number":114,"section":[1,8,3,4,4,0,0]}},{"caption":"Neural Architecture Search Flow: Automated NAS techniques iteratively refine model architectures and their weights, jointly optimizing for performance and efficiency, a departure from manual design approaches that rely on human expertise and extensive trial-and-error. This process enables the discovery of novel, high-performing architectures tailored to specific computational constraints.","key":"fig-nas-flow","order":{"number":12,"section":[1,6,4,0,0,0,0]}},{"caption":"Deployment Context","key":"sec-model-optimizations-deployment-context-c1b0","order":{"number":4,"section":[1,3,0,0,0,0,0]}},{"caption":"One-shot Pruning","key":"sec-model-optimizations-oneshot-pruning-2d51","order":{"number":26,"section":[1,6,1,8,2,0,0]}},{"caption":"FPGAs and Sparse Computations","key":"sec-model-optimizations-fpgas-sparse-computations-5933","order":{"number":113,"section":[1,8,3,4,3,0,0]}},{"caption":"TPUs and Sparse Optimization","key":"sec-model-optimizations-tpus-sparse-optimization-1c41","order":{"number":112,"section":[1,8,3,4,2,0,0]}},{"caption":"GPUs and Sparse Operations","key":"sec-model-optimizations-gpus-sparse-operations-1826","order":{"number":111,"section":[1,8,3,4,1,0,0]}},{"caption":"Neural Architecture Search","key":"sec-model-optimizations-neural-architecture-search-3915","order":{"number":47,"section":[1,6,4,0,0,0,0]}},{"caption":"Early Exit Architecture: Transformer layers dynamically adjust computation by classifying each layer’s output and enabling early termination if sufficient confidence is reached, reducing latency and power consumption for resource-constrained devices. This approach allows for parallel evaluation of different exit paths, improving throughput on hardware accelerators like gpus and tpus. Source: [@xin-etal-2021-berxit].","key":"fig-early-exit-transformers","order":{"number":26,"section":[1,8,2,1,1,0,0]}},{"caption":"Sparsity Utilization Methods","key":"sec-model-optimizations-sparsity-utilization-methods-1b03","order":{"number":108,"section":[1,8,3,2,0,0,0]}},{"caption":"Evaluation and Benchmarking","key":"sec-model-optimizations-evaluation-benchmarking-4235","order":{"number":105,"section":[1,8,2,2,5,0,0]}},{"caption":"Post-Training Quantization: Calibration with a representative dataset determines optimal quantization ranges for model weights and activations, minimizing information loss during quantization to create efficient, lower-precision models. This process converts a pre-trained model into a quantized version suitable for deployment on resource-constrained devices.","key":"fig-ptq-calibration","order":{"number":18,"section":[1,7,5,1,2,0,0]}},{"caption":"Search Space Design","key":"sec-model-optimizations-search-space-design-1b90","order":{"number":53,"section":[1,6,4,6,0,0,0]}},{"caption":"Generalization and Robustness","key":"sec-model-optimizations-generalization-robustness-db87","order":{"number":104,"section":[1,8,2,2,4,0,0]}},{"caption":"Numerical Format Comparison","key":"sec-model-optimizations-numerical-format-comparison-e4ad","order":{"number":63,"section":[1,7,3,0,0,0,0]}},{"caption":"Memory and Model Compression","key":"sec-model-optimizations-memory-model-compression-c077","order":{"number":34,"section":[1,6,2,4,1,0,0]}},{"caption":"PTQ Challenges and Limitations","key":"sec-model-optimizations-ptq-challenges-limitations-1ce5","order":{"number":79,"section":[1,7,5,1,7,0,0]}},{"caption":"Activation Distribution: Resnet50 layer activations exhibit a long tail, with a small percentage of values significantly larger than the majority; this distribution impacts quantization range selection, as outlier values can lead to inefficient use of precision if not handled carefully. Source: [@wu2020integer].","key":"fig-resnet-activations-histogram","order":{"number":19,"section":[1,7,5,1,2,1,0]}},{"caption":"Winning Ticket Discovery: Iterative pruning and weight resetting identify subnetworks within larger models that, when trained in isolation, achieve comparable or superior accuracy, challenging the conventional view of pruning as solely a compression technique. This process establishes that well-initialized subnetworks exist and can be efficiently trained, suggesting that much of a large network’s capacity may be redundant.","key":"fig-winning-ticket","order":{"number":7,"section":[1,6,1,9,0,0,0]}},{"caption":"Training and Optimization Difficulties","key":"sec-model-optimizations-training-optimization-difficulties-8a87","order":{"number":101,"section":[1,8,2,2,1,0,0]}},{"caption":"Implementation Challenges","key":"sec-model-optimizations-implementation-challenges-fbbd","order":{"number":100,"section":[1,8,2,2,0,0,0]}},{"caption":"Energy Costs: Lower precision reduces computational energy, illustrating trade-offs in model accuracy. Machine learning systems can optimize efficiency by reducing floating-point operations from 32-bit to 16-bit or even lower for significant savings. Source: IEEE spectrum.","key":"fig-quantized-energy","order":{"number":13,"section":[1,7,1,1,0,0,0]}},{"caption":"Navigation Strategies","key":"sec-model-optimizations-navigation-strategies-1c74","order":{"number":9,"section":[1,4,2,0,0,0,0]}},{"caption":"Computation Reduction","key":"sec-model-optimizations-computation-reduction-968a","order":{"number":92,"section":[1,8,1,3,0,0,0]}},{"caption":"Comparison of numerical precision formats.","key":"tbl-numerics","order":{"number":7,"section":[1,7,3,0,0,0,0]}},{"caption":"Conditional Computation","key":"sec-model-optimizations-conditional-computation-bd52","order":{"number":97,"section":[1,8,2,1,2,0,0]}},{"caption":"Magnitude-Based Pruning: Removes weights below a threshold to create sparse matrices, reducing the number of nonzero parameters from 9 to 4.","key":"lst-pruning_example","order":{"number":1,"section":[1,6,1,1,0,0,0]}},{"caption":"Quantization and Weight Precision: Reducing weight and activation precision from float32 to INT8 significantly lowers model size and computational cost during inference by representing values with fewer bits, though it may introduce a trade-off with model accuracy. This process alters the numerical representation of model parameters and intermediate results, impacting both memory usage and processing speed. Source: HarvardX.","key":"fig-weight-activations-quantization","order":{"number":22,"section":[1,7,5,1,4,0,0]}},{"caption":"Sparsity Hardware Support","key":"sec-model-optimizations-sparsity-hardware-support-18fc","order":{"number":109,"section":[1,8,3,3,0,0,0]}},{"caption":"Soft Target Distribution: Relative confidence levels indicate which classes are more likely for a given input, showing that a model can express uncertainty and provide nuanced outputs beyond simple correct or incorrect labels.","key":"fig-targets","order":{"number":9,"section":[1,6,2,3,0,0,0]}},{"caption":"Gate-Based Computation","key":"sec-model-optimizations-gatebased-computation-ea7c","order":{"number":98,"section":[1,8,2,1,3,0,0]}},{"caption":"Hardware-Specific Optimization Libraries","key":"sec-model-optimizations-hardwarespecific-optimization-libraries-a193","order":{"number":132,"section":[1,11,2,0,0,0,0]}},{"caption":"Early Exit Architectures","key":"sec-model-optimizations-early-exit-architectures-bd4f","order":{"number":96,"section":[1,8,2,1,1,0,0]}},{"caption":"Tensor Decomposition: Multi-dimensional tensors enable compact representations of high-dimensional data by factorizing them into lower-rank components, reducing computational costs and memory requirements compared to direct manipulation of the original tensor. This technique extends matrix factorization to handle the multi-way interactions common in modern machine learning models like convolutional neural networks. Source: [@xinyu].","key":"fig-tensor-decomposition","order":{"number":11,"section":[1,6,3,2,0,0,0]}},{"caption":"Sparsity and Model Design","key":"sec-model-optimizations-sparsity-model-design-324e","order":{"number":120,"section":[1,8,3,6,3,0,0]}},{"caption":"Three-Dimensional Optimization Framework","key":"sec-model-optimizations-threedimensional-optimization-framework-a60e","order":{"number":14,"section":[1,5,4,0,0,0,0]}},{"caption":"Groupwise Quantization","key":"sec-model-optimizations-groupwise-quantization-b2f7","order":{"number":73,"section":[1,7,5,1,3,2,0]}},{"caption":"Lottery Ticket Hypothesis","key":"sec-model-optimizations-lottery-ticket-hypothesis-6193","order":{"number":27,"section":[1,6,1,9,0,0,0]}},{"caption":"Granularity","key":"sec-model-optimizations-granularity-ba5d","order":{"number":71,"section":[1,7,5,1,3,0,0]}},{"caption":"Distillation Mathematics","key":"sec-model-optimizations-distillation-mathematics-4e1f","order":{"number":31,"section":[1,6,2,2,0,0,0]}},{"caption":"Distillation Theory","key":"sec-model-optimizations-distillation-theory-f2d4","order":{"number":30,"section":[1,6,2,1,0,0,0]}},{"caption":"Pruning Strategies","key":"sec-model-optimizations-pruning-strategies-00e3","order":{"number":24,"section":[1,6,1,8,0,0,0]}},{"caption":"Sparse Matrix Transformation: Pruning removes small-magnitude weights (shown as white/zero in the right matrix) while preserving large-magnitude weights (shown in color), creating a sparse representation that reduces both memory usage and computation while maintaining model accuracy.","key":"fig-sparse-matrix","order":{"number":2,"section":[1,6,1,1,0,0,0]}},{"caption":"Pruning Trade-offs","key":"sec-model-optimizations-pruning-tradeoffs-0902","order":{"number":23,"section":[1,6,1,7,0,0,0]}},{"caption":"Optimization Framework","key":"sec-model-optimizations-optimization-framework-1c8e","order":{"number":3,"section":[1,2,0,0,0,0,0]}},{"caption":"Pruning","key":"sec-model-optimizations-pruning-3f36","order":{"number":16,"section":[1,6,1,0,0,0,0]}},{"caption":"NAS Search Strategy Comparison: Trade-offs between search efficiency, use cases, and limitations for different NAS approaches. Reinforcement learning offers unconstrained exploration at high cost, evolutionary methods leverage parallelism, and gradient-based approaches achieve dramatic speedups with potential optimality trade-offs.","key":"tbl-nas-strategies","order":{"number":6,"section":[1,6,4,7,0,0,0]}},{"caption":"Pruning Strategies: Unstructured pruning achieves sparsity by removing individual weights, requiring specialized hardware for efficient computation, while structured pruning removes entire neurons or filters, preserving network structure and enabling acceleration on standard hardware. This figure contrasts the resulting weight matrices and network architectures from both approaches, highlighting the trade-offs between sparsity level and computational efficiency. Source: [@qi2021efficient].","key":"fig-structured-unstructured","order":{"number":4,"section":[1,6,1,5,0,0,0]}},{"caption":"Search Space Definition","key":"sec-model-optimizations-search-space-definition-a465","order":{"number":49,"section":[1,6,4,2,0,0,0]}},{"caption":"PTQ Functionality","key":"sec-model-optimizations-ptq-functionality-3990","order":{"number":67,"section":[1,7,5,1,1,0,0]}},{"caption":"Energy Costs","key":"sec-model-optimizations-energy-costs-d627","order":{"number":60,"section":[1,7,1,1,0,0,0]}},{"caption":"Performance Gains","key":"sec-model-optimizations-performance-gains-b199","order":{"number":61,"section":[1,7,1,2,0,0,0]}},{"caption":"Quantization error weighted by p(x).","key":"fig-quantization","order":{"number":15,"section":[1,7,1,2,0,0,0]}},{"caption":"One-Shot Pruning Impact: Aggressive removal of architectural components, like the 27% of channels shown, causes significant initial accuracy loss because the network struggles to adapt to significant structural changes simultaneously. Fine-tuning partially recovers performance, but establishes that iterative pruning preserves accuracy more effectively than single-step approaches.","key":"fig-oneshot-pruning","order":{"number":6,"section":[1,6,1,8,2,0,0]}},{"caption":"QAT Advantages","key":"sec-model-optimizations-qat-advantages-d421","order":{"number":82,"section":[1,7,5,2,2,0,0]}},{"caption":"Uniform Quantization: Converts FP32 weights to INT8 format, achieving 4x memory reduction while measuring quantization error.","key":"lst-quantization_example","order":{"number":2,"section":[1,7,5,1,1,0,0]}},{"caption":"Tensor Decomposition","key":"sec-model-optimizations-tensor-decomposition-c0e1","order":{"number":42,"section":[1,6,3,2,0,0,0]}},{"caption":"Numeric Encoding and Storage","key":"sec-model-optimizations-numeric-encoding-storage-d9b4","order":{"number":62,"section":[1,7,2,0,0,0,0]}},{"caption":"PTQ Advantages","key":"sec-model-optimizations-ptq-advantages-5a48","order":{"number":78,"section":[1,7,5,1,6,0,0]}},{"caption":"Overhead and Latency Variability","key":"sec-model-optimizations-overhead-latency-variability-8d30","order":{"number":102,"section":[1,8,2,2,2,0,0]}},{"caption":"Optimization Technique Trade-offs: Comparison of the three major optimization approaches across key performance dimensions, highlighting how each technique addresses different constraints and deployment scenarios. Pruning excels for computational reduction but requires sparse hardware support, quantization provides balanced size and speed improvements with wide hardware compatibility, while distillation produces high-quality compressed models at higher training cost.","key":"tbl-optimization-comparison","order":{"number":12,"section":[1,12,0,0,0,0,0]}},{"caption":"Balancing Trade-offs","key":"sec-model-optimizations-balancing-tradeoffs-27bb","order":{"number":6,"section":[1,3,2,0,0,0,0]}},{"caption":"Mapping Constraints","key":"sec-model-optimizations-mapping-constraints-021d","order":{"number":8,"section":[1,4,1,0,0,0,0]}},{"caption":"Framework Application and Navigation","key":"sec-model-optimizations-framework-application-navigation-03d4","order":{"number":7,"section":[1,4,0,0,0,0,0]}}],"headings":["sec-model-optimizations","purpose","sec-model-optimizations-model-optimization-fundamentals-064e","sec-model-optimizations-optimization-framework-1c8e","sec-model-optimizations-deployment-context-c1b0","sec-model-optimizations-practical-deployment-6148","sec-model-optimizations-balancing-tradeoffs-27bb","sec-model-optimizations-framework-application-navigation-03d4","sec-model-optimizations-mapping-constraints-021d","sec-model-optimizations-navigation-strategies-1c74","sec-model-optimizations-optimization-dimensions-e571","sec-model-optimizations-model-representation-051a","sec-model-optimizations-numerical-precision-a93d","sec-model-optimizations-architectural-efficiency-d507","sec-model-optimizations-threedimensional-optimization-framework-a60e","sec-model-optimizations-structural-model-optimization-methods-ca9e","sec-model-optimizations-pruning-3f36","sec-model-optimizations-pruning-example-bb9f","sec-model-optimizations-mathematical-formulation-dade","sec-model-optimizations-target-structures-82e7","sec-model-optimizations-unstructured-pruning-55ff","sec-model-optimizations-structured-pruning-fa2a","sec-model-optimizations-dynamic-pruning-ada9","sec-model-optimizations-pruning-tradeoffs-0902","sec-model-optimizations-pruning-strategies-00e3","sec-model-optimizations-iterative-pruning-5773","sec-model-optimizations-oneshot-pruning-2d51","sec-model-optimizations-lottery-ticket-hypothesis-6193","sec-model-optimizations-pruning-practice-1814","sec-model-optimizations-knowledge-distillation-72e7","sec-model-optimizations-distillation-theory-f2d4","sec-model-optimizations-distillation-mathematics-4e1f","sec-model-optimizations-distillation-intuition-bde8","sec-model-optimizations-efficiency-gains-d6b7","sec-model-optimizations-memory-model-compression-c077","sec-model-optimizations-computation-inference-speed-f4f5","sec-model-optimizations-deployment-system-considerations-7353","sec-model-optimizations-tradeoffs-a033","sec-model-optimizations-structured-approximations-83c1","sec-model-optimizations-lowrank-factorization-f5c5","sec-model-optimizations-training-mathematics-0adf","sec-model-optimizations-factorization-efficiency-challenges-0a6a","sec-model-optimizations-tensor-decomposition-c0e1","sec-model-optimizations-training-mathematics-f502","sec-model-optimizations-tensor-decomposition-applications-a0ac","sec-model-optimizations-td-tradeoffs-challenges-8c9b","sec-model-optimizations-lrmf-vs-td-2d74","sec-model-optimizations-neural-architecture-search-3915","sec-model-optimizations-model-efficiency-encoding-b2a6","sec-model-optimizations-search-space-definition-a465","sec-model-optimizations-search-space-exploration-5a0c","sec-model-optimizations-candidate-architecture-evaluation-ee24","sec-model-optimizations-nas-optimization-problem-9ba2","sec-model-optimizations-search-space-design-1b90","sec-model-optimizations-search-strategies-e9a9","sec-model-optimizations-nas-practice-e61f","sec-model-optimizations-use-nas-8419","sec-model-optimizations-architecture-examples-ebb3","sec-model-optimizations-quantization-precision-optimization-e90a","sec-model-optimizations-precision-energy-2b5a","sec-model-optimizations-energy-costs-d627","sec-model-optimizations-performance-gains-b199","sec-model-optimizations-numeric-encoding-storage-d9b4","sec-model-optimizations-numerical-format-comparison-e4ad","sec-model-optimizations-precision-reduction-tradeoffs-dcd9","sec-model-optimizations-precision-reduction-strategies-09f1","sec-model-optimizations-posttraining-quantization-e865","sec-model-optimizations-ptq-functionality-3990","sec-model-optimizations-calibration-90d4","sec-model-optimizations-calibration-methods-9cce","sec-model-optimizations-calibration-ranges-4508","sec-model-optimizations-granularity-ba5d","sec-model-optimizations-layerwise-quantization-65f9","sec-model-optimizations-groupwise-quantization-b2f7","sec-model-optimizations-channelwise-quantization-9b31","sec-model-optimizations-subchannelwise-quantization-680c","sec-model-optimizations-weights-vs-activations-f6e1","sec-model-optimizations-static-vs-dynamic-quantization-fed7","sec-model-optimizations-ptq-advantages-5a48","sec-model-optimizations-ptq-challenges-limitations-1ce5","sec-model-optimizations-quantizationaware-training-7148","sec-model-optimizations-training-mathematics-8359","sec-model-optimizations-qat-advantages-d421","sec-model-optimizations-qat-challenges-tradeoffs-1246","sec-model-optimizations-ptq-vs-qat-8429","sec-model-optimizations-extreme-quantization-a22e","sec-model-optimizations-challenges-limitations-7323","sec-model-optimizations-multitechnique-optimization-strategies-8263","sec-model-optimizations-architectural-efficiency-techniques-ba84","sec-model-optimizations-hardwareaware-design-c30a","sec-model-optimizations-efficient-design-principles-9e60","sec-model-optimizations-scaling-optimization-a193","sec-model-optimizations-computation-reduction-968a","sec-model-optimizations-memory-optimization-20b5","sec-model-optimizations-adaptive-computation-methods-4513","sec-model-optimizations-dynamic-schemes-460f","sec-model-optimizations-early-exit-architectures-bd4f","sec-model-optimizations-conditional-computation-bd52","sec-model-optimizations-gatebased-computation-ea7c","sec-model-optimizations-adaptive-inference-808b","sec-model-optimizations-implementation-challenges-fbbd","sec-model-optimizations-training-optimization-difficulties-8a87","sec-model-optimizations-overhead-latency-variability-8d30","sec-model-optimizations-hardware-execution-inefficiencies-2415","sec-model-optimizations-generalization-robustness-db87","sec-model-optimizations-evaluation-benchmarking-4235","sec-model-optimizations-sparsity-exploitation-d3d7","sec-model-optimizations-sparsity-types-c61a","sec-model-optimizations-sparsity-utilization-methods-1b03","sec-model-optimizations-sparsity-hardware-support-18fc","sec-model-optimizations-structured-patterns-8324","sec-model-optimizations-gpus-sparse-operations-1826","sec-model-optimizations-tpus-sparse-optimization-1c41","sec-model-optimizations-fpgas-sparse-computations-5933","sec-model-optimizations-memory-energy-optimization-d8dc","sec-model-optimizations-future-hardware-sparse-networks-c0f6","sec-model-optimizations-challenges-limitations-3760","sec-model-optimizations-combined-optimizations-1b0f","sec-model-optimizations-sparsity-pruning-15e1","sec-model-optimizations-sparsity-quantization-d451","sec-model-optimizations-sparsity-model-design-324e","sec-model-optimizations-sparsity-optimization-challenges-47a0","sec-model-optimizations-implementation-strategy-evaluation-a052","sec-model-optimizations-profiling-opportunity-analysis-206b","sec-model-optimizations-measuring-optimization-effectiveness-63fb","sec-model-optimizations-multitechnique-integration-strategies-70dc","sec-model-optimizations-automl-automated-optimization-strategies-329f","sec-model-optimizations-automl-optimizations-dbbf","sec-model-optimizations-optimization-strategies-c725","sec-model-optimizations-automl-optimization-challenges-be63","sec-model-optimizations-implementation-tools-software-frameworks-5681","sec-model-optimizations-model-optimization-apis-tools-5a85","sec-model-optimizations-hardwarespecific-optimization-libraries-a193","sec-model-optimizations-optimization-process-visualization-c381","sec-model-optimizations-visualizing-quantization-effects-3373","sec-model-optimizations-visualizing-sparsity-patterns-bf17","sec-model-optimizations-technique-comparison-5bec","sec-model-optimizations-fallacies-pitfalls-97f2","sec-model-optimizations-summary-98df","self-check-answers"],"options":{"appendix-delim":":","appendix-title":"Appendix","custom":["vidfloatvidVideo"]}}