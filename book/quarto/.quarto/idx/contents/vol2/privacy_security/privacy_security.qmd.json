{"title":"Security & Privacy","markdown":{"yaml":{"bibliography":"privacy_security.bib","quiz":"privacy_security_quizzes.json","concepts":"privacy_security_concepts.yml","glossary":"privacy_security_glossary.json"},"headingText":"Security & Privacy","headingAttr":{"id":"sec-security-privacy","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: An illustration on privacy and security in machine learning systems. The image shows a digital landscape with a network of interconnected nodes and data streams, symbolizing machine learning algorithms. In the foreground, there's a large lock superimposed over the network, representing privacy and security. The lock is semi-transparent, allowing the underlying network to be partially visible. The background features binary code and digital encryption symbols, emphasizing the theme of cybersecurity. The color scheme is a mix of blues, greens, and grays, suggesting a high-tech, digital environment._\n:::\n\n\\noindent\n![](images/png/cover_security_privacy.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why do privacy and security determine whether machine learning systems achieve widespread adoption and societal trust?_\n\nMachine learning systems require unprecedented access to personal data, institutional knowledge, and behavioral patterns to function effectively, creating tension between utility and protection that determines societal acceptance. Unlike traditional software that processes data transiently, ML systems learn from sensitive information and embed patterns into persistent models that can inadvertently reveal private details. This capability creates systemic risks extending beyond individual privacy violations to threaten institutional trust, competitive advantages, and democratic governance. Success of machine learning deployment across critical domains including healthcare, finance, education, and public services depends on establishing robust security and privacy foundations enabling beneficial use while preventing harmful exposure. Without these protections, even the most capable systems remain unused due to legal, ethical, and practical concerns. Understanding privacy and security principles enables engineers to design systems achieving both technical excellence and societal acceptance.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Differentiate between security and privacy in ML systems using formal definitions, threat models, and representative attack scenarios\n\n- Extract generalizable security principles from historical breaches (Stuxnet, Jeep Cherokee, Mirai) and apply them to distributed ML infrastructure vulnerabilities\n\n- Categorize ML-specific threats across the complete attack surface: model theft, data poisoning, adversarial examples, and hardware vulnerabilities\n\n- Implement privacy-preserving techniques (differential privacy, federated learning, synthetic data generation) with quantitative understanding of their mathematical guarantees and computational trade-offs\n\n- Analyze the layered defense architecture integrating data protection, model security, runtime monitoring, and hardware trust mechanisms for distributed ML deployments\n\n- Evaluate defense strategies using quantitative cost-benefit analysis including computational overhead, accuracy degradation, and implementation complexity\n\n- Design context-appropriate security architectures by applying the three-phase implementation roadmap prioritized for specific threat models and organizational constraints\n\n- Assess distributed systems attack surface expansion including gradient exchange vulnerabilities, edge endpoint multiplication, and multi-cloud dependency chains\n\n:::\n\n## Security and Privacy in ML Systems {#sec-security-privacy-security-privacy-ml-systems-0b1e}\n\nThe shift from centralized training architectures to distributed, adaptive machine learning systems has altered the threat landscape and security requirements for modern ML infrastructure. Contemporary machine learning systems, as examined in @sec-edge-intelligence, increasingly operate across heterogeneous computational environments spanning edge devices, federated networks, and hybrid cloud deployments. This architectural evolution enables new capabilities in adaptive intelligence but introduces attack vectors and privacy vulnerabilities that traditional cybersecurity frameworks cannot adequately address.\n\nMachine learning systems exhibit different security characteristics compared to conventional software applications. Traditional software systems process data transiently and deterministically, whereas machine learning systems extract and encode patterns from training data into persistent model parameters. This learned knowledge representation creates unique vulnerabilities where sensitive information can be inadvertently memorized and later exposed through model outputs or systematic interrogation. Such risks manifest across domains from healthcare systems that may leak patient information to proprietary models that can be reverse-engineered through strategic query patterns, threatening both individual privacy and organizational intellectual property.\n\nThe architectural complexity of machine learning systems compounds these security challenges through multi-layered attack surfaces. Contemporary ML deployments include data ingestion pipelines, distributed training infrastructure, model serving systems, and continuous monitoring frameworks. Each architectural component introduces distinct vulnerabilities while privacy concerns affect the entire computational stack. The distributed nature of modern deployments, with continuous adaptation at edge nodes and federated coordination protocols, expands the attack surface while complicating comprehensive security implementation.\n\nAddressing these challenges requires systematic approaches that integrate security and privacy considerations throughout the machine learning system lifecycle. This chapter establishes the foundations and methodologies necessary for engineering ML systems that achieve both computational effectiveness and trustworthy operation. We examine the application of established security principles to machine learning contexts, identify threat models specific to learning systems, and present comprehensive defense strategies that include data protection mechanisms, secure model architectures, and hardware-based security implementations.\n\nOur investigation proceeds through four interconnected frameworks. We begin by establishing distinctions between security and privacy within machine learning contexts, then examine evidence from historical security incidents to inform contemporary threat assessment. We analyze vulnerabilities that emerge from the learning process itself, before presenting layered defense architectures that span cryptographic data protection, adversarial-robust model design, and hardware security mechanisms. Throughout this analysis, we emphasize implementation guidance that enables practitioners to develop systems meeting both technical performance requirements and the trust standards necessary for societal deployment.\n\n## Foundational Concepts and Definitions {#sec-security-privacy-foundational-concepts-definitions-d529}\n\nSecurity and privacy are core concerns in machine learning system design, but they are often misunderstood or conflated. Both aim to protect systems and data, yet they do so in different ways, address different threat models, and require distinct technical responses. For ML systems, distinguishing between the two helps guide the design of robust and responsible infrastructure.\n\n### Security Defined {#sec-security-privacy-security-defined-1129}\n\nSecurity in machine learning focuses on defending systems from adversarial behavior. This includes protecting model parameters, training pipelines, deployment infrastructure, and data access pathways from manipulation or misuse.\n\n:::{.callout-definition title=\"Security\"}\n***Security*** is the protection of ML system _data_, _models_, and _infrastructure_ from _unauthorized access_, _manipulation_, and _disruption_ through _defensive mechanisms_ spanning development, deployment, and operational environments.\n:::\n\n*Example*: A facial recognition system deployed in public transit infrastructure may be targeted with adversarial inputs that cause it to misidentify individuals or fail entirely. This is a runtime security vulnerability that threatens both accuracy and system availability.\n\n### Privacy Defined {#sec-security-privacy-privacy-defined-da84}\n\nWhile security addresses adversarial threats, privacy focuses on limiting the exposure and misuse of sensitive information within ML systems. This includes protecting training data, inference inputs, and model outputs from leaking personal or proprietary information, even when systems operate correctly and no explicit attack is taking place.\n\n:::{.callout-definition title=\"Privacy\"}\n***Privacy*** is the protection of _sensitive information_ from _unauthorized disclosure_, _inference_, and _misuse_ through methods that preserve _confidentiality_ and _control over data usage_ across ML system environments.\n:::\n\n*Example*: A language model trained on medical transcripts may inadvertently memorize snippets of patient conversations. If a user later triggers this content through a public-facing chatbot, it represents a privacy failure, even in the absence of an attacker.\n\n### Security versus Privacy {#sec-security-privacy-security-versus-privacy-e0b8}\n\nAlthough they intersect in some areas (encrypted storage supports both), security and privacy differ in their objectives, threat models, and typical mitigation strategies. @tbl-security-privacy-comparison below summarizes these distinctions in the context of machine learning systems.\n\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Aspect**                  | **Security**                               | **Privacy**                                   |\n+:============================+:===========================================+:==============================================+\n| **Primary Goal**            | Prevent unauthorized access or disruption  | Limit exposure of sensitive information       |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Threat Model**            | Adversarial actors (external or internal)  | Honest-but-curious observers or passive leaks |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Typical Concerns**        | Model theft, poisoning, evasion attacks    | Data leakage, re-identification, memorization |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Example Attack**          | Adversarial inputs cause misclassification | Model inversion reveals training data         |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Representative Defenses** | Access control, adversarial training       | Differential privacy, federated learning      |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Relevance to Regulation** | Emphasized in cybersecurity standards      | Central to data protection laws (e.g., GDPR)  |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n\n: **Security-Privacy Distinctions**: Machine learning systems require distinct approaches to security and privacy; security mitigates adversarial threats targeting system functionality, while privacy protects sensitive information from both intentional and unintentional exposure through data leakage or re-identification. This table clarifies how differing goals and threat models shape the specific concerns and mitigation strategies for each domain. {#tbl-security-privacy-comparison}\n\n### Security-Privacy Interactions and Trade-offs {#sec-security-privacy-securityprivacy-interactions-tradeoffs-d153}\n\nSecurity and privacy are deeply interrelated but not interchangeable. A secure system helps maintain privacy by restricting unauthorized access to models and data. Privacy-preserving designs can improve security by reducing the attack surface, for example, minimizing the retention of sensitive data reduces the risk of exposure if a system is compromised.\n\nHowever, they can also be in tension. Techniques like differential privacy[^fn-dp-origins] reduce memorization risks but may lower model utility. Similarly, encryption enhances security but may obscure transparency and auditability, complicating privacy compliance. In machine learning systems, designers must reason about these trade-offs holistically. Systems that serve sensitive domains, including healthcare, finance, and public safety, must simultaneously protect against both misuse (security) and overexposure (privacy). Understanding the boundaries between these concerns is essential for building systems that are performant, trustworthy, and legally compliant.\n\n[^fn-dp-origins]: **Differential Privacy Origins**: Cynthia Dwork coined the term differential privacy at Microsoft Research in 2006, but the concept emerged from her frustration with the \"anonymization myth\" (the false belief that removing names from data guaranteed privacy). Her groundbreaking insight was that privacy should be mathematically provable, not just plausible, leading to the rigorous framework that now protects billions of users' data in products from Apple to Google.\n\n## Learning from Security Breaches {#sec-security-privacy-learning-security-breaches-6719}\n\nHaving established the conceptual foundations of security and privacy, we now examine how these principles manifest in real-world systems through landmark security incidents. These historical cases provide concrete illustrations of the abstract concepts we've defined, showing how security vulnerabilities emerge and propagate through complex systems. More importantly, they reveal universal patterns (supply chain compromise, insufficient isolation, and weaponized endpoints) that directly apply to modern machine learning deployments.\n\nValuable lessons can be drawn from well-known security breaches across a range of computing systems. Understanding how these patterns apply to modern ML deployments, which increasingly operate across cloud, edge, and embedded environments, provides important lessons for securing machine learning systems. These incidents demonstrate how weaknesses in system design can lead to widespread, and sometimes physical, consequences. Although the examples discussed in this section do not all involve machine learning directly, they provide important insights into designing secure systems. These lessons apply to machine learning applications deployed across cloud, edge, and embedded environments.\n\n### Supply Chain Compromise: Stuxnet {#sec-security-privacy-supply-chain-compromise-stuxnet-8a4b}\n\nIn 2010, security researchers discovered a highly sophisticated computer worm later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stuxnet]. Stuxnet exploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through networked and isolated systems.\n\n[^fn-stuxnet-discovery]: **Stuxnet Discovery**: Stuxnet was first detected by VirusBokNok, a small Belarusian antivirus company, when their client computers began crashing unexpectedly. What seemed like a routine malware investigation turned into one of the most significant cybersecurity discoveries in history: the first confirmed cyberweapon designed to cause physical destruction.\n\n[^fn-zero-day-term]: **Zero-Day Term Origin**: The term \"zero-day\" originated in software piracy circles, referring to the \"zero days\" since a program's release when pirated copies appeared. In security, it describes the \"zero days\" defenders have to patch a vulnerability before attackers exploit it, representing the ultimate race between attack and defense.\n\nUnlike typical malware designed to steal information or perform espionage, Stuxnet was engineered to cause physical damage. Its objective was to disrupt uranium enrichment by sabotaging the centrifuges used in the process. Despite the facility being air-gapped[^fn-air-gapped] from external networks, the malware is believed to have entered the system via an infected USB device[^fn-usb-attacks], demonstrating how physical access can compromise isolated environments.\n\nThe worm specifically targeted programmable logic controllers (PLCs), industrial computers that automate electromechanical processes such as controlling the speed of centrifuges. By exploiting vulnerabilities in the Windows operating system and the Siemens Step7 software used to program the PLCs, Stuxnet achieved highly targeted, real-world disruption. This represents a landmark in cybersecurity, demonstrating how malicious software can bridge the digital and physical worlds to manipulate industrial infrastructure.\n\n[^fn-air-gapped]: **Air-Gapped Systems**: Air-gapped systems are networks physically isolated from external connections, originally developed for military systems in the 1960s. Despite seeming impenetrable, studies show 90% of air-gapped systems can be breached through supply chain compromise, infected removable media, or hidden channels (acoustic, electromagnetic, thermal) [@farwell2011stuxnet].\n\n[^fn-usb-attacks]: **USB Attacks**: USB interfaces, introduced in 1996, became a primary attack vector for crossing air gaps. The 2008 Operation Olympic Games reportedly used infected USB drives to penetrate secure facilities, with some estimates suggesting 60% of organizations remain vulnerable to USB-based attacks [@farwell2011stuxnet].\n\nThe lessons from Stuxnet directly apply to modern ML systems. Training pipelines and model repositories face persistent supply chain risks analogous to those exploited by Stuxnet. Just as Stuxnet compromised industrial systems through infected USB devices and software vulnerabilities, modern ML systems face multiple attack vectors: compromised dependencies (malicious packages in PyPI/conda repositories), malicious training data (poisoned datasets on HuggingFace, Kaggle), backdoored model weights (trojan models in model repositories), and tampered hardware drivers (compromised NVIDIA CUDA libraries, firmware backdoors in AI accelerators).\n\nA concrete ML attack scenario illustrates these risks: an attacker uploads a backdoored image classification model to a popular model repository, trained to misclassify specific patterns while maintaining normal accuracy on clean data. When deployed in autonomous vehicles, this backdoored model correctly identifies most objects but fails to detect pedestrians wearing specific patterns, creating safety risks. The attack propagates through automated model deployment pipelines, affecting thousands of vehicles before detection.\n\nDefending against such supply chain attacks requires end-to-end security measures: (1) cryptographic verification to sign all model artifacts, datasets, and dependencies with cryptographic signatures; (2) provenance tracking to maintain immutable logs of all training data sources, code versions, and infrastructure used; (3) integrity validation to implement automated scanning for model backdoors, dependency vulnerabilities, and dataset poisoning before deployment; (4) air-gapped training to isolate sensitive model training in secure environments with controlled dependency management. @fig-stuxnet illustrates how these supply chain compromise patterns apply across both industrial and ML systems.\n\n::: {#fig-stuxnet fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line cap=round,line join=round,font=\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nTxtL/.style = {font=\\large\\usefont{T1}{phv}{m}{n},text width=90mm,align=justify,anchor=north},\nLine/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},\nLineA/.style={violet!50,line width=2.0pt,{-{Triangle[width=1.1*6pt,length=2.0*6pt]}},shorten <=3pt,shorten >=2pt},\nALine/.style={black!50, line width=1.1pt,{{Triangle[width=1.1*6pt,length=2*6pt]}-}},\nLarrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,\n            single arrow head indent=0pt,minimum height=9mm, minimum width=15pt}\n}\n%Skull\n\\tikzset{pics/skull/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=SKULL,scale=\\scalefac, every node/.append style={transform shape}]\n\\fill[fill=\\filllcolor](-0.225,-0.05)to[out=110,in=230](-0.215,0.2)to[out=50,in=180](0,0.315)\nto[out=0,in=130](0.218,0.2)to[out=310,in=70](0.227,-0.05) to[out=320,in=40](0.21,-0.15)\nto[out=210,in=80](0.14,-0.23) to[out=260,in=20](0.04,-0.285) to[out=200,in=340](-0.07,-0.28)\nto[out=170,in=290](-0.135,-0.23) to[out=110,in=340](-0.21,-0.15) to[out=140,in=250]cycle;\n%eyes\n\\fill[fill=\\filllcirclecolor](-0.17,-0.02)to[out=70,in=110](-0.029,-0.02)to[out=280,in=0](-0.129,-0.11)to[out=190,in=250]cycle;\n\\fill[fill=\\filllcirclecolor](0.035,-0.02)to[out=70,in=110](0.175,-0.02)to[out=300,in=340](0.12,-0.103)to[out=170,in=260]cycle;\n%nose\n\\fill[fill=\\filllcirclecolor](0.018,-0.115)to[out=70,in=110](-0.014,-0.115)to(-0.043,-0.165)\nto[out=200,in=170](-0.025,-0.19)to(0.027,-0.19)to[out=10,in=330](0.047,-0.165)to cycle;\n%above left\n\\fill[fill=\\filllcolor](-0.2,0.18)to[out=160,in=320](-0.3,0.23)to[out=140,in=0](-0.37,0.295)\nto[out=180,in=80](-0.43,0.25)to[out=230,in=90](-0.475,0.19)\nto[out=260,in=170](-0.375,0.13)to[out=350,in=170](-0.2,0.1)to cycle;\n%abover right\n\\fill[fill=\\filllcolor](0.2,0.18)to[out=20,in=220](0.3,0.23)to[out=40,in=200](0.37,0.295)\nto[out=20,in=90](0.43,0.25)to[out=230,in=90](0.475,0.19)to[out=260,in=360](0.375,0.13)\nto[out=190,in=10](0.2,0.1)to cycle;\n%below left\n\\fill[fill=\\filllcolor](-0.2,0.03)to[out=210,in=0](-0.3,0.01)to[out=180,in=0](-0.37,0.01)\nto[out=180,in=50](-0.46,0.0)to[out=230,in=120](-0.445,-0.08)\nto[out=260,in=170](-0.41,-0.14)to[out=350,in=190](-0.2,-0.051)to cycle;\n%below right\n\\fill[fill=\\filllcolor](0.2,0.03)to[out=340,in=170](0.3,0.01)to[out=350,in=190](0.37,0.01)\nto[out=20,in=110](0.47,-0.03)to[out=270,in=120](0.443,-0.09)\nto[out=270,in=0](0.36,-0.15)to[out=160,in=340](0.2,-0.051)to cycle;\n\\end{scope}\n     }\n  }\n}\n%laptop\n\\tikzset{\npics/laptop/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\node[rounded corners=2pt,rectangle,minimum width=60,minimum height=37,\nfill=\\filllcolor!60,line width=\\Linewidth,draw=black](EKV\\picname)at(0,0.53){};\n%\n\\ifnum\\Dual=1\n\\node[draw=black,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,\nfill=\\filllcolor!10,line width=\\Linewidth,](EK)at(0,0.53){};\n\\coordinate(SM1)at($(EK.south west)+(0.15,0.5)$);\n\\coordinate(SM2)at($(EK.south east)+(-1.1,0.5)$);\n\\coordinate(OK1)at($(EK.220)+(0,0.7)$);\n\\coordinate(OK2)at($(EK.240)+(0,0.7)$);\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};\n\\draw[line width=1.4pt](SM1)to [bend right=45](SM2);\n%%\n\\coordinate(4BL)at($(EK.south west)+(0.95,0.3)$);\n    \\def\\n{5}          % broj boksova\n    \\def\\w{0.12}        % box width (mm)\n    \\def\\h{0.5}       % Box height (mm)\n    \\def\\gap{0.05}      % razmak između boksova (mm)\n    % niz boksova\n    \\foreach \\i in {0,...,4} {\n      \\pgfmathsetmacro{\\x}{\\i*(\\w+\\gap)}\n      % padding (we clip inside the edges)\n      \\begin{scope}\n        \\clip[] ($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h);\n        \\fill[gray!10]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*1);\n        \\fill[fill=\\filllcirclecolor]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*\\Level);\n      \\end{scope}\n      % kontura preko\n      \\draw[line width=0.6pt,draw=black]($(4BL)+(\\x,0)$)  rectangle ++(\\w,\\h);\n    }\n\\else\n\\ifnum\\Smile=1\n\\node[draw=black,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,\nfill=\\filllcolor!10,line width=\\Linewidth,](EK)at(0,0.53){};\n\\coordinate(SM1)at($(EK.south west)+(0.32,0.5)$);\n\\coordinate(SM2)at($(EK.south east)+(-0.32,0.5)$);\n\\coordinate(OK1)at($(EK.250)+(0,0.7)$);\n\\coordinate(OK2)at($(EK.290)+(0,0.7)$);\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};\n\\draw[line width=1.4pt](SM1)to [bend right=45](SM2);\n\\else\n\\node[draw=green,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,draw=black,fill=black](EK)at(0,0.53){};\n\\pic[shift={(0,0)}] at  (EK){skull={scalefac=1.3,picname=1,filllcolor=white, filllcirclecolor=black,Linewidth=0.5pt}};\n\\fi\n\\fi\n%\n\\draw[fill=\\filllcolor!60!black!30,line width=\\Linewidth](-1.00,-0.1)--(1.0,-0.1)--(1.28,-0.6)--(-1.28,-0.6)--cycle;\n\\draw[fill=\\filllcolor!60!black!30,line width=\\Linewidth](1.28,-0.6)--(-1.28,-0.6)arc[start angle=180, end angle=270, radius=4pt]--(1.14,-0.73)\narc[start angle=270, end angle=355, radius=4pt]--cycle;\n\\draw[fill=\\filllcolor!30!black!10,line width=\\Linewidth](-0.95,-0.17)--(0.95,-0.17)--(1.03,-0.34)--(-1.03,-0.34)--cycle;\n\\draw[fill=\\filllcolor!30!black!20,line width=\\Linewidth](-0.16,-0.52)--(0.16,-0.52)--(0.14,-0.42)--(-0.14,-0.42)--cycle;\n\\end{scope}\n    }\n  }\n}\n%usb\n\\tikzset{\npics/usb/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\draw[draw=black,fill=\\filllcolor,line width=\\Linewidth](-0.65,0.94)coordinate(GL\\picname)--(0.65,0.94)coordinate(GD\\picname)--\n(0.65,-1.3)coordinate(DD\\picname)arc[start angle=0, end angle=-90, radius=2mm]\n--(-0.45,-1.5)coordinate(DL\\picname)arc[start angle=-90, end angle=-180, radius=2mm]--cycle;\n\\node[draw=none,fill=\\filllcirclecolor,minimum width=5mm,minimum height=8mm,anchor=south]at($($(DL\\picname)!0.42!(DD\\picname)$)+(0,0.35)$){};\n\\coordinate(G1)at($(GL\\picname)!0.15!(GD\\picname)$);\n\\coordinate(G2)at($(GL\\picname)!0.85!(GD\\picname)$);\n\\draw[draw=black,fill=\\filllcolor!40,line width=\\Linewidth](G1)--++(0,1)coordinate(G11)-|coordinate[pos=0.5](G22)(G2)--cycle;\n\\node[draw=none,fill=black,inner sep=0pt,minimum width=1mm,minimum height=6mm,anchor=west]at($($(G1)!0.5!(G11)$)+(0.2,0)$){};\n\\node[draw=none,fill=black,inner sep=0pt,minimum width=1mm,minimum height=6mm,anchor=east]at($($(G2)!0.5!(G22)$)+(-0.2,0)$){};\n%\\fill[red](G22)circle(2pt);\n\\end{scope}\n    }\n  }\n}\n \\tikzset{/pgf/decoration/.cd,\n    number of sines/.initial=10,\n    angle step/.initial=20,\n}\n\\newdimen\\tmpdimen\n\\pgfdeclaredecoration{complete sines}{initial}\n{\n    \\state{initial}[\n        width=+0pt,\n        next state=move,\n        persistent precomputation={\n            \\pgfmathparse{\\pgfkeysvalueof{/pgf/decoration/angle step}}%\n            \\let\\anglestep=\\pgfmathresult%\n            \\let\\currentangle=\\pgfmathresult%\n            \\pgfmathsetlengthmacro{\\pointsperanglestep}%\n                {(\\pgfdecoratedremainingdistance/\\pgfkeysvalueof{/pgf/decoration/number of sines})/360*\\anglestep}%\n        }] {}\n    \\state{move}[width=+\\pointsperanglestep, next state=draw]{\n        \\pgfpathmoveto{\\pgfpointorigin}\n    }\n    \\state{draw}[width=+\\pointsperanglestep, switch if less than=1.25*\\pointsperanglestep to final, % <- bit of a hack\n        persistent postcomputation={\n        \\pgfmathparse{mod(\\currentangle+\\anglestep, 360)}%\n        \\let\\currentangle=\\pgfmathresult%\n    }]{%\n        \\pgfmathsin{+\\currentangle}%\n        \\tmpdimen=\\pgfdecorationsegmentamplitude%\n        \\tmpdimen=\\pgfmathresult\\tmpdimen%\n        \\divide\\tmpdimen by2\\relax%\n        \\pgfpathlineto{\\pgfqpoint{0pt}{\\tmpdimen}}%\n    }\n    \\state{final}{\n        \\ifdim\\pgfdecoratedremainingdistance>0pt\\relax\n            \\pgfpathlineto{\\pgfpointdecoratedpathlast}\n        \\fi\n   }\n}\n%testing_medal\n\\tikzset{\npics/testing/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=TESTING1,shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\newcommand{\\tikzxmark}{%\n\\tikz[scale=0.18] {\n    \\draw[line width=0.7,line cap=round,GreenLine] (0,0) to [bend left=6] (1,1);\n    \\draw[line width=0.7,line cap=round,GreenLine] (0.2,0.95) to [bend right=3] (0.8,0.05);\n}}\n\\newcommand{\\tikzxcheck}{%\n\\tikz[scale=0.16] {\n\\draw[line width=0.7,line cap=round,GreenLine] (0.5,0.75)--(0.85,-0.1) to [bend left=16] (1.5,1.55);\n}}\n \\node[draw, minimum width  =15mm, minimum height = 20mm, inner sep = 0pt,\n        rounded corners,draw = \\drawcolor, fill=\\filllcolor!10, line width=\\Linewidth](COM){};\n\\node[draw=GreenLine,inner sep=4pt,fill=white](CB1) at ($(COM.north west)!0.25!(COM.south west)+(0.3,0)$){};\n\\node[draw=GreenLine,inner sep=4pt,fill=white](CB2) at ($(COM.north west)!0.5!(COM.south west)+(0.3,0)$){};\n\\node[draw=GreenLine,inner sep=4pt,fill=white](CB3) at ($(COM.north west)!0.75!(COM.south west)+(0.3,0)$){};\n\\node[xshift=0pt]at(CB1){\\tikzxcheck};\n\\node[xshift=0pt]at(CB2){\\tikzxmark};\n\\node[xshift=0pt]at(CB3){\\tikzxmark};\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,0.05)$)--++(0:0.8);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,-0.12)$)--++(0:0.7);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,0.05)$)--++(0:0.8);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,-0.12)$)--++(0:0.6);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,0.05)$)--++(0:0.8);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,-0.12)$)--++(0:0.6);\n\\end{scope}\n\\begin{scope}[shift={($(0,0)+(0.4,-0.50)$)},scale=0.5\\scalefac,every node/.append style={transform shape}]\n\\draw[draw=none,fill=\\filllcolor!60](-0.48,-0.10)--(-0.68,-0.68)--(-0.92,-1.38)--\n(-0.53,-1.28)--(-0.29,-1.61)--(-0.09,-0.93)--(0.15,-0.1)--cycle;\n\\draw[draw=none,fill=\\filllcolor!60](-0.266,-0.10)--(-0.02,-0.93)--(0.18,-1.61)--\n(0.45,-1.34)--(0.85,-1.48)--(0.61,-0.68)--(0.44,-0.1)--cycle;\n \\draw[draw=none,postaction={very thick, line join=round, draw=white,fill=\\filllcolor,\n        decorate,decoration={complete sines, number of sines=9, amplitude=\\scalefac*2pt}}] (0,0) circle [radius=0.9];\n\\node[draw=none,fill=white,circle,minimum size=11mm,line width=1pt](CM-\\picname) {};\n%\n\\end{scope}\n    }\n  }\n}\n%PLC\n\\tikzset{\npics/plc/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\node[draw=\\filllcolor,fill=\\filllcolor!30,minimum width=21mm,minimum height=24mm](R\\picname){};\n\\coordinate(P)at($(R\\picname.north west)!0.1!(R\\picname.south east)$);\n% box dimensions\n  \\def\\boxsize{1.3mm}\n  \\def\\xstep{1.8mm}  % gap between columns\n  \\def\\ystep{1.9mm}  % razmak između redova\n% Lista obojenih ćelija – bez duplih zagrada, plus čuvar-zarezi\n\\def\\coloredcells{0/0,1/1,2/1,0/2,1/3,0/4,2/4,1/5,1/6,2/6,0/7}\n\n\\foreach \\i in {0,1,2} {\n  \\foreach \\j in {0,1,2,3,4,5,6,7} {\n    \\edef\\cellid{\\i/\\j}\n    \\def\\fillcolor{cyan!10}\n    % Unutrašnja petlja pravi grupu; zato koristimo \\global\n    \\foreach \\c in \\coloredcells {%\n      \\ifx\\cellid\\c\n        \\global\\def\\fillcolor{green!60}%\n      \\fi\n    }\n    \\node[draw=black, fill=\\fillcolor, minimum size=\\boxsize, inner sep=0pt](MB\\i\\j)\n      at ($(P) + (\\i*\\xstep, -\\j*\\ystep)$) {};\n  }\n}\n\\coordinate(1BL)at($(MB07.south west)+(0,-1mm)$);\n\\node[draw=black,fill=\\filllcolor!10,anchor=north west,inner sep=0pt,\nminimum width=5mm,minimum height=1.5mm](BX1)at(1BL){};\n\\coordinate(2BL)at($(BX1.south west)+(0,-0.8mm)$);\n\\node[draw=black,fill=\\filllcolor!50!black!20,anchor=north west,inner sep=0pt,\nminimum width=5mm,minimum height=3.0mm](BX2)at(2BL){};\n\\coordinate(3BL)at($(BX2.south east)+(1mm,0)$);\n\\node[draw=black,fill=\\filllcolor!70!black!30,anchor=south west,inner sep=0pt,\nminimum width=12mm,minimum height=3.0mm](BX3)at(3BL){};\n\\path[red](BX3.north west)|-coordinate[pos=0.5](1E)(MB27.south east);\n%display\n\\ifnum\\Smile=1\n\\node[draw=black,fill=\\filllcolor!10,anchor=south west,inner sep=0pt,\nminimum width=12mm,minimum height=8.0mm](EK)at(1E){};\n\\coordinate(SM1)at($(EK.south west)+(0.32,0.35)$);\n\\coordinate(SM2)at($(EK.south east)+(-0.32,0.35)$);\n\\coordinate(OK1)at($(EK.250)+(0,0.55)$);\n\\coordinate(OK2)at($(EK.290)+(0,0.55)$);\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};\n\\draw[line width=1.0pt](SM1)to [bend right=25](SM2);\n\\else\n\\node[draw=black,fill=black,anchor=south west,inner sep=0pt,\nminimum width=12mm,minimum height=8.0mm](EK)at(1E){};\n%skull\n\\pic[shift={(0,0)}] at  (EK){skull={scalefac=1,picname=1,filllcolor=white, filllcirclecolor=black,Linewidth=0.5pt}};\n\\fi\n\\draw[fill=\\filllcolor!40!black!30](-0.2,-0.67)--(0.8,-0.67)--(0.72,-0.54)--(-0.14,-0.54)--cycle;\n\\coordinate(4BL)at($(EK.north west)+(0,1mm)$);\n % geometry\n    \\def\\n{5}          % broj boksova\n    \\def\\w{0.2}        % širina boksa (mm)\n    \\def\\h{0.5}       % visina boksa (mm)\n    \\def\\gap{0.05}      % razmak između boksova (mm)\n    % niz boksova\n    \\foreach \\i in {0,...,4} {\n      \\pgfmathsetmacro{\\x}{\\i*(\\w+\\gap)}\n      % popuna (klipujemo unutar ivica)\n      \\begin{scope}\n        \\clip[] ($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h);\n        \\fill[gray!10]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*1);\n        \\fill[fill=\\filllcirclecolor]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*\\Level);\n      \\end{scope}\n      % contour over\n      \\draw[line width=0.6pt,draw=black]($(4BL)+(\\x,0)$)  rectangle ++(\\w,\\h);\n    }\n\\end{scope}\n    }\n  }\n}\n%copier\n\\tikzset{\npics/copier/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\draw[fill=\\filllcolor!60!black,line width=\\Linewidth,draw=black](0.1,1.15)--++(150:0.85)arc[start angle=90, end angle=200,radius=1.5pt]--++(330:0.82)--cycle;\n\\draw[fill=\\filllcolor!30,line width=\\Linewidth,draw=black](-0.73,0.75)--(0.69,0.75)--(0.69,0.41)--(-0.73,0.41)--cycle;\n\\draw[fill=\\filllcolor!60,line width=\\Linewidth,draw=black](-0.80,1.05)--(-0.02,1.05)--(0.1,1.15)--(0.745,1.15)--(0.745,0.75)--(-0.80,0.75)--cycle;\n\\draw[draw=none,fill=\\filllcirclecolor](0.12,1.08)--(0.49,1.08)coordinate(DISNE)--(0.49,0.83)coordinate(DISSE)--(0.12,0.83)--cycle;\n\\node[draw=none,fill=\\filllcirclecolor,circle,inner sep=0pt,minimum size=1mm,below right=0.5pt and 2pt of DISNE]{};\n\\node[draw=none,fill=\\filllcirclecolor,circle,inner sep=0pt,minimum size=1mm,above right=0.5pt and 2pt of DISSE]{};\n%\n\\draw[fill=\\filllcolor!60!black,line width=\\Linewidth,draw=black](0.745,0.-0.07)--(0.87,0)arc[start angle=130, end angle=30,radius=2.5pt]--(1.35,0.16)\narc[start angle=120, end angle=-10,radius=1.25pt]--(0.745,0.-0.22)--cycle;\n%\n\\draw[fill=\\filllcolor!30,line width=\\Linewidth,draw=black](-0.8,0.41)--(0.745,0.41)--(0.745,-1.18)--(-0.8,-1.18)--cycle;\n%\n\\draw[line width=2*\\Linewidth](-0.72,0.0)--coordinate[pos=0.5](SR1)(0.665,0);\n\\node[draw=black,fill=\\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR1){};\n\\draw[line width=2*\\Linewidth](-0.72,-0.4)--coordinate[pos=0.5](SR2)(0.665,-0.4);\n\\node[draw=black,fill=\\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR2){};\n\\draw[line width=2*\\Linewidth](-0.72,-0.8)--coordinate[pos=0.5](SR3)(0.665,-0.8);\n\\node[draw=black,fill=\\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR3){};\n%\n\\end{scope}\n    }\n  }\n}\n%vijak\n\\tikzset{\npics/sraf/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n%fire\n\\ifnum\\Fire=1\n\\fill[fill=red!70,thick](0,1.6)--(0.4,0.6)--(1.4,2)--(2.2,0.8)--(3.1,1.8)--(3,0.5)--(3.7,0.6)--(3.0,-1.2)--\n(3.4,-1.3)--(2.0,-2.86)--(-1.8,-2.86)--(-3.3,-1.36)--(-2.8,-1.36)--(-3.2,-0.1)--(-2.6,-0.3)--(-2.9,2.5)--\n(-2.3,1.4)--(-1.4,2.9)--(-0.6,0.9)--cycle;\n\\fi\n\\foreach \\x in {-2,-0.65,0.72,2.1}{\n\\node[draw=\\drawcolor,fill=\\filllcolor!80,line width=1.5*\\Linewidth,inner sep=0pt,outer sep=0pt,\nminimum width=6mm,minimum height=50mm](SRA1)at(\\x,0){};\n\\node[draw=\\drawcolor,fill=\\filllcolor!50,line width=1.5*\\Linewidth,,trapezium,inner ysep=2pt,anchor=north,outer sep=0pt,inner xsep=3pt,\nminimum width=8mm,minimum height=4mm](TR1)at(SRA1.south){};\n\\begin{scope}\n\\clip(SRA1.south west)rectangle (SRA1.north east);\n\\foreach \\i [evaluate=\\i as \\y using {\\i+0.03}]in {0,0.07,0.14,...,0.99}{\n\\draw[black,thick]($(SRA1.south west)!\\i!(SRA1.north west)$)--($(SRA1.south east)!\\y!(SRA1.north east)$);\n\\draw[draw=\\drawcolor,line width=1.5*\\Linewidth](SRA1.south west)rectangle (SRA1.north east);\n}\n\\end{scope}\n}\n\\end{scope}\n    }\n  }\n}\n\\pgfkeys{\n  /channel/.cd,\n   Dual/.store in=\\Dual,\n   Depth/.store in=\\Depth,\n  Height/.store in=\\Height,\n  Width/.store in=\\Width,\n  Fire/.store in=\\Fire,\n  Smile/.store in=\\Smile,\n  Level/.store in=\\Level,\n  filllcirclecolor/.store in=\\filllcirclecolor,\n  filllcolor/.store in=\\filllcolor,\n  drawcolor/.store in=\\drawcolor,\n  drawcircle/.store in=\\drawcircle,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  filllcolor=BrownLine,\n  filllcirclecolor=cyan!40,\n  drawcolor=black,\n  drawcircle=violet,\n  scalefac=1,\n  Dual=1,\n  Fire=1,\n  Smile=1,\n  Level=0.52,\n  Linewidth=0.5pt,\n  Depth=1.3,\n  Height=0.8,\n  Width=1.1,\n  picname=C\n}\n%\n\\begin{scope}[local bounding box=LAPTOP1,shift={($(0,0)+(0,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.5,picname=1,drawcolor=GreenD,Dual=0,Smile=0,\nfilllcolor=GreenD!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=USB1,shift={($(LAPTOP1)+(3.7,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)},rotate=320] at  (0,0){usb={scalefac=0.6,picname=1,drawcolor=orange,filllcirclecolor=white,filllcolor=violet!50!black!50!,Linewidth=1.0pt,}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=TESTING1,shift={($(USB1)+(3.1,0.45)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,-0.5)}] at  (0,0){testing={scalefac=1,picname=1,drawcolor=OrangeLine,filllcolor=OrangeLine, Linewidth=1.0pt}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=LAPTOP2,shift={($(TESTING1)+(3.5,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=2,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=red!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n\\tikzset{%\n    LineZ/.style={-*,green!50!black,line width=1pt}\n}\n\\begin{scope}[local bounding box=LAPTOP3,shift={($(LAPTOP2)+(6.1,-0.2)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=3,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=yellow!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n\\draw[LineZ](EKV3.north)--++(90:1.2)node[above,black]{\\huge ?};\n\\draw[LineZ] ($(EKV3.north)+(0,0.2)$)--++(40:1);\n\\draw[LineZ] ($(EKV3.north)+(0,0.2)$)--++(140:1);\n\\draw[LineZ](EKV3.west)--++(180:1.2)node[left,black]{\\huge ?};\n\\draw[LineZ] ($(EKV3.west)+(-0.2,0)$)--++(140:1);\n\\draw[LineZ] ($(EKV3.west)+(-0.2,0)$)--++(220:1);\n\\draw[LineZ](EKV3.east)--++(0:1.2)node[right,black]{\\huge ?};\n\\draw[LineZ] ($(EKV3.east)+(0.2,0)$)--++(40:1);\n\\draw[LineZ] ($(EKV3.east)+(0.2,0)$)--++(320:1);\n%\n\\begin{scope}[local bounding box=LAPTOP4,shift={($(LAPTOP2)+(12.5,-0.2)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=4,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=cyan!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n%\n\\draw[LineZ](EKV4.east)--node[above,black]{\\huge !}\nnode[below=3pt,black,fill=magenta!20,circle,inner sep=1pt](CIRC1){\\large 1}++(0:1.2);\n\\end{scope}\n%\n\\begin{scope}[local bounding box=PLC1,shift={($(LAPTOP4)+(3.5,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=1,drawcolor=OrangeLine,filllcirclecolor=cyan, Smile=1,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};\n\\end{scope}\n%\n\\coordinate(SR1)at($(LAPTOP1.east)!0.45!(USB1.west)$);\n\\node[Larrow]at(SR1){};\n\\coordinate(SR2)at($(USB1.east)!0.4!(TESTING1.west)$);\n\\node[Larrow]at(SR2){};\n\\coordinate(SR3)at($(TESTING1.east)!0.5!(LAPTOP2.west)$);\n\\node[Larrow]at(SR3){};\n%text below first row\n\\node[draw=none,fit=(LAPTOP1)(LAPTOP2)](BB1){};\n\\node[TxtL,below=6pt of BB1.south west,text width=110mm,anchor=north west](GT1){\\textcolor{red}{\\textbf{1. Infection}}\\\\[0.35ex]\nStuxnet enters a system via a USB stick and proceeds\nto infect all machines running Microsoft Windows. By brandishing a digital certificate that seems to\nshow that it comes from a reliable company, the worm is able to evade automated-detection systems.};\n%\n\\path[red](BB1.south east)-|coordinate[pos=0.5](T2)(EKV3.south);\n\\node[TxtL,below=6pt of T2.south,text width=80mm,xshift=-12mm](GT2){\\textcolor{red}{\\textbf{2. Search}}\\\\[0.35ex]\nStuxnet then checks whether a given machine is part of\nthe targeted industrial control system made by Siemens. Such systems are deployed in Iran to run high-speed centrifuges that\nhelp to enrich nuclear fuel.};\n\\path[red](BB1.south east)-|coordinate[pos=0.5](T3)(CIRC1);\n\\node[TxtL,below=6pt of T3.south,text width=67mm](GT3){\\textcolor{red}{\\textbf{3. Update}}\\\\[0.35ex] If the system isn’t a target, Stuxnet does nothing;\nif it is, the worm attempts to access the Internet and download a more recent version of itself.};\n%\n\\node[draw=none,fit=(BB1)(GT2)(GT3)(PLC1)](BOX1){};\n\\draw[BrownLine,line width=2pt]([yshift=-2mm]BOX1.south west)coordinate(LE)\n--([yshift=-2mm]BOX1.south east)coordinate(DE);\n%%%%%%%%%%%%%%%%%%%\n%Row below\n%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[local bounding box=LAPTOP5,shift={($(LAPTOP1)+(-0.6,-7.5)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=5,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=green!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=COPIER1,shift={($(LAPTOP5)+(2.9,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){copier={scalefac=0.8,picname=1,drawcolor=BlueD,\nfilllcolor=BlueD!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80!}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=PLC2,shift={($(COPIER1)+(3.3,-0.1)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=2,drawcolor=OrangeLine,filllcirclecolor=cyan, Smile=0,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};\n\\end{scope}\n%%%%%%%%%%%%%%%\n\\begin{scope}[local bounding box=LAPTOP6,shift={($(LAPTOP1)+(9.6,-7.5)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=6,drawcolor=red,Dual=1,Smile=1,\nfilllcolor=brown!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n\\draw[LineZ](EKV6.15)--++(0:0.7);\n\\draw[LineZ,red](EKV6.345)--++(0:0.7);\n%\n\\begin{scope}[local bounding box=PLC3,shift={($(LAPTOP6)+(3.2,0.1)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=3,drawcolor=OrangeLine,filllcirclecolor=red, Smile=0,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};\n\\end{scope}\n\\draw[LineZ,red](R3.355)--++(0:0.9);\n\\draw[LineZ,-](R3.20)--++(0:0.35)--++(0,-0.5)\nnode[rectangle, fill=white,draw=green!50!black,minimum size=width=2mm,minimum height=4mm] {};\n%\n\\begin{scope}[local bounding box=SRAF1,shift={($(PLC3)+(3.25,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){sraf={scalefac=0.4,picname=1,drawcolor=BrownLine,filllcolor=BrownLine!50!,Linewidth=0.5pt,Fire=0}};\n\\end{scope}\n%%%%%%%%%%%%%%%\n\\begin{scope}[local bounding box=LAPTOP7,shift={($(LAPTOP6)+(10.3,-0.2)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=7,drawcolor=red,Dual=1,Smile=1,\nfilllcolor=red,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=PLC4,shift={($(LAPTOP7)+(3.5,0.1)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=4,drawcolor=OrangeLine,filllcirclecolor=red, Smile=0,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=1}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=SRAF2,shift={($(PLC4)+(3.6,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){sraf={scalefac=0.4,picname=1,drawcolor=BrownLine,filllcolor=BrownLine!50!,Linewidth=0.5pt,Fire=1}};\n\\end{scope}\n%arrows\n\\coordinate(2SR1)at($(LAPTOP5.east)!0.35!(COPIER1.west)$);\n\\node[Larrow]at(2SR1){};\n\\coordinate(2SR2)at($(COPIER1.350)!0.45!(PLC2.west)$);\n\\node[Larrow]at(2SR2){};\n\\coordinate(2SR3)at($(EKV7.25)!0.55!(PLC4.west)$);\n\\node[Larrow]at(2SR3){};\n\\coordinate(2SR4)at($(EKV7.330)!0.55!(PLC4.west)$);\n\\node[Larrow,red,rotate=180]at(2SR4){};\n\\coordinate(2SR5)at($(R4.25)!0.55!(SRAF2.west)$);\n\\node[Larrow]at(2SR5){};\n\\coordinate(2SR6)at($(R4.335)!0.55!(SRAF2.west)$);\n\\node[Larrow,rotate=180]at(2SR6){};\n%text\n\\node[draw=none,fit=(LAPTOP5)(PLC2)](2BB1){};\n\\node[TxtL,below=6pt of 2BB1.south west,text width=81mm,anchor=north west](DT1){\\textcolor{red}{\\textbf{4. Compromise}}\\\\[0.35ex] The worm then compromises\nthe target system’s logic controllers, exploiting “zero day” vulnerabilities—software weaknesses that haven’t\nbeen identified by security experts.};\n%\n\\path[red](2BB1.south east)-|coordinate[pos=0.5](2T2)(PLC3.south);\n\\node[TxtL,below=6pt of 2T2.south](DT2){\\textcolor{red}{\\textbf{5. Control}}\\\\[0.35ex] In the beginning,\nStuxnet spies on the operations of the targeted system. Then it uses the information it has gathered\nto take control of the centrifuges, making them spin themselves to failure.};\n%\n\\path[red](2BB1.south east)-|coordinate[pos=0.5](2T3)(PLC4.south);\n\\node[TxtL,below=6pt of 2T3.south,text width=80mm](DT3){\\textcolor{red}{\\textbf{6. Deceive and destroy}}\\\\[0.35ex] Meanwhile,\nit provides false feedback to outside controllers, ensuring that they won’t know what’s going wrong\nuntil it’s too late to do anything about it.};\n%\n\\path[red](LE)--++(0,-6.7)coordinate(LE2)-|coordinate(DE2)(DE);\n\\pgfdeclarehorizontalshading{mygradient}{100bp}{\n  color(0bp)=(green);\n  color(50bp)=(red)\n}\n\\shade[shading=mygradient] (LE2) rectangle ($(DE2)+(0,-5mm)$);\n\\path[red](LE)--++(0,9.5)coordinate(GLE2)-|coordinate(GDE2)(DE);\n\\fill[BrownLine!40] (GLE2) rectangle ($(GDE2)+(0,5mm)$);\n\\node[fill=white]at($([yshift=2.5mm]GLE2)!0.5!([yshift=2.5mm]GDE2)$){\\large \\bfseries HOW \\textcolor{red}{STUXNET} WORKED};\n\\draw[LineA,*-*,text=black,line width=1pt,shorten <=5pt,shorten >=5pt](EKV1.north)--++(0,1.85)-|\nnode[below=5pt,pos=0.1]{Update from source}\nnode[left=5pt,pos=0.85,black,fill=magenta!20,circle,inner sep=1pt]{2}(EKV4.north);\n\\end{tikzpicture}\n```\n**Stuxnet**: Targets PLCs by exploiting Windows and Siemens software vulnerabilities, demonstrating supply chain compromise that enabled digital malware to cause physical infrastructure damage. Modern ML systems face analogous risks through compromised training data, backdoored dependencies, and tampered model weights. @fig-stuxnet\n:::\n\n### Insufficient Isolation: Jeep Cherokee Hack {#sec-security-privacy-insufficient-isolation-jeep-cherokee-hack-6a7c}\n\nThe 2015 Jeep Cherokee hack demonstrated how connectivity in everyday products creates new vulnerabilities. Security researchers publicly demonstrated a remote cyberattack on a Jeep Cherokee that exposed important vulnerabilities in automotive system design [@miller2015remote; @miller2019lessons]. Conducted as a controlled experiment, the researchers exploited a vulnerability in the vehicle's Uconnect entertainment system, which was connected to the internet via a cellular network. By gaining remote access to this system, they sent commands that affected the vehicle's engine, transmission, and braking systems without physical access to the car.\n\nThis demonstration served as a wake-up call for the automotive industry, highlighting the risks posed by the growing connectivity of modern vehicles. Traditionally isolated automotive control systems, such as those managing steering and braking, were shown to be vulnerable when exposed through externally accessible software interfaces. The ability to remotely manipulate safety-critical functions raised serious concerns about passenger safety, regulatory oversight, and industry best practices.\n\n{{< margin-video \"https://www.youtube.com/watch?v=MK0SrxBC1xs&ab_channel=WIRED\" \"Jeep Cherokee Hack\" \"WIRED\" >}}\n\nThe incident also led to a recall of over 1.4 million vehicles to patch the vulnerability[^fn-automotive-recalls], highlighting the need for manufacturers to prioritize cybersecurity in their designs. The National Highway Traffic Safety Administration (NHTSA)[^fn-nhtsa] issued guidelines for automakers to improve vehicle cybersecurity, including recommendations for secure software development practices and incident response protocols.\n\n[^fn-automotive-recalls]: **Automotive Cybersecurity Recalls**: The Jeep Cherokee hack triggered the first-ever automotive cybersecurity recall in 2015. Since then, cybersecurity recalls have affected over 15 million vehicles globally, costing manufacturers an estimated $2.4 billion in remediation efforts and spurring new regulations.\n\n[^fn-nhtsa]: **NHTSA Cybersecurity Guidance**: NHTSA, established in 1970, issued its first cybersecurity guidance in 2016 following the Jeep hack. The agency now mandates that connected vehicles include cybersecurity by design, affecting 99% of new vehicles sold in the US that contain 100+ onboard computers.\n\nThe Jeep Cherokee hack offers critical lessons for ML system security. Connected ML systems require strict isolation between external interfaces and safety-critical components, as this incident dramatically illustrated. The architectural flaw (allowing external interfaces to reach safety-critical functions) directly threatens modern ML deployments where inference APIs often connect to physical actuators or critical systems.\n\nModern ML attack vectors exploit these same isolation failures across multiple domains: (1) Autonomous vehicles where compromised infotainment system ML APIs (voice recognition, navigation) gain access to perception models controlling steering and braking; (2) Smart home systems where exploited voice assistant wake-word detection models provide backdoor access to security systems, door locks, and cameras; (3) Industrial IoT where compromised edge ML inference endpoints (predictive maintenance, anomaly detection) manipulate actuator control logic in manufacturing systems; (4) Medical devices where attacked diagnostic ML models influence treatment recommendations and drug delivery systems.\n\nConsider a concrete attack scenario: a smart home voice assistant processes user commands through cloud-based NLP models. An attacker exploits a vulnerability in the voice processing API to inject malicious commands that bypass authentication. Through insufficient network segmentation, the compromised voice system gains access to the home security ML model responsible for facial recognition door unlocking, allowing unauthorized physical access.\n\nEffective defense requires comprehensive isolation architecture: (1) network segmentation to isolate ML inference networks from actuator control networks using firewalls and VPNs; (2) API authentication requiring cryptographic authentication for all ML API calls with rate limiting and anomaly detection; (3) privilege separation to run inference models in sandboxed environments with minimal system permissions; (4) fail-safe defaults that design actuator control logic to revert to safe states (locked doors, stopped motors) when ML systems detect anomalies or lose connectivity; (5) monitoring that implements real-time logging and alerting for suspicious ML API usage patterns.\n\n### Weaponized Endpoints: Mirai Botnet {#sec-security-privacy-weaponized-endpoints-mirai-botnet-931c}\n\nWhile the Jeep Cherokee hack demonstrated targeted exploitation of connected systems, the Mirai botnet revealed how poor security practices could be weaponized at massive scale. In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS)[^fn-ddos-attacks] attacks in internet history [@antonakakis2017understanding]. The botnet infected thousands of networked devices, including digital cameras, DVRs, and other consumer electronics. These devices, often deployed with factory-default usernames and passwords, were easily compromised by the Mirai malware and enlisted into a large-scale attack network.\n\n[^fn-mirai-scale]: **Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generating peak attacks of 1.2 Tbps (1,200 Gbps) against OVH hosting provider, making it one of the first terabit-scale DDoS attacks. The attack revealed that IoT devices with default credentials (admin/admin, root/12345) could be weaponized at unprecedented scale.\n\n[^fn-ddos-attacks]: **DDoS Attacks**: Distributed Denial-of-Service (DDoS) attacks overwhelm targets with traffic from multiple sources, first demonstrated in 1999. Modern DDoS attacks can exceed 3.47 Tbps (terabits per second), enough to take down entire internet infrastructures and costing businesses $2.3 million per incident on average.\n\nThe Mirai botnet was used to overwhelm major internet infrastructure providers, disrupting access to popular online services across the United States and beyond. The scale of the attack demonstrated how vulnerable consumer and industrial devices can become a platform for widespread disruption when security is not prioritized in their design and deployment.\n\n{{< margin-video \"https://www.youtube.com/watch?v=1pywzRTJDaY\" \"Mirai Botnet\" \"Vice News\" >}}\n\nThe Mirai botnet's lessons apply directly to modern ML deployments. Edge-deployed ML devices with weak authentication become weaponized attack infrastructure at unprecedented scale, precisely as the Mirai botnet demonstrated with traditional IoT devices. Modern ML edge devices (smart cameras running object detection, voice assistants performing wake-word detection, autonomous drones with navigation models, industrial IoT sensors with anomaly detection algorithms) face identical vulnerability patterns but with amplified consequences due to their AI capabilities and access to sensitive data.\n\nThe attack escalation with ML devices differs significantly from traditional IoT compromises. Unlike simple IoT devices that provided only computing power for DDoS attacks, compromised ML devices offer sophisticated capabilities: (1) Data exfiltration where smart cameras leak facial recognition databases, voice assistants extract conversation transcripts, and health monitors steal biometric data; (2) Model weaponization where hijacked autonomous drones coordinate swarm attacks and compromised traffic cameras misreport vehicle counts to manipulate traffic systems; (3) AI-powered reconnaissance where compromised edge ML devices use their trained models to identify high-value targets (facial recognition for VIP identification, voice analysis for emotion detection) and coordinate sophisticated multi-stage attacks.\n\nConsider a concrete attack scenario where attackers compromise 50,000 smart security cameras with default passwords, each running ML object detection models. Rather than traditional DDoS attacks, they use the compromised cameras to: (1) extract facial recognition databases from residential and commercial buildings; (2) coordinate physical surveillance of targeted individuals using distributed camera networks; (3) inject false object detection alerts to trigger emergency responses and create chaos; (4) use the cameras' computing power to train adversarial examples against other security systems.\n\nComprehensive defense against such weaponization requires zero-trust edge security: (1) Secure manufacturing that eliminates default credentials, implements hardware security modules (HSMs) for device-unique keys, and enables secure boot with cryptographic verification; (2) Encrypted communications that mandate TLS 1.3+ for all ML API communications with certificate pinning and mutual authentication; (3) Behavioral monitoring that deploys anomaly detection systems to identify unusual inference patterns, unexpected network traffic, and suspicious computational loads; (4) Automated response that implements kill switches to disable compromised devices remotely and quarantine them from networks; (5) Update security that enforces cryptographically signed firmware updates with automatic security patching and version rollback capabilities.\n\n## Systematic Threat Analysis and Risk Assessment {#sec-security-privacy-systematic-threat-analysis-risk-assessment-3ef1}\n\nThe historical incidents demonstrate how fundamental security failures manifest across different computing paradigms. Supply chain vulnerabilities enable persistent compromise, insufficient isolation allows privilege escalation, and weaponized endpoints create attack infrastructure at scale. These patterns directly apply to machine learning deployments: compromised training pipelines and model repositories inherit supply chain risks, external interfaces to safety-critical ML components require strict isolation, and compromised ML edge devices can exfiltrate inference data or participate in coordinated attacks.\n\nThese historical incidents reveal universal security patterns that translate directly to ML system vulnerabilities. Supply chain compromise, as demonstrated by Stuxnet, manifests in ML through training data poisoning and backdoored model repositories. Insufficient isolation, exemplified by the Jeep Cherokee hack, appears in ML API access to safety-critical systems and compromised inference endpoints. Weaponized endpoints, illustrated by the Mirai botnet, emerge through hijacked ML edge devices capable of coordinated AI-powered attacks.\n\nThe key insight is that traditional cybersecurity patterns amplify in ML systems because models learn from data and make autonomous decisions. While Stuxnet required sophisticated malware to manipulate industrial controllers, ML systems can be compromised through data poisoning that appears statistically normal but embeds hidden behaviors. This characteristic makes ML systems both more vulnerable to subtle attacks and more dangerous when compromised, as they can make decisions affecting physical systems autonomously. Understanding these historical patterns helps recognize how familiar attack vectors manifest in ML contexts, while the unique properties of learning systems (statistical learning, decision autonomy, and data dependency) create new attack surfaces requiring specialized defenses.\n\nMachine learning systems introduce attack vectors that extend beyond traditional computing vulnerabilities. The data-driven nature of learning creates new opportunities for adversaries: training data can be manipulated to embed backdoors, input perturbations can exploit learned decision boundaries, and systematic API queries can extract proprietary model knowledge. These ML-specific threats require specialized defenses that account for the statistical and probabilistic foundations of learning systems, complementing traditional infrastructure hardening.\n\n### Threat Prioritization Framework {#sec-security-privacy-threat-prioritization-framework-f2d5}\n\nWith the wide range of potential threats facing ML systems, practitioners need a framework to prioritize their defensive efforts effectively. Not all threats are equally likely or impactful, and security resources are always constrained. A simple prioritization matrix based on likelihood and impact helps focus attention where it matters most.\n\nConsider these threat priority categories:\n\n- **High Likelihood / High Impact**: Data poisoning in federated learning systems where training data comes from untrusted sources. These attacks are relatively easy to execute but can severely compromise model behavior.\n\n- **High Likelihood / Medium Impact**: Model extraction attacks against public APIs. These are common and technically simple but may only affect competitive advantage rather than safety or privacy.\n\n- **Low Likelihood / High Impact**: Hardware side-channel attacks on cloud-deployed models. These require sophisticated adversaries and physical access but could expose all model parameters and user data.\n\n- **Medium Likelihood / Medium Impact**: Membership inference attacks against models trained on sensitive data. These require some technical skill but mainly threaten individual privacy rather than system integrity.\n\nThis framework guides resource allocation throughout this chapter. We begin with the most common and accessible threats (model theft, data poisoning, and adversarial attacks) before examining more specialized hardware and infrastructure vulnerabilities. Understanding these priority levels helps practitioners implement defenses in a logical sequence that maximizes security benefit per invested effort.\n\n## Model-Specific Attack Vectors {#sec-security-privacy-modelspecific-attack-vectors-0575}\n\nMachine learning systems face threats spanning the entire ML lifecycle, from training-time manipulations to inference-time evasion. These threats fall into three broad categories: threats to model confidentiality (model theft), threats to training integrity (data poisoning[^fn-data-poisoning]), and threats to inference robustness (adversarial examples[^fn-adversarial-examples]). Each category targets different vulnerabilities and requires distinct defensive strategies.\n\n[^fn-data-poisoning]: **Data Poisoning Attacks**: Data poisoning is an attack technique where adversaries inject malicious data during training, first formalized in 2012 [@biggio2012poisoning]. Studies show that poisoning just 0.1% of training data can reduce model accuracy by 10-50%, making it a highly efficient attack vector against ML systems.\n\n[^fn-adversarial-examples]: **Adversarial Examples**: Adversarial examples are inputs crafted to deceive ML models, discovered by Szegedy et al. [@szegedy2014intriguing]. These attacks can fool state-of-the-art image classifiers with perturbations invisible to humans (changing <0.01% of pixel values), affecting 99%+ of deep learning models.\n\nUnderstanding when and where different attacks occur in the ML lifecycle helps prioritize defenses and understand attacker motivations. @fig-ml-lifecycle-threats maps the primary attack vectors to their target stages in the machine learning pipeline, revealing how adversaries exploit different system vulnerabilities at different times.\n\n- **During Data Collection**: Attackers can inject malicious samples or manipulate labels in training datasets, particularly in federated learning or crowdsourced data scenarios where data sources are less controlled.\n\n- **During Training**: This stage faces backdoor insertion attacks, where adversaries embed hidden behaviors that activate only under specific trigger conditions, and label manipulation attacks that systematically corrupt the learning process.\n\n- **During Deployment**: Model theft attacks target this stage because trained models become accessible through APIs, file downloads, or reverse engineering of mobile applications. This is where intellectual property is most vulnerable.\n\n- **During Inference**: Adversarial attacks occur at runtime, where attackers craft inputs designed to fool deployed models into making incorrect predictions while appearing normal to human observers.\n\nThis lifecycle perspective reveals that different threats require different defensive strategies. Data validation protects the collection phase, secure training environments protect the training phase, access controls and API design protect deployment, and input validation protects inference. By understanding which attacks target which lifecycle stages, security teams can implement appropriate defenses at the right architectural layers.\n\n::: {#fig-ml-lifecycle-threats fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.85}{%\n\\begin{tikzpicture}[scale=0.9, transform shape, line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={line width=0.75pt,black!50},\nBox/.style={inner xsep=2pt,\n    node distance=0.6,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    minimum width=28mm, minimum height=8mm\n  },\nBox2/.style={Box,  node distance=2.3,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2,}\n}\n\\node[Box](B1){Data Collection};\n\\node[Box,below=of B1](B2){Training};\n\\node[Box,below=of B2](B3){Deployment};\n\\node[Box,below=of B3](B4){Inference};\n\\node[Box2,left=2.2 of B2](LB2){Backdoors};\n\\node[Box2,right=2.2 of B2](RB2){Label\\\\ Manipulation};\n\\node[Box2,left=2.2 of B3](LB3){Model Theft};\n\\node[Box2,right=2.2 of B3](RB3){Model Inversion};\n\\node[Box2,left=2.2 of B4](LB4){Adversarial\\\\ Examples};\n\\node[Box2,right=2.2 of B4](RB4){Membership\\\\ Inference};\n\\node[Box3,above left=0.5 and 2 of B1](PL){Privacy Leakage};\n\\node[Box3,above right=0.5 and 2 of B1](DP){Data Poisoning};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=10mm,inner ysep=4mm,\nyshift=2.5mm,fill=BackColor!60,fit=(B1)(B4),line width=0.75pt](BB2){};\n\\node[below=4pt of  BB2.north,inner sep=0pt,\nanchor=north]{Lifecycle};\n%%\n\\draw[Line,-latex](PL)|-(B1);\n\\draw[Line,-latex](DP)|-(B1);\n\\foreach \\i in{1,2,3}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](B\\i)--(B\\newI);\n}\n\n\\foreach \\i in{L,R}{\n\\foreach \\x in{2,3,4}{\n\\draw[Line,-latex](\\i B\\x)--(B\\x);\n  }\n}\n\\end{tikzpicture}}\n```\n**ML Lifecycle Threats**: Model theft, data poisoning, and adversarial attacks target distinct stages of the machine learning lifecycle (from data ingestion to model deployment and inference), creating unique vulnerabilities at each step. Understanding these lifecycle positions clarifies attack surfaces and guides the development of targeted defense strategies for robust AI systems.\n:::\n\nMachine learning models are not solely passive victims of attack; in some cases, they can be employed as components of an attack strategy. Pretrained models, particularly large generative or discriminative networks, may be adapted to automate tasks such as adversarial example generation, phishing content synthesis[^fn-phishing-ai], or protocol subversion. Open-source or publicly accessible models can be fine-tuned for malicious purposes, including impersonation, surveillance, or reverse-engineering of secure systems.\n\n[^fn-phishing-ai]: **AI-Generated Phishing**: Large language models can generate convincing phishing emails with 99%+ grammatical accuracy, compared to 19% for traditional phishing. Security firms report dramatic increases in AI-generated phishing attacks since 2022, with some studies citing 1,265% growth (though methodologies and baselines vary significantly), with some campaigns achieving 30%+ success rates. This dual-use potential necessitates a broader security perspective that considers models not only as assets to defend but also as possible instruments of attack.\n\n### Model Theft {#sec-security-privacy-model-theft-1879}\n\nThe first category of model-specific threats targets confidentiality. Threats to model confidentiality arise when adversaries gain access to a trained model's parameters, architecture, or output behavior. These attacks can undermine the economic value of machine learning systems, allow competitors to replicate proprietary functionality, or expose private information encoded in model weights.\n\nSuch threats arise across a range of deployment settings, including public APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-repositories]. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats[^fn-model-serialization], or insufficient access controls, factors that create opportunities for unauthorized extraction or replication [@ateniese2015hacking].\n\n[^fn-ml-apis]: **Machine Learning APIs**: Machine learning APIs (Application Programming Interfaces) were popularized by Google's Prediction API (2010). Today's ML APIs handle billions of requests daily, with major providers processing billions of tokens monthly, creating vast attack surfaces for model extraction.\n\n[^fn-model-repositories]: **Model Repositories**: Model repositories are centralized platforms for sharing ML models, led by Hugging Face (2016) which hosts 500,000+ models. While democratizing AI access, these repositories have become targets for supply chain attacks, with researchers finding malicious models in 5% of popular repositories [@oliynyk2023know].\n\n[^fn-model-serialization]: **Model Serialization**: Model serialization is the process of converting trained models into portable formats like ONNX (2017), TensorFlow SavedModel (2016), or PyTorch's .pth files. Insecure serialization can expose model weights and enable arbitrary code execution, affecting 80%+ of deployed ML systems [@ateniese2015hacking; @tramer2016stealing].\n\nThe severity of these threats is underscored by high-profile legal cases that have highlighted the strategic and economic value of machine learning models. For example, former Google engineer Anthony Levandowski was accused of [stealing proprietary designs from Waymo](https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html), including critical components of its autonomous vehicle technology, before founding a competing startup. Such cases illustrate the potential for insider threats to bypass technical protections and gain access to sensitive intellectual property.\n\nThe consequences of model theft extend beyond economic loss. Stolen models can be used to extract sensitive information, replicate proprietary algorithms, or enable further attacks. The economic impact can be substantial: research estimates suggest that aspects of large language models can be approximated through systematic API queries at costs orders of magnitude lower than original training, though full model replication remains economically and technically challenging [@tramer2016stealing; @carlini2024stealing]. For instance, a competitor who obtains a stolen recommendation model from an e-commerce platform might gain insights into customer behavior, business analytics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks[^fn-model-inversion-attack], where an attacker attempts to infer private details about the model's training data [@fredrikson2015model].\n\n[^fn-model-inversion-attack]: **Model Inversion Attacks**: Model inversion attacks were first demonstrated in 2015 against face recognition systems, when researchers reconstructed recognizable faces from neural network outputs using only confidence scores. The attack revealed that models trained on 40 individuals could leak identifiable facial features, proving that \"black-box\" API access isn't sufficient privacy protection.\n\nIn a model inversion attack, the adversary queries the model through a legitimate interface, such as a public API, and observes its outputs. By analyzing confidence scores or output probabilities, the attacker can optimize inputs to reconstruct data resembling the model's training set. For example, a facial recognition model used for secure access could be manipulated to reveal statistical properties of the employee photos on which it was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset[^fn-netflix-deanonymization], where researchers inferred individual movie preferences from anonymized data [@narayanan2006break].\n\n[^fn-netflix-deanonymization]: **Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"anonymous\" Prize dataset with public IMDb ratings. Using as few as 8 movie ratings with dates, they identified 99% of users, leading Netflix to cancel a second competition and highlighting the futility of naive anonymization.\n\nModel theft can target two distinct objectives: extracting exact model properties, such as architecture and parameters, or replicating approximate model behavior to produce similar outputs without direct access to internal representations. Understanding neural network architectures helps recognize which architectural patterns are most vulnerable to extraction attacks. The specific architectural vulnerabilities vary by model type, with deeper networks and attention-based architectures presenting different attack surfaces than simpler convolutional or recurrent designs. Both forms of theft undermine the security and value of machine learning systems, as explored in the following subsections.\n\nThese two attack paths are illustrated in @fig-model-theft-types. In exact model theft, the attacker gains access to the model's internal components, including serialized files, weights, and architecture definitions, and reproduces the model directly. In contrast, approximate model theft relies on observing the model's input-output behavior, typically through a public API. By repeatedly querying the model and collecting responses, the attacker trains a surrogate that mimics the original model's functionality. The first approach compromises the model's internal design and training investment, while the second threatens its predictive value and can facilitate further attacks such as adversarial example transfer or model inversion.\n\n::: {#fig-model-theft-types fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={line width=1.0pt,black!50},\nBox/.style={inner xsep=2pt,inner ysep=4pt,\n    node distance=0.4,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=38mm,\n    minimum width=38mm, minimum height=8mm\n  },\nBox2/.style={Box,  node distance=1.9,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2,text width=42mm,}\n}\n\n\\node[Box](B1){Access to public API};\n\\node[Box,below=of B1](B2){Send crafted queries};\n\\node[Box,below=of B2](B3){Record responses};\n\\node[Box,below=of B3](B4){Train surrogate model};\n\\node[Box,below=of B4](B5){Replicate predictions, launch further attacks};\n\\foreach \\i in{1,2,3,4}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](B\\i)--(B\\newI);\n}\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=10mm,minimum height=71.5mm,\nyshift=2.5mm,fill=BackColor!60,fit=(B1)(B5),line width=0.75pt](BB2){};\n\\node[below=6pt of  BB2.north,inner sep=0pt,\nanchor=north]{\\textbf{Approximate Model Theft}};\n%%%\n\\node[Box3,right=5 of B1](RB1){Access to model file or deployment artifact};\n\\node[Box3,right=5 of B5](RB4){Use or resell\\\\ proprietary IP};\n\\node[Box3](RB2)at($(RB1)!0.34!(RB4)$){Extract parameters, architecture, hyperparameters};\n\\node[Box3](RB3)at($(RB1)!0.67!(RB4)$){Reconstruct original\\\\ model};\n\\foreach \\i in{1,2,3}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](RB\\i)--(RB\\newI);\n}\n\\scoped[on background layer]\n\\node[draw=GreenD,inner xsep=10mm,minimum height=71mm,,\nyshift=2.2mm,fill=green!5,fit=(RB1)(RB4),line width=0.75pt](BB2){};\n\\node[below=6pt of  BB2.north,inner sep=0pt,\nanchor=north]{\\textbf{Exact Model Theft}};\n\\end{tikzpicture}\n```\n**Model Theft Strategies**: Attackers can target either a model’s internal parameters or its external behavior to create a stolen copy. Direct theft extracts model weights and architecture, while approximate theft trains a surrogate model by querying the original’s input-output behavior, potentially enabling further attacks despite lacking direct access to internal components.\n:::\n\n#### Exact Model Theft {#sec-security-privacy-exact-model-theft-b738}\n\nExact model property theft refers to attacks aimed at extracting the internal structure and learned parameters of a machine learning model. These attacks often target deployed models that are exposed through APIs, embedded in on-device inference engines, or shared as downloadable model files on collaboration platforms. Exploiting weak access control, insecure model packaging, or unprotected deployment interfaces, attackers can recover proprietary model assets without requiring full control of the underlying infrastructure.\n\nThese attacks typically seek three types of information. The first is the model's learned parameters, such as weights and biases. By extracting these parameters, attackers can replicate the model's functionality without incurring the cost of training. This replication allows them to benefit from the model's performance while bypassing the original development effort.\n\nThe second target is the model's fine-tuned hyperparameters, including training configurations such as learning rate, batch size, and regularization settings. These hyperparameters significantly influence model performance, and stealing them allows attackers to reproduce high-quality results with minimal additional experimentation.\n\nFinally, attackers may seek to reconstruct the model's architecture. This includes the sequence and types of layers, activation functions, and connectivity patterns that define the model's behavior. Architecture theft may be accomplished through side-channel attacks[^fn-ml-side-channel], reverse engineering, or analysis of observable model behavior.\n\n[^fn-ml-side-channel]: **ML Side-Channel Attacks**: Side-channel attacks on ML were first demonstrated against neural networks in 2018, when researchers showed that power consumption patterns during inference could reveal sensitive model information. This extended traditional cryptographic side-channel attacks into the ML domain, creating new vulnerabilities for edge AI devices.\n\nRevealing the architecture not only compromises intellectual property but also gives competitors strategic insights into the design choices that provide competitive advantage.\n\nSystem designers must account for these risks by securing model serialization formats, restricting access to runtime APIs, and hardening deployment pipelines. Protecting models requires a combination of software engineering practices, including access control, encryption, and obfuscation techniques, to reduce the risk of unauthorized extraction [@tramer2016stealing].\n\n#### Approximate Model Theft {#sec-security-privacy-approximate-model-theft-1155}\n\nWhile some attackers seek to extract a model's exact internal properties, others focus on replicating its external behavior. Approximate model behavior theft refers to attacks that attempt to recreate a model's decision-making capabilities without directly accessing its parameters or architecture. Instead, attackers observe the model's inputs and outputs to build a substitute model that performs similarly on the same tasks.\n\nThis type of theft often targets models deployed as services, where the model is exposed through an API or embedded in a user-facing application. By repeatedly querying the model and recording its responses, an attacker can train their own model to mimic the behavior of the original. This process, often called model distillation[^fn-model-distillation] or knockoff modeling, allows attackers to achieve comparable functionality without access to the original model's proprietary internals [@orekondy2019knockoff].\n\n[^fn-model-distillation]: **Model Distillation**: Model distillation is a knowledge transfer technique developed by [@hinton2015distilling] where a smaller \"student\" model learns from a larger \"teacher\" model. While designed for model compression, attackers exploit this to create stolen models with 95%+ accuracy using only 1% of the original training data.\n\nAttackers may evaluate the success of behavior replication in two ways. The first is by measuring the level of effectiveness of the substitute model. This involves assessing whether the cloned model achieves similar accuracy, precision, recall, or other performance metrics on benchmark tasks. By aligning the substitute's performance with that of the original, attackers can build a model that is practically indistinguishable in effectiveness, even if its internal structure differs.\n\nThe second is by testing prediction consistency. This involves checking whether the substitute model produces the same outputs as the original model when presented with the same inputs. Matching not only correct predictions but also the original model's mistakes can provide attackers with a high-fidelity reproduction of the target model's behavior. This poses particular concern in applications such as natural language processing, where attackers might replicate sentiment analysis models to gain competitive insights or bypass proprietary systems.\n\nApproximate behavior theft proves challenging to defend against in open-access deployment settings, such as public APIs or consumer-facing applications. Limiting the rate of queries, detecting automated extraction patterns, and watermarking model outputs are among the techniques that can help mitigate this risk. However, these defenses must be balanced with usability and performance considerations, especially in production environments.\n\nOne demonstration of approximate model theft extracts internal components of black-box language models via public APIs. In their paper, @carlini2024stealing, researchers show how to reconstruct the final embedding projection matrix of several OpenAI models, including `ada`, `babbage`, and `gpt-3.5-turbo`, using only public API access. By exploiting the low-rank structure of the output projection layer and making carefully crafted queries, they recover the model's hidden dimensionality and replicate the weight matrix up to affine transformations.\n\nThe attack does not reconstruct the full model, but reveals internal architecture parameters and sets a precedent for future, deeper extractions. This work demonstrated that even partial model theft poses risks to confidentiality and competitive advantage, especially when model behavior can be probed through rich API responses such as logit bias and log-probabilities.\n\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **Model**                         | **Size**                   | **Number of**       | **RMS**                        | **Cost (USD)**             |\n|                                   | **(Dimension Extraction)** | **Queries**         | **(Weight Matrix Extraction)** |                            |\n+:==================================+:===========================+====================:+:===============================+===========================:+\n| **OpenAI ada**                    | 1024 ✓                     | &lt; 2 \\times 10^6$ | $5 \\cdot 10^{-4}$              | $1 / $4                    |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI babbage**                | 2048 ✓                     | &lt; 4 \\times 10^6$ | $7 \\cdot 10^{-4}$              | $2 / $12                   |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI babbage-002**            | 1536 ✓                     | &lt; 4 \\times 10^6$ | Not implemented                | $2 / $12                   |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI gpt-3.5-turbo-instruct** | Not disclosed              | &lt; 4 \\times 10^7$ | Not implemented                | $200 / ~$2,000 (estimated) |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI gpt-3.5-turbo-1106**     | Not disclosed              | &lt; 4 \\times 10^7$ | Not implemented                | $800 / ~$8,000 (estimated) |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n\n: **Model Stealing Costs**: Attackers can extract model weights with a relatively low query cost using publicly available apis; the table quantifies this threat for OpenAI's ada and babbage models, showing that extracting weights achieves low root mean squared error (RMSE) with fewer than \\(4 \\cdot 10^6\\) queries. Estimated costs for weight extraction range from $1 to $12, demonstrating the economic feasibility of model stealing attacks despite API rate limits and associated expenses. Source: @carlini2024stealing. {#tbl-openai-theft}\n\nAs shown in their empirical evaluation, reproduced in @tbl-openai-theft, model parameters could be extracted with root mean square errors as low as $10^{-4}$, confirming that high-fidelity approximation is achievable at scale. These findings raise important implications for system design, suggesting that innocuous API features, like returning top-k logits, can serve as significant leakage vectors if not tightly controlled.\n\n#### Case Study: Tesla IP Theft {#sec-security-privacy-case-study-tesla-ip-theft-9d78}\n\nIn 2018, Tesla filed a [lawsuit](https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf) against the self-driving car startup [Zoox](https://zoox.com/), alleging that former Tesla employees had stolen proprietary data and trade secrets related to Tesla's autonomous driving technology. According to the lawsuit, several employees transferred over 10 gigabytes of confidential files, including machine learning models and source code, before leaving Tesla to join Zoox.\n\nAmong the stolen materials was a key image recognition model used for object detection in Tesla's self-driving system. By obtaining this model, Zoox could have bypassed years of research and development, giving the company a competitive advantage. Beyond the economic implications, there were concerns that the stolen model could expose Tesla to further security risks, such as model inversion attacks aimed at extracting sensitive data from the model's training set.\n\nThe Zoox employees denied any wrongdoing, and the case was ultimately settled out of court. The incident highlights the real-world risks of model theft, especially in industries where machine learning models represent significant intellectual property. The theft of models not only undermines competitive advantage but also raises broader concerns about privacy, safety, and the potential for downstream exploitation.\n\nThis case demonstrates that model theft is not limited to theoretical attacks conducted over APIs or public interfaces. Insider threats, supply chain vulnerabilities, and unauthorized access to development infrastructure pose equally serious risks to machine learning systems deployed in commercial environments.\n\n### Data Poisoning {#sec-security-privacy-data-poisoning-351f}\n\nWhile model theft targets confidentiality, the second category of threats focuses on training integrity. Training integrity threats stem from the manipulation of data used to train machine learning models. These attacks aim to corrupt the learning process by introducing examples that appear benign but induce harmful or biased behavior in the final model.\n\nData poisoning attacks are a prominent example, in which adversaries inject carefully crafted data points into the training set to influence model behavior in targeted or systemic ways [@biggio2012poisoning]. Poisoned data may cause a model to make incorrect predictions, degrade its generalization ability, or embed failure modes that remain dormant until triggered post-deployment.\n\nData poisoning is a security threat because it involves intentional manipulation of the training data by an adversary, with the goal of embedding vulnerabilities or subverting model behavior. These attacks pose concern in applications where models retrain on data collected from external sources, including user interactions, crowdsourced annotations[^fn-crowdsourcing-risks], and online scraping, since attackers can inject poisoned data without direct access to the training pipeline.\n\n[^fn-crowdsourcing-risks]: **Crowdsourcing Risks**: Platforms like Amazon Mechanical Turk (2005) and Prolific democratized data labeling but introduced poisoning risks. Studies show 15-30% of crowdsourced labels contain errors or bias [@biggio2012poisoning; @oprea2022poisoning], with coordinated attacks capable of poisoning entire datasets at costs under $1,000.\n\nThese attacks occur across diverse threat models. From a security perspective, poisoning attacks vary depending on the attacker's level of access and knowledge. In white-box scenarios, the adversary may have detailed insight into the model architecture or training process, enabling more precise manipulation. In contrast, black-box or limited-access attacks exploit open data submission channels or indirect injection vectors. Poisoning can target different stages of the ML pipeline, ranging from data collection and preprocessing to labeling and storage, making the attack surface both broad and system-dependent. The relative priority of data poisoning threats varies by deployment context as analyzed in @sec-security-privacy-threat-prioritization-framework-f2d5.\n\nPoisoning attacks typically follow a three-stage process. First, the attacker injects malicious data into the training set. These examples are often designed to appear legitimate but introduce subtle distortions that alter the model's learning process. Second, the model trains on this compromised data, embedding the attacker's intended behavior. Finally, once the model is deployed, the attacker may exploit the altered behavior to cause mispredictions, bypass safety checks, or degrade overall reliability.\n\nTo understand these attack mechanisms precisely, data poisoning can be viewed as a bilevel optimization problem, where the attacker seeks to select poisoning data $D_p$ that maximizes the model's loss on a validation or target dataset $D_{\\text{test}}$. Let $D$ represent the original training data. The attacker's objective is to solve:\n$$\n\\max_{D_p} \\ \\mathcal{L}(f_{D \\cup D_p}, D_{\\text{test}})\n$$\nwhere $f_{D \\cup D_p}$ represents the model trained on the combined dataset of original and poisoned data. For targeted attacks, this objective can be refined to focus on specific inputs $x_t$ and target labels $y_t$:\n$$\n\\max_{D_p} \\ \\mathcal{L}(f_{D \\cup D_p}, x_t, y_t)\n$$\n\nThis formulation captures the adversary's goal of introducing carefully crafted data points to manipulate the model's decision boundaries.\n\nFor example, consider a traffic sign classification model trained to distinguish between stop signs and speed limit signs. An attacker might inject a small number of stop sign images labeled as speed limit signs into the training data. The attacker's goal is to subtly shift the model's decision boundary so that future stop signs are misclassified as speed limit signs. In this case, the poisoning data $D_p$ consists of mislabeled stop sign images, and the attacker's objective is to maximize the misclassification of legitimate stop signs $x_t$ as speed limit signs $y_t$, following the targeted attack formulation above. Even if the model performs well on other types of signs, the poisoned training process creates a predictable and exploitable vulnerability.\n\nData poisoning attacks can be classified based on their objectives and scope of impact. Availability attacks degrade overall model performance by introducing noise or label flips that reduce accuracy across tasks. Targeted attacks manipulate a specific input or class, leaving general performance intact but causing consistent misclassification in select cases. Backdoor attacks[^fn-backdoor-attacks] embed hidden triggers, which are often imperceptible patterns, that elicit malicious behavior only when the trigger is present. Subpopulation attacks degrade performance on a specific group defined by shared features, making them particularly dangerous in fairness-sensitive applications.\n\n[^fn-backdoor-attacks]: **Backdoor Attacks**: Backdoor attacks involve hidden triggers embedded in ML models during training, first demonstrated in 2017. These attacks achieve 99%+ success rates while maintaining normal accuracy, with triggers as subtle as single-pixel modifications. BadNets, the seminal backdoor attack, affected 100% of tested models.\n\nA notable real-world example of a targeted poisoning attack was demonstrated against Perspective, Google's widely-used online toxicity detection model[^fn-perspective-api] that helps platforms identify harmful content [@hosseini2017deceiving]. By injecting synthetically generated toxic comments with subtle misspellings and grammatical errors into the model's training set, researchers degraded its ability to detect harmful content[^fn-perspective-vulnerability].\n\n[^fn-perspective-api]: **Perspective API**: Google's Perspective API is a toxicity detection model launched in 2017, now processing 500+ million comments daily across platforms like The New York Times and Wikipedia. Despite sophisticated training, the API demonstrates how even billion-parameter models remain vulnerable to targeted poisoning attacks.\n\n[^fn-perspective-vulnerability]: **Perspective Vulnerability**: After retraining, the poisoned model exhibited a significantly higher false negative rate, allowing offensive language to bypass filters. This demonstrates how poisoned data can exploit feedback loops in user-generated content systems, creating long-term vulnerabilities in content moderation pipelines.\n\nMitigating data poisoning threats requires end-to-end security of the data pipeline, encompassing collection, storage, labeling, and training. Preventative measures include input validation checks, integrity verification of training datasets, and anomaly detection to flag suspicious patterns. In parallel, robust training algorithms can limit the influence of mislabeled or manipulated data by down-weighting or filtering out anomalous instances. While no single technique guarantees immunity, combining proactive data governance, automated monitoring, and robust learning practices is important for maintaining model integrity in real-world deployments.\n\n### Adversarial Attacks {#sec-security-privacy-adversarial-attacks-9f84}\n\nMoving from training-time to inference-time threats, the third category targets model robustness during deployment. Inference robustness threats occur when attackers manipulate inputs at test time to induce incorrect predictions. Unlike data poisoning, which compromises the training process, these attacks exploit vulnerabilities in the model's decision surface during inference.\n\nA central class of such threats is adversarial attacks, where carefully constructed inputs are designed to cause incorrect predictions while remaining nearly indistinguishable from legitimate data. As detailed in @sec-robust-ai, these attacks highlight vulnerabilities in ML models' sensitivity to small, targeted perturbations that can drastically alter output confidence or classification results.\n\nThese attacks create significant real-world risks in domains such as autonomous driving, biometric authentication, and content moderation. The effectiveness can be striking: research demonstrates that adversarial examples can achieve 99%+ attack success rates against state-of-the-art image classifiers while modifying less than 0.01% of pixel values, changes virtually imperceptible to humans [@szegedy2014intriguing; @goodfellow2015explaining]. In physical-world attacks, printed adversarial patches as small as 2% of an image can cause autonomous vehicles to misclassify stop signs as speed limit signs with 80%+ success rates under varying lighting conditions [@eykholt2018robust].\n\nUnlike data poisoning, which corrupts the model during training, adversarial attacks manipulate the model's behavior at test time, often without requiring any access to the training data or model internals. The attack surface thus shifts from upstream data pipelines to real-time interaction, demanding robust defense mechanisms capable of detecting or mitigating malicious inputs at the point of inference.\n\nThe mathematical foundations of adversarial example generation and comprehensive taxonomies of attack algorithms, including gradient-based, optimization-based, and transfer-based techniques, are covered in detail in @sec-robust-ai, which explores robust approaches to building adversarially resistant systems.\n\nAdversarial attacks vary based on the attacker's level of access to the model. In white-box attacks, the adversary has full knowledge of the model's architecture, parameters, and training data, allowing them to craft highly effective adversarial examples. In black-box attacks, the adversary has no internal knowledge and must rely on querying the model and observing its outputs. Grey-box attacks fall between these extremes, with the adversary possessing partial information, such as access to the model architecture but not its parameters.\n\nThese attacker models can be summarized along a spectrum of knowledge levels. @tbl-adversary-knowledge-spectrum highlights the differences in model access, data access, typical attack strategies, and common deployment scenarios. Such distinctions help characterize the practical challenges of securing ML systems across different deployment environments.\n\nCommon attack strategies include surrogate model construction, transfer attacks exploiting adversarial transferability, and GAN-based perturbation generation. The technical details of these approaches and their mathematical formulations are thoroughly covered in @sec-robust-ai.\n\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n| **Adversary Knowledge Level** | **Model Access**                             | **Training Data Access** | **Attack Example**                             | **Common Scenario**                         |\n+:==============================+:=============================================+:=========================+:===============================================+:============================================+\n| **White-box**                 | Full access to architecture and parameters   | Full access              | Crafting adversarial examples using gradients  | Insider threats, open-source model reuse    |\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n| **Grey-box**                  | Partial access (e.g., architecture only)     | Limited or no access     | Attacks based on surrogate model approximation | Known model family, unknown fine-tuning     |\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n| **Black-box**                 | No internal access; only query-response view | No access                | Query-based surrogate model training and       | Public APIs, model-as-a-service deployments |\n|                               |                                              |                          | transfer attacks                               |                                             |\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n\n: **Adversarial Knowledge Spectrum**: Varying levels of attacker access to model details and training data define distinct threat models, influencing the feasibility and sophistication of adversarial attacks and impacting deployment security strategies. The table categorizes these models by access level, typical attack methods, and common deployment scenarios, clarifying the practical challenges of securing machine learning systems. {#tbl-adversary-knowledge-spectrum}\n\nOne illustrative example involves the manipulation of traffic sign recognition systems [@eykholt2018robust]. Researchers demonstrated that placing small stickers on stop signs could cause machine learning models to misclassify them as speed limit signs. While the altered signs remained easily recognizable to humans, the model consistently misinterpreted them. Such attacks pose serious risks in applications like autonomous driving, where reliable perception is important for safety.\n\nAdversarial attacks highlight the need for robust defenses that go beyond improving model accuracy. Securing ML systems against adversarial threats requires runtime defenses such as input validation, anomaly detection, and monitoring for abnormal patterns during inference. Training-time robustness methods, including adversarial training where models learn from perturbed examples, complement these runtime strategies and are explored later in this volume. These defenses aim to enhance model resilience against adversarial examples, ensuring that machine learning systems can operate reliably even in the presence of malicious inputs.\n\n### Case Study: Traffic Sign Attack {#sec-security-privacy-case-study-traffic-sign-attack-6e93}\n\nIn 2017, researchers conducted experiments by placing small black and white stickers on stop signs [@eykholt2018robust]. As shown in @fig-adversarial-stickers, these stickers were designed to be nearly imperceptible to the human eye, yet they significantly altered the appearance of the stop sign when viewed by machine learning models. When viewed by a normal human eye, the stickers did not obscure the sign or prevent interpretability. However, when images of the stickers stop signs were fed into standard traffic sign classification ML models, they were misclassified as speed limit signs over 85% of the time.\n\n![**Adversarial Stickers**: Nearly imperceptible stickers can trick machine learning models into misclassifying stop signs as speed limit signs over 85% of the time. This emphasizes the vulnerability of ML systems to adversarial attacks. Source: @eykholt2018robust.](./images/png/stop_signs.png){#fig-adversarial-stickers}\n\nThis demonstration showed how simple adversarial stickers could trick ML systems into misreading important road signs. If deployed realistically, these attacks could endanger public safety, causing autonomous vehicles to misinterpret stop signs as speed limits. Researchers warned this could potentially cause dangerous rolling stops or acceleration into intersections.\n\nThis case study provides a concrete illustration of how adversarial examples exploit the pattern recognition mechanisms of ML models. By subtly altering the input data, attackers can induce incorrect predictions and pose significant risks to safety-important applications like self-driving cars. The attack's simplicity demonstrates how even minor, imperceptible changes can lead models astray. Consequently, developers must implement robust defenses against such threats.\n\nThese threat types span different stages of the ML lifecycle and demand distinct defensive strategies. @tbl-threats-models-summary below summarizes their key characteristics.\n\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n| **Threat Type**         | **Lifecycle Stage** | **Attack Vector**         | **Example Impact**                            |\n+:========================+:====================+:==========================+:==============================================+\n| **Model Theft**         | Deployment          | API access, insider leaks | Stolen IP, model inversion, behavioral clone  |\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n| **Data Poisoning**      | Training            | Label flipping, backdoors | Targeted misclassification, degraded accuracy |\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n| **Adversarial Attacks** | Inference           | Input perturbation        | Real-time misclassification, safety failure   |\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n\n: **Threat Landscape**: Machine learning systems face diverse threats throughout their lifecycle, ranging from data manipulation during training to model theft post-deployment. The table categorizes these threats by lifecycle stage and attack vector, clarifying how vulnerabilities manifest and enabling targeted mitigation strategies. {#tbl-threats-models-summary}\n\nThe appropriate defense for a given threat depends on its type, attack vector, and where it occurs in the ML lifecycle. @fig-threat-mitigation-flow provides a simplified decision flow that connects common threat categories, such as model theft, data poisoning, and adversarial examples, to corresponding defensive strategies. While real-world deployments may require more nuanced combinations of defenses as discussed in our layered defense framework, this flowchart serves as a conceptual guide for aligning threat models with practical mitigation techniques.\n\n::: {#fig-threat-mitigation-flow fig-env=\"figure\" fig-pos=\"H\"}\n```{.tikz}\n\\scalebox{0.65}{%\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={line width=1.0pt,black!50},\nBox/.style={inner xsep=2pt,inner ysep=2pt,\n    node distance=0.5,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=33mm,\n    minimum width=33mm, minimum height=9.5mm\n  },\nBox2/.style={Box,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2},\nBox4/.style={Box,  draw=OrangeLine,fill=OrangeL!50,text width=43mm}\n}\n\\node[Box](B1){Model Theft};\n\\node[Box,below=of B1](B2){Secure Model Access};\n\\node[Box,below=of B2](B3){Encrypt Artifacts \\& Obfuscate APIs};\n\\node[Box,below=of B3](B4){Monitor for Behavioral Clones};\n%\n\\node[Box2,right=1.75of B1](SB1){Data Poisoning};\n\\node[Box2,below=of SB1](SB2){Validate Training Data};\n\\node[Box2,below=of SB2](SB3){Use Robust Training Methods};\n\\node[Box2,below=of SB3](SB4){Apply Data Provenance Checks};\n%\n\\node[Box3,right=1.75of SB1](RB1){Adversarial Examples};\n\\node[Box3,below=of RB1](RB2){Add Input Validation};\n\\node[Box3,below=of RB2](RB3){Use Adversarial Training};\n\\node[Box3,below=of RB3](RB4){Deploy Runtime Monitors};\n%\n\\node[Box4,above=0.6of SB1](B0){Start: Identify Threat Type};\n\\foreach \\x in{,S,R}{\n\\foreach \\i in{1,2,3}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](\\x B\\i)--(\\x B\\newI);\n}\n}\n\\draw[Line,-latex](B0)--(SB1);\n\\draw[Line,-latex](B0)-|(B1);\n\\draw[Line,-latex](B0)-|(RB1);\n\\end{tikzpicture}}\n```\n**Threat Mitigation Flow**: This diagram maps common machine learning threats to corresponding defense strategies, guiding selection based on attack vector and lifecycle stage. By following this flow, practitioners can align threat models with practical mitigation techniques, such as secure model access and data sanitization, to build more robust AI systems.\n:::\n\nWhile ML models themselves present important attack surfaces, they ultimately run on hardware that can introduce vulnerabilities beyond the model's control. The transition from software-based threats to hardware-based vulnerabilities represents a significant shift in the security landscape. Where software attacks target code logic and data flows, hardware attacks exploit the physical properties of the computing substrate itself.\n\nThe specialized computing infrastructure that powers machine learning workloads creates a layered attack surface that extends far beyond traditional software vulnerabilities. This includes the processors that execute instructions, the memory systems that store data, and the interconnects that move information between components. Understanding these hardware-level risks is essential because they can bypass conventional software security mechanisms and remain difficult to detect. These risks are addressed through the hardware-based security mechanisms detailed in @sec-security-privacy-hardware-security-foundations-f5e8.\n\nIn the next section, we examine how adversaries can target the physical infrastructure that executes machine learning workloads through hardware bugs, physical tampering, side channels, and supply chain risks.\n\n## Hardware-Level Security Vulnerabilities {#sec-security-privacy-hardwarelevel-security-vulnerabilities-1ab4}\n\nAs machine learning systems move from research prototypes to large-scale, real-world deployments, their security depends on the hardware platforms they run on. Whether deployed in data centers, on edge devices, or in embedded systems, machine learning applications rely on a layered stack of processors, accelerators, memory, and communication interfaces. These hardware components, while essential for enabling efficient computation, introduce unique security risks that go beyond traditional software-based vulnerabilities.\n\nUnlike general-purpose software systems, machine learning workflows often process high-value models and sensitive data in performance-constrained environments. This makes them attractive targets not only for software attacks but also for hardware-level exploitation. Vulnerabilities in hardware can expose models to theft, leak user data, disrupt system reliability, or allow adversaries to manipulate inference results. Because hardware operates below the software stack, such attacks can bypass conventional security mechanisms and remain difficult to detect.\n\nUnderstanding hardware security threats requires considering how computing substrates implement machine learning operations. At the hardware level, CPU components like arithmetic logic units, registers, and caches execute the instructions that drive model inference and training. Memory hierarchies determine how quickly models can access parameters and intermediate results. The hardware-software interface, mediated by firmware and bootloaders, establishes the initial trust foundation for system operation. The physical properties of computation—including power consumption, timing characteristics, and electromagnetic emissions—create observable signals that attackers can exploit to extract sensitive information.\n\nHardware threats arise from multiple sources that span the entire system lifecycle. Design flaws in processor architectures, exemplified by vulnerabilities like Meltdown and Spectre, can compromise security guarantees. Physical tampering enables direct manipulation of components and data flows. Side-channel attacks exploit unintended information leakage through power traces, timing variations, and electromagnetic radiation. Supply chain compromises introduce malicious components or modifications during manufacturing and distribution. Together, these threats form a critical attack surface that must be addressed to build trustworthy machine learning systems. For readers focusing on practical deployment, the key lessons center on supply chain verification, physical access controls, and hardware trust anchors, while the defensive strategies in @sec-security-privacy-comprehensive-defense-architectures-48ab provide actionable guidance regardless of deep architectural expertise.\n\n@tbl-threat_types summarizes the major categories of hardware security threats, describing their origins, methods, and implications for machine learning system design and deployment.\n\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Threat Type**             | **Description**                                                                                 | **Relevance to ML Hardware Security**          |\n+:============================+:================================================================================================+:===============================================+\n| **Hardware Bugs**           | Intrinsic flaws in hardware designs that can compromise system integrity.                       | Foundation of hardware vulnerability.          |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Physical Attacks**        | Direct exploitation of hardware through physical access or manipulation.                        | Basic and overt threat model.                  |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Fault-injection Attacks** | Induction of faults to cause errors in hardware operation, leading to potential system crashes. | Systematic manipulation leading to failure.    |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Side-Channel Attacks**    | Exploitation of leaked information from hardware operation to extract sensitive data.           | Indirect attack via environmental observation. |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Leaky Interfaces**        | Vulnerabilities arising from interfaces that expose data unintentionally.                       | Data exposure through communication channels.  |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Counterfeit Hardware**    | Use of unauthorized hardware components that may have security flaws.                           | Compounded vulnerability issues.               |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Supply Chain Risks**      | Risks introduced through the hardware lifecycle, from production to deployment.                 | Cumulative & multifaceted security challenges. |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n\n: **Hardware Threat Landscape**: Machine learning systems face diverse hardware threats ranging from intrinsic design flaws to physical attacks and supply chain vulnerabilities. Understanding these threats, and their relevance to ML hardware, is essential for building secure and trustworthy AI deployments. {#tbl-threat_types}\n\n### Hardware Bugs {#sec-security-privacy-hardware-bugs-9efc}\n\nThe first category of hardware threats stems from design vulnerabilities. Hardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. One of the most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[^fn-meltdown-spectre-impact]—two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation and read the data of other applications and the operating system [@Lipp2018meltdown; @Kocher2018spectre].\n\n[^fn-meltdown-spectre-impact]: **Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually every processor made since 1995 (billions of devices). The disclosure triggered emergency patches across all major operating systems, causing 5-30% performance degradation in some workloads, and led to a core rethinking of processor security.\n\nThese attacks exploit speculative execution[^fn-speculative-execution], a performance optimization in CPUs that executes instructions out of order before safety checks are complete. While improving computational speed, this optimization inadvertently exposes sensitive data through microarchitectural side channels, such as CPU caches. The technical sophistication of these attacks highlights the difficulty of eliminating vulnerabilities even with extensive hardware validation.\n\n[^fn-speculative-execution]: **Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes instructions before confirming they're needed, improving performance by 10-25%. However, it creates a 20+ year attack window where speculated operations leak data through cache timing, affecting ML accelerators that rely on similar optimizations.\n\nFurther research has revealed that these were not isolated incidents. Variants such as Foreshadow, ZombieLoad, and RIDL target different microarchitectural elements, ranging from secure enclaves to CPU internal buffers, demonstrating that speculative execution flaws are a systemic hardware risk. This systemic nature means that while these attacks were first demonstrated on general-purpose CPUs, their implications extend to machine learning accelerators and specialized hardware. ML systems often rely on heterogeneous compute platforms that combine CPUs with GPUs, TPUs, FPGAs, or custom accelerators. These components process sensitive data such as personal information, medical records, or proprietary models. Vulnerabilities in any part of this stack could expose such data to attackers.\n\nFor example, an edge device like a smart camera running a face recognition model on an accelerator could be vulnerable if the hardware lacks proper cache isolation. An attacker might exploit this weakness to extract intermediate computations, model parameters, or user data. Similar risks exist in cloud inference services, where hardware multi-tenancy increases the chances of cross-tenant data leakage.\n\nSuch vulnerabilities pose concern in privacy-sensitive domains like healthcare, where ML systems routinely handle patient data. A breach could violate privacy regulations such as the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[^fn-hipaa-violations], leading to significant legal and ethical consequences. Similar regulatory risks apply globally, with GDPR[^fn-gdpr] imposing fines up to 4% of global revenue for organizations that fail to implement appropriate technical measures to protect EU citizens' data.\n\n[^fn-hipaa-violations]: **HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fines, with individual penalties reaching $16 million. The largest healthcare data breach affected 78.8 million patients at Anthem Inc. in 2015, highlighting the massive scale of exposure when ML systems handling medical data are compromised.\n\n[^fn-gdpr]: **General Data Protection Regulation (GDPR)**: Enacted by the EU in 2018, GDPR imposes fines up to 4% of global revenue (€20+ million) for privacy violations. Since enforcement began, over €4.5 billion in fines have been levied, including €746 million against Amazon in 2021, driving massive investment in privacy-preserving ML technologies.\n\nThese examples illustrate that hardware security is not solely about preventing physical tampering. It also requires architectural safeguards to prevent data leakage through the hardware itself. As new vulnerabilities continue to emerge across processors, accelerators, and memory systems, addressing these risks requires continuous mitigation efforts, often involving performance trade-offs, especially in compute- and memory-intensive ML workloads. Proactive solutions, such as confidential computing and trusted execution environments (TEEs), offer promising architectural defenses. However, achieving robust hardware security requires attention at every stage of the system lifecycle, from design to deployment.\n\n### Physical Attacks {#sec-security-privacy-physical-attacks-095a}\n\nBeyond design flaws, the second category involves direct physical manipulation. Physical tampering refers to the direct, unauthorized manipulation of computing hardware to undermine the integrity of machine learning systems. This type of attack is particularly concerning because it bypasses traditional software security defenses, directly targeting the physical components on which machine learning depends. ML systems are especially vulnerable to such attacks because they rely on hardware sensors, accelerators, and storage to process large volumes of data and produce reliable outcomes in real-world environments.\n\nWhile software security measures, including encryption, authentication, and access control, protect ML systems against remote attacks, they offer little defense against adversaries with physical access to devices. Physical tampering can range from simple actions, like inserting a malicious USB device into an edge server, to highly sophisticated manipulations such as embedding hardware trojans during chip manufacturing. These threats are particularly relevant for machine learning systems deployed at the edge or in physically exposed environments, where attackers may have opportunities to interfere with the hardware directly.\n\nTo understand how such attacks affect ML systems in practice, consider the example of an ML-powered drone used for environmental mapping or infrastructure inspection. The drone's navigation depends on machine learning models that process data from GPS, cameras, and inertial measurement units. If an attacker gains physical access to the drone, they could replace or modify its navigation module, embedding a hidden backdoor that alters flight behavior or reroutes data collection. Such manipulation not only compromises the system's reliability but also opens the door to misuse, such as surveillance or smuggling operations.\n\nThese threats extend across application domains. Physical attacks are not limited to mobility systems. Biometric access control systems, which rely on ML models to process face or fingerprint data, are also vulnerable. These systems typically use embedded hardware to capture and process biometric inputs. An attacker could physically replace a biometric sensor with a modified component designed to capture and transmit personal identification data to an unauthorized receiver. This creates multiple vulnerabilities including unauthorized data access and enabling future impersonation attacks.\n\nIn addition to tampering with external sensors, attackers may target internal hardware subsystems. For example, the sensors used in autonomous vehicles, including cameras, LiDAR, and radar, are important for ML models that interpret the surrounding environment. A malicious actor could physically misalign or obstruct these sensors, degrading the model's perception capabilities and creating safety hazards.\n\nHardware trojans pose another serious risk. Malicious modifications introduced during chip fabrication or assembly can embed dormant circuits in ML accelerators or inference chips. These trojans may remain inactive under normal conditions but trigger malicious behavior when specific inputs are processed or system states are reached. Such hidden vulnerabilities can disrupt computations, leak model outputs, or degrade system performance in ways that are extremely difficult to diagnose post-deployment.\n\nMemory subsystems are also attractive targets. Attackers with physical access to edge devices or embedded ML accelerators could manipulate memory chips to extract encrypted model parameters or training data. Fault injection techniques, including voltage manipulation and electromagnetic interference, can further degrade system reliability by corrupting model weights or forcing incorrect computations during inference.\n\nPhysical access threats extend to data center and cloud environments as well. Attackers with sufficient access could install hardware implants, such as keyloggers or data interceptors, to capture administrative credentials or monitor data streams. Such implants can provide persistent backdoor access, enabling long-term surveillance or data exfiltration from ML training and inference pipelines.\n\nIn summary, physical attacks on machine learning systems threaten both security and reliability across a wide range of deployment environments. Addressing these risks requires a combination of hardware-level protections, tamper detection mechanisms, and supply chain integrity checks. Without these safeguards, even the most secure software defenses may be undermined by vulnerabilities introduced through direct physical manipulation.\n\n### Fault Injection Attacks {#sec-security-privacy-fault-injection-attacks-8c52}\n\nBuilding on physical tampering techniques, fault injection represents a more sophisticated approach to hardware exploitation. Fault injection is a powerful class of physical attacks that deliberately disrupts hardware operations to induce errors in computation. These induced faults can compromise the integrity of machine learning models by causing them to produce incorrect outputs, degrade reliability, or leak sensitive information. For ML systems, such faults not only disrupt inference but also expose models to deeper exploitation, including reverse engineering and bypass of security protocols [@joye2012fault].\n\nAttackers achieve fault injection by applying precisely timed physical or electrical disturbances to the hardware while it is executing computations. Techniques such as low-voltage manipulation [@barenghi2010low], power spikes [@hutter2009contact], clock glitches [@amiel2006fault], electromagnetic pulses [@agrawal2003side], temperature variations [@skorobogatov2009local], and even laser strikes [@skorobogatov2003optical] have been demonstrated to corrupt specific parts of a program's execution. These disturbances can cause effects such as bit flips, skipped instructions, or corrupted memory states, which adversaries can exploit to alter ML model behavior or extract sensitive information.\n\nFor machine learning systems, these attacks pose several concrete risks. Fault injection can degrade model accuracy, force incorrect classifications, trigger denial of service, or even leak internal model parameters. For example, attackers could inject faults into an embedded ML model running on a microcontroller, forcing it to misclassify inputs in safety-important applications such as autonomous navigation or medical diagnostics. More sophisticated attackers may target memory or control logic to steal intellectual property, such as proprietary model weights or architecture details.\n\nThe practical viability of these attacks has been demonstrated through controlled experiments. One notable example is the work by @breier2018deeplaser, where researchers successfully used a laser fault injection attack on a deep neural network deployed on a microcontroller. By heating specific transistors, as shown in @fig-laser-bitflip. they forced the hardware to skip execution steps, including a ReLU activation function.\n\n![**Laser Fault Injection**: Focused laser pulses induce bit flips within microcontroller memory, enabling attackers to manipulate model execution and compromise system integrity. Researchers utilize this technique to simulate hardware errors, revealing vulnerabilities in embedded machine learning systems and informing the development of fault-tolerant designs. Source: [@breier2018deeplaser].](images/png/laser_bitflip.png){#fig-laser-bitflip fig-pos=t!}\n\nThis manipulation is illustrated in @fig-injection, which shows a segment of assembly code implementing the ReLU activation function. Normally, the code compares the most significant bit (MSB) of the accumulator to zero and uses a brge (branch if greater or equal) instruction to skip the assignment if the value is non-positive. However, the fault injection suppresses the branch, causing the processor to always execute the \"else\" block. As a result, the neuron's output is forcibly zeroed out, regardless of the input value.\n\n![**Fault Injection Attack**: Manipulating assembly code bypasses safety checks, forcing a neuron’s output to zero regardless of input and demonstrating a hardware vulnerability in machine learning systems. Source: [@breier2018deeplaser].](images/png/fault-injection_demonstrated_with_assembly_code.png){#fig-injection width=75% fig-pos=b!}\n\nFault injection attacks can also be combined with side-channel analysis, where attackers first observe power or timing characteristics to infer model structure or data flow. This reconnaissance allows them to target specific layers or operations, such as activation functions or final decision layers, maximizing the impact of the injected faults.\n\nEmbedded and edge ML systems are particularly vulnerable because they often lack physical hardening and operate under resource constraints that limit runtime defenses. Without tamper-resistant packaging or secure hardware enclaves, attackers may gain direct access to system buses and memory, enabling precise fault manipulation. Many embedded ML models are designed to be lightweight, leaving them with little redundancy or error correction to recover from induced faults.\n\nMitigating fault injection requires multiple complementary protections. Physical protections, such as tamper-proof enclosures and design obfuscation, help limit physical access. Anomaly detection techniques can monitor sensor inputs or model outputs for signs of fault-induced inconsistencies [@hsiao2023mavfi]. Error-correcting memories and secure firmware can reduce the likelihood of silent corruption. Techniques such as model watermarking may provide traceability if stolen models are later deployed by an adversary.\n\nThese protections are difficult to implement in cost- and power-constrained environments, where adding cryptographic hardware or redundancy may not be feasible. Achieving resilience to fault injection requires cross-layer design considerations that span electrical, firmware, software, and system architecture levels. Without such holistic design practices, ML systems deployed in the field may remain exposed to these low-cost yet highly effective physical attacks.\n\n### Side-Channel Attacks {#sec-security-privacy-sidechannel-attacks-cdfd}\n\nMoving from direct fault injection to indirect information leakage, side-channel attacks constitute a class of security breaches that exploit information inadvertently revealed through the physical implementation of computing systems. In contrast to direct attacks that target software or network vulnerabilities, these attacks use the system's hardware characteristics, including power consumption, electromagnetic emissions, or timing behavior, to extract sensitive information.\n\nThe core premise of a side-channel attack is that a device's operation can leak information through observable physical signals. Such leaks may originate from the electrical power the device consumes [@kocher1999differential], the electromagnetic fields it emits [@gandolfi2001electromagnetic], the time required to complete computations, or even the acoustic noise it produces. By carefully measuring and analyzing these signals, attackers can infer internal system states or recover secret data.\n\nAlthough these techniques are commonly discussed in cryptography, they are equally relevant to machine learning systems. ML models deployed on hardware accelerators, embedded devices, or edge systems often process sensitive data. Even when these models are protected by secure algorithms or encryption, their physical execution may leak side-channel signals that can be exploited by adversaries.\n\nOne of the most widely studied examples involves Advanced Encryption Standard (AES)[^fn-aes-standard] implementations. While AES is mathematically secure, the physical process of computing its encryption functions leaks measurable signals.\n\n[^fn-aes-standard]: **Advanced Encryption Standard (AES)**: Adopted by NIST in 2001 as the US government encryption standard, AES replaced DES after 24 years. Despite being mathematically secure with 2^128 possible keys for AES-128, physical implementations remain vulnerable to side-channel attacks that can extract keys in minutes. Techniques such as Differential Power Analysis (DPA), Differential Electromagnetic Analysis (DEMA), and Correlation Power Analysis (CPA) exploit these physical signals to recover secret keys.\n\nA useful example of this attack technique can be seen in a power analysis of a password authentication process. Consider a device that verifies a 5-byte password—in this case, `0x61, 0x52, 0x77, 0x6A, 0x73`. During authentication, the device receives each byte sequentially over a serial interface, and its power consumption pattern reveals how the system responds as it processes these inputs.\n\n@fig-encryption shows the device's behavior when the correct password is entered. The red waveform captures the serial data stream, marking each byte as it is received. The blue curve records the device's power consumption over time. When the full, correct password is supplied, the power profile remains stable and consistent across all five bytes, providing a clear baseline for comparison with failed attempts.\n\n![**Power Profile**: The device's power consumption remains stable during authentication when the correct password is entered, setting a baseline for comparison in subsequent figures through This figure. Source: colin o'flynn.](images/png/power_analysis_of_an_encryption_device_with_a_correct_password.png){#fig-encryption}\n\nWhen an incorrect password is entered, the power analysis chart changes as shown in @fig-encryption2. In this case, the first three bytes (`0x61, 0x52, 0x77`) are correct, so the power patterns closely match the correct password up to that point. However, when the fourth byte (`0x42`) is processed and found to be incorrect, the device halts authentication. This change is reflected in the sudden jump in the blue power line, indicating that the device has stopped processing and entered an error state.\n\n![**Side-Channel Attack Vulnerability**: Power consumption patterns reveal cryptographic key information during authentication; consistent power usage indicates correct password bytes, while abrupt changes signal incorrect input and halted processing. Even without knowing the password, an attacker can infer it by analyzing the device's power usage during authentication attempts via this figure. Source: Colin O'Flynn.](images/png/power_analysis_of_an_encryption_device_with_a_partially_wrong_password.png){#fig-encryption2}\n\n@fig-encryption3 shows the case where the password is entirely incorrect (`0x30, 0x30, 0x30, 0x30, 0x30`). Here, the device detects the mismatch immediately after the first byte and halts processing much earlier. This is again visible in the power profile, where the blue line exhibits a sharp jump following the first byte, reflecting the device's early termination of authentication.\n\n![**Power Consumption Jump**: The blue line's sharp increase after processing the first byte indicates immediate authentication failure, highlighting how incorrect passwords are quickly detected through power usage. Source: Colin O'Flynn.](images/png/power_analysis_of_an_encryption_device_with_a_wrong_password.png){#fig-encryption3}\n\nThese examples demonstrate how attackers can exploit observable power consumption differences to reduce the search space and eventually recover secret data through brute-force analysis. By systematically measuring power consumption patterns and correlating them with different inputs, attackers can extract sensitive information that should remain hidden.\n\n{{< margin-video \"https://www.youtube.com/watch?v=2iDLfuEBcs8\" \"Power Attack\" \"Colin O'Flynn\" >}}\n\nThe scope of these vulnerabilities extends beyond cryptographic applications. Machine learning applications face similar risks. For example, an ML-based speech recognition system processing voice commands on a local device could leak timing or power signals that reveal which commands are being processed. Even subtle acoustic or electromagnetic emissions may expose operational patterns that an adversary could exploit to infer user behavior.\n\nHistorically, side-channel attacks have been used to bypass even the most secure cryptographic systems. In the 1960s, British intelligence agency MI5 famously exploited acoustic emissions from a cipher machine in the Egyptian Embassy [@Burnet1989Spycatcher]. By capturing the mechanical clicks of the machine's rotors, MI5 analysts were able to dramatically reduce the complexity of breaking encrypted messages. This early example illustrates that side-channel vulnerabilities are not confined to the digital age but are rooted in the physical nature of computation.\n\nToday, these techniques have advanced to include attacks such as keyboard eavesdropping [@Asonov2004Keyboard], power analysis on cryptographic hardware [@gnad2017voltage], and voltage-based attacks on ML accelerators [@zhao2018fpga]. Timing attacks, electromagnetic leakage, and thermal emissions continue to provide adversaries with indirect channels for observing system behavior.\n\nMachine learning systems deployed on specialized accelerators or embedded platforms are especially at risk. Attackers may exploit side-channel signals to infer model structure, steal parameters, or reconstruct private training data. As ML becomes increasingly deployed in cloud, edge, and embedded environments, these side-channel vulnerabilities pose significant challenges to system security.\n\nUnderstanding the persistence and evolution of side-channel attacks is important for building resilient machine learning systems. By recognizing that where there is a signal, there is potential for exploitation, system designers can begin to address these risks through a combination of hardware shielding, algorithmic defenses, and operational safeguards.\n\n### Leaky Interfaces {#sec-security-privacy-leaky-interfaces-9206}\n\nWhile side-channel attacks exploit unintended physical signals, leaky interfaces represent a different category of vulnerability involving exposed communication channels. Interfaces in computing systems are important for enabling communication, diagnostics, and updates. However, these same interfaces can become significant security vulnerabilities when they unintentionally expose sensitive information or accept unverified inputs. Such leaky interfaces often go unnoticed during system design, yet they provide attackers with powerful entry points to extract data, manipulate functionality, or introduce malicious code.\n\nA leaky interface is any access point that reveals more information than intended, often because of weak authentication, lack of encryption, or inadequate isolation. These issues have been widely demonstrated across consumer, medical, and industrial systems.\n\nFor example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports[^fn-iot-vulnerabilities], allowing attackers to intercept live audio and video feeds from inside private homes. Similarly, researchers have identified wireless vulnerabilities in pacemakers[^fn-medical-device-security] that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening safety concerns.\n\n[^fn-iot-vulnerabilities]: **IoT Device Vulnerabilities**: Studies reveal 70-80% of IoT devices contain serious security flaws, with baby monitors among the worst offenders. Security firm Rapid7 found that popular baby monitor brands exposed unencrypted video streams, affecting millions of households globally.\n\n[^fn-medical-device-security]: **Medical Device Security**: FDA reports show 53% of medical devices contain known vulnerabilities, with pacemakers and insulin pumps most at risk. The average medical device contains 6.2 vulnerabilities, some dating back over a decade, affecting 2.4 billion medical devices worldwide.\n\nA notable case involving smart lightbulbs demonstrated that accessible debug ports[^fn-debug-ports] left on production devices leaked unencrypted WiFi credentials. This security oversight provided attackers with a pathway to infiltrate home networks without needing to bypass standard security mechanisms.\n\n[^fn-debug-ports]: **Debug Port Vulnerabilities**: Hardware debug interfaces like JTAG (1990) and SWD (2006) are essential for development but often left accessible in production. Security researchers estimate that 60-70% of embedded devices ship with unsecured debug ports, creating backdoors for attackers.\n\nThese examples reveal vulnerability patterns that directly apply to machine learning deployments. While these examples do not target machine learning systems directly, they illustrate architectural patterns that are highly relevant to ML-allowd devices. Consider a smart home security system that uses machine learning to detect user routines and automate responses. Such a system may include a maintenance or debug interface for software updates. If this interface lacks proper authentication or transmits data unencrypted, attackers on the same network could gain unauthorized access. This intrusion could expose user behavior patterns, compromise model integrity, or disable security features altogether.\n\nLeaky interfaces in ML systems can also expose training data, model parameters, or intermediate outputs. Such exposure can allow attackers to craft adversarial examples, steal proprietary models, or reverse-engineer system behavior. Worse still, these interfaces may allow attackers to tamper with firmware, introducing malicious code that disables devices or recruits them into botnets.\n\nMitigating these risks requires coordinated protections across technical and organizational domains. Technical safeguards such as strong authentication, encrypted communications, and runtime anomaly detection are important. Organizational practices such as interface inventories, access control policies, and ongoing audits are equally important. Adopting a zero-trust architecture, where no interface is trusted by default, further reduces exposure by limiting access to only what is strictly necessary.\n\nFor designers of ML-powered systems, securing interfaces must be a first-class concern alongside algorithmic and data-centric design. Whether the system operates in the cloud, on the edge, or in embedded environments, failure to secure these access points risks undermining the entire system's trustworthiness.\n\n### Counterfeit Hardware {#sec-security-privacy-counterfeit-hardware-36fd}\n\nBeyond vulnerabilities in legitimate hardware, another significant threat emerges from the supply chain itself. Machine learning systems depend on the reliability and security of the hardware on which they run. Yet, in today's globalized hardware ecosystem, the risk of counterfeit or cloned hardware has emerged as a serious threat to system integrity. Counterfeit components refer to unauthorized reproductions of genuine parts, designed to closely imitate their appearance and functionality. These components can enter machine learning systems through complex procurement and manufacturing processes that span multiple vendors and regions.\n\nA single lapse in component sourcing can introduce counterfeit hardware into important systems. For example, a facial recognition system deployed for secure facility access might unknowingly rely on counterfeit processors. These unauthorized components could fail to process biometric data correctly or introduce hidden vulnerabilities that allow attackers to bypass authentication controls.\n\nThe risks posed by counterfeit hardware are multifaceted. From a reliability perspective, such components often degrade faster, perform unpredictably, or fail under load due to substandard manufacturing. From a security perspective, counterfeit hardware may include hidden backdoors or malicious circuitry, providing attackers with undetectable pathways to compromise machine learning systems. A cloned network router installed in a data center, for instance, could silently intercept model predictions or user data, creating systemic vulnerabilities across the entire infrastructure.\n\nLegal and regulatory risks further compound the problem. Organizations that unknowingly integrate counterfeit components into their ML systems may face serious legal consequences, including penalties for violating safety, privacy, or cybersecurity regulations[^fn-cybersecurity-regulations]. This is particularly concerning in sectors such as healthcare and finance, where compliance with industry standards is non-negotiable. Healthcare organizations must demonstrate HIPAA compliance throughout their technology stack, while organizations handling EU citizens' data must meet GDPR's requirements for technical and organizational measures, including supply chain integrity.\n\n[^fn-cybersecurity-regulations]: **Cybersecurity Regulations**: Global cybersecurity compliance costs exceed $150 billion annually, with frameworks like SOC 2, ISO 27001, PCI DSS, and sector-specific rules governing ML systems. Financial services face additional requirements under regulations like SOX, while healthcare must comply with HIPAA, creating complex multi-regulatory environments.\n\nEconomic pressures often incentivize sourcing from lower-cost suppliers without rigorous verification, increasing the likelihood of counterfeit parts entering production systems. Detection is especially challenging, as counterfeit components are designed to mimic legitimate ones. Identifying them may require specialized equipment or forensic analysis, making prevention far more practical than remediation.\n\nThe stakes are particularly high in machine learning applications that require high reliability and low latency, such as real-time decision-making in autonomous vehicles, industrial automation, or important healthcare diagnostics. Hardware failure in these contexts can lead not only to system downtime but also to significant safety risks. Consequently, as machine learning continues to expand into safety-important and high-value applications, counterfeit hardware presents a growing risk that must be recognized and addressed. Organizations must treat hardware trustworthiness as a core design requirement, on par with algorithmic accuracy and data security, to ensure that ML systems can operate reliably and securely in the real world.\n\n### Supply Chain Risks {#sec-security-privacy-supply-chain-risks-c99c}\n\nCounterfeit hardware exemplifies a broader systemic challenge. While counterfeit hardware presents a serious challenge, it is only one part of the larger problem of securing the global hardware supply chain. Machine learning systems are built from components that pass through complex supply networks involving design, fabrication, assembly, distribution, and integration. Each of these stages presents opportunities for tampering, substitution, or counterfeiting—often without the knowledge of those deploying the final system.\n\nMalicious actors can exploit these vulnerabilities in various ways. A contracted manufacturer might unknowingly receive recycled electronic waste that has been relabeled as new components. A distributor might deliberately mix cloned parts into otherwise legitimate shipments. Insiders at manufacturing facilities might embed hardware Trojans that are nearly impossible to detect once the system is deployed. Advanced counterfeits can be particularly deceptive, with refurbished or repackaged components designed to pass visual inspection while concealing inferior or malicious internals.\n\nIdentifying such compromises typically requires sophisticated analysis, including micrography, X-ray screening, and functional testing. However, these methods are costly and impractical for large-scale procurement. As a result, many organizations deploy systems without fully verifying the authenticity and security of every component.\n\nThe risks extend beyond individual devices. Machine learning systems often rely on heterogeneous hardware platforms, integrating CPUs, GPUs, memory, and specialized accelerators sourced from a global supply base. Any compromise in one part of this chain can undermine the security of the entire system. These risks are further amplified when systems operate in shared or multi-tenant environments, such as cloud data centers or federated edge networks, where hardware-level isolation is important to preventing cross-tenant attacks.\n\nThe 2018 Bloomberg Businessweek report alleging that Chinese state actors inserted spy chips into Supermicro server motherboards brought these risks to mainstream attention. While the claims remain disputed, the story underscored the industry's limited visibility into its own hardware supply chains. Companies often rely on complex, opaque manufacturing and distribution networks, leaving them vulnerable to hidden compromises. Over-reliance on single manufacturers or regions, including the semiconductor industry's reliance on TSMC, further concentrates this risk. This recognition has driven policy responses like the U.S. [CHIPS and Science Act](https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2024/08/09/fact-sheet-two-years-after-the-chips-and-science-act-biden-%E2%81%A0harris-administration-celebrates-historic-achievements-in-bringing-semiconductor-supply-chains-home-creating-jobs-supporting-inn/), which aims to bring semiconductor production onshore and strengthen supply chain resilience.\n\nSecuring machine learning systems requires moving beyond trust-by-default models toward zero-trust supply chain practices. This includes screening suppliers, validating component provenance, implementing tamper-evident protections, and continuously monitoring system behavior for signs of compromise. Building fault-tolerant architectures that detect and contain failures provides an additional layer of defense.\n\nUltimately, supply chain risks must be treated as a first-class concern in ML system design. Trust in the computational models and data pipelines that power machine learning depends corely on the trustworthiness of the hardware on which they run. Without securing the hardware foundation, even the most sophisticated models remain vulnerable to compromise.\n\n### Case Study: Supermicro Controversy {#sec-security-privacy-case-study-supermicro-controversy-72b7}\n\nThe abstract nature of supply chain risks became concrete in a high-profile controversy that captured industry attention. In 2018, Bloomberg Businessweek published a widely discussed report alleging that Chinese state-sponsored actors had secretly implanted tiny surveillance chips on server motherboards manufactured by Supermicro [@TheBigHa77]. These compromised servers were reportedly deployed by more than 30 major companies, including Apple and Amazon. The chips, described as no larger than a grain of rice, were said to provide attackers with backdoor access to sensitive data and systems.\n\nThe allegations sparked immediate concern across the technology industry, raising questions about the security of global supply chains and the potential for state-level hardware manipulation. However, the companies named in the report publicly denied the claims. Apple, Amazon, and Supermicro stated that they had found no evidence of the alleged implants after conducting thorough internal investigations. Industry experts and government agencies also expressed skepticism, noting the lack of verifiable technical evidence presented in the report.\n\nDespite these denials, the story had a lasting impact on how organizations and policymakers view hardware supply chain security. Whether or not the specific claims were accurate, the report highlighted the real and growing concern that hardware supply chains are difficult to fully audit and secure. It underscored how geopolitical tensions, manufacturing outsourcing, and the complexity of modern hardware ecosystems make it increasingly challenging to guarantee the integrity of hardware components.\n\nThe Supermicro case illustrates a broader truth: once a product enters a complex global supply chain, it becomes difficult to ensure that every component is free from tampering or unauthorized modification. This risk is particularly acute for machine learning systems, which depend on a wide range of hardware accelerators, memory modules, and processing units sourced from multiple vendors across the globe.\n\nIn response to these risks, both industry and government stakeholders have begun to invest in supply chain security initiatives. The U.S. government's CHIPS and Science Act is one such effort, aiming to bring semiconductor manufacturing back onshore to improve transparency and reduce dependency on foreign suppliers. While these efforts are valuable, they do not fully eliminate supply chain risks. They must be complemented by technical safeguards, such as component validation, runtime monitoring, and fault-tolerant system design.\n\nThe Supermicro controversy serves as a cautionary tale for the machine learning community. It demonstrates that hardware security cannot be taken for granted, even when working with reputable suppliers. Ensuring the integrity of ML systems requires rigorous attention to the entire hardware lifecycle—from design and fabrication to deployment and maintenance. This case reinforces the need for organizations to adopt comprehensive supply chain security practices as a foundational element of trustworthy ML system design.\n\n## When ML Systems Become Attack Tools {#sec-security-privacy-ml-systems-become-attack-tools-2f34}\n\nThe threats examined thus far—model theft, data poisoning, adversarial attacks, hardware vulnerabilities—represent attacks targeting machine learning systems. However, a complete threat model must also account for the inverse: machine learning as an attack amplifier. The same capabilities that make ML powerful for beneficial applications also enhance adversarial operations, transforming machine learning from passive target to active weapon.\n\nWhile machine learning systems are often treated as assets to protect, they may also serve as tools for launching attacks. In adversarial settings, the same models used to enhance productivity, automate perception, or assist decision-making can be repurposed to execute or amplify offensive operations. This dual-use characteristic of machine learning, its capacity to secure systems as well as to subvert them, marks a core shift in how ML must be considered within system-level threat models.\n\nAn offensive use of machine learning refers to any scenario in which a machine learning model is employed to facilitate the compromise of another system. In such cases, the model itself is not the object under attack, but the mechanism through which an adversary advances their objectives. These applications may involve reconnaissance, inference, subversion, impersonation, or the automation of exploit strategies that would otherwise require manual execution.\n\nImportantly, such offensive applications are not speculative. Attackers are already integrating machine learning into their toolchains across a wide range of activities, from spam filtering evasion to model-driven malware generation. What distinguishes these scenarios is the deliberate use of learning-based systems to extract, manipulate, or generate information in ways that undermine the confidentiality, integrity, or availability of targeted components.\n\nTo clarify the diversity and structure of these applications, @tbl-offensive-ml-use-cases summarizes several representative use cases. For each, the table identifies the type of machine learning model typically employed, the underlying system vulnerability it exploits, and the primary advantage conferred by the use of machine learning.\n\nThese documented cases illustrate how machine learning models can serve as amplifiers of adversarial capability. For example, language models allow more convincing and adaptable phishing attacks, while clustering and classification algorithms facilitate reconnaissance by learning system-level behavioral patterns. The generative AI capabilities of large language models particularly amplify these offensive applications. Similarly, adversarial example generators and inference models systematically uncover weaknesses in decision boundaries or data privacy protections, often requiring only limited external access to deployed systems. In hardware contexts, as discussed in the next section, deep neural networks trained on side-channel data can automate the extraction of cryptographic secrets from physical measurements—transforming an expert-driven process into a learnable pattern recognition task. Deep learning foundations, including convolutional neural networks for spatial pattern recognition, recurrent architectures for temporal dependencies, and gradient-based optimization, enable attackers to apply these techniques across various hardware platforms, from GPUs and TPUs in cloud environments to edge accelerators with constrained resources.\n\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Offensive Use Case**                  | **ML Model Type**                               | **Targeted System Vulnerability**                | **Advantage of ML**                                  |\n+:========================================+:================================================+:=================================================+:=====================================================+\n| **Phishing and Social Engineering**     | Large Language Models (LLMs)                    | Human perception and communication systems       | Personalized, context-aware message crafting         |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Reconnaissance and Fingerprinting**   | Supervised classifiers, clustering models       | System configuration, network behavior           | Scalable, automated profiling of system behavior     |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Exploit Generation**                  | Code generation models, fine-tuned transformers | Software bugs, insecure code patterns            | Automated discovery of candidate exploits            |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Data Extraction (Inference Attacks)** | Classification models, inversion models         | Privacy leakage through model outputs            | Inference with limited or black-box access           |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Evasion of Detection Systems**        | Adversarial input generators                    | Detection boundaries in deployed ML systems      | Crafting minimally perturbed inputs to evade filters |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Hardware-Level Attacks**              | Deep learning models                            | Physical side-channels (e.g., power, timing, EM) | Learning leakage patterns directly from raw signals  |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n\n: **Offensive ML Use Cases**: This table categorizes how machine learning amplifies cyberattacks by enabling automated content generation, exploiting system vulnerabilities, and increasing attack sophistication; it details the typical ML model, targeted weakness, and resulting advantage for each offensive application. Understanding these use cases is important for developing effective defenses against increasingly intelligent threats. {#tbl-offensive-ml-use-cases}\n\nAlthough these applications differ in technical implementation, they share a common foundation: the adversary replaces a static exploit with a learned model capable of approximating or adapting to the target's vulnerable behavior. This shift increases flexibility, reduces manual overhead, and improves robustness in the face of evolving or partially obscured defenses.\n\nWhat makes this class of threats particularly significant is their favorable scaling behavior. Just as accuracy in computer vision or language modeling improves with additional data, larger architectures, and greater compute resources, so too does the performance of attack-oriented machine learning models. A model trained on larger corpora of phishing attempts or power traces, for instance, may generalize more effectively, evade more detectors, or require fewer inputs to succeed. The same ecosystem that drives innovation in beneficial AI, including public datasets, open-source tooling, and scalable infrastructure, also lowers the barrier to developing effective offensive models.\n\nThis dynamic creates an asymmetry between attacker and defender. While defensive measures are bounded by deployment constraints, latency budgets, and regulatory requirements, attackers can scale training pipelines with minimal marginal cost. The widespread availability of pretrained models and public ML platforms further reduces the expertise required to develop high-impact attacks.\n\nExamining these offensive capabilities serves a crucial defensive purpose. Security professionals have long recognized that effective defense requires understanding attack methodologies—this principle underlies penetration testing[^fn-penetration-testing], red team exercises[^fn-red-team-exercises], and threat modeling throughout the cybersecurity industry.\n\n[^fn-penetration-testing]: **Penetration Testing**: Authorized simulated cyberattacks to evaluate system security, formalized in the 1960s for military computer systems. The global penetration testing market reached $1.7 billion in 2022, with 89% of organizations conducting annual pen tests to identify vulnerabilities before attackers do.\n\n[^fn-red-team-exercises]: **Red Team Exercises**: Adversarial security simulations where specialized teams emulate real attackers to test organizational defenses, originated from military war games in the 1960s. Unlike penetration testing, red teams use social engineering, physical access, and advanced persistent threat techniques, with exercises lasting weeks or months to simulate sophisticated nation-state attacks. The phrase \"know your enemy\" reflects this core security principle.\n\nIn the machine learning domain, this understanding becomes essential because ML amplifies both defensive and offensive capabilities. The same computational advantages that make ML powerful for legitimate applications—pattern recognition, automation, and scalability—also enhance adversarial capabilities. By examining how machine learning can be weaponized, security professionals can anticipate attack vectors, design more robust defenses, and develop detection mechanisms.\n\nAs a result, any comprehensive treatment of machine learning system security must consider not only the vulnerabilities of ML systems themselves but also the ways in which machine learning can be used to compromise other components—whether software, data, or hardware. Understanding the offensive potential of machine-learned systems is essential for designing resilient, trustworthy, and forward-looking defenses.\n\n### Case Study: Deep Learning for SCA {#sec-security-privacy-case-study-deep-learning-sca-b0b3}\n\nTo illustrate these offensive capabilities concretely, we examine a specific case where machine learning transforms traditional attack methodologies. One of the most well-known and reproducible demonstrations of deep-learning-assisted SCA is the SCAAML framework (Side-Channel Attacks Assisted with Machine Learning) [@scaaml_2019]. Developed by researchers at Google, SCAAML provides a practical implementation of the attack pipeline described above.\n\n::: {#fig-side-channel-curves fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}]\n \\definecolor{myblue}{RGB}{31,119,180}\n\\definecolor{myorange}{RGB}{255,127,14}\n\\definecolor{mygreen}{RGB}{44,160,44}\n\\definecolor{myred}{RGB}{214,39,40}\n\\definecolor{mypurple}{RGB}{148,103,189}\n\\definecolor{mybrown}{RGB}{140,86,75}\n\n\\pgfplotsset{myaxis/.style={\nclip=false,\n  axis line style={draw=none},\n  yticklabels={},\n  xticklabels={},\n  domain=-3.6:5,\n  samples=100,\n  smooth,\n  grid=none,\n  width=10cm,\n  height=7cm,\n  major tick  style={draw=none},\n  cycle list={\n    {myblue,line width=1.75pt},\n    {mygreen,line width=1.75pt},\n    {mypurple,line width=1.75pt},\n  }\n  }\n}\n%left graph\n\\begin{scope}[local bounding box=G1,shift={(0,0)}]\n\\begin{axis}[myaxis]\n\\addplot+[] {6.5*exp(-x^2/2)};\n\\addplot+[] {6.75*exp(-x^2/2)}node[pos=0.46](S){};\n\\addplot+[] {7*exp(-x^2/2)};\n\\node[thick,draw=BrownLine,rectangle,minimum size=14mm](B1)at(S){};\n\\end{axis}\n\\end{scope}\n%right graph\n\\begin{scope}[local bounding box=G2,shift={(11,1)},scale=1.5]\n\\begin{axis}[myaxis,  width=5cm,  height=5cm]\n\\addplot+[domain=-0.75:0.75,line width=1.25pt] {6.5*exp(-x^2/2)}\nnode[pos=0.66,outer sep=0pt,inner sep=0pt](T1){};\n\\addplot+[domain=-0.796:0.796,line width=1.25pt] {6.75*exp(-x^2/2)}\nnode[pos=0.63,outer sep=0pt,inner sep=0pt](T2){};\n\\addplot+[domain=-0.84:0.84,line width=1.25pt] {7*exp(-x^2/2)}\nnode[pos=0.6,outer sep=0pt,inner sep=0pt](T3){};\n%\n\\draw[myblue,-latex](T1)--++(0:9.5mm)node[right]{0000};\n\\draw[mygreen,-latex](T2)--++(0:9.9mm)node[right]{1111};\n\\draw[mypurple,-latex](T3)--++(0:10.4 mm)node[right]{0101};\n\\end{axis}\n\\node[thick,draw=BrownLine,rectangle,\nminimum height=48mm,minimum width=67mm] (B2) at (rel axis cs:0.7,0.6) {};\n\\end{scope}\n\n\\draw[BrownLine](B1.north west)--(B2.north west);\n\\draw[BrownLine](B1.south west)--(B2.south west);\n\\draw[BrownLine,dashed](B1.north east)--(B2.north east);\n%\\draw[BrownLine,dashed](B1.south east)--(B2.south east);\n%Determine the point on the line B2.north west to B2.south west\n\\path[name path=line1] (B1.south east) -- (B2.south east);\n\\path[name path=edgeB2left] (B2.north west) -- (B2.south west);\n\\path[name intersections={of=line1 and edgeB2left, by=I}];\n\\draw[BrownLine,dashed](B1.south east)--(I);\n\\end{tikzpicture}\n```\n**Power Traces**: Cryptographic computations reveal subtle, data-dependent variations in power consumption that reflect internal states during specific operations.\n:::\n\nAs shown in @fig-side-channel-curves, cryptographic computations exhibit data-dependent variations in their power consumption. These variations, while subtle, are measurable and reflect the internal state of the algorithm at specific points in time.\n\nIn traditional side-channel attacks, experts rely on statistical techniques to extract these differences. However, a neural network can learn to associate the shape of these signals with the specific data values being processed, effectively learning to decode the signal in a manner that mimics expert-crafted models, yet with enhanced flexibility and generalization. The model is trained on labeled examples of power traces and their corresponding intermediate values (e.g., output of an S-box operation). Over time, it learns to associate patterns in the trace, similar to those depicted in @fig-side-channel-curves, with secret-dependent computational behavior. This transforms the key recovery task into a classification problem, where the goal is to infer the correct key byte based on trace shape alone.\n\nIn their study, @scaaml_2019 trained a convolutional neural network to extract AES keys from power traces collected on an STM32F415 microcontroller running the open-source TinyAES implementation. The model was trained to predict intermediate values of the AES algorithm, such as the output of the S-box in the first round, directly from raw power traces. The trained model recovered the full 128-bit key using only a small number of traces per byte.\n\nThe traces were collected using a ChipWhisperer setup with a custom STM32F target board, shown in @fig-stm32f-board. This board executes AES operations while allowing external equipment to monitor power consumption with high temporal precision. The experimental setup captures how even inexpensive, low-power embedded devices can leak information through side channels—information that modern machine learning models can learn to exploit.\n\n![**STM32F415 Target Board**: Enables monitoring of power consumption during AES operations on the microcontroller, highlighting side-channel vulnerabilities that can be exploited by machine learning models. Source: @scaaml_2019.](images/png/stm32f_board.png){#fig-stm32f-board}\n\nSubsequent work expanded on this approach by introducing long-range models capable of leveraging broader temporal dependencies in the traces, improving performance even under noise and desynchronization [@bursztein2023generic]. These developments highlight the potential for machine learning models to serve as offensive cryptanalysis tools, especially in the analysis of secure hardware.\n\nThe implications extend beyond academic interest. As deep learning models continue to scale, their application to side-channel contexts is likely to lower the cost, skill threshold, and trace requirements of hardware-level attacks—posing a growing challenge for the secure deployment of embedded machine learning systems, cryptographic modules, and trusted execution environments.\n\n## Comprehensive Defense Architectures {#sec-security-privacy-comprehensive-defense-architectures-48ab}\n\nHaving examined threats against ML systems and threats enabled by ML capabilities, we now turn to comprehensive defensive strategies. Designing secure and privacy-preserving machine learning systems requires more than identifying individual threats. It demands a layered defense strategy that integrates protections across multiple system levels to create comprehensive resilience.\n\nThis section progresses systematically through four layers of defense: Data Layer protections including differential privacy and secure computation that safeguard sensitive information during training; Model Layer defenses such as adversarial training and secure deployment that protect the models themselves; Runtime Layer measures including input validation and output monitoring that secure inference operations; and Hardware Layer foundations such as trusted execution environments that provide the trust anchor for all other protections. We conclude with practical frameworks for selecting and implementing these defenses based on your deployment context.\n\n### The Layered Defense Principle {#sec-security-privacy-layered-defense-principle-8706}\n\nLayered defense (also known as defense-in-depth) represents a core security architecture principle where multiple independent defensive mechanisms work together to protect against diverse threat vectors. In machine learning systems, this approach becomes essential due to the unique attack surfaces introduced by data dependencies, model exposures, and inference patterns. Unlike traditional software systems that primarily face code-based vulnerabilities, ML systems are vulnerable to input manipulation, data leakage, model extraction, and runtime abuse, all amplified by tight coupling between data, model behavior, and infrastructure.\n\nThe layered approach recognizes that no single defensive mechanism can address all possible threats. Instead, security emerges from the interaction of complementary protections: data-layer techniques like differential privacy and federated learning; model-layer defenses including robustness techniques and secure deployment; runtime-layer measures such as input validation and output monitoring; and hardware-layer solutions including trusted execution environments and secure boot. Each layer contributes to the system's overall resilience while compensating for potential weaknesses in other layers.\n\nThis section presents a structured framework implementing layered defense for ML systems, progressing from data-centric protections to infrastructure-level enforcement. The framework builds upon data protection practices, including encryption, access control, and lineage tracking, and connects forward to operational security measures for production deployment. By integrating safeguards across layers, organizations can build ML systems that not only perform reliably but also withstand adversarial pressure in production environments.\n\nThe layered approach is visualized in @fig-defense-stack, which shows how defensive mechanisms progress from foundational hardware-based security to runtime system protections, model-level controls, and privacy-preserving techniques at the data level. Each layer builds on the trust guarantees of the layer below it, forming an end-to-end strategy for deploying ML systems securely.\n\n::: {#fig-defense-stack fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\n\\tikzset{%\nLine/.style={line width=1.0pt,black!50},\nBox/.style={inner xsep=4pt,inner ysep=6pt,\n    node distance=0.7,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=29mm,\n    minimum width=29mm, minimum height=11mm\n  },\nBox2/.style={Box,  node distance=0.7,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  node distance=0.7, draw=VioletLine,fill=VioletL2},\nBox4/.style={Box,node distance=0.7,  draw=BlueLine,fill=BlueL}\n}\n\n\\node[Box](B1){Trusted Execution Environments};\n\\node[Box,right=of B1](B2){Secure Boot};\n\\node[Box,right=4.25of B2](B3){Hardware Security Modules};\n\\node[Box,right=of B3](B4){Physical Unclonable Functions};\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=6mm,inner ysep=6mm,\nyshift=2.5mm,fill=BackColor!60,fit=(B1)(B4),line width=0.75pt](BB1){};\n\\node[below=6pt of  BB1.north,inner sep=0pt,\nanchor=north]{\\textbf{Hardware-Level Security}};\n%\n\\node[Box3,above=2.0of B1](2B1){System Integrity Checks};\n\\node[Box3,right=of 2B1](2B2){Runtime Input Validation};\n\\node[Box3,right=of 2B2](2B3){Runtime Output Monitoring};\n\\node[Box3,right=of 2B3](2B4){Incident Response \\& Recovery};\n\\scoped[on background layer]\n\\node[draw=BlueD,inner xsep=6mm,inner ysep=6mm,\nyshift=2.5mm,fill=cyan!5,fit=(2B1)(2B4),line width=0.75pt](BB2){};\n\\node[below=6pt of  BB2.north,inner sep=0pt,\nanchor=north]{\\textbf{System-Level Security}};\n%\n\\node[Box2,above=2.0 of 2B1](3B1){Model Encryption \\& Serialization};\n\\node[Box2,right=2.5 of 3B1](3B2){Secure Model Design};\n\\path[](3B2)-|coordinate(S)(B4);\n\\node[Box2](3B3)at(S){Secure Deployment \\& Access};\n\\scoped[on background layer]\n\\node[draw=GreenD,inner xsep=6mm,inner ysep=6mm,\nyshift=2.5mm,fill=green!5,fit=(3B1)(3B3),line width=0.75pt](BB3){};\n\\node[below=6pt of  BB3.north,inner sep=0pt, xshift=4mm,\nanchor=north]{\\textbf{Model-Level Security}};\n%\n\\node[Box4,above left=2 and 0.3of 3B2](4B1){Differential Privacy};\n\\node[Box4,above right=1.9 and 0.3of 3B2](4B2){Federated Learning};\n\\node[Box4,above=of 4B1](5B1){Homomorphic Encryption};\n\\node[Box4,above=of 4B2](5B2){Synthetic Data Generation};\n%\n\\scoped[on background layer]\n\\node[draw=RedLine,inner xsep=6mm,inner ysep=6mm,\nyshift=2.3mm,fill=magenta!4,fit=(4B1)(5B2),line width=0.75pt](BB4){};\n\\node[below=6pt of  BB4.north,inner sep=0pt,\nanchor=north]{\\textbf{Data Privacy \\& Governance}};\n%\n\\draw[Line,-latex](B1)--(2B1);\n\\draw[Line,-latex](B2)--++(0,1.8)-|(2B1.310);\n\\draw[Line,-latex](B3)--++(0,1.8)-|(3B3.230);\n\\draw[Line,-latex](B4)--(3B3);\n%\n\\draw[Line,-latex](2B1)--(3B1);\n\\draw[Line,-latex](2B2.120)|-(3B2);\n\\draw[Line,-latex](2B3.80)|-(3B2);\n%\n\\draw[Line,-latex](3B2.120)--++(0,1.2)-|(4B1);\n\\draw[Line,-latex](3B2.70)--++(0,1.2)-|(4B2);\n\\draw[Line,-latex](4B1)--(5B1);\n\\draw[Line,-latex](4B2)--(5B2);\n\\end{tikzpicture}\n```\n**Layered Defense Stack**: Machine learning systems require multi-faceted security strategies that progress from foundational hardware protections to data-centric privacy techniques, building trust across all layers. This architecture integrates safeguards at the data, model, runtime, and infrastructure levels to mitigate threats and ensure robust deployment in production environments.\n:::\n\n### Privacy-Preserving Data Techniques {#sec-security-privacy-privacypreserving-data-techniques-64f8}\n\nAt the highest level of our defense stack, we begin with data privacy techniques. Protecting the privacy of individuals whose data fuels machine learning systems is a foundational requirement for trustworthy AI. Unlike traditional systems where data is often masked or anonymized before processing, ML workflows typically rely on access to raw, high-fidelity data to train effective models. This tension between utility and privacy has motivated a diverse set of techniques aimed at minimizing data exposure while preserving learning performance.\n\n#### Differential Privacy {#sec-security-privacy-differential-privacy-8c2b}\n\nOne of the most widely adopted frameworks for formalizing privacy guarantees is differential privacy (DP). DP provides a rigorous mathematical definition of privacy loss, ensuring that the inclusion or exclusion of a single individual's data has a provably limited effect on the model's output.\n\nTo understand the need for differential privacy, consider this challenge: how can we quantify privacy loss when learning from data? Traditional privacy approaches focus on removing identifying information (names, addresses, social security numbers) or applying statistical disclosure controls. However, these methods fail against sophisticated adversaries who can re-identify individuals through auxiliary data, statistical correlation attacks, or inference from model outputs.\n\nDifferential privacy takes a different approach by focusing on algorithmic behavior rather than data content. The key insight is that privacy protection should be measurable and should limit what can be learned about any individual, regardless of what external information an adversary possesses.\n\nTo build intuition for this concept, imagine you want to find the average salary of a group of people, but no one wants to reveal their actual salary. With differential privacy, you could ask everyone to write their salary on a piece of paper, but before they hand it in, they add or subtract a random number from a known distribution. When you average all the papers, the random noise tends to cancel out, giving you a very close estimate of the true average. However, if you pull out any single piece of paper, you cannot know the person's real salary because you do not know what random number they added. This is the core idea: learn aggregate patterns while making it impossible to be sure about any single individual.\n\nDifferential privacy formalizes this intuition through a comparison of algorithm behavior on similar datasets. Consider two adjacent datasets that differ only in the presence or absence of a single individual's record. Differential privacy ensures that the probability distributions of algorithm outputs remain statistically similar regardless of whether that individual's data is included. This protection is achieved through carefully calibrated noise that masks individual contributions while preserving the aggregate statistical patterns necessary for machine learning.\n\nTo make this intuition mathematically precise, differential privacy introduces a quantitative measure of privacy loss. The mathematical framework uses probability ratios to bound how much an algorithm's behavior can change when a single individual's data is added or removed. This approach allows us to prove privacy guarantees rather than simply assume them.\n\nA randomized algorithm $\\mathcal{A}$ is said to be $\\epsilon$-differentially private if, for all adjacent datasets $D$ and $D'$ differing in one record, and for all outputs $S \\subseteq \\text{Range}(\\mathcal{A})$, the following holds:\n$$\n\\Pr[\\mathcal{A}(D) \\in S] \\leq e^{\\epsilon} \\Pr[\\mathcal{A}(D') \\in S]\n$$\n\nThe parameter $\\epsilon$ quantifies the privacy budget, representing the maximum allowable privacy loss. Smaller values of $\\epsilon$ provide stronger privacy guarantees through increased noise injection, but may reduce model utility. Typical values include $\\epsilon = 0.1$ for strong privacy protection, $\\epsilon = 1.0$ for moderate protection, and $\\epsilon = 10$ for weaker but utility-preserving guarantees. The multiplicative factor $e^{\\epsilon}$ bounds the likelihood ratio between algorithm outputs on adjacent datasets, constraining how much an individual's participation can influence any particular result.\n\nThis bound ensures that the algorithm's behavior remains statistically indistinguishable regardless of whether any individual's data is present, thereby limiting the information that can be inferred about that individual. In practice, DP is implemented by adding calibrated noise to model updates or query responses, using mechanisms such as the Laplace or Gaussian mechanism. Training techniques like differentially private stochastic gradient descent[^fn-dp-sgd-adoption] integrate calibrated noise into training computations, ensuring that individual data points cannot be distinguished from the model's learned behavior.\n\n[^fn-dp-sgd-adoption]: **DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at scale in 2016, protecting 1+ billion users' data in iOS. Their implementation adds noise to emoji usage, Safari crashes, and QuickType suggestions, balancing privacy (ε=4-16) with utility for improving user experience across their ecosystem.\n\nWhile differential privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility[^fn-privacy-utility-tension] that has measurable computational and accuracy costs.\n\n[^fn-privacy-utility-tension]: **Privacy-Utility Tension**: This core tradeoff was formalized by Dwork and McSherry, who proved that perfect privacy (infinite noise) yields no utility, while perfect utility (no noise) provides no privacy. The \"privacy budget\" concept emerged from this insight—you can only spend privacy once, making every query a strategic decision.\n\nPractical DP deployment requires careful consideration of computational trade-offs, privacy budget management, and implementation challenges, as detailed in @tbl-privacy-technique-comparison.\n\nIncreasing the noise to reduce $\\epsilon$ may degrade model accuracy, especially in low-data regimes or fine-grained classification tasks. Consequently, DP is often applied selectively—either during training on sensitive datasets or at inference when returning aggregate statistics—to balance privacy with performance goals [@dwork2014algorithmic].\n\n#### Federated Learning {#sec-security-privacy-federated-learning-3834}\n\nWhile differential privacy adds mathematical guarantees to data processing, federated learning (FL) offers a complementary approach that reduces privacy risks by restructuring the learning process itself. This technique directly addresses the privacy challenges of on-device learning explored in @sec-edge-intelligence, where models must adapt to local data patterns without exposing sensitive user information. Rather than aggregating raw data at a central location, FL distributes the training across a set of client devices, each holding local data [@mcmahan2017communicationefficient]. This distributed training paradigm, which builds on the adaptive deployment concepts from on-device learning, requires careful coordination of security measures across multiple participants and infrastructure providers. Clients compute model updates locally and share only parameter deltas with a central server for aggregation:\n$$\n\\theta_{t+1} \\leftarrow \\sum_{k=1}^{K} \\frac{n_k}{n} \\cdot \\theta_{t}^{(k)}\n$$\n\nHere, $\\theta_{t}^{(k)}$ represents the model update from client $k$, $n_k$ the number of samples held by that client, and $n$ the total number of samples across all clients. This weighted aggregation allows the global model to learn from distributed data without direct access to it. FL reduces the exposure of raw data, but still leaks information through gradients, motivating the use of DP, secure aggregation, and hardware-based protections in federated settings.\n\n::: {.callout-note title=\"Real-World Example: Google Gboard Federated Learning\" icon=false}\nGoogle's Gboard keyboard uses federated learning to improve next-word prediction across 1+ billion Android devices without collecting typing data. The system works as follows:\n\n1. Local Training: Each device trains a small update to the language model using the user's recent typing (typically 100-1000 words)\n2. Secure Aggregation: Devices upload encrypted model updates (not raw text) to Google's servers\n3. Global Update: The server aggregates thousands of updates, computing an improved global model\n4. Distribution: The updated model is pushed back to devices in the next app update\n\n**Privacy Properties:** Individual typing data never leaves the device. Even Google's servers cannot decrypt individual updates, seeing only the aggregated result. The system combines FL with differential privacy $(\\varepsilon\\approx 6)$ and secure aggregation protocols.\n\n**Performance:** FL achieves 92% of the accuracy of centralized training while eliminating raw data collection. Communication efficiency optimizations (gradient compression, selective participation) reduce bandwidth to ~100 KB per device per day.\n\n**Trade-offs:** FL requires 10-100x more communication rounds than centralized training and introduces 2-5% accuracy degradation. However, for privacy-sensitive applications, these costs are acceptable compared to the alternative of not training at all.\n:::\n\nTo address scenarios requiring computation on encrypted data, homomorphic encryption (HE)[^fn-he-breakthrough] and secure multiparty computation (SMPC) allow models to perform inference or training over encrypted inputs. The computational overhead of homomorphic operations often requires efficiency optimization techniques, including model compression (quantization reduces precision requirements for encrypted operations), architectural optimization (depthwise separable convolutions minimize encrypted multiplications), and hardware acceleration (specialized cryptographic accelerators), to maintain practical performance.\n\n[^fn-he-breakthrough]: **Homomorphic Encryption Breakthrough**: Considered the \"holy grail\" of cryptography since the 1970s, fully homomorphic encryption remained theoretical until Craig Gentry's 2009 PhD thesis. His breakthrough was realizing that \"noisy\" ciphertexts could support unlimited operations if periodically \"refreshed,\" solving a decades-old puzzle that allows computation on encrypted data.\n\nIn the case of HE, operations on ciphertexts correspond to operations on plaintexts, enabling encrypted inference:\n$$\n\\text{Enc}(f(x)) = f(\\text{Enc}(x))\n$$\n\nThis property supports privacy-preserving computation in untrusted environments, such as cloud inference over sensitive health or financial records. The computational cost of HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC[^fn-smpc-overhead], by contrast, distributes the computation across multiple parties such that no single party learns the complete input or output. This is particularly useful in joint training across institutions with strict data-use policies, such as hospitals or banks[^fn-smpc-collaborative].\n\n[^fn-smpc-overhead]: **SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational overhead compared to plaintext operations. A simple neural network inference that takes milliseconds on GPU requires hours using SMPC, limiting practical applications to small models and offline scenarios.\n\n[^fn-smpc-collaborative]: **Secure Multi-Party Computation (SMPC)**: Cryptographic framework enabling multiple parties to jointly compute functions over their private inputs without revealing those inputs, first formalized in 1982 by Andrew Yao. Today's implementations allow hospitals to collaboratively train medical AI models without sharing patient records, achieving 99%+ accuracy while maintaining strict privacy compliance.\n\n#### Synthetic Data Generation {#sec-security-privacy-synthetic-data-generation-4349}\n\nBeyond cryptographic approaches like homomorphic encryption, a more pragmatic and increasingly popular alternative involves the use of synthetic data generation[^fn-synthetic-data]. This approach offers an intuitive solution to privacy protection: if we can create artificial data that looks statistically similar to real data, we can train models without ever exposing sensitive information.\n\nSynthetic data generation works by training a generative model (such as a GAN, VAE, or diffusion model) on the original sensitive dataset, then using this trained generator to produce new artificial samples. The key insight is that the generative model learns the underlying patterns and distributions in the data without memorizing specific individuals. When properly implemented, the synthetic data preserves statistical properties necessary for machine learning while removing personally identifiable information.\n\nThe generation typically follows three stages. First, distribution learning trains a generative model $G_\\theta$ on real data $D_{\\text{real}} = \\{x_1, x_2,\\ldots, x_n\\}$ to learn the data distribution $p(x)$. Second, synthetic sampling generates new samples $D_{\\text{synthetic}} = \\{G_\\theta(z_1), G_\\theta(z_2),\\ldots, G_\\theta(z_m)\\}$ by sampling from random noise $z_i \\sim \\mathcal{N}(0,I)$. Third, validation verifies that $D_{\\text{synthetic}}$ maintains statistical fidelity to $D_{\\text{real}}$ while avoiding memorization of specific records. By training generative models on real datasets and sampling new instances from the learned distribution, organizations can create datasets that approximate the statistical properties of the original data without retaining identifiable details [@goncalves2020generation].\n\nWhile appealing, synthetic data generation faces important limitations. Generative models can suffer from mode collapse, failing to capture rare but important patterns in the original data. More critically, sophisticated adversaries can potentially extract information about the original training data through generative model inversion attacks or membership inference. The privacy protection depends heavily on the generative model architecture, training procedure, and hyperparameter choices—making it difficult to provide formal privacy guarantees without additional mechanisms like differential privacy.\n\nConsider a practical example where a hospital wants to share patient data for ML research while protecting privacy. They train a generative adversarial network (GAN) on 10,000 real patient records containing demographics, lab results, and diagnoses. The GAN learns to generate synthetic patients with realistic combinations of features (e.g., diabetic patients typically have elevated glucose levels). The synthetic dataset of 50,000 artificial patients maintains clinical correlations necessary for training diagnostic models while containing no real patient information. However, the hospital also applies differential privacy during GAN training (ε = 1.0) to prevent the model from memorizing specific patients, trading a 5% reduction in statistical fidelity for formal privacy guarantees.\n\n[^fn-synthetic-data]: **Synthetic Data Growth**: The synthetic data market grew from $110 million in 2019 to $1.1 billion in 2023, driven by privacy regulations and data scarcity. Companies like Uber use synthetic trip data to protect user privacy while maintaining ML model performance, with some synthetic datasets achieving 95%+ statistical fidelity.\n\nTogether, these techniques reflect a shift from isolating data as the sole path to privacy toward embedding privacy-preserving mechanisms into the learning process itself. Each method offers distinct guarantees and trade-offs depending on the application context, threat model, and regulatory constraints. Effective system design often combines multiple approaches, such as applying differential privacy within a federated learning setup, or employing homomorphic encryption for important inference stages, to build ML systems that are both useful and respectful of user privacy.\n\n#### Comparative Properties {#sec-security-privacy-comparative-properties-9ca5}\n\nHaving examined individual techniques, it becomes clear that these privacy-preserving approaches differ not only in the guarantees they offer but also in their system-level implications. For practitioners, the choice of mechanism depends on factors such as computational constraints, deployment architecture, and regulatory requirements.\n\n@tbl-privacy-technique-comparison summarizes the comparative properties of these methods, focusing on privacy strength, runtime overhead, maturity, and common use cases. Understanding these trade-offs is important for designing privacy-aware machine learning systems that operate under real-world constraints.\n\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Technique**              | **Privacy Guarantee** | **Computational Overhead** | **Deployment Maturity** | **Typical Use Case**         | **Trade-offs**                                            |\n+:===========================+:======================+:===========================+:========================+:=============================+:==========================================================+\n| **Differential Privacy**   | Formal (ε-DP)         | Moderate to High           | Production              | Training with sensitive      | Reduced accuracy; careful tuning of ε/noise               |\n|                            |                       |                            |                         | or regulated data            | required to balance utility and protection                |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Federated Learning**     | Structural            | Moderate                   | Production              | Cross-device or cross-org    | Gradient leakage risk; requires secure aggregation        |\n|                            |                       |                            |                         | collaborative learning       | and orchestration infrastructure                          |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Homomorphic Encryption** | Strong (Encrypted)    | High                       | Experimental            | Inference in untrusted       | High latency and memory usage; suitable for limited-scope |\n|                            |                       |                            |                         | cloud environments           | inference on fixed-function models                        |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Secure MPC**             | Strong (Distributed)  | Very High                  | Experimental            | Joint training across        | Expensive communication; challenging to scale to many     |\n|                            |                       |                            |                         | mutually untrusted parties   | participants or deep models                               |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Synthetic Data**         | Weak (if standalone)  | Low to Moderate            | Emerging                | Data sharing, benchmarking   | May leak sensitive patterns if training process is not    |\n|                            |                       |                            |                         | without direct access to raw | differentially private or audited for fidelity            |\n|                            |                       |                            |                         | data                         |                                                           |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n\n: **Privacy-Accuracy Trade-Offs**: Data privacy techniques impose varying computational costs and offer different levels of formal privacy guarantees, requiring practitioners to balance privacy strength with model utility and deployment constraints. The table summarizes key properties—privacy guarantees, computational overhead, maturity, typical use cases, and trade-offs—to guide informed decisions when designing privacy-aware machine learning systems. {#tbl-privacy-technique-comparison}\n\n### Case Study: GPT-3 Data Extraction Attack {#sec-security-privacy-case-study-gpt3-data-extraction-attack-5126}\n\nIn 2020, researchers conducted a groundbreaking study demonstrating that large language models could leak sensitive training data through carefully crafted prompts [@carlini2021extracting]. The research team systematically queried OpenAI's GPT-3 model to extract verbatim content from its training dataset, revealing privacy vulnerabilities in large-scale language models.\n\nThe attack proved remarkably successful at extracting sensitive information directly from the model's outputs. By repeatedly querying the model with prompts like \"My name is\" followed by attempts to continue famous quotes or repeated phrases, researchers successfully extracted personal information including email addresses and phone numbers from the training data, verbatim passages from copyrighted books, private data that should have been filtered during training, and personally identifiable information from millions of individuals.\n\nThe technical approach exploited GPT-3's memorization of rare or repeated text sequences. The researchers used prompt engineering to craft inputs that triggered memorized sequences, continuation attacks that used partial quotes or names to extract full sensitive information, statistical analysis to identify patterns in model outputs indicating verbatim memorization, and verification methods that cross-referenced extracted data with known public sources to confirm accuracy. Out of 600,000 attempts, they successfully extracted over 16,000 unique instances of memorized training data.\n\nThis attack challenged assumptions about training data privacy. The results demonstrated that large language models can act as unintentional databases, storing and retrieving sensitive information from their training data. This violated privacy expectations that training data would be \"forgotten\" after model training, revealing that scale amplifies privacy risk as larger models (175B parameters) memorize more training data than smaller models.\n\nThe research revealed that common data protection measures proved insufficient. Even after data deduplication, models still memorized sensitive information, highlighting the tension between model utility and privacy protection. Techniques to prevent memorization such as differential privacy and aggressive data filtering reduce model quality, creating challenging trade-offs for practitioners.\n\nThe industry response was swift and comprehensive. Organizations began widespread adoption of differential privacy in large model training, enhanced data filtering and PII removal processes, development of membership inference defenses, new research into machine unlearning techniques, and regulatory discussions about training data rights and model transparency. Modern organizations now commonly implement differential privacy during training (ε ≤ 8), aggressive PII filtering using automated detection tools, regular auditing for data memorization using extraction attacks, and legal frameworks for handling training data containing personal information [@carlini2021extracting].\n\n### Secure Model Design {#sec-security-privacy-secure-model-design-69a6}\n\nMoving from data-level protections to model-level security, we address how security considerations shape the model development process. Security begins at the design phase of a machine learning system. While downstream mechanisms such as access control and encryption protect models once deployed, many vulnerabilities can be mitigated earlier—through architectural choices, defensive training strategies, and mechanisms that embed resilience directly into the model's structure or behavior. By considering security as a design constraint, system developers can reduce the model's exposure to attacks, limit its ability to leak sensitive information, and provide verifiable ownership protection.\n\nOne important design strategy is to build robust-by-construction models that reduce the risk of exploitation at inference time. For instance, models with confidence calibration or abstention mechanisms can be trained to avoid making predictions when input uncertainty is high. These techniques can help prevent overconfident misclassifications in response to adversarial or out-of-distribution inputs. Models may also employ output smoothing, regularizing the output distribution to reduce sharp decision boundaries that are especially susceptible to adversarial perturbations.\n\nCertain application contexts may also benefit from choosing simpler or compressed architectures. Limiting model capacity can reduce opportunities for memorization of sensitive training data and complicate efforts to reverse-engineer the model from output behavior. For embedded or on-device settings, smaller models are also easier to secure, as they typically require less memory and compute, lowering the likelihood of side-channel leakage or runtime manipulation.\n\nAnother design-stage consideration is the use of model watermarking[^fn-model-watermarking], a technique for embedding verifiable ownership signatures directly into the model's parameters or output behavior [@adi2018turning]. A watermark might be implemented, for example, as a hidden response pattern triggered by specific inputs, or as a parameter-space perturbation that does not affect accuracy but is statistically identifiable.\n\n[^fn-model-watermarking]: **Model Watermarking**: Technique for proving model ownership developed in 2017, analogous to digital image watermarks. Modern watermarking can embed signatures in less than 0.01% of model parameters while maintaining 99%+ accuracy, helping prove IP theft in courts where billions of dollars in AI assets are at stake.\n\nFor example, in a keyword spotting system deployed on embedded hardware for voice activation (e.g., \"Hey Alexa\" or \"OK Google\"), a secure design might use a lightweight convolutional neural network with confidence calibration to avoid false activations on uncertain audio. The model might also include an abstention threshold, below which it produces no activation at all. To protect intellectual property, a designer could embed a watermark by training the model to respond with a unique label only when presented with a specific, unused audio trigger known only to the developer. These design choices not only improve robustness and accountability, but also support future verification in case of IP disputes or performance failures in the field.\n\nIn high-risk applications, such as medical diagnosis, autonomous vehicles, or financial decision systems, designers may also prioritize interpretable model architectures, such as decision trees, rule-based classifiers, or sparsified networks, to enhance system auditability. These models are often easier to understand and explain, making it simpler to identify potential vulnerabilities or biases. Using interpretable models allows developers to provide clearer insights into how the system arrived at a particular decision, which is important for building trust with users and regulators.\n\nModel design choices often reflect trade-offs between accuracy, robustness, transparency, and system complexity. When viewed from a systems perspective, early-stage design decisions yield the highest value for long-term security. They shape what the model can learn, how it behaves under uncertainty, and what guarantees can be made about its provenance, interpretability, and resilience.\n\n### Secure Model Deployment {#sec-security-privacy-secure-model-deployment-e08c}\n\nWhile secure design establishes a foundation of robustness, protection extends beyond the model itself to how it is packaged and deployed. Protecting machine learning models from theft, abuse, and unauthorized manipulation requires security considerations throughout both the design and deployment phases. A model's vulnerability is not solely determined by its training procedure or architecture, but also by how it is serialized, packaged, deployed, and accessed during inference. As models are increasingly embedded into edge devices, served through public APIs, or integrated into multi-tenant platforms, robust security practices are important to ensure the integrity, confidentiality, and availability of model behavior.\n\nThis section addresses security mechanisms across three key stages: model design, secure packaging and serialization, and deployment and access control. These practices complement model optimization techniques such as quantization, pruning, and knowledge distillation, where performance improvements must not compromise security properties.\n\nFrom a design perspective, architectural choices can reduce a model's exposure to adversarial manipulation and unauthorized use. For example, models can incorporate confidence calibration or abstention mechanisms that allow them to reject uncertain or anomalous inputs rather than producing potentially misleading outputs. Designing models with simpler or compressed architectures can also reduce the risk of reverse engineering or information leakage through side-channel analysis. In some cases, model designers may embed imperceptible watermarks, which are unique signatures embedded in the parameters or behavior of the model, that can later be used to demonstrate ownership in cases of misappropriation [@uchida2017embedding]. These design-time protections are essential for commercially valuable models, where intellectual property rights are at stake.\n\nOnce training is complete, the model must be securely packaged for deployment. Storing models in plaintext formats, including unencrypted ONNX or PyTorch checkpoint files, can expose internal structures and parameters to attackers with access to the file system or memory. To mitigate this risk, models should be encrypted, obfuscated, or wrapped in secure containers. Decryption keys should be made available only at runtime and only within trusted environments. Additional mechanisms, such as quantization-aware encryption or integrity-checking wrappers, can prevent tampering and offline model theft.\n\nDeployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests.\n\n[^fn-oauth]: **OAuth Protocol**: Open Authorization standard developed in 2006, now used by 3+ billion users across Google, Facebook, and Microsoft services. OAuth 2.0 (2012) enables secure API access without exposing user credentials, processing trillions of authentication requests annually for ML API access.\n\n[^fn-mutual-tls]: **Mutual TLS (mTLS)**: Enhanced Transport Layer Security where both client and server authenticate each other using certificates, introduced in 1999. mTLS provides 99.9%+ secure communication but increases latency by 15-30ms, making it suitable for high-security ML API endpoints requiring end-to-end authentication.\n\n[^fn-api-keys]: **API Keys**: Simple authentication tokens first popularized by Google Maps API (2005), now ubiquitous in ML services. While convenient, API keys in URL parameters or headers can be logged or exposed, with studies showing 10-15% of GitHub repositories accidentally contain leaked API keys worth millions in compute credits.\n\n[^fn-rbac]: **Role-Based Access Control (RBAC)**: Access control model developed by NIST in the 1990s, now mandatory for government systems. RBAC reduces security administration overhead by 90%+ compared to individual permissions, with modern ML platforms supporting thousands of roles governing model access, data permissions, and compute resources.\n\nThis key authenticates the client and allows the backend to enforce usage policies, monitor for abuse, and log access patterns. Secure implementations retrieve API keys from environment variables rather than hardcoding them into source code, preventing credential exposure in version control systems or application logs. Such key-based access control mechanisms are simple to implement but require careful key management and monitoring to prevent misuse, unauthorized access, or model extraction. Additional security measures in production deployments typically include model integrity verification through SHA-256 hash checking, rate limiting to prevent abuse, input validation for size and format constraints, and comprehensive logging for security event tracking.\n\nThe secure deployment patterns established here integrate naturally with development workflows, ensuring security becomes part of standard engineering practice rather than an afterthought. Runtime monitoring (@sec-security-privacy-runtime-system-monitoring-a71c) extends these protections to operational environments.\n\n### Runtime System Monitoring {#sec-security-privacy-runtime-system-monitoring-a71c}\n\nWhile secure design and deployment establish strong foundations, protection must extend to runtime operations. Even with robust design and deployment safeguards, machine learning systems remain vulnerable to runtime threats. Attackers may craft inputs that bypass validation, exploit model behavior, or target system-level infrastructure.\n\nProduction ML systems face diverse deployment contexts—from cloud services to edge devices to embedded systems. Each environment presents unique monitoring challenges and opportunities. Defensive strategies must extend beyond static protection to include real-time monitoring, threat detection, and incident response. This section outlines operational defenses that maintain system trust under adversarial conditions.\n\nRuntime monitoring encompasses a range of techniques for observing system behavior, detecting anomalies, and triggering mitigation. These techniques can be grouped into three categories: input validation, output monitoring, and system integrity checks.\n\n#### Input Validation {#sec-security-privacy-input-validation-c96f}\n\nInput validation is the first line of defense at runtime. It ensures that incoming data conforms to expected formats, statistical properties, or semantic constraints before it is passed to a machine learning model. Without these safeguards, models are vulnerable to adversarial inputs, which are crafted examples designed to trigger incorrect predictions, or to malformed inputs that cause unexpected behavior in preprocessing or inference.\n\nMachine learning models, unlike traditional rule-based systems, often do not fail safely. Small, carefully chosen changes to input data can cause models to make high-confidence but incorrect predictions. Input validation helps detect and reject such inputs early in the pipeline [@goodfellow2015explaining].\n\nValidation techniques range from low-level checks (e.g., input size, type, and value ranges) to semantic filters (e.g., verifying whether an image contains a recognizable object or whether a voice recording includes speech). For example, a facial recognition system might validate that the uploaded image is within a certain resolution range (e.g., 224×224 to 1024×1024 pixels), contains RGB channels, and passes a lightweight face detection filter. This prevents inputs like blank images, text screenshots, or synthetic adversarial patterns from reaching the model. Similarly, a voice assistant might require that incoming audio files be between 1 and 5 seconds long, have a valid sampling rate (e.g., 16kHz), and contain detectable human speech using a speech activity detector (SAD)[^fn-speech-activity-detector]. This ensures that empty recordings, music clips, or noise bursts are filtered before model inference.\n\n[^fn-speech-activity-detector]: **Speech Activity Detector (SAD)**: Algorithm that distinguishes speech from silence, noise, or music in audio streams, essential for voice interfaces since the 1990s. Modern neural SADs achieve 95%+ accuracy and operate in <10ms latency, enabling real-time filtering before expensive speech recognition processing.\n\nIn generative systems such as DALL·E, Stable Diffusion, or Sora, input validation often involves prompt filtering. This includes scanning the user's text prompt for banned terms, brand names, profanity, or misleading medical claims. For example, a user prompt like \"Generate an image of a medication bottle labeled with Pfizer's logo\" might be rejected or rewritten due to trademark concerns. Filters may operate using keyword lists, regular expressions, or lightweight classifiers that assess prompt intent. These filters prevent the generative model from being used to produce harmful, illegal, or misleading content—even before sampling begins.\n\nIn some applications, distributional checks are also used. These assess whether the incoming data statistically resembles what the model saw during training. For instance, a computer vision pipeline might compare the color histogram of the input image to a baseline distribution, flagging outliers for manual review or rejection.\n\nThese validations can be lightweight (heuristics or threshold rules) or learned (small models trained to detect distribution shift or adversarial artifacts). In either case, input validation serves as a important pre-inference firewall—reducing exposure to adversarial behavior, improving system stability, and increasing trust in downstream model decisions.\n\n#### Output Monitoring {#sec-security-privacy-output-monitoring-cf37}\n\nEven when inputs pass validation, adversarial or unexpected behavior may still emerge at the model's output. Output monitoring helps detect such anomalies by analyzing model predictions in real time. These mechanisms observe how the model behaves across inputs, by tracking its confidence, prediction entropy, class distribution, or response patterns, to flag deviations from expected behavior.\n\nA key target for monitoring is prediction confidence. For example, if a classification model begins assigning high confidence to low-frequency or previously rare classes, this may indicate the presence of adversarial inputs or a shift in the underlying data distribution. Monitoring the entropy of the output distribution can similarly reveal when the model is overly certain in ambiguous contexts—an early signal of possible manipulation.\n\nIn content moderation systems, a model that normally outputs neutral or \"safe\" labels may suddenly begin producing high-confidence \"safe\" labels for inputs containing offensive or restricted content. Output monitoring can detect this mismatch by comparing predictions against auxiliary signals or known-safe reference sets. When deviations are detected, the system may trigger a fallback policy—such as escalating the content for human review or switching to a conservative baseline model.\n\nTime-series models also benefit from output monitoring. For instance, an anomaly detection model used in fraud detection might track predicted fraud scores for sequences of financial transactions. A sudden drop in fraud scores, especially during periods of high transaction volume, may indicate model tampering, label leakage, or evasion attempts. Monitoring the temporal evolution of predictions provides a broader perspective than static, pointwise classification.\n\nGenerative models, such as text-to-image systems, introduce unique output monitoring challenges. These models can produce high-fidelity imagery that may inadvertently violate content safety policies, platform guidelines, or user expectations. To mitigate these risks, post-generation classifiers are commonly employed to assess generated content for objectionable characteristics such as violence, nudity, or brand misuse. These classifiers operate downstream of the generative model and can suppress, blur, or reject outputs based on predefined thresholds. Some systems also inspect internal representations (e.g., attention maps[^fn-attention-maps] or latent embeddings) to anticipate potential misuse before content is rendered.\n\n[^fn-attention-maps]: **Attention Maps**: Visualization technique for understanding transformer model focus, introduced with the attention mechanism in 2015. Attention maps reveal which input tokens influence outputs most strongly, helping detect potential bias or manipulation in models processing 175+ billion parameters like GPT-3.\n\nHowever, prompt filtering alone is insufficient for safety. Research has shown that text-to-image systems can be manipulated through implicitly adversarial prompts, which are queries that appear benign but lead to policy-violating outputs. The Adversarial Nibbler project introduces an open red teaming methodology that identifies such prompts and demonstrates how models like Stable Diffusion can produce unintended content despite the absence of explicit trigger phrases [@quaye2024adversarial]. These failure cases often bypass prompt filters because their risk arises from model behavior during generation, not from syntactic or lexical cues.\n\n![**Adversarial Prompt Evasion**: Implicitly adversarial prompts bypass typical content filters by triggering unintended generations, revealing limitations of solely relying on pre-generation safety checks. these examples underscore the necessity of post-hoc content analysis as a complementary defense layer for robust generative AI systems. Source: [@quaye2024adversarial.].](images/png/adversarial_nibbler_example.png){#fig-adversarial-nibbler}\n\nAs shown in @fig-adversarial-nibbler, even prompts that appear innocuous can trigger unsafe generations. Such examples highlight the limitations of pre-generation safety checks and reinforce the necessity of output-based monitoring as a second line of defense. This two-stage pipeline—consisting of prompt filtering followed by post-hoc content analysis important for ensuring the safe deployment of generative models in open-ended or user-facing environments.\n\nIn the domain of language generation, output monitoring plays a different but equally important role. Here, the goal is often to detect toxicity, hallucinated claims, or off-distribution responses. For example, a customer support chatbot may be monitored for keyword presence, tonal alignment, or semantic coherence. If a response contains profanity, unsupported assertions, or syntactically malformed text, the system may trigger a rephrasing, initiate a fallback to scripted templates, or halt the response altogether.\n\nEffective output monitoring combines rule-based heuristics with learned detectors trained on historical outputs. These detectors are deployed to flag deviations in real time and feed alerts into incident response pipelines. In contrast to model-centric defenses like adversarial training, which aim to improve model robustness, output monitoring emphasizes containment and remediation. Its role is not to prevent exploitation but to detect its symptoms and initiate appropriate countermeasures [@savas2022ml]. In safety-important or policy-sensitive applications, such mechanisms form a important layer of operational resilience.\n\nThese principles have been implemented in recent output filtering frameworks. For example, LLM Guard combines transformer-based classifiers with safety dimensions such as toxicity, misinformation, and illegal content to assess and reject prompts or completions in instruction-tuned LLMs [@lee2023llmguard]. Similarly, [ShieldGemma](https://ai.google.dev/gemma/docs/shieldgemma), developed as part of Google's open Gemma model release, applies configurable scoring functions to detect and filter undesired outputs during inference. Both systems exemplify how safety classifiers and output monitors are being integrated into the runtime stack to support scalable, policy-aligned deployment of generative language models.\n\n#### Integrity Checks {#sec-security-privacy-integrity-checks-2989}\n\nWhile input and output monitoring focus on model behavior, system integrity checks ensure that the underlying model files, execution environment, and serving infrastructure remain untampered throughout deployment. These checks detect unauthorized modifications, verify that the model running in production is authentic, and alert operators to suspicious system-level activity.\n\nOne of the most common integrity mechanisms is cryptographic model verification. Before a model is loaded into memory, the system can compute a cryptographic hash (e.g., SHA-256)[^fn-sha256-security] of the model file and compare it against a known-good signature.\n\n[^fn-sha256-security]: **SHA-256**: Cryptographic hash function producing 256-bit digests, part of the SHA-2 family designed by the NSA in 2001. Despite processing trillions of hashes daily across Bitcoin mining and digital signatures, no practical collision attacks exist after 20+ years, making it the gold standard for file integrity verification.\n\nAccess control and audit logging complement cryptographic checks. ML systems should restrict access to model files using role-based permissions and monitor file access patterns. For instance, repeated attempts to read model checkpoints from a non-standard path, or inference requests from unauthorized IP ranges, may indicate tampering, privilege escalation, or insider threats.\n\nIn cloud environments, container- or VM-based isolation[^fn-container-vm-isolation] helps enforce process and memory boundaries, but these protections can erode over time due to misconfiguration or supply chain vulnerabilities.\n\n[^fn-container-vm-isolation]: **Container/VM Isolation**: Virtualization technologies that provide process and memory separation—containers (Docker, 2013) offer lightweight OS-level isolation with typically 0-5% overhead for CPU-bound workloads and 2-10% for I/O-intensive operations, while VMs provide stronger hardware-level isolation with 10-15% overhead. In ML deployments, containerization is widely adopted for model serving, with industry surveys suggesting 80-90% adoption in cloud environments, though VMs remain preferred for sensitive models requiring stronger isolation guarantees.\n\nFor example, in a regulated healthcare ML deployment[^fn-healthcare-ml-compliance], integrity checks might include: verifying the model hash against a signed manifest, validating that the runtime environment uses only approved Python packages, and checking that inference occurs inside a signed and attested virtual machine. These checks ensure compliance with regulations like HIPAA[^fn-hipaa-ml-requirements]'s integrity requirements and GDPR's accountability principle, limit the risk of silent failures, and create a forensic trail in case of audit or breach.\n\n[^fn-healthcare-ml-compliance]: **Healthcare ML Compliance**: FDA has approved 500+ AI-based medical devices since 2016, requiring strict validation under 21 CFR Part 820 quality systems. Healthcare ML systems must demonstrate safety, efficacy, and bias mitigation, with some approvals taking 2-5 years and costing $50+ million in clinical trials.\n\n[^fn-hipaa-ml-requirements]: **HIPAA ML Requirements**: The Health Insurance Portability and Accountability Act (1996) imposes strict data protection rules affecting 600+ million patient records in the US. For ML systems, HIPAA requires encryption of data at rest and in transit, audit logs for all data access, and business associate agreements for cloud ML services, with violations carrying fines up to $1.5 million per incident.\n\nSome systems also implement runtime memory verification, such as scanning for unexpected model parameter changes or checking that memory-mapped model weights remain unaltered during execution. While more common in high-assurance systems, such checks are becoming more feasible with the adoption of secure enclaves and trusted runtimes.\n\nTaken together, system integrity checks play a important role in protecting machine learning systems from low-level attacks that bypass the model interface. When coupled with input/output monitoring, they provide layered assurance that both the model and its execution environment remain trustworthy under adversarial conditions.\n\n#### Response and Rollback {#sec-security-privacy-response-rollback-1792}\n\nWhen a security breach, anomaly, or performance degradation is detected in a deployed machine learning system, rapid and structured incident response is important to minimizing impact. The goal is not only to contain the issue but to restore system integrity and ensure that future deployments benefit from the insights gained. Unlike traditional software systems, ML responses may require handling model state, data drift, or inference behavior, making recovery more complex.\n\nThe first step is to define incident detection thresholds that trigger escalation. These thresholds may come from input validation (e.g., invalid input rates), output monitoring (e.g., drop in prediction confidence), or system integrity checks (e.g., failed model signature verification). When a threshold is crossed, the system should initiate an automated or semi-automated response protocol.\n\nOne common strategy is model rollback, where the system reverts to a previously verified version of the model. For instance, if a newly deployed fraud detection model begins misclassifying transactions, the system may fall back to the last known-good checkpoint, restoring service while the affected version is quarantined. Rollback mechanisms require version-controlled model storage, typically supported by MLOps platforms such as MLflow, TFX, or SageMaker.\n\nIn high-availability environments, model isolation may be used to contain failures. The affected model instance can be removed from load balancers or shadowed in a canary deployment setup. This allows continued service with unaffected replicas while maintaining forensic access to the compromised model for analysis.\n\nTraffic throttling is another immediate response tool. If an adversarial actor is probing a public inference API at high volume, the system can rate-limit or temporarily block offending IP ranges while continuing to serve trusted clients. This containment technique helps prevent abuse without requiring full system shutdown.\n\nOnce immediate containment is in place, investigation and recovery can begin. This may include forensic analysis of input logs, parameter deltas between model versions, or memory snapshots from inference containers. In regulated environments, organizations may also need to notify users or auditors, particularly if personal or safety-important data was affected.\n\nRecovery typically involves retraining or patching the model. This must occur through a secure update process, using signed artifacts, trusted build pipelines, and validated data. To prevent recurrence, the incident should feed back into model evaluation pipelines—updating tests, refining monitoring thresholds, or hardening input defenses. For example, if a prompt injection attack bypassed a content filter in a generative model, retraining might include adversarially crafted prompts, and the prompt validation logic would be updated to reflect newly discovered patterns.\n\nFinally, organizations should establish post-incident review practices. This includes documenting root causes, identifying gaps in detection or response, and updating policies and playbooks. Incident reviews help translate operational failures into actionable improvements across the design-deploy-monitor lifecycle.\n\n### Hardware Security Foundations {#sec-security-privacy-hardware-security-foundations-f5e8}\n\nThe software-layer defenses we've explored—input validation, output monitoring, and integrity checks—establish important protections, but they ultimately depend on the underlying hardware and firmware being trustworthy. If an attacker compromises the operating system, gains physical access to the device, or exploits vulnerabilities in the processor itself, these software defenses can be bypassed or disabled entirely. This limitation motivates hardware-based security mechanisms that operate below the software layer, creating a hardware root of trust that remains secure even when higher-level systems are compromised.\n\nAt the foundational level of our defensive framework, hardware-based security mechanisms provide the trust anchor for all higher-layer protections. Machine learning systems deployed in edge devices, embedded systems, and untrusted cloud infrastructure increasingly rely on hardware-based security features to establish this foundation. Hardware acceleration platforms, including GPUs, TPUs, and specialized ML accelerators, often incorporate security features such as secure enclaves, trusted execution environments, and hardware cryptographic units, while edge deployment scenarios present unique security challenges due to physical accessibility and constrained resources.\n\nThese hardware security mechanisms become particularly crucial when systems must meet regulatory compliance requirements. Healthcare ML systems handling protected health information under HIPAA must implement \"appropriate technical safeguards\" including access controls and encryption. Systems processing EU citizens' data under GDPR must demonstrate \"appropriate technical and organizational measures\" with privacy by design principles embedded at the hardware level.\n\nTo understand how hardware security protects ML systems, imagine building a secure fortress for your most valuable assets. Each hardware security primitive serves a distinct defensive role:\n\n+-----------------------+----------------------------------------------------------------------+\n| **Mechanism**         | **Fortress Analogy and Function**                                    |\n+:======================+:=====================================================================+\n| **Secure Boot**       | Functions like a trusted gatekeeper checking credentials of everyone |\n|                       | entering the fortress at dawn. Before your system runs any code,     |\n|                       | Secure Boot cryptographically verifies that the firmware and         |\n|                       | operating system haven't been tampered with.                         |\n+-----------------------+----------------------------------------------------------------------+\n| **Trusted Execution** | Create secure, windowless rooms deep inside the fortress where you   |\n+-----------------------+----------------------------------------------------------------------+\n| **Environments**      | handle your most sensitive operations. When your ML model processes  |\n+-----------------------+----------------------------------------------------------------------+\n| **(TEEs)**            | private medical data or proprietary algorithms, the TEE isolates     |\n|                       | these computations from the rest of the system.                      |\n+-----------------------+----------------------------------------------------------------------+\n| **Hardware Security** | Serve as specialized, impenetrable vaults designed specifically for  |\n+-----------------------+----------------------------------------------------------------------+\n| **Modules (HSMs)**    | storing and using your most valuable cryptographic keys. Rather than |\n|                       | keeping encryption keys in regular computer memory where they might  |\n|                       | be stolen, HSMs provide tamper-resistant storage.                    |\n+-----------------------+----------------------------------------------------------------------+\n| **Physical**          | Give each device a unique biometric fingerprint at the silicon       |\n+-----------------------+----------------------------------------------------------------------+\n| **Unclonable**        | level. Just as human fingerprints cannot be perfectly replicated,    |\n+-----------------------+----------------------------------------------------------------------+\n| **Functions (PUFs)**  | PUFs exploit tiny manufacturing variations in each chip to create    |\n|                       | device-unique identifiers that cannot be cloned.                     |\n+-----------------------+----------------------------------------------------------------------+\n\n: **Hardware Security Mechanisms**: Each primitive provides distinct defensive capabilities that work together to create comprehensive protection from hardware-level threats. {#tbl-hardware-security-mechanisms}\n\nThese mechanisms work together to create comprehensive protection that begins in hardware and extends through all software layers.\n\nThis section explores how these four complementary hardware primitives work together to create comprehensive protection (@tbl-hardware-security-mechanisms). Each mechanism addresses different security challenges but works most effectively when combined: secure boot establishes initial trust, TEEs provide runtime isolation, HSMs handle cryptographic operations, and PUFs enable device-unique authentication. We begin with Trusted Execution Environments (TEEs), which provide isolated runtime environments for sensitive computations. Secure Boot ensures system integrity from power-on, creating the trusted foundation that TEEs depend upon. Hardware Security Modules (HSMs) offer specialized cryptographic processing and tamper-resistant key storage, often required for regulatory compliance. Finally, Physical Unclonable Functions (PUFs) provide device-unique identities that enable lightweight authentication and cannot be cloned or extracted.\n\nEach mechanism addresses different aspects of the security challenge, working most effectively when deployed together across hardware, firmware, and software boundaries.\n\n#### Hardware-Software Co-Design {#sec-security-privacy-hardwaresoftware-codesign-bed2}\n\nModern ML systems require holistic analysis of security trade-offs across the entire hardware-software stack, similar to how we analyze compute-memory-energy trade-offs in performance optimization. The interdependence between hardware security features and software defenses creates both opportunities and constraints that must be understood quantitatively.\n\nHardware security mechanisms introduce measurable overhead that must be factored into system design. ARM TrustZone world-switching adds approximately 300-1000 cycles depending on processor generation and cache state (0.6-2.0μs at 500MHz) of latency per transition between secure and non-secure worlds. Cryptographic operations in secure mode typically consume 15-30% additional power compared to normal execution, impacting battery life in mobile ML applications. Intel SGX context switching imposes 15-30μs overhead per inference, representing 2% energy overhead for typical edge ML workloads.\n\nSecurity features scale differently than computational resources. TEE memory limitations constrain model size regardless of available system memory. A quantized ResNet-18 model (47MB) can operate within ARM TrustZone constraints, while ResNet-50 (176MB) requires careful memory management or model partitioning. These constraints create architectural decisions that must be made early in system design.\n\nDifferent threat models and protection levels require quantitative trade-off analysis. For ML workloads requiring cryptographic verification, AES-256 operations add 0.1-0.5ms per inference depending on model size and hardware acceleration availability. Homomorphic encryption operations impose 100-100,000x computational overhead, with fully homomorphic encryption (FHE) at the higher end and somewhat homomorphic encryption (SHE) at the lower end, making them viable only for small models or offline scenarios where strong privacy guarantees justify the performance cost.\n\n#### Trusted Execution Environments {#sec-security-privacy-trusted-execution-environments-80ed}\n\nA Trusted Execution Environment (TEE)[^fn-tee] is a hardware-isolated region within a processor designed to protect sensitive computations and data from potentially compromised software. TEEs enforce confidentiality, integrity, and runtime isolation, ensuring that even if the host operating system or application layer is attacked, sensitive operations within the TEE remain secure.\n\n[^fn-tee]: **TEE Concept Origins**: The idea emerged from ARM's TrustZone development in the early 2000s, inspired by the military concept of \"compartmentalized information.\" ARM realized that mobile devices needed secure and non-secure \"worlds\" running on the same processor—leading to hardware-enforced isolation that became the template for all modern TEEs.]\n\nIn the context of machine learning, TEEs are increasingly important for preserving the confidentiality of models, securing sensitive user data during inference, and ensuring that model outputs remain trustworthy. For example, a TEE can protect model parameters from being extracted by malicious software running on the same device, or ensure that computations involving biometric inputs, including facial data or fingerprint data, are performed securely. This capability is essential in applications where model integrity, user privacy, or regulatory compliance are non-negotiable.\n\nOne widely deployed example is [Apple's Secure Enclave](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web), which provides isolated execution and secure key storage for iOS devices. By separating cryptographic operations and biometric data from the main processor, the Secure Enclave ensures that user credentials and Face ID features remain protected, even in the event of a broader system compromise.\n\nTrusted Execution Environments are important across a range of industries with high security requirements. In telecommunications, TEEs are used to safeguard encryption keys and secure important 5G control-plane operations. In finance, they allow secure mobile payments and protect PIN-based authentication workflows. In healthcare, TEEs help enforce patient data confidentiality during edge-based ML inference on wearable or diagnostic devices. In the automotive industry, they are deployed in advanced driver-assistance systems (ADAS) to ensure that safety-important perception and decision-making modules operate on verified software.\n\nIn machine learning systems, TEEs can provide several important protections. They secure the execution of model inference or training, shielding intermediate computations and final predictions from system-level observation. They protect the confidentiality of sensitive inputs, including biometric or clinical signals, used in personal identification or risk scoring tasks. TEEs also serve to prevent reverse engineering of deployed models by restricting access to weights and architecture internals. When models are updated, TEEs ensure the authenticity of new parameters and block unauthorized tampering. In distributed ML settings, TEEs can protect data exchanged between components by enabling encrypted and attested communication channels.\n\nThe core security properties of a TEE are achieved through four mechanisms: isolated execution, secure storage, integrity protection, and in-TEE data encryption. Code that runs inside the TEE is executed in a separate processor mode, inaccessible to the normal-world operating system. Sensitive assets such as cryptographic keys or authentication tokens are stored in memory that only the TEE can access. Code and data can be verified for integrity before execution using hardware-anchored hashes or signatures. Finally, data processed inside the TEE can be encrypted, ensuring that even intermediate results are inaccessible without appropriate keys, which are also managed internally by the TEE.\n\nSeveral commercial platforms provide TEE functionality tailored for different deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. [Qualcomm's Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions) supports secure mobile transactions and user authentication. Apple's Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices.\n\n[^fn-trustzone-adoption]: **ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting over 5 billion mobile devices. Despite its ubiquity, many devices underutilize TrustZone—studies show only 20-30% of Android devices implement meaningful secure world applications beyond basic key storage.\n\n[^fn-intel-sgx-limits]: **Intel SGX Constraints**: SGX enclaves are limited to approximately 128MB of protected memory (EPC) on most consumer processors, though enterprise variants support up to 512MB or 1GB, with cache misses causing 100x performance penalties. For ML workloads, a ResNet-50 requires approximately 98MB for weights alone in FP32 format (25.6M parameters × 4 bytes), consuming 77% of SGX EPC before any intermediate activations. Inference latency increases from 5ms to 150ms when model exceeds EPC capacity. This makes SGX unsuitable for large ML models but effective for protecting cryptographic keys and small inference models under 10MB.\n\n@fig-enclave illustrates a secure enclave integrated into a system-on-chip (SoC) architecture. The enclave includes a dedicated processor, an AES engine, a true random number generator (TRNG), a public key accelerator (PKA), and a secure I²C interface to nonvolatile storage. These components operate in isolation from the main application processor and memory subsystem. A memory protection engine enforces access control, while cryptographic operations such as NAND flash encryption are handled internally using enclave-managed keys. By physically separating secure execution and key management from the main system, this architecture limits the impact of system-level compromises and establishes hardware-enforced trust.\n\n::: {#fig-enclave fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n},\nscale=0.9, every node/.append style={transform shape}]\n\\tikzset{%\nLineA/.style={line width=1.0pt,black!50,latex-latex},\nLine/.style={BrownLine!60, {Triangle[width = 8pt, length = 6pt]}-{Triangle[width = 8pt, length = 6pt]}, line width = 4pt},\nLineD/.style={line width=1.0pt,black!50,latex-,dashed},\nBox/.style={inner xsep=2pt,inner ysep=6pt,\n    node distance=0.25,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=28mm,\n    minimum width=28mm, minimum height=10.5mm\n  },\nBox2/.style={Box,draw=RedLine,fill=RedL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2, text width=42mm,\n    minimum width=42mm, minimum height=8mm},\nBox4/.style={Box,anchor=west,minimum width=42mm, minimum height=48mm},\nBox4a/.style={Box,draw=BlueLine,fill=BlueL!50,anchor=west,minimum width=42mm, minimum height=48mm},\nBox5/.style={Box,draw=BlueLine,fill=BlueL!50},\nBox6/.style={Box,draw=OrangeLine,fill=OrangeL!50},\n}\n\n\\tikzset{pics/key/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=FLAG1,scale=\\scalefac, every node/.append style={transform shape}]\n\\draw[draw=\\drawchannelcolor,fill=\\channelcolor](-0.28,0.04)to[out=310,in=90](-0.16,-0.053)\nto[out=270,in=90](-0.16,-0.15)to[out=270,in=110](-0.11,-0.22)\nto[out=220,in=80](-0.14,-0.28)to(-0.05,-0.38)to(-0.12,-0.47)to(-0.08,-0.55)to(-0.14,-0.62)\nto(-0.09,-0.7)to(-0.13,-0.76)to[out=290,in=70](-0.11,-0.9)to(0.03,-1.06)to[out=30,in=270](0.15,-0.9)\nto(0.15,-0.01)to[out=10,in=350,distance=3](0.15,0.125)to[out=60,in=220](0.25,0.21)\nto[out=30,in=330](0.25,0.97)to[out=150,in=30](-0.278,0.97)\nto[out=210,in=150](-0.278,0.21)to[out=330,in=20](-0.24,0.08)to[out=200,in=80]cycle;\n\\fill[white](-0.018,0.77)circle(4pt);\n\\end{scope}\n     }\n  }\n}\n\\pgfkeys{\n  /channel/.cd,\n  channelcolor/.store in=\\channelcolor,\n  drawchannelcolor/.store in=\\drawchannelcolor,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  channelcolor=BrownLine,\n  drawchannelcolor=BrownLine,\n  scalefac=1,\n  Linewidth=1.6pt,\n  picname=C\n}\n\n\\node[Box](B1){TRNG};\n\\node[Box,below=of B1](B2){Secure Enclave AES Engine};\n\\node[Box,below=of B2](B3){PKA};\n\\node[Box2,below=of B3](B4){I2C bus};\n\\node[Box3,below=1.35of B4](B5){Secure Nonvolatile Storage};\n%\n\\node[Box4](SEP)at($(B1.north east)!0.5!(B4.south east)+(1.25,0)$){Secure Enclave\\\\ Processor};\n\\node[Box,anchor=west](MPE)at($(SEP.east)+(1.25,0)$){Memory Protection\\\\Engine};\n%\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=5mm,inner ysep=4mm,\nyshift=0mm,fill=none,fit=(B1)(B4)(MPE),line width=0.75pt](BB1){};\n%\n\\node[Box4a,anchor=south west](AP)at($(B1.north west)+(0,1.3)$){Application\\\\ Processor};\n\\node[Box5,anchor=south west](NAN)at($(AP.east)+(1,0.2)$){NAND flash controller};\n\\node[Box5,anchor=north west](AES)at($(AP.east)+(1,-0.45)$){AES engine};\n\\path[](MPE)|-coordinate(S)(AP.north east);\n\\node[Box5,anchor=north](MC)at(S){Memory controller};\n%\n\\node[Box6,above=1 of MC](DRAM){DRAM};\n\\path[](DRAM)-|coordinate(SS)(NAN);\n\\node[Box6](NAND)at(SS){NAND flash\\\\ storage};\n\\draw[Line](SEP)--(MPE);\n\\draw[Line](MPE)--(MC);\n\\draw[Line](MC)--(DRAM);\n\\draw[Line](NAN)--(NAND);\n\\draw[Line](NAN)--(AES);\n\\draw[LineD](AES)--coordinate(KEY)(AES|-BB1.north);\n\\draw[Line](NAN)--(NAN-|MC);\n\\draw[Line](AES)--(AES-|MC);\n\\draw[Line](AP.315)--(AP.315-|MC);\n\\draw[Line](B4)--(B5);\n%\n\\scoped[on background layer]\n\\node[draw=RedLine,inner xsep=5mm,inner ysep=5mm,\nyshift=-0.5mm,fill=magenta!5,fit=(BB1)(AP)(MC),line width=0.75pt](BB2){};\n\\node[above=4pt of  BB2.south,inner sep=0pt,\nanchor=south]{\\textbf{System on chip}};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=5mm,inner ysep=4mm,\nyshift=0mm,fill=BackColor!60,fit=(B1)(B4)(MPE),line width=0.75pt](BB3){};\n\\node[above=6pt of  BB3.south,inner sep=0pt,\nanchor=south]{\\textbf{Secure Enclave}};\n%\n\\begin{scope}[local bounding box=KEY1,shift={($(KEY)+(0.4,-0.30)$)},\nscale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){key={scalefac=0.4,picname=1,drawchannelcolor=none,\nchannelcolor=red!79!black!90, Linewidth=1.0pt}};\n \\end{scope}\n\\end{tikzpicture}\n```\n**Secure Enclave Architecture**: Hardware-isolated enclaves enhance system security by encapsulating sensitive data and cryptographic operations within a dedicated processor and memory. This design minimizes the attack surface and protects important keys even if the main application processor is compromised, providing a trusted execution environment for security-important tasks. Source: Apple.\n:::\n\nThis architecture underpins the secure deployment of machine learning applications on consumer devices. For example, Apple's Face ID system uses a secure enclave to perform facial recognition entirely within a hardware-isolated environment. The face embedding model is executed inside the enclave, and biometric templates are stored in secure nonvolatile memory accessible only via the enclave's I²C interface. During authentication, input data from the infrared camera is processed locally, and no facial features or predictions ever leave the secure region. Even if the application processor or operating system is compromised, the enclave prevents access to sensitive model inputs, parameters, and outputs—ensuring that biometric identity remains protected end to end.\n\nDespite their strengths, Trusted Execution Environments come with notable trade-offs. Implementing a TEE increases both direct hardware costs and indirect costs associated with developing and maintaining secure software. Integrating TEEs into existing systems may require architectural redesigns, especially for legacy infrastructure. Developers must adhere to strict protocols for isolation, attestation, and secure update management, which can extend development cycles and complicate testing workflows. TEEs can also introduce performance overhead, particularly when cryptographic operations are involved, or when context switching between trusted and untrusted modes is frequent.\n\nEnergy efficiency is another consideration, particularly in battery-constrained devices. TEEs typically consume additional power due to secure memory accesses, cryptographic computation, and hardware protection logic. In resource-limited embedded systems, these costs may limit their use. In terms of scalability and flexibility, the secure boundaries enforced by TEEs may complicate distributed training or federated inference workloads, where secure coordination between enclaves is required.\n\nMarket demand also varies. In some consumer applications, perceived threat levels may be too low to justify the integration of TEEs. Systems with TEEs may be subject to formal security certifications, such as [Common Criteria](https://www.commoncriteriaportal.org/ccra/index.cfm) or evaluation under [ENISA](https://www.enisa.europa.eu/), which can introduce additional time and expense. For this reason, TEEs are typically adopted only when the expected threat model, including adversarial users, cloud tenants, and malicious insiders, justifies the investment.\n\nNonetheless, TEEs remain a powerful hardware primitive in the machine learning security landscape. When paired with software- and system-level defenses, they provide a trusted foundation for executing ML models securely, privately, and verifiably, especially in scenarios where adversarial compromise of the host environment is a serious concern.\n\nHere is the revised 7.5.2 Secure Boot section, rewritten in formal textbook tone with all original technical content, hyperlinks, and figures preserved. The structure emphasizes narrative clarity, avoids bullet lists, and integrates the Apple Face ID case study naturally.\n\n#### Secure Boot {#sec-security-privacy-secure-boot-5242}\n\nSecure Boot is a mechanism that ensures a device only boots software components that are cryptographically verified and explicitly authorized by the manufacturer. At startup, each stage of the boot process, comprising the bootloader, kernel, and base operating system, is checked against a known-good digital signature. If any signature fails verification, the boot sequence is halted, preventing unauthorized or malicious code from executing. This chain-of-trust model establishes system integrity from the very first instruction executed.\n\nIn ML systems, especially those deployed on embedded or edge hardware, Secure Boot plays an important role. A compromised boot process may result in malicious software loading before the ML runtime begins, enabling attackers to intercept model weights, tamper with training data, or reroute inference results. Such breaches can lead to incorrect or manipulated predictions, unauthorized data access, or device repurposing for botnets or crypto-mining.\n\nFor machine learning systems, Secure Boot offers several guarantees. First, it protects model-related data, such as training data, inference inputs, and outputs, during the boot sequence, preventing pre-runtime tampering. Second, it ensures that only authenticated model binaries and supporting software are loaded, which helps guard against deployment-time model substitution. Third, Secure Boot allows secure model updates by verifying that firmware or model changes are signed and have not been altered in transit.\n\nSecure Boot frequently works in tandem with hardware-based Trusted Execution Environments (TEEs) to create a fully trusted execution stack. As shown in @fig-secure-boot, this layered boot process verifies firmware, operating system components, and TEE integrity before permitting execution of cryptographic operations or ML workloads. In embedded systems, this architecture provides resilience even under severe adversarial conditions or physical device compromise.\n\n::: {#fig-secure-boot fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={black!50,-{Triangle[width = 6pt, length = 6pt]}, line width = 1.15pt,text=black},\nBox/.style={inner xsep=2pt,inner ysep=6pt,\n    node distance=0.5,\n    draw=BlueLine,\n    line width=0.75pt,\n    fill=BlueL!40,\n    align=flush center,\n    text width=58mm,\n    minimum width=58mm, minimum height=10mm\n  },\nBox2/.style={Box,draw=RedLine,fill=RedL,rounded corners=12pt},\nBox3/.style={draw=VioletLine,fill=VioletL2, trapezium, trapezium left angle=70,\ndiamond, minimum width=45mm, minimum height=15mm, text centered,inner sep= -2ex},\nBox4/.style={Box,anchor=west,minimum width=42mm, minimum height=48mm},\nBox5/.style={Box,draw=BlueLine,fill=BlueL!50},\nBox6/.style={Box,draw=OrangeLine,fill=OrangeL!50},\n}\n\n\\node[Box2](B1){Power up};\n\\node[Box,below=of B1](B2){Hardware init};\n\\node[Box,below=of B2](B3){Get the information of MTM device, complete the boot and self-diagnosis process of MTM, store the diagnostic information of hardware platform and MTM device, set verification register};\n\\node[Box,below=of B3](B4){Copy kernel image and root FS image from FLASH to RAM};\n\\node[Box,below=of B4](B5){Checking the CRC checksum of kernel image};\n\\node[Box,below=of B5](B6){Perform integrity measurement, verification, and storage of Linux kernel image};\n\\node[Box3,below=0.6of B6](B7){Success};\n\\node[Box,below=0.7of B7](B8){Boot abort};\n%\n\\node[Box,right=3 of B3](B9){Checking the CRC checksum of root fs image};\n\\node[Box,below=1of B9](B10){Perform integrity measurement, verification, and storage of root fs image};\n\\node[Box3,below=1of B10](B11){Success};\n\\node[Box,below=1of B11](B12){Set boot parameters for kernel};\n\\node[Box,below=1of B12](B13){Booting the kernel};\n%\n\\foreach \\x in{1,2,3,4,5,6}{\n\\pgfmathtruncatemacro{\\newX}{\\x + 1} %\n\\draw[Line](B\\x)--(B\\newX);\n}\n\\foreach \\x in{9,10,12}{\n\\pgfmathtruncatemacro{\\newX}{\\x + 1} %\n\\draw[Line](B\\x)--(B\\newX);\n}\n\\draw[Line](B7)--node[right,pos=0.3]{No}(B8);\n\\draw[Line](B7.east)--node[above,pos=0.25]{Yes}++(1.8,0)--++(0,10.5)-|(B9);\n\\draw[Line](B11.west)--node[above,pos=0.25]{No}++(-1.8,0)|-(B8);\n\\draw[Line](B11)--node[right,pos=0.3]{Yes}(B12);\n\\end{tikzpicture}\n```\n**Secure Boot Sequence**: Embedded systems employ a layered boot process to verify firmware and software integrity, establishing a root of trust before executing machine learning workloads and protecting against pre-runtime attacks. This architecture ensures only authenticated code runs, safeguarding model data and preventing unauthorized model substitution or modification during deployment. Source: [@rashmi2018secure].\n:::\n\nA well-known real-world implementation of Secure Boot appears in Apple's Face ID system, which uses advanced machine learning for facial recognition. For Face ID to operate securely, the entire device stack, from the initial power-on to the execution of the model, must be verifiably trusted.\n\nUpon device startup, Secure Boot initiates within Apple's [Secure Enclave](https://support.apple.com/en-us/102381), a dedicated security coprocessor that handles biometric data. The firmware loaded onto the Secure Enclave is digitally signed by Apple, and any unauthorized modification causes the boot process to fail. Once verified, the Secure Enclave performs continuous checks in coordination with the central processor to maintain a trusted boot chain. Each system component, ranging from the iOS kernel to the application-level code, is verified using cryptographic signatures.\n\nAfter completing the secure boot sequence, the Secure Enclave activates the ML-based Face ID system. The facial recognition model projects over 30,000 infrared points to map a user's face, generating a depth image and computing a mathematical representation that is compared against a securely stored profile. These facial data artifacts are never written to disk, transmitted off-device, or shared externally. All processing occurs within the enclave to protect against eavesdropping or exfiltration, even in the presence of a compromised kernel.\n\nTo support continued integrity, Secure Boot also governs software updates. Only firmware or model updates signed by Apple are accepted, ensuring that even over-the-air patches do not introduce risk. This process maintains a robust chain of trust over time, enabling the secure evolution of the ML system while preserving user privacy and device security.\n\nWhile Secure Boot provides strong protection, its adoption presents technical and operational challenges. Managing the cryptographic keys used to sign and verify system components is complex, especially at scale. Enterprises must securely provision, rotate, and revoke keys, ensuring that no trusted root is compromised. Any such breach would undermine the entire security chain.\n\nPerformance is also a consideration. Verifying signatures during the boot process introduces latency, typically on the order of tens to hundreds of milliseconds per component. Although acceptable in many applications, these delays may be problematic for real-time or power-constrained systems. Developers must also ensure that all components, including bootloaders, firmware, kernels, drivers, and even ML models, are correctly signed. Integrating third-party software into a Secure Boot pipeline introduces additional complexity.\n\nSome systems limit user control in favor of vendor-locked security models, restricting upgradability or customization. In response, open-source bootloaders like [u-boot](https://source.denx.de/u-boot/u-boot) and [coreboot](https://www.coreboot.org/) have emerged, offering Secure Boot features while supporting extensibility and transparency. To further scale trusted device deployments, emerging industry standards such as the [Device Identifier Composition Engine (DICE)](https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/) and [IEEE 802.1AR IDevID](https://1.ieee802.org/security/802-1ar/) provide mechanisms for secure device identity, key provisioning, and cross-vendor trust assurance.\n\nSecure Boot, when implemented carefully and complemented by trusted hardware and secure software update processes, forms the backbone of system integrity for embedded and distributed ML. It provides the assurance that the machine learning model running in production is not only the correct version, but is also executing in a known-good environment, anchored to hardware-level trust.\n\n#### Hardware Security Modules {#sec-security-privacy-hardware-security-modules-4377}\n\nWhile TEEs and secure boot provide runtime isolation and integrity verification, Hardware Security Modules (HSMs) specialize in the cryptographic operations that underpin these protections. An HSM[^fn-hsm-performance] is a tamper-resistant physical device designed to perform cryptographic operations and securely manage digital keys. HSMs are widely used across security-important industries such as finance, defense, and cloud infrastructure, and they are increasingly relevant for securing the machine learning pipeline—particularly in deployments where key confidentiality, model integrity, and regulatory compliance are important.\n\n[^fn-hsm-performance]: **HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $20,000-$100,000+ per unit. In contrast, software-only cryptography on GPUs achieves 100,000+ operations/second at $1,000+ hardware cost, but without the tamper-resistance and regulatory compliance that HSMs provide.\n\nHSMs provide an isolated, hardened environment for performing sensitive operations such as key generation, digital signing, encryption, and decryption. Unlike general-purpose processors, they are engineered to withstand physical tampering and side-channel attacks, and they typically include protected storage, cryptographic accelerators, and internal audit logging. HSMs may be implemented as standalone appliances, plug-in modules, or integrated chips embedded within broader systems.\n\nIn machine learning systems, HSMs enhance security across several dimensions. They are commonly used to protect encryption keys associated with sensitive data that may be processed during training or inference. These keys might encrypt data at rest in model checkpoints or allow secure transmission of inference requests across networked environments. By ensuring that the keys are generated, stored, and used exclusively within the HSM, the system minimizes the risk of key leakage, unauthorized reuse, or tampering.\n\nHSMs also play a role in maintaining the integrity of machine learning models. In many production pipelines, models must be signed before deployment to ensure that only verified versions are accepted into runtime environments. The signing keys used to authenticate models can be stored and managed within the HSM, providing cryptographic assurance that the deployed artifact is authentic and untampered. Similarly, secure firmware updates and configuration changes, regardless of whether they pertain to models, hyperparameters, or supporting infrastructure, can be validated using signatures produced by the HSM.\n\nIn addition to protecting inference workloads, HSMs can be used to secure model training. During training, data may originate from distributed and potentially untrusted sources. HSM-backed protocols can help ensure that training pipelines perform encryption, integrity checks, and access control enforcement securely and in compliance with organizational or legal requirements. In regulated industries such as healthcare and finance, such protections are often mandatory. For instance, HIPAA requires covered entities to implement technical safeguards including \"integrity controls\" and \"encryption and decryption,\" while GDPR mandates pseudonymization and encryption as examples of appropriate technical measures.\n\nDespite these benefits, incorporating HSMs into embedded or resource-constrained ML systems introduces several trade-offs. First, HSMs are specialized hardware components and often come at a premium. Their cost may be justified in data center settings or safety-important applications but can be prohibitive for low-margin embedded products or wearables. Physical space is also a concern. Embedded systems often operate under strict size, weight, and form factor constraints, and integrating an HSM may require redesigning circuit layouts or sacrificing other functionality.\n\nFrom a performance standpoint, HSMs introduce latency, particularly for operations like key exchange, signature verification, or on-the-fly decryption. In real-time inference systems, including autonomous vehicles, industrial robotics, and live translation devices, these delays can affect responsiveness. While HSMs are typically optimized for cryptographic throughput, they are not general-purpose processors, and offloading secure operations must be carefully coordinated.\n\nPower consumption is another concern. The continuous secure handling of keys, signing of transactions, and cryptographic validations can consume more power than basic embedded components, impacting battery life in mobile or remote deployments.\n\nIntegration complexity also grows when HSMs are introduced into existing ML pipelines. Interfacing between the HSM and the host processor requires dedicated APIs and often specialized software development. Firmware and model updates must be routed through secure, signed channels, and update orchestration must account for device-specific key provisioning. These requirements increase the operational burden, especially in large deployments.\n\nScalability presents its own set of challenges. Managing a distributed fleet of HSM-equipped devices requires secure provisioning of individual keys, secure identity binding, and coordinated trust management. In large ML deployments, including fleets of smart sensors or edge inference nodes, ensuring uniform security posture across all devices is nontrivial.\n\nFinally, the use of HSMs often requires organizations to engage in certification and compliance processes[^fn-hsm-certification], particularly when handling regulated data. Meeting standards such as FIPS 140-2[^fn-fips-140] or Common Criteria adds time and cost to development.\n\n[^fn-hsm-certification]: **HSM Certification**: Hardware Security Module certification under FIPS 140-2 or Common Criteria can take 12-24 months and cost $500,000-$2 million. However, many regulated industries require these certifications, with banking, government, and healthcare sectors mandating Level 3+ certified HSMs for cryptographic operations.\n\n[^fn-fips-140]: **FIPS 140-2 Standard**: Federal Information Processing Standard for cryptographic modules, established in 2001 with four security levels. Level 4 HSMs must survive physical attacks, operating at -40°C to +85°C with tamper detection that zeroizes keys within seconds, making them suitable for the most sensitive ML applications. Access to the HSM is typically restricted to a small set of authorized personnel, which can complicate development workflows and slow iteration cycles.\n\nDespite these operational complexities, HSMs remain a valuable option for machine learning systems that require high assurance of cryptographic integrity and access control. When paired with TEEs, secure boot, and software-based defenses, HSMs contribute to a multilayered security model that spans hardware, system software, and ML runtime.\n\n#### Physical Unclonable Functions {#sec-security-privacy-physical-unclonable-functions-6533}\n\nPhysical Unclonable Functions (PUFs)[^fn-puf-adoption] provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication by leveraging physical randomness in semiconductor fabrication [@gassend2002silicon]. Unlike traditional keys stored in memory, a PUF generates secret values based on microscopic variations in a chip's physical properties—variations that are inherent to manufacturing processes and difficult to clone or predict, even by the manufacturer.\n\n[^fn-puf-adoption]: **PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT security needs. Major semiconductor companies including Intel, Xilinx, and Synopsis now offer PUF IP, with deployment in smart cards, automotive ECUs, and edge ML devices requiring device-unique authentication.\n\nThese variations arise from uncontrollable physical factors such as doping concentration, line edge roughness, and dielectric thickness. As a result, even chips fabricated with the same design masks exhibit small but measurable differences in timing, power consumption, or voltage behavior. PUF circuits amplify these variations to produce a device-unique digital output. When a specific input challenge is applied to a PUF, it generates a corresponding response based on the chip's physical fingerprint. Because these characteristics are effectively impossible to replicate, the same challenge will yield different responses across devices.\n\nThis challenge-response mechanism allows PUFs to serve several cryptographic purposes. They can be used to derive device-specific keys that never need to be stored externally, reducing the attack surface for key exfiltration. The same mechanism also supports secure authentication and attestation, where devices must prove their identity to trusted servers or hardware gateways. These properties make PUFs a natural fit for machine learning systems deployed in embedded and distributed environments.\n\nIn ML applications, PUFs offer unique advantages for securing resource-constrained systems. For example, consider a smart camera drone that uses onboard computer vision to track objects. A PUF embedded in the drone's processor can generate a private key to encrypt the model during boot. Even if the model were extracted, it would be unusable on another device lacking the same PUF response. That same PUF-derived key could also be used to watermark the model parameters, creating a cryptographically verifiable link between a deployed model and its origin hardware. If the model were leaked or pirated, the embedded watermark could help prove the source of the compromise.\n\nPUFs also support authentication in distributed ML pipelines. If the drone offloads computation to a cloud server, the PUF can help verify that the drone has not been cloned or tampered with. The cloud backend can issue a challenge, verify the correct response from the device, and permit access only if the PUF proves device authenticity. These protections enhance trust not only in the model and data, but in the execution environment itself.\n\nThe internal operation of a PUF is illustrated in @fig-pfu. At a high level, a PUF accepts a challenge input and produces a unique response determined by the physical microstructure of the chip [@gao2020physical]. Variants include optical PUFs, in which the challenge consists of a light pattern and the response is a speckle image, and electronic PUFs such as Arbiter PUFs (APUFs), where timing differences between circuit paths produce a binary output. Another common implementation is the SRAM PUF, which exploits the power-up state of uninitialized SRAM cells: due to threshold voltage mismatch, each cell tends to settle into a preferred value when power is first applied. These response patterns form a stable, reproducible hardware fingerprint.\n\n![**Physical Unclonable Functions**: Pufs generate unique hardware fingerprints from inherent manufacturing variations, enabling device authentication and secure key generation without storing secrets. Optical and electronic PUF implementations use physical phenomena—such as light speckle patterns or timing differences—to produce challenge-response pairs that are difficult to predict or replicate. Source: [@gao2020physical].](images/png/puf_basics.png){#fig-pfu}\n\nDespite their promise, PUFs present several challenges in system design. Their outputs can be sensitive to environmental variation, such as changes in temperature or voltage, which can introduce instability or bit errors in the response. To ensure reliability, PUF systems must often incorporate error correction codes or helper data schemes. Managing large sets of challenge-response pairs also raises questions about storage, consistency, and revocation. Additionally, the unique statistical structure of PUF outputs may make them vulnerable to machine learning-based modeling attacks if not carefully shielded from external observation.\n\nFrom a manufacturing perspective, incorporating PUF technology can increase device cost or require additional layout complexity. While PUFs eliminate the need for external key storage, thereby reducing long-term security risk and provisioning cost, they may require calibration and testing during fabrication to ensure consistent performance across environmental conditions and device aging.\n\nNevertheless, Physical Unclonable Functions remain a compelling building block for securing embedded machine learning systems. By embedding hardware identity directly into the chip, PUFs support lightweight cryptographic operations, reduce key management burden, and help establish root-of-trust anchors in distributed or resource-constrained environments. When integrated thoughtfully, they complement other hardware-assisted security mechanisms such as Secure Boot, TEEs, and HSMs to provide defense-in-depth across the ML system lifecycle.\n\n#### Mechanisms Comparison {#sec-security-privacy-mechanisms-comparison-2dcb}\n\nHardware-assisted security mechanisms play a foundational role in establishing trust within modern machine learning systems. While software-based defenses offer flexibility, they ultimately rely on the security of the hardware platform. As machine learning workloads increasingly operate on edge devices, embedded platforms, and untrusted infrastructure, hardware-backed protections become important for maintaining system integrity, confidentiality, and trust.\n\nTrusted Execution Environments (TEEs) provide runtime isolation for model inference and sensitive data handling. Secure Boot enforces integrity from power-on, ensuring that only verified software is executed. Hardware Security Modules (HSMs) offer tamper-resistant storage and cryptographic processing for secure key management, model signing, and firmware validation. Physical Unclonable Functions (PUFs) bind secrets and authentication to the physical characteristics of a specific device, enabling lightweight and unclonable identities.\n\nThese mechanisms address different layers of the system stack, ranging from initialization and attestation to runtime protection and identity binding, and complement one another when deployed together. @tbl-hw-security-comparison below compares their roles, use cases, and trade-offs in machine learning system design.\n\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Mechanism**                | **Primary Function**         | **Common Use in ML**                   | **Trade-offs**                               |\n+:=============================+:=============================+:=======================================+:=============================================+\n| **Trusted Execution**        | Isolated runtime environment | Secure inference and on-device privacy | Added complexity, memory limits, perf. cost  |\n|                              | for secure computation       | for sensitive inputs and outputs       |                                              |\n| **Environment (TEE)**        |                              |                                        | Requires trusted code development            |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Secure Boot**              | Verified boot sequence       | Ensures only signed ML models and      | Key management complexity, vendor lock-in    |\n|                              | and firmware validation      | firmware execute on embedded devices   | Performance impact during startup            |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Hardware Security Module** | Secure key generation and    | Signing ML models, securing training   | High cost, integration overhead, limited I/O |\n| **(HSM)**                    | storage, crypto-processing   | pipelines, verifying firmware          |                                              |\n|                              |                              |                                        | Requires infrastructure-level provisioning   |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Physical Unclonable**      | Hardware-bound identity      | Model binding, device authentication,  | Environmental sensitivity, modeling attacks  |\n|                              | and key derivation           | protecting IP in embedded deployments  |                                              |\n| **Function (PUF)**           |                              |                                        | Needs error correction and calibration       |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n\n: **Hardware Security Mechanisms**: Machine learning systems use diverse hardware defenses—trusted execution environments, secure boot, hardware security modules, and physical unclonable functions—to establish trust and protect sensitive data across the system stack. The table details how each mechanism addresses specific security challenges—from runtime isolation and integrity verification to key management and device identity—and emphasizes the associated trade-offs in performance and complexity. {#tbl-hw-security-comparison}\n\nTogether, these hardware primitives form the foundation of a defense-in-depth strategy for securing ML systems in adversarial environments. Their integration is especially important in domains that demand provable trust, such as autonomous vehicles, healthcare devices, federated learning systems, and important infrastructure.\n\n<!-- ### Toward Trustworthy Systems {#sec-security-privacy-toward-trustworthy-systems-1e6d}\n\nDefending machine learning systems against adversarial threats, misuse, and system compromise requires more than isolated countermeasures. As this section has shown, effective defense emerges from the careful integration of mechanisms at multiple layers of the ML stack—from privacy-preserving data handling and robust model design to runtime monitoring and hardware-enforced isolation. No single component can provide complete protection; instead, a trustworthy system is the result of coordinated design decisions that address risk across the data, model, system, and infrastructure layers.\n\nDefensive strategies must align with the deployment context and threat model. What is appropriate for a public cloud API may differ from the requirements of an embedded medical device or a fleet of edge-deployed sensors. Design choices must balance security, performance, and usability, recognizing that protections often introduce operational trade-offs. Monitoring and incident response mechanisms ensure resilience during live operation, while hardware-based roots of trust ensure system integrity even when higher layers are compromised.\n\nAs machine learning continues to expand into safety-important, privacy-sensitive, and decentralized environments, the need for robust, end-to-end defense becomes increasingly urgent. Building ML systems that are not only accurate, but secure, private, and auditable, is core to long-term deployment success and public trust.\n\nThe technical defenses we've established here form the foundation for broader robustness frameworks. While this chapter has focused on protecting against malicious attacks and privacy breaches, robust AI systems must extend these concepts to ensure system-wide reliability under all forms of stress—from natural distribution shifts to hardware failures. The adversarial training techniques introduced here for security become part of a comprehensive robustness strategy that includes uncertainty quantification, out-of-distribution detection, and graceful degradation. Similarly, the monitoring infrastructure we've established for security incident detection provides the foundation for the broader observability systems required for robust AI deployment.\n\nThese security and privacy foundations connect directly to operational practices for implementing protections at scale. The energy and computational overhead of security measures must be considered within sustainability frameworks, and the broader ethical implications connect to responsible AI practices explored throughout this volume.\n\nThe process of engineering trustworthy ML systems requires a structured approach that applies the layered defense principles established earlier to specific deployment contexts. @fig-trustworthy-ml-recipe provides a practical framework to guide this process across technical and deployment dimensions. The design flow begins with a thorough assessment of the threat model and deployment context, which informs the selection of appropriate defenses from our established stack. This includes data-layer protections such as differential privacy (DP), federated learning (FL), and encryption; model-layer defenses like robustness techniques, watermarking, and secure deployment practices; runtime-layer measures such as input validation and output monitoring; and hardware-layer solutions including TEEs, secure boot, and PUFs.\n\n::: {#fig-trustworthy-ml-recipe fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={BrownLine!60, -{Triangle[width = 6pt, length = 6pt]}, line width = 1.25pt},\nBox/.style={inner xsep=2pt,inner ysep=6pt,\n    node distance=0.5,\n    draw=RedLine,\n    line width=0.75pt,\n    fill=RedL!60,\n    align=flush center,\n    text width=37mm,\n    minimum width=37mm, minimum height=15.5mm\n  },\nBox2/.style={Box,draw=GreenD,fill=GreenL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2},\nBox4/.style={Box,draw=VioletLine,fill=VioletL2},\nBox5/.style={Box,draw=BlueLine,fill=BlueL!50},\nBox6/.style={Box,draw=OrangeLine,fill=OrangeL!50},\n}\n\\node[Box](B1){Data Layer: DP, FL, Encryption};\n\\node[Box,right=of B1,  text width=44mm,minimum width=44mm](B2){Model Layer: Robustness, Watermarking, Secure Deployment};\n\\node[Box,right=of B2](B3){Runtime Layer: Input Validation, Output Monitoring};\n\\node[Box,right=of B3](B4){Hardware Layer: TEEs, Secure Boot, PUFs};\n %\n\\node[Box2,above=0.7 of B2,minimum width=44mm](B22){Select Defenses Across the Stack};\n\\node[Box3,above=0.7 of B22,minimum width=44mm](B23){Assess Threat Model \\& Deployment Context};\n%\n\\node[Box6,below=0.7 of B4, text width=40mm,minimum width=44mm](B44){Plan for Runtime Adaptation and Recovery};\n\\node[Box5,below=0.7  of B44](B45){Rollback, Isolation, Incident Response};\n\\node[Box5,left=of B45](B46){Monitoring, Logging, Alerting};\n\\node[Box5,right=of B45](B47){Design Feedback\\\\ Loop};\n\\draw[Line](B23)--(B22);\n\\draw[Line](B22)--(B2);\n\\draw[Line](B22)-|(B1);\n\\draw[Line](B22)-|(B3);\n\\draw[Line](B22)-|(B4);\n\\draw[Line](B4)--(B44);\n\\draw[Line](B44)-|(B46);\n\\draw[Line](B44)-|(B47);\n\\draw[Line](B44)--(B45);\n\\end{tikzpicture}\n```\n\n**Trustworthy ML System Design**: Engineering secure and private machine learning systems requires a layered approach, integrating defenses at the data, model, runtime, and hardware levels to mitigate evolving threats and ensure responsible deployment. This design flow connects threat modeling with practical safeguards, enabling robust and auditable ML solutions for safety-important applications.\n:::\n\nThis design flow emphasizes the importance of a comprehensive approach to security, where each layer of the system is fortified against potential threats while remaining adaptable to evolving risks. By integrating these principles into the design and deployment of machine learning systems, organizations can build solutions that are not only effective but also resilient, trustworthy, and aligned with ethical standards.\n\n### Defense Selection Framework {#sec-security-privacy-defense-selection-framework-320b}\n\nGiven the breadth of security mechanisms presented, practitioners require systematic methods for selecting appropriate defenses. The choice depends on multiple interacting factors that must be evaluated holistically rather than in isolation.\n\nStep 1: Threat Model Assessment\n\nBegin by characterizing potential adversaries and their capabilities:\n\n- **Who are the adversaries?** Nation-states possess unlimited resources and advanced capabilities. Competitors seek intellectual property and strategic advantages. Curious users probe for vulnerabilities. Insider threats combine access with intent. Each adversary type demands different defensive priorities.\n\n- **What capabilities do they have?** White-box attackers with full model access require different defenses than black-box attackers limited to API queries. Physical access enables hardware attacks that remote adversaries cannot execute. Understanding attacker capabilities determines which defense mechanisms provide meaningful protection versus security theater.\n\n- **What assets require protection?** Training data privacy, model intellectual property, inference confidentiality, and system availability each require specialized defenses. Healthcare applications prioritize patient data protection (HIPAA compliance). Financial systems emphasize transaction integrity and fraud prevention. Ranking asset criticality guides defense investment.\n\n- **What is acceptable risk?** Regulatory environments define minimum acceptable security (GDPR, HIPAA, PCI-DSS). Reputational considerations establish higher bars—a data breach that is legally permissible may still be commercially devastating. Risk tolerance determines how much performance degradation and development cost organizations accept for security gains.\n\nStep 2: Deployment Context Constraints\n\nSecurity choices must respect operational realities:\n\n- **Computational Budget:** Cloud deployments afford unlimited horizontal scaling, enabling expensive techniques like homomorphic encryption or extensive monitoring. Edge devices operate under severe constraints—secure enclaves consume precious memory, cryptographic operations drain batteries. Embedded systems may lack security hardware entirely. Defense selection must match available compute resources.\n\n- **Latency Requirements:** Real-time applications (autonomous vehicles, industrial control) tolerate minimal latency overhead. Differential privacy and input validation must execute within millisecond budgets. Batch processing systems (training pipelines, offline analytics) can absorb expensive techniques like secure multi-party computation. Understanding latency budgets constrains feasible defenses.\n\n- **Update Frequency:** Continuously learning systems require runtime security that adapts as models evolve. Static deployments can rely on one-time hardening (model encryption, watermarking). Over-the-air update capabilities enable security patches but introduce new attack surfaces. Update patterns determine whether defenses must be dynamic or can be baked-in.\n\n- **Physical Security:** Data center deployments assume physical security and focus on logical defenses. Field-deployed devices face physical threats (tampering, extraction) requiring hardware security modules and tamper-evident packaging. Public-facing kiosks need different protections than secured facilities.\n\nStep 3: Regulatory and Compliance Requirements\n\nLegal mandates establish non-negotiable security baselines:\n\n- **GDPR** (EU): Mandates data minimization, purpose limitation, privacy by design. Differential privacy and federated learning help demonstrate compliance. Cross-border data transfer restrictions favor on-device processing.\n\n- **HIPAA** (Healthcare): Requires access controls, audit logging, encryption at rest and in transit. HSMs for key management and comprehensive logging become mandatory rather than optional.\n\n- **CCPA** (California): Establishes consumer privacy rights including data deletion. Systems must support cryptographic erasure and maintain data provenance.\n\n- **Industry Standards:** Payment Card Industry Data Security Standard (PCI-DSS), Federal Information Security Management Act (FISMA), and sector-specific regulations impose additional requirements. Non-compliance incurs financial penalties and operational restrictions.\n\nStep 4: Recommended Layered Approach\n\nRather than selecting individual defenses, deploy coordinated protection:\n\n1. Baseline Security (Universal): All ML systems require authentication, access control, encrypted communications (TLS), audit logging, and basic input validation. These foundational defenses cost little and prevent common attacks. Omitting them constitutes negligence.\n\n2. Domain-Specific Controls (Context-Dependent): Healthcare systems add differential privacy for training data and TEEs for inference. Financial systems deploy HSMs for cryptographic operations and extensive transaction monitoring. Public-facing APIs implement rate limiting and behavioral analysis.\n\n3. Layered Defenses (No Single Point of Failure): Assume each defense will be bypassed eventually. Differential privacy prevents training data extraction even if models are stolen. Input validation catches adversarial examples even if models lack robustness. Monitoring detects attacks that evade technical defenses. Redundancy ensures attack success requires compromising multiple independent layers.\n\n4. Validation Through Red-Team Exercises: Theoretical security assessments miss practical vulnerabilities. Hire adversarial experts to attempt realistic attacks. Document failures and iterate defenses. Treat security as a continuous improvement process rather than one-time implementation.\n\nStep 5: Trade-Off Optimization\n\nSecurity is never free. Quantify costs and benefits:\n\n- **Performance vs. Protection:** If differential privacy reduces accuracy by 5% but enables GDPR compliance, the trade-off may be mandatory. If homomorphic encryption adds 10 seconds of latency to millisecond-budget applications, alternative approaches are required.\n\n- **Development Time vs. Risk Mitigation:** Basic security (authentication, encryption) adds weeks to development. Advanced techniques (federated learning, secure enclaves) require months of specialized engineering. Prioritize defenses by risk reduction per engineering hour.\n\n- **Operational Overhead vs. Attack Detection:** Comprehensive monitoring adds 10-20% infrastructure cost. Intrusion detection systems generate false positives requiring investigation. Balance detection capabilities against operational burden.\n\nThe optimal security architecture emerges from systematically evaluating these factors rather than applying techniques prescriptively. Different deployment contexts demand different solutions—recognize this diversity and design accordingly. -->\n\n<!-- ### Defense Selection Framework {#sec-security-privacy-selecting-appropriate-defenses-decision-framework-491e-framework-a48f}\n\nTo support practical decision making, @tbl-defense-selection-framework maps common threat scenarios to appropriate defensive techniques, organized by deployment context. This framework synthesizes the strategies presented throughout this section into actionable patterns.\n\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Deployment Context**      | **Primary Threats**                 | **Recommended Defenses**                         | **Key Trade-offs**                            |\n+:============================+:====================================+:=================================================+:==============================================+\n| **Healthcare ML**           | Data leakage (HIPAA violation),     | - Differential Privacy (ε ≤ 4) for training      | 2-5% accuracy loss acceptable for compliance; |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Federated diagnostic**   | membership inference,               | - Federated Learning across hospitals            | 50-100ms inference latency from TEE overhead  |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **models)**                 | unauthorized access                 | - TEE for inference on sensitive data            |                                               |\n|                             |                                     | - Audit logging and access control (RBAC)        |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Financial ML**            | Model theft (IP loss),              | - Model encryption (AES-256) at rest             | HSM adds $10-50K capital cost; rate limiting  |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Fraud detection API)**   | adversarial evasion,                | - HSM for cryptographic key management           | may impact legitimate high-frequency users    |\n|                             | data poisoning                      | - Adversarial training (PGD-based)               |                                               |\n|                             |                                     | - Input validation + rate limiting (100 req/min) |                                               |\n|                             |                                     | - Output confidence monitoring                   |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Edge ML**                 | Physical access,                    | - Secure Boot (verified firmware)                | TEE memory limits constrain model size        |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Mobile/IoT devices)**    | side-channel attacks,               | - ARM TrustZone or similar TEE                   | (&lt;50MB); quantization required for large   |\n|                             | model extraction                    | - Model quantization + obfuscation               | models; 15-30% power overhead from encryption |\n|                             |                                     | - Encrypted model storage                        |                                               |\n|                             |                                     | - Anti-tampering hardware (PUF)                  |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Cloud ML Training**       | Data poisoning,                     | - Secure data pipelines (provenance tracking)    | Training time increases 30-120% with DP;      |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Multi-tenant platform)** | backdoor injection,                 | - Differential Privacy (DP-SGD, ε ≈ 1-10)        | gradient verification adds 10-15% compute     |\n|                             | gradient leakage                    | - Gradient verification and anomaly detection    | overhead; federated aggregation requires      |\n|                             |                                     | - Secure aggregation (if federated)              | secure communication protocols                |\n|                             |                                     | - Model watermarking for IP protection           |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Public-Facing LLM**       | Prompt injection,                   | - Input sanitization (prompt filtering)          | Aggressive filtering may block 5-10% of       |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Chatbot/API)**           | data extraction (training leakage), | - Output monitoring (PII detection)              | legitimate requests; response time increases  |\n|                             | abuse/overuse                       | - Rate limiting (per-user quotas)                | 50-100ms for content filtering; watermarking  |\n|                             |                                     | - Response watermarking                          | may be detectable by sophisticated users      |\n|                             |                                     | - Confidence thresholding (abstention)           |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Multi-Party ML**          | Data sharing restrictions,          | - Federated Learning (no raw data sharing)       | Communication overhead: 10-100x more rounds   |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Cross-organizational**   | honest-but-curious participants,    | - SMPC for secure aggregation                    | than centralized training; SMPC adds 1000x+   |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **training)**               | privacy compliance (GDPR)           | - Differential Privacy (ε ≤ 1)                   | compute cost; accuracy may degrade 5-15%;     |\n|                             |                                     | - Homomorphic Encryption (for sensitive ops)     | requires legal agreements for liability       |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Critical Infrastructure** | Supply chain compromise,            | - Hardware attestation (TPM/PUF)                 | Development cost: 6-18 months additional      |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Autonomous vehicles,**   | real-time adversarial attacks,      | - Secure Boot + runtime integrity checks         | engineering; 20-40% higher hardware costs;    |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **power grids)**            | safety-critical failures            | - Redundant model validation                     | latency constraints limit cryptographic       |\n|                             |                                     | - Fault injection detection                      | defenses; requires certified hardware         |\n|                             |                                     | - Fail-safe fallback mechanisms                  |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n\n: **Defense Selection Framework**: Maps deployment contexts to threat-specific defensive strategies with quantified trade-offs. The framework provides starting points for security architecture design, highlighting primary threats, recommended defense combinations, and key implementation trade-offs across common ML system deployment scenarios. Practitioners should adapt these patterns based on specific regulatory requirements, risk assessments, and operational constraints. {#tbl-defense-selection-framework}\n\nThis framework provides starting points rather than complete solutions. Real-world systems typically require combinations of defenses tailored to specific regulatory requirements, threat models, and operational constraints. Use these patterns as templates, adapting them based on risk assessments, compliance mandates, and resource availability. The baseline defenses identified in Step 1 (authentication, TLS, audit logging, input validation) apply universally and are assumed present in all deployment contexts above. -->\n\n<!-- ### Implementation Roadmap: Securing a Production ML System {#sec-security-privacy-implementation-roadmap-securing-production-ml-system-511c-securing-production-ml-system-511c}\n\nSecuring production machine learning systems requires a systematic approach that evolves from establishing basic protections to implementing comprehensive defense mechanisms. Organizations face the challenge of protecting valuable models and sensitive data while maintaining system performance and meeting regulatory requirements. The path from an unsecured ML deployment to a robust, trustworthy system follows a natural progression through four critical phases, each building upon the security foundations established by its predecessor. This roadmap provides the security-specific implementation details that complement broader operational frameworks, where these protections are integrated into continuous deployment pipelines, monitoring systems, and incident response workflows.\n\nThe journey begins with foundational security controls that provide immediate risk reduction across all ML deployments. Access control mechanisms form the first line of defense, determining who can interact with ML infrastructure and at what level. Organizations typically leverage existing identity providers like Active Directory or OAuth 2.0 to implement role-based access control, ensuring data scientists can train models while limiting production access to authorized deployment systems. Communication security follows, with TLS 1.3 or newer protocols protecting all ML API endpoints from eavesdropping and tampering. This encryption layer becomes particularly critical when models process sensitive data or operate across untrusted networks. Input validation and rate limiting at inference endpoints prevent both accidental misuse and intentional abuse, using API gateways to enforce schema validation and throttle excessive requests. Finally, comprehensive audit logging creates the forensic trail necessary for security monitoring and compliance reporting, capturing all model access, training operations, and data transformations in centralized systems like ELK stack or Splunk.\n\nOnce these foundations are established, typically within the first two weeks of implementation, organizations progress to data privacy protections that address regulatory requirements and ethical obligations. This phase begins with comprehensive data classification, cataloging all training and inference data sources to identify sensitive information such as personally identifiable information, protected health information, or financial records. For datasets containing sensitive information, differential privacy techniques add calibrated noise during training, with privacy budgets carefully selected based on regulatory requirements—typically ε ≤ 4 for general compliance and ε ≤ 1 for highly sensitive medical or financial data. When multiple organizations need to collaborate without sharing raw data, federated learning enables distributed training with secure aggregation protocols ensuring individual contributions remain private. Data minimization practices complement these technical measures by establishing retention policies that automatically delete sensitive training data after model completion, reducing the window of exposure for potential breaches.\n\nThe third phase shifts focus to protecting the models themselves as valuable intellectual property that may encode sensitive information or competitive advantages. Model encryption using AES-256 or stronger algorithms protects stored models from unauthorized access, with key management systems like AWS KMS or HashiCorp Vault ensuring proper key rotation and access control. For particularly sensitive applications, deployment within trusted execution environments provides hardware-enforced isolation, preventing even privileged system administrators from accessing model internals during inference. Organizations implement model watermarking techniques to embed undetectable ownership markers that survive model extraction attempts, while query monitoring systems detect suspicious patterns that might indicate ongoing theft attempts. Critical applications require adversarial robustness through specialized training techniques, using methods like Projected Gradient Descent to create models that maintain accuracy even when facing malicious inputs designed to cause misclassification.\n\nThe final phase establishes continuous monitoring and response capabilities that maintain security throughout the system's operational lifetime. Real-time anomaly detection systems monitor inference patterns, input distributions, and model confidence scores to identify potential attacks or system degradation. When threats are detected, automated response systems implement immediate countermeasures such as rate limiting suspicious users, isolating potentially compromised models, or escalating alerts to security teams. Performance monitoring ensures security measures don't degrade business operations, tracking metrics like inference latency and model accuracy to maintain the delicate balance between protection and utility. Comprehensive incident response procedures specific to ML systems enable rapid reaction to security events, including capabilities for model rollback to known-good versions, forensic data collection for post-incident analysis, and threat intelligence gathering to prevent future attacks.\n\nSuccess metrics should guide implementation effectiveness: (1) Zero security incidents involving data leakage or model theft; (2) <5% model performance degradation from security measures; (3) 100% compliance with relevant regulations (HIPAA, GDPR, SOX); (4) <100ms additional latency from security overhead.\n\nImplementation prioritization becomes critical for resource-constrained organizations. Start with high-impact, low-effort baseline defenses (TLS, authentication, logging) before progressing to specialized techniques. Focus on regulatory compliance requirements first (HIPAA differential privacy, GDPR data minimization) as these often have legal deadlines. Deploy privacy-preserving techniques based on data sensitivity—use differential privacy for PII/PHI, federated learning for cross-organizational collaboration, and synthetic data for lower-sensitivity use cases.\n\n::: {.callout-warning title=\"Troubleshooting Guide: Common ML Security Issues\" icon=false}\nIssue 1: Model Performance Degradation After Security Implementation\n\n*Symptoms*: Model accuracy drops significantly after applying differential privacy, adversarial training, or encryption.\n\n*Root Causes*:\n\n- Excessive noise injection (ε too small in differential privacy)\n- Insufficient adversarial training budget\n- Inappropriate cryptographic operations breaking model computation\n\n*Diagnostics*:\n```python\n# Check DP noise impact\ndef measure_privacy_utility_tradeoff(model, data, epsilon_values):\n    results = {}\n    for eps in epsilon_values:\n        dp_model = apply_differential_privacy(model, epsilon=eps)\n        accuracy = evaluate_model(dp_model, data)\n        results[eps] = accuracy\n    return results\n\n\n# Test different epsilon values: [0.1, 1.0, 4.0, 8.0]\n# Look for sweet spot between privacy and utility\n```\n\n*Solutions*:\n\n- **DP**: Start with ε = 8, gradually decrease while monitoring accuracy\n- **Adversarial Training**: Use mixed training (70% clean, 30% adversarial examples)\n- **Encryption**: Use approximate methods (quantization-friendly HE) for critical operations only\n\nIssue 2: High Latency from Security Overhead\n\n*Symptoms*: Inference time increases dramatically with TEEs, encryption, or secure protocols.\n\n*Debugging Steps*:\n1. Profile each security component separately\n2. Identify bottlenecks (network, crypto, context switching)\n3. Measure baseline vs. secured performance\n\n*Optimization Strategies*:\n\n- **TEE Optimization**: Batch inference calls, minimize context switches\n- **Encryption**: Use hardware acceleration (AES-NI), optimize key operations\n- **Network Security**: Pipeline TLS handshakes, use connection pooling\n\nIssue 3: Adversarial Attack Detection False Positives\n\n*Symptoms*: Legitimate inputs flagged as adversarial examples, causing service disruption.\n\n*Tuning Approach*:\n```python\n# Adjust detection thresholds\ndef calibrate_adversarial_detector(detector, clean_data, attack_data):\n    # Find threshold balancing false positive rate vs.\n    # detection rate\n    thresholds = np.linspace(0.1, 0.9, 20)\n    for threshold in thresholds:\n        fp_rate = compute_false_positive_rate(\n            detector, clean_data, threshold\n        )\n        detection_rate = compute_detection_rate(\n            detector, attack_data, threshold\n        )\n        print(\n            f\"Threshold {threshold}: FP={fp_rate:.3f}, \"\n            + f\"Detection={detection_rate:.3f}\"\n        )\n```\n\n*Best Practices*:\n\n- Set different thresholds for different input types\n- Implement human-in-the-loop for borderline cases\n- Use ensemble detection methods for robustness\n\nIssue 4: Privacy Budget Exhaustion in Production\n\n*Symptoms*: Cannot retrain models due to depleted privacy budget.\n\n*Management Strategy*:\n\n- Implement privacy budget monitoring and alerting\n- Reserve budget for emergency retraining (20% buffer)\n- Use synthetic data for non-critical model updates\n- Plan budget allocation across model lifecycle\n\nIssue 5: Federated Learning Convergence Problems\n\n*Symptoms*: Models fail to converge or converge slowly in federated settings.\n\n*Diagnostic Checklist*:\n\n- Check data distribution across clients (non-IID data)\n- Verify secure aggregation isn't corrupting gradients\n- Monitor client participation rates and dropouts\n- Analyze communication rounds and bandwidth usage\n\n*Solutions*:\n\n- Implement FedProx or FedAvgM for non-IID data\n- Use client sampling strategies to ensure representative participation\n- Add gradient compression to reduce communication overhead\n- Implement client reliability scoring and failover mechanisms\n::: -->\n\n## Practical Implementation Roadmap {#sec-security-privacy-practical-roadmap-8f3a}\n\nThe comprehensive security and privacy techniques covered in this chapter can seem overwhelming for organizations just beginning to secure their ML systems. Rather than implementing every defense simultaneously, a phased approach enables systematic security improvements while managing complexity and costs. This roadmap provides a practical sequence for building robust ML security, progressing from foundational controls to advanced defenses.\n\n### Phase 1: Foundation Security Controls {#sec-security-privacy-phase1-baseline-foundation-2d9c}\n\nBegin with basic security controls that provide the greatest risk reduction for the least complexity. These foundational measures address the most common attack vectors and create the trust infrastructure needed for more advanced defenses.\n\n- **Access Control and Authentication**: Implement role-based access control (RBAC) for all ML system components, including training data, model repositories, and inference APIs. Use multi-factor authentication for administrative access and service-to-service authentication with short-lived tokens. Establish the principle of least privilege, ensuring users and services have only the minimum permissions required for their functions.\n\n- **Data Protection**: Encrypt all data at rest using AES-256 and enforce TLS 1.3 for all data in transit. This includes training datasets, model files, and inference communications. Implement comprehensive logging of all data access and model operations to support incident investigation and compliance auditing.\n\n- **Input Validation and Basic Monitoring**: Deploy input validation for all ML APIs to reject malformed requests, implement rate limiting to prevent abuse, and establish baseline monitoring for unusual inference patterns. These measures protect against basic adversarial inputs and provide visibility into system behavior.\n\n- **Secure Development Practices**: Establish secure coding practices for ML pipelines, including dependency management with vulnerability scanning, secure model serialization that validates model integrity, and automated security testing in deployment pipelines.\n\n### Phase 2: Privacy Controls and Model Protection {#sec-security-privacy-phase2-privacy-model-protection-7a8b}\n\nWith foundational controls in place, focus on protecting sensitive data and securing your ML models from theft and manipulation. This phase addresses privacy regulations and intellectual property protection.\n\n- **Privacy-Preserving Techniques**: Implement data anonymization for non-sensitive use cases and differential privacy for scenarios requiring formal privacy guarantees. For collaborative learning scenarios, deploy federated learning architectures that keep sensitive data localized while enabling model improvement.\n\n- **Model Security**: Protect deployed models through encryption of model files, secure API design that limits information leakage, and monitoring for model extraction attempts. Implement model versioning and integrity checking to detect unauthorized modifications.\n\n- **Secure Training Infrastructure**: Harden training environments by isolating training workloads, implementing secure data pipelines with validation and provenance tracking, and establishing secure model registries with access controls and audit trails.\n\n- **Compliance Integration**: Implement necessary controls for regulatory requirements such as GDPR, HIPAA, or industry-specific standards. This includes data subject rights management, privacy impact assessments, and documentation of data processing activities.\n\n### Phase 3: Advanced Threat Defense {#sec-security-privacy-phase3-advanced-defenses-runtime-8c2d}\n\nThe final phase implements sophisticated defenses for high-stakes environments where advanced adversaries pose significant threats. These defenses require more expertise and resources but provide protection against state-of-the-art attacks.\n\n- **Adversarial Robustness**: Deploy adversarial training to improve model robustness against evasion attacks, implement certified defenses for safety-critical applications, and establish continuous testing against new adversarial techniques.\n\n- **Advanced Runtime Monitoring**: Deploy ML-specific anomaly detection systems that can identify sophisticated attacks like data poisoning effects or gradual model degradation. Implement behavioral analysis that establishes normal operation baselines and alerts on deviations.\n\n- **Hardware-Based Security**: For the highest security requirements, implement trusted execution environments (TEEs) for model inference, secure boot processes for edge devices, and hardware security modules (HSMs) for cryptographic key management.\n\n- **Incident Response and Recovery**: Establish ML-specific incident response procedures, including model rollback capabilities, contaminated data isolation procedures, and forensic analysis techniques for ML-specific attacks.\n\n### Implementation Considerations {#sec-security-privacy-implementation-considerations-9f4e}\n\nSuccess with this roadmap requires balancing security improvements with operational capabilities. Each phase should be fully implemented and stabilized before progressing to the next level. Organizations should customize this sequence based on their specific threat model: healthcare systems may prioritize Phase 2 privacy controls, financial institutions may emphasize Phase 1 data protection, and autonomous vehicle systems may fast-track to Phase 3 adversarial robustness.\n\nResource allocation should account for the increasing technical complexity and operational overhead of advanced phases. Phase 1 controls typically require standard IT security expertise, while Phase 3 defenses may require specialized ML security knowledge or external consulting. Organizations should invest in training and hiring appropriate expertise before implementing advanced controls.\n\nRegular security assessments should validate the effectiveness of implemented controls and guide progression through phases. These assessments should include penetration testing of ML-specific attack vectors, red team exercises that simulate realistic adversarial scenarios, and compliance audits that verify regulatory requirements are met effectively.\n\n## Fallacies and Pitfalls {#sec-security-privacy-fallacies-pitfalls-0c20}\n\nHaving examined both defensive and offensive capabilities, we now address common misconceptions that can undermine security efforts. Security and privacy in machine learning systems present unique challenges that extend beyond traditional cybersecurity concerns, involving sophisticated attacks on data, models, and inference processes. The complexity of modern ML pipelines, combined with the probabilistic nature of machine learning and the sensitivity of training data, creates numerous opportunities for misconceptions about effective protection strategies.\n\n**Fallacy:** _Security through obscurity provides adequate protection for machine learning models._\n\nThis outdated approach assumes that hiding model architectures, parameters, or implementation details provides meaningful security protection. Modern attacks often succeed without requiring detailed knowledge of target systems, relying instead on black-box techniques that probe system behavior through input-output relationships. Model extraction attacks can reconstruct significant model functionality through carefully designed queries, while adversarial attacks often transfer across different architectures. Effective ML security requires robust defenses that function even when attackers have complete knowledge of the system, following established security principles rather than relying on secrecy.\n\n**Pitfall:** _Assuming that differential privacy automatically ensures privacy without considering implementation details._\n\nMany practitioners treat differential privacy as a universal privacy solution without understanding the critical importance of proper implementation and parameter selection. Poorly configured privacy budgets can provide negligible protection while severely degrading model utility. Implementation vulnerabilities like floating-point precision issues, inadequate noise generation, or privacy budget exhaustion can completely compromise privacy guarantees. Real-world systems require careful analysis of privacy parameters, rigorous implementation validation, and ongoing monitoring to ensure that theoretical privacy guarantees translate to practical protection.\n\n**Fallacy:** _Federated learning inherently provides privacy protection without additional safeguards._\n\nA related privacy misconception assumes that keeping data decentralized automatically ensures privacy protection. While federated learning improves privacy compared to centralized training, gradient and model updates can still leak significant information about local training data through inference attacks. Sophisticated adversaries can reconstruct training examples, infer membership information, or extract sensitive attributes from shared model parameters. True privacy protection in federated settings requires additional mechanisms like secure aggregation, differential privacy, and careful communication protocols rather than relying solely on data locality.\n\n**Pitfall:** _Treating security as an isolated component rather than a system-wide property._\n\nBeyond specific technical misconceptions, a key architectural pitfall involves organizations that approach ML security by adding security features to individual components without considering system-level interactions and attack vectors. This piecemeal approach fails to address sophisticated attacks that span multiple components or exploit interfaces between subsystems. Effective ML security requires holistic threat modeling that considers the entire system lifecycle from data collection through model deployment and maintenance, following the threat prioritization principles established in @sec-security-privacy-threat-prioritization-framework-f2d5. Security must be integrated into every stage of the ML pipeline rather than treated as an add-on feature or post-deployment consideration.\n\n**Pitfall:** _Underestimating the attack surface expansion in distributed ML systems._\n\nMany organizations focus on securing individual components without recognizing how distributed ML architectures increase the attack surface and introduce new vulnerability classes. Distributed training across multiple data centers creates opportunities for man-in-the-middle attacks on gradient exchanges, certificate spoofing, and unauthorized participation in training rounds. Edge deployment multiplies endpoints that require security updates, monitoring, and incident response capabilities. Model serving infrastructure spanning multiple clouds introduces dependency chain attacks, where compromising any component in the distributed system can affect overall security. Orchestration systems, load balancers, model registries, and monitoring infrastructure each present potential entry points for sophisticated attackers. Effective distributed ML security requires thorough threat modeling that accounts for network communication security, endpoint hardening, identity management across multiple domains, and coordination of security policies across heterogeneous infrastructure components.\n\n## Summary {#sec-security-privacy-summary-831c}\n\nThis chapter has explored the complex landscape of security and privacy in machine learning systems, from core threats to comprehensive defense strategies. Security and privacy represent essential requirements for deploying machine learning systems in production environments. As these systems handle sensitive data, operate across diverse platforms, and face sophisticated threats, their security posture must encompass the entire technology stack. Modern machine learning systems encounter attack vectors ranging from data poisoning and model extraction to adversarial examples and hardware-level compromises that can undermine system integrity and user trust.\n\nEffective security strategies employ defense-in-depth approaches that operate across multiple layers of the system architecture. Privacy-preserving techniques like differential privacy and federated learning protect sensitive data while enabling model training. Robust model design incorporates adversarial training and input validation to resist manipulation. Hardware security features provide trusted execution environments and secure boot processes. Runtime monitoring detects anomalous behavior and potential attacks during operation. These complementary defenses create resilient systems that can withstand coordinated attacks across multiple attack surfaces.\n\n:::: {.callout-important title=\"Key Takeaways\"}\n\n* Security and privacy must be integrated from initial architecture design rather than added as afterthoughts to ML systems\n* ML systems face threats across three categories: model confidentiality (theft), training integrity (poisoning), and inference robustness (adversarial attacks)\n* Historical security patterns (supply chain compromise, insufficient isolation, weaponized endpoints) amplify in ML contexts due to autonomous decision-making capabilities\n* Effective defense requires layered protection spanning data privacy, model security, runtime monitoring, and hardware trust anchors\n* Context drives defense selection: healthcare prioritizes regulatory compliance, autonomous vehicles prioritize adversarial robustness, financial systems prioritize model theft prevention\n* Privacy-preserving techniques include differential privacy, federated learning, homomorphic encryption, and synthetic data generation, each with distinct trade-offs\n* Hardware security mechanisms (TEEs, secure boot, HSMs, PUFs) provide foundational trust for software-level protections\n* Security introduces inevitable trade-offs in computational cost, accuracy degradation, and implementation complexity that must be balanced against protection benefits\n\n:::\n\nLooking forward, the security and privacy foundations established in this chapter form critical building blocks for the comprehensive robustness framework explored in @sec-robust-ai. While we've focused on defending against malicious actors and protecting sensitive information, true system reliability requires expanding these concepts to handle all forms of operational stress. The monitoring infrastructure, defensive mechanisms, and layered architectures we've developed here provide the foundation for detecting distribution shifts, managing uncertainty, and ensuring graceful degradation under adverse conditions—topics that will be central to our exploration of robust AI.\n","srcMarkdownNoYaml":"\n\n# Security & Privacy {#sec-security-privacy}\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: An illustration on privacy and security in machine learning systems. The image shows a digital landscape with a network of interconnected nodes and data streams, symbolizing machine learning algorithms. In the foreground, there's a large lock superimposed over the network, representing privacy and security. The lock is semi-transparent, allowing the underlying network to be partially visible. The background features binary code and digital encryption symbols, emphasizing the theme of cybersecurity. The color scheme is a mix of blues, greens, and grays, suggesting a high-tech, digital environment._\n:::\n\n\\noindent\n![](images/png/cover_security_privacy.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why do privacy and security determine whether machine learning systems achieve widespread adoption and societal trust?_\n\nMachine learning systems require unprecedented access to personal data, institutional knowledge, and behavioral patterns to function effectively, creating tension between utility and protection that determines societal acceptance. Unlike traditional software that processes data transiently, ML systems learn from sensitive information and embed patterns into persistent models that can inadvertently reveal private details. This capability creates systemic risks extending beyond individual privacy violations to threaten institutional trust, competitive advantages, and democratic governance. Success of machine learning deployment across critical domains including healthcare, finance, education, and public services depends on establishing robust security and privacy foundations enabling beneficial use while preventing harmful exposure. Without these protections, even the most capable systems remain unused due to legal, ethical, and practical concerns. Understanding privacy and security principles enables engineers to design systems achieving both technical excellence and societal acceptance.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Differentiate between security and privacy in ML systems using formal definitions, threat models, and representative attack scenarios\n\n- Extract generalizable security principles from historical breaches (Stuxnet, Jeep Cherokee, Mirai) and apply them to distributed ML infrastructure vulnerabilities\n\n- Categorize ML-specific threats across the complete attack surface: model theft, data poisoning, adversarial examples, and hardware vulnerabilities\n\n- Implement privacy-preserving techniques (differential privacy, federated learning, synthetic data generation) with quantitative understanding of their mathematical guarantees and computational trade-offs\n\n- Analyze the layered defense architecture integrating data protection, model security, runtime monitoring, and hardware trust mechanisms for distributed ML deployments\n\n- Evaluate defense strategies using quantitative cost-benefit analysis including computational overhead, accuracy degradation, and implementation complexity\n\n- Design context-appropriate security architectures by applying the three-phase implementation roadmap prioritized for specific threat models and organizational constraints\n\n- Assess distributed systems attack surface expansion including gradient exchange vulnerabilities, edge endpoint multiplication, and multi-cloud dependency chains\n\n:::\n\n## Security and Privacy in ML Systems {#sec-security-privacy-security-privacy-ml-systems-0b1e}\n\nThe shift from centralized training architectures to distributed, adaptive machine learning systems has altered the threat landscape and security requirements for modern ML infrastructure. Contemporary machine learning systems, as examined in @sec-edge-intelligence, increasingly operate across heterogeneous computational environments spanning edge devices, federated networks, and hybrid cloud deployments. This architectural evolution enables new capabilities in adaptive intelligence but introduces attack vectors and privacy vulnerabilities that traditional cybersecurity frameworks cannot adequately address.\n\nMachine learning systems exhibit different security characteristics compared to conventional software applications. Traditional software systems process data transiently and deterministically, whereas machine learning systems extract and encode patterns from training data into persistent model parameters. This learned knowledge representation creates unique vulnerabilities where sensitive information can be inadvertently memorized and later exposed through model outputs or systematic interrogation. Such risks manifest across domains from healthcare systems that may leak patient information to proprietary models that can be reverse-engineered through strategic query patterns, threatening both individual privacy and organizational intellectual property.\n\nThe architectural complexity of machine learning systems compounds these security challenges through multi-layered attack surfaces. Contemporary ML deployments include data ingestion pipelines, distributed training infrastructure, model serving systems, and continuous monitoring frameworks. Each architectural component introduces distinct vulnerabilities while privacy concerns affect the entire computational stack. The distributed nature of modern deployments, with continuous adaptation at edge nodes and federated coordination protocols, expands the attack surface while complicating comprehensive security implementation.\n\nAddressing these challenges requires systematic approaches that integrate security and privacy considerations throughout the machine learning system lifecycle. This chapter establishes the foundations and methodologies necessary for engineering ML systems that achieve both computational effectiveness and trustworthy operation. We examine the application of established security principles to machine learning contexts, identify threat models specific to learning systems, and present comprehensive defense strategies that include data protection mechanisms, secure model architectures, and hardware-based security implementations.\n\nOur investigation proceeds through four interconnected frameworks. We begin by establishing distinctions between security and privacy within machine learning contexts, then examine evidence from historical security incidents to inform contemporary threat assessment. We analyze vulnerabilities that emerge from the learning process itself, before presenting layered defense architectures that span cryptographic data protection, adversarial-robust model design, and hardware security mechanisms. Throughout this analysis, we emphasize implementation guidance that enables practitioners to develop systems meeting both technical performance requirements and the trust standards necessary for societal deployment.\n\n## Foundational Concepts and Definitions {#sec-security-privacy-foundational-concepts-definitions-d529}\n\nSecurity and privacy are core concerns in machine learning system design, but they are often misunderstood or conflated. Both aim to protect systems and data, yet they do so in different ways, address different threat models, and require distinct technical responses. For ML systems, distinguishing between the two helps guide the design of robust and responsible infrastructure.\n\n### Security Defined {#sec-security-privacy-security-defined-1129}\n\nSecurity in machine learning focuses on defending systems from adversarial behavior. This includes protecting model parameters, training pipelines, deployment infrastructure, and data access pathways from manipulation or misuse.\n\n:::{.callout-definition title=\"Security\"}\n***Security*** is the protection of ML system _data_, _models_, and _infrastructure_ from _unauthorized access_, _manipulation_, and _disruption_ through _defensive mechanisms_ spanning development, deployment, and operational environments.\n:::\n\n*Example*: A facial recognition system deployed in public transit infrastructure may be targeted with adversarial inputs that cause it to misidentify individuals or fail entirely. This is a runtime security vulnerability that threatens both accuracy and system availability.\n\n### Privacy Defined {#sec-security-privacy-privacy-defined-da84}\n\nWhile security addresses adversarial threats, privacy focuses on limiting the exposure and misuse of sensitive information within ML systems. This includes protecting training data, inference inputs, and model outputs from leaking personal or proprietary information, even when systems operate correctly and no explicit attack is taking place.\n\n:::{.callout-definition title=\"Privacy\"}\n***Privacy*** is the protection of _sensitive information_ from _unauthorized disclosure_, _inference_, and _misuse_ through methods that preserve _confidentiality_ and _control over data usage_ across ML system environments.\n:::\n\n*Example*: A language model trained on medical transcripts may inadvertently memorize snippets of patient conversations. If a user later triggers this content through a public-facing chatbot, it represents a privacy failure, even in the absence of an attacker.\n\n### Security versus Privacy {#sec-security-privacy-security-versus-privacy-e0b8}\n\nAlthough they intersect in some areas (encrypted storage supports both), security and privacy differ in their objectives, threat models, and typical mitigation strategies. @tbl-security-privacy-comparison below summarizes these distinctions in the context of machine learning systems.\n\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Aspect**                  | **Security**                               | **Privacy**                                   |\n+:============================+:===========================================+:==============================================+\n| **Primary Goal**            | Prevent unauthorized access or disruption  | Limit exposure of sensitive information       |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Threat Model**            | Adversarial actors (external or internal)  | Honest-but-curious observers or passive leaks |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Typical Concerns**        | Model theft, poisoning, evasion attacks    | Data leakage, re-identification, memorization |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Example Attack**          | Adversarial inputs cause misclassification | Model inversion reveals training data         |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Representative Defenses** | Access control, adversarial training       | Differential privacy, federated learning      |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n| **Relevance to Regulation** | Emphasized in cybersecurity standards      | Central to data protection laws (e.g., GDPR)  |\n+-----------------------------+--------------------------------------------+-----------------------------------------------+\n\n: **Security-Privacy Distinctions**: Machine learning systems require distinct approaches to security and privacy; security mitigates adversarial threats targeting system functionality, while privacy protects sensitive information from both intentional and unintentional exposure through data leakage or re-identification. This table clarifies how differing goals and threat models shape the specific concerns and mitigation strategies for each domain. {#tbl-security-privacy-comparison}\n\n### Security-Privacy Interactions and Trade-offs {#sec-security-privacy-securityprivacy-interactions-tradeoffs-d153}\n\nSecurity and privacy are deeply interrelated but not interchangeable. A secure system helps maintain privacy by restricting unauthorized access to models and data. Privacy-preserving designs can improve security by reducing the attack surface, for example, minimizing the retention of sensitive data reduces the risk of exposure if a system is compromised.\n\nHowever, they can also be in tension. Techniques like differential privacy[^fn-dp-origins] reduce memorization risks but may lower model utility. Similarly, encryption enhances security but may obscure transparency and auditability, complicating privacy compliance. In machine learning systems, designers must reason about these trade-offs holistically. Systems that serve sensitive domains, including healthcare, finance, and public safety, must simultaneously protect against both misuse (security) and overexposure (privacy). Understanding the boundaries between these concerns is essential for building systems that are performant, trustworthy, and legally compliant.\n\n[^fn-dp-origins]: **Differential Privacy Origins**: Cynthia Dwork coined the term differential privacy at Microsoft Research in 2006, but the concept emerged from her frustration with the \"anonymization myth\" (the false belief that removing names from data guaranteed privacy). Her groundbreaking insight was that privacy should be mathematically provable, not just plausible, leading to the rigorous framework that now protects billions of users' data in products from Apple to Google.\n\n## Learning from Security Breaches {#sec-security-privacy-learning-security-breaches-6719}\n\nHaving established the conceptual foundations of security and privacy, we now examine how these principles manifest in real-world systems through landmark security incidents. These historical cases provide concrete illustrations of the abstract concepts we've defined, showing how security vulnerabilities emerge and propagate through complex systems. More importantly, they reveal universal patterns (supply chain compromise, insufficient isolation, and weaponized endpoints) that directly apply to modern machine learning deployments.\n\nValuable lessons can be drawn from well-known security breaches across a range of computing systems. Understanding how these patterns apply to modern ML deployments, which increasingly operate across cloud, edge, and embedded environments, provides important lessons for securing machine learning systems. These incidents demonstrate how weaknesses in system design can lead to widespread, and sometimes physical, consequences. Although the examples discussed in this section do not all involve machine learning directly, they provide important insights into designing secure systems. These lessons apply to machine learning applications deployed across cloud, edge, and embedded environments.\n\n### Supply Chain Compromise: Stuxnet {#sec-security-privacy-supply-chain-compromise-stuxnet-8a4b}\n\nIn 2010, security researchers discovered a highly sophisticated computer worm later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stuxnet]. Stuxnet exploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through networked and isolated systems.\n\n[^fn-stuxnet-discovery]: **Stuxnet Discovery**: Stuxnet was first detected by VirusBokNok, a small Belarusian antivirus company, when their client computers began crashing unexpectedly. What seemed like a routine malware investigation turned into one of the most significant cybersecurity discoveries in history: the first confirmed cyberweapon designed to cause physical destruction.\n\n[^fn-zero-day-term]: **Zero-Day Term Origin**: The term \"zero-day\" originated in software piracy circles, referring to the \"zero days\" since a program's release when pirated copies appeared. In security, it describes the \"zero days\" defenders have to patch a vulnerability before attackers exploit it, representing the ultimate race between attack and defense.\n\nUnlike typical malware designed to steal information or perform espionage, Stuxnet was engineered to cause physical damage. Its objective was to disrupt uranium enrichment by sabotaging the centrifuges used in the process. Despite the facility being air-gapped[^fn-air-gapped] from external networks, the malware is believed to have entered the system via an infected USB device[^fn-usb-attacks], demonstrating how physical access can compromise isolated environments.\n\nThe worm specifically targeted programmable logic controllers (PLCs), industrial computers that automate electromechanical processes such as controlling the speed of centrifuges. By exploiting vulnerabilities in the Windows operating system and the Siemens Step7 software used to program the PLCs, Stuxnet achieved highly targeted, real-world disruption. This represents a landmark in cybersecurity, demonstrating how malicious software can bridge the digital and physical worlds to manipulate industrial infrastructure.\n\n[^fn-air-gapped]: **Air-Gapped Systems**: Air-gapped systems are networks physically isolated from external connections, originally developed for military systems in the 1960s. Despite seeming impenetrable, studies show 90% of air-gapped systems can be breached through supply chain compromise, infected removable media, or hidden channels (acoustic, electromagnetic, thermal) [@farwell2011stuxnet].\n\n[^fn-usb-attacks]: **USB Attacks**: USB interfaces, introduced in 1996, became a primary attack vector for crossing air gaps. The 2008 Operation Olympic Games reportedly used infected USB drives to penetrate secure facilities, with some estimates suggesting 60% of organizations remain vulnerable to USB-based attacks [@farwell2011stuxnet].\n\nThe lessons from Stuxnet directly apply to modern ML systems. Training pipelines and model repositories face persistent supply chain risks analogous to those exploited by Stuxnet. Just as Stuxnet compromised industrial systems through infected USB devices and software vulnerabilities, modern ML systems face multiple attack vectors: compromised dependencies (malicious packages in PyPI/conda repositories), malicious training data (poisoned datasets on HuggingFace, Kaggle), backdoored model weights (trojan models in model repositories), and tampered hardware drivers (compromised NVIDIA CUDA libraries, firmware backdoors in AI accelerators).\n\nA concrete ML attack scenario illustrates these risks: an attacker uploads a backdoored image classification model to a popular model repository, trained to misclassify specific patterns while maintaining normal accuracy on clean data. When deployed in autonomous vehicles, this backdoored model correctly identifies most objects but fails to detect pedestrians wearing specific patterns, creating safety risks. The attack propagates through automated model deployment pipelines, affecting thousands of vehicles before detection.\n\nDefending against such supply chain attacks requires end-to-end security measures: (1) cryptographic verification to sign all model artifacts, datasets, and dependencies with cryptographic signatures; (2) provenance tracking to maintain immutable logs of all training data sources, code versions, and infrastructure used; (3) integrity validation to implement automated scanning for model backdoors, dependency vulnerabilities, and dataset poisoning before deployment; (4) air-gapped training to isolate sensitive model training in secure environments with controlled dependency management. @fig-stuxnet illustrates how these supply chain compromise patterns apply across both industrial and ML systems.\n\n::: {#fig-stuxnet fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line cap=round,line join=round,font=\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nTxtL/.style = {font=\\large\\usefont{T1}{phv}{m}{n},text width=90mm,align=justify,anchor=north},\nLine/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},\nLineA/.style={violet!50,line width=2.0pt,{-{Triangle[width=1.1*6pt,length=2.0*6pt]}},shorten <=3pt,shorten >=2pt},\nALine/.style={black!50, line width=1.1pt,{{Triangle[width=1.1*6pt,length=2*6pt]}-}},\nLarrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,\n            single arrow head indent=0pt,minimum height=9mm, minimum width=15pt}\n}\n%Skull\n\\tikzset{pics/skull/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=SKULL,scale=\\scalefac, every node/.append style={transform shape}]\n\\fill[fill=\\filllcolor](-0.225,-0.05)to[out=110,in=230](-0.215,0.2)to[out=50,in=180](0,0.315)\nto[out=0,in=130](0.218,0.2)to[out=310,in=70](0.227,-0.05) to[out=320,in=40](0.21,-0.15)\nto[out=210,in=80](0.14,-0.23) to[out=260,in=20](0.04,-0.285) to[out=200,in=340](-0.07,-0.28)\nto[out=170,in=290](-0.135,-0.23) to[out=110,in=340](-0.21,-0.15) to[out=140,in=250]cycle;\n%eyes\n\\fill[fill=\\filllcirclecolor](-0.17,-0.02)to[out=70,in=110](-0.029,-0.02)to[out=280,in=0](-0.129,-0.11)to[out=190,in=250]cycle;\n\\fill[fill=\\filllcirclecolor](0.035,-0.02)to[out=70,in=110](0.175,-0.02)to[out=300,in=340](0.12,-0.103)to[out=170,in=260]cycle;\n%nose\n\\fill[fill=\\filllcirclecolor](0.018,-0.115)to[out=70,in=110](-0.014,-0.115)to(-0.043,-0.165)\nto[out=200,in=170](-0.025,-0.19)to(0.027,-0.19)to[out=10,in=330](0.047,-0.165)to cycle;\n%above left\n\\fill[fill=\\filllcolor](-0.2,0.18)to[out=160,in=320](-0.3,0.23)to[out=140,in=0](-0.37,0.295)\nto[out=180,in=80](-0.43,0.25)to[out=230,in=90](-0.475,0.19)\nto[out=260,in=170](-0.375,0.13)to[out=350,in=170](-0.2,0.1)to cycle;\n%abover right\n\\fill[fill=\\filllcolor](0.2,0.18)to[out=20,in=220](0.3,0.23)to[out=40,in=200](0.37,0.295)\nto[out=20,in=90](0.43,0.25)to[out=230,in=90](0.475,0.19)to[out=260,in=360](0.375,0.13)\nto[out=190,in=10](0.2,0.1)to cycle;\n%below left\n\\fill[fill=\\filllcolor](-0.2,0.03)to[out=210,in=0](-0.3,0.01)to[out=180,in=0](-0.37,0.01)\nto[out=180,in=50](-0.46,0.0)to[out=230,in=120](-0.445,-0.08)\nto[out=260,in=170](-0.41,-0.14)to[out=350,in=190](-0.2,-0.051)to cycle;\n%below right\n\\fill[fill=\\filllcolor](0.2,0.03)to[out=340,in=170](0.3,0.01)to[out=350,in=190](0.37,0.01)\nto[out=20,in=110](0.47,-0.03)to[out=270,in=120](0.443,-0.09)\nto[out=270,in=0](0.36,-0.15)to[out=160,in=340](0.2,-0.051)to cycle;\n\\end{scope}\n     }\n  }\n}\n%laptop\n\\tikzset{\npics/laptop/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\node[rounded corners=2pt,rectangle,minimum width=60,minimum height=37,\nfill=\\filllcolor!60,line width=\\Linewidth,draw=black](EKV\\picname)at(0,0.53){};\n%\n\\ifnum\\Dual=1\n\\node[draw=black,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,\nfill=\\filllcolor!10,line width=\\Linewidth,](EK)at(0,0.53){};\n\\coordinate(SM1)at($(EK.south west)+(0.15,0.5)$);\n\\coordinate(SM2)at($(EK.south east)+(-1.1,0.5)$);\n\\coordinate(OK1)at($(EK.220)+(0,0.7)$);\n\\coordinate(OK2)at($(EK.240)+(0,0.7)$);\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};\n\\draw[line width=1.4pt](SM1)to [bend right=45](SM2);\n%%\n\\coordinate(4BL)at($(EK.south west)+(0.95,0.3)$);\n    \\def\\n{5}          % broj boksova\n    \\def\\w{0.12}        % box width (mm)\n    \\def\\h{0.5}       % Box height (mm)\n    \\def\\gap{0.05}      % razmak između boksova (mm)\n    % niz boksova\n    \\foreach \\i in {0,...,4} {\n      \\pgfmathsetmacro{\\x}{\\i*(\\w+\\gap)}\n      % padding (we clip inside the edges)\n      \\begin{scope}\n        \\clip[] ($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h);\n        \\fill[gray!10]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*1);\n        \\fill[fill=\\filllcirclecolor]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*\\Level);\n      \\end{scope}\n      % kontura preko\n      \\draw[line width=0.6pt,draw=black]($(4BL)+(\\x,0)$)  rectangle ++(\\w,\\h);\n    }\n\\else\n\\ifnum\\Smile=1\n\\node[draw=black,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,\nfill=\\filllcolor!10,line width=\\Linewidth,](EK)at(0,0.53){};\n\\coordinate(SM1)at($(EK.south west)+(0.32,0.5)$);\n\\coordinate(SM2)at($(EK.south east)+(-0.32,0.5)$);\n\\coordinate(OK1)at($(EK.250)+(0,0.7)$);\n\\coordinate(OK2)at($(EK.290)+(0,0.7)$);\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};\n\\draw[line width=1.4pt](SM1)to [bend right=45](SM2);\n\\else\n\\node[draw=green,rounded corners=2pt,rectangle,minimum width=53,minimum height=30,draw=black,fill=black](EK)at(0,0.53){};\n\\pic[shift={(0,0)}] at  (EK){skull={scalefac=1.3,picname=1,filllcolor=white, filllcirclecolor=black,Linewidth=0.5pt}};\n\\fi\n\\fi\n%\n\\draw[fill=\\filllcolor!60!black!30,line width=\\Linewidth](-1.00,-0.1)--(1.0,-0.1)--(1.28,-0.6)--(-1.28,-0.6)--cycle;\n\\draw[fill=\\filllcolor!60!black!30,line width=\\Linewidth](1.28,-0.6)--(-1.28,-0.6)arc[start angle=180, end angle=270, radius=4pt]--(1.14,-0.73)\narc[start angle=270, end angle=355, radius=4pt]--cycle;\n\\draw[fill=\\filllcolor!30!black!10,line width=\\Linewidth](-0.95,-0.17)--(0.95,-0.17)--(1.03,-0.34)--(-1.03,-0.34)--cycle;\n\\draw[fill=\\filllcolor!30!black!20,line width=\\Linewidth](-0.16,-0.52)--(0.16,-0.52)--(0.14,-0.42)--(-0.14,-0.42)--cycle;\n\\end{scope}\n    }\n  }\n}\n%usb\n\\tikzset{\npics/usb/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\draw[draw=black,fill=\\filllcolor,line width=\\Linewidth](-0.65,0.94)coordinate(GL\\picname)--(0.65,0.94)coordinate(GD\\picname)--\n(0.65,-1.3)coordinate(DD\\picname)arc[start angle=0, end angle=-90, radius=2mm]\n--(-0.45,-1.5)coordinate(DL\\picname)arc[start angle=-90, end angle=-180, radius=2mm]--cycle;\n\\node[draw=none,fill=\\filllcirclecolor,minimum width=5mm,minimum height=8mm,anchor=south]at($($(DL\\picname)!0.42!(DD\\picname)$)+(0,0.35)$){};\n\\coordinate(G1)at($(GL\\picname)!0.15!(GD\\picname)$);\n\\coordinate(G2)at($(GL\\picname)!0.85!(GD\\picname)$);\n\\draw[draw=black,fill=\\filllcolor!40,line width=\\Linewidth](G1)--++(0,1)coordinate(G11)-|coordinate[pos=0.5](G22)(G2)--cycle;\n\\node[draw=none,fill=black,inner sep=0pt,minimum width=1mm,minimum height=6mm,anchor=west]at($($(G1)!0.5!(G11)$)+(0.2,0)$){};\n\\node[draw=none,fill=black,inner sep=0pt,minimum width=1mm,minimum height=6mm,anchor=east]at($($(G2)!0.5!(G22)$)+(-0.2,0)$){};\n%\\fill[red](G22)circle(2pt);\n\\end{scope}\n    }\n  }\n}\n \\tikzset{/pgf/decoration/.cd,\n    number of sines/.initial=10,\n    angle step/.initial=20,\n}\n\\newdimen\\tmpdimen\n\\pgfdeclaredecoration{complete sines}{initial}\n{\n    \\state{initial}[\n        width=+0pt,\n        next state=move,\n        persistent precomputation={\n            \\pgfmathparse{\\pgfkeysvalueof{/pgf/decoration/angle step}}%\n            \\let\\anglestep=\\pgfmathresult%\n            \\let\\currentangle=\\pgfmathresult%\n            \\pgfmathsetlengthmacro{\\pointsperanglestep}%\n                {(\\pgfdecoratedremainingdistance/\\pgfkeysvalueof{/pgf/decoration/number of sines})/360*\\anglestep}%\n        }] {}\n    \\state{move}[width=+\\pointsperanglestep, next state=draw]{\n        \\pgfpathmoveto{\\pgfpointorigin}\n    }\n    \\state{draw}[width=+\\pointsperanglestep, switch if less than=1.25*\\pointsperanglestep to final, % <- bit of a hack\n        persistent postcomputation={\n        \\pgfmathparse{mod(\\currentangle+\\anglestep, 360)}%\n        \\let\\currentangle=\\pgfmathresult%\n    }]{%\n        \\pgfmathsin{+\\currentangle}%\n        \\tmpdimen=\\pgfdecorationsegmentamplitude%\n        \\tmpdimen=\\pgfmathresult\\tmpdimen%\n        \\divide\\tmpdimen by2\\relax%\n        \\pgfpathlineto{\\pgfqpoint{0pt}{\\tmpdimen}}%\n    }\n    \\state{final}{\n        \\ifdim\\pgfdecoratedremainingdistance>0pt\\relax\n            \\pgfpathlineto{\\pgfpointdecoratedpathlast}\n        \\fi\n   }\n}\n%testing_medal\n\\tikzset{\npics/testing/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=TESTING1,shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\newcommand{\\tikzxmark}{%\n\\tikz[scale=0.18] {\n    \\draw[line width=0.7,line cap=round,GreenLine] (0,0) to [bend left=6] (1,1);\n    \\draw[line width=0.7,line cap=round,GreenLine] (0.2,0.95) to [bend right=3] (0.8,0.05);\n}}\n\\newcommand{\\tikzxcheck}{%\n\\tikz[scale=0.16] {\n\\draw[line width=0.7,line cap=round,GreenLine] (0.5,0.75)--(0.85,-0.1) to [bend left=16] (1.5,1.55);\n}}\n \\node[draw, minimum width  =15mm, minimum height = 20mm, inner sep = 0pt,\n        rounded corners,draw = \\drawcolor, fill=\\filllcolor!10, line width=\\Linewidth](COM){};\n\\node[draw=GreenLine,inner sep=4pt,fill=white](CB1) at ($(COM.north west)!0.25!(COM.south west)+(0.3,0)$){};\n\\node[draw=GreenLine,inner sep=4pt,fill=white](CB2) at ($(COM.north west)!0.5!(COM.south west)+(0.3,0)$){};\n\\node[draw=GreenLine,inner sep=4pt,fill=white](CB3) at ($(COM.north west)!0.75!(COM.south west)+(0.3,0)$){};\n\\node[xshift=0pt]at(CB1){\\tikzxcheck};\n\\node[xshift=0pt]at(CB2){\\tikzxmark};\n\\node[xshift=0pt]at(CB3){\\tikzxmark};\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,0.05)$)--++(0:0.8);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,-0.12)$)--++(0:0.7);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,0.05)$)--++(0:0.8);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,-0.12)$)--++(0:0.6);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,0.05)$)--++(0:0.8);\n\\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,-0.12)$)--++(0:0.6);\n\\end{scope}\n\\begin{scope}[shift={($(0,0)+(0.4,-0.50)$)},scale=0.5\\scalefac,every node/.append style={transform shape}]\n\\draw[draw=none,fill=\\filllcolor!60](-0.48,-0.10)--(-0.68,-0.68)--(-0.92,-1.38)--\n(-0.53,-1.28)--(-0.29,-1.61)--(-0.09,-0.93)--(0.15,-0.1)--cycle;\n\\draw[draw=none,fill=\\filllcolor!60](-0.266,-0.10)--(-0.02,-0.93)--(0.18,-1.61)--\n(0.45,-1.34)--(0.85,-1.48)--(0.61,-0.68)--(0.44,-0.1)--cycle;\n \\draw[draw=none,postaction={very thick, line join=round, draw=white,fill=\\filllcolor,\n        decorate,decoration={complete sines, number of sines=9, amplitude=\\scalefac*2pt}}] (0,0) circle [radius=0.9];\n\\node[draw=none,fill=white,circle,minimum size=11mm,line width=1pt](CM-\\picname) {};\n%\n\\end{scope}\n    }\n  }\n}\n%PLC\n\\tikzset{\npics/plc/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\node[draw=\\filllcolor,fill=\\filllcolor!30,minimum width=21mm,minimum height=24mm](R\\picname){};\n\\coordinate(P)at($(R\\picname.north west)!0.1!(R\\picname.south east)$);\n% box dimensions\n  \\def\\boxsize{1.3mm}\n  \\def\\xstep{1.8mm}  % gap between columns\n  \\def\\ystep{1.9mm}  % razmak između redova\n% Lista obojenih ćelija – bez duplih zagrada, plus čuvar-zarezi\n\\def\\coloredcells{0/0,1/1,2/1,0/2,1/3,0/4,2/4,1/5,1/6,2/6,0/7}\n\n\\foreach \\i in {0,1,2} {\n  \\foreach \\j in {0,1,2,3,4,5,6,7} {\n    \\edef\\cellid{\\i/\\j}\n    \\def\\fillcolor{cyan!10}\n    % Unutrašnja petlja pravi grupu; zato koristimo \\global\n    \\foreach \\c in \\coloredcells {%\n      \\ifx\\cellid\\c\n        \\global\\def\\fillcolor{green!60}%\n      \\fi\n    }\n    \\node[draw=black, fill=\\fillcolor, minimum size=\\boxsize, inner sep=0pt](MB\\i\\j)\n      at ($(P) + (\\i*\\xstep, -\\j*\\ystep)$) {};\n  }\n}\n\\coordinate(1BL)at($(MB07.south west)+(0,-1mm)$);\n\\node[draw=black,fill=\\filllcolor!10,anchor=north west,inner sep=0pt,\nminimum width=5mm,minimum height=1.5mm](BX1)at(1BL){};\n\\coordinate(2BL)at($(BX1.south west)+(0,-0.8mm)$);\n\\node[draw=black,fill=\\filllcolor!50!black!20,anchor=north west,inner sep=0pt,\nminimum width=5mm,minimum height=3.0mm](BX2)at(2BL){};\n\\coordinate(3BL)at($(BX2.south east)+(1mm,0)$);\n\\node[draw=black,fill=\\filllcolor!70!black!30,anchor=south west,inner sep=0pt,\nminimum width=12mm,minimum height=3.0mm](BX3)at(3BL){};\n\\path[red](BX3.north west)|-coordinate[pos=0.5](1E)(MB27.south east);\n%display\n\\ifnum\\Smile=1\n\\node[draw=black,fill=\\filllcolor!10,anchor=south west,inner sep=0pt,\nminimum width=12mm,minimum height=8.0mm](EK)at(1E){};\n\\coordinate(SM1)at($(EK.south west)+(0.32,0.35)$);\n\\coordinate(SM2)at($(EK.south east)+(-0.32,0.35)$);\n\\coordinate(OK1)at($(EK.250)+(0,0.55)$);\n\\coordinate(OK2)at($(EK.290)+(0,0.55)$);\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO1)at(OK1){};\n\\node[fill=black,inner sep=0pt,ellipse,minimum width=2pt,minimum height=3pt](OKO2)at(OK2){};\n\\draw[line width=1.0pt](SM1)to [bend right=25](SM2);\n\\else\n\\node[draw=black,fill=black,anchor=south west,inner sep=0pt,\nminimum width=12mm,minimum height=8.0mm](EK)at(1E){};\n%skull\n\\pic[shift={(0,0)}] at  (EK){skull={scalefac=1,picname=1,filllcolor=white, filllcirclecolor=black,Linewidth=0.5pt}};\n\\fi\n\\draw[fill=\\filllcolor!40!black!30](-0.2,-0.67)--(0.8,-0.67)--(0.72,-0.54)--(-0.14,-0.54)--cycle;\n\\coordinate(4BL)at($(EK.north west)+(0,1mm)$);\n % geometry\n    \\def\\n{5}          % broj boksova\n    \\def\\w{0.2}        % širina boksa (mm)\n    \\def\\h{0.5}       % visina boksa (mm)\n    \\def\\gap{0.05}      % razmak između boksova (mm)\n    % niz boksova\n    \\foreach \\i in {0,...,4} {\n      \\pgfmathsetmacro{\\x}{\\i*(\\w+\\gap)}\n      % popuna (klipujemo unutar ivica)\n      \\begin{scope}\n        \\clip[] ($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h);\n        \\fill[gray!10]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*1);\n        \\fill[fill=\\filllcirclecolor]($(4BL)+(\\x,0)$) rectangle ++(\\w,\\h*\\Level);\n      \\end{scope}\n      % contour over\n      \\draw[line width=0.6pt,draw=black]($(4BL)+(\\x,0)$)  rectangle ++(\\w,\\h);\n    }\n\\end{scope}\n    }\n  }\n}\n%copier\n\\tikzset{\npics/copier/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n\\draw[fill=\\filllcolor!60!black,line width=\\Linewidth,draw=black](0.1,1.15)--++(150:0.85)arc[start angle=90, end angle=200,radius=1.5pt]--++(330:0.82)--cycle;\n\\draw[fill=\\filllcolor!30,line width=\\Linewidth,draw=black](-0.73,0.75)--(0.69,0.75)--(0.69,0.41)--(-0.73,0.41)--cycle;\n\\draw[fill=\\filllcolor!60,line width=\\Linewidth,draw=black](-0.80,1.05)--(-0.02,1.05)--(0.1,1.15)--(0.745,1.15)--(0.745,0.75)--(-0.80,0.75)--cycle;\n\\draw[draw=none,fill=\\filllcirclecolor](0.12,1.08)--(0.49,1.08)coordinate(DISNE)--(0.49,0.83)coordinate(DISSE)--(0.12,0.83)--cycle;\n\\node[draw=none,fill=\\filllcirclecolor,circle,inner sep=0pt,minimum size=1mm,below right=0.5pt and 2pt of DISNE]{};\n\\node[draw=none,fill=\\filllcirclecolor,circle,inner sep=0pt,minimum size=1mm,above right=0.5pt and 2pt of DISSE]{};\n%\n\\draw[fill=\\filllcolor!60!black,line width=\\Linewidth,draw=black](0.745,0.-0.07)--(0.87,0)arc[start angle=130, end angle=30,radius=2.5pt]--(1.35,0.16)\narc[start angle=120, end angle=-10,radius=1.25pt]--(0.745,0.-0.22)--cycle;\n%\n\\draw[fill=\\filllcolor!30,line width=\\Linewidth,draw=black](-0.8,0.41)--(0.745,0.41)--(0.745,-1.18)--(-0.8,-1.18)--cycle;\n%\n\\draw[line width=2*\\Linewidth](-0.72,0.0)--coordinate[pos=0.5](SR1)(0.665,0);\n\\node[draw=black,fill=\\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR1){};\n\\draw[line width=2*\\Linewidth](-0.72,-0.4)--coordinate[pos=0.5](SR2)(0.665,-0.4);\n\\node[draw=black,fill=\\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR2){};\n\\draw[line width=2*\\Linewidth](-0.72,-0.8)--coordinate[pos=0.5](SR3)(0.665,-0.8);\n\\node[draw=black,fill=\\filllcirclecolor!60!black!30,rectangle,inner sep=0pt,anchor=north,minimum width=10,minimum height=2]at(SR3){};\n%\n\\end{scope}\n    }\n  }\n}\n%vijak\n\\tikzset{\npics/sraf/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[shift={($(0,0)+(0,0)$)},scale=\\scalefac,every node/.append style={transform shape}]\n%fire\n\\ifnum\\Fire=1\n\\fill[fill=red!70,thick](0,1.6)--(0.4,0.6)--(1.4,2)--(2.2,0.8)--(3.1,1.8)--(3,0.5)--(3.7,0.6)--(3.0,-1.2)--\n(3.4,-1.3)--(2.0,-2.86)--(-1.8,-2.86)--(-3.3,-1.36)--(-2.8,-1.36)--(-3.2,-0.1)--(-2.6,-0.3)--(-2.9,2.5)--\n(-2.3,1.4)--(-1.4,2.9)--(-0.6,0.9)--cycle;\n\\fi\n\\foreach \\x in {-2,-0.65,0.72,2.1}{\n\\node[draw=\\drawcolor,fill=\\filllcolor!80,line width=1.5*\\Linewidth,inner sep=0pt,outer sep=0pt,\nminimum width=6mm,minimum height=50mm](SRA1)at(\\x,0){};\n\\node[draw=\\drawcolor,fill=\\filllcolor!50,line width=1.5*\\Linewidth,,trapezium,inner ysep=2pt,anchor=north,outer sep=0pt,inner xsep=3pt,\nminimum width=8mm,minimum height=4mm](TR1)at(SRA1.south){};\n\\begin{scope}\n\\clip(SRA1.south west)rectangle (SRA1.north east);\n\\foreach \\i [evaluate=\\i as \\y using {\\i+0.03}]in {0,0.07,0.14,...,0.99}{\n\\draw[black,thick]($(SRA1.south west)!\\i!(SRA1.north west)$)--($(SRA1.south east)!\\y!(SRA1.north east)$);\n\\draw[draw=\\drawcolor,line width=1.5*\\Linewidth](SRA1.south west)rectangle (SRA1.north east);\n}\n\\end{scope}\n}\n\\end{scope}\n    }\n  }\n}\n\\pgfkeys{\n  /channel/.cd,\n   Dual/.store in=\\Dual,\n   Depth/.store in=\\Depth,\n  Height/.store in=\\Height,\n  Width/.store in=\\Width,\n  Fire/.store in=\\Fire,\n  Smile/.store in=\\Smile,\n  Level/.store in=\\Level,\n  filllcirclecolor/.store in=\\filllcirclecolor,\n  filllcolor/.store in=\\filllcolor,\n  drawcolor/.store in=\\drawcolor,\n  drawcircle/.store in=\\drawcircle,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  filllcolor=BrownLine,\n  filllcirclecolor=cyan!40,\n  drawcolor=black,\n  drawcircle=violet,\n  scalefac=1,\n  Dual=1,\n  Fire=1,\n  Smile=1,\n  Level=0.52,\n  Linewidth=0.5pt,\n  Depth=1.3,\n  Height=0.8,\n  Width=1.1,\n  picname=C\n}\n%\n\\begin{scope}[local bounding box=LAPTOP1,shift={($(0,0)+(0,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.5,picname=1,drawcolor=GreenD,Dual=0,Smile=0,\nfilllcolor=GreenD!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=USB1,shift={($(LAPTOP1)+(3.7,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)},rotate=320] at  (0,0){usb={scalefac=0.6,picname=1,drawcolor=orange,filllcirclecolor=white,filllcolor=violet!50!black!50!,Linewidth=1.0pt,}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=TESTING1,shift={($(USB1)+(3.1,0.45)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,-0.5)}] at  (0,0){testing={scalefac=1,picname=1,drawcolor=OrangeLine,filllcolor=OrangeLine, Linewidth=1.0pt}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=LAPTOP2,shift={($(TESTING1)+(3.5,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=2,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=red!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n\\tikzset{%\n    LineZ/.style={-*,green!50!black,line width=1pt}\n}\n\\begin{scope}[local bounding box=LAPTOP3,shift={($(LAPTOP2)+(6.1,-0.2)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=3,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=yellow!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n\\draw[LineZ](EKV3.north)--++(90:1.2)node[above,black]{\\huge ?};\n\\draw[LineZ] ($(EKV3.north)+(0,0.2)$)--++(40:1);\n\\draw[LineZ] ($(EKV3.north)+(0,0.2)$)--++(140:1);\n\\draw[LineZ](EKV3.west)--++(180:1.2)node[left,black]{\\huge ?};\n\\draw[LineZ] ($(EKV3.west)+(-0.2,0)$)--++(140:1);\n\\draw[LineZ] ($(EKV3.west)+(-0.2,0)$)--++(220:1);\n\\draw[LineZ](EKV3.east)--++(0:1.2)node[right,black]{\\huge ?};\n\\draw[LineZ] ($(EKV3.east)+(0.2,0)$)--++(40:1);\n\\draw[LineZ] ($(EKV3.east)+(0.2,0)$)--++(320:1);\n%\n\\begin{scope}[local bounding box=LAPTOP4,shift={($(LAPTOP2)+(12.5,-0.2)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=4,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=cyan!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n%\n\\draw[LineZ](EKV4.east)--node[above,black]{\\huge !}\nnode[below=3pt,black,fill=magenta!20,circle,inner sep=1pt](CIRC1){\\large 1}++(0:1.2);\n\\end{scope}\n%\n\\begin{scope}[local bounding box=PLC1,shift={($(LAPTOP4)+(3.5,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=1,drawcolor=OrangeLine,filllcirclecolor=cyan, Smile=1,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};\n\\end{scope}\n%\n\\coordinate(SR1)at($(LAPTOP1.east)!0.45!(USB1.west)$);\n\\node[Larrow]at(SR1){};\n\\coordinate(SR2)at($(USB1.east)!0.4!(TESTING1.west)$);\n\\node[Larrow]at(SR2){};\n\\coordinate(SR3)at($(TESTING1.east)!0.5!(LAPTOP2.west)$);\n\\node[Larrow]at(SR3){};\n%text below first row\n\\node[draw=none,fit=(LAPTOP1)(LAPTOP2)](BB1){};\n\\node[TxtL,below=6pt of BB1.south west,text width=110mm,anchor=north west](GT1){\\textcolor{red}{\\textbf{1. Infection}}\\\\[0.35ex]\nStuxnet enters a system via a USB stick and proceeds\nto infect all machines running Microsoft Windows. By brandishing a digital certificate that seems to\nshow that it comes from a reliable company, the worm is able to evade automated-detection systems.};\n%\n\\path[red](BB1.south east)-|coordinate[pos=0.5](T2)(EKV3.south);\n\\node[TxtL,below=6pt of T2.south,text width=80mm,xshift=-12mm](GT2){\\textcolor{red}{\\textbf{2. Search}}\\\\[0.35ex]\nStuxnet then checks whether a given machine is part of\nthe targeted industrial control system made by Siemens. Such systems are deployed in Iran to run high-speed centrifuges that\nhelp to enrich nuclear fuel.};\n\\path[red](BB1.south east)-|coordinate[pos=0.5](T3)(CIRC1);\n\\node[TxtL,below=6pt of T3.south,text width=67mm](GT3){\\textcolor{red}{\\textbf{3. Update}}\\\\[0.35ex] If the system isn’t a target, Stuxnet does nothing;\nif it is, the worm attempts to access the Internet and download a more recent version of itself.};\n%\n\\node[draw=none,fit=(BB1)(GT2)(GT3)(PLC1)](BOX1){};\n\\draw[BrownLine,line width=2pt]([yshift=-2mm]BOX1.south west)coordinate(LE)\n--([yshift=-2mm]BOX1.south east)coordinate(DE);\n%%%%%%%%%%%%%%%%%%%\n%Row below\n%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[local bounding box=LAPTOP5,shift={($(LAPTOP1)+(-0.6,-7.5)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=5,drawcolor=red,Dual=0,Smile=1,\nfilllcolor=green!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=COPIER1,shift={($(LAPTOP5)+(2.9,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){copier={scalefac=0.8,picname=1,drawcolor=BlueD,\nfilllcolor=BlueD!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80!}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=PLC2,shift={($(COPIER1)+(3.3,-0.1)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=2,drawcolor=OrangeLine,filllcirclecolor=cyan, Smile=0,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};\n\\end{scope}\n%%%%%%%%%%%%%%%\n\\begin{scope}[local bounding box=LAPTOP6,shift={($(LAPTOP1)+(9.6,-7.5)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=6,drawcolor=red,Dual=1,Smile=1,\nfilllcolor=brown!70!,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n\\draw[LineZ](EKV6.15)--++(0:0.7);\n\\draw[LineZ,red](EKV6.345)--++(0:0.7);\n%\n\\begin{scope}[local bounding box=PLC3,shift={($(LAPTOP6)+(3.2,0.1)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=3,drawcolor=OrangeLine,filllcirclecolor=red, Smile=0,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=0.42}};\n\\end{scope}\n\\draw[LineZ,red](R3.355)--++(0:0.9);\n\\draw[LineZ,-](R3.20)--++(0:0.35)--++(0,-0.5)\nnode[rectangle, fill=white,draw=green!50!black,minimum size=width=2mm,minimum height=4mm] {};\n%\n\\begin{scope}[local bounding box=SRAF1,shift={($(PLC3)+(3.25,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){sraf={scalefac=0.4,picname=1,drawcolor=BrownLine,filllcolor=BrownLine!50!,Linewidth=0.5pt,Fire=0}};\n\\end{scope}\n%%%%%%%%%%%%%%%\n\\begin{scope}[local bounding box=LAPTOP7,shift={($(LAPTOP6)+(10.3,-0.2)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){laptop={scalefac=1.0,picname=7,drawcolor=red,Dual=1,Smile=1,\nfilllcolor=red,Linewidth=1.0pt, filllcirclecolor=yellow!80}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=PLC4,shift={($(LAPTOP7)+(3.5,0.1)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){plc={scalefac=1.2,picname=4,drawcolor=OrangeLine,filllcirclecolor=red, Smile=0,\nfilllcolor=OrangeLine,Linewidth=1.0pt,Level=1}};\n\\end{scope}\n%\n\\begin{scope}[local bounding box=SRAF2,shift={($(PLC4)+(3.6,0)$)},scale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){sraf={scalefac=0.4,picname=1,drawcolor=BrownLine,filllcolor=BrownLine!50!,Linewidth=0.5pt,Fire=1}};\n\\end{scope}\n%arrows\n\\coordinate(2SR1)at($(LAPTOP5.east)!0.35!(COPIER1.west)$);\n\\node[Larrow]at(2SR1){};\n\\coordinate(2SR2)at($(COPIER1.350)!0.45!(PLC2.west)$);\n\\node[Larrow]at(2SR2){};\n\\coordinate(2SR3)at($(EKV7.25)!0.55!(PLC4.west)$);\n\\node[Larrow]at(2SR3){};\n\\coordinate(2SR4)at($(EKV7.330)!0.55!(PLC4.west)$);\n\\node[Larrow,red,rotate=180]at(2SR4){};\n\\coordinate(2SR5)at($(R4.25)!0.55!(SRAF2.west)$);\n\\node[Larrow]at(2SR5){};\n\\coordinate(2SR6)at($(R4.335)!0.55!(SRAF2.west)$);\n\\node[Larrow,rotate=180]at(2SR6){};\n%text\n\\node[draw=none,fit=(LAPTOP5)(PLC2)](2BB1){};\n\\node[TxtL,below=6pt of 2BB1.south west,text width=81mm,anchor=north west](DT1){\\textcolor{red}{\\textbf{4. Compromise}}\\\\[0.35ex] The worm then compromises\nthe target system’s logic controllers, exploiting “zero day” vulnerabilities—software weaknesses that haven’t\nbeen identified by security experts.};\n%\n\\path[red](2BB1.south east)-|coordinate[pos=0.5](2T2)(PLC3.south);\n\\node[TxtL,below=6pt of 2T2.south](DT2){\\textcolor{red}{\\textbf{5. Control}}\\\\[0.35ex] In the beginning,\nStuxnet spies on the operations of the targeted system. Then it uses the information it has gathered\nto take control of the centrifuges, making them spin themselves to failure.};\n%\n\\path[red](2BB1.south east)-|coordinate[pos=0.5](2T3)(PLC4.south);\n\\node[TxtL,below=6pt of 2T3.south,text width=80mm](DT3){\\textcolor{red}{\\textbf{6. Deceive and destroy}}\\\\[0.35ex] Meanwhile,\nit provides false feedback to outside controllers, ensuring that they won’t know what’s going wrong\nuntil it’s too late to do anything about it.};\n%\n\\path[red](LE)--++(0,-6.7)coordinate(LE2)-|coordinate(DE2)(DE);\n\\pgfdeclarehorizontalshading{mygradient}{100bp}{\n  color(0bp)=(green);\n  color(50bp)=(red)\n}\n\\shade[shading=mygradient] (LE2) rectangle ($(DE2)+(0,-5mm)$);\n\\path[red](LE)--++(0,9.5)coordinate(GLE2)-|coordinate(GDE2)(DE);\n\\fill[BrownLine!40] (GLE2) rectangle ($(GDE2)+(0,5mm)$);\n\\node[fill=white]at($([yshift=2.5mm]GLE2)!0.5!([yshift=2.5mm]GDE2)$){\\large \\bfseries HOW \\textcolor{red}{STUXNET} WORKED};\n\\draw[LineA,*-*,text=black,line width=1pt,shorten <=5pt,shorten >=5pt](EKV1.north)--++(0,1.85)-|\nnode[below=5pt,pos=0.1]{Update from source}\nnode[left=5pt,pos=0.85,black,fill=magenta!20,circle,inner sep=1pt]{2}(EKV4.north);\n\\end{tikzpicture}\n```\n**Stuxnet**: Targets PLCs by exploiting Windows and Siemens software vulnerabilities, demonstrating supply chain compromise that enabled digital malware to cause physical infrastructure damage. Modern ML systems face analogous risks through compromised training data, backdoored dependencies, and tampered model weights. @fig-stuxnet\n:::\n\n### Insufficient Isolation: Jeep Cherokee Hack {#sec-security-privacy-insufficient-isolation-jeep-cherokee-hack-6a7c}\n\nThe 2015 Jeep Cherokee hack demonstrated how connectivity in everyday products creates new vulnerabilities. Security researchers publicly demonstrated a remote cyberattack on a Jeep Cherokee that exposed important vulnerabilities in automotive system design [@miller2015remote; @miller2019lessons]. Conducted as a controlled experiment, the researchers exploited a vulnerability in the vehicle's Uconnect entertainment system, which was connected to the internet via a cellular network. By gaining remote access to this system, they sent commands that affected the vehicle's engine, transmission, and braking systems without physical access to the car.\n\nThis demonstration served as a wake-up call for the automotive industry, highlighting the risks posed by the growing connectivity of modern vehicles. Traditionally isolated automotive control systems, such as those managing steering and braking, were shown to be vulnerable when exposed through externally accessible software interfaces. The ability to remotely manipulate safety-critical functions raised serious concerns about passenger safety, regulatory oversight, and industry best practices.\n\n{{< margin-video \"https://www.youtube.com/watch?v=MK0SrxBC1xs&ab_channel=WIRED\" \"Jeep Cherokee Hack\" \"WIRED\" >}}\n\nThe incident also led to a recall of over 1.4 million vehicles to patch the vulnerability[^fn-automotive-recalls], highlighting the need for manufacturers to prioritize cybersecurity in their designs. The National Highway Traffic Safety Administration (NHTSA)[^fn-nhtsa] issued guidelines for automakers to improve vehicle cybersecurity, including recommendations for secure software development practices and incident response protocols.\n\n[^fn-automotive-recalls]: **Automotive Cybersecurity Recalls**: The Jeep Cherokee hack triggered the first-ever automotive cybersecurity recall in 2015. Since then, cybersecurity recalls have affected over 15 million vehicles globally, costing manufacturers an estimated $2.4 billion in remediation efforts and spurring new regulations.\n\n[^fn-nhtsa]: **NHTSA Cybersecurity Guidance**: NHTSA, established in 1970, issued its first cybersecurity guidance in 2016 following the Jeep hack. The agency now mandates that connected vehicles include cybersecurity by design, affecting 99% of new vehicles sold in the US that contain 100+ onboard computers.\n\nThe Jeep Cherokee hack offers critical lessons for ML system security. Connected ML systems require strict isolation between external interfaces and safety-critical components, as this incident dramatically illustrated. The architectural flaw (allowing external interfaces to reach safety-critical functions) directly threatens modern ML deployments where inference APIs often connect to physical actuators or critical systems.\n\nModern ML attack vectors exploit these same isolation failures across multiple domains: (1) Autonomous vehicles where compromised infotainment system ML APIs (voice recognition, navigation) gain access to perception models controlling steering and braking; (2) Smart home systems where exploited voice assistant wake-word detection models provide backdoor access to security systems, door locks, and cameras; (3) Industrial IoT where compromised edge ML inference endpoints (predictive maintenance, anomaly detection) manipulate actuator control logic in manufacturing systems; (4) Medical devices where attacked diagnostic ML models influence treatment recommendations and drug delivery systems.\n\nConsider a concrete attack scenario: a smart home voice assistant processes user commands through cloud-based NLP models. An attacker exploits a vulnerability in the voice processing API to inject malicious commands that bypass authentication. Through insufficient network segmentation, the compromised voice system gains access to the home security ML model responsible for facial recognition door unlocking, allowing unauthorized physical access.\n\nEffective defense requires comprehensive isolation architecture: (1) network segmentation to isolate ML inference networks from actuator control networks using firewalls and VPNs; (2) API authentication requiring cryptographic authentication for all ML API calls with rate limiting and anomaly detection; (3) privilege separation to run inference models in sandboxed environments with minimal system permissions; (4) fail-safe defaults that design actuator control logic to revert to safe states (locked doors, stopped motors) when ML systems detect anomalies or lose connectivity; (5) monitoring that implements real-time logging and alerting for suspicious ML API usage patterns.\n\n### Weaponized Endpoints: Mirai Botnet {#sec-security-privacy-weaponized-endpoints-mirai-botnet-931c}\n\nWhile the Jeep Cherokee hack demonstrated targeted exploitation of connected systems, the Mirai botnet revealed how poor security practices could be weaponized at massive scale. In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS)[^fn-ddos-attacks] attacks in internet history [@antonakakis2017understanding]. The botnet infected thousands of networked devices, including digital cameras, DVRs, and other consumer electronics. These devices, often deployed with factory-default usernames and passwords, were easily compromised by the Mirai malware and enlisted into a large-scale attack network.\n\n[^fn-mirai-scale]: **Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generating peak attacks of 1.2 Tbps (1,200 Gbps) against OVH hosting provider, making it one of the first terabit-scale DDoS attacks. The attack revealed that IoT devices with default credentials (admin/admin, root/12345) could be weaponized at unprecedented scale.\n\n[^fn-ddos-attacks]: **DDoS Attacks**: Distributed Denial-of-Service (DDoS) attacks overwhelm targets with traffic from multiple sources, first demonstrated in 1999. Modern DDoS attacks can exceed 3.47 Tbps (terabits per second), enough to take down entire internet infrastructures and costing businesses $2.3 million per incident on average.\n\nThe Mirai botnet was used to overwhelm major internet infrastructure providers, disrupting access to popular online services across the United States and beyond. The scale of the attack demonstrated how vulnerable consumer and industrial devices can become a platform for widespread disruption when security is not prioritized in their design and deployment.\n\n{{< margin-video \"https://www.youtube.com/watch?v=1pywzRTJDaY\" \"Mirai Botnet\" \"Vice News\" >}}\n\nThe Mirai botnet's lessons apply directly to modern ML deployments. Edge-deployed ML devices with weak authentication become weaponized attack infrastructure at unprecedented scale, precisely as the Mirai botnet demonstrated with traditional IoT devices. Modern ML edge devices (smart cameras running object detection, voice assistants performing wake-word detection, autonomous drones with navigation models, industrial IoT sensors with anomaly detection algorithms) face identical vulnerability patterns but with amplified consequences due to their AI capabilities and access to sensitive data.\n\nThe attack escalation with ML devices differs significantly from traditional IoT compromises. Unlike simple IoT devices that provided only computing power for DDoS attacks, compromised ML devices offer sophisticated capabilities: (1) Data exfiltration where smart cameras leak facial recognition databases, voice assistants extract conversation transcripts, and health monitors steal biometric data; (2) Model weaponization where hijacked autonomous drones coordinate swarm attacks and compromised traffic cameras misreport vehicle counts to manipulate traffic systems; (3) AI-powered reconnaissance where compromised edge ML devices use their trained models to identify high-value targets (facial recognition for VIP identification, voice analysis for emotion detection) and coordinate sophisticated multi-stage attacks.\n\nConsider a concrete attack scenario where attackers compromise 50,000 smart security cameras with default passwords, each running ML object detection models. Rather than traditional DDoS attacks, they use the compromised cameras to: (1) extract facial recognition databases from residential and commercial buildings; (2) coordinate physical surveillance of targeted individuals using distributed camera networks; (3) inject false object detection alerts to trigger emergency responses and create chaos; (4) use the cameras' computing power to train adversarial examples against other security systems.\n\nComprehensive defense against such weaponization requires zero-trust edge security: (1) Secure manufacturing that eliminates default credentials, implements hardware security modules (HSMs) for device-unique keys, and enables secure boot with cryptographic verification; (2) Encrypted communications that mandate TLS 1.3+ for all ML API communications with certificate pinning and mutual authentication; (3) Behavioral monitoring that deploys anomaly detection systems to identify unusual inference patterns, unexpected network traffic, and suspicious computational loads; (4) Automated response that implements kill switches to disable compromised devices remotely and quarantine them from networks; (5) Update security that enforces cryptographically signed firmware updates with automatic security patching and version rollback capabilities.\n\n## Systematic Threat Analysis and Risk Assessment {#sec-security-privacy-systematic-threat-analysis-risk-assessment-3ef1}\n\nThe historical incidents demonstrate how fundamental security failures manifest across different computing paradigms. Supply chain vulnerabilities enable persistent compromise, insufficient isolation allows privilege escalation, and weaponized endpoints create attack infrastructure at scale. These patterns directly apply to machine learning deployments: compromised training pipelines and model repositories inherit supply chain risks, external interfaces to safety-critical ML components require strict isolation, and compromised ML edge devices can exfiltrate inference data or participate in coordinated attacks.\n\nThese historical incidents reveal universal security patterns that translate directly to ML system vulnerabilities. Supply chain compromise, as demonstrated by Stuxnet, manifests in ML through training data poisoning and backdoored model repositories. Insufficient isolation, exemplified by the Jeep Cherokee hack, appears in ML API access to safety-critical systems and compromised inference endpoints. Weaponized endpoints, illustrated by the Mirai botnet, emerge through hijacked ML edge devices capable of coordinated AI-powered attacks.\n\nThe key insight is that traditional cybersecurity patterns amplify in ML systems because models learn from data and make autonomous decisions. While Stuxnet required sophisticated malware to manipulate industrial controllers, ML systems can be compromised through data poisoning that appears statistically normal but embeds hidden behaviors. This characteristic makes ML systems both more vulnerable to subtle attacks and more dangerous when compromised, as they can make decisions affecting physical systems autonomously. Understanding these historical patterns helps recognize how familiar attack vectors manifest in ML contexts, while the unique properties of learning systems (statistical learning, decision autonomy, and data dependency) create new attack surfaces requiring specialized defenses.\n\nMachine learning systems introduce attack vectors that extend beyond traditional computing vulnerabilities. The data-driven nature of learning creates new opportunities for adversaries: training data can be manipulated to embed backdoors, input perturbations can exploit learned decision boundaries, and systematic API queries can extract proprietary model knowledge. These ML-specific threats require specialized defenses that account for the statistical and probabilistic foundations of learning systems, complementing traditional infrastructure hardening.\n\n### Threat Prioritization Framework {#sec-security-privacy-threat-prioritization-framework-f2d5}\n\nWith the wide range of potential threats facing ML systems, practitioners need a framework to prioritize their defensive efforts effectively. Not all threats are equally likely or impactful, and security resources are always constrained. A simple prioritization matrix based on likelihood and impact helps focus attention where it matters most.\n\nConsider these threat priority categories:\n\n- **High Likelihood / High Impact**: Data poisoning in federated learning systems where training data comes from untrusted sources. These attacks are relatively easy to execute but can severely compromise model behavior.\n\n- **High Likelihood / Medium Impact**: Model extraction attacks against public APIs. These are common and technically simple but may only affect competitive advantage rather than safety or privacy.\n\n- **Low Likelihood / High Impact**: Hardware side-channel attacks on cloud-deployed models. These require sophisticated adversaries and physical access but could expose all model parameters and user data.\n\n- **Medium Likelihood / Medium Impact**: Membership inference attacks against models trained on sensitive data. These require some technical skill but mainly threaten individual privacy rather than system integrity.\n\nThis framework guides resource allocation throughout this chapter. We begin with the most common and accessible threats (model theft, data poisoning, and adversarial attacks) before examining more specialized hardware and infrastructure vulnerabilities. Understanding these priority levels helps practitioners implement defenses in a logical sequence that maximizes security benefit per invested effort.\n\n## Model-Specific Attack Vectors {#sec-security-privacy-modelspecific-attack-vectors-0575}\n\nMachine learning systems face threats spanning the entire ML lifecycle, from training-time manipulations to inference-time evasion. These threats fall into three broad categories: threats to model confidentiality (model theft), threats to training integrity (data poisoning[^fn-data-poisoning]), and threats to inference robustness (adversarial examples[^fn-adversarial-examples]). Each category targets different vulnerabilities and requires distinct defensive strategies.\n\n[^fn-data-poisoning]: **Data Poisoning Attacks**: Data poisoning is an attack technique where adversaries inject malicious data during training, first formalized in 2012 [@biggio2012poisoning]. Studies show that poisoning just 0.1% of training data can reduce model accuracy by 10-50%, making it a highly efficient attack vector against ML systems.\n\n[^fn-adversarial-examples]: **Adversarial Examples**: Adversarial examples are inputs crafted to deceive ML models, discovered by Szegedy et al. [@szegedy2014intriguing]. These attacks can fool state-of-the-art image classifiers with perturbations invisible to humans (changing <0.01% of pixel values), affecting 99%+ of deep learning models.\n\nUnderstanding when and where different attacks occur in the ML lifecycle helps prioritize defenses and understand attacker motivations. @fig-ml-lifecycle-threats maps the primary attack vectors to their target stages in the machine learning pipeline, revealing how adversaries exploit different system vulnerabilities at different times.\n\n- **During Data Collection**: Attackers can inject malicious samples or manipulate labels in training datasets, particularly in federated learning or crowdsourced data scenarios where data sources are less controlled.\n\n- **During Training**: This stage faces backdoor insertion attacks, where adversaries embed hidden behaviors that activate only under specific trigger conditions, and label manipulation attacks that systematically corrupt the learning process.\n\n- **During Deployment**: Model theft attacks target this stage because trained models become accessible through APIs, file downloads, or reverse engineering of mobile applications. This is where intellectual property is most vulnerable.\n\n- **During Inference**: Adversarial attacks occur at runtime, where attackers craft inputs designed to fool deployed models into making incorrect predictions while appearing normal to human observers.\n\nThis lifecycle perspective reveals that different threats require different defensive strategies. Data validation protects the collection phase, secure training environments protect the training phase, access controls and API design protect deployment, and input validation protects inference. By understanding which attacks target which lifecycle stages, security teams can implement appropriate defenses at the right architectural layers.\n\n::: {#fig-ml-lifecycle-threats fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.85}{%\n\\begin{tikzpicture}[scale=0.9, transform shape, line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={line width=0.75pt,black!50},\nBox/.style={inner xsep=2pt,\n    node distance=0.6,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    minimum width=28mm, minimum height=8mm\n  },\nBox2/.style={Box,  node distance=2.3,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2,}\n}\n\\node[Box](B1){Data Collection};\n\\node[Box,below=of B1](B2){Training};\n\\node[Box,below=of B2](B3){Deployment};\n\\node[Box,below=of B3](B4){Inference};\n\\node[Box2,left=2.2 of B2](LB2){Backdoors};\n\\node[Box2,right=2.2 of B2](RB2){Label\\\\ Manipulation};\n\\node[Box2,left=2.2 of B3](LB3){Model Theft};\n\\node[Box2,right=2.2 of B3](RB3){Model Inversion};\n\\node[Box2,left=2.2 of B4](LB4){Adversarial\\\\ Examples};\n\\node[Box2,right=2.2 of B4](RB4){Membership\\\\ Inference};\n\\node[Box3,above left=0.5 and 2 of B1](PL){Privacy Leakage};\n\\node[Box3,above right=0.5 and 2 of B1](DP){Data Poisoning};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=10mm,inner ysep=4mm,\nyshift=2.5mm,fill=BackColor!60,fit=(B1)(B4),line width=0.75pt](BB2){};\n\\node[below=4pt of  BB2.north,inner sep=0pt,\nanchor=north]{Lifecycle};\n%%\n\\draw[Line,-latex](PL)|-(B1);\n\\draw[Line,-latex](DP)|-(B1);\n\\foreach \\i in{1,2,3}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](B\\i)--(B\\newI);\n}\n\n\\foreach \\i in{L,R}{\n\\foreach \\x in{2,3,4}{\n\\draw[Line,-latex](\\i B\\x)--(B\\x);\n  }\n}\n\\end{tikzpicture}}\n```\n**ML Lifecycle Threats**: Model theft, data poisoning, and adversarial attacks target distinct stages of the machine learning lifecycle (from data ingestion to model deployment and inference), creating unique vulnerabilities at each step. Understanding these lifecycle positions clarifies attack surfaces and guides the development of targeted defense strategies for robust AI systems.\n:::\n\nMachine learning models are not solely passive victims of attack; in some cases, they can be employed as components of an attack strategy. Pretrained models, particularly large generative or discriminative networks, may be adapted to automate tasks such as adversarial example generation, phishing content synthesis[^fn-phishing-ai], or protocol subversion. Open-source or publicly accessible models can be fine-tuned for malicious purposes, including impersonation, surveillance, or reverse-engineering of secure systems.\n\n[^fn-phishing-ai]: **AI-Generated Phishing**: Large language models can generate convincing phishing emails with 99%+ grammatical accuracy, compared to 19% for traditional phishing. Security firms report dramatic increases in AI-generated phishing attacks since 2022, with some studies citing 1,265% growth (though methodologies and baselines vary significantly), with some campaigns achieving 30%+ success rates. This dual-use potential necessitates a broader security perspective that considers models not only as assets to defend but also as possible instruments of attack.\n\n### Model Theft {#sec-security-privacy-model-theft-1879}\n\nThe first category of model-specific threats targets confidentiality. Threats to model confidentiality arise when adversaries gain access to a trained model's parameters, architecture, or output behavior. These attacks can undermine the economic value of machine learning systems, allow competitors to replicate proprietary functionality, or expose private information encoded in model weights.\n\nSuch threats arise across a range of deployment settings, including public APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-repositories]. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats[^fn-model-serialization], or insufficient access controls, factors that create opportunities for unauthorized extraction or replication [@ateniese2015hacking].\n\n[^fn-ml-apis]: **Machine Learning APIs**: Machine learning APIs (Application Programming Interfaces) were popularized by Google's Prediction API (2010). Today's ML APIs handle billions of requests daily, with major providers processing billions of tokens monthly, creating vast attack surfaces for model extraction.\n\n[^fn-model-repositories]: **Model Repositories**: Model repositories are centralized platforms for sharing ML models, led by Hugging Face (2016) which hosts 500,000+ models. While democratizing AI access, these repositories have become targets for supply chain attacks, with researchers finding malicious models in 5% of popular repositories [@oliynyk2023know].\n\n[^fn-model-serialization]: **Model Serialization**: Model serialization is the process of converting trained models into portable formats like ONNX (2017), TensorFlow SavedModel (2016), or PyTorch's .pth files. Insecure serialization can expose model weights and enable arbitrary code execution, affecting 80%+ of deployed ML systems [@ateniese2015hacking; @tramer2016stealing].\n\nThe severity of these threats is underscored by high-profile legal cases that have highlighted the strategic and economic value of machine learning models. For example, former Google engineer Anthony Levandowski was accused of [stealing proprietary designs from Waymo](https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html), including critical components of its autonomous vehicle technology, before founding a competing startup. Such cases illustrate the potential for insider threats to bypass technical protections and gain access to sensitive intellectual property.\n\nThe consequences of model theft extend beyond economic loss. Stolen models can be used to extract sensitive information, replicate proprietary algorithms, or enable further attacks. The economic impact can be substantial: research estimates suggest that aspects of large language models can be approximated through systematic API queries at costs orders of magnitude lower than original training, though full model replication remains economically and technically challenging [@tramer2016stealing; @carlini2024stealing]. For instance, a competitor who obtains a stolen recommendation model from an e-commerce platform might gain insights into customer behavior, business analytics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks[^fn-model-inversion-attack], where an attacker attempts to infer private details about the model's training data [@fredrikson2015model].\n\n[^fn-model-inversion-attack]: **Model Inversion Attacks**: Model inversion attacks were first demonstrated in 2015 against face recognition systems, when researchers reconstructed recognizable faces from neural network outputs using only confidence scores. The attack revealed that models trained on 40 individuals could leak identifiable facial features, proving that \"black-box\" API access isn't sufficient privacy protection.\n\nIn a model inversion attack, the adversary queries the model through a legitimate interface, such as a public API, and observes its outputs. By analyzing confidence scores or output probabilities, the attacker can optimize inputs to reconstruct data resembling the model's training set. For example, a facial recognition model used for secure access could be manipulated to reveal statistical properties of the employee photos on which it was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset[^fn-netflix-deanonymization], where researchers inferred individual movie preferences from anonymized data [@narayanan2006break].\n\n[^fn-netflix-deanonymization]: **Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"anonymous\" Prize dataset with public IMDb ratings. Using as few as 8 movie ratings with dates, they identified 99% of users, leading Netflix to cancel a second competition and highlighting the futility of naive anonymization.\n\nModel theft can target two distinct objectives: extracting exact model properties, such as architecture and parameters, or replicating approximate model behavior to produce similar outputs without direct access to internal representations. Understanding neural network architectures helps recognize which architectural patterns are most vulnerable to extraction attacks. The specific architectural vulnerabilities vary by model type, with deeper networks and attention-based architectures presenting different attack surfaces than simpler convolutional or recurrent designs. Both forms of theft undermine the security and value of machine learning systems, as explored in the following subsections.\n\nThese two attack paths are illustrated in @fig-model-theft-types. In exact model theft, the attacker gains access to the model's internal components, including serialized files, weights, and architecture definitions, and reproduces the model directly. In contrast, approximate model theft relies on observing the model's input-output behavior, typically through a public API. By repeatedly querying the model and collecting responses, the attacker trains a surrogate that mimics the original model's functionality. The first approach compromises the model's internal design and training investment, while the second threatens its predictive value and can facilitate further attacks such as adversarial example transfer or model inversion.\n\n::: {#fig-model-theft-types fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={line width=1.0pt,black!50},\nBox/.style={inner xsep=2pt,inner ysep=4pt,\n    node distance=0.4,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=38mm,\n    minimum width=38mm, minimum height=8mm\n  },\nBox2/.style={Box,  node distance=1.9,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2,text width=42mm,}\n}\n\n\\node[Box](B1){Access to public API};\n\\node[Box,below=of B1](B2){Send crafted queries};\n\\node[Box,below=of B2](B3){Record responses};\n\\node[Box,below=of B3](B4){Train surrogate model};\n\\node[Box,below=of B4](B5){Replicate predictions, launch further attacks};\n\\foreach \\i in{1,2,3,4}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](B\\i)--(B\\newI);\n}\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=10mm,minimum height=71.5mm,\nyshift=2.5mm,fill=BackColor!60,fit=(B1)(B5),line width=0.75pt](BB2){};\n\\node[below=6pt of  BB2.north,inner sep=0pt,\nanchor=north]{\\textbf{Approximate Model Theft}};\n%%%\n\\node[Box3,right=5 of B1](RB1){Access to model file or deployment artifact};\n\\node[Box3,right=5 of B5](RB4){Use or resell\\\\ proprietary IP};\n\\node[Box3](RB2)at($(RB1)!0.34!(RB4)$){Extract parameters, architecture, hyperparameters};\n\\node[Box3](RB3)at($(RB1)!0.67!(RB4)$){Reconstruct original\\\\ model};\n\\foreach \\i in{1,2,3}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](RB\\i)--(RB\\newI);\n}\n\\scoped[on background layer]\n\\node[draw=GreenD,inner xsep=10mm,minimum height=71mm,,\nyshift=2.2mm,fill=green!5,fit=(RB1)(RB4),line width=0.75pt](BB2){};\n\\node[below=6pt of  BB2.north,inner sep=0pt,\nanchor=north]{\\textbf{Exact Model Theft}};\n\\end{tikzpicture}\n```\n**Model Theft Strategies**: Attackers can target either a model’s internal parameters or its external behavior to create a stolen copy. Direct theft extracts model weights and architecture, while approximate theft trains a surrogate model by querying the original’s input-output behavior, potentially enabling further attacks despite lacking direct access to internal components.\n:::\n\n#### Exact Model Theft {#sec-security-privacy-exact-model-theft-b738}\n\nExact model property theft refers to attacks aimed at extracting the internal structure and learned parameters of a machine learning model. These attacks often target deployed models that are exposed through APIs, embedded in on-device inference engines, or shared as downloadable model files on collaboration platforms. Exploiting weak access control, insecure model packaging, or unprotected deployment interfaces, attackers can recover proprietary model assets without requiring full control of the underlying infrastructure.\n\nThese attacks typically seek three types of information. The first is the model's learned parameters, such as weights and biases. By extracting these parameters, attackers can replicate the model's functionality without incurring the cost of training. This replication allows them to benefit from the model's performance while bypassing the original development effort.\n\nThe second target is the model's fine-tuned hyperparameters, including training configurations such as learning rate, batch size, and regularization settings. These hyperparameters significantly influence model performance, and stealing them allows attackers to reproduce high-quality results with minimal additional experimentation.\n\nFinally, attackers may seek to reconstruct the model's architecture. This includes the sequence and types of layers, activation functions, and connectivity patterns that define the model's behavior. Architecture theft may be accomplished through side-channel attacks[^fn-ml-side-channel], reverse engineering, or analysis of observable model behavior.\n\n[^fn-ml-side-channel]: **ML Side-Channel Attacks**: Side-channel attacks on ML were first demonstrated against neural networks in 2018, when researchers showed that power consumption patterns during inference could reveal sensitive model information. This extended traditional cryptographic side-channel attacks into the ML domain, creating new vulnerabilities for edge AI devices.\n\nRevealing the architecture not only compromises intellectual property but also gives competitors strategic insights into the design choices that provide competitive advantage.\n\nSystem designers must account for these risks by securing model serialization formats, restricting access to runtime APIs, and hardening deployment pipelines. Protecting models requires a combination of software engineering practices, including access control, encryption, and obfuscation techniques, to reduce the risk of unauthorized extraction [@tramer2016stealing].\n\n#### Approximate Model Theft {#sec-security-privacy-approximate-model-theft-1155}\n\nWhile some attackers seek to extract a model's exact internal properties, others focus on replicating its external behavior. Approximate model behavior theft refers to attacks that attempt to recreate a model's decision-making capabilities without directly accessing its parameters or architecture. Instead, attackers observe the model's inputs and outputs to build a substitute model that performs similarly on the same tasks.\n\nThis type of theft often targets models deployed as services, where the model is exposed through an API or embedded in a user-facing application. By repeatedly querying the model and recording its responses, an attacker can train their own model to mimic the behavior of the original. This process, often called model distillation[^fn-model-distillation] or knockoff modeling, allows attackers to achieve comparable functionality without access to the original model's proprietary internals [@orekondy2019knockoff].\n\n[^fn-model-distillation]: **Model Distillation**: Model distillation is a knowledge transfer technique developed by [@hinton2015distilling] where a smaller \"student\" model learns from a larger \"teacher\" model. While designed for model compression, attackers exploit this to create stolen models with 95%+ accuracy using only 1% of the original training data.\n\nAttackers may evaluate the success of behavior replication in two ways. The first is by measuring the level of effectiveness of the substitute model. This involves assessing whether the cloned model achieves similar accuracy, precision, recall, or other performance metrics on benchmark tasks. By aligning the substitute's performance with that of the original, attackers can build a model that is practically indistinguishable in effectiveness, even if its internal structure differs.\n\nThe second is by testing prediction consistency. This involves checking whether the substitute model produces the same outputs as the original model when presented with the same inputs. Matching not only correct predictions but also the original model's mistakes can provide attackers with a high-fidelity reproduction of the target model's behavior. This poses particular concern in applications such as natural language processing, where attackers might replicate sentiment analysis models to gain competitive insights or bypass proprietary systems.\n\nApproximate behavior theft proves challenging to defend against in open-access deployment settings, such as public APIs or consumer-facing applications. Limiting the rate of queries, detecting automated extraction patterns, and watermarking model outputs are among the techniques that can help mitigate this risk. However, these defenses must be balanced with usability and performance considerations, especially in production environments.\n\nOne demonstration of approximate model theft extracts internal components of black-box language models via public APIs. In their paper, @carlini2024stealing, researchers show how to reconstruct the final embedding projection matrix of several OpenAI models, including `ada`, `babbage`, and `gpt-3.5-turbo`, using only public API access. By exploiting the low-rank structure of the output projection layer and making carefully crafted queries, they recover the model's hidden dimensionality and replicate the weight matrix up to affine transformations.\n\nThe attack does not reconstruct the full model, but reveals internal architecture parameters and sets a precedent for future, deeper extractions. This work demonstrated that even partial model theft poses risks to confidentiality and competitive advantage, especially when model behavior can be probed through rich API responses such as logit bias and log-probabilities.\n\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **Model**                         | **Size**                   | **Number of**       | **RMS**                        | **Cost (USD)**             |\n|                                   | **(Dimension Extraction)** | **Queries**         | **(Weight Matrix Extraction)** |                            |\n+:==================================+:===========================+====================:+:===============================+===========================:+\n| **OpenAI ada**                    | 1024 ✓                     | &lt; 2 \\times 10^6$ | $5 \\cdot 10^{-4}$              | $1 / $4                    |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI babbage**                | 2048 ✓                     | &lt; 4 \\times 10^6$ | $7 \\cdot 10^{-4}$              | $2 / $12                   |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI babbage-002**            | 1536 ✓                     | &lt; 4 \\times 10^6$ | Not implemented                | $2 / $12                   |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI gpt-3.5-turbo-instruct** | Not disclosed              | &lt; 4 \\times 10^7$ | Not implemented                | $200 / ~$2,000 (estimated) |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n| **OpenAI gpt-3.5-turbo-1106**     | Not disclosed              | &lt; 4 \\times 10^7$ | Not implemented                | $800 / ~$8,000 (estimated) |\n+-----------------------------------+----------------------------+---------------------+--------------------------------+----------------------------+\n\n: **Model Stealing Costs**: Attackers can extract model weights with a relatively low query cost using publicly available apis; the table quantifies this threat for OpenAI's ada and babbage models, showing that extracting weights achieves low root mean squared error (RMSE) with fewer than \\(4 \\cdot 10^6\\) queries. Estimated costs for weight extraction range from $1 to $12, demonstrating the economic feasibility of model stealing attacks despite API rate limits and associated expenses. Source: @carlini2024stealing. {#tbl-openai-theft}\n\nAs shown in their empirical evaluation, reproduced in @tbl-openai-theft, model parameters could be extracted with root mean square errors as low as $10^{-4}$, confirming that high-fidelity approximation is achievable at scale. These findings raise important implications for system design, suggesting that innocuous API features, like returning top-k logits, can serve as significant leakage vectors if not tightly controlled.\n\n#### Case Study: Tesla IP Theft {#sec-security-privacy-case-study-tesla-ip-theft-9d78}\n\nIn 2018, Tesla filed a [lawsuit](https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf) against the self-driving car startup [Zoox](https://zoox.com/), alleging that former Tesla employees had stolen proprietary data and trade secrets related to Tesla's autonomous driving technology. According to the lawsuit, several employees transferred over 10 gigabytes of confidential files, including machine learning models and source code, before leaving Tesla to join Zoox.\n\nAmong the stolen materials was a key image recognition model used for object detection in Tesla's self-driving system. By obtaining this model, Zoox could have bypassed years of research and development, giving the company a competitive advantage. Beyond the economic implications, there were concerns that the stolen model could expose Tesla to further security risks, such as model inversion attacks aimed at extracting sensitive data from the model's training set.\n\nThe Zoox employees denied any wrongdoing, and the case was ultimately settled out of court. The incident highlights the real-world risks of model theft, especially in industries where machine learning models represent significant intellectual property. The theft of models not only undermines competitive advantage but also raises broader concerns about privacy, safety, and the potential for downstream exploitation.\n\nThis case demonstrates that model theft is not limited to theoretical attacks conducted over APIs or public interfaces. Insider threats, supply chain vulnerabilities, and unauthorized access to development infrastructure pose equally serious risks to machine learning systems deployed in commercial environments.\n\n### Data Poisoning {#sec-security-privacy-data-poisoning-351f}\n\nWhile model theft targets confidentiality, the second category of threats focuses on training integrity. Training integrity threats stem from the manipulation of data used to train machine learning models. These attacks aim to corrupt the learning process by introducing examples that appear benign but induce harmful or biased behavior in the final model.\n\nData poisoning attacks are a prominent example, in which adversaries inject carefully crafted data points into the training set to influence model behavior in targeted or systemic ways [@biggio2012poisoning]. Poisoned data may cause a model to make incorrect predictions, degrade its generalization ability, or embed failure modes that remain dormant until triggered post-deployment.\n\nData poisoning is a security threat because it involves intentional manipulation of the training data by an adversary, with the goal of embedding vulnerabilities or subverting model behavior. These attacks pose concern in applications where models retrain on data collected from external sources, including user interactions, crowdsourced annotations[^fn-crowdsourcing-risks], and online scraping, since attackers can inject poisoned data without direct access to the training pipeline.\n\n[^fn-crowdsourcing-risks]: **Crowdsourcing Risks**: Platforms like Amazon Mechanical Turk (2005) and Prolific democratized data labeling but introduced poisoning risks. Studies show 15-30% of crowdsourced labels contain errors or bias [@biggio2012poisoning; @oprea2022poisoning], with coordinated attacks capable of poisoning entire datasets at costs under $1,000.\n\nThese attacks occur across diverse threat models. From a security perspective, poisoning attacks vary depending on the attacker's level of access and knowledge. In white-box scenarios, the adversary may have detailed insight into the model architecture or training process, enabling more precise manipulation. In contrast, black-box or limited-access attacks exploit open data submission channels or indirect injection vectors. Poisoning can target different stages of the ML pipeline, ranging from data collection and preprocessing to labeling and storage, making the attack surface both broad and system-dependent. The relative priority of data poisoning threats varies by deployment context as analyzed in @sec-security-privacy-threat-prioritization-framework-f2d5.\n\nPoisoning attacks typically follow a three-stage process. First, the attacker injects malicious data into the training set. These examples are often designed to appear legitimate but introduce subtle distortions that alter the model's learning process. Second, the model trains on this compromised data, embedding the attacker's intended behavior. Finally, once the model is deployed, the attacker may exploit the altered behavior to cause mispredictions, bypass safety checks, or degrade overall reliability.\n\nTo understand these attack mechanisms precisely, data poisoning can be viewed as a bilevel optimization problem, where the attacker seeks to select poisoning data $D_p$ that maximizes the model's loss on a validation or target dataset $D_{\\text{test}}$. Let $D$ represent the original training data. The attacker's objective is to solve:\n$$\n\\max_{D_p} \\ \\mathcal{L}(f_{D \\cup D_p}, D_{\\text{test}})\n$$\nwhere $f_{D \\cup D_p}$ represents the model trained on the combined dataset of original and poisoned data. For targeted attacks, this objective can be refined to focus on specific inputs $x_t$ and target labels $y_t$:\n$$\n\\max_{D_p} \\ \\mathcal{L}(f_{D \\cup D_p}, x_t, y_t)\n$$\n\nThis formulation captures the adversary's goal of introducing carefully crafted data points to manipulate the model's decision boundaries.\n\nFor example, consider a traffic sign classification model trained to distinguish between stop signs and speed limit signs. An attacker might inject a small number of stop sign images labeled as speed limit signs into the training data. The attacker's goal is to subtly shift the model's decision boundary so that future stop signs are misclassified as speed limit signs. In this case, the poisoning data $D_p$ consists of mislabeled stop sign images, and the attacker's objective is to maximize the misclassification of legitimate stop signs $x_t$ as speed limit signs $y_t$, following the targeted attack formulation above. Even if the model performs well on other types of signs, the poisoned training process creates a predictable and exploitable vulnerability.\n\nData poisoning attacks can be classified based on their objectives and scope of impact. Availability attacks degrade overall model performance by introducing noise or label flips that reduce accuracy across tasks. Targeted attacks manipulate a specific input or class, leaving general performance intact but causing consistent misclassification in select cases. Backdoor attacks[^fn-backdoor-attacks] embed hidden triggers, which are often imperceptible patterns, that elicit malicious behavior only when the trigger is present. Subpopulation attacks degrade performance on a specific group defined by shared features, making them particularly dangerous in fairness-sensitive applications.\n\n[^fn-backdoor-attacks]: **Backdoor Attacks**: Backdoor attacks involve hidden triggers embedded in ML models during training, first demonstrated in 2017. These attacks achieve 99%+ success rates while maintaining normal accuracy, with triggers as subtle as single-pixel modifications. BadNets, the seminal backdoor attack, affected 100% of tested models.\n\nA notable real-world example of a targeted poisoning attack was demonstrated against Perspective, Google's widely-used online toxicity detection model[^fn-perspective-api] that helps platforms identify harmful content [@hosseini2017deceiving]. By injecting synthetically generated toxic comments with subtle misspellings and grammatical errors into the model's training set, researchers degraded its ability to detect harmful content[^fn-perspective-vulnerability].\n\n[^fn-perspective-api]: **Perspective API**: Google's Perspective API is a toxicity detection model launched in 2017, now processing 500+ million comments daily across platforms like The New York Times and Wikipedia. Despite sophisticated training, the API demonstrates how even billion-parameter models remain vulnerable to targeted poisoning attacks.\n\n[^fn-perspective-vulnerability]: **Perspective Vulnerability**: After retraining, the poisoned model exhibited a significantly higher false negative rate, allowing offensive language to bypass filters. This demonstrates how poisoned data can exploit feedback loops in user-generated content systems, creating long-term vulnerabilities in content moderation pipelines.\n\nMitigating data poisoning threats requires end-to-end security of the data pipeline, encompassing collection, storage, labeling, and training. Preventative measures include input validation checks, integrity verification of training datasets, and anomaly detection to flag suspicious patterns. In parallel, robust training algorithms can limit the influence of mislabeled or manipulated data by down-weighting or filtering out anomalous instances. While no single technique guarantees immunity, combining proactive data governance, automated monitoring, and robust learning practices is important for maintaining model integrity in real-world deployments.\n\n### Adversarial Attacks {#sec-security-privacy-adversarial-attacks-9f84}\n\nMoving from training-time to inference-time threats, the third category targets model robustness during deployment. Inference robustness threats occur when attackers manipulate inputs at test time to induce incorrect predictions. Unlike data poisoning, which compromises the training process, these attacks exploit vulnerabilities in the model's decision surface during inference.\n\nA central class of such threats is adversarial attacks, where carefully constructed inputs are designed to cause incorrect predictions while remaining nearly indistinguishable from legitimate data. As detailed in @sec-robust-ai, these attacks highlight vulnerabilities in ML models' sensitivity to small, targeted perturbations that can drastically alter output confidence or classification results.\n\nThese attacks create significant real-world risks in domains such as autonomous driving, biometric authentication, and content moderation. The effectiveness can be striking: research demonstrates that adversarial examples can achieve 99%+ attack success rates against state-of-the-art image classifiers while modifying less than 0.01% of pixel values, changes virtually imperceptible to humans [@szegedy2014intriguing; @goodfellow2015explaining]. In physical-world attacks, printed adversarial patches as small as 2% of an image can cause autonomous vehicles to misclassify stop signs as speed limit signs with 80%+ success rates under varying lighting conditions [@eykholt2018robust].\n\nUnlike data poisoning, which corrupts the model during training, adversarial attacks manipulate the model's behavior at test time, often without requiring any access to the training data or model internals. The attack surface thus shifts from upstream data pipelines to real-time interaction, demanding robust defense mechanisms capable of detecting or mitigating malicious inputs at the point of inference.\n\nThe mathematical foundations of adversarial example generation and comprehensive taxonomies of attack algorithms, including gradient-based, optimization-based, and transfer-based techniques, are covered in detail in @sec-robust-ai, which explores robust approaches to building adversarially resistant systems.\n\nAdversarial attacks vary based on the attacker's level of access to the model. In white-box attacks, the adversary has full knowledge of the model's architecture, parameters, and training data, allowing them to craft highly effective adversarial examples. In black-box attacks, the adversary has no internal knowledge and must rely on querying the model and observing its outputs. Grey-box attacks fall between these extremes, with the adversary possessing partial information, such as access to the model architecture but not its parameters.\n\nThese attacker models can be summarized along a spectrum of knowledge levels. @tbl-adversary-knowledge-spectrum highlights the differences in model access, data access, typical attack strategies, and common deployment scenarios. Such distinctions help characterize the practical challenges of securing ML systems across different deployment environments.\n\nCommon attack strategies include surrogate model construction, transfer attacks exploiting adversarial transferability, and GAN-based perturbation generation. The technical details of these approaches and their mathematical formulations are thoroughly covered in @sec-robust-ai.\n\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n| **Adversary Knowledge Level** | **Model Access**                             | **Training Data Access** | **Attack Example**                             | **Common Scenario**                         |\n+:==============================+:=============================================+:=========================+:===============================================+:============================================+\n| **White-box**                 | Full access to architecture and parameters   | Full access              | Crafting adversarial examples using gradients  | Insider threats, open-source model reuse    |\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n| **Grey-box**                  | Partial access (e.g., architecture only)     | Limited or no access     | Attacks based on surrogate model approximation | Known model family, unknown fine-tuning     |\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n| **Black-box**                 | No internal access; only query-response view | No access                | Query-based surrogate model training and       | Public APIs, model-as-a-service deployments |\n|                               |                                              |                          | transfer attacks                               |                                             |\n+-------------------------------+----------------------------------------------+--------------------------+------------------------------------------------+---------------------------------------------+\n\n: **Adversarial Knowledge Spectrum**: Varying levels of attacker access to model details and training data define distinct threat models, influencing the feasibility and sophistication of adversarial attacks and impacting deployment security strategies. The table categorizes these models by access level, typical attack methods, and common deployment scenarios, clarifying the practical challenges of securing machine learning systems. {#tbl-adversary-knowledge-spectrum}\n\nOne illustrative example involves the manipulation of traffic sign recognition systems [@eykholt2018robust]. Researchers demonstrated that placing small stickers on stop signs could cause machine learning models to misclassify them as speed limit signs. While the altered signs remained easily recognizable to humans, the model consistently misinterpreted them. Such attacks pose serious risks in applications like autonomous driving, where reliable perception is important for safety.\n\nAdversarial attacks highlight the need for robust defenses that go beyond improving model accuracy. Securing ML systems against adversarial threats requires runtime defenses such as input validation, anomaly detection, and monitoring for abnormal patterns during inference. Training-time robustness methods, including adversarial training where models learn from perturbed examples, complement these runtime strategies and are explored later in this volume. These defenses aim to enhance model resilience against adversarial examples, ensuring that machine learning systems can operate reliably even in the presence of malicious inputs.\n\n### Case Study: Traffic Sign Attack {#sec-security-privacy-case-study-traffic-sign-attack-6e93}\n\nIn 2017, researchers conducted experiments by placing small black and white stickers on stop signs [@eykholt2018robust]. As shown in @fig-adversarial-stickers, these stickers were designed to be nearly imperceptible to the human eye, yet they significantly altered the appearance of the stop sign when viewed by machine learning models. When viewed by a normal human eye, the stickers did not obscure the sign or prevent interpretability. However, when images of the stickers stop signs were fed into standard traffic sign classification ML models, they were misclassified as speed limit signs over 85% of the time.\n\n![**Adversarial Stickers**: Nearly imperceptible stickers can trick machine learning models into misclassifying stop signs as speed limit signs over 85% of the time. This emphasizes the vulnerability of ML systems to adversarial attacks. Source: @eykholt2018robust.](./images/png/stop_signs.png){#fig-adversarial-stickers}\n\nThis demonstration showed how simple adversarial stickers could trick ML systems into misreading important road signs. If deployed realistically, these attacks could endanger public safety, causing autonomous vehicles to misinterpret stop signs as speed limits. Researchers warned this could potentially cause dangerous rolling stops or acceleration into intersections.\n\nThis case study provides a concrete illustration of how adversarial examples exploit the pattern recognition mechanisms of ML models. By subtly altering the input data, attackers can induce incorrect predictions and pose significant risks to safety-important applications like self-driving cars. The attack's simplicity demonstrates how even minor, imperceptible changes can lead models astray. Consequently, developers must implement robust defenses against such threats.\n\nThese threat types span different stages of the ML lifecycle and demand distinct defensive strategies. @tbl-threats-models-summary below summarizes their key characteristics.\n\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n| **Threat Type**         | **Lifecycle Stage** | **Attack Vector**         | **Example Impact**                            |\n+:========================+:====================+:==========================+:==============================================+\n| **Model Theft**         | Deployment          | API access, insider leaks | Stolen IP, model inversion, behavioral clone  |\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n| **Data Poisoning**      | Training            | Label flipping, backdoors | Targeted misclassification, degraded accuracy |\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n| **Adversarial Attacks** | Inference           | Input perturbation        | Real-time misclassification, safety failure   |\n+-------------------------+---------------------+---------------------------+-----------------------------------------------+\n\n: **Threat Landscape**: Machine learning systems face diverse threats throughout their lifecycle, ranging from data manipulation during training to model theft post-deployment. The table categorizes these threats by lifecycle stage and attack vector, clarifying how vulnerabilities manifest and enabling targeted mitigation strategies. {#tbl-threats-models-summary}\n\nThe appropriate defense for a given threat depends on its type, attack vector, and where it occurs in the ML lifecycle. @fig-threat-mitigation-flow provides a simplified decision flow that connects common threat categories, such as model theft, data poisoning, and adversarial examples, to corresponding defensive strategies. While real-world deployments may require more nuanced combinations of defenses as discussed in our layered defense framework, this flowchart serves as a conceptual guide for aligning threat models with practical mitigation techniques.\n\n::: {#fig-threat-mitigation-flow fig-env=\"figure\" fig-pos=\"H\"}\n```{.tikz}\n\\scalebox{0.65}{%\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={line width=1.0pt,black!50},\nBox/.style={inner xsep=2pt,inner ysep=2pt,\n    node distance=0.5,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=33mm,\n    minimum width=33mm, minimum height=9.5mm\n  },\nBox2/.style={Box,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2},\nBox4/.style={Box,  draw=OrangeLine,fill=OrangeL!50,text width=43mm}\n}\n\\node[Box](B1){Model Theft};\n\\node[Box,below=of B1](B2){Secure Model Access};\n\\node[Box,below=of B2](B3){Encrypt Artifacts \\& Obfuscate APIs};\n\\node[Box,below=of B3](B4){Monitor for Behavioral Clones};\n%\n\\node[Box2,right=1.75of B1](SB1){Data Poisoning};\n\\node[Box2,below=of SB1](SB2){Validate Training Data};\n\\node[Box2,below=of SB2](SB3){Use Robust Training Methods};\n\\node[Box2,below=of SB3](SB4){Apply Data Provenance Checks};\n%\n\\node[Box3,right=1.75of SB1](RB1){Adversarial Examples};\n\\node[Box3,below=of RB1](RB2){Add Input Validation};\n\\node[Box3,below=of RB2](RB3){Use Adversarial Training};\n\\node[Box3,below=of RB3](RB4){Deploy Runtime Monitors};\n%\n\\node[Box4,above=0.6of SB1](B0){Start: Identify Threat Type};\n\\foreach \\x in{,S,R}{\n\\foreach \\i in{1,2,3}{\n\\pgfmathtruncatemacro{\\newI}{\\i + 1}\n\\draw[Line,-latex](\\x B\\i)--(\\x B\\newI);\n}\n}\n\\draw[Line,-latex](B0)--(SB1);\n\\draw[Line,-latex](B0)-|(B1);\n\\draw[Line,-latex](B0)-|(RB1);\n\\end{tikzpicture}}\n```\n**Threat Mitigation Flow**: This diagram maps common machine learning threats to corresponding defense strategies, guiding selection based on attack vector and lifecycle stage. By following this flow, practitioners can align threat models with practical mitigation techniques, such as secure model access and data sanitization, to build more robust AI systems.\n:::\n\nWhile ML models themselves present important attack surfaces, they ultimately run on hardware that can introduce vulnerabilities beyond the model's control. The transition from software-based threats to hardware-based vulnerabilities represents a significant shift in the security landscape. Where software attacks target code logic and data flows, hardware attacks exploit the physical properties of the computing substrate itself.\n\nThe specialized computing infrastructure that powers machine learning workloads creates a layered attack surface that extends far beyond traditional software vulnerabilities. This includes the processors that execute instructions, the memory systems that store data, and the interconnects that move information between components. Understanding these hardware-level risks is essential because they can bypass conventional software security mechanisms and remain difficult to detect. These risks are addressed through the hardware-based security mechanisms detailed in @sec-security-privacy-hardware-security-foundations-f5e8.\n\nIn the next section, we examine how adversaries can target the physical infrastructure that executes machine learning workloads through hardware bugs, physical tampering, side channels, and supply chain risks.\n\n## Hardware-Level Security Vulnerabilities {#sec-security-privacy-hardwarelevel-security-vulnerabilities-1ab4}\n\nAs machine learning systems move from research prototypes to large-scale, real-world deployments, their security depends on the hardware platforms they run on. Whether deployed in data centers, on edge devices, or in embedded systems, machine learning applications rely on a layered stack of processors, accelerators, memory, and communication interfaces. These hardware components, while essential for enabling efficient computation, introduce unique security risks that go beyond traditional software-based vulnerabilities.\n\nUnlike general-purpose software systems, machine learning workflows often process high-value models and sensitive data in performance-constrained environments. This makes them attractive targets not only for software attacks but also for hardware-level exploitation. Vulnerabilities in hardware can expose models to theft, leak user data, disrupt system reliability, or allow adversaries to manipulate inference results. Because hardware operates below the software stack, such attacks can bypass conventional security mechanisms and remain difficult to detect.\n\nUnderstanding hardware security threats requires considering how computing substrates implement machine learning operations. At the hardware level, CPU components like arithmetic logic units, registers, and caches execute the instructions that drive model inference and training. Memory hierarchies determine how quickly models can access parameters and intermediate results. The hardware-software interface, mediated by firmware and bootloaders, establishes the initial trust foundation for system operation. The physical properties of computation—including power consumption, timing characteristics, and electromagnetic emissions—create observable signals that attackers can exploit to extract sensitive information.\n\nHardware threats arise from multiple sources that span the entire system lifecycle. Design flaws in processor architectures, exemplified by vulnerabilities like Meltdown and Spectre, can compromise security guarantees. Physical tampering enables direct manipulation of components and data flows. Side-channel attacks exploit unintended information leakage through power traces, timing variations, and electromagnetic radiation. Supply chain compromises introduce malicious components or modifications during manufacturing and distribution. Together, these threats form a critical attack surface that must be addressed to build trustworthy machine learning systems. For readers focusing on practical deployment, the key lessons center on supply chain verification, physical access controls, and hardware trust anchors, while the defensive strategies in @sec-security-privacy-comprehensive-defense-architectures-48ab provide actionable guidance regardless of deep architectural expertise.\n\n@tbl-threat_types summarizes the major categories of hardware security threats, describing their origins, methods, and implications for machine learning system design and deployment.\n\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Threat Type**             | **Description**                                                                                 | **Relevance to ML Hardware Security**          |\n+:============================+:================================================================================================+:===============================================+\n| **Hardware Bugs**           | Intrinsic flaws in hardware designs that can compromise system integrity.                       | Foundation of hardware vulnerability.          |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Physical Attacks**        | Direct exploitation of hardware through physical access or manipulation.                        | Basic and overt threat model.                  |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Fault-injection Attacks** | Induction of faults to cause errors in hardware operation, leading to potential system crashes. | Systematic manipulation leading to failure.    |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Side-Channel Attacks**    | Exploitation of leaked information from hardware operation to extract sensitive data.           | Indirect attack via environmental observation. |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Leaky Interfaces**        | Vulnerabilities arising from interfaces that expose data unintentionally.                       | Data exposure through communication channels.  |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Counterfeit Hardware**    | Use of unauthorized hardware components that may have security flaws.                           | Compounded vulnerability issues.               |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n| **Supply Chain Risks**      | Risks introduced through the hardware lifecycle, from production to deployment.                 | Cumulative & multifaceted security challenges. |\n+-----------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------+\n\n: **Hardware Threat Landscape**: Machine learning systems face diverse hardware threats ranging from intrinsic design flaws to physical attacks and supply chain vulnerabilities. Understanding these threats, and their relevance to ML hardware, is essential for building secure and trustworthy AI deployments. {#tbl-threat_types}\n\n### Hardware Bugs {#sec-security-privacy-hardware-bugs-9efc}\n\nThe first category of hardware threats stems from design vulnerabilities. Hardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. One of the most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[^fn-meltdown-spectre-impact]—two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation and read the data of other applications and the operating system [@Lipp2018meltdown; @Kocher2018spectre].\n\n[^fn-meltdown-spectre-impact]: **Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually every processor made since 1995 (billions of devices). The disclosure triggered emergency patches across all major operating systems, causing 5-30% performance degradation in some workloads, and led to a core rethinking of processor security.\n\nThese attacks exploit speculative execution[^fn-speculative-execution], a performance optimization in CPUs that executes instructions out of order before safety checks are complete. While improving computational speed, this optimization inadvertently exposes sensitive data through microarchitectural side channels, such as CPU caches. The technical sophistication of these attacks highlights the difficulty of eliminating vulnerabilities even with extensive hardware validation.\n\n[^fn-speculative-execution]: **Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes instructions before confirming they're needed, improving performance by 10-25%. However, it creates a 20+ year attack window where speculated operations leak data through cache timing, affecting ML accelerators that rely on similar optimizations.\n\nFurther research has revealed that these were not isolated incidents. Variants such as Foreshadow, ZombieLoad, and RIDL target different microarchitectural elements, ranging from secure enclaves to CPU internal buffers, demonstrating that speculative execution flaws are a systemic hardware risk. This systemic nature means that while these attacks were first demonstrated on general-purpose CPUs, their implications extend to machine learning accelerators and specialized hardware. ML systems often rely on heterogeneous compute platforms that combine CPUs with GPUs, TPUs, FPGAs, or custom accelerators. These components process sensitive data such as personal information, medical records, or proprietary models. Vulnerabilities in any part of this stack could expose such data to attackers.\n\nFor example, an edge device like a smart camera running a face recognition model on an accelerator could be vulnerable if the hardware lacks proper cache isolation. An attacker might exploit this weakness to extract intermediate computations, model parameters, or user data. Similar risks exist in cloud inference services, where hardware multi-tenancy increases the chances of cross-tenant data leakage.\n\nSuch vulnerabilities pose concern in privacy-sensitive domains like healthcare, where ML systems routinely handle patient data. A breach could violate privacy regulations such as the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[^fn-hipaa-violations], leading to significant legal and ethical consequences. Similar regulatory risks apply globally, with GDPR[^fn-gdpr] imposing fines up to 4% of global revenue for organizations that fail to implement appropriate technical measures to protect EU citizens' data.\n\n[^fn-hipaa-violations]: **HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fines, with individual penalties reaching $16 million. The largest healthcare data breach affected 78.8 million patients at Anthem Inc. in 2015, highlighting the massive scale of exposure when ML systems handling medical data are compromised.\n\n[^fn-gdpr]: **General Data Protection Regulation (GDPR)**: Enacted by the EU in 2018, GDPR imposes fines up to 4% of global revenue (€20+ million) for privacy violations. Since enforcement began, over €4.5 billion in fines have been levied, including €746 million against Amazon in 2021, driving massive investment in privacy-preserving ML technologies.\n\nThese examples illustrate that hardware security is not solely about preventing physical tampering. It also requires architectural safeguards to prevent data leakage through the hardware itself. As new vulnerabilities continue to emerge across processors, accelerators, and memory systems, addressing these risks requires continuous mitigation efforts, often involving performance trade-offs, especially in compute- and memory-intensive ML workloads. Proactive solutions, such as confidential computing and trusted execution environments (TEEs), offer promising architectural defenses. However, achieving robust hardware security requires attention at every stage of the system lifecycle, from design to deployment.\n\n### Physical Attacks {#sec-security-privacy-physical-attacks-095a}\n\nBeyond design flaws, the second category involves direct physical manipulation. Physical tampering refers to the direct, unauthorized manipulation of computing hardware to undermine the integrity of machine learning systems. This type of attack is particularly concerning because it bypasses traditional software security defenses, directly targeting the physical components on which machine learning depends. ML systems are especially vulnerable to such attacks because they rely on hardware sensors, accelerators, and storage to process large volumes of data and produce reliable outcomes in real-world environments.\n\nWhile software security measures, including encryption, authentication, and access control, protect ML systems against remote attacks, they offer little defense against adversaries with physical access to devices. Physical tampering can range from simple actions, like inserting a malicious USB device into an edge server, to highly sophisticated manipulations such as embedding hardware trojans during chip manufacturing. These threats are particularly relevant for machine learning systems deployed at the edge or in physically exposed environments, where attackers may have opportunities to interfere with the hardware directly.\n\nTo understand how such attacks affect ML systems in practice, consider the example of an ML-powered drone used for environmental mapping or infrastructure inspection. The drone's navigation depends on machine learning models that process data from GPS, cameras, and inertial measurement units. If an attacker gains physical access to the drone, they could replace or modify its navigation module, embedding a hidden backdoor that alters flight behavior or reroutes data collection. Such manipulation not only compromises the system's reliability but also opens the door to misuse, such as surveillance or smuggling operations.\n\nThese threats extend across application domains. Physical attacks are not limited to mobility systems. Biometric access control systems, which rely on ML models to process face or fingerprint data, are also vulnerable. These systems typically use embedded hardware to capture and process biometric inputs. An attacker could physically replace a biometric sensor with a modified component designed to capture and transmit personal identification data to an unauthorized receiver. This creates multiple vulnerabilities including unauthorized data access and enabling future impersonation attacks.\n\nIn addition to tampering with external sensors, attackers may target internal hardware subsystems. For example, the sensors used in autonomous vehicles, including cameras, LiDAR, and radar, are important for ML models that interpret the surrounding environment. A malicious actor could physically misalign or obstruct these sensors, degrading the model's perception capabilities and creating safety hazards.\n\nHardware trojans pose another serious risk. Malicious modifications introduced during chip fabrication or assembly can embed dormant circuits in ML accelerators or inference chips. These trojans may remain inactive under normal conditions but trigger malicious behavior when specific inputs are processed or system states are reached. Such hidden vulnerabilities can disrupt computations, leak model outputs, or degrade system performance in ways that are extremely difficult to diagnose post-deployment.\n\nMemory subsystems are also attractive targets. Attackers with physical access to edge devices or embedded ML accelerators could manipulate memory chips to extract encrypted model parameters or training data. Fault injection techniques, including voltage manipulation and electromagnetic interference, can further degrade system reliability by corrupting model weights or forcing incorrect computations during inference.\n\nPhysical access threats extend to data center and cloud environments as well. Attackers with sufficient access could install hardware implants, such as keyloggers or data interceptors, to capture administrative credentials or monitor data streams. Such implants can provide persistent backdoor access, enabling long-term surveillance or data exfiltration from ML training and inference pipelines.\n\nIn summary, physical attacks on machine learning systems threaten both security and reliability across a wide range of deployment environments. Addressing these risks requires a combination of hardware-level protections, tamper detection mechanisms, and supply chain integrity checks. Without these safeguards, even the most secure software defenses may be undermined by vulnerabilities introduced through direct physical manipulation.\n\n### Fault Injection Attacks {#sec-security-privacy-fault-injection-attacks-8c52}\n\nBuilding on physical tampering techniques, fault injection represents a more sophisticated approach to hardware exploitation. Fault injection is a powerful class of physical attacks that deliberately disrupts hardware operations to induce errors in computation. These induced faults can compromise the integrity of machine learning models by causing them to produce incorrect outputs, degrade reliability, or leak sensitive information. For ML systems, such faults not only disrupt inference but also expose models to deeper exploitation, including reverse engineering and bypass of security protocols [@joye2012fault].\n\nAttackers achieve fault injection by applying precisely timed physical or electrical disturbances to the hardware while it is executing computations. Techniques such as low-voltage manipulation [@barenghi2010low], power spikes [@hutter2009contact], clock glitches [@amiel2006fault], electromagnetic pulses [@agrawal2003side], temperature variations [@skorobogatov2009local], and even laser strikes [@skorobogatov2003optical] have been demonstrated to corrupt specific parts of a program's execution. These disturbances can cause effects such as bit flips, skipped instructions, or corrupted memory states, which adversaries can exploit to alter ML model behavior or extract sensitive information.\n\nFor machine learning systems, these attacks pose several concrete risks. Fault injection can degrade model accuracy, force incorrect classifications, trigger denial of service, or even leak internal model parameters. For example, attackers could inject faults into an embedded ML model running on a microcontroller, forcing it to misclassify inputs in safety-important applications such as autonomous navigation or medical diagnostics. More sophisticated attackers may target memory or control logic to steal intellectual property, such as proprietary model weights or architecture details.\n\nThe practical viability of these attacks has been demonstrated through controlled experiments. One notable example is the work by @breier2018deeplaser, where researchers successfully used a laser fault injection attack on a deep neural network deployed on a microcontroller. By heating specific transistors, as shown in @fig-laser-bitflip. they forced the hardware to skip execution steps, including a ReLU activation function.\n\n![**Laser Fault Injection**: Focused laser pulses induce bit flips within microcontroller memory, enabling attackers to manipulate model execution and compromise system integrity. Researchers utilize this technique to simulate hardware errors, revealing vulnerabilities in embedded machine learning systems and informing the development of fault-tolerant designs. Source: [@breier2018deeplaser].](images/png/laser_bitflip.png){#fig-laser-bitflip fig-pos=t!}\n\nThis manipulation is illustrated in @fig-injection, which shows a segment of assembly code implementing the ReLU activation function. Normally, the code compares the most significant bit (MSB) of the accumulator to zero and uses a brge (branch if greater or equal) instruction to skip the assignment if the value is non-positive. However, the fault injection suppresses the branch, causing the processor to always execute the \"else\" block. As a result, the neuron's output is forcibly zeroed out, regardless of the input value.\n\n![**Fault Injection Attack**: Manipulating assembly code bypasses safety checks, forcing a neuron’s output to zero regardless of input and demonstrating a hardware vulnerability in machine learning systems. Source: [@breier2018deeplaser].](images/png/fault-injection_demonstrated_with_assembly_code.png){#fig-injection width=75% fig-pos=b!}\n\nFault injection attacks can also be combined with side-channel analysis, where attackers first observe power or timing characteristics to infer model structure or data flow. This reconnaissance allows them to target specific layers or operations, such as activation functions or final decision layers, maximizing the impact of the injected faults.\n\nEmbedded and edge ML systems are particularly vulnerable because they often lack physical hardening and operate under resource constraints that limit runtime defenses. Without tamper-resistant packaging or secure hardware enclaves, attackers may gain direct access to system buses and memory, enabling precise fault manipulation. Many embedded ML models are designed to be lightweight, leaving them with little redundancy or error correction to recover from induced faults.\n\nMitigating fault injection requires multiple complementary protections. Physical protections, such as tamper-proof enclosures and design obfuscation, help limit physical access. Anomaly detection techniques can monitor sensor inputs or model outputs for signs of fault-induced inconsistencies [@hsiao2023mavfi]. Error-correcting memories and secure firmware can reduce the likelihood of silent corruption. Techniques such as model watermarking may provide traceability if stolen models are later deployed by an adversary.\n\nThese protections are difficult to implement in cost- and power-constrained environments, where adding cryptographic hardware or redundancy may not be feasible. Achieving resilience to fault injection requires cross-layer design considerations that span electrical, firmware, software, and system architecture levels. Without such holistic design practices, ML systems deployed in the field may remain exposed to these low-cost yet highly effective physical attacks.\n\n### Side-Channel Attacks {#sec-security-privacy-sidechannel-attacks-cdfd}\n\nMoving from direct fault injection to indirect information leakage, side-channel attacks constitute a class of security breaches that exploit information inadvertently revealed through the physical implementation of computing systems. In contrast to direct attacks that target software or network vulnerabilities, these attacks use the system's hardware characteristics, including power consumption, electromagnetic emissions, or timing behavior, to extract sensitive information.\n\nThe core premise of a side-channel attack is that a device's operation can leak information through observable physical signals. Such leaks may originate from the electrical power the device consumes [@kocher1999differential], the electromagnetic fields it emits [@gandolfi2001electromagnetic], the time required to complete computations, or even the acoustic noise it produces. By carefully measuring and analyzing these signals, attackers can infer internal system states or recover secret data.\n\nAlthough these techniques are commonly discussed in cryptography, they are equally relevant to machine learning systems. ML models deployed on hardware accelerators, embedded devices, or edge systems often process sensitive data. Even when these models are protected by secure algorithms or encryption, their physical execution may leak side-channel signals that can be exploited by adversaries.\n\nOne of the most widely studied examples involves Advanced Encryption Standard (AES)[^fn-aes-standard] implementations. While AES is mathematically secure, the physical process of computing its encryption functions leaks measurable signals.\n\n[^fn-aes-standard]: **Advanced Encryption Standard (AES)**: Adopted by NIST in 2001 as the US government encryption standard, AES replaced DES after 24 years. Despite being mathematically secure with 2^128 possible keys for AES-128, physical implementations remain vulnerable to side-channel attacks that can extract keys in minutes. Techniques such as Differential Power Analysis (DPA), Differential Electromagnetic Analysis (DEMA), and Correlation Power Analysis (CPA) exploit these physical signals to recover secret keys.\n\nA useful example of this attack technique can be seen in a power analysis of a password authentication process. Consider a device that verifies a 5-byte password—in this case, `0x61, 0x52, 0x77, 0x6A, 0x73`. During authentication, the device receives each byte sequentially over a serial interface, and its power consumption pattern reveals how the system responds as it processes these inputs.\n\n@fig-encryption shows the device's behavior when the correct password is entered. The red waveform captures the serial data stream, marking each byte as it is received. The blue curve records the device's power consumption over time. When the full, correct password is supplied, the power profile remains stable and consistent across all five bytes, providing a clear baseline for comparison with failed attempts.\n\n![**Power Profile**: The device's power consumption remains stable during authentication when the correct password is entered, setting a baseline for comparison in subsequent figures through This figure. Source: colin o'flynn.](images/png/power_analysis_of_an_encryption_device_with_a_correct_password.png){#fig-encryption}\n\nWhen an incorrect password is entered, the power analysis chart changes as shown in @fig-encryption2. In this case, the first three bytes (`0x61, 0x52, 0x77`) are correct, so the power patterns closely match the correct password up to that point. However, when the fourth byte (`0x42`) is processed and found to be incorrect, the device halts authentication. This change is reflected in the sudden jump in the blue power line, indicating that the device has stopped processing and entered an error state.\n\n![**Side-Channel Attack Vulnerability**: Power consumption patterns reveal cryptographic key information during authentication; consistent power usage indicates correct password bytes, while abrupt changes signal incorrect input and halted processing. Even without knowing the password, an attacker can infer it by analyzing the device's power usage during authentication attempts via this figure. Source: Colin O'Flynn.](images/png/power_analysis_of_an_encryption_device_with_a_partially_wrong_password.png){#fig-encryption2}\n\n@fig-encryption3 shows the case where the password is entirely incorrect (`0x30, 0x30, 0x30, 0x30, 0x30`). Here, the device detects the mismatch immediately after the first byte and halts processing much earlier. This is again visible in the power profile, where the blue line exhibits a sharp jump following the first byte, reflecting the device's early termination of authentication.\n\n![**Power Consumption Jump**: The blue line's sharp increase after processing the first byte indicates immediate authentication failure, highlighting how incorrect passwords are quickly detected through power usage. Source: Colin O'Flynn.](images/png/power_analysis_of_an_encryption_device_with_a_wrong_password.png){#fig-encryption3}\n\nThese examples demonstrate how attackers can exploit observable power consumption differences to reduce the search space and eventually recover secret data through brute-force analysis. By systematically measuring power consumption patterns and correlating them with different inputs, attackers can extract sensitive information that should remain hidden.\n\n{{< margin-video \"https://www.youtube.com/watch?v=2iDLfuEBcs8\" \"Power Attack\" \"Colin O'Flynn\" >}}\n\nThe scope of these vulnerabilities extends beyond cryptographic applications. Machine learning applications face similar risks. For example, an ML-based speech recognition system processing voice commands on a local device could leak timing or power signals that reveal which commands are being processed. Even subtle acoustic or electromagnetic emissions may expose operational patterns that an adversary could exploit to infer user behavior.\n\nHistorically, side-channel attacks have been used to bypass even the most secure cryptographic systems. In the 1960s, British intelligence agency MI5 famously exploited acoustic emissions from a cipher machine in the Egyptian Embassy [@Burnet1989Spycatcher]. By capturing the mechanical clicks of the machine's rotors, MI5 analysts were able to dramatically reduce the complexity of breaking encrypted messages. This early example illustrates that side-channel vulnerabilities are not confined to the digital age but are rooted in the physical nature of computation.\n\nToday, these techniques have advanced to include attacks such as keyboard eavesdropping [@Asonov2004Keyboard], power analysis on cryptographic hardware [@gnad2017voltage], and voltage-based attacks on ML accelerators [@zhao2018fpga]. Timing attacks, electromagnetic leakage, and thermal emissions continue to provide adversaries with indirect channels for observing system behavior.\n\nMachine learning systems deployed on specialized accelerators or embedded platforms are especially at risk. Attackers may exploit side-channel signals to infer model structure, steal parameters, or reconstruct private training data. As ML becomes increasingly deployed in cloud, edge, and embedded environments, these side-channel vulnerabilities pose significant challenges to system security.\n\nUnderstanding the persistence and evolution of side-channel attacks is important for building resilient machine learning systems. By recognizing that where there is a signal, there is potential for exploitation, system designers can begin to address these risks through a combination of hardware shielding, algorithmic defenses, and operational safeguards.\n\n### Leaky Interfaces {#sec-security-privacy-leaky-interfaces-9206}\n\nWhile side-channel attacks exploit unintended physical signals, leaky interfaces represent a different category of vulnerability involving exposed communication channels. Interfaces in computing systems are important for enabling communication, diagnostics, and updates. However, these same interfaces can become significant security vulnerabilities when they unintentionally expose sensitive information or accept unverified inputs. Such leaky interfaces often go unnoticed during system design, yet they provide attackers with powerful entry points to extract data, manipulate functionality, or introduce malicious code.\n\nA leaky interface is any access point that reveals more information than intended, often because of weak authentication, lack of encryption, or inadequate isolation. These issues have been widely demonstrated across consumer, medical, and industrial systems.\n\nFor example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports[^fn-iot-vulnerabilities], allowing attackers to intercept live audio and video feeds from inside private homes. Similarly, researchers have identified wireless vulnerabilities in pacemakers[^fn-medical-device-security] that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening safety concerns.\n\n[^fn-iot-vulnerabilities]: **IoT Device Vulnerabilities**: Studies reveal 70-80% of IoT devices contain serious security flaws, with baby monitors among the worst offenders. Security firm Rapid7 found that popular baby monitor brands exposed unencrypted video streams, affecting millions of households globally.\n\n[^fn-medical-device-security]: **Medical Device Security**: FDA reports show 53% of medical devices contain known vulnerabilities, with pacemakers and insulin pumps most at risk. The average medical device contains 6.2 vulnerabilities, some dating back over a decade, affecting 2.4 billion medical devices worldwide.\n\nA notable case involving smart lightbulbs demonstrated that accessible debug ports[^fn-debug-ports] left on production devices leaked unencrypted WiFi credentials. This security oversight provided attackers with a pathway to infiltrate home networks without needing to bypass standard security mechanisms.\n\n[^fn-debug-ports]: **Debug Port Vulnerabilities**: Hardware debug interfaces like JTAG (1990) and SWD (2006) are essential for development but often left accessible in production. Security researchers estimate that 60-70% of embedded devices ship with unsecured debug ports, creating backdoors for attackers.\n\nThese examples reveal vulnerability patterns that directly apply to machine learning deployments. While these examples do not target machine learning systems directly, they illustrate architectural patterns that are highly relevant to ML-allowd devices. Consider a smart home security system that uses machine learning to detect user routines and automate responses. Such a system may include a maintenance or debug interface for software updates. If this interface lacks proper authentication or transmits data unencrypted, attackers on the same network could gain unauthorized access. This intrusion could expose user behavior patterns, compromise model integrity, or disable security features altogether.\n\nLeaky interfaces in ML systems can also expose training data, model parameters, or intermediate outputs. Such exposure can allow attackers to craft adversarial examples, steal proprietary models, or reverse-engineer system behavior. Worse still, these interfaces may allow attackers to tamper with firmware, introducing malicious code that disables devices or recruits them into botnets.\n\nMitigating these risks requires coordinated protections across technical and organizational domains. Technical safeguards such as strong authentication, encrypted communications, and runtime anomaly detection are important. Organizational practices such as interface inventories, access control policies, and ongoing audits are equally important. Adopting a zero-trust architecture, where no interface is trusted by default, further reduces exposure by limiting access to only what is strictly necessary.\n\nFor designers of ML-powered systems, securing interfaces must be a first-class concern alongside algorithmic and data-centric design. Whether the system operates in the cloud, on the edge, or in embedded environments, failure to secure these access points risks undermining the entire system's trustworthiness.\n\n### Counterfeit Hardware {#sec-security-privacy-counterfeit-hardware-36fd}\n\nBeyond vulnerabilities in legitimate hardware, another significant threat emerges from the supply chain itself. Machine learning systems depend on the reliability and security of the hardware on which they run. Yet, in today's globalized hardware ecosystem, the risk of counterfeit or cloned hardware has emerged as a serious threat to system integrity. Counterfeit components refer to unauthorized reproductions of genuine parts, designed to closely imitate their appearance and functionality. These components can enter machine learning systems through complex procurement and manufacturing processes that span multiple vendors and regions.\n\nA single lapse in component sourcing can introduce counterfeit hardware into important systems. For example, a facial recognition system deployed for secure facility access might unknowingly rely on counterfeit processors. These unauthorized components could fail to process biometric data correctly or introduce hidden vulnerabilities that allow attackers to bypass authentication controls.\n\nThe risks posed by counterfeit hardware are multifaceted. From a reliability perspective, such components often degrade faster, perform unpredictably, or fail under load due to substandard manufacturing. From a security perspective, counterfeit hardware may include hidden backdoors or malicious circuitry, providing attackers with undetectable pathways to compromise machine learning systems. A cloned network router installed in a data center, for instance, could silently intercept model predictions or user data, creating systemic vulnerabilities across the entire infrastructure.\n\nLegal and regulatory risks further compound the problem. Organizations that unknowingly integrate counterfeit components into their ML systems may face serious legal consequences, including penalties for violating safety, privacy, or cybersecurity regulations[^fn-cybersecurity-regulations]. This is particularly concerning in sectors such as healthcare and finance, where compliance with industry standards is non-negotiable. Healthcare organizations must demonstrate HIPAA compliance throughout their technology stack, while organizations handling EU citizens' data must meet GDPR's requirements for technical and organizational measures, including supply chain integrity.\n\n[^fn-cybersecurity-regulations]: **Cybersecurity Regulations**: Global cybersecurity compliance costs exceed $150 billion annually, with frameworks like SOC 2, ISO 27001, PCI DSS, and sector-specific rules governing ML systems. Financial services face additional requirements under regulations like SOX, while healthcare must comply with HIPAA, creating complex multi-regulatory environments.\n\nEconomic pressures often incentivize sourcing from lower-cost suppliers without rigorous verification, increasing the likelihood of counterfeit parts entering production systems. Detection is especially challenging, as counterfeit components are designed to mimic legitimate ones. Identifying them may require specialized equipment or forensic analysis, making prevention far more practical than remediation.\n\nThe stakes are particularly high in machine learning applications that require high reliability and low latency, such as real-time decision-making in autonomous vehicles, industrial automation, or important healthcare diagnostics. Hardware failure in these contexts can lead not only to system downtime but also to significant safety risks. Consequently, as machine learning continues to expand into safety-important and high-value applications, counterfeit hardware presents a growing risk that must be recognized and addressed. Organizations must treat hardware trustworthiness as a core design requirement, on par with algorithmic accuracy and data security, to ensure that ML systems can operate reliably and securely in the real world.\n\n### Supply Chain Risks {#sec-security-privacy-supply-chain-risks-c99c}\n\nCounterfeit hardware exemplifies a broader systemic challenge. While counterfeit hardware presents a serious challenge, it is only one part of the larger problem of securing the global hardware supply chain. Machine learning systems are built from components that pass through complex supply networks involving design, fabrication, assembly, distribution, and integration. Each of these stages presents opportunities for tampering, substitution, or counterfeiting—often without the knowledge of those deploying the final system.\n\nMalicious actors can exploit these vulnerabilities in various ways. A contracted manufacturer might unknowingly receive recycled electronic waste that has been relabeled as new components. A distributor might deliberately mix cloned parts into otherwise legitimate shipments. Insiders at manufacturing facilities might embed hardware Trojans that are nearly impossible to detect once the system is deployed. Advanced counterfeits can be particularly deceptive, with refurbished or repackaged components designed to pass visual inspection while concealing inferior or malicious internals.\n\nIdentifying such compromises typically requires sophisticated analysis, including micrography, X-ray screening, and functional testing. However, these methods are costly and impractical for large-scale procurement. As a result, many organizations deploy systems without fully verifying the authenticity and security of every component.\n\nThe risks extend beyond individual devices. Machine learning systems often rely on heterogeneous hardware platforms, integrating CPUs, GPUs, memory, and specialized accelerators sourced from a global supply base. Any compromise in one part of this chain can undermine the security of the entire system. These risks are further amplified when systems operate in shared or multi-tenant environments, such as cloud data centers or federated edge networks, where hardware-level isolation is important to preventing cross-tenant attacks.\n\nThe 2018 Bloomberg Businessweek report alleging that Chinese state actors inserted spy chips into Supermicro server motherboards brought these risks to mainstream attention. While the claims remain disputed, the story underscored the industry's limited visibility into its own hardware supply chains. Companies often rely on complex, opaque manufacturing and distribution networks, leaving them vulnerable to hidden compromises. Over-reliance on single manufacturers or regions, including the semiconductor industry's reliance on TSMC, further concentrates this risk. This recognition has driven policy responses like the U.S. [CHIPS and Science Act](https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2024/08/09/fact-sheet-two-years-after-the-chips-and-science-act-biden-%E2%81%A0harris-administration-celebrates-historic-achievements-in-bringing-semiconductor-supply-chains-home-creating-jobs-supporting-inn/), which aims to bring semiconductor production onshore and strengthen supply chain resilience.\n\nSecuring machine learning systems requires moving beyond trust-by-default models toward zero-trust supply chain practices. This includes screening suppliers, validating component provenance, implementing tamper-evident protections, and continuously monitoring system behavior for signs of compromise. Building fault-tolerant architectures that detect and contain failures provides an additional layer of defense.\n\nUltimately, supply chain risks must be treated as a first-class concern in ML system design. Trust in the computational models and data pipelines that power machine learning depends corely on the trustworthiness of the hardware on which they run. Without securing the hardware foundation, even the most sophisticated models remain vulnerable to compromise.\n\n### Case Study: Supermicro Controversy {#sec-security-privacy-case-study-supermicro-controversy-72b7}\n\nThe abstract nature of supply chain risks became concrete in a high-profile controversy that captured industry attention. In 2018, Bloomberg Businessweek published a widely discussed report alleging that Chinese state-sponsored actors had secretly implanted tiny surveillance chips on server motherboards manufactured by Supermicro [@TheBigHa77]. These compromised servers were reportedly deployed by more than 30 major companies, including Apple and Amazon. The chips, described as no larger than a grain of rice, were said to provide attackers with backdoor access to sensitive data and systems.\n\nThe allegations sparked immediate concern across the technology industry, raising questions about the security of global supply chains and the potential for state-level hardware manipulation. However, the companies named in the report publicly denied the claims. Apple, Amazon, and Supermicro stated that they had found no evidence of the alleged implants after conducting thorough internal investigations. Industry experts and government agencies also expressed skepticism, noting the lack of verifiable technical evidence presented in the report.\n\nDespite these denials, the story had a lasting impact on how organizations and policymakers view hardware supply chain security. Whether or not the specific claims were accurate, the report highlighted the real and growing concern that hardware supply chains are difficult to fully audit and secure. It underscored how geopolitical tensions, manufacturing outsourcing, and the complexity of modern hardware ecosystems make it increasingly challenging to guarantee the integrity of hardware components.\n\nThe Supermicro case illustrates a broader truth: once a product enters a complex global supply chain, it becomes difficult to ensure that every component is free from tampering or unauthorized modification. This risk is particularly acute for machine learning systems, which depend on a wide range of hardware accelerators, memory modules, and processing units sourced from multiple vendors across the globe.\n\nIn response to these risks, both industry and government stakeholders have begun to invest in supply chain security initiatives. The U.S. government's CHIPS and Science Act is one such effort, aiming to bring semiconductor manufacturing back onshore to improve transparency and reduce dependency on foreign suppliers. While these efforts are valuable, they do not fully eliminate supply chain risks. They must be complemented by technical safeguards, such as component validation, runtime monitoring, and fault-tolerant system design.\n\nThe Supermicro controversy serves as a cautionary tale for the machine learning community. It demonstrates that hardware security cannot be taken for granted, even when working with reputable suppliers. Ensuring the integrity of ML systems requires rigorous attention to the entire hardware lifecycle—from design and fabrication to deployment and maintenance. This case reinforces the need for organizations to adopt comprehensive supply chain security practices as a foundational element of trustworthy ML system design.\n\n## When ML Systems Become Attack Tools {#sec-security-privacy-ml-systems-become-attack-tools-2f34}\n\nThe threats examined thus far—model theft, data poisoning, adversarial attacks, hardware vulnerabilities—represent attacks targeting machine learning systems. However, a complete threat model must also account for the inverse: machine learning as an attack amplifier. The same capabilities that make ML powerful for beneficial applications also enhance adversarial operations, transforming machine learning from passive target to active weapon.\n\nWhile machine learning systems are often treated as assets to protect, they may also serve as tools for launching attacks. In adversarial settings, the same models used to enhance productivity, automate perception, or assist decision-making can be repurposed to execute or amplify offensive operations. This dual-use characteristic of machine learning, its capacity to secure systems as well as to subvert them, marks a core shift in how ML must be considered within system-level threat models.\n\nAn offensive use of machine learning refers to any scenario in which a machine learning model is employed to facilitate the compromise of another system. In such cases, the model itself is not the object under attack, but the mechanism through which an adversary advances their objectives. These applications may involve reconnaissance, inference, subversion, impersonation, or the automation of exploit strategies that would otherwise require manual execution.\n\nImportantly, such offensive applications are not speculative. Attackers are already integrating machine learning into their toolchains across a wide range of activities, from spam filtering evasion to model-driven malware generation. What distinguishes these scenarios is the deliberate use of learning-based systems to extract, manipulate, or generate information in ways that undermine the confidentiality, integrity, or availability of targeted components.\n\nTo clarify the diversity and structure of these applications, @tbl-offensive-ml-use-cases summarizes several representative use cases. For each, the table identifies the type of machine learning model typically employed, the underlying system vulnerability it exploits, and the primary advantage conferred by the use of machine learning.\n\nThese documented cases illustrate how machine learning models can serve as amplifiers of adversarial capability. For example, language models allow more convincing and adaptable phishing attacks, while clustering and classification algorithms facilitate reconnaissance by learning system-level behavioral patterns. The generative AI capabilities of large language models particularly amplify these offensive applications. Similarly, adversarial example generators and inference models systematically uncover weaknesses in decision boundaries or data privacy protections, often requiring only limited external access to deployed systems. In hardware contexts, as discussed in the next section, deep neural networks trained on side-channel data can automate the extraction of cryptographic secrets from physical measurements—transforming an expert-driven process into a learnable pattern recognition task. Deep learning foundations, including convolutional neural networks for spatial pattern recognition, recurrent architectures for temporal dependencies, and gradient-based optimization, enable attackers to apply these techniques across various hardware platforms, from GPUs and TPUs in cloud environments to edge accelerators with constrained resources.\n\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Offensive Use Case**                  | **ML Model Type**                               | **Targeted System Vulnerability**                | **Advantage of ML**                                  |\n+:========================================+:================================================+:=================================================+:=====================================================+\n| **Phishing and Social Engineering**     | Large Language Models (LLMs)                    | Human perception and communication systems       | Personalized, context-aware message crafting         |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Reconnaissance and Fingerprinting**   | Supervised classifiers, clustering models       | System configuration, network behavior           | Scalable, automated profiling of system behavior     |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Exploit Generation**                  | Code generation models, fine-tuned transformers | Software bugs, insecure code patterns            | Automated discovery of candidate exploits            |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Data Extraction (Inference Attacks)** | Classification models, inversion models         | Privacy leakage through model outputs            | Inference with limited or black-box access           |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Evasion of Detection Systems**        | Adversarial input generators                    | Detection boundaries in deployed ML systems      | Crafting minimally perturbed inputs to evade filters |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n| **Hardware-Level Attacks**              | Deep learning models                            | Physical side-channels (e.g., power, timing, EM) | Learning leakage patterns directly from raw signals  |\n+-----------------------------------------+-------------------------------------------------+--------------------------------------------------+------------------------------------------------------+\n\n: **Offensive ML Use Cases**: This table categorizes how machine learning amplifies cyberattacks by enabling automated content generation, exploiting system vulnerabilities, and increasing attack sophistication; it details the typical ML model, targeted weakness, and resulting advantage for each offensive application. Understanding these use cases is important for developing effective defenses against increasingly intelligent threats. {#tbl-offensive-ml-use-cases}\n\nAlthough these applications differ in technical implementation, they share a common foundation: the adversary replaces a static exploit with a learned model capable of approximating or adapting to the target's vulnerable behavior. This shift increases flexibility, reduces manual overhead, and improves robustness in the face of evolving or partially obscured defenses.\n\nWhat makes this class of threats particularly significant is their favorable scaling behavior. Just as accuracy in computer vision or language modeling improves with additional data, larger architectures, and greater compute resources, so too does the performance of attack-oriented machine learning models. A model trained on larger corpora of phishing attempts or power traces, for instance, may generalize more effectively, evade more detectors, or require fewer inputs to succeed. The same ecosystem that drives innovation in beneficial AI, including public datasets, open-source tooling, and scalable infrastructure, also lowers the barrier to developing effective offensive models.\n\nThis dynamic creates an asymmetry between attacker and defender. While defensive measures are bounded by deployment constraints, latency budgets, and regulatory requirements, attackers can scale training pipelines with minimal marginal cost. The widespread availability of pretrained models and public ML platforms further reduces the expertise required to develop high-impact attacks.\n\nExamining these offensive capabilities serves a crucial defensive purpose. Security professionals have long recognized that effective defense requires understanding attack methodologies—this principle underlies penetration testing[^fn-penetration-testing], red team exercises[^fn-red-team-exercises], and threat modeling throughout the cybersecurity industry.\n\n[^fn-penetration-testing]: **Penetration Testing**: Authorized simulated cyberattacks to evaluate system security, formalized in the 1960s for military computer systems. The global penetration testing market reached $1.7 billion in 2022, with 89% of organizations conducting annual pen tests to identify vulnerabilities before attackers do.\n\n[^fn-red-team-exercises]: **Red Team Exercises**: Adversarial security simulations where specialized teams emulate real attackers to test organizational defenses, originated from military war games in the 1960s. Unlike penetration testing, red teams use social engineering, physical access, and advanced persistent threat techniques, with exercises lasting weeks or months to simulate sophisticated nation-state attacks. The phrase \"know your enemy\" reflects this core security principle.\n\nIn the machine learning domain, this understanding becomes essential because ML amplifies both defensive and offensive capabilities. The same computational advantages that make ML powerful for legitimate applications—pattern recognition, automation, and scalability—also enhance adversarial capabilities. By examining how machine learning can be weaponized, security professionals can anticipate attack vectors, design more robust defenses, and develop detection mechanisms.\n\nAs a result, any comprehensive treatment of machine learning system security must consider not only the vulnerabilities of ML systems themselves but also the ways in which machine learning can be used to compromise other components—whether software, data, or hardware. Understanding the offensive potential of machine-learned systems is essential for designing resilient, trustworthy, and forward-looking defenses.\n\n### Case Study: Deep Learning for SCA {#sec-security-privacy-case-study-deep-learning-sca-b0b3}\n\nTo illustrate these offensive capabilities concretely, we examine a specific case where machine learning transforms traditional attack methodologies. One of the most well-known and reproducible demonstrations of deep-learning-assisted SCA is the SCAAML framework (Side-Channel Attacks Assisted with Machine Learning) [@scaaml_2019]. Developed by researchers at Google, SCAAML provides a practical implementation of the attack pipeline described above.\n\n::: {#fig-side-channel-curves fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}]\n \\definecolor{myblue}{RGB}{31,119,180}\n\\definecolor{myorange}{RGB}{255,127,14}\n\\definecolor{mygreen}{RGB}{44,160,44}\n\\definecolor{myred}{RGB}{214,39,40}\n\\definecolor{mypurple}{RGB}{148,103,189}\n\\definecolor{mybrown}{RGB}{140,86,75}\n\n\\pgfplotsset{myaxis/.style={\nclip=false,\n  axis line style={draw=none},\n  yticklabels={},\n  xticklabels={},\n  domain=-3.6:5,\n  samples=100,\n  smooth,\n  grid=none,\n  width=10cm,\n  height=7cm,\n  major tick  style={draw=none},\n  cycle list={\n    {myblue,line width=1.75pt},\n    {mygreen,line width=1.75pt},\n    {mypurple,line width=1.75pt},\n  }\n  }\n}\n%left graph\n\\begin{scope}[local bounding box=G1,shift={(0,0)}]\n\\begin{axis}[myaxis]\n\\addplot+[] {6.5*exp(-x^2/2)};\n\\addplot+[] {6.75*exp(-x^2/2)}node[pos=0.46](S){};\n\\addplot+[] {7*exp(-x^2/2)};\n\\node[thick,draw=BrownLine,rectangle,minimum size=14mm](B1)at(S){};\n\\end{axis}\n\\end{scope}\n%right graph\n\\begin{scope}[local bounding box=G2,shift={(11,1)},scale=1.5]\n\\begin{axis}[myaxis,  width=5cm,  height=5cm]\n\\addplot+[domain=-0.75:0.75,line width=1.25pt] {6.5*exp(-x^2/2)}\nnode[pos=0.66,outer sep=0pt,inner sep=0pt](T1){};\n\\addplot+[domain=-0.796:0.796,line width=1.25pt] {6.75*exp(-x^2/2)}\nnode[pos=0.63,outer sep=0pt,inner sep=0pt](T2){};\n\\addplot+[domain=-0.84:0.84,line width=1.25pt] {7*exp(-x^2/2)}\nnode[pos=0.6,outer sep=0pt,inner sep=0pt](T3){};\n%\n\\draw[myblue,-latex](T1)--++(0:9.5mm)node[right]{0000};\n\\draw[mygreen,-latex](T2)--++(0:9.9mm)node[right]{1111};\n\\draw[mypurple,-latex](T3)--++(0:10.4 mm)node[right]{0101};\n\\end{axis}\n\\node[thick,draw=BrownLine,rectangle,\nminimum height=48mm,minimum width=67mm] (B2) at (rel axis cs:0.7,0.6) {};\n\\end{scope}\n\n\\draw[BrownLine](B1.north west)--(B2.north west);\n\\draw[BrownLine](B1.south west)--(B2.south west);\n\\draw[BrownLine,dashed](B1.north east)--(B2.north east);\n%\\draw[BrownLine,dashed](B1.south east)--(B2.south east);\n%Determine the point on the line B2.north west to B2.south west\n\\path[name path=line1] (B1.south east) -- (B2.south east);\n\\path[name path=edgeB2left] (B2.north west) -- (B2.south west);\n\\path[name intersections={of=line1 and edgeB2left, by=I}];\n\\draw[BrownLine,dashed](B1.south east)--(I);\n\\end{tikzpicture}\n```\n**Power Traces**: Cryptographic computations reveal subtle, data-dependent variations in power consumption that reflect internal states during specific operations.\n:::\n\nAs shown in @fig-side-channel-curves, cryptographic computations exhibit data-dependent variations in their power consumption. These variations, while subtle, are measurable and reflect the internal state of the algorithm at specific points in time.\n\nIn traditional side-channel attacks, experts rely on statistical techniques to extract these differences. However, a neural network can learn to associate the shape of these signals with the specific data values being processed, effectively learning to decode the signal in a manner that mimics expert-crafted models, yet with enhanced flexibility and generalization. The model is trained on labeled examples of power traces and their corresponding intermediate values (e.g., output of an S-box operation). Over time, it learns to associate patterns in the trace, similar to those depicted in @fig-side-channel-curves, with secret-dependent computational behavior. This transforms the key recovery task into a classification problem, where the goal is to infer the correct key byte based on trace shape alone.\n\nIn their study, @scaaml_2019 trained a convolutional neural network to extract AES keys from power traces collected on an STM32F415 microcontroller running the open-source TinyAES implementation. The model was trained to predict intermediate values of the AES algorithm, such as the output of the S-box in the first round, directly from raw power traces. The trained model recovered the full 128-bit key using only a small number of traces per byte.\n\nThe traces were collected using a ChipWhisperer setup with a custom STM32F target board, shown in @fig-stm32f-board. This board executes AES operations while allowing external equipment to monitor power consumption with high temporal precision. The experimental setup captures how even inexpensive, low-power embedded devices can leak information through side channels—information that modern machine learning models can learn to exploit.\n\n![**STM32F415 Target Board**: Enables monitoring of power consumption during AES operations on the microcontroller, highlighting side-channel vulnerabilities that can be exploited by machine learning models. Source: @scaaml_2019.](images/png/stm32f_board.png){#fig-stm32f-board}\n\nSubsequent work expanded on this approach by introducing long-range models capable of leveraging broader temporal dependencies in the traces, improving performance even under noise and desynchronization [@bursztein2023generic]. These developments highlight the potential for machine learning models to serve as offensive cryptanalysis tools, especially in the analysis of secure hardware.\n\nThe implications extend beyond academic interest. As deep learning models continue to scale, their application to side-channel contexts is likely to lower the cost, skill threshold, and trace requirements of hardware-level attacks—posing a growing challenge for the secure deployment of embedded machine learning systems, cryptographic modules, and trusted execution environments.\n\n## Comprehensive Defense Architectures {#sec-security-privacy-comprehensive-defense-architectures-48ab}\n\nHaving examined threats against ML systems and threats enabled by ML capabilities, we now turn to comprehensive defensive strategies. Designing secure and privacy-preserving machine learning systems requires more than identifying individual threats. It demands a layered defense strategy that integrates protections across multiple system levels to create comprehensive resilience.\n\nThis section progresses systematically through four layers of defense: Data Layer protections including differential privacy and secure computation that safeguard sensitive information during training; Model Layer defenses such as adversarial training and secure deployment that protect the models themselves; Runtime Layer measures including input validation and output monitoring that secure inference operations; and Hardware Layer foundations such as trusted execution environments that provide the trust anchor for all other protections. We conclude with practical frameworks for selecting and implementing these defenses based on your deployment context.\n\n### The Layered Defense Principle {#sec-security-privacy-layered-defense-principle-8706}\n\nLayered defense (also known as defense-in-depth) represents a core security architecture principle where multiple independent defensive mechanisms work together to protect against diverse threat vectors. In machine learning systems, this approach becomes essential due to the unique attack surfaces introduced by data dependencies, model exposures, and inference patterns. Unlike traditional software systems that primarily face code-based vulnerabilities, ML systems are vulnerable to input manipulation, data leakage, model extraction, and runtime abuse, all amplified by tight coupling between data, model behavior, and infrastructure.\n\nThe layered approach recognizes that no single defensive mechanism can address all possible threats. Instead, security emerges from the interaction of complementary protections: data-layer techniques like differential privacy and federated learning; model-layer defenses including robustness techniques and secure deployment; runtime-layer measures such as input validation and output monitoring; and hardware-layer solutions including trusted execution environments and secure boot. Each layer contributes to the system's overall resilience while compensating for potential weaknesses in other layers.\n\nThis section presents a structured framework implementing layered defense for ML systems, progressing from data-centric protections to infrastructure-level enforcement. The framework builds upon data protection practices, including encryption, access control, and lineage tracking, and connects forward to operational security measures for production deployment. By integrating safeguards across layers, organizations can build ML systems that not only perform reliably but also withstand adversarial pressure in production environments.\n\nThe layered approach is visualized in @fig-defense-stack, which shows how defensive mechanisms progress from foundational hardware-based security to runtime system protections, model-level controls, and privacy-preserving techniques at the data level. Each layer builds on the trust guarantees of the layer below it, forming an end-to-end strategy for deploying ML systems securely.\n\n::: {#fig-defense-stack fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\n\\tikzset{%\nLine/.style={line width=1.0pt,black!50},\nBox/.style={inner xsep=4pt,inner ysep=6pt,\n    node distance=0.7,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=29mm,\n    minimum width=29mm, minimum height=11mm\n  },\nBox2/.style={Box,  node distance=0.7,draw=BrownLine,fill=BrownL},\nBox3/.style={Box,  node distance=0.7, draw=VioletLine,fill=VioletL2},\nBox4/.style={Box,node distance=0.7,  draw=BlueLine,fill=BlueL}\n}\n\n\\node[Box](B1){Trusted Execution Environments};\n\\node[Box,right=of B1](B2){Secure Boot};\n\\node[Box,right=4.25of B2](B3){Hardware Security Modules};\n\\node[Box,right=of B3](B4){Physical Unclonable Functions};\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=6mm,inner ysep=6mm,\nyshift=2.5mm,fill=BackColor!60,fit=(B1)(B4),line width=0.75pt](BB1){};\n\\node[below=6pt of  BB1.north,inner sep=0pt,\nanchor=north]{\\textbf{Hardware-Level Security}};\n%\n\\node[Box3,above=2.0of B1](2B1){System Integrity Checks};\n\\node[Box3,right=of 2B1](2B2){Runtime Input Validation};\n\\node[Box3,right=of 2B2](2B3){Runtime Output Monitoring};\n\\node[Box3,right=of 2B3](2B4){Incident Response \\& Recovery};\n\\scoped[on background layer]\n\\node[draw=BlueD,inner xsep=6mm,inner ysep=6mm,\nyshift=2.5mm,fill=cyan!5,fit=(2B1)(2B4),line width=0.75pt](BB2){};\n\\node[below=6pt of  BB2.north,inner sep=0pt,\nanchor=north]{\\textbf{System-Level Security}};\n%\n\\node[Box2,above=2.0 of 2B1](3B1){Model Encryption \\& Serialization};\n\\node[Box2,right=2.5 of 3B1](3B2){Secure Model Design};\n\\path[](3B2)-|coordinate(S)(B4);\n\\node[Box2](3B3)at(S){Secure Deployment \\& Access};\n\\scoped[on background layer]\n\\node[draw=GreenD,inner xsep=6mm,inner ysep=6mm,\nyshift=2.5mm,fill=green!5,fit=(3B1)(3B3),line width=0.75pt](BB3){};\n\\node[below=6pt of  BB3.north,inner sep=0pt, xshift=4mm,\nanchor=north]{\\textbf{Model-Level Security}};\n%\n\\node[Box4,above left=2 and 0.3of 3B2](4B1){Differential Privacy};\n\\node[Box4,above right=1.9 and 0.3of 3B2](4B2){Federated Learning};\n\\node[Box4,above=of 4B1](5B1){Homomorphic Encryption};\n\\node[Box4,above=of 4B2](5B2){Synthetic Data Generation};\n%\n\\scoped[on background layer]\n\\node[draw=RedLine,inner xsep=6mm,inner ysep=6mm,\nyshift=2.3mm,fill=magenta!4,fit=(4B1)(5B2),line width=0.75pt](BB4){};\n\\node[below=6pt of  BB4.north,inner sep=0pt,\nanchor=north]{\\textbf{Data Privacy \\& Governance}};\n%\n\\draw[Line,-latex](B1)--(2B1);\n\\draw[Line,-latex](B2)--++(0,1.8)-|(2B1.310);\n\\draw[Line,-latex](B3)--++(0,1.8)-|(3B3.230);\n\\draw[Line,-latex](B4)--(3B3);\n%\n\\draw[Line,-latex](2B1)--(3B1);\n\\draw[Line,-latex](2B2.120)|-(3B2);\n\\draw[Line,-latex](2B3.80)|-(3B2);\n%\n\\draw[Line,-latex](3B2.120)--++(0,1.2)-|(4B1);\n\\draw[Line,-latex](3B2.70)--++(0,1.2)-|(4B2);\n\\draw[Line,-latex](4B1)--(5B1);\n\\draw[Line,-latex](4B2)--(5B2);\n\\end{tikzpicture}\n```\n**Layered Defense Stack**: Machine learning systems require multi-faceted security strategies that progress from foundational hardware protections to data-centric privacy techniques, building trust across all layers. This architecture integrates safeguards at the data, model, runtime, and infrastructure levels to mitigate threats and ensure robust deployment in production environments.\n:::\n\n### Privacy-Preserving Data Techniques {#sec-security-privacy-privacypreserving-data-techniques-64f8}\n\nAt the highest level of our defense stack, we begin with data privacy techniques. Protecting the privacy of individuals whose data fuels machine learning systems is a foundational requirement for trustworthy AI. Unlike traditional systems where data is often masked or anonymized before processing, ML workflows typically rely on access to raw, high-fidelity data to train effective models. This tension between utility and privacy has motivated a diverse set of techniques aimed at minimizing data exposure while preserving learning performance.\n\n#### Differential Privacy {#sec-security-privacy-differential-privacy-8c2b}\n\nOne of the most widely adopted frameworks for formalizing privacy guarantees is differential privacy (DP). DP provides a rigorous mathematical definition of privacy loss, ensuring that the inclusion or exclusion of a single individual's data has a provably limited effect on the model's output.\n\nTo understand the need for differential privacy, consider this challenge: how can we quantify privacy loss when learning from data? Traditional privacy approaches focus on removing identifying information (names, addresses, social security numbers) or applying statistical disclosure controls. However, these methods fail against sophisticated adversaries who can re-identify individuals through auxiliary data, statistical correlation attacks, or inference from model outputs.\n\nDifferential privacy takes a different approach by focusing on algorithmic behavior rather than data content. The key insight is that privacy protection should be measurable and should limit what can be learned about any individual, regardless of what external information an adversary possesses.\n\nTo build intuition for this concept, imagine you want to find the average salary of a group of people, but no one wants to reveal their actual salary. With differential privacy, you could ask everyone to write their salary on a piece of paper, but before they hand it in, they add or subtract a random number from a known distribution. When you average all the papers, the random noise tends to cancel out, giving you a very close estimate of the true average. However, if you pull out any single piece of paper, you cannot know the person's real salary because you do not know what random number they added. This is the core idea: learn aggregate patterns while making it impossible to be sure about any single individual.\n\nDifferential privacy formalizes this intuition through a comparison of algorithm behavior on similar datasets. Consider two adjacent datasets that differ only in the presence or absence of a single individual's record. Differential privacy ensures that the probability distributions of algorithm outputs remain statistically similar regardless of whether that individual's data is included. This protection is achieved through carefully calibrated noise that masks individual contributions while preserving the aggregate statistical patterns necessary for machine learning.\n\nTo make this intuition mathematically precise, differential privacy introduces a quantitative measure of privacy loss. The mathematical framework uses probability ratios to bound how much an algorithm's behavior can change when a single individual's data is added or removed. This approach allows us to prove privacy guarantees rather than simply assume them.\n\nA randomized algorithm $\\mathcal{A}$ is said to be $\\epsilon$-differentially private if, for all adjacent datasets $D$ and $D'$ differing in one record, and for all outputs $S \\subseteq \\text{Range}(\\mathcal{A})$, the following holds:\n$$\n\\Pr[\\mathcal{A}(D) \\in S] \\leq e^{\\epsilon} \\Pr[\\mathcal{A}(D') \\in S]\n$$\n\nThe parameter $\\epsilon$ quantifies the privacy budget, representing the maximum allowable privacy loss. Smaller values of $\\epsilon$ provide stronger privacy guarantees through increased noise injection, but may reduce model utility. Typical values include $\\epsilon = 0.1$ for strong privacy protection, $\\epsilon = 1.0$ for moderate protection, and $\\epsilon = 10$ for weaker but utility-preserving guarantees. The multiplicative factor $e^{\\epsilon}$ bounds the likelihood ratio between algorithm outputs on adjacent datasets, constraining how much an individual's participation can influence any particular result.\n\nThis bound ensures that the algorithm's behavior remains statistically indistinguishable regardless of whether any individual's data is present, thereby limiting the information that can be inferred about that individual. In practice, DP is implemented by adding calibrated noise to model updates or query responses, using mechanisms such as the Laplace or Gaussian mechanism. Training techniques like differentially private stochastic gradient descent[^fn-dp-sgd-adoption] integrate calibrated noise into training computations, ensuring that individual data points cannot be distinguished from the model's learned behavior.\n\n[^fn-dp-sgd-adoption]: **DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at scale in 2016, protecting 1+ billion users' data in iOS. Their implementation adds noise to emoji usage, Safari crashes, and QuickType suggestions, balancing privacy (ε=4-16) with utility for improving user experience across their ecosystem.\n\nWhile differential privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility[^fn-privacy-utility-tension] that has measurable computational and accuracy costs.\n\n[^fn-privacy-utility-tension]: **Privacy-Utility Tension**: This core tradeoff was formalized by Dwork and McSherry, who proved that perfect privacy (infinite noise) yields no utility, while perfect utility (no noise) provides no privacy. The \"privacy budget\" concept emerged from this insight—you can only spend privacy once, making every query a strategic decision.\n\nPractical DP deployment requires careful consideration of computational trade-offs, privacy budget management, and implementation challenges, as detailed in @tbl-privacy-technique-comparison.\n\nIncreasing the noise to reduce $\\epsilon$ may degrade model accuracy, especially in low-data regimes or fine-grained classification tasks. Consequently, DP is often applied selectively—either during training on sensitive datasets or at inference when returning aggregate statistics—to balance privacy with performance goals [@dwork2014algorithmic].\n\n#### Federated Learning {#sec-security-privacy-federated-learning-3834}\n\nWhile differential privacy adds mathematical guarantees to data processing, federated learning (FL) offers a complementary approach that reduces privacy risks by restructuring the learning process itself. This technique directly addresses the privacy challenges of on-device learning explored in @sec-edge-intelligence, where models must adapt to local data patterns without exposing sensitive user information. Rather than aggregating raw data at a central location, FL distributes the training across a set of client devices, each holding local data [@mcmahan2017communicationefficient]. This distributed training paradigm, which builds on the adaptive deployment concepts from on-device learning, requires careful coordination of security measures across multiple participants and infrastructure providers. Clients compute model updates locally and share only parameter deltas with a central server for aggregation:\n$$\n\\theta_{t+1} \\leftarrow \\sum_{k=1}^{K} \\frac{n_k}{n} \\cdot \\theta_{t}^{(k)}\n$$\n\nHere, $\\theta_{t}^{(k)}$ represents the model update from client $k$, $n_k$ the number of samples held by that client, and $n$ the total number of samples across all clients. This weighted aggregation allows the global model to learn from distributed data without direct access to it. FL reduces the exposure of raw data, but still leaks information through gradients, motivating the use of DP, secure aggregation, and hardware-based protections in federated settings.\n\n::: {.callout-note title=\"Real-World Example: Google Gboard Federated Learning\" icon=false}\nGoogle's Gboard keyboard uses federated learning to improve next-word prediction across 1+ billion Android devices without collecting typing data. The system works as follows:\n\n1. Local Training: Each device trains a small update to the language model using the user's recent typing (typically 100-1000 words)\n2. Secure Aggregation: Devices upload encrypted model updates (not raw text) to Google's servers\n3. Global Update: The server aggregates thousands of updates, computing an improved global model\n4. Distribution: The updated model is pushed back to devices in the next app update\n\n**Privacy Properties:** Individual typing data never leaves the device. Even Google's servers cannot decrypt individual updates, seeing only the aggregated result. The system combines FL with differential privacy $(\\varepsilon\\approx 6)$ and secure aggregation protocols.\n\n**Performance:** FL achieves 92% of the accuracy of centralized training while eliminating raw data collection. Communication efficiency optimizations (gradient compression, selective participation) reduce bandwidth to ~100 KB per device per day.\n\n**Trade-offs:** FL requires 10-100x more communication rounds than centralized training and introduces 2-5% accuracy degradation. However, for privacy-sensitive applications, these costs are acceptable compared to the alternative of not training at all.\n:::\n\nTo address scenarios requiring computation on encrypted data, homomorphic encryption (HE)[^fn-he-breakthrough] and secure multiparty computation (SMPC) allow models to perform inference or training over encrypted inputs. The computational overhead of homomorphic operations often requires efficiency optimization techniques, including model compression (quantization reduces precision requirements for encrypted operations), architectural optimization (depthwise separable convolutions minimize encrypted multiplications), and hardware acceleration (specialized cryptographic accelerators), to maintain practical performance.\n\n[^fn-he-breakthrough]: **Homomorphic Encryption Breakthrough**: Considered the \"holy grail\" of cryptography since the 1970s, fully homomorphic encryption remained theoretical until Craig Gentry's 2009 PhD thesis. His breakthrough was realizing that \"noisy\" ciphertexts could support unlimited operations if periodically \"refreshed,\" solving a decades-old puzzle that allows computation on encrypted data.\n\nIn the case of HE, operations on ciphertexts correspond to operations on plaintexts, enabling encrypted inference:\n$$\n\\text{Enc}(f(x)) = f(\\text{Enc}(x))\n$$\n\nThis property supports privacy-preserving computation in untrusted environments, such as cloud inference over sensitive health or financial records. The computational cost of HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC[^fn-smpc-overhead], by contrast, distributes the computation across multiple parties such that no single party learns the complete input or output. This is particularly useful in joint training across institutions with strict data-use policies, such as hospitals or banks[^fn-smpc-collaborative].\n\n[^fn-smpc-overhead]: **SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational overhead compared to plaintext operations. A simple neural network inference that takes milliseconds on GPU requires hours using SMPC, limiting practical applications to small models and offline scenarios.\n\n[^fn-smpc-collaborative]: **Secure Multi-Party Computation (SMPC)**: Cryptographic framework enabling multiple parties to jointly compute functions over their private inputs without revealing those inputs, first formalized in 1982 by Andrew Yao. Today's implementations allow hospitals to collaboratively train medical AI models without sharing patient records, achieving 99%+ accuracy while maintaining strict privacy compliance.\n\n#### Synthetic Data Generation {#sec-security-privacy-synthetic-data-generation-4349}\n\nBeyond cryptographic approaches like homomorphic encryption, a more pragmatic and increasingly popular alternative involves the use of synthetic data generation[^fn-synthetic-data]. This approach offers an intuitive solution to privacy protection: if we can create artificial data that looks statistically similar to real data, we can train models without ever exposing sensitive information.\n\nSynthetic data generation works by training a generative model (such as a GAN, VAE, or diffusion model) on the original sensitive dataset, then using this trained generator to produce new artificial samples. The key insight is that the generative model learns the underlying patterns and distributions in the data without memorizing specific individuals. When properly implemented, the synthetic data preserves statistical properties necessary for machine learning while removing personally identifiable information.\n\nThe generation typically follows three stages. First, distribution learning trains a generative model $G_\\theta$ on real data $D_{\\text{real}} = \\{x_1, x_2,\\ldots, x_n\\}$ to learn the data distribution $p(x)$. Second, synthetic sampling generates new samples $D_{\\text{synthetic}} = \\{G_\\theta(z_1), G_\\theta(z_2),\\ldots, G_\\theta(z_m)\\}$ by sampling from random noise $z_i \\sim \\mathcal{N}(0,I)$. Third, validation verifies that $D_{\\text{synthetic}}$ maintains statistical fidelity to $D_{\\text{real}}$ while avoiding memorization of specific records. By training generative models on real datasets and sampling new instances from the learned distribution, organizations can create datasets that approximate the statistical properties of the original data without retaining identifiable details [@goncalves2020generation].\n\nWhile appealing, synthetic data generation faces important limitations. Generative models can suffer from mode collapse, failing to capture rare but important patterns in the original data. More critically, sophisticated adversaries can potentially extract information about the original training data through generative model inversion attacks or membership inference. The privacy protection depends heavily on the generative model architecture, training procedure, and hyperparameter choices—making it difficult to provide formal privacy guarantees without additional mechanisms like differential privacy.\n\nConsider a practical example where a hospital wants to share patient data for ML research while protecting privacy. They train a generative adversarial network (GAN) on 10,000 real patient records containing demographics, lab results, and diagnoses. The GAN learns to generate synthetic patients with realistic combinations of features (e.g., diabetic patients typically have elevated glucose levels). The synthetic dataset of 50,000 artificial patients maintains clinical correlations necessary for training diagnostic models while containing no real patient information. However, the hospital also applies differential privacy during GAN training (ε = 1.0) to prevent the model from memorizing specific patients, trading a 5% reduction in statistical fidelity for formal privacy guarantees.\n\n[^fn-synthetic-data]: **Synthetic Data Growth**: The synthetic data market grew from $110 million in 2019 to $1.1 billion in 2023, driven by privacy regulations and data scarcity. Companies like Uber use synthetic trip data to protect user privacy while maintaining ML model performance, with some synthetic datasets achieving 95%+ statistical fidelity.\n\nTogether, these techniques reflect a shift from isolating data as the sole path to privacy toward embedding privacy-preserving mechanisms into the learning process itself. Each method offers distinct guarantees and trade-offs depending on the application context, threat model, and regulatory constraints. Effective system design often combines multiple approaches, such as applying differential privacy within a federated learning setup, or employing homomorphic encryption for important inference stages, to build ML systems that are both useful and respectful of user privacy.\n\n#### Comparative Properties {#sec-security-privacy-comparative-properties-9ca5}\n\nHaving examined individual techniques, it becomes clear that these privacy-preserving approaches differ not only in the guarantees they offer but also in their system-level implications. For practitioners, the choice of mechanism depends on factors such as computational constraints, deployment architecture, and regulatory requirements.\n\n@tbl-privacy-technique-comparison summarizes the comparative properties of these methods, focusing on privacy strength, runtime overhead, maturity, and common use cases. Understanding these trade-offs is important for designing privacy-aware machine learning systems that operate under real-world constraints.\n\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Technique**              | **Privacy Guarantee** | **Computational Overhead** | **Deployment Maturity** | **Typical Use Case**         | **Trade-offs**                                            |\n+:===========================+:======================+:===========================+:========================+:=============================+:==========================================================+\n| **Differential Privacy**   | Formal (ε-DP)         | Moderate to High           | Production              | Training with sensitive      | Reduced accuracy; careful tuning of ε/noise               |\n|                            |                       |                            |                         | or regulated data            | required to balance utility and protection                |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Federated Learning**     | Structural            | Moderate                   | Production              | Cross-device or cross-org    | Gradient leakage risk; requires secure aggregation        |\n|                            |                       |                            |                         | collaborative learning       | and orchestration infrastructure                          |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Homomorphic Encryption** | Strong (Encrypted)    | High                       | Experimental            | Inference in untrusted       | High latency and memory usage; suitable for limited-scope |\n|                            |                       |                            |                         | cloud environments           | inference on fixed-function models                        |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Secure MPC**             | Strong (Distributed)  | Very High                  | Experimental            | Joint training across        | Expensive communication; challenging to scale to many     |\n|                            |                       |                            |                         | mutually untrusted parties   | participants or deep models                               |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n| **Synthetic Data**         | Weak (if standalone)  | Low to Moderate            | Emerging                | Data sharing, benchmarking   | May leak sensitive patterns if training process is not    |\n|                            |                       |                            |                         | without direct access to raw | differentially private or audited for fidelity            |\n|                            |                       |                            |                         | data                         |                                                           |\n+----------------------------+-----------------------+----------------------------+-------------------------+------------------------------+-----------------------------------------------------------+\n\n: **Privacy-Accuracy Trade-Offs**: Data privacy techniques impose varying computational costs and offer different levels of formal privacy guarantees, requiring practitioners to balance privacy strength with model utility and deployment constraints. The table summarizes key properties—privacy guarantees, computational overhead, maturity, typical use cases, and trade-offs—to guide informed decisions when designing privacy-aware machine learning systems. {#tbl-privacy-technique-comparison}\n\n### Case Study: GPT-3 Data Extraction Attack {#sec-security-privacy-case-study-gpt3-data-extraction-attack-5126}\n\nIn 2020, researchers conducted a groundbreaking study demonstrating that large language models could leak sensitive training data through carefully crafted prompts [@carlini2021extracting]. The research team systematically queried OpenAI's GPT-3 model to extract verbatim content from its training dataset, revealing privacy vulnerabilities in large-scale language models.\n\nThe attack proved remarkably successful at extracting sensitive information directly from the model's outputs. By repeatedly querying the model with prompts like \"My name is\" followed by attempts to continue famous quotes or repeated phrases, researchers successfully extracted personal information including email addresses and phone numbers from the training data, verbatim passages from copyrighted books, private data that should have been filtered during training, and personally identifiable information from millions of individuals.\n\nThe technical approach exploited GPT-3's memorization of rare or repeated text sequences. The researchers used prompt engineering to craft inputs that triggered memorized sequences, continuation attacks that used partial quotes or names to extract full sensitive information, statistical analysis to identify patterns in model outputs indicating verbatim memorization, and verification methods that cross-referenced extracted data with known public sources to confirm accuracy. Out of 600,000 attempts, they successfully extracted over 16,000 unique instances of memorized training data.\n\nThis attack challenged assumptions about training data privacy. The results demonstrated that large language models can act as unintentional databases, storing and retrieving sensitive information from their training data. This violated privacy expectations that training data would be \"forgotten\" after model training, revealing that scale amplifies privacy risk as larger models (175B parameters) memorize more training data than smaller models.\n\nThe research revealed that common data protection measures proved insufficient. Even after data deduplication, models still memorized sensitive information, highlighting the tension between model utility and privacy protection. Techniques to prevent memorization such as differential privacy and aggressive data filtering reduce model quality, creating challenging trade-offs for practitioners.\n\nThe industry response was swift and comprehensive. Organizations began widespread adoption of differential privacy in large model training, enhanced data filtering and PII removal processes, development of membership inference defenses, new research into machine unlearning techniques, and regulatory discussions about training data rights and model transparency. Modern organizations now commonly implement differential privacy during training (ε ≤ 8), aggressive PII filtering using automated detection tools, regular auditing for data memorization using extraction attacks, and legal frameworks for handling training data containing personal information [@carlini2021extracting].\n\n### Secure Model Design {#sec-security-privacy-secure-model-design-69a6}\n\nMoving from data-level protections to model-level security, we address how security considerations shape the model development process. Security begins at the design phase of a machine learning system. While downstream mechanisms such as access control and encryption protect models once deployed, many vulnerabilities can be mitigated earlier—through architectural choices, defensive training strategies, and mechanisms that embed resilience directly into the model's structure or behavior. By considering security as a design constraint, system developers can reduce the model's exposure to attacks, limit its ability to leak sensitive information, and provide verifiable ownership protection.\n\nOne important design strategy is to build robust-by-construction models that reduce the risk of exploitation at inference time. For instance, models with confidence calibration or abstention mechanisms can be trained to avoid making predictions when input uncertainty is high. These techniques can help prevent overconfident misclassifications in response to adversarial or out-of-distribution inputs. Models may also employ output smoothing, regularizing the output distribution to reduce sharp decision boundaries that are especially susceptible to adversarial perturbations.\n\nCertain application contexts may also benefit from choosing simpler or compressed architectures. Limiting model capacity can reduce opportunities for memorization of sensitive training data and complicate efforts to reverse-engineer the model from output behavior. For embedded or on-device settings, smaller models are also easier to secure, as they typically require less memory and compute, lowering the likelihood of side-channel leakage or runtime manipulation.\n\nAnother design-stage consideration is the use of model watermarking[^fn-model-watermarking], a technique for embedding verifiable ownership signatures directly into the model's parameters or output behavior [@adi2018turning]. A watermark might be implemented, for example, as a hidden response pattern triggered by specific inputs, or as a parameter-space perturbation that does not affect accuracy but is statistically identifiable.\n\n[^fn-model-watermarking]: **Model Watermarking**: Technique for proving model ownership developed in 2017, analogous to digital image watermarks. Modern watermarking can embed signatures in less than 0.01% of model parameters while maintaining 99%+ accuracy, helping prove IP theft in courts where billions of dollars in AI assets are at stake.\n\nFor example, in a keyword spotting system deployed on embedded hardware for voice activation (e.g., \"Hey Alexa\" or \"OK Google\"), a secure design might use a lightweight convolutional neural network with confidence calibration to avoid false activations on uncertain audio. The model might also include an abstention threshold, below which it produces no activation at all. To protect intellectual property, a designer could embed a watermark by training the model to respond with a unique label only when presented with a specific, unused audio trigger known only to the developer. These design choices not only improve robustness and accountability, but also support future verification in case of IP disputes or performance failures in the field.\n\nIn high-risk applications, such as medical diagnosis, autonomous vehicles, or financial decision systems, designers may also prioritize interpretable model architectures, such as decision trees, rule-based classifiers, or sparsified networks, to enhance system auditability. These models are often easier to understand and explain, making it simpler to identify potential vulnerabilities or biases. Using interpretable models allows developers to provide clearer insights into how the system arrived at a particular decision, which is important for building trust with users and regulators.\n\nModel design choices often reflect trade-offs between accuracy, robustness, transparency, and system complexity. When viewed from a systems perspective, early-stage design decisions yield the highest value for long-term security. They shape what the model can learn, how it behaves under uncertainty, and what guarantees can be made about its provenance, interpretability, and resilience.\n\n### Secure Model Deployment {#sec-security-privacy-secure-model-deployment-e08c}\n\nWhile secure design establishes a foundation of robustness, protection extends beyond the model itself to how it is packaged and deployed. Protecting machine learning models from theft, abuse, and unauthorized manipulation requires security considerations throughout both the design and deployment phases. A model's vulnerability is not solely determined by its training procedure or architecture, but also by how it is serialized, packaged, deployed, and accessed during inference. As models are increasingly embedded into edge devices, served through public APIs, or integrated into multi-tenant platforms, robust security practices are important to ensure the integrity, confidentiality, and availability of model behavior.\n\nThis section addresses security mechanisms across three key stages: model design, secure packaging and serialization, and deployment and access control. These practices complement model optimization techniques such as quantization, pruning, and knowledge distillation, where performance improvements must not compromise security properties.\n\nFrom a design perspective, architectural choices can reduce a model's exposure to adversarial manipulation and unauthorized use. For example, models can incorporate confidence calibration or abstention mechanisms that allow them to reject uncertain or anomalous inputs rather than producing potentially misleading outputs. Designing models with simpler or compressed architectures can also reduce the risk of reverse engineering or information leakage through side-channel analysis. In some cases, model designers may embed imperceptible watermarks, which are unique signatures embedded in the parameters or behavior of the model, that can later be used to demonstrate ownership in cases of misappropriation [@uchida2017embedding]. These design-time protections are essential for commercially valuable models, where intellectual property rights are at stake.\n\nOnce training is complete, the model must be securely packaged for deployment. Storing models in plaintext formats, including unencrypted ONNX or PyTorch checkpoint files, can expose internal structures and parameters to attackers with access to the file system or memory. To mitigate this risk, models should be encrypted, obfuscated, or wrapped in secure containers. Decryption keys should be made available only at runtime and only within trusted environments. Additional mechanisms, such as quantization-aware encryption or integrity-checking wrappers, can prevent tampering and offline model theft.\n\nDeployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests.\n\n[^fn-oauth]: **OAuth Protocol**: Open Authorization standard developed in 2006, now used by 3+ billion users across Google, Facebook, and Microsoft services. OAuth 2.0 (2012) enables secure API access without exposing user credentials, processing trillions of authentication requests annually for ML API access.\n\n[^fn-mutual-tls]: **Mutual TLS (mTLS)**: Enhanced Transport Layer Security where both client and server authenticate each other using certificates, introduced in 1999. mTLS provides 99.9%+ secure communication but increases latency by 15-30ms, making it suitable for high-security ML API endpoints requiring end-to-end authentication.\n\n[^fn-api-keys]: **API Keys**: Simple authentication tokens first popularized by Google Maps API (2005), now ubiquitous in ML services. While convenient, API keys in URL parameters or headers can be logged or exposed, with studies showing 10-15% of GitHub repositories accidentally contain leaked API keys worth millions in compute credits.\n\n[^fn-rbac]: **Role-Based Access Control (RBAC)**: Access control model developed by NIST in the 1990s, now mandatory for government systems. RBAC reduces security administration overhead by 90%+ compared to individual permissions, with modern ML platforms supporting thousands of roles governing model access, data permissions, and compute resources.\n\nThis key authenticates the client and allows the backend to enforce usage policies, monitor for abuse, and log access patterns. Secure implementations retrieve API keys from environment variables rather than hardcoding them into source code, preventing credential exposure in version control systems or application logs. Such key-based access control mechanisms are simple to implement but require careful key management and monitoring to prevent misuse, unauthorized access, or model extraction. Additional security measures in production deployments typically include model integrity verification through SHA-256 hash checking, rate limiting to prevent abuse, input validation for size and format constraints, and comprehensive logging for security event tracking.\n\nThe secure deployment patterns established here integrate naturally with development workflows, ensuring security becomes part of standard engineering practice rather than an afterthought. Runtime monitoring (@sec-security-privacy-runtime-system-monitoring-a71c) extends these protections to operational environments.\n\n### Runtime System Monitoring {#sec-security-privacy-runtime-system-monitoring-a71c}\n\nWhile secure design and deployment establish strong foundations, protection must extend to runtime operations. Even with robust design and deployment safeguards, machine learning systems remain vulnerable to runtime threats. Attackers may craft inputs that bypass validation, exploit model behavior, or target system-level infrastructure.\n\nProduction ML systems face diverse deployment contexts—from cloud services to edge devices to embedded systems. Each environment presents unique monitoring challenges and opportunities. Defensive strategies must extend beyond static protection to include real-time monitoring, threat detection, and incident response. This section outlines operational defenses that maintain system trust under adversarial conditions.\n\nRuntime monitoring encompasses a range of techniques for observing system behavior, detecting anomalies, and triggering mitigation. These techniques can be grouped into three categories: input validation, output monitoring, and system integrity checks.\n\n#### Input Validation {#sec-security-privacy-input-validation-c96f}\n\nInput validation is the first line of defense at runtime. It ensures that incoming data conforms to expected formats, statistical properties, or semantic constraints before it is passed to a machine learning model. Without these safeguards, models are vulnerable to adversarial inputs, which are crafted examples designed to trigger incorrect predictions, or to malformed inputs that cause unexpected behavior in preprocessing or inference.\n\nMachine learning models, unlike traditional rule-based systems, often do not fail safely. Small, carefully chosen changes to input data can cause models to make high-confidence but incorrect predictions. Input validation helps detect and reject such inputs early in the pipeline [@goodfellow2015explaining].\n\nValidation techniques range from low-level checks (e.g., input size, type, and value ranges) to semantic filters (e.g., verifying whether an image contains a recognizable object or whether a voice recording includes speech). For example, a facial recognition system might validate that the uploaded image is within a certain resolution range (e.g., 224×224 to 1024×1024 pixels), contains RGB channels, and passes a lightweight face detection filter. This prevents inputs like blank images, text screenshots, or synthetic adversarial patterns from reaching the model. Similarly, a voice assistant might require that incoming audio files be between 1 and 5 seconds long, have a valid sampling rate (e.g., 16kHz), and contain detectable human speech using a speech activity detector (SAD)[^fn-speech-activity-detector]. This ensures that empty recordings, music clips, or noise bursts are filtered before model inference.\n\n[^fn-speech-activity-detector]: **Speech Activity Detector (SAD)**: Algorithm that distinguishes speech from silence, noise, or music in audio streams, essential for voice interfaces since the 1990s. Modern neural SADs achieve 95%+ accuracy and operate in <10ms latency, enabling real-time filtering before expensive speech recognition processing.\n\nIn generative systems such as DALL·E, Stable Diffusion, or Sora, input validation often involves prompt filtering. This includes scanning the user's text prompt for banned terms, brand names, profanity, or misleading medical claims. For example, a user prompt like \"Generate an image of a medication bottle labeled with Pfizer's logo\" might be rejected or rewritten due to trademark concerns. Filters may operate using keyword lists, regular expressions, or lightweight classifiers that assess prompt intent. These filters prevent the generative model from being used to produce harmful, illegal, or misleading content—even before sampling begins.\n\nIn some applications, distributional checks are also used. These assess whether the incoming data statistically resembles what the model saw during training. For instance, a computer vision pipeline might compare the color histogram of the input image to a baseline distribution, flagging outliers for manual review or rejection.\n\nThese validations can be lightweight (heuristics or threshold rules) or learned (small models trained to detect distribution shift or adversarial artifacts). In either case, input validation serves as a important pre-inference firewall—reducing exposure to adversarial behavior, improving system stability, and increasing trust in downstream model decisions.\n\n#### Output Monitoring {#sec-security-privacy-output-monitoring-cf37}\n\nEven when inputs pass validation, adversarial or unexpected behavior may still emerge at the model's output. Output monitoring helps detect such anomalies by analyzing model predictions in real time. These mechanisms observe how the model behaves across inputs, by tracking its confidence, prediction entropy, class distribution, or response patterns, to flag deviations from expected behavior.\n\nA key target for monitoring is prediction confidence. For example, if a classification model begins assigning high confidence to low-frequency or previously rare classes, this may indicate the presence of adversarial inputs or a shift in the underlying data distribution. Monitoring the entropy of the output distribution can similarly reveal when the model is overly certain in ambiguous contexts—an early signal of possible manipulation.\n\nIn content moderation systems, a model that normally outputs neutral or \"safe\" labels may suddenly begin producing high-confidence \"safe\" labels for inputs containing offensive or restricted content. Output monitoring can detect this mismatch by comparing predictions against auxiliary signals or known-safe reference sets. When deviations are detected, the system may trigger a fallback policy—such as escalating the content for human review or switching to a conservative baseline model.\n\nTime-series models also benefit from output monitoring. For instance, an anomaly detection model used in fraud detection might track predicted fraud scores for sequences of financial transactions. A sudden drop in fraud scores, especially during periods of high transaction volume, may indicate model tampering, label leakage, or evasion attempts. Monitoring the temporal evolution of predictions provides a broader perspective than static, pointwise classification.\n\nGenerative models, such as text-to-image systems, introduce unique output monitoring challenges. These models can produce high-fidelity imagery that may inadvertently violate content safety policies, platform guidelines, or user expectations. To mitigate these risks, post-generation classifiers are commonly employed to assess generated content for objectionable characteristics such as violence, nudity, or brand misuse. These classifiers operate downstream of the generative model and can suppress, blur, or reject outputs based on predefined thresholds. Some systems also inspect internal representations (e.g., attention maps[^fn-attention-maps] or latent embeddings) to anticipate potential misuse before content is rendered.\n\n[^fn-attention-maps]: **Attention Maps**: Visualization technique for understanding transformer model focus, introduced with the attention mechanism in 2015. Attention maps reveal which input tokens influence outputs most strongly, helping detect potential bias or manipulation in models processing 175+ billion parameters like GPT-3.\n\nHowever, prompt filtering alone is insufficient for safety. Research has shown that text-to-image systems can be manipulated through implicitly adversarial prompts, which are queries that appear benign but lead to policy-violating outputs. The Adversarial Nibbler project introduces an open red teaming methodology that identifies such prompts and demonstrates how models like Stable Diffusion can produce unintended content despite the absence of explicit trigger phrases [@quaye2024adversarial]. These failure cases often bypass prompt filters because their risk arises from model behavior during generation, not from syntactic or lexical cues.\n\n![**Adversarial Prompt Evasion**: Implicitly adversarial prompts bypass typical content filters by triggering unintended generations, revealing limitations of solely relying on pre-generation safety checks. these examples underscore the necessity of post-hoc content analysis as a complementary defense layer for robust generative AI systems. Source: [@quaye2024adversarial.].](images/png/adversarial_nibbler_example.png){#fig-adversarial-nibbler}\n\nAs shown in @fig-adversarial-nibbler, even prompts that appear innocuous can trigger unsafe generations. Such examples highlight the limitations of pre-generation safety checks and reinforce the necessity of output-based monitoring as a second line of defense. This two-stage pipeline—consisting of prompt filtering followed by post-hoc content analysis important for ensuring the safe deployment of generative models in open-ended or user-facing environments.\n\nIn the domain of language generation, output monitoring plays a different but equally important role. Here, the goal is often to detect toxicity, hallucinated claims, or off-distribution responses. For example, a customer support chatbot may be monitored for keyword presence, tonal alignment, or semantic coherence. If a response contains profanity, unsupported assertions, or syntactically malformed text, the system may trigger a rephrasing, initiate a fallback to scripted templates, or halt the response altogether.\n\nEffective output monitoring combines rule-based heuristics with learned detectors trained on historical outputs. These detectors are deployed to flag deviations in real time and feed alerts into incident response pipelines. In contrast to model-centric defenses like adversarial training, which aim to improve model robustness, output monitoring emphasizes containment and remediation. Its role is not to prevent exploitation but to detect its symptoms and initiate appropriate countermeasures [@savas2022ml]. In safety-important or policy-sensitive applications, such mechanisms form a important layer of operational resilience.\n\nThese principles have been implemented in recent output filtering frameworks. For example, LLM Guard combines transformer-based classifiers with safety dimensions such as toxicity, misinformation, and illegal content to assess and reject prompts or completions in instruction-tuned LLMs [@lee2023llmguard]. Similarly, [ShieldGemma](https://ai.google.dev/gemma/docs/shieldgemma), developed as part of Google's open Gemma model release, applies configurable scoring functions to detect and filter undesired outputs during inference. Both systems exemplify how safety classifiers and output monitors are being integrated into the runtime stack to support scalable, policy-aligned deployment of generative language models.\n\n#### Integrity Checks {#sec-security-privacy-integrity-checks-2989}\n\nWhile input and output monitoring focus on model behavior, system integrity checks ensure that the underlying model files, execution environment, and serving infrastructure remain untampered throughout deployment. These checks detect unauthorized modifications, verify that the model running in production is authentic, and alert operators to suspicious system-level activity.\n\nOne of the most common integrity mechanisms is cryptographic model verification. Before a model is loaded into memory, the system can compute a cryptographic hash (e.g., SHA-256)[^fn-sha256-security] of the model file and compare it against a known-good signature.\n\n[^fn-sha256-security]: **SHA-256**: Cryptographic hash function producing 256-bit digests, part of the SHA-2 family designed by the NSA in 2001. Despite processing trillions of hashes daily across Bitcoin mining and digital signatures, no practical collision attacks exist after 20+ years, making it the gold standard for file integrity verification.\n\nAccess control and audit logging complement cryptographic checks. ML systems should restrict access to model files using role-based permissions and monitor file access patterns. For instance, repeated attempts to read model checkpoints from a non-standard path, or inference requests from unauthorized IP ranges, may indicate tampering, privilege escalation, or insider threats.\n\nIn cloud environments, container- or VM-based isolation[^fn-container-vm-isolation] helps enforce process and memory boundaries, but these protections can erode over time due to misconfiguration or supply chain vulnerabilities.\n\n[^fn-container-vm-isolation]: **Container/VM Isolation**: Virtualization technologies that provide process and memory separation—containers (Docker, 2013) offer lightweight OS-level isolation with typically 0-5% overhead for CPU-bound workloads and 2-10% for I/O-intensive operations, while VMs provide stronger hardware-level isolation with 10-15% overhead. In ML deployments, containerization is widely adopted for model serving, with industry surveys suggesting 80-90% adoption in cloud environments, though VMs remain preferred for sensitive models requiring stronger isolation guarantees.\n\nFor example, in a regulated healthcare ML deployment[^fn-healthcare-ml-compliance], integrity checks might include: verifying the model hash against a signed manifest, validating that the runtime environment uses only approved Python packages, and checking that inference occurs inside a signed and attested virtual machine. These checks ensure compliance with regulations like HIPAA[^fn-hipaa-ml-requirements]'s integrity requirements and GDPR's accountability principle, limit the risk of silent failures, and create a forensic trail in case of audit or breach.\n\n[^fn-healthcare-ml-compliance]: **Healthcare ML Compliance**: FDA has approved 500+ AI-based medical devices since 2016, requiring strict validation under 21 CFR Part 820 quality systems. Healthcare ML systems must demonstrate safety, efficacy, and bias mitigation, with some approvals taking 2-5 years and costing $50+ million in clinical trials.\n\n[^fn-hipaa-ml-requirements]: **HIPAA ML Requirements**: The Health Insurance Portability and Accountability Act (1996) imposes strict data protection rules affecting 600+ million patient records in the US. For ML systems, HIPAA requires encryption of data at rest and in transit, audit logs for all data access, and business associate agreements for cloud ML services, with violations carrying fines up to $1.5 million per incident.\n\nSome systems also implement runtime memory verification, such as scanning for unexpected model parameter changes or checking that memory-mapped model weights remain unaltered during execution. While more common in high-assurance systems, such checks are becoming more feasible with the adoption of secure enclaves and trusted runtimes.\n\nTaken together, system integrity checks play a important role in protecting machine learning systems from low-level attacks that bypass the model interface. When coupled with input/output monitoring, they provide layered assurance that both the model and its execution environment remain trustworthy under adversarial conditions.\n\n#### Response and Rollback {#sec-security-privacy-response-rollback-1792}\n\nWhen a security breach, anomaly, or performance degradation is detected in a deployed machine learning system, rapid and structured incident response is important to minimizing impact. The goal is not only to contain the issue but to restore system integrity and ensure that future deployments benefit from the insights gained. Unlike traditional software systems, ML responses may require handling model state, data drift, or inference behavior, making recovery more complex.\n\nThe first step is to define incident detection thresholds that trigger escalation. These thresholds may come from input validation (e.g., invalid input rates), output monitoring (e.g., drop in prediction confidence), or system integrity checks (e.g., failed model signature verification). When a threshold is crossed, the system should initiate an automated or semi-automated response protocol.\n\nOne common strategy is model rollback, where the system reverts to a previously verified version of the model. For instance, if a newly deployed fraud detection model begins misclassifying transactions, the system may fall back to the last known-good checkpoint, restoring service while the affected version is quarantined. Rollback mechanisms require version-controlled model storage, typically supported by MLOps platforms such as MLflow, TFX, or SageMaker.\n\nIn high-availability environments, model isolation may be used to contain failures. The affected model instance can be removed from load balancers or shadowed in a canary deployment setup. This allows continued service with unaffected replicas while maintaining forensic access to the compromised model for analysis.\n\nTraffic throttling is another immediate response tool. If an adversarial actor is probing a public inference API at high volume, the system can rate-limit or temporarily block offending IP ranges while continuing to serve trusted clients. This containment technique helps prevent abuse without requiring full system shutdown.\n\nOnce immediate containment is in place, investigation and recovery can begin. This may include forensic analysis of input logs, parameter deltas between model versions, or memory snapshots from inference containers. In regulated environments, organizations may also need to notify users or auditors, particularly if personal or safety-important data was affected.\n\nRecovery typically involves retraining or patching the model. This must occur through a secure update process, using signed artifacts, trusted build pipelines, and validated data. To prevent recurrence, the incident should feed back into model evaluation pipelines—updating tests, refining monitoring thresholds, or hardening input defenses. For example, if a prompt injection attack bypassed a content filter in a generative model, retraining might include adversarially crafted prompts, and the prompt validation logic would be updated to reflect newly discovered patterns.\n\nFinally, organizations should establish post-incident review practices. This includes documenting root causes, identifying gaps in detection or response, and updating policies and playbooks. Incident reviews help translate operational failures into actionable improvements across the design-deploy-monitor lifecycle.\n\n### Hardware Security Foundations {#sec-security-privacy-hardware-security-foundations-f5e8}\n\nThe software-layer defenses we've explored—input validation, output monitoring, and integrity checks—establish important protections, but they ultimately depend on the underlying hardware and firmware being trustworthy. If an attacker compromises the operating system, gains physical access to the device, or exploits vulnerabilities in the processor itself, these software defenses can be bypassed or disabled entirely. This limitation motivates hardware-based security mechanisms that operate below the software layer, creating a hardware root of trust that remains secure even when higher-level systems are compromised.\n\nAt the foundational level of our defensive framework, hardware-based security mechanisms provide the trust anchor for all higher-layer protections. Machine learning systems deployed in edge devices, embedded systems, and untrusted cloud infrastructure increasingly rely on hardware-based security features to establish this foundation. Hardware acceleration platforms, including GPUs, TPUs, and specialized ML accelerators, often incorporate security features such as secure enclaves, trusted execution environments, and hardware cryptographic units, while edge deployment scenarios present unique security challenges due to physical accessibility and constrained resources.\n\nThese hardware security mechanisms become particularly crucial when systems must meet regulatory compliance requirements. Healthcare ML systems handling protected health information under HIPAA must implement \"appropriate technical safeguards\" including access controls and encryption. Systems processing EU citizens' data under GDPR must demonstrate \"appropriate technical and organizational measures\" with privacy by design principles embedded at the hardware level.\n\nTo understand how hardware security protects ML systems, imagine building a secure fortress for your most valuable assets. Each hardware security primitive serves a distinct defensive role:\n\n+-----------------------+----------------------------------------------------------------------+\n| **Mechanism**         | **Fortress Analogy and Function**                                    |\n+:======================+:=====================================================================+\n| **Secure Boot**       | Functions like a trusted gatekeeper checking credentials of everyone |\n|                       | entering the fortress at dawn. Before your system runs any code,     |\n|                       | Secure Boot cryptographically verifies that the firmware and         |\n|                       | operating system haven't been tampered with.                         |\n+-----------------------+----------------------------------------------------------------------+\n| **Trusted Execution** | Create secure, windowless rooms deep inside the fortress where you   |\n+-----------------------+----------------------------------------------------------------------+\n| **Environments**      | handle your most sensitive operations. When your ML model processes  |\n+-----------------------+----------------------------------------------------------------------+\n| **(TEEs)**            | private medical data or proprietary algorithms, the TEE isolates     |\n|                       | these computations from the rest of the system.                      |\n+-----------------------+----------------------------------------------------------------------+\n| **Hardware Security** | Serve as specialized, impenetrable vaults designed specifically for  |\n+-----------------------+----------------------------------------------------------------------+\n| **Modules (HSMs)**    | storing and using your most valuable cryptographic keys. Rather than |\n|                       | keeping encryption keys in regular computer memory where they might  |\n|                       | be stolen, HSMs provide tamper-resistant storage.                    |\n+-----------------------+----------------------------------------------------------------------+\n| **Physical**          | Give each device a unique biometric fingerprint at the silicon       |\n+-----------------------+----------------------------------------------------------------------+\n| **Unclonable**        | level. Just as human fingerprints cannot be perfectly replicated,    |\n+-----------------------+----------------------------------------------------------------------+\n| **Functions (PUFs)**  | PUFs exploit tiny manufacturing variations in each chip to create    |\n|                       | device-unique identifiers that cannot be cloned.                     |\n+-----------------------+----------------------------------------------------------------------+\n\n: **Hardware Security Mechanisms**: Each primitive provides distinct defensive capabilities that work together to create comprehensive protection from hardware-level threats. {#tbl-hardware-security-mechanisms}\n\nThese mechanisms work together to create comprehensive protection that begins in hardware and extends through all software layers.\n\nThis section explores how these four complementary hardware primitives work together to create comprehensive protection (@tbl-hardware-security-mechanisms). Each mechanism addresses different security challenges but works most effectively when combined: secure boot establishes initial trust, TEEs provide runtime isolation, HSMs handle cryptographic operations, and PUFs enable device-unique authentication. We begin with Trusted Execution Environments (TEEs), which provide isolated runtime environments for sensitive computations. Secure Boot ensures system integrity from power-on, creating the trusted foundation that TEEs depend upon. Hardware Security Modules (HSMs) offer specialized cryptographic processing and tamper-resistant key storage, often required for regulatory compliance. Finally, Physical Unclonable Functions (PUFs) provide device-unique identities that enable lightweight authentication and cannot be cloned or extracted.\n\nEach mechanism addresses different aspects of the security challenge, working most effectively when deployed together across hardware, firmware, and software boundaries.\n\n#### Hardware-Software Co-Design {#sec-security-privacy-hardwaresoftware-codesign-bed2}\n\nModern ML systems require holistic analysis of security trade-offs across the entire hardware-software stack, similar to how we analyze compute-memory-energy trade-offs in performance optimization. The interdependence between hardware security features and software defenses creates both opportunities and constraints that must be understood quantitatively.\n\nHardware security mechanisms introduce measurable overhead that must be factored into system design. ARM TrustZone world-switching adds approximately 300-1000 cycles depending on processor generation and cache state (0.6-2.0μs at 500MHz) of latency per transition between secure and non-secure worlds. Cryptographic operations in secure mode typically consume 15-30% additional power compared to normal execution, impacting battery life in mobile ML applications. Intel SGX context switching imposes 15-30μs overhead per inference, representing 2% energy overhead for typical edge ML workloads.\n\nSecurity features scale differently than computational resources. TEE memory limitations constrain model size regardless of available system memory. A quantized ResNet-18 model (47MB) can operate within ARM TrustZone constraints, while ResNet-50 (176MB) requires careful memory management or model partitioning. These constraints create architectural decisions that must be made early in system design.\n\nDifferent threat models and protection levels require quantitative trade-off analysis. For ML workloads requiring cryptographic verification, AES-256 operations add 0.1-0.5ms per inference depending on model size and hardware acceleration availability. Homomorphic encryption operations impose 100-100,000x computational overhead, with fully homomorphic encryption (FHE) at the higher end and somewhat homomorphic encryption (SHE) at the lower end, making them viable only for small models or offline scenarios where strong privacy guarantees justify the performance cost.\n\n#### Trusted Execution Environments {#sec-security-privacy-trusted-execution-environments-80ed}\n\nA Trusted Execution Environment (TEE)[^fn-tee] is a hardware-isolated region within a processor designed to protect sensitive computations and data from potentially compromised software. TEEs enforce confidentiality, integrity, and runtime isolation, ensuring that even if the host operating system or application layer is attacked, sensitive operations within the TEE remain secure.\n\n[^fn-tee]: **TEE Concept Origins**: The idea emerged from ARM's TrustZone development in the early 2000s, inspired by the military concept of \"compartmentalized information.\" ARM realized that mobile devices needed secure and non-secure \"worlds\" running on the same processor—leading to hardware-enforced isolation that became the template for all modern TEEs.]\n\nIn the context of machine learning, TEEs are increasingly important for preserving the confidentiality of models, securing sensitive user data during inference, and ensuring that model outputs remain trustworthy. For example, a TEE can protect model parameters from being extracted by malicious software running on the same device, or ensure that computations involving biometric inputs, including facial data or fingerprint data, are performed securely. This capability is essential in applications where model integrity, user privacy, or regulatory compliance are non-negotiable.\n\nOne widely deployed example is [Apple's Secure Enclave](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web), which provides isolated execution and secure key storage for iOS devices. By separating cryptographic operations and biometric data from the main processor, the Secure Enclave ensures that user credentials and Face ID features remain protected, even in the event of a broader system compromise.\n\nTrusted Execution Environments are important across a range of industries with high security requirements. In telecommunications, TEEs are used to safeguard encryption keys and secure important 5G control-plane operations. In finance, they allow secure mobile payments and protect PIN-based authentication workflows. In healthcare, TEEs help enforce patient data confidentiality during edge-based ML inference on wearable or diagnostic devices. In the automotive industry, they are deployed in advanced driver-assistance systems (ADAS) to ensure that safety-important perception and decision-making modules operate on verified software.\n\nIn machine learning systems, TEEs can provide several important protections. They secure the execution of model inference or training, shielding intermediate computations and final predictions from system-level observation. They protect the confidentiality of sensitive inputs, including biometric or clinical signals, used in personal identification or risk scoring tasks. TEEs also serve to prevent reverse engineering of deployed models by restricting access to weights and architecture internals. When models are updated, TEEs ensure the authenticity of new parameters and block unauthorized tampering. In distributed ML settings, TEEs can protect data exchanged between components by enabling encrypted and attested communication channels.\n\nThe core security properties of a TEE are achieved through four mechanisms: isolated execution, secure storage, integrity protection, and in-TEE data encryption. Code that runs inside the TEE is executed in a separate processor mode, inaccessible to the normal-world operating system. Sensitive assets such as cryptographic keys or authentication tokens are stored in memory that only the TEE can access. Code and data can be verified for integrity before execution using hardware-anchored hashes or signatures. Finally, data processed inside the TEE can be encrypted, ensuring that even intermediate results are inaccessible without appropriate keys, which are also managed internally by the TEE.\n\nSeveral commercial platforms provide TEE functionality tailored for different deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. [Qualcomm's Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions) supports secure mobile transactions and user authentication. Apple's Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices.\n\n[^fn-trustzone-adoption]: **ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting over 5 billion mobile devices. Despite its ubiquity, many devices underutilize TrustZone—studies show only 20-30% of Android devices implement meaningful secure world applications beyond basic key storage.\n\n[^fn-intel-sgx-limits]: **Intel SGX Constraints**: SGX enclaves are limited to approximately 128MB of protected memory (EPC) on most consumer processors, though enterprise variants support up to 512MB or 1GB, with cache misses causing 100x performance penalties. For ML workloads, a ResNet-50 requires approximately 98MB for weights alone in FP32 format (25.6M parameters × 4 bytes), consuming 77% of SGX EPC before any intermediate activations. Inference latency increases from 5ms to 150ms when model exceeds EPC capacity. This makes SGX unsuitable for large ML models but effective for protecting cryptographic keys and small inference models under 10MB.\n\n@fig-enclave illustrates a secure enclave integrated into a system-on-chip (SoC) architecture. The enclave includes a dedicated processor, an AES engine, a true random number generator (TRNG), a public key accelerator (PKA), and a secure I²C interface to nonvolatile storage. These components operate in isolation from the main application processor and memory subsystem. A memory protection engine enforces access control, while cryptographic operations such as NAND flash encryption are handled internally using enclave-managed keys. By physically separating secure execution and key management from the main system, this architecture limits the impact of system-level compromises and establishes hardware-enforced trust.\n\n::: {#fig-enclave fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n},\nscale=0.9, every node/.append style={transform shape}]\n\\tikzset{%\nLineA/.style={line width=1.0pt,black!50,latex-latex},\nLine/.style={BrownLine!60, {Triangle[width = 8pt, length = 6pt]}-{Triangle[width = 8pt, length = 6pt]}, line width = 4pt},\nLineD/.style={line width=1.0pt,black!50,latex-,dashed},\nBox/.style={inner xsep=2pt,inner ysep=6pt,\n    node distance=0.25,\n    draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    align=flush center,\n    text width=28mm,\n    minimum width=28mm, minimum height=10.5mm\n  },\nBox2/.style={Box,draw=RedLine,fill=RedL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2, text width=42mm,\n    minimum width=42mm, minimum height=8mm},\nBox4/.style={Box,anchor=west,minimum width=42mm, minimum height=48mm},\nBox4a/.style={Box,draw=BlueLine,fill=BlueL!50,anchor=west,minimum width=42mm, minimum height=48mm},\nBox5/.style={Box,draw=BlueLine,fill=BlueL!50},\nBox6/.style={Box,draw=OrangeLine,fill=OrangeL!50},\n}\n\n\\tikzset{pics/key/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=FLAG1,scale=\\scalefac, every node/.append style={transform shape}]\n\\draw[draw=\\drawchannelcolor,fill=\\channelcolor](-0.28,0.04)to[out=310,in=90](-0.16,-0.053)\nto[out=270,in=90](-0.16,-0.15)to[out=270,in=110](-0.11,-0.22)\nto[out=220,in=80](-0.14,-0.28)to(-0.05,-0.38)to(-0.12,-0.47)to(-0.08,-0.55)to(-0.14,-0.62)\nto(-0.09,-0.7)to(-0.13,-0.76)to[out=290,in=70](-0.11,-0.9)to(0.03,-1.06)to[out=30,in=270](0.15,-0.9)\nto(0.15,-0.01)to[out=10,in=350,distance=3](0.15,0.125)to[out=60,in=220](0.25,0.21)\nto[out=30,in=330](0.25,0.97)to[out=150,in=30](-0.278,0.97)\nto[out=210,in=150](-0.278,0.21)to[out=330,in=20](-0.24,0.08)to[out=200,in=80]cycle;\n\\fill[white](-0.018,0.77)circle(4pt);\n\\end{scope}\n     }\n  }\n}\n\\pgfkeys{\n  /channel/.cd,\n  channelcolor/.store in=\\channelcolor,\n  drawchannelcolor/.store in=\\drawchannelcolor,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  channelcolor=BrownLine,\n  drawchannelcolor=BrownLine,\n  scalefac=1,\n  Linewidth=1.6pt,\n  picname=C\n}\n\n\\node[Box](B1){TRNG};\n\\node[Box,below=of B1](B2){Secure Enclave AES Engine};\n\\node[Box,below=of B2](B3){PKA};\n\\node[Box2,below=of B3](B4){I2C bus};\n\\node[Box3,below=1.35of B4](B5){Secure Nonvolatile Storage};\n%\n\\node[Box4](SEP)at($(B1.north east)!0.5!(B4.south east)+(1.25,0)$){Secure Enclave\\\\ Processor};\n\\node[Box,anchor=west](MPE)at($(SEP.east)+(1.25,0)$){Memory Protection\\\\Engine};\n%\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=5mm,inner ysep=4mm,\nyshift=0mm,fill=none,fit=(B1)(B4)(MPE),line width=0.75pt](BB1){};\n%\n\\node[Box4a,anchor=south west](AP)at($(B1.north west)+(0,1.3)$){Application\\\\ Processor};\n\\node[Box5,anchor=south west](NAN)at($(AP.east)+(1,0.2)$){NAND flash controller};\n\\node[Box5,anchor=north west](AES)at($(AP.east)+(1,-0.45)$){AES engine};\n\\path[](MPE)|-coordinate(S)(AP.north east);\n\\node[Box5,anchor=north](MC)at(S){Memory controller};\n%\n\\node[Box6,above=1 of MC](DRAM){DRAM};\n\\path[](DRAM)-|coordinate(SS)(NAN);\n\\node[Box6](NAND)at(SS){NAND flash\\\\ storage};\n\\draw[Line](SEP)--(MPE);\n\\draw[Line](MPE)--(MC);\n\\draw[Line](MC)--(DRAM);\n\\draw[Line](NAN)--(NAND);\n\\draw[Line](NAN)--(AES);\n\\draw[LineD](AES)--coordinate(KEY)(AES|-BB1.north);\n\\draw[Line](NAN)--(NAN-|MC);\n\\draw[Line](AES)--(AES-|MC);\n\\draw[Line](AP.315)--(AP.315-|MC);\n\\draw[Line](B4)--(B5);\n%\n\\scoped[on background layer]\n\\node[draw=RedLine,inner xsep=5mm,inner ysep=5mm,\nyshift=-0.5mm,fill=magenta!5,fit=(BB1)(AP)(MC),line width=0.75pt](BB2){};\n\\node[above=4pt of  BB2.south,inner sep=0pt,\nanchor=south]{\\textbf{System on chip}};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=5mm,inner ysep=4mm,\nyshift=0mm,fill=BackColor!60,fit=(B1)(B4)(MPE),line width=0.75pt](BB3){};\n\\node[above=6pt of  BB3.south,inner sep=0pt,\nanchor=south]{\\textbf{Secure Enclave}};\n%\n\\begin{scope}[local bounding box=KEY1,shift={($(KEY)+(0.4,-0.30)$)},\nscale=1, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){key={scalefac=0.4,picname=1,drawchannelcolor=none,\nchannelcolor=red!79!black!90, Linewidth=1.0pt}};\n \\end{scope}\n\\end{tikzpicture}\n```\n**Secure Enclave Architecture**: Hardware-isolated enclaves enhance system security by encapsulating sensitive data and cryptographic operations within a dedicated processor and memory. This design minimizes the attack surface and protects important keys even if the main application processor is compromised, providing a trusted execution environment for security-important tasks. Source: Apple.\n:::\n\nThis architecture underpins the secure deployment of machine learning applications on consumer devices. For example, Apple's Face ID system uses a secure enclave to perform facial recognition entirely within a hardware-isolated environment. The face embedding model is executed inside the enclave, and biometric templates are stored in secure nonvolatile memory accessible only via the enclave's I²C interface. During authentication, input data from the infrared camera is processed locally, and no facial features or predictions ever leave the secure region. Even if the application processor or operating system is compromised, the enclave prevents access to sensitive model inputs, parameters, and outputs—ensuring that biometric identity remains protected end to end.\n\nDespite their strengths, Trusted Execution Environments come with notable trade-offs. Implementing a TEE increases both direct hardware costs and indirect costs associated with developing and maintaining secure software. Integrating TEEs into existing systems may require architectural redesigns, especially for legacy infrastructure. Developers must adhere to strict protocols for isolation, attestation, and secure update management, which can extend development cycles and complicate testing workflows. TEEs can also introduce performance overhead, particularly when cryptographic operations are involved, or when context switching between trusted and untrusted modes is frequent.\n\nEnergy efficiency is another consideration, particularly in battery-constrained devices. TEEs typically consume additional power due to secure memory accesses, cryptographic computation, and hardware protection logic. In resource-limited embedded systems, these costs may limit their use. In terms of scalability and flexibility, the secure boundaries enforced by TEEs may complicate distributed training or federated inference workloads, where secure coordination between enclaves is required.\n\nMarket demand also varies. In some consumer applications, perceived threat levels may be too low to justify the integration of TEEs. Systems with TEEs may be subject to formal security certifications, such as [Common Criteria](https://www.commoncriteriaportal.org/ccra/index.cfm) or evaluation under [ENISA](https://www.enisa.europa.eu/), which can introduce additional time and expense. For this reason, TEEs are typically adopted only when the expected threat model, including adversarial users, cloud tenants, and malicious insiders, justifies the investment.\n\nNonetheless, TEEs remain a powerful hardware primitive in the machine learning security landscape. When paired with software- and system-level defenses, they provide a trusted foundation for executing ML models securely, privately, and verifiably, especially in scenarios where adversarial compromise of the host environment is a serious concern.\n\nHere is the revised 7.5.2 Secure Boot section, rewritten in formal textbook tone with all original technical content, hyperlinks, and figures preserved. The structure emphasizes narrative clarity, avoids bullet lists, and integrates the Apple Face ID case study naturally.\n\n#### Secure Boot {#sec-security-privacy-secure-boot-5242}\n\nSecure Boot is a mechanism that ensures a device only boots software components that are cryptographically verified and explicitly authorized by the manufacturer. At startup, each stage of the boot process, comprising the bootloader, kernel, and base operating system, is checked against a known-good digital signature. If any signature fails verification, the boot sequence is halted, preventing unauthorized or malicious code from executing. This chain-of-trust model establishes system integrity from the very first instruction executed.\n\nIn ML systems, especially those deployed on embedded or edge hardware, Secure Boot plays an important role. A compromised boot process may result in malicious software loading before the ML runtime begins, enabling attackers to intercept model weights, tamper with training data, or reroute inference results. Such breaches can lead to incorrect or manipulated predictions, unauthorized data access, or device repurposing for botnets or crypto-mining.\n\nFor machine learning systems, Secure Boot offers several guarantees. First, it protects model-related data, such as training data, inference inputs, and outputs, during the boot sequence, preventing pre-runtime tampering. Second, it ensures that only authenticated model binaries and supporting software are loaded, which helps guard against deployment-time model substitution. Third, Secure Boot allows secure model updates by verifying that firmware or model changes are signed and have not been altered in transit.\n\nSecure Boot frequently works in tandem with hardware-based Trusted Execution Environments (TEEs) to create a fully trusted execution stack. As shown in @fig-secure-boot, this layered boot process verifies firmware, operating system components, and TEE integrity before permitting execution of cryptographic operations or ML workloads. In embedded systems, this architecture provides resilience even under severe adversarial conditions or physical device compromise.\n\n::: {#fig-secure-boot fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={black!50,-{Triangle[width = 6pt, length = 6pt]}, line width = 1.15pt,text=black},\nBox/.style={inner xsep=2pt,inner ysep=6pt,\n    node distance=0.5,\n    draw=BlueLine,\n    line width=0.75pt,\n    fill=BlueL!40,\n    align=flush center,\n    text width=58mm,\n    minimum width=58mm, minimum height=10mm\n  },\nBox2/.style={Box,draw=RedLine,fill=RedL,rounded corners=12pt},\nBox3/.style={draw=VioletLine,fill=VioletL2, trapezium, trapezium left angle=70,\ndiamond, minimum width=45mm, minimum height=15mm, text centered,inner sep= -2ex},\nBox4/.style={Box,anchor=west,minimum width=42mm, minimum height=48mm},\nBox5/.style={Box,draw=BlueLine,fill=BlueL!50},\nBox6/.style={Box,draw=OrangeLine,fill=OrangeL!50},\n}\n\n\\node[Box2](B1){Power up};\n\\node[Box,below=of B1](B2){Hardware init};\n\\node[Box,below=of B2](B3){Get the information of MTM device, complete the boot and self-diagnosis process of MTM, store the diagnostic information of hardware platform and MTM device, set verification register};\n\\node[Box,below=of B3](B4){Copy kernel image and root FS image from FLASH to RAM};\n\\node[Box,below=of B4](B5){Checking the CRC checksum of kernel image};\n\\node[Box,below=of B5](B6){Perform integrity measurement, verification, and storage of Linux kernel image};\n\\node[Box3,below=0.6of B6](B7){Success};\n\\node[Box,below=0.7of B7](B8){Boot abort};\n%\n\\node[Box,right=3 of B3](B9){Checking the CRC checksum of root fs image};\n\\node[Box,below=1of B9](B10){Perform integrity measurement, verification, and storage of root fs image};\n\\node[Box3,below=1of B10](B11){Success};\n\\node[Box,below=1of B11](B12){Set boot parameters for kernel};\n\\node[Box,below=1of B12](B13){Booting the kernel};\n%\n\\foreach \\x in{1,2,3,4,5,6}{\n\\pgfmathtruncatemacro{\\newX}{\\x + 1} %\n\\draw[Line](B\\x)--(B\\newX);\n}\n\\foreach \\x in{9,10,12}{\n\\pgfmathtruncatemacro{\\newX}{\\x + 1} %\n\\draw[Line](B\\x)--(B\\newX);\n}\n\\draw[Line](B7)--node[right,pos=0.3]{No}(B8);\n\\draw[Line](B7.east)--node[above,pos=0.25]{Yes}++(1.8,0)--++(0,10.5)-|(B9);\n\\draw[Line](B11.west)--node[above,pos=0.25]{No}++(-1.8,0)|-(B8);\n\\draw[Line](B11)--node[right,pos=0.3]{Yes}(B12);\n\\end{tikzpicture}\n```\n**Secure Boot Sequence**: Embedded systems employ a layered boot process to verify firmware and software integrity, establishing a root of trust before executing machine learning workloads and protecting against pre-runtime attacks. This architecture ensures only authenticated code runs, safeguarding model data and preventing unauthorized model substitution or modification during deployment. Source: [@rashmi2018secure].\n:::\n\nA well-known real-world implementation of Secure Boot appears in Apple's Face ID system, which uses advanced machine learning for facial recognition. For Face ID to operate securely, the entire device stack, from the initial power-on to the execution of the model, must be verifiably trusted.\n\nUpon device startup, Secure Boot initiates within Apple's [Secure Enclave](https://support.apple.com/en-us/102381), a dedicated security coprocessor that handles biometric data. The firmware loaded onto the Secure Enclave is digitally signed by Apple, and any unauthorized modification causes the boot process to fail. Once verified, the Secure Enclave performs continuous checks in coordination with the central processor to maintain a trusted boot chain. Each system component, ranging from the iOS kernel to the application-level code, is verified using cryptographic signatures.\n\nAfter completing the secure boot sequence, the Secure Enclave activates the ML-based Face ID system. The facial recognition model projects over 30,000 infrared points to map a user's face, generating a depth image and computing a mathematical representation that is compared against a securely stored profile. These facial data artifacts are never written to disk, transmitted off-device, or shared externally. All processing occurs within the enclave to protect against eavesdropping or exfiltration, even in the presence of a compromised kernel.\n\nTo support continued integrity, Secure Boot also governs software updates. Only firmware or model updates signed by Apple are accepted, ensuring that even over-the-air patches do not introduce risk. This process maintains a robust chain of trust over time, enabling the secure evolution of the ML system while preserving user privacy and device security.\n\nWhile Secure Boot provides strong protection, its adoption presents technical and operational challenges. Managing the cryptographic keys used to sign and verify system components is complex, especially at scale. Enterprises must securely provision, rotate, and revoke keys, ensuring that no trusted root is compromised. Any such breach would undermine the entire security chain.\n\nPerformance is also a consideration. Verifying signatures during the boot process introduces latency, typically on the order of tens to hundreds of milliseconds per component. Although acceptable in many applications, these delays may be problematic for real-time or power-constrained systems. Developers must also ensure that all components, including bootloaders, firmware, kernels, drivers, and even ML models, are correctly signed. Integrating third-party software into a Secure Boot pipeline introduces additional complexity.\n\nSome systems limit user control in favor of vendor-locked security models, restricting upgradability or customization. In response, open-source bootloaders like [u-boot](https://source.denx.de/u-boot/u-boot) and [coreboot](https://www.coreboot.org/) have emerged, offering Secure Boot features while supporting extensibility and transparency. To further scale trusted device deployments, emerging industry standards such as the [Device Identifier Composition Engine (DICE)](https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/) and [IEEE 802.1AR IDevID](https://1.ieee802.org/security/802-1ar/) provide mechanisms for secure device identity, key provisioning, and cross-vendor trust assurance.\n\nSecure Boot, when implemented carefully and complemented by trusted hardware and secure software update processes, forms the backbone of system integrity for embedded and distributed ML. It provides the assurance that the machine learning model running in production is not only the correct version, but is also executing in a known-good environment, anchored to hardware-level trust.\n\n#### Hardware Security Modules {#sec-security-privacy-hardware-security-modules-4377}\n\nWhile TEEs and secure boot provide runtime isolation and integrity verification, Hardware Security Modules (HSMs) specialize in the cryptographic operations that underpin these protections. An HSM[^fn-hsm-performance] is a tamper-resistant physical device designed to perform cryptographic operations and securely manage digital keys. HSMs are widely used across security-important industries such as finance, defense, and cloud infrastructure, and they are increasingly relevant for securing the machine learning pipeline—particularly in deployments where key confidentiality, model integrity, and regulatory compliance are important.\n\n[^fn-hsm-performance]: **HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $20,000-$100,000+ per unit. In contrast, software-only cryptography on GPUs achieves 100,000+ operations/second at $1,000+ hardware cost, but without the tamper-resistance and regulatory compliance that HSMs provide.\n\nHSMs provide an isolated, hardened environment for performing sensitive operations such as key generation, digital signing, encryption, and decryption. Unlike general-purpose processors, they are engineered to withstand physical tampering and side-channel attacks, and they typically include protected storage, cryptographic accelerators, and internal audit logging. HSMs may be implemented as standalone appliances, plug-in modules, or integrated chips embedded within broader systems.\n\nIn machine learning systems, HSMs enhance security across several dimensions. They are commonly used to protect encryption keys associated with sensitive data that may be processed during training or inference. These keys might encrypt data at rest in model checkpoints or allow secure transmission of inference requests across networked environments. By ensuring that the keys are generated, stored, and used exclusively within the HSM, the system minimizes the risk of key leakage, unauthorized reuse, or tampering.\n\nHSMs also play a role in maintaining the integrity of machine learning models. In many production pipelines, models must be signed before deployment to ensure that only verified versions are accepted into runtime environments. The signing keys used to authenticate models can be stored and managed within the HSM, providing cryptographic assurance that the deployed artifact is authentic and untampered. Similarly, secure firmware updates and configuration changes, regardless of whether they pertain to models, hyperparameters, or supporting infrastructure, can be validated using signatures produced by the HSM.\n\nIn addition to protecting inference workloads, HSMs can be used to secure model training. During training, data may originate from distributed and potentially untrusted sources. HSM-backed protocols can help ensure that training pipelines perform encryption, integrity checks, and access control enforcement securely and in compliance with organizational or legal requirements. In regulated industries such as healthcare and finance, such protections are often mandatory. For instance, HIPAA requires covered entities to implement technical safeguards including \"integrity controls\" and \"encryption and decryption,\" while GDPR mandates pseudonymization and encryption as examples of appropriate technical measures.\n\nDespite these benefits, incorporating HSMs into embedded or resource-constrained ML systems introduces several trade-offs. First, HSMs are specialized hardware components and often come at a premium. Their cost may be justified in data center settings or safety-important applications but can be prohibitive for low-margin embedded products or wearables. Physical space is also a concern. Embedded systems often operate under strict size, weight, and form factor constraints, and integrating an HSM may require redesigning circuit layouts or sacrificing other functionality.\n\nFrom a performance standpoint, HSMs introduce latency, particularly for operations like key exchange, signature verification, or on-the-fly decryption. In real-time inference systems, including autonomous vehicles, industrial robotics, and live translation devices, these delays can affect responsiveness. While HSMs are typically optimized for cryptographic throughput, they are not general-purpose processors, and offloading secure operations must be carefully coordinated.\n\nPower consumption is another concern. The continuous secure handling of keys, signing of transactions, and cryptographic validations can consume more power than basic embedded components, impacting battery life in mobile or remote deployments.\n\nIntegration complexity also grows when HSMs are introduced into existing ML pipelines. Interfacing between the HSM and the host processor requires dedicated APIs and often specialized software development. Firmware and model updates must be routed through secure, signed channels, and update orchestration must account for device-specific key provisioning. These requirements increase the operational burden, especially in large deployments.\n\nScalability presents its own set of challenges. Managing a distributed fleet of HSM-equipped devices requires secure provisioning of individual keys, secure identity binding, and coordinated trust management. In large ML deployments, including fleets of smart sensors or edge inference nodes, ensuring uniform security posture across all devices is nontrivial.\n\nFinally, the use of HSMs often requires organizations to engage in certification and compliance processes[^fn-hsm-certification], particularly when handling regulated data. Meeting standards such as FIPS 140-2[^fn-fips-140] or Common Criteria adds time and cost to development.\n\n[^fn-hsm-certification]: **HSM Certification**: Hardware Security Module certification under FIPS 140-2 or Common Criteria can take 12-24 months and cost $500,000-$2 million. However, many regulated industries require these certifications, with banking, government, and healthcare sectors mandating Level 3+ certified HSMs for cryptographic operations.\n\n[^fn-fips-140]: **FIPS 140-2 Standard**: Federal Information Processing Standard for cryptographic modules, established in 2001 with four security levels. Level 4 HSMs must survive physical attacks, operating at -40°C to +85°C with tamper detection that zeroizes keys within seconds, making them suitable for the most sensitive ML applications. Access to the HSM is typically restricted to a small set of authorized personnel, which can complicate development workflows and slow iteration cycles.\n\nDespite these operational complexities, HSMs remain a valuable option for machine learning systems that require high assurance of cryptographic integrity and access control. When paired with TEEs, secure boot, and software-based defenses, HSMs contribute to a multilayered security model that spans hardware, system software, and ML runtime.\n\n#### Physical Unclonable Functions {#sec-security-privacy-physical-unclonable-functions-6533}\n\nPhysical Unclonable Functions (PUFs)[^fn-puf-adoption] provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication by leveraging physical randomness in semiconductor fabrication [@gassend2002silicon]. Unlike traditional keys stored in memory, a PUF generates secret values based on microscopic variations in a chip's physical properties—variations that are inherent to manufacturing processes and difficult to clone or predict, even by the manufacturer.\n\n[^fn-puf-adoption]: **PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT security needs. Major semiconductor companies including Intel, Xilinx, and Synopsis now offer PUF IP, with deployment in smart cards, automotive ECUs, and edge ML devices requiring device-unique authentication.\n\nThese variations arise from uncontrollable physical factors such as doping concentration, line edge roughness, and dielectric thickness. As a result, even chips fabricated with the same design masks exhibit small but measurable differences in timing, power consumption, or voltage behavior. PUF circuits amplify these variations to produce a device-unique digital output. When a specific input challenge is applied to a PUF, it generates a corresponding response based on the chip's physical fingerprint. Because these characteristics are effectively impossible to replicate, the same challenge will yield different responses across devices.\n\nThis challenge-response mechanism allows PUFs to serve several cryptographic purposes. They can be used to derive device-specific keys that never need to be stored externally, reducing the attack surface for key exfiltration. The same mechanism also supports secure authentication and attestation, where devices must prove their identity to trusted servers or hardware gateways. These properties make PUFs a natural fit for machine learning systems deployed in embedded and distributed environments.\n\nIn ML applications, PUFs offer unique advantages for securing resource-constrained systems. For example, consider a smart camera drone that uses onboard computer vision to track objects. A PUF embedded in the drone's processor can generate a private key to encrypt the model during boot. Even if the model were extracted, it would be unusable on another device lacking the same PUF response. That same PUF-derived key could also be used to watermark the model parameters, creating a cryptographically verifiable link between a deployed model and its origin hardware. If the model were leaked or pirated, the embedded watermark could help prove the source of the compromise.\n\nPUFs also support authentication in distributed ML pipelines. If the drone offloads computation to a cloud server, the PUF can help verify that the drone has not been cloned or tampered with. The cloud backend can issue a challenge, verify the correct response from the device, and permit access only if the PUF proves device authenticity. These protections enhance trust not only in the model and data, but in the execution environment itself.\n\nThe internal operation of a PUF is illustrated in @fig-pfu. At a high level, a PUF accepts a challenge input and produces a unique response determined by the physical microstructure of the chip [@gao2020physical]. Variants include optical PUFs, in which the challenge consists of a light pattern and the response is a speckle image, and electronic PUFs such as Arbiter PUFs (APUFs), where timing differences between circuit paths produce a binary output. Another common implementation is the SRAM PUF, which exploits the power-up state of uninitialized SRAM cells: due to threshold voltage mismatch, each cell tends to settle into a preferred value when power is first applied. These response patterns form a stable, reproducible hardware fingerprint.\n\n![**Physical Unclonable Functions**: Pufs generate unique hardware fingerprints from inherent manufacturing variations, enabling device authentication and secure key generation without storing secrets. Optical and electronic PUF implementations use physical phenomena—such as light speckle patterns or timing differences—to produce challenge-response pairs that are difficult to predict or replicate. Source: [@gao2020physical].](images/png/puf_basics.png){#fig-pfu}\n\nDespite their promise, PUFs present several challenges in system design. Their outputs can be sensitive to environmental variation, such as changes in temperature or voltage, which can introduce instability or bit errors in the response. To ensure reliability, PUF systems must often incorporate error correction codes or helper data schemes. Managing large sets of challenge-response pairs also raises questions about storage, consistency, and revocation. Additionally, the unique statistical structure of PUF outputs may make them vulnerable to machine learning-based modeling attacks if not carefully shielded from external observation.\n\nFrom a manufacturing perspective, incorporating PUF technology can increase device cost or require additional layout complexity. While PUFs eliminate the need for external key storage, thereby reducing long-term security risk and provisioning cost, they may require calibration and testing during fabrication to ensure consistent performance across environmental conditions and device aging.\n\nNevertheless, Physical Unclonable Functions remain a compelling building block for securing embedded machine learning systems. By embedding hardware identity directly into the chip, PUFs support lightweight cryptographic operations, reduce key management burden, and help establish root-of-trust anchors in distributed or resource-constrained environments. When integrated thoughtfully, they complement other hardware-assisted security mechanisms such as Secure Boot, TEEs, and HSMs to provide defense-in-depth across the ML system lifecycle.\n\n#### Mechanisms Comparison {#sec-security-privacy-mechanisms-comparison-2dcb}\n\nHardware-assisted security mechanisms play a foundational role in establishing trust within modern machine learning systems. While software-based defenses offer flexibility, they ultimately rely on the security of the hardware platform. As machine learning workloads increasingly operate on edge devices, embedded platforms, and untrusted infrastructure, hardware-backed protections become important for maintaining system integrity, confidentiality, and trust.\n\nTrusted Execution Environments (TEEs) provide runtime isolation for model inference and sensitive data handling. Secure Boot enforces integrity from power-on, ensuring that only verified software is executed. Hardware Security Modules (HSMs) offer tamper-resistant storage and cryptographic processing for secure key management, model signing, and firmware validation. Physical Unclonable Functions (PUFs) bind secrets and authentication to the physical characteristics of a specific device, enabling lightweight and unclonable identities.\n\nThese mechanisms address different layers of the system stack, ranging from initialization and attestation to runtime protection and identity binding, and complement one another when deployed together. @tbl-hw-security-comparison below compares their roles, use cases, and trade-offs in machine learning system design.\n\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Mechanism**                | **Primary Function**         | **Common Use in ML**                   | **Trade-offs**                               |\n+:=============================+:=============================+:=======================================+:=============================================+\n| **Trusted Execution**        | Isolated runtime environment | Secure inference and on-device privacy | Added complexity, memory limits, perf. cost  |\n|                              | for secure computation       | for sensitive inputs and outputs       |                                              |\n| **Environment (TEE)**        |                              |                                        | Requires trusted code development            |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Secure Boot**              | Verified boot sequence       | Ensures only signed ML models and      | Key management complexity, vendor lock-in    |\n|                              | and firmware validation      | firmware execute on embedded devices   | Performance impact during startup            |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Hardware Security Module** | Secure key generation and    | Signing ML models, securing training   | High cost, integration overhead, limited I/O |\n| **(HSM)**                    | storage, crypto-processing   | pipelines, verifying firmware          |                                              |\n|                              |                              |                                        | Requires infrastructure-level provisioning   |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n| **Physical Unclonable**      | Hardware-bound identity      | Model binding, device authentication,  | Environmental sensitivity, modeling attacks  |\n|                              | and key derivation           | protecting IP in embedded deployments  |                                              |\n| **Function (PUF)**           |                              |                                        | Needs error correction and calibration       |\n+------------------------------+------------------------------+----------------------------------------+----------------------------------------------+\n\n: **Hardware Security Mechanisms**: Machine learning systems use diverse hardware defenses—trusted execution environments, secure boot, hardware security modules, and physical unclonable functions—to establish trust and protect sensitive data across the system stack. The table details how each mechanism addresses specific security challenges—from runtime isolation and integrity verification to key management and device identity—and emphasizes the associated trade-offs in performance and complexity. {#tbl-hw-security-comparison}\n\nTogether, these hardware primitives form the foundation of a defense-in-depth strategy for securing ML systems in adversarial environments. Their integration is especially important in domains that demand provable trust, such as autonomous vehicles, healthcare devices, federated learning systems, and important infrastructure.\n\n<!-- ### Toward Trustworthy Systems {#sec-security-privacy-toward-trustworthy-systems-1e6d}\n\nDefending machine learning systems against adversarial threats, misuse, and system compromise requires more than isolated countermeasures. As this section has shown, effective defense emerges from the careful integration of mechanisms at multiple layers of the ML stack—from privacy-preserving data handling and robust model design to runtime monitoring and hardware-enforced isolation. No single component can provide complete protection; instead, a trustworthy system is the result of coordinated design decisions that address risk across the data, model, system, and infrastructure layers.\n\nDefensive strategies must align with the deployment context and threat model. What is appropriate for a public cloud API may differ from the requirements of an embedded medical device or a fleet of edge-deployed sensors. Design choices must balance security, performance, and usability, recognizing that protections often introduce operational trade-offs. Monitoring and incident response mechanisms ensure resilience during live operation, while hardware-based roots of trust ensure system integrity even when higher layers are compromised.\n\nAs machine learning continues to expand into safety-important, privacy-sensitive, and decentralized environments, the need for robust, end-to-end defense becomes increasingly urgent. Building ML systems that are not only accurate, but secure, private, and auditable, is core to long-term deployment success and public trust.\n\nThe technical defenses we've established here form the foundation for broader robustness frameworks. While this chapter has focused on protecting against malicious attacks and privacy breaches, robust AI systems must extend these concepts to ensure system-wide reliability under all forms of stress—from natural distribution shifts to hardware failures. The adversarial training techniques introduced here for security become part of a comprehensive robustness strategy that includes uncertainty quantification, out-of-distribution detection, and graceful degradation. Similarly, the monitoring infrastructure we've established for security incident detection provides the foundation for the broader observability systems required for robust AI deployment.\n\nThese security and privacy foundations connect directly to operational practices for implementing protections at scale. The energy and computational overhead of security measures must be considered within sustainability frameworks, and the broader ethical implications connect to responsible AI practices explored throughout this volume.\n\nThe process of engineering trustworthy ML systems requires a structured approach that applies the layered defense principles established earlier to specific deployment contexts. @fig-trustworthy-ml-recipe provides a practical framework to guide this process across technical and deployment dimensions. The design flow begins with a thorough assessment of the threat model and deployment context, which informs the selection of appropriate defenses from our established stack. This includes data-layer protections such as differential privacy (DP), federated learning (FL), and encryption; model-layer defenses like robustness techniques, watermarking, and secure deployment practices; runtime-layer measures such as input validation and output monitoring; and hardware-layer solutions including TEEs, secure boot, and PUFs.\n\n::: {#fig-trustworthy-ml-recipe fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}]\n\\tikzset{%\nLine/.style={BrownLine!60, -{Triangle[width = 6pt, length = 6pt]}, line width = 1.25pt},\nBox/.style={inner xsep=2pt,inner ysep=6pt,\n    node distance=0.5,\n    draw=RedLine,\n    line width=0.75pt,\n    fill=RedL!60,\n    align=flush center,\n    text width=37mm,\n    minimum width=37mm, minimum height=15.5mm\n  },\nBox2/.style={Box,draw=GreenD,fill=GreenL},\nBox3/.style={Box,  draw=VioletLine,fill=VioletL2},\nBox4/.style={Box,draw=VioletLine,fill=VioletL2},\nBox5/.style={Box,draw=BlueLine,fill=BlueL!50},\nBox6/.style={Box,draw=OrangeLine,fill=OrangeL!50},\n}\n\\node[Box](B1){Data Layer: DP, FL, Encryption};\n\\node[Box,right=of B1,  text width=44mm,minimum width=44mm](B2){Model Layer: Robustness, Watermarking, Secure Deployment};\n\\node[Box,right=of B2](B3){Runtime Layer: Input Validation, Output Monitoring};\n\\node[Box,right=of B3](B4){Hardware Layer: TEEs, Secure Boot, PUFs};\n %\n\\node[Box2,above=0.7 of B2,minimum width=44mm](B22){Select Defenses Across the Stack};\n\\node[Box3,above=0.7 of B22,minimum width=44mm](B23){Assess Threat Model \\& Deployment Context};\n%\n\\node[Box6,below=0.7 of B4, text width=40mm,minimum width=44mm](B44){Plan for Runtime Adaptation and Recovery};\n\\node[Box5,below=0.7  of B44](B45){Rollback, Isolation, Incident Response};\n\\node[Box5,left=of B45](B46){Monitoring, Logging, Alerting};\n\\node[Box5,right=of B45](B47){Design Feedback\\\\ Loop};\n\\draw[Line](B23)--(B22);\n\\draw[Line](B22)--(B2);\n\\draw[Line](B22)-|(B1);\n\\draw[Line](B22)-|(B3);\n\\draw[Line](B22)-|(B4);\n\\draw[Line](B4)--(B44);\n\\draw[Line](B44)-|(B46);\n\\draw[Line](B44)-|(B47);\n\\draw[Line](B44)--(B45);\n\\end{tikzpicture}\n```\n\n**Trustworthy ML System Design**: Engineering secure and private machine learning systems requires a layered approach, integrating defenses at the data, model, runtime, and hardware levels to mitigate evolving threats and ensure responsible deployment. This design flow connects threat modeling with practical safeguards, enabling robust and auditable ML solutions for safety-important applications.\n:::\n\nThis design flow emphasizes the importance of a comprehensive approach to security, where each layer of the system is fortified against potential threats while remaining adaptable to evolving risks. By integrating these principles into the design and deployment of machine learning systems, organizations can build solutions that are not only effective but also resilient, trustworthy, and aligned with ethical standards.\n\n### Defense Selection Framework {#sec-security-privacy-defense-selection-framework-320b}\n\nGiven the breadth of security mechanisms presented, practitioners require systematic methods for selecting appropriate defenses. The choice depends on multiple interacting factors that must be evaluated holistically rather than in isolation.\n\nStep 1: Threat Model Assessment\n\nBegin by characterizing potential adversaries and their capabilities:\n\n- **Who are the adversaries?** Nation-states possess unlimited resources and advanced capabilities. Competitors seek intellectual property and strategic advantages. Curious users probe for vulnerabilities. Insider threats combine access with intent. Each adversary type demands different defensive priorities.\n\n- **What capabilities do they have?** White-box attackers with full model access require different defenses than black-box attackers limited to API queries. Physical access enables hardware attacks that remote adversaries cannot execute. Understanding attacker capabilities determines which defense mechanisms provide meaningful protection versus security theater.\n\n- **What assets require protection?** Training data privacy, model intellectual property, inference confidentiality, and system availability each require specialized defenses. Healthcare applications prioritize patient data protection (HIPAA compliance). Financial systems emphasize transaction integrity and fraud prevention. Ranking asset criticality guides defense investment.\n\n- **What is acceptable risk?** Regulatory environments define minimum acceptable security (GDPR, HIPAA, PCI-DSS). Reputational considerations establish higher bars—a data breach that is legally permissible may still be commercially devastating. Risk tolerance determines how much performance degradation and development cost organizations accept for security gains.\n\nStep 2: Deployment Context Constraints\n\nSecurity choices must respect operational realities:\n\n- **Computational Budget:** Cloud deployments afford unlimited horizontal scaling, enabling expensive techniques like homomorphic encryption or extensive monitoring. Edge devices operate under severe constraints—secure enclaves consume precious memory, cryptographic operations drain batteries. Embedded systems may lack security hardware entirely. Defense selection must match available compute resources.\n\n- **Latency Requirements:** Real-time applications (autonomous vehicles, industrial control) tolerate minimal latency overhead. Differential privacy and input validation must execute within millisecond budgets. Batch processing systems (training pipelines, offline analytics) can absorb expensive techniques like secure multi-party computation. Understanding latency budgets constrains feasible defenses.\n\n- **Update Frequency:** Continuously learning systems require runtime security that adapts as models evolve. Static deployments can rely on one-time hardening (model encryption, watermarking). Over-the-air update capabilities enable security patches but introduce new attack surfaces. Update patterns determine whether defenses must be dynamic or can be baked-in.\n\n- **Physical Security:** Data center deployments assume physical security and focus on logical defenses. Field-deployed devices face physical threats (tampering, extraction) requiring hardware security modules and tamper-evident packaging. Public-facing kiosks need different protections than secured facilities.\n\nStep 3: Regulatory and Compliance Requirements\n\nLegal mandates establish non-negotiable security baselines:\n\n- **GDPR** (EU): Mandates data minimization, purpose limitation, privacy by design. Differential privacy and federated learning help demonstrate compliance. Cross-border data transfer restrictions favor on-device processing.\n\n- **HIPAA** (Healthcare): Requires access controls, audit logging, encryption at rest and in transit. HSMs for key management and comprehensive logging become mandatory rather than optional.\n\n- **CCPA** (California): Establishes consumer privacy rights including data deletion. Systems must support cryptographic erasure and maintain data provenance.\n\n- **Industry Standards:** Payment Card Industry Data Security Standard (PCI-DSS), Federal Information Security Management Act (FISMA), and sector-specific regulations impose additional requirements. Non-compliance incurs financial penalties and operational restrictions.\n\nStep 4: Recommended Layered Approach\n\nRather than selecting individual defenses, deploy coordinated protection:\n\n1. Baseline Security (Universal): All ML systems require authentication, access control, encrypted communications (TLS), audit logging, and basic input validation. These foundational defenses cost little and prevent common attacks. Omitting them constitutes negligence.\n\n2. Domain-Specific Controls (Context-Dependent): Healthcare systems add differential privacy for training data and TEEs for inference. Financial systems deploy HSMs for cryptographic operations and extensive transaction monitoring. Public-facing APIs implement rate limiting and behavioral analysis.\n\n3. Layered Defenses (No Single Point of Failure): Assume each defense will be bypassed eventually. Differential privacy prevents training data extraction even if models are stolen. Input validation catches adversarial examples even if models lack robustness. Monitoring detects attacks that evade technical defenses. Redundancy ensures attack success requires compromising multiple independent layers.\n\n4. Validation Through Red-Team Exercises: Theoretical security assessments miss practical vulnerabilities. Hire adversarial experts to attempt realistic attacks. Document failures and iterate defenses. Treat security as a continuous improvement process rather than one-time implementation.\n\nStep 5: Trade-Off Optimization\n\nSecurity is never free. Quantify costs and benefits:\n\n- **Performance vs. Protection:** If differential privacy reduces accuracy by 5% but enables GDPR compliance, the trade-off may be mandatory. If homomorphic encryption adds 10 seconds of latency to millisecond-budget applications, alternative approaches are required.\n\n- **Development Time vs. Risk Mitigation:** Basic security (authentication, encryption) adds weeks to development. Advanced techniques (federated learning, secure enclaves) require months of specialized engineering. Prioritize defenses by risk reduction per engineering hour.\n\n- **Operational Overhead vs. Attack Detection:** Comprehensive monitoring adds 10-20% infrastructure cost. Intrusion detection systems generate false positives requiring investigation. Balance detection capabilities against operational burden.\n\nThe optimal security architecture emerges from systematically evaluating these factors rather than applying techniques prescriptively. Different deployment contexts demand different solutions—recognize this diversity and design accordingly. -->\n\n<!-- ### Defense Selection Framework {#sec-security-privacy-selecting-appropriate-defenses-decision-framework-491e-framework-a48f}\n\nTo support practical decision making, @tbl-defense-selection-framework maps common threat scenarios to appropriate defensive techniques, organized by deployment context. This framework synthesizes the strategies presented throughout this section into actionable patterns.\n\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Deployment Context**      | **Primary Threats**                 | **Recommended Defenses**                         | **Key Trade-offs**                            |\n+:============================+:====================================+:=================================================+:==============================================+\n| **Healthcare ML**           | Data leakage (HIPAA violation),     | - Differential Privacy (ε ≤ 4) for training      | 2-5% accuracy loss acceptable for compliance; |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Federated diagnostic**   | membership inference,               | - Federated Learning across hospitals            | 50-100ms inference latency from TEE overhead  |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **models)**                 | unauthorized access                 | - TEE for inference on sensitive data            |                                               |\n|                             |                                     | - Audit logging and access control (RBAC)        |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Financial ML**            | Model theft (IP loss),              | - Model encryption (AES-256) at rest             | HSM adds $10-50K capital cost; rate limiting  |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Fraud detection API)**   | adversarial evasion,                | - HSM for cryptographic key management           | may impact legitimate high-frequency users    |\n|                             | data poisoning                      | - Adversarial training (PGD-based)               |                                               |\n|                             |                                     | - Input validation + rate limiting (100 req/min) |                                               |\n|                             |                                     | - Output confidence monitoring                   |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Edge ML**                 | Physical access,                    | - Secure Boot (verified firmware)                | TEE memory limits constrain model size        |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Mobile/IoT devices)**    | side-channel attacks,               | - ARM TrustZone or similar TEE                   | (&lt;50MB); quantization required for large   |\n|                             | model extraction                    | - Model quantization + obfuscation               | models; 15-30% power overhead from encryption |\n|                             |                                     | - Encrypted model storage                        |                                               |\n|                             |                                     | - Anti-tampering hardware (PUF)                  |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Cloud ML Training**       | Data poisoning,                     | - Secure data pipelines (provenance tracking)    | Training time increases 30-120% with DP;      |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Multi-tenant platform)** | backdoor injection,                 | - Differential Privacy (DP-SGD, ε ≈ 1-10)        | gradient verification adds 10-15% compute     |\n|                             | gradient leakage                    | - Gradient verification and anomaly detection    | overhead; federated aggregation requires      |\n|                             |                                     | - Secure aggregation (if federated)              | secure communication protocols                |\n|                             |                                     | - Model watermarking for IP protection           |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Public-Facing LLM**       | Prompt injection,                   | - Input sanitization (prompt filtering)          | Aggressive filtering may block 5-10% of       |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Chatbot/API)**           | data extraction (training leakage), | - Output monitoring (PII detection)              | legitimate requests; response time increases  |\n|                             | abuse/overuse                       | - Rate limiting (per-user quotas)                | 50-100ms for content filtering; watermarking  |\n|                             |                                     | - Response watermarking                          | may be detectable by sophisticated users      |\n|                             |                                     | - Confidence thresholding (abstention)           |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Multi-Party ML**          | Data sharing restrictions,          | - Federated Learning (no raw data sharing)       | Communication overhead: 10-100x more rounds   |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Cross-organizational**   | honest-but-curious participants,    | - SMPC for secure aggregation                    | than centralized training; SMPC adds 1000x+   |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **training)**               | privacy compliance (GDPR)           | - Differential Privacy (ε ≤ 1)                   | compute cost; accuracy may degrade 5-15%;     |\n|                             |                                     | - Homomorphic Encryption (for sensitive ops)     | requires legal agreements for liability       |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **Critical Infrastructure** | Supply chain compromise,            | - Hardware attestation (TPM/PUF)                 | Development cost: 6-18 months additional      |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **(Autonomous vehicles,**   | real-time adversarial attacks,      | - Secure Boot + runtime integrity checks         | engineering; 20-40% higher hardware costs;    |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n| **power grids)**            | safety-critical failures            | - Redundant model validation                     | latency constraints limit cryptographic       |\n|                             |                                     | - Fault injection detection                      | defenses; requires certified hardware         |\n|                             |                                     | - Fail-safe fallback mechanisms                  |                                               |\n+-----------------------------+-------------------------------------+--------------------------------------------------+-----------------------------------------------+\n\n: **Defense Selection Framework**: Maps deployment contexts to threat-specific defensive strategies with quantified trade-offs. The framework provides starting points for security architecture design, highlighting primary threats, recommended defense combinations, and key implementation trade-offs across common ML system deployment scenarios. Practitioners should adapt these patterns based on specific regulatory requirements, risk assessments, and operational constraints. {#tbl-defense-selection-framework}\n\nThis framework provides starting points rather than complete solutions. Real-world systems typically require combinations of defenses tailored to specific regulatory requirements, threat models, and operational constraints. Use these patterns as templates, adapting them based on risk assessments, compliance mandates, and resource availability. The baseline defenses identified in Step 1 (authentication, TLS, audit logging, input validation) apply universally and are assumed present in all deployment contexts above. -->\n\n<!-- ### Implementation Roadmap: Securing a Production ML System {#sec-security-privacy-implementation-roadmap-securing-production-ml-system-511c-securing-production-ml-system-511c}\n\nSecuring production machine learning systems requires a systematic approach that evolves from establishing basic protections to implementing comprehensive defense mechanisms. Organizations face the challenge of protecting valuable models and sensitive data while maintaining system performance and meeting regulatory requirements. The path from an unsecured ML deployment to a robust, trustworthy system follows a natural progression through four critical phases, each building upon the security foundations established by its predecessor. This roadmap provides the security-specific implementation details that complement broader operational frameworks, where these protections are integrated into continuous deployment pipelines, monitoring systems, and incident response workflows.\n\nThe journey begins with foundational security controls that provide immediate risk reduction across all ML deployments. Access control mechanisms form the first line of defense, determining who can interact with ML infrastructure and at what level. Organizations typically leverage existing identity providers like Active Directory or OAuth 2.0 to implement role-based access control, ensuring data scientists can train models while limiting production access to authorized deployment systems. Communication security follows, with TLS 1.3 or newer protocols protecting all ML API endpoints from eavesdropping and tampering. This encryption layer becomes particularly critical when models process sensitive data or operate across untrusted networks. Input validation and rate limiting at inference endpoints prevent both accidental misuse and intentional abuse, using API gateways to enforce schema validation and throttle excessive requests. Finally, comprehensive audit logging creates the forensic trail necessary for security monitoring and compliance reporting, capturing all model access, training operations, and data transformations in centralized systems like ELK stack or Splunk.\n\nOnce these foundations are established, typically within the first two weeks of implementation, organizations progress to data privacy protections that address regulatory requirements and ethical obligations. This phase begins with comprehensive data classification, cataloging all training and inference data sources to identify sensitive information such as personally identifiable information, protected health information, or financial records. For datasets containing sensitive information, differential privacy techniques add calibrated noise during training, with privacy budgets carefully selected based on regulatory requirements—typically ε ≤ 4 for general compliance and ε ≤ 1 for highly sensitive medical or financial data. When multiple organizations need to collaborate without sharing raw data, federated learning enables distributed training with secure aggregation protocols ensuring individual contributions remain private. Data minimization practices complement these technical measures by establishing retention policies that automatically delete sensitive training data after model completion, reducing the window of exposure for potential breaches.\n\nThe third phase shifts focus to protecting the models themselves as valuable intellectual property that may encode sensitive information or competitive advantages. Model encryption using AES-256 or stronger algorithms protects stored models from unauthorized access, with key management systems like AWS KMS or HashiCorp Vault ensuring proper key rotation and access control. For particularly sensitive applications, deployment within trusted execution environments provides hardware-enforced isolation, preventing even privileged system administrators from accessing model internals during inference. Organizations implement model watermarking techniques to embed undetectable ownership markers that survive model extraction attempts, while query monitoring systems detect suspicious patterns that might indicate ongoing theft attempts. Critical applications require adversarial robustness through specialized training techniques, using methods like Projected Gradient Descent to create models that maintain accuracy even when facing malicious inputs designed to cause misclassification.\n\nThe final phase establishes continuous monitoring and response capabilities that maintain security throughout the system's operational lifetime. Real-time anomaly detection systems monitor inference patterns, input distributions, and model confidence scores to identify potential attacks or system degradation. When threats are detected, automated response systems implement immediate countermeasures such as rate limiting suspicious users, isolating potentially compromised models, or escalating alerts to security teams. Performance monitoring ensures security measures don't degrade business operations, tracking metrics like inference latency and model accuracy to maintain the delicate balance between protection and utility. Comprehensive incident response procedures specific to ML systems enable rapid reaction to security events, including capabilities for model rollback to known-good versions, forensic data collection for post-incident analysis, and threat intelligence gathering to prevent future attacks.\n\nSuccess metrics should guide implementation effectiveness: (1) Zero security incidents involving data leakage or model theft; (2) <5% model performance degradation from security measures; (3) 100% compliance with relevant regulations (HIPAA, GDPR, SOX); (4) <100ms additional latency from security overhead.\n\nImplementation prioritization becomes critical for resource-constrained organizations. Start with high-impact, low-effort baseline defenses (TLS, authentication, logging) before progressing to specialized techniques. Focus on regulatory compliance requirements first (HIPAA differential privacy, GDPR data minimization) as these often have legal deadlines. Deploy privacy-preserving techniques based on data sensitivity—use differential privacy for PII/PHI, federated learning for cross-organizational collaboration, and synthetic data for lower-sensitivity use cases.\n\n::: {.callout-warning title=\"Troubleshooting Guide: Common ML Security Issues\" icon=false}\nIssue 1: Model Performance Degradation After Security Implementation\n\n*Symptoms*: Model accuracy drops significantly after applying differential privacy, adversarial training, or encryption.\n\n*Root Causes*:\n\n- Excessive noise injection (ε too small in differential privacy)\n- Insufficient adversarial training budget\n- Inappropriate cryptographic operations breaking model computation\n\n*Diagnostics*:\n```python\n# Check DP noise impact\ndef measure_privacy_utility_tradeoff(model, data, epsilon_values):\n    results = {}\n    for eps in epsilon_values:\n        dp_model = apply_differential_privacy(model, epsilon=eps)\n        accuracy = evaluate_model(dp_model, data)\n        results[eps] = accuracy\n    return results\n\n\n# Test different epsilon values: [0.1, 1.0, 4.0, 8.0]\n# Look for sweet spot between privacy and utility\n```\n\n*Solutions*:\n\n- **DP**: Start with ε = 8, gradually decrease while monitoring accuracy\n- **Adversarial Training**: Use mixed training (70% clean, 30% adversarial examples)\n- **Encryption**: Use approximate methods (quantization-friendly HE) for critical operations only\n\nIssue 2: High Latency from Security Overhead\n\n*Symptoms*: Inference time increases dramatically with TEEs, encryption, or secure protocols.\n\n*Debugging Steps*:\n1. Profile each security component separately\n2. Identify bottlenecks (network, crypto, context switching)\n3. Measure baseline vs. secured performance\n\n*Optimization Strategies*:\n\n- **TEE Optimization**: Batch inference calls, minimize context switches\n- **Encryption**: Use hardware acceleration (AES-NI), optimize key operations\n- **Network Security**: Pipeline TLS handshakes, use connection pooling\n\nIssue 3: Adversarial Attack Detection False Positives\n\n*Symptoms*: Legitimate inputs flagged as adversarial examples, causing service disruption.\n\n*Tuning Approach*:\n```python\n# Adjust detection thresholds\ndef calibrate_adversarial_detector(detector, clean_data, attack_data):\n    # Find threshold balancing false positive rate vs.\n    # detection rate\n    thresholds = np.linspace(0.1, 0.9, 20)\n    for threshold in thresholds:\n        fp_rate = compute_false_positive_rate(\n            detector, clean_data, threshold\n        )\n        detection_rate = compute_detection_rate(\n            detector, attack_data, threshold\n        )\n        print(\n            f\"Threshold {threshold}: FP={fp_rate:.3f}, \"\n            + f\"Detection={detection_rate:.3f}\"\n        )\n```\n\n*Best Practices*:\n\n- Set different thresholds for different input types\n- Implement human-in-the-loop for borderline cases\n- Use ensemble detection methods for robustness\n\nIssue 4: Privacy Budget Exhaustion in Production\n\n*Symptoms*: Cannot retrain models due to depleted privacy budget.\n\n*Management Strategy*:\n\n- Implement privacy budget monitoring and alerting\n- Reserve budget for emergency retraining (20% buffer)\n- Use synthetic data for non-critical model updates\n- Plan budget allocation across model lifecycle\n\nIssue 5: Federated Learning Convergence Problems\n\n*Symptoms*: Models fail to converge or converge slowly in federated settings.\n\n*Diagnostic Checklist*:\n\n- Check data distribution across clients (non-IID data)\n- Verify secure aggregation isn't corrupting gradients\n- Monitor client participation rates and dropouts\n- Analyze communication rounds and bandwidth usage\n\n*Solutions*:\n\n- Implement FedProx or FedAvgM for non-IID data\n- Use client sampling strategies to ensure representative participation\n- Add gradient compression to reduce communication overhead\n- Implement client reliability scoring and failover mechanisms\n::: -->\n\n## Practical Implementation Roadmap {#sec-security-privacy-practical-roadmap-8f3a}\n\nThe comprehensive security and privacy techniques covered in this chapter can seem overwhelming for organizations just beginning to secure their ML systems. Rather than implementing every defense simultaneously, a phased approach enables systematic security improvements while managing complexity and costs. This roadmap provides a practical sequence for building robust ML security, progressing from foundational controls to advanced defenses.\n\n### Phase 1: Foundation Security Controls {#sec-security-privacy-phase1-baseline-foundation-2d9c}\n\nBegin with basic security controls that provide the greatest risk reduction for the least complexity. These foundational measures address the most common attack vectors and create the trust infrastructure needed for more advanced defenses.\n\n- **Access Control and Authentication**: Implement role-based access control (RBAC) for all ML system components, including training data, model repositories, and inference APIs. Use multi-factor authentication for administrative access and service-to-service authentication with short-lived tokens. Establish the principle of least privilege, ensuring users and services have only the minimum permissions required for their functions.\n\n- **Data Protection**: Encrypt all data at rest using AES-256 and enforce TLS 1.3 for all data in transit. This includes training datasets, model files, and inference communications. Implement comprehensive logging of all data access and model operations to support incident investigation and compliance auditing.\n\n- **Input Validation and Basic Monitoring**: Deploy input validation for all ML APIs to reject malformed requests, implement rate limiting to prevent abuse, and establish baseline monitoring for unusual inference patterns. These measures protect against basic adversarial inputs and provide visibility into system behavior.\n\n- **Secure Development Practices**: Establish secure coding practices for ML pipelines, including dependency management with vulnerability scanning, secure model serialization that validates model integrity, and automated security testing in deployment pipelines.\n\n### Phase 2: Privacy Controls and Model Protection {#sec-security-privacy-phase2-privacy-model-protection-7a8b}\n\nWith foundational controls in place, focus on protecting sensitive data and securing your ML models from theft and manipulation. This phase addresses privacy regulations and intellectual property protection.\n\n- **Privacy-Preserving Techniques**: Implement data anonymization for non-sensitive use cases and differential privacy for scenarios requiring formal privacy guarantees. For collaborative learning scenarios, deploy federated learning architectures that keep sensitive data localized while enabling model improvement.\n\n- **Model Security**: Protect deployed models through encryption of model files, secure API design that limits information leakage, and monitoring for model extraction attempts. Implement model versioning and integrity checking to detect unauthorized modifications.\n\n- **Secure Training Infrastructure**: Harden training environments by isolating training workloads, implementing secure data pipelines with validation and provenance tracking, and establishing secure model registries with access controls and audit trails.\n\n- **Compliance Integration**: Implement necessary controls for regulatory requirements such as GDPR, HIPAA, or industry-specific standards. This includes data subject rights management, privacy impact assessments, and documentation of data processing activities.\n\n### Phase 3: Advanced Threat Defense {#sec-security-privacy-phase3-advanced-defenses-runtime-8c2d}\n\nThe final phase implements sophisticated defenses for high-stakes environments where advanced adversaries pose significant threats. These defenses require more expertise and resources but provide protection against state-of-the-art attacks.\n\n- **Adversarial Robustness**: Deploy adversarial training to improve model robustness against evasion attacks, implement certified defenses for safety-critical applications, and establish continuous testing against new adversarial techniques.\n\n- **Advanced Runtime Monitoring**: Deploy ML-specific anomaly detection systems that can identify sophisticated attacks like data poisoning effects or gradual model degradation. Implement behavioral analysis that establishes normal operation baselines and alerts on deviations.\n\n- **Hardware-Based Security**: For the highest security requirements, implement trusted execution environments (TEEs) for model inference, secure boot processes for edge devices, and hardware security modules (HSMs) for cryptographic key management.\n\n- **Incident Response and Recovery**: Establish ML-specific incident response procedures, including model rollback capabilities, contaminated data isolation procedures, and forensic analysis techniques for ML-specific attacks.\n\n### Implementation Considerations {#sec-security-privacy-implementation-considerations-9f4e}\n\nSuccess with this roadmap requires balancing security improvements with operational capabilities. Each phase should be fully implemented and stabilized before progressing to the next level. Organizations should customize this sequence based on their specific threat model: healthcare systems may prioritize Phase 2 privacy controls, financial institutions may emphasize Phase 1 data protection, and autonomous vehicle systems may fast-track to Phase 3 adversarial robustness.\n\nResource allocation should account for the increasing technical complexity and operational overhead of advanced phases. Phase 1 controls typically require standard IT security expertise, while Phase 3 defenses may require specialized ML security knowledge or external consulting. Organizations should invest in training and hiring appropriate expertise before implementing advanced controls.\n\nRegular security assessments should validate the effectiveness of implemented controls and guide progression through phases. These assessments should include penetration testing of ML-specific attack vectors, red team exercises that simulate realistic adversarial scenarios, and compliance audits that verify regulatory requirements are met effectively.\n\n## Fallacies and Pitfalls {#sec-security-privacy-fallacies-pitfalls-0c20}\n\nHaving examined both defensive and offensive capabilities, we now address common misconceptions that can undermine security efforts. Security and privacy in machine learning systems present unique challenges that extend beyond traditional cybersecurity concerns, involving sophisticated attacks on data, models, and inference processes. The complexity of modern ML pipelines, combined with the probabilistic nature of machine learning and the sensitivity of training data, creates numerous opportunities for misconceptions about effective protection strategies.\n\n**Fallacy:** _Security through obscurity provides adequate protection for machine learning models._\n\nThis outdated approach assumes that hiding model architectures, parameters, or implementation details provides meaningful security protection. Modern attacks often succeed without requiring detailed knowledge of target systems, relying instead on black-box techniques that probe system behavior through input-output relationships. Model extraction attacks can reconstruct significant model functionality through carefully designed queries, while adversarial attacks often transfer across different architectures. Effective ML security requires robust defenses that function even when attackers have complete knowledge of the system, following established security principles rather than relying on secrecy.\n\n**Pitfall:** _Assuming that differential privacy automatically ensures privacy without considering implementation details._\n\nMany practitioners treat differential privacy as a universal privacy solution without understanding the critical importance of proper implementation and parameter selection. Poorly configured privacy budgets can provide negligible protection while severely degrading model utility. Implementation vulnerabilities like floating-point precision issues, inadequate noise generation, or privacy budget exhaustion can completely compromise privacy guarantees. Real-world systems require careful analysis of privacy parameters, rigorous implementation validation, and ongoing monitoring to ensure that theoretical privacy guarantees translate to practical protection.\n\n**Fallacy:** _Federated learning inherently provides privacy protection without additional safeguards._\n\nA related privacy misconception assumes that keeping data decentralized automatically ensures privacy protection. While federated learning improves privacy compared to centralized training, gradient and model updates can still leak significant information about local training data through inference attacks. Sophisticated adversaries can reconstruct training examples, infer membership information, or extract sensitive attributes from shared model parameters. True privacy protection in federated settings requires additional mechanisms like secure aggregation, differential privacy, and careful communication protocols rather than relying solely on data locality.\n\n**Pitfall:** _Treating security as an isolated component rather than a system-wide property._\n\nBeyond specific technical misconceptions, a key architectural pitfall involves organizations that approach ML security by adding security features to individual components without considering system-level interactions and attack vectors. This piecemeal approach fails to address sophisticated attacks that span multiple components or exploit interfaces between subsystems. Effective ML security requires holistic threat modeling that considers the entire system lifecycle from data collection through model deployment and maintenance, following the threat prioritization principles established in @sec-security-privacy-threat-prioritization-framework-f2d5. Security must be integrated into every stage of the ML pipeline rather than treated as an add-on feature or post-deployment consideration.\n\n**Pitfall:** _Underestimating the attack surface expansion in distributed ML systems._\n\nMany organizations focus on securing individual components without recognizing how distributed ML architectures increase the attack surface and introduce new vulnerability classes. Distributed training across multiple data centers creates opportunities for man-in-the-middle attacks on gradient exchanges, certificate spoofing, and unauthorized participation in training rounds. Edge deployment multiplies endpoints that require security updates, monitoring, and incident response capabilities. Model serving infrastructure spanning multiple clouds introduces dependency chain attacks, where compromising any component in the distributed system can affect overall security. Orchestration systems, load balancers, model registries, and monitoring infrastructure each present potential entry points for sophisticated attackers. Effective distributed ML security requires thorough threat modeling that accounts for network communication security, endpoint hardening, identity management across multiple domains, and coordination of security policies across heterogeneous infrastructure components.\n\n## Summary {#sec-security-privacy-summary-831c}\n\nThis chapter has explored the complex landscape of security and privacy in machine learning systems, from core threats to comprehensive defense strategies. Security and privacy represent essential requirements for deploying machine learning systems in production environments. As these systems handle sensitive data, operate across diverse platforms, and face sophisticated threats, their security posture must encompass the entire technology stack. Modern machine learning systems encounter attack vectors ranging from data poisoning and model extraction to adversarial examples and hardware-level compromises that can undermine system integrity and user trust.\n\nEffective security strategies employ defense-in-depth approaches that operate across multiple layers of the system architecture. Privacy-preserving techniques like differential privacy and federated learning protect sensitive data while enabling model training. Robust model design incorporates adversarial training and input validation to resist manipulation. Hardware security features provide trusted execution environments and secure boot processes. Runtime monitoring detects anomalous behavior and potential attacks during operation. These complementary defenses create resilient systems that can withstand coordinated attacks across multiple attack surfaces.\n\n:::: {.callout-important title=\"Key Takeaways\"}\n\n* Security and privacy must be integrated from initial architecture design rather than added as afterthoughts to ML systems\n* ML systems face threats across three categories: model confidentiality (theft), training integrity (poisoning), and inference robustness (adversarial attacks)\n* Historical security patterns (supply chain compromise, insufficient isolation, weaponized endpoints) amplify in ML contexts due to autonomous decision-making capabilities\n* Effective defense requires layered protection spanning data privacy, model security, runtime monitoring, and hardware trust anchors\n* Context drives defense selection: healthcare prioritizes regulatory compliance, autonomous vehicles prioritize adversarial robustness, financial systems prioritize model theft prevention\n* Privacy-preserving techniques include differential privacy, federated learning, homomorphic encryption, and synthetic data generation, each with distinct trade-offs\n* Hardware security mechanisms (TEEs, secure boot, HSMs, PUFs) provide foundational trust for software-level protections\n* Security introduces inevitable trade-offs in computational cost, accuracy degradation, and implementation complexity that must be balanced against protection benefits\n\n:::\n\nLooking forward, the security and privacy foundations established in this chapter form critical building blocks for the comprehensive robustness framework explored in @sec-robust-ai. While we've focused on defending against malicious actors and protecting sensitive information, true system reliability requires expanding these concepts to handle all forms of operational stress. The monitoring infrastructure, defensive mechanisms, and layered architectures we've developed here provide the foundation for detecting distribution shifts, managing uncertainty, and ensuring graceful degradation under adverse conditions—topics that will be central to our exploration of robust AI.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"privacy_security.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","privacy_security.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3,"quiz":"privacy_security_quizzes.json","concepts":"privacy_security_concepts.yml","glossary":"privacy_security_glossary.json"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}