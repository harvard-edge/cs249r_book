{"title":"Inference at Scale","markdown":{"yaml":{"title":"Inference at Scale","bibliography":"inference.bib"},"headingText":"<!--","containsRefs":false,"markdown":"\n\nEDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR INFERENCE AT SCALE\n================================================================================\n\nEXPERT FEEDBACK FROM SERVING CHAPTER REVIEW (January 2025):\nThe following topics were identified by experts (Jeff Dean, Ion Stoica, Chip Huyen,\nVijay Reddi, Song Han) as important for distributed inference but appropriately\ndeferred from Vol I Serving chapter to this chapter:\n\nFROM JEFF DEAN:\n\n- Load balancing and request routing (power-of-two-choices, consistent hashing)\n- M/M/c multi-server queue model and Erlang C formula for capacity planning\n- Health checking and service discovery (liveness vs readiness probes)\n- Timeout and deadline propagation across service layers\n- Request coalescing and deduplication\n- Multi-tenancy and isolation (noisy neighbor problems)\n\nFROM ION STOICA:\n\n- Stateless vs stateful serving (critical for scaling decisions)\n- Idempotency requirements for hedged/retry strategies\n- Circuit breakers for cascade failure prevention\n- Bulkhead pattern for failure isolation\n- Backpressure mechanisms beyond admission control\n\nFROM CHIP HUYEN:\n\n- Canary deployments and traffic shifting (1%→5%→25%→100%)\n- Health checks and Kubernetes readiness probes\n- Model versioning and rollback procedures\n- Distributed tracing and observability patterns\n\nFROM SONG HAN:\n\n- Speculative decoding implementation details (draft models, acceptance rates)\n- KV cache memory management and prefix caching\n- Model sharding strategies (tensor parallelism, pipeline parallelism)\n\nFROM VIJAY REDDI:\n\n- MLPerf inference Server scenario implementation at scale\n- Hierarchical edge-cloud serving patterns\n\n================================================================================\n\nCORE PRINCIPLE: Inference workloads vary DRAMATICALLY by model type.\nRecommendation systems dominate production inference volume, not LLMs.\n\nCRITICAL INSIGHT: By request volume, the ML inference landscape is:\n\n- Recommendation/ranking: ~80-90% of inference requests at major tech companies\n- Vision/image processing: ~5-10%\n- NLP/LLM: ~1-5% (but growing rapidly)\n\nMODEL-SPECIFIC INFERENCE CHARACTERISTICS:\n\n| Model Type      | Latency Target | Batching Strategy    | Key Bottleneck       |\n|-----------------|----------------|----------------------|----------------------|\n| Recommendation  | <10ms p99      | Feature-parallel     | Embedding lookup     |\n| Vision (CNN)    | 20-50ms        | Dynamic batching     | Compute-bound        |\n| LLM             | 100ms-seconds  | Continuous batching  | Memory bandwidth     |\n| Speech          | Real-time      | Streaming            | Sequential decode    |\n| Multimodal      | Varies         | Request-level        | Cross-modal sync     |\n\nREQUIRED COVERAGE FOR THIS CHAPTER:\n\nBATCHING STRATEGIES:\n\n- Static batching: Vision models, simpler serving\n- Dynamic batching: Variable request arrival, timeout-based\n- Continuous batching: LLM-specific (Orca paper), KV cache management\n- Feature batching: Recommendation systems, parallel feature lookup\n\nSERVING ARCHITECTURES:\n\n- Single-model serving: Most vision/NLP models\n- Ensemble serving: Recommendation pipelines (multiple models in sequence)\n- Cascade serving: Early-exit, model routing\n- Include: Why RecSys often needs 10+ models per request\n\nMODEL SHARDING FOR INFERENCE:\n\n- Tensor parallelism: LLM serving across GPUs\n- Embedding sharding: Recommendation serving\n- Include: Different sharding strategies for different model types\n\nLOAD BALANCING:\n\n- Request-level: Stateless models (vision, some NLP)\n- Session-level: Stateful models (conversational LLM)\n- Feature-level: Recommendation (route by user/item shards)\n\nCASE STUDIES TO INCLUDE:\n\n- Meta recommendation serving (billions of requests/day)\n- Netflix ranking system architecture\n- OpenAI API serving (LLM-specific challenges)\n- Google Search ranking (ensemble of models)\n- TikTok video recommendation (multimodal)\n\nLATENCY ANALYSIS DIVERSITY:\n\n- Include p50/p99/p999 for different model types\n- Show where latency budget goes (network, compute, memory)\n- Compare: RecSys (feature lookup dominates) vs LLM (decode dominates)\n\nANTI-PATTERNS TO AVOID:\n\n- Treating inference as synonymous with \"LLM serving\"\n- Ignoring embedding lookup latency (critical for RecSys)\n- Only discussing KV cache (LLM-specific optimization)\n- Forgetting that most production ML is NOT generative\n\n================================================================================\n-->\n\n# Inference at Scale {#sec-inference-at-scale}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A dynamic illustration of a global inference serving system handling millions of requests. The scene shows a central model representation being replicated across multiple serving nodes arranged in a globe-spanning network. Incoming request streams appear as flowing arrows from users worldwide, processed through load balancers depicted as traffic controllers, then distributed to model replicas. Visual elements include batching queues shown as organized request groups, latency meters displaying millisecond readings, and auto-scaling indicators showing nodes spawning and despawning. The composition emphasizes real-time responsiveness with clock symbols and speed indicators. Color palette features cool silvers and whites for infrastructure, warm oranges for active requests, and green for successful responses. Technical yet accessible style suitable for a production systems textbook._\n:::\n\n\\noindent\n![](images/png/cover_inference_at_scale.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why does serving machine learning predictions at scale require fundamentally different engineering approaches than training the models that generate them?_\n\nTraining optimizes for throughput over extended periods, tolerating latency variations that would be unacceptable when users await responses in real time. Inference at scale inverts these priorities: serving systems must deliver predictions within strict latency bounds while handling request volumes that fluctuate unpredictably across hours, days, and seasons. The engineering challenges of inference extend beyond raw performance to encompass resource efficiency when serving costs dwarf training costs over a model's lifetime, availability when users expect continuous service regardless of infrastructure conditions, and consistency when the same input must produce the same output across globally distributed serving infrastructure. Understanding how serving systems, batching strategies, model sharding, and load balancing address these challenges determines whether sophisticated models deliver value to users or remain impressive but impractical demonstrations. Mastering inference at scale transforms model capabilities into reliable services that operate continuously at the speed and scale users demand.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain why serving cost dominates training cost over a model's lifetime and apply the total cost of serving equation to infrastructure planning decisions\n\n- Compare and contrast batching strategies across model types (static for vision, continuous for LLMs, feature-parallel for recommendation systems) and select appropriate strategies based on workload characteristics\n\n- Apply the serving hierarchy framework (request, replica, service, platform) to decompose inference optimization problems and identify bottlenecks at each level\n\n- Design model sharding strategies for inference, distinguishing tensor parallelism, pipeline parallelism, expert parallelism, and embedding sharding by their communication patterns and use cases\n\n- Analyze load balancing algorithms using queuing theory, demonstrating why power-of-two-choices achieves exponentially better load distribution than random assignment\n\n- Implement KV cache management techniques including PagedAttention, prefix caching, and speculative decoding to optimize LLM serving throughput and memory efficiency\n\n- Evaluate autoscaling strategies by quantifying cold start latency components and designing predictive scaling policies that minimize both cost and SLO violations\n\n:::\n\n## Scaling Inference Beyond Single Machines {#sec-inference-scaling-beyond}\n\nVolume I established the foundations of model serving on a single machine: the inversion from training's throughput focus to serving's latency imperative, queuing theory fundamentals that govern system behavior under load, and the latency budget framework that reveals where time actually goes within a request. Those foundations assume a single server handling requests, which suffices for many deployments. This chapter addresses what happens when single-machine serving proves insufficient and examines the engineering principles that govern inference at scale across multiple machines.\n\nThe transition from single-machine to distributed inference parallels the transition from single-machine to distributed training, but with fundamentally different constraints. Training optimizes throughput over extended periods and tolerates latency variations measured in minutes or hours. Inference at scale must maintain strict latency bounds measured in milliseconds while handling request volumes that fluctuate unpredictably. This inversion of priorities transforms every design decision, from how requests are batched to how failures are handled.\n\nDistributed inference systems must solve problems that simply do not exist at single-machine scale. Load balancing becomes critical when requests must be distributed across hundreds of GPU instances while maintaining latency guarantees. Request routing must account for model-specific characteristics: a recommendation system with trillion-parameter embedding tables requires different placement strategies than a large language model that generates responses token by token. Autoscaling must anticipate demand fluctuations that can change request volume by orders of magnitude within minutes, all while maintaining latency bounds that users expect.\n\nThe economics of inference at scale also differ fundamentally from training economics. Training costs are dominated by compute time and can be amortized over the lifetime of the resulting model. Inference costs, by contrast, are directly tied to user traffic and revenue. An e-commerce recommendation system might serve millions of requests per second during peak shopping periods, with each request contributing directly to potential revenue. The cost of overprovisioning during quiet periods or underprovisioning during peaks translates immediately to business impact, making inference efficiency a first-order concern in ways that training efficiency rarely achieves.\n\nThis chapter develops the principles and techniques for building inference systems that scale to meet these demands. We examine when distributed inference becomes necessary, how to architect systems that maintain latency bounds under varying load, and how to optimize the economics of inference through efficient resource utilization. The goal is not merely to make inference work at scale, but to make it work efficiently, reliably, and economically.\n\n### When Single-Machine Serving Is Insufficient {#sec-inference-when-insufficient}\n\nThree distinct signals indicate when distributed inference becomes necessary rather than merely optional. Understanding these thresholds guides infrastructure decisions and prevents both premature complexity and delayed scaling.\n\n**Memory exhaustion** occurs when model parameters, key-value caches, or embedding tables exceed single-device capacity. A single NVIDIA H100 GPU provides 80GB of HBM3 memory. GPT-4 class models with hundreds of billions of parameters require 200-400GB just for weights in FP16 precision, forcing distribution across multiple GPUs regardless of throughput requirements. Recommendation systems with trillion-parameter embedding tables face similar constraints: Meta's DLRM model stores embedding tables that require multiple terabytes of memory.\n\n**Throughput limitations** emerge when request volume exceeds single-machine capacity even with optimal batching. Consider a recommendation system serving 100,000 queries per second with a 10ms latency budget. If single-machine throughput peaks at 10,000 QPS, no amount of optimization on that machine can satisfy demand. Horizontal scaling across multiple replicas becomes mandatory.\n\n**Latency requirements** drive distribution when model execution time exceeds latency budgets even at batch size one. Large language models generating responses token by token face this constraint acutely: a 70-billion parameter model requires approximately 140GB of memory and achieves roughly 30 tokens per second on a single GPU due to memory bandwidth limitations. Sharding the model across multiple GPUs enables parallel computation that reduces time-to-first-token below acceptable thresholds.\n\n| Constraint | Single-Machine Limit | Example Workload | Distribution Strategy |\n|------------|---------------------|------------------|----------------------|\n| Memory | 80GB (H100) | GPT-4 (400GB+) | Tensor/pipeline parallelism |\n| Throughput | ~10K QPS (vision) | 100K QPS RecSys | Horizontal replication |\n| Latency | Model execution time | 500ms LLM TTFT | Model sharding |\n\n: **Triggers for Distributed Inference**: Each constraint type indicates different distribution strategies. Memory constraints require model sharding; throughput constraints require replication; latency constraints may require either depending on whether the bottleneck is compute or memory bandwidth. {#tbl-distribution-triggers}\n\n### The Fundamental Inversion: Training vs Inference {#sec-inference-inversion}\n\nThe contrast between training and inference optimization extends beyond the throughput-latency distinction introduced in @sec-serving. At scale, this inversion manifests in system architecture, resource allocation, and operational priorities.\n\n| Aspect | Distributed Training | Distributed Inference |\n|--------|---------------------|----------------------|\n| Primary metric | Throughput (samples/hour) | Latency (P99 ms) |\n| Acceptable variance | Hours | Milliseconds |\n| State management | Checkpoints (periodic) | Session state (continuous) |\n| Batch formation | Large, controlled | Request-driven, variable |\n| Failure tolerance | Restart from checkpoint | Redirect without user impact |\n| Cost structure | Fixed duration, variable rate | Variable duration, fixed SLO |\n\n: **Training vs Inference System Requirements**: The fundamental inversion from throughput to latency optimization ripples through every aspect of system design. {#tbl-training-inference-inversion}\n\nTraining tolerates substantial latency variance because the optimization target is aggregate progress over hours or days. A training iteration that takes 2 seconds instead of the usual 1 second represents acceptable variation. An inference request that takes 2 seconds instead of 100 milliseconds represents catastrophic failure, potentially causing user abandonment or cascading timeouts in dependent services.\n\nState management differs fundamentally. Training maintains model state (parameters, optimizer states) that evolves gradually and can be captured in periodic checkpoints. Inference often maintains session state (conversation history, key-value caches, user context) that must be preserved across requests and cannot tolerate the staleness that checkpoint-based recovery would introduce.\n\nFailure handling diverges correspondingly. Training failures trigger checkpoint restoration and continuation, with minutes of lost progress being acceptable. Inference failures must be invisible to users: requests redirect to healthy replicas, degraded results substitute for unavailable models, and SLOs must be maintained despite infrastructure instability.\n\n### The Serving Tax: Overhead of Distribution {#sec-inference-serving-tax}\n\nDistributing inference across multiple machines introduces overhead absent from single-machine serving. This \"serving tax\" must be understood and budgeted within latency constraints.\n\n**Network communication** adds latency for every cross-machine interaction. Within a datacenter, network round-trip times range from 50-500 microseconds depending on topology and congestion. For model sharding that requires synchronization between GPUs on different machines, each synchronization point adds this overhead. A model sharded across 8 machines with 4 synchronization points per inference adds 200 microseconds to 2 milliseconds of network latency.\n\n**Serialization overhead** converts in-memory tensors to network-transmittable formats. While modern serialization libraries like FlatBuffers and Cap'n Proto minimize this overhead, large activation tensors still require meaningful time to serialize and deserialize. A 1GB activation tensor takes approximately 100 milliseconds to serialize, even with optimized libraries.\n\n**Load balancer latency** adds another layer. Requests must be routed to appropriate replicas, which requires examining request metadata, consulting routing tables, and forwarding to selected backends. Well-optimized load balancers add 100-500 microseconds; poorly configured ones can add milliseconds.\n\n**Coordination overhead** emerges when requests require fan-out to multiple services. A recommendation system that queries a user model, item model, and ranking model in parallel must coordinate these queries and aggregate results. The coordination logic itself consumes CPU cycles and introduces latency variation.\n\nThe total serving tax often consumes 10-30% of the latency budget in distributed systems:\n\n$$L_{total} = L_{compute} + L_{network} + L_{serialization} + L_{coordination} + L_{queuing}$$ {#eq-serving-tax}\n\nMinimizing this tax requires co-locating communicating components, using high-bandwidth interconnects, and designing communication patterns that minimize round trips.\n\n### Serving Cost Dominates Training Cost {#sec-inference-cost-dominance}\n\nA critical insight for infrastructure planning is that serving cost typically dominates training cost over a model's operational lifetime. This reversal from the training-centric view of model development has profound implications for where optimization effort should focus.\n\nThe total cost of operating a model comprises training cost (a one-time expense) and serving cost (an ongoing expense):\n\n$$C_{total} = C_{training} + C_{serving} \\times T_{deployment} \\times Q_{rate}$$ {#eq-total-cost}\n\nwhere $C_{training}$ is the one-time cost to train the model, $C_{serving}$ is the cost per query served, $T_{deployment}$ is the deployment duration in appropriate time units, and $Q_{rate}$ is the query rate.\n\n::: {.callout-note title=\"Worked Example: Cost Dominance Analysis\"}\n\nConsider a recommendation model with the following characteristics:\n\n**Training costs**:\n\n- 1,000 GPU-hours on H100 GPUs at $3/GPU-hour = $3,000\n- Data preparation and experimentation overhead (3x): $9,000\n- **Total training cost**: $12,000\n\n**Serving costs**:\n\n- Deployment duration: 2 years\n- Query rate: 10,000 QPS average\n- Cost per query: $0.00001 (on optimized infrastructure)\n\n**Total queries over lifetime**:\n$$Q_{total} = 10,000 \\text{ QPS} \\times 86,400 \\text{ s/day} \\times 730 \\text{ days} = 631 \\text{ billion queries}$$\n\n**Total serving cost**:\n$$C_{serving} = 631 \\times 10^9 \\times \\$0.00001 = \\$6,310,000$$\n\n**Ratio**: Serving cost is **526x** the training cost.\n\nEven reducing serving cost by 10% saves $631,000, far exceeding the entire training budget. This analysis explains why production ML teams often dedicate more engineering resources to serving optimization than training optimization.\n\n:::\n\nThe cost dominance ratio varies by application:\n\n| Application | Training Cost | Annual Serving Cost | Ratio |\n|-------------|--------------|-------------------|-------|\n| Recommendation (high QPS) | $10K-100K | $1M-10M | 100-1000x |\n| Search ranking | $100K-1M | $10M-100M | 100-1000x |\n| LLM API | $1M-100M | $10M-1B | 10-100x |\n| Internal analytics | $1K-10K | $10K-100K | 10-100x |\n\n: **Training vs Serving Cost Ratios**: High-QPS applications like recommendation systems show the most extreme cost dominance of serving over training. {#tbl-cost-ratios}\n\nThis cost structure motivates the optimization techniques throughout this chapter. Every percentage point of serving efficiency improvement yields ongoing cost reduction over the model's operational lifetime.\n\n### The Inference Landscape: Beyond LLMs {#sec-inference-landscape}\n\nA critical misconception in current discourse frames inference at scale as synonymous with LLM serving. While large language models present distinctive challenges and attract significant attention, they represent a small fraction of production inference volume. Understanding the full inference landscape is essential for appropriate technique selection.\n\n::: {.callout-important title=\"Production Inference by Request Volume\"}\n\nBy request count, production ML inference at major technology companies breaks down approximately as:\n\n- **Recommendation and ranking**: 80-90% of requests\n- **Vision and image processing**: 5-10% of requests\n- **NLP/LLM**: 1-5% of requests (but growing rapidly)\n- **Other (fraud detection, ads, etc.)**: 2-5% of requests\n\nSource: Industry reports from Meta, Google, and Netflix infrastructure teams.\n\n:::\n\nRecommendation systems dominate because they serve predictions for every user interaction: every page load, scroll, or click triggers inference. A user browsing an e-commerce site might generate 100 recommendation requests in a single session. In contrast, LLM queries typically require explicit user action and occur less frequently.\n\nThis distribution has important implications. Recommendation systems have driven most production inference innovation: dynamic batching, embedding sharding, feature store architectures, and low-latency serving were developed primarily for recommendation workloads. LLM-specific techniques like continuous batching and KV cache management are important but address a narrower slice of production inference.\n\n| Model Type | Request Volume | Latency Target | Key Challenge |\n|------------|---------------|----------------|---------------|\n| Recommendation | Very high (80-90%) | <10ms P99 | Embedding lookup |\n| Vision (CNN) | Moderate (5-10%) | 20-100ms | Batch efficiency |\n| LLM | Lower (1-5%) | 100ms-10s | Memory bandwidth |\n| Speech/Audio | Lower | Real-time | Sequential decode |\n| Multimodal | Growing | Varies | Cross-modal sync |\n\n: **Production Inference Landscape**: Different model types have different volume, latency requirements, and optimization challenges. Technique selection must match the specific workload. {#tbl-inference-landscape}\n\n### The Serving Hierarchy {#sec-inference-serving-hierarchy}\n\nTo organize the optimization techniques in this chapter, we introduce the serving hierarchy as a conceptual framework. Like the memory hierarchy in computer architecture, the serving hierarchy identifies distinct levels at which optimization occurs, each with different targets and techniques.\n\n**Request level**: Optimizations that affect individual request processing. Batching strategies, caching, and preprocessing optimizations operate at this level. The target metric is per-request latency.\n\n**Replica level**: Optimizations within a single model instance. GPU utilization, memory management, and model optimization operate here. The target metric is single-replica throughput.\n\n**Service level**: Optimizations across multiple replicas of the same model. Load balancing, request routing, and replica management operate at this level. The target metric is aggregate service throughput while meeting latency SLOs.\n\n**Platform level**: Optimizations across multiple services and tenants. Resource allocation, multi-tenancy, scheduling, and cluster management operate here. The target metric is overall resource efficiency while meeting diverse SLO requirements.\n\n```\nPlatform Level    [Multi-tenancy, Scheduling, Resource Allocation]\n      │\n      ▼\nService Level     [Load Balancing, Routing, Autoscaling]\n      │\n      ▼\nReplica Level     [GPU Utilization, Memory Management]\n      │\n      ▼\nRequest Level     [Batching, Caching, Preprocessing]\n```\n\nEach level has distinct optimization levers:\n\n| Level | Optimization Target | Key Techniques |\n|-------|-------------------|----------------|\n| Request | Per-request latency | Dynamic batching, caching, prefetching |\n| Replica | Throughput, utilization | Memory optimization, kernel fusion |\n| Service | Aggregate capacity | Load balancing, routing, autoscaling |\n| Platform | Resource efficiency | Multi-tenancy, scheduling, placement |\n\n: **Serving Hierarchy Optimization Targets**: Each level of the hierarchy addresses different metrics with different techniques. {#tbl-serving-hierarchy}\n\nThe remainder of this chapter progresses through these levels: batching and caching (request level), model sharding (replica level), load balancing and autoscaling (service level), and multi-tenancy (platform level).\n\n### Chapter Roadmap {#sec-inference-roadmap}\n\nThis chapter develops the techniques for inference at scale through the lens of the serving hierarchy:\n\n**Batching Strategies at Scale** (@sec-inference-batching) examines how different model types require fundamentally different batching approaches. We contrast static batching for vision models, continuous batching for LLMs, and feature-parallel batching for recommendation systems, providing quantitative analysis of throughput-latency tradeoffs.\n\n**Model Sharding for Inference** (@sec-inference-sharding) addresses when and how to distribute model computation across multiple devices. We examine tensor parallelism, pipeline parallelism, expert parallelism, and embedding sharding, with emphasis on communication patterns and overhead.\n\n**Load Balancing and Request Routing** (@sec-inference-load-balancing) develops the theory and practice of distributing requests across replicas. We derive why power-of-two-choices achieves exponentially better load distribution than random assignment and examine routing strategies for stateful workloads.\n\n**KV Cache Management** (@sec-inference-kv-cache) focuses on the memory management challenges specific to autoregressive language models, including PagedAttention, prefix caching, and speculative decoding.\n\n**Multi-Tenancy and Isolation** (@sec-inference-multitenancy) examines platform-level concerns: sharing infrastructure across multiple models and users while maintaining isolation and fairness.\n\n**Autoscaling** (@sec-inference-autoscaling) addresses dynamic capacity management, including the cold start problem unique to GPU-based serving and predictive scaling strategies.\n\n**Case Studies** (@sec-inference-case-studies) grounds these principles in production systems at Meta, OpenAI, Google, and TikTok, demonstrating how the techniques combine in real deployments.\n\nThroughout, we maintain the model-type diversity essential for practitioners: every major concept is illustrated across LLMs, recommendation systems, vision models, and other production workloads.\n\n## Serving Framework Selection {#sec-inference-frameworks}\n\nBefore examining specific serving techniques, practitioners must select appropriate serving infrastructure. The choice of serving framework determines which optimizations are available, how models are deployed, and what performance characteristics are achievable. This section provides a systematic framework for selecting among the major options.\n\n### Framework Categories {#sec-inference-framework-categories}\n\nServing frameworks fall into distinct categories based on their design philosophy and target workloads:\n\n**General-purpose inference servers** provide broad model support with configurable optimization:\n\n- **Triton Inference Server** (NVIDIA): Multi-framework support (PyTorch, TensorFlow, ONNX, TensorRT), dynamic batching, model ensemble orchestration, concurrent model execution\n- **TensorFlow Serving**: Native TensorFlow support, gRPC/REST APIs, model versioning, batching scheduler\n- **TorchServe** (PyTorch): Native PyTorch support, model archiving, metrics, multi-model serving\n\n**LLM-specialized servers** optimize specifically for autoregressive generation:\n\n- **vLLM**: PagedAttention, continuous batching, tensor parallelism, OpenAI-compatible API\n- **TensorRT-LLM**: NVIDIA-optimized kernels, in-flight batching, quantization, multi-GPU support\n- **Text Generation Inference (TGI)**: Hugging Face integration, flash attention, tensor parallelism, watermarking\n\n**Optimization-focused runtimes** maximize inference speed through compilation:\n\n- **TensorRT**: Graph optimization, kernel fusion, precision calibration, NVIDIA GPU specific\n- **ONNX Runtime**: Cross-platform optimization, execution providers for different hardware\n- **OpenVINO**: Intel hardware optimization, model compression, heterogeneous execution\n\n### Framework Selection Criteria {#sec-inference-framework-criteria}\n\nSelection depends on model type, deployment constraints, and organizational factors:\n\n**Model architecture determines primary candidates**:\n\n| Model Type | Primary Options | Key Consideration |\n|------------|-----------------|-------------------|\n| LLM (>7B params) | vLLM, TensorRT-LLM, TGI | KV cache management, continuous batching |\n| LLM (<7B params) | vLLM, TGI, Triton | Simpler deployment, less memory pressure |\n| Vision (CNN/ViT) | Triton, TensorRT, ONNX RT | Static batching, throughput optimization |\n| Recommendation | Triton, custom | Feature preprocessing, embedding lookup |\n| Multi-modal | Triton, custom | Pipeline orchestration |\n\n**Hardware constraints narrow options**:\n\n| Hardware | Supported Frameworks |\n|----------|---------------------|\n| NVIDIA datacenter GPU | All options |\n| NVIDIA consumer GPU | vLLM, TGI (limited TensorRT-LLM) |\n| AMD GPU | vLLM (ROCm), ONNX RT |\n| Intel CPU/GPU | OpenVINO, ONNX RT |\n| Apple Silicon | MLX, Core ML, ONNX RT |\n| AWS Inferentia | Neuron SDK |\n\n**Operational requirements influence choice**:\n\n- **Multi-model serving**: Triton excels with concurrent model execution\n- **Rapid iteration**: TorchServe, TGI offer simpler deployment cycles\n- **Maximum throughput**: TensorRT-LLM, vLLM with optimized kernels\n- **Cross-platform**: ONNX Runtime provides broadest hardware support\n\n### vLLM Architecture {#sec-inference-vllm}\n\nvLLM has emerged as the leading open-source LLM serving framework due to its PagedAttention innovation. Understanding its architecture illustrates key LLM serving principles.\n\n**Core innovations**:\n\n1. **PagedAttention**: Virtual memory for KV cache (covered in @sec-inference-paged-attention)\n2. **Continuous batching**: Add/remove requests mid-generation\n3. **Optimized attention kernels**: Flash attention integration\n4. **Tensor parallelism**: Automatic model sharding across GPUs\n\n**Architecture overview**:\n\n```\n┌─────────────────────────────────────────────────────┐\n│                   vLLM Engine                        │\n├─────────────────────────────────────────────────────┤\n│  Scheduler          │  Block Manager                │\n│  - Request queue    │  - Physical blocks            │\n│  - Preemption       │  - Block tables               │\n│  - Priority         │  - Copy-on-write              │\n├─────────────────────┼───────────────────────────────┤\n│  Model Executor     │  Cache Engine                 │\n│  - Attention        │  - GPU cache                  │\n│  - Sampling         │  - CPU swap space             │\n│  - Tensor parallel  │  - Prefix caching             │\n└─────────────────────┴───────────────────────────────┘\n```\n\n**Deployment example**:\n\n```python\nfrom vllm import LLM, SamplingParams\n\n# Initialize with automatic GPU detection\nllm = LLM(\n    model=\"meta-llama/Llama-2-70b-hf\",\n    tensor_parallel_size=4,  # Shard across 4 GPUs\n    gpu_memory_utilization=0.9,\n    max_model_len=4096,\n)\n\n# Efficient batch inference\nprompts = [\"Explain quantum computing\", \"Write a poem about AI\"]\nsampling_params = SamplingParams(temperature=0.7, max_tokens=256)\noutputs = llm.generate(prompts, sampling_params)\n```\n\n**Performance characteristics**:\n\n| Metric | Typical Value | Notes |\n|--------|---------------|-------|\n| Throughput vs baseline | 2-4x | Compared to naive HF generation |\n| Memory efficiency | 90%+ utilization | PagedAttention eliminates fragmentation |\n| Latency overhead | <5ms | Scheduling and batching overhead |\n| Max concurrent requests | 100s-1000s | Depends on model size and GPU memory |\n\n### TensorRT-LLM Architecture {#sec-inference-tensorrt-llm}\n\nTensorRT-LLM provides NVIDIA-optimized LLM inference with deep hardware integration.\n\n**Core capabilities**:\n\n1. **Optimized kernels**: Custom CUDA kernels for attention, GEMM, and layer norms\n2. **In-flight batching**: NVIDIA's continuous batching implementation\n3. **Quantization**: INT8, INT4, FP8 with minimal accuracy loss\n4. **Multi-GPU**: Tensor and pipeline parallelism with NVLink optimization\n\n**Build and deployment workflow**:\n\n```bash\n# Step 1: Convert model to TensorRT-LLM format\npython convert_checkpoint.py \\\n    --model_dir /models/llama-70b \\\n    --output_dir /models/llama-70b-trt \\\n    --dtype float16 \\\n    --tp_size 4\n\n# Step 2: Build optimized engine\ntrtllm-build \\\n    --checkpoint_dir /models/llama-70b-trt \\\n    --output_dir /engines/llama-70b \\\n    --gemm_plugin float16 \\\n    --max_batch_size 64 \\\n    --max_input_len 2048 \\\n    --max_output_len 512\n\n# Step 3: Deploy with Triton\n# (Configuration in model_repository/)\n```\n\n**Performance comparison with vLLM**:\n\n| Scenario | TensorRT-LLM | vLLM | Winner |\n|----------|--------------|------|--------|\n| A100 throughput | Higher | Good | TensorRT-LLM |\n| H100 throughput | Highest | High | TensorRT-LLM |\n| Deployment simplicity | Complex | Simple | vLLM |\n| Model support | NVIDIA curated | Broad HF | vLLM |\n| Quantization options | Extensive | Good | TensorRT-LLM |\n\nTensorRT-LLM typically achieves 20-50% higher throughput than vLLM on NVIDIA hardware but requires more complex deployment pipelines.\n\n### Triton Inference Server {#sec-inference-triton}\n\nTriton provides enterprise-grade multi-model serving with sophisticated orchestration capabilities.\n\n**Key features for production**:\n\n1. **Multi-framework**: Single server hosts PyTorch, TensorFlow, TensorRT, ONNX models\n2. **Dynamic batching**: Configurable batching with latency targets\n3. **Model ensembles**: Chain models in inference pipelines\n4. **Concurrent execution**: Multiple models share GPU resources\n5. **Metrics and monitoring**: Prometheus integration, detailed latency breakdown\n\n**Model repository structure**:\n\n```\nmodel_repository/\n├── text_encoder/\n│   ├── config.pbtxt\n│   └── 1/\n│       └── model.onnx\n├── image_classifier/\n│   ├── config.pbtxt\n│   └── 1/\n│       └── model.plan  # TensorRT engine\n└── ensemble_pipeline/\n    ├── config.pbtxt    # Orchestrates above models\n    └── 1/\n```\n\n**Dynamic batching configuration**:\n\n```protobuf\n# config.pbtxt\ndynamic_batching {\n    preferred_batch_size: [4, 8, 16, 32]\n    max_queue_delay_microseconds: 100000  # 100ms max wait\n}\ninstance_group [\n    {\n        count: 2\n        kind: KIND_GPU\n        gpus: [0, 1]\n    }\n]\n```\n\n**Use cases where Triton excels**:\n\n- Multi-model pipelines (e.g., detection → classification → ranking)\n- Mixed workloads on shared GPU clusters\n- Organizations with diverse model frameworks\n- Production systems requiring detailed observability\n\n### Framework Selection Decision Tree {#sec-inference-framework-decision}\n\n```\nStart\n  │\n  ├─ Is this an LLM (autoregressive generation)?\n  │   ├─ Yes → Is maximum throughput critical?\n  │   │         ├─ Yes, NVIDIA hardware → TensorRT-LLM\n  │   │         └─ No, or mixed hardware → vLLM\n  │   │\n  │   └─ No → Is this multi-model serving?\n  │           ├─ Yes → Triton Inference Server\n  │           └─ No → What's the deployment target?\n  │                   ├─ NVIDIA GPU → TensorRT + Triton\n  │                   ├─ Intel → OpenVINO\n  │                   ├─ Cross-platform → ONNX Runtime\n  │                   └─ Edge/Mobile → Platform-specific (Core ML, TFLite)\n```\n\n**Common deployment patterns**:\n\n| Pattern | Frameworks | Use Case |\n|---------|-----------|----------|\n| LLM API service | vLLM + nginx | ChatGPT-like applications |\n| High-throughput LLM | TensorRT-LLM + Triton | Batch processing, enterprise |\n| Vision pipeline | TensorRT + Triton | Object detection, classification |\n| Recommendation | Triton + custom embedding | E-commerce, content platforms |\n| Multi-modal | Triton ensemble | Vision-language, document understanding |\n\n### Framework Performance Benchmarking {#sec-inference-framework-benchmarks}\n\nWhen evaluating frameworks, benchmark on representative workloads:\n\n**LLM benchmark methodology**:\n\n```python\n# Standard benchmark parameters\nbenchmark_config = {\n    \"input_lengths\": [128, 512, 2048],\n    \"output_lengths\": [64, 256, 512],\n    \"batch_sizes\": [1, 8, 32, 64],\n    \"concurrent_requests\": [1, 10, 50, 100],\n    \"metrics\": [\"ttft\", \"tpot\", \"throughput\", \"gpu_util\"],\n}\n\n# Time to First Token (TTFT): Latency until first token generated\n# Time Per Output Token (TPOT): Average latency per subsequent token\n# Throughput: Total tokens/second across all requests\n# GPU utilization: Compute and memory utilization\n```\n\n**Representative benchmark results** (Llama-2-70B on 4xA100-80GB):\n\n| Framework | TTFT (ms) | TPOT (ms) | Throughput (tok/s) |\n|-----------|-----------|-----------|-------------------|\n| TensorRT-LLM | 180 | 28 | 2,400 |\n| vLLM | 220 | 32 | 1,900 |\n| TGI | 250 | 35 | 1,700 |\n| HF Transformers | 400 | 85 | 600 |\n\nNote: Results vary significantly with configuration, input/output lengths, and batch sizes. Always benchmark on your specific workload.\n\n### Migration and Integration Considerations {#sec-inference-framework-migration}\n\n**Migrating between frameworks**:\n\n- **Model compatibility**: Most frameworks support standard formats (HF, ONNX)\n- **API differences**: vLLM uses OpenAI-compatible API; Triton uses gRPC/HTTP\n- **Configuration translation**: Batching, parallelism settings differ by framework\n\n**Integration with ML infrastructure**:\n\n| Component | Integration Pattern |\n|-----------|-------------------|\n| Model registry | Pull models on startup, version management |\n| Feature store | Triton ensemble preprocessing, custom backends |\n| Monitoring | Prometheus metrics, distributed tracing |\n| Load balancer | Health checks, request routing |\n| Autoscaler | Custom metrics (queue depth, GPU utilization) |\n\nThe framework selection made here influences all subsequent serving optimizations. The techniques in following sections (batching, sharding, caching) are implemented differently across frameworks but follow the same underlying principles.\n\n### Orchestration Platforms for Production Serving {#sec-inference-orchestration}\n\nIndividual inference runtimes (vLLM, TensorRT-LLM, Triton) handle the mechanics of efficient inference on a single node or small cluster. Production systems require an **orchestration layer** that manages service composition, autoscaling, traffic management, fault tolerance, and multi-tenancy. Choosing an inference runtime without considering the orchestration layer is like choosing a database engine without considering connection pooling, replication, and query routing.\n\n**Key orchestration responsibilities**:\n\n1. **Service composition**: Combining multiple models and components into request pipelines\n2. **Autoscaling**: Scaling replicas based on traffic patterns and resource utilization\n3. **Traffic management**: Load balancing, canary deployments, A/B testing\n4. **Fault tolerance**: Replica health monitoring, automatic recovery\n5. **Multi-tenancy**: Isolating workloads and managing resource allocation across teams\n\n**Major orchestration platforms**:\n\n| Platform | Key Capability | Production Users |\n|----------|----------------|------------------|\n| **Ray Serve** | Scalable Python-native serving, composable deployments | OpenAI, Uber, Instacart |\n| **KServe** | Kubernetes-native serving, serverless inference | Bloomberg, Zillow, enterprises |\n| **BentoML** | ML model packaging and unified serving API | Various production deployments |\n| **Seldon Core** | Kubernetes deployment, A/B testing, canary releases | Financial services, retail |\n\n**Ray Serve architecture**:\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    Ray Serve Controller                      │\n├─────────────────────────────────────────────────────────────┤\n│  HTTP Proxy         │  Autoscaler           │  Router        │\n│  - Request routing  │  - Replica management │  - Load balance│\n│  - Request batching │  - Scale up/down      │  - Affinity    │\n├─────────────────────┼───────────────────────┴────────────────┤\n│                    Ray Actor Pool                            │\n│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐        │\n│  │ Replica 1│ │ Replica 2│ │ Replica 3│ │ Replica N│        │\n│  │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │        │\n│  └──────────┘ └──────────┘ └──────────┘ └──────────┘        │\n└─────────────────────────────────────────────────────────────┘\n```\n\n**Production-ready vLLM deployment with Ray Serve**:\n\n```python\nfrom ray import serve\nfrom vllm import LLM, SamplingParams\n\n\n@serve.deployment(\n    num_replicas=4,\n    ray_actor_options={\"num_gpus\": 4},\n    autoscaling_config={\n        \"min_replicas\": 2,\n        \"max_replicas\": 16,\n        \"target_num_ongoing_requests_per_replica\": 10,\n    },\n)\nclass LLMDeployment:\n    def __init__(self):\n        self.llm = LLM(\n            model=\"meta-llama/Llama-2-70b-hf\", tensor_parallel_size=4\n        )\n        self.params = SamplingParams(temperature=0.7, max_tokens=256)\n\n    async def __call__(self, request):\n        return self.llm.generate([request.prompt], self.params)[0]\n\n\n# Deploy with automatic scaling\napp = LLMDeployment.bind()\nserve.run(app)\n```\n\nThis pattern provides automatic scaling from 2 to 16 replicas based on load, request batching at the serve layer, fault tolerance with automatic replica restart, and zero-downtime deployments.\n\n**Stateless vs stateful serving**: The orchestration layer must account for whether inference is stateless or stateful.\n\n| Serving Type | State Location | Scaling Model | Failure Recovery |\n|--------------|---------------|---------------|------------------|\n| **Stateless** | None or external | Horizontal, trivial | Redirect to any replica |\n| **Stateful** | In-process (KV cache) | Complex, sticky | Session loss or migration |\n\nVision models and embedding lookups are typically stateless: any replica can serve any request. LLM serving with KV cache is stateful: the cache accumulated during conversation creates replica-specific state.\n\n**Implications of stateful LLM serving**:\n\n- **Sticky routing required**: Subsequent requests in a conversation must reach the same replica holding the KV cache\n- **Failure recovery is complex**: When a stateful replica fails, cached state is lost. Options include regenerating cache from history (high latency), replicating cache to backups (high bandwidth), or accepting session restart (poor experience)\n- **Autoscaling must account for sessions**: Scaling down stateful replicas requires draining active sessions, which can take minutes for long conversations\n- **Memory sizing determines capacity**: Each active session consumes KV cache memory, limiting concurrent sessions per replica regardless of compute capacity\n\n**Consistency during model updates**:\n\nWhen updating models across distributed replicas, requests may hit different model versions during the rollout. Different deployment strategies provide different consistency guarantees:\n\n| Deployment Strategy | Consistency | Rollout Speed | Risk |\n|---------------------|-------------|---------------|------|\n| **Blue-green** | Strong (atomic switch) | Instant | High (all-or-nothing) |\n| **Canary** | Eventual (gradual shift) | Slow (hours) | Low (progressive) |\n| **Rolling update** | Weak (mixed versions) | Medium | Medium |\n\nFor applications requiring deterministic outputs (compliance, audit trails, reproducible debugging):\n\n1. **Pin model version**: Include version identifier in request routing to ensure same model handles related requests\n2. **Use temperature=0**: Eliminates sampling variance, though beam search still has implementation-dependent tiebreaking\n3. **Version-aware caching**: Cache responses with model version tags; invalidate on version change\n\n**Failure scenario: Stateful replica crash**\n\nWhen a stateful LLM replica crashes mid-conversation:\n\n```\n1. Load balancer detects health check failure (1-10 seconds)\n2. New requests route to healthy replicas\n3. In-flight requests fail; clients must retry\n4. Session state (KV cache) is lost\n5. Recovery options:\n   a. Regenerate: Client resends conversation history (high latency)\n   b. Redirect: Route to replica with replicated state (if available)\n   c. Restart: Begin new session (poor user experience)\n```\n\nProduction systems often accept option (a) with optimizations: the regenerated prefill can process the full conversation history in a single batch, taking seconds rather than the minutes the original conversation took.\n\n**Build vs buy: Managed serving services**:\n\nBefore selecting frameworks, teams must decide whether to self-host or use managed services:\n\n| Service | Provider | Key Features | Trade-offs |\n|---------|----------|--------------|------------|\n| **SageMaker Endpoints** | AWS | Managed hosting, autoscaling, A/B testing | Lock-in, cost, limited customization |\n| **Vertex AI Endpoints** | GCP | TPU support, traffic splitting, model monitoring | GCP ecosystem dependency |\n| **Azure ML Endpoints** | Azure | Enterprise integration, ONNX optimization | Azure ecosystem dependency |\n| **Anyscale Endpoints** | Anyscale | Ray-native, fine-grained autoscaling | Emerging platform |\n\n| Approach | Advantages | Disadvantages |\n|----------|------------|---------------|\n| Self-hosted (vLLM/Triton) | Full control, cost optimization at scale | Operational burden, expertise required |\n| Managed (SageMaker/Vertex) | Operational simplicity, integrated tooling | Lock-in, cost at scale, limited customization |\n| Hybrid (Ray Serve + cloud) | Flexibility, gradual migration | Complexity in managing both |\n\n**Decision factors**:\n\n- **Team size**: Teams with fewer than 5 ML engineers often benefit from managed services\n- **Scale**: More than 1M daily requests typically makes self-hosting cost-effective\n- **Customization needs**: Novel architectures require self-hosting\n- **Latency requirements**: Self-hosting enables co-location and deeper optimization\n\n**Enhanced framework selection with orchestration**:\n\n```\nStart\n  │\n  ├─ What scale?\n  │   ├─ <100 QPS → Simple deployment (managed services or single instance)\n  │   ├─ 100-10K QPS → Need autoscaling\n  │   │   ├─ Managed acceptable → SageMaker/Vertex\n  │   │   └─ Self-hosted required → Ray Serve + vLLM/TensorRT-LLM\n  │   └─ >10K QPS → Need distributed orchestration\n  │       ├─ LLM → Ray Serve + vLLM with sharding\n  │       ├─ RecSys → Custom or Triton with embedding sharding\n  │       └─ Vision → Triton with dynamic batching\n  │\n  ├─ How many model types?\n  │   ├─ Single model → Direct runtime deployment\n  │   └─ Multiple models/pipelines → Triton or Ray Serve composition\n  │\n  └─ Stateful or stateless?\n      ├─ Stateless → Any load balancer, simple scaling\n      └─ Stateful (LLM with cache) → Sticky routing, session management\n```\n\n**Case study: Startup serving Llama-70B for customer support**\n\n*Scenario*: A startup is launching an LLM-powered customer support chatbot using a fine-tuned Llama-70B model.\n\n| Constraint | Value |\n|------------|-------|\n| Expected traffic | 100 QPS average, 500 QPS peak |\n| Latency requirement | <2s time to first token |\n| Budget | 4 H100 GPUs (leased) |\n| Team | 2 ML engineers, no dedicated SRE |\n| Conversation length | Average 8 turns, max 32K context |\n\n*Analysis*:\n\n1. **Memory**: Llama-70B requires ~140GB in FP16. With 4-bit quantization (AWQ), this drops to ~35GB, fitting on a single H100 (80GB) with room for KV cache.\n\n2. **Throughput**: At 500 QPS peak with average 100 output tokens, the system must sustain 50,000 tokens/second. A single quantized Llama-70B on H100 achieves ~1,000 tokens/second with continuous batching, so 4 GPUs provide headroom.\n\n3. **Tensor parallelism vs replicas**: Two options exist:\n   - 2 replicas × 2-way TP: Higher availability, lower per-request latency\n   - 4 replicas × 1-way TP: Maximum throughput, simpler scaling\n\n   With AWQ fitting on single GPU, option (b) is preferred for this traffic level.\n\n4. **Team size**: 2 ML engineers without SRE experience suggests managed services or simple orchestration.\n\n*Decision*:\n\n| Option | Architecture | Pros | Cons |\n|--------|--------------|------|------|\n| A | SageMaker + HF TGI | Minimal ops burden | Cost, limited optimization |\n| B | vLLM + Ray Serve on EC2 | Good balance | Some ops required |\n| C | TensorRT-LLM + Triton | Maximum throughput | Complex, overkill for 500 QPS |\n\n**Recommendation**: Option B (vLLM + Ray Serve) provides the best balance. At 500 QPS, the 30% throughput advantage of TensorRT-LLM does not justify the deployment complexity. Start with vLLM; migrate to TensorRT-LLM only if traffic grows beyond 2,000 QPS.\n\n```python\n# Recommended production configuration\n@serve.deployment(\n    num_replicas=4,\n    ray_actor_options={\"num_gpus\": 1},\n    autoscaling_config={\n        \"min_replicas\": 2,\n        \"max_replicas\": 8,\n        \"target_num_ongoing_requests_per_replica\": 20,\n    },\n)\nclass CustomerSupportLLM:\n    def __init__(self):\n        self.llm = LLM(\n            model=\"your-finetuned-llama-70b-awq\",\n            quantization=\"awq\",\n            max_model_len=32768,\n        )\n```\n\nThis configuration handles the 500 QPS peak with 4 replicas, can scale to 8 during unexpected spikes, and scales down to 2 during low-traffic periods to reduce cost.\n\n## Batching Strategies at Scale {#sec-inference-batching}\n\nVolume I introduced batching fundamentals for single-machine serving: how dynamic batching trades latency for throughput, and how traffic patterns determine optimal batch sizes. At scale, batching becomes more complex because different model architectures have fundamentally different batching requirements. A strategy optimal for vision models may be catastrophic for LLMs, and techniques developed for recommendation systems may not apply to either.\n\n**Production reality**: Recommendation systems constitute 80-90% of inference requests at major technology companies, with vision models handling most of the remainder, and LLMs currently representing 1-5% of request volume (though growing rapidly). Despite this distribution, we present batching strategies in order of conceptual complexity: vision (straightforward batching), LLMs (continuous batching with KV cache), and recommendation (feature-parallel batching with distributed embedding). This pedagogical ordering builds understanding progressively, even though practitioners will most frequently encounter recommendation workloads first.\n\nThis section develops a taxonomy of batching strategies matched to model characteristics, providing quantitative analysis of when each approach applies and what performance to expect.\n\n### Why Batching Differs Across Model Types {#sec-inference-batching-differences}\n\nThe core insight is that batching efficiency depends on how computation scales with batch size relative to how memory and communication scale. Different model architectures exhibit different scaling relationships, requiring different batching strategies.\n\nFor **vision models** (CNNs, ViTs processing fixed-size images), computation scales linearly with batch size while memory scales sub-linearly due to weight sharing. Larger batches improve GPU utilization with minimal overhead, making static or dynamic batching with large batch sizes optimal.\n\nFor **LLMs in the decode phase**, computation per token is small relative to memory bandwidth requirements for loading model weights. The bottleneck is memory bandwidth, not compute. Larger batches amortize weight loading across more tokens, dramatically improving throughput but with diminishing returns as batch size grows.\n\nFor **recommendation systems**, the bottleneck is often embedding lookup rather than dense computation. Batching strategies must optimize for parallel embedding access patterns rather than matrix multiplication throughput.\n\n| Model Type | Batching Strategy | Typical Batch Size | Key Constraint | Throughput Scaling |\n|-----------|------------------|-------------------|----------------|-------------------|\n| Vision (CNN) | Static/Dynamic | 32-256 | GPU compute | Near-linear to 64+ |\n| LLM (prefill) | Dynamic | 1-64 | Memory capacity | Sub-linear |\n| LLM (decode) | Continuous | 100-1000s | Memory bandwidth | Log-linear |\n| RecSys | Feature-parallel | 1000-10000s | Embedding lookup | Depends on sharding |\n| Speech | Streaming | 1 | Real-time | N/A (latency-bound) |\n\n: **Batching Strategy by Model Type**: Each model type has characteristic batching behavior determined by its computational bottleneck. {#tbl-batching-by-model}\n\n### Static and Dynamic Batching for Vision Models {#sec-inference-static-dynamic-batching}\n\nVision models represent the simplest batching case because inputs have uniform size (after preprocessing) and computation follows a predictable pattern. The foundations from @sec-serving apply directly, with scale introducing considerations of batch formation across multiple replicas.\n\n**Static batching** collects exactly $B$ requests before processing. This maximizes GPU utilization when request arrival is predictable but causes unbounded latency during low-traffic periods.\n\n**Dynamic batching** collects requests for a maximum time window $T_{window}$ or until reaching maximum batch size $B_{max}$, whichever occurs first. The expected latency under Poisson arrivals with rate $\\lambda$ follows:\n\n$$E[L_{total}] = E[L_{queue}] + L_{batch} + L_{inference}(B)$$ {#eq-dynamic-batch-latency}\n\nwhere $E[L_{queue}]$ is the expected queuing delay, $L_{batch}$ is the batch formation delay (up to $T_{window}$), and $L_{inference}(B)$ is the inference time for batch size $B$.\n\n::: {.callout-note title=\"Worked Example: Dynamic Batching for ResNet-50 at Scale\"}\n\nConsider a vision classification service with the following requirements:\n\n- **Arrival rate**: 5,000 QPS\n- **Latency SLO**: 50ms P99\n- **Per-image inference time**: 5ms at batch=1, 25ms at batch=32\n- **Number of replicas**: 10 (each handling 500 QPS)\n\nFor a single replica with Poisson arrivals at $\\lambda = 500$ QPS:\n\n**Option A: No batching (batch=1)**\n\n- Service time: 5ms per request\n- Utilization: $\\rho = \\lambda \\times S = 500 \\times 0.005 = 2.5$ (impossible, system is overloaded)\n\nThis configuration cannot meet demand. Batching is required.\n\n**Option B: Dynamic batching with $B_{max}=16$, $T_{window}=10ms$**\n\nExpected requests per window: $E[B] = \\lambda \\times T_{window} = 500 \\times 0.01 = 5$\n\nWith 5 requests per batch:\n\n- Inference time: approximately 8ms (interpolating between batch=1 and batch=32)\n- Per-request compute: 8ms / 5 = 1.6ms\n- Maximum batch delay: 10ms\n- Expected total latency: ~15ms mean, ~30ms P99\n\nUtilization: $\\rho = 500 \\times 0.0016 = 0.8$ (sustainable)\n\n**Option C: Dynamic batching with $B_{max}=32$, $T_{window}=20ms$**\n\nExpected requests per window: $E[B] = 500 \\times 0.02 = 10$\n\nWith 10 requests per batch:\n\n- Inference time: approximately 12ms\n- Per-request compute: 12ms / 10 = 1.2ms\n- Maximum batch delay: 20ms\n- Expected total latency: ~22ms mean, ~42ms P99\n\nUtilization: $\\rho = 500 \\times 0.0012 = 0.6$ (comfortable)\n\n**Tradeoff**: Option C achieves 25% better throughput (lower utilization) at the cost of higher average latency (22ms vs 15ms). Both meet the 50ms P99 SLO.\n\n:::\n\nAt scale with multiple replicas, batch formation can occur either at individual replicas or at a centralized batching layer:\n\n**Replica-local batching**: Each replica independently forms batches from its assigned traffic. Simpler to implement but may result in uneven batch sizes across replicas when load is imbalanced.\n\n**Centralized batching**: A batching service collects requests and dispatches formed batches to replicas. Achieves more uniform batch sizes but adds a centralization bottleneck and additional network hop.\n\nProduction systems typically use replica-local batching with load balancing that ensures roughly equal traffic distribution, achieving the benefits of centralized batching without the complexity.\n\n### Continuous Batching for LLM Inference {#sec-inference-continuous-batching}\n\nAutoregressive language models present a unique batching challenge that static and dynamic approaches handle poorly. The key insight comes from the Orca paper: traditional batching forces all sequences in a batch to complete before any new sequences can join, wasting compute when sequences finish at different times.\n\nConsider a batch of 8 sequences. If one sequence completes after 10 tokens while others require 100 tokens, the completed sequence's GPU resources sit idle for 90 iterations. With traditional batching:\n\n$$\\text{Wasted compute} = \\frac{(100 - 10) \\times 1}{100 \\times 8} = 11.25\\%$$\n\nFor realistic output length distributions with high variance, wasted compute can exceed 50%.\n\n**Continuous batching** (also called iteration-level batching) decouples batch membership from iteration boundaries:\n\n1. At each decode iteration, check for completed sequences\n2. Remove completed sequences from the batch immediately\n3. Insert waiting sequences into freed slots\n4. Process the reorganized batch for the next iteration\n\nThis dynamic batch management maintains high GPU utilization regardless of sequence length variance.\n\nThe throughput improvement from continuous batching depends on sequence length distribution. For a distribution with coefficient of variation $CV = \\sigma / \\mu$:\n\n$$\\text{Throughput gain} \\approx 1 + \\frac{CV^2}{2}$$ {#eq-continuous-batching-gain}\n\nWith typical LLM output lengths having $CV \\approx 1.0$, continuous batching achieves approximately 1.5x throughput improvement. For highly variable outputs (conversational vs. code generation), gains can reach 2-4x.\n\n::: {.callout-note title=\"Implementation: Continuous Batching in vLLM\"}\n\nvLLM implements continuous batching with the following key mechanisms:\n\n**Iteration-level scheduling**: At each decode step, the scheduler evaluates:\n\n- Which sequences have generated end-of-sequence tokens (remove from batch)\n- Which waiting sequences can fit in available KV cache slots (add to batch)\n- Which sequences should be preempted if memory pressure exists (swap to CPU)\n\n**Memory management**: The KV cache is managed using PagedAttention (see @sec-inference-kv-cache), which enables dynamic allocation without fragmentation. When a sequence completes, its KV cache pages are immediately available for new sequences.\n\n**Batched decode kernel**: Despite dynamic batch composition, the decode kernel processes all active sequences in a single batched operation. Sequences at different generation lengths are padded to a common shape within the kernel.\n\n**Typical performance (Llama-2 70B on 8xA100)**:\n\n| Batching Strategy | Throughput (tokens/s) | GPU Utilization |\n|------------------|----------------------|-----------------|\n| Static (batch=8) | 400 | 45% |\n| Dynamic (timeout=50ms) | 580 | 65% |\n| Continuous | 1,200 | 92% |\n\nThe 3x throughput improvement from continuous batching comes from eliminating idle GPU cycles during sequence length variation.\n\n:::\n\n### Prefill vs Decode: The Two-Phase Challenge {#sec-inference-prefill-decode}\n\nLLM inference consists of two distinct phases with different computational characteristics, requiring different batching strategies within the same request:\n\n**Prefill phase**: Process the entire input prompt in parallel. Computation scales with prompt length. Memory access pattern is compute-bound (high arithmetic intensity).\n\n**Decode phase**: Generate output tokens one at a time. Each token requires loading entire model weights. Memory access pattern is bandwidth-bound (low arithmetic intensity).\n\n| Phase | Computation | Memory Access | Bottleneck | Optimal Batch |\n|-------|-------------|---------------|------------|---------------|\n| Prefill | O(prompt_length²) | Weight loading | Compute | Small (1-8) |\n| Decode | O(1) per token | Weight loading | Bandwidth | Large (100s) |\n\n: **Prefill vs Decode Characteristics**: The two phases have opposite optimization requirements. {#tbl-prefill-decode}\n\nThis dichotomy creates a scheduling challenge: prefill operations are long-running and compute-intensive, while decode operations are short and bandwidth-limited. Mixing them in the same batch can cause interference.\n\n**Chunked prefill** addresses this by breaking long prompts into fixed-size chunks that interleave with decode operations:\n\n$$\\text{Chunk latency} = \\frac{\\text{Chunk size}}{\\text{Prefill throughput}}$$\n\nWith chunk size chosen to match decode iteration time, prefill and decode can share GPU resources without decode latency spikes.\n\n**Prefill-decode disaggregation** takes this further by running prefill and decode on separate GPU pools:\n\n- Prefill pool: Optimized for compute (larger batch sizes, no KV cache persistence)\n- Decode pool: Optimized for bandwidth (small batches, maximum KV cache capacity)\n\nThis separation enables independent scaling: prefill capacity scales with input volume while decode capacity scales with output volume.\n\n::: {.callout-note title=\"Sarathi: Chunked Prefill Implementation\"}\n\nThe Sarathi system implements chunked prefill with the following design:\n\n**Chunk sizing**: Chunks are sized to complete in approximately the same time as one decode iteration (typically 10-50ms). For a prefill throughput of 10,000 tokens/second, a 20ms chunk processes 200 tokens.\n\n**Interleaving schedule**: Each GPU iteration processes either:\n\n- One prefill chunk for a new request, OR\n- One decode step for all active sequences\n\nThis ensures decode latency remains bounded regardless of incoming prompt lengths.\n\n**KV cache transfer**: When prefill completes, the generated KV cache transfers to decode slots. With NVLink, this transfer adds <1ms for typical prompt lengths.\n\n**Performance impact**:\n\n- Without chunking: Long prompts cause decode latency spikes of 100ms+\n- With chunking: Decode latency bounded to 30ms P99 regardless of prompt length\n\n:::\n\n### Feature-Parallel Batching for Recommendation Systems {#sec-inference-feature-parallel-batching}\n\nRecommendation systems have fundamentally different batching requirements than vision or language models. The computation pattern involves:\n\n1. **Sparse feature lookup**: Retrieve embeddings for user, item, and context features\n2. **Dense feature processing**: Transform and normalize dense features\n3. **Feature interaction**: Compute interactions between features (often via attention or factorization)\n4. **Ranking head**: Produce final scores\n\nThe sparse embedding lookup often dominates latency and determines batching strategy.\n\n**Feature-parallel batching** processes different feature types in parallel rather than batching entire requests:\n\n```\nRequest 1: [user_id_1, item_ids_1, context_1]\nRequest 2: [user_id_2, item_ids_2, context_2]\nRequest 3: [user_id_3, item_ids_3, context_3]\n\nFeature-parallel view:\nUser embeddings:  [lookup(user_1), lookup(user_2), lookup(user_3)]  → parallel\nItem embeddings:  [lookup(items_1), lookup(items_2), lookup(items_3)]  → parallel\nContext features: [process(ctx_1), process(ctx_2), process(ctx_3)]  → parallel\n\nThen: Combine features per request for ranking\n```\n\nThis parallelization is natural when embeddings are sharded across servers: each embedding server handles lookups for its shard across all requests in the batch.\n\n::: {.callout-note title=\"Worked Example: Recommendation System Batching at Meta Scale\"}\n\nConsider Meta's recommendation infrastructure serving 10 million QPS across the platform:\n\n**Request characteristics**:\n\n- Each request queries ~100 items (candidate ranking)\n- Each item requires 50 embedding lookups (user features, item features, cross features)\n- Total embedding lookups: 5,000 per request\n- Embedding table size: 100TB across 1,000 shards\n\n**Batching strategy**:\n\nWith 10M QPS and 1,000 embedding shards, each shard receives:\n\n$$\\text{Lookups per shard} = \\frac{10M \\times 5000}{1000} = 50 \\text{ billion lookups/sec}$$\n\nThis is clearly infeasible for single-threaded processing. Instead:\n\n**Batch accumulation window**: 1ms\n**Requests per batch**: 10,000 (at 10M QPS)\n**Lookups per shard per batch**: 50M\n\nEach embedding shard processes 50M lookups in a batched operation, achieving memory bandwidth utilization of 90%+ through sequential memory access patterns.\n\n**Latency breakdown**:\n\n| Phase | Duration | Notes |\n|-------|----------|-------|\n| Request routing | 0.2ms | Consistent hashing to shard |\n| Batch accumulation | 0.5ms (avg) | 1ms window |\n| Embedding lookup | 2ms | Batched, SSD-backed |\n| Feature processing | 1ms | Dense computation |\n| Ranking model | 1.5ms | Final scoring |\n| **Total** | **5.2ms** | Within 10ms SLO |\n\n:::\n\n### Streaming Inference for Real-Time Applications {#sec-inference-streaming}\n\nSome applications cannot tolerate batching delay of any kind. Real-time speech recognition, video analysis, and robotics require processing inputs as they arrive with minimal latency.\n\n**Streaming inference** processes inputs incrementally without waiting for batch formation:\n\n- **Speech**: Process audio frames (10-20ms chunks) as they arrive from the microphone\n- **Video**: Process frames at capture rate (30-60 FPS) without buffering\n- **Robotics**: Process sensor readings at control loop frequency (100-1000 Hz)\n\nFor streaming applications, the relevant metric is not throughput but **time to process each input**:\n\n$$L_{streaming} = L_{capture} + L_{transfer} + L_{inference} + L_{action}$$\n\nwhere all components must complete within the inter-frame interval.\n\n::: {.callout-note title=\"Streaming Speech Recognition Pipeline\"}\n\nConsider a streaming speech-to-text system with 20ms audio frames:\n\n**Latency budget**: 100ms end-to-end (5 frames of delay)\n\n**Pipeline stages**:\n\n| Stage | Duration | Notes |\n|-------|----------|-------|\n| Audio capture | 0ms (continuous) | Microphone buffer |\n| Network to server | 20ms | Including jitter buffer |\n| Feature extraction | 5ms | MFCC computation |\n| Encoder inference | 30ms | Streaming Conformer |\n| Decoder step | 15ms | Autoregressive CTC |\n| Text formatting | 5ms | Capitalization, punctuation |\n| Network to client | 15ms | Response transmission |\n| **Total** | **90ms** | Within 100ms budget |\n\n**Key constraints**:\n\n- No batching: Each frame processes individually\n- Stateful model: Encoder maintains context across frames\n- Pipeline parallelism: While frame N is in decoder, frame N+1 is in encoder\n\nGPU utilization is typically 30-50% for streaming workloads, traded for latency guarantee.\n\n:::\n\n### Adaptive Batching Strategies {#sec-inference-adaptive-batching}\n\nProduction systems rarely use fixed batching parameters. Instead, they adapt batching behavior based on current conditions:\n\n**Traffic-adaptive batching** adjusts batch window based on arrival rate:\n\n$$T_{window} = \\min\\left(T_{max}, \\frac{B_{target}}{\\lambda_{current}}\\right)$$\n\nWhen traffic is high, the window shrinks because the target batch size fills quickly. When traffic is low, the window extends but is capped to bound maximum latency.\n\n**SLO-adaptive batching** monitors latency percentiles and adjusts batching aggressively:\n\n```\nif P99_latency > 0.9 * SLO:\n    reduce B_max by 20%\n    reduce T_window by 20%\nelif P99_latency < 0.5 * SLO:\n    increase B_max by 10%\n    increase T_window by 10%\n```\n\nThis feedback loop maintains latency headroom while maximizing throughput during normal operation.\n\n**Request-aware batching** considers request characteristics when forming batches. For LLMs:\n\n- Group requests by expected output length (inferred from prompt type)\n- Group requests by prompt length to minimize padding\n- Prioritize latency-sensitive requests in smaller batches\n\n::: {.callout-note title=\"Production Adaptive Batching: The NVIDIA Triton Approach\"}\n\nTriton Inference Server implements adaptive batching with three configurable parameters:\n\n1. **max_batch_size**: Upper bound on batch size\n2. **batching_timeout_ms**: Maximum time to wait for batch formation\n3. **preferred_batch_size**: Target batch sizes that align with kernel efficiency\n\nThe scheduler maintains separate queues for each preferred batch size and routes requests to minimize total latency:\n\n$$\\text{Queue selection} = \\arg\\min_{q} \\left( \\text{wait}_q + \\text{exec}(|q| + 1) \\right)$$\n\nThis optimization considers both the current queue length and the efficiency of the resulting batch size.\n\n**Observed behavior on ResNet-50 (V100)**:\n\n| Traffic Level | Avg Batch Size | Avg Latency | Throughput |\n|--------------|---------------|-------------|------------|\n| 100 QPS | 2.1 | 8ms | 100 QPS |\n| 500 QPS | 6.3 | 12ms | 500 QPS |\n| 1000 QPS | 12.4 | 18ms | 1000 QPS |\n| 2000 QPS | 24.1 | 28ms | 1980 QPS |\n\nThe system automatically increases batch size to maintain throughput as traffic grows.\n\n:::\n\n### Quantitative Summary: Batching Strategy Selection {#sec-inference-batching-summary}\n\nThe choice of batching strategy depends on model characteristics, traffic patterns, and latency requirements. The following decision framework guides selection:\n\n```\nIs the model autoregressive (LLM, speech)?\n├─ Yes → Continuous batching with prefill chunking\n└─ No → Does the model have embedding lookups dominating latency?\n        ├─ Yes → Feature-parallel batching (RecSys)\n        └─ No → Dynamic batching with adaptive parameters\n```\n\nFor each strategy, the key parameters to tune are:\n\n| Strategy | Key Parameters | Tuning Goal |\n|----------|---------------|-------------|\n| Static | Batch size | Maximize throughput |\n| Dynamic | Window, max batch | Balance latency vs throughput |\n| Continuous | Chunk size, max batch | Minimize decode latency variance |\n| Feature-parallel | Accumulation window | Match embedding shard capacity |\n| Streaming | Pipeline depth | Meet real-time deadline |\n\n: **Batching Strategy Parameters**: Each strategy has distinct parameters requiring tuning for the specific deployment. {#tbl-batching-parameters}\n\n## Model Sharding for Inference {#sec-inference-sharding}\n\nWhen models exceed single-GPU memory or when latency requirements demand parallel computation, model sharding distributes inference across multiple devices. Unlike training, where throughput is the primary concern, inference sharding must carefully balance parallelization benefits against communication overhead within strict latency budgets.\n\nThis section examines four sharding strategies, each suited to different model architectures and deployment requirements: tensor parallelism for attention-heavy models, pipeline parallelism for sequential architectures, expert parallelism for mixture-of-experts models, and embedding sharding for recommendation systems.\n\n### When Sharding Becomes Necessary {#sec-inference-sharding-when}\n\nModel sharding for inference is driven by two distinct requirements:\n\n**Memory requirements**: A model that cannot fit in single-GPU memory must be sharded regardless of performance considerations. For a model with $P$ parameters at precision $b$ bits:\n\n$$\\text{Memory}_{weights} = P \\times \\frac{b}{8} \\text{ bytes}$$ {#eq-weight-memory}\n\nA 70-billion parameter model in FP16 (16 bits) requires:\n\n$$\\text{Memory} = 70 \\times 10^9 \\times \\frac{16}{8} = 140 \\text{ GB}$$\n\nThis exceeds the 80GB capacity of an H100 GPU, requiring at minimum 2-way sharding.\n\n**Latency requirements**: Even when a model fits in memory, sharding can reduce latency by parallelizing computation. The potential speedup depends on the parallelization efficiency:\n\n$$T_{parallel} = \\frac{T_{sequential}}{P} + T_{communication}$$ {#eq-parallel-time}\n\nwhere $P$ is the parallelism degree and $T_{communication}$ is the synchronization overhead. Sharding provides latency benefit only when the communication overhead is smaller than the time saved through parallelization.\n\n| Sharding Trigger | Model Examples | Minimum Sharding | Strategy |\n|-----------------|----------------|------------------|----------|\n| Memory (weights) | Llama-70B (140GB) | 2-way | Tensor or pipeline |\n| Memory (KV cache) | GPT-4 (long context) | 4-8 way | Tensor (for cache) |\n| Memory (embeddings) | DLRM (100TB) | 1000+ way | Embedding sharding |\n| Latency | Any large model | Varies | Tensor parallelism |\n\n: **Sharding Triggers**: Different constraints lead to different sharding requirements and strategies. {#tbl-sharding-triggers}\n\n### Tensor Parallelism {#sec-inference-tensor-parallelism}\n\nTensor parallelism distributes individual layers across multiple devices, enabling parallel computation within each layer. For transformer models, the primary target is the attention mechanism and feed-forward layers, which contain the majority of computation.\n\n**Attention layer parallelism**: The multi-head attention computation naturally partitions across attention heads. For a model with $H$ attention heads distributed across $P$ devices, each device computes $H/P$ heads:\n\n$$\\text{Attention}_i = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_i \\text{ for heads } i \\in \\{1, ..., H/P\\}$$\n\nAfter computing local attention, an all-reduce operation combines results across devices.\n\n**Feed-forward layer parallelism**: The feed-forward layer (typically two linear transformations with activation) partitions along the hidden dimension. For the first linear layer, columns are distributed; for the second, rows are distributed. This column-row partitioning requires only one all-reduce per feed-forward block.\n\nThe communication pattern for tensor-parallel inference follows:\n\n```\nInput activations (replicated on all devices)\n    │\n    ▼\nAttention heads (computed in parallel, H/P heads per device)\n    │\n    ▼\nAllReduce (combine attention outputs)\n    │\n    ▼\nFeed-forward layer 1 (column-parallel)\n    │\n    ▼\nFeed-forward layer 2 (row-parallel)\n    │\n    ▼\nAllReduce (combine FF outputs)\n    │\n    ▼\nOutput activations (replicated on all devices)\n```\n\nThe inference time with tensor parallelism follows:\n\n$$T_{inference} = \\frac{T_{compute}}{P} + 2 \\times T_{allreduce}\\left(\\frac{A}{P}\\right)$$ {#eq-tensor-parallel-time}\n\nwhere $T_{compute}$ is the sequential compute time, $P$ is the parallelism degree, and $A$ is the activation size being reduced. The factor of 2 accounts for the two all-reduce operations per transformer layer (attention and feed-forward).\n\n::: {.callout-note title=\"Worked Example: Tensor Parallelism for Llama-70B\"}\n\nConsider serving Llama-70B with the following configuration:\n\n**Model specifications**:\n\n- Parameters: 70 billion\n- Hidden dimension: 8,192\n- Attention heads: 64\n- Layers: 80\n\n**Memory per GPU** (weight only, FP16):\n\n$$\\text{Memory}_{70B} = 70 \\times 10^9 \\times 2 = 140\\text{ GB}$$\n\n**Minimum sharding**: 2-way (140GB / 80GB per H100)\n\n**Recommended sharding**: 8-way for optimal latency\n\n**With 8-way tensor parallelism on 8xH100 (NVLink interconnect)**:\n\n| Component | Sequential (1 GPU) | 8-way TP | Speedup |\n|-----------|-------------------|----------|---------|\n| Attention compute | 12ms | 1.5ms | 8x |\n| AllReduce (attention) | 0ms | 0.3ms | N/A |\n| Feed-forward compute | 18ms | 2.25ms | 8x |\n| AllReduce (FF) | 0ms | 0.3ms | N/A |\n| **Total per layer** | **30ms** | **4.35ms** | **6.9x** |\n\n**For 80 layers**:\n\n- Sequential: 2,400ms per token\n- 8-way TP: 348ms per token\n\nThe 6.9x speedup (vs theoretical 8x) reflects communication overhead. With 600 GB/s NVLink bandwidth, each 8MB activation all-reduce takes ~0.3ms.\n\n**Time-to-first-token** (1024-token prompt):\n\n- Prefill compute: ~50ms (compute-bound, near-linear scaling)\n- Total TTFT: ~60ms with preprocessing\n\n:::\n\n### Pipeline Parallelism for Inference {#sec-inference-pipeline-parallelism}\n\nPipeline parallelism distributes layers across devices sequentially, with each device handling a subset of layers. Unlike tensor parallelism, there is no synchronization within a layer, only between pipeline stages.\n\nFor inference, pipeline parallelism creates bubbles differently than in training:\n\n```\nDevice 0 (Layers 1-20):  [Prefill] ─────────────────────────────\n                                  \\\nDevice 1 (Layers 21-40):           [Prefill] ──────────────────\n                                            \\\nDevice 2 (Layers 41-60):                     [Prefill] ────────\n                                                      \\\nDevice 3 (Layers 61-80):                               [Prefill]\n```\n\nFor a single request, pipeline parallelism provides no latency benefit: the request must traverse all stages sequentially. The pipeline fill time equals the sequential execution time.\n\nHowever, pipeline parallelism enables **throughput scaling** through pipelining multiple requests:\n\n```\nTime →\nDevice 0: [Req1] [Req2] [Req3] [Req4] ...\nDevice 1:        [Req1] [Req2] [Req3] [Req4] ...\nDevice 2:               [Req1] [Req2] [Req3] [Req4] ...\nDevice 3:                      [Req1] [Req2] [Req3] [Req4] ...\n```\n\nOnce the pipeline is full, throughput equals $P$ times single-stage throughput, where $P$ is the number of pipeline stages. The steady-state latency remains approximately the single-device latency (sum of all stage times), but throughput scales with parallelism.\n\n**When to use pipeline parallelism for inference**:\n\n- When memory constraints require sharding but latency requirements are relaxed\n- When throughput is more important than individual request latency\n- When network bandwidth between devices is limited (only point-to-point communication)\n\n**Pipeline vs tensor parallelism tradeoffs**:\n\n| Aspect | Tensor Parallelism | Pipeline Parallelism |\n|--------|-------------------|---------------------|\n| Single-request latency | Reduced by ~$P$x | No improvement |\n| Throughput | $P$x | $P$x (when pipelined) |\n| Communication pattern | AllReduce (bandwidth-intensive) | Point-to-point (latency-sensitive) |\n| Memory efficiency | Activations replicated | Activations passed along |\n| Complexity | Higher (requires custom kernels) | Lower (layer-level partitioning) |\n\n: **Pipeline vs Tensor Parallelism**: Each strategy has distinct tradeoffs in latency, throughput, and implementation complexity. {#tbl-pipeline-tensor-comparison}\n\n### Expert Parallelism for MoE Models {#sec-inference-expert-parallelism}\n\nMixture-of-Experts (MoE) models present unique sharding challenges because computation is dynamically routed to different experts based on input. Popular models like Mixtral use MoE to achieve high capacity with lower inference cost.\n\nIn an MoE layer, a gating network selects $k$ experts (out of $E$ total) for each token:\n\n$$\\text{Output} = \\sum_{i \\in \\text{top-}k} g_i \\cdot \\text{Expert}_i(\\text{input})$$\n\n**Expert parallelism** distributes experts across devices, with each device hosting $E/P$ experts:\n\n```\nToken arrives with gating decision: [Expert 2, Expert 7]\n\nDevice 0 (Experts 0-3):   Compute Expert 2\nDevice 1 (Experts 4-7):   Compute Expert 7\n\nAllToAll: Gather results back to original device\n```\n\nThe communication pattern differs from tensor parallelism: instead of all-reduce (same data to all devices), expert parallelism uses all-to-all (different data to different devices based on routing).\n\n**Load balancing challenge**: If gating decisions cluster on certain experts, devices hosting popular experts become bottlenecks while others sit idle. MoE training includes auxiliary losses to encourage balanced routing, but inference still exhibits routing imbalance.\n\n::: {.callout-note title=\"Expert Parallelism for Mixtral-8x7B\"}\n\nMixtral-8x7B uses 8 experts per MoE layer with top-2 routing:\n\n**Model characteristics**:\n\n- Total parameters: 47B (but only ~13B active per token)\n- Experts per layer: 8\n- Active experts per token: 2 (top-k = 2)\n- MoE layers: Every other feed-forward layer\n\n**Sharding strategy** (4-way expert parallelism):\n\n- Experts 0-1 on Device 0\n- Experts 2-3 on Device 1\n- Experts 4-5 on Device 2\n- Experts 6-7 on Device 3\n\n**Communication pattern per token**:\n\n1. Gating: Determine which 2 experts to use (~0.1ms)\n2. AllToAll dispatch: Send token to devices hosting selected experts (~0.2ms)\n3. Expert compute: Process token through selected experts (~1ms each, parallel)\n4. AllToAll gather: Collect results back (~0.2ms)\n\n**Total MoE layer time**: ~1.5ms (vs ~4ms for equivalent dense layer)\n\n**Load balancing metrics**:\n\n| Routing Distribution | GPU Utilization | Throughput Impact |\n|---------------------|-----------------|-------------------|\n| Perfectly balanced | 100% | Baseline |\n| Moderate imbalance (20%) | 83% | -17% |\n| Severe imbalance (50%) | 67% | -33% |\n\nProduction systems monitor routing statistics and may retrain or fine-tune gating to improve balance.\n\n:::\n\n### Embedding Sharding for Recommendation Systems {#sec-inference-embedding-sharding}\n\nRecommendation systems typically contain embedding tables that dwarf model weights in size. Meta's DLRM-scale models have embedding tables exceeding 100TB, requiring aggressive sharding strategies fundamentally different from tensor or pipeline parallelism.\n\n**Row-wise sharding** partitions embedding tables by row (entity ID):\n\n$$\\text{Shard}_i = \\{e_j : \\text{hash}(j) \\mod P = i\\}$$\n\nEach shard contains approximately $N/P$ embeddings, where $N$ is the total number of entities and $P$ is the shard count.\n\n**Column-wise sharding** partitions each embedding vector across devices:\n\n$$e_j = [e_j^{(0)}, e_j^{(1)}, ..., e_j^{(P-1)}]$$\n\nEach device stores a slice of every embedding.\n\n**Hybrid sharding** combines both approaches: frequently accessed embeddings are column-sharded for faster access, while the long tail uses row sharding.\n\n| Sharding Strategy | Lookup Pattern | Communication | Best For |\n|------------------|----------------|---------------|----------|\n| Row-wise | Single device per lookup | AllToAll gather | Uniform access patterns |\n| Column-wise | All devices per lookup | AllGather | Hot embeddings |\n| Hybrid | Varies by embedding | Mixed | Production RecSys |\n\n: **Embedding Sharding Strategies**: Different strategies trade off lookup locality against load balance. {#tbl-embedding-sharding}\n\n::: {.callout-note title=\"Embedding Sharding at Scale: Meta Infrastructure\"}\n\nMeta's recommendation infrastructure demonstrates embedding sharding at extreme scale:\n\n**Scale**:\n\n- Embedding tables: 100+ TB total\n- Unique entities: 10+ trillion\n- Embedding dimension: 128-256\n- Shards: 1,000+ servers\n\n**Sharding strategy**:\n\n- **Hot embeddings** (top 1% by access frequency): Replicated across all shards\n- **Warm embeddings** (next 10%): Column-sharded with 8-way parallelism\n- **Cold embeddings** (remaining 89%): Row-sharded with consistent hashing\n\n**Access pattern optimization**:\n\nEach inference request requires ~5,000 embedding lookups. Without optimization, this would require 5,000 network round trips. Instead:\n\n1. **Batch accumulation**: Collect lookups for 1ms\n2. **Lookup deduplication**: Remove duplicate entities across requests\n3. **Shard-aware batching**: Group lookups by destination shard\n4. **Parallel dispatch**: Send batched requests to all shards simultaneously\n5. **Streaming assembly**: Reconstruct embeddings as responses arrive\n\n**Performance**:\n\n| Metric | Without Optimization | With Optimization |\n|--------|---------------------|-------------------|\n| Network round trips | 5,000 | 1 (batched) |\n| Lookup latency | 50ms | 2ms |\n| Network bandwidth | 10 Gbps | 40 Gbps (burst) |\n\n:::\n\n### Hybrid Sharding Strategies {#sec-inference-hybrid-sharding}\n\nProduction systems often combine multiple sharding strategies to handle different model components optimally:\n\n**Tensor + Pipeline parallelism**: For very large models that require both memory distribution and latency reduction:\n\n```\n8 GPUs organized as 2 pipeline stages × 4 tensor parallel:\n\nStage 0 (Layers 1-40):  TP across GPUs 0,1,2,3\nStage 1 (Layers 41-80): TP across GPUs 4,5,6,7\n```\n\nThis achieves 4x latency reduction (from TP) while handling models requiring 8-way sharding for memory.\n\n**Expert + Tensor parallelism**: For MoE models where individual experts are large:\n\n```\nMixtral with large experts:\n\n- Expert parallelism: Distribute 8 experts across 8 GPU groups\n- Tensor parallelism: Each expert spread across 2 GPUs\n- Total GPUs: 16\n```\n\n**Embedding + Dense parallelism**: For recommendation models with both large embeddings and large dense components:\n\n```\nDLRM-scale model:\n\n- Embedding sharding: 1,000 shards across CPU servers\n- Dense model: 8-way tensor parallel across GPUs\n- Communication: Embeddings gathered to GPU, processed, returned\n```\n\n### Communication Overhead Analysis {#sec-inference-sharding-communication}\n\nThe practical speedup from sharding depends critically on communication efficiency. Each sharding strategy has characteristic communication patterns with different bandwidth and latency requirements.\n\n**AllReduce** (tensor parallelism): Combines data from all devices, with result available on all devices.\n\n$$T_{allreduce} = 2 \\times \\frac{(P-1)}{P} \\times \\frac{M}{B}$$ {#eq-allreduce-time}\n\nwhere $P$ is the number of devices, $M$ is the message size, and $B$ is the interconnect bandwidth. The factor of 2 accounts for the reduce-scatter and all-gather phases.\n\n**Point-to-point** (pipeline parallelism): Sends data from one device to the next.\n\n$$T_{p2p} = L + \\frac{M}{B}$$ {#eq-p2p-time}\n\nwhere $L$ is the network latency and $M/B$ is the transfer time.\n\n**AllToAll** (expert parallelism): Exchanges data where each device sends different data to each other device.\n\n$$T_{alltoall} = (P-1) \\times \\left(L + \\frac{M/P}{B}\\right)$$ {#eq-alltoall-time}\n\n::: {.callout-note title=\"Interconnect Technology Comparison\"}\n\nCommunication overhead depends heavily on the interconnect technology:\n\n| Interconnect | Bandwidth | Latency | Use Case |\n|--------------|-----------|---------|----------|\n| NVLink (H100) | 900 GB/s | 1μs | Intra-node TP |\n| PCIe Gen5 | 64 GB/s | 5μs | Intra-node (no NVLink) |\n| InfiniBand HDR | 200 Gb/s (25 GB/s) | 1μs | Inter-node |\n| Ethernet 100G | 100 Gb/s (12.5 GB/s) | 10μs | Inter-node (commodity) |\n\n**Example: 8-way tensor parallelism communication**\n\nActivation size: 8MB per all-reduce (batch=1, hidden=8192)\n\n| Interconnect | AllReduce Time | % of 30ms Layer |\n|--------------|----------------|-----------------|\n| NVLink | 0.02ms | 0.07% |\n| InfiniBand | 0.7ms | 2.3% |\n| 100G Ethernet | 1.5ms | 5% |\n\nNVLink enables efficient tensor parallelism within a node. Cross-node tensor parallelism requires InfiniBand for acceptable overhead.\n\n:::\n\n### Sharding Strategy Selection {#sec-inference-sharding-selection}\n\nSelecting the appropriate sharding strategy depends on model architecture, deployment constraints, and optimization priorities:\n\n```\nDoes the model fit in single-GPU memory?\n├─ Yes → Is latency reduction needed?\n│        ├─ Yes → Tensor parallelism (within node)\n│        └─ No → No sharding needed\n└─ No → What is the memory bottleneck?\n         ├─ Weights → Tensor or pipeline parallelism\n         ├─ KV cache → Tensor parallelism (distributes cache)\n         ├─ Embeddings → Embedding sharding\n         └─ Experts → Expert parallelism\n```\n\n**Decision factors**:\n\n| Factor | Tensor Parallel | Pipeline Parallel | Expert Parallel | Embedding Shard |\n|--------|----------------|-------------------|-----------------|-----------------|\n| Latency priority | Best | Worst | Moderate | N/A |\n| Throughput priority | Good | Best (pipelined) | Good | Best |\n| Interconnect limited | Poor fit | Good fit | Moderate | Good fit |\n| Implementation effort | High | Low | Moderate | High |\n\n: **Sharding Strategy Selection Guide**: Match strategy to deployment priorities and constraints. {#tbl-sharding-selection}\n\n## Load Balancing and Request Routing {#sec-inference-load-balancing}\n\nWith models deployed across multiple replicas, the next challenge is distributing requests effectively. Load balancing determines which replica handles each request, directly impacting latency, throughput, and resource utilization. Seemingly simple choices, like random assignment versus informed selection, produce dramatically different performance at scale.\n\nThis section develops the theory and practice of load balancing for inference, from basic algorithms through the power-of-two-choices insight that provides exponentially better performance with minimal overhead.\n\n### Load Balancing Fundamentals {#sec-inference-lb-fundamentals}\n\nLoad balancing serves two primary goals that sometimes conflict:\n\n**Latency minimization**: Route requests to replicas that can serve them fastest, considering current queue depth and processing time.\n\n**Utilization maximization**: Spread load evenly to avoid both idle replicas and overloaded replicas.\n\nThe tension arises because latency-optimal routing may concentrate load on fast replicas, reducing their performance and leaving other replicas underutilized.\n\n**Key metrics** for load balancing evaluation:\n\n- **Maximum queue length**: The longest queue across all replicas (determines worst-case latency)\n- **Load variance**: Standard deviation of queue lengths (measures balance)\n- **Utilization spread**: Difference between most and least utilized replicas\n- **Decision overhead**: Time required to make routing decisions\n\n### Round-Robin and Random Assignment {#sec-inference-lb-round-robin}\n\nThe simplest load balancing strategies assign requests without considering server state:\n\n**Round-robin** assigns requests in circular order: request 1 to server 1, request 2 to server 2, and so on. This guarantees perfect distribution when servers are homogeneous and request processing times are identical.\n\n**Random assignment** selects a server uniformly at random for each request. With large numbers of requests, this converges to even distribution but with higher variance than round-robin.\n\nFor homogeneous servers with identical service times, both achieve near-optimal load distribution. However, production systems rarely meet these assumptions:\n\n- **Heterogeneous hardware**: Different GPU generations, memory configurations\n- **Variable request sizes**: Some requests take 10x longer than others\n- **Server state variations**: Some replicas warming up, others near memory limits\n\nUnder these realistic conditions, uninformed strategies perform poorly. The maximum queue length under random assignment follows:\n\n$$E[\\text{max queue}] = \\Theta\\left(\\frac{\\log n}{\\log \\log n}\\right)$$ {#eq-random-max-queue}\n\nwhere $n$ is the number of servers. For 1,000 servers, this is approximately 4-5 requests. This seems small, but the unlucky requests in long queues experience significantly higher latency.\n\n### The Power of Two Choices {#sec-inference-two-choices}\n\nA remarkable result in load balancing theory shows that querying just two random servers before making a routing decision provides exponentially better load distribution than random assignment.\n\n**Power-of-two-choices algorithm**:\n\n1. Select two servers uniformly at random\n2. Query both for their current queue length\n3. Route the request to the server with the shorter queue\n\nThis simple modification reduces maximum queue length from $O(\\log n / \\log \\log n)$ to $O(\\log \\log n)$:\n\n$$E[\\text{max queue}]_{\\text{two choices}} = \\Theta(\\log \\log n)$$ {#eq-two-choices-max-queue}\n\nFor 1,000 servers:\n\n- Random assignment max queue: ~4-5 requests\n- Two choices max queue: ~2 requests\n\nThe improvement is exponential: two choices with 1,000 servers achieves better balance than random with just 10 servers.\n\n::: {.callout-important title=\"Exponential Improvement from a Simple Change\"}\n\nThe power-of-two-choices result is one of the most impactful findings in distributed systems theory. By examining just one additional server, maximum queue length improves from $O(\\log n / \\log \\log n)$ to $O(\\log \\log n)$, an exponential improvement.\n\nThis has profound practical implications:\n\n- Near-optimal load balancing with minimal overhead (2 probes vs n probes)\n- Scalable: improvement increases with system size\n- Robust: works with heterogeneous servers and variable request sizes\n- Simple: easy to implement in any load balancer\n\nProduction systems at Google, Meta, and AWS all use variants of power-of-two-choices.\n\n:::\n\n**Why does this work?** Intuitively, random assignment occasionally makes poor choices (routing to an already-busy server), and these mistakes compound. With two choices, the algorithm almost never makes the worst choice, avoiding the tail behavior that creates long queues.\n\nMathematically, the key insight is that with random assignment, when $d$ servers have queue length $k$, the probability of queue length $k+1$ growing is proportional to $d/n$. With two choices, this probability drops to $(d/n)^2$, creating a super-exponential decay in queue length distribution.\n\n### Weighted and Adaptive Load Balancing {#sec-inference-weighted-lb}\n\nWhen servers have different capacities, naive load balancing creates imbalance. A mix of A100 GPUs (high capacity) and T4 GPUs (lower capacity) receiving equal request rates will have T4 servers overloaded while A100 servers are underutilized.\n\n**Weighted round-robin** assigns requests proportional to server capacity:\n\n$$P(\\text{route to server } i) = \\frac{w_i}{\\sum_j w_j}$$\n\nwhere $w_i$ is the weight (capacity) of server $i$.\n\n**Weighted two-choices** applies the same principle:\n\n1. Select two servers with probability proportional to their weights\n2. Query both for current load relative to their capacity\n3. Route to the server with lower relative load\n\n::: {.callout-note title=\"Worked Example: Heterogeneous GPU Cluster\"}\n\nConsider a cluster with mixed GPU types:\n\n- 10 H100 GPUs (capacity: 1000 QPS each)\n- 20 A100 GPUs (capacity: 600 QPS each)\n- Total capacity: 10×1000 + 20×600 = 22,000 QPS\n\n**Target traffic**: 15,000 QPS\n\n**Weighted assignment**:\n\n- H100 weight: 1000 / 22000 = 4.5%\n- A100 weight: 600 / 22000 = 2.7%\n\n**Expected load per server**:\n\n- H100: 15000 × 0.045 = 682 QPS (68% utilization)\n- A100: 15000 × 0.027 = 409 QPS (68% utilization)\n\nBoth server types operate at equal utilization, maximizing overall capacity while maintaining latency consistency.\n\n**Without weighting** (equal distribution):\n\n- Per-server load: 15000 / 30 = 500 QPS\n- H100 utilization: 50% (underutilized)\n- A100 utilization: 83% (overloaded, latency spikes)\n\n:::\n\n**Adaptive load balancing** adjusts weights dynamically based on observed performance:\n\n```\nFor each server i:\n    latency[i] = exponential_moving_average(observed_latency)\n    weight[i] = 1 / latency[i]  # Inverse latency weighting\n```\n\nThis automatically adapts to:\n\n- Server degradation (memory pressure, thermal throttling)\n- Request size variations (some traffic patterns harder to serve)\n- Background tasks consuming resources\n\n### Least-Connections Load Balancing {#sec-inference-least-connections}\n\nAn alternative to random selection is routing to the server with the fewest active connections (or shortest queue). This requires maintaining global state but provides better balance for variable-size requests.\n\n**Least-connections algorithm**:\n\n1. Maintain a count of active requests per server\n2. Route each new request to the server with the minimum count\n3. Increment count on dispatch, decrement on completion\n\nFor long-running requests (common in LLM serving), least-connections significantly outperforms round-robin because it accounts for current load rather than just historical assignments.\n\nThe challenge is maintaining accurate connection counts in a distributed system. Options include:\n\n- **Centralized counter**: Single source of truth, potential bottleneck\n- **Distributed counters with gossip**: Eventually consistent, may route to stale information\n- **Sampled least-connections**: Query a subset of servers, choose minimum (combines with two-choices)\n\n::: {.callout-note title=\"Least-Connections for LLM Serving\"}\n\nLLM inference has highly variable request durations based on output length:\n\n- Short response (10 tokens): 500ms\n- Long response (500 tokens): 25s\n- Ratio: 50x\n\nWith round-robin at 100 QPS across 10 servers:\n\n- Each server receives 10 requests/second\n- If one server gets multiple long requests, it falls behind\n- Queue builds while other servers sit idle\n\nWith least-connections:\n\n- New requests route away from servers processing long responses\n- Servers finishing short requests receive new work immediately\n- Load naturally balances based on actual work remaining\n\n**Observed improvement** (production LLM serving):\n\n| Algorithm | P99 Latency | Load Variance |\n|-----------|-------------|---------------|\n| Round-robin | 45s | 3.2 requests |\n| Least-connections | 28s | 0.8 requests |\n| Two-choices + LC | 26s | 0.5 requests |\n\nLeast-connections reduces P99 by 38%; combining with two-choices provides additional improvement.\n\n:::\n\n### Consistent Hashing for Stateful Routing {#sec-inference-consistent-hashing}\n\nMany inference workloads maintain state that benefits from routing affinity:\n\n- **LLM conversations**: KV cache from previous turns\n- **Recommendation sessions**: User context and recent interactions\n- **Streaming inference**: Model state from previous frames\n\nFor these workloads, routing the same user or session to the same server improves performance by avoiding cache misses and state reconstruction.\n\n**Consistent hashing** maps requests to servers based on a hash of the routing key (user ID, session ID):\n\n$$\\text{server}(request) = \\arg\\min_{s \\in S} \\text{distance}(\\text{hash}(key), \\text{hash}(s))$$\n\nwhere servers and keys are mapped onto a ring, and each request routes to the nearest server clockwise.\n\nKey properties:\n\n- **Deterministic**: Same key always routes to same server\n- **Minimal disruption**: Adding/removing servers only remaps $K/N$ keys on average\n- **Load balancing**: With virtual nodes, load distributes evenly\n\n::: {.callout-note title=\"Consistent Hashing for KV Cache Affinity\"}\n\nConsider an LLM serving system where each user's conversation maintains KV cache state:\n\n**Without affinity**:\n\n- User sends message, routed to Server A, KV cache built\n- Next message routes to Server B (random)\n- KV cache rebuilt from scratch, 500ms penalty\n- Average conversation: 10 turns, 4.5s wasted on cache rebuilds\n\n**With consistent hashing**:\n\n- User ID hashed to Server A\n- All messages from this user route to Server A\n- KV cache reused across turns\n- Rebuild only on server changes or cache eviction\n\n**Implementation with virtual nodes**:\n\nEach physical server has 100 virtual nodes on the hash ring, ensuring even distribution despite server heterogeneity.\n\n```\nHash ring positions:\nServer A: [0.01, 0.03, 0.07, 0.12, ...]  (100 positions)\nServer B: [0.02, 0.05, 0.09, 0.15, ...]  (100 positions)\n...\n\nRequest for user \"alice\":\nhash(\"alice\") = 0.0834\nNearest server clockwise: Server A (at 0.09)\n```\n\n**Handling server failures**:\n\nWhen Server A fails, its 100 virtual nodes are removed from the ring. Requests that would have routed to Server A now route to the next server clockwise. Only ~$1/N$ of requests are affected, where $N$ is the number of servers.\n\n:::\n\n### Request Routing for Sharded Models {#sec-inference-sharded-routing}\n\nWhen models are sharded across devices (see @sec-inference-sharding), routing becomes more complex. A single inference request may require computation on multiple devices, necessitating coordination.\n\n**Routing patterns for sharded models**:\n\n**Tensor parallelism**: Request is broadcast to all devices in the shard group. Each device processes its portion of each layer. Results are synchronized via all-reduce.\n\n```\nRequest → Load Balancer → Shard Group\n                              │\n            ┌────────────────┼────────────────┐\n            ▼                ▼                ▼\n         GPU 0            GPU 1            GPU 2\n      (heads 0-7)      (heads 8-15)    (heads 16-23)\n            │                │                │\n            └────────AllReduce────────────────┘\n                              │\n                              ▼\n                          Response\n```\n\n**Pipeline parallelism**: Request flows through stages sequentially. Each stage forwards to the next.\n\n```\nRequest → Stage 0 → Stage 1 → Stage 2 → Stage 3 → Response\n         (L1-20)   (L21-40)  (L41-60)  (L61-80)\n```\n\n**Expert parallelism**: Request is dispatched to devices hosting selected experts based on gating decision.\n\n```\nRequest → Gating → AllToAll dispatch → Expert compute → AllToAll gather → Response\n                   (to selected experts)              (results back)\n```\n\n**Routing to shard groups**: With multiple shard groups for horizontal scaling, the load balancer routes to groups rather than individual devices:\n\n```\n                    Load Balancer\n                         │\n          ┌──────────────┼──────────────┐\n          ▼              ▼              ▼\n     Shard Group 0  Shard Group 1  Shard Group 2\n      (8 GPUs)       (8 GPUs)       (8 GPUs)\n```\n\nThe load balancer treats each shard group as a single logical server, applying standard algorithms (round-robin, two-choices, consistent hashing) at the group level.\n\n### Health Checking and Failover {#sec-inference-health-checking}\n\nLoad balancers must detect unhealthy servers and route around them. Health checking mechanisms include:\n\n**Liveness probes**: Verify the server process is running.\n\n```\nGET /health/live\nResponse: 200 OK (process alive) or timeout (process dead)\n```\n\n**Readiness probes**: Verify the server can handle requests (model loaded, GPU initialized).\n\n```\nGET /health/ready\nResponse: 200 OK (ready to serve) or 503 (not ready)\n```\n\n**Deep health checks**: Verify actual inference works by running a test request.\n\n```\nPOST /health/inference\nBody: {\"prompt\": \"test\"}\nResponse: 200 OK with valid output, or error\n```\n\n::: {.callout-note title=\"Health Check Configuration for GPU Inference\"}\n\nGPU inference servers have unique health check considerations:\n\n**GPU memory pressure**: Server may be alive but unable to allocate memory for new requests.\n\n```python\ndef readiness_check():\n    free_memory = (\n        torch.cuda.memory_reserved() - torch.cuda.memory_allocated()\n    )\n    if free_memory < MIN_REQUEST_MEMORY:\n        return {\n            \"status\": \"not_ready\",\n            \"reason\": \"insufficient GPU memory\",\n        }\n    return {\"status\": \"ready\"}\n```\n\n**Model warm-up**: First inference after load is slower. Mark ready only after warm-up.\n\n```python\nasync def startup():\n    model = load_model()\n    # Warm up with dummy requests\n    for _ in range(10):\n        model.generate(dummy_input)\n    # Now mark as ready\n    global ready\n    ready = True\n```\n\n**Timeout configuration**:\n\n| Check Type | Interval | Timeout | Failure Threshold |\n|------------|----------|---------|-------------------|\n| Liveness | 10s | 5s | 3 failures |\n| Readiness | 5s | 3s | 2 failures |\n| Deep health | 30s | 10s | 1 failure |\n\nDeep health checks run less frequently because they consume GPU resources.\n\n:::\n\n### Quantitative Analysis: Load Balancing Impact {#sec-inference-lb-analysis}\n\nThe choice of load balancing algorithm has quantitative impact on system performance. Consider a system with 100 servers, 10,000 QPS, and variable request sizes (CV = 0.5).\n\n| Algorithm | Max Queue | P99 Latency | CPU Overhead |\n|-----------|-----------|-------------|--------------|\n| Random | 4.2 requests | 45ms | Minimal |\n| Round-robin | 2.8 requests | 32ms | Minimal |\n| Two-choices | 1.9 requests | 24ms | 2 probes/request |\n| Least-connections | 1.4 requests | 19ms | Global state |\n| Two-choices + LC | 1.2 requests | 17ms | 2 probes + state |\n\n: **Load Balancing Algorithm Comparison**: More sophisticated algorithms reduce queue lengths and latency at the cost of increased overhead. {#tbl-lb-comparison}\n\nThe progression shows clear tradeoffs:\n\n- Random/round-robin: Zero overhead but higher latency variance\n- Two-choices: Minimal overhead (2 probes), 47% latency improvement\n- Least-connections: State maintenance overhead, 58% latency improvement\n- Combined: Best performance, highest complexity\n\nFor most production systems, two-choices provides the best tradeoff between performance improvement and implementation complexity. Least-connections adds value for workloads with high request size variance (LLM serving, recommendation ranking).\n\n### Circuit Breakers and Backpressure {#sec-inference-circuit-breakers}\n\nWhen servers become overloaded, routing more requests exacerbates the problem. Circuit breakers and backpressure mechanisms protect the system from cascading failures.\n\n**Circuit breaker pattern**:\n\n```\nStates: CLOSED → OPEN → HALF-OPEN → CLOSED\n\nCLOSED: Normal operation, route requests\nOPEN: Server unhealthy, immediately reject requests\nHALF-OPEN: Allow limited requests to test recovery\n\nTransitions:\n\n- CLOSED → OPEN: Error rate exceeds threshold (e.g., 50%)\n- OPEN → HALF-OPEN: After timeout (e.g., 30s)\n- HALF-OPEN → CLOSED: Test requests succeed\n- HALF-OPEN → OPEN: Test requests fail\n```\n\n**Backpressure propagation**: When servers are overloaded, they signal upstream to reduce request rate:\n\n```\nServer queue depth > threshold\n    → Return 503 Service Unavailable\n    → Load balancer marks server as degraded\n    → Routes fewer requests to this server\n    → If all servers degraded, apply admission control\n```\n\n::: {.callout-note title=\"Cascading Failure Prevention\"}\n\nConsider a scenario where one server becomes slow (thermal throttling):\n\n**Without circuit breaker**:\n\n1. Server A slows down (processing 500ms instead of 50ms)\n2. Load balancer continues routing to Server A\n3. Requests queue on Server A, timeouts begin\n4. Retry logic sends failed requests to other servers\n5. Other servers overload from retry traffic\n6. System-wide failure\n\n**With circuit breaker**:\n\n1. Server A slows down\n2. Error rate on Server A rises above 50%\n3. Circuit breaker opens for Server A\n4. All requests route to Servers B, C, D\n5. System operates at reduced capacity but remains stable\n6. After recovery, circuit breaker closes, Server A rejoins\n\n**Configuration for GPU inference**:\n\n| Parameter | Value | Rationale |\n|-----------|-------|-----------|\n| Error threshold | 30% | GPU OOM failures are serious |\n| Latency threshold | 2x baseline | Detect throttling early |\n| Open duration | 60s | GPU recovery takes time |\n| Half-open requests | 5 | Careful testing before full reopening |\n\n:::\n\n## KV Cache Management {#sec-inference-kv-cache}\n\nAutoregressive language models maintain key-value (KV) caches that store attention context from previous tokens, enabling efficient generation without recomputing attention over the entire sequence history. As context lengths grow and serving scales, KV cache management becomes a critical bottleneck. A 70B parameter model with 128K context can require over 100GB just for KV cache, exceeding the model weights themselves.\n\nThis section examines the memory management techniques that enable efficient LLM serving at scale: PagedAttention for fragmentation-free allocation, prefix caching for common prompt sharing, and speculative decoding for latency reduction.\n\n### KV Cache Fundamentals {#sec-inference-kv-cache-fundamentals}\n\nDuring autoregressive generation, each transformer layer computes attention over all previous tokens. Without caching, generating token $t$ would require recomputing attention keys and values for tokens $1$ through $t-1$, an $O(t^2)$ cost per generated token.\n\nThe KV cache stores these computed key and value vectors, reducing generation to $O(t)$ per token. However, this memory savings comes at the cost of storing past context:\n\n$$\\text{KV cache size} = 2 \\times L \\times H \\times S \\times B \\times P$$ {#eq-kv-cache-size}\n\nwhere:\n\n- $L$ = number of layers\n- $H$ = hidden dimension\n- $S$ = sequence length\n- $B$ = batch size\n- $P$ = precision (bytes per element)\n- Factor of 2 accounts for both keys and values\n\n::: {.callout-note title=\"Worked Example: KV Cache Memory for Llama-70B\"}\n\nFor Llama-70B with typical serving configuration:\n\n- Layers ($L$): 80\n- Hidden dimension ($H$): 8,192\n- Context length ($S$): 4,096 tokens\n- Batch size ($B$): 32 concurrent requests\n- Precision ($P$): 2 bytes (FP16)\n\n$$\\text{KV cache} = 2 \\times 80 \\times 8192 \\times 4096 \\times 32 \\times 2 = 344 \\text{ GB}$$\n\nThis exceeds the model weights (140GB) by 2.5x!\n\n**Memory breakdown for single H100 (80GB)**:\n\n| Component | Memory | Percentage |\n|-----------|--------|------------|\n| Model weights | 140GB | Cannot fit on single GPU |\n\n**With 8-way tensor parallelism across 8 H100s (640GB total)**:\n\n| Component | Memory per GPU | Total |\n|-----------|---------------|-------|\n| Model weights | 17.5GB | 140GB |\n| KV cache (theoretical) | 43GB | 344GB |\n| Activations | 4GB | 32GB |\n| **Available for KV** | ~15GB | 120GB |\n\nThe 120GB available KV cache limits concurrent batch size to ~11 requests at 4K context, not 32.\n\n**Implication**: KV cache capacity, not compute, limits LLM serving throughput for long contexts.\n\n:::\n\n### The Fragmentation Problem {#sec-inference-kv-fragmentation}\n\nTraditional memory allocation for KV cache pre-allocates contiguous memory for each sequence based on maximum expected length. This creates two forms of waste:\n\n**Internal fragmentation**: Sequences shorter than the maximum allocation waste the unused portion. If maximum length is 4,096 but average output is 100 tokens, 97.5% of allocated memory is wasted.\n\n**External fragmentation**: As sequences complete and new ones start, memory becomes fragmented into non-contiguous free blocks. Even with sufficient total free memory, no single block may be large enough for a new maximum-length allocation.\n\nConsider a simplified example with 8 memory slots and maximum sequence length of 4:\n\n```\nTime 0: Allocate Seq A (slots 0-3), Seq B (slots 4-7)\n        [A][A][A][A][B][B][B][B]\n\nTime 1: Seq A completes (2 tokens), Seq B continues\n        [ ][ ][A][A][B][B][B][ ]  <- A only used 2 slots\n\nTime 2: Try to allocate Seq C (needs 4 slots)\n        [ ][ ][A][A][B][B][B][ ]  <- No contiguous block of 4!\n\nResult: 4 free slots but cannot allocate new sequence\n```\n\nProduction systems report 60-80% memory waste from fragmentation under realistic workloads, severely limiting batch sizes and throughput.\n\n### PagedAttention {#sec-inference-paged-attention}\n\nPagedAttention, introduced in vLLM, applies virtual memory concepts to KV cache management. Instead of contiguous allocation, the KV cache is divided into fixed-size pages (typically 16-256 tokens), and sequences are allocated pages on demand.\n\n**Key concepts**:\n\n**Page table**: Maps logical sequence positions to physical memory pages.\n\n**Block size**: Number of tokens per page (typically 16 tokens).\n\n**Physical blocks**: Fixed-size memory allocations that can be assigned to any sequence.\n\n```\nSequence A (150 tokens, 10 pages allocated):\nLogical:  [Page 0][Page 1][Page 2]...[Page 9]\nPhysical: [Block 5][Block 12][Block 3]...[Block 8]\n\nSequence B (80 tokens, 5 pages allocated):\nLogical:  [Page 0][Page 1][Page 2][Page 3][Page 4]\nPhysical: [Block 1][Block 7][Block 15][Block 2][Block 11]\n```\n\n**Benefits**:\n\n1. **No internal fragmentation**: Allocate only the pages needed for actual tokens\n2. **No external fragmentation**: Any free page can be used by any sequence\n3. **Dynamic growth**: Sequences can grow without pre-allocation\n4. **Memory sharing**: Common prefixes can share physical pages\n\n::: {.callout-note title=\"PagedAttention Implementation Details\"}\n\n**Memory layout**:\n\n```\nPhysical blocks (16 tokens × hidden_dim × 2 × precision):\nBlock 0:  [K₀...K₁₅, V₀...V₁₅]\nBlock 1:  [K₀...K₁₅, V₀...V₁₅]\n...\nBlock N:  [K₀...K₁₅, V₀...V₁₅]\n```\n\n**Page table per sequence**:\n\n```python\nclass PageTable:\n    def __init__(self, max_blocks):\n        self.block_map = {}  # logical_block -> physical_block\n\n    def allocate_block(self, logical_idx, physical_block):\n        self.block_map[logical_idx] = physical_block\n\n    def get_physical(self, logical_idx):\n        return self.block_map[logical_idx]\n```\n\n**Attention kernel modification**:\n\nStandard attention: `output = softmax(Q @ K.T / sqrt(d)) @ V`\n\nPagedAttention:\n```python\ndef paged_attention(Q, page_table, physical_blocks, block_size):\n    # Gather K, V from non-contiguous physical blocks\n    for logical_idx in range(num_logical_blocks):\n        physical_idx = page_table[logical_idx]\n        K_block = physical_blocks[physical_idx].K\n        V_block = physical_blocks[physical_idx].V\n        # Compute attention for this block\n        attention_scores = Q @ K_block.T / sqrt(d)\n        output += softmax(attention_scores) @ V_block\n    return output\n```\n\n**Performance impact**:\n\nThe gather operations add overhead, but it is minimal compared to the memory savings:\n\n| Approach | Memory Utilization | Throughput (relative) |\n|----------|-------------------|----------------------|\n| Contiguous | 30-40% | 1.0x (baseline) |\n| PagedAttention | 95%+ | 2.5-4x |\n\nThe 2.5-4x throughput improvement comes from fitting more concurrent sequences in the same memory.\n\n:::\n\n### Prefix Caching {#sec-inference-prefix-caching}\n\nMany LLM workloads share common prefixes across requests:\n\n- **System prompts**: \"You are a helpful assistant...\" prepended to every request\n- **Few-shot examples**: Same examples used for many queries\n- **Document context**: Multiple questions about the same document\n\nRecomputing these shared prefixes wastes both compute (prefill) and memory (duplicate KV cache entries).\n\n**Prefix caching** shares KV cache entries across requests with common prefixes:\n\n```\nRequest A: [System prompt (500 tokens)] + [User query A (50 tokens)]\nRequest B: [System prompt (500 tokens)] + [User query B (75 tokens)]\nRequest C: [System prompt (500 tokens)] + [User query C (30 tokens)]\n\nWithout prefix caching:\n\n- 3 × 500 = 1500 tokens of prefill compute\n- 3 × 500 tokens of KV cache storage\n\nWith prefix caching:\n\n- 500 tokens of prefill compute (cached)\n- 500 tokens of KV cache storage (shared)\n- 155 tokens of unique prefill compute\n- 155 tokens of unique KV cache storage\n```\n\n**Implementation with PagedAttention**:\n\nPrefix caching integrates naturally with PagedAttention through copy-on-write semantics:\n\n```\nSystem prompt → Physical blocks [0, 1, 2, 3, 4, 5]\n\nRequest A page table: [0, 1, 2, 3, 4, 5, 10, 11]  <- shares prefix blocks\nRequest B page table: [0, 1, 2, 3, 4, 5, 12, 13, 14]  <- shares prefix blocks\nRequest C page table: [0, 1, 2, 3, 4, 5, 15]  <- shares prefix blocks\n```\n\nAll three requests reference the same physical blocks for the system prompt. Only when generating unique tokens do they allocate new blocks.\n\n::: {.callout-note title=\"Prefix Caching at Scale\"}\n\nConsider a chatbot service with a 2000-token system prompt and 1000 concurrent users:\n\n**Without prefix caching**:\n\n- KV cache per user: 2000 + 500 (avg response) = 2500 tokens\n- Total KV cache: 2500 × 1000 × 2 × 80 × 8192 × 2 = 6.5 TB\n\n**With prefix caching**:\n\n- Shared prefix: 2000 tokens (once)\n- Unique per user: 500 tokens\n- Total: (2000 × 1) + (500 × 1000) = 502,000 tokens\n- Memory: 502,000 × 2 × 80 × 8192 × 2 = 1.3 TB\n\n**Savings**: 80% reduction in KV cache memory, enabling 5x more concurrent users.\n\n**Prefix hit rate** determines effectiveness:\n\n| Workload | Prefix Hit Rate | Memory Savings |\n|----------|-----------------|----------------|\n| Chatbot (same system prompt) | 95%+ | 70-80% |\n| Document QA (same doc) | 80-90% | 50-70% |\n| General API (diverse) | 20-40% | 10-30% |\n\n:::\n\n### KV Cache Compression {#sec-inference-kv-compression}\n\nBeyond efficient allocation, reducing the size of cached values provides additional memory savings. Several techniques compress the KV cache:\n\n**Quantization**: Store cached keys and values at reduced precision.\n\n$$\\text{Compressed size} = \\text{Original size} \\times \\frac{b_{compressed}}{b_{original}}$$\n\n| Precision | Memory per Token | Quality Impact |\n|-----------|------------------|----------------|\n| FP16 (baseline) | 2 bytes | None |\n| FP8 | 1 byte | <1% degradation |\n| INT8 | 1 byte | 1-2% degradation |\n| INT4 | 0.5 bytes | 3-5% degradation |\n\n**Key observation**: KV cache values are more tolerant of quantization than model weights because they are intermediate activations, not learned parameters.\n\n**Sliding window attention**: For very long contexts, maintain full cache only for recent tokens:\n\n```\nFull context: 100,000 tokens\nSliding window: 4,096 tokens\n\nCache strategy:\n\n- Tokens 0-95,904: Discarded or compressed\n- Tokens 95,904-100,000: Full precision cache\n\nTrade-off: Cannot attend to very old tokens, but sufficient for most tasks.\n```\n\n**Grouped-query attention (GQA)**: Architectural change that reduces KV cache by sharing key-value heads:\n\n| Attention Type | KV Heads | Cache Size (relative) |\n|---------------|----------|----------------------|\n| Multi-head (MHA) | 64 | 1.0x |\n| Grouped-query (GQA) | 8 | 0.125x |\n| Multi-query (MQA) | 1 | 0.016x |\n\nModern models like Llama 2 and Mistral use GQA specifically to reduce KV cache requirements.\n\n### Speculative Decoding {#sec-inference-speculative-decoding}\n\nAutoregressive generation is inherently sequential: each token depends on previous tokens. Speculative decoding breaks this bottleneck by using a smaller draft model to predict multiple tokens, then verifying them in parallel with the target model.\n\n**Algorithm**:\n\n1. Draft model generates $k$ tokens speculatively: $t_1, t_2, ..., t_k$\n2. Target model verifies all $k$ tokens in a single forward pass\n3. Accept prefix of correct tokens, reject from first incorrect token\n4. Continue from last accepted token\n\n**Why this works**: The draft model is much smaller (7B vs 70B) and can generate $k$ tokens in the time the target model generates 1 token. Verification is cheap because the target model can process all $k$ tokens in parallel (like prefill).\n\n::: {.callout-note title=\"Speculative Decoding Example\"}\n\n**Target model**: Llama-70B (30 tokens/second)\n**Draft model**: Llama-7B (300 tokens/second)\n**Speculation length**: $k = 4$ tokens\n\n**Scenario**: Generating \"The quick brown fox jumps\"\n\n```\nStep 1: Draft model generates 4 tokens\n        \"The\" → [quick, brown, fox, jumps]\n        Time: 4/300 = 13ms\n\nStep 2: Target model verifies in parallel\n        Input: \"The quick brown fox jumps\"\n        Accepts: \"quick\", \"brown\" (match)\n        Rejects: \"fox\" → target predicted \"lazy\"\n        Time: 1/30 = 33ms\n\nStep 3: Output \"quick brown\", continue from \"brown\"\n        Effective tokens: 2 in 46ms = 43 tokens/second\n\nStep 4: Repeat from \"brown\"\n```\n\n**Effective speedup**: 43/30 = 1.43x\n\n**Factors affecting speedup**:\n\n| Acceptance Rate | Speedup |\n|-----------------|---------|\n| 90% (easy text) | 2.5-3x |\n| 70% (typical) | 1.5-2x |\n| 50% (hard text) | 1.2-1.5x |\n| 30% (very hard) | <1x (overhead) |\n\nSpeedup depends on how well the draft model predicts the target model's output. For well-aligned model pairs (same training data, similar architecture), acceptance rates of 70-80% are common.\n\n:::\n\n**Self-speculative decoding** uses early exit from the target model itself as the draft, avoiding the need for a separate model:\n\n```\nTarget model layers: 80\n\nDraft: Layers 1-20 → predict next token\nVerify: Layers 1-80 → confirm or reject\n```\n\nThis eliminates the need to load and manage a separate draft model, at the cost of lower acceptance rates than a dedicated draft model.\n\n### KV Cache in Distributed Settings {#sec-inference-kv-cache-distributed}\n\nWhen models are sharded across devices (see @sec-inference-sharding), KV cache management gains additional complexity:\n\n**Tensor parallelism**: KV cache is sharded across devices along with attention heads. Each device stores cache for its subset of heads.\n\n```\n8-way tensor parallelism:\nDevice 0: KV cache for heads 0-7\nDevice 1: KV cache for heads 8-15\n...\nDevice 7: KV cache for heads 56-63\n```\n\n**Cross-device sharing**: Prefix caching across tensor-parallel devices requires cache to be sharded identically on all devices. This is automatic when prefixes are processed with the same tensor-parallel configuration.\n\n**KV cache migration**: When consistent hashing routes a conversation to a different replica (due to failure or rebalancing), the KV cache must be migrated:\n\n```\nMigration options:\n1. Rebuild: Re-run prefill on new replica (500ms+ for long context)\n2. Transfer: Send KV cache over network (100MB at 100Gbps = 8ms)\n3. Hybrid: Transfer if small, rebuild if large\n\nDecision threshold:\nif cache_size_bytes / network_bandwidth < prefill_time:\n    transfer()\nelse:\n    rebuild()\n```\n\nFor Llama-70B with 4K context, KV cache is ~80MB per sequence. At 100 Gbps, transfer takes 6.4ms versus ~500ms for prefill. Transfer is clearly better.\n\n### Memory Management Best Practices {#sec-inference-kv-best-practices}\n\nEffective KV cache management combines multiple techniques:\n\n**Sizing the KV cache pool**:\n\n```\nAvailable GPU memory = Total - Weights - Activations - Overhead\nKV pool size = 0.9 × Available  # Leave 10% headroom\n\nMax concurrent sequences = KV pool size / (avg_seq_length × per_token_cache)\n```\n\n**Eviction policies** when cache is full:\n\n- **LRU (Least Recently Used)**: Evict sequences with oldest last access\n- **Size-based**: Evict longest sequences first (free most memory)\n- **Priority-based**: Protect high-priority or paid-tier requests\n\n**Preemption** for continuous batching:\n\nWhen a new high-priority request cannot fit:\n\n1. Select victim sequence(s) using eviction policy\n2. Swap victim's KV cache to CPU memory\n3. Allocate GPU memory to new request\n4. When victim is resumed, swap back from CPU\n\n::: {.callout-note title=\"KV Cache Memory Hierarchy\"}\n\nProduction systems use a memory hierarchy for KV cache:\n\n| Tier | Capacity | Latency | Use Case |\n|------|----------|---------|----------|\n| GPU HBM | 80GB | 0ms | Active sequences |\n| CPU DRAM | 1TB | 1-5ms | Swapped sequences |\n| NVMe SSD | 10TB | 10-50ms | Long-term cache |\n\n**Swap implementation**:\n\n```python\nasync def swap_to_cpu(sequence_id):\n    kv_cache = gpu_cache[sequence_id]\n    cpu_cache[sequence_id] = kv_cache.cpu()  # Async transfer\n    gpu_cache.free(sequence_id)\n\n\nasync def swap_to_gpu(sequence_id):\n    cpu_kv = cpu_cache[sequence_id]\n    gpu_cache[sequence_id] = cpu_kv.cuda()  # Async transfer\n    cpu_cache.free(sequence_id)\n```\n\n**Observed performance**:\n\n- GPU-only (no swapping): 50 concurrent sequences\n- GPU+CPU swapping: 500 concurrent sequences (10x)\n- Average swap latency: 3ms (acceptable for non-urgent requests)\n\n:::\n\n## Weight Quantization for Serving {#sec-inference-weight-quantization}\n\n::: {.callout-note title=\"Building on Volume I\"}\nThis section assumes familiarity with quantization fundamentals covered in @sec-model-optimizations-quantization-precision-optimization-e90a. Here we focus on production inference-specific quantization challenges: post-training methods designed for LLM deployment, hardware-deployment co-design, and the interaction between quantization and serving system design.\n:::\n\nQuantization reduces numerical precision of model weights and activations, decreasing memory footprint and increasing throughput. While Volume I covers quantization fundamentals including post-training quantization (PTQ), quantization-aware training (QAT), and precision format tradeoffs, serving at scale introduces distinct challenges. Models must be quantized after training without access to training data. Quantization must preserve quality across diverse inputs. Hardware deployment targets vary from datacenter GPUs to edge accelerators. This section examines quantization techniques specifically designed for production inference.\n\n### LLM-Specific Quantization Challenges {#sec-inference-llm-quantization}\n\nLarge language models present unique quantization challenges distinct from vision or recommendation models. The outlier activation problem occurs because certain attention heads produce activation magnitudes orders of magnitude larger than typical values. Naive quantization clips these outliers, causing significant quality degradation.\n\nConsider a Llama-70B layer where most activations fall within [-10, 10] but specific channels reach magnitudes of 1000+. Symmetric INT8 quantization with range [-127, 127] must choose:\n\n- **Wide range** [-1000, 1000]: Most values map to 0, losing information\n- **Narrow range** [-10, 10]: Outliers clip, causing large errors\n\nThis outlier distribution motivates the specialized quantization methods that follow.\n\n### GPTQ: Layer-by-Layer Weight Quantization {#sec-inference-gptq}\n\nGPTQ (Generalized Post-Training Quantization) quantizes LLM weights using Hessian-based error compensation. Rather than quantizing all weights independently, GPTQ adjusts remaining weights to compensate for errors introduced by quantization.\n\n**Algorithm overview**:\n\n1. Process model layer by layer\n2. For each layer, calibrate using small dataset (128-256 samples)\n3. Quantize weights in order of decreasing Hessian magnitude\n4. Adjust remaining weights to minimize output error\n\n**Key insight**: The Hessian matrix $H = X^T X$ captures which weights most affect outputs. GPTQ builds on the Optimal Brain Surgeon (OBS) framework, using the inverse Hessian to guide error compensation.\n\n**Column-wise processing algorithm**:\n\nGPTQ processes each weight matrix column by column, using Cholesky decomposition of $H^{-1}$ for efficiency:\n\n```\nFor each layer's weight matrix W:\n  1. Compute Hessian: H = X^T X from calibration activations\n  2. Apply Cholesky factorization to H^{-1}\n  3. For each column q = 1 to d_col:\n     a. Quantize column: w_q = round(W[:,q] / Δ) × Δ\n     b. Compute quantization error: δ = W[:,q] - w_q\n     c. Update remaining columns to compensate:\n        W[:,q+1:] += δ × [H^{-1}][:,q+1:] / [H^{-1}]_{qq}\n```\n\nThe compensation step propagates quantization error to unquantized columns, where the Hessian-weighted update minimizes output deviation. This achieves $O(d_{row} \\cdot d_{col}^2)$ complexity versus $O(d_{row} \\cdot d_{col}^3)$ for naive OBS.\n\n**Quantization formula**:\n\n$$w_q = \\text{round}\\left(\\frac{w}{\\Delta}\\right) \\cdot \\Delta$$\n\nwhere $\\Delta$ is the quantization step size. GPTQ uses per-group quantization with group sizes of 128 weights, enabling finer-grained scaling that reduces quantization error for outlier channels.\n\n**Performance characteristics**:\n\n| Model | Bits | Perplexity Increase | Memory Reduction | Quantization Time |\n|-------|------|---------------------|------------------|-------------------|\n| Llama-7B | 4 | +0.3 | 4x | 15 min |\n| Llama-13B | 4 | +0.2 | 4x | 30 min |\n| Llama-70B | 4 | +0.15 | 4x | 3 hours |\n\nGPTQ's strengths include fast quantization without retraining, minimal quality loss for 4-bit weights, and broad hardware compatibility. Its limitations include requiring calibration data, sensitivity to calibration set selection, and per-layer processing that cannot leverage cross-layer information.\n\n### AWQ: Activation-Aware Weight Quantization {#sec-inference-awq}\n\nAWQ (Activation-Aware Weight Quantization) observes that not all weights are equally important. Weights connected to channels with large activation magnitudes have disproportionate impact on outputs.\n\n**Key insight**: Rather than protecting weights based on their own magnitude, protect weights based on the magnitude of activations they produce.\n\n**Algorithm**:\n\n1. Run calibration samples to measure per-channel activation magnitudes\n2. Identify \"salient\" channels with large activations\n3. Scale weights for salient channels up before quantization\n4. Scale outputs down correspondingly (fused into subsequent layer)\n\n**Scaling formulation**:\n\nFor weight matrix $W$ and activation statistics $s$ (per-channel activation magnitudes):\n\n$$W' = W \\cdot \\text{diag}(s^\\alpha)$$\n\nwhere $\\alpha \\in [0.5, 1.0]$ controls scaling aggressiveness. This preserves salient channels while allowing aggressive quantization of less important weights.\n\n**Comparison with GPTQ**:\n\n| Aspect | GPTQ | AWQ |\n|--------|------|-----|\n| Error compensation | Adjusts remaining weights | Scales salient channels |\n| Calibration data | 128-256 samples | 128 samples |\n| Quality (4-bit) | Very good | Excellent |\n| Speed | Faster | Slightly slower |\n| Hardware compatibility | Broad | Broad |\n\nAWQ typically achieves 0.5-1% lower perplexity degradation than GPTQ at the same bit-width, making it preferred for production deployments where quality is paramount.\n\n### SmoothQuant: Migrating Quantization Difficulty {#sec-inference-smoothquant}\n\nSmoothQuant addresses the activation outlier problem by migrating quantization difficulty from activations to weights. Weights have predictable distributions; activations have unpredictable outliers. SmoothQuant transfers the outlier problem to weights where it can be handled with per-channel scaling.\n\n**Core technique**: Insert smoothing operations that divide activations by per-channel scales while multiplying weights by the same scales:\n\n$$Y = X W = (X \\cdot \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s) \\cdot W) = \\hat{X} \\hat{W}$$\n\nThis transformation is mathematically equivalent but produces smoother activation distributions.\n\n**Migration strength** $\\alpha$ controls the trade-off:\n\n$$s_j = \\max(|X_j|)^\\alpha / \\max(|W_j|)^{1-\\alpha}$$\n\n- $\\alpha = 0$: No migration, activations remain difficult\n- $\\alpha = 1$: Full migration, weights absorb all difficulty\n- $\\alpha = 0.5$: Balanced (typical setting)\n\n**W8A8 deployment**: SmoothQuant enables INT8 quantization for both weights and activations:\n\n| Configuration | Memory | Prefill Speedup | Decode Speedup | Quality |\n|---------------|--------|-----------------|----------------|---------|\n| FP16 (baseline) | 1x | 1x | 1x | Baseline |\n| W8A16 (weights only) | 2x | 1.3x | 1.8x | <0.5% loss |\n| W8A8 (SmoothQuant) | 2x | 1.8-2x | 1.3-1.5x | <1% loss |\n\n**Critical distinction**: W8A8 provides near 2x speedup for compute-bound prefill (large batch processing initial prompt), but only 1.3-1.5x speedup for memory-bound decode (generating tokens one at a time). LLM serving is typically decode-heavy, so real-world throughput improvements from W8A8 are often 1.3-1.7x rather than the theoretical 2x compute throughput of INT8 Tensor Cores.\n\n### KV Cache Quantization {#sec-inference-kv-cache-quantization}\n\nWhile weight quantization reduces model storage, the KV cache dominates memory consumption for long-context LLM serving. At 32K+ token context lengths, KV cache can exceed model weights in memory usage. KV cache quantization addresses this critical bottleneck.\n\n**KV cache memory scaling**:\n\n$$\\text{KV Cache} = 2 \\times \\text{layers} \\times \\text{heads} \\times d_{head} \\times \\text{seq\\_len} \\times \\text{batch} \\times \\text{bytes}$$\n\nFor a 70B model (80 layers, 64 heads, 128 $d_{head}$) with 32K context in FP16:\n\n$$\\text{KV per sequence} = 2 \\times 80 \\times 64 \\times 128 \\times 32768 \\times 2 = 85.9 \\text{ GB}$$\n\nA single long-context sequence consumes more memory than the model weights.\n\n**Key insight**: KV cache values exhibit different distributions than model weights, enabling targeted quantization strategies:\n\n- **Keys**: Relatively uniform distributions, tolerate aggressive quantization (2-4 bits)\n- **Values**: More sensitive to quantization, require careful calibration (4-6 bits)\n\n**KIVI (Key-Value cache quantization for Inference)**:\n\nKIVI quantizes keys and values asymmetrically based on their sensitivity:\n\n| Component | Precision | Grouping | Quality Impact |\n|-----------|-----------|----------|----------------|\n| Keys | 2-bit | Per-channel | Minimal |\n| Values | 4-bit | Per-token | <0.5% perplexity |\n| Combined | ~3-bit average | Mixed | <1% perplexity |\n\n**Memory impact of combined weight and KV quantization**:\n\n| Configuration | Weight Size | KV Cache (32K) | Total Memory |\n|---------------|-------------|----------------|--------------|\n| FP16/FP16 | 140GB | 86GB | 226GB |\n| W4A16/FP16 | 35GB | 86GB | 121GB |\n| W4A16/KV4 | 35GB | 21GB | 56GB |\n\nKV cache quantization enables 4x longer contexts or 4x higher batch sizes on the same hardware.\n\n**Integration with PagedAttention**: Quantized KV cache requires quantization-aware block management:\n\n```python\n# Conceptual: vLLM with KV cache quantization\nfrom vllm import LLM\n\nllm = LLM(\n    model=\"meta-llama/Llama-2-70b-hf\",\n    kv_cache_dtype=\"fp8\",  # FP8 E4M3 for KV cache\n    quantization=\"awq\",  # 4-bit weights\n)\n```\n\nWith FP8 KV cache, the per-sequence memory drops from 2.5MB to 1.25MB per 1K tokens, doubling maximum concurrent sequences.\n\n### Rotation-Based Quantization {#sec-inference-rotation-quantization}\n\nTraditional quantization methods (GPTQ, AWQ, SmoothQuant) address outliers through compensation or migration. **Rotation-based quantization** takes a fundamentally different approach: mathematically transforming the weight and activation space to eliminate outliers entirely.\n\n**Key insight**: Outliers are artifacts of the coordinate basis representation. Rotating to a different basis spreads extreme values uniformly, making all values quantization-friendly.\n\n**QuaRot (Quantization with Rotation)**:\n\nQuaRot applies orthogonal Hadamard transforms to weights and activations:\n\n$$X' = X \\cdot H, \\quad W' = H^T \\cdot W$$\n\nwhere $H$ is a Hadamard matrix (orthogonal, efficiently computable without storage).\n\n**Why Hadamard transforms work**: For a vector with one extreme outlier, the Hadamard transform distributes that outlier's magnitude across all dimensions:\n\n```\nOriginal:  [1000, 1, 1, 1]     # One large outlier\nHadamard:  [252, 250, 250, 250] # Uniform distribution\n```\n\nAfter transformation, no single value dominates, enabling uniform quantization.\n\n**Advantages over migration-based methods**:\n\n| Aspect | SmoothQuant | QuaRot |\n|--------|-------------|--------|\n| Calibration data | Required | Not required (data-free) |\n| Minimum precision | W8A8 | W4A4 |\n| Runtime overhead | ~0% | ~3% (Hadamard transforms) |\n| Outlier handling | Migration | Elimination |\n\n**Performance comparison**:\n\n| Method | Precision | LLaMA-2-70B Perplexity | vs Baseline |\n|--------|-----------|------------------------|-------------|\n| FP16 | W16A16 | 3.12 | Baseline |\n| SmoothQuant | W8A8 | 3.18 | +0.06 |\n| GPTQ | W4A16 | 3.24 | +0.12 |\n| QuaRot | W4A4 | 3.31 | +0.19 |\n\nQuaRot achieves 4-bit weights AND 4-bit activations with quality competitive to GPTQ's 4-bit weights only. This enables approximately 8x memory reduction versus FP16.\n\n**SpinQuant**: An extension of QuaRot that learns optimal rotation matrices during a short fine-tuning phase, improving quality at the cost of training compute.\n\n### Hardware-Deployment Co-design {#sec-inference-quant-hardware}\n\nQuantization strategies must match target hardware capabilities. Different accelerators support different precisions with varying performance multipliers.\n\n**NVIDIA Tensor Core Support**:\n\n| Format | Ampere (A100) | Hopper (H100) | Speedup vs FP16 |\n|--------|---------------|---------------|-----------------|\n| FP16 | Yes | Yes | 1x |\n| BF16 | Yes | Yes | 1x |\n| INT8 | Yes | Yes | 2x |\n| FP8 (E4M3) | No | Yes | 2x |\n| INT4 | Via CUTLASS | Native | 4x |\n\n**Memory bandwidth dominance**: For autoregressive LLM decode, memory bandwidth limits throughput since each token reads the entire model:\n\n$$\\text{Decode throughput} \\propto \\frac{\\text{Memory bandwidth}}{\\text{Model size in bytes}}$$\n\n4-bit quantization delivers 4x throughput improvement for memory-bound decode, making it highly valuable despite modest compute gains.\n\n**Deployment configurations**:\n\n| Quantization | Best For | Framework Support |\n|--------------|----------|-------------------|\n| W4A16 (GPTQ/AWQ) | Consumer GPUs, memory-constrained | vLLM, TensorRT-LLM, llama.cpp |\n| W8A8 (SmoothQuant) | INT8 accelerators, high throughput | TensorRT-LLM, ONNX Runtime |\n| FP8 | H100/H200 deployments | TensorRT-LLM |\n| W4A4 | Research, extreme compression | Limited |\n\n### Framework Integration {#sec-inference-quant-frameworks}\n\nProduction serving frameworks integrate quantization with their batching and memory management systems.\n\n**vLLM quantization**:\n\n```python\nfrom vllm import LLM\n\n# Load AWQ-quantized model\nllm = LLM(\n    model=\"TheBloke/Llama-2-70B-AWQ\",\n    quantization=\"awq\",\n    dtype=\"float16\",  # Activations in FP16\n    gpu_memory_utilization=0.9,\n)\n```\n\nvLLM automatically handles quantized weight loading, kernel selection for W4A16 GEMM operations, and KV cache management.\n\n**TensorRT-LLM quantization**:\n\n```bash\n# Quantize model with AWQ\npython quantize.py --model_dir /path/to/llama-70b \\\n                   --output_dir /path/to/llama-70b-awq \\\n                   --qformat int4_awq \\\n                   --calib_size 512\n```\n\nTensorRT-LLM generates optimized kernels for specific GPU architectures, fuses operations to minimize memory traffic, and supports both weight-only and W8A8 quantization.\n\n**Quantization + PagedAttention**: Quantized models combine with PagedAttention for maximum memory efficiency:\n\n$$\\text{Max batch} = \\frac{\\text{GPU Memory} - \\text{Quantized Weights}}{\\text{KV Cache per Sequence}}$$\n\nA 70B model with 4-bit weights requires approximately 35GB, leaving 45GB on an 80GB A100 for KV cache. With FP16 KV cache at 2.5MB per 1K tokens per sequence, this supports ~18 concurrent 1K-token sequences versus ~8 with FP16 weights.\n\n### Quantization Selection Guidelines {#sec-inference-quant-selection}\n\nChoosing the appropriate quantization method depends on deployment constraints:\n\n**Decision framework**:\n\n```\n1. Is latency or throughput the primary goal?\n   - Latency-sensitive: Prefer FP16/BF16 (no quantization overhead)\n   - Throughput-oriented: Quantization typically beneficial\n\n2. What hardware is available?\n   - H100/H200: Consider FP8 (native support, minimal quality loss)\n   - A100/A10G: W8A8 or W4A16 depending on workload\n   - Consumer GPUs: W4A16 often necessary for memory\n\n3. What quality requirements exist?\n   - <0.5% degradation acceptable: AWQ 4-bit\n   - <1% degradation acceptable: GPTQ 4-bit or SmoothQuant W8A8\n   - No degradation acceptable: FP16/BF16 only\n\n4. Is the model compute-bound or memory-bound?\n   - Compute-bound (prefill): W8A8 provides 2x speedup\n   - Memory-bound (decode): W4A16 provides 4x memory bandwidth\n```\n\n**Quantization impact on serving cost**:\n\n| Configuration | Cost per 1M tokens (estimated) |\n|---------------|--------------------------------|\n| FP16 on 8xA100 | $2.40 |\n| AWQ 4-bit on 4xA100 | $1.20 |\n| AWQ 4-bit on 2xA100 | $0.60 |\n\nQuantization can reduce serving costs by 2-4x while maintaining acceptable quality, making it essential for cost-effective LLM deployment.\n\n## Multi-Tenancy and Isolation {#sec-inference-multitenancy}\n\nProduction inference platforms serve multiple customers, models, and workloads on shared infrastructure. Multi-tenancy enables efficient resource utilization but introduces challenges around isolation, fairness, and quality of service guarantees. A noisy neighbor consuming excessive resources can degrade performance for all other tenants.\n\nThis section examines the techniques for sharing inference infrastructure while maintaining isolation between tenants.\n\n### The Multi-Tenancy Challenge {#sec-inference-multitenancy-challenge}\n\nMulti-tenancy provides significant benefits:\n\n- **Cost efficiency**: Sharing infrastructure across tenants improves utilization\n- **Operational simplicity**: Fewer clusters to manage, monitor, and upgrade\n- **Statistical multiplexing**: Aggregate traffic is more predictable than per-tenant traffic\n\nHowever, sharing introduces risks:\n\n- **Noisy neighbors**: One tenant's burst traffic impacts others\n- **Resource contention**: GPU memory, network bandwidth, CPU cycles\n- **Security boundaries**: Tenant data must remain isolated\n- **SLO complexity**: Different tenants have different requirements\n\n| Aspect | Single-Tenant | Multi-Tenant |\n|--------|--------------|--------------|\n| Resource utilization | 30-50% | 70-90% |\n| Cost per request | Higher | Lower |\n| SLO guarantees | Simple | Complex |\n| Isolation | Complete | Requires engineering |\n| Operational overhead | Higher (many clusters) | Lower (fewer clusters) |\n\n: **Single vs Multi-Tenant Tradeoffs**: Multi-tenancy reduces cost but requires careful isolation engineering. {#tbl-tenancy-comparison}\n\n### Noisy Neighbor Problems {#sec-inference-noisy-neighbor}\n\nThe noisy neighbor problem occurs when one tenant's workload degrades performance for others sharing the same infrastructure.\n\n**GPU memory contention**: A tenant with unexpectedly long sequences consumes KV cache memory, forcing evictions that impact other tenants.\n\n```\nScenario: 3 tenants sharing GPU with 60GB KV cache pool\n\nNormal state:\n  Tenant A: 20GB (200 sequences)\n  Tenant B: 20GB (200 sequences)\n  Tenant C: 20GB (200 sequences)\n\nNoisy neighbor (Tenant C starts long-context requests):\n  Tenant C: 45GB (150 sequences, longer context)\n  Tenant A: 7.5GB (evicted to 75 sequences)\n  Tenant B: 7.5GB (evicted to 75 sequences)\n\nImpact: Tenants A and B see 62% reduction in batch size\n```\n\n**Network bandwidth saturation**: A tenant streaming many large responses saturates network bandwidth, increasing latency for all tenants.\n\n**Compute interference**: GPU time-sharing between tenants introduces context-switching overhead and unpredictable latency.\n\n::: {.callout-note title=\"Quantifying Noisy Neighbor Impact\"}\n\nConsider an inference platform serving 10 tenants on shared H100 GPUs:\n\n**Baseline (even load)**:\n\n- Each tenant: 100 QPS, 10ms P99 latency\n- GPU utilization: 70%\n- All SLOs met\n\n**Noisy neighbor scenario** (Tenant 3 bursts to 500 QPS):\n\n| Tenant | QPS | P99 Latency | SLO Status |\n|--------|-----|-------------|------------|\n| Tenant 1 | 100 | 25ms | Violated |\n| Tenant 2 | 100 | 28ms | Violated |\n| Tenant 3 | 500 | 45ms | Violated |\n| Tenants 4-10 | 100 each | 22-30ms | Violated |\n\nWithout isolation, one tenant's burst causes cascade failures for all tenants.\n\n**With isolation** (per-tenant resource quotas):\n\n| Tenant | QPS (actual) | P99 Latency | SLO Status |\n|--------|-------------|-------------|------------|\n| Tenant 1 | 100 | 11ms | Met |\n| Tenant 2 | 100 | 11ms | Met |\n| Tenant 3 | 120 (throttled) | 50ms | Violated (only for them) |\n| Tenants 4-10 | 100 each | 11ms | Met |\n\nIsolation contains the impact to the offending tenant.\n\n:::\n\n### Resource Quotas and Fair Sharing {#sec-inference-quotas}\n\nResource quotas limit what each tenant can consume, preventing any single tenant from monopolizing shared resources.\n\n**Hard quotas** enforce strict limits:\n\n```python\nclass TenantQuota:\n    max_concurrent_requests: int  # e.g., 100\n    max_kv_cache_mb: int  # e.g., 20,000\n    max_qps: int  # e.g., 1,000\n    max_batch_tokens: int  # e.g., 50,000\n\n\ndef admit_request(tenant_id, request):\n    quota = get_quota(tenant_id)\n    usage = get_usage(tenant_id)\n\n    if usage.concurrent >= quota.max_concurrent:\n        return RateLimitError(\"concurrent request limit\")\n    if usage.kv_cache_mb >= quota.max_kv_cache_mb:\n        return RateLimitError(\"memory limit\")\n    if usage.qps >= quota.max_qps:\n        return RateLimitError(\"rate limit\")\n\n    return admit(request)\n```\n\n**Soft quotas** with fair sharing allow exceeding limits when resources are available:\n\n```\nTenant quota: 100 QPS (soft limit)\n\nWhen cluster is underutilized (50%):\n  Tenant can burst to 200 QPS (2x quota)\n\nWhen cluster is saturated (90%):\n  Tenant limited to 100 QPS (quota enforced)\n```\n\nThis approach maximizes utilization while protecting tenants during contention.\n\n**Max-min fairness** allocates resources to maximize the minimum allocation:\n\n```\nTotal capacity: 1000 QPS\nTenants: A (demand 300), B (demand 200), C (demand 800)\nTotal demand: 1300 QPS (exceeds capacity)\n\nMax-min allocation:\n1. Give each tenant equal share: 333 QPS\n2. A needs only 300, donate 33 to others\n3. B needs only 200, donate 133 to others\n4. C receives donations: 333 + 33 + 133 = 499 QPS\n\nFinal: A=300, B=200, C=500\nAll demands met up to fair share, C limited proportionally\n```\n\n### Priority Scheduling {#sec-inference-priority-scheduling}\n\nWhen tenants have different SLO requirements, priority scheduling ensures high-priority requests receive resources first.\n\n**Priority classes**:\n\n| Class | Use Case | Preemption | Resource Guarantee |\n|-------|----------|------------|-------------------|\n| Critical | Revenue-generating | Can preempt lower | 100% reserved |\n| Standard | General traffic | Can preempt best-effort | Weighted share |\n| Best-effort | Background, batch | Cannot preempt | No guarantee |\n\n**Priority-aware queuing**:\n\n```\nIncoming requests sorted by priority, then arrival time:\n\nQueue state:\n  [Critical-001] [Critical-002] [Standard-001] [Standard-002] [BestEffort-001]\n       ↑ Process first\n\nNew Critical-003 arrives:\n  [Critical-001] [Critical-002] [Critical-003] [Standard-001] [Standard-002]\n       ↑ Jumps ahead of Standard requests\n```\n\n**Preemption for LLM serving**:\n\nWhen a critical request arrives but all GPU slots are occupied by lower-priority requests:\n\n1. Select victim request(s) from lowest priority class\n2. Pause victim's generation (save KV cache state)\n3. Allocate GPU slot to critical request\n4. When critical request completes, resume victim\n\n```\nBefore preemption:\n  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]\n\nCritical request arrives:\n  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]\n                ↓ preempt\n  GPU slots: [Critical E] [Standard B] [Standard C] [Standard D]\n  Paused: Best-effort A (KV cache saved to CPU)\n\nAfter Critical E completes:\n  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]\n  (A resumed from saved state)\n```\n\n### Bulkhead Pattern {#sec-inference-bulkhead}\n\nThe bulkhead pattern physically isolates tenant workloads, preventing failures from propagating across tenants. Named after ship compartments that contain flooding to isolated sections.\n\n**Deployment-level bulkheads**: Dedicate replicas to specific tenants or tenant groups.\n\n```\nGPU Cluster (24 GPUs):\n\nTenant A (gold tier): GPUs 0-7 (dedicated)\nTenant B (gold tier): GPUs 8-15 (dedicated)\nTenants C-Z (shared): GPUs 16-23 (shared pool)\n```\n\n**Pros**: Complete isolation for premium tenants\n**Cons**: Lower utilization, more operational overhead\n\n**Request-level bulkheads**: Limit the fraction of resources any single request can consume.\n\n```\nPer-request limits:\n  max_input_tokens: 8,000\n  max_output_tokens: 2,000\n  max_execution_time: 30s\n\nPrevents single request from consuming excessive resources.\n```\n\n**Failure isolation**: Errors in one tenant's requests do not affect others.\n\n```\nTenant A sends malformed input causing model error:\n  Without bulkhead: Error may crash shared inference worker\n  With bulkhead: Error caught, only Tenant A's request fails\n                 Other tenants continue normally\n```\n\n::: {.callout-note title=\"Bulkhead Configuration for API Tiers\"}\n\nConsider an LLM API with three service tiers:\n\n**Enterprise tier**:\n\n- Dedicated GPU pool (no sharing)\n- Custom model fine-tuning\n- 99.9% availability SLO\n- Price: $$$\n\n**Professional tier**:\n\n- Shared GPU pool with guaranteed capacity\n- Priority scheduling over free tier\n- 99.5% availability SLO\n- Price: $$\n\n**Free tier**:\n\n- Shared GPU pool, best-effort\n- Rate limited (10 QPS)\n- No SLO guarantee\n- Price: Free\n\n**Bulkhead configuration**:\n\n```yaml\ntiers:\n  enterprise:\n    gpu_pool: \"dedicated\"\n    isolation: \"hardware\"\n    replicas: 8\n    preemption: false\n\n  professional:\n    gpu_pool: \"shared-premium\"\n    isolation: \"resource-quota\"\n    quota_fraction: 0.7  # 70% of shared pool\n    preemption: true\n\n  free:\n    gpu_pool: \"shared-premium\"\n    isolation: \"resource-quota\"\n    quota_fraction: 0.3  # 30% of shared pool\n    preemption: false  # can be preempted\n```\n\n:::\n\n### Model Isolation {#sec-inference-model-isolation}\n\nWhen multiple models run on shared infrastructure, additional isolation is needed:\n\n**Memory isolation**: Ensure one model's memory usage does not impact others.\n\n```\nGPU memory partitioning (80GB H100):\n\nModel A: 40GB reserved (50%)\nModel B: 30GB reserved (37.5%)\nShared pool: 10GB (12.5%)\n```\n\n**Compute isolation**: GPU time-sharing between models introduces latency variance. Options include:\n\n- **MIG (Multi-Instance GPU)**: Hardware partitioning of A100/H100 into isolated GPU instances\n- **Time-slicing**: Cooperative scheduling between models (higher overhead)\n- **Dedicated GPUs**: Each model gets dedicated hardware (lower utilization)\n\n**Model loading isolation**: Loading one model should not evict another from GPU memory.\n\n```python\nclass ModelManager:\n    def load_model(self, model_id, priority):\n        required_memory = get_model_size(model_id)\n        available = get_free_gpu_memory()\n\n        if required_memory > available:\n            # Check if eviction would violate isolation\n            evictable = get_evictable_memory(priority)\n            if required_memory > evictable:\n                raise InsufficientMemoryError(\n                    \"Cannot load without violating isolation constraints\"\n                )\n            evict_lower_priority_models(priority, required_memory)\n\n        load_to_gpu(model_id)\n```\n\n### Observability for Multi-Tenancy {#sec-inference-multitenancy-observability}\n\nEffective multi-tenancy requires per-tenant visibility into resource consumption and performance:\n\n**Per-tenant metrics**:\n\n- Request count, latency distribution (P50, P95, P99)\n- GPU memory usage, KV cache utilization\n- Throttling events, preemption counts\n- Error rates by error type\n\n**Alerting thresholds**:\n\n```yaml\nalerts:\n  - name: tenant_slo_violation\n    condition: p99_latency > slo_target * 1.1\n    for: 5m\n    severity: warning\n\n  - name: tenant_quota_exhaustion\n    condition: usage > quota * 0.9\n    for: 1m\n    severity: warning\n\n  - name: noisy_neighbor_detection\n    condition: usage > fair_share * 2.0\n    for: 5m\n    severity: info\n```\n\n**Chargeback and attribution**: Track resource consumption for billing and capacity planning.\n\n```\nTenant A monthly report:\n  Total requests: 10,000,000\n  GPU-seconds consumed: 50,000\n  KV cache GB-hours: 2,500\n  Network egress GB: 100\n\n  Billed: $X based on consumption\n```\n\n## Autoscaling {#sec-inference-autoscaling}\n\nProduction inference systems experience traffic fluctuations that make static provisioning inefficient. Autoscaling dynamically adjusts capacity to match demand, reducing costs during low-traffic periods while maintaining SLOs during peaks.\n\nThis section examines autoscaling strategies for inference, with particular attention to the cold start problem that makes GPU-based scaling more challenging than traditional web services.\n\n### Scaling Dimensions {#sec-inference-scaling-dimensions}\n\nInference systems can scale along multiple dimensions:\n\n**Horizontal scaling (replicas)**: Add or remove model replicas to adjust throughput.\n\n$$\\text{Capacity} = \\text{Replicas} \\times \\text{Per-replica throughput}$$\n\n**Vertical scaling (GPU type)**: Use more powerful GPUs for higher per-replica throughput.\n\n$$\\text{Cost efficiency} = \\frac{\\text{Throughput}}{\\text{GPU cost}}$$\n\n**Batch size scaling**: Adjust batch sizes to trade latency for throughput.\n\n$$\\text{Latency} \\uparrow \\text{ as } \\text{Batch size} \\uparrow \\text{, Throughput} \\uparrow$$\n\n| Scaling Type | Latency | Cost | Speed |\n|--------------|---------|------|-------|\n| Horizontal (add replicas) | Unchanged | Linear | Slow (minutes) |\n| Batch size | Increases | Unchanged | Instant |\n| Vertical (better GPU) | Unchanged | Non-linear | Very slow (redeployment) |\n\n: **Scaling Dimension Tradeoffs**: Each scaling approach has different characteristics. {#tbl-scaling-dimensions}\n\n### The Cold Start Problem {#sec-inference-cold-start}\n\nUnlike stateless web services that start in seconds, inference services have significant startup latency:\n\n$$T_{cold start} = T_{provision} + T_{load} + T_{warmup}$$ {#eq-cold-start-time}\n\n**Provisioning time** ($T_{provision}$): Acquiring a GPU instance takes 30 seconds to several minutes depending on cloud provider and GPU type.\n\n**Model loading time** ($T_{load}$): Loading model weights from storage to GPU memory. For large models:\n\n| Model Size | Load Time (SSD) | Load Time (S3) |\n|------------|-----------------|----------------|\n| 7B (14GB) | 5s | 30s |\n| 70B (140GB) | 45s | 5min |\n| 175B (350GB) | 2min | 12min |\n\n**Warmup time** ($T_{warmup}$): First inference after loading is slower due to:\n\n- JIT compilation of kernels\n- CUDA context initialization\n- Memory pool allocation\n- Cache population\n\nWarmup typically requires 10-30 dummy inferences, adding 5-30 seconds.\n\n::: {.callout-note title=\"Cold Start Timeline for Llama-70B\"}\n\nBringing up a new replica for Llama-70B on H100:\n\n| Phase | Duration | Cumulative |\n|-------|----------|------------|\n| Cloud API request | 5s | 5s |\n| GPU instance provisioning | 60s | 65s |\n| Container startup | 10s | 75s |\n| Model download (S3) | 180s | 255s |\n| Model load to GPU | 45s | 300s |\n| CUDA warmup | 15s | 315s |\n| Readiness probe pass | 5s | 320s |\n| **Total cold start** | **5 min 20 sec** | |\n\n**Implication**: Scaling decisions must anticipate demand 5+ minutes in advance. Reactive scaling alone cannot handle sudden traffic spikes.\n\n:::\n\n### Reactive Scaling {#sec-inference-reactive-scaling}\n\nReactive scaling adjusts capacity based on observed metrics:\n\n**Metric-based scaling**:\n\n```yaml\nautoscaling:\n  metric: cpu_utilization  # or gpu_utilization, queue_depth\n  target_value: 70%\n  scale_up_threshold: 80%\n  scale_down_threshold: 50%\n  cooldown_period: 300s\n```\n\n**Queue-depth scaling**: Scale based on request queue length.\n\n$$\\text{Desired replicas} = \\left\\lceil \\frac{\\text{Queue depth}}{\\text{Queue target}} \\times \\text{Current replicas} \\right\\rceil$$\n\n**Latency-based scaling**: Scale to maintain latency SLO.\n\n$$\\text{Desired replicas} = \\left\\lceil \\frac{P99_{observed}}{P99_{target}} \\times \\text{Current replicas} \\right\\rceil$$\n\n**Reactive scaling limitations**:\n\n1. Response delay: Cold start time prevents rapid response\n2. Oscillation: Can create scale-up/scale-down cycles\n3. Over-provisioning: Must provision for worst-case during cold start\n\n::: {.callout-note title=\"Reactive Scaling Response Analysis\"}\n\nConsider traffic spike from 1000 to 3000 QPS:\n\n**Current state**: 10 replicas, 100 QPS each, 70% utilization\n\n**Target state**: 30 replicas for 3000 QPS\n\n**Without pre-warming**:\n\n- T=0: Spike detected, scale-up triggered\n- T=0 to T=5min: Cold start for 20 new replicas\n- T=0 to T=5min: Existing 10 replicas handle 3000 QPS (300 QPS each)\n- Utilization: 210% (overloaded)\n- P99 latency: 500ms+ (SLO violated)\n\n**With pool of warm spares (5 replicas)**:\n\n- T=0: Spike detected, warm spares activated immediately\n- T=0: 15 replicas handle 3000 QPS (200 QPS each)\n- T=0 to T=5min: Scale up 15 more replicas\n- Utilization: 140% (elevated but manageable)\n- P99 latency: 80ms (SLO maintained)\n\nWarm spares provide buffer during cold start period.\n\n:::\n\n### Predictive Scaling {#sec-inference-predictive-scaling}\n\nPredictive scaling anticipates demand before it occurs, initiating scaling ahead of traffic changes.\n\n**Time-series forecasting**: Use historical patterns to predict future demand.\n\n```python\ndef predict_demand(current_time, history):\n    # Seasonal decomposition\n    daily_pattern = extract_daily_seasonality(history)\n    weekly_pattern = extract_weekly_seasonality(history)\n\n    # Trend estimation\n    trend = estimate_trend(history)\n\n    # Forecast\n    predicted = (\n        daily_pattern[current_time.hour]\n        * weekly_pattern[current_time.weekday()]\n        * trend\n    )\n    return predicted\n```\n\n**Event-driven scaling**: Scale proactively for known events.\n\n```yaml\nscheduled_scaling:\n  - event: \"product_launch\"\n    time: \"2024-03-15 09:00 UTC\"\n    target_replicas: 50  # 5x normal\n    ramp_up: 30min  # Start scaling 30min before\n\n  - event: \"weekly_newsletter\"\n    cron: \"0 10 * * 1\"  # Every Monday 10am\n    target_replicas: 20  # 2x normal\n    duration: 2h\n```\n\n**Hybrid approach**: Combine predictive baseline with reactive adjustment.\n\n$$\\text{Target replicas} = \\max(\\text{Predicted}, \\text{Reactive}) + \\text{Buffer}$$\n\n::: {.callout-note title=\"Predictive Scaling for Daily Traffic Patterns\"}\n\nA chatbot service shows predictable daily patterns:\n\n| Time (UTC) | Typical QPS | Replicas Needed |\n|------------|-------------|-----------------|\n| 00:00-06:00 | 500 | 5 |\n| 06:00-09:00 | 1500 | 15 (ramp up) |\n| 09:00-17:00 | 3000 | 30 (peak) |\n| 17:00-20:00 | 2000 | 20 (ramp down) |\n| 20:00-00:00 | 1000 | 10 |\n\n**Predictive schedule** (accounting for cold start):\n\n| Time | Action | Replicas Active | Replicas Starting |\n|------|--------|-----------------|-------------------|\n| 05:30 | Scale up | 5 | +10 warming |\n| 06:00 | Traffic ramp | 15 | - |\n| 08:30 | Scale up | 15 | +15 warming |\n| 09:00 | Peak traffic | 30 | - |\n| 17:00 | Scale down | 20 | -10 terminating |\n| 20:00 | Scale down | 10 | -10 terminating |\n| 00:00 | Scale down | 5 | -5 terminating |\n\n**Cost comparison**:\n\n- Reactive only: Must over-provision during ramp (45 replicas peak)\n- Predictive: Right-sized provisioning (30 replicas peak)\n- Savings: 33% GPU cost reduction\n\n:::\n\n### Warm Pool Management {#sec-inference-warm-pools}\n\nMaintaining a pool of pre-warmed replicas reduces effective cold start time:\n\n**Warm pool sizing**:\n\n$$\\text{Warm pool size} = \\frac{\\text{Max expected spike}}{\\text{Per-replica throughput}} \\times \\text{Headroom factor}$$\n\nFor example, if max spike is 2x normal and headroom factor is 1.5:\n\n$$\\text{Warm pool} = 2 \\times 1.5 = 3\\text{x minimum pool capacity}$$\n\n**Warm pool cost**: Maintaining warm replicas costs money even when idle.\n\n$$\\text{Warm pool cost} = \\text{Pool size} \\times \\text{GPU cost/hour} \\times \\text{Idle fraction}$$\n\nTrade-off: More warm replicas = faster response but higher cost.\n\n**Tiered warm pools**: Different readiness levels with different costs.\n\n| Tier | State | Response Time | Cost (relative) |\n|------|-------|---------------|-----------------|\n| Hot | GPU loaded, running | Instant | 100% |\n| Warm | GPU allocated, model loaded | 30s | 60% |\n| Cold | GPU not allocated | 5+ min | 0% |\n\n```\nPool configuration:\n  Hot: 2 replicas (instant burst capacity)\n  Warm: 5 replicas (30s activation)\n  Cold: Unlimited (cloud provider)\n\nScaling sequence:\n  1. Activate Hot replicas immediately\n  2. Activate Warm replicas within 30s\n  3. Cold start new replicas if demand persists\n```\n\n### Scaling Response Time Analysis {#sec-inference-scaling-response}\n\nThe total time to respond to a scaling event:\n\n$$T_{response} = T_{detect} + T_{decide} + T_{provision} + T_{warmup}$$ {#eq-scaling-response}\n\n| Component | Duration | Optimization |\n|-----------|----------|--------------|\n| Detection | 10-60s | Reduce metrics interval |\n| Decision | 1-5s | Faster autoscaler |\n| Provisioning | 30s-5min | Warm pools |\n| Warmup | 5-30s | Pre-compilation |\n\n**Optimizing each component**:\n\n**Detection speed**: Use high-frequency metrics (1s vs 60s intervals) for faster detection. Trade-off: More metric volume, potentially noisier signals.\n\n**Decision speed**: Pre-compute scaling plans based on predicted scenarios. When trigger occurs, execute pre-computed plan immediately.\n\n**Provisioning speed**: Warm pools eliminate provisioning for anticipated demand. Spot/preemptible instances can reduce provisioning time (already running, just need allocation).\n\n**Warmup speed**: Pre-compiled TensorRT engines skip JIT compilation. Lazy loading defers some initialization to first request.\n\n### Spot and Preemptible Instances {#sec-inference-spot-instances}\n\nCloud providers offer discounted GPU instances that can be reclaimed with short notice:\n\n| Instance Type | Discount | Interruption Notice | Use Case |\n|--------------|----------|--------------------| ---------|\n| On-demand | 0% | Never | SLO-critical |\n| Reserved | 30-60% | Never | Steady baseline |\n| Spot/Preemptible | 60-90% | 30s-2min | Burst capacity |\n\n**Graceful handling of spot termination**:\n\n```python\ndef handle_spot_termination():\n    # Received 2-minute warning\n    # 1. Stop accepting new requests\n    stop_accepting_requests()\n\n    # 2. Complete in-flight requests (if possible)\n    await complete_inflight(timeout=90)\n\n    # 3. Save state for resumption elsewhere\n    save_kv_cache_to_storage()\n\n    # 4. Signal load balancer to redirect traffic\n    deregister_from_loadbalancer()\n\n    # 5. Terminate gracefully\n    shutdown()\n```\n\n**Spot-aware architecture**:\n\n```\nTraffic distribution:\n\nRequest arrives\n    │\n    ▼\nLoad balancer\n    │\n    ├── 70% → On-demand replicas (guaranteed capacity)\n    │\n    └── 30% → Spot replicas (cost savings, may be interrupted)\n```\n\nBest-effort requests route to spot instances; SLO-critical requests use on-demand.\n\n## Global Inference Infrastructure {#sec-inference-global}\n\nProduction inference systems serving global user bases must operate across multiple geographic regions. A user in Tokyo expects low-latency responses regardless of where models were trained or where the company headquarters is located. This section examines the architectural patterns for multi-region inference deployment.\n\n### Why Multi-Region Matters {#sec-inference-global-why}\n\nSingle-region deployment creates fundamental limitations:\n\n**Latency floor**: Network round-trip time (RTT) to distant users cannot be optimized away:\n\n| User Location | RTT to US-East | RTT to Local Region |\n|---------------|----------------|---------------------|\n| New York | 10ms | 10ms |\n| London | 75ms | 10ms |\n| Tokyo | 150ms | 10ms |\n| Sydney | 200ms | 10ms |\n\nFor interactive applications (chatbots, autocomplete), these delays compound across multiple model calls per request.\n\n**Availability**: Single-region deployment creates a single point of failure. Cloud region outages, while rare, affect all users simultaneously.\n\n**Regulatory compliance**: Data residency requirements (GDPR, data sovereignty laws) may require processing user data within specific geographic boundaries.\n\n### Multi-Region Architecture Patterns {#sec-inference-global-patterns}\n\n**Pattern 1: Global load balancing with regional replicas**\n\n```\n                    Global Load Balancer\n                    (Latency-based routing)\n                           │\n         ┌─────────────────┼─────────────────┐\n         ▼                 ▼                 ▼\n    US-East           EU-West           Asia-Pacific\n    ┌─────────┐       ┌─────────┐       ┌─────────┐\n    │ vLLM    │       │ vLLM    │       │ vLLM    │\n    │ Replicas│       │ Replicas│       │ Replicas│\n    └─────────┘       └─────────┘       └─────────┘\n         │                 │                 │\n         └─────────────────┼─────────────────┘\n                           ▼\n                    Model Registry\n                    (Synchronized)\n```\n\nEach region runs independent inference replicas with identical models. The global load balancer routes users to the nearest region based on latency.\n\n**Key considerations**:\n\n- **Model synchronization**: Model updates must propagate to all regions. Options include:\n  - Push-based: Central registry pushes to all regions (simple, potential inconsistency window)\n  - Pull-based: Regions poll for updates (higher latency, guaranteed consistency)\n  - Hybrid: Push notification + pull verification\n\n- **Version consistency**: During model rollouts, different regions may briefly serve different versions. For most applications this is acceptable; for applications requiring strict consistency, implement version pinning in request routing.\n\n**Pattern 2: Edge caching with central inference**\n\nFor models too large to replicate globally, cache responses at the edge:\n\n```\nUser → Edge Cache (CDN) → Regional Proxy → Central Inference\n           │                    │\n           └── Cache hit ───────┘\n               (< 10ms)\n\n           └── Cache miss ──────────────────→\n               (Full latency, populate cache)\n```\n\n**Effectiveness depends on request repeatability**:\n\n| Workload | Cache Hit Rate | Suitability |\n|----------|----------------|-------------|\n| Autocomplete | 60-80% | Excellent |\n| FAQ chatbot | 40-60% | Good |\n| Open-ended chat | 5-15% | Poor |\n| Code generation | 20-40% | Moderate |\n\nSemantic caching (caching based on embedding similarity rather than exact match) can improve hit rates for open-ended workloads.\n\n**Pattern 3: Federated inference with model sharding**\n\nFor the largest models, shard across regions:\n\n```\nUser request\n    │\n    ▼\nRequest Router\n    │\n    ├── Layers 1-40  → US-East GPUs\n    │\n    └── Layers 41-80 → EU-West GPUs\n\n    Pipeline parallelism across regions\n```\n\nThis pattern is rarely practical due to inter-region latency dominating compute time, but may apply for extremely large models where no single region has sufficient GPU capacity.\n\n### Cross-Region Failover {#sec-inference-global-failover}\n\nWhen a region becomes unavailable, traffic must reroute to healthy regions:\n\n**Active-active failover**:\n\n```python\n# Simplified global routing logic\ndef route_request(user_region, request):\n    primary = get_nearest_healthy_region(user_region)\n    secondary = get_second_nearest_healthy_region(user_region)\n\n    try:\n        return call_region(primary, request, timeout=2.0)\n    except (Timeout, RegionUnavailable):\n        # Failover with increased latency\n        return call_region(secondary, request, timeout=5.0)\n```\n\n**Failover considerations for stateful LLM serving**:\n\n- **Session affinity loss**: Users mid-conversation lose KV cache state. The fallback region must regenerate context from conversation history.\n- **Capacity spike**: The receiving region sees sudden traffic increase. Pre-provision headroom (typically 30-50% over steady-state) or accept degraded latency during failover.\n- **Gradual recovery**: When the failed region recovers, gradually shift traffic back to avoid oscillation.\n\n### Global Model Deployment {#sec-inference-global-deployment}\n\nDeploying model updates across regions requires careful coordination:\n\n**Phased rollout strategy**:\n\n```\n1. Deploy to canary region (e.g., 1% traffic in US-East)\n2. Monitor metrics for 1 hour\n3. If healthy, deploy to remaining US-East replicas\n4. Monitor for 4 hours\n5. Deploy to EU-West (different user population)\n6. Monitor for 4 hours\n7. Deploy to Asia-Pacific\n8. Complete rollout\n```\n\n**Rollback across regions**:\n\nIf issues are detected after partial deployment:\n\n```\nRegion Status:\n  US-East:      v2.1 (new) ← Issue detected\n  EU-West:      v2.0 (old)\n  Asia-Pacific: v2.0 (old)\n\nAction: Rollback US-East to v2.0\n  - Switch traffic to v2.0 replicas\n  - Maintain v2.1 replicas for debugging\n  - Do not proceed with EU-West deployment\n```\n\n**Metrics for global deployment health**:\n\n| Metric | Per-Region | Global |\n|--------|-----------|--------|\n| Error rate | < 0.1% | < 0.1% |\n| P99 latency | < target | < 2x single-region |\n| Throughput | Stable | Stable |\n| Model quality | Within bounds | Consistent across regions |\n\n### Cost Optimization Across Regions {#sec-inference-global-cost}\n\nGPU pricing varies by region. Optimize placement for cost while meeting latency requirements:\n\n| Region | H100 Spot Price | On-Demand | Latency to US Users |\n|--------|-----------------|-----------|---------------------|\n| US-East | $2.50/hr | $4.00/hr | 10-50ms |\n| US-West | $2.30/hr | $3.80/hr | 30-70ms |\n| EU-West | $2.80/hr | $4.20/hr | 75-100ms |\n\n**Cost-aware routing**:\n\nFor latency-tolerant workloads (batch inference, background processing), route to the cheapest available region:\n\n```python\ndef route_batch_request(request):\n    if request.priority == \"low\":\n        # Route to cheapest region with capacity\n        return get_cheapest_region_with_capacity()\n    else:\n        # Route to nearest region\n        return get_nearest_region(request.user_location)\n```\n\nThis can reduce costs by 20-40% for batch workloads while maintaining SLOs for interactive traffic.\n\n## Case Studies {#sec-inference-case-studies}\n\nThe techniques presented throughout this chapter come together in production systems serving billions of requests daily. This section examines four case studies that illustrate different points in the inference design space: Meta's recommendation serving (high volume, low latency), OpenAI's API infrastructure (LLM-focused), Google's search ranking (ensemble models), and TikTok's multimodal recommendation (video understanding combined with user modeling).\n\nEach case study demonstrates how the principles of batching, sharding, load balancing, and autoscaling combine to meet specific requirements.\n\n### Meta Recommendation Serving {#sec-inference-case-meta}\n\nMeta's recommendation infrastructure serves predictions for feeds, ads, and content ranking across Facebook, Instagram, WhatsApp, and Messenger. This represents one of the largest production inference deployments in the world.\n\n**Scale and requirements**:\n\n- Request volume: Billions of requests per day\n- Latency target: <10ms P99\n- Model diversity: Hundreds of model variants\n- Feature cardinality: Trillions of unique entities\n\n**Architecture overview**:\n\n```\nUser request → Feature collection → Embedding lookup → Model inference → Response\n\n                    │                     │                  │\n                    ▼                     ▼                  ▼\n             Feature Store        Embedding Servers      GPU Inference\n             (CPU, DRAM)         (CPU + SSD, 1000s)     (GPU, 100s)\n```\n\n**Key design decisions**:\n\n**Embedding sharding at scale**: Embedding tables total over 100TB, requiring 1000+ shards. Meta uses a hybrid sharding strategy:\n\n- Hot embeddings (top 1%): Replicated across memory on all inference servers\n- Warm embeddings (next 10%): Column-sharded with 8-way parallelism\n- Cold embeddings (remaining 89%): Row-sharded with consistent hashing, SSD-backed\n\nThis reduces embedding lookup latency from 50ms (naive) to 2ms through batching and locality optimization.\n\n**Feature-parallel batching**: Instead of batching entire requests, Meta batches at the feature level. Each inference request triggers 5,000+ embedding lookups, but these lookups are batched across requests within a 1ms window. This achieves 90%+ memory bandwidth utilization on embedding servers.\n\n**GPU-CPU hybrid architecture**: Dense model computation (ranking towers) runs on GPUs, while sparse embedding lookups run on CPU servers with large memory and SSD storage. This matches hardware to workload characteristics:\n\n| Component | Hardware | Latency | Throughput |\n|-----------|----------|---------|------------|\n| Embedding lookup | CPU + SSD | 2ms | 50M lookups/s |\n| Feature processing | CPU | 1ms | 10M ops/s |\n| Dense ranking | GPU | 1.5ms | 100K infs/s |\n\n**Lessons learned**:\n\n1. Embedding lookup, not model inference, often dominates latency for recommendation systems\n2. Feature-parallel batching achieves higher efficiency than request-level batching\n3. Hybrid CPU-GPU architectures match hardware to workload characteristics\n\n### OpenAI API Infrastructure {#sec-inference-case-openai}\n\nOpenAI's API serves GPT-4, GPT-3.5-turbo, and other models to millions of developers. The infrastructure must handle highly variable request sizes (from 10 tokens to 128K tokens) while maintaining quality of service across diverse workloads.\n\n**Scale and requirements**:\n\n- Request volume: Millions of requests per hour\n- Latency target: Time-to-first-token (TTFT) <2s, throughput varies by model\n- Model sizes: 7B to 175B+ parameters\n- Context lengths: Up to 128K tokens\n\n**Architecture overview**:\n\n```\nAPI Gateway → Rate Limiting → Request Router → Model Cluster → Response Streaming\n\n                                     │\n                                     ▼\n                              ┌─────────────┐\n                              │ Model Pool  │\n                              │ ┌─────────┐ │\n                              │ │ GPT-4   │ │\n                              │ │ 8xH100  │ │\n                              │ └─────────┘ │\n                              │ ┌─────────┐ │\n                              │ │GPT-3.5  │ │\n                              │ │ 4xA100  │ │\n                              │ └─────────┘ │\n                              └─────────────┘\n```\n\n**Key design decisions**:\n\n**Continuous batching with chunked prefill**: OpenAI was an early adopter of continuous batching (Orca-style) to maintain high GPU utilization despite variable output lengths. Chunked prefill bounds decode latency by processing long prompts in chunks that interleave with ongoing generation.\n\n| Batching Strategy | GPU Utilization | TTFT (128K prompt) |\n|------------------|-----------------|-------------------|\n| Static batching | 45% | 30s (blocked) |\n| Continuous batching | 75% | 30s (blocked) |\n| Continuous + chunked | 85% | 3s (streamed) |\n\n**Tensor parallelism for large models**: GPT-4 class models require 8-way or greater tensor parallelism for memory capacity and latency:\n\n- 8xH100 per GPT-4 shard group\n- NVLink for intra-node communication\n- Consistent hashing for session affinity (KV cache reuse)\n\n**Multi-tier rate limiting**: OpenAI implements rate limiting at multiple levels to prevent noisy neighbors:\n\n- Per-API-key request rate limits\n- Per-API-key token-per-minute limits\n- Organization-level capacity quotas\n- Global model capacity limits\n\n**Dynamic capacity allocation**: During peak demand, OpenAI shifts capacity between models based on queue depth:\n\n```\nif gpt4_queue_depth > threshold:\n    # Migrate some GPT-3.5 capacity to GPT-4\n    reallocate_cluster_capacity(from=\"gpt-3.5\", to=\"gpt-4\", fraction=0.2)\n```\n\n**Lessons learned**:\n\n1. Continuous batching is essential for LLM serving at scale\n2. Prefix caching provides 2-3x efficiency for conversational workloads\n3. Multi-tier rate limiting prevents cascade failures from traffic spikes\n\n### Google Search Ranking {#sec-inference-case-google}\n\nGoogle Search uses ensemble serving to combine multiple specialized models for query understanding, document relevance, and result ranking. This represents a different inference pattern: many smaller models coordinated for each request rather than one large model.\n\n**Scale and requirements**:\n\n- Request volume: Billions of searches per day\n- Latency target: <200ms end-to-end\n- Model count: Dozens of models per query\n- Result processing: Thousands of documents per query\n\n**Architecture overview**:\n\n```\nQuery → Query Understanding → Candidate Retrieval → Ranking Cascade → Results\n\n              │                        │                    │\n              ▼                        ▼                    ▼\n         BERT QU                  Embeddings            L1 → L2 → L3\n        (10 models)              (100s shards)       (progressively complex)\n```\n\n**Key design decisions**:\n\n**Cascading model architecture**: Rather than running one expensive model on all candidates, Google uses a ranking cascade:\n\n| Stage | Model Complexity | Candidates | Latency Budget |\n|-------|-----------------|------------|----------------|\n| L0 (Retrieval) | Embedding lookup | 1,000,000 → 10,000 | 10ms |\n| L1 (First pass) | Linear model | 10,000 → 1,000 | 20ms |\n| L2 (Second pass) | Small transformer | 1,000 → 100 | 50ms |\n| L3 (Final rank) | Large ensemble | 100 → 10 | 100ms |\n\nThis achieves 100x cost reduction compared to running L3 on all candidates.\n\n**Speculative execution**: Given tight latency budgets, Google uses speculative execution for model ensembles:\n\n```\n# Instead of sequential:\n#   q1 = model1(query)\n#   q2 = model2(query)\n#   q3 = model3(query, q1, q2)\n\n# Speculative parallel:\nasync_q1 = async model1(query)\nasync_q2 = async model2(query)\nasync_q3 = async model3(query, predicted_q1, predicted_q2)\n\n# Use actual results if they arrive in time, otherwise use speculative\n```\n\n**Custom TPU infrastructure**: Google runs ranking models on TPUs optimized for transformer inference. TPU pods provide:\n\n- 2D mesh topology for efficient AllReduce\n- High memory bandwidth for attention operations\n- Custom quantization for serving efficiency\n\n**Deadline-aware scheduling**: Each sub-request carries a deadline, and workers prioritize by deadline proximity:\n\n```\nWorker queue: [Doc1: 50ms left] [Doc2: 30ms left] [Doc3: 80ms left]\n                                      ↑ Process first\n\nIf deadline will be missed:\n  Return cached/default result rather than timing out\n```\n\n**Lessons learned**:\n\n1. Ranking cascades provide dramatic cost reduction for large candidate sets\n2. Deadline propagation and priority scheduling are essential for ensemble serving\n3. Custom hardware (TPU) enables efficiency that commodity GPUs cannot match\n\n### TikTok Multimodal Recommendation {#sec-inference-case-tiktok}\n\nTikTok's recommendation system combines video understanding (vision) with user modeling (recommendation) for personalized content ranking. This represents a multimodal inference challenge where different model types must coordinate.\n\n**Scale and requirements**:\n\n- Request volume: Millions of video rankings per second\n- Latency target: <50ms P99\n- Content volume: Millions of new videos daily\n- Modalities: Video, audio, text, user signals\n\n**Architecture overview**:\n\n```\nUser request → User embedding → Candidate videos → Video understanding → Ranking\n\n                    │                  │                    │\n                    ▼                  ▼                    ▼\n             User Tower          Video Cache          Vision Models\n           (Transformer)        (Pre-computed)        (On-demand)\n```\n\n**Key design decisions**:\n\n**Two-tower architecture with caching**: TikTok separates user understanding (online) from content understanding (offline):\n\n| Tower | Update Frequency | Latency | Compute |\n|-------|-----------------|---------|---------|\n| User tower | Real-time | 5ms | GPU (online) |\n| Video tower | Hourly | N/A | GPU (batch) |\n\nVideo embeddings are pre-computed and cached, eliminating vision inference from the critical path for most requests. Only new videos (uploaded within the hour) require online vision inference.\n\n**Hybrid CPU-GPU inference**: Like Meta, TikTok uses CPU for embedding operations and GPU for dense model computation:\n\n```\nUser features → CPU preprocessing (1ms)\n             → Embedding lookup (2ms, CPU+DRAM)\n             → Dense ranking (10ms, GPU)\n             → Response formatting (1ms)\n```\n\n**Priority-based video analysis**: New video content is processed with different priorities:\n\n| Priority | SLA | Use Case |\n|----------|-----|----------|\n| Critical | 5 min | Creator with large following |\n| Standard | 30 min | Normal uploads |\n| Background | 2 hours | Bulk/imported content |\n\nThis ensures popular creators' content reaches recommendations quickly while managing compute costs.\n\n**Multimodal fusion**: TikTok combines multiple understanding modalities through late fusion:\n\n```\nVideo embedding (512d) ─┐\nAudio embedding (256d) ─┼─ Concat → Fusion MLP → Final embedding (256d)\nText embedding (256d)  ─┘\n```\n\nThis allows independent updates to each modality's model without retraining the full system.\n\n**Lessons learned**:\n\n1. Separating online and offline components enables aggressive caching\n2. Two-tower architectures scale better than joint models for user-item systems\n3. Priority-based processing balances freshness against compute cost\n\n### Cross-Cutting Observations {#sec-inference-case-observations}\n\nSeveral patterns emerge across these case studies:\n\n**Separation of concerns**: All systems separate embedding/retrieval from ranking/generation. This enables specialized optimization for each component.\n\n**Hybrid architectures**: No system uses GPUs exclusively. CPU+GPU combinations match hardware to workload characteristics.\n\n**Caching at multiple levels**: Embedding caching, result caching, and intermediate representation caching all appear. Caching reduces compute at the cost of staleness.\n\n**Progressive refinement**: Cascades and early-exit strategies reduce average compute by quickly filtering unlikely candidates.\n\n**Deadline awareness**: All systems propagate deadlines and make explicit tradeoffs between quality and latency when under pressure.\n\n| System | Primary Technique | Key Innovation |\n|--------|------------------|----------------|\n| Meta | Embedding sharding | Feature-parallel batching |\n| OpenAI | Continuous batching | Chunked prefill |\n| Google | Ranking cascade | Speculative execution |\n| TikTok | Two-tower caching | Multimodal fusion |\n\n: **Case Study Summary**: Each system innovates on a core technique matched to its workload characteristics. {#tbl-case-studies-summary}\n\n## Fallacies and Pitfalls {#sec-inference-fallacies-pitfalls}\n\nThe techniques presented throughout this chapter address real engineering challenges, but misconceptions about inference at scale remain common. Recognizing these fallacies and pitfalls helps practitioners avoid costly mistakes in system design and capacity planning.\n\n**Fallacy: Inference at scale is synonymous with LLM serving.**\n\nThis misconception, reinforced by current discourse, leads to over-focus on LLM-specific techniques while ignoring the broader inference landscape. By request volume, recommendation systems constitute 80-90% of production inference at major technology companies, with vision and other models comprising most of the remainder. LLMs currently represent 1-5% of requests, though this is growing. A practitioner who only understands continuous batching and KV cache management will be unprepared for the feature-parallel batching and embedding sharding that dominate production inference. Technique selection must match the actual workload.\n\n**Pitfall: Using training infrastructure for production serving.**\n\nTraining and serving have fundamentally different requirements. Training optimizes for aggregate throughput over hours or days; serving optimizes for per-request latency under strict SLOs. Training tolerates batch sizes of thousands; serving often requires batch sizes in single digits. Training accepts checkpoint-based recovery; serving requires graceful failover without user impact. Teams that deploy training clusters for serving often discover unacceptable latency variance, poor resource utilization, and difficulty meeting SLOs. Purpose-built serving infrastructure with appropriate batching, load balancing, and autoscaling is essential.\n\n**Fallacy: Continuous batching solves all LLM serving problems.**\n\nContinuous batching dramatically improves GPU utilization for LLM serving, but it addresses only one dimension of the problem. Prefill remains a bottleneck for long contexts, as the quadratic attention computation cannot be avoided regardless of how subsequent decode iterations are batched. KV cache memory, not compute, often limits batch size. Network bandwidth between sharded model components can dominate latency for large models. Continuous batching is necessary but not sufficient for efficient LLM serving.\n\n**Pitfall: Sizing capacity based on average throughput.**\n\nThe nonlinear relationship between utilization and latency (from queuing theory) means that systems provisioned for average load will violate SLOs during traffic peaks. At 80% average utilization, a modest 25% traffic spike pushes utilization above 100%, causing unbounded queue growth and latency degradation. The cold start problem exacerbates this: by the time new capacity is available (5+ minutes for GPU instances), the spike may have caused significant SLO violations. Capacity planning must account for peak load plus headroom, not average load.\n\n**Fallacy: Load balancing does not matter much for inference.**\n\nSimple load balancing strategies like round-robin seem adequate until examined quantitatively. Random assignment produces maximum queue lengths of $O(\\log n / \\log \\log n)$ across $n$ servers. Power-of-two-choices reduces this to $O(\\log \\log n)$, an exponential improvement. For a 1,000-server cluster, this translates from ~4-5 requests maximum queue to ~2 requests. At tail latencies that matter for SLOs, this difference is substantial. For LLM workloads with highly variable request durations, least-connections further improves balance. The choice of load balancing algorithm has first-order impact on system performance.\n\n**Pitfall: Ignoring the serving tax in latency budgets.**\n\nDistributed inference introduces overhead absent from single-machine serving: network round-trips, serialization, load balancer decisions, and coordination for sharded models. This \"serving tax\" often consumes 10-30% of the latency budget. A team that achieves 70ms model inference on a single GPU may be surprised when end-to-end latency reaches 100ms in production due to these overheads. Latency budgets must explicitly account for distribution overhead, not just compute time.\n\n**Fallacy: More GPU memory always means more batch size and throughput.**\n\nWhile larger GPU memory enables larger batches for models that fit in memory, the bottleneck often shifts before memory is exhausted. Memory bandwidth limits throughput for bandwidth-bound operations (LLM decode). Compute limits throughput for compute-bound operations (prefill, vision inference). Adding memory to a bandwidth-bound workload provides no benefit. Understanding whether the workload is compute-bound, memory-bound, or capacity-bound guides appropriate resource allocation.\n\n**Pitfall: Neglecting multi-tenancy isolation until production.**\n\nIn development and staging, single-tenant deployments work well. In production, noisy neighbors cause sudden, unpredictable performance degradation that is difficult to diagnose and resolve. A tenant bursting to 5x normal traffic can degrade latency for all other tenants on shared infrastructure. Resource quotas, priority scheduling, and bulkhead isolation must be designed into the system from the start, not retrofitted after production incidents.\n\n::: {.callout-important title=\"Three Things to Remember\"}\n\n1. **Serving cost dominates training cost over a model's lifetime.** For high-volume applications, serving cost exceeds training cost by 100x or more. Every percentage point of serving efficiency improvement yields ongoing cost reduction. Optimize serving ruthlessly.\n\n2. **Different model types require fundamentally different batching strategies.** Static batching for vision, continuous batching for LLMs, feature-parallel batching for recommendation systems. There is no universal optimal strategy. Match technique to workload.\n\n3. **Power-of-two-choices provides exponential load balancing improvement.** Maximum queue length improves from $O(\\log n / \\log \\log n)$ to $O(\\log \\log n)$ with minimal overhead (two probes per request). This simple technique should be standard for any distributed inference deployment.\n\n:::\n\n## Summary {#sec-inference-summary}\n\nInference at scale transforms the single-machine serving foundations from Volume I into distributed systems that handle billions of requests across global infrastructure. Throughout this chapter, we have developed the principles governing this transformation, from batching strategies matched to model architectures through load balancing algorithms that provide exponentially better performance.\n\nThe serving hierarchy provides our organizing framework: request-level optimizations (batching, caching), replica-level optimizations (GPU utilization, memory management), service-level optimizations (load balancing, routing), and platform-level optimizations (multi-tenancy, scheduling). Each level has distinct metrics and techniques, and effective inference systems optimize at all levels simultaneously.\n\nWe began by establishing when distributed inference becomes necessary: memory exhaustion for models exceeding single-GPU capacity, throughput requirements beyond single-machine limits, or latency targets that demand parallel computation. The fundamental inversion from training's throughput focus to serving's latency imperative shapes every subsequent design decision. We quantified the serving tax, the overhead of distribution that must be budgeted within latency constraints, and demonstrated that serving cost dominates training cost over a model's operational lifetime.\n\nBatching strategies vary dramatically across model types. Vision models benefit from large static or dynamic batches that maximize GPU utilization. LLMs require continuous batching to handle variable output lengths efficiently, with chunked prefill to bound decode latency during long prompt processing. Recommendation systems use feature-parallel batching that aligns with embedding shard architectures rather than request-level batching. Streaming applications for speech and video cannot tolerate batching delay at all. Selecting the wrong batching strategy for a workload can reduce throughput by 3-4x or violate latency SLOs entirely.\n\nModel sharding distributes computation across devices when single-GPU memory or latency constraints require it. Tensor parallelism achieves latency reduction through parallel attention and feed-forward computation, with all-reduce synchronization between layers. Pipeline parallelism distributes layers across stages for memory relief without latency benefit for single requests. Expert parallelism handles mixture-of-experts models with dynamic routing. Embedding sharding scales recommendation systems' trillion-parameter tables across thousands of servers. Each strategy has distinct communication patterns and overhead characteristics that must match deployment constraints.\n\nLoad balancing determines how requests reach replicas, with seemingly simple choices producing dramatically different performance. Random assignment yields maximum queue lengths of $O(\\log n / \\log \\log n)$, while power-of-two-choices achieves $O(\\log \\log n)$, an exponential improvement from a trivial modification. Consistent hashing enables session affinity for stateful workloads like LLM conversations. Circuit breakers and backpressure mechanisms protect systems from cascading failures when replicas become overloaded.\n\nKV cache management has emerged as a critical bottleneck for LLM serving. PagedAttention eliminates memory fragmentation through virtual memory techniques, achieving 2.5-4x throughput improvement. Prefix caching shares common prompt prefixes across requests, reducing both compute and memory for conversational workloads. Speculative decoding breaks the sequential generation bottleneck by using draft models to predict multiple tokens verified in parallel. These techniques combine to make previously impractical LLM deployments economically viable.\n\nMulti-tenancy enables cost-effective infrastructure sharing but requires careful isolation engineering. Noisy neighbors can degrade performance for all tenants without proper resource quotas, priority scheduling, and bulkhead isolation. The tradeoff between utilization and isolation must be explicitly managed based on service tier and SLO requirements.\n\nAutoscaling addresses traffic fluctuations but faces the cold start problem unique to GPU-based inference. Model loading times of minutes make reactive scaling insufficient for sudden traffic spikes. Predictive scaling, warm pools, and tiered readiness levels combine to provide rapid scaling response while managing cost. Spot instances offer significant savings for burst capacity when graceful handling of interruptions is implemented.\n\nThe case studies demonstrate how these principles combine in production: Meta's feature-parallel batching and embedding sharding for recommendation, OpenAI's continuous batching and tensor parallelism for LLMs, Google's ranking cascades and deadline-aware scheduling for search, and TikTok's two-tower caching for multimodal recommendation. Each system innovates on techniques matched to its specific workload characteristics, but all share common patterns of separation of concerns, hybrid architectures, and progressive refinement.\n\n::: {.callout-important title=\"Key Takeaways\"}\n\n* Serving cost dominates training cost by 100x or more for high-volume applications, making serving optimization the primary driver of ML infrastructure economics\n\n* Different model types require fundamentally different batching strategies: static/dynamic for vision, continuous for LLMs, feature-parallel for recommendation\n\n* The serving hierarchy (request, replica, service, platform) provides a framework for decomposing optimization opportunities at each level\n\n* Power-of-two-choices load balancing achieves exponentially better queue balance ($O(\\log \\log n)$ vs $O(\\log n / \\log \\log n)$) with minimal overhead\n\n* KV cache management through PagedAttention and prefix caching enables 2-4x LLM serving efficiency improvement\n\n* Cold start times of 5+ minutes for GPU-based serving require predictive scaling and warm pools, not just reactive autoscaling\n\n* Production inference is dominated by recommendation systems (80-90% of requests), not LLMs, requiring model-type diversity in technique selection\n\n:::\n\nThe techniques in this chapter enable inference systems that scale from single machines to global deployments. The next chapter, @sec-edge-intelligence, examines the other end of the deployment spectrum: inference at the edge, where devices with limited compute, memory, and power must still deliver predictions reliably. The distributed coordination patterns established here inform edge-cloud architectures that partition computation between constrained edge devices and capable cloud infrastructure.\n\n<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->\n::: { .quiz-end }\n:::\n","srcMarkdownNoYaml":"\n\n<!--\n================================================================================\nEDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR INFERENCE AT SCALE\n================================================================================\n\nEXPERT FEEDBACK FROM SERVING CHAPTER REVIEW (January 2025):\nThe following topics were identified by experts (Jeff Dean, Ion Stoica, Chip Huyen,\nVijay Reddi, Song Han) as important for distributed inference but appropriately\ndeferred from Vol I Serving chapter to this chapter:\n\nFROM JEFF DEAN:\n\n- Load balancing and request routing (power-of-two-choices, consistent hashing)\n- M/M/c multi-server queue model and Erlang C formula for capacity planning\n- Health checking and service discovery (liveness vs readiness probes)\n- Timeout and deadline propagation across service layers\n- Request coalescing and deduplication\n- Multi-tenancy and isolation (noisy neighbor problems)\n\nFROM ION STOICA:\n\n- Stateless vs stateful serving (critical for scaling decisions)\n- Idempotency requirements for hedged/retry strategies\n- Circuit breakers for cascade failure prevention\n- Bulkhead pattern for failure isolation\n- Backpressure mechanisms beyond admission control\n\nFROM CHIP HUYEN:\n\n- Canary deployments and traffic shifting (1%→5%→25%→100%)\n- Health checks and Kubernetes readiness probes\n- Model versioning and rollback procedures\n- Distributed tracing and observability patterns\n\nFROM SONG HAN:\n\n- Speculative decoding implementation details (draft models, acceptance rates)\n- KV cache memory management and prefix caching\n- Model sharding strategies (tensor parallelism, pipeline parallelism)\n\nFROM VIJAY REDDI:\n\n- MLPerf inference Server scenario implementation at scale\n- Hierarchical edge-cloud serving patterns\n\n================================================================================\n\nCORE PRINCIPLE: Inference workloads vary DRAMATICALLY by model type.\nRecommendation systems dominate production inference volume, not LLMs.\n\nCRITICAL INSIGHT: By request volume, the ML inference landscape is:\n\n- Recommendation/ranking: ~80-90% of inference requests at major tech companies\n- Vision/image processing: ~5-10%\n- NLP/LLM: ~1-5% (but growing rapidly)\n\nMODEL-SPECIFIC INFERENCE CHARACTERISTICS:\n\n| Model Type      | Latency Target | Batching Strategy    | Key Bottleneck       |\n|-----------------|----------------|----------------------|----------------------|\n| Recommendation  | <10ms p99      | Feature-parallel     | Embedding lookup     |\n| Vision (CNN)    | 20-50ms        | Dynamic batching     | Compute-bound        |\n| LLM             | 100ms-seconds  | Continuous batching  | Memory bandwidth     |\n| Speech          | Real-time      | Streaming            | Sequential decode    |\n| Multimodal      | Varies         | Request-level        | Cross-modal sync     |\n\nREQUIRED COVERAGE FOR THIS CHAPTER:\n\nBATCHING STRATEGIES:\n\n- Static batching: Vision models, simpler serving\n- Dynamic batching: Variable request arrival, timeout-based\n- Continuous batching: LLM-specific (Orca paper), KV cache management\n- Feature batching: Recommendation systems, parallel feature lookup\n\nSERVING ARCHITECTURES:\n\n- Single-model serving: Most vision/NLP models\n- Ensemble serving: Recommendation pipelines (multiple models in sequence)\n- Cascade serving: Early-exit, model routing\n- Include: Why RecSys often needs 10+ models per request\n\nMODEL SHARDING FOR INFERENCE:\n\n- Tensor parallelism: LLM serving across GPUs\n- Embedding sharding: Recommendation serving\n- Include: Different sharding strategies for different model types\n\nLOAD BALANCING:\n\n- Request-level: Stateless models (vision, some NLP)\n- Session-level: Stateful models (conversational LLM)\n- Feature-level: Recommendation (route by user/item shards)\n\nCASE STUDIES TO INCLUDE:\n\n- Meta recommendation serving (billions of requests/day)\n- Netflix ranking system architecture\n- OpenAI API serving (LLM-specific challenges)\n- Google Search ranking (ensemble of models)\n- TikTok video recommendation (multimodal)\n\nLATENCY ANALYSIS DIVERSITY:\n\n- Include p50/p99/p999 for different model types\n- Show where latency budget goes (network, compute, memory)\n- Compare: RecSys (feature lookup dominates) vs LLM (decode dominates)\n\nANTI-PATTERNS TO AVOID:\n\n- Treating inference as synonymous with \"LLM serving\"\n- Ignoring embedding lookup latency (critical for RecSys)\n- Only discussing KV cache (LLM-specific optimization)\n- Forgetting that most production ML is NOT generative\n\n================================================================================\n-->\n\n# Inference at Scale {#sec-inference-at-scale}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A dynamic illustration of a global inference serving system handling millions of requests. The scene shows a central model representation being replicated across multiple serving nodes arranged in a globe-spanning network. Incoming request streams appear as flowing arrows from users worldwide, processed through load balancers depicted as traffic controllers, then distributed to model replicas. Visual elements include batching queues shown as organized request groups, latency meters displaying millisecond readings, and auto-scaling indicators showing nodes spawning and despawning. The composition emphasizes real-time responsiveness with clock symbols and speed indicators. Color palette features cool silvers and whites for infrastructure, warm oranges for active requests, and green for successful responses. Technical yet accessible style suitable for a production systems textbook._\n:::\n\n\\noindent\n![](images/png/cover_inference_at_scale.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why does serving machine learning predictions at scale require fundamentally different engineering approaches than training the models that generate them?_\n\nTraining optimizes for throughput over extended periods, tolerating latency variations that would be unacceptable when users await responses in real time. Inference at scale inverts these priorities: serving systems must deliver predictions within strict latency bounds while handling request volumes that fluctuate unpredictably across hours, days, and seasons. The engineering challenges of inference extend beyond raw performance to encompass resource efficiency when serving costs dwarf training costs over a model's lifetime, availability when users expect continuous service regardless of infrastructure conditions, and consistency when the same input must produce the same output across globally distributed serving infrastructure. Understanding how serving systems, batching strategies, model sharding, and load balancing address these challenges determines whether sophisticated models deliver value to users or remain impressive but impractical demonstrations. Mastering inference at scale transforms model capabilities into reliable services that operate continuously at the speed and scale users demand.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain why serving cost dominates training cost over a model's lifetime and apply the total cost of serving equation to infrastructure planning decisions\n\n- Compare and contrast batching strategies across model types (static for vision, continuous for LLMs, feature-parallel for recommendation systems) and select appropriate strategies based on workload characteristics\n\n- Apply the serving hierarchy framework (request, replica, service, platform) to decompose inference optimization problems and identify bottlenecks at each level\n\n- Design model sharding strategies for inference, distinguishing tensor parallelism, pipeline parallelism, expert parallelism, and embedding sharding by their communication patterns and use cases\n\n- Analyze load balancing algorithms using queuing theory, demonstrating why power-of-two-choices achieves exponentially better load distribution than random assignment\n\n- Implement KV cache management techniques including PagedAttention, prefix caching, and speculative decoding to optimize LLM serving throughput and memory efficiency\n\n- Evaluate autoscaling strategies by quantifying cold start latency components and designing predictive scaling policies that minimize both cost and SLO violations\n\n:::\n\n## Scaling Inference Beyond Single Machines {#sec-inference-scaling-beyond}\n\nVolume I established the foundations of model serving on a single machine: the inversion from training's throughput focus to serving's latency imperative, queuing theory fundamentals that govern system behavior under load, and the latency budget framework that reveals where time actually goes within a request. Those foundations assume a single server handling requests, which suffices for many deployments. This chapter addresses what happens when single-machine serving proves insufficient and examines the engineering principles that govern inference at scale across multiple machines.\n\nThe transition from single-machine to distributed inference parallels the transition from single-machine to distributed training, but with fundamentally different constraints. Training optimizes throughput over extended periods and tolerates latency variations measured in minutes or hours. Inference at scale must maintain strict latency bounds measured in milliseconds while handling request volumes that fluctuate unpredictably. This inversion of priorities transforms every design decision, from how requests are batched to how failures are handled.\n\nDistributed inference systems must solve problems that simply do not exist at single-machine scale. Load balancing becomes critical when requests must be distributed across hundreds of GPU instances while maintaining latency guarantees. Request routing must account for model-specific characteristics: a recommendation system with trillion-parameter embedding tables requires different placement strategies than a large language model that generates responses token by token. Autoscaling must anticipate demand fluctuations that can change request volume by orders of magnitude within minutes, all while maintaining latency bounds that users expect.\n\nThe economics of inference at scale also differ fundamentally from training economics. Training costs are dominated by compute time and can be amortized over the lifetime of the resulting model. Inference costs, by contrast, are directly tied to user traffic and revenue. An e-commerce recommendation system might serve millions of requests per second during peak shopping periods, with each request contributing directly to potential revenue. The cost of overprovisioning during quiet periods or underprovisioning during peaks translates immediately to business impact, making inference efficiency a first-order concern in ways that training efficiency rarely achieves.\n\nThis chapter develops the principles and techniques for building inference systems that scale to meet these demands. We examine when distributed inference becomes necessary, how to architect systems that maintain latency bounds under varying load, and how to optimize the economics of inference through efficient resource utilization. The goal is not merely to make inference work at scale, but to make it work efficiently, reliably, and economically.\n\n### When Single-Machine Serving Is Insufficient {#sec-inference-when-insufficient}\n\nThree distinct signals indicate when distributed inference becomes necessary rather than merely optional. Understanding these thresholds guides infrastructure decisions and prevents both premature complexity and delayed scaling.\n\n**Memory exhaustion** occurs when model parameters, key-value caches, or embedding tables exceed single-device capacity. A single NVIDIA H100 GPU provides 80GB of HBM3 memory. GPT-4 class models with hundreds of billions of parameters require 200-400GB just for weights in FP16 precision, forcing distribution across multiple GPUs regardless of throughput requirements. Recommendation systems with trillion-parameter embedding tables face similar constraints: Meta's DLRM model stores embedding tables that require multiple terabytes of memory.\n\n**Throughput limitations** emerge when request volume exceeds single-machine capacity even with optimal batching. Consider a recommendation system serving 100,000 queries per second with a 10ms latency budget. If single-machine throughput peaks at 10,000 QPS, no amount of optimization on that machine can satisfy demand. Horizontal scaling across multiple replicas becomes mandatory.\n\n**Latency requirements** drive distribution when model execution time exceeds latency budgets even at batch size one. Large language models generating responses token by token face this constraint acutely: a 70-billion parameter model requires approximately 140GB of memory and achieves roughly 30 tokens per second on a single GPU due to memory bandwidth limitations. Sharding the model across multiple GPUs enables parallel computation that reduces time-to-first-token below acceptable thresholds.\n\n| Constraint | Single-Machine Limit | Example Workload | Distribution Strategy |\n|------------|---------------------|------------------|----------------------|\n| Memory | 80GB (H100) | GPT-4 (400GB+) | Tensor/pipeline parallelism |\n| Throughput | ~10K QPS (vision) | 100K QPS RecSys | Horizontal replication |\n| Latency | Model execution time | 500ms LLM TTFT | Model sharding |\n\n: **Triggers for Distributed Inference**: Each constraint type indicates different distribution strategies. Memory constraints require model sharding; throughput constraints require replication; latency constraints may require either depending on whether the bottleneck is compute or memory bandwidth. {#tbl-distribution-triggers}\n\n### The Fundamental Inversion: Training vs Inference {#sec-inference-inversion}\n\nThe contrast between training and inference optimization extends beyond the throughput-latency distinction introduced in @sec-serving. At scale, this inversion manifests in system architecture, resource allocation, and operational priorities.\n\n| Aspect | Distributed Training | Distributed Inference |\n|--------|---------------------|----------------------|\n| Primary metric | Throughput (samples/hour) | Latency (P99 ms) |\n| Acceptable variance | Hours | Milliseconds |\n| State management | Checkpoints (periodic) | Session state (continuous) |\n| Batch formation | Large, controlled | Request-driven, variable |\n| Failure tolerance | Restart from checkpoint | Redirect without user impact |\n| Cost structure | Fixed duration, variable rate | Variable duration, fixed SLO |\n\n: **Training vs Inference System Requirements**: The fundamental inversion from throughput to latency optimization ripples through every aspect of system design. {#tbl-training-inference-inversion}\n\nTraining tolerates substantial latency variance because the optimization target is aggregate progress over hours or days. A training iteration that takes 2 seconds instead of the usual 1 second represents acceptable variation. An inference request that takes 2 seconds instead of 100 milliseconds represents catastrophic failure, potentially causing user abandonment or cascading timeouts in dependent services.\n\nState management differs fundamentally. Training maintains model state (parameters, optimizer states) that evolves gradually and can be captured in periodic checkpoints. Inference often maintains session state (conversation history, key-value caches, user context) that must be preserved across requests and cannot tolerate the staleness that checkpoint-based recovery would introduce.\n\nFailure handling diverges correspondingly. Training failures trigger checkpoint restoration and continuation, with minutes of lost progress being acceptable. Inference failures must be invisible to users: requests redirect to healthy replicas, degraded results substitute for unavailable models, and SLOs must be maintained despite infrastructure instability.\n\n### The Serving Tax: Overhead of Distribution {#sec-inference-serving-tax}\n\nDistributing inference across multiple machines introduces overhead absent from single-machine serving. This \"serving tax\" must be understood and budgeted within latency constraints.\n\n**Network communication** adds latency for every cross-machine interaction. Within a datacenter, network round-trip times range from 50-500 microseconds depending on topology and congestion. For model sharding that requires synchronization between GPUs on different machines, each synchronization point adds this overhead. A model sharded across 8 machines with 4 synchronization points per inference adds 200 microseconds to 2 milliseconds of network latency.\n\n**Serialization overhead** converts in-memory tensors to network-transmittable formats. While modern serialization libraries like FlatBuffers and Cap'n Proto minimize this overhead, large activation tensors still require meaningful time to serialize and deserialize. A 1GB activation tensor takes approximately 100 milliseconds to serialize, even with optimized libraries.\n\n**Load balancer latency** adds another layer. Requests must be routed to appropriate replicas, which requires examining request metadata, consulting routing tables, and forwarding to selected backends. Well-optimized load balancers add 100-500 microseconds; poorly configured ones can add milliseconds.\n\n**Coordination overhead** emerges when requests require fan-out to multiple services. A recommendation system that queries a user model, item model, and ranking model in parallel must coordinate these queries and aggregate results. The coordination logic itself consumes CPU cycles and introduces latency variation.\n\nThe total serving tax often consumes 10-30% of the latency budget in distributed systems:\n\n$$L_{total} = L_{compute} + L_{network} + L_{serialization} + L_{coordination} + L_{queuing}$$ {#eq-serving-tax}\n\nMinimizing this tax requires co-locating communicating components, using high-bandwidth interconnects, and designing communication patterns that minimize round trips.\n\n### Serving Cost Dominates Training Cost {#sec-inference-cost-dominance}\n\nA critical insight for infrastructure planning is that serving cost typically dominates training cost over a model's operational lifetime. This reversal from the training-centric view of model development has profound implications for where optimization effort should focus.\n\nThe total cost of operating a model comprises training cost (a one-time expense) and serving cost (an ongoing expense):\n\n$$C_{total} = C_{training} + C_{serving} \\times T_{deployment} \\times Q_{rate}$$ {#eq-total-cost}\n\nwhere $C_{training}$ is the one-time cost to train the model, $C_{serving}$ is the cost per query served, $T_{deployment}$ is the deployment duration in appropriate time units, and $Q_{rate}$ is the query rate.\n\n::: {.callout-note title=\"Worked Example: Cost Dominance Analysis\"}\n\nConsider a recommendation model with the following characteristics:\n\n**Training costs**:\n\n- 1,000 GPU-hours on H100 GPUs at $3/GPU-hour = $3,000\n- Data preparation and experimentation overhead (3x): $9,000\n- **Total training cost**: $12,000\n\n**Serving costs**:\n\n- Deployment duration: 2 years\n- Query rate: 10,000 QPS average\n- Cost per query: $0.00001 (on optimized infrastructure)\n\n**Total queries over lifetime**:\n$$Q_{total} = 10,000 \\text{ QPS} \\times 86,400 \\text{ s/day} \\times 730 \\text{ days} = 631 \\text{ billion queries}$$\n\n**Total serving cost**:\n$$C_{serving} = 631 \\times 10^9 \\times \\$0.00001 = \\$6,310,000$$\n\n**Ratio**: Serving cost is **526x** the training cost.\n\nEven reducing serving cost by 10% saves $631,000, far exceeding the entire training budget. This analysis explains why production ML teams often dedicate more engineering resources to serving optimization than training optimization.\n\n:::\n\nThe cost dominance ratio varies by application:\n\n| Application | Training Cost | Annual Serving Cost | Ratio |\n|-------------|--------------|-------------------|-------|\n| Recommendation (high QPS) | $10K-100K | $1M-10M | 100-1000x |\n| Search ranking | $100K-1M | $10M-100M | 100-1000x |\n| LLM API | $1M-100M | $10M-1B | 10-100x |\n| Internal analytics | $1K-10K | $10K-100K | 10-100x |\n\n: **Training vs Serving Cost Ratios**: High-QPS applications like recommendation systems show the most extreme cost dominance of serving over training. {#tbl-cost-ratios}\n\nThis cost structure motivates the optimization techniques throughout this chapter. Every percentage point of serving efficiency improvement yields ongoing cost reduction over the model's operational lifetime.\n\n### The Inference Landscape: Beyond LLMs {#sec-inference-landscape}\n\nA critical misconception in current discourse frames inference at scale as synonymous with LLM serving. While large language models present distinctive challenges and attract significant attention, they represent a small fraction of production inference volume. Understanding the full inference landscape is essential for appropriate technique selection.\n\n::: {.callout-important title=\"Production Inference by Request Volume\"}\n\nBy request count, production ML inference at major technology companies breaks down approximately as:\n\n- **Recommendation and ranking**: 80-90% of requests\n- **Vision and image processing**: 5-10% of requests\n- **NLP/LLM**: 1-5% of requests (but growing rapidly)\n- **Other (fraud detection, ads, etc.)**: 2-5% of requests\n\nSource: Industry reports from Meta, Google, and Netflix infrastructure teams.\n\n:::\n\nRecommendation systems dominate because they serve predictions for every user interaction: every page load, scroll, or click triggers inference. A user browsing an e-commerce site might generate 100 recommendation requests in a single session. In contrast, LLM queries typically require explicit user action and occur less frequently.\n\nThis distribution has important implications. Recommendation systems have driven most production inference innovation: dynamic batching, embedding sharding, feature store architectures, and low-latency serving were developed primarily for recommendation workloads. LLM-specific techniques like continuous batching and KV cache management are important but address a narrower slice of production inference.\n\n| Model Type | Request Volume | Latency Target | Key Challenge |\n|------------|---------------|----------------|---------------|\n| Recommendation | Very high (80-90%) | <10ms P99 | Embedding lookup |\n| Vision (CNN) | Moderate (5-10%) | 20-100ms | Batch efficiency |\n| LLM | Lower (1-5%) | 100ms-10s | Memory bandwidth |\n| Speech/Audio | Lower | Real-time | Sequential decode |\n| Multimodal | Growing | Varies | Cross-modal sync |\n\n: **Production Inference Landscape**: Different model types have different volume, latency requirements, and optimization challenges. Technique selection must match the specific workload. {#tbl-inference-landscape}\n\n### The Serving Hierarchy {#sec-inference-serving-hierarchy}\n\nTo organize the optimization techniques in this chapter, we introduce the serving hierarchy as a conceptual framework. Like the memory hierarchy in computer architecture, the serving hierarchy identifies distinct levels at which optimization occurs, each with different targets and techniques.\n\n**Request level**: Optimizations that affect individual request processing. Batching strategies, caching, and preprocessing optimizations operate at this level. The target metric is per-request latency.\n\n**Replica level**: Optimizations within a single model instance. GPU utilization, memory management, and model optimization operate here. The target metric is single-replica throughput.\n\n**Service level**: Optimizations across multiple replicas of the same model. Load balancing, request routing, and replica management operate at this level. The target metric is aggregate service throughput while meeting latency SLOs.\n\n**Platform level**: Optimizations across multiple services and tenants. Resource allocation, multi-tenancy, scheduling, and cluster management operate here. The target metric is overall resource efficiency while meeting diverse SLO requirements.\n\n```\nPlatform Level    [Multi-tenancy, Scheduling, Resource Allocation]\n      │\n      ▼\nService Level     [Load Balancing, Routing, Autoscaling]\n      │\n      ▼\nReplica Level     [GPU Utilization, Memory Management]\n      │\n      ▼\nRequest Level     [Batching, Caching, Preprocessing]\n```\n\nEach level has distinct optimization levers:\n\n| Level | Optimization Target | Key Techniques |\n|-------|-------------------|----------------|\n| Request | Per-request latency | Dynamic batching, caching, prefetching |\n| Replica | Throughput, utilization | Memory optimization, kernel fusion |\n| Service | Aggregate capacity | Load balancing, routing, autoscaling |\n| Platform | Resource efficiency | Multi-tenancy, scheduling, placement |\n\n: **Serving Hierarchy Optimization Targets**: Each level of the hierarchy addresses different metrics with different techniques. {#tbl-serving-hierarchy}\n\nThe remainder of this chapter progresses through these levels: batching and caching (request level), model sharding (replica level), load balancing and autoscaling (service level), and multi-tenancy (platform level).\n\n### Chapter Roadmap {#sec-inference-roadmap}\n\nThis chapter develops the techniques for inference at scale through the lens of the serving hierarchy:\n\n**Batching Strategies at Scale** (@sec-inference-batching) examines how different model types require fundamentally different batching approaches. We contrast static batching for vision models, continuous batching for LLMs, and feature-parallel batching for recommendation systems, providing quantitative analysis of throughput-latency tradeoffs.\n\n**Model Sharding for Inference** (@sec-inference-sharding) addresses when and how to distribute model computation across multiple devices. We examine tensor parallelism, pipeline parallelism, expert parallelism, and embedding sharding, with emphasis on communication patterns and overhead.\n\n**Load Balancing and Request Routing** (@sec-inference-load-balancing) develops the theory and practice of distributing requests across replicas. We derive why power-of-two-choices achieves exponentially better load distribution than random assignment and examine routing strategies for stateful workloads.\n\n**KV Cache Management** (@sec-inference-kv-cache) focuses on the memory management challenges specific to autoregressive language models, including PagedAttention, prefix caching, and speculative decoding.\n\n**Multi-Tenancy and Isolation** (@sec-inference-multitenancy) examines platform-level concerns: sharing infrastructure across multiple models and users while maintaining isolation and fairness.\n\n**Autoscaling** (@sec-inference-autoscaling) addresses dynamic capacity management, including the cold start problem unique to GPU-based serving and predictive scaling strategies.\n\n**Case Studies** (@sec-inference-case-studies) grounds these principles in production systems at Meta, OpenAI, Google, and TikTok, demonstrating how the techniques combine in real deployments.\n\nThroughout, we maintain the model-type diversity essential for practitioners: every major concept is illustrated across LLMs, recommendation systems, vision models, and other production workloads.\n\n## Serving Framework Selection {#sec-inference-frameworks}\n\nBefore examining specific serving techniques, practitioners must select appropriate serving infrastructure. The choice of serving framework determines which optimizations are available, how models are deployed, and what performance characteristics are achievable. This section provides a systematic framework for selecting among the major options.\n\n### Framework Categories {#sec-inference-framework-categories}\n\nServing frameworks fall into distinct categories based on their design philosophy and target workloads:\n\n**General-purpose inference servers** provide broad model support with configurable optimization:\n\n- **Triton Inference Server** (NVIDIA): Multi-framework support (PyTorch, TensorFlow, ONNX, TensorRT), dynamic batching, model ensemble orchestration, concurrent model execution\n- **TensorFlow Serving**: Native TensorFlow support, gRPC/REST APIs, model versioning, batching scheduler\n- **TorchServe** (PyTorch): Native PyTorch support, model archiving, metrics, multi-model serving\n\n**LLM-specialized servers** optimize specifically for autoregressive generation:\n\n- **vLLM**: PagedAttention, continuous batching, tensor parallelism, OpenAI-compatible API\n- **TensorRT-LLM**: NVIDIA-optimized kernels, in-flight batching, quantization, multi-GPU support\n- **Text Generation Inference (TGI)**: Hugging Face integration, flash attention, tensor parallelism, watermarking\n\n**Optimization-focused runtimes** maximize inference speed through compilation:\n\n- **TensorRT**: Graph optimization, kernel fusion, precision calibration, NVIDIA GPU specific\n- **ONNX Runtime**: Cross-platform optimization, execution providers for different hardware\n- **OpenVINO**: Intel hardware optimization, model compression, heterogeneous execution\n\n### Framework Selection Criteria {#sec-inference-framework-criteria}\n\nSelection depends on model type, deployment constraints, and organizational factors:\n\n**Model architecture determines primary candidates**:\n\n| Model Type | Primary Options | Key Consideration |\n|------------|-----------------|-------------------|\n| LLM (>7B params) | vLLM, TensorRT-LLM, TGI | KV cache management, continuous batching |\n| LLM (<7B params) | vLLM, TGI, Triton | Simpler deployment, less memory pressure |\n| Vision (CNN/ViT) | Triton, TensorRT, ONNX RT | Static batching, throughput optimization |\n| Recommendation | Triton, custom | Feature preprocessing, embedding lookup |\n| Multi-modal | Triton, custom | Pipeline orchestration |\n\n**Hardware constraints narrow options**:\n\n| Hardware | Supported Frameworks |\n|----------|---------------------|\n| NVIDIA datacenter GPU | All options |\n| NVIDIA consumer GPU | vLLM, TGI (limited TensorRT-LLM) |\n| AMD GPU | vLLM (ROCm), ONNX RT |\n| Intel CPU/GPU | OpenVINO, ONNX RT |\n| Apple Silicon | MLX, Core ML, ONNX RT |\n| AWS Inferentia | Neuron SDK |\n\n**Operational requirements influence choice**:\n\n- **Multi-model serving**: Triton excels with concurrent model execution\n- **Rapid iteration**: TorchServe, TGI offer simpler deployment cycles\n- **Maximum throughput**: TensorRT-LLM, vLLM with optimized kernels\n- **Cross-platform**: ONNX Runtime provides broadest hardware support\n\n### vLLM Architecture {#sec-inference-vllm}\n\nvLLM has emerged as the leading open-source LLM serving framework due to its PagedAttention innovation. Understanding its architecture illustrates key LLM serving principles.\n\n**Core innovations**:\n\n1. **PagedAttention**: Virtual memory for KV cache (covered in @sec-inference-paged-attention)\n2. **Continuous batching**: Add/remove requests mid-generation\n3. **Optimized attention kernels**: Flash attention integration\n4. **Tensor parallelism**: Automatic model sharding across GPUs\n\n**Architecture overview**:\n\n```\n┌─────────────────────────────────────────────────────┐\n│                   vLLM Engine                        │\n├─────────────────────────────────────────────────────┤\n│  Scheduler          │  Block Manager                │\n│  - Request queue    │  - Physical blocks            │\n│  - Preemption       │  - Block tables               │\n│  - Priority         │  - Copy-on-write              │\n├─────────────────────┼───────────────────────────────┤\n│  Model Executor     │  Cache Engine                 │\n│  - Attention        │  - GPU cache                  │\n│  - Sampling         │  - CPU swap space             │\n│  - Tensor parallel  │  - Prefix caching             │\n└─────────────────────┴───────────────────────────────┘\n```\n\n**Deployment example**:\n\n```python\nfrom vllm import LLM, SamplingParams\n\n# Initialize with automatic GPU detection\nllm = LLM(\n    model=\"meta-llama/Llama-2-70b-hf\",\n    tensor_parallel_size=4,  # Shard across 4 GPUs\n    gpu_memory_utilization=0.9,\n    max_model_len=4096,\n)\n\n# Efficient batch inference\nprompts = [\"Explain quantum computing\", \"Write a poem about AI\"]\nsampling_params = SamplingParams(temperature=0.7, max_tokens=256)\noutputs = llm.generate(prompts, sampling_params)\n```\n\n**Performance characteristics**:\n\n| Metric | Typical Value | Notes |\n|--------|---------------|-------|\n| Throughput vs baseline | 2-4x | Compared to naive HF generation |\n| Memory efficiency | 90%+ utilization | PagedAttention eliminates fragmentation |\n| Latency overhead | <5ms | Scheduling and batching overhead |\n| Max concurrent requests | 100s-1000s | Depends on model size and GPU memory |\n\n### TensorRT-LLM Architecture {#sec-inference-tensorrt-llm}\n\nTensorRT-LLM provides NVIDIA-optimized LLM inference with deep hardware integration.\n\n**Core capabilities**:\n\n1. **Optimized kernels**: Custom CUDA kernels for attention, GEMM, and layer norms\n2. **In-flight batching**: NVIDIA's continuous batching implementation\n3. **Quantization**: INT8, INT4, FP8 with minimal accuracy loss\n4. **Multi-GPU**: Tensor and pipeline parallelism with NVLink optimization\n\n**Build and deployment workflow**:\n\n```bash\n# Step 1: Convert model to TensorRT-LLM format\npython convert_checkpoint.py \\\n    --model_dir /models/llama-70b \\\n    --output_dir /models/llama-70b-trt \\\n    --dtype float16 \\\n    --tp_size 4\n\n# Step 2: Build optimized engine\ntrtllm-build \\\n    --checkpoint_dir /models/llama-70b-trt \\\n    --output_dir /engines/llama-70b \\\n    --gemm_plugin float16 \\\n    --max_batch_size 64 \\\n    --max_input_len 2048 \\\n    --max_output_len 512\n\n# Step 3: Deploy with Triton\n# (Configuration in model_repository/)\n```\n\n**Performance comparison with vLLM**:\n\n| Scenario | TensorRT-LLM | vLLM | Winner |\n|----------|--------------|------|--------|\n| A100 throughput | Higher | Good | TensorRT-LLM |\n| H100 throughput | Highest | High | TensorRT-LLM |\n| Deployment simplicity | Complex | Simple | vLLM |\n| Model support | NVIDIA curated | Broad HF | vLLM |\n| Quantization options | Extensive | Good | TensorRT-LLM |\n\nTensorRT-LLM typically achieves 20-50% higher throughput than vLLM on NVIDIA hardware but requires more complex deployment pipelines.\n\n### Triton Inference Server {#sec-inference-triton}\n\nTriton provides enterprise-grade multi-model serving with sophisticated orchestration capabilities.\n\n**Key features for production**:\n\n1. **Multi-framework**: Single server hosts PyTorch, TensorFlow, TensorRT, ONNX models\n2. **Dynamic batching**: Configurable batching with latency targets\n3. **Model ensembles**: Chain models in inference pipelines\n4. **Concurrent execution**: Multiple models share GPU resources\n5. **Metrics and monitoring**: Prometheus integration, detailed latency breakdown\n\n**Model repository structure**:\n\n```\nmodel_repository/\n├── text_encoder/\n│   ├── config.pbtxt\n│   └── 1/\n│       └── model.onnx\n├── image_classifier/\n│   ├── config.pbtxt\n│   └── 1/\n│       └── model.plan  # TensorRT engine\n└── ensemble_pipeline/\n    ├── config.pbtxt    # Orchestrates above models\n    └── 1/\n```\n\n**Dynamic batching configuration**:\n\n```protobuf\n# config.pbtxt\ndynamic_batching {\n    preferred_batch_size: [4, 8, 16, 32]\n    max_queue_delay_microseconds: 100000  # 100ms max wait\n}\ninstance_group [\n    {\n        count: 2\n        kind: KIND_GPU\n        gpus: [0, 1]\n    }\n]\n```\n\n**Use cases where Triton excels**:\n\n- Multi-model pipelines (e.g., detection → classification → ranking)\n- Mixed workloads on shared GPU clusters\n- Organizations with diverse model frameworks\n- Production systems requiring detailed observability\n\n### Framework Selection Decision Tree {#sec-inference-framework-decision}\n\n```\nStart\n  │\n  ├─ Is this an LLM (autoregressive generation)?\n  │   ├─ Yes → Is maximum throughput critical?\n  │   │         ├─ Yes, NVIDIA hardware → TensorRT-LLM\n  │   │         └─ No, or mixed hardware → vLLM\n  │   │\n  │   └─ No → Is this multi-model serving?\n  │           ├─ Yes → Triton Inference Server\n  │           └─ No → What's the deployment target?\n  │                   ├─ NVIDIA GPU → TensorRT + Triton\n  │                   ├─ Intel → OpenVINO\n  │                   ├─ Cross-platform → ONNX Runtime\n  │                   └─ Edge/Mobile → Platform-specific (Core ML, TFLite)\n```\n\n**Common deployment patterns**:\n\n| Pattern | Frameworks | Use Case |\n|---------|-----------|----------|\n| LLM API service | vLLM + nginx | ChatGPT-like applications |\n| High-throughput LLM | TensorRT-LLM + Triton | Batch processing, enterprise |\n| Vision pipeline | TensorRT + Triton | Object detection, classification |\n| Recommendation | Triton + custom embedding | E-commerce, content platforms |\n| Multi-modal | Triton ensemble | Vision-language, document understanding |\n\n### Framework Performance Benchmarking {#sec-inference-framework-benchmarks}\n\nWhen evaluating frameworks, benchmark on representative workloads:\n\n**LLM benchmark methodology**:\n\n```python\n# Standard benchmark parameters\nbenchmark_config = {\n    \"input_lengths\": [128, 512, 2048],\n    \"output_lengths\": [64, 256, 512],\n    \"batch_sizes\": [1, 8, 32, 64],\n    \"concurrent_requests\": [1, 10, 50, 100],\n    \"metrics\": [\"ttft\", \"tpot\", \"throughput\", \"gpu_util\"],\n}\n\n# Time to First Token (TTFT): Latency until first token generated\n# Time Per Output Token (TPOT): Average latency per subsequent token\n# Throughput: Total tokens/second across all requests\n# GPU utilization: Compute and memory utilization\n```\n\n**Representative benchmark results** (Llama-2-70B on 4xA100-80GB):\n\n| Framework | TTFT (ms) | TPOT (ms) | Throughput (tok/s) |\n|-----------|-----------|-----------|-------------------|\n| TensorRT-LLM | 180 | 28 | 2,400 |\n| vLLM | 220 | 32 | 1,900 |\n| TGI | 250 | 35 | 1,700 |\n| HF Transformers | 400 | 85 | 600 |\n\nNote: Results vary significantly with configuration, input/output lengths, and batch sizes. Always benchmark on your specific workload.\n\n### Migration and Integration Considerations {#sec-inference-framework-migration}\n\n**Migrating between frameworks**:\n\n- **Model compatibility**: Most frameworks support standard formats (HF, ONNX)\n- **API differences**: vLLM uses OpenAI-compatible API; Triton uses gRPC/HTTP\n- **Configuration translation**: Batching, parallelism settings differ by framework\n\n**Integration with ML infrastructure**:\n\n| Component | Integration Pattern |\n|-----------|-------------------|\n| Model registry | Pull models on startup, version management |\n| Feature store | Triton ensemble preprocessing, custom backends |\n| Monitoring | Prometheus metrics, distributed tracing |\n| Load balancer | Health checks, request routing |\n| Autoscaler | Custom metrics (queue depth, GPU utilization) |\n\nThe framework selection made here influences all subsequent serving optimizations. The techniques in following sections (batching, sharding, caching) are implemented differently across frameworks but follow the same underlying principles.\n\n### Orchestration Platforms for Production Serving {#sec-inference-orchestration}\n\nIndividual inference runtimes (vLLM, TensorRT-LLM, Triton) handle the mechanics of efficient inference on a single node or small cluster. Production systems require an **orchestration layer** that manages service composition, autoscaling, traffic management, fault tolerance, and multi-tenancy. Choosing an inference runtime without considering the orchestration layer is like choosing a database engine without considering connection pooling, replication, and query routing.\n\n**Key orchestration responsibilities**:\n\n1. **Service composition**: Combining multiple models and components into request pipelines\n2. **Autoscaling**: Scaling replicas based on traffic patterns and resource utilization\n3. **Traffic management**: Load balancing, canary deployments, A/B testing\n4. **Fault tolerance**: Replica health monitoring, automatic recovery\n5. **Multi-tenancy**: Isolating workloads and managing resource allocation across teams\n\n**Major orchestration platforms**:\n\n| Platform | Key Capability | Production Users |\n|----------|----------------|------------------|\n| **Ray Serve** | Scalable Python-native serving, composable deployments | OpenAI, Uber, Instacart |\n| **KServe** | Kubernetes-native serving, serverless inference | Bloomberg, Zillow, enterprises |\n| **BentoML** | ML model packaging and unified serving API | Various production deployments |\n| **Seldon Core** | Kubernetes deployment, A/B testing, canary releases | Financial services, retail |\n\n**Ray Serve architecture**:\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    Ray Serve Controller                      │\n├─────────────────────────────────────────────────────────────┤\n│  HTTP Proxy         │  Autoscaler           │  Router        │\n│  - Request routing  │  - Replica management │  - Load balance│\n│  - Request batching │  - Scale up/down      │  - Affinity    │\n├─────────────────────┼───────────────────────┴────────────────┤\n│                    Ray Actor Pool                            │\n│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐        │\n│  │ Replica 1│ │ Replica 2│ │ Replica 3│ │ Replica N│        │\n│  │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │ │ (vLLM)   │        │\n│  └──────────┘ └──────────┘ └──────────┘ └──────────┘        │\n└─────────────────────────────────────────────────────────────┘\n```\n\n**Production-ready vLLM deployment with Ray Serve**:\n\n```python\nfrom ray import serve\nfrom vllm import LLM, SamplingParams\n\n\n@serve.deployment(\n    num_replicas=4,\n    ray_actor_options={\"num_gpus\": 4},\n    autoscaling_config={\n        \"min_replicas\": 2,\n        \"max_replicas\": 16,\n        \"target_num_ongoing_requests_per_replica\": 10,\n    },\n)\nclass LLMDeployment:\n    def __init__(self):\n        self.llm = LLM(\n            model=\"meta-llama/Llama-2-70b-hf\", tensor_parallel_size=4\n        )\n        self.params = SamplingParams(temperature=0.7, max_tokens=256)\n\n    async def __call__(self, request):\n        return self.llm.generate([request.prompt], self.params)[0]\n\n\n# Deploy with automatic scaling\napp = LLMDeployment.bind()\nserve.run(app)\n```\n\nThis pattern provides automatic scaling from 2 to 16 replicas based on load, request batching at the serve layer, fault tolerance with automatic replica restart, and zero-downtime deployments.\n\n**Stateless vs stateful serving**: The orchestration layer must account for whether inference is stateless or stateful.\n\n| Serving Type | State Location | Scaling Model | Failure Recovery |\n|--------------|---------------|---------------|------------------|\n| **Stateless** | None or external | Horizontal, trivial | Redirect to any replica |\n| **Stateful** | In-process (KV cache) | Complex, sticky | Session loss or migration |\n\nVision models and embedding lookups are typically stateless: any replica can serve any request. LLM serving with KV cache is stateful: the cache accumulated during conversation creates replica-specific state.\n\n**Implications of stateful LLM serving**:\n\n- **Sticky routing required**: Subsequent requests in a conversation must reach the same replica holding the KV cache\n- **Failure recovery is complex**: When a stateful replica fails, cached state is lost. Options include regenerating cache from history (high latency), replicating cache to backups (high bandwidth), or accepting session restart (poor experience)\n- **Autoscaling must account for sessions**: Scaling down stateful replicas requires draining active sessions, which can take minutes for long conversations\n- **Memory sizing determines capacity**: Each active session consumes KV cache memory, limiting concurrent sessions per replica regardless of compute capacity\n\n**Consistency during model updates**:\n\nWhen updating models across distributed replicas, requests may hit different model versions during the rollout. Different deployment strategies provide different consistency guarantees:\n\n| Deployment Strategy | Consistency | Rollout Speed | Risk |\n|---------------------|-------------|---------------|------|\n| **Blue-green** | Strong (atomic switch) | Instant | High (all-or-nothing) |\n| **Canary** | Eventual (gradual shift) | Slow (hours) | Low (progressive) |\n| **Rolling update** | Weak (mixed versions) | Medium | Medium |\n\nFor applications requiring deterministic outputs (compliance, audit trails, reproducible debugging):\n\n1. **Pin model version**: Include version identifier in request routing to ensure same model handles related requests\n2. **Use temperature=0**: Eliminates sampling variance, though beam search still has implementation-dependent tiebreaking\n3. **Version-aware caching**: Cache responses with model version tags; invalidate on version change\n\n**Failure scenario: Stateful replica crash**\n\nWhen a stateful LLM replica crashes mid-conversation:\n\n```\n1. Load balancer detects health check failure (1-10 seconds)\n2. New requests route to healthy replicas\n3. In-flight requests fail; clients must retry\n4. Session state (KV cache) is lost\n5. Recovery options:\n   a. Regenerate: Client resends conversation history (high latency)\n   b. Redirect: Route to replica with replicated state (if available)\n   c. Restart: Begin new session (poor user experience)\n```\n\nProduction systems often accept option (a) with optimizations: the regenerated prefill can process the full conversation history in a single batch, taking seconds rather than the minutes the original conversation took.\n\n**Build vs buy: Managed serving services**:\n\nBefore selecting frameworks, teams must decide whether to self-host or use managed services:\n\n| Service | Provider | Key Features | Trade-offs |\n|---------|----------|--------------|------------|\n| **SageMaker Endpoints** | AWS | Managed hosting, autoscaling, A/B testing | Lock-in, cost, limited customization |\n| **Vertex AI Endpoints** | GCP | TPU support, traffic splitting, model monitoring | GCP ecosystem dependency |\n| **Azure ML Endpoints** | Azure | Enterprise integration, ONNX optimization | Azure ecosystem dependency |\n| **Anyscale Endpoints** | Anyscale | Ray-native, fine-grained autoscaling | Emerging platform |\n\n| Approach | Advantages | Disadvantages |\n|----------|------------|---------------|\n| Self-hosted (vLLM/Triton) | Full control, cost optimization at scale | Operational burden, expertise required |\n| Managed (SageMaker/Vertex) | Operational simplicity, integrated tooling | Lock-in, cost at scale, limited customization |\n| Hybrid (Ray Serve + cloud) | Flexibility, gradual migration | Complexity in managing both |\n\n**Decision factors**:\n\n- **Team size**: Teams with fewer than 5 ML engineers often benefit from managed services\n- **Scale**: More than 1M daily requests typically makes self-hosting cost-effective\n- **Customization needs**: Novel architectures require self-hosting\n- **Latency requirements**: Self-hosting enables co-location and deeper optimization\n\n**Enhanced framework selection with orchestration**:\n\n```\nStart\n  │\n  ├─ What scale?\n  │   ├─ <100 QPS → Simple deployment (managed services or single instance)\n  │   ├─ 100-10K QPS → Need autoscaling\n  │   │   ├─ Managed acceptable → SageMaker/Vertex\n  │   │   └─ Self-hosted required → Ray Serve + vLLM/TensorRT-LLM\n  │   └─ >10K QPS → Need distributed orchestration\n  │       ├─ LLM → Ray Serve + vLLM with sharding\n  │       ├─ RecSys → Custom or Triton with embedding sharding\n  │       └─ Vision → Triton with dynamic batching\n  │\n  ├─ How many model types?\n  │   ├─ Single model → Direct runtime deployment\n  │   └─ Multiple models/pipelines → Triton or Ray Serve composition\n  │\n  └─ Stateful or stateless?\n      ├─ Stateless → Any load balancer, simple scaling\n      └─ Stateful (LLM with cache) → Sticky routing, session management\n```\n\n**Case study: Startup serving Llama-70B for customer support**\n\n*Scenario*: A startup is launching an LLM-powered customer support chatbot using a fine-tuned Llama-70B model.\n\n| Constraint | Value |\n|------------|-------|\n| Expected traffic | 100 QPS average, 500 QPS peak |\n| Latency requirement | <2s time to first token |\n| Budget | 4 H100 GPUs (leased) |\n| Team | 2 ML engineers, no dedicated SRE |\n| Conversation length | Average 8 turns, max 32K context |\n\n*Analysis*:\n\n1. **Memory**: Llama-70B requires ~140GB in FP16. With 4-bit quantization (AWQ), this drops to ~35GB, fitting on a single H100 (80GB) with room for KV cache.\n\n2. **Throughput**: At 500 QPS peak with average 100 output tokens, the system must sustain 50,000 tokens/second. A single quantized Llama-70B on H100 achieves ~1,000 tokens/second with continuous batching, so 4 GPUs provide headroom.\n\n3. **Tensor parallelism vs replicas**: Two options exist:\n   - 2 replicas × 2-way TP: Higher availability, lower per-request latency\n   - 4 replicas × 1-way TP: Maximum throughput, simpler scaling\n\n   With AWQ fitting on single GPU, option (b) is preferred for this traffic level.\n\n4. **Team size**: 2 ML engineers without SRE experience suggests managed services or simple orchestration.\n\n*Decision*:\n\n| Option | Architecture | Pros | Cons |\n|--------|--------------|------|------|\n| A | SageMaker + HF TGI | Minimal ops burden | Cost, limited optimization |\n| B | vLLM + Ray Serve on EC2 | Good balance | Some ops required |\n| C | TensorRT-LLM + Triton | Maximum throughput | Complex, overkill for 500 QPS |\n\n**Recommendation**: Option B (vLLM + Ray Serve) provides the best balance. At 500 QPS, the 30% throughput advantage of TensorRT-LLM does not justify the deployment complexity. Start with vLLM; migrate to TensorRT-LLM only if traffic grows beyond 2,000 QPS.\n\n```python\n# Recommended production configuration\n@serve.deployment(\n    num_replicas=4,\n    ray_actor_options={\"num_gpus\": 1},\n    autoscaling_config={\n        \"min_replicas\": 2,\n        \"max_replicas\": 8,\n        \"target_num_ongoing_requests_per_replica\": 20,\n    },\n)\nclass CustomerSupportLLM:\n    def __init__(self):\n        self.llm = LLM(\n            model=\"your-finetuned-llama-70b-awq\",\n            quantization=\"awq\",\n            max_model_len=32768,\n        )\n```\n\nThis configuration handles the 500 QPS peak with 4 replicas, can scale to 8 during unexpected spikes, and scales down to 2 during low-traffic periods to reduce cost.\n\n## Batching Strategies at Scale {#sec-inference-batching}\n\nVolume I introduced batching fundamentals for single-machine serving: how dynamic batching trades latency for throughput, and how traffic patterns determine optimal batch sizes. At scale, batching becomes more complex because different model architectures have fundamentally different batching requirements. A strategy optimal for vision models may be catastrophic for LLMs, and techniques developed for recommendation systems may not apply to either.\n\n**Production reality**: Recommendation systems constitute 80-90% of inference requests at major technology companies, with vision models handling most of the remainder, and LLMs currently representing 1-5% of request volume (though growing rapidly). Despite this distribution, we present batching strategies in order of conceptual complexity: vision (straightforward batching), LLMs (continuous batching with KV cache), and recommendation (feature-parallel batching with distributed embedding). This pedagogical ordering builds understanding progressively, even though practitioners will most frequently encounter recommendation workloads first.\n\nThis section develops a taxonomy of batching strategies matched to model characteristics, providing quantitative analysis of when each approach applies and what performance to expect.\n\n### Why Batching Differs Across Model Types {#sec-inference-batching-differences}\n\nThe core insight is that batching efficiency depends on how computation scales with batch size relative to how memory and communication scale. Different model architectures exhibit different scaling relationships, requiring different batching strategies.\n\nFor **vision models** (CNNs, ViTs processing fixed-size images), computation scales linearly with batch size while memory scales sub-linearly due to weight sharing. Larger batches improve GPU utilization with minimal overhead, making static or dynamic batching with large batch sizes optimal.\n\nFor **LLMs in the decode phase**, computation per token is small relative to memory bandwidth requirements for loading model weights. The bottleneck is memory bandwidth, not compute. Larger batches amortize weight loading across more tokens, dramatically improving throughput but with diminishing returns as batch size grows.\n\nFor **recommendation systems**, the bottleneck is often embedding lookup rather than dense computation. Batching strategies must optimize for parallel embedding access patterns rather than matrix multiplication throughput.\n\n| Model Type | Batching Strategy | Typical Batch Size | Key Constraint | Throughput Scaling |\n|-----------|------------------|-------------------|----------------|-------------------|\n| Vision (CNN) | Static/Dynamic | 32-256 | GPU compute | Near-linear to 64+ |\n| LLM (prefill) | Dynamic | 1-64 | Memory capacity | Sub-linear |\n| LLM (decode) | Continuous | 100-1000s | Memory bandwidth | Log-linear |\n| RecSys | Feature-parallel | 1000-10000s | Embedding lookup | Depends on sharding |\n| Speech | Streaming | 1 | Real-time | N/A (latency-bound) |\n\n: **Batching Strategy by Model Type**: Each model type has characteristic batching behavior determined by its computational bottleneck. {#tbl-batching-by-model}\n\n### Static and Dynamic Batching for Vision Models {#sec-inference-static-dynamic-batching}\n\nVision models represent the simplest batching case because inputs have uniform size (after preprocessing) and computation follows a predictable pattern. The foundations from @sec-serving apply directly, with scale introducing considerations of batch formation across multiple replicas.\n\n**Static batching** collects exactly $B$ requests before processing. This maximizes GPU utilization when request arrival is predictable but causes unbounded latency during low-traffic periods.\n\n**Dynamic batching** collects requests for a maximum time window $T_{window}$ or until reaching maximum batch size $B_{max}$, whichever occurs first. The expected latency under Poisson arrivals with rate $\\lambda$ follows:\n\n$$E[L_{total}] = E[L_{queue}] + L_{batch} + L_{inference}(B)$$ {#eq-dynamic-batch-latency}\n\nwhere $E[L_{queue}]$ is the expected queuing delay, $L_{batch}$ is the batch formation delay (up to $T_{window}$), and $L_{inference}(B)$ is the inference time for batch size $B$.\n\n::: {.callout-note title=\"Worked Example: Dynamic Batching for ResNet-50 at Scale\"}\n\nConsider a vision classification service with the following requirements:\n\n- **Arrival rate**: 5,000 QPS\n- **Latency SLO**: 50ms P99\n- **Per-image inference time**: 5ms at batch=1, 25ms at batch=32\n- **Number of replicas**: 10 (each handling 500 QPS)\n\nFor a single replica with Poisson arrivals at $\\lambda = 500$ QPS:\n\n**Option A: No batching (batch=1)**\n\n- Service time: 5ms per request\n- Utilization: $\\rho = \\lambda \\times S = 500 \\times 0.005 = 2.5$ (impossible, system is overloaded)\n\nThis configuration cannot meet demand. Batching is required.\n\n**Option B: Dynamic batching with $B_{max}=16$, $T_{window}=10ms$**\n\nExpected requests per window: $E[B] = \\lambda \\times T_{window} = 500 \\times 0.01 = 5$\n\nWith 5 requests per batch:\n\n- Inference time: approximately 8ms (interpolating between batch=1 and batch=32)\n- Per-request compute: 8ms / 5 = 1.6ms\n- Maximum batch delay: 10ms\n- Expected total latency: ~15ms mean, ~30ms P99\n\nUtilization: $\\rho = 500 \\times 0.0016 = 0.8$ (sustainable)\n\n**Option C: Dynamic batching with $B_{max}=32$, $T_{window}=20ms$**\n\nExpected requests per window: $E[B] = 500 \\times 0.02 = 10$\n\nWith 10 requests per batch:\n\n- Inference time: approximately 12ms\n- Per-request compute: 12ms / 10 = 1.2ms\n- Maximum batch delay: 20ms\n- Expected total latency: ~22ms mean, ~42ms P99\n\nUtilization: $\\rho = 500 \\times 0.0012 = 0.6$ (comfortable)\n\n**Tradeoff**: Option C achieves 25% better throughput (lower utilization) at the cost of higher average latency (22ms vs 15ms). Both meet the 50ms P99 SLO.\n\n:::\n\nAt scale with multiple replicas, batch formation can occur either at individual replicas or at a centralized batching layer:\n\n**Replica-local batching**: Each replica independently forms batches from its assigned traffic. Simpler to implement but may result in uneven batch sizes across replicas when load is imbalanced.\n\n**Centralized batching**: A batching service collects requests and dispatches formed batches to replicas. Achieves more uniform batch sizes but adds a centralization bottleneck and additional network hop.\n\nProduction systems typically use replica-local batching with load balancing that ensures roughly equal traffic distribution, achieving the benefits of centralized batching without the complexity.\n\n### Continuous Batching for LLM Inference {#sec-inference-continuous-batching}\n\nAutoregressive language models present a unique batching challenge that static and dynamic approaches handle poorly. The key insight comes from the Orca paper: traditional batching forces all sequences in a batch to complete before any new sequences can join, wasting compute when sequences finish at different times.\n\nConsider a batch of 8 sequences. If one sequence completes after 10 tokens while others require 100 tokens, the completed sequence's GPU resources sit idle for 90 iterations. With traditional batching:\n\n$$\\text{Wasted compute} = \\frac{(100 - 10) \\times 1}{100 \\times 8} = 11.25\\%$$\n\nFor realistic output length distributions with high variance, wasted compute can exceed 50%.\n\n**Continuous batching** (also called iteration-level batching) decouples batch membership from iteration boundaries:\n\n1. At each decode iteration, check for completed sequences\n2. Remove completed sequences from the batch immediately\n3. Insert waiting sequences into freed slots\n4. Process the reorganized batch for the next iteration\n\nThis dynamic batch management maintains high GPU utilization regardless of sequence length variance.\n\nThe throughput improvement from continuous batching depends on sequence length distribution. For a distribution with coefficient of variation $CV = \\sigma / \\mu$:\n\n$$\\text{Throughput gain} \\approx 1 + \\frac{CV^2}{2}$$ {#eq-continuous-batching-gain}\n\nWith typical LLM output lengths having $CV \\approx 1.0$, continuous batching achieves approximately 1.5x throughput improvement. For highly variable outputs (conversational vs. code generation), gains can reach 2-4x.\n\n::: {.callout-note title=\"Implementation: Continuous Batching in vLLM\"}\n\nvLLM implements continuous batching with the following key mechanisms:\n\n**Iteration-level scheduling**: At each decode step, the scheduler evaluates:\n\n- Which sequences have generated end-of-sequence tokens (remove from batch)\n- Which waiting sequences can fit in available KV cache slots (add to batch)\n- Which sequences should be preempted if memory pressure exists (swap to CPU)\n\n**Memory management**: The KV cache is managed using PagedAttention (see @sec-inference-kv-cache), which enables dynamic allocation without fragmentation. When a sequence completes, its KV cache pages are immediately available for new sequences.\n\n**Batched decode kernel**: Despite dynamic batch composition, the decode kernel processes all active sequences in a single batched operation. Sequences at different generation lengths are padded to a common shape within the kernel.\n\n**Typical performance (Llama-2 70B on 8xA100)**:\n\n| Batching Strategy | Throughput (tokens/s) | GPU Utilization |\n|------------------|----------------------|-----------------|\n| Static (batch=8) | 400 | 45% |\n| Dynamic (timeout=50ms) | 580 | 65% |\n| Continuous | 1,200 | 92% |\n\nThe 3x throughput improvement from continuous batching comes from eliminating idle GPU cycles during sequence length variation.\n\n:::\n\n### Prefill vs Decode: The Two-Phase Challenge {#sec-inference-prefill-decode}\n\nLLM inference consists of two distinct phases with different computational characteristics, requiring different batching strategies within the same request:\n\n**Prefill phase**: Process the entire input prompt in parallel. Computation scales with prompt length. Memory access pattern is compute-bound (high arithmetic intensity).\n\n**Decode phase**: Generate output tokens one at a time. Each token requires loading entire model weights. Memory access pattern is bandwidth-bound (low arithmetic intensity).\n\n| Phase | Computation | Memory Access | Bottleneck | Optimal Batch |\n|-------|-------------|---------------|------------|---------------|\n| Prefill | O(prompt_length²) | Weight loading | Compute | Small (1-8) |\n| Decode | O(1) per token | Weight loading | Bandwidth | Large (100s) |\n\n: **Prefill vs Decode Characteristics**: The two phases have opposite optimization requirements. {#tbl-prefill-decode}\n\nThis dichotomy creates a scheduling challenge: prefill operations are long-running and compute-intensive, while decode operations are short and bandwidth-limited. Mixing them in the same batch can cause interference.\n\n**Chunked prefill** addresses this by breaking long prompts into fixed-size chunks that interleave with decode operations:\n\n$$\\text{Chunk latency} = \\frac{\\text{Chunk size}}{\\text{Prefill throughput}}$$\n\nWith chunk size chosen to match decode iteration time, prefill and decode can share GPU resources without decode latency spikes.\n\n**Prefill-decode disaggregation** takes this further by running prefill and decode on separate GPU pools:\n\n- Prefill pool: Optimized for compute (larger batch sizes, no KV cache persistence)\n- Decode pool: Optimized for bandwidth (small batches, maximum KV cache capacity)\n\nThis separation enables independent scaling: prefill capacity scales with input volume while decode capacity scales with output volume.\n\n::: {.callout-note title=\"Sarathi: Chunked Prefill Implementation\"}\n\nThe Sarathi system implements chunked prefill with the following design:\n\n**Chunk sizing**: Chunks are sized to complete in approximately the same time as one decode iteration (typically 10-50ms). For a prefill throughput of 10,000 tokens/second, a 20ms chunk processes 200 tokens.\n\n**Interleaving schedule**: Each GPU iteration processes either:\n\n- One prefill chunk for a new request, OR\n- One decode step for all active sequences\n\nThis ensures decode latency remains bounded regardless of incoming prompt lengths.\n\n**KV cache transfer**: When prefill completes, the generated KV cache transfers to decode slots. With NVLink, this transfer adds <1ms for typical prompt lengths.\n\n**Performance impact**:\n\n- Without chunking: Long prompts cause decode latency spikes of 100ms+\n- With chunking: Decode latency bounded to 30ms P99 regardless of prompt length\n\n:::\n\n### Feature-Parallel Batching for Recommendation Systems {#sec-inference-feature-parallel-batching}\n\nRecommendation systems have fundamentally different batching requirements than vision or language models. The computation pattern involves:\n\n1. **Sparse feature lookup**: Retrieve embeddings for user, item, and context features\n2. **Dense feature processing**: Transform and normalize dense features\n3. **Feature interaction**: Compute interactions between features (often via attention or factorization)\n4. **Ranking head**: Produce final scores\n\nThe sparse embedding lookup often dominates latency and determines batching strategy.\n\n**Feature-parallel batching** processes different feature types in parallel rather than batching entire requests:\n\n```\nRequest 1: [user_id_1, item_ids_1, context_1]\nRequest 2: [user_id_2, item_ids_2, context_2]\nRequest 3: [user_id_3, item_ids_3, context_3]\n\nFeature-parallel view:\nUser embeddings:  [lookup(user_1), lookup(user_2), lookup(user_3)]  → parallel\nItem embeddings:  [lookup(items_1), lookup(items_2), lookup(items_3)]  → parallel\nContext features: [process(ctx_1), process(ctx_2), process(ctx_3)]  → parallel\n\nThen: Combine features per request for ranking\n```\n\nThis parallelization is natural when embeddings are sharded across servers: each embedding server handles lookups for its shard across all requests in the batch.\n\n::: {.callout-note title=\"Worked Example: Recommendation System Batching at Meta Scale\"}\n\nConsider Meta's recommendation infrastructure serving 10 million QPS across the platform:\n\n**Request characteristics**:\n\n- Each request queries ~100 items (candidate ranking)\n- Each item requires 50 embedding lookups (user features, item features, cross features)\n- Total embedding lookups: 5,000 per request\n- Embedding table size: 100TB across 1,000 shards\n\n**Batching strategy**:\n\nWith 10M QPS and 1,000 embedding shards, each shard receives:\n\n$$\\text{Lookups per shard} = \\frac{10M \\times 5000}{1000} = 50 \\text{ billion lookups/sec}$$\n\nThis is clearly infeasible for single-threaded processing. Instead:\n\n**Batch accumulation window**: 1ms\n**Requests per batch**: 10,000 (at 10M QPS)\n**Lookups per shard per batch**: 50M\n\nEach embedding shard processes 50M lookups in a batched operation, achieving memory bandwidth utilization of 90%+ through sequential memory access patterns.\n\n**Latency breakdown**:\n\n| Phase | Duration | Notes |\n|-------|----------|-------|\n| Request routing | 0.2ms | Consistent hashing to shard |\n| Batch accumulation | 0.5ms (avg) | 1ms window |\n| Embedding lookup | 2ms | Batched, SSD-backed |\n| Feature processing | 1ms | Dense computation |\n| Ranking model | 1.5ms | Final scoring |\n| **Total** | **5.2ms** | Within 10ms SLO |\n\n:::\n\n### Streaming Inference for Real-Time Applications {#sec-inference-streaming}\n\nSome applications cannot tolerate batching delay of any kind. Real-time speech recognition, video analysis, and robotics require processing inputs as they arrive with minimal latency.\n\n**Streaming inference** processes inputs incrementally without waiting for batch formation:\n\n- **Speech**: Process audio frames (10-20ms chunks) as they arrive from the microphone\n- **Video**: Process frames at capture rate (30-60 FPS) without buffering\n- **Robotics**: Process sensor readings at control loop frequency (100-1000 Hz)\n\nFor streaming applications, the relevant metric is not throughput but **time to process each input**:\n\n$$L_{streaming} = L_{capture} + L_{transfer} + L_{inference} + L_{action}$$\n\nwhere all components must complete within the inter-frame interval.\n\n::: {.callout-note title=\"Streaming Speech Recognition Pipeline\"}\n\nConsider a streaming speech-to-text system with 20ms audio frames:\n\n**Latency budget**: 100ms end-to-end (5 frames of delay)\n\n**Pipeline stages**:\n\n| Stage | Duration | Notes |\n|-------|----------|-------|\n| Audio capture | 0ms (continuous) | Microphone buffer |\n| Network to server | 20ms | Including jitter buffer |\n| Feature extraction | 5ms | MFCC computation |\n| Encoder inference | 30ms | Streaming Conformer |\n| Decoder step | 15ms | Autoregressive CTC |\n| Text formatting | 5ms | Capitalization, punctuation |\n| Network to client | 15ms | Response transmission |\n| **Total** | **90ms** | Within 100ms budget |\n\n**Key constraints**:\n\n- No batching: Each frame processes individually\n- Stateful model: Encoder maintains context across frames\n- Pipeline parallelism: While frame N is in decoder, frame N+1 is in encoder\n\nGPU utilization is typically 30-50% for streaming workloads, traded for latency guarantee.\n\n:::\n\n### Adaptive Batching Strategies {#sec-inference-adaptive-batching}\n\nProduction systems rarely use fixed batching parameters. Instead, they adapt batching behavior based on current conditions:\n\n**Traffic-adaptive batching** adjusts batch window based on arrival rate:\n\n$$T_{window} = \\min\\left(T_{max}, \\frac{B_{target}}{\\lambda_{current}}\\right)$$\n\nWhen traffic is high, the window shrinks because the target batch size fills quickly. When traffic is low, the window extends but is capped to bound maximum latency.\n\n**SLO-adaptive batching** monitors latency percentiles and adjusts batching aggressively:\n\n```\nif P99_latency > 0.9 * SLO:\n    reduce B_max by 20%\n    reduce T_window by 20%\nelif P99_latency < 0.5 * SLO:\n    increase B_max by 10%\n    increase T_window by 10%\n```\n\nThis feedback loop maintains latency headroom while maximizing throughput during normal operation.\n\n**Request-aware batching** considers request characteristics when forming batches. For LLMs:\n\n- Group requests by expected output length (inferred from prompt type)\n- Group requests by prompt length to minimize padding\n- Prioritize latency-sensitive requests in smaller batches\n\n::: {.callout-note title=\"Production Adaptive Batching: The NVIDIA Triton Approach\"}\n\nTriton Inference Server implements adaptive batching with three configurable parameters:\n\n1. **max_batch_size**: Upper bound on batch size\n2. **batching_timeout_ms**: Maximum time to wait for batch formation\n3. **preferred_batch_size**: Target batch sizes that align with kernel efficiency\n\nThe scheduler maintains separate queues for each preferred batch size and routes requests to minimize total latency:\n\n$$\\text{Queue selection} = \\arg\\min_{q} \\left( \\text{wait}_q + \\text{exec}(|q| + 1) \\right)$$\n\nThis optimization considers both the current queue length and the efficiency of the resulting batch size.\n\n**Observed behavior on ResNet-50 (V100)**:\n\n| Traffic Level | Avg Batch Size | Avg Latency | Throughput |\n|--------------|---------------|-------------|------------|\n| 100 QPS | 2.1 | 8ms | 100 QPS |\n| 500 QPS | 6.3 | 12ms | 500 QPS |\n| 1000 QPS | 12.4 | 18ms | 1000 QPS |\n| 2000 QPS | 24.1 | 28ms | 1980 QPS |\n\nThe system automatically increases batch size to maintain throughput as traffic grows.\n\n:::\n\n### Quantitative Summary: Batching Strategy Selection {#sec-inference-batching-summary}\n\nThe choice of batching strategy depends on model characteristics, traffic patterns, and latency requirements. The following decision framework guides selection:\n\n```\nIs the model autoregressive (LLM, speech)?\n├─ Yes → Continuous batching with prefill chunking\n└─ No → Does the model have embedding lookups dominating latency?\n        ├─ Yes → Feature-parallel batching (RecSys)\n        └─ No → Dynamic batching with adaptive parameters\n```\n\nFor each strategy, the key parameters to tune are:\n\n| Strategy | Key Parameters | Tuning Goal |\n|----------|---------------|-------------|\n| Static | Batch size | Maximize throughput |\n| Dynamic | Window, max batch | Balance latency vs throughput |\n| Continuous | Chunk size, max batch | Minimize decode latency variance |\n| Feature-parallel | Accumulation window | Match embedding shard capacity |\n| Streaming | Pipeline depth | Meet real-time deadline |\n\n: **Batching Strategy Parameters**: Each strategy has distinct parameters requiring tuning for the specific deployment. {#tbl-batching-parameters}\n\n## Model Sharding for Inference {#sec-inference-sharding}\n\nWhen models exceed single-GPU memory or when latency requirements demand parallel computation, model sharding distributes inference across multiple devices. Unlike training, where throughput is the primary concern, inference sharding must carefully balance parallelization benefits against communication overhead within strict latency budgets.\n\nThis section examines four sharding strategies, each suited to different model architectures and deployment requirements: tensor parallelism for attention-heavy models, pipeline parallelism for sequential architectures, expert parallelism for mixture-of-experts models, and embedding sharding for recommendation systems.\n\n### When Sharding Becomes Necessary {#sec-inference-sharding-when}\n\nModel sharding for inference is driven by two distinct requirements:\n\n**Memory requirements**: A model that cannot fit in single-GPU memory must be sharded regardless of performance considerations. For a model with $P$ parameters at precision $b$ bits:\n\n$$\\text{Memory}_{weights} = P \\times \\frac{b}{8} \\text{ bytes}$$ {#eq-weight-memory}\n\nA 70-billion parameter model in FP16 (16 bits) requires:\n\n$$\\text{Memory} = 70 \\times 10^9 \\times \\frac{16}{8} = 140 \\text{ GB}$$\n\nThis exceeds the 80GB capacity of an H100 GPU, requiring at minimum 2-way sharding.\n\n**Latency requirements**: Even when a model fits in memory, sharding can reduce latency by parallelizing computation. The potential speedup depends on the parallelization efficiency:\n\n$$T_{parallel} = \\frac{T_{sequential}}{P} + T_{communication}$$ {#eq-parallel-time}\n\nwhere $P$ is the parallelism degree and $T_{communication}$ is the synchronization overhead. Sharding provides latency benefit only when the communication overhead is smaller than the time saved through parallelization.\n\n| Sharding Trigger | Model Examples | Minimum Sharding | Strategy |\n|-----------------|----------------|------------------|----------|\n| Memory (weights) | Llama-70B (140GB) | 2-way | Tensor or pipeline |\n| Memory (KV cache) | GPT-4 (long context) | 4-8 way | Tensor (for cache) |\n| Memory (embeddings) | DLRM (100TB) | 1000+ way | Embedding sharding |\n| Latency | Any large model | Varies | Tensor parallelism |\n\n: **Sharding Triggers**: Different constraints lead to different sharding requirements and strategies. {#tbl-sharding-triggers}\n\n### Tensor Parallelism {#sec-inference-tensor-parallelism}\n\nTensor parallelism distributes individual layers across multiple devices, enabling parallel computation within each layer. For transformer models, the primary target is the attention mechanism and feed-forward layers, which contain the majority of computation.\n\n**Attention layer parallelism**: The multi-head attention computation naturally partitions across attention heads. For a model with $H$ attention heads distributed across $P$ devices, each device computes $H/P$ heads:\n\n$$\\text{Attention}_i = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_i \\text{ for heads } i \\in \\{1, ..., H/P\\}$$\n\nAfter computing local attention, an all-reduce operation combines results across devices.\n\n**Feed-forward layer parallelism**: The feed-forward layer (typically two linear transformations with activation) partitions along the hidden dimension. For the first linear layer, columns are distributed; for the second, rows are distributed. This column-row partitioning requires only one all-reduce per feed-forward block.\n\nThe communication pattern for tensor-parallel inference follows:\n\n```\nInput activations (replicated on all devices)\n    │\n    ▼\nAttention heads (computed in parallel, H/P heads per device)\n    │\n    ▼\nAllReduce (combine attention outputs)\n    │\n    ▼\nFeed-forward layer 1 (column-parallel)\n    │\n    ▼\nFeed-forward layer 2 (row-parallel)\n    │\n    ▼\nAllReduce (combine FF outputs)\n    │\n    ▼\nOutput activations (replicated on all devices)\n```\n\nThe inference time with tensor parallelism follows:\n\n$$T_{inference} = \\frac{T_{compute}}{P} + 2 \\times T_{allreduce}\\left(\\frac{A}{P}\\right)$$ {#eq-tensor-parallel-time}\n\nwhere $T_{compute}$ is the sequential compute time, $P$ is the parallelism degree, and $A$ is the activation size being reduced. The factor of 2 accounts for the two all-reduce operations per transformer layer (attention and feed-forward).\n\n::: {.callout-note title=\"Worked Example: Tensor Parallelism for Llama-70B\"}\n\nConsider serving Llama-70B with the following configuration:\n\n**Model specifications**:\n\n- Parameters: 70 billion\n- Hidden dimension: 8,192\n- Attention heads: 64\n- Layers: 80\n\n**Memory per GPU** (weight only, FP16):\n\n$$\\text{Memory}_{70B} = 70 \\times 10^9 \\times 2 = 140\\text{ GB}$$\n\n**Minimum sharding**: 2-way (140GB / 80GB per H100)\n\n**Recommended sharding**: 8-way for optimal latency\n\n**With 8-way tensor parallelism on 8xH100 (NVLink interconnect)**:\n\n| Component | Sequential (1 GPU) | 8-way TP | Speedup |\n|-----------|-------------------|----------|---------|\n| Attention compute | 12ms | 1.5ms | 8x |\n| AllReduce (attention) | 0ms | 0.3ms | N/A |\n| Feed-forward compute | 18ms | 2.25ms | 8x |\n| AllReduce (FF) | 0ms | 0.3ms | N/A |\n| **Total per layer** | **30ms** | **4.35ms** | **6.9x** |\n\n**For 80 layers**:\n\n- Sequential: 2,400ms per token\n- 8-way TP: 348ms per token\n\nThe 6.9x speedup (vs theoretical 8x) reflects communication overhead. With 600 GB/s NVLink bandwidth, each 8MB activation all-reduce takes ~0.3ms.\n\n**Time-to-first-token** (1024-token prompt):\n\n- Prefill compute: ~50ms (compute-bound, near-linear scaling)\n- Total TTFT: ~60ms with preprocessing\n\n:::\n\n### Pipeline Parallelism for Inference {#sec-inference-pipeline-parallelism}\n\nPipeline parallelism distributes layers across devices sequentially, with each device handling a subset of layers. Unlike tensor parallelism, there is no synchronization within a layer, only between pipeline stages.\n\nFor inference, pipeline parallelism creates bubbles differently than in training:\n\n```\nDevice 0 (Layers 1-20):  [Prefill] ─────────────────────────────\n                                  \\\nDevice 1 (Layers 21-40):           [Prefill] ──────────────────\n                                            \\\nDevice 2 (Layers 41-60):                     [Prefill] ────────\n                                                      \\\nDevice 3 (Layers 61-80):                               [Prefill]\n```\n\nFor a single request, pipeline parallelism provides no latency benefit: the request must traverse all stages sequentially. The pipeline fill time equals the sequential execution time.\n\nHowever, pipeline parallelism enables **throughput scaling** through pipelining multiple requests:\n\n```\nTime →\nDevice 0: [Req1] [Req2] [Req3] [Req4] ...\nDevice 1:        [Req1] [Req2] [Req3] [Req4] ...\nDevice 2:               [Req1] [Req2] [Req3] [Req4] ...\nDevice 3:                      [Req1] [Req2] [Req3] [Req4] ...\n```\n\nOnce the pipeline is full, throughput equals $P$ times single-stage throughput, where $P$ is the number of pipeline stages. The steady-state latency remains approximately the single-device latency (sum of all stage times), but throughput scales with parallelism.\n\n**When to use pipeline parallelism for inference**:\n\n- When memory constraints require sharding but latency requirements are relaxed\n- When throughput is more important than individual request latency\n- When network bandwidth between devices is limited (only point-to-point communication)\n\n**Pipeline vs tensor parallelism tradeoffs**:\n\n| Aspect | Tensor Parallelism | Pipeline Parallelism |\n|--------|-------------------|---------------------|\n| Single-request latency | Reduced by ~$P$x | No improvement |\n| Throughput | $P$x | $P$x (when pipelined) |\n| Communication pattern | AllReduce (bandwidth-intensive) | Point-to-point (latency-sensitive) |\n| Memory efficiency | Activations replicated | Activations passed along |\n| Complexity | Higher (requires custom kernels) | Lower (layer-level partitioning) |\n\n: **Pipeline vs Tensor Parallelism**: Each strategy has distinct tradeoffs in latency, throughput, and implementation complexity. {#tbl-pipeline-tensor-comparison}\n\n### Expert Parallelism for MoE Models {#sec-inference-expert-parallelism}\n\nMixture-of-Experts (MoE) models present unique sharding challenges because computation is dynamically routed to different experts based on input. Popular models like Mixtral use MoE to achieve high capacity with lower inference cost.\n\nIn an MoE layer, a gating network selects $k$ experts (out of $E$ total) for each token:\n\n$$\\text{Output} = \\sum_{i \\in \\text{top-}k} g_i \\cdot \\text{Expert}_i(\\text{input})$$\n\n**Expert parallelism** distributes experts across devices, with each device hosting $E/P$ experts:\n\n```\nToken arrives with gating decision: [Expert 2, Expert 7]\n\nDevice 0 (Experts 0-3):   Compute Expert 2\nDevice 1 (Experts 4-7):   Compute Expert 7\n\nAllToAll: Gather results back to original device\n```\n\nThe communication pattern differs from tensor parallelism: instead of all-reduce (same data to all devices), expert parallelism uses all-to-all (different data to different devices based on routing).\n\n**Load balancing challenge**: If gating decisions cluster on certain experts, devices hosting popular experts become bottlenecks while others sit idle. MoE training includes auxiliary losses to encourage balanced routing, but inference still exhibits routing imbalance.\n\n::: {.callout-note title=\"Expert Parallelism for Mixtral-8x7B\"}\n\nMixtral-8x7B uses 8 experts per MoE layer with top-2 routing:\n\n**Model characteristics**:\n\n- Total parameters: 47B (but only ~13B active per token)\n- Experts per layer: 8\n- Active experts per token: 2 (top-k = 2)\n- MoE layers: Every other feed-forward layer\n\n**Sharding strategy** (4-way expert parallelism):\n\n- Experts 0-1 on Device 0\n- Experts 2-3 on Device 1\n- Experts 4-5 on Device 2\n- Experts 6-7 on Device 3\n\n**Communication pattern per token**:\n\n1. Gating: Determine which 2 experts to use (~0.1ms)\n2. AllToAll dispatch: Send token to devices hosting selected experts (~0.2ms)\n3. Expert compute: Process token through selected experts (~1ms each, parallel)\n4. AllToAll gather: Collect results back (~0.2ms)\n\n**Total MoE layer time**: ~1.5ms (vs ~4ms for equivalent dense layer)\n\n**Load balancing metrics**:\n\n| Routing Distribution | GPU Utilization | Throughput Impact |\n|---------------------|-----------------|-------------------|\n| Perfectly balanced | 100% | Baseline |\n| Moderate imbalance (20%) | 83% | -17% |\n| Severe imbalance (50%) | 67% | -33% |\n\nProduction systems monitor routing statistics and may retrain or fine-tune gating to improve balance.\n\n:::\n\n### Embedding Sharding for Recommendation Systems {#sec-inference-embedding-sharding}\n\nRecommendation systems typically contain embedding tables that dwarf model weights in size. Meta's DLRM-scale models have embedding tables exceeding 100TB, requiring aggressive sharding strategies fundamentally different from tensor or pipeline parallelism.\n\n**Row-wise sharding** partitions embedding tables by row (entity ID):\n\n$$\\text{Shard}_i = \\{e_j : \\text{hash}(j) \\mod P = i\\}$$\n\nEach shard contains approximately $N/P$ embeddings, where $N$ is the total number of entities and $P$ is the shard count.\n\n**Column-wise sharding** partitions each embedding vector across devices:\n\n$$e_j = [e_j^{(0)}, e_j^{(1)}, ..., e_j^{(P-1)}]$$\n\nEach device stores a slice of every embedding.\n\n**Hybrid sharding** combines both approaches: frequently accessed embeddings are column-sharded for faster access, while the long tail uses row sharding.\n\n| Sharding Strategy | Lookup Pattern | Communication | Best For |\n|------------------|----------------|---------------|----------|\n| Row-wise | Single device per lookup | AllToAll gather | Uniform access patterns |\n| Column-wise | All devices per lookup | AllGather | Hot embeddings |\n| Hybrid | Varies by embedding | Mixed | Production RecSys |\n\n: **Embedding Sharding Strategies**: Different strategies trade off lookup locality against load balance. {#tbl-embedding-sharding}\n\n::: {.callout-note title=\"Embedding Sharding at Scale: Meta Infrastructure\"}\n\nMeta's recommendation infrastructure demonstrates embedding sharding at extreme scale:\n\n**Scale**:\n\n- Embedding tables: 100+ TB total\n- Unique entities: 10+ trillion\n- Embedding dimension: 128-256\n- Shards: 1,000+ servers\n\n**Sharding strategy**:\n\n- **Hot embeddings** (top 1% by access frequency): Replicated across all shards\n- **Warm embeddings** (next 10%): Column-sharded with 8-way parallelism\n- **Cold embeddings** (remaining 89%): Row-sharded with consistent hashing\n\n**Access pattern optimization**:\n\nEach inference request requires ~5,000 embedding lookups. Without optimization, this would require 5,000 network round trips. Instead:\n\n1. **Batch accumulation**: Collect lookups for 1ms\n2. **Lookup deduplication**: Remove duplicate entities across requests\n3. **Shard-aware batching**: Group lookups by destination shard\n4. **Parallel dispatch**: Send batched requests to all shards simultaneously\n5. **Streaming assembly**: Reconstruct embeddings as responses arrive\n\n**Performance**:\n\n| Metric | Without Optimization | With Optimization |\n|--------|---------------------|-------------------|\n| Network round trips | 5,000 | 1 (batched) |\n| Lookup latency | 50ms | 2ms |\n| Network bandwidth | 10 Gbps | 40 Gbps (burst) |\n\n:::\n\n### Hybrid Sharding Strategies {#sec-inference-hybrid-sharding}\n\nProduction systems often combine multiple sharding strategies to handle different model components optimally:\n\n**Tensor + Pipeline parallelism**: For very large models that require both memory distribution and latency reduction:\n\n```\n8 GPUs organized as 2 pipeline stages × 4 tensor parallel:\n\nStage 0 (Layers 1-40):  TP across GPUs 0,1,2,3\nStage 1 (Layers 41-80): TP across GPUs 4,5,6,7\n```\n\nThis achieves 4x latency reduction (from TP) while handling models requiring 8-way sharding for memory.\n\n**Expert + Tensor parallelism**: For MoE models where individual experts are large:\n\n```\nMixtral with large experts:\n\n- Expert parallelism: Distribute 8 experts across 8 GPU groups\n- Tensor parallelism: Each expert spread across 2 GPUs\n- Total GPUs: 16\n```\n\n**Embedding + Dense parallelism**: For recommendation models with both large embeddings and large dense components:\n\n```\nDLRM-scale model:\n\n- Embedding sharding: 1,000 shards across CPU servers\n- Dense model: 8-way tensor parallel across GPUs\n- Communication: Embeddings gathered to GPU, processed, returned\n```\n\n### Communication Overhead Analysis {#sec-inference-sharding-communication}\n\nThe practical speedup from sharding depends critically on communication efficiency. Each sharding strategy has characteristic communication patterns with different bandwidth and latency requirements.\n\n**AllReduce** (tensor parallelism): Combines data from all devices, with result available on all devices.\n\n$$T_{allreduce} = 2 \\times \\frac{(P-1)}{P} \\times \\frac{M}{B}$$ {#eq-allreduce-time}\n\nwhere $P$ is the number of devices, $M$ is the message size, and $B$ is the interconnect bandwidth. The factor of 2 accounts for the reduce-scatter and all-gather phases.\n\n**Point-to-point** (pipeline parallelism): Sends data from one device to the next.\n\n$$T_{p2p} = L + \\frac{M}{B}$$ {#eq-p2p-time}\n\nwhere $L$ is the network latency and $M/B$ is the transfer time.\n\n**AllToAll** (expert parallelism): Exchanges data where each device sends different data to each other device.\n\n$$T_{alltoall} = (P-1) \\times \\left(L + \\frac{M/P}{B}\\right)$$ {#eq-alltoall-time}\n\n::: {.callout-note title=\"Interconnect Technology Comparison\"}\n\nCommunication overhead depends heavily on the interconnect technology:\n\n| Interconnect | Bandwidth | Latency | Use Case |\n|--------------|-----------|---------|----------|\n| NVLink (H100) | 900 GB/s | 1μs | Intra-node TP |\n| PCIe Gen5 | 64 GB/s | 5μs | Intra-node (no NVLink) |\n| InfiniBand HDR | 200 Gb/s (25 GB/s) | 1μs | Inter-node |\n| Ethernet 100G | 100 Gb/s (12.5 GB/s) | 10μs | Inter-node (commodity) |\n\n**Example: 8-way tensor parallelism communication**\n\nActivation size: 8MB per all-reduce (batch=1, hidden=8192)\n\n| Interconnect | AllReduce Time | % of 30ms Layer |\n|--------------|----------------|-----------------|\n| NVLink | 0.02ms | 0.07% |\n| InfiniBand | 0.7ms | 2.3% |\n| 100G Ethernet | 1.5ms | 5% |\n\nNVLink enables efficient tensor parallelism within a node. Cross-node tensor parallelism requires InfiniBand for acceptable overhead.\n\n:::\n\n### Sharding Strategy Selection {#sec-inference-sharding-selection}\n\nSelecting the appropriate sharding strategy depends on model architecture, deployment constraints, and optimization priorities:\n\n```\nDoes the model fit in single-GPU memory?\n├─ Yes → Is latency reduction needed?\n│        ├─ Yes → Tensor parallelism (within node)\n│        └─ No → No sharding needed\n└─ No → What is the memory bottleneck?\n         ├─ Weights → Tensor or pipeline parallelism\n         ├─ KV cache → Tensor parallelism (distributes cache)\n         ├─ Embeddings → Embedding sharding\n         └─ Experts → Expert parallelism\n```\n\n**Decision factors**:\n\n| Factor | Tensor Parallel | Pipeline Parallel | Expert Parallel | Embedding Shard |\n|--------|----------------|-------------------|-----------------|-----------------|\n| Latency priority | Best | Worst | Moderate | N/A |\n| Throughput priority | Good | Best (pipelined) | Good | Best |\n| Interconnect limited | Poor fit | Good fit | Moderate | Good fit |\n| Implementation effort | High | Low | Moderate | High |\n\n: **Sharding Strategy Selection Guide**: Match strategy to deployment priorities and constraints. {#tbl-sharding-selection}\n\n## Load Balancing and Request Routing {#sec-inference-load-balancing}\n\nWith models deployed across multiple replicas, the next challenge is distributing requests effectively. Load balancing determines which replica handles each request, directly impacting latency, throughput, and resource utilization. Seemingly simple choices, like random assignment versus informed selection, produce dramatically different performance at scale.\n\nThis section develops the theory and practice of load balancing for inference, from basic algorithms through the power-of-two-choices insight that provides exponentially better performance with minimal overhead.\n\n### Load Balancing Fundamentals {#sec-inference-lb-fundamentals}\n\nLoad balancing serves two primary goals that sometimes conflict:\n\n**Latency minimization**: Route requests to replicas that can serve them fastest, considering current queue depth and processing time.\n\n**Utilization maximization**: Spread load evenly to avoid both idle replicas and overloaded replicas.\n\nThe tension arises because latency-optimal routing may concentrate load on fast replicas, reducing their performance and leaving other replicas underutilized.\n\n**Key metrics** for load balancing evaluation:\n\n- **Maximum queue length**: The longest queue across all replicas (determines worst-case latency)\n- **Load variance**: Standard deviation of queue lengths (measures balance)\n- **Utilization spread**: Difference between most and least utilized replicas\n- **Decision overhead**: Time required to make routing decisions\n\n### Round-Robin and Random Assignment {#sec-inference-lb-round-robin}\n\nThe simplest load balancing strategies assign requests without considering server state:\n\n**Round-robin** assigns requests in circular order: request 1 to server 1, request 2 to server 2, and so on. This guarantees perfect distribution when servers are homogeneous and request processing times are identical.\n\n**Random assignment** selects a server uniformly at random for each request. With large numbers of requests, this converges to even distribution but with higher variance than round-robin.\n\nFor homogeneous servers with identical service times, both achieve near-optimal load distribution. However, production systems rarely meet these assumptions:\n\n- **Heterogeneous hardware**: Different GPU generations, memory configurations\n- **Variable request sizes**: Some requests take 10x longer than others\n- **Server state variations**: Some replicas warming up, others near memory limits\n\nUnder these realistic conditions, uninformed strategies perform poorly. The maximum queue length under random assignment follows:\n\n$$E[\\text{max queue}] = \\Theta\\left(\\frac{\\log n}{\\log \\log n}\\right)$$ {#eq-random-max-queue}\n\nwhere $n$ is the number of servers. For 1,000 servers, this is approximately 4-5 requests. This seems small, but the unlucky requests in long queues experience significantly higher latency.\n\n### The Power of Two Choices {#sec-inference-two-choices}\n\nA remarkable result in load balancing theory shows that querying just two random servers before making a routing decision provides exponentially better load distribution than random assignment.\n\n**Power-of-two-choices algorithm**:\n\n1. Select two servers uniformly at random\n2. Query both for their current queue length\n3. Route the request to the server with the shorter queue\n\nThis simple modification reduces maximum queue length from $O(\\log n / \\log \\log n)$ to $O(\\log \\log n)$:\n\n$$E[\\text{max queue}]_{\\text{two choices}} = \\Theta(\\log \\log n)$$ {#eq-two-choices-max-queue}\n\nFor 1,000 servers:\n\n- Random assignment max queue: ~4-5 requests\n- Two choices max queue: ~2 requests\n\nThe improvement is exponential: two choices with 1,000 servers achieves better balance than random with just 10 servers.\n\n::: {.callout-important title=\"Exponential Improvement from a Simple Change\"}\n\nThe power-of-two-choices result is one of the most impactful findings in distributed systems theory. By examining just one additional server, maximum queue length improves from $O(\\log n / \\log \\log n)$ to $O(\\log \\log n)$, an exponential improvement.\n\nThis has profound practical implications:\n\n- Near-optimal load balancing with minimal overhead (2 probes vs n probes)\n- Scalable: improvement increases with system size\n- Robust: works with heterogeneous servers and variable request sizes\n- Simple: easy to implement in any load balancer\n\nProduction systems at Google, Meta, and AWS all use variants of power-of-two-choices.\n\n:::\n\n**Why does this work?** Intuitively, random assignment occasionally makes poor choices (routing to an already-busy server), and these mistakes compound. With two choices, the algorithm almost never makes the worst choice, avoiding the tail behavior that creates long queues.\n\nMathematically, the key insight is that with random assignment, when $d$ servers have queue length $k$, the probability of queue length $k+1$ growing is proportional to $d/n$. With two choices, this probability drops to $(d/n)^2$, creating a super-exponential decay in queue length distribution.\n\n### Weighted and Adaptive Load Balancing {#sec-inference-weighted-lb}\n\nWhen servers have different capacities, naive load balancing creates imbalance. A mix of A100 GPUs (high capacity) and T4 GPUs (lower capacity) receiving equal request rates will have T4 servers overloaded while A100 servers are underutilized.\n\n**Weighted round-robin** assigns requests proportional to server capacity:\n\n$$P(\\text{route to server } i) = \\frac{w_i}{\\sum_j w_j}$$\n\nwhere $w_i$ is the weight (capacity) of server $i$.\n\n**Weighted two-choices** applies the same principle:\n\n1. Select two servers with probability proportional to their weights\n2. Query both for current load relative to their capacity\n3. Route to the server with lower relative load\n\n::: {.callout-note title=\"Worked Example: Heterogeneous GPU Cluster\"}\n\nConsider a cluster with mixed GPU types:\n\n- 10 H100 GPUs (capacity: 1000 QPS each)\n- 20 A100 GPUs (capacity: 600 QPS each)\n- Total capacity: 10×1000 + 20×600 = 22,000 QPS\n\n**Target traffic**: 15,000 QPS\n\n**Weighted assignment**:\n\n- H100 weight: 1000 / 22000 = 4.5%\n- A100 weight: 600 / 22000 = 2.7%\n\n**Expected load per server**:\n\n- H100: 15000 × 0.045 = 682 QPS (68% utilization)\n- A100: 15000 × 0.027 = 409 QPS (68% utilization)\n\nBoth server types operate at equal utilization, maximizing overall capacity while maintaining latency consistency.\n\n**Without weighting** (equal distribution):\n\n- Per-server load: 15000 / 30 = 500 QPS\n- H100 utilization: 50% (underutilized)\n- A100 utilization: 83% (overloaded, latency spikes)\n\n:::\n\n**Adaptive load balancing** adjusts weights dynamically based on observed performance:\n\n```\nFor each server i:\n    latency[i] = exponential_moving_average(observed_latency)\n    weight[i] = 1 / latency[i]  # Inverse latency weighting\n```\n\nThis automatically adapts to:\n\n- Server degradation (memory pressure, thermal throttling)\n- Request size variations (some traffic patterns harder to serve)\n- Background tasks consuming resources\n\n### Least-Connections Load Balancing {#sec-inference-least-connections}\n\nAn alternative to random selection is routing to the server with the fewest active connections (or shortest queue). This requires maintaining global state but provides better balance for variable-size requests.\n\n**Least-connections algorithm**:\n\n1. Maintain a count of active requests per server\n2. Route each new request to the server with the minimum count\n3. Increment count on dispatch, decrement on completion\n\nFor long-running requests (common in LLM serving), least-connections significantly outperforms round-robin because it accounts for current load rather than just historical assignments.\n\nThe challenge is maintaining accurate connection counts in a distributed system. Options include:\n\n- **Centralized counter**: Single source of truth, potential bottleneck\n- **Distributed counters with gossip**: Eventually consistent, may route to stale information\n- **Sampled least-connections**: Query a subset of servers, choose minimum (combines with two-choices)\n\n::: {.callout-note title=\"Least-Connections for LLM Serving\"}\n\nLLM inference has highly variable request durations based on output length:\n\n- Short response (10 tokens): 500ms\n- Long response (500 tokens): 25s\n- Ratio: 50x\n\nWith round-robin at 100 QPS across 10 servers:\n\n- Each server receives 10 requests/second\n- If one server gets multiple long requests, it falls behind\n- Queue builds while other servers sit idle\n\nWith least-connections:\n\n- New requests route away from servers processing long responses\n- Servers finishing short requests receive new work immediately\n- Load naturally balances based on actual work remaining\n\n**Observed improvement** (production LLM serving):\n\n| Algorithm | P99 Latency | Load Variance |\n|-----------|-------------|---------------|\n| Round-robin | 45s | 3.2 requests |\n| Least-connections | 28s | 0.8 requests |\n| Two-choices + LC | 26s | 0.5 requests |\n\nLeast-connections reduces P99 by 38%; combining with two-choices provides additional improvement.\n\n:::\n\n### Consistent Hashing for Stateful Routing {#sec-inference-consistent-hashing}\n\nMany inference workloads maintain state that benefits from routing affinity:\n\n- **LLM conversations**: KV cache from previous turns\n- **Recommendation sessions**: User context and recent interactions\n- **Streaming inference**: Model state from previous frames\n\nFor these workloads, routing the same user or session to the same server improves performance by avoiding cache misses and state reconstruction.\n\n**Consistent hashing** maps requests to servers based on a hash of the routing key (user ID, session ID):\n\n$$\\text{server}(request) = \\arg\\min_{s \\in S} \\text{distance}(\\text{hash}(key), \\text{hash}(s))$$\n\nwhere servers and keys are mapped onto a ring, and each request routes to the nearest server clockwise.\n\nKey properties:\n\n- **Deterministic**: Same key always routes to same server\n- **Minimal disruption**: Adding/removing servers only remaps $K/N$ keys on average\n- **Load balancing**: With virtual nodes, load distributes evenly\n\n::: {.callout-note title=\"Consistent Hashing for KV Cache Affinity\"}\n\nConsider an LLM serving system where each user's conversation maintains KV cache state:\n\n**Without affinity**:\n\n- User sends message, routed to Server A, KV cache built\n- Next message routes to Server B (random)\n- KV cache rebuilt from scratch, 500ms penalty\n- Average conversation: 10 turns, 4.5s wasted on cache rebuilds\n\n**With consistent hashing**:\n\n- User ID hashed to Server A\n- All messages from this user route to Server A\n- KV cache reused across turns\n- Rebuild only on server changes or cache eviction\n\n**Implementation with virtual nodes**:\n\nEach physical server has 100 virtual nodes on the hash ring, ensuring even distribution despite server heterogeneity.\n\n```\nHash ring positions:\nServer A: [0.01, 0.03, 0.07, 0.12, ...]  (100 positions)\nServer B: [0.02, 0.05, 0.09, 0.15, ...]  (100 positions)\n...\n\nRequest for user \"alice\":\nhash(\"alice\") = 0.0834\nNearest server clockwise: Server A (at 0.09)\n```\n\n**Handling server failures**:\n\nWhen Server A fails, its 100 virtual nodes are removed from the ring. Requests that would have routed to Server A now route to the next server clockwise. Only ~$1/N$ of requests are affected, where $N$ is the number of servers.\n\n:::\n\n### Request Routing for Sharded Models {#sec-inference-sharded-routing}\n\nWhen models are sharded across devices (see @sec-inference-sharding), routing becomes more complex. A single inference request may require computation on multiple devices, necessitating coordination.\n\n**Routing patterns for sharded models**:\n\n**Tensor parallelism**: Request is broadcast to all devices in the shard group. Each device processes its portion of each layer. Results are synchronized via all-reduce.\n\n```\nRequest → Load Balancer → Shard Group\n                              │\n            ┌────────────────┼────────────────┐\n            ▼                ▼                ▼\n         GPU 0            GPU 1            GPU 2\n      (heads 0-7)      (heads 8-15)    (heads 16-23)\n            │                │                │\n            └────────AllReduce────────────────┘\n                              │\n                              ▼\n                          Response\n```\n\n**Pipeline parallelism**: Request flows through stages sequentially. Each stage forwards to the next.\n\n```\nRequest → Stage 0 → Stage 1 → Stage 2 → Stage 3 → Response\n         (L1-20)   (L21-40)  (L41-60)  (L61-80)\n```\n\n**Expert parallelism**: Request is dispatched to devices hosting selected experts based on gating decision.\n\n```\nRequest → Gating → AllToAll dispatch → Expert compute → AllToAll gather → Response\n                   (to selected experts)              (results back)\n```\n\n**Routing to shard groups**: With multiple shard groups for horizontal scaling, the load balancer routes to groups rather than individual devices:\n\n```\n                    Load Balancer\n                         │\n          ┌──────────────┼──────────────┐\n          ▼              ▼              ▼\n     Shard Group 0  Shard Group 1  Shard Group 2\n      (8 GPUs)       (8 GPUs)       (8 GPUs)\n```\n\nThe load balancer treats each shard group as a single logical server, applying standard algorithms (round-robin, two-choices, consistent hashing) at the group level.\n\n### Health Checking and Failover {#sec-inference-health-checking}\n\nLoad balancers must detect unhealthy servers and route around them. Health checking mechanisms include:\n\n**Liveness probes**: Verify the server process is running.\n\n```\nGET /health/live\nResponse: 200 OK (process alive) or timeout (process dead)\n```\n\n**Readiness probes**: Verify the server can handle requests (model loaded, GPU initialized).\n\n```\nGET /health/ready\nResponse: 200 OK (ready to serve) or 503 (not ready)\n```\n\n**Deep health checks**: Verify actual inference works by running a test request.\n\n```\nPOST /health/inference\nBody: {\"prompt\": \"test\"}\nResponse: 200 OK with valid output, or error\n```\n\n::: {.callout-note title=\"Health Check Configuration for GPU Inference\"}\n\nGPU inference servers have unique health check considerations:\n\n**GPU memory pressure**: Server may be alive but unable to allocate memory for new requests.\n\n```python\ndef readiness_check():\n    free_memory = (\n        torch.cuda.memory_reserved() - torch.cuda.memory_allocated()\n    )\n    if free_memory < MIN_REQUEST_MEMORY:\n        return {\n            \"status\": \"not_ready\",\n            \"reason\": \"insufficient GPU memory\",\n        }\n    return {\"status\": \"ready\"}\n```\n\n**Model warm-up**: First inference after load is slower. Mark ready only after warm-up.\n\n```python\nasync def startup():\n    model = load_model()\n    # Warm up with dummy requests\n    for _ in range(10):\n        model.generate(dummy_input)\n    # Now mark as ready\n    global ready\n    ready = True\n```\n\n**Timeout configuration**:\n\n| Check Type | Interval | Timeout | Failure Threshold |\n|------------|----------|---------|-------------------|\n| Liveness | 10s | 5s | 3 failures |\n| Readiness | 5s | 3s | 2 failures |\n| Deep health | 30s | 10s | 1 failure |\n\nDeep health checks run less frequently because they consume GPU resources.\n\n:::\n\n### Quantitative Analysis: Load Balancing Impact {#sec-inference-lb-analysis}\n\nThe choice of load balancing algorithm has quantitative impact on system performance. Consider a system with 100 servers, 10,000 QPS, and variable request sizes (CV = 0.5).\n\n| Algorithm | Max Queue | P99 Latency | CPU Overhead |\n|-----------|-----------|-------------|--------------|\n| Random | 4.2 requests | 45ms | Minimal |\n| Round-robin | 2.8 requests | 32ms | Minimal |\n| Two-choices | 1.9 requests | 24ms | 2 probes/request |\n| Least-connections | 1.4 requests | 19ms | Global state |\n| Two-choices + LC | 1.2 requests | 17ms | 2 probes + state |\n\n: **Load Balancing Algorithm Comparison**: More sophisticated algorithms reduce queue lengths and latency at the cost of increased overhead. {#tbl-lb-comparison}\n\nThe progression shows clear tradeoffs:\n\n- Random/round-robin: Zero overhead but higher latency variance\n- Two-choices: Minimal overhead (2 probes), 47% latency improvement\n- Least-connections: State maintenance overhead, 58% latency improvement\n- Combined: Best performance, highest complexity\n\nFor most production systems, two-choices provides the best tradeoff between performance improvement and implementation complexity. Least-connections adds value for workloads with high request size variance (LLM serving, recommendation ranking).\n\n### Circuit Breakers and Backpressure {#sec-inference-circuit-breakers}\n\nWhen servers become overloaded, routing more requests exacerbates the problem. Circuit breakers and backpressure mechanisms protect the system from cascading failures.\n\n**Circuit breaker pattern**:\n\n```\nStates: CLOSED → OPEN → HALF-OPEN → CLOSED\n\nCLOSED: Normal operation, route requests\nOPEN: Server unhealthy, immediately reject requests\nHALF-OPEN: Allow limited requests to test recovery\n\nTransitions:\n\n- CLOSED → OPEN: Error rate exceeds threshold (e.g., 50%)\n- OPEN → HALF-OPEN: After timeout (e.g., 30s)\n- HALF-OPEN → CLOSED: Test requests succeed\n- HALF-OPEN → OPEN: Test requests fail\n```\n\n**Backpressure propagation**: When servers are overloaded, they signal upstream to reduce request rate:\n\n```\nServer queue depth > threshold\n    → Return 503 Service Unavailable\n    → Load balancer marks server as degraded\n    → Routes fewer requests to this server\n    → If all servers degraded, apply admission control\n```\n\n::: {.callout-note title=\"Cascading Failure Prevention\"}\n\nConsider a scenario where one server becomes slow (thermal throttling):\n\n**Without circuit breaker**:\n\n1. Server A slows down (processing 500ms instead of 50ms)\n2. Load balancer continues routing to Server A\n3. Requests queue on Server A, timeouts begin\n4. Retry logic sends failed requests to other servers\n5. Other servers overload from retry traffic\n6. System-wide failure\n\n**With circuit breaker**:\n\n1. Server A slows down\n2. Error rate on Server A rises above 50%\n3. Circuit breaker opens for Server A\n4. All requests route to Servers B, C, D\n5. System operates at reduced capacity but remains stable\n6. After recovery, circuit breaker closes, Server A rejoins\n\n**Configuration for GPU inference**:\n\n| Parameter | Value | Rationale |\n|-----------|-------|-----------|\n| Error threshold | 30% | GPU OOM failures are serious |\n| Latency threshold | 2x baseline | Detect throttling early |\n| Open duration | 60s | GPU recovery takes time |\n| Half-open requests | 5 | Careful testing before full reopening |\n\n:::\n\n## KV Cache Management {#sec-inference-kv-cache}\n\nAutoregressive language models maintain key-value (KV) caches that store attention context from previous tokens, enabling efficient generation without recomputing attention over the entire sequence history. As context lengths grow and serving scales, KV cache management becomes a critical bottleneck. A 70B parameter model with 128K context can require over 100GB just for KV cache, exceeding the model weights themselves.\n\nThis section examines the memory management techniques that enable efficient LLM serving at scale: PagedAttention for fragmentation-free allocation, prefix caching for common prompt sharing, and speculative decoding for latency reduction.\n\n### KV Cache Fundamentals {#sec-inference-kv-cache-fundamentals}\n\nDuring autoregressive generation, each transformer layer computes attention over all previous tokens. Without caching, generating token $t$ would require recomputing attention keys and values for tokens $1$ through $t-1$, an $O(t^2)$ cost per generated token.\n\nThe KV cache stores these computed key and value vectors, reducing generation to $O(t)$ per token. However, this memory savings comes at the cost of storing past context:\n\n$$\\text{KV cache size} = 2 \\times L \\times H \\times S \\times B \\times P$$ {#eq-kv-cache-size}\n\nwhere:\n\n- $L$ = number of layers\n- $H$ = hidden dimension\n- $S$ = sequence length\n- $B$ = batch size\n- $P$ = precision (bytes per element)\n- Factor of 2 accounts for both keys and values\n\n::: {.callout-note title=\"Worked Example: KV Cache Memory for Llama-70B\"}\n\nFor Llama-70B with typical serving configuration:\n\n- Layers ($L$): 80\n- Hidden dimension ($H$): 8,192\n- Context length ($S$): 4,096 tokens\n- Batch size ($B$): 32 concurrent requests\n- Precision ($P$): 2 bytes (FP16)\n\n$$\\text{KV cache} = 2 \\times 80 \\times 8192 \\times 4096 \\times 32 \\times 2 = 344 \\text{ GB}$$\n\nThis exceeds the model weights (140GB) by 2.5x!\n\n**Memory breakdown for single H100 (80GB)**:\n\n| Component | Memory | Percentage |\n|-----------|--------|------------|\n| Model weights | 140GB | Cannot fit on single GPU |\n\n**With 8-way tensor parallelism across 8 H100s (640GB total)**:\n\n| Component | Memory per GPU | Total |\n|-----------|---------------|-------|\n| Model weights | 17.5GB | 140GB |\n| KV cache (theoretical) | 43GB | 344GB |\n| Activations | 4GB | 32GB |\n| **Available for KV** | ~15GB | 120GB |\n\nThe 120GB available KV cache limits concurrent batch size to ~11 requests at 4K context, not 32.\n\n**Implication**: KV cache capacity, not compute, limits LLM serving throughput for long contexts.\n\n:::\n\n### The Fragmentation Problem {#sec-inference-kv-fragmentation}\n\nTraditional memory allocation for KV cache pre-allocates contiguous memory for each sequence based on maximum expected length. This creates two forms of waste:\n\n**Internal fragmentation**: Sequences shorter than the maximum allocation waste the unused portion. If maximum length is 4,096 but average output is 100 tokens, 97.5% of allocated memory is wasted.\n\n**External fragmentation**: As sequences complete and new ones start, memory becomes fragmented into non-contiguous free blocks. Even with sufficient total free memory, no single block may be large enough for a new maximum-length allocation.\n\nConsider a simplified example with 8 memory slots and maximum sequence length of 4:\n\n```\nTime 0: Allocate Seq A (slots 0-3), Seq B (slots 4-7)\n        [A][A][A][A][B][B][B][B]\n\nTime 1: Seq A completes (2 tokens), Seq B continues\n        [ ][ ][A][A][B][B][B][ ]  <- A only used 2 slots\n\nTime 2: Try to allocate Seq C (needs 4 slots)\n        [ ][ ][A][A][B][B][B][ ]  <- No contiguous block of 4!\n\nResult: 4 free slots but cannot allocate new sequence\n```\n\nProduction systems report 60-80% memory waste from fragmentation under realistic workloads, severely limiting batch sizes and throughput.\n\n### PagedAttention {#sec-inference-paged-attention}\n\nPagedAttention, introduced in vLLM, applies virtual memory concepts to KV cache management. Instead of contiguous allocation, the KV cache is divided into fixed-size pages (typically 16-256 tokens), and sequences are allocated pages on demand.\n\n**Key concepts**:\n\n**Page table**: Maps logical sequence positions to physical memory pages.\n\n**Block size**: Number of tokens per page (typically 16 tokens).\n\n**Physical blocks**: Fixed-size memory allocations that can be assigned to any sequence.\n\n```\nSequence A (150 tokens, 10 pages allocated):\nLogical:  [Page 0][Page 1][Page 2]...[Page 9]\nPhysical: [Block 5][Block 12][Block 3]...[Block 8]\n\nSequence B (80 tokens, 5 pages allocated):\nLogical:  [Page 0][Page 1][Page 2][Page 3][Page 4]\nPhysical: [Block 1][Block 7][Block 15][Block 2][Block 11]\n```\n\n**Benefits**:\n\n1. **No internal fragmentation**: Allocate only the pages needed for actual tokens\n2. **No external fragmentation**: Any free page can be used by any sequence\n3. **Dynamic growth**: Sequences can grow without pre-allocation\n4. **Memory sharing**: Common prefixes can share physical pages\n\n::: {.callout-note title=\"PagedAttention Implementation Details\"}\n\n**Memory layout**:\n\n```\nPhysical blocks (16 tokens × hidden_dim × 2 × precision):\nBlock 0:  [K₀...K₁₅, V₀...V₁₅]\nBlock 1:  [K₀...K₁₅, V₀...V₁₅]\n...\nBlock N:  [K₀...K₁₅, V₀...V₁₅]\n```\n\n**Page table per sequence**:\n\n```python\nclass PageTable:\n    def __init__(self, max_blocks):\n        self.block_map = {}  # logical_block -> physical_block\n\n    def allocate_block(self, logical_idx, physical_block):\n        self.block_map[logical_idx] = physical_block\n\n    def get_physical(self, logical_idx):\n        return self.block_map[logical_idx]\n```\n\n**Attention kernel modification**:\n\nStandard attention: `output = softmax(Q @ K.T / sqrt(d)) @ V`\n\nPagedAttention:\n```python\ndef paged_attention(Q, page_table, physical_blocks, block_size):\n    # Gather K, V from non-contiguous physical blocks\n    for logical_idx in range(num_logical_blocks):\n        physical_idx = page_table[logical_idx]\n        K_block = physical_blocks[physical_idx].K\n        V_block = physical_blocks[physical_idx].V\n        # Compute attention for this block\n        attention_scores = Q @ K_block.T / sqrt(d)\n        output += softmax(attention_scores) @ V_block\n    return output\n```\n\n**Performance impact**:\n\nThe gather operations add overhead, but it is minimal compared to the memory savings:\n\n| Approach | Memory Utilization | Throughput (relative) |\n|----------|-------------------|----------------------|\n| Contiguous | 30-40% | 1.0x (baseline) |\n| PagedAttention | 95%+ | 2.5-4x |\n\nThe 2.5-4x throughput improvement comes from fitting more concurrent sequences in the same memory.\n\n:::\n\n### Prefix Caching {#sec-inference-prefix-caching}\n\nMany LLM workloads share common prefixes across requests:\n\n- **System prompts**: \"You are a helpful assistant...\" prepended to every request\n- **Few-shot examples**: Same examples used for many queries\n- **Document context**: Multiple questions about the same document\n\nRecomputing these shared prefixes wastes both compute (prefill) and memory (duplicate KV cache entries).\n\n**Prefix caching** shares KV cache entries across requests with common prefixes:\n\n```\nRequest A: [System prompt (500 tokens)] + [User query A (50 tokens)]\nRequest B: [System prompt (500 tokens)] + [User query B (75 tokens)]\nRequest C: [System prompt (500 tokens)] + [User query C (30 tokens)]\n\nWithout prefix caching:\n\n- 3 × 500 = 1500 tokens of prefill compute\n- 3 × 500 tokens of KV cache storage\n\nWith prefix caching:\n\n- 500 tokens of prefill compute (cached)\n- 500 tokens of KV cache storage (shared)\n- 155 tokens of unique prefill compute\n- 155 tokens of unique KV cache storage\n```\n\n**Implementation with PagedAttention**:\n\nPrefix caching integrates naturally with PagedAttention through copy-on-write semantics:\n\n```\nSystem prompt → Physical blocks [0, 1, 2, 3, 4, 5]\n\nRequest A page table: [0, 1, 2, 3, 4, 5, 10, 11]  <- shares prefix blocks\nRequest B page table: [0, 1, 2, 3, 4, 5, 12, 13, 14]  <- shares prefix blocks\nRequest C page table: [0, 1, 2, 3, 4, 5, 15]  <- shares prefix blocks\n```\n\nAll three requests reference the same physical blocks for the system prompt. Only when generating unique tokens do they allocate new blocks.\n\n::: {.callout-note title=\"Prefix Caching at Scale\"}\n\nConsider a chatbot service with a 2000-token system prompt and 1000 concurrent users:\n\n**Without prefix caching**:\n\n- KV cache per user: 2000 + 500 (avg response) = 2500 tokens\n- Total KV cache: 2500 × 1000 × 2 × 80 × 8192 × 2 = 6.5 TB\n\n**With prefix caching**:\n\n- Shared prefix: 2000 tokens (once)\n- Unique per user: 500 tokens\n- Total: (2000 × 1) + (500 × 1000) = 502,000 tokens\n- Memory: 502,000 × 2 × 80 × 8192 × 2 = 1.3 TB\n\n**Savings**: 80% reduction in KV cache memory, enabling 5x more concurrent users.\n\n**Prefix hit rate** determines effectiveness:\n\n| Workload | Prefix Hit Rate | Memory Savings |\n|----------|-----------------|----------------|\n| Chatbot (same system prompt) | 95%+ | 70-80% |\n| Document QA (same doc) | 80-90% | 50-70% |\n| General API (diverse) | 20-40% | 10-30% |\n\n:::\n\n### KV Cache Compression {#sec-inference-kv-compression}\n\nBeyond efficient allocation, reducing the size of cached values provides additional memory savings. Several techniques compress the KV cache:\n\n**Quantization**: Store cached keys and values at reduced precision.\n\n$$\\text{Compressed size} = \\text{Original size} \\times \\frac{b_{compressed}}{b_{original}}$$\n\n| Precision | Memory per Token | Quality Impact |\n|-----------|------------------|----------------|\n| FP16 (baseline) | 2 bytes | None |\n| FP8 | 1 byte | <1% degradation |\n| INT8 | 1 byte | 1-2% degradation |\n| INT4 | 0.5 bytes | 3-5% degradation |\n\n**Key observation**: KV cache values are more tolerant of quantization than model weights because they are intermediate activations, not learned parameters.\n\n**Sliding window attention**: For very long contexts, maintain full cache only for recent tokens:\n\n```\nFull context: 100,000 tokens\nSliding window: 4,096 tokens\n\nCache strategy:\n\n- Tokens 0-95,904: Discarded or compressed\n- Tokens 95,904-100,000: Full precision cache\n\nTrade-off: Cannot attend to very old tokens, but sufficient for most tasks.\n```\n\n**Grouped-query attention (GQA)**: Architectural change that reduces KV cache by sharing key-value heads:\n\n| Attention Type | KV Heads | Cache Size (relative) |\n|---------------|----------|----------------------|\n| Multi-head (MHA) | 64 | 1.0x |\n| Grouped-query (GQA) | 8 | 0.125x |\n| Multi-query (MQA) | 1 | 0.016x |\n\nModern models like Llama 2 and Mistral use GQA specifically to reduce KV cache requirements.\n\n### Speculative Decoding {#sec-inference-speculative-decoding}\n\nAutoregressive generation is inherently sequential: each token depends on previous tokens. Speculative decoding breaks this bottleneck by using a smaller draft model to predict multiple tokens, then verifying them in parallel with the target model.\n\n**Algorithm**:\n\n1. Draft model generates $k$ tokens speculatively: $t_1, t_2, ..., t_k$\n2. Target model verifies all $k$ tokens in a single forward pass\n3. Accept prefix of correct tokens, reject from first incorrect token\n4. Continue from last accepted token\n\n**Why this works**: The draft model is much smaller (7B vs 70B) and can generate $k$ tokens in the time the target model generates 1 token. Verification is cheap because the target model can process all $k$ tokens in parallel (like prefill).\n\n::: {.callout-note title=\"Speculative Decoding Example\"}\n\n**Target model**: Llama-70B (30 tokens/second)\n**Draft model**: Llama-7B (300 tokens/second)\n**Speculation length**: $k = 4$ tokens\n\n**Scenario**: Generating \"The quick brown fox jumps\"\n\n```\nStep 1: Draft model generates 4 tokens\n        \"The\" → [quick, brown, fox, jumps]\n        Time: 4/300 = 13ms\n\nStep 2: Target model verifies in parallel\n        Input: \"The quick brown fox jumps\"\n        Accepts: \"quick\", \"brown\" (match)\n        Rejects: \"fox\" → target predicted \"lazy\"\n        Time: 1/30 = 33ms\n\nStep 3: Output \"quick brown\", continue from \"brown\"\n        Effective tokens: 2 in 46ms = 43 tokens/second\n\nStep 4: Repeat from \"brown\"\n```\n\n**Effective speedup**: 43/30 = 1.43x\n\n**Factors affecting speedup**:\n\n| Acceptance Rate | Speedup |\n|-----------------|---------|\n| 90% (easy text) | 2.5-3x |\n| 70% (typical) | 1.5-2x |\n| 50% (hard text) | 1.2-1.5x |\n| 30% (very hard) | <1x (overhead) |\n\nSpeedup depends on how well the draft model predicts the target model's output. For well-aligned model pairs (same training data, similar architecture), acceptance rates of 70-80% are common.\n\n:::\n\n**Self-speculative decoding** uses early exit from the target model itself as the draft, avoiding the need for a separate model:\n\n```\nTarget model layers: 80\n\nDraft: Layers 1-20 → predict next token\nVerify: Layers 1-80 → confirm or reject\n```\n\nThis eliminates the need to load and manage a separate draft model, at the cost of lower acceptance rates than a dedicated draft model.\n\n### KV Cache in Distributed Settings {#sec-inference-kv-cache-distributed}\n\nWhen models are sharded across devices (see @sec-inference-sharding), KV cache management gains additional complexity:\n\n**Tensor parallelism**: KV cache is sharded across devices along with attention heads. Each device stores cache for its subset of heads.\n\n```\n8-way tensor parallelism:\nDevice 0: KV cache for heads 0-7\nDevice 1: KV cache for heads 8-15\n...\nDevice 7: KV cache for heads 56-63\n```\n\n**Cross-device sharing**: Prefix caching across tensor-parallel devices requires cache to be sharded identically on all devices. This is automatic when prefixes are processed with the same tensor-parallel configuration.\n\n**KV cache migration**: When consistent hashing routes a conversation to a different replica (due to failure or rebalancing), the KV cache must be migrated:\n\n```\nMigration options:\n1. Rebuild: Re-run prefill on new replica (500ms+ for long context)\n2. Transfer: Send KV cache over network (100MB at 100Gbps = 8ms)\n3. Hybrid: Transfer if small, rebuild if large\n\nDecision threshold:\nif cache_size_bytes / network_bandwidth < prefill_time:\n    transfer()\nelse:\n    rebuild()\n```\n\nFor Llama-70B with 4K context, KV cache is ~80MB per sequence. At 100 Gbps, transfer takes 6.4ms versus ~500ms for prefill. Transfer is clearly better.\n\n### Memory Management Best Practices {#sec-inference-kv-best-practices}\n\nEffective KV cache management combines multiple techniques:\n\n**Sizing the KV cache pool**:\n\n```\nAvailable GPU memory = Total - Weights - Activations - Overhead\nKV pool size = 0.9 × Available  # Leave 10% headroom\n\nMax concurrent sequences = KV pool size / (avg_seq_length × per_token_cache)\n```\n\n**Eviction policies** when cache is full:\n\n- **LRU (Least Recently Used)**: Evict sequences with oldest last access\n- **Size-based**: Evict longest sequences first (free most memory)\n- **Priority-based**: Protect high-priority or paid-tier requests\n\n**Preemption** for continuous batching:\n\nWhen a new high-priority request cannot fit:\n\n1. Select victim sequence(s) using eviction policy\n2. Swap victim's KV cache to CPU memory\n3. Allocate GPU memory to new request\n4. When victim is resumed, swap back from CPU\n\n::: {.callout-note title=\"KV Cache Memory Hierarchy\"}\n\nProduction systems use a memory hierarchy for KV cache:\n\n| Tier | Capacity | Latency | Use Case |\n|------|----------|---------|----------|\n| GPU HBM | 80GB | 0ms | Active sequences |\n| CPU DRAM | 1TB | 1-5ms | Swapped sequences |\n| NVMe SSD | 10TB | 10-50ms | Long-term cache |\n\n**Swap implementation**:\n\n```python\nasync def swap_to_cpu(sequence_id):\n    kv_cache = gpu_cache[sequence_id]\n    cpu_cache[sequence_id] = kv_cache.cpu()  # Async transfer\n    gpu_cache.free(sequence_id)\n\n\nasync def swap_to_gpu(sequence_id):\n    cpu_kv = cpu_cache[sequence_id]\n    gpu_cache[sequence_id] = cpu_kv.cuda()  # Async transfer\n    cpu_cache.free(sequence_id)\n```\n\n**Observed performance**:\n\n- GPU-only (no swapping): 50 concurrent sequences\n- GPU+CPU swapping: 500 concurrent sequences (10x)\n- Average swap latency: 3ms (acceptable for non-urgent requests)\n\n:::\n\n## Weight Quantization for Serving {#sec-inference-weight-quantization}\n\n::: {.callout-note title=\"Building on Volume I\"}\nThis section assumes familiarity with quantization fundamentals covered in @sec-model-optimizations-quantization-precision-optimization-e90a. Here we focus on production inference-specific quantization challenges: post-training methods designed for LLM deployment, hardware-deployment co-design, and the interaction between quantization and serving system design.\n:::\n\nQuantization reduces numerical precision of model weights and activations, decreasing memory footprint and increasing throughput. While Volume I covers quantization fundamentals including post-training quantization (PTQ), quantization-aware training (QAT), and precision format tradeoffs, serving at scale introduces distinct challenges. Models must be quantized after training without access to training data. Quantization must preserve quality across diverse inputs. Hardware deployment targets vary from datacenter GPUs to edge accelerators. This section examines quantization techniques specifically designed for production inference.\n\n### LLM-Specific Quantization Challenges {#sec-inference-llm-quantization}\n\nLarge language models present unique quantization challenges distinct from vision or recommendation models. The outlier activation problem occurs because certain attention heads produce activation magnitudes orders of magnitude larger than typical values. Naive quantization clips these outliers, causing significant quality degradation.\n\nConsider a Llama-70B layer where most activations fall within [-10, 10] but specific channels reach magnitudes of 1000+. Symmetric INT8 quantization with range [-127, 127] must choose:\n\n- **Wide range** [-1000, 1000]: Most values map to 0, losing information\n- **Narrow range** [-10, 10]: Outliers clip, causing large errors\n\nThis outlier distribution motivates the specialized quantization methods that follow.\n\n### GPTQ: Layer-by-Layer Weight Quantization {#sec-inference-gptq}\n\nGPTQ (Generalized Post-Training Quantization) quantizes LLM weights using Hessian-based error compensation. Rather than quantizing all weights independently, GPTQ adjusts remaining weights to compensate for errors introduced by quantization.\n\n**Algorithm overview**:\n\n1. Process model layer by layer\n2. For each layer, calibrate using small dataset (128-256 samples)\n3. Quantize weights in order of decreasing Hessian magnitude\n4. Adjust remaining weights to minimize output error\n\n**Key insight**: The Hessian matrix $H = X^T X$ captures which weights most affect outputs. GPTQ builds on the Optimal Brain Surgeon (OBS) framework, using the inverse Hessian to guide error compensation.\n\n**Column-wise processing algorithm**:\n\nGPTQ processes each weight matrix column by column, using Cholesky decomposition of $H^{-1}$ for efficiency:\n\n```\nFor each layer's weight matrix W:\n  1. Compute Hessian: H = X^T X from calibration activations\n  2. Apply Cholesky factorization to H^{-1}\n  3. For each column q = 1 to d_col:\n     a. Quantize column: w_q = round(W[:,q] / Δ) × Δ\n     b. Compute quantization error: δ = W[:,q] - w_q\n     c. Update remaining columns to compensate:\n        W[:,q+1:] += δ × [H^{-1}][:,q+1:] / [H^{-1}]_{qq}\n```\n\nThe compensation step propagates quantization error to unquantized columns, where the Hessian-weighted update minimizes output deviation. This achieves $O(d_{row} \\cdot d_{col}^2)$ complexity versus $O(d_{row} \\cdot d_{col}^3)$ for naive OBS.\n\n**Quantization formula**:\n\n$$w_q = \\text{round}\\left(\\frac{w}{\\Delta}\\right) \\cdot \\Delta$$\n\nwhere $\\Delta$ is the quantization step size. GPTQ uses per-group quantization with group sizes of 128 weights, enabling finer-grained scaling that reduces quantization error for outlier channels.\n\n**Performance characteristics**:\n\n| Model | Bits | Perplexity Increase | Memory Reduction | Quantization Time |\n|-------|------|---------------------|------------------|-------------------|\n| Llama-7B | 4 | +0.3 | 4x | 15 min |\n| Llama-13B | 4 | +0.2 | 4x | 30 min |\n| Llama-70B | 4 | +0.15 | 4x | 3 hours |\n\nGPTQ's strengths include fast quantization without retraining, minimal quality loss for 4-bit weights, and broad hardware compatibility. Its limitations include requiring calibration data, sensitivity to calibration set selection, and per-layer processing that cannot leverage cross-layer information.\n\n### AWQ: Activation-Aware Weight Quantization {#sec-inference-awq}\n\nAWQ (Activation-Aware Weight Quantization) observes that not all weights are equally important. Weights connected to channels with large activation magnitudes have disproportionate impact on outputs.\n\n**Key insight**: Rather than protecting weights based on their own magnitude, protect weights based on the magnitude of activations they produce.\n\n**Algorithm**:\n\n1. Run calibration samples to measure per-channel activation magnitudes\n2. Identify \"salient\" channels with large activations\n3. Scale weights for salient channels up before quantization\n4. Scale outputs down correspondingly (fused into subsequent layer)\n\n**Scaling formulation**:\n\nFor weight matrix $W$ and activation statistics $s$ (per-channel activation magnitudes):\n\n$$W' = W \\cdot \\text{diag}(s^\\alpha)$$\n\nwhere $\\alpha \\in [0.5, 1.0]$ controls scaling aggressiveness. This preserves salient channels while allowing aggressive quantization of less important weights.\n\n**Comparison with GPTQ**:\n\n| Aspect | GPTQ | AWQ |\n|--------|------|-----|\n| Error compensation | Adjusts remaining weights | Scales salient channels |\n| Calibration data | 128-256 samples | 128 samples |\n| Quality (4-bit) | Very good | Excellent |\n| Speed | Faster | Slightly slower |\n| Hardware compatibility | Broad | Broad |\n\nAWQ typically achieves 0.5-1% lower perplexity degradation than GPTQ at the same bit-width, making it preferred for production deployments where quality is paramount.\n\n### SmoothQuant: Migrating Quantization Difficulty {#sec-inference-smoothquant}\n\nSmoothQuant addresses the activation outlier problem by migrating quantization difficulty from activations to weights. Weights have predictable distributions; activations have unpredictable outliers. SmoothQuant transfers the outlier problem to weights where it can be handled with per-channel scaling.\n\n**Core technique**: Insert smoothing operations that divide activations by per-channel scales while multiplying weights by the same scales:\n\n$$Y = X W = (X \\cdot \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s) \\cdot W) = \\hat{X} \\hat{W}$$\n\nThis transformation is mathematically equivalent but produces smoother activation distributions.\n\n**Migration strength** $\\alpha$ controls the trade-off:\n\n$$s_j = \\max(|X_j|)^\\alpha / \\max(|W_j|)^{1-\\alpha}$$\n\n- $\\alpha = 0$: No migration, activations remain difficult\n- $\\alpha = 1$: Full migration, weights absorb all difficulty\n- $\\alpha = 0.5$: Balanced (typical setting)\n\n**W8A8 deployment**: SmoothQuant enables INT8 quantization for both weights and activations:\n\n| Configuration | Memory | Prefill Speedup | Decode Speedup | Quality |\n|---------------|--------|-----------------|----------------|---------|\n| FP16 (baseline) | 1x | 1x | 1x | Baseline |\n| W8A16 (weights only) | 2x | 1.3x | 1.8x | <0.5% loss |\n| W8A8 (SmoothQuant) | 2x | 1.8-2x | 1.3-1.5x | <1% loss |\n\n**Critical distinction**: W8A8 provides near 2x speedup for compute-bound prefill (large batch processing initial prompt), but only 1.3-1.5x speedup for memory-bound decode (generating tokens one at a time). LLM serving is typically decode-heavy, so real-world throughput improvements from W8A8 are often 1.3-1.7x rather than the theoretical 2x compute throughput of INT8 Tensor Cores.\n\n### KV Cache Quantization {#sec-inference-kv-cache-quantization}\n\nWhile weight quantization reduces model storage, the KV cache dominates memory consumption for long-context LLM serving. At 32K+ token context lengths, KV cache can exceed model weights in memory usage. KV cache quantization addresses this critical bottleneck.\n\n**KV cache memory scaling**:\n\n$$\\text{KV Cache} = 2 \\times \\text{layers} \\times \\text{heads} \\times d_{head} \\times \\text{seq\\_len} \\times \\text{batch} \\times \\text{bytes}$$\n\nFor a 70B model (80 layers, 64 heads, 128 $d_{head}$) with 32K context in FP16:\n\n$$\\text{KV per sequence} = 2 \\times 80 \\times 64 \\times 128 \\times 32768 \\times 2 = 85.9 \\text{ GB}$$\n\nA single long-context sequence consumes more memory than the model weights.\n\n**Key insight**: KV cache values exhibit different distributions than model weights, enabling targeted quantization strategies:\n\n- **Keys**: Relatively uniform distributions, tolerate aggressive quantization (2-4 bits)\n- **Values**: More sensitive to quantization, require careful calibration (4-6 bits)\n\n**KIVI (Key-Value cache quantization for Inference)**:\n\nKIVI quantizes keys and values asymmetrically based on their sensitivity:\n\n| Component | Precision | Grouping | Quality Impact |\n|-----------|-----------|----------|----------------|\n| Keys | 2-bit | Per-channel | Minimal |\n| Values | 4-bit | Per-token | <0.5% perplexity |\n| Combined | ~3-bit average | Mixed | <1% perplexity |\n\n**Memory impact of combined weight and KV quantization**:\n\n| Configuration | Weight Size | KV Cache (32K) | Total Memory |\n|---------------|-------------|----------------|--------------|\n| FP16/FP16 | 140GB | 86GB | 226GB |\n| W4A16/FP16 | 35GB | 86GB | 121GB |\n| W4A16/KV4 | 35GB | 21GB | 56GB |\n\nKV cache quantization enables 4x longer contexts or 4x higher batch sizes on the same hardware.\n\n**Integration with PagedAttention**: Quantized KV cache requires quantization-aware block management:\n\n```python\n# Conceptual: vLLM with KV cache quantization\nfrom vllm import LLM\n\nllm = LLM(\n    model=\"meta-llama/Llama-2-70b-hf\",\n    kv_cache_dtype=\"fp8\",  # FP8 E4M3 for KV cache\n    quantization=\"awq\",  # 4-bit weights\n)\n```\n\nWith FP8 KV cache, the per-sequence memory drops from 2.5MB to 1.25MB per 1K tokens, doubling maximum concurrent sequences.\n\n### Rotation-Based Quantization {#sec-inference-rotation-quantization}\n\nTraditional quantization methods (GPTQ, AWQ, SmoothQuant) address outliers through compensation or migration. **Rotation-based quantization** takes a fundamentally different approach: mathematically transforming the weight and activation space to eliminate outliers entirely.\n\n**Key insight**: Outliers are artifacts of the coordinate basis representation. Rotating to a different basis spreads extreme values uniformly, making all values quantization-friendly.\n\n**QuaRot (Quantization with Rotation)**:\n\nQuaRot applies orthogonal Hadamard transforms to weights and activations:\n\n$$X' = X \\cdot H, \\quad W' = H^T \\cdot W$$\n\nwhere $H$ is a Hadamard matrix (orthogonal, efficiently computable without storage).\n\n**Why Hadamard transforms work**: For a vector with one extreme outlier, the Hadamard transform distributes that outlier's magnitude across all dimensions:\n\n```\nOriginal:  [1000, 1, 1, 1]     # One large outlier\nHadamard:  [252, 250, 250, 250] # Uniform distribution\n```\n\nAfter transformation, no single value dominates, enabling uniform quantization.\n\n**Advantages over migration-based methods**:\n\n| Aspect | SmoothQuant | QuaRot |\n|--------|-------------|--------|\n| Calibration data | Required | Not required (data-free) |\n| Minimum precision | W8A8 | W4A4 |\n| Runtime overhead | ~0% | ~3% (Hadamard transforms) |\n| Outlier handling | Migration | Elimination |\n\n**Performance comparison**:\n\n| Method | Precision | LLaMA-2-70B Perplexity | vs Baseline |\n|--------|-----------|------------------------|-------------|\n| FP16 | W16A16 | 3.12 | Baseline |\n| SmoothQuant | W8A8 | 3.18 | +0.06 |\n| GPTQ | W4A16 | 3.24 | +0.12 |\n| QuaRot | W4A4 | 3.31 | +0.19 |\n\nQuaRot achieves 4-bit weights AND 4-bit activations with quality competitive to GPTQ's 4-bit weights only. This enables approximately 8x memory reduction versus FP16.\n\n**SpinQuant**: An extension of QuaRot that learns optimal rotation matrices during a short fine-tuning phase, improving quality at the cost of training compute.\n\n### Hardware-Deployment Co-design {#sec-inference-quant-hardware}\n\nQuantization strategies must match target hardware capabilities. Different accelerators support different precisions with varying performance multipliers.\n\n**NVIDIA Tensor Core Support**:\n\n| Format | Ampere (A100) | Hopper (H100) | Speedup vs FP16 |\n|--------|---------------|---------------|-----------------|\n| FP16 | Yes | Yes | 1x |\n| BF16 | Yes | Yes | 1x |\n| INT8 | Yes | Yes | 2x |\n| FP8 (E4M3) | No | Yes | 2x |\n| INT4 | Via CUTLASS | Native | 4x |\n\n**Memory bandwidth dominance**: For autoregressive LLM decode, memory bandwidth limits throughput since each token reads the entire model:\n\n$$\\text{Decode throughput} \\propto \\frac{\\text{Memory bandwidth}}{\\text{Model size in bytes}}$$\n\n4-bit quantization delivers 4x throughput improvement for memory-bound decode, making it highly valuable despite modest compute gains.\n\n**Deployment configurations**:\n\n| Quantization | Best For | Framework Support |\n|--------------|----------|-------------------|\n| W4A16 (GPTQ/AWQ) | Consumer GPUs, memory-constrained | vLLM, TensorRT-LLM, llama.cpp |\n| W8A8 (SmoothQuant) | INT8 accelerators, high throughput | TensorRT-LLM, ONNX Runtime |\n| FP8 | H100/H200 deployments | TensorRT-LLM |\n| W4A4 | Research, extreme compression | Limited |\n\n### Framework Integration {#sec-inference-quant-frameworks}\n\nProduction serving frameworks integrate quantization with their batching and memory management systems.\n\n**vLLM quantization**:\n\n```python\nfrom vllm import LLM\n\n# Load AWQ-quantized model\nllm = LLM(\n    model=\"TheBloke/Llama-2-70B-AWQ\",\n    quantization=\"awq\",\n    dtype=\"float16\",  # Activations in FP16\n    gpu_memory_utilization=0.9,\n)\n```\n\nvLLM automatically handles quantized weight loading, kernel selection for W4A16 GEMM operations, and KV cache management.\n\n**TensorRT-LLM quantization**:\n\n```bash\n# Quantize model with AWQ\npython quantize.py --model_dir /path/to/llama-70b \\\n                   --output_dir /path/to/llama-70b-awq \\\n                   --qformat int4_awq \\\n                   --calib_size 512\n```\n\nTensorRT-LLM generates optimized kernels for specific GPU architectures, fuses operations to minimize memory traffic, and supports both weight-only and W8A8 quantization.\n\n**Quantization + PagedAttention**: Quantized models combine with PagedAttention for maximum memory efficiency:\n\n$$\\text{Max batch} = \\frac{\\text{GPU Memory} - \\text{Quantized Weights}}{\\text{KV Cache per Sequence}}$$\n\nA 70B model with 4-bit weights requires approximately 35GB, leaving 45GB on an 80GB A100 for KV cache. With FP16 KV cache at 2.5MB per 1K tokens per sequence, this supports ~18 concurrent 1K-token sequences versus ~8 with FP16 weights.\n\n### Quantization Selection Guidelines {#sec-inference-quant-selection}\n\nChoosing the appropriate quantization method depends on deployment constraints:\n\n**Decision framework**:\n\n```\n1. Is latency or throughput the primary goal?\n   - Latency-sensitive: Prefer FP16/BF16 (no quantization overhead)\n   - Throughput-oriented: Quantization typically beneficial\n\n2. What hardware is available?\n   - H100/H200: Consider FP8 (native support, minimal quality loss)\n   - A100/A10G: W8A8 or W4A16 depending on workload\n   - Consumer GPUs: W4A16 often necessary for memory\n\n3. What quality requirements exist?\n   - <0.5% degradation acceptable: AWQ 4-bit\n   - <1% degradation acceptable: GPTQ 4-bit or SmoothQuant W8A8\n   - No degradation acceptable: FP16/BF16 only\n\n4. Is the model compute-bound or memory-bound?\n   - Compute-bound (prefill): W8A8 provides 2x speedup\n   - Memory-bound (decode): W4A16 provides 4x memory bandwidth\n```\n\n**Quantization impact on serving cost**:\n\n| Configuration | Cost per 1M tokens (estimated) |\n|---------------|--------------------------------|\n| FP16 on 8xA100 | $2.40 |\n| AWQ 4-bit on 4xA100 | $1.20 |\n| AWQ 4-bit on 2xA100 | $0.60 |\n\nQuantization can reduce serving costs by 2-4x while maintaining acceptable quality, making it essential for cost-effective LLM deployment.\n\n## Multi-Tenancy and Isolation {#sec-inference-multitenancy}\n\nProduction inference platforms serve multiple customers, models, and workloads on shared infrastructure. Multi-tenancy enables efficient resource utilization but introduces challenges around isolation, fairness, and quality of service guarantees. A noisy neighbor consuming excessive resources can degrade performance for all other tenants.\n\nThis section examines the techniques for sharing inference infrastructure while maintaining isolation between tenants.\n\n### The Multi-Tenancy Challenge {#sec-inference-multitenancy-challenge}\n\nMulti-tenancy provides significant benefits:\n\n- **Cost efficiency**: Sharing infrastructure across tenants improves utilization\n- **Operational simplicity**: Fewer clusters to manage, monitor, and upgrade\n- **Statistical multiplexing**: Aggregate traffic is more predictable than per-tenant traffic\n\nHowever, sharing introduces risks:\n\n- **Noisy neighbors**: One tenant's burst traffic impacts others\n- **Resource contention**: GPU memory, network bandwidth, CPU cycles\n- **Security boundaries**: Tenant data must remain isolated\n- **SLO complexity**: Different tenants have different requirements\n\n| Aspect | Single-Tenant | Multi-Tenant |\n|--------|--------------|--------------|\n| Resource utilization | 30-50% | 70-90% |\n| Cost per request | Higher | Lower |\n| SLO guarantees | Simple | Complex |\n| Isolation | Complete | Requires engineering |\n| Operational overhead | Higher (many clusters) | Lower (fewer clusters) |\n\n: **Single vs Multi-Tenant Tradeoffs**: Multi-tenancy reduces cost but requires careful isolation engineering. {#tbl-tenancy-comparison}\n\n### Noisy Neighbor Problems {#sec-inference-noisy-neighbor}\n\nThe noisy neighbor problem occurs when one tenant's workload degrades performance for others sharing the same infrastructure.\n\n**GPU memory contention**: A tenant with unexpectedly long sequences consumes KV cache memory, forcing evictions that impact other tenants.\n\n```\nScenario: 3 tenants sharing GPU with 60GB KV cache pool\n\nNormal state:\n  Tenant A: 20GB (200 sequences)\n  Tenant B: 20GB (200 sequences)\n  Tenant C: 20GB (200 sequences)\n\nNoisy neighbor (Tenant C starts long-context requests):\n  Tenant C: 45GB (150 sequences, longer context)\n  Tenant A: 7.5GB (evicted to 75 sequences)\n  Tenant B: 7.5GB (evicted to 75 sequences)\n\nImpact: Tenants A and B see 62% reduction in batch size\n```\n\n**Network bandwidth saturation**: A tenant streaming many large responses saturates network bandwidth, increasing latency for all tenants.\n\n**Compute interference**: GPU time-sharing between tenants introduces context-switching overhead and unpredictable latency.\n\n::: {.callout-note title=\"Quantifying Noisy Neighbor Impact\"}\n\nConsider an inference platform serving 10 tenants on shared H100 GPUs:\n\n**Baseline (even load)**:\n\n- Each tenant: 100 QPS, 10ms P99 latency\n- GPU utilization: 70%\n- All SLOs met\n\n**Noisy neighbor scenario** (Tenant 3 bursts to 500 QPS):\n\n| Tenant | QPS | P99 Latency | SLO Status |\n|--------|-----|-------------|------------|\n| Tenant 1 | 100 | 25ms | Violated |\n| Tenant 2 | 100 | 28ms | Violated |\n| Tenant 3 | 500 | 45ms | Violated |\n| Tenants 4-10 | 100 each | 22-30ms | Violated |\n\nWithout isolation, one tenant's burst causes cascade failures for all tenants.\n\n**With isolation** (per-tenant resource quotas):\n\n| Tenant | QPS (actual) | P99 Latency | SLO Status |\n|--------|-------------|-------------|------------|\n| Tenant 1 | 100 | 11ms | Met |\n| Tenant 2 | 100 | 11ms | Met |\n| Tenant 3 | 120 (throttled) | 50ms | Violated (only for them) |\n| Tenants 4-10 | 100 each | 11ms | Met |\n\nIsolation contains the impact to the offending tenant.\n\n:::\n\n### Resource Quotas and Fair Sharing {#sec-inference-quotas}\n\nResource quotas limit what each tenant can consume, preventing any single tenant from monopolizing shared resources.\n\n**Hard quotas** enforce strict limits:\n\n```python\nclass TenantQuota:\n    max_concurrent_requests: int  # e.g., 100\n    max_kv_cache_mb: int  # e.g., 20,000\n    max_qps: int  # e.g., 1,000\n    max_batch_tokens: int  # e.g., 50,000\n\n\ndef admit_request(tenant_id, request):\n    quota = get_quota(tenant_id)\n    usage = get_usage(tenant_id)\n\n    if usage.concurrent >= quota.max_concurrent:\n        return RateLimitError(\"concurrent request limit\")\n    if usage.kv_cache_mb >= quota.max_kv_cache_mb:\n        return RateLimitError(\"memory limit\")\n    if usage.qps >= quota.max_qps:\n        return RateLimitError(\"rate limit\")\n\n    return admit(request)\n```\n\n**Soft quotas** with fair sharing allow exceeding limits when resources are available:\n\n```\nTenant quota: 100 QPS (soft limit)\n\nWhen cluster is underutilized (50%):\n  Tenant can burst to 200 QPS (2x quota)\n\nWhen cluster is saturated (90%):\n  Tenant limited to 100 QPS (quota enforced)\n```\n\nThis approach maximizes utilization while protecting tenants during contention.\n\n**Max-min fairness** allocates resources to maximize the minimum allocation:\n\n```\nTotal capacity: 1000 QPS\nTenants: A (demand 300), B (demand 200), C (demand 800)\nTotal demand: 1300 QPS (exceeds capacity)\n\nMax-min allocation:\n1. Give each tenant equal share: 333 QPS\n2. A needs only 300, donate 33 to others\n3. B needs only 200, donate 133 to others\n4. C receives donations: 333 + 33 + 133 = 499 QPS\n\nFinal: A=300, B=200, C=500\nAll demands met up to fair share, C limited proportionally\n```\n\n### Priority Scheduling {#sec-inference-priority-scheduling}\n\nWhen tenants have different SLO requirements, priority scheduling ensures high-priority requests receive resources first.\n\n**Priority classes**:\n\n| Class | Use Case | Preemption | Resource Guarantee |\n|-------|----------|------------|-------------------|\n| Critical | Revenue-generating | Can preempt lower | 100% reserved |\n| Standard | General traffic | Can preempt best-effort | Weighted share |\n| Best-effort | Background, batch | Cannot preempt | No guarantee |\n\n**Priority-aware queuing**:\n\n```\nIncoming requests sorted by priority, then arrival time:\n\nQueue state:\n  [Critical-001] [Critical-002] [Standard-001] [Standard-002] [BestEffort-001]\n       ↑ Process first\n\nNew Critical-003 arrives:\n  [Critical-001] [Critical-002] [Critical-003] [Standard-001] [Standard-002]\n       ↑ Jumps ahead of Standard requests\n```\n\n**Preemption for LLM serving**:\n\nWhen a critical request arrives but all GPU slots are occupied by lower-priority requests:\n\n1. Select victim request(s) from lowest priority class\n2. Pause victim's generation (save KV cache state)\n3. Allocate GPU slot to critical request\n4. When critical request completes, resume victim\n\n```\nBefore preemption:\n  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]\n\nCritical request arrives:\n  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]\n                ↓ preempt\n  GPU slots: [Critical E] [Standard B] [Standard C] [Standard D]\n  Paused: Best-effort A (KV cache saved to CPU)\n\nAfter Critical E completes:\n  GPU slots: [Best-effort A] [Standard B] [Standard C] [Standard D]\n  (A resumed from saved state)\n```\n\n### Bulkhead Pattern {#sec-inference-bulkhead}\n\nThe bulkhead pattern physically isolates tenant workloads, preventing failures from propagating across tenants. Named after ship compartments that contain flooding to isolated sections.\n\n**Deployment-level bulkheads**: Dedicate replicas to specific tenants or tenant groups.\n\n```\nGPU Cluster (24 GPUs):\n\nTenant A (gold tier): GPUs 0-7 (dedicated)\nTenant B (gold tier): GPUs 8-15 (dedicated)\nTenants C-Z (shared): GPUs 16-23 (shared pool)\n```\n\n**Pros**: Complete isolation for premium tenants\n**Cons**: Lower utilization, more operational overhead\n\n**Request-level bulkheads**: Limit the fraction of resources any single request can consume.\n\n```\nPer-request limits:\n  max_input_tokens: 8,000\n  max_output_tokens: 2,000\n  max_execution_time: 30s\n\nPrevents single request from consuming excessive resources.\n```\n\n**Failure isolation**: Errors in one tenant's requests do not affect others.\n\n```\nTenant A sends malformed input causing model error:\n  Without bulkhead: Error may crash shared inference worker\n  With bulkhead: Error caught, only Tenant A's request fails\n                 Other tenants continue normally\n```\n\n::: {.callout-note title=\"Bulkhead Configuration for API Tiers\"}\n\nConsider an LLM API with three service tiers:\n\n**Enterprise tier**:\n\n- Dedicated GPU pool (no sharing)\n- Custom model fine-tuning\n- 99.9% availability SLO\n- Price: $$$\n\n**Professional tier**:\n\n- Shared GPU pool with guaranteed capacity\n- Priority scheduling over free tier\n- 99.5% availability SLO\n- Price: $$\n\n**Free tier**:\n\n- Shared GPU pool, best-effort\n- Rate limited (10 QPS)\n- No SLO guarantee\n- Price: Free\n\n**Bulkhead configuration**:\n\n```yaml\ntiers:\n  enterprise:\n    gpu_pool: \"dedicated\"\n    isolation: \"hardware\"\n    replicas: 8\n    preemption: false\n\n  professional:\n    gpu_pool: \"shared-premium\"\n    isolation: \"resource-quota\"\n    quota_fraction: 0.7  # 70% of shared pool\n    preemption: true\n\n  free:\n    gpu_pool: \"shared-premium\"\n    isolation: \"resource-quota\"\n    quota_fraction: 0.3  # 30% of shared pool\n    preemption: false  # can be preempted\n```\n\n:::\n\n### Model Isolation {#sec-inference-model-isolation}\n\nWhen multiple models run on shared infrastructure, additional isolation is needed:\n\n**Memory isolation**: Ensure one model's memory usage does not impact others.\n\n```\nGPU memory partitioning (80GB H100):\n\nModel A: 40GB reserved (50%)\nModel B: 30GB reserved (37.5%)\nShared pool: 10GB (12.5%)\n```\n\n**Compute isolation**: GPU time-sharing between models introduces latency variance. Options include:\n\n- **MIG (Multi-Instance GPU)**: Hardware partitioning of A100/H100 into isolated GPU instances\n- **Time-slicing**: Cooperative scheduling between models (higher overhead)\n- **Dedicated GPUs**: Each model gets dedicated hardware (lower utilization)\n\n**Model loading isolation**: Loading one model should not evict another from GPU memory.\n\n```python\nclass ModelManager:\n    def load_model(self, model_id, priority):\n        required_memory = get_model_size(model_id)\n        available = get_free_gpu_memory()\n\n        if required_memory > available:\n            # Check if eviction would violate isolation\n            evictable = get_evictable_memory(priority)\n            if required_memory > evictable:\n                raise InsufficientMemoryError(\n                    \"Cannot load without violating isolation constraints\"\n                )\n            evict_lower_priority_models(priority, required_memory)\n\n        load_to_gpu(model_id)\n```\n\n### Observability for Multi-Tenancy {#sec-inference-multitenancy-observability}\n\nEffective multi-tenancy requires per-tenant visibility into resource consumption and performance:\n\n**Per-tenant metrics**:\n\n- Request count, latency distribution (P50, P95, P99)\n- GPU memory usage, KV cache utilization\n- Throttling events, preemption counts\n- Error rates by error type\n\n**Alerting thresholds**:\n\n```yaml\nalerts:\n  - name: tenant_slo_violation\n    condition: p99_latency > slo_target * 1.1\n    for: 5m\n    severity: warning\n\n  - name: tenant_quota_exhaustion\n    condition: usage > quota * 0.9\n    for: 1m\n    severity: warning\n\n  - name: noisy_neighbor_detection\n    condition: usage > fair_share * 2.0\n    for: 5m\n    severity: info\n```\n\n**Chargeback and attribution**: Track resource consumption for billing and capacity planning.\n\n```\nTenant A monthly report:\n  Total requests: 10,000,000\n  GPU-seconds consumed: 50,000\n  KV cache GB-hours: 2,500\n  Network egress GB: 100\n\n  Billed: $X based on consumption\n```\n\n## Autoscaling {#sec-inference-autoscaling}\n\nProduction inference systems experience traffic fluctuations that make static provisioning inefficient. Autoscaling dynamically adjusts capacity to match demand, reducing costs during low-traffic periods while maintaining SLOs during peaks.\n\nThis section examines autoscaling strategies for inference, with particular attention to the cold start problem that makes GPU-based scaling more challenging than traditional web services.\n\n### Scaling Dimensions {#sec-inference-scaling-dimensions}\n\nInference systems can scale along multiple dimensions:\n\n**Horizontal scaling (replicas)**: Add or remove model replicas to adjust throughput.\n\n$$\\text{Capacity} = \\text{Replicas} \\times \\text{Per-replica throughput}$$\n\n**Vertical scaling (GPU type)**: Use more powerful GPUs for higher per-replica throughput.\n\n$$\\text{Cost efficiency} = \\frac{\\text{Throughput}}{\\text{GPU cost}}$$\n\n**Batch size scaling**: Adjust batch sizes to trade latency for throughput.\n\n$$\\text{Latency} \\uparrow \\text{ as } \\text{Batch size} \\uparrow \\text{, Throughput} \\uparrow$$\n\n| Scaling Type | Latency | Cost | Speed |\n|--------------|---------|------|-------|\n| Horizontal (add replicas) | Unchanged | Linear | Slow (minutes) |\n| Batch size | Increases | Unchanged | Instant |\n| Vertical (better GPU) | Unchanged | Non-linear | Very slow (redeployment) |\n\n: **Scaling Dimension Tradeoffs**: Each scaling approach has different characteristics. {#tbl-scaling-dimensions}\n\n### The Cold Start Problem {#sec-inference-cold-start}\n\nUnlike stateless web services that start in seconds, inference services have significant startup latency:\n\n$$T_{cold start} = T_{provision} + T_{load} + T_{warmup}$$ {#eq-cold-start-time}\n\n**Provisioning time** ($T_{provision}$): Acquiring a GPU instance takes 30 seconds to several minutes depending on cloud provider and GPU type.\n\n**Model loading time** ($T_{load}$): Loading model weights from storage to GPU memory. For large models:\n\n| Model Size | Load Time (SSD) | Load Time (S3) |\n|------------|-----------------|----------------|\n| 7B (14GB) | 5s | 30s |\n| 70B (140GB) | 45s | 5min |\n| 175B (350GB) | 2min | 12min |\n\n**Warmup time** ($T_{warmup}$): First inference after loading is slower due to:\n\n- JIT compilation of kernels\n- CUDA context initialization\n- Memory pool allocation\n- Cache population\n\nWarmup typically requires 10-30 dummy inferences, adding 5-30 seconds.\n\n::: {.callout-note title=\"Cold Start Timeline for Llama-70B\"}\n\nBringing up a new replica for Llama-70B on H100:\n\n| Phase | Duration | Cumulative |\n|-------|----------|------------|\n| Cloud API request | 5s | 5s |\n| GPU instance provisioning | 60s | 65s |\n| Container startup | 10s | 75s |\n| Model download (S3) | 180s | 255s |\n| Model load to GPU | 45s | 300s |\n| CUDA warmup | 15s | 315s |\n| Readiness probe pass | 5s | 320s |\n| **Total cold start** | **5 min 20 sec** | |\n\n**Implication**: Scaling decisions must anticipate demand 5+ minutes in advance. Reactive scaling alone cannot handle sudden traffic spikes.\n\n:::\n\n### Reactive Scaling {#sec-inference-reactive-scaling}\n\nReactive scaling adjusts capacity based on observed metrics:\n\n**Metric-based scaling**:\n\n```yaml\nautoscaling:\n  metric: cpu_utilization  # or gpu_utilization, queue_depth\n  target_value: 70%\n  scale_up_threshold: 80%\n  scale_down_threshold: 50%\n  cooldown_period: 300s\n```\n\n**Queue-depth scaling**: Scale based on request queue length.\n\n$$\\text{Desired replicas} = \\left\\lceil \\frac{\\text{Queue depth}}{\\text{Queue target}} \\times \\text{Current replicas} \\right\\rceil$$\n\n**Latency-based scaling**: Scale to maintain latency SLO.\n\n$$\\text{Desired replicas} = \\left\\lceil \\frac{P99_{observed}}{P99_{target}} \\times \\text{Current replicas} \\right\\rceil$$\n\n**Reactive scaling limitations**:\n\n1. Response delay: Cold start time prevents rapid response\n2. Oscillation: Can create scale-up/scale-down cycles\n3. Over-provisioning: Must provision for worst-case during cold start\n\n::: {.callout-note title=\"Reactive Scaling Response Analysis\"}\n\nConsider traffic spike from 1000 to 3000 QPS:\n\n**Current state**: 10 replicas, 100 QPS each, 70% utilization\n\n**Target state**: 30 replicas for 3000 QPS\n\n**Without pre-warming**:\n\n- T=0: Spike detected, scale-up triggered\n- T=0 to T=5min: Cold start for 20 new replicas\n- T=0 to T=5min: Existing 10 replicas handle 3000 QPS (300 QPS each)\n- Utilization: 210% (overloaded)\n- P99 latency: 500ms+ (SLO violated)\n\n**With pool of warm spares (5 replicas)**:\n\n- T=0: Spike detected, warm spares activated immediately\n- T=0: 15 replicas handle 3000 QPS (200 QPS each)\n- T=0 to T=5min: Scale up 15 more replicas\n- Utilization: 140% (elevated but manageable)\n- P99 latency: 80ms (SLO maintained)\n\nWarm spares provide buffer during cold start period.\n\n:::\n\n### Predictive Scaling {#sec-inference-predictive-scaling}\n\nPredictive scaling anticipates demand before it occurs, initiating scaling ahead of traffic changes.\n\n**Time-series forecasting**: Use historical patterns to predict future demand.\n\n```python\ndef predict_demand(current_time, history):\n    # Seasonal decomposition\n    daily_pattern = extract_daily_seasonality(history)\n    weekly_pattern = extract_weekly_seasonality(history)\n\n    # Trend estimation\n    trend = estimate_trend(history)\n\n    # Forecast\n    predicted = (\n        daily_pattern[current_time.hour]\n        * weekly_pattern[current_time.weekday()]\n        * trend\n    )\n    return predicted\n```\n\n**Event-driven scaling**: Scale proactively for known events.\n\n```yaml\nscheduled_scaling:\n  - event: \"product_launch\"\n    time: \"2024-03-15 09:00 UTC\"\n    target_replicas: 50  # 5x normal\n    ramp_up: 30min  # Start scaling 30min before\n\n  - event: \"weekly_newsletter\"\n    cron: \"0 10 * * 1\"  # Every Monday 10am\n    target_replicas: 20  # 2x normal\n    duration: 2h\n```\n\n**Hybrid approach**: Combine predictive baseline with reactive adjustment.\n\n$$\\text{Target replicas} = \\max(\\text{Predicted}, \\text{Reactive}) + \\text{Buffer}$$\n\n::: {.callout-note title=\"Predictive Scaling for Daily Traffic Patterns\"}\n\nA chatbot service shows predictable daily patterns:\n\n| Time (UTC) | Typical QPS | Replicas Needed |\n|------------|-------------|-----------------|\n| 00:00-06:00 | 500 | 5 |\n| 06:00-09:00 | 1500 | 15 (ramp up) |\n| 09:00-17:00 | 3000 | 30 (peak) |\n| 17:00-20:00 | 2000 | 20 (ramp down) |\n| 20:00-00:00 | 1000 | 10 |\n\n**Predictive schedule** (accounting for cold start):\n\n| Time | Action | Replicas Active | Replicas Starting |\n|------|--------|-----------------|-------------------|\n| 05:30 | Scale up | 5 | +10 warming |\n| 06:00 | Traffic ramp | 15 | - |\n| 08:30 | Scale up | 15 | +15 warming |\n| 09:00 | Peak traffic | 30 | - |\n| 17:00 | Scale down | 20 | -10 terminating |\n| 20:00 | Scale down | 10 | -10 terminating |\n| 00:00 | Scale down | 5 | -5 terminating |\n\n**Cost comparison**:\n\n- Reactive only: Must over-provision during ramp (45 replicas peak)\n- Predictive: Right-sized provisioning (30 replicas peak)\n- Savings: 33% GPU cost reduction\n\n:::\n\n### Warm Pool Management {#sec-inference-warm-pools}\n\nMaintaining a pool of pre-warmed replicas reduces effective cold start time:\n\n**Warm pool sizing**:\n\n$$\\text{Warm pool size} = \\frac{\\text{Max expected spike}}{\\text{Per-replica throughput}} \\times \\text{Headroom factor}$$\n\nFor example, if max spike is 2x normal and headroom factor is 1.5:\n\n$$\\text{Warm pool} = 2 \\times 1.5 = 3\\text{x minimum pool capacity}$$\n\n**Warm pool cost**: Maintaining warm replicas costs money even when idle.\n\n$$\\text{Warm pool cost} = \\text{Pool size} \\times \\text{GPU cost/hour} \\times \\text{Idle fraction}$$\n\nTrade-off: More warm replicas = faster response but higher cost.\n\n**Tiered warm pools**: Different readiness levels with different costs.\n\n| Tier | State | Response Time | Cost (relative) |\n|------|-------|---------------|-----------------|\n| Hot | GPU loaded, running | Instant | 100% |\n| Warm | GPU allocated, model loaded | 30s | 60% |\n| Cold | GPU not allocated | 5+ min | 0% |\n\n```\nPool configuration:\n  Hot: 2 replicas (instant burst capacity)\n  Warm: 5 replicas (30s activation)\n  Cold: Unlimited (cloud provider)\n\nScaling sequence:\n  1. Activate Hot replicas immediately\n  2. Activate Warm replicas within 30s\n  3. Cold start new replicas if demand persists\n```\n\n### Scaling Response Time Analysis {#sec-inference-scaling-response}\n\nThe total time to respond to a scaling event:\n\n$$T_{response} = T_{detect} + T_{decide} + T_{provision} + T_{warmup}$$ {#eq-scaling-response}\n\n| Component | Duration | Optimization |\n|-----------|----------|--------------|\n| Detection | 10-60s | Reduce metrics interval |\n| Decision | 1-5s | Faster autoscaler |\n| Provisioning | 30s-5min | Warm pools |\n| Warmup | 5-30s | Pre-compilation |\n\n**Optimizing each component**:\n\n**Detection speed**: Use high-frequency metrics (1s vs 60s intervals) for faster detection. Trade-off: More metric volume, potentially noisier signals.\n\n**Decision speed**: Pre-compute scaling plans based on predicted scenarios. When trigger occurs, execute pre-computed plan immediately.\n\n**Provisioning speed**: Warm pools eliminate provisioning for anticipated demand. Spot/preemptible instances can reduce provisioning time (already running, just need allocation).\n\n**Warmup speed**: Pre-compiled TensorRT engines skip JIT compilation. Lazy loading defers some initialization to first request.\n\n### Spot and Preemptible Instances {#sec-inference-spot-instances}\n\nCloud providers offer discounted GPU instances that can be reclaimed with short notice:\n\n| Instance Type | Discount | Interruption Notice | Use Case |\n|--------------|----------|--------------------| ---------|\n| On-demand | 0% | Never | SLO-critical |\n| Reserved | 30-60% | Never | Steady baseline |\n| Spot/Preemptible | 60-90% | 30s-2min | Burst capacity |\n\n**Graceful handling of spot termination**:\n\n```python\ndef handle_spot_termination():\n    # Received 2-minute warning\n    # 1. Stop accepting new requests\n    stop_accepting_requests()\n\n    # 2. Complete in-flight requests (if possible)\n    await complete_inflight(timeout=90)\n\n    # 3. Save state for resumption elsewhere\n    save_kv_cache_to_storage()\n\n    # 4. Signal load balancer to redirect traffic\n    deregister_from_loadbalancer()\n\n    # 5. Terminate gracefully\n    shutdown()\n```\n\n**Spot-aware architecture**:\n\n```\nTraffic distribution:\n\nRequest arrives\n    │\n    ▼\nLoad balancer\n    │\n    ├── 70% → On-demand replicas (guaranteed capacity)\n    │\n    └── 30% → Spot replicas (cost savings, may be interrupted)\n```\n\nBest-effort requests route to spot instances; SLO-critical requests use on-demand.\n\n## Global Inference Infrastructure {#sec-inference-global}\n\nProduction inference systems serving global user bases must operate across multiple geographic regions. A user in Tokyo expects low-latency responses regardless of where models were trained or where the company headquarters is located. This section examines the architectural patterns for multi-region inference deployment.\n\n### Why Multi-Region Matters {#sec-inference-global-why}\n\nSingle-region deployment creates fundamental limitations:\n\n**Latency floor**: Network round-trip time (RTT) to distant users cannot be optimized away:\n\n| User Location | RTT to US-East | RTT to Local Region |\n|---------------|----------------|---------------------|\n| New York | 10ms | 10ms |\n| London | 75ms | 10ms |\n| Tokyo | 150ms | 10ms |\n| Sydney | 200ms | 10ms |\n\nFor interactive applications (chatbots, autocomplete), these delays compound across multiple model calls per request.\n\n**Availability**: Single-region deployment creates a single point of failure. Cloud region outages, while rare, affect all users simultaneously.\n\n**Regulatory compliance**: Data residency requirements (GDPR, data sovereignty laws) may require processing user data within specific geographic boundaries.\n\n### Multi-Region Architecture Patterns {#sec-inference-global-patterns}\n\n**Pattern 1: Global load balancing with regional replicas**\n\n```\n                    Global Load Balancer\n                    (Latency-based routing)\n                           │\n         ┌─────────────────┼─────────────────┐\n         ▼                 ▼                 ▼\n    US-East           EU-West           Asia-Pacific\n    ┌─────────┐       ┌─────────┐       ┌─────────┐\n    │ vLLM    │       │ vLLM    │       │ vLLM    │\n    │ Replicas│       │ Replicas│       │ Replicas│\n    └─────────┘       └─────────┘       └─────────┘\n         │                 │                 │\n         └─────────────────┼─────────────────┘\n                           ▼\n                    Model Registry\n                    (Synchronized)\n```\n\nEach region runs independent inference replicas with identical models. The global load balancer routes users to the nearest region based on latency.\n\n**Key considerations**:\n\n- **Model synchronization**: Model updates must propagate to all regions. Options include:\n  - Push-based: Central registry pushes to all regions (simple, potential inconsistency window)\n  - Pull-based: Regions poll for updates (higher latency, guaranteed consistency)\n  - Hybrid: Push notification + pull verification\n\n- **Version consistency**: During model rollouts, different regions may briefly serve different versions. For most applications this is acceptable; for applications requiring strict consistency, implement version pinning in request routing.\n\n**Pattern 2: Edge caching with central inference**\n\nFor models too large to replicate globally, cache responses at the edge:\n\n```\nUser → Edge Cache (CDN) → Regional Proxy → Central Inference\n           │                    │\n           └── Cache hit ───────┘\n               (< 10ms)\n\n           └── Cache miss ──────────────────→\n               (Full latency, populate cache)\n```\n\n**Effectiveness depends on request repeatability**:\n\n| Workload | Cache Hit Rate | Suitability |\n|----------|----------------|-------------|\n| Autocomplete | 60-80% | Excellent |\n| FAQ chatbot | 40-60% | Good |\n| Open-ended chat | 5-15% | Poor |\n| Code generation | 20-40% | Moderate |\n\nSemantic caching (caching based on embedding similarity rather than exact match) can improve hit rates for open-ended workloads.\n\n**Pattern 3: Federated inference with model sharding**\n\nFor the largest models, shard across regions:\n\n```\nUser request\n    │\n    ▼\nRequest Router\n    │\n    ├── Layers 1-40  → US-East GPUs\n    │\n    └── Layers 41-80 → EU-West GPUs\n\n    Pipeline parallelism across regions\n```\n\nThis pattern is rarely practical due to inter-region latency dominating compute time, but may apply for extremely large models where no single region has sufficient GPU capacity.\n\n### Cross-Region Failover {#sec-inference-global-failover}\n\nWhen a region becomes unavailable, traffic must reroute to healthy regions:\n\n**Active-active failover**:\n\n```python\n# Simplified global routing logic\ndef route_request(user_region, request):\n    primary = get_nearest_healthy_region(user_region)\n    secondary = get_second_nearest_healthy_region(user_region)\n\n    try:\n        return call_region(primary, request, timeout=2.0)\n    except (Timeout, RegionUnavailable):\n        # Failover with increased latency\n        return call_region(secondary, request, timeout=5.0)\n```\n\n**Failover considerations for stateful LLM serving**:\n\n- **Session affinity loss**: Users mid-conversation lose KV cache state. The fallback region must regenerate context from conversation history.\n- **Capacity spike**: The receiving region sees sudden traffic increase. Pre-provision headroom (typically 30-50% over steady-state) or accept degraded latency during failover.\n- **Gradual recovery**: When the failed region recovers, gradually shift traffic back to avoid oscillation.\n\n### Global Model Deployment {#sec-inference-global-deployment}\n\nDeploying model updates across regions requires careful coordination:\n\n**Phased rollout strategy**:\n\n```\n1. Deploy to canary region (e.g., 1% traffic in US-East)\n2. Monitor metrics for 1 hour\n3. If healthy, deploy to remaining US-East replicas\n4. Monitor for 4 hours\n5. Deploy to EU-West (different user population)\n6. Monitor for 4 hours\n7. Deploy to Asia-Pacific\n8. Complete rollout\n```\n\n**Rollback across regions**:\n\nIf issues are detected after partial deployment:\n\n```\nRegion Status:\n  US-East:      v2.1 (new) ← Issue detected\n  EU-West:      v2.0 (old)\n  Asia-Pacific: v2.0 (old)\n\nAction: Rollback US-East to v2.0\n  - Switch traffic to v2.0 replicas\n  - Maintain v2.1 replicas for debugging\n  - Do not proceed with EU-West deployment\n```\n\n**Metrics for global deployment health**:\n\n| Metric | Per-Region | Global |\n|--------|-----------|--------|\n| Error rate | < 0.1% | < 0.1% |\n| P99 latency | < target | < 2x single-region |\n| Throughput | Stable | Stable |\n| Model quality | Within bounds | Consistent across regions |\n\n### Cost Optimization Across Regions {#sec-inference-global-cost}\n\nGPU pricing varies by region. Optimize placement for cost while meeting latency requirements:\n\n| Region | H100 Spot Price | On-Demand | Latency to US Users |\n|--------|-----------------|-----------|---------------------|\n| US-East | $2.50/hr | $4.00/hr | 10-50ms |\n| US-West | $2.30/hr | $3.80/hr | 30-70ms |\n| EU-West | $2.80/hr | $4.20/hr | 75-100ms |\n\n**Cost-aware routing**:\n\nFor latency-tolerant workloads (batch inference, background processing), route to the cheapest available region:\n\n```python\ndef route_batch_request(request):\n    if request.priority == \"low\":\n        # Route to cheapest region with capacity\n        return get_cheapest_region_with_capacity()\n    else:\n        # Route to nearest region\n        return get_nearest_region(request.user_location)\n```\n\nThis can reduce costs by 20-40% for batch workloads while maintaining SLOs for interactive traffic.\n\n## Case Studies {#sec-inference-case-studies}\n\nThe techniques presented throughout this chapter come together in production systems serving billions of requests daily. This section examines four case studies that illustrate different points in the inference design space: Meta's recommendation serving (high volume, low latency), OpenAI's API infrastructure (LLM-focused), Google's search ranking (ensemble models), and TikTok's multimodal recommendation (video understanding combined with user modeling).\n\nEach case study demonstrates how the principles of batching, sharding, load balancing, and autoscaling combine to meet specific requirements.\n\n### Meta Recommendation Serving {#sec-inference-case-meta}\n\nMeta's recommendation infrastructure serves predictions for feeds, ads, and content ranking across Facebook, Instagram, WhatsApp, and Messenger. This represents one of the largest production inference deployments in the world.\n\n**Scale and requirements**:\n\n- Request volume: Billions of requests per day\n- Latency target: <10ms P99\n- Model diversity: Hundreds of model variants\n- Feature cardinality: Trillions of unique entities\n\n**Architecture overview**:\n\n```\nUser request → Feature collection → Embedding lookup → Model inference → Response\n\n                    │                     │                  │\n                    ▼                     ▼                  ▼\n             Feature Store        Embedding Servers      GPU Inference\n             (CPU, DRAM)         (CPU + SSD, 1000s)     (GPU, 100s)\n```\n\n**Key design decisions**:\n\n**Embedding sharding at scale**: Embedding tables total over 100TB, requiring 1000+ shards. Meta uses a hybrid sharding strategy:\n\n- Hot embeddings (top 1%): Replicated across memory on all inference servers\n- Warm embeddings (next 10%): Column-sharded with 8-way parallelism\n- Cold embeddings (remaining 89%): Row-sharded with consistent hashing, SSD-backed\n\nThis reduces embedding lookup latency from 50ms (naive) to 2ms through batching and locality optimization.\n\n**Feature-parallel batching**: Instead of batching entire requests, Meta batches at the feature level. Each inference request triggers 5,000+ embedding lookups, but these lookups are batched across requests within a 1ms window. This achieves 90%+ memory bandwidth utilization on embedding servers.\n\n**GPU-CPU hybrid architecture**: Dense model computation (ranking towers) runs on GPUs, while sparse embedding lookups run on CPU servers with large memory and SSD storage. This matches hardware to workload characteristics:\n\n| Component | Hardware | Latency | Throughput |\n|-----------|----------|---------|------------|\n| Embedding lookup | CPU + SSD | 2ms | 50M lookups/s |\n| Feature processing | CPU | 1ms | 10M ops/s |\n| Dense ranking | GPU | 1.5ms | 100K infs/s |\n\n**Lessons learned**:\n\n1. Embedding lookup, not model inference, often dominates latency for recommendation systems\n2. Feature-parallel batching achieves higher efficiency than request-level batching\n3. Hybrid CPU-GPU architectures match hardware to workload characteristics\n\n### OpenAI API Infrastructure {#sec-inference-case-openai}\n\nOpenAI's API serves GPT-4, GPT-3.5-turbo, and other models to millions of developers. The infrastructure must handle highly variable request sizes (from 10 tokens to 128K tokens) while maintaining quality of service across diverse workloads.\n\n**Scale and requirements**:\n\n- Request volume: Millions of requests per hour\n- Latency target: Time-to-first-token (TTFT) <2s, throughput varies by model\n- Model sizes: 7B to 175B+ parameters\n- Context lengths: Up to 128K tokens\n\n**Architecture overview**:\n\n```\nAPI Gateway → Rate Limiting → Request Router → Model Cluster → Response Streaming\n\n                                     │\n                                     ▼\n                              ┌─────────────┐\n                              │ Model Pool  │\n                              │ ┌─────────┐ │\n                              │ │ GPT-4   │ │\n                              │ │ 8xH100  │ │\n                              │ └─────────┘ │\n                              │ ┌─────────┐ │\n                              │ │GPT-3.5  │ │\n                              │ │ 4xA100  │ │\n                              │ └─────────┘ │\n                              └─────────────┘\n```\n\n**Key design decisions**:\n\n**Continuous batching with chunked prefill**: OpenAI was an early adopter of continuous batching (Orca-style) to maintain high GPU utilization despite variable output lengths. Chunked prefill bounds decode latency by processing long prompts in chunks that interleave with ongoing generation.\n\n| Batching Strategy | GPU Utilization | TTFT (128K prompt) |\n|------------------|-----------------|-------------------|\n| Static batching | 45% | 30s (blocked) |\n| Continuous batching | 75% | 30s (blocked) |\n| Continuous + chunked | 85% | 3s (streamed) |\n\n**Tensor parallelism for large models**: GPT-4 class models require 8-way or greater tensor parallelism for memory capacity and latency:\n\n- 8xH100 per GPT-4 shard group\n- NVLink for intra-node communication\n- Consistent hashing for session affinity (KV cache reuse)\n\n**Multi-tier rate limiting**: OpenAI implements rate limiting at multiple levels to prevent noisy neighbors:\n\n- Per-API-key request rate limits\n- Per-API-key token-per-minute limits\n- Organization-level capacity quotas\n- Global model capacity limits\n\n**Dynamic capacity allocation**: During peak demand, OpenAI shifts capacity between models based on queue depth:\n\n```\nif gpt4_queue_depth > threshold:\n    # Migrate some GPT-3.5 capacity to GPT-4\n    reallocate_cluster_capacity(from=\"gpt-3.5\", to=\"gpt-4\", fraction=0.2)\n```\n\n**Lessons learned**:\n\n1. Continuous batching is essential for LLM serving at scale\n2. Prefix caching provides 2-3x efficiency for conversational workloads\n3. Multi-tier rate limiting prevents cascade failures from traffic spikes\n\n### Google Search Ranking {#sec-inference-case-google}\n\nGoogle Search uses ensemble serving to combine multiple specialized models for query understanding, document relevance, and result ranking. This represents a different inference pattern: many smaller models coordinated for each request rather than one large model.\n\n**Scale and requirements**:\n\n- Request volume: Billions of searches per day\n- Latency target: <200ms end-to-end\n- Model count: Dozens of models per query\n- Result processing: Thousands of documents per query\n\n**Architecture overview**:\n\n```\nQuery → Query Understanding → Candidate Retrieval → Ranking Cascade → Results\n\n              │                        │                    │\n              ▼                        ▼                    ▼\n         BERT QU                  Embeddings            L1 → L2 → L3\n        (10 models)              (100s shards)       (progressively complex)\n```\n\n**Key design decisions**:\n\n**Cascading model architecture**: Rather than running one expensive model on all candidates, Google uses a ranking cascade:\n\n| Stage | Model Complexity | Candidates | Latency Budget |\n|-------|-----------------|------------|----------------|\n| L0 (Retrieval) | Embedding lookup | 1,000,000 → 10,000 | 10ms |\n| L1 (First pass) | Linear model | 10,000 → 1,000 | 20ms |\n| L2 (Second pass) | Small transformer | 1,000 → 100 | 50ms |\n| L3 (Final rank) | Large ensemble | 100 → 10 | 100ms |\n\nThis achieves 100x cost reduction compared to running L3 on all candidates.\n\n**Speculative execution**: Given tight latency budgets, Google uses speculative execution for model ensembles:\n\n```\n# Instead of sequential:\n#   q1 = model1(query)\n#   q2 = model2(query)\n#   q3 = model3(query, q1, q2)\n\n# Speculative parallel:\nasync_q1 = async model1(query)\nasync_q2 = async model2(query)\nasync_q3 = async model3(query, predicted_q1, predicted_q2)\n\n# Use actual results if they arrive in time, otherwise use speculative\n```\n\n**Custom TPU infrastructure**: Google runs ranking models on TPUs optimized for transformer inference. TPU pods provide:\n\n- 2D mesh topology for efficient AllReduce\n- High memory bandwidth for attention operations\n- Custom quantization for serving efficiency\n\n**Deadline-aware scheduling**: Each sub-request carries a deadline, and workers prioritize by deadline proximity:\n\n```\nWorker queue: [Doc1: 50ms left] [Doc2: 30ms left] [Doc3: 80ms left]\n                                      ↑ Process first\n\nIf deadline will be missed:\n  Return cached/default result rather than timing out\n```\n\n**Lessons learned**:\n\n1. Ranking cascades provide dramatic cost reduction for large candidate sets\n2. Deadline propagation and priority scheduling are essential for ensemble serving\n3. Custom hardware (TPU) enables efficiency that commodity GPUs cannot match\n\n### TikTok Multimodal Recommendation {#sec-inference-case-tiktok}\n\nTikTok's recommendation system combines video understanding (vision) with user modeling (recommendation) for personalized content ranking. This represents a multimodal inference challenge where different model types must coordinate.\n\n**Scale and requirements**:\n\n- Request volume: Millions of video rankings per second\n- Latency target: <50ms P99\n- Content volume: Millions of new videos daily\n- Modalities: Video, audio, text, user signals\n\n**Architecture overview**:\n\n```\nUser request → User embedding → Candidate videos → Video understanding → Ranking\n\n                    │                  │                    │\n                    ▼                  ▼                    ▼\n             User Tower          Video Cache          Vision Models\n           (Transformer)        (Pre-computed)        (On-demand)\n```\n\n**Key design decisions**:\n\n**Two-tower architecture with caching**: TikTok separates user understanding (online) from content understanding (offline):\n\n| Tower | Update Frequency | Latency | Compute |\n|-------|-----------------|---------|---------|\n| User tower | Real-time | 5ms | GPU (online) |\n| Video tower | Hourly | N/A | GPU (batch) |\n\nVideo embeddings are pre-computed and cached, eliminating vision inference from the critical path for most requests. Only new videos (uploaded within the hour) require online vision inference.\n\n**Hybrid CPU-GPU inference**: Like Meta, TikTok uses CPU for embedding operations and GPU for dense model computation:\n\n```\nUser features → CPU preprocessing (1ms)\n             → Embedding lookup (2ms, CPU+DRAM)\n             → Dense ranking (10ms, GPU)\n             → Response formatting (1ms)\n```\n\n**Priority-based video analysis**: New video content is processed with different priorities:\n\n| Priority | SLA | Use Case |\n|----------|-----|----------|\n| Critical | 5 min | Creator with large following |\n| Standard | 30 min | Normal uploads |\n| Background | 2 hours | Bulk/imported content |\n\nThis ensures popular creators' content reaches recommendations quickly while managing compute costs.\n\n**Multimodal fusion**: TikTok combines multiple understanding modalities through late fusion:\n\n```\nVideo embedding (512d) ─┐\nAudio embedding (256d) ─┼─ Concat → Fusion MLP → Final embedding (256d)\nText embedding (256d)  ─┘\n```\n\nThis allows independent updates to each modality's model without retraining the full system.\n\n**Lessons learned**:\n\n1. Separating online and offline components enables aggressive caching\n2. Two-tower architectures scale better than joint models for user-item systems\n3. Priority-based processing balances freshness against compute cost\n\n### Cross-Cutting Observations {#sec-inference-case-observations}\n\nSeveral patterns emerge across these case studies:\n\n**Separation of concerns**: All systems separate embedding/retrieval from ranking/generation. This enables specialized optimization for each component.\n\n**Hybrid architectures**: No system uses GPUs exclusively. CPU+GPU combinations match hardware to workload characteristics.\n\n**Caching at multiple levels**: Embedding caching, result caching, and intermediate representation caching all appear. Caching reduces compute at the cost of staleness.\n\n**Progressive refinement**: Cascades and early-exit strategies reduce average compute by quickly filtering unlikely candidates.\n\n**Deadline awareness**: All systems propagate deadlines and make explicit tradeoffs between quality and latency when under pressure.\n\n| System | Primary Technique | Key Innovation |\n|--------|------------------|----------------|\n| Meta | Embedding sharding | Feature-parallel batching |\n| OpenAI | Continuous batching | Chunked prefill |\n| Google | Ranking cascade | Speculative execution |\n| TikTok | Two-tower caching | Multimodal fusion |\n\n: **Case Study Summary**: Each system innovates on a core technique matched to its workload characteristics. {#tbl-case-studies-summary}\n\n## Fallacies and Pitfalls {#sec-inference-fallacies-pitfalls}\n\nThe techniques presented throughout this chapter address real engineering challenges, but misconceptions about inference at scale remain common. Recognizing these fallacies and pitfalls helps practitioners avoid costly mistakes in system design and capacity planning.\n\n**Fallacy: Inference at scale is synonymous with LLM serving.**\n\nThis misconception, reinforced by current discourse, leads to over-focus on LLM-specific techniques while ignoring the broader inference landscape. By request volume, recommendation systems constitute 80-90% of production inference at major technology companies, with vision and other models comprising most of the remainder. LLMs currently represent 1-5% of requests, though this is growing. A practitioner who only understands continuous batching and KV cache management will be unprepared for the feature-parallel batching and embedding sharding that dominate production inference. Technique selection must match the actual workload.\n\n**Pitfall: Using training infrastructure for production serving.**\n\nTraining and serving have fundamentally different requirements. Training optimizes for aggregate throughput over hours or days; serving optimizes for per-request latency under strict SLOs. Training tolerates batch sizes of thousands; serving often requires batch sizes in single digits. Training accepts checkpoint-based recovery; serving requires graceful failover without user impact. Teams that deploy training clusters for serving often discover unacceptable latency variance, poor resource utilization, and difficulty meeting SLOs. Purpose-built serving infrastructure with appropriate batching, load balancing, and autoscaling is essential.\n\n**Fallacy: Continuous batching solves all LLM serving problems.**\n\nContinuous batching dramatically improves GPU utilization for LLM serving, but it addresses only one dimension of the problem. Prefill remains a bottleneck for long contexts, as the quadratic attention computation cannot be avoided regardless of how subsequent decode iterations are batched. KV cache memory, not compute, often limits batch size. Network bandwidth between sharded model components can dominate latency for large models. Continuous batching is necessary but not sufficient for efficient LLM serving.\n\n**Pitfall: Sizing capacity based on average throughput.**\n\nThe nonlinear relationship between utilization and latency (from queuing theory) means that systems provisioned for average load will violate SLOs during traffic peaks. At 80% average utilization, a modest 25% traffic spike pushes utilization above 100%, causing unbounded queue growth and latency degradation. The cold start problem exacerbates this: by the time new capacity is available (5+ minutes for GPU instances), the spike may have caused significant SLO violations. Capacity planning must account for peak load plus headroom, not average load.\n\n**Fallacy: Load balancing does not matter much for inference.**\n\nSimple load balancing strategies like round-robin seem adequate until examined quantitatively. Random assignment produces maximum queue lengths of $O(\\log n / \\log \\log n)$ across $n$ servers. Power-of-two-choices reduces this to $O(\\log \\log n)$, an exponential improvement. For a 1,000-server cluster, this translates from ~4-5 requests maximum queue to ~2 requests. At tail latencies that matter for SLOs, this difference is substantial. For LLM workloads with highly variable request durations, least-connections further improves balance. The choice of load balancing algorithm has first-order impact on system performance.\n\n**Pitfall: Ignoring the serving tax in latency budgets.**\n\nDistributed inference introduces overhead absent from single-machine serving: network round-trips, serialization, load balancer decisions, and coordination for sharded models. This \"serving tax\" often consumes 10-30% of the latency budget. A team that achieves 70ms model inference on a single GPU may be surprised when end-to-end latency reaches 100ms in production due to these overheads. Latency budgets must explicitly account for distribution overhead, not just compute time.\n\n**Fallacy: More GPU memory always means more batch size and throughput.**\n\nWhile larger GPU memory enables larger batches for models that fit in memory, the bottleneck often shifts before memory is exhausted. Memory bandwidth limits throughput for bandwidth-bound operations (LLM decode). Compute limits throughput for compute-bound operations (prefill, vision inference). Adding memory to a bandwidth-bound workload provides no benefit. Understanding whether the workload is compute-bound, memory-bound, or capacity-bound guides appropriate resource allocation.\n\n**Pitfall: Neglecting multi-tenancy isolation until production.**\n\nIn development and staging, single-tenant deployments work well. In production, noisy neighbors cause sudden, unpredictable performance degradation that is difficult to diagnose and resolve. A tenant bursting to 5x normal traffic can degrade latency for all other tenants on shared infrastructure. Resource quotas, priority scheduling, and bulkhead isolation must be designed into the system from the start, not retrofitted after production incidents.\n\n::: {.callout-important title=\"Three Things to Remember\"}\n\n1. **Serving cost dominates training cost over a model's lifetime.** For high-volume applications, serving cost exceeds training cost by 100x or more. Every percentage point of serving efficiency improvement yields ongoing cost reduction. Optimize serving ruthlessly.\n\n2. **Different model types require fundamentally different batching strategies.** Static batching for vision, continuous batching for LLMs, feature-parallel batching for recommendation systems. There is no universal optimal strategy. Match technique to workload.\n\n3. **Power-of-two-choices provides exponential load balancing improvement.** Maximum queue length improves from $O(\\log n / \\log \\log n)$ to $O(\\log \\log n)$ with minimal overhead (two probes per request). This simple technique should be standard for any distributed inference deployment.\n\n:::\n\n## Summary {#sec-inference-summary}\n\nInference at scale transforms the single-machine serving foundations from Volume I into distributed systems that handle billions of requests across global infrastructure. Throughout this chapter, we have developed the principles governing this transformation, from batching strategies matched to model architectures through load balancing algorithms that provide exponentially better performance.\n\nThe serving hierarchy provides our organizing framework: request-level optimizations (batching, caching), replica-level optimizations (GPU utilization, memory management), service-level optimizations (load balancing, routing), and platform-level optimizations (multi-tenancy, scheduling). Each level has distinct metrics and techniques, and effective inference systems optimize at all levels simultaneously.\n\nWe began by establishing when distributed inference becomes necessary: memory exhaustion for models exceeding single-GPU capacity, throughput requirements beyond single-machine limits, or latency targets that demand parallel computation. The fundamental inversion from training's throughput focus to serving's latency imperative shapes every subsequent design decision. We quantified the serving tax, the overhead of distribution that must be budgeted within latency constraints, and demonstrated that serving cost dominates training cost over a model's operational lifetime.\n\nBatching strategies vary dramatically across model types. Vision models benefit from large static or dynamic batches that maximize GPU utilization. LLMs require continuous batching to handle variable output lengths efficiently, with chunked prefill to bound decode latency during long prompt processing. Recommendation systems use feature-parallel batching that aligns with embedding shard architectures rather than request-level batching. Streaming applications for speech and video cannot tolerate batching delay at all. Selecting the wrong batching strategy for a workload can reduce throughput by 3-4x or violate latency SLOs entirely.\n\nModel sharding distributes computation across devices when single-GPU memory or latency constraints require it. Tensor parallelism achieves latency reduction through parallel attention and feed-forward computation, with all-reduce synchronization between layers. Pipeline parallelism distributes layers across stages for memory relief without latency benefit for single requests. Expert parallelism handles mixture-of-experts models with dynamic routing. Embedding sharding scales recommendation systems' trillion-parameter tables across thousands of servers. Each strategy has distinct communication patterns and overhead characteristics that must match deployment constraints.\n\nLoad balancing determines how requests reach replicas, with seemingly simple choices producing dramatically different performance. Random assignment yields maximum queue lengths of $O(\\log n / \\log \\log n)$, while power-of-two-choices achieves $O(\\log \\log n)$, an exponential improvement from a trivial modification. Consistent hashing enables session affinity for stateful workloads like LLM conversations. Circuit breakers and backpressure mechanisms protect systems from cascading failures when replicas become overloaded.\n\nKV cache management has emerged as a critical bottleneck for LLM serving. PagedAttention eliminates memory fragmentation through virtual memory techniques, achieving 2.5-4x throughput improvement. Prefix caching shares common prompt prefixes across requests, reducing both compute and memory for conversational workloads. Speculative decoding breaks the sequential generation bottleneck by using draft models to predict multiple tokens verified in parallel. These techniques combine to make previously impractical LLM deployments economically viable.\n\nMulti-tenancy enables cost-effective infrastructure sharing but requires careful isolation engineering. Noisy neighbors can degrade performance for all tenants without proper resource quotas, priority scheduling, and bulkhead isolation. The tradeoff between utilization and isolation must be explicitly managed based on service tier and SLO requirements.\n\nAutoscaling addresses traffic fluctuations but faces the cold start problem unique to GPU-based inference. Model loading times of minutes make reactive scaling insufficient for sudden traffic spikes. Predictive scaling, warm pools, and tiered readiness levels combine to provide rapid scaling response while managing cost. Spot instances offer significant savings for burst capacity when graceful handling of interruptions is implemented.\n\nThe case studies demonstrate how these principles combine in production: Meta's feature-parallel batching and embedding sharding for recommendation, OpenAI's continuous batching and tensor parallelism for LLMs, Google's ranking cascades and deadline-aware scheduling for search, and TikTok's two-tower caching for multimodal recommendation. Each system innovates on techniques matched to its specific workload characteristics, but all share common patterns of separation of concerns, hybrid architectures, and progressive refinement.\n\n::: {.callout-important title=\"Key Takeaways\"}\n\n* Serving cost dominates training cost by 100x or more for high-volume applications, making serving optimization the primary driver of ML infrastructure economics\n\n* Different model types require fundamentally different batching strategies: static/dynamic for vision, continuous for LLMs, feature-parallel for recommendation\n\n* The serving hierarchy (request, replica, service, platform) provides a framework for decomposing optimization opportunities at each level\n\n* Power-of-two-choices load balancing achieves exponentially better queue balance ($O(\\log \\log n)$ vs $O(\\log n / \\log \\log n)$) with minimal overhead\n\n* KV cache management through PagedAttention and prefix caching enables 2-4x LLM serving efficiency improvement\n\n* Cold start times of 5+ minutes for GPU-based serving require predictive scaling and warm pools, not just reactive autoscaling\n\n* Production inference is dominated by recommendation systems (80-90% of requests), not LLMs, requiring model-type diversity in technique selection\n\n:::\n\nThe techniques in this chapter enable inference systems that scale from single machines to global deployments. The next chapter, @sec-edge-intelligence, examines the other end of the deployment spectrum: inference at the edge, where devices with limited compute, memory, and power must still deliver predictions reliably. The distributed coordination patterns established here inform edge-cloud architectures that partition computation between constrained edge devices and capable cloud infrastructure.\n\n<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->\n::: { .quiz-end }\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"inference.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","inference.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3,"title":"Inference at Scale"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}