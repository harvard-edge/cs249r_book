{"title":"Large-Scale ML Infrastructure","markdown":{"yaml":{"title":"Large-Scale ML Infrastructure","bibliography":"infrastructure.bib"},"headingText":"<!--","containsRefs":false,"markdown":"\n\nEDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR INFRASTRUCTURE\n================================================================================\n\nCORE PRINCIPLE: Infrastructure requirements vary by workload type.\nTraining clusters differ from serving infrastructure. Different model\ntypes have different compute, memory, and networking needs.\n\nMODEL-SPECIFIC INFRASTRUCTURE CONSIDERATIONS:\n\n| Model Type      | Compute Profile     | Memory Profile      | Network Need        |\n|-----------------|---------------------|---------------------|---------------------|\n| LLMs            | GPU-heavy           | HBM-bound           | High (tensor par.)  |\n| Recommendation  | CPU+GPU hybrid      | DRAM for embeddings | Moderate            |\n| Vision          | GPU-heavy           | Moderate            | Moderate            |\n| Scientific      | Varies              | Often huge          | Problem-dependent   |\n\nREQUIRED COVERAGE FOR THIS CHAPTER:\n\nDATACENTER ARCHITECTURE:\n\n- GPU clusters: Dominant for training transformers, vision\n- CPU clusters: Feature serving, preprocessing (RecSys)\n- Hybrid: Recommendation training (embedding on CPU, dense on GPU)\n- Include: Why different workloads need different architectures\n\nACCELERATOR SELECTION:\n\n- GPU (NVIDIA): General-purpose ML, dominant for training\n- TPU: Large-scale training, specific model types\n- Custom ASICs: Inference optimization (recommendation, vision)\n- Include: Different accelerators suit different workloads\n\nNETWORKING:\n\n- InfiniBand: Training clusters, high-bandwidth collective ops\n- Ethernet: Serving infrastructure, feature stores\n- Include: Why training and serving have different network needs\n\nRESOURCE MANAGEMENT:\n\n- Batch scheduling: Training jobs (Slurm, Kubernetes)\n- Online serving: Request routing, autoscaling\n- Include: Different scheduling for different workload types\n\nCASE STUDIES TO INCLUDE:\n\n- NVIDIA DGX SuperPOD architecture\n- Google TPU pod infrastructure\n- Meta recommendation infrastructure (CPU+GPU hybrid)\n- Tesla Dojo for vision training\n\nQUANTITATIVE ANALYSIS:\n\n- TCO breakdown by workload type\n- Power/performance efficiency for different accelerators\n- Network utilization patterns by model type\n- Include: Same cluster, different efficiency for different models\n\nANTI-PATTERNS TO AVOID:\n\n- Assuming all ML infrastructure is GPU clusters\n- Ignoring CPU infrastructure for recommendation\n- One-size-fits-all datacenter design\n- Only discussing training infrastructure (serving matters too)\n\n================================================================================\n-->\n\n# Large-Scale ML Infrastructure {#sec-infrastructure}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A sweeping architectural visualization of a modern AI datacenter infrastructure. The scene reveals a massive facility with rows of GPU clusters arranged in pods, connected by high-bandwidth networking fabric depicted as glowing fiber optic pathways. Cooling systems appear as flowing blue currents between server racks. The visualization includes multiple layers: physical infrastructure at the bottom with power and cooling, compute infrastructure in the middle with thousands of interconnected accelerators, and orchestration software at the top represented as an abstract control plane. Visual elements include resource managers allocating workloads, capacity graphs showing utilization, and geographic connections to other datacenters. The color scheme uses industrial grays and silvers with accent colors of electric blue for networking and amber for active computation. Photorealistic technical illustration style suitable for infrastructure engineering documentation._\n:::\n\n\\noindent\n![](images/png/cover_infrastructure.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why does the ability to build and manage computational infrastructure determine which organizations can realize their most ambitious machine learning goals?_\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\nAfter completing this chapter, you will be able to:\n\n- Calculate power and cooling requirements for GPU cluster deployments using PUE and thermal dissipation constraints\n- Compare network topology trade-offs (fat-tree, rail-optimized, torus) using bisection bandwidth and hop count metrics\n- Analyze total cost of ownership for infrastructure decisions by computing amortized CapEx plus OpEx\n- Design resource management policies that achieve target GPU utilization (70%+) in multi-tenant clusters\n- Evaluate accelerator options (GPU, TPU, custom ASIC) by matching workload characteristics to hardware capabilities\n- Apply the roofline model to determine whether workloads are compute-bound or memory-bound\n\n:::\n\nMachine learning systems that transform industries operate on infrastructure far exceeding the scale of single machines or small clusters. Training a frontier model may require thousands of GPUs coordinated across multiple datacenters, each machine contributing to a unified computation that can span weeks or months. Managing such infrastructure demands expertise in datacenter design, high-bandwidth networking, and distributed systems orchestration. The infrastructure must satisfy competing requirements: computational efficiency alongside fault tolerance, coordination across thousands of machines without prohibitive communication overhead, and dynamic capacity provisioning that controls costs. These challenges have become central to machine learning advancement, as organizations cannot access frontier capabilities without mastering the physical and software systems that make large-scale computation possible. Understanding infrastructure architecture is essential for building systems beyond prototype experiments, shaping whether organizations deploy efficiently, scale reliably, and compete effectively in an increasingly infrastructure-dependent landscape.\n\n## Datacenter Architecture for ML Workloads\n\nThe transition from single-machine ML systems to distributed training fundamentally changes the infrastructure requirements. Where Volume I focused on optimizing computations within a single node, production ML at scale demands purpose-built datacenters designed around the unique characteristics of ML workloads: massive power consumption, extreme heat density, and communication patterns that differ markedly from traditional cloud computing.\n\nThis section examines the physical and compute infrastructure that enables large-scale ML, providing the foundation for understanding how distributed training systems leverage these resources.\n\n### Physical Infrastructure Fundamentals\n\nML datacenters differ from traditional cloud facilities in three critical dimensions: power density per rack is 5-10x higher, cooling requirements demand liquid rather than air-based solutions, and physical layout must optimize for high-bandwidth interconnects rather than flexible networking.\n\n#### Power Delivery and Distribution\n\nA single NVIDIA DGX H100 system consumes 10.2 kW at peak load. A rack containing four such systems requires over 40 kW, compared to 5-10 kW for traditional server racks. This density fundamentally changes power infrastructure design.\n\n**Utility and Backup Power.** Production ML facilities require redundant power feeds, typically N+1 or 2N configurations where N represents the load requirement. Uninterruptible power supplies (UPS) bridge the gap during utility failures, but the massive power draw of GPU clusters limits battery backup duration to minutes rather than hours. Diesel generators provide extended backup, with automatic transfer switches completing failover within 10-15 seconds.\n\n**Power Distribution Architecture.** Modern ML datacenters use a tiered distribution model:\n\n+------------------------+------------------+--------------------------------+\n| Distribution Level     | Typical Voltage  | Purpose                        |\n+========================+==================+================================+\n| Utility feed           | 13.8-69 kV       | Grid connection                |\n+------------------------+------------------+--------------------------------+\n| Substation transformer | 480V (US)        | Building distribution          |\n+------------------------+------------------+--------------------------------+\n| PDU (Power Distribution| 208V             | Rack-level distribution        |\n| Unit)                  |                  |                                |\n+------------------------+------------------+--------------------------------+\n| Server PSU             | 12V DC           | Component-level power          |\n+------------------------+------------------+--------------------------------+\n\n**Power Usage Effectiveness.** The PUE metric quantifies datacenter energy efficiency:\n\n$$\n\\text{PUE} = \\frac{\\text{Total Facility Power}}{\\text{IT Equipment Power}}\n$$\n\nA PUE of 1.0 represents perfect efficiency where all power goes to computing. Traditional datacenters achieve PUE values of 1.5-2.0, while hyperscale facilities target 1.1-1.2. ML datacenters face a challenge: the extreme heat density of GPU clusters increases cooling overhead, pushing PUE higher unless advanced cooling technologies are deployed.\n\n#### Cooling Systems at Scale\n\nHeat dissipation represents the primary constraint on ML cluster density. An H100 GPU generates 700W of thermal output from a surface area smaller than a dinner plate, producing heat flux comparable to a nuclear reactor's fuel rod surface.\n\n**Air Cooling Limitations.** Traditional air cooling becomes impractical above 30-40 kW per rack. The physics are straightforward: air's low heat capacity (approximately 1 kJ/kg-K) requires massive airflow to remove heat. A 40 kW rack requires roughly 10,000 CFM (cubic feet per minute) of airflow, creating acoustic levels exceeding 80 dB and significant fan power overhead.\n\n**Hot Aisle/Cold Aisle Containment.** This architectural pattern separates cold supply air from hot exhaust air using physical barriers. Cold air enters through raised floor vents or overhead ducts, passes through servers front-to-back, and exhausts into a contained hot aisle. Containment improves cooling efficiency by preventing mixing, but cannot solve the fundamental heat density challenge of modern GPU clusters.\n\n**Direct-to-Chip Liquid Cooling.** Liquid cooling addresses heat density through water's superior heat capacity (4.2 kJ/kg-K, roughly four times air). Cold plates mounted directly on GPUs and CPUs transfer heat to circulating coolant, which flows to facility-level heat exchangers. This approach enables rack densities of 100+ kW while reducing cooling power consumption by 30-40% compared to air cooling.\n\n::: {.callout-note}\n## Liquid Cooling Adoption\n\nAs of 2024, liquid cooling has transitioned from specialty option to requirement for large-scale ML clusters. NVIDIA's GB200 NVL72 systems require liquid cooling, with no air-cooled option available. Facilities planning for next-generation hardware must include liquid cooling infrastructure from initial design.\n:::\n\n**Immersion Cooling.** The most aggressive thermal solution submerges entire servers in dielectric fluid. Single-phase immersion uses non-conductive oils that remain liquid, while two-phase systems use fluids that boil at low temperatures, leveraging latent heat of vaporization for efficient heat transfer. Immersion enables rack densities exceeding 200 kW but requires specialized maintenance procedures and component compatibility.\n\n#### Physical Layout Optimization\n\nML cluster performance depends critically on physical topology. Unlike web serving workloads where any server can handle any request, distributed training requires specific communication patterns between specific nodes.\n\n**Rack Density Considerations.** Higher density reduces cable lengths and switch hops but concentrates power and cooling requirements. Production deployments balance these factors based on workload characteristics:\n\n+----------------------+------------------+---------------------------+\n| Workload Type        | Typical Density  | Limiting Factor           |\n+======================+==================+===========================+\n| LLM training         | 80-120 kW/rack   | Cooling capacity          |\n+----------------------+------------------+---------------------------+\n| Recommendation       | 30-50 kW/rack    | CPU/memory balance        |\n| inference            |                  |                           |\n+----------------------+------------------+---------------------------+\n| Vision training      | 60-80 kW/rack    | Network bandwidth         |\n+----------------------+------------------+---------------------------+\n\n**Cable Management.** High-bandwidth interconnects like InfiniBand use copper cables for distances under 3 meters and fiber optics beyond. Cable routing must maintain bend radius requirements (typically 10x cable diameter) while enabling airflow for any air-cooled components. Active optical cables (AOCs) simplify routing but add latency and power consumption compared to passive copper.\n\n### Compute Infrastructure Design\n\nML clusters combine multiple node types, each optimized for different phases of the training and inference pipeline. Understanding these roles clarifies infrastructure design decisions.\n\n#### GPU Cluster Architectures\n\nModern GPU clusters are built from dense multi-GPU nodes connected via high-bandwidth fabrics. Two reference architectures dominate production deployments.\n\n**DGX-Style Dense Nodes.** NVIDIA's DGX systems package 8 GPUs with NVLink interconnects, high-bandwidth networking, and substantial local storage in a single chassis. The DGX H100 provides:\n\n- 8x H100 GPUs with 640GB total HBM3 memory\n- NVSwitch fabric enabling 900 GB/s GPU-to-GPU bandwidth\n- 8x 400 Gbps InfiniBand or Ethernet ports\n- 2x Intel Xeon CPUs for preprocessing\n- 30TB NVMe storage for dataset staging\n\nThis integrated design simplifies deployment but limits flexibility. Each DGX H100 costs approximately $300,000 (pricing reflects 2024 market conditions and fluctuates significantly based on supply, demand, and generation transitions), making component-level upgrades economically impractical.\n\n**HGX Baseboard Designs.** For organizations building custom infrastructure, NVIDIA's HGX baseboards provide the GPU and interconnect components for integration into custom server designs. Cloud providers and large enterprises use HGX to optimize for their specific power, cooling, and networking requirements while maintaining compatibility with NVIDIA's software stack.\n\n**PCIe vs. NVLink Configurations.** The choice between PCIe and NVLink connectivity involves fundamental trade-offs:\n\n+------------------+------------------+------------------+------------------------+\n| Interconnect     | Bandwidth        | Latency          | Use Case               |\n+==================+==================+==================+========================+\n| PCIe Gen5 x16    | 64 GB/s          | ~1 microsecond   | Inference, small       |\n|                  |                  |                  | models                 |\n+------------------+------------------+------------------+------------------------+\n| NVLink 4.0       | 900 GB/s         | ~0.5 microsecond | Large model training   |\n| (bidirectional)  |                  |                  |                        |\n+------------------+------------------+------------------+------------------------+\n\nFor models requiring tensor parallelism across GPUs (as detailed in @sec-distributed-training), NVLink's 14x bandwidth advantage directly translates to training throughput. PCIe-based systems suffice for data-parallel workloads where gradient synchronization occurs less frequently.\n\n#### CPU Infrastructure Roles\n\nWhile GPUs dominate ML computation, CPUs perform essential supporting functions that bottleneck overall system performance if under-provisioned.\n\n**Preprocessing and Data Preparation.** Training data pipelines involve decompression, augmentation, tokenization, and batching. These operations execute on CPUs, which must supply data fast enough to keep GPUs utilized. A common rule of thumb allocates 4-8 CPU cores per GPU for training workloads, though data-intensive pipelines (video, large images) may require more.\n\n**Feature Serving for Recommendation Systems.** Recommendation models present a distinct infrastructure pattern. These systems combine deep learning components with massive embedding tables that may exceed 1TB. The embedding lookups are memory-bound CPU operations, while neural network components benefit from GPU acceleration. Production recommendation systems often use CPU-heavy nodes for embedding serving alongside GPU nodes for model computation, connected via low-latency networks.\n\n**Control Plane and Orchestration.** Cluster management, job scheduling, and monitoring run on dedicated CPU nodes separate from the training cluster. This isolation prevents resource contention and enables management operations even when the training cluster is fully utilized.\n\n#### Hybrid Architectures\n\nReal production systems rarely use homogeneous hardware throughout. Workload-aware placement matches job characteristics to appropriate resources.\n\n**Embedding Table Placement.** For recommendation systems with embedding tables exceeding GPU memory, hybrid architectures place embeddings in CPU DRAM while compute-intensive layers execute on GPUs. Facebook's DLRM architecture pioneered this pattern, with embeddings distributed across CPU nodes communicating with GPU nodes via high-bandwidth networks.\n\n**Heterogeneous Scheduling.** Modern orchestration systems support mixed node types within a single cluster. Kubernetes with GPU support and Slurm with GRES (Generic Resource Scheduling) enable jobs to request specific hardware combinations. A training job might request 64 GPU nodes for model computation plus 16 high-memory CPU nodes for embedding tables, scheduled as a coordinated allocation.\n\n### Accelerator Selection by Workload Type\n\nThe accelerator landscape has expanded beyond NVIDIA GPUs to include Google TPUs, custom ASICs, and emerging architectures. Selection requires matching accelerator characteristics to workload requirements.\n\n#### NVIDIA GPU Ecosystem\n\nNVIDIA maintains market dominance through integrated hardware-software offerings. Understanding the architecture evolution clarifies capability differences.\n\n**Architecture Progression.** Each generation brings substantial improvements in compute density and memory bandwidth:\n\n+-------------+-------------+----------------+------------------+---------------+\n| GPU         | FP16        | HBM            | Memory           | TDP           |\n|             | Tensor      | Capacity       | Bandwidth        |               |\n|             | TFLOPS      |                |                  |               |\n+=============+=============+================+==================+===============+\n| A100        | 312         | 80 GB HBM2e    | 2.0 TB/s         | 400W          |\n+-------------+-------------+----------------+------------------+---------------+\n| H100        | 990         | 80 GB HBM3     | 3.4 TB/s         | 700W          |\n+-------------+-------------+----------------+------------------+---------------+\n| B100*       | ~1,800      | 192 GB HBM3e   | 8.0 TB/s         | ~700W         |\n+-------------+-------------+----------------+------------------+---------------+\n\n*B100 specifications are preliminary estimates based on NVIDIA announcements. Verify against official specifications for production planning.\n\nThe H100 delivers approximately 3x the tensor TFLOPS of A100 at 1.75x the power. To derive the efficiency improvement: A100 achieves 312 TF / 400W = 0.78 TF/W, while H100 achieves 990 TF / 700W = 1.41 TF/W, yielding approximately 80% improvement in FLOPS/watt. Memory bandwidth increases proportionally, maintaining the compute-to-memory ratio critical for transformer models.\n\n**Tensor Core Utilization.** Tensor Cores accelerate matrix operations but require specific data layouts and sizes for full utilization. Dimensions should be multiples of 8 (FP16) or 16 (INT8) for optimal performance. Underutilized Tensor Cores represent the most common source of poor GPU efficiency in production, with many workloads achieving only 30-50% of theoretical peak FLOPS.\n\n**NVLink Topology.** Within a node, NVSwitch provides full-bandwidth connectivity between all GPUs. Across nodes, NVLink Network (available in H100 and later) extends high-bandwidth connectivity, though at reduced bandwidth compared to intra-node links. Topology-aware job placement, discussed in @sec-communication, is essential for multi-node training performance.\n\n#### Google TPU Infrastructure\n\nGoogle's Tensor Processing Units offer an alternative architecture optimized for matrix operations with a distinct programming model.\n\n**TPU Pod Architecture.** TPUs connect via proprietary Inter-Chip Interconnect (ICI) forming 2D or 3D torus topologies. A TPU v4 pod contains 4,096 chips with 1.1 exaFLOPS of aggregate compute. Unlike GPU clusters where networking is separate from compute nodes, TPU pods integrate interconnect into the chip design.\n\n**TPU Slices and Multislice.** Users allocate TPU slices, contiguous subsets of a pod. Multislice training connects multiple slices via datacenter network for jobs exceeding single-slice capacity. The programming model (JAX with pjit) abstracts the physical topology, enabling code portability across slice sizes.\n\n**TPU vs. GPU Trade-offs.** TPUs excel for large-scale training with regular computation patterns:\n\n+------------------------+---------------------------+---------------------------+\n| Factor                 | TPU Advantage             | GPU Advantage             |\n+========================+===========================+===========================+\n| Large transformer      | Optimized matrix units,   | Broader operator support  |\n| training               | integrated interconnect   |                           |\n+------------------------+---------------------------+---------------------------+\n| Custom operations      | Limited flexibility       | CUDA extensibility        |\n+------------------------+---------------------------+---------------------------+\n| Software ecosystem     | JAX-centric               | PyTorch, TensorFlow,      |\n|                        |                           | many frameworks           |\n+------------------------+---------------------------+---------------------------+\n| Availability           | Google Cloud only         | Multiple cloud and        |\n|                        |                           | on-premise options        |\n+------------------------+---------------------------+---------------------------+\n\n#### Custom ASICs and Specialized Accelerators\n\nThe ML accelerator landscape continues to diversify as organizations optimize for specific workloads.\n\n**Inference-Optimized Accelerators.** Training and inference present different requirements. Training needs high-precision arithmetic, large memory for activations and optimizer state, and high interconnect bandwidth. Inference prioritizes low latency, high throughput, and power efficiency. Accelerators like Google's TPU Inference chips and AWS Inferentia optimize for inference characteristics, achieving 2-4x better performance per watt than training-focused hardware for appropriate workloads.\n\n**Emerging Architectures.** Several companies offer alternative approaches:\n\n- **Cerebras WSE**: Wafer-scale integration places an entire ML accelerator on a single silicon wafer, eliminating chip-to-chip communication for models that fit on-chip\n- **Graphcore IPU**: Bulk Synchronous Parallel (BSP) execution model with distributed on-chip memory targeting sparse and dynamic workloads\n- **SambaNova**: Reconfigurable dataflow architecture for enterprise AI applications\n\nThese alternatives find niches where their architectural trade-offs align with workload requirements, though NVIDIA and Google maintain dominant market positions for general ML training.\n\n### Quantitative Infrastructure Analysis\n\nEffective infrastructure decisions require quantitative comparison across accelerator options and workload types.\n\n**FLOPS per Watt Comparison.** Energy efficiency varies significantly across accelerator types and precision levels:\n\n+---------------------+------------------+------------------+------------------+\n| Accelerator         | FP16 TFLOPS      | TDP (Watts)      | TFLOPS/Watt      |\n+=====================+==================+==================+==================+\n| NVIDIA H100 SXM     | 990              | 700              | 1.41             |\n+---------------------+------------------+------------------+------------------+\n| NVIDIA H100 PCIe    | 756              | 350              | 2.16             |\n+---------------------+------------------+------------------+------------------+\n| Google TPU v5p      | 459              | 250-400*         | 1.1-1.8          |\n+---------------------+------------------+------------------+------------------+\n| AWS Trainium        | 210              | 150 (estimated)  | 1.40             |\n+---------------------+------------------+------------------+------------------+\n\n*TPU power varies significantly by deployment configuration and is not officially published. Direct TFLOPS/Watt comparisons across architectures are problematic because utilization profiles differ. These figures should be treated as approximate.\n\nThe PCIe variant's higher efficiency reflects reduced interconnect power, acceptable for inference but limiting for distributed training.\n\n**Memory Bandwidth Utilization.** Different model types exhibit distinct memory access patterns:\n\n- **LLM training**: Memory-bound for attention computation, achieving 70-85% bandwidth utilization\n- **CNN training**: Compute-bound for convolutions, 30-50% bandwidth utilization\n- **Recommendation inference**: Memory-bound for embeddings, often exceeding available bandwidth\n\nUnderstanding these patterns guides accelerator selection: memory-bound workloads benefit from HBM3's bandwidth improvements, while compute-bound workloads prioritize FLOPS per dollar.\n\n**The Roofline Model.** The roofline model provides a systematic framework for understanding whether workloads are compute-bound or memory-bound. Achievable performance is limited by the minimum of peak compute and memory bandwidth:\n\n$$\n\\text{Achievable FLOPS} = \\min\\left(\\text{Peak Compute}, \\text{Memory Bandwidth} \\times \\text{Arithmetic Intensity}\\right)\n$$\n\nArithmetic intensity measures FLOPS per byte of memory traffic. The \"ridge point\" where compute and memory limits intersect determines which workloads benefit from each resource:\n\n+------------------+------------------+------------------+------------------+\n| Accelerator      | Peak Compute     | Memory BW        | Ridge Point      |\n|                  | (TF FP16)        | (TB/s)           | (FLOP/byte)      |\n+==================+==================+==================+==================+\n| H100 SXM         | 990              | 3.4              | 291              |\n+------------------+------------------+------------------+------------------+\n| A100 80GB        | 312              | 2.0              | 156              |\n+------------------+------------------+------------------+------------------+\n| TPU v4           | 275              | 1.2              | 229              |\n+------------------+------------------+------------------+------------------+\n\nMost LLM training operates at 50-100 FLOP/byte arithmetic intensity, well below the ridge point, making these workloads memory-bound. At 75 FLOP/byte on H100, achievable performance is $3.4 \\times 75 = 255$ TF, only 26% of peak compute. This explains why production training achieves 30-50% of theoretical FLOPS: the bottleneck is memory bandwidth, not compute capacity.\n\nCNN training with large batch sizes operates near 200 FLOP/byte, approaching the ridge point where both resources limit performance. Recommendation inference with random embedding lookups operates at extremely low arithmetic intensity (1-10 FLOP/byte), fundamentally memory-bound regardless of accelerator choice.\n\n**Cost per PFLOP.** Infrastructure economics depend on utilization and workload fit:\n\n$$\n\\text{Effective Cost per PFLOP} = \\frac{\\text{Hardware Cost} + \\text{3-year OpEx}}{\\text{Peak PFLOPS} \\times \\text{Average Utilization} \\times 3 \\text{ years}}\n$$\n\nFor a DGX H100 at $300,000 with $50,000 annual power and cooling costs, achieving 50% average utilization yields an effective cost of approximately $0.35 per PFLOP-hour. Cloud instances at $30 per hour for equivalent hardware cost $0.15 per PFLOP-hour at 100% utilization, but on-premise becomes favorable above 40% sustained utilization over three years.\n\n::: {.callout-warning}\n## Utilization Reality\n\nQuoted peak FLOPS numbers assume perfect utilization. Production training jobs typically achieve 30-50% of peak due to communication overhead, data pipeline stalls, and suboptimal kernel efficiency. Infrastructure planning must account for realistic utilization rates rather than theoretical peaks.\n:::\n\nThis infrastructure foundation enables the distributed training strategies explored in @sec-distributed-training and the communication patterns detailed in @sec-communication. The physical constraints examined here, particularly power delivery, cooling capacity, and interconnect topology, ultimately determine what scale of training is achievable.\n\n## Networking for Large-Scale ML\n\nThe networking fabric connecting accelerators determines whether a distributed training job achieves near-linear scaling or collapses into communication-bound inefficiency. While the previous section examined intra-node connectivity through NVLink and NVSwitch, this section extends to the datacenter-scale networks that enable training across hundreds or thousands of accelerators. The transition from intra-node to inter-node communication introduces fundamentally different constraints: where NVLink provides 900 GB/s between GPUs on a single baseboard, inter-node networks must traverse switches, cables, and protocol stacks that introduce both bandwidth limitations and latency penalties.\n\n### Training Network Requirements\n\nLarge-scale training workloads impose unique demands on network infrastructure. Unlike traditional datacenter traffic patterns dominated by short flows and request-response interactions, distributed training generates sustained, synchronized bulk transfers. A single AllReduce operation across 1024 GPUs may move terabytes of gradient data, with all participants blocked until the collective completes. This pattern demands networks optimized for bandwidth rather than connection establishment latency.\n\n#### High-Bandwidth Interconnects\n\nInfiniBand has emerged as the dominant interconnect for ML training clusters due to its RDMA (Remote Direct Memory Access) capabilities and consistent low latency. The technology enables direct memory-to-memory transfers without CPU involvement, reducing both latency and processor overhead.\n\n+---------------------+-------------+----------------+------------------+\n| Generation          | Bandwidth   | Latency        | Common Deployment|\n+=====================+=============+================+==================+\n| HDR (200 Gb/s)      | 25 GB/s     | 0.6 μs         | Legacy clusters  |\n+---------------------+-------------+----------------+------------------+\n| HDR100 (100 Gb/s)   | 12.5 GB/s   | 0.6 μs         | Cost-optimized   |\n+---------------------+-------------+----------------+------------------+\n| NDR (400 Gb/s)      | 50 GB/s     | 0.5 μs         | Current standard |\n+---------------------+-------------+----------------+------------------+\n| NDR200 (200 Gb/s)   | 25 GB/s     | 0.5 μs         | Disaggregated    |\n+---------------------+-------------+----------------+------------------+\n| XDR (800 Gb/s)      | 100 GB/s    | < 0.5 μs       | Emerging         |\n+---------------------+-------------+----------------+------------------+\n\nRoCE (RDMA over Converged Ethernet) provides an alternative that leverages existing Ethernet infrastructure. RoCEv2 operates over UDP/IP, enabling RDMA semantics across routed networks. While RoCE offers lower capital costs and operational familiarity, it requires careful configuration of Priority Flow Control (PFC) and Explicit Congestion Notification (ECN) to prevent packet loss. In ML workloads, even small packet loss rates cause significant performance degradation because collective operations must wait for retransmissions.\n\nThe choice between InfiniBand and RoCE involves trade-offs beyond raw performance:\n\n$$\n\\text{Effective Bandwidth} = \\text{Link Rate} \\times (1 - \\text{Loss Rate}) \\times \\text{Protocol Efficiency}\n$$\n\nInfiniBand achieves protocol efficiencies above 95%, while RoCE typically operates at 85-92% depending on network congestion and flow control configuration. For a 400 Gb/s link, this difference translates to 47.5 GB/s versus 42.5 GB/s effective throughput, a gap that compounds across thousands of collective operations per training step.\n\nNetwork interface cards for ML workloads increasingly integrate compute capabilities. NVIDIA's ConnectX-7 adapters include programmable engines for in-network aggregation, enabling switch-based gradient reduction that reduces traffic volumes. These SmartNICs offload collective operations from the GPU, overlapping communication with computation more effectively than software-only approaches.\n\n#### Network Topology Design\n\nThe physical arrangement of switches and links fundamentally constrains distributed training performance. Fat-tree topologies, derived from Clos network theory, provide full bisection bandwidth: any partition of the network can communicate at full link rate with the other half. For AllReduce operations that require all-to-all communication patterns, this property ensures no bottlenecks regardless of job placement.\n\nA three-tier fat-tree with radix-64 switches supports 65,536 endpoints while maintaining non-blocking connectivity. The bandwidth at each tier equals:\n\n$$\nB_{\\text{tier}} = \\frac{k}{2} \\times B_{\\text{link}} \\times N_{\\text{switches}}\n$$\n\nwhere $k$ is the switch radix and $N_{\\text{switches}}$ is the number of switches at that tier. For NDR InfiniBand with 64-port switches, each spine switch contributes 1.6 TB/s of bisection bandwidth.\n\nRail-optimized topologies offer an alternative for workloads dominated by tensor parallelism. In these designs, GPUs at the same position across multiple nodes connect through dedicated \"rails\" with minimal switch hops. An 8-rail design connects GPU 0 from each of 32 nodes through a single leaf switch, enabling efficient pipeline parallelism where activations flow between corresponding GPUs across nodes. This approach sacrifices the flexibility of fat-tree networks for reduced latency on structured communication patterns.\n\n+---------------------+------------------+------------------+------------------+\n| Topology            | Bisection BW     | Latency (hops)   | Best Workload    |\n+=====================+==================+==================+==================+\n| Fat-tree (3-tier)   | Full             | 4-6              | General/AllReduce|\n+---------------------+------------------+------------------+------------------+\n| Rail-optimized      | Rail-limited     | 2                | Tensor parallel  |\n+---------------------+------------------+------------------+------------------+\n| Dragonfly           | Variable         | 3-5              | Large scale      |\n+---------------------+------------------+------------------+------------------+\n| Torus (TPU)         | Dimension-based  | O(√N)            | Structured comms |\n+---------------------+------------------+------------------+------------------+\n\nThe distinction between non-blocking and oversubscribed networks carries significant implications for ML workloads. A 2:1 oversubscription ratio halves the effective bisection bandwidth, potentially doubling AllReduce time for large collectives. While oversubscription reduces infrastructure costs, the impact on training throughput often negates the savings. Most production ML clusters deploy non-blocking networks for training, reserving oversubscribed designs for serving traffic where request-response patterns tolerate contention.\n\n#### Multi-Rack and Multi-Datacenter Training\n\nScaling beyond a single rack introduces inter-rack connectivity as a potential bottleneck. Even with non-blocking leaf-spine architectures, cable lengths increase from meters to tens of meters, adding propagation delay. More significantly, the spine layer becomes a shared resource across all training jobs, requiring careful traffic engineering to prevent interference.\n\nCross-datacenter training enables access to geographically distributed GPU resources but faces fundamental latency constraints. A 100 km fiber link introduces approximately 0.5 ms round-trip latency from propagation alone, before considering switch processing or protocol overhead. For synchronous training with tight AllReduce coupling, this latency directly extends iteration time:\n\n$$\nT_{\\text{iteration}} = T_{\\text{compute}} + T_{\\text{comm}} + T_{\\text{latency}} \\times N_{\\text{rounds}}\n$$\n\nAsynchronous methods like Local SGD reduce communication frequency but introduce staleness that affects convergence. Practical cross-datacenter training typically employs hierarchical aggregation: workers synchronize within each datacenter, then datacenters exchange aggregated gradients at lower frequency.\n\nNetwork partitions present a more severe challenge than performance degradation. When connectivity between datacenters fails, training jobs must either pause (wasting expensive GPU time) or continue with partial gradients (risking divergence). Partition-tolerant training algorithms remain an active research area, with approaches ranging from elastic data parallelism to speculative gradient accumulation. We examine fault tolerance mechanisms in detail in @sec-fault-tolerance.\n\n### Serving Network Requirements\n\nInference serving demands different network characteristics than training. Where training optimizes for bulk throughput, serving prioritizes latency consistency across diverse request patterns. A recommendation model serving millions of queries per second cannot tolerate the tail latency variance acceptable in batch training.\n\n#### Load Balancer Architectures\n\nML serving deployments require load balancers that understand model-specific traffic patterns. Layer 4 (L4) load balancers operate on TCP/UDP flows, distributing connections based on IP addresses and ports. They offer high throughput with minimal latency overhead but cannot inspect request content for intelligent routing.\n\nLayer 7 (L7) load balancers parse application protocols, enabling routing decisions based on request characteristics. For ML serving, this enables routing requests to model versions, directing traffic based on input features, or implementing request coalescing for batch inference. The cost is increased latency, typically 0.5-2 ms per hop for TLS termination and HTTP parsing.\n\nConsistent hashing provides session affinity for stateful inference scenarios. When serving autoregressive language models, subsequent tokens in a generation session should route to the same replica to reuse KV cache state. The hash function maps session identifiers to replicas:\n\n$$\n\\text{replica} = \\text{hash}(\\text{session\\_id}) \\mod N_{\\text{replicas}}\n$$\n\nVirtual nodes improve load distribution when replicas have heterogeneous capacity. Each physical replica appears multiple times in the hash ring proportional to its capacity, naturally directing more traffic to more capable instances.\n\nGeographic load distribution becomes essential for global ML services. DNS-based global load balancing directs users to nearby deployments, reducing round-trip latency. However, model updates must propagate across all regions consistently, requiring coordination between deployment systems and traffic management.\n\n#### Service Mesh for ML\n\nService mesh architectures insert proxy sidecars alongside ML services, enabling consistent observability and traffic management without application changes. For ML deployments, sidecars capture request latencies, model versions, and input characteristics that feed monitoring and debugging systems.\n\nTraffic routing through service mesh enables sophisticated A/B testing beyond simple traffic splitting. Requests can route based on user segments, input features, or model confidence scores. A recommendation system might route uncertain predictions to an ensemble while serving confident predictions from a faster single model.\n\nCircuit breaker patterns prevent cascade failures when model replicas become unhealthy. When error rates exceed thresholds, the circuit opens and redirects traffic to healthy replicas or fallback models. For ML serving, circuit breakers must account for model-specific health indicators: high latency might indicate GPU memory pressure rather than failure, warranting throttling rather than failover.\n\n### Network Performance Analysis\n\nQuantitative understanding of network performance enables informed decisions about infrastructure investment and training configurations. The interplay between model architecture, parallelism strategy, and network capability determines overall system efficiency.\n\n#### Bandwidth Utilization Patterns\n\nAllReduce bandwidth requirements scale with model size and parallelism configuration. For data parallelism with $N$ workers and model parameters $P$, each worker must send and receive approximately $2P$ bytes per iteration (assuming ring AllReduce). The required bandwidth to hide communication behind computation is:\n\n$$\nB_{\\text{required}} = \\frac{2P}{T_{\\text{compute}}}\n$$\n\nFor a 175B parameter model with FP16 gradients (350 GB), achieving 50% compute utilization on hardware with 1 second compute time requires 700 GB/s aggregate bandwidth, far exceeding single-link capacity and motivating sophisticated parallelism strategies.\n\n::: {.callout-warning}\n## Practical AllReduce Efficiency\n\nTheoretical bandwidth calculations assume ideal conditions. Production AllReduce operations achieve 60-80% of theoretical bandwidth due to multiple overheads:\n\n- **Startup latency**: Each ring stage incurs 5-20 μs fixed overhead per chunk\n- **Memory copy overhead**: CPU-GPU and GPU-NIC transfers add latency\n- **Protocol overhead**: NCCL/Gloo software stack processing\n- **Network contention**: Shared fabric with concurrent jobs reduces effective bandwidth\n\nWell-tuned systems on dedicated InfiniBand fabric achieve 75-85% efficiency. Many deployments, particularly those using RoCE or shared networks, achieve only 40-60%. Always benchmark actual collective performance rather than relying on theoretical link rates.\n:::\n\nGradient compression reduces bandwidth requirements at the cost of computation and potential accuracy impact. Top-k sparsification transmits only the largest gradient components, achieving 100-1000x compression ratios for some models. Error feedback mechanisms accumulate untransmitted gradients, maintaining convergence despite aggressive compression:\n\n$$\n\\tilde{g}_t = \\text{TopK}(g_t + e_{t-1}), \\quad e_t = g_t + e_{t-1} - \\tilde{g}_t\n$$\n\nPipeline parallelism communication patterns differ fundamentally from data parallelism. Rather than bulk AllReduce, pipeline stages exchange activation tensors between adjacent stages. The communication volume depends on activation size rather than parameter count, favoring models with small intermediate representations.\n\n#### Latency Analysis\n\nNetwork latency accumulates from multiple sources, each contributing to collective operation time:\n\n+---------------------+------------------+----------------------------------+\n| Source              | Typical Latency  | Mitigation                       |\n+=====================+==================+==================================+\n| Switch hop          | 100-400 ns       | Topology optimization            |\n+---------------------+------------------+----------------------------------+\n| Cable propagation   | 5 ns/m           | Compact layout                   |\n+---------------------+------------------+----------------------------------+\n| NIC processing      | 1-2 μs           | Hardware offload                 |\n+---------------------+------------------+----------------------------------+\n| NCCL software       | 5-20 μs          | Kernel fusion, persistent kernels|\n+---------------------+------------------+----------------------------------+\n| Memory copy         | Variable         | Zero-copy RDMA                   |\n+---------------------+------------------+----------------------------------+\n\nFor small messages, latency dominates over bandwidth. The crossover point where bandwidth becomes the limiting factor occurs at:\n\n$$\nM_{\\text{crossover}} = \\text{Latency} \\times \\text{Bandwidth}\n$$\n\nFor a system with 5 μs latency and 50 GB/s bandwidth, messages smaller than 250 KB are latency-bound. This motivates message aggregation in collective implementations, batching small gradient tensors to amortize latency overhead.\n\nCongestion control algorithms significantly impact performance under contention. Traditional TCP congestion control, designed for fairness across independent flows, performs poorly for synchronized ML traffic where all flows compete simultaneously. DCQCN (Data Center Quantized Congestion Notification) for RoCE and hardware-based credit flow control for InfiniBand provide faster response to congestion, reducing tail latency. NCCL implements topology-aware algorithms that schedule transfers to minimize contention, exploiting knowledge of collective patterns that general-purpose congestion control lacks. We examine these collective operation implementations in detail in @sec-communication.\n\n### Case Study: NVIDIA DGX SuperPOD Networking\n\nThe DGX SuperPOD architecture illustrates production-scale ML networking design. A SuperPOD combines multiple DGX systems into a unified training cluster, with networking designed to maintain near-linear scaling.\n\nThe baseline SuperPOD configuration connects 32 DGX H100 systems (256 GPUs) through a two-tier InfiniBand network. Each DGX H100 node provides eight NVIDIA ConnectX-7 adapters, one per GPU, each delivering 400 Gb/s (NDR) bandwidth. The leaf tier consists of 32 QM9700 switches, each connecting eight GPUs from a single node. The spine tier uses eight QM9700 switches, each connecting to all 32 leaf switches.\n\nThis configuration provides 1:1 bandwidth between any GPU pair, enabling efficient AllReduce regardless of job placement. The aggregate bisection bandwidth reaches 51.2 TB/s, supporting concurrent training of multiple large models.\n\nLarger SuperPOD deployments extend to 4096 GPUs across 512 DGX systems. At this scale, three-tier fat-tree topologies maintain non-blocking connectivity while managing cable plant complexity. The network includes 256 leaf switches, 128 spine switches, and 32 super-spine switches, totaling over 50,000 optical connections.\n\nRail-optimized variants reduce switch count by exploiting structured communication patterns. In tensor-parallel configurations, each GPU primarily communicates with corresponding GPUs in other nodes, a pattern well-served by dedicated rails with single-hop connectivity. The trade-off is reduced flexibility: jobs with different parallelism configurations may experience suboptimal placement.\n\nNetwork management in SuperPOD deployments integrates with cluster schedulers to enable topology-aware job placement. The Unified Fabric Manager monitors link health, detects failures, and can reroute traffic around failed components. Adaptive routing distributes load across multiple paths, improving utilization when traffic patterns create hotspots.\n\nThe SuperPOD design embodies the principles examined throughout this section: high-bandwidth interconnects (NDR InfiniBand), topology optimization (configurable fat-tree or rail), and integration with higher-level systems (NCCL, scheduler). These infrastructure choices directly determine training efficiency, making networking architecture a critical factor in ML system design.\n\n## Resource Management and Scheduling\n\nThe datacenter infrastructure and high-speed networks discussed in @sec-datacenter-architecture and @sec-networking-ml provide the physical foundation for large-scale ML. However, translating these resources into productive workloads requires sophisticated scheduling systems that balance utilization, fairness, and job completion time. This section examines the scheduling challenges unique to ML workloads and the systems designed to address them.\n\nML workloads present scheduling challenges distinct from traditional computing. Training jobs require coordinated access to multiple GPUs, often spanning nodes connected via the InfiniBand fabric discussed in the previous section. Inference workloads demand consistent latency while handling unpredictable traffic patterns. Both compete for the same accelerator resources, creating tension between throughput-oriented batch processing and latency-sensitive serving.\n\n### Why Distributed Scheduling is Hard\n\nBefore examining specific schedulers, understanding why cluster scheduling differs fundamentally from single-machine scheduling clarifies the design constraints these systems face.\n\n**Distributed Systems Challenges.** Cluster scheduling is not merely \"putting jobs on machines\" at larger scale. Several fundamental distributed systems problems make it intrinsically harder:\n\n1. **Partial failures**: A node can fail between allocation and job start. The scheduler may successfully allocate 32 GPUs across 4 nodes, only to have one node fail before the job launches. The remaining 24 GPUs sit idle while the scheduler detects the failure and re-allocates.\n\n2. **Network partitions**: The scheduler may lose connectivity to a subset of nodes while those nodes continue operating. From the scheduler's perspective, the nodes appear failed. From the nodes' perspective, jobs may still be running.\n\n3. **State inconsistency**: Resource state may differ between the scheduler's view and reality on individual nodes. A GPU the scheduler believes is free may still be cleaning up from a previous job.\n\n4. **Ordering without global time**: Without a global clock, determining whether allocation A happened before allocation B across different nodes requires careful protocol design. Two jobs may both believe they \"own\" the same GPU if the system is not carefully designed.\n\n**CAP Theorem Implications.** The CAP theorem applies directly to cluster scheduling: a scheduler cannot simultaneously provide perfect consistency (every allocation reflects true cluster state), availability (every request gets a response), and partition tolerance (system operates despite network failures).\n\nProduction schedulers make different trade-offs:\n\n- **Slurm** prioritizes consistency, blocking allocations during uncertainty\n- **Kubernetes** prioritizes availability, using eventual consistency with reconciliation loops\n- **Custom ML schedulers** often accept bounded inconsistency for performance\n\n**Failure Rates at Scale.** At scale, failure is normal operation, not exceptional. With 99.9% annual GPU reliability (typical for datacenter hardware), a 4096-GPU cluster experiences:\n\n$$\n\\text{Expected failures per day} = 4096 \\times \\frac{0.001}{365} \\approx 0.01 \\text{ GPU failures/day}\n$$\n\nMore realistically, including software failures, driver issues, and thermal events, production clusters see 1-4 failures per day per 1000 GPUs. A multi-week training run on 4096 GPUs will experience multiple failures. Infrastructure and scheduling systems must anticipate this reality.\n\n### Batch Scheduling for Training\n\nTraining large models requires allocating substantial GPU resources for extended periods. A 175B parameter model may require 1024 GPUs for weeks, while smaller experiments need only a few GPUs for hours. Scheduling systems must efficiently pack these diverse workloads while respecting resource constraints and fairness policies.\n\n#### Slurm for HPC-Style ML\n\nSlurm (Simple Linux Utility for Resource Management) dominates HPC environments and extends naturally to GPU-intensive ML training. Its partition-based architecture maps well to heterogeneous accelerator pools.\n\nA typical ML cluster configuration defines partitions by accelerator type and interconnect:\n\n+----------------------+------------+-----------------+------------------+\n| Partition            | GPUs/Node  | Interconnect    | Typical Use      |\n+======================+============+=================+==================+\n| dgx-a100             | 8 x A100   | NVLink + IB NDR | Large LLM training|\n+----------------------+------------+-----------------+------------------+\n| a100-pcie            | 4 x A100   | PCIe + IB HDR   | Medium training  |\n+----------------------+------------+-----------------+------------------+\n| inference            | 2 x A10G   | Ethernet        | Model serving    |\n+----------------------+------------+-----------------+------------------+\n| debug                | 1 x V100   | Ethernet        | Development      |\n+----------------------+------------+-----------------+------------------+\n\nGPU allocation strategies significantly impact utilization. The `--gres=gpu:N` flag requests N GPUs, but naive allocation can fragment nodes. Consider a 64-node cluster with 8 GPUs each (512 total). If jobs request 6 GPUs, each job wastes 2 GPUs per node, reducing effective capacity to 75%. Slurm's `SelectTypeParameters=CR_GPU` enables GPU-level scheduling, while `--gpus-per-node` ensures jobs receive full nodes when beneficial for NVLink communication.\n\nFair-share scheduling prevents any single user or project from monopolizing resources. The classic fair-share formula computes effective priority as:\n\n$$P_{effective} = P_{base} \\times \\frac{F_{target}}{F_{actual} + \\epsilon}$$\n\nwhere $F_{target}$ represents the user's allocated share and $F_{actual}$ their recent usage. This naturally deprioritizes heavy users while allowing burst access when resources are idle.\n\nPreemption policies enable high-priority jobs to reclaim resources from running workloads. For ML training, this requires checkpoint-aware preemption. Jobs receive SIGTERM with configurable grace periods (typically 60-300 seconds) to save checkpoints before SIGKILL. The `PreemptMode=REQUEUE` setting automatically restarts preempted jobs, while `GraceTime` controls the checkpoint window.\n\n#### Kubernetes for ML Workloads\n\nKubernetes has become the standard platform for ML infrastructure, particularly for organizations requiring unified management of training and serving workloads. Native Kubernetes lacks ML-aware scheduling, but extensions address this gap.\n\nGPU scheduling relies on device plugins that expose accelerators as extended resources. The NVIDIA device plugin registers GPUs with the kubelet, enabling pod specifications like:\n\n```yaml\nresources:\n  limits:\n    nvidia.com/gpu: 4\n```\n\nHowever, this binary allocation model wastes resources when workloads need less than a full GPU. Multi-Instance GPU (MIG) technology addresses this by partitioning A100 and H100 GPUs into isolated instances. An A100-80GB can be divided into configurations ranging from 7 small instances (10GB each) to 2 large instances (40GB each). The device plugin exposes MIG instances as separate resources:\n\n+----------------------+------------+-----------------+------------------+\n| MIG Profile          | GPU Memory | SM Count        | Typical Workload |\n+======================+============+=================+==================+\n| 1g.10gb              | 10 GB      | 14 SMs          | Small inference  |\n+----------------------+------------+-----------------+------------------+\n| 2g.20gb              | 20 GB      | 28 SMs          | Medium inference |\n+----------------------+------------+-----------------+------------------+\n| 3g.40gb              | 40 GB      | 42 SMs          | Large inference  |\n+----------------------+------------+-----------------+------------------+\n| 7g.80gb              | 80 GB      | 98 SMs          | Training         |\n+----------------------+------------+-----------------+------------------+\n\nGang scheduling ensures distributed training jobs receive all requested resources simultaneously. Without gang scheduling, a job requesting 32 GPUs might receive 24 immediately while waiting indefinitely for the remaining 8, wasting the already-allocated resources. The Volcano batch scheduler and scheduler plugins like Coscheduling implement gang semantics through PodGroup abstractions. Jobs specify minimum member counts, and the scheduler delays placement until all pods can be scheduled together.\n\nPriority classes control preemption behavior. A typical hierarchy assigns training workloads medium priority, inference high priority, and development jobs low priority. The `preemptionPolicy: PreemptLowerPriority` setting enables inference pods to reclaim resources from training when latency SLOs are threatened, though organizations must balance this against training job completion times.\n\n#### Custom ML Schedulers\n\nResearch schedulers have demonstrated significant improvements over general-purpose systems by exploiting ML-specific characteristics.\n\n**Tiresias** observes that ML training jobs have predictable resource requirements after initial epochs. Rather than requiring users to estimate job duration (often inaccurate by 2-5x), Tiresias uses a two-dimensional attained service scheduler. Jobs accumulate \"service\" based on GPU-time consumed, with priority decreasing as service increases. A discretized version groups jobs into service bins, promoting short jobs without requiring duration estimates. Experiments show 40-60% reduction in average job completion time compared to FIFO scheduling.\n\n**Gandiva** exploits the iterative nature of deep learning. Training alternates between GPU-intensive forward/backward passes and CPU-intensive data loading. Gandiva time-slices GPU access at iteration boundaries, enabling higher utilization through oversubscription. A cluster with 100 GPUs might support 120 concurrent jobs if each spends 20% of time waiting for data. Gandiva also implements grow-shrink elasticity, automatically adjusting data parallelism degree based on resource availability.\n\n**Themis** addresses fairness in long-running ML workloads. Traditional fair-share treats all GPU-seconds equally, but ML jobs have diminishing returns as training progresses. Themis defines a finish-time fairness metric, allocating resources to minimize the maximum slowdown any job experiences relative to exclusive access. This approach benefits shorter jobs without excessive penalty to longer ones.\n\nLocality-aware scheduling recognizes that communication topology matters for distributed training. A 64-GPU job performs better on 8 nodes of 8 GPUs each than 16 nodes of 4 GPUs, due to higher NVLink bandwidth within nodes. Advanced schedulers consider the fat-tree topology discussed in @sec-networking-ml, preferring allocations that share fewer switch hops. Experiments show 15-30% training throughput improvement from topology-aware placement.\n\n### Online Serving Resource Management\n\nInference workloads require different scheduling strategies than training. Latency matters more than throughput, traffic fluctuates unpredictably, and resource requirements vary by model size and request characteristics.\n\n#### Autoscaling for Inference\n\nHorizontal Pod Autoscaling (HPA) adjusts replica counts based on metrics. Default CPU utilization targets (often 50-70%) poorly reflect GPU inference workloads. Effective ML autoscaling uses custom metrics:\n\n+----------------------+------------------+----------------------------------+\n| Metric               | Target Range     | Considerations                   |\n+======================+==================+==================================+\n| GPU utilization      | 60-80%           | Varies by model batch efficiency |\n+----------------------+------------------+----------------------------------+\n| Request queue depth  | 10-50 requests   | Prevents latency spikes          |\n+----------------------+------------------+----------------------------------+\n| P99 latency          | < SLO target     | Reactive, lags demand changes    |\n+----------------------+------------------+----------------------------------+\n| Pending tokens       | Model-specific   | LLM-specific, accounts for KV    |\n+----------------------+------------------+----------------------------------+\n\nVertical Pod Autoscaling (VPA) adjusts resource requests and limits for individual pods. For inference, VPA can right-size memory allocations based on observed usage. However, GPU resources cannot be vertically scaled without pod restart, limiting VPA's utility for accelerated workloads.\n\nLLM inference requires specialized scaling due to the key-value cache. A 70B parameter model serving long-context requests may require 80GB+ of GPU memory for KV cache alone, even with PagedAttention optimizations. Scaling decisions must account for both request rate and context length distribution.\n\n#### Resource Isolation\n\nNoisy neighbor problems occur when colocated workloads interfere with each other. On GPUs, interference manifests through shared memory bandwidth, L2 cache contention, and PCIe bottlenecks. MIG provides hardware isolation but at the cost of flexibility. Software approaches include careful placement policies and time-based resource contracts.\n\nGPU memory isolation prevents one model from consuming memory needed by another. Without explicit limits, a memory leak or unexpectedly large batch can crash colocated workloads. Container runtimes can enforce memory limits through CUDA MPS (Multi-Process Service), though this adds latency overhead of approximately 5-10 microseconds per kernel launch.\n\nCPU pinning assigns specific cores to inference pods, preventing scheduler migration that causes cache invalidation. For latency-sensitive workloads, isolating cores using `isolcpus` kernel parameters and `taskset` affinity removes OS scheduling jitter. Combined with NUMA-aware placement, this reduces P99 latency by 10-30% for sub-millisecond inference tasks.\n\n### Multi-Tenancy Considerations\n\nProduction ML platforms serve multiple teams with competing priorities. Quota systems balance guaranteed access against overall utilization, while security isolation protects sensitive models and data.\n\n#### Quota Management\n\nGPU quota allocation typically operates at the namespace or project level. A simple approach allocates fixed GPU counts, but this leads to underutilization when teams have variable workloads. Hierarchical quotas enable departmental limits with sub-team flexibility:\n\n$$Q_{effective} = \\min(Q_{team}, Q_{department} - \\sum_{other\\ teams} U_{allocated})$$\n\nFair-share across teams extends the single-user formula to organizational hierarchies. When aggregate demand exceeds capacity, each team receives resources proportional to their share allocation. Unused capacity borrows down the hierarchy, maximizing utilization while respecting priorities.\n\nBurst capacity handling enables teams to temporarily exceed quotas when resources are available. Overcommitment ratios of 1.2-1.5x are common, with admission controllers tracking actual versus requested resources. When contention occurs, jobs using burst capacity face preemption first.\n\n#### Security Isolation\n\nNamespace separation provides the fundamental isolation boundary in Kubernetes. Each team operates within dedicated namespaces with role-based access control (RBAC) limiting visibility to their own resources. Network policies extend isolation to the network layer, preventing cross-namespace communication except through explicitly permitted services.\n\nNetwork policies for ML workloads must balance isolation with distributed training requirements. A policy might allow all-to-all communication within a namespace (for ring-AllReduce) while blocking ingress from other namespaces. Egress policies prevent training jobs from accessing external networks, reducing data exfiltration risk.\n\nGPU virtualization options range from time-slicing (low isolation, high flexibility) to MIG (hardware isolation, fixed partitions) to full device passthrough (complete isolation, lowest utilization). The choice depends on workload sensitivity and trust boundaries. Multi-tenant inference platforms typically use MIG for isolation between tenants, while single-tenant training clusters favor device passthrough for maximum performance.\n\nThe scheduling and resource management infrastructure discussed here enables efficient use of the datacenter resources and networks from previous sections. Effective schedulers achieve 70-85% GPU utilization in production clusters, compared to 30-50% with naive approaches. This efficiency translates directly to cost: a 1000-GPU cluster at 80% utilization delivers the equivalent capacity of 1600 GPUs at 50% utilization. As organizations scale ML infrastructure, scheduling sophistication becomes a primary determinant of both cost efficiency and researcher productivity. These resource management capabilities also provide the foundation for the fault-tolerant systems discussed in @sec-fault-tolerance and the operational practices covered in @sec-ops-scale.\n\n## Total Cost of Ownership Analysis\n\nUnderstanding the complete financial picture of ML infrastructure requires moving beyond simple hardware acquisition costs to comprehensive Total Cost of Ownership (TCO) analysis. This section provides quantitative frameworks for evaluating infrastructure investments, comparing deployment strategies, and optimizing long-term operational efficiency.\n\n### Capital Expenditure Components\n\nCapital expenditure (CapEx) encompasses all upfront investments required to establish ML infrastructure. These costs are typically amortized over 3-5 years, though the rapid pace of GPU advancement often compresses effective useful life.\n\n#### Hardware Costs\n\nGPU and accelerator acquisition represents the dominant CapEx component for ML infrastructure. Current market pricing reflects both performance capabilities and supply constraints.\n\n+------------------+-------------+----------------+------------------+\n| System           | Base Cost   | Memory Config  | Cost per PFLOP   |\n+==================+=============+================+==================+\n| DGX H100         | ~$300,000   | 640 GB HBM3    | ~$75,000         |\n+------------------+-------------+----------------+------------------+\n| DGX B100*        | ~$450,000   | 1.4 TB HBM3e   | ~$25,000         |\n+------------------+-------------+----------------+------------------+\n| HGX H100 (8-way) | ~$250,000   | 640 GB HBM3    | ~$62,500         |\n+------------------+-------------+----------------+------------------+\n| TPU v5p Pod      | Variable    | 95 GB HBM      | ~$30,000         |\n+------------------+-------------+----------------+------------------+\n\n*B100 pricing is estimated. All prices reflect approximate 2024 market conditions and should be verified for current planning. GPU pricing fluctuates 20-40% based on supply constraints and generation transitions.\n\nServer and storage costs add substantial overhead beyond accelerators. A complete DGX H100 deployment requires NVMe storage ($15,000-30,000 per node), high-speed networking cards ($8,000-15,000), and rack infrastructure ($5,000-10,000). Storage architecture for large-scale training demands parallel file systems capable of sustaining the I/O bandwidth required by hundreds of GPUs, with enterprise solutions like Lustre or GPFS adding $500-1,000 per terabyte of high-performance capacity.\n\nNetworking equipment costs scale superlinearly with cluster size due to the hierarchical nature of high-bandwidth fabrics. A 256-GPU cluster using InfiniBand HDR requires approximately $800,000-1,200,000 in networking equipment.\n\n$$C_{\\text{network}} = N_{\\text{switches}} \\cdot P_{\\text{switch}} + N_{\\text{cables}} \\cdot P_{\\text{cable}} + N_{\\text{adapters}} \\cdot P_{\\text{adapter}}$$\n\nFor a 256-GPU deployment with 2:1 oversubscription:\n\n$$C_{\\text{network}} \\approx 32 \\times \\$15,000 + 512 \\times \\$800 + 256 \\times \\$3,000 \\approx \\$1,660,000$$\n\nRefresh cycle planning significantly impacts TCO calculations. GPU generations advance every 2-3 years with typical performance improvements of 2-3x per generation. Organizations must balance the benefits of newer hardware against the disruption costs of migration. A common strategy employs staggered refresh cycles, replacing 25-33% of infrastructure annually to maintain competitive capability while avoiding wholesale replacement costs.\n\n#### Facility Costs\n\nDatacenter construction costs range from $7-12 million per megawatt of IT capacity for purpose-built facilities. ML workloads, with their high power density requirements (30-50 kW per rack versus 5-10 kW for traditional compute), demand specialized cooling infrastructure that increases construction costs by 20-40%.\n\nPower infrastructure represents a substantial portion of facility investment. Electrical distribution systems including transformers, switchgear, uninterruptible power supplies (UPS), and power distribution units (PDUs) typically cost $2-4 million per megawatt. Redundancy requirements (N+1 or 2N configurations) can double these costs for mission-critical deployments.\n\nCooling systems for high-density ML infrastructure increasingly require liquid cooling solutions. Direct-to-chip liquid cooling adds $50,000-100,000 per rack in capital costs but enables the power densities required for modern GPU configurations. The DGX H100 systems referenced in our datacenter architecture discussion require liquid cooling for sustained operation, representing a non-optional facility cost.\n\n### Operational Expenditure Components\n\nOperational expenditure (OpEx) captures ongoing costs that accumulate throughout infrastructure lifetime. For ML systems, power costs and specialized staffing dominate this category.\n\n#### Power Costs\n\nElectricity represents the largest operational cost for ML infrastructure. Power costs vary dramatically by geography, with industrial rates ranging from $0.04/kWh in regions with abundant hydroelectric power to $0.20/kWh in constrained markets.\n\nThe total power cost calculation must account for PUE overhead. As established in our datacenter architecture discussion, hyperscale facilities achieve PUE values of 1.1-1.2, meaning 10-20% additional power supports cooling and infrastructure. The annual power cost for a single DGX H100 system can be calculated as:\n\n$$C_{\\text{power}} = P_{\\text{system}} \\times \\text{PUE} \\times H_{\\text{annual}} \\times R_{\\text{electricity}} \\times U$$\n\nwhere $P_{\\text{system}}$ is system power (10.2 kW for DGX H100), $H_{\\text{annual}}$ is hours per year (8,760), $R_{\\text{electricity}}$ is the electricity rate, and $U$ is utilization factor.\n\nFor a DGX H100 at 80% utilization with $0.08/kWh electricity and 1.15 PUE:\n\n$$C_{\\text{power}} = 10.2 \\times 1.15 \\times 8,760 \\times 0.08 \\times 0.80 \\approx \\$6,600 \\text{ annually}$$\n\nElectricity pricing models significantly impact operational costs. Time-of-use pricing creates opportunities for training workload scheduling during off-peak hours (typically nights and weekends), potentially reducing power costs by 20-40%. Demand charges, which price peak power consumption, incentivize workload smoothing to avoid utilization spikes.\n\nRenewable energy considerations extend beyond environmental responsibility to economic optimization. Power Purchase Agreements (PPAs) for renewable energy often provide long-term price stability, hedging against electricity market volatility. Many organizations target 100% renewable energy matching through a combination of on-site generation, PPAs, and Renewable Energy Certificates (RECs). Environmental implications of energy choices are examined comprehensively in @sec-sustainable-ai.\n\n#### Staffing and Operations\n\nML infrastructure requires specialized operational expertise across multiple domains. Staffing costs often represent 15-25% of total operational expenditure for well-run facilities.\n\nHardware operations teams manage physical infrastructure including installation, maintenance, and failure response. For clusters of 500+ GPUs, dedicated hardware technicians are essential, with typical ratios of 1 technician per 200-400 GPUs depending on hardware heterogeneity and SLA requirements.\n\nSoftware platform teams maintain the scheduling systems, container infrastructure, and ML frameworks that enable productive use of hardware resources. These roles command premium compensation due to the specialized intersection of systems engineering and ML expertise required.\n\nUtilization monitoring represents both a staffing function and a key lever for TCO optimization. Continuous monitoring of GPU utilization, memory bandwidth, and job efficiency enables identification of optimization opportunities. Organizations achieving 70%+ sustained GPU utilization versus the more common 30-50% effectively halve their per-computation infrastructure costs.\n\n### Build vs. Buy Analysis\n\nThe fundamental infrastructure decision is whether to operate private infrastructure or consume cloud capacity. This choice involves complex trade-offs that depend on workload characteristics, scale, and organizational capabilities.\n\n#### Cloud vs. On-Premises Trade-offs\n\nCloud computing offers compelling advantages for specific use cases. Variable workloads with unpredictable demand benefit from cloud elasticity, avoiding stranded capacity during low-demand periods. Experimentation and research phases, where hardware requirements remain uncertain, benefit from the ability to test different configurations without capital commitment. Geographic distribution requirements for inference serving often favor cloud deployment due to the substantial investment required for multi-region presence.\n\nOn-premises infrastructure wins economically under sustained high utilization. The break-even analysis requires comparing amortized CapEx plus OpEx against equivalent cloud costs:\n\n$$\\text{Break-even utilization} = \\frac{C_{\\text{cloud}} \\times H_{\\text{annual}}}{\\frac{C_{\\text{capex}}}{Y_{\\text{amortization}}} + C_{\\text{opex}}}$$\n\nConsider a DGX H100 system with $300,000 CapEx, 3-year amortization, and $25,000 annual OpEx (power, maintenance, proportional staff). Cloud equivalent (8x H100 instance at ~$25/hour):\n\n$$\\text{Break-even} = \\frac{25 \\times 8,760}{\\frac{300,000}{3} + 25,000} = \\frac{219,000}{125,000} \\approx 1.75$$\n\nThis calculation suggests on-premises becomes favorable when utilization exceeds approximately 57% (1/1.75). In practice, organizations report break-even utilization thresholds of 40-60% depending on specific cloud pricing and operational efficiency.\n\n+-------------------+----------------------+----------------------+\n| Factor            | Favors Cloud         | Favors On-Premises   |\n+===================+======================+======================+\n| Utilization       | <40% average         | >60% sustained       |\n+-------------------+----------------------+----------------------+\n| Workload pattern  | Variable, bursty     | Steady, predictable  |\n+-------------------+----------------------+----------------------+\n| Data volume       | Moderate             | Petabyte-scale       |\n+-------------------+----------------------+----------------------+\n| Time horizon      | <2 years             | >3 years             |\n+-------------------+----------------------+----------------------+\n| Team capability   | Limited ops staff    | Strong infrastructure|\n+-------------------+----------------------+----------------------+\n\nHybrid strategies combine cloud burst capacity with on-premises baseline infrastructure. Organizations maintain on-premises systems sized for typical load (e.g., 60th percentile demand) while using cloud for peak periods. This approach captures most on-premises economic benefits while retaining cloud flexibility.\n\n#### Reserved Capacity vs. Spot Instances\n\nCloud providers offer commitment discount programs that substantially reduce effective pricing. Reserved instances with 1-year commitments typically offer 30-40% discounts, while 3-year commitments reach 50-60% discounts relative to on-demand pricing. These discounts shift cloud economics but introduce utilization risk similar to on-premises ownership.\n\nSpot instance strategies enable dramatic cost reduction (60-80% below on-demand) for fault-tolerant training workloads. Effective spot utilization requires:\n\n1. **Checkpoint integration**: Training frameworks must save state frequently enough that spot interruption costs remain acceptable. Modern distributed training checkpoints every 10-30 minutes, limiting maximum lost computation.\n\n2. **Fallback mechanisms**: Automated job migration to alternative instance types or regions when spot capacity becomes unavailable.\n\n3. **Heterogeneous training**: Frameworks capable of operating across mixed instance types to maximize spot availability.\n\nThe effective spot discount must account for interruption overhead:\n\n$$C_{\\text{effective}} = C_{\\text{spot}} \\times (1 + R_{\\text{interrupt}} \\times T_{\\text{recovery}})$$\n\nwhere $R_{\\text{interrupt}}$ is the hourly interruption rate and $T_{\\text{recovery}}$ is recovery time as a fraction of checkpoint interval. With 5% hourly interruption rate and 10-minute recovery on 30-minute checkpoints:\n\n$$C_{\\text{effective}} = 0.30 \\times C_{\\text{ondemand}} \\times (1 + 0.05 \\times 0.33) \\approx 0.31 \\times C_{\\text{ondemand}}$$\n\nEven accounting for interruption overhead, spot instances provide compelling economics for training workloads with proper checkpoint infrastructure.\n\n### Comprehensive TCO Model\n\nA complete TCO model integrates capital and operational components across the infrastructure lifetime:\n\n$$\\text{TCO} = \\sum_{t=1}^{Y} \\frac{C_{\\text{capex}}^{(t)} + C_{\\text{opex}}^{(t)}}{(1+r)^t}$$\n\nwhere $r$ is the discount rate reflecting cost of capital. For a 256-GPU cluster over 4 years:\n\n| Component           | Year 1      | Year 2      | Year 3      | Year 4      |\n|---------------------|-------------|-------------|-------------|-------------|\n| Hardware CapEx      | $9,600,000  | $0          | $0          | $3,200,000  |\n| Network CapEx       | $1,660,000  | $0          | $0          | $0          |\n| Power (at 70% util) | $1,690,000  | $1,690,000  | $1,690,000  | $1,690,000  |\n| Maintenance         | $480,000    | $576,000    | $691,000    | $829,000    |\n| Staff (allocated)   | $800,000    | $840,000    | $882,000    | $926,000    |\n| **Annual Total**    | $14,230,000 | $3,106,000  | $3,263,000  | $6,645,000  |\n\nThe NPV at 8% discount rate equals approximately $24.1 million, yielding a 4-year cost per GPU-hour of $4.30 at 70% utilization. This compares favorably to cloud A100 pricing of $3-4/hour only when accounting for the H100's 3x performance advantage, yielding effective cost per computation approximately 40% below cloud alternatives at this utilization level.\n\nPower cost sensitivity analysis reveals the importance of electricity pricing in deployment decisions. A $0.04/kWh difference in electricity rates shifts the 4-year TCO by approximately $2.7 million for a 256-GPU cluster, potentially changing the optimal deployment strategy. Organizations with access to low-cost renewable energy enjoy structural cost advantages that compound over multi-year infrastructure investments.\n\n## Case Studies\n\nThe infrastructure patterns examined in previous sections combine in different configurations depending on workload characteristics and organizational constraints. Four production deployments illustrate how datacenter architecture, networking, and resource management decisions interact to enable distinct ML workloads. Each case study represents a different point in the design space: GPU-centric dense training, TPU-based transformer optimization, hybrid CPU-GPU recommendation serving, and custom silicon for domain-specific acceleration.\n\n### NVIDIA DGX SuperPOD Architecture\n\nThe DGX SuperPOD represents NVIDIA's reference architecture for large-scale training, combining the dense GPU packaging of DGX systems with purpose-built networking. While the previous section examined SuperPOD networking topology, this case study addresses the complete system architecture including physical deployment, management infrastructure, and operational characteristics.\n\n#### Physical Layout and Cooling Integration\n\nA production SuperPOD deployment with 512 DGX H100 systems (4096 GPUs) occupies approximately 2000 square meters of datacenter floor space. The layout follows a pod-based organization where groups of 32 DGX systems share common power and cooling infrastructure. Each pod dissipates over 300 kW, requiring direct liquid cooling loops with facility-level heat exchangers.\n\nThe cooling architecture uses a closed-loop system with water temperature maintained at 35-45C entering the cold plates. Unlike traditional datacenter cooling that targets low air temperatures, warm-water cooling improves efficiency by enabling free cooling in moderate climates. Heat removed from GPU cold plates transfers to building cooling towers without mechanical refrigeration for ambient temperatures below 25C.\n\nPower distribution follows the N+1 redundancy model at the pod level, with each DGX system receiving dual power feeds. A complete SuperPOD installation requires 5-7 MW of utility power including cooling overhead, corresponding to PUE values of 1.2-1.3 for liquid-cooled deployments.\n\n#### Management Plane Architecture\n\nSuperPOD management integrates multiple control systems spanning hardware, networking, and workload orchestration. Base Controller Manager (BCM) provides hardware-level management including firmware updates, health monitoring, and out-of-band access. The Unified Fabric Manager coordinates InfiniBand network configuration, adaptive routing policies, and link health monitoring.\n\nAt the workload level, SuperPOD deployments typically integrate with either Slurm or Kubernetes for job scheduling. The NVIDIA GPU Operator handles GPU driver installation, monitoring integration, and device plugin management for Kubernetes environments. Slurm configurations use GRES scheduling with topology-aware placement to ensure jobs receive contiguous GPU allocations that minimize inter-node communication.\n\nStorage integration varies by deployment, but reference architectures include NVIDIA's GPUDirect Storage for direct data paths between NVMe storage and GPU memory. A typical SuperPOD includes 30-50 PB of high-performance storage providing 200+ GB/s aggregate throughput, staging training data close to compute.\n\n### Google TPU Pod Infrastructure\n\nGoogle's TPU pods represent an alternative architectural philosophy: vertically integrated accelerators designed specifically for transformer training, with interconnect capabilities built into the chip rather than added as external networking.\n\n#### TPU v4 Pod Architecture\n\nA TPU v4 pod contains 4096 TPU chips arranged in a 3D torus topology. Each chip provides approximately 275 TFLOPS of bfloat16 compute with 32 GB of HBM2e memory, yielding aggregate pod capacity of 1.1 exaFLOPS and 128 TB of memory. The power envelope for a complete pod is approximately 4-5 MW, competitive with GPU-based systems at similar compute density.\n\nThe physical packaging differs fundamentally from GPU systems. TPU chips mount in trays of 4, with trays assembled into racks of 64 chips each. Sixty-four racks form the complete pod, arranged in a cube topology that matches the 3D torus interconnect structure. Cooling uses rear-door heat exchangers with facility water, maintaining chip temperatures below 85C under sustained load.\n\n#### Inter-Chip Interconnect Topology\n\nThe ICI (Inter-Chip Interconnect) fabric provides direct chip-to-chip connectivity without external switches. Each TPU v4 chip has six ICI links at 100 GB/s each, enabling 3D torus connectivity:\n\n$$\n\\text{Bisection Bandwidth} = 2 \\times \\sqrt[3]{N} \\times B_{\\text{link}} \\times N/2\n$$\n\nFor N=4096 chips with 100 GB/s links, the torus bisection bandwidth reaches approximately 32 TB/s. While lower than fat-tree alternatives, the consistent latency characteristics of torus topology benefit the regular communication patterns of transformer training.\n\nThe topology choice optimizes for AllReduce patterns where each chip communicates with neighbors rather than arbitrary endpoints. For a model using 3D parallelism with 4 tensor-parallel chips, 16 pipeline stages, and 64-way data parallelism, the workload maps naturally onto a 4x16x64 slice of the pod topology.\n\n#### Software Stack Integration\n\nTPU software centers on JAX and XLA, with pjit (partitioned JIT compilation) managing distributed execution. XLA compiles high-level model descriptions to TPU-specific operations, automatically inserting communication collectives based on partition specifications. This approach differs from the explicit communication programming required for GPU clusters.\n\nMultislice training extends beyond single pods by connecting multiple TPU slices via datacenter network. A PaLM-scale training run might utilize four TPU v4 pods (16,384 chips) with cross-slice communication at lower bandwidth than intra-slice ICI. The software stack handles this hierarchy transparently, using different collective algorithms for intra-slice versus inter-slice operations.\n\n### Meta Recommendation Infrastructure\n\nMeta's recommendation systems illustrate infrastructure optimized for a fundamentally different workload pattern: models combining massive embedding tables with relatively modest dense computation. This architecture serves billions of daily recommendation queries across products including Facebook Feed, Instagram, and Reels.\n\n#### CPU-GPU Hybrid Architecture\n\nRecommendation models like DLRM (Deep Learning Recommendation Model) partition naturally between embedding operations and dense neural network computation. Embedding tables for production systems can exceed 10 TB, far exceeding GPU memory capacity. The hybrid architecture addresses this by placing embeddings in CPU DRAM while dense layers execute on GPUs.\n\nA production recommendation training node combines multiple CPUs totaling 2-4 TB of DRAM with 8 GPUs for dense computation. The CPUs handle embedding lookups, concatenation, and feature preprocessing. Resulting feature vectors transfer to GPUs via PCIe for the dense forward and backward passes. Gradient updates for embeddings return to CPU memory via the same path.\n\nThis architecture requires careful balancing. The ratio of embedding lookups to dense computation determines optimal CPU-to-GPU allocation. For Meta's workloads, approximately 4:1 CPU socket to GPU ratios provide balanced utilization, though this varies by model architecture.\n\n#### Embedding Table Serving at Scale\n\nInference architecture differs from training by emphasizing latency over throughput. Production serving distributes embedding tables across a fleet of CPU-based servers using consistent hashing for shard assignment. A single recommendation query may access hundreds of embedding shards, requiring parallel lookups that complete within the 50-100 ms latency budget.\n\nThe embedding serving tier operates separately from the dense model serving tier. This separation enables independent scaling: embedding servers scale with table size and query rate, while dense model servers scale with compute requirements. Cross-tier communication uses low-latency RPC, typically completing in under 5 ms for local datacenter deployments.\n\nFeature stores cache frequently accessed embeddings and precomputed features, reducing embedding server load for popular items. A tiered caching architecture places hot embeddings in GPU memory (microsecond access), warm embeddings in CPU DRAM (sub-millisecond), and cold embeddings in distributed storage (milliseconds). Cache hit rates above 90% are typical for recommendation workloads due to power-law popularity distributions.\n\n#### Training and Serving Coordination\n\nThe separation between training and serving infrastructure creates coordination challenges for model updates. Meta's approach uses a staged rollout pipeline: models train on dedicated GPU clusters, export to serving format, deploy to staging clusters for validation, then gradually roll out to production serving. The complete pipeline from training completion to full production deployment spans hours to days depending on model criticality.\n\nTraining clusters optimize for throughput using large batch sizes and aggressive gradient accumulation. Serving clusters optimize for latency using quantized models, batched inference, and result caching. The different optimization targets justify separate infrastructure rather than shared clusters.\n\n### Tesla Dojo for Vision Training\n\nTesla's Dojo system represents the custom silicon approach to ML infrastructure: building purpose-designed chips and packaging for a specific workload rather than using general-purpose accelerators.\n\n#### Custom Silicon Architecture\n\nThe Dojo D1 chip provides 1024 custom-designed cores in a 645 mm2 die. Each core combines an 8-wide vector unit, 64-bit scalar unit, and 1.25 MB of SRAM, yielding approximately 22.6 TFLOPS of BF16 compute per chip. The design optimizes for convolutional and attention operations typical of vision models, with dataflow execution patterns that minimize memory traffic.\n\nTwenty-five D1 chips mount on a single training tile, connected via a 2D mesh interconnect providing 4 TB/s aggregate bandwidth. Six tiles combine into a system tray, and multiple trays assemble into a complete ExaPOD delivering over 1 exaFLOP of aggregate compute. The modular architecture enables deployments from single tiles (0.5 PFLOPS) to multi-ExaPOD installations.\n\n#### Wafer-Scale Considerations\n\nWhile Dojo uses conventional chip packaging, the architecture addresses similar challenges to wafer-scale integration: maximizing on-chip bandwidth while managing thermal and yield constraints. The 2D mesh topology within each tile provides nearest-neighbor bandwidth of 18 GB/s between chips, avoiding the bottlenecks of hierarchical topologies for spatially-local operations common in vision processing.\n\nPower density presents the primary challenge: a fully populated system tray dissipates over 100 kW in a compact form factor. Tesla's thermal solution uses direct liquid cooling with custom manifolds delivering coolant to each training tile. The aggressive cooling enables sustained operation at power densities exceeding traditional datacenter limits.\n\nYield management for custom silicon requires careful attention. Unlike commodity GPU purchases where defective units return to the vendor, custom chip production creates internal yield loss. Dojo's design includes redundant cores and interconnect paths, enabling graceful degradation when manufacturing defects occur. Production testing identifies defective units, and the software stack maps computation around unavailable resources.\n\n#### Training Video Data at Scale\n\nDojo's primary workload is training vision models on Tesla's fleet data: over 1 million video clips per day from vehicles worldwide. The data pipeline presents distinct challenges from text or image training. Video requires decompression, temporal alignment, sensor calibration, and often 3D scene reconstruction before training.\n\nThe preprocessing pipeline runs on CPU clusters adjacent to Dojo compute, staging prepared batches to high-speed storage. Storage bandwidth of 10+ GB/s per training tile ensures compute utilization despite the data-intensive nature of video processing. The complete system integrates 10 PB of flash storage providing over 100 GB/s aggregate throughput.\n\nThis infrastructure supports auto-labeling workflows where preliminary models identify scenarios of interest in raw video, generating training data for improved models. The closed-loop between deployment, data collection, and training enables rapid iteration cycles measured in days rather than weeks.\n\n---\n\nThese case studies demonstrate that production ML infrastructure defies one-size-fits-all solutions. DGX SuperPOD optimizes for flexible general-purpose training with emphasis on GPU density and high-bandwidth networking. TPU pods sacrifice flexibility for vertical integration that excels at transformer workloads. Meta's hybrid architecture addresses the embedding-heavy patterns unique to recommendation systems. Tesla's Dojo pursues custom silicon for domain-specific acceleration where scale justifies development costs. The choice among these approaches depends on workload characteristics, scale requirements, and organizational capabilities rather than any universal optimum. Understanding these trade-offs enables informed infrastructure decisions as models and training requirements continue to evolve. For implementation details of the distributed training algorithms that leverage these infrastructure platforms, see @sec-distributed-training. Network-level considerations for collective operations are examined in @sec-communication.\n\n## Fallacies and Pitfalls\n\nThe complexity of large-scale ML infrastructure creates numerous opportunities for costly miscalculations. These misconceptions often stem from oversimplified mental models that fail to account for the non-linear interactions between compute, networking, power, and cooling systems. Understanding these fallacies and pitfalls helps practitioners avoid expensive mistakes that can waste millions of dollars in infrastructure investment or months of delayed projects.\n\n**Fallacy:** _More GPUs always means faster training._\n\nThis intuition fails catastrophically beyond modest cluster sizes. Distributed training introduces communication overhead, and the relationship between GPU count and training speed is far from linear. For a 175B parameter model using data parallelism, each training step requires exchanging approximately 350 GB of gradient data (FP16). Ring AllReduce achieves near-optimal bandwidth utilization: each GPU sends and receives $2P \\cdot (N-1)/N$ bytes, approaching $2P$ as $N$ grows. With 64 GPUs connected via 400 Gbps (50 GB/s) InfiniBand, the communication time for 350 GB approaches:\n\n$$\nT_{\\text{comm}} = \\frac{2P}{B} = \\frac{700 \\text{ GB}}{50 \\text{ GB/s}} \\approx 14 \\text{ seconds}\n$$\n\nCritically, this communication time is bounded as $N$ increases. The scaling efficiency depends on the ratio of compute time to total time:\n\n$$\n\\text{Efficiency} = \\frac{T_{\\text{compute}}}{T_{\\text{compute}} + T_{\\text{comm}}}\n$$\n\nFor data parallelism, $T_{\\text{compute}}$ scales as $1/N$ while $T_{\\text{comm}}$ approaches a constant. When compute time per GPU drops below communication time, adding more GPUs yields diminishing returns. For a workload with 60 seconds single-GPU compute time, scaling to 64 GPUs reduces compute to ~1 second while communication remains ~14 seconds, yielding only 7% efficiency. Many organizations discover that their 512-GPU cluster trains models slower than a well-optimized 128-GPU deployment due to crossing this inflection point. The solution requires moving beyond naive data parallelism to hybrid parallelism strategies that minimize cross-node communication, topics explored in @sec-distributed-training.\n\n**Fallacy:** _Peak FLOPS determines training throughput._\n\nVendors advertise peak FLOPS prominently: the H100 delivers 990 TF of FP16 compute. But production training jobs typically achieve 30-50% of peak due to memory bandwidth limitations, communication overhead, and kernel efficiency. For attention-dominated transformer training, memory bandwidth, not compute, is the limiting factor.\n\nThe roofline model reveals this clearly: with arithmetic intensity below the ridge point (~290 FLOP/byte for H100), the accelerator is memory-bound. Most LLM training operates at 50-100 FLOP/byte, achieving only 170-340 TF effective throughput on H100, roughly 20-35% of peak. Comparing accelerators by peak FLOPS alone misleads: a lower-FLOPS accelerator with higher memory bandwidth may outperform a higher-FLOPS alternative for memory-bound workloads.\n\n**Fallacy:** _All ML infrastructure should be GPU-based._\n\nThe assumption that GPUs represent the optimal accelerator for all ML workloads ignores the fundamental architectural differences between model types. Recommendation systems, which drive the majority of inference cycles at companies like Meta and Google, exhibit a workload profile where embedding table lookups dominate compute time. These lookups are memory-bound operations that GPUs handle poorly since they require random access to terabytes of embedding tables that cannot fit in GPU HBM.\n\nMeta's production recommendation infrastructure uses a hybrid architecture where CPU clusters handle embedding lookups from DRAM-based feature stores while GPU clusters process the dense neural network layers. This split architecture achieves 3x better cost efficiency than GPU-only deployments for their workload. Similarly, preprocessing pipelines for training data, including decompression, tokenization, and augmentation, execute more efficiently on CPUs. A DGX H100 with only 2 Intel Xeon CPUs can become CPU-bottlenecked on data preprocessing, starving the 8 H100 GPUs that represent 95% of the system cost.\n\n**Pitfall:** _Ignoring power and cooling constraints during infrastructure planning._\n\nPower and cooling represent hard physical limits that cannot be resolved through software optimization. A single rack of 4 DGX H100 systems requires over 40 kW of power and generates equivalent thermal load. Many organizations plan GPU purchases based on compute requirements without verifying datacenter capacity, only to discover their facility cannot support the power density.\n\nThe consequences compound over time. Current-generation H100 GPUs consume 700W each, but next-generation B100 GPUs maintain similar power envelopes while delivering 2x compute. However, the industry trend toward higher-density deployments means power requirements per rack continue to increase. Organizations that build datacenters for 30 kW per rack today will face costly retrofits within 2-3 years as GPU density increases.\n\nThermal throttling presents an equally insidious challenge. When cooling systems cannot remove heat fast enough, GPUs reduce clock speeds to prevent damage. A cluster designed for 100% utilization may achieve only 70% sustained throughput due to thermal constraints. At $30 per GPU-hour for H100 instances, this represents $7.20 per hour per GPU in wasted capacity. For a 1000-GPU cluster running training jobs, thermal inefficiency costs over $170,000 per month.\n\n**Pitfall:** _Underestimating network requirements for distributed training._\n\nNetwork bandwidth is rarely the only consideration. Latency, topology, and software overhead combine to determine actual training throughput. A cluster with theoretical 400 Gbps InfiniBand connectivity may achieve only 300 Gbps effective throughput due to protocol overhead in NCCL, suboptimal job placement, and contention from concurrent training jobs.\n\nFat-tree topologies provide full bisection bandwidth at significant switch cost, while rail-optimized topologies reduce hardware requirements but constrain job placement flexibility. Choosing the wrong topology for a workload can halve training throughput. For tensor-parallel workloads where adjacent GPUs exchange activations frequently, rail-optimized networks with 2-hop maximum latency outperform fat-tree networks with 4-6 hops. For data-parallel workloads with AllReduce patterns, fat-tree's guaranteed bisection bandwidth prevents congestion-induced slowdowns.\n\nSoftware overhead adds latency that theoretical bandwidth calculations ignore. NCCL introduces 5-20 microseconds of software latency per collective operation. For small message sizes common in pipeline parallelism, this overhead dominates transfer time, reducing effective bandwidth by 50% or more. The crossover point where bandwidth dominates latency occurs at approximately 250 KB message size for typical InfiniBand configurations. Operators that fail to aggregate small tensors before collective operations waste substantial network capacity. These communication optimization strategies are examined in detail in @sec-communication.\n\n## Summary\n\nLarge-scale ML infrastructure represents the physical foundation upon which all distributed training and serving systems operate. The constraints examined in this chapter, from power delivery and cooling capacity to network topology and accelerator selection, ultimately determine what scale of ML systems an organization can build and operate effectively.\n\nInfrastructure design must match workload characteristics. LLM training demands GPU-dense configurations with high-bandwidth NVLink and InfiniBand connectivity, while recommendation systems require hybrid CPU-GPU architectures optimized for embedding table access. Vision workloads fall between these extremes, benefiting from GPU acceleration but tolerating moderate network bandwidth. Attempting to serve all workload types from homogeneous infrastructure wastes resources and constrains performance.\n\nTraining and serving have fundamentally different infrastructure requirements. Training workloads tolerate batch scheduling, require sustained high bandwidth for collective operations, and can checkpoint through failures. Serving workloads demand low-latency networking, consistent response times, and immediate failover capabilities. Organizations that attempt to share infrastructure between training and serving often compromise both workloads.\n\nTotal cost of ownership extends far beyond hardware acquisition. Power consumption for a 1000-GPU cluster can exceed $2 million annually at typical datacenter rates. Cooling infrastructure may cost more than the GPUs it supports. Operational overhead including monitoring, maintenance, and administration adds 20-30% to hardware costs over a three-year depreciation cycle. Cloud versus on-premises decisions depend critically on utilization rates, with break-even typically occurring around 40% sustained utilization.\n\nNetwork topology choices determine distributed training efficiency. The decision between fat-tree and rail-optimized topologies, InfiniBand and RoCE, 2-tier and 3-tier switching architectures shapes what parallelism strategies perform well on the resulting infrastructure. Topology decisions made during datacenter construction constrain training architectures for years afterward.\n\nDifferent model types require different infrastructure patterns. The SuperPOD architecture optimized for LLM training differs fundamentally from Meta's recommendation serving infrastructure or Tesla's Dojo system optimized for video processing. No single infrastructure design serves all ML workloads optimally, and organizations must either specialize their infrastructure or accept efficiency losses from generalization.\n\n::: {.callout-important title=\"Key Takeaways\"}\n\n* Power density and cooling capacity represent hard physical limits that constrain cluster design independent of budget\n* Network topology determines training efficiency, with fat-tree providing flexibility while rail-optimized reduces latency for structured communication patterns\n* Hybrid CPU-GPU architectures outperform GPU-only configurations for recommendation systems and other embedding-heavy workloads\n* Communication overhead limits scaling efficiency, with Amdahl's Law applying to gradient synchronization and collective operations\n* Total cost of ownership must include power, cooling, operations, and realistic utilization rates rather than theoretical peak performance\n\n:::\n\nThe infrastructure foundations established here enable the distributed training strategies in @sec-distributed-training, which examines how parallelism approaches map onto physical hardware. The communication patterns and collective operations detailed in @sec-communication depend directly on the network topologies and bandwidth characteristics discussed in this chapter. For production deployments, @sec-fault-tolerance addresses the reliability requirements that infrastructure must satisfy to support multi-week training runs across thousands of accelerators.\n","srcMarkdownNoYaml":"\n\n<!--\n================================================================================\nEDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR INFRASTRUCTURE\n================================================================================\n\nCORE PRINCIPLE: Infrastructure requirements vary by workload type.\nTraining clusters differ from serving infrastructure. Different model\ntypes have different compute, memory, and networking needs.\n\nMODEL-SPECIFIC INFRASTRUCTURE CONSIDERATIONS:\n\n| Model Type      | Compute Profile     | Memory Profile      | Network Need        |\n|-----------------|---------------------|---------------------|---------------------|\n| LLMs            | GPU-heavy           | HBM-bound           | High (tensor par.)  |\n| Recommendation  | CPU+GPU hybrid      | DRAM for embeddings | Moderate            |\n| Vision          | GPU-heavy           | Moderate            | Moderate            |\n| Scientific      | Varies              | Often huge          | Problem-dependent   |\n\nREQUIRED COVERAGE FOR THIS CHAPTER:\n\nDATACENTER ARCHITECTURE:\n\n- GPU clusters: Dominant for training transformers, vision\n- CPU clusters: Feature serving, preprocessing (RecSys)\n- Hybrid: Recommendation training (embedding on CPU, dense on GPU)\n- Include: Why different workloads need different architectures\n\nACCELERATOR SELECTION:\n\n- GPU (NVIDIA): General-purpose ML, dominant for training\n- TPU: Large-scale training, specific model types\n- Custom ASICs: Inference optimization (recommendation, vision)\n- Include: Different accelerators suit different workloads\n\nNETWORKING:\n\n- InfiniBand: Training clusters, high-bandwidth collective ops\n- Ethernet: Serving infrastructure, feature stores\n- Include: Why training and serving have different network needs\n\nRESOURCE MANAGEMENT:\n\n- Batch scheduling: Training jobs (Slurm, Kubernetes)\n- Online serving: Request routing, autoscaling\n- Include: Different scheduling for different workload types\n\nCASE STUDIES TO INCLUDE:\n\n- NVIDIA DGX SuperPOD architecture\n- Google TPU pod infrastructure\n- Meta recommendation infrastructure (CPU+GPU hybrid)\n- Tesla Dojo for vision training\n\nQUANTITATIVE ANALYSIS:\n\n- TCO breakdown by workload type\n- Power/performance efficiency for different accelerators\n- Network utilization patterns by model type\n- Include: Same cluster, different efficiency for different models\n\nANTI-PATTERNS TO AVOID:\n\n- Assuming all ML infrastructure is GPU clusters\n- Ignoring CPU infrastructure for recommendation\n- One-size-fits-all datacenter design\n- Only discussing training infrastructure (serving matters too)\n\n================================================================================\n-->\n\n# Large-Scale ML Infrastructure {#sec-infrastructure}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A sweeping architectural visualization of a modern AI datacenter infrastructure. The scene reveals a massive facility with rows of GPU clusters arranged in pods, connected by high-bandwidth networking fabric depicted as glowing fiber optic pathways. Cooling systems appear as flowing blue currents between server racks. The visualization includes multiple layers: physical infrastructure at the bottom with power and cooling, compute infrastructure in the middle with thousands of interconnected accelerators, and orchestration software at the top represented as an abstract control plane. Visual elements include resource managers allocating workloads, capacity graphs showing utilization, and geographic connections to other datacenters. The color scheme uses industrial grays and silvers with accent colors of electric blue for networking and amber for active computation. Photorealistic technical illustration style suitable for infrastructure engineering documentation._\n:::\n\n\\noindent\n![](images/png/cover_infrastructure.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why does the ability to build and manage computational infrastructure determine which organizations can realize their most ambitious machine learning goals?_\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\nAfter completing this chapter, you will be able to:\n\n- Calculate power and cooling requirements for GPU cluster deployments using PUE and thermal dissipation constraints\n- Compare network topology trade-offs (fat-tree, rail-optimized, torus) using bisection bandwidth and hop count metrics\n- Analyze total cost of ownership for infrastructure decisions by computing amortized CapEx plus OpEx\n- Design resource management policies that achieve target GPU utilization (70%+) in multi-tenant clusters\n- Evaluate accelerator options (GPU, TPU, custom ASIC) by matching workload characteristics to hardware capabilities\n- Apply the roofline model to determine whether workloads are compute-bound or memory-bound\n\n:::\n\nMachine learning systems that transform industries operate on infrastructure far exceeding the scale of single machines or small clusters. Training a frontier model may require thousands of GPUs coordinated across multiple datacenters, each machine contributing to a unified computation that can span weeks or months. Managing such infrastructure demands expertise in datacenter design, high-bandwidth networking, and distributed systems orchestration. The infrastructure must satisfy competing requirements: computational efficiency alongside fault tolerance, coordination across thousands of machines without prohibitive communication overhead, and dynamic capacity provisioning that controls costs. These challenges have become central to machine learning advancement, as organizations cannot access frontier capabilities without mastering the physical and software systems that make large-scale computation possible. Understanding infrastructure architecture is essential for building systems beyond prototype experiments, shaping whether organizations deploy efficiently, scale reliably, and compete effectively in an increasingly infrastructure-dependent landscape.\n\n## Datacenter Architecture for ML Workloads\n\nThe transition from single-machine ML systems to distributed training fundamentally changes the infrastructure requirements. Where Volume I focused on optimizing computations within a single node, production ML at scale demands purpose-built datacenters designed around the unique characteristics of ML workloads: massive power consumption, extreme heat density, and communication patterns that differ markedly from traditional cloud computing.\n\nThis section examines the physical and compute infrastructure that enables large-scale ML, providing the foundation for understanding how distributed training systems leverage these resources.\n\n### Physical Infrastructure Fundamentals\n\nML datacenters differ from traditional cloud facilities in three critical dimensions: power density per rack is 5-10x higher, cooling requirements demand liquid rather than air-based solutions, and physical layout must optimize for high-bandwidth interconnects rather than flexible networking.\n\n#### Power Delivery and Distribution\n\nA single NVIDIA DGX H100 system consumes 10.2 kW at peak load. A rack containing four such systems requires over 40 kW, compared to 5-10 kW for traditional server racks. This density fundamentally changes power infrastructure design.\n\n**Utility and Backup Power.** Production ML facilities require redundant power feeds, typically N+1 or 2N configurations where N represents the load requirement. Uninterruptible power supplies (UPS) bridge the gap during utility failures, but the massive power draw of GPU clusters limits battery backup duration to minutes rather than hours. Diesel generators provide extended backup, with automatic transfer switches completing failover within 10-15 seconds.\n\n**Power Distribution Architecture.** Modern ML datacenters use a tiered distribution model:\n\n+------------------------+------------------+--------------------------------+\n| Distribution Level     | Typical Voltage  | Purpose                        |\n+========================+==================+================================+\n| Utility feed           | 13.8-69 kV       | Grid connection                |\n+------------------------+------------------+--------------------------------+\n| Substation transformer | 480V (US)        | Building distribution          |\n+------------------------+------------------+--------------------------------+\n| PDU (Power Distribution| 208V             | Rack-level distribution        |\n| Unit)                  |                  |                                |\n+------------------------+------------------+--------------------------------+\n| Server PSU             | 12V DC           | Component-level power          |\n+------------------------+------------------+--------------------------------+\n\n**Power Usage Effectiveness.** The PUE metric quantifies datacenter energy efficiency:\n\n$$\n\\text{PUE} = \\frac{\\text{Total Facility Power}}{\\text{IT Equipment Power}}\n$$\n\nA PUE of 1.0 represents perfect efficiency where all power goes to computing. Traditional datacenters achieve PUE values of 1.5-2.0, while hyperscale facilities target 1.1-1.2. ML datacenters face a challenge: the extreme heat density of GPU clusters increases cooling overhead, pushing PUE higher unless advanced cooling technologies are deployed.\n\n#### Cooling Systems at Scale\n\nHeat dissipation represents the primary constraint on ML cluster density. An H100 GPU generates 700W of thermal output from a surface area smaller than a dinner plate, producing heat flux comparable to a nuclear reactor's fuel rod surface.\n\n**Air Cooling Limitations.** Traditional air cooling becomes impractical above 30-40 kW per rack. The physics are straightforward: air's low heat capacity (approximately 1 kJ/kg-K) requires massive airflow to remove heat. A 40 kW rack requires roughly 10,000 CFM (cubic feet per minute) of airflow, creating acoustic levels exceeding 80 dB and significant fan power overhead.\n\n**Hot Aisle/Cold Aisle Containment.** This architectural pattern separates cold supply air from hot exhaust air using physical barriers. Cold air enters through raised floor vents or overhead ducts, passes through servers front-to-back, and exhausts into a contained hot aisle. Containment improves cooling efficiency by preventing mixing, but cannot solve the fundamental heat density challenge of modern GPU clusters.\n\n**Direct-to-Chip Liquid Cooling.** Liquid cooling addresses heat density through water's superior heat capacity (4.2 kJ/kg-K, roughly four times air). Cold plates mounted directly on GPUs and CPUs transfer heat to circulating coolant, which flows to facility-level heat exchangers. This approach enables rack densities of 100+ kW while reducing cooling power consumption by 30-40% compared to air cooling.\n\n::: {.callout-note}\n## Liquid Cooling Adoption\n\nAs of 2024, liquid cooling has transitioned from specialty option to requirement for large-scale ML clusters. NVIDIA's GB200 NVL72 systems require liquid cooling, with no air-cooled option available. Facilities planning for next-generation hardware must include liquid cooling infrastructure from initial design.\n:::\n\n**Immersion Cooling.** The most aggressive thermal solution submerges entire servers in dielectric fluid. Single-phase immersion uses non-conductive oils that remain liquid, while two-phase systems use fluids that boil at low temperatures, leveraging latent heat of vaporization for efficient heat transfer. Immersion enables rack densities exceeding 200 kW but requires specialized maintenance procedures and component compatibility.\n\n#### Physical Layout Optimization\n\nML cluster performance depends critically on physical topology. Unlike web serving workloads where any server can handle any request, distributed training requires specific communication patterns between specific nodes.\n\n**Rack Density Considerations.** Higher density reduces cable lengths and switch hops but concentrates power and cooling requirements. Production deployments balance these factors based on workload characteristics:\n\n+----------------------+------------------+---------------------------+\n| Workload Type        | Typical Density  | Limiting Factor           |\n+======================+==================+===========================+\n| LLM training         | 80-120 kW/rack   | Cooling capacity          |\n+----------------------+------------------+---------------------------+\n| Recommendation       | 30-50 kW/rack    | CPU/memory balance        |\n| inference            |                  |                           |\n+----------------------+------------------+---------------------------+\n| Vision training      | 60-80 kW/rack    | Network bandwidth         |\n+----------------------+------------------+---------------------------+\n\n**Cable Management.** High-bandwidth interconnects like InfiniBand use copper cables for distances under 3 meters and fiber optics beyond. Cable routing must maintain bend radius requirements (typically 10x cable diameter) while enabling airflow for any air-cooled components. Active optical cables (AOCs) simplify routing but add latency and power consumption compared to passive copper.\n\n### Compute Infrastructure Design\n\nML clusters combine multiple node types, each optimized for different phases of the training and inference pipeline. Understanding these roles clarifies infrastructure design decisions.\n\n#### GPU Cluster Architectures\n\nModern GPU clusters are built from dense multi-GPU nodes connected via high-bandwidth fabrics. Two reference architectures dominate production deployments.\n\n**DGX-Style Dense Nodes.** NVIDIA's DGX systems package 8 GPUs with NVLink interconnects, high-bandwidth networking, and substantial local storage in a single chassis. The DGX H100 provides:\n\n- 8x H100 GPUs with 640GB total HBM3 memory\n- NVSwitch fabric enabling 900 GB/s GPU-to-GPU bandwidth\n- 8x 400 Gbps InfiniBand or Ethernet ports\n- 2x Intel Xeon CPUs for preprocessing\n- 30TB NVMe storage for dataset staging\n\nThis integrated design simplifies deployment but limits flexibility. Each DGX H100 costs approximately $300,000 (pricing reflects 2024 market conditions and fluctuates significantly based on supply, demand, and generation transitions), making component-level upgrades economically impractical.\n\n**HGX Baseboard Designs.** For organizations building custom infrastructure, NVIDIA's HGX baseboards provide the GPU and interconnect components for integration into custom server designs. Cloud providers and large enterprises use HGX to optimize for their specific power, cooling, and networking requirements while maintaining compatibility with NVIDIA's software stack.\n\n**PCIe vs. NVLink Configurations.** The choice between PCIe and NVLink connectivity involves fundamental trade-offs:\n\n+------------------+------------------+------------------+------------------------+\n| Interconnect     | Bandwidth        | Latency          | Use Case               |\n+==================+==================+==================+========================+\n| PCIe Gen5 x16    | 64 GB/s          | ~1 microsecond   | Inference, small       |\n|                  |                  |                  | models                 |\n+------------------+------------------+------------------+------------------------+\n| NVLink 4.0       | 900 GB/s         | ~0.5 microsecond | Large model training   |\n| (bidirectional)  |                  |                  |                        |\n+------------------+------------------+------------------+------------------------+\n\nFor models requiring tensor parallelism across GPUs (as detailed in @sec-distributed-training), NVLink's 14x bandwidth advantage directly translates to training throughput. PCIe-based systems suffice for data-parallel workloads where gradient synchronization occurs less frequently.\n\n#### CPU Infrastructure Roles\n\nWhile GPUs dominate ML computation, CPUs perform essential supporting functions that bottleneck overall system performance if under-provisioned.\n\n**Preprocessing and Data Preparation.** Training data pipelines involve decompression, augmentation, tokenization, and batching. These operations execute on CPUs, which must supply data fast enough to keep GPUs utilized. A common rule of thumb allocates 4-8 CPU cores per GPU for training workloads, though data-intensive pipelines (video, large images) may require more.\n\n**Feature Serving for Recommendation Systems.** Recommendation models present a distinct infrastructure pattern. These systems combine deep learning components with massive embedding tables that may exceed 1TB. The embedding lookups are memory-bound CPU operations, while neural network components benefit from GPU acceleration. Production recommendation systems often use CPU-heavy nodes for embedding serving alongside GPU nodes for model computation, connected via low-latency networks.\n\n**Control Plane and Orchestration.** Cluster management, job scheduling, and monitoring run on dedicated CPU nodes separate from the training cluster. This isolation prevents resource contention and enables management operations even when the training cluster is fully utilized.\n\n#### Hybrid Architectures\n\nReal production systems rarely use homogeneous hardware throughout. Workload-aware placement matches job characteristics to appropriate resources.\n\n**Embedding Table Placement.** For recommendation systems with embedding tables exceeding GPU memory, hybrid architectures place embeddings in CPU DRAM while compute-intensive layers execute on GPUs. Facebook's DLRM architecture pioneered this pattern, with embeddings distributed across CPU nodes communicating with GPU nodes via high-bandwidth networks.\n\n**Heterogeneous Scheduling.** Modern orchestration systems support mixed node types within a single cluster. Kubernetes with GPU support and Slurm with GRES (Generic Resource Scheduling) enable jobs to request specific hardware combinations. A training job might request 64 GPU nodes for model computation plus 16 high-memory CPU nodes for embedding tables, scheduled as a coordinated allocation.\n\n### Accelerator Selection by Workload Type\n\nThe accelerator landscape has expanded beyond NVIDIA GPUs to include Google TPUs, custom ASICs, and emerging architectures. Selection requires matching accelerator characteristics to workload requirements.\n\n#### NVIDIA GPU Ecosystem\n\nNVIDIA maintains market dominance through integrated hardware-software offerings. Understanding the architecture evolution clarifies capability differences.\n\n**Architecture Progression.** Each generation brings substantial improvements in compute density and memory bandwidth:\n\n+-------------+-------------+----------------+------------------+---------------+\n| GPU         | FP16        | HBM            | Memory           | TDP           |\n|             | Tensor      | Capacity       | Bandwidth        |               |\n|             | TFLOPS      |                |                  |               |\n+=============+=============+================+==================+===============+\n| A100        | 312         | 80 GB HBM2e    | 2.0 TB/s         | 400W          |\n+-------------+-------------+----------------+------------------+---------------+\n| H100        | 990         | 80 GB HBM3     | 3.4 TB/s         | 700W          |\n+-------------+-------------+----------------+------------------+---------------+\n| B100*       | ~1,800      | 192 GB HBM3e   | 8.0 TB/s         | ~700W         |\n+-------------+-------------+----------------+------------------+---------------+\n\n*B100 specifications are preliminary estimates based on NVIDIA announcements. Verify against official specifications for production planning.\n\nThe H100 delivers approximately 3x the tensor TFLOPS of A100 at 1.75x the power. To derive the efficiency improvement: A100 achieves 312 TF / 400W = 0.78 TF/W, while H100 achieves 990 TF / 700W = 1.41 TF/W, yielding approximately 80% improvement in FLOPS/watt. Memory bandwidth increases proportionally, maintaining the compute-to-memory ratio critical for transformer models.\n\n**Tensor Core Utilization.** Tensor Cores accelerate matrix operations but require specific data layouts and sizes for full utilization. Dimensions should be multiples of 8 (FP16) or 16 (INT8) for optimal performance. Underutilized Tensor Cores represent the most common source of poor GPU efficiency in production, with many workloads achieving only 30-50% of theoretical peak FLOPS.\n\n**NVLink Topology.** Within a node, NVSwitch provides full-bandwidth connectivity between all GPUs. Across nodes, NVLink Network (available in H100 and later) extends high-bandwidth connectivity, though at reduced bandwidth compared to intra-node links. Topology-aware job placement, discussed in @sec-communication, is essential for multi-node training performance.\n\n#### Google TPU Infrastructure\n\nGoogle's Tensor Processing Units offer an alternative architecture optimized for matrix operations with a distinct programming model.\n\n**TPU Pod Architecture.** TPUs connect via proprietary Inter-Chip Interconnect (ICI) forming 2D or 3D torus topologies. A TPU v4 pod contains 4,096 chips with 1.1 exaFLOPS of aggregate compute. Unlike GPU clusters where networking is separate from compute nodes, TPU pods integrate interconnect into the chip design.\n\n**TPU Slices and Multislice.** Users allocate TPU slices, contiguous subsets of a pod. Multislice training connects multiple slices via datacenter network for jobs exceeding single-slice capacity. The programming model (JAX with pjit) abstracts the physical topology, enabling code portability across slice sizes.\n\n**TPU vs. GPU Trade-offs.** TPUs excel for large-scale training with regular computation patterns:\n\n+------------------------+---------------------------+---------------------------+\n| Factor                 | TPU Advantage             | GPU Advantage             |\n+========================+===========================+===========================+\n| Large transformer      | Optimized matrix units,   | Broader operator support  |\n| training               | integrated interconnect   |                           |\n+------------------------+---------------------------+---------------------------+\n| Custom operations      | Limited flexibility       | CUDA extensibility        |\n+------------------------+---------------------------+---------------------------+\n| Software ecosystem     | JAX-centric               | PyTorch, TensorFlow,      |\n|                        |                           | many frameworks           |\n+------------------------+---------------------------+---------------------------+\n| Availability           | Google Cloud only         | Multiple cloud and        |\n|                        |                           | on-premise options        |\n+------------------------+---------------------------+---------------------------+\n\n#### Custom ASICs and Specialized Accelerators\n\nThe ML accelerator landscape continues to diversify as organizations optimize for specific workloads.\n\n**Inference-Optimized Accelerators.** Training and inference present different requirements. Training needs high-precision arithmetic, large memory for activations and optimizer state, and high interconnect bandwidth. Inference prioritizes low latency, high throughput, and power efficiency. Accelerators like Google's TPU Inference chips and AWS Inferentia optimize for inference characteristics, achieving 2-4x better performance per watt than training-focused hardware for appropriate workloads.\n\n**Emerging Architectures.** Several companies offer alternative approaches:\n\n- **Cerebras WSE**: Wafer-scale integration places an entire ML accelerator on a single silicon wafer, eliminating chip-to-chip communication for models that fit on-chip\n- **Graphcore IPU**: Bulk Synchronous Parallel (BSP) execution model with distributed on-chip memory targeting sparse and dynamic workloads\n- **SambaNova**: Reconfigurable dataflow architecture for enterprise AI applications\n\nThese alternatives find niches where their architectural trade-offs align with workload requirements, though NVIDIA and Google maintain dominant market positions for general ML training.\n\n### Quantitative Infrastructure Analysis\n\nEffective infrastructure decisions require quantitative comparison across accelerator options and workload types.\n\n**FLOPS per Watt Comparison.** Energy efficiency varies significantly across accelerator types and precision levels:\n\n+---------------------+------------------+------------------+------------------+\n| Accelerator         | FP16 TFLOPS      | TDP (Watts)      | TFLOPS/Watt      |\n+=====================+==================+==================+==================+\n| NVIDIA H100 SXM     | 990              | 700              | 1.41             |\n+---------------------+------------------+------------------+------------------+\n| NVIDIA H100 PCIe    | 756              | 350              | 2.16             |\n+---------------------+------------------+------------------+------------------+\n| Google TPU v5p      | 459              | 250-400*         | 1.1-1.8          |\n+---------------------+------------------+------------------+------------------+\n| AWS Trainium        | 210              | 150 (estimated)  | 1.40             |\n+---------------------+------------------+------------------+------------------+\n\n*TPU power varies significantly by deployment configuration and is not officially published. Direct TFLOPS/Watt comparisons across architectures are problematic because utilization profiles differ. These figures should be treated as approximate.\n\nThe PCIe variant's higher efficiency reflects reduced interconnect power, acceptable for inference but limiting for distributed training.\n\n**Memory Bandwidth Utilization.** Different model types exhibit distinct memory access patterns:\n\n- **LLM training**: Memory-bound for attention computation, achieving 70-85% bandwidth utilization\n- **CNN training**: Compute-bound for convolutions, 30-50% bandwidth utilization\n- **Recommendation inference**: Memory-bound for embeddings, often exceeding available bandwidth\n\nUnderstanding these patterns guides accelerator selection: memory-bound workloads benefit from HBM3's bandwidth improvements, while compute-bound workloads prioritize FLOPS per dollar.\n\n**The Roofline Model.** The roofline model provides a systematic framework for understanding whether workloads are compute-bound or memory-bound. Achievable performance is limited by the minimum of peak compute and memory bandwidth:\n\n$$\n\\text{Achievable FLOPS} = \\min\\left(\\text{Peak Compute}, \\text{Memory Bandwidth} \\times \\text{Arithmetic Intensity}\\right)\n$$\n\nArithmetic intensity measures FLOPS per byte of memory traffic. The \"ridge point\" where compute and memory limits intersect determines which workloads benefit from each resource:\n\n+------------------+------------------+------------------+------------------+\n| Accelerator      | Peak Compute     | Memory BW        | Ridge Point      |\n|                  | (TF FP16)        | (TB/s)           | (FLOP/byte)      |\n+==================+==================+==================+==================+\n| H100 SXM         | 990              | 3.4              | 291              |\n+------------------+------------------+------------------+------------------+\n| A100 80GB        | 312              | 2.0              | 156              |\n+------------------+------------------+------------------+------------------+\n| TPU v4           | 275              | 1.2              | 229              |\n+------------------+------------------+------------------+------------------+\n\nMost LLM training operates at 50-100 FLOP/byte arithmetic intensity, well below the ridge point, making these workloads memory-bound. At 75 FLOP/byte on H100, achievable performance is $3.4 \\times 75 = 255$ TF, only 26% of peak compute. This explains why production training achieves 30-50% of theoretical FLOPS: the bottleneck is memory bandwidth, not compute capacity.\n\nCNN training with large batch sizes operates near 200 FLOP/byte, approaching the ridge point where both resources limit performance. Recommendation inference with random embedding lookups operates at extremely low arithmetic intensity (1-10 FLOP/byte), fundamentally memory-bound regardless of accelerator choice.\n\n**Cost per PFLOP.** Infrastructure economics depend on utilization and workload fit:\n\n$$\n\\text{Effective Cost per PFLOP} = \\frac{\\text{Hardware Cost} + \\text{3-year OpEx}}{\\text{Peak PFLOPS} \\times \\text{Average Utilization} \\times 3 \\text{ years}}\n$$\n\nFor a DGX H100 at $300,000 with $50,000 annual power and cooling costs, achieving 50% average utilization yields an effective cost of approximately $0.35 per PFLOP-hour. Cloud instances at $30 per hour for equivalent hardware cost $0.15 per PFLOP-hour at 100% utilization, but on-premise becomes favorable above 40% sustained utilization over three years.\n\n::: {.callout-warning}\n## Utilization Reality\n\nQuoted peak FLOPS numbers assume perfect utilization. Production training jobs typically achieve 30-50% of peak due to communication overhead, data pipeline stalls, and suboptimal kernel efficiency. Infrastructure planning must account for realistic utilization rates rather than theoretical peaks.\n:::\n\nThis infrastructure foundation enables the distributed training strategies explored in @sec-distributed-training and the communication patterns detailed in @sec-communication. The physical constraints examined here, particularly power delivery, cooling capacity, and interconnect topology, ultimately determine what scale of training is achievable.\n\n## Networking for Large-Scale ML\n\nThe networking fabric connecting accelerators determines whether a distributed training job achieves near-linear scaling or collapses into communication-bound inefficiency. While the previous section examined intra-node connectivity through NVLink and NVSwitch, this section extends to the datacenter-scale networks that enable training across hundreds or thousands of accelerators. The transition from intra-node to inter-node communication introduces fundamentally different constraints: where NVLink provides 900 GB/s between GPUs on a single baseboard, inter-node networks must traverse switches, cables, and protocol stacks that introduce both bandwidth limitations and latency penalties.\n\n### Training Network Requirements\n\nLarge-scale training workloads impose unique demands on network infrastructure. Unlike traditional datacenter traffic patterns dominated by short flows and request-response interactions, distributed training generates sustained, synchronized bulk transfers. A single AllReduce operation across 1024 GPUs may move terabytes of gradient data, with all participants blocked until the collective completes. This pattern demands networks optimized for bandwidth rather than connection establishment latency.\n\n#### High-Bandwidth Interconnects\n\nInfiniBand has emerged as the dominant interconnect for ML training clusters due to its RDMA (Remote Direct Memory Access) capabilities and consistent low latency. The technology enables direct memory-to-memory transfers without CPU involvement, reducing both latency and processor overhead.\n\n+---------------------+-------------+----------------+------------------+\n| Generation          | Bandwidth   | Latency        | Common Deployment|\n+=====================+=============+================+==================+\n| HDR (200 Gb/s)      | 25 GB/s     | 0.6 μs         | Legacy clusters  |\n+---------------------+-------------+----------------+------------------+\n| HDR100 (100 Gb/s)   | 12.5 GB/s   | 0.6 μs         | Cost-optimized   |\n+---------------------+-------------+----------------+------------------+\n| NDR (400 Gb/s)      | 50 GB/s     | 0.5 μs         | Current standard |\n+---------------------+-------------+----------------+------------------+\n| NDR200 (200 Gb/s)   | 25 GB/s     | 0.5 μs         | Disaggregated    |\n+---------------------+-------------+----------------+------------------+\n| XDR (800 Gb/s)      | 100 GB/s    | < 0.5 μs       | Emerging         |\n+---------------------+-------------+----------------+------------------+\n\nRoCE (RDMA over Converged Ethernet) provides an alternative that leverages existing Ethernet infrastructure. RoCEv2 operates over UDP/IP, enabling RDMA semantics across routed networks. While RoCE offers lower capital costs and operational familiarity, it requires careful configuration of Priority Flow Control (PFC) and Explicit Congestion Notification (ECN) to prevent packet loss. In ML workloads, even small packet loss rates cause significant performance degradation because collective operations must wait for retransmissions.\n\nThe choice between InfiniBand and RoCE involves trade-offs beyond raw performance:\n\n$$\n\\text{Effective Bandwidth} = \\text{Link Rate} \\times (1 - \\text{Loss Rate}) \\times \\text{Protocol Efficiency}\n$$\n\nInfiniBand achieves protocol efficiencies above 95%, while RoCE typically operates at 85-92% depending on network congestion and flow control configuration. For a 400 Gb/s link, this difference translates to 47.5 GB/s versus 42.5 GB/s effective throughput, a gap that compounds across thousands of collective operations per training step.\n\nNetwork interface cards for ML workloads increasingly integrate compute capabilities. NVIDIA's ConnectX-7 adapters include programmable engines for in-network aggregation, enabling switch-based gradient reduction that reduces traffic volumes. These SmartNICs offload collective operations from the GPU, overlapping communication with computation more effectively than software-only approaches.\n\n#### Network Topology Design\n\nThe physical arrangement of switches and links fundamentally constrains distributed training performance. Fat-tree topologies, derived from Clos network theory, provide full bisection bandwidth: any partition of the network can communicate at full link rate with the other half. For AllReduce operations that require all-to-all communication patterns, this property ensures no bottlenecks regardless of job placement.\n\nA three-tier fat-tree with radix-64 switches supports 65,536 endpoints while maintaining non-blocking connectivity. The bandwidth at each tier equals:\n\n$$\nB_{\\text{tier}} = \\frac{k}{2} \\times B_{\\text{link}} \\times N_{\\text{switches}}\n$$\n\nwhere $k$ is the switch radix and $N_{\\text{switches}}$ is the number of switches at that tier. For NDR InfiniBand with 64-port switches, each spine switch contributes 1.6 TB/s of bisection bandwidth.\n\nRail-optimized topologies offer an alternative for workloads dominated by tensor parallelism. In these designs, GPUs at the same position across multiple nodes connect through dedicated \"rails\" with minimal switch hops. An 8-rail design connects GPU 0 from each of 32 nodes through a single leaf switch, enabling efficient pipeline parallelism where activations flow between corresponding GPUs across nodes. This approach sacrifices the flexibility of fat-tree networks for reduced latency on structured communication patterns.\n\n+---------------------+------------------+------------------+------------------+\n| Topology            | Bisection BW     | Latency (hops)   | Best Workload    |\n+=====================+==================+==================+==================+\n| Fat-tree (3-tier)   | Full             | 4-6              | General/AllReduce|\n+---------------------+------------------+------------------+------------------+\n| Rail-optimized      | Rail-limited     | 2                | Tensor parallel  |\n+---------------------+------------------+------------------+------------------+\n| Dragonfly           | Variable         | 3-5              | Large scale      |\n+---------------------+------------------+------------------+------------------+\n| Torus (TPU)         | Dimension-based  | O(√N)            | Structured comms |\n+---------------------+------------------+------------------+------------------+\n\nThe distinction between non-blocking and oversubscribed networks carries significant implications for ML workloads. A 2:1 oversubscription ratio halves the effective bisection bandwidth, potentially doubling AllReduce time for large collectives. While oversubscription reduces infrastructure costs, the impact on training throughput often negates the savings. Most production ML clusters deploy non-blocking networks for training, reserving oversubscribed designs for serving traffic where request-response patterns tolerate contention.\n\n#### Multi-Rack and Multi-Datacenter Training\n\nScaling beyond a single rack introduces inter-rack connectivity as a potential bottleneck. Even with non-blocking leaf-spine architectures, cable lengths increase from meters to tens of meters, adding propagation delay. More significantly, the spine layer becomes a shared resource across all training jobs, requiring careful traffic engineering to prevent interference.\n\nCross-datacenter training enables access to geographically distributed GPU resources but faces fundamental latency constraints. A 100 km fiber link introduces approximately 0.5 ms round-trip latency from propagation alone, before considering switch processing or protocol overhead. For synchronous training with tight AllReduce coupling, this latency directly extends iteration time:\n\n$$\nT_{\\text{iteration}} = T_{\\text{compute}} + T_{\\text{comm}} + T_{\\text{latency}} \\times N_{\\text{rounds}}\n$$\n\nAsynchronous methods like Local SGD reduce communication frequency but introduce staleness that affects convergence. Practical cross-datacenter training typically employs hierarchical aggregation: workers synchronize within each datacenter, then datacenters exchange aggregated gradients at lower frequency.\n\nNetwork partitions present a more severe challenge than performance degradation. When connectivity between datacenters fails, training jobs must either pause (wasting expensive GPU time) or continue with partial gradients (risking divergence). Partition-tolerant training algorithms remain an active research area, with approaches ranging from elastic data parallelism to speculative gradient accumulation. We examine fault tolerance mechanisms in detail in @sec-fault-tolerance.\n\n### Serving Network Requirements\n\nInference serving demands different network characteristics than training. Where training optimizes for bulk throughput, serving prioritizes latency consistency across diverse request patterns. A recommendation model serving millions of queries per second cannot tolerate the tail latency variance acceptable in batch training.\n\n#### Load Balancer Architectures\n\nML serving deployments require load balancers that understand model-specific traffic patterns. Layer 4 (L4) load balancers operate on TCP/UDP flows, distributing connections based on IP addresses and ports. They offer high throughput with minimal latency overhead but cannot inspect request content for intelligent routing.\n\nLayer 7 (L7) load balancers parse application protocols, enabling routing decisions based on request characteristics. For ML serving, this enables routing requests to model versions, directing traffic based on input features, or implementing request coalescing for batch inference. The cost is increased latency, typically 0.5-2 ms per hop for TLS termination and HTTP parsing.\n\nConsistent hashing provides session affinity for stateful inference scenarios. When serving autoregressive language models, subsequent tokens in a generation session should route to the same replica to reuse KV cache state. The hash function maps session identifiers to replicas:\n\n$$\n\\text{replica} = \\text{hash}(\\text{session\\_id}) \\mod N_{\\text{replicas}}\n$$\n\nVirtual nodes improve load distribution when replicas have heterogeneous capacity. Each physical replica appears multiple times in the hash ring proportional to its capacity, naturally directing more traffic to more capable instances.\n\nGeographic load distribution becomes essential for global ML services. DNS-based global load balancing directs users to nearby deployments, reducing round-trip latency. However, model updates must propagate across all regions consistently, requiring coordination between deployment systems and traffic management.\n\n#### Service Mesh for ML\n\nService mesh architectures insert proxy sidecars alongside ML services, enabling consistent observability and traffic management without application changes. For ML deployments, sidecars capture request latencies, model versions, and input characteristics that feed monitoring and debugging systems.\n\nTraffic routing through service mesh enables sophisticated A/B testing beyond simple traffic splitting. Requests can route based on user segments, input features, or model confidence scores. A recommendation system might route uncertain predictions to an ensemble while serving confident predictions from a faster single model.\n\nCircuit breaker patterns prevent cascade failures when model replicas become unhealthy. When error rates exceed thresholds, the circuit opens and redirects traffic to healthy replicas or fallback models. For ML serving, circuit breakers must account for model-specific health indicators: high latency might indicate GPU memory pressure rather than failure, warranting throttling rather than failover.\n\n### Network Performance Analysis\n\nQuantitative understanding of network performance enables informed decisions about infrastructure investment and training configurations. The interplay between model architecture, parallelism strategy, and network capability determines overall system efficiency.\n\n#### Bandwidth Utilization Patterns\n\nAllReduce bandwidth requirements scale with model size and parallelism configuration. For data parallelism with $N$ workers and model parameters $P$, each worker must send and receive approximately $2P$ bytes per iteration (assuming ring AllReduce). The required bandwidth to hide communication behind computation is:\n\n$$\nB_{\\text{required}} = \\frac{2P}{T_{\\text{compute}}}\n$$\n\nFor a 175B parameter model with FP16 gradients (350 GB), achieving 50% compute utilization on hardware with 1 second compute time requires 700 GB/s aggregate bandwidth, far exceeding single-link capacity and motivating sophisticated parallelism strategies.\n\n::: {.callout-warning}\n## Practical AllReduce Efficiency\n\nTheoretical bandwidth calculations assume ideal conditions. Production AllReduce operations achieve 60-80% of theoretical bandwidth due to multiple overheads:\n\n- **Startup latency**: Each ring stage incurs 5-20 μs fixed overhead per chunk\n- **Memory copy overhead**: CPU-GPU and GPU-NIC transfers add latency\n- **Protocol overhead**: NCCL/Gloo software stack processing\n- **Network contention**: Shared fabric with concurrent jobs reduces effective bandwidth\n\nWell-tuned systems on dedicated InfiniBand fabric achieve 75-85% efficiency. Many deployments, particularly those using RoCE or shared networks, achieve only 40-60%. Always benchmark actual collective performance rather than relying on theoretical link rates.\n:::\n\nGradient compression reduces bandwidth requirements at the cost of computation and potential accuracy impact. Top-k sparsification transmits only the largest gradient components, achieving 100-1000x compression ratios for some models. Error feedback mechanisms accumulate untransmitted gradients, maintaining convergence despite aggressive compression:\n\n$$\n\\tilde{g}_t = \\text{TopK}(g_t + e_{t-1}), \\quad e_t = g_t + e_{t-1} - \\tilde{g}_t\n$$\n\nPipeline parallelism communication patterns differ fundamentally from data parallelism. Rather than bulk AllReduce, pipeline stages exchange activation tensors between adjacent stages. The communication volume depends on activation size rather than parameter count, favoring models with small intermediate representations.\n\n#### Latency Analysis\n\nNetwork latency accumulates from multiple sources, each contributing to collective operation time:\n\n+---------------------+------------------+----------------------------------+\n| Source              | Typical Latency  | Mitigation                       |\n+=====================+==================+==================================+\n| Switch hop          | 100-400 ns       | Topology optimization            |\n+---------------------+------------------+----------------------------------+\n| Cable propagation   | 5 ns/m           | Compact layout                   |\n+---------------------+------------------+----------------------------------+\n| NIC processing      | 1-2 μs           | Hardware offload                 |\n+---------------------+------------------+----------------------------------+\n| NCCL software       | 5-20 μs          | Kernel fusion, persistent kernels|\n+---------------------+------------------+----------------------------------+\n| Memory copy         | Variable         | Zero-copy RDMA                   |\n+---------------------+------------------+----------------------------------+\n\nFor small messages, latency dominates over bandwidth. The crossover point where bandwidth becomes the limiting factor occurs at:\n\n$$\nM_{\\text{crossover}} = \\text{Latency} \\times \\text{Bandwidth}\n$$\n\nFor a system with 5 μs latency and 50 GB/s bandwidth, messages smaller than 250 KB are latency-bound. This motivates message aggregation in collective implementations, batching small gradient tensors to amortize latency overhead.\n\nCongestion control algorithms significantly impact performance under contention. Traditional TCP congestion control, designed for fairness across independent flows, performs poorly for synchronized ML traffic where all flows compete simultaneously. DCQCN (Data Center Quantized Congestion Notification) for RoCE and hardware-based credit flow control for InfiniBand provide faster response to congestion, reducing tail latency. NCCL implements topology-aware algorithms that schedule transfers to minimize contention, exploiting knowledge of collective patterns that general-purpose congestion control lacks. We examine these collective operation implementations in detail in @sec-communication.\n\n### Case Study: NVIDIA DGX SuperPOD Networking\n\nThe DGX SuperPOD architecture illustrates production-scale ML networking design. A SuperPOD combines multiple DGX systems into a unified training cluster, with networking designed to maintain near-linear scaling.\n\nThe baseline SuperPOD configuration connects 32 DGX H100 systems (256 GPUs) through a two-tier InfiniBand network. Each DGX H100 node provides eight NVIDIA ConnectX-7 adapters, one per GPU, each delivering 400 Gb/s (NDR) bandwidth. The leaf tier consists of 32 QM9700 switches, each connecting eight GPUs from a single node. The spine tier uses eight QM9700 switches, each connecting to all 32 leaf switches.\n\nThis configuration provides 1:1 bandwidth between any GPU pair, enabling efficient AllReduce regardless of job placement. The aggregate bisection bandwidth reaches 51.2 TB/s, supporting concurrent training of multiple large models.\n\nLarger SuperPOD deployments extend to 4096 GPUs across 512 DGX systems. At this scale, three-tier fat-tree topologies maintain non-blocking connectivity while managing cable plant complexity. The network includes 256 leaf switches, 128 spine switches, and 32 super-spine switches, totaling over 50,000 optical connections.\n\nRail-optimized variants reduce switch count by exploiting structured communication patterns. In tensor-parallel configurations, each GPU primarily communicates with corresponding GPUs in other nodes, a pattern well-served by dedicated rails with single-hop connectivity. The trade-off is reduced flexibility: jobs with different parallelism configurations may experience suboptimal placement.\n\nNetwork management in SuperPOD deployments integrates with cluster schedulers to enable topology-aware job placement. The Unified Fabric Manager monitors link health, detects failures, and can reroute traffic around failed components. Adaptive routing distributes load across multiple paths, improving utilization when traffic patterns create hotspots.\n\nThe SuperPOD design embodies the principles examined throughout this section: high-bandwidth interconnects (NDR InfiniBand), topology optimization (configurable fat-tree or rail), and integration with higher-level systems (NCCL, scheduler). These infrastructure choices directly determine training efficiency, making networking architecture a critical factor in ML system design.\n\n## Resource Management and Scheduling\n\nThe datacenter infrastructure and high-speed networks discussed in @sec-datacenter-architecture and @sec-networking-ml provide the physical foundation for large-scale ML. However, translating these resources into productive workloads requires sophisticated scheduling systems that balance utilization, fairness, and job completion time. This section examines the scheduling challenges unique to ML workloads and the systems designed to address them.\n\nML workloads present scheduling challenges distinct from traditional computing. Training jobs require coordinated access to multiple GPUs, often spanning nodes connected via the InfiniBand fabric discussed in the previous section. Inference workloads demand consistent latency while handling unpredictable traffic patterns. Both compete for the same accelerator resources, creating tension between throughput-oriented batch processing and latency-sensitive serving.\n\n### Why Distributed Scheduling is Hard\n\nBefore examining specific schedulers, understanding why cluster scheduling differs fundamentally from single-machine scheduling clarifies the design constraints these systems face.\n\n**Distributed Systems Challenges.** Cluster scheduling is not merely \"putting jobs on machines\" at larger scale. Several fundamental distributed systems problems make it intrinsically harder:\n\n1. **Partial failures**: A node can fail between allocation and job start. The scheduler may successfully allocate 32 GPUs across 4 nodes, only to have one node fail before the job launches. The remaining 24 GPUs sit idle while the scheduler detects the failure and re-allocates.\n\n2. **Network partitions**: The scheduler may lose connectivity to a subset of nodes while those nodes continue operating. From the scheduler's perspective, the nodes appear failed. From the nodes' perspective, jobs may still be running.\n\n3. **State inconsistency**: Resource state may differ between the scheduler's view and reality on individual nodes. A GPU the scheduler believes is free may still be cleaning up from a previous job.\n\n4. **Ordering without global time**: Without a global clock, determining whether allocation A happened before allocation B across different nodes requires careful protocol design. Two jobs may both believe they \"own\" the same GPU if the system is not carefully designed.\n\n**CAP Theorem Implications.** The CAP theorem applies directly to cluster scheduling: a scheduler cannot simultaneously provide perfect consistency (every allocation reflects true cluster state), availability (every request gets a response), and partition tolerance (system operates despite network failures).\n\nProduction schedulers make different trade-offs:\n\n- **Slurm** prioritizes consistency, blocking allocations during uncertainty\n- **Kubernetes** prioritizes availability, using eventual consistency with reconciliation loops\n- **Custom ML schedulers** often accept bounded inconsistency for performance\n\n**Failure Rates at Scale.** At scale, failure is normal operation, not exceptional. With 99.9% annual GPU reliability (typical for datacenter hardware), a 4096-GPU cluster experiences:\n\n$$\n\\text{Expected failures per day} = 4096 \\times \\frac{0.001}{365} \\approx 0.01 \\text{ GPU failures/day}\n$$\n\nMore realistically, including software failures, driver issues, and thermal events, production clusters see 1-4 failures per day per 1000 GPUs. A multi-week training run on 4096 GPUs will experience multiple failures. Infrastructure and scheduling systems must anticipate this reality.\n\n### Batch Scheduling for Training\n\nTraining large models requires allocating substantial GPU resources for extended periods. A 175B parameter model may require 1024 GPUs for weeks, while smaller experiments need only a few GPUs for hours. Scheduling systems must efficiently pack these diverse workloads while respecting resource constraints and fairness policies.\n\n#### Slurm for HPC-Style ML\n\nSlurm (Simple Linux Utility for Resource Management) dominates HPC environments and extends naturally to GPU-intensive ML training. Its partition-based architecture maps well to heterogeneous accelerator pools.\n\nA typical ML cluster configuration defines partitions by accelerator type and interconnect:\n\n+----------------------+------------+-----------------+------------------+\n| Partition            | GPUs/Node  | Interconnect    | Typical Use      |\n+======================+============+=================+==================+\n| dgx-a100             | 8 x A100   | NVLink + IB NDR | Large LLM training|\n+----------------------+------------+-----------------+------------------+\n| a100-pcie            | 4 x A100   | PCIe + IB HDR   | Medium training  |\n+----------------------+------------+-----------------+------------------+\n| inference            | 2 x A10G   | Ethernet        | Model serving    |\n+----------------------+------------+-----------------+------------------+\n| debug                | 1 x V100   | Ethernet        | Development      |\n+----------------------+------------+-----------------+------------------+\n\nGPU allocation strategies significantly impact utilization. The `--gres=gpu:N` flag requests N GPUs, but naive allocation can fragment nodes. Consider a 64-node cluster with 8 GPUs each (512 total). If jobs request 6 GPUs, each job wastes 2 GPUs per node, reducing effective capacity to 75%. Slurm's `SelectTypeParameters=CR_GPU` enables GPU-level scheduling, while `--gpus-per-node` ensures jobs receive full nodes when beneficial for NVLink communication.\n\nFair-share scheduling prevents any single user or project from monopolizing resources. The classic fair-share formula computes effective priority as:\n\n$$P_{effective} = P_{base} \\times \\frac{F_{target}}{F_{actual} + \\epsilon}$$\n\nwhere $F_{target}$ represents the user's allocated share and $F_{actual}$ their recent usage. This naturally deprioritizes heavy users while allowing burst access when resources are idle.\n\nPreemption policies enable high-priority jobs to reclaim resources from running workloads. For ML training, this requires checkpoint-aware preemption. Jobs receive SIGTERM with configurable grace periods (typically 60-300 seconds) to save checkpoints before SIGKILL. The `PreemptMode=REQUEUE` setting automatically restarts preempted jobs, while `GraceTime` controls the checkpoint window.\n\n#### Kubernetes for ML Workloads\n\nKubernetes has become the standard platform for ML infrastructure, particularly for organizations requiring unified management of training and serving workloads. Native Kubernetes lacks ML-aware scheduling, but extensions address this gap.\n\nGPU scheduling relies on device plugins that expose accelerators as extended resources. The NVIDIA device plugin registers GPUs with the kubelet, enabling pod specifications like:\n\n```yaml\nresources:\n  limits:\n    nvidia.com/gpu: 4\n```\n\nHowever, this binary allocation model wastes resources when workloads need less than a full GPU. Multi-Instance GPU (MIG) technology addresses this by partitioning A100 and H100 GPUs into isolated instances. An A100-80GB can be divided into configurations ranging from 7 small instances (10GB each) to 2 large instances (40GB each). The device plugin exposes MIG instances as separate resources:\n\n+----------------------+------------+-----------------+------------------+\n| MIG Profile          | GPU Memory | SM Count        | Typical Workload |\n+======================+============+=================+==================+\n| 1g.10gb              | 10 GB      | 14 SMs          | Small inference  |\n+----------------------+------------+-----------------+------------------+\n| 2g.20gb              | 20 GB      | 28 SMs          | Medium inference |\n+----------------------+------------+-----------------+------------------+\n| 3g.40gb              | 40 GB      | 42 SMs          | Large inference  |\n+----------------------+------------+-----------------+------------------+\n| 7g.80gb              | 80 GB      | 98 SMs          | Training         |\n+----------------------+------------+-----------------+------------------+\n\nGang scheduling ensures distributed training jobs receive all requested resources simultaneously. Without gang scheduling, a job requesting 32 GPUs might receive 24 immediately while waiting indefinitely for the remaining 8, wasting the already-allocated resources. The Volcano batch scheduler and scheduler plugins like Coscheduling implement gang semantics through PodGroup abstractions. Jobs specify minimum member counts, and the scheduler delays placement until all pods can be scheduled together.\n\nPriority classes control preemption behavior. A typical hierarchy assigns training workloads medium priority, inference high priority, and development jobs low priority. The `preemptionPolicy: PreemptLowerPriority` setting enables inference pods to reclaim resources from training when latency SLOs are threatened, though organizations must balance this against training job completion times.\n\n#### Custom ML Schedulers\n\nResearch schedulers have demonstrated significant improvements over general-purpose systems by exploiting ML-specific characteristics.\n\n**Tiresias** observes that ML training jobs have predictable resource requirements after initial epochs. Rather than requiring users to estimate job duration (often inaccurate by 2-5x), Tiresias uses a two-dimensional attained service scheduler. Jobs accumulate \"service\" based on GPU-time consumed, with priority decreasing as service increases. A discretized version groups jobs into service bins, promoting short jobs without requiring duration estimates. Experiments show 40-60% reduction in average job completion time compared to FIFO scheduling.\n\n**Gandiva** exploits the iterative nature of deep learning. Training alternates between GPU-intensive forward/backward passes and CPU-intensive data loading. Gandiva time-slices GPU access at iteration boundaries, enabling higher utilization through oversubscription. A cluster with 100 GPUs might support 120 concurrent jobs if each spends 20% of time waiting for data. Gandiva also implements grow-shrink elasticity, automatically adjusting data parallelism degree based on resource availability.\n\n**Themis** addresses fairness in long-running ML workloads. Traditional fair-share treats all GPU-seconds equally, but ML jobs have diminishing returns as training progresses. Themis defines a finish-time fairness metric, allocating resources to minimize the maximum slowdown any job experiences relative to exclusive access. This approach benefits shorter jobs without excessive penalty to longer ones.\n\nLocality-aware scheduling recognizes that communication topology matters for distributed training. A 64-GPU job performs better on 8 nodes of 8 GPUs each than 16 nodes of 4 GPUs, due to higher NVLink bandwidth within nodes. Advanced schedulers consider the fat-tree topology discussed in @sec-networking-ml, preferring allocations that share fewer switch hops. Experiments show 15-30% training throughput improvement from topology-aware placement.\n\n### Online Serving Resource Management\n\nInference workloads require different scheduling strategies than training. Latency matters more than throughput, traffic fluctuates unpredictably, and resource requirements vary by model size and request characteristics.\n\n#### Autoscaling for Inference\n\nHorizontal Pod Autoscaling (HPA) adjusts replica counts based on metrics. Default CPU utilization targets (often 50-70%) poorly reflect GPU inference workloads. Effective ML autoscaling uses custom metrics:\n\n+----------------------+------------------+----------------------------------+\n| Metric               | Target Range     | Considerations                   |\n+======================+==================+==================================+\n| GPU utilization      | 60-80%           | Varies by model batch efficiency |\n+----------------------+------------------+----------------------------------+\n| Request queue depth  | 10-50 requests   | Prevents latency spikes          |\n+----------------------+------------------+----------------------------------+\n| P99 latency          | < SLO target     | Reactive, lags demand changes    |\n+----------------------+------------------+----------------------------------+\n| Pending tokens       | Model-specific   | LLM-specific, accounts for KV    |\n+----------------------+------------------+----------------------------------+\n\nVertical Pod Autoscaling (VPA) adjusts resource requests and limits for individual pods. For inference, VPA can right-size memory allocations based on observed usage. However, GPU resources cannot be vertically scaled without pod restart, limiting VPA's utility for accelerated workloads.\n\nLLM inference requires specialized scaling due to the key-value cache. A 70B parameter model serving long-context requests may require 80GB+ of GPU memory for KV cache alone, even with PagedAttention optimizations. Scaling decisions must account for both request rate and context length distribution.\n\n#### Resource Isolation\n\nNoisy neighbor problems occur when colocated workloads interfere with each other. On GPUs, interference manifests through shared memory bandwidth, L2 cache contention, and PCIe bottlenecks. MIG provides hardware isolation but at the cost of flexibility. Software approaches include careful placement policies and time-based resource contracts.\n\nGPU memory isolation prevents one model from consuming memory needed by another. Without explicit limits, a memory leak or unexpectedly large batch can crash colocated workloads. Container runtimes can enforce memory limits through CUDA MPS (Multi-Process Service), though this adds latency overhead of approximately 5-10 microseconds per kernel launch.\n\nCPU pinning assigns specific cores to inference pods, preventing scheduler migration that causes cache invalidation. For latency-sensitive workloads, isolating cores using `isolcpus` kernel parameters and `taskset` affinity removes OS scheduling jitter. Combined with NUMA-aware placement, this reduces P99 latency by 10-30% for sub-millisecond inference tasks.\n\n### Multi-Tenancy Considerations\n\nProduction ML platforms serve multiple teams with competing priorities. Quota systems balance guaranteed access against overall utilization, while security isolation protects sensitive models and data.\n\n#### Quota Management\n\nGPU quota allocation typically operates at the namespace or project level. A simple approach allocates fixed GPU counts, but this leads to underutilization when teams have variable workloads. Hierarchical quotas enable departmental limits with sub-team flexibility:\n\n$$Q_{effective} = \\min(Q_{team}, Q_{department} - \\sum_{other\\ teams} U_{allocated})$$\n\nFair-share across teams extends the single-user formula to organizational hierarchies. When aggregate demand exceeds capacity, each team receives resources proportional to their share allocation. Unused capacity borrows down the hierarchy, maximizing utilization while respecting priorities.\n\nBurst capacity handling enables teams to temporarily exceed quotas when resources are available. Overcommitment ratios of 1.2-1.5x are common, with admission controllers tracking actual versus requested resources. When contention occurs, jobs using burst capacity face preemption first.\n\n#### Security Isolation\n\nNamespace separation provides the fundamental isolation boundary in Kubernetes. Each team operates within dedicated namespaces with role-based access control (RBAC) limiting visibility to their own resources. Network policies extend isolation to the network layer, preventing cross-namespace communication except through explicitly permitted services.\n\nNetwork policies for ML workloads must balance isolation with distributed training requirements. A policy might allow all-to-all communication within a namespace (for ring-AllReduce) while blocking ingress from other namespaces. Egress policies prevent training jobs from accessing external networks, reducing data exfiltration risk.\n\nGPU virtualization options range from time-slicing (low isolation, high flexibility) to MIG (hardware isolation, fixed partitions) to full device passthrough (complete isolation, lowest utilization). The choice depends on workload sensitivity and trust boundaries. Multi-tenant inference platforms typically use MIG for isolation between tenants, while single-tenant training clusters favor device passthrough for maximum performance.\n\nThe scheduling and resource management infrastructure discussed here enables efficient use of the datacenter resources and networks from previous sections. Effective schedulers achieve 70-85% GPU utilization in production clusters, compared to 30-50% with naive approaches. This efficiency translates directly to cost: a 1000-GPU cluster at 80% utilization delivers the equivalent capacity of 1600 GPUs at 50% utilization. As organizations scale ML infrastructure, scheduling sophistication becomes a primary determinant of both cost efficiency and researcher productivity. These resource management capabilities also provide the foundation for the fault-tolerant systems discussed in @sec-fault-tolerance and the operational practices covered in @sec-ops-scale.\n\n## Total Cost of Ownership Analysis\n\nUnderstanding the complete financial picture of ML infrastructure requires moving beyond simple hardware acquisition costs to comprehensive Total Cost of Ownership (TCO) analysis. This section provides quantitative frameworks for evaluating infrastructure investments, comparing deployment strategies, and optimizing long-term operational efficiency.\n\n### Capital Expenditure Components\n\nCapital expenditure (CapEx) encompasses all upfront investments required to establish ML infrastructure. These costs are typically amortized over 3-5 years, though the rapid pace of GPU advancement often compresses effective useful life.\n\n#### Hardware Costs\n\nGPU and accelerator acquisition represents the dominant CapEx component for ML infrastructure. Current market pricing reflects both performance capabilities and supply constraints.\n\n+------------------+-------------+----------------+------------------+\n| System           | Base Cost   | Memory Config  | Cost per PFLOP   |\n+==================+=============+================+==================+\n| DGX H100         | ~$300,000   | 640 GB HBM3    | ~$75,000         |\n+------------------+-------------+----------------+------------------+\n| DGX B100*        | ~$450,000   | 1.4 TB HBM3e   | ~$25,000         |\n+------------------+-------------+----------------+------------------+\n| HGX H100 (8-way) | ~$250,000   | 640 GB HBM3    | ~$62,500         |\n+------------------+-------------+----------------+------------------+\n| TPU v5p Pod      | Variable    | 95 GB HBM      | ~$30,000         |\n+------------------+-------------+----------------+------------------+\n\n*B100 pricing is estimated. All prices reflect approximate 2024 market conditions and should be verified for current planning. GPU pricing fluctuates 20-40% based on supply constraints and generation transitions.\n\nServer and storage costs add substantial overhead beyond accelerators. A complete DGX H100 deployment requires NVMe storage ($15,000-30,000 per node), high-speed networking cards ($8,000-15,000), and rack infrastructure ($5,000-10,000). Storage architecture for large-scale training demands parallel file systems capable of sustaining the I/O bandwidth required by hundreds of GPUs, with enterprise solutions like Lustre or GPFS adding $500-1,000 per terabyte of high-performance capacity.\n\nNetworking equipment costs scale superlinearly with cluster size due to the hierarchical nature of high-bandwidth fabrics. A 256-GPU cluster using InfiniBand HDR requires approximately $800,000-1,200,000 in networking equipment.\n\n$$C_{\\text{network}} = N_{\\text{switches}} \\cdot P_{\\text{switch}} + N_{\\text{cables}} \\cdot P_{\\text{cable}} + N_{\\text{adapters}} \\cdot P_{\\text{adapter}}$$\n\nFor a 256-GPU deployment with 2:1 oversubscription:\n\n$$C_{\\text{network}} \\approx 32 \\times \\$15,000 + 512 \\times \\$800 + 256 \\times \\$3,000 \\approx \\$1,660,000$$\n\nRefresh cycle planning significantly impacts TCO calculations. GPU generations advance every 2-3 years with typical performance improvements of 2-3x per generation. Organizations must balance the benefits of newer hardware against the disruption costs of migration. A common strategy employs staggered refresh cycles, replacing 25-33% of infrastructure annually to maintain competitive capability while avoiding wholesale replacement costs.\n\n#### Facility Costs\n\nDatacenter construction costs range from $7-12 million per megawatt of IT capacity for purpose-built facilities. ML workloads, with their high power density requirements (30-50 kW per rack versus 5-10 kW for traditional compute), demand specialized cooling infrastructure that increases construction costs by 20-40%.\n\nPower infrastructure represents a substantial portion of facility investment. Electrical distribution systems including transformers, switchgear, uninterruptible power supplies (UPS), and power distribution units (PDUs) typically cost $2-4 million per megawatt. Redundancy requirements (N+1 or 2N configurations) can double these costs for mission-critical deployments.\n\nCooling systems for high-density ML infrastructure increasingly require liquid cooling solutions. Direct-to-chip liquid cooling adds $50,000-100,000 per rack in capital costs but enables the power densities required for modern GPU configurations. The DGX H100 systems referenced in our datacenter architecture discussion require liquid cooling for sustained operation, representing a non-optional facility cost.\n\n### Operational Expenditure Components\n\nOperational expenditure (OpEx) captures ongoing costs that accumulate throughout infrastructure lifetime. For ML systems, power costs and specialized staffing dominate this category.\n\n#### Power Costs\n\nElectricity represents the largest operational cost for ML infrastructure. Power costs vary dramatically by geography, with industrial rates ranging from $0.04/kWh in regions with abundant hydroelectric power to $0.20/kWh in constrained markets.\n\nThe total power cost calculation must account for PUE overhead. As established in our datacenter architecture discussion, hyperscale facilities achieve PUE values of 1.1-1.2, meaning 10-20% additional power supports cooling and infrastructure. The annual power cost for a single DGX H100 system can be calculated as:\n\n$$C_{\\text{power}} = P_{\\text{system}} \\times \\text{PUE} \\times H_{\\text{annual}} \\times R_{\\text{electricity}} \\times U$$\n\nwhere $P_{\\text{system}}$ is system power (10.2 kW for DGX H100), $H_{\\text{annual}}$ is hours per year (8,760), $R_{\\text{electricity}}$ is the electricity rate, and $U$ is utilization factor.\n\nFor a DGX H100 at 80% utilization with $0.08/kWh electricity and 1.15 PUE:\n\n$$C_{\\text{power}} = 10.2 \\times 1.15 \\times 8,760 \\times 0.08 \\times 0.80 \\approx \\$6,600 \\text{ annually}$$\n\nElectricity pricing models significantly impact operational costs. Time-of-use pricing creates opportunities for training workload scheduling during off-peak hours (typically nights and weekends), potentially reducing power costs by 20-40%. Demand charges, which price peak power consumption, incentivize workload smoothing to avoid utilization spikes.\n\nRenewable energy considerations extend beyond environmental responsibility to economic optimization. Power Purchase Agreements (PPAs) for renewable energy often provide long-term price stability, hedging against electricity market volatility. Many organizations target 100% renewable energy matching through a combination of on-site generation, PPAs, and Renewable Energy Certificates (RECs). Environmental implications of energy choices are examined comprehensively in @sec-sustainable-ai.\n\n#### Staffing and Operations\n\nML infrastructure requires specialized operational expertise across multiple domains. Staffing costs often represent 15-25% of total operational expenditure for well-run facilities.\n\nHardware operations teams manage physical infrastructure including installation, maintenance, and failure response. For clusters of 500+ GPUs, dedicated hardware technicians are essential, with typical ratios of 1 technician per 200-400 GPUs depending on hardware heterogeneity and SLA requirements.\n\nSoftware platform teams maintain the scheduling systems, container infrastructure, and ML frameworks that enable productive use of hardware resources. These roles command premium compensation due to the specialized intersection of systems engineering and ML expertise required.\n\nUtilization monitoring represents both a staffing function and a key lever for TCO optimization. Continuous monitoring of GPU utilization, memory bandwidth, and job efficiency enables identification of optimization opportunities. Organizations achieving 70%+ sustained GPU utilization versus the more common 30-50% effectively halve their per-computation infrastructure costs.\n\n### Build vs. Buy Analysis\n\nThe fundamental infrastructure decision is whether to operate private infrastructure or consume cloud capacity. This choice involves complex trade-offs that depend on workload characteristics, scale, and organizational capabilities.\n\n#### Cloud vs. On-Premises Trade-offs\n\nCloud computing offers compelling advantages for specific use cases. Variable workloads with unpredictable demand benefit from cloud elasticity, avoiding stranded capacity during low-demand periods. Experimentation and research phases, where hardware requirements remain uncertain, benefit from the ability to test different configurations without capital commitment. Geographic distribution requirements for inference serving often favor cloud deployment due to the substantial investment required for multi-region presence.\n\nOn-premises infrastructure wins economically under sustained high utilization. The break-even analysis requires comparing amortized CapEx plus OpEx against equivalent cloud costs:\n\n$$\\text{Break-even utilization} = \\frac{C_{\\text{cloud}} \\times H_{\\text{annual}}}{\\frac{C_{\\text{capex}}}{Y_{\\text{amortization}}} + C_{\\text{opex}}}$$\n\nConsider a DGX H100 system with $300,000 CapEx, 3-year amortization, and $25,000 annual OpEx (power, maintenance, proportional staff). Cloud equivalent (8x H100 instance at ~$25/hour):\n\n$$\\text{Break-even} = \\frac{25 \\times 8,760}{\\frac{300,000}{3} + 25,000} = \\frac{219,000}{125,000} \\approx 1.75$$\n\nThis calculation suggests on-premises becomes favorable when utilization exceeds approximately 57% (1/1.75). In practice, organizations report break-even utilization thresholds of 40-60% depending on specific cloud pricing and operational efficiency.\n\n+-------------------+----------------------+----------------------+\n| Factor            | Favors Cloud         | Favors On-Premises   |\n+===================+======================+======================+\n| Utilization       | <40% average         | >60% sustained       |\n+-------------------+----------------------+----------------------+\n| Workload pattern  | Variable, bursty     | Steady, predictable  |\n+-------------------+----------------------+----------------------+\n| Data volume       | Moderate             | Petabyte-scale       |\n+-------------------+----------------------+----------------------+\n| Time horizon      | <2 years             | >3 years             |\n+-------------------+----------------------+----------------------+\n| Team capability   | Limited ops staff    | Strong infrastructure|\n+-------------------+----------------------+----------------------+\n\nHybrid strategies combine cloud burst capacity with on-premises baseline infrastructure. Organizations maintain on-premises systems sized for typical load (e.g., 60th percentile demand) while using cloud for peak periods. This approach captures most on-premises economic benefits while retaining cloud flexibility.\n\n#### Reserved Capacity vs. Spot Instances\n\nCloud providers offer commitment discount programs that substantially reduce effective pricing. Reserved instances with 1-year commitments typically offer 30-40% discounts, while 3-year commitments reach 50-60% discounts relative to on-demand pricing. These discounts shift cloud economics but introduce utilization risk similar to on-premises ownership.\n\nSpot instance strategies enable dramatic cost reduction (60-80% below on-demand) for fault-tolerant training workloads. Effective spot utilization requires:\n\n1. **Checkpoint integration**: Training frameworks must save state frequently enough that spot interruption costs remain acceptable. Modern distributed training checkpoints every 10-30 minutes, limiting maximum lost computation.\n\n2. **Fallback mechanisms**: Automated job migration to alternative instance types or regions when spot capacity becomes unavailable.\n\n3. **Heterogeneous training**: Frameworks capable of operating across mixed instance types to maximize spot availability.\n\nThe effective spot discount must account for interruption overhead:\n\n$$C_{\\text{effective}} = C_{\\text{spot}} \\times (1 + R_{\\text{interrupt}} \\times T_{\\text{recovery}})$$\n\nwhere $R_{\\text{interrupt}}$ is the hourly interruption rate and $T_{\\text{recovery}}$ is recovery time as a fraction of checkpoint interval. With 5% hourly interruption rate and 10-minute recovery on 30-minute checkpoints:\n\n$$C_{\\text{effective}} = 0.30 \\times C_{\\text{ondemand}} \\times (1 + 0.05 \\times 0.33) \\approx 0.31 \\times C_{\\text{ondemand}}$$\n\nEven accounting for interruption overhead, spot instances provide compelling economics for training workloads with proper checkpoint infrastructure.\n\n### Comprehensive TCO Model\n\nA complete TCO model integrates capital and operational components across the infrastructure lifetime:\n\n$$\\text{TCO} = \\sum_{t=1}^{Y} \\frac{C_{\\text{capex}}^{(t)} + C_{\\text{opex}}^{(t)}}{(1+r)^t}$$\n\nwhere $r$ is the discount rate reflecting cost of capital. For a 256-GPU cluster over 4 years:\n\n| Component           | Year 1      | Year 2      | Year 3      | Year 4      |\n|---------------------|-------------|-------------|-------------|-------------|\n| Hardware CapEx      | $9,600,000  | $0          | $0          | $3,200,000  |\n| Network CapEx       | $1,660,000  | $0          | $0          | $0          |\n| Power (at 70% util) | $1,690,000  | $1,690,000  | $1,690,000  | $1,690,000  |\n| Maintenance         | $480,000    | $576,000    | $691,000    | $829,000    |\n| Staff (allocated)   | $800,000    | $840,000    | $882,000    | $926,000    |\n| **Annual Total**    | $14,230,000 | $3,106,000  | $3,263,000  | $6,645,000  |\n\nThe NPV at 8% discount rate equals approximately $24.1 million, yielding a 4-year cost per GPU-hour of $4.30 at 70% utilization. This compares favorably to cloud A100 pricing of $3-4/hour only when accounting for the H100's 3x performance advantage, yielding effective cost per computation approximately 40% below cloud alternatives at this utilization level.\n\nPower cost sensitivity analysis reveals the importance of electricity pricing in deployment decisions. A $0.04/kWh difference in electricity rates shifts the 4-year TCO by approximately $2.7 million for a 256-GPU cluster, potentially changing the optimal deployment strategy. Organizations with access to low-cost renewable energy enjoy structural cost advantages that compound over multi-year infrastructure investments.\n\n## Case Studies\n\nThe infrastructure patterns examined in previous sections combine in different configurations depending on workload characteristics and organizational constraints. Four production deployments illustrate how datacenter architecture, networking, and resource management decisions interact to enable distinct ML workloads. Each case study represents a different point in the design space: GPU-centric dense training, TPU-based transformer optimization, hybrid CPU-GPU recommendation serving, and custom silicon for domain-specific acceleration.\n\n### NVIDIA DGX SuperPOD Architecture\n\nThe DGX SuperPOD represents NVIDIA's reference architecture for large-scale training, combining the dense GPU packaging of DGX systems with purpose-built networking. While the previous section examined SuperPOD networking topology, this case study addresses the complete system architecture including physical deployment, management infrastructure, and operational characteristics.\n\n#### Physical Layout and Cooling Integration\n\nA production SuperPOD deployment with 512 DGX H100 systems (4096 GPUs) occupies approximately 2000 square meters of datacenter floor space. The layout follows a pod-based organization where groups of 32 DGX systems share common power and cooling infrastructure. Each pod dissipates over 300 kW, requiring direct liquid cooling loops with facility-level heat exchangers.\n\nThe cooling architecture uses a closed-loop system with water temperature maintained at 35-45C entering the cold plates. Unlike traditional datacenter cooling that targets low air temperatures, warm-water cooling improves efficiency by enabling free cooling in moderate climates. Heat removed from GPU cold plates transfers to building cooling towers without mechanical refrigeration for ambient temperatures below 25C.\n\nPower distribution follows the N+1 redundancy model at the pod level, with each DGX system receiving dual power feeds. A complete SuperPOD installation requires 5-7 MW of utility power including cooling overhead, corresponding to PUE values of 1.2-1.3 for liquid-cooled deployments.\n\n#### Management Plane Architecture\n\nSuperPOD management integrates multiple control systems spanning hardware, networking, and workload orchestration. Base Controller Manager (BCM) provides hardware-level management including firmware updates, health monitoring, and out-of-band access. The Unified Fabric Manager coordinates InfiniBand network configuration, adaptive routing policies, and link health monitoring.\n\nAt the workload level, SuperPOD deployments typically integrate with either Slurm or Kubernetes for job scheduling. The NVIDIA GPU Operator handles GPU driver installation, monitoring integration, and device plugin management for Kubernetes environments. Slurm configurations use GRES scheduling with topology-aware placement to ensure jobs receive contiguous GPU allocations that minimize inter-node communication.\n\nStorage integration varies by deployment, but reference architectures include NVIDIA's GPUDirect Storage for direct data paths between NVMe storage and GPU memory. A typical SuperPOD includes 30-50 PB of high-performance storage providing 200+ GB/s aggregate throughput, staging training data close to compute.\n\n### Google TPU Pod Infrastructure\n\nGoogle's TPU pods represent an alternative architectural philosophy: vertically integrated accelerators designed specifically for transformer training, with interconnect capabilities built into the chip rather than added as external networking.\n\n#### TPU v4 Pod Architecture\n\nA TPU v4 pod contains 4096 TPU chips arranged in a 3D torus topology. Each chip provides approximately 275 TFLOPS of bfloat16 compute with 32 GB of HBM2e memory, yielding aggregate pod capacity of 1.1 exaFLOPS and 128 TB of memory. The power envelope for a complete pod is approximately 4-5 MW, competitive with GPU-based systems at similar compute density.\n\nThe physical packaging differs fundamentally from GPU systems. TPU chips mount in trays of 4, with trays assembled into racks of 64 chips each. Sixty-four racks form the complete pod, arranged in a cube topology that matches the 3D torus interconnect structure. Cooling uses rear-door heat exchangers with facility water, maintaining chip temperatures below 85C under sustained load.\n\n#### Inter-Chip Interconnect Topology\n\nThe ICI (Inter-Chip Interconnect) fabric provides direct chip-to-chip connectivity without external switches. Each TPU v4 chip has six ICI links at 100 GB/s each, enabling 3D torus connectivity:\n\n$$\n\\text{Bisection Bandwidth} = 2 \\times \\sqrt[3]{N} \\times B_{\\text{link}} \\times N/2\n$$\n\nFor N=4096 chips with 100 GB/s links, the torus bisection bandwidth reaches approximately 32 TB/s. While lower than fat-tree alternatives, the consistent latency characteristics of torus topology benefit the regular communication patterns of transformer training.\n\nThe topology choice optimizes for AllReduce patterns where each chip communicates with neighbors rather than arbitrary endpoints. For a model using 3D parallelism with 4 tensor-parallel chips, 16 pipeline stages, and 64-way data parallelism, the workload maps naturally onto a 4x16x64 slice of the pod topology.\n\n#### Software Stack Integration\n\nTPU software centers on JAX and XLA, with pjit (partitioned JIT compilation) managing distributed execution. XLA compiles high-level model descriptions to TPU-specific operations, automatically inserting communication collectives based on partition specifications. This approach differs from the explicit communication programming required for GPU clusters.\n\nMultislice training extends beyond single pods by connecting multiple TPU slices via datacenter network. A PaLM-scale training run might utilize four TPU v4 pods (16,384 chips) with cross-slice communication at lower bandwidth than intra-slice ICI. The software stack handles this hierarchy transparently, using different collective algorithms for intra-slice versus inter-slice operations.\n\n### Meta Recommendation Infrastructure\n\nMeta's recommendation systems illustrate infrastructure optimized for a fundamentally different workload pattern: models combining massive embedding tables with relatively modest dense computation. This architecture serves billions of daily recommendation queries across products including Facebook Feed, Instagram, and Reels.\n\n#### CPU-GPU Hybrid Architecture\n\nRecommendation models like DLRM (Deep Learning Recommendation Model) partition naturally between embedding operations and dense neural network computation. Embedding tables for production systems can exceed 10 TB, far exceeding GPU memory capacity. The hybrid architecture addresses this by placing embeddings in CPU DRAM while dense layers execute on GPUs.\n\nA production recommendation training node combines multiple CPUs totaling 2-4 TB of DRAM with 8 GPUs for dense computation. The CPUs handle embedding lookups, concatenation, and feature preprocessing. Resulting feature vectors transfer to GPUs via PCIe for the dense forward and backward passes. Gradient updates for embeddings return to CPU memory via the same path.\n\nThis architecture requires careful balancing. The ratio of embedding lookups to dense computation determines optimal CPU-to-GPU allocation. For Meta's workloads, approximately 4:1 CPU socket to GPU ratios provide balanced utilization, though this varies by model architecture.\n\n#### Embedding Table Serving at Scale\n\nInference architecture differs from training by emphasizing latency over throughput. Production serving distributes embedding tables across a fleet of CPU-based servers using consistent hashing for shard assignment. A single recommendation query may access hundreds of embedding shards, requiring parallel lookups that complete within the 50-100 ms latency budget.\n\nThe embedding serving tier operates separately from the dense model serving tier. This separation enables independent scaling: embedding servers scale with table size and query rate, while dense model servers scale with compute requirements. Cross-tier communication uses low-latency RPC, typically completing in under 5 ms for local datacenter deployments.\n\nFeature stores cache frequently accessed embeddings and precomputed features, reducing embedding server load for popular items. A tiered caching architecture places hot embeddings in GPU memory (microsecond access), warm embeddings in CPU DRAM (sub-millisecond), and cold embeddings in distributed storage (milliseconds). Cache hit rates above 90% are typical for recommendation workloads due to power-law popularity distributions.\n\n#### Training and Serving Coordination\n\nThe separation between training and serving infrastructure creates coordination challenges for model updates. Meta's approach uses a staged rollout pipeline: models train on dedicated GPU clusters, export to serving format, deploy to staging clusters for validation, then gradually roll out to production serving. The complete pipeline from training completion to full production deployment spans hours to days depending on model criticality.\n\nTraining clusters optimize for throughput using large batch sizes and aggressive gradient accumulation. Serving clusters optimize for latency using quantized models, batched inference, and result caching. The different optimization targets justify separate infrastructure rather than shared clusters.\n\n### Tesla Dojo for Vision Training\n\nTesla's Dojo system represents the custom silicon approach to ML infrastructure: building purpose-designed chips and packaging for a specific workload rather than using general-purpose accelerators.\n\n#### Custom Silicon Architecture\n\nThe Dojo D1 chip provides 1024 custom-designed cores in a 645 mm2 die. Each core combines an 8-wide vector unit, 64-bit scalar unit, and 1.25 MB of SRAM, yielding approximately 22.6 TFLOPS of BF16 compute per chip. The design optimizes for convolutional and attention operations typical of vision models, with dataflow execution patterns that minimize memory traffic.\n\nTwenty-five D1 chips mount on a single training tile, connected via a 2D mesh interconnect providing 4 TB/s aggregate bandwidth. Six tiles combine into a system tray, and multiple trays assemble into a complete ExaPOD delivering over 1 exaFLOP of aggregate compute. The modular architecture enables deployments from single tiles (0.5 PFLOPS) to multi-ExaPOD installations.\n\n#### Wafer-Scale Considerations\n\nWhile Dojo uses conventional chip packaging, the architecture addresses similar challenges to wafer-scale integration: maximizing on-chip bandwidth while managing thermal and yield constraints. The 2D mesh topology within each tile provides nearest-neighbor bandwidth of 18 GB/s between chips, avoiding the bottlenecks of hierarchical topologies for spatially-local operations common in vision processing.\n\nPower density presents the primary challenge: a fully populated system tray dissipates over 100 kW in a compact form factor. Tesla's thermal solution uses direct liquid cooling with custom manifolds delivering coolant to each training tile. The aggressive cooling enables sustained operation at power densities exceeding traditional datacenter limits.\n\nYield management for custom silicon requires careful attention. Unlike commodity GPU purchases where defective units return to the vendor, custom chip production creates internal yield loss. Dojo's design includes redundant cores and interconnect paths, enabling graceful degradation when manufacturing defects occur. Production testing identifies defective units, and the software stack maps computation around unavailable resources.\n\n#### Training Video Data at Scale\n\nDojo's primary workload is training vision models on Tesla's fleet data: over 1 million video clips per day from vehicles worldwide. The data pipeline presents distinct challenges from text or image training. Video requires decompression, temporal alignment, sensor calibration, and often 3D scene reconstruction before training.\n\nThe preprocessing pipeline runs on CPU clusters adjacent to Dojo compute, staging prepared batches to high-speed storage. Storage bandwidth of 10+ GB/s per training tile ensures compute utilization despite the data-intensive nature of video processing. The complete system integrates 10 PB of flash storage providing over 100 GB/s aggregate throughput.\n\nThis infrastructure supports auto-labeling workflows where preliminary models identify scenarios of interest in raw video, generating training data for improved models. The closed-loop between deployment, data collection, and training enables rapid iteration cycles measured in days rather than weeks.\n\n---\n\nThese case studies demonstrate that production ML infrastructure defies one-size-fits-all solutions. DGX SuperPOD optimizes for flexible general-purpose training with emphasis on GPU density and high-bandwidth networking. TPU pods sacrifice flexibility for vertical integration that excels at transformer workloads. Meta's hybrid architecture addresses the embedding-heavy patterns unique to recommendation systems. Tesla's Dojo pursues custom silicon for domain-specific acceleration where scale justifies development costs. The choice among these approaches depends on workload characteristics, scale requirements, and organizational capabilities rather than any universal optimum. Understanding these trade-offs enables informed infrastructure decisions as models and training requirements continue to evolve. For implementation details of the distributed training algorithms that leverage these infrastructure platforms, see @sec-distributed-training. Network-level considerations for collective operations are examined in @sec-communication.\n\n## Fallacies and Pitfalls\n\nThe complexity of large-scale ML infrastructure creates numerous opportunities for costly miscalculations. These misconceptions often stem from oversimplified mental models that fail to account for the non-linear interactions between compute, networking, power, and cooling systems. Understanding these fallacies and pitfalls helps practitioners avoid expensive mistakes that can waste millions of dollars in infrastructure investment or months of delayed projects.\n\n**Fallacy:** _More GPUs always means faster training._\n\nThis intuition fails catastrophically beyond modest cluster sizes. Distributed training introduces communication overhead, and the relationship between GPU count and training speed is far from linear. For a 175B parameter model using data parallelism, each training step requires exchanging approximately 350 GB of gradient data (FP16). Ring AllReduce achieves near-optimal bandwidth utilization: each GPU sends and receives $2P \\cdot (N-1)/N$ bytes, approaching $2P$ as $N$ grows. With 64 GPUs connected via 400 Gbps (50 GB/s) InfiniBand, the communication time for 350 GB approaches:\n\n$$\nT_{\\text{comm}} = \\frac{2P}{B} = \\frac{700 \\text{ GB}}{50 \\text{ GB/s}} \\approx 14 \\text{ seconds}\n$$\n\nCritically, this communication time is bounded as $N$ increases. The scaling efficiency depends on the ratio of compute time to total time:\n\n$$\n\\text{Efficiency} = \\frac{T_{\\text{compute}}}{T_{\\text{compute}} + T_{\\text{comm}}}\n$$\n\nFor data parallelism, $T_{\\text{compute}}$ scales as $1/N$ while $T_{\\text{comm}}$ approaches a constant. When compute time per GPU drops below communication time, adding more GPUs yields diminishing returns. For a workload with 60 seconds single-GPU compute time, scaling to 64 GPUs reduces compute to ~1 second while communication remains ~14 seconds, yielding only 7% efficiency. Many organizations discover that their 512-GPU cluster trains models slower than a well-optimized 128-GPU deployment due to crossing this inflection point. The solution requires moving beyond naive data parallelism to hybrid parallelism strategies that minimize cross-node communication, topics explored in @sec-distributed-training.\n\n**Fallacy:** _Peak FLOPS determines training throughput._\n\nVendors advertise peak FLOPS prominently: the H100 delivers 990 TF of FP16 compute. But production training jobs typically achieve 30-50% of peak due to memory bandwidth limitations, communication overhead, and kernel efficiency. For attention-dominated transformer training, memory bandwidth, not compute, is the limiting factor.\n\nThe roofline model reveals this clearly: with arithmetic intensity below the ridge point (~290 FLOP/byte for H100), the accelerator is memory-bound. Most LLM training operates at 50-100 FLOP/byte, achieving only 170-340 TF effective throughput on H100, roughly 20-35% of peak. Comparing accelerators by peak FLOPS alone misleads: a lower-FLOPS accelerator with higher memory bandwidth may outperform a higher-FLOPS alternative for memory-bound workloads.\n\n**Fallacy:** _All ML infrastructure should be GPU-based._\n\nThe assumption that GPUs represent the optimal accelerator for all ML workloads ignores the fundamental architectural differences between model types. Recommendation systems, which drive the majority of inference cycles at companies like Meta and Google, exhibit a workload profile where embedding table lookups dominate compute time. These lookups are memory-bound operations that GPUs handle poorly since they require random access to terabytes of embedding tables that cannot fit in GPU HBM.\n\nMeta's production recommendation infrastructure uses a hybrid architecture where CPU clusters handle embedding lookups from DRAM-based feature stores while GPU clusters process the dense neural network layers. This split architecture achieves 3x better cost efficiency than GPU-only deployments for their workload. Similarly, preprocessing pipelines for training data, including decompression, tokenization, and augmentation, execute more efficiently on CPUs. A DGX H100 with only 2 Intel Xeon CPUs can become CPU-bottlenecked on data preprocessing, starving the 8 H100 GPUs that represent 95% of the system cost.\n\n**Pitfall:** _Ignoring power and cooling constraints during infrastructure planning._\n\nPower and cooling represent hard physical limits that cannot be resolved through software optimization. A single rack of 4 DGX H100 systems requires over 40 kW of power and generates equivalent thermal load. Many organizations plan GPU purchases based on compute requirements without verifying datacenter capacity, only to discover their facility cannot support the power density.\n\nThe consequences compound over time. Current-generation H100 GPUs consume 700W each, but next-generation B100 GPUs maintain similar power envelopes while delivering 2x compute. However, the industry trend toward higher-density deployments means power requirements per rack continue to increase. Organizations that build datacenters for 30 kW per rack today will face costly retrofits within 2-3 years as GPU density increases.\n\nThermal throttling presents an equally insidious challenge. When cooling systems cannot remove heat fast enough, GPUs reduce clock speeds to prevent damage. A cluster designed for 100% utilization may achieve only 70% sustained throughput due to thermal constraints. At $30 per GPU-hour for H100 instances, this represents $7.20 per hour per GPU in wasted capacity. For a 1000-GPU cluster running training jobs, thermal inefficiency costs over $170,000 per month.\n\n**Pitfall:** _Underestimating network requirements for distributed training._\n\nNetwork bandwidth is rarely the only consideration. Latency, topology, and software overhead combine to determine actual training throughput. A cluster with theoretical 400 Gbps InfiniBand connectivity may achieve only 300 Gbps effective throughput due to protocol overhead in NCCL, suboptimal job placement, and contention from concurrent training jobs.\n\nFat-tree topologies provide full bisection bandwidth at significant switch cost, while rail-optimized topologies reduce hardware requirements but constrain job placement flexibility. Choosing the wrong topology for a workload can halve training throughput. For tensor-parallel workloads where adjacent GPUs exchange activations frequently, rail-optimized networks with 2-hop maximum latency outperform fat-tree networks with 4-6 hops. For data-parallel workloads with AllReduce patterns, fat-tree's guaranteed bisection bandwidth prevents congestion-induced slowdowns.\n\nSoftware overhead adds latency that theoretical bandwidth calculations ignore. NCCL introduces 5-20 microseconds of software latency per collective operation. For small message sizes common in pipeline parallelism, this overhead dominates transfer time, reducing effective bandwidth by 50% or more. The crossover point where bandwidth dominates latency occurs at approximately 250 KB message size for typical InfiniBand configurations. Operators that fail to aggregate small tensors before collective operations waste substantial network capacity. These communication optimization strategies are examined in detail in @sec-communication.\n\n## Summary\n\nLarge-scale ML infrastructure represents the physical foundation upon which all distributed training and serving systems operate. The constraints examined in this chapter, from power delivery and cooling capacity to network topology and accelerator selection, ultimately determine what scale of ML systems an organization can build and operate effectively.\n\nInfrastructure design must match workload characteristics. LLM training demands GPU-dense configurations with high-bandwidth NVLink and InfiniBand connectivity, while recommendation systems require hybrid CPU-GPU architectures optimized for embedding table access. Vision workloads fall between these extremes, benefiting from GPU acceleration but tolerating moderate network bandwidth. Attempting to serve all workload types from homogeneous infrastructure wastes resources and constrains performance.\n\nTraining and serving have fundamentally different infrastructure requirements. Training workloads tolerate batch scheduling, require sustained high bandwidth for collective operations, and can checkpoint through failures. Serving workloads demand low-latency networking, consistent response times, and immediate failover capabilities. Organizations that attempt to share infrastructure between training and serving often compromise both workloads.\n\nTotal cost of ownership extends far beyond hardware acquisition. Power consumption for a 1000-GPU cluster can exceed $2 million annually at typical datacenter rates. Cooling infrastructure may cost more than the GPUs it supports. Operational overhead including monitoring, maintenance, and administration adds 20-30% to hardware costs over a three-year depreciation cycle. Cloud versus on-premises decisions depend critically on utilization rates, with break-even typically occurring around 40% sustained utilization.\n\nNetwork topology choices determine distributed training efficiency. The decision between fat-tree and rail-optimized topologies, InfiniBand and RoCE, 2-tier and 3-tier switching architectures shapes what parallelism strategies perform well on the resulting infrastructure. Topology decisions made during datacenter construction constrain training architectures for years afterward.\n\nDifferent model types require different infrastructure patterns. The SuperPOD architecture optimized for LLM training differs fundamentally from Meta's recommendation serving infrastructure or Tesla's Dojo system optimized for video processing. No single infrastructure design serves all ML workloads optimally, and organizations must either specialize their infrastructure or accept efficiency losses from generalization.\n\n::: {.callout-important title=\"Key Takeaways\"}\n\n* Power density and cooling capacity represent hard physical limits that constrain cluster design independent of budget\n* Network topology determines training efficiency, with fat-tree providing flexibility while rail-optimized reduces latency for structured communication patterns\n* Hybrid CPU-GPU architectures outperform GPU-only configurations for recommendation systems and other embedding-heavy workloads\n* Communication overhead limits scaling efficiency, with Amdahl's Law applying to gradient synchronization and collective operations\n* Total cost of ownership must include power, cooling, operations, and realistic utilization rates rather than theoretical peak performance\n\n:::\n\nThe infrastructure foundations established here enable the distributed training strategies in @sec-distributed-training, which examines how parallelism approaches map onto physical hardware. The communication patterns and collective operations detailed in @sec-communication depend directly on the network topologies and bandwidth characteristics discussed in this chapter. For production deployments, @sec-fault-tolerance addresses the reliability requirements that infrastructure must satisfy to support multi-week training runs across thousands of accelerators.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"infrastructure.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","infrastructure.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3,"title":"Large-Scale ML Infrastructure"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}