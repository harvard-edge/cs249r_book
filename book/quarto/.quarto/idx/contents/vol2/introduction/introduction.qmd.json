{"title":"<!--","markdown":{"yaml":{"bibliography":"introduction.bib"},"headingText":"<!--","containsRefs":false,"markdown":"\n\nVOLUME II EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY\n================================================================================\n\nPHILOSOPHY: This textbook teaches generalizable ML systems principles, NOT\n\"LLM infrastructure.\" Every technique should apply across model architectures.\nStudents who master these concepts can work on any production ML system.\n\nWHEN WRITING CONTENT, ENSURE EXAMPLES SPAN THESE MODEL TYPES:\n\n| Model Type          | Unique Systems Challenges                              |\n|---------------------|--------------------------------------------------------|\n| LLMs/Transformers   | KV cache memory, attention compute scaling, long       |\n|                     | context, autoregressive decoding latency               |\n| Recommendation      | Massive embedding tables (trillion+ params), feature   |\n|                     | lookup latency, real-time updates, CTR prediction      |\n| Vision (CNN/ViT)    | Data augmentation pipelines, batch size sensitivity,   |\n|                     | spatial locality, multi-resolution processing          |\n| Scientific/GNN      | Irregular compute patterns, graph partitioning,        |\n|                     | sparse operations, physics constraints                 |\n| Multimodal          | Cross-encoder coordination, modality-specific          |\n|                     | preprocessing, heterogeneous compute requirements      |\n| Speech/Audio        | Streaming inference, variable-length sequences,        |\n|                     | real-time latency constraints                          |\n\nCHAPTER-SPECIFIC GUIDANCE:\n\nDISTRIBUTED TRAINING (@sec-distributed-training):\n\n- Data parallelism: ResNet, BERT, recommendation models\n- Model parallelism: GPT-3, Megatron, DLRM embedding sharding\n- Pipeline parallelism: GPT, T5, large vision models\n- Include: DLRM has DIFFERENT parallelism needs (embedding-heavy vs compute-heavy)\n\nINFERENCE AT SCALE (@sec-inference-at-scale):\n\n- Batching strategies differ: LLMs (continuous batching) vs RecSys (feature lookup)\n- Latency profiles: recommendation <10ms, LLMs 100ms-seconds, vision 20-50ms\n- Include ensemble serving (multiple models in pipeline)\n- RecSys is the DOMINANT inference workload by volume at Meta, TikTok, Netflix\n\nCOMMUNICATION (@sec-communication):\n\n- AllReduce for dense gradients (vision, transformers)\n- AlltoAll for embedding lookups (recommendation)\n- Gradient compression benefits vary by model type\n\nSTORAGE (@sec-storage):\n\n- Feature stores: critical for RecSys, less relevant for LLMs\n- Checkpoint sizes: LLMs (TB), vision (GB), recommendation (embedding tables)\n- Data lakes: training data diversity matters\n\nFAULT TOLERANCE (@sec-fault-tolerance):\n\n- Checkpointing frequency depends on model size and iteration time\n- Recommendation systems: real-time feature stores need different recovery\n- Training vs serving fault tolerance requirements differ\n\nEDGE INTELLIGENCE (@sec-edge-intelligence):\n\n- On-device: vision (phones), speech (assistants), NLP (keyboards)\n- Federated learning works differently for different model types\n- Model compression: quantization effects vary by architecture\n\nQUANTITATIVE DIVERSITY CHECKLIST:\n\n- [ ] Does this section have examples from at least 2-3 model types?\n- [ ] Are performance numbers given for multiple architectures?\n- [ ] Would a RecSys engineer find this content applicable to their work?\n- [ ] Would a vision ML engineer find this content applicable?\n- [ ] Are we avoiding LLM-centric framing of general concepts?\n\nEXAMPLE OF GOOD FRAMING:\n  BAD:  \"Training GPT-3 on a single V100 would require 355 years\"\n  GOOD: \"Training frontier models—whether GPT-3 (175B params), ViT-22B\n         (vision), or DLRM (trillion-param embeddings)—would require\n         centuries on single devices, making distributed approaches essential.\"\n\nEXAMPLE OF INCLUSIVE CASE STUDIES:\n  - Meta DLRM: recommendation at scale\n  - Google BERT/T5: NLP training infrastructure\n  - Tesla Autopilot: vision model deployment\n  - Spotify: audio/recommendation hybrid\n  - TikTok: multimodal recommendation\n\n================================================================================\n-->\n\n# Introduction {#sec-vol2-introduction}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting advanced ML systems at scale, set on a crisp, clean white background. The image features interconnected data centers, distributed computing networks, and multiple deployment environments. Visual elements include server racks connected by glowing data streams, edge devices at the periphery, and governance frameworks represented as protective layers. Icons represent the three core themes: scale (expanding infrastructure), distribute (networked nodes), and govern (protective oversight). The style is clean, modern, and flat, suitable for a technical book._\n:::\n\n\\noindent\n![](images/png/cover_introduction.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why must we master the engineering principles that enable machine learning systems to operate reliably at massive scale, across distributed infrastructure, and under responsible governance?_\n\nThe systems that transform industries and affect billions of lives cannot run on individual machines or small clusters. Production ML systems operate at scales where the fundamental nature of engineering challenges changes: communication dominates computation, failures become routine rather than exceptional, and architectural decisions carry consequences that reach far beyond technical performance metrics. At this frontier, the ability to coordinate learning across thousands of machines, to serve predictions to hundreds of millions of users with consistent reliability, and to ensure these systems operate fairly and sustainably determines which capabilities remain laboratory demonstrations and which reshape how humanity solves problems. Volume I established how to build, optimize, and operate ML systems; this volume extends those foundations to the production scale where most consequential AI systems must operate, introducing the three imperatives of scale, distribution, and governance that define advanced ML systems engineering. Mastering these principles determines your ability to architect the infrastructure and establish the practices that enable transformative AI capabilities to reach the people and applications that need them most.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain why ML systems exhibit qualitatively different behaviors at production scale compared to single-machine systems, including the dominance of communication over computation and the transition from exceptional to routine failure\n\n- Analyze the historical evolution of ML compute requirements from AlexNet to frontier models, calculating the implications for infrastructure design\n\n- Compare synchronous versus asynchronous distributed training approaches using the CAP theorem framework to evaluate consistency-availability trade-offs\n\n- Differentiate between datacenter distribution and edge distribution challenges, identifying unique constraints for each deployment context\n\n- Classify ML security threats (model extraction, membership inference, adversarial examples) and explain why scale amplifies their economic attractiveness to attackers\n\n- Apply the AI Triad and Five-Pillar Framework from Volume I to reason about distributed systems challenges across data, algorithms, and infrastructure\n\n- Evaluate the textbook's five-part structure (Foundations of Scale, Distributed Training, Deployment at Scale, Production Concerns, Responsible AI at Scale) to select appropriate chapters for specific engineering challenges\n\n:::\n\n## The Scale Transformation {#sec-vol2-introduction-scale-transformation}\n\nThe history of machine learning is a history of scale. Each major capability leap has come not from algorithmic breakthroughs alone, but from the ability to apply computation at previously impossible scales. Understanding this progression reveals why systems engineering has become central to AI advancement.\n\nConsider the trajectory of compute requirements. AlexNet (2012) trained on two GTX 580 GPUs for approximately 5 to 6 days [@krizhevsky2012imagenet]. BERT (2018) required 64 TPU chips for 4 days, roughly 6,144 chip hours [@devlin2018bert]. GPT-3 (2020) consumed an estimated 3.14×10²³ FLOPS during training, requiring thousands of V100 GPUs running for weeks [@brown2020language]. PaLM (2022) trained on 6,144 TPU v4 chips for roughly 60 days, consuming approximately 10²⁴ FLOPS [@chowdhery2022palm]. GPT-4 (2023) reportedly trained on approximately 25,000 A100 GPUs over 90 to 100 days [@openai2023gpt4]. This progression represents approximately a 10 million fold increase in training compute over a single decade, from roughly 10¹⁸ FLOPS for AlexNet to 10²⁵ FLOPS for GPT-4.\n\n::: {.callout-example title=\"Training Compute Evolution\"}\n```\nModel           Year    GPUs/TPUs    Training Time    Estimated FLOPS\n─────────────────────────────────────────────────────────────────────\nAlexNet         2012    2 GPUs       5-6 days         ~10¹⁸\nBERT-Large      2018    64 TPUs      4 days           ~10²⁰\nGPT-3           2020    ~1000 GPUs   ~30 days         ~10²³\nPaLM            2022    6144 TPUs    ~60 days         ~10²⁴\nGPT-4           2023    ~25000 GPUs  ~100 days        ~10²⁵\n```\n:::\n\nThis exponential growth in compute requirements has transformed ML from a discipline where algorithms dominate to one where systems engineering determines success. A sophisticated algorithm that cannot scale often provides less practical value than a simpler algorithm deployed efficiently across scalable infrastructure.\n\nThe transition from single-machine to distributed training introduces qualitative changes in system behavior. On a single GPU, training proceeds deterministically: the same code, data, and random seed produce identical results. At the scale of thousands of GPUs, new phenomena emerge. Network partitions can split clusters into groups that train independently, causing model divergence[^fn-network-partition]. Stragglers, workers that process data slower than peers due to hardware variation or thermal throttling, can bottleneck entire training runs[^fn-straggler-problem]. Hardware failures that occur once per machine-year become daily events when operating 10,000 machines, meaning systems must checkpoint frequently enough that losing a day's progress becomes acceptable rather than catastrophic[^fn-failure-rates].\n\n[^fn-network-partition]: **Network Partition in Distributed Training**: When network failures isolate groups of workers, each group may continue training independently with different gradient updates, causing models to diverge. In synchronous distributed training, partitions typically halt progress until connectivity restores. In asynchronous training, partitions can cause split brain scenarios where different model versions evolve in parallel. Production systems implement partition detection through heartbeat mechanisms and quorum protocols. Google's approach with TPU pods uses dedicated high bandwidth interconnects (ICI at 4.5 TB/s bidirectional per chip) to minimize partition probability, while software level protocols detect and recover from rare partitions within minutes rather than allowing extended divergent training.\n\n[^fn-straggler-problem]: **The Straggler Problem**: In synchronous distributed training, all workers must complete their computation before proceeding to the next iteration. If one worker runs 10% slower due to thermal throttling, memory pressure, or background processes, the entire cluster waits, reducing effective utilization. At 1,000 workers, the probability of at least one straggler per iteration approaches certainty. Solutions include backup workers that redundantly compute slow partitions, gradient coding that tolerates missing results, bounded staleness that allows proceeding with partial results, and careful hardware provisioning that minimizes variation. Meta's research found that stragglers can reduce large scale training throughput by 20 to 30% without mitigation strategies.\n\n[^fn-failure-rates]: **Hardware Failure Rates at Scale**: Individual server components fail infrequently. Enterprise SSDs show annual failure rates of 0.5 to 2%; GPUs fail at roughly 1 to 2% annually under typical datacenter conditions, though rates can exceed 9% under intensive AI training workloads; memory DIMMs fail at 1 to 2% per year. These rates seem manageable until multiplied by scale. A cluster of 10,000 GPUs with 2% annual failure rate experiences 200 GPU failures per year, or roughly one every two days. A training run lasting 100 days will statistically encounter 50 or more GPU failures. Production systems implement automatic detection, workload migration, and checkpoint based recovery to handle failures as routine events rather than emergencies. Meta reported that during LLaMA training, they experienced hardware failures roughly every few hours, requiring automated recovery systems to maintain progress.\n\nThese scale-induced challenges explain why the largest AI organizations invest heavily in infrastructure. Meta's Research SuperCluster (RSC) contains 16,000 NVIDIA A100 GPUs connected by 200 Gb/s InfiniBand networking [@meta2022rsc]. Google's TPU v4 pods contain 4,096 chips with 1.1 exaFLOPS of aggregate compute capacity. Microsoft's Azure AI infrastructure spans multiple datacenters with tens of thousands of GPUs dedicated to AI workloads. These investments reflect that frontier AI capabilities require frontier infrastructure.\n\n## Why Scale Changes Everything {#sec-vol2-introduction-why-scale-changes}\n\nScale is not merely a larger version of small. Systems that work perfectly at modest scale exhibit qualitatively different behaviors at production scale. Understanding these transitions prepares you for the engineering challenges examined throughout this volume.\n\n### Communication Becomes Dominant\n\nAt small scale, computation dominates. Training a model on a single GPU spends most time performing matrix multiplications. Communication overhead, moving data between CPU and GPU memory, represents a small fraction of total time.\n\nAt large scale, communication often dominates. Distributed training requires synchronizing gradients across workers after each batch. For a model with 175 billion parameters using 32-bit gradients, each synchronization must transfer 700GB of data (175 billion parameters × 4 bytes per parameter). Using the ring all-reduce algorithm[^fn-all-reduce] across 1,000 workers connected by 200 Gb/s InfiniBand (25 GB/s), theoretical completion time for the full synchronization approaches 56 seconds (2 × 700 GB ÷ 25 GB/s). Practical implementations achieve 60-80% of theoretical bandwidth, making communication consume a significant fraction of total training time even with optimized networks.\n\n[^fn-all-reduce]: **Ring All-Reduce**: The dominant collective communication algorithm for distributed training gradient synchronization [@patarasuk2009bandwidth]. In ring all-reduce, N workers arrange logically in a ring. Each worker sends 1/N of its gradient to its neighbor, which adds the received gradient to its own before forwarding. After N minus 1 steps, each worker has the sum of all gradients for 1/N of parameters. A second ring pass distributes the complete sum to all workers. The algorithm achieves optimal bandwidth utilization. Total data transferred equals 2(N minus 1)/N times the gradient size, approaching 2× regardless of worker count. For 1,000 workers with 700GB of gradients and 200 Gb/s links, theoretical completion time is approximately 2 × 700GB / 200 Gb/s = 56 seconds for the full reduce scatter plus all gather. Practical implementations achieve 60 to 80% of theoretical bandwidth, making communication a significant fraction of large scale training time.\n\nThis ratio explains why distributed training systems optimize communication aggressively. Gradient compression reduces transfer volume by 10-100× at the cost of some accuracy. Overlapping communication with computation hides transfer latency during the next batch's forward pass. Hierarchical aggregation reduces cross-rack traffic by combining gradients locally first. These optimizations, unnecessary at small scale, become essential at production scale.\n\n::: {.callout-definition title=\"Communication-Computation Ratio\"}\n***Communication-Computation Ratio*** describes the relative time spent transferring data versus performing computation in distributed systems. A ratio of 1:1 means equal time on each; higher ratios indicate communication-bound workloads. Modern distributed training systems typically achieve ratios between 1:3 and 1:1, making communication optimization critical for efficiency. The ratio depends on model size (larger models have more gradients to synchronize), batch size (larger batches amortize communication over more computation), and network bandwidth (faster networks reduce communication time).\n:::\n\n### Failure Becomes Routine\n\nAt small scale, failure is exceptional. A well maintained server might run for years without hardware issues. Software bugs, once fixed, stay fixed. Administrators can manually investigate and remediate problems.\n\nAt large scale, failure becomes statistical certainty. With 10,000 GPUs, multiple failures occur weekly. With 100,000 concurrent user sessions, software edge cases that occur one in a million times happen hundreds of times daily. Manual intervention becomes impossible; systems must self heal. This transition requires architectural changes from the beginning. Small scale systems optimize for the common case and handle failures through manual recovery, while large scale systems embed failure handling into their core design:\n\n- **Checkpointing**: Saving model state frequently enough that losing hours of progress is acceptable when failures occur\n- **Redundancy**: Running extra workers that can absorb failed workers' tasks without restart\n- **Isolation**: Containing failures so that one component's crash does not cascade through the system\n- **Detection**: Monitoring that identifies failures within seconds\n- **Recovery**: Automated procedures that restore service without human intervention\n\n### Heterogeneity Emerges\n\nAt small scale, systems are homogeneous. A single GPU training job runs on one type of hardware with one software configuration. Behavior is predictable and reproducible.\n\nAt large scale, heterogeneity becomes unavoidable. A fleet of 10,000 GPUs contains multiple hardware generations purchased over years. Different racks have different thermal characteristics affecting clock speeds. Software updates roll out gradually, creating version skew. Network paths vary in latency and bandwidth.\n\nThis heterogeneity creates engineering challenges absent at small scale. Load balancing must account for hardware capability differences. Gradient aggregation must handle workers completing at different rates. Inference routing must direct requests to servers with appropriate model versions. Testing must verify behavior across the combinatorial explosion of configuration variants.\n\n## Why Distribution is Hard {#sec-vol2-introduction-why-distribution-hard}\n\nDistribution introduces challenges beyond those of scale. Coordinating computation across physically separated machines connected by finite-bandwidth, non-zero-latency networks creates fundamental constraints that no amount of engineering cleverness can eliminate.\n\n### The CAP Theorem Reality\n\nThe CAP theorem[^fn-cap-theorem] establishes that distributed systems can provide at most two of three properties: consistency (all nodes see the same data), availability (every request receives a response), and partition tolerance (the system continues operating despite network failures). Since network partitions can always occur, practical systems must choose between consistency and availability during partitions.\n\n[^fn-cap-theorem]: **CAP Theorem**: Proven by Eric Brewer and formalized by Gilbert and Lynch [@gilbert2002brewer], the CAP theorem states that a distributed data store cannot simultaneously provide more than two of Consistency (every read receives the most recent write), Availability (every request receives a non-error response), and Partition tolerance (the system continues operating despite network partitions). Since partitions are unavoidable in distributed systems, the practical choice is between CP (consistent but potentially unavailable during partitions) and AP (available but potentially inconsistent). Distributed ML systems make different choices. Synchronous training is CP (training halts during partitions to maintain model consistency), while asynchronous training is AP (training continues with potentially stale gradients). Understanding this trade-off informs architecture decisions throughout distributed ML systems.\n\nDistributed ML systems make different CAP trade-offs depending on their requirements. Synchronous distributed training chooses consistency: all workers see the same model state, but training halts if any worker becomes unreachable. Asynchronous training chooses availability: training continues even with stragglers or failures, but workers may operate on slightly stale model versions. Federated learning often chooses availability with eventual consistency: edge devices train locally and periodically synchronize, accepting temporary inconsistency for continuous operation.\n\n### Coordination Overhead\n\nDistributed systems require coordination that consumes resources. Every synchronization point introduces latency. Every consensus protocol requires network round-trips. Every distributed lock limits parallelism.\n\nConsider the overhead of distributed training synchronization. Each training iteration requires:\n\n1. Forward pass computation (parallelizable)\n2. Loss computation (local to each worker)\n3. Backward pass computation (parallelizable)\n4. Gradient aggregation (requires network communication)\n5. Parameter update (can parallelize with next iteration)\n\nSteps 1-3 and 5 parallelize efficiently. Step 4, gradient aggregation, requires global coordination. Even with optimized all-reduce algorithms and high-bandwidth networks, this coordination can consume a substantial fraction of total training time for large models [@shoeybi2019megatron]. This overhead is fundamental: no algorithm can aggregate globally distributed values without communication proportional to the data volume.\n\n### Edge Distribution Complexity\n\nDatacenter distribution is challenging but controlled. All machines run in managed facilities with reliable power, cooling, and networking. Administrators can access any machine for diagnosis and repair.\n\nEdge distribution amplifies every challenge. Billions of smartphones, IoT devices, and embedded systems operate in uncontrolled environments with unreliable connectivity, limited power, and heterogeneous capabilities. Google's Gboard keyboard runs on over 1 billion Android devices, each potentially participating in federated learning[^fn-federated-learning] to improve predictions [@hard2018federated]. This deployment context introduces unique constraints:\n\n- **Intermittent connectivity**: Devices may be reachable only when on WiFi and charging\n- **Heterogeneous hardware**: Model must run efficiently across devices spanning a 100× performance range\n- **Privacy requirements**: Raw data cannot leave devices, requiring on-device processing\n- **Update complexity**: Pushing model updates to billions of devices takes weeks\n- **Monitoring limitations**: Cannot install arbitrary diagnostics on user devices\n\nThese constraints require architectural approaches that differ from datacenter ML in essential ways. Federated learning aggregates model updates without collecting data. On-device inference optimizes for varied hardware capabilities. Differential privacy, which adds calibrated noise to protect individual data points while preserving aggregate statistical properties, provides mathematical guarantees about information leakage. These techniques, largely unnecessary for centralized ML, become essential for edge deployment.\n\n## Why Governance Matters at Scale {#sec-vol2-introduction-why-governance-matters}\n\nScale amplifies impact. A bug in a small system affects few users; a bug in a system serving billions affects society. This amplification creates governance requirements that small-scale systems can ignore.\n\n### Security Threats Intensify\n\nML systems face unique security threats beyond traditional software vulnerabilities[^fn-ml-security-threats]. Model extraction attacks can steal proprietary models through query access. Researchers demonstrated extracting functionally equivalent copies of production ML models using only API access [@tramer2016stealing]. Membership inference attacks can determine whether specific data was used in training, creating privacy violations from seemingly innocuous model access [@shokri2017membership]. Adversarial examples can cause misclassification with perturbations imperceptible to humans, demonstrated against production systems including Tesla Autopilot and content moderation systems [@goodfellow2014explaining].\n\n[^fn-ml-security-threats]: **ML-Specific Security Threats**: Traditional software security focuses on preventing unauthorized code execution and data access. ML systems face additional threats that exploit the learned behavior of models. Data poisoning attacks inject malicious training examples that cause targeted misbehavior. Researchers demonstrated that controlling 0.1% of training data can implant backdoors that cause misclassification on specific inputs. Model inversion attacks reconstruct training data from model access. Facial recognition models can leak enough information to reconstruct recognizable images of training individuals. Adversarial reprogramming hijacks models to perform unintended tasks through specially crafted inputs. These threats require defenses beyond traditional security including differential privacy, certified robustness, and continuous monitoring of model behavior in production.\n\nAt production scale, these threats become economically attractive to attackers. A model serving millions of users represents substantial intellectual property worth stealing. A model making consequential decisions (loans, hiring, content moderation) offers high-value manipulation targets. A model processing sensitive data (health records, financial information) provides valuable inference targets.\n\nDefending against these threats requires systematic approaches including access controls that limit query rates and patterns, output perturbation that provides differential privacy guarantees, adversarial training that improves robustness to perturbations, and monitoring that detects anomalous query patterns indicative of attacks. These defenses impose overhead unnecessary for small-scale systems but essential for production deployment.\n\n### Regulatory Requirements Emerge\n\nSystems operating at scale attract regulatory attention. The European Union's General Data Protection Regulation (GDPR) imposes obligations for systems processing EU residents' data, including the right to explanation for automated decisions. The EU AI Act establishes risk-based requirements for AI systems, with high-risk applications (healthcare, employment, law enforcement) requiring conformity assessments, human oversight, and accuracy documentation. Similar regulations exist or are developing in jurisdictions worldwide.\n\nMeeting these regulatory requirements demands technical capabilities:\n\n- **Audit trails**: Recording inputs, outputs, and model versions for every decision\n- **Explanation generation**: Producing human-interpretable justifications for model outputs\n- **Consent management**: Tracking and honoring user preferences for data usage\n- **Data deletion**: Removing specific users' data from training sets and retraining affected models\n- **Bias testing**: Evaluating model performance across protected demographic groups\n\nThese capabilities impose engineering costs absent for unregulated systems but mandatory for production deployment in regulated contexts.\n\n### Societal Impact Demands Responsibility\n\nBeyond legal compliance, systems affecting billions of users carry ethical obligations. Recommendation algorithms shape public discourse. Researchers have documented how engagement-optimizing systems can amplify misinformation and polarization [@ribeiro2020auditing]. Hiring algorithms affect employment opportunities. Amazon discontinued an AI recruiting tool that exhibited bias against women [@dastin2018amazon]. Content moderation systems determine what speech is visible. Errors can suppress legitimate expression or fail to remove harmful content. Responsible engineering practices must address these impacts:\n\n- **Fairness evaluation**: Testing for disparate impact across demographic groups before deployment\n- **Impact assessment**: Analyzing potential harms before launching new capabilities\n- **Human oversight**: Maintaining human review for high-stakes decisions\n- **Incident response**: Processes for rapidly addressing identified harms\n- **Transparency**: Documentation of system capabilities, limitations, and decision factors\n\n::: {.callout-definition title=\"Responsible AI\"}\n***Responsible AI*** encompasses the practices, frameworks, and technical approaches that ensure ML systems operate in ways that are fair, transparent, accountable, and beneficial to society. Responsible AI addresses the ethical implications of automated decision making, the potential for algorithmic bias, the environmental impact of large scale computation, and the governance structures needed to maintain human oversight over consequential systems. Unlike traditional software quality assurance focused on correctness and performance, responsible AI evaluates systems against societal values and human rights considerations.\n:::\n\n## Bridging from Volume I {#sec-vol2-introduction-bridging}\n\nVolume I established the foundations that this textbook extends. If you are beginning here, this section provides essential context. If you completed Volume I, consider this a brief reminder before we proceed to advanced topics.\n\nVolume I introduced the AI Triad as the organizing framework for understanding ML systems. Every machine learning system comprises three interdependent components: data that guides behavior, algorithms that learn patterns, and computational infrastructure that enables both training and inference. These components exist in dynamic tension. Larger models require more data and compute. Larger datasets enable more sophisticated models but demand storage and processing infrastructure. At the scales examined in this volume, these interdependencies intensify. Distributed training requires coordinating the AI Triad's components across thousands of machines rather than optimizing them on a single system.\n\nThe Five-Pillar Framework structures the ML systems engineering discipline into interconnected domains: data engineering establishes pipelines for collecting, processing, and serving training and inference data; model development encompasses architecture design, training procedures, and validation methodologies; optimization techniques compress and accelerate models for deployment constraints; deployment infrastructure spans cloud platforms, edge devices, and embedded systems; and operations practices ensure systems remain reliable, secure, and effective throughout their lifecycle. This textbook extends each pillar to production scale, where the engineering challenges multiply.\n\nThe Six Systems Engineering Principles provide guidance for design decisions across all five pillars:\n\n1. *Measure Everything*: At scale, measurement itself becomes a systems challenge requiring distributed monitoring infrastructure\n2. *Design for 10x Scale*: Production deployment reveals whether 10× design was adequate or optimistic\n3. *Optimize the Bottleneck*: Scale shifts bottlenecks from compute to communication to coordination\n4. *Plan for Failure*: At scale, failure is not exceptional but routine\n5. *Design Cost-Consciously*: Scale makes efficiency improvements worth millions of dollars\n6. *Co-Design for Hardware*: Distributed hardware introduces network topology and storage hierarchy as co-design considerations\n\nVolume I taught you to build, optimize, and operate ML systems. This textbook teaches you to scale, distribute, and govern them.\n\n## The Structure of This Textbook {#sec-vol2-introduction-structure}\n\nThis textbook organizes around the three imperatives, progressing from infrastructure foundations through distribution techniques to governance practices. @tbl-vol2-structure summarizes this five-part structure.\n\n+--------------------------------+---------------------------------+----------------------------------------+\n| **Part**                       | **Theme**                       | **Key Chapters**                       |\n+:===============================+:================================+:=======================================+\n| **I: Foundations of Scale**    | **Scale**: Physical and data    | Infrastructure, Storage                |\n|                                | foundations for distributed ML  |                                        |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **II: Distributed Training**   | **Distribute**: Training models | Distributed Training, Communication,   |\n|                                | across multiple machines        | Fault Tolerance                        |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **III: Deployment at Scale**   | **Deploy**: Serving predictions | Inference at Scale, Edge Intelligence, |\n|                                | to millions of users            | ML Operations at Scale                 |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **IV: Production Concerns**    | **Operate**: Running systems    | Privacy & Security, Robust AI,         |\n|                                | safely and sustainably          | Sustainable AI                         |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **V: Responsible AI at Scale** | **Govern**: Ensuring beneficial | Responsible AI, AI for Good,           |\n|                                | societal impact                 | AGI Systems, Conclusion                |\n+--------------------------------+---------------------------------+----------------------------------------+\n\n: The five parts progress from infrastructure foundations through distributed training and deployment to production concerns and responsible governance. {#tbl-vol2-structure}\n\n### Part I: Foundations of Scale\n\nBefore systems can scale, they require infrastructure designed for scale.\n\n**Infrastructure** examines how datacenters, accelerators, and orchestration systems enable large-scale ML. You will understand the hardware and software stack that makes distributed ML possible, from GPU clusters to high-bandwidth networks, resource scheduling, and container orchestration.\n\n**Storage Systems** addresses data infrastructure at scale. Training datasets for modern models exceed any single storage system, requiring distributed architectures optimized for ML access patterns. You will understand storage hierarchies, data lakes, feature stores, and artifact management that enable efficient data serving.\n\n### Part II: Distributed Training\n\nWith infrastructure foundations established, distribution techniques enable training across multiple machines.\n\n**Distributed Training** develops techniques for training models across devices and machines. Data parallelism, model parallelism, and pipeline parallelism each address different constraints. You will understand when each applies, how they combine, and what synchronization and consistency they require.\n\n**Communication** analyzes the collective operations that coordinate distributed training. AllReduce, AllGather, and other primitives dominate training communication. You will understand algorithms, topologies, and optimization techniques that minimize communication overhead and maximize bandwidth utilization.\n\n**Fault Tolerance** ensures distributed systems continue operating despite failures. At production scale, failures occur daily. Checkpointing, redundancy, and recovery procedures enable training to continue despite inevitable component failures.\n\n### Part III: Deployment at Scale\n\nTraining produces models; deployment delivers value to users.\n\n**Inference at Scale** examines serving systems that deliver predictions with low latency and high throughput. Request routing, load balancing, autoscaling, and geographic distribution enable production inference to meet demanding performance requirements.\n\n**Edge Intelligence** extends ML to resource-constrained devices at the network edge. Model compression, runtime optimization, and edge-cloud coordination enable deployment where centralized inference is infeasible.\n\n**ML Operations at Scale** encompasses practices that maintain large ML systems in production. Monitoring, debugging, deployment pipelines, and incident response adapt for ML-specific requirements at production scale.\n\n### Part IV: Production Concerns\n\nDistribution creates operational challenges that require systematic approaches.\n\n**Privacy and Security** addresses threats specific to ML systems. Model extraction, membership inference, adversarial examples, and data poisoning require defenses including differential privacy, secure computation, and adversarial training.\n\n**Robust AI** ensures reliable operation under uncertainty. Distribution shift, out-of-distribution inputs, and novel situations require systems that recognize the limits of their competence and respond appropriately.\n\n**Sustainable AI** addresses environmental impact. Efficient algorithms, appropriate model sizing, and renewable energy sourcing minimize the environmental footprint of large-scale ML.\n\n### Part V: Responsible AI at Scale\n\nTechnical excellence is insufficient for systems affecting human lives at scale.\n\n**Responsible AI** addresses fairness, transparency, and accountability. Systems must operate equitably across populations, enable stakeholder understanding, and ensure harms can be identified and remediated.\n\n**AI for Good** demonstrates how ML systems address societal challenges. Applications in healthcare, climate, education, and accessibility illustrate how systems engineering principles enable beneficial impact.\n\n**AGI Systems** examines emerging directions including foundation models, compound AI systems, and novel computing paradigms. Understanding these trajectories prepares you for continued learning as the field evolves.\n\nFor detailed guidance on reading paths, learning outcomes, prerequisite knowledge, and navigation strategies for both volumes, refer to the [About](../../frontmatter/about/about.qmd) section.\n\n## The Journey Ahead {#sec-vol2-introduction-journey-ahead}\n\nVolume I concluded with six systems engineering principles and a vision of building ML systems that matter. This volume extends that vision to the scale at which most consequential ML systems operate.\n\nThe transition from building systems that work to building systems that scale, distribute, and govern responsibly represents significant professional growth. The ML systems that will define this era require precisely these capabilities: foundation models serving hundreds of millions of users, edge deployments spanning billions of devices, and AI systems making consequential decisions about human lives.\n\nThroughout this volume, you will learn to architect infrastructure that processes petabytes of training data across tens of thousands of accelerators. You will design inference systems that serve billions of predictions with consistent low latency. You will implement security, privacy, and robustness measures that protect systems against adversarial conditions. You will establish governance practices that ensure systems operate fairly, sustainably, and accountably.\n\nThe engineering challenges are substantial, and so is the impact of addressing them correctly.\n\nThe path forward begins with infrastructure: the datacenters, accelerators, storage systems, and communication networks that make everything else possible. Once you understand how this infrastructure enables distributed ML, you will be prepared to build systems that leverage these capabilities effectively.\n\nLet us begin.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Volume II addresses the shift from single-machine ML to distributed systems where communication costs, routine failures, and governance requirements become dominant engineering concerns\n* Scale creates qualitative, not merely quantitative, changes: techniques that work for 8 GPUs may fail at 8,000 GPUs due to emergent phenomena like network congestion, straggler effects, and coordination overhead\n* The three pillars of Volume II, scaling infrastructure, distributing computation, and governing responsibly, are interdependent: infrastructure determines what distribution strategies are feasible, and governance constraints shape both\n* Production ML systems diverge from research prototypes in their requirements for fault tolerance, security, privacy, and accountability to stakeholders beyond the development team\n:::\n\n::: { .quiz-end }\n:::\n","srcMarkdownNoYaml":"\n\n<!--\n================================================================================\nVOLUME II EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY\n================================================================================\n\nPHILOSOPHY: This textbook teaches generalizable ML systems principles, NOT\n\"LLM infrastructure.\" Every technique should apply across model architectures.\nStudents who master these concepts can work on any production ML system.\n\nWHEN WRITING CONTENT, ENSURE EXAMPLES SPAN THESE MODEL TYPES:\n\n| Model Type          | Unique Systems Challenges                              |\n|---------------------|--------------------------------------------------------|\n| LLMs/Transformers   | KV cache memory, attention compute scaling, long       |\n|                     | context, autoregressive decoding latency               |\n| Recommendation      | Massive embedding tables (trillion+ params), feature   |\n|                     | lookup latency, real-time updates, CTR prediction      |\n| Vision (CNN/ViT)    | Data augmentation pipelines, batch size sensitivity,   |\n|                     | spatial locality, multi-resolution processing          |\n| Scientific/GNN      | Irregular compute patterns, graph partitioning,        |\n|                     | sparse operations, physics constraints                 |\n| Multimodal          | Cross-encoder coordination, modality-specific          |\n|                     | preprocessing, heterogeneous compute requirements      |\n| Speech/Audio        | Streaming inference, variable-length sequences,        |\n|                     | real-time latency constraints                          |\n\nCHAPTER-SPECIFIC GUIDANCE:\n\nDISTRIBUTED TRAINING (@sec-distributed-training):\n\n- Data parallelism: ResNet, BERT, recommendation models\n- Model parallelism: GPT-3, Megatron, DLRM embedding sharding\n- Pipeline parallelism: GPT, T5, large vision models\n- Include: DLRM has DIFFERENT parallelism needs (embedding-heavy vs compute-heavy)\n\nINFERENCE AT SCALE (@sec-inference-at-scale):\n\n- Batching strategies differ: LLMs (continuous batching) vs RecSys (feature lookup)\n- Latency profiles: recommendation <10ms, LLMs 100ms-seconds, vision 20-50ms\n- Include ensemble serving (multiple models in pipeline)\n- RecSys is the DOMINANT inference workload by volume at Meta, TikTok, Netflix\n\nCOMMUNICATION (@sec-communication):\n\n- AllReduce for dense gradients (vision, transformers)\n- AlltoAll for embedding lookups (recommendation)\n- Gradient compression benefits vary by model type\n\nSTORAGE (@sec-storage):\n\n- Feature stores: critical for RecSys, less relevant for LLMs\n- Checkpoint sizes: LLMs (TB), vision (GB), recommendation (embedding tables)\n- Data lakes: training data diversity matters\n\nFAULT TOLERANCE (@sec-fault-tolerance):\n\n- Checkpointing frequency depends on model size and iteration time\n- Recommendation systems: real-time feature stores need different recovery\n- Training vs serving fault tolerance requirements differ\n\nEDGE INTELLIGENCE (@sec-edge-intelligence):\n\n- On-device: vision (phones), speech (assistants), NLP (keyboards)\n- Federated learning works differently for different model types\n- Model compression: quantization effects vary by architecture\n\nQUANTITATIVE DIVERSITY CHECKLIST:\n\n- [ ] Does this section have examples from at least 2-3 model types?\n- [ ] Are performance numbers given for multiple architectures?\n- [ ] Would a RecSys engineer find this content applicable to their work?\n- [ ] Would a vision ML engineer find this content applicable?\n- [ ] Are we avoiding LLM-centric framing of general concepts?\n\nEXAMPLE OF GOOD FRAMING:\n  BAD:  \"Training GPT-3 on a single V100 would require 355 years\"\n  GOOD: \"Training frontier models—whether GPT-3 (175B params), ViT-22B\n         (vision), or DLRM (trillion-param embeddings)—would require\n         centuries on single devices, making distributed approaches essential.\"\n\nEXAMPLE OF INCLUSIVE CASE STUDIES:\n  - Meta DLRM: recommendation at scale\n  - Google BERT/T5: NLP training infrastructure\n  - Tesla Autopilot: vision model deployment\n  - Spotify: audio/recommendation hybrid\n  - TikTok: multimodal recommendation\n\n================================================================================\n-->\n\n# Introduction {#sec-vol2-introduction}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting advanced ML systems at scale, set on a crisp, clean white background. The image features interconnected data centers, distributed computing networks, and multiple deployment environments. Visual elements include server racks connected by glowing data streams, edge devices at the periphery, and governance frameworks represented as protective layers. Icons represent the three core themes: scale (expanding infrastructure), distribute (networked nodes), and govern (protective oversight). The style is clean, modern, and flat, suitable for a technical book._\n:::\n\n\\noindent\n![](images/png/cover_introduction.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why must we master the engineering principles that enable machine learning systems to operate reliably at massive scale, across distributed infrastructure, and under responsible governance?_\n\nThe systems that transform industries and affect billions of lives cannot run on individual machines or small clusters. Production ML systems operate at scales where the fundamental nature of engineering challenges changes: communication dominates computation, failures become routine rather than exceptional, and architectural decisions carry consequences that reach far beyond technical performance metrics. At this frontier, the ability to coordinate learning across thousands of machines, to serve predictions to hundreds of millions of users with consistent reliability, and to ensure these systems operate fairly and sustainably determines which capabilities remain laboratory demonstrations and which reshape how humanity solves problems. Volume I established how to build, optimize, and operate ML systems; this volume extends those foundations to the production scale where most consequential AI systems must operate, introducing the three imperatives of scale, distribution, and governance that define advanced ML systems engineering. Mastering these principles determines your ability to architect the infrastructure and establish the practices that enable transformative AI capabilities to reach the people and applications that need them most.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain why ML systems exhibit qualitatively different behaviors at production scale compared to single-machine systems, including the dominance of communication over computation and the transition from exceptional to routine failure\n\n- Analyze the historical evolution of ML compute requirements from AlexNet to frontier models, calculating the implications for infrastructure design\n\n- Compare synchronous versus asynchronous distributed training approaches using the CAP theorem framework to evaluate consistency-availability trade-offs\n\n- Differentiate between datacenter distribution and edge distribution challenges, identifying unique constraints for each deployment context\n\n- Classify ML security threats (model extraction, membership inference, adversarial examples) and explain why scale amplifies their economic attractiveness to attackers\n\n- Apply the AI Triad and Five-Pillar Framework from Volume I to reason about distributed systems challenges across data, algorithms, and infrastructure\n\n- Evaluate the textbook's five-part structure (Foundations of Scale, Distributed Training, Deployment at Scale, Production Concerns, Responsible AI at Scale) to select appropriate chapters for specific engineering challenges\n\n:::\n\n## The Scale Transformation {#sec-vol2-introduction-scale-transformation}\n\nThe history of machine learning is a history of scale. Each major capability leap has come not from algorithmic breakthroughs alone, but from the ability to apply computation at previously impossible scales. Understanding this progression reveals why systems engineering has become central to AI advancement.\n\nConsider the trajectory of compute requirements. AlexNet (2012) trained on two GTX 580 GPUs for approximately 5 to 6 days [@krizhevsky2012imagenet]. BERT (2018) required 64 TPU chips for 4 days, roughly 6,144 chip hours [@devlin2018bert]. GPT-3 (2020) consumed an estimated 3.14×10²³ FLOPS during training, requiring thousands of V100 GPUs running for weeks [@brown2020language]. PaLM (2022) trained on 6,144 TPU v4 chips for roughly 60 days, consuming approximately 10²⁴ FLOPS [@chowdhery2022palm]. GPT-4 (2023) reportedly trained on approximately 25,000 A100 GPUs over 90 to 100 days [@openai2023gpt4]. This progression represents approximately a 10 million fold increase in training compute over a single decade, from roughly 10¹⁸ FLOPS for AlexNet to 10²⁵ FLOPS for GPT-4.\n\n::: {.callout-example title=\"Training Compute Evolution\"}\n```\nModel           Year    GPUs/TPUs    Training Time    Estimated FLOPS\n─────────────────────────────────────────────────────────────────────\nAlexNet         2012    2 GPUs       5-6 days         ~10¹⁸\nBERT-Large      2018    64 TPUs      4 days           ~10²⁰\nGPT-3           2020    ~1000 GPUs   ~30 days         ~10²³\nPaLM            2022    6144 TPUs    ~60 days         ~10²⁴\nGPT-4           2023    ~25000 GPUs  ~100 days        ~10²⁵\n```\n:::\n\nThis exponential growth in compute requirements has transformed ML from a discipline where algorithms dominate to one where systems engineering determines success. A sophisticated algorithm that cannot scale often provides less practical value than a simpler algorithm deployed efficiently across scalable infrastructure.\n\nThe transition from single-machine to distributed training introduces qualitative changes in system behavior. On a single GPU, training proceeds deterministically: the same code, data, and random seed produce identical results. At the scale of thousands of GPUs, new phenomena emerge. Network partitions can split clusters into groups that train independently, causing model divergence[^fn-network-partition]. Stragglers, workers that process data slower than peers due to hardware variation or thermal throttling, can bottleneck entire training runs[^fn-straggler-problem]. Hardware failures that occur once per machine-year become daily events when operating 10,000 machines, meaning systems must checkpoint frequently enough that losing a day's progress becomes acceptable rather than catastrophic[^fn-failure-rates].\n\n[^fn-network-partition]: **Network Partition in Distributed Training**: When network failures isolate groups of workers, each group may continue training independently with different gradient updates, causing models to diverge. In synchronous distributed training, partitions typically halt progress until connectivity restores. In asynchronous training, partitions can cause split brain scenarios where different model versions evolve in parallel. Production systems implement partition detection through heartbeat mechanisms and quorum protocols. Google's approach with TPU pods uses dedicated high bandwidth interconnects (ICI at 4.5 TB/s bidirectional per chip) to minimize partition probability, while software level protocols detect and recover from rare partitions within minutes rather than allowing extended divergent training.\n\n[^fn-straggler-problem]: **The Straggler Problem**: In synchronous distributed training, all workers must complete their computation before proceeding to the next iteration. If one worker runs 10% slower due to thermal throttling, memory pressure, or background processes, the entire cluster waits, reducing effective utilization. At 1,000 workers, the probability of at least one straggler per iteration approaches certainty. Solutions include backup workers that redundantly compute slow partitions, gradient coding that tolerates missing results, bounded staleness that allows proceeding with partial results, and careful hardware provisioning that minimizes variation. Meta's research found that stragglers can reduce large scale training throughput by 20 to 30% without mitigation strategies.\n\n[^fn-failure-rates]: **Hardware Failure Rates at Scale**: Individual server components fail infrequently. Enterprise SSDs show annual failure rates of 0.5 to 2%; GPUs fail at roughly 1 to 2% annually under typical datacenter conditions, though rates can exceed 9% under intensive AI training workloads; memory DIMMs fail at 1 to 2% per year. These rates seem manageable until multiplied by scale. A cluster of 10,000 GPUs with 2% annual failure rate experiences 200 GPU failures per year, or roughly one every two days. A training run lasting 100 days will statistically encounter 50 or more GPU failures. Production systems implement automatic detection, workload migration, and checkpoint based recovery to handle failures as routine events rather than emergencies. Meta reported that during LLaMA training, they experienced hardware failures roughly every few hours, requiring automated recovery systems to maintain progress.\n\nThese scale-induced challenges explain why the largest AI organizations invest heavily in infrastructure. Meta's Research SuperCluster (RSC) contains 16,000 NVIDIA A100 GPUs connected by 200 Gb/s InfiniBand networking [@meta2022rsc]. Google's TPU v4 pods contain 4,096 chips with 1.1 exaFLOPS of aggregate compute capacity. Microsoft's Azure AI infrastructure spans multiple datacenters with tens of thousands of GPUs dedicated to AI workloads. These investments reflect that frontier AI capabilities require frontier infrastructure.\n\n## Why Scale Changes Everything {#sec-vol2-introduction-why-scale-changes}\n\nScale is not merely a larger version of small. Systems that work perfectly at modest scale exhibit qualitatively different behaviors at production scale. Understanding these transitions prepares you for the engineering challenges examined throughout this volume.\n\n### Communication Becomes Dominant\n\nAt small scale, computation dominates. Training a model on a single GPU spends most time performing matrix multiplications. Communication overhead, moving data between CPU and GPU memory, represents a small fraction of total time.\n\nAt large scale, communication often dominates. Distributed training requires synchronizing gradients across workers after each batch. For a model with 175 billion parameters using 32-bit gradients, each synchronization must transfer 700GB of data (175 billion parameters × 4 bytes per parameter). Using the ring all-reduce algorithm[^fn-all-reduce] across 1,000 workers connected by 200 Gb/s InfiniBand (25 GB/s), theoretical completion time for the full synchronization approaches 56 seconds (2 × 700 GB ÷ 25 GB/s). Practical implementations achieve 60-80% of theoretical bandwidth, making communication consume a significant fraction of total training time even with optimized networks.\n\n[^fn-all-reduce]: **Ring All-Reduce**: The dominant collective communication algorithm for distributed training gradient synchronization [@patarasuk2009bandwidth]. In ring all-reduce, N workers arrange logically in a ring. Each worker sends 1/N of its gradient to its neighbor, which adds the received gradient to its own before forwarding. After N minus 1 steps, each worker has the sum of all gradients for 1/N of parameters. A second ring pass distributes the complete sum to all workers. The algorithm achieves optimal bandwidth utilization. Total data transferred equals 2(N minus 1)/N times the gradient size, approaching 2× regardless of worker count. For 1,000 workers with 700GB of gradients and 200 Gb/s links, theoretical completion time is approximately 2 × 700GB / 200 Gb/s = 56 seconds for the full reduce scatter plus all gather. Practical implementations achieve 60 to 80% of theoretical bandwidth, making communication a significant fraction of large scale training time.\n\nThis ratio explains why distributed training systems optimize communication aggressively. Gradient compression reduces transfer volume by 10-100× at the cost of some accuracy. Overlapping communication with computation hides transfer latency during the next batch's forward pass. Hierarchical aggregation reduces cross-rack traffic by combining gradients locally first. These optimizations, unnecessary at small scale, become essential at production scale.\n\n::: {.callout-definition title=\"Communication-Computation Ratio\"}\n***Communication-Computation Ratio*** describes the relative time spent transferring data versus performing computation in distributed systems. A ratio of 1:1 means equal time on each; higher ratios indicate communication-bound workloads. Modern distributed training systems typically achieve ratios between 1:3 and 1:1, making communication optimization critical for efficiency. The ratio depends on model size (larger models have more gradients to synchronize), batch size (larger batches amortize communication over more computation), and network bandwidth (faster networks reduce communication time).\n:::\n\n### Failure Becomes Routine\n\nAt small scale, failure is exceptional. A well maintained server might run for years without hardware issues. Software bugs, once fixed, stay fixed. Administrators can manually investigate and remediate problems.\n\nAt large scale, failure becomes statistical certainty. With 10,000 GPUs, multiple failures occur weekly. With 100,000 concurrent user sessions, software edge cases that occur one in a million times happen hundreds of times daily. Manual intervention becomes impossible; systems must self heal. This transition requires architectural changes from the beginning. Small scale systems optimize for the common case and handle failures through manual recovery, while large scale systems embed failure handling into their core design:\n\n- **Checkpointing**: Saving model state frequently enough that losing hours of progress is acceptable when failures occur\n- **Redundancy**: Running extra workers that can absorb failed workers' tasks without restart\n- **Isolation**: Containing failures so that one component's crash does not cascade through the system\n- **Detection**: Monitoring that identifies failures within seconds\n- **Recovery**: Automated procedures that restore service without human intervention\n\n### Heterogeneity Emerges\n\nAt small scale, systems are homogeneous. A single GPU training job runs on one type of hardware with one software configuration. Behavior is predictable and reproducible.\n\nAt large scale, heterogeneity becomes unavoidable. A fleet of 10,000 GPUs contains multiple hardware generations purchased over years. Different racks have different thermal characteristics affecting clock speeds. Software updates roll out gradually, creating version skew. Network paths vary in latency and bandwidth.\n\nThis heterogeneity creates engineering challenges absent at small scale. Load balancing must account for hardware capability differences. Gradient aggregation must handle workers completing at different rates. Inference routing must direct requests to servers with appropriate model versions. Testing must verify behavior across the combinatorial explosion of configuration variants.\n\n## Why Distribution is Hard {#sec-vol2-introduction-why-distribution-hard}\n\nDistribution introduces challenges beyond those of scale. Coordinating computation across physically separated machines connected by finite-bandwidth, non-zero-latency networks creates fundamental constraints that no amount of engineering cleverness can eliminate.\n\n### The CAP Theorem Reality\n\nThe CAP theorem[^fn-cap-theorem] establishes that distributed systems can provide at most two of three properties: consistency (all nodes see the same data), availability (every request receives a response), and partition tolerance (the system continues operating despite network failures). Since network partitions can always occur, practical systems must choose between consistency and availability during partitions.\n\n[^fn-cap-theorem]: **CAP Theorem**: Proven by Eric Brewer and formalized by Gilbert and Lynch [@gilbert2002brewer], the CAP theorem states that a distributed data store cannot simultaneously provide more than two of Consistency (every read receives the most recent write), Availability (every request receives a non-error response), and Partition tolerance (the system continues operating despite network partitions). Since partitions are unavoidable in distributed systems, the practical choice is between CP (consistent but potentially unavailable during partitions) and AP (available but potentially inconsistent). Distributed ML systems make different choices. Synchronous training is CP (training halts during partitions to maintain model consistency), while asynchronous training is AP (training continues with potentially stale gradients). Understanding this trade-off informs architecture decisions throughout distributed ML systems.\n\nDistributed ML systems make different CAP trade-offs depending on their requirements. Synchronous distributed training chooses consistency: all workers see the same model state, but training halts if any worker becomes unreachable. Asynchronous training chooses availability: training continues even with stragglers or failures, but workers may operate on slightly stale model versions. Federated learning often chooses availability with eventual consistency: edge devices train locally and periodically synchronize, accepting temporary inconsistency for continuous operation.\n\n### Coordination Overhead\n\nDistributed systems require coordination that consumes resources. Every synchronization point introduces latency. Every consensus protocol requires network round-trips. Every distributed lock limits parallelism.\n\nConsider the overhead of distributed training synchronization. Each training iteration requires:\n\n1. Forward pass computation (parallelizable)\n2. Loss computation (local to each worker)\n3. Backward pass computation (parallelizable)\n4. Gradient aggregation (requires network communication)\n5. Parameter update (can parallelize with next iteration)\n\nSteps 1-3 and 5 parallelize efficiently. Step 4, gradient aggregation, requires global coordination. Even with optimized all-reduce algorithms and high-bandwidth networks, this coordination can consume a substantial fraction of total training time for large models [@shoeybi2019megatron]. This overhead is fundamental: no algorithm can aggregate globally distributed values without communication proportional to the data volume.\n\n### Edge Distribution Complexity\n\nDatacenter distribution is challenging but controlled. All machines run in managed facilities with reliable power, cooling, and networking. Administrators can access any machine for diagnosis and repair.\n\nEdge distribution amplifies every challenge. Billions of smartphones, IoT devices, and embedded systems operate in uncontrolled environments with unreliable connectivity, limited power, and heterogeneous capabilities. Google's Gboard keyboard runs on over 1 billion Android devices, each potentially participating in federated learning[^fn-federated-learning] to improve predictions [@hard2018federated]. This deployment context introduces unique constraints:\n\n- **Intermittent connectivity**: Devices may be reachable only when on WiFi and charging\n- **Heterogeneous hardware**: Model must run efficiently across devices spanning a 100× performance range\n- **Privacy requirements**: Raw data cannot leave devices, requiring on-device processing\n- **Update complexity**: Pushing model updates to billions of devices takes weeks\n- **Monitoring limitations**: Cannot install arbitrary diagnostics on user devices\n\nThese constraints require architectural approaches that differ from datacenter ML in essential ways. Federated learning aggregates model updates without collecting data. On-device inference optimizes for varied hardware capabilities. Differential privacy, which adds calibrated noise to protect individual data points while preserving aggregate statistical properties, provides mathematical guarantees about information leakage. These techniques, largely unnecessary for centralized ML, become essential for edge deployment.\n\n## Why Governance Matters at Scale {#sec-vol2-introduction-why-governance-matters}\n\nScale amplifies impact. A bug in a small system affects few users; a bug in a system serving billions affects society. This amplification creates governance requirements that small-scale systems can ignore.\n\n### Security Threats Intensify\n\nML systems face unique security threats beyond traditional software vulnerabilities[^fn-ml-security-threats]. Model extraction attacks can steal proprietary models through query access. Researchers demonstrated extracting functionally equivalent copies of production ML models using only API access [@tramer2016stealing]. Membership inference attacks can determine whether specific data was used in training, creating privacy violations from seemingly innocuous model access [@shokri2017membership]. Adversarial examples can cause misclassification with perturbations imperceptible to humans, demonstrated against production systems including Tesla Autopilot and content moderation systems [@goodfellow2014explaining].\n\n[^fn-ml-security-threats]: **ML-Specific Security Threats**: Traditional software security focuses on preventing unauthorized code execution and data access. ML systems face additional threats that exploit the learned behavior of models. Data poisoning attacks inject malicious training examples that cause targeted misbehavior. Researchers demonstrated that controlling 0.1% of training data can implant backdoors that cause misclassification on specific inputs. Model inversion attacks reconstruct training data from model access. Facial recognition models can leak enough information to reconstruct recognizable images of training individuals. Adversarial reprogramming hijacks models to perform unintended tasks through specially crafted inputs. These threats require defenses beyond traditional security including differential privacy, certified robustness, and continuous monitoring of model behavior in production.\n\nAt production scale, these threats become economically attractive to attackers. A model serving millions of users represents substantial intellectual property worth stealing. A model making consequential decisions (loans, hiring, content moderation) offers high-value manipulation targets. A model processing sensitive data (health records, financial information) provides valuable inference targets.\n\nDefending against these threats requires systematic approaches including access controls that limit query rates and patterns, output perturbation that provides differential privacy guarantees, adversarial training that improves robustness to perturbations, and monitoring that detects anomalous query patterns indicative of attacks. These defenses impose overhead unnecessary for small-scale systems but essential for production deployment.\n\n### Regulatory Requirements Emerge\n\nSystems operating at scale attract regulatory attention. The European Union's General Data Protection Regulation (GDPR) imposes obligations for systems processing EU residents' data, including the right to explanation for automated decisions. The EU AI Act establishes risk-based requirements for AI systems, with high-risk applications (healthcare, employment, law enforcement) requiring conformity assessments, human oversight, and accuracy documentation. Similar regulations exist or are developing in jurisdictions worldwide.\n\nMeeting these regulatory requirements demands technical capabilities:\n\n- **Audit trails**: Recording inputs, outputs, and model versions for every decision\n- **Explanation generation**: Producing human-interpretable justifications for model outputs\n- **Consent management**: Tracking and honoring user preferences for data usage\n- **Data deletion**: Removing specific users' data from training sets and retraining affected models\n- **Bias testing**: Evaluating model performance across protected demographic groups\n\nThese capabilities impose engineering costs absent for unregulated systems but mandatory for production deployment in regulated contexts.\n\n### Societal Impact Demands Responsibility\n\nBeyond legal compliance, systems affecting billions of users carry ethical obligations. Recommendation algorithms shape public discourse. Researchers have documented how engagement-optimizing systems can amplify misinformation and polarization [@ribeiro2020auditing]. Hiring algorithms affect employment opportunities. Amazon discontinued an AI recruiting tool that exhibited bias against women [@dastin2018amazon]. Content moderation systems determine what speech is visible. Errors can suppress legitimate expression or fail to remove harmful content. Responsible engineering practices must address these impacts:\n\n- **Fairness evaluation**: Testing for disparate impact across demographic groups before deployment\n- **Impact assessment**: Analyzing potential harms before launching new capabilities\n- **Human oversight**: Maintaining human review for high-stakes decisions\n- **Incident response**: Processes for rapidly addressing identified harms\n- **Transparency**: Documentation of system capabilities, limitations, and decision factors\n\n::: {.callout-definition title=\"Responsible AI\"}\n***Responsible AI*** encompasses the practices, frameworks, and technical approaches that ensure ML systems operate in ways that are fair, transparent, accountable, and beneficial to society. Responsible AI addresses the ethical implications of automated decision making, the potential for algorithmic bias, the environmental impact of large scale computation, and the governance structures needed to maintain human oversight over consequential systems. Unlike traditional software quality assurance focused on correctness and performance, responsible AI evaluates systems against societal values and human rights considerations.\n:::\n\n## Bridging from Volume I {#sec-vol2-introduction-bridging}\n\nVolume I established the foundations that this textbook extends. If you are beginning here, this section provides essential context. If you completed Volume I, consider this a brief reminder before we proceed to advanced topics.\n\nVolume I introduced the AI Triad as the organizing framework for understanding ML systems. Every machine learning system comprises three interdependent components: data that guides behavior, algorithms that learn patterns, and computational infrastructure that enables both training and inference. These components exist in dynamic tension. Larger models require more data and compute. Larger datasets enable more sophisticated models but demand storage and processing infrastructure. At the scales examined in this volume, these interdependencies intensify. Distributed training requires coordinating the AI Triad's components across thousands of machines rather than optimizing them on a single system.\n\nThe Five-Pillar Framework structures the ML systems engineering discipline into interconnected domains: data engineering establishes pipelines for collecting, processing, and serving training and inference data; model development encompasses architecture design, training procedures, and validation methodologies; optimization techniques compress and accelerate models for deployment constraints; deployment infrastructure spans cloud platforms, edge devices, and embedded systems; and operations practices ensure systems remain reliable, secure, and effective throughout their lifecycle. This textbook extends each pillar to production scale, where the engineering challenges multiply.\n\nThe Six Systems Engineering Principles provide guidance for design decisions across all five pillars:\n\n1. *Measure Everything*: At scale, measurement itself becomes a systems challenge requiring distributed monitoring infrastructure\n2. *Design for 10x Scale*: Production deployment reveals whether 10× design was adequate or optimistic\n3. *Optimize the Bottleneck*: Scale shifts bottlenecks from compute to communication to coordination\n4. *Plan for Failure*: At scale, failure is not exceptional but routine\n5. *Design Cost-Consciously*: Scale makes efficiency improvements worth millions of dollars\n6. *Co-Design for Hardware*: Distributed hardware introduces network topology and storage hierarchy as co-design considerations\n\nVolume I taught you to build, optimize, and operate ML systems. This textbook teaches you to scale, distribute, and govern them.\n\n## The Structure of This Textbook {#sec-vol2-introduction-structure}\n\nThis textbook organizes around the three imperatives, progressing from infrastructure foundations through distribution techniques to governance practices. @tbl-vol2-structure summarizes this five-part structure.\n\n+--------------------------------+---------------------------------+----------------------------------------+\n| **Part**                       | **Theme**                       | **Key Chapters**                       |\n+:===============================+:================================+:=======================================+\n| **I: Foundations of Scale**    | **Scale**: Physical and data    | Infrastructure, Storage                |\n|                                | foundations for distributed ML  |                                        |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **II: Distributed Training**   | **Distribute**: Training models | Distributed Training, Communication,   |\n|                                | across multiple machines        | Fault Tolerance                        |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **III: Deployment at Scale**   | **Deploy**: Serving predictions | Inference at Scale, Edge Intelligence, |\n|                                | to millions of users            | ML Operations at Scale                 |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **IV: Production Concerns**    | **Operate**: Running systems    | Privacy & Security, Robust AI,         |\n|                                | safely and sustainably          | Sustainable AI                         |\n+--------------------------------+---------------------------------+----------------------------------------+\n| **V: Responsible AI at Scale** | **Govern**: Ensuring beneficial | Responsible AI, AI for Good,           |\n|                                | societal impact                 | AGI Systems, Conclusion                |\n+--------------------------------+---------------------------------+----------------------------------------+\n\n: The five parts progress from infrastructure foundations through distributed training and deployment to production concerns and responsible governance. {#tbl-vol2-structure}\n\n### Part I: Foundations of Scale\n\nBefore systems can scale, they require infrastructure designed for scale.\n\n**Infrastructure** examines how datacenters, accelerators, and orchestration systems enable large-scale ML. You will understand the hardware and software stack that makes distributed ML possible, from GPU clusters to high-bandwidth networks, resource scheduling, and container orchestration.\n\n**Storage Systems** addresses data infrastructure at scale. Training datasets for modern models exceed any single storage system, requiring distributed architectures optimized for ML access patterns. You will understand storage hierarchies, data lakes, feature stores, and artifact management that enable efficient data serving.\n\n### Part II: Distributed Training\n\nWith infrastructure foundations established, distribution techniques enable training across multiple machines.\n\n**Distributed Training** develops techniques for training models across devices and machines. Data parallelism, model parallelism, and pipeline parallelism each address different constraints. You will understand when each applies, how they combine, and what synchronization and consistency they require.\n\n**Communication** analyzes the collective operations that coordinate distributed training. AllReduce, AllGather, and other primitives dominate training communication. You will understand algorithms, topologies, and optimization techniques that minimize communication overhead and maximize bandwidth utilization.\n\n**Fault Tolerance** ensures distributed systems continue operating despite failures. At production scale, failures occur daily. Checkpointing, redundancy, and recovery procedures enable training to continue despite inevitable component failures.\n\n### Part III: Deployment at Scale\n\nTraining produces models; deployment delivers value to users.\n\n**Inference at Scale** examines serving systems that deliver predictions with low latency and high throughput. Request routing, load balancing, autoscaling, and geographic distribution enable production inference to meet demanding performance requirements.\n\n**Edge Intelligence** extends ML to resource-constrained devices at the network edge. Model compression, runtime optimization, and edge-cloud coordination enable deployment where centralized inference is infeasible.\n\n**ML Operations at Scale** encompasses practices that maintain large ML systems in production. Monitoring, debugging, deployment pipelines, and incident response adapt for ML-specific requirements at production scale.\n\n### Part IV: Production Concerns\n\nDistribution creates operational challenges that require systematic approaches.\n\n**Privacy and Security** addresses threats specific to ML systems. Model extraction, membership inference, adversarial examples, and data poisoning require defenses including differential privacy, secure computation, and adversarial training.\n\n**Robust AI** ensures reliable operation under uncertainty. Distribution shift, out-of-distribution inputs, and novel situations require systems that recognize the limits of their competence and respond appropriately.\n\n**Sustainable AI** addresses environmental impact. Efficient algorithms, appropriate model sizing, and renewable energy sourcing minimize the environmental footprint of large-scale ML.\n\n### Part V: Responsible AI at Scale\n\nTechnical excellence is insufficient for systems affecting human lives at scale.\n\n**Responsible AI** addresses fairness, transparency, and accountability. Systems must operate equitably across populations, enable stakeholder understanding, and ensure harms can be identified and remediated.\n\n**AI for Good** demonstrates how ML systems address societal challenges. Applications in healthcare, climate, education, and accessibility illustrate how systems engineering principles enable beneficial impact.\n\n**AGI Systems** examines emerging directions including foundation models, compound AI systems, and novel computing paradigms. Understanding these trajectories prepares you for continued learning as the field evolves.\n\nFor detailed guidance on reading paths, learning outcomes, prerequisite knowledge, and navigation strategies for both volumes, refer to the [About](../../frontmatter/about/about.qmd) section.\n\n## The Journey Ahead {#sec-vol2-introduction-journey-ahead}\n\nVolume I concluded with six systems engineering principles and a vision of building ML systems that matter. This volume extends that vision to the scale at which most consequential ML systems operate.\n\nThe transition from building systems that work to building systems that scale, distribute, and govern responsibly represents significant professional growth. The ML systems that will define this era require precisely these capabilities: foundation models serving hundreds of millions of users, edge deployments spanning billions of devices, and AI systems making consequential decisions about human lives.\n\nThroughout this volume, you will learn to architect infrastructure that processes petabytes of training data across tens of thousands of accelerators. You will design inference systems that serve billions of predictions with consistent low latency. You will implement security, privacy, and robustness measures that protect systems against adversarial conditions. You will establish governance practices that ensure systems operate fairly, sustainably, and accountably.\n\nThe engineering challenges are substantial, and so is the impact of addressing them correctly.\n\nThe path forward begins with infrastructure: the datacenters, accelerators, storage systems, and communication networks that make everything else possible. Once you understand how this infrastructure enables distributed ML, you will be prepared to build systems that leverage these capabilities effectively.\n\nLet us begin.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Volume II addresses the shift from single-machine ML to distributed systems where communication costs, routine failures, and governance requirements become dominant engineering concerns\n* Scale creates qualitative, not merely quantitative, changes: techniques that work for 8 GPUs may fail at 8,000 GPUs due to emergent phenomena like network congestion, straggler effects, and coordination overhead\n* The three pillars of Volume II, scaling infrastructure, distributing computation, and governing responsibly, are interdependent: infrastructure determines what distribution strategies are feasible, and governance constraints shape both\n* Production ML systems diverge from research prototypes in their requirements for fault tolerance, security, privacy, and accountability to stakeholders beyond the development team\n:::\n\n::: { .quiz-end }\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"introduction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","introduction.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}