{"title":"Distributed Training Systems","markdown":{"yaml":{"title":"Distributed Training Systems","bibliography":"distributed_training.bib"},"headingText":"<!--","containsRefs":false,"markdown":"\n\nEDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR DISTRIBUTED TRAINING\n================================================================================\n\nCORE PRINCIPLE: Distributed training techniques apply across ALL model types,\nnot just LLMs. Ensure examples span the full spectrum of production ML.\n\nMODEL-SPECIFIC PARALLELISM CONSIDERATIONS:\n\n| Model Type      | Primary Strategy      | Key Challenge                        |\n|-----------------|-----------------------|--------------------------------------|\n| LLMs            | Tensor + Pipeline     | Attention memory, autoregressive     |\n| Recommendation  | Embedding sharding    | Trillion-param embedding tables      |\n| Vision (ResNet) | Data parallelism      | Batch size scaling, BN sync          |\n| Vision (ViT)    | Tensor parallelism    | Large attention layers               |\n| Scientific/GNN  | Graph partitioning    | Irregular communication patterns     |\n| Speech          | Data parallelism      | Variable sequence lengths            |\n\nREQUIRED COVERAGE FOR THIS CHAPTER:\n\nDATA PARALLELISM:\n\n- ResNet/EfficientNet: Classic example, batch norm synchronization\n- BERT: Widely used, good baseline for transformer data parallelism\n- Recommendation: Different gradient sparsity patterns\n\nMODEL PARALLELISM:\n\n- GPT/Megatron: Tensor parallelism for large transformers\n- DLRM: Embedding table sharding (FUNDAMENTALLY DIFFERENT from tensor parallelism)\n- Include: Why embedding parallelism differs from attention parallelism\n\nPIPELINE PARALLELISM:\n\n- GPipe: Original work on vision models\n- Megatron-LM: Application to transformers\n- Include: Micro-batch scheduling differences by model type\n\nHYBRID PARALLELISM:\n\n- 3D parallelism for LLMs (data + tensor + pipeline)\n- Embedding + data parallelism for recommendation\n- Include: Why different model types need different hybrid strategies\n\nCASE STUDIES TO INCLUDE:\n\n- Meta DLRM training infrastructure (recommendation)\n- Google BERT/T5 training (NLP)\n- ResNet ImageNet training (vision baseline)\n- AlphaFold distributed training (scientific)\n\nQUANTITATIVE DIVERSITY:\n\n- Communication/computation ratios differ by model type\n- Scaling efficiency curves differ (dense vs sparse models)\n- Memory footprint breakdown differs (activations vs embeddings vs weights)\n\nANTI-PATTERNS TO AVOID:\n\n- Framing all parallelism as \"for large language models\"\n- Ignoring embedding table challenges unique to recommendation\n- Assuming dense gradients (recommendation has sparse gradients)\n- Only showing transformer examples for tensor parallelism\n\n================================================================================\n-->\n\n# Distributed Training Systems {#sec-distributed-training}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A technical illustration showing multiple interconnected GPU clusters working in coordination to train a massive neural network. The scene depicts hundreds of GPU nodes arranged in a circular pattern, connected by luminous data streams representing gradient synchronization. At the center, a giant neural network model spans across all nodes, with each node responsible for a different portion. Visual elements include ring allreduce patterns showing data flowing between nodes, pipeline stages depicted as sequential processing units, and synchronization barriers represented as glowing checkpoints. The color palette uses deep blues and electric purples for computation, with bright orange and gold for communication paths. The style is technical and precise, suitable for an advanced distributed systems textbook._\n:::\n\n\\noindent\n![](images/png/cover_distributed.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_What makes distributed training a fundamental shift in how we approach machine learning at scale, rather than merely a technical optimization?_\n\nDistributed training becomes an architectural necessity when training demands exceed the capabilities of individual machines, and the principles governing coordination across distributed systems fundamentally reshape how we approach optimization, fault tolerance, and cost efficiency. Moving from single-machine to distributed training requires understanding how communication overhead, synchronization costs, and system heterogeneity create entirely new constraints that affect algorithm design and system architecture. The challenges of distributed training, including parameter synchronization across thousands of devices, absorbing failures without losing progress, and balancing communication with computation, demand different engineering solutions than single-device scenarios. As models grow to billions of parameters and training datasets expand to continental scales, distributed training becomes the foundation upon which feasible machine learning systems are built, directly determining which problems can be tackled, which timelines are realistic, and which organizations can meaningfully participate in advanced AI development.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain data parallelism mechanisms including gradient computation, synchronization via AllReduce algorithms (ring, tree, hierarchical), and the relationship between batch size scaling and convergence behavior\n\n- Analyze multi-machine training requirements by identifying when models exceed single-device memory, when training duration becomes unacceptable, and when datasets exceed single-machine storage, using quantitative thresholds\n\n- Implement data parallel training by applying gradient synchronization algorithms and achieving target parallel efficiency of 85-95% in the linear scaling regime (2-32 devices)\n\n- Design model parallelism strategies using tensor parallelism, pipeline parallelism with microbatching, and embedding sharding to train models exceeding single-device memory while managing pipeline bubble overhead\n\n- Construct hybrid parallelism systems combining data, model, and pipeline strategies across multi-node clusters, selecting appropriate combinations for different model architectures (transformers, recommendation systems, vision models)\n\n- Evaluate distributed training efficiency using quantitative metrics including communication-computation ratio, scaling efficiency, bandwidth utilization, and synchronization costs\n\n:::\n\n## Multi-Machine Scaling Fundamentals {#sec-distributed-training-multimachine-scaling-fundamentals}\n\nPart I established the infrastructure foundations that make distributed training possible. The datacenter architectures and accelerator topologies examined in @sec-infrastructure provide the compute fabric, while the distributed storage systems and data pipelines developed in @sec-storage ensure training data flows efficiently to thousands of workers. These foundations answer the question of what physical resources exist. Part II addresses the central question those resources enable: how do we coordinate training across distributed infrastructure to achieve performance that no single machine could deliver?\n\nThe transition from single-machine to distributed training represents a major shift in optimization strategy and system complexity. Single-machine optimization focuses on efficiently utilizing available resources through techniques such as prefetching, mixed precision, and gradient accumulation. Distributed training introduces different challenges: communication overhead, fault tolerance, and synchronization that require new conceptual frameworks and engineering approaches.\n\n### Multi-Machine Training Requirements {#sec-distributed-training-multimachine-training-requirements}\n\nThree concrete signals indicate when distributed training becomes necessary rather than merely beneficial. First, memory exhaustion occurs when model parameters, optimizer states, and activation storage exceed single-device capacity even after applying gradient checkpointing and mixed precision. For transformer models, this threshold typically occurs around 10-20 billion parameters on current generation GPUs with 40-80GB memory [@rajbhandari2020zero]. Second, unacceptable training duration emerges when single-device training would require weeks or months to converge, making iteration impossible. Training GPT-3 on a single V100 GPU would require approximately 355 years [@brown2020language], making distributed approaches not optional but essential. Third, dataset scale exceeds single-machine storage when training data reaches multiple terabytes, as occurs in large-scale vision or language modeling tasks.\n\n### Distributed Training Complexity Trade-offs {#sec-distributed-training-complexity-tradeoffs}\n\nDistributed training introduces three primary complexity dimensions absent from single-machine scenarios. Communication overhead emerges from gradient synchronization, where each training step must aggregate gradients across all devices. For a model with $N$ parameters distributed across $D$ devices, all-reduce operations must transfer approximately $2N(D-1)/D$ bytes per step. On commodity network infrastructure (10-100 Gbps), this communication can dominate computation time for models under 1 billion parameters [@sergeev2018horovod]. Fault tolerance requirements increase exponentially with cluster size: a 100-node cluster with 99.9% per-node reliability experiences failures every few hours on average, necessitating checkpoint and recovery mechanisms. Algorithmic considerations change because distributed training alters optimization dynamics—large batch sizes from data parallelism affect convergence behavior, requiring learning rate scaling and warmup strategies that single-machine training does not require [@goyal2017accurate].\n\n### Single-Machine to Distributed Transition {#sec-distributed-training-singlemachine-distributed-transition}\n\nThe systematic optimization methodology established for single-machine training extends to distributed environments with important adaptations. Profiling must now capture inter-device communication patterns and synchronization overhead in addition to computation and memory metrics. Tools like NVIDIA Nsight Systems and PyTorch's distributed profiler reveal whether training is communication-bound or computation-bound, guiding the choice between parallelization strategies. The solution space expands from single-machine techniques to include data parallelism (distributing training examples), model parallelism (distributing model parameters), pipeline parallelism (distributing model layers), and hybrid approaches that combine multiple strategies. The principles remain consistent—identify bottlenecks, select appropriate techniques, compose solutions—but the implementation complexity increases substantially.\n\n## Distributed Training Fundamentals {#sec-distributed-training-fundamentals}\n\nBuilding upon single-machine optimization foundations, distributed training extends systematic optimization to multiple machines. When single-machine techniques have been exhausted—prefetching eliminates data loading bottlenecks, mixed-precision maximizes memory efficiency, and gradient accumulation reaches practical limits—distributed approaches provide the next level of scaling capability.\n\n::: {.callout-definition title=\"Distributed Training\"}\n\n***Distributed Training*** is the parallelization of model training across _multiple compute devices_ through coordinated _data partitioning_ and _gradient synchronization_, enabling training of models that exceed single-device memory or time constraints.\n\n:::\n\nThe progression from single-machine to distributed training follows a natural scaling path: optimize locally first, then scale horizontally. This progression ensures that distributed systems operate efficiently at each node while adding the coordination mechanisms necessary for multi-machine training. Training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training[^fn-distributed-training] addresses this challenge by spreading the workload across multiple machines, which coordinate to train a single model efficiently.\n\n[^fn-distributed-training]: **Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorch's DistributedDataParallel, democratizing distributed training for researchers worldwide.\n\nThis coordination relies on consensus protocols and synchronization primitives to ensure parameter updates remain consistent across nodes. While basic barrier synchronization suffices for research, production deployments require careful fault tolerance, checkpointing, and recovery mechanisms covered in @sec-fault-tolerance.\n\nThe path from single-device to distributed training involves distinct complexity stages, each building upon the previous level's challenges. Single GPU training requires only local memory management and straightforward forward/backward passes, establishing the baseline computational patterns. Scaling to multiple GPUs within a single node introduces high-bandwidth communication requirements, typically handled through NVLink[^nvlink] or PCIe connections with NCCL[^nccl] optimization, while preserving the single-machine simplicity of fault tolerance and scheduling.\n\n[^nvlink]: **NVLink**: NVIDIA's proprietary high-speed interconnect providing up to 600 GB/s bidirectional bandwidth between GPUs, roughly 10x faster than PCIe Gen4. Essential for efficient multi-GPU training as it enables rapid gradient synchronization and tensor exchanges between devices.\n\n[^nccl]: **NCCL (NVIDIA Collective Communications Library)**: Optimized library implementing efficient collective operations (AllReduce, Broadcast, etc.) for multi-GPU and multi-node communication. Automatically selects optimal communication patterns based on hardware topology, critical for distributed training performance. The leap to multi-node distributed training brings substantially greater complexity: network communication overhead, fault tolerance requirements, and cluster orchestration challenges. Each scaling stage compounds the previous challenges—communication bottlenecks intensify, synchronization overhead grows, and failure probability increases. This progression underscores why practitioners should optimize single-GPU performance before scaling, ensuring efficient resource utilization at each level.\n\n::: {.callout-note title=\"Practical Distributed Training Complexity\"}\n\nWhile frameworks like PyTorch (FSDP) and DeepSpeed abstract away much of the complexity, implementing distributed training efficiently remains a significant engineering challenge. Production deployments require careful network configuration (e.g., InfiniBand), infrastructure management (e.g., Kubernetes, Slurm), and debugging of complex, non-local issues like synchronization hangs or communication bottlenecks. For most teams, leveraging managed cloud services is often more practical than building and maintaining this infrastructure from scratch.\n\n:::\n\nThe distributed training process itself involves splitting the dataset into non-overlapping subsets, assigning each subset to a different GPU, and performing forward and backward passes independently on each device. Once gradients are computed on each GPU, they are synchronized and aggregated before updating the model parameters, ensuring that all devices learn in a consistent manner. @fig-distributed-training illustrates this process, showing how input data is divided, assigned to multiple GPUs for computation, and later synchronized to update the model collectively.\n\n::: {#fig-distributed-training fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{%\n  mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,\nminimum width=20mm,minimum height=9mm,line width=1pt},\n  mycycle/.style={circle, draw=none, fill=red, minimum width=5mm},\n  myline/.style={line width=1.15pt,draw=cyan},\n%\n  Box/.style={align= flush center,\n    inner xsep=2pt,\n    draw=RedLine,\n    line width=0.75pt,\n    fill=RedL!20,\n    text width=22mm,\n    minimum width=22mm, minimum height=8mm\n  },\n%\nLine/.style={line width=1.0pt,black!50}\n}\n\n\\begin{scope}[node distance=-1.7,local bounding box = SC1]]\n\\node[mycylinder,fill=red!30] (A) {};\n\\scoped[on background layer]\n\\node[mycylinder, above=of A,fill=red!50] (C) {};\n\\node[mycylinder, below=of A,fill=red!10] (B) {};\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(3.5,5))},local bounding box = SC2]\n\\node[mycycle] (C1) {};\n\\node[mycycle,below=of C1] (C2) {};\n\\node[mycycle,below=of C2] (C3) {};\n\\node[mycycle,below=of C3] (C4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(C1)!0.5!(C2)$] (CL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(C2)!0.5!(C3)$] (CL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(C3)!0.5!(C4)$] (CL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(C1)!0.4!(C3)$] (CD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(C2)!0.6!(C4)$] (CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {CL1, CL2, CL3, CD1, CD2} {\n    \\draw[myline] (\\y) -- (C\\x);\n  }\n}\n\\node[Box,below=0.8 of C4](B1){GPU 1};\n\\draw[myline,dashed](C4)--(B1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(11.5,5))},local bounding box = SC3]\n\\node[mycycle] (3C1) {};\n\\node[mycycle,below=of 3C1] (3C2) {};\n\\node[mycycle,below=of 3C2] (3C3) {};\n\\node[mycycle,below=of 3C3] (3C4) {};\n\\node[mycycle,fill=violet,right=0.6 of $(3C1)!0.5!(3C2)$] (3CL1) {};\n\\node[mycycle,fill=violet,right=0.6 of $(3C2)!0.5!(3C3)$] (3CL2) {};\n\\node[mycycle,fill=violet,right=0.6 of $(3C3)!0.5!(3C4)$] (3CL3) {};\n%\n\\node[mycycle,fill=green,left=0.6 of $(3C1)!0.4!(3C3)$] (3CD1) {};\n\\node[mycycle,fill=green,left=0.6 of $(3C2)!0.6!(3C4)$] (3CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {3CL1, 3CL2, 3CL3, 3CD1, 3CD2} {\n    \\draw[myline] (\\y) -- (3C\\x);\n  }\n}\n\n\\node[Box,below=0.8 of 3C4](3B1){GPU 1};\n\\draw[myline,dashed](3C4)--(3B1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(20.0,5))},local bounding box = SC4]\n\\node[mycycle] (4C1) {};\n\\node[mycycle,below=of 4C1] (4C2) {};\n\\node[mycycle,below=of 4C2] (4C3) {};\n\\node[mycycle,below=of 4C3] (4C4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(4C1)!0.5!(4C2)$] (4CL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(4C2)!0.5!(4C3)$] (4CL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(4C3)!0.5!(4C4)$] (4CL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(4C1)!0.4!(4C3)$] (4CD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(4C2)!0.6!(4C4)$] (4CD2) {};\n%\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {4CL1, 4CL2, 4CL3, 4CD1, 4CD2} {\n    \\draw[myline] (\\y) -- (4C\\x);\n  }\n}\n\\node[Box,below=0.8 of 4C4](4B1){GPU 1};\n\\draw[myline,dashed](4C4)--(4B1);\n\\end{scope}\n\\coordinate(X)at($(CD1)!0.5!(CD2)$);\n\\coordinate(Y)at($(3CD1)!0.5!(3CD2)$);\n\n\\node[fill=white,minimum height=45](ER)at($(X)!0.3!(Y)$){Error};\n\\node[fill=white,align=center,minimum height=45](CO)at($(X)!0.7!(Y)$){Compute\\\\ loss\\\\ function};\n\\draw[myline,-latex,shorten <=3mm](X)--(ER.west);\n\\draw[myline,-latex](ER.east)--(CO.west);\n\\draw[myline,-latex,shorten >=3mm](CO.east)--(Y);\n\\draw[myline,dashed](CO.south)--++(270:1)-|node[fill=white,align=center,\npos=0.25](COM){Compare\\\\ predicted\\\\ label with\\\\ annotation}\n(ER.south);\n\n\\node[fill=white,align=center,minimum height=45](OP)at($(3CL2)!0.7!(4CL2)$){Avg\\\\ global\\\\ gradient};\n\\draw[myline,latex-,shorten <=1mm](4CL2)--(OP.east);\n%\n\\draw[myline,latex-,shorten <=3mm,shorten >=3mm](CL2)--(SC1.east|-CL2);\n%\n\\draw[myline,latex-,shorten <=3mm,shorten >=3mm](CL2)-|node[fill=white,pos=0.75]{Chunk}(SC1.north);\n%\n\\path[myline,draw=none,dashed](OP.north west)--++(90:1.2)coordinate(OP1);\n\\draw[myline,dashed]($(ER.north east)!0.5!(CO.north west)$)--++(90:1.2)coordinate(ER1);\n\\coordinate (C) at ($(OP1) + (0,5mm)$);\n\\coordinate (B) at ($(ER1) + (0,5mm)$);\n\\path[red](C)-|coordinate(D1)(4CD1);\n\\path[red](B)-|coordinate(A1)(SC1);\n\\coordinate (D) at ($(D1) + (15mm,0)$);\n\\coordinate (A) at ($(A1) + (-15mm,0)$);\n\\draw[myline,dashed,shorten >=3mm,shorten <=3mm](B)--\nnode[fill=white]{Step 2 -- Compute gradients}(C);\n\\draw[myline,dashed,shorten >=3mm,pos=0.46](C)--\nnode[fill=white]{Step 3 -- Update Parameters}(D);\n\\draw[myline,dashed,shorten >=3mm,pos=0.46](B)--\nnode[fill=white]{Step 1 -- Predict a label}(A);\n\n\\node[above=0.2 of SC2]{Forward pass};\n\\node[above=0.2 of SC3]{Backward pass};\n%%%%%%%%%%%%%%%%%%%%%%%\n%down\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[node distance=0.2,shift={(3.5,-2))},local bounding box = DSC2]\n\\node[mycycle] (DC1) {};\n\\node[mycycle,below=of DC1] (DC2) {};\n\\node[mycycle,below=of DC2] (DC3) {};\n\\node[mycycle,below=of DC3] (DC4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(DC1)!0.5!(DC2)$] (DCL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(DC2)!0.5!(DC3)$] (DCL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(DC3)!0.5!(DC4)$] (DCL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(DC1)!0.4!(DC3)$] (DCD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(DC2)!0.6!(DC4)$] (DCD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {DCL1, DCL2, DCL3, DCD1, DCD2} {\n    \\draw[myline] (\\y) -- (DC\\x);\n  }\n}\n\\node[Box,above=0.8 of DC1](DB1){GPU 2};\n\\draw[myline,dashed](DC1)--(DB1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(11.5,-2))},local bounding box = DSC3]\n\\node[mycycle] (D3C1) {};\n\\node[mycycle,below=of D3C1] (D3C2) {};\n\\node[mycycle,below=of D3C2] (D3C3) {};\n\\node[mycycle,below=of D3C3] (D3C4) {};\n\\node[mycycle,fill=violet,right=0.6 of $(D3C1)!0.5!(D3C2)$] (D3CL1) {};\n\\node[mycycle,fill=violet,right=0.6 of $(D3C2)!0.5!(D3C3)$] (D3CL2) {};\n\\node[mycycle,fill=violet,right=0.6 of $(D3C3)!0.5!(D3C4)$] (D3CL3) {};\n%\n\\node[mycycle,fill=green,left=0.6 of $(D3C1)!0.4!(D3C3)$] (D3CD1) {};\n\\node[mycycle,fill=green,left=0.6 of $(D3C2)!0.6!(D3C4)$] (D3CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {D3CL1, D3CL2, D3CL3, D3CD1, D3CD2} {\n    \\draw[myline] (\\y) -- (D3C\\x);\n  }\n}\n\n\\node[Box,above=0.8 of D3C1](D3B1){GPU 2};\n\\draw[myline,dashed](D3C1)--(D3B1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(20.0,-2))},local bounding box = DSC4]\n\\node[mycycle] (D4C1) {};\n\\node[mycycle,below=of D4C1] (D4C2) {};\n\\node[mycycle,below=of D4C2] (D4C3) {};\n\\node[mycycle,below=of D4C3] (D4C4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(D4C1)!0.5!(D4C2)$] (D4CL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(D4C2)!0.5!(D4C3)$] (D4CL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(D4C3)!0.5!(D4C4)$] (D4CL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(D4C1)!0.4!(D4C3)$] (D4CD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(D4C2)!0.6!(D4C4)$] (D4CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {D4CL1, D4CL2, D4CL3, D4CD1, D4CD2} {\n    \\draw[myline] (\\y) -- (D4C\\x);\n  }\n}\n\\node[Box,above=0.8 of D4C1](D4B1){GPU 2};\n\\draw[myline,dashed](D4C1)--(D4B1);\n\\end{scope}\n%%%%%\n\\coordinate(DX)at($(DCD1)!0.5!(DCD2)$);\n\\coordinate(DY)at($(D3CD1)!0.5!(D3CD2)$);\n\n\\node[fill=white,minimum height=45](DER)at($(DX)!0.3!(DY)$){Error};\n\\node[fill=white,align=center,minimum height=45](DCO)at($(DX)!0.7!(DY)$){Compute\\\\ loss\\\\ function};\n\\draw[myline,-latex,shorten <=3mm](DX)--(DER.west);\n\\draw[myline,-latex](DER.east)--(DCO.west);\n\\draw[myline,-latex,shorten >=3mm](DCO.east)--(DY);\n\\draw[myline,dashed](DCO.north)--++(90:1)-|node[fill=white,align=center,\npos=0.25](DCOM){Compare\\\\ predicted\\\\ label with\\\\ annotation}(DER.north);\n\n\\node[fill=white,align=center,minimum height=45](DOP)at($(D3CL2)!0.7!(D4CL2)$){Avg\\\\ global\\\\ gradient};\n\\draw[myline,latex-,shorten <=1mm](D4CL2)--(DOP.east);\n%\n\\draw[myline,latex-,shorten <=3mm,shorten >=3mm](DCL2)-|\nnode[fill=white,pos=0.75]{Chunk}(SC1.south);\n%\n\\node[below=0.2 of DSC2]{Forward pass};\n\\node[below=0.2 of DSC3]{Backward pass};\n%%%\n\\coordinate(S1)at($(3B1)!0.5!(4B1)$);\n\\coordinate(S2)at($(D3B1)!0.5!(D4B1)$);\n\\coordinate(S)at($(S1)!0.5!(S2)$);\n\n\\node[draw=none,fill=green!50!black!90,text=white,inner xsep=10pt,\n             inner ysep=9pt, outer sep=5pt](CGG)at(S){\\textbf{Calculate Global Gradients}};\n%\n\\draw[myline,shorten <=1mm](OP.west)-|(CGG.80);\n\\draw[myline,-latex,shorten <=2mm](3CL2)-|(CGG.130);\n%\n\\draw[myline,shorten <=1mm](DOP.west)-|(CGG.280);\n\\draw[myline,-latex,shorten <=2mm](D3CL2)-|(CGG.230);\n \\end{tikzpicture}\n```\nDistributed machine learning scales model training by partitioning datasets across multiple GPUs, enabling concurrent computation of gradients, and then aggregating these gradients to update shared model parameters. This approach accelerates training through parallel processing while maintaining model consistency across all devices.\n:::\n\nThis coordination introduces several key challenges that distributed training systems must address. A distributed training system must orchestrate multi-machine computation by splitting up the work, managing communication between machines, and maintaining synchronization throughout the training process. Understanding these basic requirements provides the foundation for examining the main approaches to distributed training: data parallelism, which divides the training data across machines; model parallelism, which splits the model itself; pipeline parallelism, which combines aspects of both; and hybrid approaches that integrate multiple strategies.\n\n## Distributed Training Efficiency Metrics {#sec-distributed-training-efficiency-metrics}\n\nBefore examining specific parallelism strategies, understanding the quantitative metrics that govern distributed training efficiency is essential. These metrics provide the foundation for making informed decisions about scaling strategies, hardware selection, and optimization approaches.\n\nCommunication overhead represents the primary bottleneck in distributed training systems. AllReduce operations consume 10-40% of total training time in data parallel systems, with this overhead increasing significantly at scale. For BERT-Large on 128 GPUs, communication overhead reaches 35% of total runtime, while GPT-3 scale models experience 55% overhead on 1,024 GPUs.\n\n::: {.callout-note title=\"AllReduce Communication Complexity\"}\nAllReduce complexity depends on two components: latency ($\\alpha$) and bandwidth ($\\beta$). For a message of size $M$ across $N$ workers:\n\n**Ring AllReduce**:\n\n- Time: $2(N-1) \\cdot \\alpha + 2 \\cdot \\frac{N-1}{N} \\cdot M \\cdot \\beta$\n- Bandwidth utilization: $(N-1)/N$, approaching optimal as $N$ grows\n- Each device sends/receives approximately $2M$ bytes total (not $2M \\cdot N$)\n\n**Tree AllReduce**:\n\n- Time: $2 \\log_2(N) \\cdot \\alpha + 2 \\log_2(N) \\cdot M \\cdot \\beta$\n- Bandwidth utilization: $1/\\log_2(N)$, decreasing with scale\n- Latency: $O(\\log N)$ steps\n\nThe crossover point depends on message size: tree AllReduce wins for small messages (latency-dominated), while ring AllReduce wins for large gradients (bandwidth-dominated). Modern implementations like NCCL use hierarchical algorithms that achieve tree latency within nodes (using NVLink) and ring bandwidth between nodes (using InfiniBand).\n:::\n\nInterconnect selection is critical for large-scale deployments, as these complexity characteristics determine when communication dominates computation.\n\nThe bandwidth requirements for efficient distributed training are substantial, particularly for transformer models. Efficient systems require 100-400 GB/s aggregate bandwidth per node for transformer architectures. BERT-Base needs 8 GB parameter synchronization per iteration across 64 GPUs, demanding 200 GB/s sustained bandwidth for <50ms synchronization latency. Language models with 175B parameters require 700 GB/s aggregate bandwidth to maintain 80% parallel efficiency, necessitating InfiniBand HDR or equivalent interconnects.\n\nSynchronization frequency presents a trade-off between communication efficiency and convergence behavior. Gradient accumulation reduces synchronization frequency but increases memory requirements and may impact convergence. Synchronizing every 4 steps reduces communication overhead by 60% while increasing memory usage by 3x for gradient storage. Asynchronous methods eliminate synchronization costs entirely but introduce staleness that degrades convergence by 15-30% for large learning rates.\n\nScaling efficiency follows predictable patterns across different GPU counts. In the linear scaling regime of 2-32 GPUs, systems typically achieve 85-95% parallel efficiency as communication overhead remains minimal. The communication bound regime emerges at 64-256 GPUs, where efficiency drops to 60-80% even with optimal interconnects. Beyond 512 GPUs, coordination overhead becomes dominant, limiting efficiency to 40-60% due to collective operation latency.\n\nHardware selection critically impacts these scaling characteristics. NVIDIA DGX systems with NVLink achieve 600 GB/s bisection bandwidth, enabling 90% parallel efficiency up to 8 GPUs per node. Multi-node scaling requires InfiniBand networks, where EDR (100 Gbps) supports efficient training up to 64 nodes, while HDR (200 Gbps) enables scaling to 256+ nodes with >70% efficiency.\n\nThese efficiency metrics directly influence the choice of parallelism strategy. Data parallelism works well in the linear scaling regime but becomes communication-bound at scale. Model parallelism addresses memory constraints but introduces sequential dependencies that limit efficiency. Pipeline parallelism reduces device idle time but introduces complexity in managing microbatches. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads.\n\n## Data Parallelism {#sec-distributed-training-data-parallelism}\n\nBuilding on the efficiency characteristics outlined above, data parallelism represents the most straightforward distributed approach, particularly effective in the linear scaling regime where communication overhead remains manageable. This method distributes the training process across multiple devices by splitting the dataset into smaller subsets. Each device trains a complete copy of the model using its assigned subset of the data. For example, when training an image classification model on 1 million images using 4 GPUs, each GPU would process 250,000 images while maintaining an identical copy of the model architecture.\n\nIt is particularly effective when the dataset size is large but the model size is manageable, as each device must store a full copy of the model in memory. This method is widely used in scenarios such as image classification and natural language processing, where the dataset can be processed in parallel without dependencies between data samples. For instance, when training a ResNet model on ImageNet, each GPU can independently process its portion of images since the classification of one image doesn't depend on the results of another.\n\nThe effectiveness of data parallelism stems from a property of stochastic gradient descent established in optimization foundations. Gradients computed on different minibatches can be averaged while preserving mathematical equivalence to single-device training. This property enables parallel computation across devices, with the mathematical foundation following directly from the linearity of expectation.\n\nConsider a model with parameters $θ$ training on a dataset $D$. The loss function for a single data point $x_i$ is $L(θ, x_i)$. In standard SGD with batch size $B$, the gradient update for a minibatch is:\n$$\ng = \\frac{1}{B} \\sum_{i=1}^B \\nabla_θ L(θ, x_i)\n$$\n\nIn data parallelism with $N$ devices, each device $k$ computes gradients on its own minibatch $B_k$:\n$$\ng_k = \\frac{1}{|B_k|} \\sum_{x_i \\in B_k} \\nabla_θ L(θ, x_i)\n$$\n\nThe global update averages these local gradients:\n$$\ng_{\\text{global}} = \\frac{1}{N} \\sum_{k=1}^N g_k\n$$\n\nThis averaging is mathematically equivalent to computing the gradient on the combined batch $B_{\\text{total}} = \\bigcup_{k=1}^N B_k$:\n$$\ng_{\\text{global}} = \\frac{1}{|B_{\\text{total}}|} \\sum_{x_i \\in B_{\\text{total}}} \\nabla_θ L(θ, x_i)\n$$\n\nThis equivalence shows why data parallelism maintains the statistical properties of SGD training. The approach distributes distinct data subsets across devices, computes local gradients independently, and averages these gradients to approximate the full-batch gradient.\n\nThe method parallels gradient accumulation, where a single device accumulates gradients over multiple forward passes before updating parameters. Both techniques use the additive properties of gradients to process large batches efficiently.\n\n::: {.callout-note title=\"Production Reality: Data Parallelism at Scale\"}\n\nData parallelism in production environments involves several operational considerations beyond the theoretical framework:\n\n- **Communication efficiency**: AllReduce operations for gradient synchronization become the bottleneck at scale. Production systems use optimized libraries like NCCL with ring or tree communication patterns to minimize overhead\n- **Fault tolerance**: Node failures during large-scale training require checkpoint/restart strategies. Production systems implement hierarchical checkpointing with both local and distributed storage\n- **Dynamic scaling**: Cloud environments require elastic scaling capabilities to add/remove workers based on demand and cost constraints, complicated by the need to maintain gradient synchronization\n- **Cost optimization**: Production data parallelism considers cost per GPU-hour across different instance types and preemptible instances, balancing training time against infrastructure costs\n- **Network bandwidth requirements**: Large models require careful network topology planning as gradient communication can consume 10-50% of training time depending on model size and batch size\n\nProduction teams typically benchmark communication patterns and scaling efficiency before deploying large distributed training jobs to identify optimal configurations.\n\n:::\n\n### Data Parallelism Implementation {#sec-distributed-training-data-parallelism-implementation}\n\nThe process of data parallelism can be broken into a series of distinct steps, each with its role in ensuring the system operates efficiently. These steps are illustrated in @fig-train-data-parallelism.\n\n::: {#fig-train-data-parallelism fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=0.75pt,black!50,text=black\n},\n  Box/.style={inner xsep=2pt,\n    line width=0.75pt,\n    node distance=2.0,\n    fill=VioletL2,\n    draw=VioletLine2,\n    text width=27mm,\n    align=flush center,\n    minimum width=27mm,\n    minimum height=9mm\n  },\n  Box2/.style={Box,\n    draw=BlueLine,\n    fill=BlueL,\n    text width=21mm,\n    minimum width=22mm,\n    minimum height=9mm\n  },\n  Text/.style={inner xsep=6pt,\n  inner ysep=4pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor!80,\n    font=\\usefont{T1}{phv}{m}{n}\\footnotesize,\n    align=flush center,\n    minimum width=22mm, minimum height=5mm\n  },\n}\n\n\\node[Box,node distance=1](B1){GPU 1\\\\Forward \\& Backward Pass};\n\\node[Box,node distance=1.2,right=of B1](B2){GPU 2\\\\Forward \\& Backward Pass};\n\\node[Box,node distance=1.2,right=of B2](B3){GPU 3\\\\Forward \\& Backward Pass};\n\\node[Box,node distance=1.2,right=of B3](B4){GPU 4\\\\Forward \\& Backward Pass};\n%\n\\node[Box2,above=1.06 of B1](GB1){Batch 1};\n\\node[Box2,above=1.06 of B2](GB2){Batch 2};\n\\node[Box2,above=1.06 of B3](GB3){Batch 3};\n\\node[Box2,above=1.06 of B4](GB4){Batch 4};\n%\n\\node[Box2,above=1.8of $(GB2)!0.5!(GB3)$,fill=RedL,draw=RedLine](GGB1){Input Data};\n%\n\\node[Box,below=of $(B2)!0.5!(B3)$,fill=GreenL,draw=GreenLine](DB1){Gradients GPU N};\n\\node[Box,below=1.05 of DB1,fill=GreenL,draw=GreenLine](DB2){Gradient Aggregation};\n\\node[Box,below=1.05 of DB2,fill=GreenL,draw=GreenLine](DB3){Model Update};\n%\n\\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB2);\n\\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB3);\n\\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB4);\n\\draw[Line,-latex](GGB1)--node[Text,pos=0.5,anchor=center]{Split into Non-Overlapping Subsets}++(270:1.4)-|(GB1);\n%%\n\\draw[Line,-latex](GB1)--node[Text,pos=0.45]{Assigned to GPU 1}(B1);\n\\draw[Line,-latex](GB2)--node[Text,pos=0.45]{Assigned to GPU 2}(B2);\n\\draw[Line,-latex](GB3)--node[Text,pos=0.45]{Assigned to GPU 3}(B3);\n\\draw[Line,-latex](GB4)--node[Text,pos=0.45]{Assigned to GPU 4}(B4);\n%\n\\draw[Line,-latex](B3)--++(270:0.9)-|(DB1);\n\\draw[Line,-latex](B2)--++(270:0.9)-|(DB1);\n\\draw[Line,-latex](B1)--++(270:0.9)-|(DB1);\n\\draw[Line,-latex](B4)--++(270:0.9)-|node[Text,pos=0.72,text=black]{Compute Gradients}(DB1);\n%\n\\draw[Line,-latex](DB1)--node[Text,pos=0.45]{Synchronize Gradients}(DB2);\n\\draw[Line,-latex](DB2)--node[Text,pos=0.45]{Aggregate Gradients and Update Parameters}(DB3);\n%\n\\draw[Line,-latex](GGB1.east)--++(0:6.8)|-node[Text,pos=0.8,text=black]{Next Mini-Batch}(DB3.east);\n\\end{tikzpicture}\n```\nDistributed training replicates the model across multiple devices, each processing a subset of the data before aggregating gradients to update model parameters. This accelerates the training process compared to single-device training, and contrasts with model parallelism where the model itself is partitioned across devices.\n:::\n\n#### Dataset Splitting {#sec-distributed-training-dataset-splitting}\n\nThe first step in data parallelism involves dividing the dataset into smaller, non-overlapping subsets. This ensures that each device processes a unique portion of the data, avoiding redundancy and enabling efficient utilization of available hardware. For instance, with a dataset of 100,000 training examples and 4 GPUs, each GPU would be assigned 25,000 examples. Modern frameworks like PyTorch's DistributedSampler handle this distribution automatically, implementing prefetching and caching mechanisms to ensure data is readily available for processing.\n\n#### Device Forward Pass {#sec-distributed-training-device-forward-pass}\n\nOnce the data subsets are distributed, each device performs the forward pass independently. During this stage, the model processes its assigned batch of data, generating predictions and calculating the loss. For example, in a ResNet-50 model, each GPU would independently compute the convolutions, activations, and final loss for its batch. The forward pass is computationally intensive and benefits from hardware accelerators like NVIDIA V100 GPUs or Google TPUs, which are optimized for matrix operations.\n\n#### Backward Pass and Calculation {#sec-distributed-training-backward-pass-calculation}\n\nFollowing the forward pass, each device computes the gradients of the loss with respect to the model's parameters during the backward pass. Modern frameworks like PyTorch and TensorFlow handle this automatically through their autograd systems. For instance, if a model has 50 million parameters, each device calculates gradients for all parameters but based only on its local data subset.\n\n#### Gradient Synchronization {#sec-distributed-training-gradient-synchronization}\n\nTo maintain consistency across the distributed system, the gradients computed by each device must be synchronized. This coordination represents a distributed systems challenge: achieving global consensus while minimizing communication complexity. The ring all-reduce algorithm exemplifies this trade-off, organizing devices in a logical ring where each GPU communicates only with its neighbors. The algorithm complexity is O(n) in communication rounds but requires sequential dependencies that can limit parallelism.\n\nFor example, with 8 GPUs sharing gradients for a 100 MB model, ring all-reduce requires only 7 communication steps instead of the 56 steps needed for naive all-to-all synchronization. The ring topology creates bottlenecks: the slowest link in the ring determines the overall synchronization time, and network partitions can halt the entire training process. Alternative algorithms like tree-reduce achieve O(log n) latency at the cost of increased bandwidth requirements on root nodes. Modern systems often implement hierarchical topologies, using high-speed links within nodes and lower-bandwidth connections between nodes to optimize these trade-offs.\n\n#### Synchronization Models {#sec-distributed-training-sync-models}\n\nDistributed training systems operate under explicit synchronization models that govern when workers observe each other's updates. Understanding these models is essential for reasoning about correctness and performance.\n\nThe default model, Bulk Synchronous Parallel (BSP), requires all workers to complete their local computation (forward and backward pass), synchronize gradients through a barrier (AllReduce), and then simultaneously update parameters. BSP provides strong guarantees: every worker sees identical parameter values at each step, ensuring mathematical equivalence to single-device training. The cost is that the slowest worker determines iteration time, creating the \"straggler problem.\"\n\nStale Synchronous Parallel (SSP) relaxes this constraint, allowing workers to proceed up to $s$ iterations ahead of the slowest worker before blocking. This bounds staleness while reducing synchronization delays. SSP requires careful learning rate tuning since workers compute gradients on slightly different parameter versions. The bounded staleness guarantee ($s$ typically 2-5) provides a middle ground between BSP's strong consistency and fully asynchronous approaches.\n\nAsynchronous SGD eliminates synchronization barriers entirely, with workers updating parameters independently. This maximizes hardware utilization but introduces gradient staleness that can degrade convergence. When a worker computes gradients on parameters that are already $\\tau$ steps stale, the effective learning rate decreases. Compensation techniques include learning rate scaling ($\\eta' = \\eta / \\sqrt{\\tau}$) or momentum correction.\n\n::: {.callout-note title=\"Synchronization Model Trade-offs\"}\n| Model | Consistency | Throughput | Convergence | Use Case |\n|-------|-------------|------------|-------------|----------|\n| BSP | Strong | Bounded by slowest worker | Equivalent to single-GPU | Final training runs, reproducibility |\n| SSP | Bounded staleness | Higher than BSP | Near-equivalent with tuning | Hyperparameter search |\n| Async | Weak | Maximum | Degraded, requires compensation | Large heterogeneous clusters |\n:::\n\nThe choice of synchronization model directly affects both system throughput and model convergence. Production systems typically use BSP for final training runs to ensure reproducibility, while exploring SSP or async approaches during hyperparameter search where exact reproducibility is less critical.\n\n#### Barrier Semantics and Failure Modes {#sec-distributed-training-barrier-failures}\n\nAllReduce operations implement implicit barriers: no worker can proceed until all workers have contributed their gradients. This coupling creates failure modes absent from single-device training.\n\nWorker failures during AllReduce cause all other workers to block indefinitely, waiting for the missing contribution. Without timeout mechanisms, the entire training job hangs rather than failing cleanly. Production systems implement watchdog timers, typically set to 5-10 minutes, to detect and terminate stuck jobs.\n\nGradient mismatches occur when workers disagree on which tensors to synchronize due to conditional computation paths or dynamic batching. AllReduce operations may block waiting for tensors that some workers never send. This commonly occurs with variable-length sequences in NLP models, dynamic computation graphs, and mixture-of-experts with different routing decisions.\n\nStraggler-induced delays arise because iteration time equals the slowest worker's time plus synchronization overhead. A single slow worker, whether due to thermal throttling, network congestion, or OS jitter, delays all workers and reduces cluster utilization. At 1000 GPUs with 1% probability of straggler per GPU per step, approximately 10 GPUs straggle every iteration.\n\nProduction systems address these issues through:\n\n- **Timeouts**: AllReduce operations with configurable timeouts that trigger failure handling rather than indefinite blocking\n- **Heartbeat monitoring**: Detecting unresponsive workers before AllReduce blocks\n- **Elastic training**: Removing failed workers and continuing with reduced parallelism (see @sec-fault-tolerance)\n- **Backup workers**: Redundant computation to mask stragglers\n\n#### Parameter Updating {#sec-distributed-training-parameter-updating}\n\nAfter gradient aggregation, each device independently updates model parameters using the chosen optimization algorithm, such as SGD with momentum or Adam. This decentralized update strategy, implemented in frameworks like PyTorch's DistributedDataParallel (DDP), enables efficient parameter updates without requiring a central coordination server. Since all devices have identical gradient values after synchronization, they perform mathematically equivalent updates to maintain model consistency across the distributed system.\n\nFor example, in a system with 8 GPUs training a ResNet model, each GPU computes local gradients based on its data subset. After gradient averaging via ring all-reduce, every GPU has the same global gradient values. Each device then independently applies these gradients using the optimizer's update rule. If using SGD with learning rate 0.1, the update would be `weights = weights - 0.1 * gradients`. This process maintains mathematical equivalence to single-device training while enabling distributed computation.\n\nThis process, which involves splitting data, performing computations, synchronizing results, and updating parameters, repeats for each batch of data. Modern frameworks automate this cycle, allowing developers to focus on model architecture and hyperparameter tuning rather than distributed computing logistics.\n\n### Data Parallelism Advantages {#sec-distributed-training-data-parallelism-advantages}\n\nData parallelism offers several key benefits that make it the predominant approach for distributed training. By splitting the dataset across multiple devices and allowing each device to train an identical copy of the model, this approach effectively addresses the core challenges in modern AI training systems.\n\nThe primary advantage of data parallelism is its linear scaling capability with large datasets. As datasets grow into the terabyte range, processing them on a single machine becomes prohibitively time-consuming. For example, training a vision transformer on ImageNet (1.2 million images) might take weeks on a single GPU, but only days when distributed across 8 GPUs. This scalability is particularly valuable in domains like language modeling, where datasets can exceed billions of tokens.\n\nHardware utilization efficiency represents another important benefit. Data parallelism maintains high GPU utilization rates, typically, above 85%, by ensuring each device actively processes its data portion. Modern implementations achieve this through asynchronous data loading and gradient computation overlapping with communication. For instance, while one batch computes gradients, the next batch's data is already being loaded and preprocessed.\n\nImplementation simplicity sets data parallelism apart from other distribution strategies. Modern frameworks have reduced complex distributed training to just a few lines of code. For example, converting a PyTorch model to use data parallelism often requires only wrapping it in `DistributedDataParallel` and initializing a distributed environment. This accessibility has contributed significantly to its widespread adoption in both research and industry.\n\nThe approach also offers flexibility across model architectures. Whether training a ResNet (vision), BERT (language), or Graph Neural Network (graph data), the same data parallelism principles apply without modification. This universality makes it particularly valuable as a default choice for distributed training.\n\nTraining time reduction is perhaps the most immediate benefit. Given proper implementation, data parallelism can achieve near-linear speedup with additional devices. Training that takes 100 hours on a single GPU might complete in roughly 13 hours on 8 GPUs, assuming efficient gradient synchronization and minimal communication overhead.\n\nWhile these benefits make data parallelism compelling, achieving these advantages requires careful system design. Several challenges must be addressed to fully realize these benefits.\n\n::: {.callout-tip title=\"GPT-2 Data Parallel Scaling: 1→8→32 GPUs\" collapse=\"true\"}\n\nThis example demonstrates how data parallelism scales in practice, including efficiency degradation.\n\n**Single GPU Baseline**\n\n- Batch size: 16 (with gradient checkpointing, fits in 32GB)\n- Time per step: 1.8 seconds\n- Training throughput: ~9 samples/second\n- Time to 50K steps: **25 hours**\n\n**8 GPUs: Single Node with NVLink**\n\nConfiguration:\n\n- Per-GPU batch: 16, global batch: 128\n- Gradient synchronization: 3GB @ 600 GB/s (NVLink) = 5ms\n\nPerformance results:\n\n- Computation: 180ms per step\n- Communication: 5ms per step\n- Total: 185ms per step\n- Speedup: 1.8s ÷ 0.185s = 9.7× (not quite 8×)\n- Parallel efficiency: 9.7 ÷ 8 = 121%\n\nWhy over 100% efficiency? Larger global batch (128 vs 16) improves GPU utilization from 72% to 89%. This \"super-linear\" speedup is common in ML at small scales when the baseline has poor utilization.\n\nTraining time: 25 hours ÷ 9.7 = **2.6 hours**\n\n**32 GPUs: 4 Nodes with InfiniBand**\n\nConfiguration:\n\n- Per-GPU batch: 16, global batch: 512\n- Intra-node communication: 5ms (NVLink)\n- Inter-node communication: 3GB @ 12.5 GB/s (InfiniBand) = 240ms\n\nPerformance results:\n\n- Computation: 180ms (42% of time)\n- Communication: 245ms (58% of time)\n- Total: 425ms per step\n- Speedup: 1.8s ÷ 0.425s = 4.2× faster → 5.9 hours\n- Parallel efficiency: 4.2 ÷ 32 = 13%\n\nCommunication dominates and becomes the bottleneck.\n\n**Better Approach: 8 GPUs with Gradient Accumulation**\n\n- Configuration: 8 GPUs × batch 16 × 4 accumulation steps = 512 effective batch\n- Communication overhead: 5ms ÷ (4 × 180ms) = 0.7%\n- Training time: 3.8 hours\n- Cost: $128/hour × 3.8 hours = $486 vs. $3,021 for 32 GPUs\n- Savings: $2,535 (84% reduction) with only 1 hour longer training\n\n**Key Insights**\n\n1. NVLink enables efficient scaling within single nodes (97% efficiency)\n2. Inter-node communication kills efficiency (drops to 13%)\n3. Gradient accumulation beats naive scaling for memory-bound models\n4. Sweet spot for GPT-2: 8 GPUs per node with gradient accumulation, not naive scaling to 32+ GPUs\n\nOpenAI's GPT-2 paper reports training on 32 V100s across 4 nodes using optimized communication (likely gradient accumulation combined with pipeline parallelism), not pure data parallelism.\n\n:::\n\n### Data Parallelism Limitations {#sec-distributed-training-data-parallelism-limitations}\n\nWhile data parallelism is an effective approach for distributed training, it introduces several challenges that must be addressed to achieve efficient and scalable training systems. These challenges stem from the inherent trade-offs between computation and communication, as well as the limitations imposed by hardware and network infrastructures.\n\nCommunication overhead represents the most significant bottleneck in data parallelism. During gradient synchronization, each device must exchange gradient updates, often hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter model, each synchronization step might require transferring several gigabytes of data across the network. While high-speed interconnects like NVLink (300 GB/s) or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCL's ring-allreduce algorithm[^fn-allreduce-algorithm] reduces this burden by organizing devices in a ring topology, but communication costs still grow with model size and device count.\n\n[^fn-allreduce-algorithm]: **AllReduce Algorithm**: A collective communication primitive where each process contributes data and all processes receive the same combined result (typically a sum). Naive implementations require O(n²) messages for n devices. The ring-allreduce algorithm, developed for high-performance computing in the 1980s, reduces this to O(n) by organizing devices in a logical ring where each device communicates only with its neighbors, making it scalable for modern ML with hundreds of GPUs.\n\nScalability limitations become apparent as device count increases. While 8 GPUs might achieve $7\\times$ speedup (87.5% scaling efficiency), scaling to 64 GPUs typically yields only 45-50$\\times$ speedup (70-78% efficiency) due to growing synchronization costs. Scaling efficiency, calculated as speedup divided by the number of devices—quantifies how effectively additional hardware translates to reduced training time. Perfect linear scaling would yield 100% efficiency, but communication overhead and synchronization barriers typically degrade efficiency as device count grows. This non-linear scaling means that doubling the number of devices rarely halves the training time, particularly in configurations exceeding 16-32 devices.\n\nMemory constraints present a hard limit for data parallelism. Consider a transformer model with 175 billion parameters, which requires approximately 350 GB just to store model parameters in FP32. When accounting for optimizer states and activation memories, the total requirement often exceeds 1 TB per device. Since even high-end GPUs typically offer 80 GB or less, such models cannot use pure data parallelism.\n\nWorkload imbalance affects heterogeneous systems significantly. In a cluster mixing A100 and V100 GPUs, the A100s might process batches $1.7\\times$ faster, forcing them to wait for the V100s to catch up. This idle time can reduce cluster utilization by 20-30% without proper load balancing mechanisms.\n\nFinally, there are critical challenges related to fault tolerance and reliability in distributed training systems. Node failures become inevitable at scale: with 100 GPUs running continuously, hardware failures occur multiple times per week. A training run that costs millions of dollars cannot restart from scratch each time a single GPU fails. Modern distributed training systems implement sophisticated checkpointing strategies, storing model state every N iterations to minimize lost computation. Checkpoint frequency creates trade-offs: frequent checkpointing reduces the potential loss from failures but increases storage I/O overhead and training latency. Production systems typically checkpoint every 100-1000 iterations, balancing fault tolerance against performance.\n\nImplementation complexity compounds these reliability challenges. While modern frameworks abstract much of the complexity, implementing robust distributed training systems requires significant engineering expertise. Graceful degradation when subsets of nodes fail, consistent gradient synchronization despite network partitions, and automatic recovery from transient failures demand deep understanding of both machine learning and distributed systems principles.\n\nDespite these challenges, data parallelism remains an important technique for distributed training, with many strategies available to address its limitations. Monitoring these distributed systems requires specialized tooling for tracking gradient norms, communication patterns, and hardware utilization across nodes. Before examining model parallelism, we first explore memory optimization techniques that extend data parallelism to larger models.\n\n### Memory-Efficient Data Parallelism: ZeRO and FSDP {#sec-distributed-training-zero-fsdp}\n\nThe memory constraints of data parallelism motivate a family of techniques that shard memory state across workers while preserving the simplicity of data parallel training. ZeRO (Zero Redundancy Optimizer) and its PyTorch implementation FSDP (Fully Sharded Data Parallel) enable training models that would otherwise require model parallelism.\n\nIn standard data parallelism, each GPU maintains a complete copy of:\n\n- **Model parameters**: 4 bytes/param (FP32) or 2 bytes/param (FP16/BF16)\n- **Gradients**: Same size as parameters\n- **Optimizer states**: For Adam, 8 bytes/param (momentum + variance in FP32)\n\nFor a 7B parameter model with Adam optimizer, each GPU requires: $7B \\times (4 + 4 + 8) = 112$ GB, exceeding A100-80GB capacity even before accounting for activations.\n\nZeRO addresses this redundancy through progressive sharding:\n\n| Stage | What is Sharded | Memory Reduction | Communication Overhead |\n|-------|-----------------|------------------|------------------------|\n| ZeRO-1 | Optimizer states only | ~4x | None (same as DDP) |\n| ZeRO-2 | + Gradients | ~8x | ReduceScatter replaces AllReduce |\n| ZeRO-3 / FSDP | + Parameters | ~$N$ (linear in workers) | AllGather before each layer |\n\nZeRO-1 shards optimizer states across GPUs. Each GPU stores only $1/N$ of the Adam momentum and variance tensors. After gradient AllReduce, each GPU updates only its shard of parameters, then broadcasts updates to other GPUs. Memory savings: optimizer states reduced from $8N$ bytes/param to $8$ bytes/param total across cluster.\n\nZeRO-2 additionally shards gradients. Instead of AllReduce, which leaves full gradients on each GPU, ZeRO-2 uses ReduceScatter so each GPU receives $1/N$ of the reduced gradients. Memory savings: gradients reduced from $4N$ bytes/param to $4$ bytes/param total.\n\nZeRO-3 and FSDP shard parameters themselves. Each GPU stores only $1/N$ of the model. Before each layer's forward pass, parameters are gathered via AllGather; after backward pass, gradients are reduced via ReduceScatter, then parameters are discarded. This achieves maximum memory efficiency at the cost of additional communication.\n\n::: {.callout-note title=\"FSDP Communication Analysis\"}\nFSDP introduces communication on the critical path that DDP avoids:\n\n- **Forward pass**: AllGather to reconstruct parameters ($M$ bytes × 2 for each layer)\n- **Backward pass**: ReduceScatter for gradients ($M$ bytes × 2 for each layer)\n\nFor a model with $L$ layers, FSDP performs $2L$ collective operations per training step versus 1 AllReduce for DDP. However, FSDP enables overlapping: while layer $i$ computes, layer $i+1$ can prefetch parameters.\n\nTotal FSDP communication volume: approximately $3M$ bytes (vs. $2M$ for DDP AllReduce), but spread across more operations with overlap opportunities.\n:::\n\nThe choice between FSDP and DDP depends on model size and memory constraints. Use DDP when the model fits in GPU memory with room for activations, as it has lower overhead. Use FSDP ZeRO-2 when the model barely fits or requires activation checkpointing. Use FSDP ZeRO-3 when model parameters exceed single-GPU memory. For training 70B+ models on 80GB GPUs, combine FSDP with tensor parallelism.\n\nFSDP configuration requires careful tuning of sharding strategy (by layer, by transformer block, or flat) and mixed precision settings. The `auto_wrap_policy` determines sharding granularity, with finer sharding reducing memory but increasing communication frequency.\n\n## Model Parallelism {#sec-distributed-training-model-parallelism}\n\nWhile data parallelism scales dataset processing, some models themselves exceed the memory capacity of individual devices. Model parallelism splits neural networks across multiple computing devices when the model's parameters exceed single-device memory limits. Unlike data parallelism, where each device contains a complete model copy, model parallelism assigns different model components to different devices [@shazeer_mixture_of_experts_2017].\n\nSeveral implementations of model parallelism exist. In layer-based splitting, devices process distinct groups of layers sequentially. For instance, the first device might compute layers 1-4 while the second handles layers 5-8. Channel-based splitting divides the channels within each layer across devices, such as the first device processing 512 channels while the second manages the remaining ones. For transformer architectures, attention head splitting distributes different attention heads to separate devices.\n\nThis distribution method enables training of large-scale models. GPT-3, with 175 billion parameters, relies on model parallelism for training. Vision transformers processing high-resolution 16k × 16k pixel images use model parallelism to manage memory constraints. Mixture-of-Expert architectures use this approach to distribute their conditional computation paths across hardware.\n\nDevice coordination follows a specific pattern during training. In the forward pass, data flows sequentially through model segments on different devices. The backward pass propagates gradients in reverse order through these segments. During parameter updates, each device modifies only its assigned portion of the model. This coordination ensures mathematical equivalence to training on a single device while enabling the handling of models that exceed individual device memory capacities.\n\n### Model Parallelism Implementation {#sec-distributed-training-model-parallelism-implementation}\n\nModel parallelism divides neural networks across multiple computing devices, with each device computing a distinct portion of the model's operations. This division allows training of models whose parameter counts exceed single-device memory capacity. The technique encompasses device coordination, data flow management, and gradient computation across distributed model segments. The mechanics of model parallelism are illustrated in @fig-model-parallelism. These steps are described next:\n\n::: {#fig-model-parallelism fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=1.0pt,black!50,text=black\n},\n    Box/.style={inner xsep=2pt,\n    draw=GreenLine,\n    node distance=1.5,\n    line width=0.75pt,\n    fill=GreenL,\n    anchor=west,\n    text width=23mm,\n    align=flush center,\n    minimum width=23mm,\n    minimum height=10mm\n  },\n  Text/.style={inner xsep=4pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor!80,\n    font=\\usefont{T1}{phv}{m}{n}\\footnotesize,\n    align=flush center,\n    minimum width=22mm, minimum height=6mm\n  },\n}\n\n\\node[Box](B1){Input Data};\n\\node[Box,right=of B1](B2){Model Part 1\\\\ on Device 1};\n\\node[Box,right=of B2](B3){Model Part 2\\ on Device 2};\n\\node[Box,right=of B3](B4){Model Part 3\\\\ on Device 3};\n\\node[Box,right=of B4](B5){Predictions};\n%\n\\draw[Line,-latex](B1)--++(90:12mm)\n-|node[Text,pos=0.25]{Forward Pass}(B2.120);\n\\draw[Line,latex-](B1)--++(270:12mm)\n-|node[Text,pos=0.25]{Gradient Updates}(B2.240);\n%\n\\draw[Line,-latex](B2)--++(90:12mm)\n-|node[Text,pos=0.25]{Intermediate Data}(B3.120);\n\\draw[Line,latex-](B2)--++(270:12mm)\n-|node[Text,pos=0.25]{Gradient Updates}(B3.240);\n%\n\\draw[Line,-latex](B3)--++(90:12mm)\n-|node[Text,pos=0.25]{Intermediate Data}(B4.120);\n\\draw[Line,latex-](B3)--++(270:12mm)\n-|node[Text,pos=0.25]{Gradient Updates}(B4.240);\n%\n\\draw[Line,-latex](B4)--++(90:12mm)\n-|node[Text,pos=0.25]{Output}(B5.120);\n\\draw[Line,latex-](B4)--++(270:12mm)\n-|node[Text,pos=0.25]{Backward Pass}(B5.240);\n\\end{tikzpicture}\n```\nDistributing a neural network across multiple devices enables training models larger than the memory capacity of a single device. This approach requires careful coordination of data flow and gradient computation between devices to maintain training efficiency.\n:::\n\n#### Model Partitioning {#sec-distributed-training-model-partitioning}\n\nThe first step in model parallelism is dividing the model into smaller segments. For instance, in a deep neural network, layers are often divided among devices. In a system with two GPUs, the first half of the layers might reside on GPU 1, while the second half resides on GPU 2. Another approach is to split computations within a single layer, such as dividing matrix multiplications in transformer models across devices.\n\n#### Model Forward Pass {#sec-distributed-training-model-forward-pass}\n\nDuring the forward pass, data flows sequentially through the partitions. For example, data processed by the first set of layers on GPU 1 is sent to GPU 2 for processing by the next set of layers. This sequential flow ensures that the entire model is used, even though it is distributed across multiple devices. Efficient inter-device communication is important to minimize delays during this step [@deepspeed_training_system_2021].\n\n#### Backward Pass and Calculation {#sec-distributed-training-backward-pass-calculation-model}\n\nThe backward pass computes gradients through the distributed model segments in reverse order. Each device calculates local gradients for its parameters and propagates necessary gradient information to previous devices. In transformer models, this means backpropagating through attention computations and feed-forward networks across device boundaries.\n\nFor example, in a two-device setup with attention mechanisms split between devices, the backward computation works as follows: The second device computes gradients for the final feed-forward layers and attention heads. It then sends the gradient tensors for the attention output to the first device. The first device uses these received gradients to compute updates for its attention parameters and earlier layer weights.\n\n#### Parameter Updates {#sec-distributed-training-parameter-updates-model}\n\nParameter updates occur independently on each device using the computed gradients and an optimization algorithm. A device holding attention layer parameters applies updates using only the gradients computed for those specific parameters. This localized update approach differs from data parallelism, which requires gradient averaging across devices.\n\nThe optimization step proceeds as follows: Each device applies its chosen optimizer (such as Adam or AdaFactor) to update its portion of the model parameters. A device holding the first six transformer layers updates only those layers' weights and biases. This local parameter update eliminates the need for cross-device synchronization during the optimization step, reducing communication overhead.\n\n#### Iterative Process {#sec-distributed-training-iterative-process}\n\nLike other training strategies, model parallelism repeats these steps for every batch of data. As the dataset is processed over multiple iterations, the distributed model converges toward optimal performance.\n\n### Parallelism Variations {#sec-distributed-training-parallelism-variations}\n\nModel parallelism can be implemented through different strategies for dividing the model across devices. The three primary approaches are layer-wise partitioning, operator-level partitioning, and pipeline parallelism, each suited to different model structures and computational needs.\n\n#### Layer-wise Partitioning {#sec-distributed-training-layerwise-partitioning}\n\nLayer-wise partitioning assigns distinct model layers to separate computing devices. In transformer architectures, this translates to specific devices managing defined sets of attention and feed-forward blocks. As illustrated in @fig-layers-blocks, a 24-layer transformer model distributed across four devices assigns six consecutive transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and so forth.\n\n::: {#fig-layers-blocks fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{Line/.style={line width=1.0pt,black!50\n},\n  Box/.style={inner xsep=2pt,\n    draw=VioletLine2,\n    line width=0.75pt,\n    node distance=1.8,\n    fill=VioletL2,\n    align=flush center,\n    text width=19mm,\n    minimum width=19mm,\n    minimum height=8mm\n  },\n}\n\\node[Box,fill=RedL,draw=RedLine](B1){Blocks 1-6};\n\\node[Box,right=of B1,fill=OrangeL,draw=OrangeLine](B2){Blocks 7-12};\n\\node[Box,right=of B2,fill=GreenL,draw=GreenLine](B3){Blocks 13-18};\n\\node[Box,right=of B3,fill=BlueL,draw=BlueLine](B4){Blocks 19-24};\n%\n\\node[Box,below=1.3 of B1,fill=VioletL2,draw=VioletLine2](G1){GPU 1};\n\\node[Box,below=1.3 of B2,fill=VioletL2,draw=VioletLine2](G2){GPU 2};\n\\node[Box,below=1.3 of B3,fill=VioletL2,draw=VioletLine2](G3){GPU 3};\n\\node[Box,below=1.3 of B4,fill=VioletL2,draw=VioletLine2](G4){GPU 4};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B1)(G1)](BB1){};\n\\node[below=1pt of BB1.north,anchor=north]{Device 1};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B2)(G2)](BB2){};\n\\node[below=1pt of BB2.north,anchor=north]{Device 2};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B3)(G3)](BB3){};\n\\node[below=1pt of BB3.north,anchor=north]{Device 3};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B4)(G4)](BB4){};\n\\node[below=1pt of BB4.north,anchor=north]{Device 4};\n\n\\foreach \\x in {1,2,3} {\n    \\pgfmathtruncatemacro{\\newX}{\\x + 1}\n    \\draw[-latex,Line] (B\\x) -- (B\\newX);\n}\n\\foreach \\x in {4,3,2} {\n    \\pgfmathtruncatemacro{\\newX}{\\x - 1}\n\\draw[red,-latex,Line](B\\x.230)to[out=230,in=300](B\\newX.300);\n}\n\\end{tikzpicture}\n```\n**Layer-Wise Model Parallelism**: Distributing a transformer model across multiple gpus assigns consecutive layers to each device, enabling parallel processing of input data and accelerating training. This partitioning strategy allows each GPU to operate on a subset of the model's layers, reducing the memory footprint and computational load per device.\n:::\n\nThis sequential processing introduces device idle time, as each device must wait for the previous device to complete its computation before beginning work. For example, while device 1 processes the initial blocks, devices 2, 3, and 4 remain inactive. Similarly, when device 2 begins its computation, device 1 sits idle. This pattern of waiting and idle time reduces hardware utilization efficiency compared to other parallelization strategies.\n\n#### Pipeline Parallelism {#sec-distributed-training-pipeline-parallelism}\n\nPipeline parallelism extends layer-wise partitioning by introducing microbatching to minimize device idle time, as illustrated in @fig-pipline-parallelism. Instead of waiting for an entire batch to sequentially pass through all devices, the computation is divided into smaller segments called microbatches [@harlap2018pipedream]. Each device, as represented by the rows in the drawing, processes its assigned model layers for different microbatches simultaneously. For example, the forward pass involves devices passing activations to the next stage (e.g., $F_{0,0}$ to $F_{1,0}$). The backward pass transfers gradients back through the pipeline (e.g., $B_{3,3}$ to $B_{2,3}$). This overlapping computation reduces idle time and increases throughput while maintaining the logical sequence of operations across devices.\n\n::: {#fig-pipline-parallelism}\n```{.tikz}\n\\begin{tikzpicture}[\n    every node/.style={font=\\sffamily, draw, minimum width=1cm, minimum height=0.7cm, align=center, outer sep=0},\n    fill0/.style={fill=red!20}, % Complementary to lightgray\n    fill1/.style={fill=blue!20}, % Complementary to orange\n    fill2/.style={fill=orange!20}, % Complementary to blue\n    fill3/.style={fill=yellow!20}, % Complementary to purple\n    back3/.style={fill=yellow!20} % Same as fill3\n]\n\n% Row 0\n\\node[fill0] (F0_0) {$F_{0,0}$};\n\\node[fill0, right=0cm of F0_0] (F0_1) {$F_{0,1}$};\n\\node[fill0, right=0cm of F0_1] (F0_2) {$F_{0,2}$};\n\\node[fill0, right=0cm of F0_2] (F0_3) {$F_{0,3}$};\n\n% Row 1\n\\node[fill1, above right=0cm and 0cm of F0_0] (F1_0) {$F_{1,0}$};\n\\node[fill1, right=0cm of F1_0] (F1_1) {$F_{1,1}$};\n\\node[fill1, right=0cm of F1_1] (F1_2) {$F_{1,2}$};\n\\node[fill1, right=0cm of F1_2] (F1_3) {$F_{1,3}$};\n\n% Row 2 (stacked above F1)\n\\node[fill2, above right=0cm and 0cm of F1_0] (F2_0) {$F_{2,0}$};\n\\node[fill2, right=0cm of F2_0] (F2_1) {$F_{2,1}$};\n\\node[fill2, right=0cm of F2_1] (F2_2) {$F_{2,2}$};\n\\node[fill2, right=0cm of F2_2] (F2_3) {$F_{2,3}$};\n\n% Row 3 (stacked above F2)\n\\node[fill3, above right=0cm and 0cm of F2_0] (F3_0) {$F_{3,0}$};\n\\node[fill3, right=0cm of F3_0] (F3_1) {$F_{3,1}$};\n\\node[fill3, right=0cm of F3_1] (F3_2) {$F_{3,2}$};\n\\node[fill3, right=0cm of F3_2] (F3_3) {$F_{3,3}$};\n\n% Row 3 (backward pass)\n\\node[back3, right=0cm of F3_3] (B3_3) {$B_{3,3}$};\n\\node[back3, right=0cm of B3_3] (B3_2) {$B_{3,2}$};\n\\node[back3, right=0cm of B3_2] (B3_1) {$B_{3,1}$};\n\\node[back3, right=0cm of B3_1] (B3_0) {$B_{3,0}$};\n\n% Row 2 (backward pass)\n\\node[fill2, below=0cm and 0cm of B3_2] (B2_3) {$B_{2,3}$};\n\\node[fill2, right=0cm of B2_3] (B2_2) {$B_{2,2}$};\n\\node[fill2, right=0cm of B2_2] (B2_1) {$B_{2,1}$};\n\\node[fill2, right=0cm of B2_1] (B2_0) {$B_{2,0}$};\n\n% Row 1 (backward pass)\n\\node[fill1, below=0cm of B2_2] (B1_3) {$B_{1,3}$};\n\\node[fill1, right=0cm of B1_3] (B1_2) {$B_{1,2}$};\n\\node[fill1, right=0cm of B1_2] (B1_1) {$B_{1,1}$};\n\\node[fill1, right=0cm of B1_1] (B1_0) {$B_{1,0}$};\n\n% Row 0 (backward pass)\n\\node[fill0, below=0cm of B1_2] (B0_3) {$B_{0,3}$};\n\\node[fill0, right=0cm of B0_3] (B0_2) {$B_{0,2}$};\n\\node[fill0, right=0cm of B0_2] (B0_1) {$B_{0,1}$};\n\\node[fill0, right=0cm of B0_1] (B0_0) {$B_{0,0}$};\n\n% Update nodes\n\\node[fill0, right=0cm of B0_0] (U0_0) {Update};\n\\node[fill1, above=0cm of U0_0] (U0_1) {Update};\n\\node[fill2, above=0cm of U0_1] (U0_2) {Update};\n\\node[fill3, above=0cm of U0_2] (U0_3) {Update};\n\n%\\node[draw=none, minimum width=4cm, minimum height=1cm, align=center, right=1cm of F0_3] (Bubble) {Bubble};\n\\end{tikzpicture}\n```\nWith model layers distributed across devices, microbatching divides each training batch into smaller segments that flow through the pipeline concurrently. This approach minimizes device idle time during both forward and backward passes by allowing multiple microbatches to be processed simultaneously at different pipeline stages. Activations flow sequentially between devices during the forward pass, while gradients propagate in reverse during backpropagation.\n:::\n\nIn a transformer model distributed across four devices, device 1 would process blocks 1-6 for microbatch $N+1$ while device 2 computes blocks 7-12 for microbatch $N$. Simultaneously, device 3 executes blocks 13-18 for microbatch $N-1$, and device 4 processes blocks 19-24 for microbatch $N-2$. Each device maintains its assigned transformer blocks but operates on a different microbatch, creating a continuous flow of computation.\n\nThe transfer of hidden states between devices occurs continuously rather than in distinct phases. When device 1 completes processing a microbatch, it immediately transfers the output tensor of shape [microbatch_size, sequence_length, hidden_dimension] to device 2 and begins processing the next microbatch. This overlapping computation pattern maintains full hardware utilization while preserving the model's mathematical properties.\n\n#### Tensor Parallelism {#sec-distributed-training-tensor-parallelism}\n\nTensor parallelism (also called operator-level or intra-layer parallelism) divides individual neural network operations across devices. Unlike pipeline parallelism which assigns complete layers to devices, tensor parallelism splits the weight matrices within each layer. This distinction is critical: tensor parallelism requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer, while pipeline parallelism tolerates lower bandwidth between stages.\n\n::: {.callout-note title=\"Terminology: Tensor Parallelism vs. Pipeline Parallelism\"}\nModern literature distinguishes two forms of model parallelism:\n\n**Tensor Parallelism** (intra-layer): Splits individual operations (matrix multiplies, attention heads) across devices. Requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer.\n\n**Pipeline Parallelism** (inter-layer): Assigns complete layers to different devices. Requires only point-to-point communication between pipeline stages, tolerating lower bandwidth interconnects.\n\nThe Megatron-LM framework popularized this distinction, using tensor parallelism within nodes (8 GPUs on NVLink) and pipeline parallelism across nodes (InfiniBand).\n:::\n\nMegatron-style tensor parallelism partitions matrix multiplications in two ways.\n\nColumn-parallel linear layers split weights along columns. For input $X$ and weight matrix $W = [W_1 | W_2]$ split across 2 GPUs:\n$$Y = XW = X[W_1 | W_2] = [XW_1 | XW_2]$$\nEach GPU computes its partition independently. Outputs are concatenated (no communication needed if followed by row-parallel layer).\n\nRow-parallel linear layers split weights along rows. For $W = \\begin{bmatrix} W_1 \\\\ W_2 \\end{bmatrix}$:\n$$Y = XW = X_1 W_1 + X_2 W_2$$\nEach GPU computes a partial sum. Outputs require AllReduce to combine.\n\nIn transformer architectures, Megatron applies this pattern:\n\n1. **QKV projection**: Column-parallel (weights split, outputs concatenated across heads)\n2. **Attention output projection**: Row-parallel (requires AllReduce after)\n3. **First FFN layer**: Column-parallel (split intermediate dimension)\n4. **Second FFN layer**: Row-parallel (requires AllReduce after)\n\nThis design places AllReduce operations strategically: one after attention, one after FFN, totaling 2 AllReduce operations per transformer layer.\n\nCommunication volume per transformer layer depends on sequence length $S$, hidden dimension $H$, and batch size $B$:\n$$\\text{Communication} = 2 \\times B \\times S \\times H \\times \\text{sizeof(dtype)}$$\n\nWith $S=2048$, $H=4096$, $B=4$, and FP16: $2 \\times 4 \\times 2048 \\times 4096 \\times 2 = 134$ MB per layer. For a 96-layer model, this totals 12.6 GB per training step, requiring NVLink bandwidth to avoid becoming the bottleneck.\n\nTensor parallelism scaling degrades rapidly beyond 8-way parallelism because:\n\n- Communication volume grows linearly with tensor parallel degree\n- Computation per GPU decreases (less work to hide communication latency)\n- NVLink bandwidth becomes saturated\n\nProduction systems (GPT-4, LLaMA, Gemini) use 8-way tensor parallelism within nodes, combined with pipeline parallelism across nodes, achieving the best balance of memory distribution and communication efficiency.\n\n### Model Parallelism Advantages {#sec-distributed-training-model-parallelism-advantages}\n\nModel parallelism offers several significant benefits, making it an essential strategy for training large-scale models that exceed the capacity of individual devices. These advantages stem from its ability to partition the workload across multiple devices, enabling the training of more complex and resource-intensive architectures.\n\nMemory scaling represents the primary advantage of model parallelism. Current transformer architectures contain up to hundreds of billions of parameters. A 175 billion parameter model with 32-bit floating point precision requires 700 GB of memory just to store its parameters. When accounting for activations, optimizer states, and gradients during training, the memory requirement multiplies several fold. Model parallelism makes training such architectures feasible by distributing these memory requirements across devices.\n\nAnother key advantage is the efficient utilization of device memory and compute power. Since each device only needs to store and process a portion of the model, memory usage is distributed across the system. This allows practitioners to work with larger batch sizes or more complex layers without exceeding memory limits, which can also improve training stability and convergence.\n\nModel parallelism also provides flexibility for different model architectures. Whether the model is sequential, as in many natural language processing tasks, or composed of computationally intensive operations, as in attention-based models or convolutional networks, there is a partitioning strategy that fits the architecture. This adaptability makes model parallelism applicable to a wide variety of tasks and domains.\n\nFinally, model parallelism is a natural complement to other distributed training strategies, such as data parallelism and pipeline parallelism. By combining these approaches, it becomes possible to train models that are both large in size and require extensive data. This hybrid flexibility is especially valuable in advanced research and production environments, where scaling models and datasets simultaneously is critical for achieving optimal performance.\n\nWhile model parallelism offers these benefits, its effectiveness depends on careful partitioning strategy design, with specific challenges addressed in the following sections and the trade-offs involved in its use.\n\n### Model Parallelism Limitations {#sec-distributed-training-model-parallelism-limitations}\n\nWhile model parallelism provides an effective approach for training large-scale models, it also introduces unique challenges. These challenges arise from the complexity of partitioning the model and the dependencies between partitions during training. Addressing these issues requires careful system design and optimization.\n\nOne major challenge in model parallelism is balancing the workload across devices. Not all parts of a model require the same amount of computation. For instance, in layer-wise partitioning, some layers may perform significantly more operations than others, leading to an uneven distribution of work. Devices responsible for the heavier computations may become bottlenecks, leaving others underutilized. This imbalance reduces overall efficiency and slows down training. Identifying optimal partitioning strategies is critical to ensuring all devices contribute evenly.\n\nAnother challenge is data dependency between devices. During the forward pass, activation tensors of shape [batch_size, sequence_length, hidden_dimension] must transfer between devices. For a typical transformer model with batch size 32, sequence length 2048, and hidden dimension 2048, each transfer moves approximately 512 MB of data at float32 precision. With gradient transfers in the backward pass, a single training step can require several gigabytes of inter-device communication. On systems using PCIe interconnects with 64 GB/s theoretical bandwidth, these transfers introduce significant latency.\n\nModel parallelism also increases the complexity of implementation and debugging. Partitioning the model, ensuring proper data flow, and synchronizing gradients across devices require detailed coordination. Errors in any of these steps can lead to incorrect gradient updates or even model divergence. Debugging such errors is often more difficult in distributed systems, as issues may arise only under specific conditions or workloads.\n\nA further challenge is pipeline bubbles in pipeline parallelism. With $m$ pipeline stages, the first $m-1$ steps operate at reduced efficiency as the pipeline fills. For example, in an 8-device pipeline, the first device begins processing immediately, but the eighth device remains idle for 7 steps. This warmup period reduces hardware utilization by a fraction of approximately $(m-1)/b$, where $b$ is the number of microbatches in the training step.\n\nFinally, model parallelism may be less effective for certain architectures, such as models with highly interdependent operations. In these cases, splitting the model may lead to excessive communication overhead, outweighing the benefits of parallel computation. For such models, alternative strategies like data parallelism or hybrid approaches might be more suitable.\n\nDespite these challenges, model parallelism remains an indispensable tool for training large models. With careful optimization and the use of modern frameworks, many of these issues can be mitigated, enabling efficient distributed training at scale.\n\n## Hybrid Parallelism {#sec-distributed-training-hybrid-parallelism}\n\nRecognizing that both data and model constraints can occur simultaneously, hybrid parallelism combines model parallelism and data parallelism when training neural networks [@narayanan_pipeline_parallelism_2021]. A model might be too large to store on one GPU (requiring model parallelism) while simultaneously needing to process large batches of data efficiently (requiring data parallelism).\n\nTraining a 175-billion parameter language model on a dataset of 300 billion tokens demonstrates hybrid parallelism in practice. The neural network layers distribute across multiple GPUs through model parallelism, while data parallelism enables different GPU groups to process separate batches. The hybrid approach coordinates these two forms of parallelization.\n\nThis strategy addresses two key constraints. First, memory constraints arise when model parameters exceed single-device memory capacity. Second, computational demands increase when dataset size necessitates distributed processing.\n\n### Hybrid Parallelism Implementation {#sec-distributed-training-hybrid-parallelism-implementation}\n\nHybrid parallelism operates by combining the processes of model partitioning and dataset splitting, ensuring efficient utilization of both memory and computation across devices. This integration allows large-scale machine learning systems to overcome the constraints imposed by individual parallelism strategies.\n\n#### Model and Data Partitioning {#sec-distributed-training-model-data-partitioning}\n\nHybrid parallelism divides both model architecture and training data across devices. The model divides through layer-wise or operator-level partitioning, where GPUs process distinct neural network segments. Simultaneously, the dataset splits into subsets, allowing each device group to train on different batches. A transformer model might distribute its attention layers across four GPUs, while each GPU group processes a unique 1,000-example batch. This dual partitioning distributes memory requirements and computational workload.\n\n#### Forward Pass {#sec-distributed-training-forward-pass-hybrid}\n\nDuring the forward pass, input data flows through the distributed model. Each device processes its assigned portion of the model using the data subset it holds. For example, in a hybrid system with four devices, two devices might handle different layers of the model (model parallelism) while simultaneously processing distinct data batches (data parallelism). Communication between devices ensures that intermediate outputs from model partitions are passed seamlessly to subsequent partitions.\n\n#### Backward Pass and Gradient Calculation {#sec-distributed-training-backward-pass-gradient-calculation-hybrid}\n\nDuring the backward pass, gradients are calculated for the model partitions stored on each device. Data-parallel devices that process the same subset of the model but different data batches aggregate their gradients, ensuring that updates reflect contributions from the entire dataset. For model-parallel devices, gradients are computed locally and passed to the next layer in reverse order. In a two-device model-parallel configuration, for example, the first device computes gradients for layers 1-3, then transmits these to the second device for layers 4-6. This combination of gradient synchronization and inter-device communication ensures consistency across the distributed system.\n\n#### Parameter Updates {#sec-distributed-training-parameter-updates-hybrid}\n\nAfter gradient synchronization, model parameters are updated using the chosen optimization algorithm. Devices working in data parallelism update their shared model partitions consistently, while model-parallel devices apply updates to their local segments. Efficient communication is critical in this step to minimize delays and ensure that updates are correctly propagated across all devices.\n\n#### Iterative Process {#sec-distributed-training-iterative-process-hybrid}\n\nHybrid parallelism follows an iterative process similar to other training strategies. The combination of model and data distribution allows the system to process large datasets and complex models efficiently over multiple training epochs. By balancing the computational workload and memory requirements, hybrid parallelism enables the training of advanced machine learning models that would otherwise be infeasible.\n\n### Parallelism Variations {#sec-distributed-training-parallelism-variations-hybrid}\n\nHybrid parallelism can be implemented in different configurations, depending on the model architecture, dataset characteristics, and available hardware. These variations allow for tailored solutions that optimize performance and scalability for specific training requirements.\n\n#### Hierarchical Parallelism {#sec-distributed-training-hierarchical-parallelism}\n\nHierarchical hybrid parallelism applies model parallelism to divide the model across devices first and then layers data parallelism on top to handle the dataset distribution. For example, in a system with eight devices, four devices may hold different partitions of the model, while each partition is replicated across the other four devices for data parallel processing. This approach is well-suited for large models with billions of parameters, where memory constraints are a primary concern.\n\nHierarchical hybrid parallelism ensures that the model size is distributed across devices, reducing memory requirements, while data parallelism ensures that multiple data samples are processed simultaneously, improving throughput. This dual-layered approach is particularly effective for models like transformers, where each layer may have a significant memory footprint.\n\n#### Intra-layer Parallelism {#sec-distributed-training-intralayer-parallelism}\n\nIntra-layer hybrid parallelism combines model and data parallelism within individual layers of the model. For instance, in a transformer architecture, the attention mechanism can be split across multiple devices (model parallelism), while each device processes distinct batches of data (data parallelism). This fine-grained integration allows the system to optimize resource usage at the level of individual operations, enabling training for models with extremely large intermediate computations.\n\nThis variation is particularly useful in scenarios where specific layers, such as attention or feedforward layers, have computationally intensive operations that are difficult to distribute effectively using model or data parallelism alone. Intra-layer hybrid parallelism addresses this challenge by applying both strategies simultaneously.\n\n#### Inter-layer Parallelism {#sec-distributed-training-interlayer-parallelism}\n\nInter-layer hybrid parallelism focuses on distributing the workload between model and data parallelism at the level of distinct model layers. For example, early layers of a neural network may be distributed using model parallelism, while later layers use data parallelism. This approach aligns with the observation that certain layers in a model may be more memory-intensive, while others benefit from increased data throughput.\n\nThis configuration allows for dynamic allocation of resources, adapting to the specific demands of different layers in the model. By tailoring the parallelism strategy to the unique characteristics of each layer, inter-layer hybrid parallelism achieves an optimal balance between memory usage and computational efficiency.\n\n### Hybrid Parallelism Advantages {#sec-distributed-training-hybrid-parallelism-advantages}\n\nThe adoption of hybrid parallelism in machine learning systems addresses some of the most significant challenges posed by the ever-growing scale of models and datasets. By blending the strengths of model parallelism and data parallelism, this approach provides a solution to scaling modern machine learning workloads.\n\nOne of the most prominent benefits of hybrid parallelism is its ability to scale seamlessly across both the model and the dataset. Modern neural networks, particularly transformers used in natural language processing and vision applications, often contain billions of parameters. These models, paired with massive datasets, make training on a single device impractical or even impossible. Hybrid parallelism enables the division of the model across multiple devices to manage memory constraints while simultaneously distributing the dataset to process vast amounts of data efficiently. This dual capability ensures that training systems can handle the computational and memory demands of the largest models and datasets without compromise.\n\nAnother critical advantage lies in hardware utilization. In many distributed training systems, inefficiencies can arise when devices sit idle during different stages of computation or synchronization. Hybrid parallelism mitigates this issue by ensuring that all devices are actively engaged. Whether a device is computing forward passes through its portion of the model or processing data batches, hybrid strategies maximize resource usage, leading to faster training times and improved throughput.\n\nFlexibility is another hallmark of hybrid parallelism. Machine learning models vary widely in architecture and computational demands. For instance, convolutional neural networks prioritize spatial data processing, while transformers require intensive operations like matrix multiplications in attention mechanisms. Hybrid parallelism adapts to these diverse needs by allowing practitioners to apply model and data parallelism selectively. This adaptability ensures that hybrid approaches can be tailored to the specific requirements of a given model, making it a versatile solution for diverse training scenarios.\n\nHybrid parallelism reduces communication bottlenecks, a common issue in distributed systems. By striking a balance between distributing model computations and spreading data processing, hybrid strategies minimize the amount of inter-device communication required during training. This efficient coordination not only speeds up the training process but also enables the effective use of large-scale distributed systems where network latency might otherwise limit performance.\n\nFinally, hybrid parallelism supports the ambitious scale of modern AI research and development. It provides a framework for leveraging advanced hardware infrastructures, including clusters of GPUs or TPUs, to train models that push the boundaries of what's possible. Without hybrid parallelism, many of the breakthroughs in AI, including large language models and advanced vision systems, would remain unattainable due to resource limitations.\n\nBy enabling scalability, maximizing hardware efficiency, and offering flexibility, hybrid parallelism has become an essential strategy for training the most complex machine learning systems. It is not just a solution to today's challenges but also a foundation for the future of AI, where models and datasets will continue to grow in complexity and size.\n\n### Hybrid Parallelism Limitations {#sec-distributed-training-hybrid-parallelism-limitations}\n\nWhile hybrid parallelism provides a robust framework for scaling machine learning training, it also introduces complexities that require careful consideration. These challenges stem from the intricate coordination needed to integrate both model and data parallelism effectively. Understanding these obstacles is crucial for designing efficient hybrid systems and avoiding potential bottlenecks.\n\nOne of the primary challenges of hybrid parallelism is communication overhead. Both model and data parallelism involve significant inter-device communication. In model parallelism, devices must exchange intermediate outputs and gradients to maintain the sequential flow of computation. In data parallelism, gradients computed on separate data subsets must be synchronized across devices. Hybrid parallelism compounds these demands, as it requires efficient communication for both processes simultaneously. If not managed properly, the resulting overhead can negate the benefits of parallelization, particularly in large-scale systems with slower interconnects or high network latency.\n\nAnother critical challenge is the complexity of implementation. Hybrid parallelism demands a nuanced understanding of both model and data parallelism techniques, as well as the underlying hardware and software infrastructure. Designing efficient hybrid strategies involves making decisions about how to partition the model, how to distribute data, and how to synchronize computations across devices. This process often requires extensive experimentation and optimization, particularly for custom architectures or non-standard hardware setups. While modern frameworks like PyTorch and TensorFlow provide tools for distributed training, implementing hybrid parallelism at scale still requires significant engineering expertise.\n\nWorkload balancing also presents a challenge in hybrid parallelism. In a distributed system, not all devices may have equal computational capacity. Some devices may process data or compute gradients faster than others, leading to inefficiencies as faster devices wait for slower ones to complete their tasks. Additionally, certain model layers or operations may require more resources than others, creating imbalances in computational load. Managing this disparity requires careful tuning of partitioning strategies and the use of dynamic workload distribution techniques.\n\nMemory constraints remain a concern, even in hybrid setups. While model parallelism addresses the issue of fitting large models into device memory, the additional memory requirements for data parallelism, such as storing multiple data batches and gradient buffers, can still exceed available capacity. This is especially true for models with extremely large intermediate computations, such as transformers with high-dimensional attention mechanisms. Balancing memory usage across devices is essential to prevent resource exhaustion during training.\n\nLastly, hybrid parallelism poses challenges related to fault tolerance and debugging. Distributed systems are inherently more prone to hardware failures and synchronization errors. Debugging issues in hybrid setups can be significantly more complex than in standalone model or data parallelism systems, as errors may arise from interactions between the two approaches. Ensuring robust fault-tolerance mechanisms and designing tools for monitoring and debugging distributed systems are essential for maintaining reliability.\n\nDespite these challenges, hybrid parallelism remains an indispensable strategy for training large-scale machine learning models. By addressing these obstacles through optimized communication protocols, intelligent partitioning strategies, and robust fault-tolerance systems, practitioners can unlock the full potential of hybrid parallelism and drive innovation in AI research and applications.\n\n## Parallelism Strategy Comparison {#sec-distributed-training-parallelism-strategy-comparison}\n\nThe features of data parallelism, model parallelism, pipeline parallelism, and hybrid parallelism are summarized in @tbl-parallelism-compare. This comparison highlights their respective focuses, memory requirements, communication overheads, scalability, implementation complexity, and ideal use cases. By examining these factors, practitioners can determine the most suitable approach for their training needs.\n\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Aspect**                        | **Data Parallelism**               | **Model Parallelism**              | **Pipeline Parallelism**           | **Hybrid Parallelism**              |\n+:==================================+:===================================+:===================================+:===================================+:====================================+\n| **Focus**                         | Distributes dataset across         | Distributes the model across       | Distributes model stages in        | Combines multiple parallelism       |\n|                                   | devices, each with a full model    | devices, each handling a portion   | pipeline, processing microbatches  | strategies for balanced             |\n|                                   | copy                               | of the model                       | concurrently                       | scalability                         |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Memory Requirement per Device** | High (entire model on each device) | Low (model split across devices)   | Low to Moderate (stages split      | Moderate (splits model and dataset  |\n|                                   |                                    |                                    | across devices)                    | across devices)                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Communication Overhead**        | Moderate to High (gradient         | High (communication for            | Moderate (activation passing       | Very High (requires synchronization |\n|                                   | synchronization across devices)    | intermediate activations and       | between stages)                    | for both model and data)            |\n|                                   |                                    | gradients)                         |                                    |                                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Scalability**                   | Good for large datasets with       | Good for very large models with    | Good for deep models with many     | Excellent for extremely large       |\n|                                   | moderate model sizes               | smaller datasets                   | layers                             | models and datasets                 |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Implementation Complexity**     | Low to Moderate (relatively        | Moderate to High (requires         | Moderate to High (requires         | High (complex integration of        |\n|                                   | straightforward with existing      | careful partitioning and           | pipeline scheduling and            | multiple parallelism strategies)    |\n|                                   | tools)                             | coordination)                      | microbatch management)             |                                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Ideal Use Case**                | Large datasets where model fits    | Extremely large models that exceed | Deep models with sequential stages | Training massive models on vast     |\n|                                   | within a single device             | single-device memory limits        | that can tolerate microbatch       | datasets in large-scale systems     |\n|                                   |                                    |                                    | latency                            |                                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n\n: **Parallel Training Strategies**: Data, model, pipeline, and hybrid parallelism each address the challenges of scaling machine learning training by distributing workload across devices, differing in how they partition data and model parameters to optimize memory usage, communication, and scalability. Understanding these trade-offs enables practitioners to select the most effective approach for their specific model and infrastructure. {#tbl-parallelism-compare}\n\n@fig-parallelism-flowchart provides a general guideline for selecting parallelism strategies in distributed training systems. While the chart offers a structured decision-making process based on model size, dataset size, and scaling constraints, it is intentionally simplified. Real-world scenarios often involve additional complexities such as hardware heterogeneity, communication bandwidth, and workload imbalance, which may influence the choice of parallelism techniques. The chart is best viewed as a foundational tool for understanding the trade-offs and decision points in parallelism strategy selection. Practitioners should consider this guideline as a starting point and adapt it to the specific requirements and constraints of their systems to achieve optimal performance.\n\n::: {#fig-parallelism-flowchart fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=1.0pt,black!50,text=black\n},\n  Box/.style={inner xsep=2pt,\n    node distance=11mm,\n    draw=GreenLine, line width=0.75pt,\n    fill=GreenL,\n    text width=27mm,align=flush center,\n    minimum width=27mm, minimum height=9mm\n  },\n    Box1/.style={Box,\n    draw=RedLine, fill=RedL,\n    text width=31mm,\n    minimum width=32mm,\n    minimum height=10mm\n  },\n  Text/.style={inner xsep=2pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor,\n    font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n    align=flush center,\n    minimum width=7mm,\n    minimum height=5mm\n  },\n decision/.style = {align=flush center,text width=42mm,diamond, aspect=2.2, node distance=6mm,\n                             inner xsep=-3pt,  inner ysep=-2.95ex,fill=VioletL2, draw=VioletLine},\n}\n\\node[Box](B1){Hybrid\\\\ Parallelism};\n\\node[Box,node distance=16mm,right=of B1](B2){Model\\\\Parallelism};\n\\node[Box,node distance=16 mm,right=of B2](B3){Data\\\\ Parallelism};\n\\node[Box,right=of B3,fill=RedL, draw=RedLine](B4){Single Device Optimization};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,\nyshift=-1mm,\nfill=BackColor,fit=(B1)(B3),line width=0.75pt](BB){};\n\\node[decision,node distance=18mm,\nabove=of B4](G1B4){Is\\\\ the dataset\\\\ very large?};\n\n\\node[Box1,node distance=15mm,\nabove=of $(B2.north)!0.5!(B3.north)$](G1B3){Is scaling the model\\\\ or data more critical?};\n\\node[decision,above=of G1B3](G2B3){Are\\\\ both constraints\\\\ significant?};\n\\node[decision,above=of G2B3](G3B3){Does\\\\ the dataset fit in a\\\\  single device?};\n\\node[decision,above=of G3B3](G4B3){Does\\\\ the model fit in a\\\\ single device?};\n\\node[Box,node distance=5mm,above=of G4B3,fill=BlueL, draw=BlueLine](G5B3){Start};\n%\n\\node[Box,below=1 of B2,fill=BlueL, draw=BlueLine](DB2){End};\n%\n\\draw[Line,-latex](G5B3)--(G4B3);\n\\draw[Line,-latex](G4B3)--node[right,pos=0.35]{No}(G3B3);\n\\draw[Line,-latex](G4B3)-|node[above,pos=0.05]{Yes}(G1B4);\n\\draw[Line,-latex](G3B3)--node[right,pos=0.35]{No}(G2B3);\n\\draw[Line,-latex](G2B3)--node[right,pos=0.35]{No}(G1B3);\n\\draw[Line,-latex](G1B4)--node[right,pos=0.15]{No}(B4);\n%\n\\draw[Line,-latex](G3B3.west)--node[above,pos=0.25]{Yes}++(180:2.3)|-(B2.west);\n\\draw[Line,-latex](G2B3)-|node[above,pos=0.05]{Yes}(B1);\n\\draw[Line,-latex](G1B3.south)--node[left,align=center,pos=0.45]{Scaling Model}++(270:8mm)-|(B2);\n\\draw[Line,-latex](G1B3.south)--++(270:8mm)-|(B3);\n\\draw[Line,-latex](G1B4)-|node[above,pos=0.22,text=black]{Yes}(B3.40);\n%\n\\draw[Line,-latex](B1)|-(DB2);\n\\draw[Line,-latex](B3)|-(DB2);\n\\draw[Line,-latex](B2)--(DB2);\n\\node[above=2pt of  BB.204,inner sep=0pt,anchor=south,fill=BackColor]{Parallelism Opportunities};\n\\end{tikzpicture}\n```\nDistributed training systems use data, model, or hybrid parallelism based on model size, dataset size, and scaling constraints to accelerate training and efficiently utilize resources. This flowchart guides practitioners through a decision process, recognizing that real-world deployments often require adaptation due to factors like hardware heterogeneity and workload imbalance.\n:::\n\n## Framework Integration {#sec-distributed-training-framework-integration}\n\nWhile the theoretical foundations of distributed training establish the mathematical principles for scaling across multiple devices, modern frameworks provide abstractions that make these concepts accessible to practitioners. Understanding how frameworks like PyTorch translate distributed training theory into practical APIs bridges the gap between mathematical concepts and implementation.\n\n### Data Parallel Framework APIs {#sec-distributed-training-data-parallel-framework-apis}\n\nThe data parallelism mechanisms we explored earlier—gradient averaging, AllReduce communication, and parameter synchronization—are abstracted through framework APIs that handle the complex coordination automatically. PyTorch provides two primary approaches that demonstrate different trade-offs between simplicity and performance.\n\n`torch.nn.DataParallel` represents the simpler approach, automatically replicating the model across available GPUs within a single node. This API abstracts the gradient collection and averaging process, requiring minimal code changes to existing single-GPU training scripts. However, this simplicity comes with performance limitations, as the implementation uses a parameter server approach that can create communication bottlenecks when scaling beyond 4-8 GPUs.\n\n```python\n# Simple data parallelism - framework handles gradient synchronization\nmodel = torch.nn.DataParallel(model)\n# Training loop remains unchanged - framework automatically:\n# 1. Splits batch across GPUs\n# 2. Replicates model on each device\n# 3. Gathers gradients and averages them\n# 4. Broadcasts updated parameters\n```\n\nFor production scale training, `torch.distributed` provides the high-performance alternative that implements the efficient AllReduce communication patterns discussed earlier. This API requires explicit initialization of process groups and distributed coordination but enables the linear scaling characteristics essential for large-scale training.\n\n```python\n# Production distributed training - explicit control over communication\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\")  # NCCL for GPU communication\nmodel = torch.nn.parallel.DistributedDataParallel(model)\n# Framework now uses optimized AllReduce instead of parameter server\n```\n\nThe key insight is that `DistributedDataParallel` implements the efficient ring AllReduce algorithm automatically, transforming the O(n) communication complexity we discussed into practical code that achieves 90%+ parallel efficiency at scale. The framework handles device placement, gradient bucketing for efficient communication, and overlapping computation with communication.\n\n### Model Parallel Framework Support {#sec-distributed-training-model-parallel-framework-support}\n\nModel parallelism requires more explicit coordination since frameworks must manage cross-device tensor placement and data flow. PyTorch addresses this through manual device placement and the emerging `torch.distributed.pipeline` API for pipeline parallelism.\n\n```python\n# Manual model parallelism - explicit device placement\nclass ModelParallelNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers_gpu0 = nn.Sequential(...).to(\"cuda:0\")\n        self.layers_gpu1 = nn.Sequential(...).to(\"cuda:1\")\n\n    def forward(self, x):\n        x = self.layers_gpu0(x.to(\"cuda:0\"))\n        x = self.layers_gpu1(\n            x.to(\"cuda:1\")\n        )  # Cross-GPU data transfer\n        return x\n```\n\nThis manual approach exposes the sequential dependencies and communication overhead inherent in model parallelism, requiring careful management of tensor movement between devices. The framework automatically handles the backward pass gradient flow across device boundaries, but practitioners must consider the performance implications of frequent device transfers.\n\n### Communication Primitives {#sec-distributed-training-communication-primitives}\n\nModern frameworks expose the communication operations that enable distributed training through high-level APIs. These primitives abstract the low-level NCCL operations while maintaining performance:\n\n```python\n# Framework-provided collective operations\ndist.all_reduce(tensor)  # Gradient averaging across all devices\ndist.broadcast(tensor, src=0)  # Parameter broadcasting from master\ndist.all_gather(\n    tensor_list, tensor\n)  # Collecting tensors from all devices\n```\n\nThese APIs translate directly to the NCCL collective operations that implement the efficient communication patterns discussed earlier, demonstrating how frameworks provide accessible interfaces to complex distributed systems concepts while maintaining the performance characteristics essential for production training.\n\nThe framework abstractions enable practitioners to focus on model architecture and training dynamics while leveraging sophisticated distributed systems optimizations. This separation of concerns—mathematical foundations handled by the framework, model design controlled by the practitioner—exemplifies how modern ML systems balance accessibility with performance.\n\n## Hardware Infrastructure for Scale {#sec-distributed-training-hardware-infrastructure}\n\nThe parallelism strategies examined in previous sections assume underlying hardware capable of efficient inter-device communication. This section examines the hardware architectures that enable AI systems to scale from individual accelerators to warehouse-scale computing, analyzing how multi-chip systems introduce qualitatively different constraints around communication overhead, memory coherence, and fault tolerance.\n\n### Multi-Chip AI Acceleration {#sec-distributed-training-multichip-acceleration}\n\nThe transition from single-chip to multi-chip architectures represents more than simple replication. It requires rethinking how computations distribute across processors, how data flows between chips, and how systems maintain coherence at scale. Where single-chip optimization focuses on maximizing utilization within fixed resources, multi-chip systems must balance computational distribution against communication overhead, memory coherence costs, and synchronization complexity. These challenges fundamentally transform the optimization landscape, requiring new abstractions and techniques beyond those developed for individual accelerators.\n\nThe scaling of AI systems follows a natural progression: integration within a single package through chiplet architectures, multi-GPU configurations within a server, distributed accelerator pods, and wafer-scale integration. Each approach presents unique trade-offs between computational density, communication overhead, and system complexity. Chiplet architectures maintain high-speed interconnects within a package, while distributed systems sacrifice communication latency for massive parallelism.\n\n#### Chiplet-Based Architectures {#sec-distributed-training-chiplet-architectures}\n\nChiplet architectures achieve scaling by partitioning large designs into smaller, modular dies that are interconnected within a single package. Small, specialized semiconductor dies connect together within a single package to create larger, more complex processors. AMD's EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies, with compute chiplets in 7 nm paired with I/O chiplets in 14 nm, optimizing each function independently.\n\nModern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning. AMD's Instinct MI300 exemplifies this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.\n\nHowever, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence, and thermal management become critical factors as more chiplets are integrated. Memory coherence presents particular complexity: ensuring all processors in a system see the same consistent view of shared memory when multiple cores or chips access the same data. Traditional cache coherence protocols like MESI add 10-50 ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive, so most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.\n\n#### Multi-GPU Systems {#sec-distributed-training-multi-gpu-systems}\n\nBeyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.\n\nA common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network).\n\nNVSwitch interconnects enable high-speed communication between GPUs, reducing bottlenecks in distributed training. However, scaling up the number of GPUs introduces fundamental distributed coordination challenges that become the dominant performance constraint. The arithmetic intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient synchronization across GPUs, where AllReduce operations must aggregate 175 billion parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth, but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds available capacity.\n\nThe coordination complexity compounds exponentially. While two GPUs require a single communication channel, eight GPUs need 28 interconnect paths, and fault tolerance requirements mandate redundant communication patterns. Memory consistency protocols further complicate coordination as different GPUs may observe weight updates at different times, requiring sophisticated synchronization primitives that can add 10-50 microseconds latency per training step. These seemingly small delays aggregate to hours of training time across million-iteration runs.\n\n#### Communication Overhead and Amdahl's Law {#sec-distributed-training-amdahl-analysis}\n\nThe fundamental limitation of distributed AI training stems from Amdahl's Law, which quantifies how communication overhead constrains parallel speedup regardless of available compute power. For distributed neural network training, communication overhead during gradient synchronization creates a sequential bottleneck that limits scalability even with infinite parallelism.\n\nThe maximum speedup achievable with distributed training is bound by Amdahl's Law:\n$$\n\\text{Speedup} = \\frac{1}{(1-P) + \\frac{P}{N}}\n$$\nwhere $P$ is the fraction of work that can be parallelized and $N$ is the number of processors. For AI training, the correct formulation accounts for communication time that does not decrease with more workers:\n$$\n\\text{Speedup} = \\frac{T_{compute}}{T_{compute}/N + T_{comm}}\n$$\nwhere $T_{comm}$ is largely independent of $N$ for ring AllReduce (or grows as $\\log N$ for tree-based approaches). This can be rewritten as:\n$$\n\\text{Speedup} = \\frac{N}{1 + N \\cdot (T_{comm}/T_{compute})}\n$$\n\nConsider training a 175 B parameter model with 1000 H100 GPUs as a concrete example:\n\n- **Computation time per iteration**: 100 ms of forward/backward passes per GPU\n- **Gradient size**: 175 B parameters × 4 bytes = 700 GB in FP32\n- **Ring AllReduce time**: For ring AllReduce, each GPU sends/receives $2 \\times (N-1)/N \\times 700\\text{GB} \\approx 1.4\\text{TB}$\n\nWith ring AllReduce across 1000 GPUs connected via 600 GB/s links:\n\n- **Intra-node (8 GPUs via NVLink)**: $1.4\\text{TB} / 600\\text{GB/s} \\approx 2.3$ seconds\n- **Inter-node adds latency**: $1000 \\times \\alpha$ where $\\alpha \\approx 1\\mu s$ per hop\n\nThe resulting scaling efficiency is:\n$$\n\\text{Efficiency} = \\frac{T_{compute}}{T_{compute} + T_{comm}} = \\frac{100\\text{ms}}{100\\text{ms} + 2300\\text{ms}} \\approx 4\\%\n$$\n\nThis is why real systems use mixed precision (FP16 = 350 GB, halving communication), gradient compression, and pipeline parallelism. With FP16 and 4-way pipeline parallelism reducing synchronization to 1/4 of parameters per stage, efficiency improves dramatically:\n$$\n\\text{Efficiency} = \\frac{100\\text{ms}}{100\\text{ms} + 290\\text{ms}} \\approx 26\\%\n$$\n\nThis demonstrates why pure data parallelism fails at scale and why hybrid strategies are essential.\n\nCommunication requirements scale superlinearly with model size and linearly with the number of parameters. Modern transformer models require gradient synchronization across all parameters during each training step:\n\n- **GPT-3 (175 B parameters)**: 700 GB gradient exchange per step\n- **GPT-4 (estimated 1.8 T parameters)**: approximately 7 TB gradient exchange per step\n- **Future 10 T parameter models**: approximately 40 TB gradient exchange per step\n\nEven with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization for 10 T parameter models would require 22+ seconds per training step, making distributed training impractical without algorithmic innovations like gradient compression or asynchronous updates.\n\n#### TPU Pods {#sec-distributed-training-tpu-pods}\n\nAs models and datasets continue to expand, training and inference workloads must extend beyond single-server configurations. Google's TPU Pods represent a pioneering approach to this challenge, interconnecting hundreds of TPUs to function as a unified system.\n\nThe architectural design of TPU Pods differs fundamentally from traditional multi-GPU systems. While multi-GPU configurations typically rely on NVLink or PCIe connections within a single machine, TPU Pods employ high-bandwidth optical links to interconnect accelerators at data center scale. This design implements a 2D torus interconnect topology, enabling efficient data exchange between accelerators while minimizing communication bottlenecks as workloads scale across nodes.\n\nThe effectiveness of this architecture is demonstrated in its performance scaling capabilities. TPU Pod performance exhibits near-linear scaling when running ResNet-50, from quarter-pod to full-pod configurations. The system achieves a remarkable 33.0x speedup when scaled to 1024 chips compared to a 16-TPU baseline.\n\nHowever, distributing AI workloads across an entire data center introduces distributed coordination challenges that fundamentally differ from single-node systems. The 2D torus interconnect, while providing high bisection bandwidth, creates communication bottlenecks when training large transformer models that require AllReduce operations across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through the torus network, with worst-case communication requiring 32 hops between distant TPUs, creating latency penalties that compound with model size.\n\nThe energy cost of coordination also scales dramatically: moving data across the pod's optical interconnect consumes 1000x more energy than on-chip communication within individual TPUs, transforming distributed training into a careful balance between computation parallelism and communication efficiency where AllReduce bandwidth, not compute capacity, determines overall training throughput.\n\n#### Wafer-Scale AI {#sec-distributed-training-wafer-scale}\n\nAt the frontier of AI scaling, wafer-scale integration represents a paradigm shift that abandons traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication.\n\nWafer-scale integration uses an entire 300 mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores, 125x more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23 kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.\n\nThe primary advantage of wafer-scale AI is its ultra-fast on-die communication. Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries between separate devices, wafer-scale AI enables near-instantaneous data transfer across its vast compute array. This architecture drastically reduces communication latency, achieving performance levels that remain out of reach for conventional multi-chip systems.\n\nAchieving this level of integration introduces substantial engineering challenges. Thermal dissipation, fault tolerance, and manufacturing yield become major constraints when fabricating a processor of this scale. Unlike distributed TPU systems, which mitigate failures by dynamically re-routing workloads, wafer-scale AI must incorporate built-in redundancy mechanisms to tolerate localized defects in the silicon.\n\n### Hardware Scaling Trade-offs {#sec-distributed-training-scaling-tradeoffs}\n\nThe progressive scaling of AI acceleration introduces new challenges at each step, from single-chip processors to increasingly complex architectures. @tbl-distributed-scaling-trajectory summarizes these trade-offs across different scaling approaches.\n\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **Scaling Approach** | **Key Feature**                     | **Challenges**                                      |\n+:=====================+:====================================+:====================================================+\n| **Chiplets**         | Modular scaling within a package    | Inter-chiplet latency, memory coherence             |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **Multi-GPU**        | External GPU interconnects (NVLink) | Synchronization overhead, communication bottlenecks |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **TPU Pods**         | Distributed accelerator clusters    | Interconnect congestion, workload partitioning      |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **Wafer-Scale AI**   | Entire wafer as a single processor  | Thermal dissipation, fault tolerance                |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n\n: **AI Acceleration Scaling Trade-offs**: Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution. {#tbl-distributed-scaling-trajectory}\n\nWhile chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.\n\n### Multi-Chip Execution Strategies {#sec-distributed-training-execution-strategies}\n\nAs AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.\n\nExecution mapping in multi-chip systems requires computation placement that considers workload partitioning across multiple accelerators, with explicit coordination of execution order and dependencies. Computation scheduling must be interconnect-aware to manage communication delays effectively. Load balancing across accelerators is vital, as uneven task distribution results in some accelerators remaining underutilized while others operate at full capacity.\n\nDistributed memory allocation requires each accelerator to manage its own local memory, necessitating explicit allocation of model parameters, activations, and intermediate data across devices. Unlike single-chip execution where data is fetched once and reused, multi-chip setups require deliberate strategies to minimize redundant data transfers.\n\nData movement optimization addresses inter-chip data transfer, which becomes the primary bottleneck rather than memory hierarchy latency. Techniques include overlapping computation and communication so accelerators process data while simultaneously sending and receiving, and locality-aware scheduling that places computations on accelerators already holding required data.\n\nCompiler and runtime adaptations extend single-chip execution models to handle dynamic workload distribution across accelerators. Interconnect-aware workload partitioning enables compilers to distribute computations strategically based on communication cost. In TPU Pods, the runtime schedules computations across multiple TPU cores to minimize communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution synchronizes operations across GPUs while maintaining execution order.\n\n## Summary {#sec-distributed-training-summary}\n\nDistributed training transforms the computational challenge of training large-scale machine learning models into an orchestration problem across multiple devices and hardware architectures. Throughout this chapter, we have examined how parallelism strategies—data, model, pipeline, and hybrid—address different constraints in the training process. Data parallelism scales dataset processing by distributing training examples across devices while synchronizing gradients, achieving near-linear speedups when communication overhead remains manageable. Model parallelism addresses memory constraints by partitioning models across devices, enabling training of architectures that exceed single-device capacity. Pipeline parallelism improves hardware utilization through microbatching, while hybrid approaches combine these strategies for the largest-scale training workloads.\n\nThe hardware infrastructure enabling distributed training spans multiple scales: chiplet-based architectures within a single package, multi-GPU systems connected via NVLink and NVSwitch, TPU Pods with 2D torus interconnects, and wafer-scale integration. Each approach presents distinct trade-offs between communication bandwidth, memory coherence, and system complexity. Amdahl's Law quantifies the fundamental scaling limits: communication overhead during gradient synchronization creates sequential bottlenecks that constrain speedup regardless of available compute power, explaining why adding GPUs beyond approximately 100 provides diminishing returns without algorithmic innovations.\n\nThe efficiency metrics governing distributed training—communication overhead, scaling efficiency, and synchronization costs—directly influence system design decisions. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads, balancing the complexity of distributed coordination against the computational benefits of parallelization. Framework abstractions like PyTorch's DistributedDataParallel translate these concepts into practical implementations, allowing practitioners to leverage sophisticated distributed systems optimizations while focusing on model development.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Data parallelism achieves near-linear scaling when communication overhead remains below 30-40% of training time\n* Model parallelism enables training of models exceeding single-device memory but introduces sequential dependencies\n* Pipeline parallelism reduces device idle time through microbatching, improving hardware utilization\n* Hybrid parallelism combines strategies for training the largest models on the largest datasets\n* Multi-chip hardware (chiplets, multi-GPU, TPU Pods, wafer-scale) each present distinct trade-offs between integration density, interconnect bandwidth, and system complexity\n* Amdahl's Law quantifies scaling limits: communication overhead constrains speedup regardless of available compute power\n* Framework APIs abstract distributed complexity while preserving the performance characteristics essential for production training\n:::\n\n## Fallacies and Pitfalls {#sec-distributed-training-fallacies-pitfalls}\n\nDistributed training involves counterintuitive behavior that leads to common misconceptions. These fallacies and pitfalls capture errors that waste compute resources and delay research progress.\n\nLinear speedup remains theoretically impossible regardless of engineering effort. Amdahl's Law establishes hard limits: any sequential component bounds maximum speedup regardless of parallelism. In distributed training, gradient synchronization is inherently sequential since all gradients must be collected before any update proceeds.\n\nEven with perfect overlap and optimal algorithms, communication overhead grows with cluster size. For data parallelism, AllReduce time increases logarithmically with tree algorithms or linearly in the latency term with ring algorithms as GPU count grows. A 1000-GPU cluster will never train 1000x faster than a single GPU; achieving 500x speedup would be exceptional, and 100-200x is more typical for communication-heavy workloads.\n\nOrganizations that budget projects assuming linear scaling inevitably miss deadlines and overspend on compute.\n\nHyperparameters tuned on small clusters fail catastrophically at large scale. The most critical is learning rate: as batch size increases with data parallelism, learning rate typically must increase proportionally to maintain convergence rate. The \"linear scaling rule\" suggests $\\eta_{large} = \\eta_{base} \\times (B_{large}/B_{base})$.\n\nHowever, this rule has limits. Beyond the \"critical batch size\" (model and dataset dependent, often 8K-32K for vision models), increasing batch size provides diminishing returns. Larger batches find sharper minima that generalize poorly. Training that converges beautifully at 256 GPUs may diverge or produce worse models at 1024 GPUs with naive scaling.\n\nWarmup schedules, weight decay, and dropout rates also require adjustment. The only reliable approach is systematic scaling studies that validate hyperparameters at target scale.\n\nData parallelism does not scale indefinitely by adding more GPUs. Data parallelism increases effective batch size proportionally with GPU count, but statistical efficiency (loss reduction per sample) decreases with batch size beyond model-specific thresholds. A 100K-sample batch may provide only 2x the gradient quality of a 10K-sample batch, not 10x.\n\nThe critical batch size defines where marginal returns collapse. Beyond this point, additional GPUs increase throughput (samples per second) but not training efficiency (loss reduction per compute dollar). For BERT, critical batch size is approximately 8K; for ResNet, approximately 32K. Scaling beyond critical batch size wastes compute.\n\nLarge organizations have trained models to convergence using 1024 GPUs in the same wall-clock time as 512 GPUs at 2x the cost, because they exceeded critical batch size.\n\nPipeline parallelism and tensor parallelism both distribute model weights across devices, but their memory and compute characteristics differ dramatically.\n\nTensor parallelism splits each layer across devices, requiring AllReduce communication within each layer. This reduces memory proportionally but introduces communication overhead on the critical path. Pipeline parallelism assigns complete layers to devices, requiring only point-to-point communication between stages but introducing bubble overhead.\n\nFor memory-constrained scenarios where a model barely fits with splitting, tensor parallelism's even distribution helps. For throughput-maximizing scenarios with adequate memory, pipeline parallelism's lower communication overhead helps. Choosing based on one dimension (memory or compute) without considering the other leads to suboptimal configurations.\n\nFSDP and ZeRO do not always improve training efficiency. FSDP (Fully Sharded Data Parallel) and ZeRO reduce memory footprint by sharding optimizer state and gradients across GPUs. This enables larger batch sizes or larger models per GPU. However, sharding introduces communication overhead: AllGather before forward pass, ReduceScatter after backward pass.\n\nFor models that fit comfortably in GPU memory without sharding, FSDP adds overhead without benefit. A 7B model training on A100-80GB with batch size 4 runs faster with DDP than FSDP because the model fits entirely with room for activations.\n\nFSDP provides value when:\n\n- Model + optimizer state exceeds single-GPU memory\n- Enabling larger batch sizes justifies communication overhead\n- ZeRO-Offload to CPU extends effective memory\n\nApplying FSDP universally, as some tutorials suggest, degrades performance for models that do not require it.\n\nParallelism overhead is roughly constant regardless of model size: AllReduce time depends on gradient size, not model computation time. For small models where forward/backward pass takes 10ms and AllReduce takes 5ms, communication overhead is 50%. For large models where forward/backward takes 1000ms and AllReduce takes 5ms, overhead is 0.5%.\n\nDecisions made based on small-model benchmarks (\"pipeline parallelism is always slower\") invert at scale. The 20% overhead acceptable for a 1B model becomes 0.2% for a 100B model. Parallelism strategy evaluation must occur at target scale, or at minimum with analytical models that extrapolate appropriately.\n\nGradient accumulation is not free. Gradient accumulation simulates larger batch sizes by accumulating gradients across multiple forward/backward passes before synchronizing. This reduces communication frequency proportionally. However, accumulation has costs:\n\n1. **Memory**: Accumulated gradients consume memory throughout the accumulation window\n2. **Latency**: Effective step time increases proportionally with accumulation steps\n3. **Precision**: Accumulated FP16 gradients may overflow or underflow\n\nFor loss-sensitive early training, gradient accumulation can introduce instability from accumulated numerical errors. Organizations that use gradient accumulation to work around infrastructure limitations (slow network, small GPUs) sometimes discover training divergence that disappears with proper infrastructure.\n\nThe principles established in this chapter provide the foundation for understanding fault tolerance mechanisms, which become increasingly critical as distributed training scales to thousands of devices where failures become statistically inevitable.\n","srcMarkdownNoYaml":"\n\n<!--\n================================================================================\nEDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR DISTRIBUTED TRAINING\n================================================================================\n\nCORE PRINCIPLE: Distributed training techniques apply across ALL model types,\nnot just LLMs. Ensure examples span the full spectrum of production ML.\n\nMODEL-SPECIFIC PARALLELISM CONSIDERATIONS:\n\n| Model Type      | Primary Strategy      | Key Challenge                        |\n|-----------------|-----------------------|--------------------------------------|\n| LLMs            | Tensor + Pipeline     | Attention memory, autoregressive     |\n| Recommendation  | Embedding sharding    | Trillion-param embedding tables      |\n| Vision (ResNet) | Data parallelism      | Batch size scaling, BN sync          |\n| Vision (ViT)    | Tensor parallelism    | Large attention layers               |\n| Scientific/GNN  | Graph partitioning    | Irregular communication patterns     |\n| Speech          | Data parallelism      | Variable sequence lengths            |\n\nREQUIRED COVERAGE FOR THIS CHAPTER:\n\nDATA PARALLELISM:\n\n- ResNet/EfficientNet: Classic example, batch norm synchronization\n- BERT: Widely used, good baseline for transformer data parallelism\n- Recommendation: Different gradient sparsity patterns\n\nMODEL PARALLELISM:\n\n- GPT/Megatron: Tensor parallelism for large transformers\n- DLRM: Embedding table sharding (FUNDAMENTALLY DIFFERENT from tensor parallelism)\n- Include: Why embedding parallelism differs from attention parallelism\n\nPIPELINE PARALLELISM:\n\n- GPipe: Original work on vision models\n- Megatron-LM: Application to transformers\n- Include: Micro-batch scheduling differences by model type\n\nHYBRID PARALLELISM:\n\n- 3D parallelism for LLMs (data + tensor + pipeline)\n- Embedding + data parallelism for recommendation\n- Include: Why different model types need different hybrid strategies\n\nCASE STUDIES TO INCLUDE:\n\n- Meta DLRM training infrastructure (recommendation)\n- Google BERT/T5 training (NLP)\n- ResNet ImageNet training (vision baseline)\n- AlphaFold distributed training (scientific)\n\nQUANTITATIVE DIVERSITY:\n\n- Communication/computation ratios differ by model type\n- Scaling efficiency curves differ (dense vs sparse models)\n- Memory footprint breakdown differs (activations vs embeddings vs weights)\n\nANTI-PATTERNS TO AVOID:\n\n- Framing all parallelism as \"for large language models\"\n- Ignoring embedding table challenges unique to recommendation\n- Assuming dense gradients (recommendation has sparse gradients)\n- Only showing transformer examples for tensor parallelism\n\n================================================================================\n-->\n\n# Distributed Training Systems {#sec-distributed-training}\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A technical illustration showing multiple interconnected GPU clusters working in coordination to train a massive neural network. The scene depicts hundreds of GPU nodes arranged in a circular pattern, connected by luminous data streams representing gradient synchronization. At the center, a giant neural network model spans across all nodes, with each node responsible for a different portion. Visual elements include ring allreduce patterns showing data flowing between nodes, pipeline stages depicted as sequential processing units, and synchronization barriers represented as glowing checkpoints. The color palette uses deep blues and electric purples for computation, with bright orange and gold for communication paths. The style is technical and precise, suitable for an advanced distributed systems textbook._\n:::\n\n\\noindent\n![](images/png/cover_distributed.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_What makes distributed training a fundamental shift in how we approach machine learning at scale, rather than merely a technical optimization?_\n\nDistributed training becomes an architectural necessity when training demands exceed the capabilities of individual machines, and the principles governing coordination across distributed systems fundamentally reshape how we approach optimization, fault tolerance, and cost efficiency. Moving from single-machine to distributed training requires understanding how communication overhead, synchronization costs, and system heterogeneity create entirely new constraints that affect algorithm design and system architecture. The challenges of distributed training, including parameter synchronization across thousands of devices, absorbing failures without losing progress, and balancing communication with computation, demand different engineering solutions than single-device scenarios. As models grow to billions of parameters and training datasets expand to continental scales, distributed training becomes the foundation upon which feasible machine learning systems are built, directly determining which problems can be tackled, which timelines are realistic, and which organizations can meaningfully participate in advanced AI development.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain data parallelism mechanisms including gradient computation, synchronization via AllReduce algorithms (ring, tree, hierarchical), and the relationship between batch size scaling and convergence behavior\n\n- Analyze multi-machine training requirements by identifying when models exceed single-device memory, when training duration becomes unacceptable, and when datasets exceed single-machine storage, using quantitative thresholds\n\n- Implement data parallel training by applying gradient synchronization algorithms and achieving target parallel efficiency of 85-95% in the linear scaling regime (2-32 devices)\n\n- Design model parallelism strategies using tensor parallelism, pipeline parallelism with microbatching, and embedding sharding to train models exceeding single-device memory while managing pipeline bubble overhead\n\n- Construct hybrid parallelism systems combining data, model, and pipeline strategies across multi-node clusters, selecting appropriate combinations for different model architectures (transformers, recommendation systems, vision models)\n\n- Evaluate distributed training efficiency using quantitative metrics including communication-computation ratio, scaling efficiency, bandwidth utilization, and synchronization costs\n\n:::\n\n## Multi-Machine Scaling Fundamentals {#sec-distributed-training-multimachine-scaling-fundamentals}\n\nPart I established the infrastructure foundations that make distributed training possible. The datacenter architectures and accelerator topologies examined in @sec-infrastructure provide the compute fabric, while the distributed storage systems and data pipelines developed in @sec-storage ensure training data flows efficiently to thousands of workers. These foundations answer the question of what physical resources exist. Part II addresses the central question those resources enable: how do we coordinate training across distributed infrastructure to achieve performance that no single machine could deliver?\n\nThe transition from single-machine to distributed training represents a major shift in optimization strategy and system complexity. Single-machine optimization focuses on efficiently utilizing available resources through techniques such as prefetching, mixed precision, and gradient accumulation. Distributed training introduces different challenges: communication overhead, fault tolerance, and synchronization that require new conceptual frameworks and engineering approaches.\n\n### Multi-Machine Training Requirements {#sec-distributed-training-multimachine-training-requirements}\n\nThree concrete signals indicate when distributed training becomes necessary rather than merely beneficial. First, memory exhaustion occurs when model parameters, optimizer states, and activation storage exceed single-device capacity even after applying gradient checkpointing and mixed precision. For transformer models, this threshold typically occurs around 10-20 billion parameters on current generation GPUs with 40-80GB memory [@rajbhandari2020zero]. Second, unacceptable training duration emerges when single-device training would require weeks or months to converge, making iteration impossible. Training GPT-3 on a single V100 GPU would require approximately 355 years [@brown2020language], making distributed approaches not optional but essential. Third, dataset scale exceeds single-machine storage when training data reaches multiple terabytes, as occurs in large-scale vision or language modeling tasks.\n\n### Distributed Training Complexity Trade-offs {#sec-distributed-training-complexity-tradeoffs}\n\nDistributed training introduces three primary complexity dimensions absent from single-machine scenarios. Communication overhead emerges from gradient synchronization, where each training step must aggregate gradients across all devices. For a model with $N$ parameters distributed across $D$ devices, all-reduce operations must transfer approximately $2N(D-1)/D$ bytes per step. On commodity network infrastructure (10-100 Gbps), this communication can dominate computation time for models under 1 billion parameters [@sergeev2018horovod]. Fault tolerance requirements increase exponentially with cluster size: a 100-node cluster with 99.9% per-node reliability experiences failures every few hours on average, necessitating checkpoint and recovery mechanisms. Algorithmic considerations change because distributed training alters optimization dynamics—large batch sizes from data parallelism affect convergence behavior, requiring learning rate scaling and warmup strategies that single-machine training does not require [@goyal2017accurate].\n\n### Single-Machine to Distributed Transition {#sec-distributed-training-singlemachine-distributed-transition}\n\nThe systematic optimization methodology established for single-machine training extends to distributed environments with important adaptations. Profiling must now capture inter-device communication patterns and synchronization overhead in addition to computation and memory metrics. Tools like NVIDIA Nsight Systems and PyTorch's distributed profiler reveal whether training is communication-bound or computation-bound, guiding the choice between parallelization strategies. The solution space expands from single-machine techniques to include data parallelism (distributing training examples), model parallelism (distributing model parameters), pipeline parallelism (distributing model layers), and hybrid approaches that combine multiple strategies. The principles remain consistent—identify bottlenecks, select appropriate techniques, compose solutions—but the implementation complexity increases substantially.\n\n## Distributed Training Fundamentals {#sec-distributed-training-fundamentals}\n\nBuilding upon single-machine optimization foundations, distributed training extends systematic optimization to multiple machines. When single-machine techniques have been exhausted—prefetching eliminates data loading bottlenecks, mixed-precision maximizes memory efficiency, and gradient accumulation reaches practical limits—distributed approaches provide the next level of scaling capability.\n\n::: {.callout-definition title=\"Distributed Training\"}\n\n***Distributed Training*** is the parallelization of model training across _multiple compute devices_ through coordinated _data partitioning_ and _gradient synchronization_, enabling training of models that exceed single-device memory or time constraints.\n\n:::\n\nThe progression from single-machine to distributed training follows a natural scaling path: optimize locally first, then scale horizontally. This progression ensures that distributed systems operate efficiently at each node while adding the coordination mechanisms necessary for multi-machine training. Training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training[^fn-distributed-training] addresses this challenge by spreading the workload across multiple machines, which coordinate to train a single model efficiently.\n\n[^fn-distributed-training]: **Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorch's DistributedDataParallel, democratizing distributed training for researchers worldwide.\n\nThis coordination relies on consensus protocols and synchronization primitives to ensure parameter updates remain consistent across nodes. While basic barrier synchronization suffices for research, production deployments require careful fault tolerance, checkpointing, and recovery mechanisms covered in @sec-fault-tolerance.\n\nThe path from single-device to distributed training involves distinct complexity stages, each building upon the previous level's challenges. Single GPU training requires only local memory management and straightforward forward/backward passes, establishing the baseline computational patterns. Scaling to multiple GPUs within a single node introduces high-bandwidth communication requirements, typically handled through NVLink[^nvlink] or PCIe connections with NCCL[^nccl] optimization, while preserving the single-machine simplicity of fault tolerance and scheduling.\n\n[^nvlink]: **NVLink**: NVIDIA's proprietary high-speed interconnect providing up to 600 GB/s bidirectional bandwidth between GPUs, roughly 10x faster than PCIe Gen4. Essential for efficient multi-GPU training as it enables rapid gradient synchronization and tensor exchanges between devices.\n\n[^nccl]: **NCCL (NVIDIA Collective Communications Library)**: Optimized library implementing efficient collective operations (AllReduce, Broadcast, etc.) for multi-GPU and multi-node communication. Automatically selects optimal communication patterns based on hardware topology, critical for distributed training performance. The leap to multi-node distributed training brings substantially greater complexity: network communication overhead, fault tolerance requirements, and cluster orchestration challenges. Each scaling stage compounds the previous challenges—communication bottlenecks intensify, synchronization overhead grows, and failure probability increases. This progression underscores why practitioners should optimize single-GPU performance before scaling, ensuring efficient resource utilization at each level.\n\n::: {.callout-note title=\"Practical Distributed Training Complexity\"}\n\nWhile frameworks like PyTorch (FSDP) and DeepSpeed abstract away much of the complexity, implementing distributed training efficiently remains a significant engineering challenge. Production deployments require careful network configuration (e.g., InfiniBand), infrastructure management (e.g., Kubernetes, Slurm), and debugging of complex, non-local issues like synchronization hangs or communication bottlenecks. For most teams, leveraging managed cloud services is often more practical than building and maintaining this infrastructure from scratch.\n\n:::\n\nThe distributed training process itself involves splitting the dataset into non-overlapping subsets, assigning each subset to a different GPU, and performing forward and backward passes independently on each device. Once gradients are computed on each GPU, they are synchronized and aggregated before updating the model parameters, ensuring that all devices learn in a consistent manner. @fig-distributed-training illustrates this process, showing how input data is divided, assigned to multiple GPUs for computation, and later synchronized to update the model collectively.\n\n::: {#fig-distributed-training fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{%\n  mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,\nminimum width=20mm,minimum height=9mm,line width=1pt},\n  mycycle/.style={circle, draw=none, fill=red, minimum width=5mm},\n  myline/.style={line width=1.15pt,draw=cyan},\n%\n  Box/.style={align= flush center,\n    inner xsep=2pt,\n    draw=RedLine,\n    line width=0.75pt,\n    fill=RedL!20,\n    text width=22mm,\n    minimum width=22mm, minimum height=8mm\n  },\n%\nLine/.style={line width=1.0pt,black!50}\n}\n\n\\begin{scope}[node distance=-1.7,local bounding box = SC1]]\n\\node[mycylinder,fill=red!30] (A) {};\n\\scoped[on background layer]\n\\node[mycylinder, above=of A,fill=red!50] (C) {};\n\\node[mycylinder, below=of A,fill=red!10] (B) {};\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(3.5,5))},local bounding box = SC2]\n\\node[mycycle] (C1) {};\n\\node[mycycle,below=of C1] (C2) {};\n\\node[mycycle,below=of C2] (C3) {};\n\\node[mycycle,below=of C3] (C4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(C1)!0.5!(C2)$] (CL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(C2)!0.5!(C3)$] (CL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(C3)!0.5!(C4)$] (CL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(C1)!0.4!(C3)$] (CD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(C2)!0.6!(C4)$] (CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {CL1, CL2, CL3, CD1, CD2} {\n    \\draw[myline] (\\y) -- (C\\x);\n  }\n}\n\\node[Box,below=0.8 of C4](B1){GPU 1};\n\\draw[myline,dashed](C4)--(B1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(11.5,5))},local bounding box = SC3]\n\\node[mycycle] (3C1) {};\n\\node[mycycle,below=of 3C1] (3C2) {};\n\\node[mycycle,below=of 3C2] (3C3) {};\n\\node[mycycle,below=of 3C3] (3C4) {};\n\\node[mycycle,fill=violet,right=0.6 of $(3C1)!0.5!(3C2)$] (3CL1) {};\n\\node[mycycle,fill=violet,right=0.6 of $(3C2)!0.5!(3C3)$] (3CL2) {};\n\\node[mycycle,fill=violet,right=0.6 of $(3C3)!0.5!(3C4)$] (3CL3) {};\n%\n\\node[mycycle,fill=green,left=0.6 of $(3C1)!0.4!(3C3)$] (3CD1) {};\n\\node[mycycle,fill=green,left=0.6 of $(3C2)!0.6!(3C4)$] (3CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {3CL1, 3CL2, 3CL3, 3CD1, 3CD2} {\n    \\draw[myline] (\\y) -- (3C\\x);\n  }\n}\n\n\\node[Box,below=0.8 of 3C4](3B1){GPU 1};\n\\draw[myline,dashed](3C4)--(3B1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(20.0,5))},local bounding box = SC4]\n\\node[mycycle] (4C1) {};\n\\node[mycycle,below=of 4C1] (4C2) {};\n\\node[mycycle,below=of 4C2] (4C3) {};\n\\node[mycycle,below=of 4C3] (4C4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(4C1)!0.5!(4C2)$] (4CL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(4C2)!0.5!(4C3)$] (4CL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(4C3)!0.5!(4C4)$] (4CL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(4C1)!0.4!(4C3)$] (4CD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(4C2)!0.6!(4C4)$] (4CD2) {};\n%\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {4CL1, 4CL2, 4CL3, 4CD1, 4CD2} {\n    \\draw[myline] (\\y) -- (4C\\x);\n  }\n}\n\\node[Box,below=0.8 of 4C4](4B1){GPU 1};\n\\draw[myline,dashed](4C4)--(4B1);\n\\end{scope}\n\\coordinate(X)at($(CD1)!0.5!(CD2)$);\n\\coordinate(Y)at($(3CD1)!0.5!(3CD2)$);\n\n\\node[fill=white,minimum height=45](ER)at($(X)!0.3!(Y)$){Error};\n\\node[fill=white,align=center,minimum height=45](CO)at($(X)!0.7!(Y)$){Compute\\\\ loss\\\\ function};\n\\draw[myline,-latex,shorten <=3mm](X)--(ER.west);\n\\draw[myline,-latex](ER.east)--(CO.west);\n\\draw[myline,-latex,shorten >=3mm](CO.east)--(Y);\n\\draw[myline,dashed](CO.south)--++(270:1)-|node[fill=white,align=center,\npos=0.25](COM){Compare\\\\ predicted\\\\ label with\\\\ annotation}\n(ER.south);\n\n\\node[fill=white,align=center,minimum height=45](OP)at($(3CL2)!0.7!(4CL2)$){Avg\\\\ global\\\\ gradient};\n\\draw[myline,latex-,shorten <=1mm](4CL2)--(OP.east);\n%\n\\draw[myline,latex-,shorten <=3mm,shorten >=3mm](CL2)--(SC1.east|-CL2);\n%\n\\draw[myline,latex-,shorten <=3mm,shorten >=3mm](CL2)-|node[fill=white,pos=0.75]{Chunk}(SC1.north);\n%\n\\path[myline,draw=none,dashed](OP.north west)--++(90:1.2)coordinate(OP1);\n\\draw[myline,dashed]($(ER.north east)!0.5!(CO.north west)$)--++(90:1.2)coordinate(ER1);\n\\coordinate (C) at ($(OP1) + (0,5mm)$);\n\\coordinate (B) at ($(ER1) + (0,5mm)$);\n\\path[red](C)-|coordinate(D1)(4CD1);\n\\path[red](B)-|coordinate(A1)(SC1);\n\\coordinate (D) at ($(D1) + (15mm,0)$);\n\\coordinate (A) at ($(A1) + (-15mm,0)$);\n\\draw[myline,dashed,shorten >=3mm,shorten <=3mm](B)--\nnode[fill=white]{Step 2 -- Compute gradients}(C);\n\\draw[myline,dashed,shorten >=3mm,pos=0.46](C)--\nnode[fill=white]{Step 3 -- Update Parameters}(D);\n\\draw[myline,dashed,shorten >=3mm,pos=0.46](B)--\nnode[fill=white]{Step 1 -- Predict a label}(A);\n\n\\node[above=0.2 of SC2]{Forward pass};\n\\node[above=0.2 of SC3]{Backward pass};\n%%%%%%%%%%%%%%%%%%%%%%%\n%down\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[node distance=0.2,shift={(3.5,-2))},local bounding box = DSC2]\n\\node[mycycle] (DC1) {};\n\\node[mycycle,below=of DC1] (DC2) {};\n\\node[mycycle,below=of DC2] (DC3) {};\n\\node[mycycle,below=of DC3] (DC4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(DC1)!0.5!(DC2)$] (DCL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(DC2)!0.5!(DC3)$] (DCL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(DC3)!0.5!(DC4)$] (DCL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(DC1)!0.4!(DC3)$] (DCD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(DC2)!0.6!(DC4)$] (DCD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {DCL1, DCL2, DCL3, DCD1, DCD2} {\n    \\draw[myline] (\\y) -- (DC\\x);\n  }\n}\n\\node[Box,above=0.8 of DC1](DB1){GPU 2};\n\\draw[myline,dashed](DC1)--(DB1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(11.5,-2))},local bounding box = DSC3]\n\\node[mycycle] (D3C1) {};\n\\node[mycycle,below=of D3C1] (D3C2) {};\n\\node[mycycle,below=of D3C2] (D3C3) {};\n\\node[mycycle,below=of D3C3] (D3C4) {};\n\\node[mycycle,fill=violet,right=0.6 of $(D3C1)!0.5!(D3C2)$] (D3CL1) {};\n\\node[mycycle,fill=violet,right=0.6 of $(D3C2)!0.5!(D3C3)$] (D3CL2) {};\n\\node[mycycle,fill=violet,right=0.6 of $(D3C3)!0.5!(D3C4)$] (D3CL3) {};\n%\n\\node[mycycle,fill=green,left=0.6 of $(D3C1)!0.4!(D3C3)$] (D3CD1) {};\n\\node[mycycle,fill=green,left=0.6 of $(D3C2)!0.6!(D3C4)$] (D3CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {D3CL1, D3CL2, D3CL3, D3CD1, D3CD2} {\n    \\draw[myline] (\\y) -- (D3C\\x);\n  }\n}\n\n\\node[Box,above=0.8 of D3C1](D3B1){GPU 2};\n\\draw[myline,dashed](D3C1)--(D3B1);\n\\end{scope}\n\n\\begin{scope}[node distance=0.2,shift={(20.0,-2))},local bounding box = DSC4]\n\\node[mycycle] (D4C1) {};\n\\node[mycycle,below=of D4C1] (D4C2) {};\n\\node[mycycle,below=of D4C2] (D4C3) {};\n\\node[mycycle,below=of D4C3] (D4C4) {};\n\\node[mycycle,fill=violet,left=0.6 of $(D4C1)!0.5!(D4C2)$] (D4CL1) {};\n\\node[mycycle,fill=violet,left=0.6 of $(D4C2)!0.5!(D4C3)$] (D4CL2) {};\n\\node[mycycle,fill=violet,left=0.6 of $(D4C3)!0.5!(D4C4)$] (D4CL3) {};\n%\n\\node[mycycle,fill=green,right=0.6 of $(D4C1)!0.4!(D4C3)$] (D4CD1) {};\n\\node[mycycle,fill=green,right=0.6 of $(D4C2)!0.6!(D4C4)$] (D4CD2) {};\n%\n\\foreach \\x in {1,2,3,4} {\n  \\foreach \\y in {D4CL1, D4CL2, D4CL3, D4CD1, D4CD2} {\n    \\draw[myline] (\\y) -- (D4C\\x);\n  }\n}\n\\node[Box,above=0.8 of D4C1](D4B1){GPU 2};\n\\draw[myline,dashed](D4C1)--(D4B1);\n\\end{scope}\n%%%%%\n\\coordinate(DX)at($(DCD1)!0.5!(DCD2)$);\n\\coordinate(DY)at($(D3CD1)!0.5!(D3CD2)$);\n\n\\node[fill=white,minimum height=45](DER)at($(DX)!0.3!(DY)$){Error};\n\\node[fill=white,align=center,minimum height=45](DCO)at($(DX)!0.7!(DY)$){Compute\\\\ loss\\\\ function};\n\\draw[myline,-latex,shorten <=3mm](DX)--(DER.west);\n\\draw[myline,-latex](DER.east)--(DCO.west);\n\\draw[myline,-latex,shorten >=3mm](DCO.east)--(DY);\n\\draw[myline,dashed](DCO.north)--++(90:1)-|node[fill=white,align=center,\npos=0.25](DCOM){Compare\\\\ predicted\\\\ label with\\\\ annotation}(DER.north);\n\n\\node[fill=white,align=center,minimum height=45](DOP)at($(D3CL2)!0.7!(D4CL2)$){Avg\\\\ global\\\\ gradient};\n\\draw[myline,latex-,shorten <=1mm](D4CL2)--(DOP.east);\n%\n\\draw[myline,latex-,shorten <=3mm,shorten >=3mm](DCL2)-|\nnode[fill=white,pos=0.75]{Chunk}(SC1.south);\n%\n\\node[below=0.2 of DSC2]{Forward pass};\n\\node[below=0.2 of DSC3]{Backward pass};\n%%%\n\\coordinate(S1)at($(3B1)!0.5!(4B1)$);\n\\coordinate(S2)at($(D3B1)!0.5!(D4B1)$);\n\\coordinate(S)at($(S1)!0.5!(S2)$);\n\n\\node[draw=none,fill=green!50!black!90,text=white,inner xsep=10pt,\n             inner ysep=9pt, outer sep=5pt](CGG)at(S){\\textbf{Calculate Global Gradients}};\n%\n\\draw[myline,shorten <=1mm](OP.west)-|(CGG.80);\n\\draw[myline,-latex,shorten <=2mm](3CL2)-|(CGG.130);\n%\n\\draw[myline,shorten <=1mm](DOP.west)-|(CGG.280);\n\\draw[myline,-latex,shorten <=2mm](D3CL2)-|(CGG.230);\n \\end{tikzpicture}\n```\nDistributed machine learning scales model training by partitioning datasets across multiple GPUs, enabling concurrent computation of gradients, and then aggregating these gradients to update shared model parameters. This approach accelerates training through parallel processing while maintaining model consistency across all devices.\n:::\n\nThis coordination introduces several key challenges that distributed training systems must address. A distributed training system must orchestrate multi-machine computation by splitting up the work, managing communication between machines, and maintaining synchronization throughout the training process. Understanding these basic requirements provides the foundation for examining the main approaches to distributed training: data parallelism, which divides the training data across machines; model parallelism, which splits the model itself; pipeline parallelism, which combines aspects of both; and hybrid approaches that integrate multiple strategies.\n\n## Distributed Training Efficiency Metrics {#sec-distributed-training-efficiency-metrics}\n\nBefore examining specific parallelism strategies, understanding the quantitative metrics that govern distributed training efficiency is essential. These metrics provide the foundation for making informed decisions about scaling strategies, hardware selection, and optimization approaches.\n\nCommunication overhead represents the primary bottleneck in distributed training systems. AllReduce operations consume 10-40% of total training time in data parallel systems, with this overhead increasing significantly at scale. For BERT-Large on 128 GPUs, communication overhead reaches 35% of total runtime, while GPT-3 scale models experience 55% overhead on 1,024 GPUs.\n\n::: {.callout-note title=\"AllReduce Communication Complexity\"}\nAllReduce complexity depends on two components: latency ($\\alpha$) and bandwidth ($\\beta$). For a message of size $M$ across $N$ workers:\n\n**Ring AllReduce**:\n\n- Time: $2(N-1) \\cdot \\alpha + 2 \\cdot \\frac{N-1}{N} \\cdot M \\cdot \\beta$\n- Bandwidth utilization: $(N-1)/N$, approaching optimal as $N$ grows\n- Each device sends/receives approximately $2M$ bytes total (not $2M \\cdot N$)\n\n**Tree AllReduce**:\n\n- Time: $2 \\log_2(N) \\cdot \\alpha + 2 \\log_2(N) \\cdot M \\cdot \\beta$\n- Bandwidth utilization: $1/\\log_2(N)$, decreasing with scale\n- Latency: $O(\\log N)$ steps\n\nThe crossover point depends on message size: tree AllReduce wins for small messages (latency-dominated), while ring AllReduce wins for large gradients (bandwidth-dominated). Modern implementations like NCCL use hierarchical algorithms that achieve tree latency within nodes (using NVLink) and ring bandwidth between nodes (using InfiniBand).\n:::\n\nInterconnect selection is critical for large-scale deployments, as these complexity characteristics determine when communication dominates computation.\n\nThe bandwidth requirements for efficient distributed training are substantial, particularly for transformer models. Efficient systems require 100-400 GB/s aggregate bandwidth per node for transformer architectures. BERT-Base needs 8 GB parameter synchronization per iteration across 64 GPUs, demanding 200 GB/s sustained bandwidth for <50ms synchronization latency. Language models with 175B parameters require 700 GB/s aggregate bandwidth to maintain 80% parallel efficiency, necessitating InfiniBand HDR or equivalent interconnects.\n\nSynchronization frequency presents a trade-off between communication efficiency and convergence behavior. Gradient accumulation reduces synchronization frequency but increases memory requirements and may impact convergence. Synchronizing every 4 steps reduces communication overhead by 60% while increasing memory usage by 3x for gradient storage. Asynchronous methods eliminate synchronization costs entirely but introduce staleness that degrades convergence by 15-30% for large learning rates.\n\nScaling efficiency follows predictable patterns across different GPU counts. In the linear scaling regime of 2-32 GPUs, systems typically achieve 85-95% parallel efficiency as communication overhead remains minimal. The communication bound regime emerges at 64-256 GPUs, where efficiency drops to 60-80% even with optimal interconnects. Beyond 512 GPUs, coordination overhead becomes dominant, limiting efficiency to 40-60% due to collective operation latency.\n\nHardware selection critically impacts these scaling characteristics. NVIDIA DGX systems with NVLink achieve 600 GB/s bisection bandwidth, enabling 90% parallel efficiency up to 8 GPUs per node. Multi-node scaling requires InfiniBand networks, where EDR (100 Gbps) supports efficient training up to 64 nodes, while HDR (200 Gbps) enables scaling to 256+ nodes with >70% efficiency.\n\nThese efficiency metrics directly influence the choice of parallelism strategy. Data parallelism works well in the linear scaling regime but becomes communication-bound at scale. Model parallelism addresses memory constraints but introduces sequential dependencies that limit efficiency. Pipeline parallelism reduces device idle time but introduces complexity in managing microbatches. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads.\n\n## Data Parallelism {#sec-distributed-training-data-parallelism}\n\nBuilding on the efficiency characteristics outlined above, data parallelism represents the most straightforward distributed approach, particularly effective in the linear scaling regime where communication overhead remains manageable. This method distributes the training process across multiple devices by splitting the dataset into smaller subsets. Each device trains a complete copy of the model using its assigned subset of the data. For example, when training an image classification model on 1 million images using 4 GPUs, each GPU would process 250,000 images while maintaining an identical copy of the model architecture.\n\nIt is particularly effective when the dataset size is large but the model size is manageable, as each device must store a full copy of the model in memory. This method is widely used in scenarios such as image classification and natural language processing, where the dataset can be processed in parallel without dependencies between data samples. For instance, when training a ResNet model on ImageNet, each GPU can independently process its portion of images since the classification of one image doesn't depend on the results of another.\n\nThe effectiveness of data parallelism stems from a property of stochastic gradient descent established in optimization foundations. Gradients computed on different minibatches can be averaged while preserving mathematical equivalence to single-device training. This property enables parallel computation across devices, with the mathematical foundation following directly from the linearity of expectation.\n\nConsider a model with parameters $θ$ training on a dataset $D$. The loss function for a single data point $x_i$ is $L(θ, x_i)$. In standard SGD with batch size $B$, the gradient update for a minibatch is:\n$$\ng = \\frac{1}{B} \\sum_{i=1}^B \\nabla_θ L(θ, x_i)\n$$\n\nIn data parallelism with $N$ devices, each device $k$ computes gradients on its own minibatch $B_k$:\n$$\ng_k = \\frac{1}{|B_k|} \\sum_{x_i \\in B_k} \\nabla_θ L(θ, x_i)\n$$\n\nThe global update averages these local gradients:\n$$\ng_{\\text{global}} = \\frac{1}{N} \\sum_{k=1}^N g_k\n$$\n\nThis averaging is mathematically equivalent to computing the gradient on the combined batch $B_{\\text{total}} = \\bigcup_{k=1}^N B_k$:\n$$\ng_{\\text{global}} = \\frac{1}{|B_{\\text{total}}|} \\sum_{x_i \\in B_{\\text{total}}} \\nabla_θ L(θ, x_i)\n$$\n\nThis equivalence shows why data parallelism maintains the statistical properties of SGD training. The approach distributes distinct data subsets across devices, computes local gradients independently, and averages these gradients to approximate the full-batch gradient.\n\nThe method parallels gradient accumulation, where a single device accumulates gradients over multiple forward passes before updating parameters. Both techniques use the additive properties of gradients to process large batches efficiently.\n\n::: {.callout-note title=\"Production Reality: Data Parallelism at Scale\"}\n\nData parallelism in production environments involves several operational considerations beyond the theoretical framework:\n\n- **Communication efficiency**: AllReduce operations for gradient synchronization become the bottleneck at scale. Production systems use optimized libraries like NCCL with ring or tree communication patterns to minimize overhead\n- **Fault tolerance**: Node failures during large-scale training require checkpoint/restart strategies. Production systems implement hierarchical checkpointing with both local and distributed storage\n- **Dynamic scaling**: Cloud environments require elastic scaling capabilities to add/remove workers based on demand and cost constraints, complicated by the need to maintain gradient synchronization\n- **Cost optimization**: Production data parallelism considers cost per GPU-hour across different instance types and preemptible instances, balancing training time against infrastructure costs\n- **Network bandwidth requirements**: Large models require careful network topology planning as gradient communication can consume 10-50% of training time depending on model size and batch size\n\nProduction teams typically benchmark communication patterns and scaling efficiency before deploying large distributed training jobs to identify optimal configurations.\n\n:::\n\n### Data Parallelism Implementation {#sec-distributed-training-data-parallelism-implementation}\n\nThe process of data parallelism can be broken into a series of distinct steps, each with its role in ensuring the system operates efficiently. These steps are illustrated in @fig-train-data-parallelism.\n\n::: {#fig-train-data-parallelism fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=0.75pt,black!50,text=black\n},\n  Box/.style={inner xsep=2pt,\n    line width=0.75pt,\n    node distance=2.0,\n    fill=VioletL2,\n    draw=VioletLine2,\n    text width=27mm,\n    align=flush center,\n    minimum width=27mm,\n    minimum height=9mm\n  },\n  Box2/.style={Box,\n    draw=BlueLine,\n    fill=BlueL,\n    text width=21mm,\n    minimum width=22mm,\n    minimum height=9mm\n  },\n  Text/.style={inner xsep=6pt,\n  inner ysep=4pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor!80,\n    font=\\usefont{T1}{phv}{m}{n}\\footnotesize,\n    align=flush center,\n    minimum width=22mm, minimum height=5mm\n  },\n}\n\n\\node[Box,node distance=1](B1){GPU 1\\\\Forward \\& Backward Pass};\n\\node[Box,node distance=1.2,right=of B1](B2){GPU 2\\\\Forward \\& Backward Pass};\n\\node[Box,node distance=1.2,right=of B2](B3){GPU 3\\\\Forward \\& Backward Pass};\n\\node[Box,node distance=1.2,right=of B3](B4){GPU 4\\\\Forward \\& Backward Pass};\n%\n\\node[Box2,above=1.06 of B1](GB1){Batch 1};\n\\node[Box2,above=1.06 of B2](GB2){Batch 2};\n\\node[Box2,above=1.06 of B3](GB3){Batch 3};\n\\node[Box2,above=1.06 of B4](GB4){Batch 4};\n%\n\\node[Box2,above=1.8of $(GB2)!0.5!(GB3)$,fill=RedL,draw=RedLine](GGB1){Input Data};\n%\n\\node[Box,below=of $(B2)!0.5!(B3)$,fill=GreenL,draw=GreenLine](DB1){Gradients GPU N};\n\\node[Box,below=1.05 of DB1,fill=GreenL,draw=GreenLine](DB2){Gradient Aggregation};\n\\node[Box,below=1.05 of DB2,fill=GreenL,draw=GreenLine](DB3){Model Update};\n%\n\\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB2);\n\\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB3);\n\\draw[Line,-latex](GGB1)--++(270:1.4)-|(GB4);\n\\draw[Line,-latex](GGB1)--node[Text,pos=0.5,anchor=center]{Split into Non-Overlapping Subsets}++(270:1.4)-|(GB1);\n%%\n\\draw[Line,-latex](GB1)--node[Text,pos=0.45]{Assigned to GPU 1}(B1);\n\\draw[Line,-latex](GB2)--node[Text,pos=0.45]{Assigned to GPU 2}(B2);\n\\draw[Line,-latex](GB3)--node[Text,pos=0.45]{Assigned to GPU 3}(B3);\n\\draw[Line,-latex](GB4)--node[Text,pos=0.45]{Assigned to GPU 4}(B4);\n%\n\\draw[Line,-latex](B3)--++(270:0.9)-|(DB1);\n\\draw[Line,-latex](B2)--++(270:0.9)-|(DB1);\n\\draw[Line,-latex](B1)--++(270:0.9)-|(DB1);\n\\draw[Line,-latex](B4)--++(270:0.9)-|node[Text,pos=0.72,text=black]{Compute Gradients}(DB1);\n%\n\\draw[Line,-latex](DB1)--node[Text,pos=0.45]{Synchronize Gradients}(DB2);\n\\draw[Line,-latex](DB2)--node[Text,pos=0.45]{Aggregate Gradients and Update Parameters}(DB3);\n%\n\\draw[Line,-latex](GGB1.east)--++(0:6.8)|-node[Text,pos=0.8,text=black]{Next Mini-Batch}(DB3.east);\n\\end{tikzpicture}\n```\nDistributed training replicates the model across multiple devices, each processing a subset of the data before aggregating gradients to update model parameters. This accelerates the training process compared to single-device training, and contrasts with model parallelism where the model itself is partitioned across devices.\n:::\n\n#### Dataset Splitting {#sec-distributed-training-dataset-splitting}\n\nThe first step in data parallelism involves dividing the dataset into smaller, non-overlapping subsets. This ensures that each device processes a unique portion of the data, avoiding redundancy and enabling efficient utilization of available hardware. For instance, with a dataset of 100,000 training examples and 4 GPUs, each GPU would be assigned 25,000 examples. Modern frameworks like PyTorch's DistributedSampler handle this distribution automatically, implementing prefetching and caching mechanisms to ensure data is readily available for processing.\n\n#### Device Forward Pass {#sec-distributed-training-device-forward-pass}\n\nOnce the data subsets are distributed, each device performs the forward pass independently. During this stage, the model processes its assigned batch of data, generating predictions and calculating the loss. For example, in a ResNet-50 model, each GPU would independently compute the convolutions, activations, and final loss for its batch. The forward pass is computationally intensive and benefits from hardware accelerators like NVIDIA V100 GPUs or Google TPUs, which are optimized for matrix operations.\n\n#### Backward Pass and Calculation {#sec-distributed-training-backward-pass-calculation}\n\nFollowing the forward pass, each device computes the gradients of the loss with respect to the model's parameters during the backward pass. Modern frameworks like PyTorch and TensorFlow handle this automatically through their autograd systems. For instance, if a model has 50 million parameters, each device calculates gradients for all parameters but based only on its local data subset.\n\n#### Gradient Synchronization {#sec-distributed-training-gradient-synchronization}\n\nTo maintain consistency across the distributed system, the gradients computed by each device must be synchronized. This coordination represents a distributed systems challenge: achieving global consensus while minimizing communication complexity. The ring all-reduce algorithm exemplifies this trade-off, organizing devices in a logical ring where each GPU communicates only with its neighbors. The algorithm complexity is O(n) in communication rounds but requires sequential dependencies that can limit parallelism.\n\nFor example, with 8 GPUs sharing gradients for a 100 MB model, ring all-reduce requires only 7 communication steps instead of the 56 steps needed for naive all-to-all synchronization. The ring topology creates bottlenecks: the slowest link in the ring determines the overall synchronization time, and network partitions can halt the entire training process. Alternative algorithms like tree-reduce achieve O(log n) latency at the cost of increased bandwidth requirements on root nodes. Modern systems often implement hierarchical topologies, using high-speed links within nodes and lower-bandwidth connections between nodes to optimize these trade-offs.\n\n#### Synchronization Models {#sec-distributed-training-sync-models}\n\nDistributed training systems operate under explicit synchronization models that govern when workers observe each other's updates. Understanding these models is essential for reasoning about correctness and performance.\n\nThe default model, Bulk Synchronous Parallel (BSP), requires all workers to complete their local computation (forward and backward pass), synchronize gradients through a barrier (AllReduce), and then simultaneously update parameters. BSP provides strong guarantees: every worker sees identical parameter values at each step, ensuring mathematical equivalence to single-device training. The cost is that the slowest worker determines iteration time, creating the \"straggler problem.\"\n\nStale Synchronous Parallel (SSP) relaxes this constraint, allowing workers to proceed up to $s$ iterations ahead of the slowest worker before blocking. This bounds staleness while reducing synchronization delays. SSP requires careful learning rate tuning since workers compute gradients on slightly different parameter versions. The bounded staleness guarantee ($s$ typically 2-5) provides a middle ground between BSP's strong consistency and fully asynchronous approaches.\n\nAsynchronous SGD eliminates synchronization barriers entirely, with workers updating parameters independently. This maximizes hardware utilization but introduces gradient staleness that can degrade convergence. When a worker computes gradients on parameters that are already $\\tau$ steps stale, the effective learning rate decreases. Compensation techniques include learning rate scaling ($\\eta' = \\eta / \\sqrt{\\tau}$) or momentum correction.\n\n::: {.callout-note title=\"Synchronization Model Trade-offs\"}\n| Model | Consistency | Throughput | Convergence | Use Case |\n|-------|-------------|------------|-------------|----------|\n| BSP | Strong | Bounded by slowest worker | Equivalent to single-GPU | Final training runs, reproducibility |\n| SSP | Bounded staleness | Higher than BSP | Near-equivalent with tuning | Hyperparameter search |\n| Async | Weak | Maximum | Degraded, requires compensation | Large heterogeneous clusters |\n:::\n\nThe choice of synchronization model directly affects both system throughput and model convergence. Production systems typically use BSP for final training runs to ensure reproducibility, while exploring SSP or async approaches during hyperparameter search where exact reproducibility is less critical.\n\n#### Barrier Semantics and Failure Modes {#sec-distributed-training-barrier-failures}\n\nAllReduce operations implement implicit barriers: no worker can proceed until all workers have contributed their gradients. This coupling creates failure modes absent from single-device training.\n\nWorker failures during AllReduce cause all other workers to block indefinitely, waiting for the missing contribution. Without timeout mechanisms, the entire training job hangs rather than failing cleanly. Production systems implement watchdog timers, typically set to 5-10 minutes, to detect and terminate stuck jobs.\n\nGradient mismatches occur when workers disagree on which tensors to synchronize due to conditional computation paths or dynamic batching. AllReduce operations may block waiting for tensors that some workers never send. This commonly occurs with variable-length sequences in NLP models, dynamic computation graphs, and mixture-of-experts with different routing decisions.\n\nStraggler-induced delays arise because iteration time equals the slowest worker's time plus synchronization overhead. A single slow worker, whether due to thermal throttling, network congestion, or OS jitter, delays all workers and reduces cluster utilization. At 1000 GPUs with 1% probability of straggler per GPU per step, approximately 10 GPUs straggle every iteration.\n\nProduction systems address these issues through:\n\n- **Timeouts**: AllReduce operations with configurable timeouts that trigger failure handling rather than indefinite blocking\n- **Heartbeat monitoring**: Detecting unresponsive workers before AllReduce blocks\n- **Elastic training**: Removing failed workers and continuing with reduced parallelism (see @sec-fault-tolerance)\n- **Backup workers**: Redundant computation to mask stragglers\n\n#### Parameter Updating {#sec-distributed-training-parameter-updating}\n\nAfter gradient aggregation, each device independently updates model parameters using the chosen optimization algorithm, such as SGD with momentum or Adam. This decentralized update strategy, implemented in frameworks like PyTorch's DistributedDataParallel (DDP), enables efficient parameter updates without requiring a central coordination server. Since all devices have identical gradient values after synchronization, they perform mathematically equivalent updates to maintain model consistency across the distributed system.\n\nFor example, in a system with 8 GPUs training a ResNet model, each GPU computes local gradients based on its data subset. After gradient averaging via ring all-reduce, every GPU has the same global gradient values. Each device then independently applies these gradients using the optimizer's update rule. If using SGD with learning rate 0.1, the update would be `weights = weights - 0.1 * gradients`. This process maintains mathematical equivalence to single-device training while enabling distributed computation.\n\nThis process, which involves splitting data, performing computations, synchronizing results, and updating parameters, repeats for each batch of data. Modern frameworks automate this cycle, allowing developers to focus on model architecture and hyperparameter tuning rather than distributed computing logistics.\n\n### Data Parallelism Advantages {#sec-distributed-training-data-parallelism-advantages}\n\nData parallelism offers several key benefits that make it the predominant approach for distributed training. By splitting the dataset across multiple devices and allowing each device to train an identical copy of the model, this approach effectively addresses the core challenges in modern AI training systems.\n\nThe primary advantage of data parallelism is its linear scaling capability with large datasets. As datasets grow into the terabyte range, processing them on a single machine becomes prohibitively time-consuming. For example, training a vision transformer on ImageNet (1.2 million images) might take weeks on a single GPU, but only days when distributed across 8 GPUs. This scalability is particularly valuable in domains like language modeling, where datasets can exceed billions of tokens.\n\nHardware utilization efficiency represents another important benefit. Data parallelism maintains high GPU utilization rates, typically, above 85%, by ensuring each device actively processes its data portion. Modern implementations achieve this through asynchronous data loading and gradient computation overlapping with communication. For instance, while one batch computes gradients, the next batch's data is already being loaded and preprocessed.\n\nImplementation simplicity sets data parallelism apart from other distribution strategies. Modern frameworks have reduced complex distributed training to just a few lines of code. For example, converting a PyTorch model to use data parallelism often requires only wrapping it in `DistributedDataParallel` and initializing a distributed environment. This accessibility has contributed significantly to its widespread adoption in both research and industry.\n\nThe approach also offers flexibility across model architectures. Whether training a ResNet (vision), BERT (language), or Graph Neural Network (graph data), the same data parallelism principles apply without modification. This universality makes it particularly valuable as a default choice for distributed training.\n\nTraining time reduction is perhaps the most immediate benefit. Given proper implementation, data parallelism can achieve near-linear speedup with additional devices. Training that takes 100 hours on a single GPU might complete in roughly 13 hours on 8 GPUs, assuming efficient gradient synchronization and minimal communication overhead.\n\nWhile these benefits make data parallelism compelling, achieving these advantages requires careful system design. Several challenges must be addressed to fully realize these benefits.\n\n::: {.callout-tip title=\"GPT-2 Data Parallel Scaling: 1→8→32 GPUs\" collapse=\"true\"}\n\nThis example demonstrates how data parallelism scales in practice, including efficiency degradation.\n\n**Single GPU Baseline**\n\n- Batch size: 16 (with gradient checkpointing, fits in 32GB)\n- Time per step: 1.8 seconds\n- Training throughput: ~9 samples/second\n- Time to 50K steps: **25 hours**\n\n**8 GPUs: Single Node with NVLink**\n\nConfiguration:\n\n- Per-GPU batch: 16, global batch: 128\n- Gradient synchronization: 3GB @ 600 GB/s (NVLink) = 5ms\n\nPerformance results:\n\n- Computation: 180ms per step\n- Communication: 5ms per step\n- Total: 185ms per step\n- Speedup: 1.8s ÷ 0.185s = 9.7× (not quite 8×)\n- Parallel efficiency: 9.7 ÷ 8 = 121%\n\nWhy over 100% efficiency? Larger global batch (128 vs 16) improves GPU utilization from 72% to 89%. This \"super-linear\" speedup is common in ML at small scales when the baseline has poor utilization.\n\nTraining time: 25 hours ÷ 9.7 = **2.6 hours**\n\n**32 GPUs: 4 Nodes with InfiniBand**\n\nConfiguration:\n\n- Per-GPU batch: 16, global batch: 512\n- Intra-node communication: 5ms (NVLink)\n- Inter-node communication: 3GB @ 12.5 GB/s (InfiniBand) = 240ms\n\nPerformance results:\n\n- Computation: 180ms (42% of time)\n- Communication: 245ms (58% of time)\n- Total: 425ms per step\n- Speedup: 1.8s ÷ 0.425s = 4.2× faster → 5.9 hours\n- Parallel efficiency: 4.2 ÷ 32 = 13%\n\nCommunication dominates and becomes the bottleneck.\n\n**Better Approach: 8 GPUs with Gradient Accumulation**\n\n- Configuration: 8 GPUs × batch 16 × 4 accumulation steps = 512 effective batch\n- Communication overhead: 5ms ÷ (4 × 180ms) = 0.7%\n- Training time: 3.8 hours\n- Cost: $128/hour × 3.8 hours = $486 vs. $3,021 for 32 GPUs\n- Savings: $2,535 (84% reduction) with only 1 hour longer training\n\n**Key Insights**\n\n1. NVLink enables efficient scaling within single nodes (97% efficiency)\n2. Inter-node communication kills efficiency (drops to 13%)\n3. Gradient accumulation beats naive scaling for memory-bound models\n4. Sweet spot for GPT-2: 8 GPUs per node with gradient accumulation, not naive scaling to 32+ GPUs\n\nOpenAI's GPT-2 paper reports training on 32 V100s across 4 nodes using optimized communication (likely gradient accumulation combined with pipeline parallelism), not pure data parallelism.\n\n:::\n\n### Data Parallelism Limitations {#sec-distributed-training-data-parallelism-limitations}\n\nWhile data parallelism is an effective approach for distributed training, it introduces several challenges that must be addressed to achieve efficient and scalable training systems. These challenges stem from the inherent trade-offs between computation and communication, as well as the limitations imposed by hardware and network infrastructures.\n\nCommunication overhead represents the most significant bottleneck in data parallelism. During gradient synchronization, each device must exchange gradient updates, often hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter model, each synchronization step might require transferring several gigabytes of data across the network. While high-speed interconnects like NVLink (300 GB/s) or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCL's ring-allreduce algorithm[^fn-allreduce-algorithm] reduces this burden by organizing devices in a ring topology, but communication costs still grow with model size and device count.\n\n[^fn-allreduce-algorithm]: **AllReduce Algorithm**: A collective communication primitive where each process contributes data and all processes receive the same combined result (typically a sum). Naive implementations require O(n²) messages for n devices. The ring-allreduce algorithm, developed for high-performance computing in the 1980s, reduces this to O(n) by organizing devices in a logical ring where each device communicates only with its neighbors, making it scalable for modern ML with hundreds of GPUs.\n\nScalability limitations become apparent as device count increases. While 8 GPUs might achieve $7\\times$ speedup (87.5% scaling efficiency), scaling to 64 GPUs typically yields only 45-50$\\times$ speedup (70-78% efficiency) due to growing synchronization costs. Scaling efficiency, calculated as speedup divided by the number of devices—quantifies how effectively additional hardware translates to reduced training time. Perfect linear scaling would yield 100% efficiency, but communication overhead and synchronization barriers typically degrade efficiency as device count grows. This non-linear scaling means that doubling the number of devices rarely halves the training time, particularly in configurations exceeding 16-32 devices.\n\nMemory constraints present a hard limit for data parallelism. Consider a transformer model with 175 billion parameters, which requires approximately 350 GB just to store model parameters in FP32. When accounting for optimizer states and activation memories, the total requirement often exceeds 1 TB per device. Since even high-end GPUs typically offer 80 GB or less, such models cannot use pure data parallelism.\n\nWorkload imbalance affects heterogeneous systems significantly. In a cluster mixing A100 and V100 GPUs, the A100s might process batches $1.7\\times$ faster, forcing them to wait for the V100s to catch up. This idle time can reduce cluster utilization by 20-30% without proper load balancing mechanisms.\n\nFinally, there are critical challenges related to fault tolerance and reliability in distributed training systems. Node failures become inevitable at scale: with 100 GPUs running continuously, hardware failures occur multiple times per week. A training run that costs millions of dollars cannot restart from scratch each time a single GPU fails. Modern distributed training systems implement sophisticated checkpointing strategies, storing model state every N iterations to minimize lost computation. Checkpoint frequency creates trade-offs: frequent checkpointing reduces the potential loss from failures but increases storage I/O overhead and training latency. Production systems typically checkpoint every 100-1000 iterations, balancing fault tolerance against performance.\n\nImplementation complexity compounds these reliability challenges. While modern frameworks abstract much of the complexity, implementing robust distributed training systems requires significant engineering expertise. Graceful degradation when subsets of nodes fail, consistent gradient synchronization despite network partitions, and automatic recovery from transient failures demand deep understanding of both machine learning and distributed systems principles.\n\nDespite these challenges, data parallelism remains an important technique for distributed training, with many strategies available to address its limitations. Monitoring these distributed systems requires specialized tooling for tracking gradient norms, communication patterns, and hardware utilization across nodes. Before examining model parallelism, we first explore memory optimization techniques that extend data parallelism to larger models.\n\n### Memory-Efficient Data Parallelism: ZeRO and FSDP {#sec-distributed-training-zero-fsdp}\n\nThe memory constraints of data parallelism motivate a family of techniques that shard memory state across workers while preserving the simplicity of data parallel training. ZeRO (Zero Redundancy Optimizer) and its PyTorch implementation FSDP (Fully Sharded Data Parallel) enable training models that would otherwise require model parallelism.\n\nIn standard data parallelism, each GPU maintains a complete copy of:\n\n- **Model parameters**: 4 bytes/param (FP32) or 2 bytes/param (FP16/BF16)\n- **Gradients**: Same size as parameters\n- **Optimizer states**: For Adam, 8 bytes/param (momentum + variance in FP32)\n\nFor a 7B parameter model with Adam optimizer, each GPU requires: $7B \\times (4 + 4 + 8) = 112$ GB, exceeding A100-80GB capacity even before accounting for activations.\n\nZeRO addresses this redundancy through progressive sharding:\n\n| Stage | What is Sharded | Memory Reduction | Communication Overhead |\n|-------|-----------------|------------------|------------------------|\n| ZeRO-1 | Optimizer states only | ~4x | None (same as DDP) |\n| ZeRO-2 | + Gradients | ~8x | ReduceScatter replaces AllReduce |\n| ZeRO-3 / FSDP | + Parameters | ~$N$ (linear in workers) | AllGather before each layer |\n\nZeRO-1 shards optimizer states across GPUs. Each GPU stores only $1/N$ of the Adam momentum and variance tensors. After gradient AllReduce, each GPU updates only its shard of parameters, then broadcasts updates to other GPUs. Memory savings: optimizer states reduced from $8N$ bytes/param to $8$ bytes/param total across cluster.\n\nZeRO-2 additionally shards gradients. Instead of AllReduce, which leaves full gradients on each GPU, ZeRO-2 uses ReduceScatter so each GPU receives $1/N$ of the reduced gradients. Memory savings: gradients reduced from $4N$ bytes/param to $4$ bytes/param total.\n\nZeRO-3 and FSDP shard parameters themselves. Each GPU stores only $1/N$ of the model. Before each layer's forward pass, parameters are gathered via AllGather; after backward pass, gradients are reduced via ReduceScatter, then parameters are discarded. This achieves maximum memory efficiency at the cost of additional communication.\n\n::: {.callout-note title=\"FSDP Communication Analysis\"}\nFSDP introduces communication on the critical path that DDP avoids:\n\n- **Forward pass**: AllGather to reconstruct parameters ($M$ bytes × 2 for each layer)\n- **Backward pass**: ReduceScatter for gradients ($M$ bytes × 2 for each layer)\n\nFor a model with $L$ layers, FSDP performs $2L$ collective operations per training step versus 1 AllReduce for DDP. However, FSDP enables overlapping: while layer $i$ computes, layer $i+1$ can prefetch parameters.\n\nTotal FSDP communication volume: approximately $3M$ bytes (vs. $2M$ for DDP AllReduce), but spread across more operations with overlap opportunities.\n:::\n\nThe choice between FSDP and DDP depends on model size and memory constraints. Use DDP when the model fits in GPU memory with room for activations, as it has lower overhead. Use FSDP ZeRO-2 when the model barely fits or requires activation checkpointing. Use FSDP ZeRO-3 when model parameters exceed single-GPU memory. For training 70B+ models on 80GB GPUs, combine FSDP with tensor parallelism.\n\nFSDP configuration requires careful tuning of sharding strategy (by layer, by transformer block, or flat) and mixed precision settings. The `auto_wrap_policy` determines sharding granularity, with finer sharding reducing memory but increasing communication frequency.\n\n## Model Parallelism {#sec-distributed-training-model-parallelism}\n\nWhile data parallelism scales dataset processing, some models themselves exceed the memory capacity of individual devices. Model parallelism splits neural networks across multiple computing devices when the model's parameters exceed single-device memory limits. Unlike data parallelism, where each device contains a complete model copy, model parallelism assigns different model components to different devices [@shazeer_mixture_of_experts_2017].\n\nSeveral implementations of model parallelism exist. In layer-based splitting, devices process distinct groups of layers sequentially. For instance, the first device might compute layers 1-4 while the second handles layers 5-8. Channel-based splitting divides the channels within each layer across devices, such as the first device processing 512 channels while the second manages the remaining ones. For transformer architectures, attention head splitting distributes different attention heads to separate devices.\n\nThis distribution method enables training of large-scale models. GPT-3, with 175 billion parameters, relies on model parallelism for training. Vision transformers processing high-resolution 16k × 16k pixel images use model parallelism to manage memory constraints. Mixture-of-Expert architectures use this approach to distribute their conditional computation paths across hardware.\n\nDevice coordination follows a specific pattern during training. In the forward pass, data flows sequentially through model segments on different devices. The backward pass propagates gradients in reverse order through these segments. During parameter updates, each device modifies only its assigned portion of the model. This coordination ensures mathematical equivalence to training on a single device while enabling the handling of models that exceed individual device memory capacities.\n\n### Model Parallelism Implementation {#sec-distributed-training-model-parallelism-implementation}\n\nModel parallelism divides neural networks across multiple computing devices, with each device computing a distinct portion of the model's operations. This division allows training of models whose parameter counts exceed single-device memory capacity. The technique encompasses device coordination, data flow management, and gradient computation across distributed model segments. The mechanics of model parallelism are illustrated in @fig-model-parallelism. These steps are described next:\n\n::: {#fig-model-parallelism fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=1.0pt,black!50,text=black\n},\n    Box/.style={inner xsep=2pt,\n    draw=GreenLine,\n    node distance=1.5,\n    line width=0.75pt,\n    fill=GreenL,\n    anchor=west,\n    text width=23mm,\n    align=flush center,\n    minimum width=23mm,\n    minimum height=10mm\n  },\n  Text/.style={inner xsep=4pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor!80,\n    font=\\usefont{T1}{phv}{m}{n}\\footnotesize,\n    align=flush center,\n    minimum width=22mm, minimum height=6mm\n  },\n}\n\n\\node[Box](B1){Input Data};\n\\node[Box,right=of B1](B2){Model Part 1\\\\ on Device 1};\n\\node[Box,right=of B2](B3){Model Part 2\\ on Device 2};\n\\node[Box,right=of B3](B4){Model Part 3\\\\ on Device 3};\n\\node[Box,right=of B4](B5){Predictions};\n%\n\\draw[Line,-latex](B1)--++(90:12mm)\n-|node[Text,pos=0.25]{Forward Pass}(B2.120);\n\\draw[Line,latex-](B1)--++(270:12mm)\n-|node[Text,pos=0.25]{Gradient Updates}(B2.240);\n%\n\\draw[Line,-latex](B2)--++(90:12mm)\n-|node[Text,pos=0.25]{Intermediate Data}(B3.120);\n\\draw[Line,latex-](B2)--++(270:12mm)\n-|node[Text,pos=0.25]{Gradient Updates}(B3.240);\n%\n\\draw[Line,-latex](B3)--++(90:12mm)\n-|node[Text,pos=0.25]{Intermediate Data}(B4.120);\n\\draw[Line,latex-](B3)--++(270:12mm)\n-|node[Text,pos=0.25]{Gradient Updates}(B4.240);\n%\n\\draw[Line,-latex](B4)--++(90:12mm)\n-|node[Text,pos=0.25]{Output}(B5.120);\n\\draw[Line,latex-](B4)--++(270:12mm)\n-|node[Text,pos=0.25]{Backward Pass}(B5.240);\n\\end{tikzpicture}\n```\nDistributing a neural network across multiple devices enables training models larger than the memory capacity of a single device. This approach requires careful coordination of data flow and gradient computation between devices to maintain training efficiency.\n:::\n\n#### Model Partitioning {#sec-distributed-training-model-partitioning}\n\nThe first step in model parallelism is dividing the model into smaller segments. For instance, in a deep neural network, layers are often divided among devices. In a system with two GPUs, the first half of the layers might reside on GPU 1, while the second half resides on GPU 2. Another approach is to split computations within a single layer, such as dividing matrix multiplications in transformer models across devices.\n\n#### Model Forward Pass {#sec-distributed-training-model-forward-pass}\n\nDuring the forward pass, data flows sequentially through the partitions. For example, data processed by the first set of layers on GPU 1 is sent to GPU 2 for processing by the next set of layers. This sequential flow ensures that the entire model is used, even though it is distributed across multiple devices. Efficient inter-device communication is important to minimize delays during this step [@deepspeed_training_system_2021].\n\n#### Backward Pass and Calculation {#sec-distributed-training-backward-pass-calculation-model}\n\nThe backward pass computes gradients through the distributed model segments in reverse order. Each device calculates local gradients for its parameters and propagates necessary gradient information to previous devices. In transformer models, this means backpropagating through attention computations and feed-forward networks across device boundaries.\n\nFor example, in a two-device setup with attention mechanisms split between devices, the backward computation works as follows: The second device computes gradients for the final feed-forward layers and attention heads. It then sends the gradient tensors for the attention output to the first device. The first device uses these received gradients to compute updates for its attention parameters and earlier layer weights.\n\n#### Parameter Updates {#sec-distributed-training-parameter-updates-model}\n\nParameter updates occur independently on each device using the computed gradients and an optimization algorithm. A device holding attention layer parameters applies updates using only the gradients computed for those specific parameters. This localized update approach differs from data parallelism, which requires gradient averaging across devices.\n\nThe optimization step proceeds as follows: Each device applies its chosen optimizer (such as Adam or AdaFactor) to update its portion of the model parameters. A device holding the first six transformer layers updates only those layers' weights and biases. This local parameter update eliminates the need for cross-device synchronization during the optimization step, reducing communication overhead.\n\n#### Iterative Process {#sec-distributed-training-iterative-process}\n\nLike other training strategies, model parallelism repeats these steps for every batch of data. As the dataset is processed over multiple iterations, the distributed model converges toward optimal performance.\n\n### Parallelism Variations {#sec-distributed-training-parallelism-variations}\n\nModel parallelism can be implemented through different strategies for dividing the model across devices. The three primary approaches are layer-wise partitioning, operator-level partitioning, and pipeline parallelism, each suited to different model structures and computational needs.\n\n#### Layer-wise Partitioning {#sec-distributed-training-layerwise-partitioning}\n\nLayer-wise partitioning assigns distinct model layers to separate computing devices. In transformer architectures, this translates to specific devices managing defined sets of attention and feed-forward blocks. As illustrated in @fig-layers-blocks, a 24-layer transformer model distributed across four devices assigns six consecutive transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and so forth.\n\n::: {#fig-layers-blocks fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{Line/.style={line width=1.0pt,black!50\n},\n  Box/.style={inner xsep=2pt,\n    draw=VioletLine2,\n    line width=0.75pt,\n    node distance=1.8,\n    fill=VioletL2,\n    align=flush center,\n    text width=19mm,\n    minimum width=19mm,\n    minimum height=8mm\n  },\n}\n\\node[Box,fill=RedL,draw=RedLine](B1){Blocks 1-6};\n\\node[Box,right=of B1,fill=OrangeL,draw=OrangeLine](B2){Blocks 7-12};\n\\node[Box,right=of B2,fill=GreenL,draw=GreenLine](B3){Blocks 13-18};\n\\node[Box,right=of B3,fill=BlueL,draw=BlueLine](B4){Blocks 19-24};\n%\n\\node[Box,below=1.3 of B1,fill=VioletL2,draw=VioletLine2](G1){GPU 1};\n\\node[Box,below=1.3 of B2,fill=VioletL2,draw=VioletLine2](G2){GPU 2};\n\\node[Box,below=1.3 of B3,fill=VioletL2,draw=VioletLine2](G3){GPU 3};\n\\node[Box,below=1.3 of B4,fill=VioletL2,draw=VioletLine2](G4){GPU 4};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B1)(G1)](BB1){};\n\\node[below=1pt of BB1.north,anchor=north]{Device 1};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B2)(G2)](BB2){};\n\\node[below=1pt of BB2.north,anchor=north]{Device 2};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B3)(G3)](BB3){};\n\\node[below=1pt of BB3.north,anchor=north]{Device 3};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=13,\nline width=0.75pt,\ninner ysep=18,\nfill=BackColor,yshift=6,\nfit=(B4)(G4)](BB4){};\n\\node[below=1pt of BB4.north,anchor=north]{Device 4};\n\n\\foreach \\x in {1,2,3} {\n    \\pgfmathtruncatemacro{\\newX}{\\x + 1}\n    \\draw[-latex,Line] (B\\x) -- (B\\newX);\n}\n\\foreach \\x in {4,3,2} {\n    \\pgfmathtruncatemacro{\\newX}{\\x - 1}\n\\draw[red,-latex,Line](B\\x.230)to[out=230,in=300](B\\newX.300);\n}\n\\end{tikzpicture}\n```\n**Layer-Wise Model Parallelism**: Distributing a transformer model across multiple gpus assigns consecutive layers to each device, enabling parallel processing of input data and accelerating training. This partitioning strategy allows each GPU to operate on a subset of the model's layers, reducing the memory footprint and computational load per device.\n:::\n\nThis sequential processing introduces device idle time, as each device must wait for the previous device to complete its computation before beginning work. For example, while device 1 processes the initial blocks, devices 2, 3, and 4 remain inactive. Similarly, when device 2 begins its computation, device 1 sits idle. This pattern of waiting and idle time reduces hardware utilization efficiency compared to other parallelization strategies.\n\n#### Pipeline Parallelism {#sec-distributed-training-pipeline-parallelism}\n\nPipeline parallelism extends layer-wise partitioning by introducing microbatching to minimize device idle time, as illustrated in @fig-pipline-parallelism. Instead of waiting for an entire batch to sequentially pass through all devices, the computation is divided into smaller segments called microbatches [@harlap2018pipedream]. Each device, as represented by the rows in the drawing, processes its assigned model layers for different microbatches simultaneously. For example, the forward pass involves devices passing activations to the next stage (e.g., $F_{0,0}$ to $F_{1,0}$). The backward pass transfers gradients back through the pipeline (e.g., $B_{3,3}$ to $B_{2,3}$). This overlapping computation reduces idle time and increases throughput while maintaining the logical sequence of operations across devices.\n\n::: {#fig-pipline-parallelism}\n```{.tikz}\n\\begin{tikzpicture}[\n    every node/.style={font=\\sffamily, draw, minimum width=1cm, minimum height=0.7cm, align=center, outer sep=0},\n    fill0/.style={fill=red!20}, % Complementary to lightgray\n    fill1/.style={fill=blue!20}, % Complementary to orange\n    fill2/.style={fill=orange!20}, % Complementary to blue\n    fill3/.style={fill=yellow!20}, % Complementary to purple\n    back3/.style={fill=yellow!20} % Same as fill3\n]\n\n% Row 0\n\\node[fill0] (F0_0) {$F_{0,0}$};\n\\node[fill0, right=0cm of F0_0] (F0_1) {$F_{0,1}$};\n\\node[fill0, right=0cm of F0_1] (F0_2) {$F_{0,2}$};\n\\node[fill0, right=0cm of F0_2] (F0_3) {$F_{0,3}$};\n\n% Row 1\n\\node[fill1, above right=0cm and 0cm of F0_0] (F1_0) {$F_{1,0}$};\n\\node[fill1, right=0cm of F1_0] (F1_1) {$F_{1,1}$};\n\\node[fill1, right=0cm of F1_1] (F1_2) {$F_{1,2}$};\n\\node[fill1, right=0cm of F1_2] (F1_3) {$F_{1,3}$};\n\n% Row 2 (stacked above F1)\n\\node[fill2, above right=0cm and 0cm of F1_0] (F2_0) {$F_{2,0}$};\n\\node[fill2, right=0cm of F2_0] (F2_1) {$F_{2,1}$};\n\\node[fill2, right=0cm of F2_1] (F2_2) {$F_{2,2}$};\n\\node[fill2, right=0cm of F2_2] (F2_3) {$F_{2,3}$};\n\n% Row 3 (stacked above F2)\n\\node[fill3, above right=0cm and 0cm of F2_0] (F3_0) {$F_{3,0}$};\n\\node[fill3, right=0cm of F3_0] (F3_1) {$F_{3,1}$};\n\\node[fill3, right=0cm of F3_1] (F3_2) {$F_{3,2}$};\n\\node[fill3, right=0cm of F3_2] (F3_3) {$F_{3,3}$};\n\n% Row 3 (backward pass)\n\\node[back3, right=0cm of F3_3] (B3_3) {$B_{3,3}$};\n\\node[back3, right=0cm of B3_3] (B3_2) {$B_{3,2}$};\n\\node[back3, right=0cm of B3_2] (B3_1) {$B_{3,1}$};\n\\node[back3, right=0cm of B3_1] (B3_0) {$B_{3,0}$};\n\n% Row 2 (backward pass)\n\\node[fill2, below=0cm and 0cm of B3_2] (B2_3) {$B_{2,3}$};\n\\node[fill2, right=0cm of B2_3] (B2_2) {$B_{2,2}$};\n\\node[fill2, right=0cm of B2_2] (B2_1) {$B_{2,1}$};\n\\node[fill2, right=0cm of B2_1] (B2_0) {$B_{2,0}$};\n\n% Row 1 (backward pass)\n\\node[fill1, below=0cm of B2_2] (B1_3) {$B_{1,3}$};\n\\node[fill1, right=0cm of B1_3] (B1_2) {$B_{1,2}$};\n\\node[fill1, right=0cm of B1_2] (B1_1) {$B_{1,1}$};\n\\node[fill1, right=0cm of B1_1] (B1_0) {$B_{1,0}$};\n\n% Row 0 (backward pass)\n\\node[fill0, below=0cm of B1_2] (B0_3) {$B_{0,3}$};\n\\node[fill0, right=0cm of B0_3] (B0_2) {$B_{0,2}$};\n\\node[fill0, right=0cm of B0_2] (B0_1) {$B_{0,1}$};\n\\node[fill0, right=0cm of B0_1] (B0_0) {$B_{0,0}$};\n\n% Update nodes\n\\node[fill0, right=0cm of B0_0] (U0_0) {Update};\n\\node[fill1, above=0cm of U0_0] (U0_1) {Update};\n\\node[fill2, above=0cm of U0_1] (U0_2) {Update};\n\\node[fill3, above=0cm of U0_2] (U0_3) {Update};\n\n%\\node[draw=none, minimum width=4cm, minimum height=1cm, align=center, right=1cm of F0_3] (Bubble) {Bubble};\n\\end{tikzpicture}\n```\nWith model layers distributed across devices, microbatching divides each training batch into smaller segments that flow through the pipeline concurrently. This approach minimizes device idle time during both forward and backward passes by allowing multiple microbatches to be processed simultaneously at different pipeline stages. Activations flow sequentially between devices during the forward pass, while gradients propagate in reverse during backpropagation.\n:::\n\nIn a transformer model distributed across four devices, device 1 would process blocks 1-6 for microbatch $N+1$ while device 2 computes blocks 7-12 for microbatch $N$. Simultaneously, device 3 executes blocks 13-18 for microbatch $N-1$, and device 4 processes blocks 19-24 for microbatch $N-2$. Each device maintains its assigned transformer blocks but operates on a different microbatch, creating a continuous flow of computation.\n\nThe transfer of hidden states between devices occurs continuously rather than in distinct phases. When device 1 completes processing a microbatch, it immediately transfers the output tensor of shape [microbatch_size, sequence_length, hidden_dimension] to device 2 and begins processing the next microbatch. This overlapping computation pattern maintains full hardware utilization while preserving the model's mathematical properties.\n\n#### Tensor Parallelism {#sec-distributed-training-tensor-parallelism}\n\nTensor parallelism (also called operator-level or intra-layer parallelism) divides individual neural network operations across devices. Unlike pipeline parallelism which assigns complete layers to devices, tensor parallelism splits the weight matrices within each layer. This distinction is critical: tensor parallelism requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer, while pipeline parallelism tolerates lower bandwidth between stages.\n\n::: {.callout-note title=\"Terminology: Tensor Parallelism vs. Pipeline Parallelism\"}\nModern literature distinguishes two forms of model parallelism:\n\n**Tensor Parallelism** (intra-layer): Splits individual operations (matrix multiplies, attention heads) across devices. Requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer.\n\n**Pipeline Parallelism** (inter-layer): Assigns complete layers to different devices. Requires only point-to-point communication between pipeline stages, tolerating lower bandwidth interconnects.\n\nThe Megatron-LM framework popularized this distinction, using tensor parallelism within nodes (8 GPUs on NVLink) and pipeline parallelism across nodes (InfiniBand).\n:::\n\nMegatron-style tensor parallelism partitions matrix multiplications in two ways.\n\nColumn-parallel linear layers split weights along columns. For input $X$ and weight matrix $W = [W_1 | W_2]$ split across 2 GPUs:\n$$Y = XW = X[W_1 | W_2] = [XW_1 | XW_2]$$\nEach GPU computes its partition independently. Outputs are concatenated (no communication needed if followed by row-parallel layer).\n\nRow-parallel linear layers split weights along rows. For $W = \\begin{bmatrix} W_1 \\\\ W_2 \\end{bmatrix}$:\n$$Y = XW = X_1 W_1 + X_2 W_2$$\nEach GPU computes a partial sum. Outputs require AllReduce to combine.\n\nIn transformer architectures, Megatron applies this pattern:\n\n1. **QKV projection**: Column-parallel (weights split, outputs concatenated across heads)\n2. **Attention output projection**: Row-parallel (requires AllReduce after)\n3. **First FFN layer**: Column-parallel (split intermediate dimension)\n4. **Second FFN layer**: Row-parallel (requires AllReduce after)\n\nThis design places AllReduce operations strategically: one after attention, one after FFN, totaling 2 AllReduce operations per transformer layer.\n\nCommunication volume per transformer layer depends on sequence length $S$, hidden dimension $H$, and batch size $B$:\n$$\\text{Communication} = 2 \\times B \\times S \\times H \\times \\text{sizeof(dtype)}$$\n\nWith $S=2048$, $H=4096$, $B=4$, and FP16: $2 \\times 4 \\times 2048 \\times 4096 \\times 2 = 134$ MB per layer. For a 96-layer model, this totals 12.6 GB per training step, requiring NVLink bandwidth to avoid becoming the bottleneck.\n\nTensor parallelism scaling degrades rapidly beyond 8-way parallelism because:\n\n- Communication volume grows linearly with tensor parallel degree\n- Computation per GPU decreases (less work to hide communication latency)\n- NVLink bandwidth becomes saturated\n\nProduction systems (GPT-4, LLaMA, Gemini) use 8-way tensor parallelism within nodes, combined with pipeline parallelism across nodes, achieving the best balance of memory distribution and communication efficiency.\n\n### Model Parallelism Advantages {#sec-distributed-training-model-parallelism-advantages}\n\nModel parallelism offers several significant benefits, making it an essential strategy for training large-scale models that exceed the capacity of individual devices. These advantages stem from its ability to partition the workload across multiple devices, enabling the training of more complex and resource-intensive architectures.\n\nMemory scaling represents the primary advantage of model parallelism. Current transformer architectures contain up to hundreds of billions of parameters. A 175 billion parameter model with 32-bit floating point precision requires 700 GB of memory just to store its parameters. When accounting for activations, optimizer states, and gradients during training, the memory requirement multiplies several fold. Model parallelism makes training such architectures feasible by distributing these memory requirements across devices.\n\nAnother key advantage is the efficient utilization of device memory and compute power. Since each device only needs to store and process a portion of the model, memory usage is distributed across the system. This allows practitioners to work with larger batch sizes or more complex layers without exceeding memory limits, which can also improve training stability and convergence.\n\nModel parallelism also provides flexibility for different model architectures. Whether the model is sequential, as in many natural language processing tasks, or composed of computationally intensive operations, as in attention-based models or convolutional networks, there is a partitioning strategy that fits the architecture. This adaptability makes model parallelism applicable to a wide variety of tasks and domains.\n\nFinally, model parallelism is a natural complement to other distributed training strategies, such as data parallelism and pipeline parallelism. By combining these approaches, it becomes possible to train models that are both large in size and require extensive data. This hybrid flexibility is especially valuable in advanced research and production environments, where scaling models and datasets simultaneously is critical for achieving optimal performance.\n\nWhile model parallelism offers these benefits, its effectiveness depends on careful partitioning strategy design, with specific challenges addressed in the following sections and the trade-offs involved in its use.\n\n### Model Parallelism Limitations {#sec-distributed-training-model-parallelism-limitations}\n\nWhile model parallelism provides an effective approach for training large-scale models, it also introduces unique challenges. These challenges arise from the complexity of partitioning the model and the dependencies between partitions during training. Addressing these issues requires careful system design and optimization.\n\nOne major challenge in model parallelism is balancing the workload across devices. Not all parts of a model require the same amount of computation. For instance, in layer-wise partitioning, some layers may perform significantly more operations than others, leading to an uneven distribution of work. Devices responsible for the heavier computations may become bottlenecks, leaving others underutilized. This imbalance reduces overall efficiency and slows down training. Identifying optimal partitioning strategies is critical to ensuring all devices contribute evenly.\n\nAnother challenge is data dependency between devices. During the forward pass, activation tensors of shape [batch_size, sequence_length, hidden_dimension] must transfer between devices. For a typical transformer model with batch size 32, sequence length 2048, and hidden dimension 2048, each transfer moves approximately 512 MB of data at float32 precision. With gradient transfers in the backward pass, a single training step can require several gigabytes of inter-device communication. On systems using PCIe interconnects with 64 GB/s theoretical bandwidth, these transfers introduce significant latency.\n\nModel parallelism also increases the complexity of implementation and debugging. Partitioning the model, ensuring proper data flow, and synchronizing gradients across devices require detailed coordination. Errors in any of these steps can lead to incorrect gradient updates or even model divergence. Debugging such errors is often more difficult in distributed systems, as issues may arise only under specific conditions or workloads.\n\nA further challenge is pipeline bubbles in pipeline parallelism. With $m$ pipeline stages, the first $m-1$ steps operate at reduced efficiency as the pipeline fills. For example, in an 8-device pipeline, the first device begins processing immediately, but the eighth device remains idle for 7 steps. This warmup period reduces hardware utilization by a fraction of approximately $(m-1)/b$, where $b$ is the number of microbatches in the training step.\n\nFinally, model parallelism may be less effective for certain architectures, such as models with highly interdependent operations. In these cases, splitting the model may lead to excessive communication overhead, outweighing the benefits of parallel computation. For such models, alternative strategies like data parallelism or hybrid approaches might be more suitable.\n\nDespite these challenges, model parallelism remains an indispensable tool for training large models. With careful optimization and the use of modern frameworks, many of these issues can be mitigated, enabling efficient distributed training at scale.\n\n## Hybrid Parallelism {#sec-distributed-training-hybrid-parallelism}\n\nRecognizing that both data and model constraints can occur simultaneously, hybrid parallelism combines model parallelism and data parallelism when training neural networks [@narayanan_pipeline_parallelism_2021]. A model might be too large to store on one GPU (requiring model parallelism) while simultaneously needing to process large batches of data efficiently (requiring data parallelism).\n\nTraining a 175-billion parameter language model on a dataset of 300 billion tokens demonstrates hybrid parallelism in practice. The neural network layers distribute across multiple GPUs through model parallelism, while data parallelism enables different GPU groups to process separate batches. The hybrid approach coordinates these two forms of parallelization.\n\nThis strategy addresses two key constraints. First, memory constraints arise when model parameters exceed single-device memory capacity. Second, computational demands increase when dataset size necessitates distributed processing.\n\n### Hybrid Parallelism Implementation {#sec-distributed-training-hybrid-parallelism-implementation}\n\nHybrid parallelism operates by combining the processes of model partitioning and dataset splitting, ensuring efficient utilization of both memory and computation across devices. This integration allows large-scale machine learning systems to overcome the constraints imposed by individual parallelism strategies.\n\n#### Model and Data Partitioning {#sec-distributed-training-model-data-partitioning}\n\nHybrid parallelism divides both model architecture and training data across devices. The model divides through layer-wise or operator-level partitioning, where GPUs process distinct neural network segments. Simultaneously, the dataset splits into subsets, allowing each device group to train on different batches. A transformer model might distribute its attention layers across four GPUs, while each GPU group processes a unique 1,000-example batch. This dual partitioning distributes memory requirements and computational workload.\n\n#### Forward Pass {#sec-distributed-training-forward-pass-hybrid}\n\nDuring the forward pass, input data flows through the distributed model. Each device processes its assigned portion of the model using the data subset it holds. For example, in a hybrid system with four devices, two devices might handle different layers of the model (model parallelism) while simultaneously processing distinct data batches (data parallelism). Communication between devices ensures that intermediate outputs from model partitions are passed seamlessly to subsequent partitions.\n\n#### Backward Pass and Gradient Calculation {#sec-distributed-training-backward-pass-gradient-calculation-hybrid}\n\nDuring the backward pass, gradients are calculated for the model partitions stored on each device. Data-parallel devices that process the same subset of the model but different data batches aggregate their gradients, ensuring that updates reflect contributions from the entire dataset. For model-parallel devices, gradients are computed locally and passed to the next layer in reverse order. In a two-device model-parallel configuration, for example, the first device computes gradients for layers 1-3, then transmits these to the second device for layers 4-6. This combination of gradient synchronization and inter-device communication ensures consistency across the distributed system.\n\n#### Parameter Updates {#sec-distributed-training-parameter-updates-hybrid}\n\nAfter gradient synchronization, model parameters are updated using the chosen optimization algorithm. Devices working in data parallelism update their shared model partitions consistently, while model-parallel devices apply updates to their local segments. Efficient communication is critical in this step to minimize delays and ensure that updates are correctly propagated across all devices.\n\n#### Iterative Process {#sec-distributed-training-iterative-process-hybrid}\n\nHybrid parallelism follows an iterative process similar to other training strategies. The combination of model and data distribution allows the system to process large datasets and complex models efficiently over multiple training epochs. By balancing the computational workload and memory requirements, hybrid parallelism enables the training of advanced machine learning models that would otherwise be infeasible.\n\n### Parallelism Variations {#sec-distributed-training-parallelism-variations-hybrid}\n\nHybrid parallelism can be implemented in different configurations, depending on the model architecture, dataset characteristics, and available hardware. These variations allow for tailored solutions that optimize performance and scalability for specific training requirements.\n\n#### Hierarchical Parallelism {#sec-distributed-training-hierarchical-parallelism}\n\nHierarchical hybrid parallelism applies model parallelism to divide the model across devices first and then layers data parallelism on top to handle the dataset distribution. For example, in a system with eight devices, four devices may hold different partitions of the model, while each partition is replicated across the other four devices for data parallel processing. This approach is well-suited for large models with billions of parameters, where memory constraints are a primary concern.\n\nHierarchical hybrid parallelism ensures that the model size is distributed across devices, reducing memory requirements, while data parallelism ensures that multiple data samples are processed simultaneously, improving throughput. This dual-layered approach is particularly effective for models like transformers, where each layer may have a significant memory footprint.\n\n#### Intra-layer Parallelism {#sec-distributed-training-intralayer-parallelism}\n\nIntra-layer hybrid parallelism combines model and data parallelism within individual layers of the model. For instance, in a transformer architecture, the attention mechanism can be split across multiple devices (model parallelism), while each device processes distinct batches of data (data parallelism). This fine-grained integration allows the system to optimize resource usage at the level of individual operations, enabling training for models with extremely large intermediate computations.\n\nThis variation is particularly useful in scenarios where specific layers, such as attention or feedforward layers, have computationally intensive operations that are difficult to distribute effectively using model or data parallelism alone. Intra-layer hybrid parallelism addresses this challenge by applying both strategies simultaneously.\n\n#### Inter-layer Parallelism {#sec-distributed-training-interlayer-parallelism}\n\nInter-layer hybrid parallelism focuses on distributing the workload between model and data parallelism at the level of distinct model layers. For example, early layers of a neural network may be distributed using model parallelism, while later layers use data parallelism. This approach aligns with the observation that certain layers in a model may be more memory-intensive, while others benefit from increased data throughput.\n\nThis configuration allows for dynamic allocation of resources, adapting to the specific demands of different layers in the model. By tailoring the parallelism strategy to the unique characteristics of each layer, inter-layer hybrid parallelism achieves an optimal balance between memory usage and computational efficiency.\n\n### Hybrid Parallelism Advantages {#sec-distributed-training-hybrid-parallelism-advantages}\n\nThe adoption of hybrid parallelism in machine learning systems addresses some of the most significant challenges posed by the ever-growing scale of models and datasets. By blending the strengths of model parallelism and data parallelism, this approach provides a solution to scaling modern machine learning workloads.\n\nOne of the most prominent benefits of hybrid parallelism is its ability to scale seamlessly across both the model and the dataset. Modern neural networks, particularly transformers used in natural language processing and vision applications, often contain billions of parameters. These models, paired with massive datasets, make training on a single device impractical or even impossible. Hybrid parallelism enables the division of the model across multiple devices to manage memory constraints while simultaneously distributing the dataset to process vast amounts of data efficiently. This dual capability ensures that training systems can handle the computational and memory demands of the largest models and datasets without compromise.\n\nAnother critical advantage lies in hardware utilization. In many distributed training systems, inefficiencies can arise when devices sit idle during different stages of computation or synchronization. Hybrid parallelism mitigates this issue by ensuring that all devices are actively engaged. Whether a device is computing forward passes through its portion of the model or processing data batches, hybrid strategies maximize resource usage, leading to faster training times and improved throughput.\n\nFlexibility is another hallmark of hybrid parallelism. Machine learning models vary widely in architecture and computational demands. For instance, convolutional neural networks prioritize spatial data processing, while transformers require intensive operations like matrix multiplications in attention mechanisms. Hybrid parallelism adapts to these diverse needs by allowing practitioners to apply model and data parallelism selectively. This adaptability ensures that hybrid approaches can be tailored to the specific requirements of a given model, making it a versatile solution for diverse training scenarios.\n\nHybrid parallelism reduces communication bottlenecks, a common issue in distributed systems. By striking a balance between distributing model computations and spreading data processing, hybrid strategies minimize the amount of inter-device communication required during training. This efficient coordination not only speeds up the training process but also enables the effective use of large-scale distributed systems where network latency might otherwise limit performance.\n\nFinally, hybrid parallelism supports the ambitious scale of modern AI research and development. It provides a framework for leveraging advanced hardware infrastructures, including clusters of GPUs or TPUs, to train models that push the boundaries of what's possible. Without hybrid parallelism, many of the breakthroughs in AI, including large language models and advanced vision systems, would remain unattainable due to resource limitations.\n\nBy enabling scalability, maximizing hardware efficiency, and offering flexibility, hybrid parallelism has become an essential strategy for training the most complex machine learning systems. It is not just a solution to today's challenges but also a foundation for the future of AI, where models and datasets will continue to grow in complexity and size.\n\n### Hybrid Parallelism Limitations {#sec-distributed-training-hybrid-parallelism-limitations}\n\nWhile hybrid parallelism provides a robust framework for scaling machine learning training, it also introduces complexities that require careful consideration. These challenges stem from the intricate coordination needed to integrate both model and data parallelism effectively. Understanding these obstacles is crucial for designing efficient hybrid systems and avoiding potential bottlenecks.\n\nOne of the primary challenges of hybrid parallelism is communication overhead. Both model and data parallelism involve significant inter-device communication. In model parallelism, devices must exchange intermediate outputs and gradients to maintain the sequential flow of computation. In data parallelism, gradients computed on separate data subsets must be synchronized across devices. Hybrid parallelism compounds these demands, as it requires efficient communication for both processes simultaneously. If not managed properly, the resulting overhead can negate the benefits of parallelization, particularly in large-scale systems with slower interconnects or high network latency.\n\nAnother critical challenge is the complexity of implementation. Hybrid parallelism demands a nuanced understanding of both model and data parallelism techniques, as well as the underlying hardware and software infrastructure. Designing efficient hybrid strategies involves making decisions about how to partition the model, how to distribute data, and how to synchronize computations across devices. This process often requires extensive experimentation and optimization, particularly for custom architectures or non-standard hardware setups. While modern frameworks like PyTorch and TensorFlow provide tools for distributed training, implementing hybrid parallelism at scale still requires significant engineering expertise.\n\nWorkload balancing also presents a challenge in hybrid parallelism. In a distributed system, not all devices may have equal computational capacity. Some devices may process data or compute gradients faster than others, leading to inefficiencies as faster devices wait for slower ones to complete their tasks. Additionally, certain model layers or operations may require more resources than others, creating imbalances in computational load. Managing this disparity requires careful tuning of partitioning strategies and the use of dynamic workload distribution techniques.\n\nMemory constraints remain a concern, even in hybrid setups. While model parallelism addresses the issue of fitting large models into device memory, the additional memory requirements for data parallelism, such as storing multiple data batches and gradient buffers, can still exceed available capacity. This is especially true for models with extremely large intermediate computations, such as transformers with high-dimensional attention mechanisms. Balancing memory usage across devices is essential to prevent resource exhaustion during training.\n\nLastly, hybrid parallelism poses challenges related to fault tolerance and debugging. Distributed systems are inherently more prone to hardware failures and synchronization errors. Debugging issues in hybrid setups can be significantly more complex than in standalone model or data parallelism systems, as errors may arise from interactions between the two approaches. Ensuring robust fault-tolerance mechanisms and designing tools for monitoring and debugging distributed systems are essential for maintaining reliability.\n\nDespite these challenges, hybrid parallelism remains an indispensable strategy for training large-scale machine learning models. By addressing these obstacles through optimized communication protocols, intelligent partitioning strategies, and robust fault-tolerance systems, practitioners can unlock the full potential of hybrid parallelism and drive innovation in AI research and applications.\n\n## Parallelism Strategy Comparison {#sec-distributed-training-parallelism-strategy-comparison}\n\nThe features of data parallelism, model parallelism, pipeline parallelism, and hybrid parallelism are summarized in @tbl-parallelism-compare. This comparison highlights their respective focuses, memory requirements, communication overheads, scalability, implementation complexity, and ideal use cases. By examining these factors, practitioners can determine the most suitable approach for their training needs.\n\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Aspect**                        | **Data Parallelism**               | **Model Parallelism**              | **Pipeline Parallelism**           | **Hybrid Parallelism**              |\n+:==================================+:===================================+:===================================+:===================================+:====================================+\n| **Focus**                         | Distributes dataset across         | Distributes the model across       | Distributes model stages in        | Combines multiple parallelism       |\n|                                   | devices, each with a full model    | devices, each handling a portion   | pipeline, processing microbatches  | strategies for balanced             |\n|                                   | copy                               | of the model                       | concurrently                       | scalability                         |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Memory Requirement per Device** | High (entire model on each device) | Low (model split across devices)   | Low to Moderate (stages split      | Moderate (splits model and dataset  |\n|                                   |                                    |                                    | across devices)                    | across devices)                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Communication Overhead**        | Moderate to High (gradient         | High (communication for            | Moderate (activation passing       | Very High (requires synchronization |\n|                                   | synchronization across devices)    | intermediate activations and       | between stages)                    | for both model and data)            |\n|                                   |                                    | gradients)                         |                                    |                                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Scalability**                   | Good for large datasets with       | Good for very large models with    | Good for deep models with many     | Excellent for extremely large       |\n|                                   | moderate model sizes               | smaller datasets                   | layers                             | models and datasets                 |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Implementation Complexity**     | Low to Moderate (relatively        | Moderate to High (requires         | Moderate to High (requires         | High (complex integration of        |\n|                                   | straightforward with existing      | careful partitioning and           | pipeline scheduling and            | multiple parallelism strategies)    |\n|                                   | tools)                             | coordination)                      | microbatch management)             |                                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n| **Ideal Use Case**                | Large datasets where model fits    | Extremely large models that exceed | Deep models with sequential stages | Training massive models on vast     |\n|                                   | within a single device             | single-device memory limits        | that can tolerate microbatch       | datasets in large-scale systems     |\n|                                   |                                    |                                    | latency                            |                                     |\n+-----------------------------------+------------------------------------+------------------------------------+------------------------------------+-------------------------------------+\n\n: **Parallel Training Strategies**: Data, model, pipeline, and hybrid parallelism each address the challenges of scaling machine learning training by distributing workload across devices, differing in how they partition data and model parameters to optimize memory usage, communication, and scalability. Understanding these trade-offs enables practitioners to select the most effective approach for their specific model and infrastructure. {#tbl-parallelism-compare}\n\n@fig-parallelism-flowchart provides a general guideline for selecting parallelism strategies in distributed training systems. While the chart offers a structured decision-making process based on model size, dataset size, and scaling constraints, it is intentionally simplified. Real-world scenarios often involve additional complexities such as hardware heterogeneity, communication bandwidth, and workload imbalance, which may influence the choice of parallelism techniques. The chart is best viewed as a foundational tool for understanding the trade-offs and decision points in parallelism strategy selection. Practitioners should consider this guideline as a starting point and adapt it to the specific requirements and constraints of their systems to achieve optimal performance.\n\n::: {#fig-parallelism-flowchart fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=1.0pt,black!50,text=black\n},\n  Box/.style={inner xsep=2pt,\n    node distance=11mm,\n    draw=GreenLine, line width=0.75pt,\n    fill=GreenL,\n    text width=27mm,align=flush center,\n    minimum width=27mm, minimum height=9mm\n  },\n    Box1/.style={Box,\n    draw=RedLine, fill=RedL,\n    text width=31mm,\n    minimum width=32mm,\n    minimum height=10mm\n  },\n  Text/.style={inner xsep=2pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor,\n    font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n    align=flush center,\n    minimum width=7mm,\n    minimum height=5mm\n  },\n decision/.style = {align=flush center,text width=42mm,diamond, aspect=2.2, node distance=6mm,\n                             inner xsep=-3pt,  inner ysep=-2.95ex,fill=VioletL2, draw=VioletLine},\n}\n\\node[Box](B1){Hybrid\\\\ Parallelism};\n\\node[Box,node distance=16mm,right=of B1](B2){Model\\\\Parallelism};\n\\node[Box,node distance=16 mm,right=of B2](B3){Data\\\\ Parallelism};\n\\node[Box,right=of B3,fill=RedL, draw=RedLine](B4){Single Device Optimization};\n%\n\\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,\nyshift=-1mm,\nfill=BackColor,fit=(B1)(B3),line width=0.75pt](BB){};\n\\node[decision,node distance=18mm,\nabove=of B4](G1B4){Is\\\\ the dataset\\\\ very large?};\n\n\\node[Box1,node distance=15mm,\nabove=of $(B2.north)!0.5!(B3.north)$](G1B3){Is scaling the model\\\\ or data more critical?};\n\\node[decision,above=of G1B3](G2B3){Are\\\\ both constraints\\\\ significant?};\n\\node[decision,above=of G2B3](G3B3){Does\\\\ the dataset fit in a\\\\  single device?};\n\\node[decision,above=of G3B3](G4B3){Does\\\\ the model fit in a\\\\ single device?};\n\\node[Box,node distance=5mm,above=of G4B3,fill=BlueL, draw=BlueLine](G5B3){Start};\n%\n\\node[Box,below=1 of B2,fill=BlueL, draw=BlueLine](DB2){End};\n%\n\\draw[Line,-latex](G5B3)--(G4B3);\n\\draw[Line,-latex](G4B3)--node[right,pos=0.35]{No}(G3B3);\n\\draw[Line,-latex](G4B3)-|node[above,pos=0.05]{Yes}(G1B4);\n\\draw[Line,-latex](G3B3)--node[right,pos=0.35]{No}(G2B3);\n\\draw[Line,-latex](G2B3)--node[right,pos=0.35]{No}(G1B3);\n\\draw[Line,-latex](G1B4)--node[right,pos=0.15]{No}(B4);\n%\n\\draw[Line,-latex](G3B3.west)--node[above,pos=0.25]{Yes}++(180:2.3)|-(B2.west);\n\\draw[Line,-latex](G2B3)-|node[above,pos=0.05]{Yes}(B1);\n\\draw[Line,-latex](G1B3.south)--node[left,align=center,pos=0.45]{Scaling Model}++(270:8mm)-|(B2);\n\\draw[Line,-latex](G1B3.south)--++(270:8mm)-|(B3);\n\\draw[Line,-latex](G1B4)-|node[above,pos=0.22,text=black]{Yes}(B3.40);\n%\n\\draw[Line,-latex](B1)|-(DB2);\n\\draw[Line,-latex](B3)|-(DB2);\n\\draw[Line,-latex](B2)--(DB2);\n\\node[above=2pt of  BB.204,inner sep=0pt,anchor=south,fill=BackColor]{Parallelism Opportunities};\n\\end{tikzpicture}\n```\nDistributed training systems use data, model, or hybrid parallelism based on model size, dataset size, and scaling constraints to accelerate training and efficiently utilize resources. This flowchart guides practitioners through a decision process, recognizing that real-world deployments often require adaptation due to factors like hardware heterogeneity and workload imbalance.\n:::\n\n## Framework Integration {#sec-distributed-training-framework-integration}\n\nWhile the theoretical foundations of distributed training establish the mathematical principles for scaling across multiple devices, modern frameworks provide abstractions that make these concepts accessible to practitioners. Understanding how frameworks like PyTorch translate distributed training theory into practical APIs bridges the gap between mathematical concepts and implementation.\n\n### Data Parallel Framework APIs {#sec-distributed-training-data-parallel-framework-apis}\n\nThe data parallelism mechanisms we explored earlier—gradient averaging, AllReduce communication, and parameter synchronization—are abstracted through framework APIs that handle the complex coordination automatically. PyTorch provides two primary approaches that demonstrate different trade-offs between simplicity and performance.\n\n`torch.nn.DataParallel` represents the simpler approach, automatically replicating the model across available GPUs within a single node. This API abstracts the gradient collection and averaging process, requiring minimal code changes to existing single-GPU training scripts. However, this simplicity comes with performance limitations, as the implementation uses a parameter server approach that can create communication bottlenecks when scaling beyond 4-8 GPUs.\n\n```python\n# Simple data parallelism - framework handles gradient synchronization\nmodel = torch.nn.DataParallel(model)\n# Training loop remains unchanged - framework automatically:\n# 1. Splits batch across GPUs\n# 2. Replicates model on each device\n# 3. Gathers gradients and averages them\n# 4. Broadcasts updated parameters\n```\n\nFor production scale training, `torch.distributed` provides the high-performance alternative that implements the efficient AllReduce communication patterns discussed earlier. This API requires explicit initialization of process groups and distributed coordination but enables the linear scaling characteristics essential for large-scale training.\n\n```python\n# Production distributed training - explicit control over communication\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\")  # NCCL for GPU communication\nmodel = torch.nn.parallel.DistributedDataParallel(model)\n# Framework now uses optimized AllReduce instead of parameter server\n```\n\nThe key insight is that `DistributedDataParallel` implements the efficient ring AllReduce algorithm automatically, transforming the O(n) communication complexity we discussed into practical code that achieves 90%+ parallel efficiency at scale. The framework handles device placement, gradient bucketing for efficient communication, and overlapping computation with communication.\n\n### Model Parallel Framework Support {#sec-distributed-training-model-parallel-framework-support}\n\nModel parallelism requires more explicit coordination since frameworks must manage cross-device tensor placement and data flow. PyTorch addresses this through manual device placement and the emerging `torch.distributed.pipeline` API for pipeline parallelism.\n\n```python\n# Manual model parallelism - explicit device placement\nclass ModelParallelNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers_gpu0 = nn.Sequential(...).to(\"cuda:0\")\n        self.layers_gpu1 = nn.Sequential(...).to(\"cuda:1\")\n\n    def forward(self, x):\n        x = self.layers_gpu0(x.to(\"cuda:0\"))\n        x = self.layers_gpu1(\n            x.to(\"cuda:1\")\n        )  # Cross-GPU data transfer\n        return x\n```\n\nThis manual approach exposes the sequential dependencies and communication overhead inherent in model parallelism, requiring careful management of tensor movement between devices. The framework automatically handles the backward pass gradient flow across device boundaries, but practitioners must consider the performance implications of frequent device transfers.\n\n### Communication Primitives {#sec-distributed-training-communication-primitives}\n\nModern frameworks expose the communication operations that enable distributed training through high-level APIs. These primitives abstract the low-level NCCL operations while maintaining performance:\n\n```python\n# Framework-provided collective operations\ndist.all_reduce(tensor)  # Gradient averaging across all devices\ndist.broadcast(tensor, src=0)  # Parameter broadcasting from master\ndist.all_gather(\n    tensor_list, tensor\n)  # Collecting tensors from all devices\n```\n\nThese APIs translate directly to the NCCL collective operations that implement the efficient communication patterns discussed earlier, demonstrating how frameworks provide accessible interfaces to complex distributed systems concepts while maintaining the performance characteristics essential for production training.\n\nThe framework abstractions enable practitioners to focus on model architecture and training dynamics while leveraging sophisticated distributed systems optimizations. This separation of concerns—mathematical foundations handled by the framework, model design controlled by the practitioner—exemplifies how modern ML systems balance accessibility with performance.\n\n## Hardware Infrastructure for Scale {#sec-distributed-training-hardware-infrastructure}\n\nThe parallelism strategies examined in previous sections assume underlying hardware capable of efficient inter-device communication. This section examines the hardware architectures that enable AI systems to scale from individual accelerators to warehouse-scale computing, analyzing how multi-chip systems introduce qualitatively different constraints around communication overhead, memory coherence, and fault tolerance.\n\n### Multi-Chip AI Acceleration {#sec-distributed-training-multichip-acceleration}\n\nThe transition from single-chip to multi-chip architectures represents more than simple replication. It requires rethinking how computations distribute across processors, how data flows between chips, and how systems maintain coherence at scale. Where single-chip optimization focuses on maximizing utilization within fixed resources, multi-chip systems must balance computational distribution against communication overhead, memory coherence costs, and synchronization complexity. These challenges fundamentally transform the optimization landscape, requiring new abstractions and techniques beyond those developed for individual accelerators.\n\nThe scaling of AI systems follows a natural progression: integration within a single package through chiplet architectures, multi-GPU configurations within a server, distributed accelerator pods, and wafer-scale integration. Each approach presents unique trade-offs between computational density, communication overhead, and system complexity. Chiplet architectures maintain high-speed interconnects within a package, while distributed systems sacrifice communication latency for massive parallelism.\n\n#### Chiplet-Based Architectures {#sec-distributed-training-chiplet-architectures}\n\nChiplet architectures achieve scaling by partitioning large designs into smaller, modular dies that are interconnected within a single package. Small, specialized semiconductor dies connect together within a single package to create larger, more complex processors. AMD's EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies, with compute chiplets in 7 nm paired with I/O chiplets in 14 nm, optimizing each function independently.\n\nModern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning. AMD's Instinct MI300 exemplifies this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.\n\nHowever, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence, and thermal management become critical factors as more chiplets are integrated. Memory coherence presents particular complexity: ensuring all processors in a system see the same consistent view of shared memory when multiple cores or chips access the same data. Traditional cache coherence protocols like MESI add 10-50 ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive, so most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.\n\n#### Multi-GPU Systems {#sec-distributed-training-multi-gpu-systems}\n\nBeyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.\n\nA common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network).\n\nNVSwitch interconnects enable high-speed communication between GPUs, reducing bottlenecks in distributed training. However, scaling up the number of GPUs introduces fundamental distributed coordination challenges that become the dominant performance constraint. The arithmetic intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient synchronization across GPUs, where AllReduce operations must aggregate 175 billion parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth, but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds available capacity.\n\nThe coordination complexity compounds exponentially. While two GPUs require a single communication channel, eight GPUs need 28 interconnect paths, and fault tolerance requirements mandate redundant communication patterns. Memory consistency protocols further complicate coordination as different GPUs may observe weight updates at different times, requiring sophisticated synchronization primitives that can add 10-50 microseconds latency per training step. These seemingly small delays aggregate to hours of training time across million-iteration runs.\n\n#### Communication Overhead and Amdahl's Law {#sec-distributed-training-amdahl-analysis}\n\nThe fundamental limitation of distributed AI training stems from Amdahl's Law, which quantifies how communication overhead constrains parallel speedup regardless of available compute power. For distributed neural network training, communication overhead during gradient synchronization creates a sequential bottleneck that limits scalability even with infinite parallelism.\n\nThe maximum speedup achievable with distributed training is bound by Amdahl's Law:\n$$\n\\text{Speedup} = \\frac{1}{(1-P) + \\frac{P}{N}}\n$$\nwhere $P$ is the fraction of work that can be parallelized and $N$ is the number of processors. For AI training, the correct formulation accounts for communication time that does not decrease with more workers:\n$$\n\\text{Speedup} = \\frac{T_{compute}}{T_{compute}/N + T_{comm}}\n$$\nwhere $T_{comm}$ is largely independent of $N$ for ring AllReduce (or grows as $\\log N$ for tree-based approaches). This can be rewritten as:\n$$\n\\text{Speedup} = \\frac{N}{1 + N \\cdot (T_{comm}/T_{compute})}\n$$\n\nConsider training a 175 B parameter model with 1000 H100 GPUs as a concrete example:\n\n- **Computation time per iteration**: 100 ms of forward/backward passes per GPU\n- **Gradient size**: 175 B parameters × 4 bytes = 700 GB in FP32\n- **Ring AllReduce time**: For ring AllReduce, each GPU sends/receives $2 \\times (N-1)/N \\times 700\\text{GB} \\approx 1.4\\text{TB}$\n\nWith ring AllReduce across 1000 GPUs connected via 600 GB/s links:\n\n- **Intra-node (8 GPUs via NVLink)**: $1.4\\text{TB} / 600\\text{GB/s} \\approx 2.3$ seconds\n- **Inter-node adds latency**: $1000 \\times \\alpha$ where $\\alpha \\approx 1\\mu s$ per hop\n\nThe resulting scaling efficiency is:\n$$\n\\text{Efficiency} = \\frac{T_{compute}}{T_{compute} + T_{comm}} = \\frac{100\\text{ms}}{100\\text{ms} + 2300\\text{ms}} \\approx 4\\%\n$$\n\nThis is why real systems use mixed precision (FP16 = 350 GB, halving communication), gradient compression, and pipeline parallelism. With FP16 and 4-way pipeline parallelism reducing synchronization to 1/4 of parameters per stage, efficiency improves dramatically:\n$$\n\\text{Efficiency} = \\frac{100\\text{ms}}{100\\text{ms} + 290\\text{ms}} \\approx 26\\%\n$$\n\nThis demonstrates why pure data parallelism fails at scale and why hybrid strategies are essential.\n\nCommunication requirements scale superlinearly with model size and linearly with the number of parameters. Modern transformer models require gradient synchronization across all parameters during each training step:\n\n- **GPT-3 (175 B parameters)**: 700 GB gradient exchange per step\n- **GPT-4 (estimated 1.8 T parameters)**: approximately 7 TB gradient exchange per step\n- **Future 10 T parameter models**: approximately 40 TB gradient exchange per step\n\nEven with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization for 10 T parameter models would require 22+ seconds per training step, making distributed training impractical without algorithmic innovations like gradient compression or asynchronous updates.\n\n#### TPU Pods {#sec-distributed-training-tpu-pods}\n\nAs models and datasets continue to expand, training and inference workloads must extend beyond single-server configurations. Google's TPU Pods represent a pioneering approach to this challenge, interconnecting hundreds of TPUs to function as a unified system.\n\nThe architectural design of TPU Pods differs fundamentally from traditional multi-GPU systems. While multi-GPU configurations typically rely on NVLink or PCIe connections within a single machine, TPU Pods employ high-bandwidth optical links to interconnect accelerators at data center scale. This design implements a 2D torus interconnect topology, enabling efficient data exchange between accelerators while minimizing communication bottlenecks as workloads scale across nodes.\n\nThe effectiveness of this architecture is demonstrated in its performance scaling capabilities. TPU Pod performance exhibits near-linear scaling when running ResNet-50, from quarter-pod to full-pod configurations. The system achieves a remarkable 33.0x speedup when scaled to 1024 chips compared to a 16-TPU baseline.\n\nHowever, distributing AI workloads across an entire data center introduces distributed coordination challenges that fundamentally differ from single-node systems. The 2D torus interconnect, while providing high bisection bandwidth, creates communication bottlenecks when training large transformer models that require AllReduce operations across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through the torus network, with worst-case communication requiring 32 hops between distant TPUs, creating latency penalties that compound with model size.\n\nThe energy cost of coordination also scales dramatically: moving data across the pod's optical interconnect consumes 1000x more energy than on-chip communication within individual TPUs, transforming distributed training into a careful balance between computation parallelism and communication efficiency where AllReduce bandwidth, not compute capacity, determines overall training throughput.\n\n#### Wafer-Scale AI {#sec-distributed-training-wafer-scale}\n\nAt the frontier of AI scaling, wafer-scale integration represents a paradigm shift that abandons traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication.\n\nWafer-scale integration uses an entire 300 mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores, 125x more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23 kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.\n\nThe primary advantage of wafer-scale AI is its ultra-fast on-die communication. Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries between separate devices, wafer-scale AI enables near-instantaneous data transfer across its vast compute array. This architecture drastically reduces communication latency, achieving performance levels that remain out of reach for conventional multi-chip systems.\n\nAchieving this level of integration introduces substantial engineering challenges. Thermal dissipation, fault tolerance, and manufacturing yield become major constraints when fabricating a processor of this scale. Unlike distributed TPU systems, which mitigate failures by dynamically re-routing workloads, wafer-scale AI must incorporate built-in redundancy mechanisms to tolerate localized defects in the silicon.\n\n### Hardware Scaling Trade-offs {#sec-distributed-training-scaling-tradeoffs}\n\nThe progressive scaling of AI acceleration introduces new challenges at each step, from single-chip processors to increasingly complex architectures. @tbl-distributed-scaling-trajectory summarizes these trade-offs across different scaling approaches.\n\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **Scaling Approach** | **Key Feature**                     | **Challenges**                                      |\n+:=====================+:====================================+:====================================================+\n| **Chiplets**         | Modular scaling within a package    | Inter-chiplet latency, memory coherence             |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **Multi-GPU**        | External GPU interconnects (NVLink) | Synchronization overhead, communication bottlenecks |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **TPU Pods**         | Distributed accelerator clusters    | Interconnect congestion, workload partitioning      |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n| **Wafer-Scale AI**   | Entire wafer as a single processor  | Thermal dissipation, fault tolerance                |\n+----------------------+-------------------------------------+-----------------------------------------------------+\n\n: **AI Acceleration Scaling Trade-offs**: Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution. {#tbl-distributed-scaling-trajectory}\n\nWhile chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.\n\n### Multi-Chip Execution Strategies {#sec-distributed-training-execution-strategies}\n\nAs AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.\n\nExecution mapping in multi-chip systems requires computation placement that considers workload partitioning across multiple accelerators, with explicit coordination of execution order and dependencies. Computation scheduling must be interconnect-aware to manage communication delays effectively. Load balancing across accelerators is vital, as uneven task distribution results in some accelerators remaining underutilized while others operate at full capacity.\n\nDistributed memory allocation requires each accelerator to manage its own local memory, necessitating explicit allocation of model parameters, activations, and intermediate data across devices. Unlike single-chip execution where data is fetched once and reused, multi-chip setups require deliberate strategies to minimize redundant data transfers.\n\nData movement optimization addresses inter-chip data transfer, which becomes the primary bottleneck rather than memory hierarchy latency. Techniques include overlapping computation and communication so accelerators process data while simultaneously sending and receiving, and locality-aware scheduling that places computations on accelerators already holding required data.\n\nCompiler and runtime adaptations extend single-chip execution models to handle dynamic workload distribution across accelerators. Interconnect-aware workload partitioning enables compilers to distribute computations strategically based on communication cost. In TPU Pods, the runtime schedules computations across multiple TPU cores to minimize communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution synchronizes operations across GPUs while maintaining execution order.\n\n## Summary {#sec-distributed-training-summary}\n\nDistributed training transforms the computational challenge of training large-scale machine learning models into an orchestration problem across multiple devices and hardware architectures. Throughout this chapter, we have examined how parallelism strategies—data, model, pipeline, and hybrid—address different constraints in the training process. Data parallelism scales dataset processing by distributing training examples across devices while synchronizing gradients, achieving near-linear speedups when communication overhead remains manageable. Model parallelism addresses memory constraints by partitioning models across devices, enabling training of architectures that exceed single-device capacity. Pipeline parallelism improves hardware utilization through microbatching, while hybrid approaches combine these strategies for the largest-scale training workloads.\n\nThe hardware infrastructure enabling distributed training spans multiple scales: chiplet-based architectures within a single package, multi-GPU systems connected via NVLink and NVSwitch, TPU Pods with 2D torus interconnects, and wafer-scale integration. Each approach presents distinct trade-offs between communication bandwidth, memory coherence, and system complexity. Amdahl's Law quantifies the fundamental scaling limits: communication overhead during gradient synchronization creates sequential bottlenecks that constrain speedup regardless of available compute power, explaining why adding GPUs beyond approximately 100 provides diminishing returns without algorithmic innovations.\n\nThe efficiency metrics governing distributed training—communication overhead, scaling efficiency, and synchronization costs—directly influence system design decisions. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads, balancing the complexity of distributed coordination against the computational benefits of parallelization. Framework abstractions like PyTorch's DistributedDataParallel translate these concepts into practical implementations, allowing practitioners to leverage sophisticated distributed systems optimizations while focusing on model development.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Data parallelism achieves near-linear scaling when communication overhead remains below 30-40% of training time\n* Model parallelism enables training of models exceeding single-device memory but introduces sequential dependencies\n* Pipeline parallelism reduces device idle time through microbatching, improving hardware utilization\n* Hybrid parallelism combines strategies for training the largest models on the largest datasets\n* Multi-chip hardware (chiplets, multi-GPU, TPU Pods, wafer-scale) each present distinct trade-offs between integration density, interconnect bandwidth, and system complexity\n* Amdahl's Law quantifies scaling limits: communication overhead constrains speedup regardless of available compute power\n* Framework APIs abstract distributed complexity while preserving the performance characteristics essential for production training\n:::\n\n## Fallacies and Pitfalls {#sec-distributed-training-fallacies-pitfalls}\n\nDistributed training involves counterintuitive behavior that leads to common misconceptions. These fallacies and pitfalls capture errors that waste compute resources and delay research progress.\n\nLinear speedup remains theoretically impossible regardless of engineering effort. Amdahl's Law establishes hard limits: any sequential component bounds maximum speedup regardless of parallelism. In distributed training, gradient synchronization is inherently sequential since all gradients must be collected before any update proceeds.\n\nEven with perfect overlap and optimal algorithms, communication overhead grows with cluster size. For data parallelism, AllReduce time increases logarithmically with tree algorithms or linearly in the latency term with ring algorithms as GPU count grows. A 1000-GPU cluster will never train 1000x faster than a single GPU; achieving 500x speedup would be exceptional, and 100-200x is more typical for communication-heavy workloads.\n\nOrganizations that budget projects assuming linear scaling inevitably miss deadlines and overspend on compute.\n\nHyperparameters tuned on small clusters fail catastrophically at large scale. The most critical is learning rate: as batch size increases with data parallelism, learning rate typically must increase proportionally to maintain convergence rate. The \"linear scaling rule\" suggests $\\eta_{large} = \\eta_{base} \\times (B_{large}/B_{base})$.\n\nHowever, this rule has limits. Beyond the \"critical batch size\" (model and dataset dependent, often 8K-32K for vision models), increasing batch size provides diminishing returns. Larger batches find sharper minima that generalize poorly. Training that converges beautifully at 256 GPUs may diverge or produce worse models at 1024 GPUs with naive scaling.\n\nWarmup schedules, weight decay, and dropout rates also require adjustment. The only reliable approach is systematic scaling studies that validate hyperparameters at target scale.\n\nData parallelism does not scale indefinitely by adding more GPUs. Data parallelism increases effective batch size proportionally with GPU count, but statistical efficiency (loss reduction per sample) decreases with batch size beyond model-specific thresholds. A 100K-sample batch may provide only 2x the gradient quality of a 10K-sample batch, not 10x.\n\nThe critical batch size defines where marginal returns collapse. Beyond this point, additional GPUs increase throughput (samples per second) but not training efficiency (loss reduction per compute dollar). For BERT, critical batch size is approximately 8K; for ResNet, approximately 32K. Scaling beyond critical batch size wastes compute.\n\nLarge organizations have trained models to convergence using 1024 GPUs in the same wall-clock time as 512 GPUs at 2x the cost, because they exceeded critical batch size.\n\nPipeline parallelism and tensor parallelism both distribute model weights across devices, but their memory and compute characteristics differ dramatically.\n\nTensor parallelism splits each layer across devices, requiring AllReduce communication within each layer. This reduces memory proportionally but introduces communication overhead on the critical path. Pipeline parallelism assigns complete layers to devices, requiring only point-to-point communication between stages but introducing bubble overhead.\n\nFor memory-constrained scenarios where a model barely fits with splitting, tensor parallelism's even distribution helps. For throughput-maximizing scenarios with adequate memory, pipeline parallelism's lower communication overhead helps. Choosing based on one dimension (memory or compute) without considering the other leads to suboptimal configurations.\n\nFSDP and ZeRO do not always improve training efficiency. FSDP (Fully Sharded Data Parallel) and ZeRO reduce memory footprint by sharding optimizer state and gradients across GPUs. This enables larger batch sizes or larger models per GPU. However, sharding introduces communication overhead: AllGather before forward pass, ReduceScatter after backward pass.\n\nFor models that fit comfortably in GPU memory without sharding, FSDP adds overhead without benefit. A 7B model training on A100-80GB with batch size 4 runs faster with DDP than FSDP because the model fits entirely with room for activations.\n\nFSDP provides value when:\n\n- Model + optimizer state exceeds single-GPU memory\n- Enabling larger batch sizes justifies communication overhead\n- ZeRO-Offload to CPU extends effective memory\n\nApplying FSDP universally, as some tutorials suggest, degrades performance for models that do not require it.\n\nParallelism overhead is roughly constant regardless of model size: AllReduce time depends on gradient size, not model computation time. For small models where forward/backward pass takes 10ms and AllReduce takes 5ms, communication overhead is 50%. For large models where forward/backward takes 1000ms and AllReduce takes 5ms, overhead is 0.5%.\n\nDecisions made based on small-model benchmarks (\"pipeline parallelism is always slower\") invert at scale. The 20% overhead acceptable for a 1B model becomes 0.2% for a 100B model. Parallelism strategy evaluation must occur at target scale, or at minimum with analytical models that extrapolate appropriately.\n\nGradient accumulation is not free. Gradient accumulation simulates larger batch sizes by accumulating gradients across multiple forward/backward passes before synchronizing. This reduces communication frequency proportionally. However, accumulation has costs:\n\n1. **Memory**: Accumulated gradients consume memory throughout the accumulation window\n2. **Latency**: Effective step time increases proportionally with accumulation steps\n3. **Precision**: Accumulated FP16 gradients may overflow or underflow\n\nFor loss-sensitive early training, gradient accumulation can introduce instability from accumulated numerical errors. Organizations that use gradient accumulation to work around infrastructure limitations (slow network, small GPUs) sometimes discover training divergence that disappears with proper infrastructure.\n\nThe principles established in this chapter provide the foundation for understanding fault tolerance mechanisms, which become increasingly critical as distributed training scales to thousands of devices where failures become statistically inevitable.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"distributed_training.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","distributed_training.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3,"title":"Distributed Training Systems"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}