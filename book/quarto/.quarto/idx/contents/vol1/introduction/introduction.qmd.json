{"title":"Introduction","markdown":{"yaml":{"bibliography":"introduction.bib","quiz":"introduction_quizzes.json","concepts":"introduction_concepts.yml","glossary":"introduction_glossary.json"},"headingText":"Introduction","headingAttr":{"id":"sec-introduction","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting a roadmap of a book's chapters on machine learning systems, set on a crisp, clean white background. The image features a winding road traveling through various symbolic landmarks. Each landmark represents a chapter topic: Introduction, ML Systems, Deep Learning, AI Workflow, Data Engineering, AI Frameworks, AI Training, Efficient AI, Model Optimizations, AI Acceleration, Benchmarking AI, On-Device Learning, Embedded AIOps, Security & Privacy, Responsible AI, Sustainable AI, AI for Good, Robust AI, Generative AI. The style is clean, modern, and flat, suitable for a technical book, with each landmark clearly labeled with its chapter title._\n:::\n\n\\noindent\n![](images/png/cover_introduction.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why must we master the engineering principles that govern systems capable of learning, adapting, and operating at massive scale?_\n\nMachine learning represents the most significant transformation in computing since programmable computers, enabling systems whose behavior emerges from data rather than explicit instructions. This transformation requires new engineering foundations because traditional software engineering principles cannot address systems that learn and adapt based on experience. Every major technological challenge, from climate modeling and medical diagnosis to autonomous transportation, requires systems that process vast amounts of data and operate reliably despite uncertainty. Understanding ML systems engineering determines our ability to solve complex problems that exceed human cognitive capacity. This discipline provides the foundation for building systems that scale across deployment environments, from massive data centers to resource-constrained edge devices, establishing the technical groundwork for technological progress in the 21st century.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain machine learning systems as integrated architectures comprising data, algorithms, and computational infrastructure (AI Triad framework)\n\n- Distinguish ML systems from traditional software through their unique failure patterns and silent performance degradation characteristics\n\n- Trace the historical evolution of AI from symbolic reasoning through expert systems and statistical learning to modern deep learning architectures\n\n- Explain why general computational methods that leverage scale consistently outperform hand-crafted knowledge encoding approaches (the Bitter Lesson), establishing systems engineering as central to AI advancement\n\n- Describe how the ML development lifecycle differs from traditional software development through its continuous iteration and data-dependent adaptation\n\n- Identify the core engineering challenges in ML systems across data quality, model complexity, infrastructure scalability, and ethical considerations\n\n- Explain how the five-pillar framework (Data Engineering, Training Systems, Deployment Infrastructure, Operations, Ethics) organizes ML systems engineering to address data, training, deployment, operations, and ethics challenges\n\n- Analyze how deployment context (cloud, edge, mobile, embedded) influences engineering decisions across data collection, model design, infrastructure requirements, and operational practices\n\n- Evaluate whether a given problem is appropriate for ML-based solutions by considering data availability, success criteria measurability, tolerance for probabilistic behavior, and maintenance requirements\n\n:::\n\n## The Engineering Revolution in Artificial Intelligence {#sec-introduction-engineering-revolution-artificial-intelligence-a3eb}\n\nEngineering practice today stands at an inflection point comparable to the most transformative periods in technological history. The Industrial Revolution established mechanical engineering as a discipline for managing physical forces, while the Digital Revolution formalized computational engineering to handle algorithmic complexity. Today, artificial intelligence systems require a new engineering paradigm for systems that exhibit learned behaviors, autonomous adaptation, and operational scales that exceed conventional software engineering methods.\n\nThis shift reconceptualizes the nature of engineered systems. Traditional deterministic software operates according to explicitly programmed instructions, yielding predictable outputs for given inputs. Machine learning systems, by contrast, are probabilistic architectures whose behaviors emerge from statistical patterns extracted from training data. This transformation introduces engineering challenges that define ML systems engineering: ensuring reliability in systems whose behaviors are learned rather than programmed, achieving scalability for systems processing petabyte-scale[^fn-petabyte-scale] datasets while serving billions of concurrent users, and maintaining robustness when operational data distributions diverge from training distributions.\n\nThis textbook organizes around three foundational imperatives that address these challenges systematically. First, we must build the components of ML systems from data pipelines through model architectures, establishing the infrastructure and workflows that enable machine learning. Second, we must optimize these systems for efficiency, performance, and deployment constraints, ensuring they can operate effectively under real-world resource limitations. Third, we must operate them reliably in production environments, maintaining performance and adapting to changing conditions over time.\n\nThese challenges establish the theoretical and practical foundations of ML systems engineering as a distinct academic discipline. This chapter provides the conceptual foundation for understanding both the historical evolution that created this field and the engineering principles that differentiate machine learning systems from traditional software architectures. The analysis synthesizes perspectives from computer science, systems engineering, and statistical learning theory to establish a framework for the systematic study of intelligent systems.\n\nOur investigation begins with the relationship between artificial intelligence as a research objective and machine learning as the computational methodology for achieving intelligent behavior. We then establish what constitutes a machine learning system, the integrated computing systems comprising data, algorithms, and infrastructure that this discipline builds. Through historical analysis, we trace the evolution of AI paradigms from symbolic reasoning systems through statistical learning approaches to contemporary deep learning architectures, demonstrating how each transition required new engineering solutions. This progression illuminates Sutton's \"bitter lesson\" of AI research: that domain-general computational methods ultimately supersede hand-crafted knowledge representations, positioning systems engineering as central to AI advancement.\n\nThis historical and technical foundation enables us to formally define this discipline. Following the pattern established by Computer Engineering's emergence from Electrical Engineering and Computer Science, we establish it as a field focused on building reliable, efficient, and scalable machine learning systems across computational platforms. This formal definition addresses both the nomenclature used in practice and the technical scope of what practitioners actually build.\n\nBuilding upon this foundation, we introduce the theoretical frameworks that structure the analysis of ML systems throughout this text. The AI Triad provides a conceptual model for understanding the interdependencies among data, algorithms, and computational infrastructure. We examine the machine learning system lifecycle, contrasting it with traditional software development methodologies to highlight the unique phases of problem formulation, data curation, model development, validation, deployment, and continuous maintenance that characterize ML system engineering.\n\nThese theoretical frameworks are substantiated through examination of representative deployment scenarios that demonstrate the diversity of engineering requirements across application domains. From autonomous vehicles operating under stringent latency constraints at the network edge to recommendation systems serving billions of users through cloud infrastructure, these case studies illustrate how deployment context shapes system architecture and engineering trade-offs.\n\nThe analysis culminates by identifying the core challenges that establish ML systems engineering as both a necessary and complex discipline: silent performance degradation patterns that require specialized monitoring approaches, data quality issues and distribution shifts that compromise model validity, requirements for model robustness and interpretability in high-stakes applications, infrastructure scalability demands that exceed conventional distributed systems, and ethical considerations that impose new categories of system requirements. These challenges provide the foundation for the five-pillar organizational framework that structures this text, partitioning ML systems engineering into interconnected sub-disciplines that enable the development of robust, scalable, and responsible artificial intelligence systems.\n\nThis chapter establishes the theoretical foundation for Part I: Systems Foundations, introducing the principles that underlie all subsequent analysis of ML systems engineering. The conceptual frameworks introduced here provide the analytical tools that will be refined and applied throughout subsequent chapters, culminating in a methodology for engineering systems capable of reliably delivering artificial intelligence capabilities in production environments.\n\n## From Artificial Intelligence Vision to Machine Learning Practice {#sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a}\n\nHaving established AI's transformative impact across society, a question emerges: How do we actually create these intelligent capabilities? Understanding the relationship between Artificial Intelligence and Machine Learning provides the key to answering this question and is central to everything that follows.\n\nAI represents the broad goal of creating systems that can perform tasks requiring human-like intelligence: recognizing images, understanding language, making decisions, and solving problems. AI is the what, the vision of intelligent machines that can learn, reason, and adapt.\n\nMachine Learning (ML) represents the methodological approach and practical discipline for creating systems that demonstrate intelligent behavior. Rather than implementing intelligence through predetermined rules, machine learning provides the computational techniques to automatically discover patterns in data through mathematical processes. This methodology transforms AI's theoretical insights into functioning systems.\n\nConsider the evolution of chess-playing systems as an example of this shift. The AI goal remains constant: \"Create a system that can play chess like a human.\" However, the approaches differ:\n\n- **Symbolic AI Approach (Pre-ML)**: Program the computer with all chess rules and hand-craft strategies like \"control the center\" and \"protect the king.\" This requires expert programmers to explicitly encode thousands of chess principles, creating brittle systems that struggle with novel positions.\n\n- **Machine Learning Approach**: Have the computer analyze millions of chess games to learn winning strategies automatically from data. Rather than programming specific moves, the system discovers patterns that lead to victory through statistical analysis of game outcomes.\n\nThis transformation illustrates why ML has become the dominant approach: In rule-based systems, humans translate domain expertise directly into code. In ML systems, humans curate training data, design learning architectures, and define success metrics, allowing the system to extract its own operational logic from examples. Data-driven systems can adapt to situations that programmers never anticipated, while rule-based systems remain constrained by their original programming.\n\nMachine learning systems acquire recognition capabilities through processes that parallel human learning patterns. Object recognition develops through exposure to numerous examples, while natural language processing systems acquire linguistic capabilities through extensive textual analysis. These learning approaches operationalize theories of intelligence developed in AI research, building on mathematical foundations that we establish systematically throughout this text.\n\nThe distinction between AI as research vision and ML as engineering methodology carries significant implications for system design. Modern ML's data-driven approach requires infrastructure capable of collecting, processing, and learning from data at massive scale. Machine learning emerged as a practical approach to artificial intelligence through extensive research and major paradigm shifts[^fn-paradigm-shift], transforming theoretical principles about intelligence into functioning systems that form the algorithmic foundation of today's intelligent capabilities.\n\n[^fn-paradigm-shift]: **Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 [@kuhn1962structure] to describe major changes in scientific approach. In AI, the key paradigm shift was moving from symbolic reasoning (encoding human knowledge as rules) to statistical learning (discovering patterns from data). This shift had profound systems implications: rule-based systems scaled with programmer effort, requiring manual encoding of each new rule. Data-driven ML scales with compute and data infrastructure—achieving better performance by adding more GPUs and training data rather than more programmers. This transformation made systems engineering critical: success now depends on building infrastructure to collect massive datasets, train billion-parameter models, and serve predictions at scale, rather than encoding expert knowledge.\n\n[^fn-petabyte-scale]: **Petabyte-Scale Data**: One petabyte equals 1,000 terabytes or roughly 1 million gigabytes—enough to store 13.3 years of HD video or the entire written works of humanity 50 times over. Modern ML systems routinely process petabyte-scale datasets: Meta processes over 4 petabytes of data daily for its recommendation systems, while Google's search index contains hundreds of petabytes of web content. Managing this scale requires distributed storage systems (like HDFS or S3) that shard data across thousands of servers, parallel processing frameworks (like Apache Spark) that coordinate computation across clusters, and sophisticated data engineering pipelines that can validate, transform, and serve data at rates exceeding 100 GB/s. The engineering challenge isn't just storage capacity, but the bandwidth, fault tolerance, and consistency guarantees needed to make petabyte datasets useful for training and inference.\n\n::: {.callout-definition title=\"Key Definitions\"}\n***Artificial Intelligence (AI)*** is the field of computer science focused on creating systems that perform tasks requiring human-like _intelligence_, including _learning_, _reasoning_, and _adaptation_.\n\n***Machine Learning (ML)*** is the approach to AI that enables systems to automatically learn _patterns_ and make _decisions_ from _data_ rather than following explicit programmed rules.\n:::\n\nThe evolution from rule-based AI to data-driven ML represents one of the most significant shifts in computing history. This transformation explains why ML systems engineering has emerged as a discipline: the path to intelligent systems now runs through the engineering challenge of building systems that can effectively learn from data at massive scale.\n\n## Defining ML Systems {#sec-introduction-defining-ml-systems-bf7d}\n\nBefore exploring how we arrived at modern machine learning systems, we must first establish what we mean by an \"ML system.\" This definition provides the conceptual framework for understanding both the historical evolution and contemporary challenges that follow.\n\nNo universally accepted definition of machine learning systems exists, reflecting the field's rapid evolution and multidisciplinary nature. However, building on our understanding that modern ML relies on data-driven approaches at scale, this textbook adopts a perspective that encompasses the entire ecosystem in which algorithms operate:\n\n:::{.callout-definition title=\"Machine Learning System\"}\n***Machine Learning Systems*** are integrated computing systems comprising three interdependent components: _data_ that guides behavior, _algorithms_ that learn patterns, and _computational infrastructure_ that enables both _training_ and _inference_.\n:::\n\nAs illustrated in @fig-ai-triad, the core of any machine learning system consists of three interrelated components that form a triangular dependency: Models/Algorithms, Data, and Computing Infrastructure. Each element shapes the possibilities of the others. The model architecture dictates both the computational demands for training and inference, as well as the volume and structure of data required for effective learning. The data's scale and complexity influence what infrastructure is needed for storage and processing, while determining which model architectures are feasible. The infrastructure capabilities establish practical limits on both model scale and data processing capacity, creating a framework within which the other components must operate.\n\n::: {#fig-ai-triad fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.8}{%\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{\n Line/.style={line width=0.35pt,black!50,text=black},\n ALineA/.style={violet!80!black!50,line width=3pt,shorten <=2pt,shorten >=2pt,\n  {Triangle[width=1.1*6pt,length=0.8*6pt]}-{Triangle[width=1.1*6pt,length=0.8*6pt]}},\nLineD/.style={line width=0.75pt,black!50,text=black,dashed,dash pattern=on 5pt off 3pt},\nCircle/.style={inner xsep=2pt,\n  circle,\n    draw=BrownLine,\n    line width=0.75pt,\n    fill=BrownL!40,\n    minimum size=16mm\n  },\n circles/.pic={\n\\pgfkeys{/channel/.cd, #1}\n\\node[circle,draw=\\channelcolor,line width=\\Linewidth,fill=\\channelcolor!10,\nminimum size=2.5mm](\\picname){};\n        }\n}\n\\tikzset {\npics/cloud/.style = {\n        code = {\n\\colorlet{red}{RedLine}\n\\begin{scope}[local bounding box=CLO,scale=0.5, every node/.append style={transform shape}]\n\\draw[red,fill=white,line width=0.9pt](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)\nto[out=360,in=30,distance=9](1.68,0.42);\n\\draw[red,fill=white,line width=0.9pt](0,0)to[out=170,in=180,distance=11](0.1,0.61)\nto[out=90,in=105,distance=17](1.07,0.71)\nto[out=20,in=75,distance=7](1.48,0.36)\nto[out=350,in=0,distance=7](1.48,0)--(0,0);\n\\draw[red,fill=white,line width=0.9pt](0.27,0.71)to[bend left=25](0.49,0.96);\n\n\\end{scope}\n    }\n  }\n}\n%streaming\n\\tikzset{%\n LineST/.style={-{Circle[\\channelcolor,fill=RedLine,length=4pt]},draw=\\channelcolor,line width=\\Linewidth,rounded corners},\n ellipseST/.style={fill=\\channelcolor,ellipse,minimum width = 2.5mm, inner sep=2pt, minimum height =1.5mm},\n BoxST/.style={line width=\\Linewidth,fill=white,draw=\\channelcolor,rectangle,minimum width=56,\n minimum height=16,rounded corners=1.2pt},\n pics/streaming/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=STREAMING,scale=\\scalefac, every node/.append style={transform shape}]\n\\node[BoxST,minimum width=44,minimum height=48](\\picname-RE1){};\n\\foreach \\i/\\j in{1/north,2/center,3/south}{\n\\node[BoxST](\\picname-GR\\i)at(\\picname-RE1.\\j){};\n\\node[ellipseST]at($(\\picname-GR\\i.west)!0.2!(\\picname-GR\\i.east)$){};\n\\node[ellipseST]at($(\\picname-GR\\i.west)!0.4!(\\picname-GR\\i.east)$){};\n}\n\\draw[LineST](\\picname-GR3)--++(2,0)coordinate(\\picname-C4);\n\\draw[LineST](\\picname-GR3.320)--++(0,-0.7)--++(0.8,0)coordinate(\\picname-C5);\n\\draw[LineST](\\picname-GR3.220)--++(0,-0.7)--++(-0.8,0)coordinate(\\picname-C6);\n\\draw[LineST](\\picname-GR3)--++(-2,0)coordinate(\\picname-C7);\n \\end{scope}\n     }\n  }\n}\n%data\n\\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,\nminimum width=25mm,minimum height=11mm,line width=\\Linewidth,node distance=-0.15},\npics/data/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=STREAMING,scale=\\scalefac, every node/.append style={transform shape}]\n\\node[mycylinder,fill=\\channelcolor!50] (A) {};\n\\node[mycylinder, above=of A,fill=\\channelcolor!30] (B) {};\n\\node[mycylinder, above=of B,fill=\\channelcolor!10] (C) {};\n \\end{scope}\n     }\n  }\n}\n\\pgfkeys{\n  /channel/.cd,\n  channelcolor/.store in=\\channelcolor,\n  drawchannelcolor/.store in=\\drawchannelcolor,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  channelcolor=BrownLine,\n  drawchannelcolor=BrownLine,\n  scalefac=1,\n  Linewidth=0.5pt,\n  picname=C\n}\n\\node[Circle](MO){};\n\\node[Circle,below left=1 and 2.5 of MO,draw=GreenLine,fill=GreenL!40,](IN){};\n\\node[Circle,below right=1 and 2.5 of MO,draw=OrangeLine,fill=OrangeL!40,](DA){};\n\\draw[ALineA](MO)--(IN);\n\\draw[ALineA](MO)--(DA);\n\\draw[ALineA](DA)--(IN);\n\\node[below=2pt of MO]{Model};\n\\node[below=2pt of IN]{Infra};\n\\node[below=2pt of DA]{Data};\n%%\n\\begin{scope}[local bounding box=CIRCLE1,shift={($(MO)+(0.04,-0.24)$)},\nscale=0.55, every node/.append style={transform shape}]\n%1 column\n\\foreach \\j in {1,2,3} {\n  \\pgfmathsetmacro{\\y}{(1.5-\\j)*0.43 + 0.7}\n  \\pic at (-0.8,\\y) {circles={channelcolor=RedLine,picname=1CD\\j}};\n}\n%2 column\n\\foreach \\i in {1,...,4} {\n  \\pgfmathsetmacro{\\y}{(2-\\i)*0.43+0.7}\n  \\pic at (0,\\y) {circles={channelcolor=RedLine, picname=2CD\\i}};\n}\n%3 column\n\\foreach \\j in {1,2} {\n  \\pgfmathsetmacro{\\y}{(1-\\j)*0.43 + 0.7}\n  \\pic at (0.8,\\y) {circles={channelcolor=RedLine,picname=3CD\\j}};\n}\n\\foreach \\i in {1,2,3}{\n  \\foreach \\j in {1,2,3,4}{\n\\draw[Line](1CD\\i)--(2CD\\j);\n}}\n\\foreach \\i in {1,2,3,4}{\n  \\foreach \\j in {1,2}{\n\\draw[Line](2CD\\i)--(3CD\\j);\n}}\n\\end{scope}\n%\n\\pic[shift={(-0.4,-0.08)}] at (IN) {cloud};\n%\n\\pic[shift={(-0.05,-0.13)}] at  (IN){streaming={scalefac=0.25,picname=2,channelcolor=RedLine, Linewidth=0.65pt}};\n%\n\\pic[shift={(0,-0.3)}] at  (DA){data={scalefac=0.3,picname=1,channelcolor=green!70!black, Linewidth=0.4pt}};\n\\end{tikzpicture}}\n```\n**Component Interdependencies**: Machine learning system performance relies on the coordinated interaction of models, data, and computing infrastructure; limitations in any one component constrain the capabilities of the others. Effective system design requires balancing these interdependencies to optimize overall performance and feasibility.\n:::\n\nEach component serves a distinct but interconnected purpose:\n\n- **Algorithms**: Mathematical models and methods that learn patterns from data to make predictions or decisions\n\n- **Data**: Processes and infrastructure for collecting, storing, processing, managing, and serving data for both training and inference\n\n- **Computing**: Hardware and software infrastructure that enables training, serving, and operation of models at scale\n\nAs the triangle illustrates, no single element can function in isolation. Algorithms require data and computing resources, large datasets require algorithms and infrastructure to be useful, and infrastructure requires algorithms and data to serve any purpose.\n\nThis triangular dependency means that advancing any single component in isolation provides limited benefit. Improved algorithms cannot realize their potential without sufficient data and computational capacity. Larger datasets become burdensome without algorithms capable of extracting meaningful patterns and infrastructure capable of processing them efficiently. More powerful hardware accelerates computation but cannot compensate for poor data quality or unsuitable algorithmic approaches. Machine learning systems demand careful orchestration of all three components, with each constraining and enabling the others.\n\nThese interdependencies become clear when examining breakthrough moments in AI history. The 2012 AlexNet[^fn-alexnet-breakthrough] breakthrough illustrates the principle of hardware-software co-design that defines modern ML systems engineering. This deep learning revolution succeeded because the algorithmic innovation (convolutional neural networks) matched the hardware capability (parallel GPU architectures), graphics processing units originally designed for gaming but repurposed for AI computations, providing 10-100x speedups over traditional CPUs for machine learning tasks. Convolutional operations are inherently parallel, making them naturally suited to GPU's thousands of parallel cores. This co-design approach continues to shape ML system development across the industry.\n\n[^fn-alexnet-breakthrough]: **AlexNet**: A breakthrough deep learning model created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that won the 2012 ImageNet competition by a massive margin, reducing top-5 error rates from 26.2% to 15.3%. This was the \"ImageNet moment\" that proved deep learning could outperform traditional computer vision approaches and sparked the modern AI revolution. AlexNet demonstrated that with enough data (1.2 million images), computing power (two GPUs for 6 days), and clever engineering (dropout, data augmentation), neural networks could achieve superhuman performance on complex visual tasks.\n\nWith this three-component framework established, we must understand a fundamental difference that distinguishes ML systems from traditional software: how failures manifest across the AI Triad's components.\n\n## How ML Systems Differ from Traditional Software {#sec-introduction-ml-systems-differ-traditional-software-4370}\n\nThe AI Triad framework reveals what ML systems comprise: data that guides behavior, algorithms that extract patterns, and infrastructure that enables learning and inference. However, understanding these components alone does not capture what makes ML systems engineering fundamentally different from traditional software engineering. The critical distinction lies in how these systems fail.\n\nTraditional software exhibits explicit failure modes. When code breaks, applications crash, error messages propagate, and monitoring systems trigger alerts. This immediate feedback enables rapid diagnosis and remediation. The system operates correctly or fails observably. Machine learning systems operate under a fundamentally different paradigm: they can continue functioning while their performance degrades silently without triggering conventional error detection mechanisms. The algorithms continue executing, the infrastructure maintains prediction serving, yet the learned behavior becomes progressively less accurate or contextually relevant.\n\nConsider how an autonomous vehicle's perception system illustrates this distinction. Traditional automotive software exhibits binary operational states: the engine control unit either manages fuel injection correctly or triggers diagnostic warnings. The failure mode remains observable through standard monitoring. An ML-based perception system presents a qualitatively different challenge: the system's accuracy in detecting pedestrians might decline from 95% to 85% over several months due to seasonal changes, as different lighting conditions, clothing patterns, or weather phenomena underrepresented in training data affect model performance. The vehicle continues operating, successfully detecting most pedestrians, yet the degraded performance creates safety risks that become apparent only through systematic monitoring of edge cases and comprehensive evaluation. Conventional error logging and alerting mechanisms remain silent while the system becomes measurably less safe.\n\nThe magnitude of this degradation matters profoundly in safety-critical contexts. For autonomous vehicles, even 95% accuracy may be inadequate: safety-critical systems typically require 99.9% or higher reliability. The 10% degradation from 95% to 85% is especially concerning because failures concentrate in edge cases where detection was already marginal, precisely the scenarios where human safety is most at risk.\n\nThis silent degradation manifests across all three AI Triad components. The data distribution shifts as the world changes: user behavior evolves, seasonal patterns emerge, new edge cases appear. The algorithms continue making predictions based on outdated learned patterns, unaware that their training distribution no longer matches operational reality. The infrastructure faithfully serves these increasingly inaccurate predictions at scale, amplifying the problem. A recommendation system experiencing this degradation might decline from 85% accuracy to 60% over six months as user preferences evolve and training data becomes stale. The system continues generating recommendations, users receive results, the infrastructure reports healthy uptime metrics, yet business value silently erodes. This degradation often stems from training-serving skew, where features computed differently between training and serving pipelines cause model performance to degrade despite unchanged code, which is an infrastructure issue that manifests as algorithmic failure.\n\nThis fundamental difference in failure modes distinguishes ML systems from traditional software in ways that demand new engineering practices. Traditional software development focuses on eliminating bugs and ensuring deterministic behavior. ML systems engineering must additionally address probabilistic behaviors, evolving data distributions, and performance degradation that occurs without code changes. The monitoring systems must track not just infrastructure health but also model performance, data quality, and prediction distributions. The deployment practices must enable continuous model updates as data distributions shift. The entire system lifecycle, from data collection through model training to inference serving, must be designed with silent degradation in mind.\n\nThis operational reality establishes why ML systems developed in research settings require specialized engineering practices to reach production deployment. The unique lifecycle and monitoring requirements that ML systems demand stem directly from this failure characteristic, establishing the fundamental motivation for ML systems engineering as a distinct discipline.\n\nUnderstanding how ML systems fail differently raises an important question: given the three components of the AI Triad (data, algorithms, and infrastructure), which should we prioritize to advance AI capabilities? Should we invest in better algorithms, larger datasets, or more powerful computing infrastructure? The answer to this question reveals why systems engineering has become central to AI progress.\n\n## The Bitter Lesson: Why Systems Engineering Matters {#sec-introduction-bitter-lesson-systems-engineering-matters-dede}\n\nThe single biggest lesson from 70 years of AI research is that systems that can leverage massive computation ultimately win. This is why systems engineering, not just algorithmic cleverness, has become the bottleneck for progress in AI.\n\nThe evolution from symbolic AI through statistical learning to deep learning raises a fundamental question for system builders: Should we focus on developing more sophisticated algorithms, curating better datasets, or building more powerful infrastructure?\n\nThe answer to this question shapes how we approach building AI systems and reveals why systems engineering has emerged as a discipline.\n\nHistory provides a consistent answer. Across decades of AI research, the greatest breakthroughs have not come from better encoding of human knowledge or more algorithmic techniques, but from finding ways to leverage greater computational resources more effectively. This pattern, articulated by reinforcement learning pioneer Richard Sutton[^fn-richard-sutton] in his 2019 essay \"The Bitter Lesson\" [@sutton2019bitter], suggests that systems engineering has become the determinant of AI success.\n\n[^fn-richard-sutton]: **Richard Sutton**: A pioneering AI researcher who transformed how machines learn through reinforcement learning—teaching AI systems to learn from trial and error, like how you learned to ride a bike through practice rather than instruction manuals. At the University of Alberta, Sutton co-authored the foundational textbook \"Reinforcement Learning: An Introduction\" and developed key algorithms (TD-learning, policy gradients) that power everything from AlphaGo to modern robotics. He received the 2024 ACM Turing Award (computing's highest honor, often called the \"Nobel Prize of Computing\") shared with Andrew Barto for their decades of foundational contributions to how AI systems learn and adapt. His \"Bitter Lesson\" essay distills 70 years of AI history into one profound insight: general methods leveraging computation consistently beat approaches that encode human expertise.\n\nSutton observed that approaches emphasizing human expertise and domain knowledge, while providing short-term improvements, are consistently surpassed by general methods that can leverage massive computational resources. He writes: \"The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.\"\n\nThis principle finds validation across AI breakthroughs. In chess, IBM's Deep Blue defeated world champion Garry Kasparov in 1997 [@campbell2002deep] not by encoding chess strategies, but through brute-force search evaluating millions of positions per second. In Go, DeepMind's AlphaGo [@silver2016mastering] achieved superhuman performance by learning from self-play rather than studying centuries of human Go wisdom. In computer vision, convolutional neural networks that learn features directly from data have surpassed decades of hand-crafted feature engineering. In speech recognition, end-to-end deep learning systems have outperformed approaches built on detailed models of human phonetics and linguistics.\n\nThe \"bitter\" aspect of this lesson is that our intuition misleads us. We naturally assume that encoding human expertise should be the path to artificial intelligence. Yet repeatedly, systems that leverage computation to learn from data outperform systems that rely on human knowledge, given sufficient scale. This pattern has held across symbolic AI, statistical learning, and deep learning eras, a consistency we will examine in detail when we trace AI's historical evolution in the next section.\n\nConsider modern language models like GPT-4 or image generation systems like DALL-E. Their capabilities emerge not from linguistic or artistic theories encoded by humans, but from training general-purpose neural networks on vast amounts of data using enormous computational resources. Training large language models consumes substantial energy: estimates for GPT-3-scale training suggest several thousand MWh based on hardware specifications and reported training duration [@patterson2021carbon], equivalent to hundreds of U.S. homes for a year. Serving models to millions of users requires data centers consuming megawatts of continuous power. The engineering challenge is building systems that can manage this scale: collecting and processing petabytes of training data, coordinating training across thousands of GPUs, serving models to millions of users with millisecond latency, and continuously updating systems based on real-world performance.\n\nThese scale requirements reveal a fundamental engineering reality: building systems capable of training on petabytes of data and serving millions of users requires expertise in distributed systems, data engineering, and hardware optimization that goes far beyond algorithmic innovation. The computational infrastructure needed to realize modern AI capabilities has become the primary engineering challenge, from managing data movement between storage and processing units[^fn-memory-bandwidth] to coordinating thousands of processors and optimizing for both performance and energy efficiency. We explore these hardware constraints quantitatively in @sec-ai-acceleration, where students will have the prerequisite background to analyze memory bandwidth limitations and their implications for system design.\n\n[^fn-thermal-power-constraints]: **Thermal and Power Constraints**: The physical limits imposed by heat generation and power consumption in computing hardware. Modern GPUs consume 300-700W each (equivalent to 3-7 hair dryers running continuously) and generate enormous heat that must be removed via sophisticated cooling systems. A single AI training cluster with 1,000 GPUs consumes 300-700 kW of power just for computation, plus 30-50% more for cooling, totaling ~1MW—equivalent to powering 750 homes. Data centers hit thermal density limits: you can only pack so many hot chips together before cooling becomes impossible or prohibitively expensive. These constraints drive hardware design choices (chip architectures optimized for performance-per-watt), infrastructure decisions (liquid cooling vs. air cooling), and economic trade-offs (power costs can exceed hardware costs over 3-year lifespans). Power/thermal management explains many ML system architecture decisions, from edge deployment to model compression.\n\n[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be transferred between memory and processors, measured in GB/s (gigabytes per second). Modern GPUs like the H100 provide ~3TB/s memory bandwidth, while CPUs typically provide 100-200 GB/s. This seemingly large number becomes the bottleneck for ML workloads: a transformer model with 70 billion parameters requires 140GB just to store weights, taking 47ms to load at 3TB/s before any computation begins. The bandwidth constraint explains why ML accelerators focus on higher bandwidth memory (HBM) rather than just faster compute units. For comparison, arithmetic operations are relatively cheap: a GPU can perform trillions of multiply-add operations in the time it takes to move 1GB from memory, creating a fundamental tension where processors spend more time waiting for data than computing.\n\n[^fn-distributed-systems]: **Distributed Systems**: Computing systems where components run on multiple networked machines and coordinate through message passing. Modern ML training exemplifies distributed systems complexity: training GPT-3 required coordinating 1,024 V100 GPUs across multiple data centers, each processing different data batches while synchronizing gradient updates. Key challenges include fault tolerance (handling machine failures mid-training), network bottlenecks (all-reduce operations can consume 40%+ of total training time), and consistency (ensuring all nodes use the same model weights). Unlike traditional distributed systems focused on serving requests, ML distributed systems must coordinate massive data movement and maintain numerical precision across thousands of nodes, making consensus algorithms and load balancing far more complex.\n\n[^fn-amdahls-law]: **Amdahl's Law**: Formulated by computer architect Gene Amdahl in 1967, this law quantifies the theoretical speedup of a program when only part of it can be parallelized. The speedup is limited by the sequential portion: if P is the fraction that can be parallelized, maximum speedup = 1/(1-P). In ML systems, this explains why memory bandwidth and data movement often become the primary bottlenecks rather than compute capacity. We develop this concept with worked examples in @sec-ai-acceleration.\n\nSutton's bitter lesson helps explain the motivation for this book. If AI progress depends on our ability to scale computation effectively, then understanding how to build, deploy, and maintain these computational systems becomes the most important skill for AI practitioners. ML systems engineering has become important because creating modern systems requires coordinating thousands of GPUs across multiple data centers, processing petabytes of text data, and serving resulting models to millions of users with millisecond latency requirements. This challenge demands expertise in distributed systems[^fn-distributed-systems], data engineering, hardware optimization, and operational practices that represent an entirely new engineering discipline.\n\nThe convergence of these systems-level challenges suggests that no existing discipline addresses what modern AI requires. While Computer Science advances ML algorithms and Electrical Engineering develops specialized AI hardware, neither discipline alone provides the engineering principles needed to deploy, optimize, and sustain ML systems at scale. This gap requires a new engineering discipline. But to understand why this discipline has emerged now and what form it takes, we must first trace the evolution of AI itself, from early symbolic systems to modern machine learning.\n\n## Historical Evolution of AI Paradigms {#sec-introduction-historical-evolution-ai-paradigms-796e}\n\nThe systems-centric perspective we've established through the Bitter Lesson didn't emerge overnight. It developed through decades of AI research where each major transition revealed new insights about the relationship between algorithms, data, and computational infrastructure. Tracing this evolution helps us understand not just technological progress, but the shifts in approach that explain today's emphasis on scalable systems.\n\nUnderstanding why this transition to systems-focused ML is happening now requires recognizing the convergence of three factors in the last decade:\n\n1. **Massive Datasets**: The internet age created unprecedented data volumes through web content, social media, sensor networks, and digital transactions. Public datasets like ImageNet (millions of labeled images) and Common Crawl (billions of web pages) provide the raw material for learning complex patterns.\n\n2. **Algorithmic Breakthroughs**: Deep learning proved remarkably effective across diverse domains, from computer vision to natural language processing. Techniques like transformers, attention mechanisms, and transfer learning enabled models to learn generalizable representations from data.\n\n3. **Hardware Acceleration**: Graphics Processing Units (GPUs) originally designed for gaming provided 10-100x speedups for machine learning computations. Cloud computing infrastructure made this computational power accessible without massive capital investments.\n\nThis convergence explains why we've moved from theoretical models to large-scale deployed systems requiring a new engineering discipline. Each factor amplified the others: bigger datasets demanded more computation, better algorithms justified larger datasets, and faster hardware enabled more algorithms. This convergence transformed AI from an academic curiosity to a production technology requiring robust engineering practices.\n\nThe evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt [@rosenblatt1957perceptron], an early computational learning algorithm. Computer labs in 1965 contained room-sized mainframes[^fn-mainframes] running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, though groundbreaking for their time, differed substantially from today's machine learning systems that detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997 [@campbell2002deep]. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023 [@openai2023gpt4], demonstrating the dramatic evolution and increasing complexity of AI systems over the decades.\n\n[^fn-early]: **Perceptron**: One of the first computational learning algorithms (1957), simple enough to implement in hardware with minimal memory—1950s mainframes could only store thousands of weights, not millions. This hardware constraint shaped early AI research toward simple, interpretable models. The Perceptron's limitation to linearly separable problems wasn't just algorithmic—multi-layer networks (which could solve non-linear problems) were proposed in the 1960s but remained computationally intractable until the 1980s when memory became cheaper and CPUs faster. This 20-year gap between algorithmic insight and practical implementation foreshadowed a pattern in AI: breakthrough algorithms often wait decades for hardware to catch up, explaining why ML systems engineering focuses on co-designing algorithms with available infrastructure.\n\n[^fn-mainframes]: **Mainframes**: Room-sized computers that dominated the 1960s-70s, typically costing millions of dollars and requiring dedicated cooling systems. IBM's System/360 mainframe from 1964 weighed up to 20,000 pounds and had 8KB-1MB of memory depending on model, about 1/millionth the memory of a modern smartphone, yet represented the cutting edge of computing power that enabled early AI research.\n\n[^fn-eliza]: **ELIZA**: Created by MIT's Joseph Weizenbaum in 1966 [@weizenbaum1966eliza], ELIZA was one of the first chatbots that could simulate human conversation by pattern matching and substitution. From a systems perspective, ELIZA ran on 256KB mainframes using simple pattern matching—no learning, no data storage, no training phase. This computational simplicity allowed real-time interaction on 1960s hardware but resulted in brittleness that motivated the shift to data-driven ML. Modern chatbots like GPT-3 require vastly more infrastructure (350GB model parameters when uncompressed, $4.6M training cost estimate, GPU servers for inference) but handle conversations ELIZA couldn't—illustrating the systems trade-off: rule-based systems are computationally cheap but brittle, while ML systems are infrastructure-intensive but flexible. Ironically, Weizenbaum was horrified when people formed emotional attachments to his simple program, leading him to become a critic of AI.\n\n::: {#fig-ai-timeline fig-env=\"figure\" fig-pos=\"t!\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\definecolor{bluegraph}{RGB}{0,102,204}\n    \\pgfmathsetlengthmacro\\MajorTickLength{\n      \\pgfkeysvalueof{/pgfplots/major tick length} * 1.5\n    }\n\\tikzset{%\n   textt/.style={line width=0.5pt,draw=bluegraph,text width=26mm,align=flush center,\n                        font=\\usefont{T1}{phv}{m}{n}\\footnotesize,fill=cyan!7},\n   Line/.style={line width=0.85pt,draw=bluegraph,dash pattern=on 3pt off 2pt,\n   {Circle[bluegraph,length=4.5pt]}-   }\n}\n\n\\begin{axis}[clip=false,\n  axis line style={thick},\n  axis lines*=left,\n  axis on top,\n  width=18cm,\n  height=20cm,\n  xmin=1950,\n  xmax=2023,\n  ymin=0.000000,\n  ymax=0.00033,\n  xtick={1950,1960,1970,1980,1990,2000,2010,2020},\n  extra x ticks={1955,1965,1975,1985,1995,2005,2015},\n  extra x tick labels={},\n  xticklabels={1950,1960,1970,1980,1990,2000,2010,2020},\n  ytick={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},\n  yticklabels={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},\n  grid=none,\n    tick label style={/pgf/number format/assume math mode=true},\n    xticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n},\n   yticklabel style={\n  font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n  /pgf/number format/fixed,\n  /pgf/number format/fixed zerofill,\n  /pgf/number format/precision=5\n},\nscaled y ticks=false,\ntick style = {line width=1.0pt},\ntick align = outside,\nmajor tick length=\\MajorTickLength,\n]\n\\fill[fill=BrownL!70](axis cs:1974,0)rectangle(axis cs:1980,0.00031)\n        node[above,align=center,xshift=-7mm]{1st AI \\\\ Winter};\n\\fill[fill=BrownL!70](axis cs:1987,0)rectangle(axis cs:1993,0.00031)\n        node[above,align=center,xshift=-7mm]{2nd AI \\\\ Winter};\n\\addplot[line width=2pt,color=RedLine,smooth,samples=100] coordinates {\n(1950,0.0000006281)\n(1951,0.0000000683)\n(1952,0.0000003056)\n(1953,0.0000002927)\n(1954,0.0000004296)\n(1955,0.0000004593)\n(1956,0.0000016705)\n(1957,0.0000006570)\n(1958,0.0000021902)\n(1959,0.0000032832)\n(1960,0.0000126863)\n(1961,0.0000063721)\n(1962,0.0000240680)\n(1963,0.0000141502)\n(1964,0.0000111442)\n(1965,0.0000143832)\n(1966,0.0000147726)\n(1967,0.0000169539)\n(1968,0.0000167880)\n(1969,0.0000175559)\n(1970,0.0000155680)\n(1971,0.0000206809)\n(1972,0.0000223804)\n(1973,0.0000218203)\n(1974,0.0000256138)\n(1975,0.0000282924)\n(1976,0.0000247784)\n(1977,0.0000404966)\n(1978,0.0000358032)\n(1979,0.0000436903)\n(1980,0.0000472788)\n(1981,0.0000561471)\n(1982,0.0000767864)\n(1983,0.0001064465)\n(1984,0.0001592212)\n(1985,0.0002133700)\n(1986,0.0002559067)\n(1987,0.0002608470)\n(1988,0.0002623321)\n(1989,0.0002358150)\n(1990,0.0002301105)\n(1991,0.0002051343)\n(1992,0.0001789229)\n(1993,0.0001560935)\n(1994,0.0001508219)\n(1995,0.0001401406)\n(1996,0.0001169577)\n(1997,0.0001150365)\n(1998,0.0001051385)\n(1999,0.0000981740)\n(2000,0.0001010236)\n(2001,0.0000976966)\n(2002,0.0001038084)\n(2003,0.0000980004)\n(2004,0.0000989412)\n(2005,0.0000977251)\n(2006,0.0000899964)\n(2007,0.0000864005)\n(2008,0.0000911872)\n(2009,0.0000852932)\n(2010,0.0000822649)\n(2011,0.0000913442)\n(2012,0.0001104912)\n(2013,0.0001023061)\n(2014,0.0001022477)\n(2015,0.0000919719)\n(2016,0.0001134797)\n(2017,0.0001384348)\n(2018,0.0002057324)\n(2019,0.0002328642)\n}\nnode[left,pos=1,align=center,black]{Last year of\\\\ date: 2019};\n\n\\node[textt,text width=20mm](1950)at(axis cs:1957,0.00014){\\textcolor{red}{1950}\\\\\nAlan Turing publishes \\textbf{``Computing Machinery and Intelligence''} in the journal \\textit{Mind}.};\n\\node[red,align=center,above=2mm of 1950]{Milestones\\\\ in AI};\n\\draw[Line] (axis cs:1950,0) -- (1950.235);\n%\n\\node[textt,text width=19mm](1956)at(axis cs:1958,0.00007){\\textcolor{red}{Summer 1956}\\\\\n\\textbf{Dartmouth Workshop} A formative conference organized by AI pioneer John McCarthy.};\n\\draw[Line] (axis cs:1956,0) -- (1956.255);\n%\n\\node[textt](1957)at(axis cs:1969,0.00022){\\textcolor{red}{1957}\\\\\n\\textbf{Cornell psychologist Frank Rosenblatt invents the perceptron}, a system that paves the way for\nmodern neural networks\n(see \"The Turbulent Past and Uncertain Future of Artificial Intelligence,\" p. 26).};\n\\draw[Line] (axis cs:1957,0) -- ++(0mm,17mm)-|(1957.248);\n%\n\\node[textt,text width=21mm](1966)at(axis cs:1972,0.00012){\\textcolor{red}{1966}\\\\\n\\textbf{ELIZA chatbot} An early example of natural-language programming created by\nMIT professor Joseph Weizenbaum.};\n\\draw[Line] (axis cs:1966,0) -- ++(0mm,17mm)-|(1966);\n%\n\\node[textt,text width=20mm](1979)at(axis cs:1985,0.00012){\\textcolor{red}{1979}\\\\\nHans Moravec builds the \\textbf{Stanford Cart}, one of the first autonomous vehicles.};\n\\draw[Line] (axis cs:1979,0) -- ++(0mm,17mm)-|(1979.245);\n%\n\\node[textt,text width=21mm](1981)at(axis cs:1990,0.00006){\\textcolor{red}{1981}\\\\\nJapanese \\textbf{Fifth-Generation Computer Systems} project begins. The infusion of\nresearch funding helps end first \"AI winter.\"};\n\\draw[Line] (axis cs:1981,0) -- ++(0mm,10mm)-|(1981);\n%\n\\node[textt,text width=15mm](1997)at(axis cs:2001,0.00007){\\textcolor{red}{1997}\\\\\n\\textbf{IBM's Deep Blue} beats world chess champion Garry Kasparov};\n\\draw[Line] (axis cs:1997,0) -- ++(0mm,10mm)-|(1997);\n%\n\\node[textt,text width=15mm](2011)at(axis cs:2014,0.00003){\\textcolor{red}{2011}\\\\\n\\textbf{IBM's Watson} wins at Jeopardy!};\n\\draw[Line] (axis cs:2011,0) -- (2011);\n%\n\\node[textt,text width=19mm](2005)at(axis cs:2012,0.00009){\\textcolor{red}{2005}\\\\\n\\textbf{DARPA Grand Challenge} Stanford wins the agency's second driverless-car\ncompetition by driving 212 kilometers on an unrehearsed trail};\n\\draw[Line] (axis cs:2005,0) -- (2005);\n%\n\\node[textt,text width=30mm](2020)at(axis cs:2010,0.00017){\\textcolor{red}{2020}\\\\\n\\textbf{OpenAI introduces GPT-3}. The enormously powerful natural-language model\nlater causes an outcry when it begins spouting bigoted remarks};\n\\draw[Line] (axis cs:2020,0) |- (2020);\n%\n\\draw[Line,solid,-] (axis cs:1991,0.0002) --++(50:35mm)\nnode[bluegraph,above,align=center,text width=30mm]{Percent of U.S.-published books\nin Google's database that mention artificial intelligence};\n\\end{axis}\n\\end{tikzpicture}\n```\n**AI Development Timeline**: Early AI research focused on symbolic reasoning and rule-based systems, while modern AI leverages data-driven approaches like neural networks to achieve increasingly complex tasks. This progression exposes a shift from hand-coded intelligence to learned intelligence, marked by milestones such as the perceptron, deep blue, and large language models like GPT-3.\n:::\n\nExamining this timeline reveals several distinct eras of development, each building upon the lessons of its predecessors while addressing limitations that prevented earlier approaches from achieving their promise.\n\n### Symbolic AI Era {#sec-introduction-symbolic-ai-era-9d27}\n\nThe story of machine learning begins at the historic Dartmouth Conference[^fn-dartmouth-conference] in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term \"artificial intelligence\" [@mccarthy1956dartmouth]. Their approach assumed that intelligence could be reduced to symbol manipulation. Daniel Bobrow's STUDENT system from 1964 [@bobrow1964student] exemplifies this era by solving algebra word problems through natural language understanding.\n\n[^fn-dartmouth-conference]: **Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was officially born. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, it was the first time researchers gathered specifically to discuss \"artificial intelligence,\" a term McCarthy coined for the proposal. The ambitious goal was to make machines \"simulate every aspect of learning or any other feature of intelligence.\" From a systems perspective, participants fundamentally underestimated resource requirements—they assumed AI would fit on 1950s hardware (64KB memory maximum, kilohertz to low megahertz processors). Reality required 1,000,000x more resources: modern language models use 350GB memory and exaflops of training compute. This million-fold miscalculation of scale requirements helps explain why early symbolic AI failed: researchers focused on algorithmic cleverness while ignoring infrastructure constraints. The lesson: AI progress requires both algorithmic innovation AND systems engineering to provide necessary computational resources.\n\n::: {.callout-example title=\"STUDENT (1964)\"}\n```\nProblem: \"If the number of customers Tom gets is twice the\nsquare of 20% of the number of advertisements he runs, and\nthe number of advertisements is 45, what is the number of\ncustomers Tom gets?\"\n\nSTUDENT would:\n\n1. Parse the English text\n2. Convert it to algebraic equations\n3. Solve the equation: n = 2(0.2 × 45)²\n4. Provide the answer: 162 customers\n```\n:::\n\nEarly AI like STUDENT suffered from a limitation: they could only handle inputs that exactly matched their pre-programmed patterns and rules. This \"brittleness\"[^fn-brittleness] meant that while these solutions could appear intelligent when handling very specific cases they were designed for, they would break down completely when faced with even minor variations or real-world complexity. This limitation drove the evolution toward statistical approaches that we'll examine in the next section.\n\n[^fn-brittleness]: **Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle. This contrasts with human intelligence, which can adapt and make reasonable guesses even in unfamiliar situations. From a systems perspective, brittleness made deployment infeasible beyond controlled lab conditions—each new edge case required programmer intervention, creating unsustainable operational overhead. A speech recognition system encountering a new accent would fail rather than degrade gracefully, requiring system updates rather than continuous operation. ML's ability to generalize enables real-world deployment despite unpredictable inputs, shifting the challenge from explicit rule programming to infrastructure for collecting training data and continuously updating models as new patterns emerge.\n\n### Expert Systems Era {#sec-introduction-expert-systems-era-c7dd}\n\nRecognizing the limitations of symbolic AI, researchers by the mid-1970s acknowledged that general AI was overly ambitious and shifted their focus to capturing human expert knowledge in specific, well-defined domains. MYCIN [@shortliffe1976mycin], developed at Stanford, emerged as one of the first large-scale expert systems designed to diagnose blood infections.\n\n::: {.callout-example title=\"MYCIN (1976)\"}\n```\nRule Example from MYCIN:\nIF\n  The infection is primary-bacteremia\n  The site of the culture is one of the sterile sites\n  The suspected portal of entry is the gastrointestinal tract\nTHEN\n  Found suggestive evidence (0.7) that infection is bacteroid\n```\n:::\n\nMYCIN represented a major advance in medical AI with 600 expert rules for diagnosing blood infections, yet it revealed key challenges persisting in contemporary ML. Getting domain knowledge from human experts and converting it into precise rules proved time-consuming and difficult, as doctors often couldn't explain exactly how they made decisions. MYCIN struggled with uncertain or incomplete information, unlike human doctors who could make educated guesses. Maintaining and updating the rule base became more complex as MYCIN grew, as adding new rules frequently conflicted with existing ones, while medical knowledge itself continued to evolve. Knowledge capture, uncertainty handling, and maintenance remain concerns in modern machine learning, addressed through different technical approaches.\n\n### Statistical Learning Era {#sec-introduction-statistical-learning-era-8116}\n\nThese challenges with knowledge capture and system maintenance drove researchers toward a different approach. The 1990s marked a transformation in artificial intelligence as the field shifted from hand-coded rules toward statistical learning approaches.\n\nThree converging factors made statistical methods possible and powerful. First, the digital revolution meant massive amounts of data were available to train algorithms. Second, Moore's Law [@moore1965cramming][^fn-mooreslaw] delivered the computational power needed to process this data effectively. Third, researchers developed new algorithms like Support Vector Machines and improved neural networks that could learn patterns from data rather than following pre-programmed rules.\n\nThis combination transformed AI development: rather than encoding human knowledge directly, machines could discover patterns automatically from examples, creating more robust and adaptable systems.\n\n[^fn-mooreslaw]: **Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. Moore's Law enabled ML by providing approximately 1,000x more transistor density from 2000-2020, making previously impossible algorithms practical—neural networks proposed in the 1980s became viable only after 2010. However, slowing Moore's Law (transistor doubling now takes 3-4 years) drives innovation in specialized accelerators (TPUs provide 15-30x gains over GPUs through custom ML hardware) and algorithmic efficiency (techniques like quantization and pruning reduce compute requirements 4-10x). The systems lesson: when general hardware improvements slow, specialized hardware and efficient algorithms become critical.\n\nEmail spam filtering evolution illustrates this transformation. Early rule-based systems used explicit patterns but exhibited the same brittleness we saw with symbolic AI systems, proving easily circumvented. Statistical systems took a different approach: if the word 'viagra' appears in 90% of spam emails but only 1% of normal emails, we can use this pattern to identify spam. Rather than writing explicit rules, statistical systems learn these patterns automatically from thousands of example emails, making them adaptable to new spam techniques. The mathematical foundation relies on Bayes' theorem to calculate the probability that an email is spam given specific words: $P(\\text{spam}|\\text{word}) = P(\\text{word}|\\text{spam}) \\times P(\\text{spam}) / P(\\text{word})$. For emails with multiple words, we combine these probabilities across the entire message assuming conditional independence of words given the class (spam or not spam), which allows efficient computation despite the simplifying assumption that words don't depend on each other.\n\n::: {.callout-example title=\"Early Spam Detection Systems\"}\n```\nRule-based (1980s):\nIF contains(\"viagra\") OR contains(\"winner\") THEN spam\n\nStatistical (1990s):\nP(spam|word) = (frequency in spam emails) / (total frequency)\n\nCombined using Naive Bayes:\nP(spam|email) ∝ P(spam) × ∏ P(word|spam)\n```\n:::\n\nStatistical approaches introduced three concepts that remain central to AI development. First, the quality and quantity of training data became as important as the algorithms themselves. AI could only learn patterns that were present in its training examples. Second, rigorous evaluation methods became necessary to measure AI performance, leading to metrics that could measure success and compare different approaches. Third, a tension exists between precision (being right when making a prediction) and recall (catching all the cases we should find), forcing designers to make explicit trade-offs based on their application's needs. These challenges require systematic approaches: @sec-data-engineering covers data quality and drift detection, while @sec-benchmarking-ai addresses evaluation metrics and precision-recall trade-offs. Spam filters might tolerate some spam to avoid blocking important emails, while medical diagnosis systems prioritize catching every potential case despite increased false alarms.\n\n@tbl-ai-evolution-strengths summarizes the evolutionary journey of AI approaches, highlighting key strengths and capabilities emerging with each paradigm. Moving from left to right reveals important trends. Before examining shallow and deep learning, understanding trade-offs between existing approaches provides important context.\n\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Aspect**             | **Symbolic AI**          | **Expert Systems**       | **Statistical Learning** | **Shallow / Deep Learning**  |\n+:=======================+:=========================+:=========================+:=========================+:=============================+\n| **Key Strength**       | Logical reasoning        | Domain expertise         | Versatility              | Pattern recognition          |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Best Use Case**      | Well-defined, rule-based | Specific domain problems | Various structured data  | Complex, unstructured data   |\n|                        | problems                 |                          | problems                 | problems                     |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Data Handling**      | Minimal data needed      | Domain knowledge-based   | Moderate data required   | Large-scale data processing  |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Adaptability**       | Fixed rules              | Domain-specific          | Adaptable to various     | Highly adaptable to diverse  |\n|                        |                          | adaptability             | domains                  | tasks                        |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Problem Complexity** | Simple, logic-based      | Complicated, domain-     | Complex, structured      | Highly complex, unstructured |\n|                        |                          | specific                 |                          |                              |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n\n: **AI Paradigm Evolution**: Shifting from symbolic AI to statistical approaches transformed machine learning by prioritizing data quantity and quality, enabling rigorous performance evaluation, and necessitating explicit trade-offs between precision and recall to optimize system behavior for specific applications. The table outlines how each paradigm addressed these challenges, revealing a progression towards data-driven systems capable of handling complex, real-world problems. {#tbl-ai-evolution-strengths}\n\nThis analysis bridges early approaches with recent developments in shallow and deep learning. It explains why certain approaches gained prominence in different eras and how each paradigm built upon predecessors while addressing their limitations. Earlier approaches continue to influence modern AI techniques, particularly in foundation model development.\n\nThese core concepts that emerged from statistical learning (data quality, evaluation metrics, and precision-recall trade-offs) became the foundation for all subsequent developments in machine learning.\n\n### Shallow Learning Era {#sec-introduction-shallow-learning-era-2500}\n\nBuilding on these statistical foundations, the 2000s marked a significant period in machine learning history known as the \"shallow learning\" era. The term \"shallow\" refers to architectural depth: shallow learning typically employed one or two processing levels, contrasting with deep learning's multiple hierarchical layers that emerged later.\n\nDuring this time, several algorithms dominated the machine learning landscape. Each brought unique strengths to different problems: Decision trees[^fn-decision-trees] provided interpretable results by making choices much like a flowchart. K-nearest neighbors made predictions by finding similar examples in past data, like asking your most experienced neighbors for advice. Linear and logistic regression offered straightforward, interpretable models that worked well for many real-world problems. Support Vector Machines[^fn-svms] (SVMs) excelled at finding complex boundaries between categories using the \"kernel trick\"[^fn-kernel-trick]. This technique transforms complex patterns by projecting data into higher dimensions where linear separation becomes possible. These algorithms formed the foundation of practical machine learning.\n\n[^fn-decision-trees]: **Decision Trees**: A machine learning algorithm that makes predictions by following a series of yes/no questions, much like a flowchart. Popularized in the 1980s, decision trees are highly interpretable—you can trace exactly why the algorithm made each decision. From a systems perspective, decision trees require minimal memory and compute compared to neural networks: a typical decision tree model might be 1-10MB versus 100MB-10GB for deep learning models, with inference taking microseconds on a single CPU core. This makes them ideal for resource-constrained deployments where model size matters more than maximum accuracy—embedded systems, mobile devices, or scenarios requiring real-time decisions with minimal latency. They remain widely used in medical diagnosis and loan approval where regulations require explainability.\n\n[^fn-svms]: **Support Vector Machines (SVMs)**: A powerful machine learning algorithm developed by Vladimir Vapnik in the 1990s that finds the optimal boundary between different categories of data. SVMs were the dominant technique for many classification problems before deep learning emerged, winning numerous machine learning competitions. From a systems perspective, SVMs excel with small datasets (thousands of examples vs millions needed for deep learning), requiring less training infrastructure—a high-end workstation can train SVMs that would require GPU clusters for equivalent deep learning models. However, SVMs don't scale well beyond ~100K data points due to O(n²) to O(n³) training complexity, limiting their use for massive modern datasets. They remain deployed in text classification, bioinformatics, and scenarios where data is limited but accuracy is crucial.\n\n[^fn-kernel-trick]: **Kernel Trick**: A mathematical technique that allows algorithms like SVMs to find complex, non-linear patterns by transforming data into higher-dimensional spaces where linear separation becomes possible. For example, data points that form a circle in 2D space can be projected into 3D space where they become linearly separable. From a systems view, the kernel trick trades memory for computation efficiency: precomputing kernel matrices requires O(n²) memory, limiting SVMs to datasets under ~100K points on typical hardware (a 100K×100K matrix with 8-byte entries requires 80GB RAM). This memory constraint explains why deep learning, despite requiring more computation, scales better to massive datasets—neural networks' memory requirements grow linearly with data size, not quadratically.\n\nA typical computer vision solution from 2005 exemplifies this approach:\n\n::: {.callout-example title=\"Traditional Computer Vision Pipeline\"}\n```\n1. Manual Feature Extraction\n  - SIFT (Scale-Invariant Feature Transform)\n  - HOG (Histogram of Oriented Gradients)\n  - Gabor filters\n2. Feature Selection/Engineering\n3. \"Shallow\" Learning Model (e.g., SVM)\n4. Post-processing\n```\n:::\n\nThis era's hybrid approach combined human-engineered features with statistical learning. They had strong mathematical foundations (researchers could prove why they worked). They performed well even with limited data. They were computationally efficient. They produced reliable, reproducible results.\n\nThe Viola-Jones algorithm [@viola2001rapidobject][^fn-viola-jones] (2001) exemplifies this era, achieving real-time face detection using simple rectangular features and cascaded classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade.\n\n[^fn-viola-jones]: **Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in real-time by using simple rectangular patterns (like comparing the brightness of eye regions versus cheek regions) and making decisions in stages, filtering out non-faces quickly and spending more computation only on promising candidates. The algorithm achieved real-time performance (24 fps) on 2001 hardware by computing features in <0.001ms using integral images—a clever preprocessing technique that enables constant-time rectangle sum computation. This efficiency enabled embedded camera deployment in consumer devices (digital cameras, phones), demonstrating how algorithm-hardware co-design enables new applications. The cascade approach reduced computation 10-100x by rejecting easy negatives early, making real-time vision feasible on CPUs that would be 1000x slower than modern GPUs.\n\n[^fn-cascade]: **Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quickly rejecting obvious non-matches and passing promising candidates to the next, more sophisticated stage. This approach is similar to how security screening works at airports with multiple checkpoints of increasing thoroughness. From a systems perspective, cascades achieve 10-100x computational savings by focusing expensive computation only on promising candidates—early stages might reject 95% of inputs with 1% of total computation. This compute-saving pattern appears throughout edge ML systems where power budgets matter: modern mobile face detection uses neural network cascades that process most frames with tiny networks (<1MB), escalating to larger networks (>10MB) only for ambiguous cases, enabling continuous face detection on milliwatt power budgets.\n\n### Deep Learning Era {#sec-introduction-deep-learning-era-f6c0}\n\nWhile Support Vector Machines excelled at finding complex category boundaries through mathematical transformations, deep learning adopted a different approach inspired by brain architecture. Rather than relying on human-engineered features, deep learning employs layers of simple computational units inspired by brain neurons, with each layer transforming input data into increasingly abstract representations. While @sec-dl-primer establishes the mathematical foundations of neural networks, @sec-dnn-architectures explores the detailed architectures that enable this layered learning approach.\n\nIn image processing, this layered approach works systematically. The first layer detects simple edges and contrasts, subsequent layers combine these into basic shapes and textures, higher layers recognize specific features like whiskers and ears, and final layers assemble these into concepts like \"cat.\"\n\nUnlike shallow learning methods requiring carefully engineered features, deep learning networks automatically discover useful features from raw data. This layered approach to learning, building from simple patterns to complex concepts, defines \"deep\" learning and proves effective for complex, real-world data like images, speech, and text.\n\nAlexNet, shown in @fig-alexnet, achieved a breakthrough in the 2012 ImageNet[^fn-intro-imagenet] competition that transformed machine learning through a perfect alignment of algorithmic innovation and hardware capability. The network required two NVIDIA GTX 580 GPUs, with 60 million parameters trained on 1.2 million images. Training consumed approximately 1,287 GPU-hours over 6 days, achieving 15.3% top-5 error rate compared to 26.2% for second place, a 42% relative improvement. The parallel architecture of GPUs proved naturally suited to the matrix operations underlying neural network computation, enabling 10-100x speedups over CPU implementations and reducing training time from months to days. This demonstrated that specialized hardware could unlock previously intractable algorithms when algorithm and hardware capabilities were aligned [@krizhevsky2012imagenet].\n\n[^fn-intro-imagenet]: **ImageNet**: A massive visual database containing over 14 million labeled images across 21,841 categories (full dataset), created by Stanford's Fei-Fei Li starting in 2009 [@deng2009imagenet]. The annual ImageNet challenge became the Olympics of computer vision, driving breakthrough after breakthrough in image recognition until neural networks became so good they essentially solved the competition. From a systems perspective, ImageNet's ~150GB size (2009) was manageable on single-server storage systems. Modern vision datasets like LAION-5B (5 billion image-text pairs, ~240TB of images) require distributed storage infrastructure and parallel data loading pipelines during training. This 1000x growth in dataset size drove innovations in distributed data engineering—systems must now shard datasets across dozens of storage nodes and coordinate parallel data loading to keep thousands of GPUs fed with training examples.\n\nThe success of AlexNet wasn't just a technical achievement; it was a watershed moment that demonstrated the practical viability of deep learning. This breakthrough required both algorithmic innovation and systems engineering advances. The achievement wasn't just algorithmic, it was enabled by framework infrastructure like Theano that could orchestrate GPU parallelism, handle automatic differentiation at scale, and manage the complex computational workflows that deep learning demands. Without these framework foundations, the algorithmic insights would have remained computationally intractable.\n\nThis pattern of requiring both algorithmic and systems breakthroughs has defined every major AI advance since. Modern frameworks represent infrastructure that transforms algorithmic possibilities into practical realities. Automatic differentiation (autograd) systems represent perhaps the most important innovation that makes modern deep learning possible, handling gradient computation automatically and enabling the complex architectures we use today. Understanding this framework-centric perspective (that major AI capabilities emerge from the intersection of algorithms and systems engineering) is important for building robust, scalable machine learning systems. This single result triggered an explosion of research and applications in deep learning that continues to this day. The infrastructure requirements that enabled this breakthrough represent the convergence of algorithmic innovation with systems engineering that this book explores.\n\n::: {#fig-alexnet fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\clip (-11.2,-2) rectangle (15.5,5.45);\n%\\draw[red](-11.2,-1.7) rectangle (15.5,5.45);\n\\tikzset{%\n LineD/.style={line width=0.7pt,black!50,dashed,dash pattern=on 3pt off 2pt},\n  LineG/.style={line width=0.75pt,GreenLine},\n  LineR/.style={line width=0.75pt,RedLine},\n  LineA/.style={line width=0.75pt,BrownLine,-latex,text=black}\n}\n\\newcommand\\FillCube[4]{\n\\def\\depth{#2}\n\\def\\width{#3}\n\\def\\height{#4}\n\\def\\nc{#1}\n% Lower front left corner\n\\coordinate (A\\nc) at (0, 0);\n% Donji prednji desni\n\\coordinate (B\\nc) at (\\width, 0);\n% Upper front right\n\\coordinate (C\\nc) at (\\width, \\height);\n% Upper front left\n\\coordinate (D\\nc) at (0, \\height);\n% Pomak u \"dubinu\"\n\\coordinate (shift) at (-0.7*\\depth, \\depth);\n% Last points (moved)\n\\coordinate (E\\nc) at ($(A\\nc) + (shift)$);\n\\coordinate (F\\nc) at ($(B\\nc) + (shift)$);\n\\coordinate (G\\nc) at ($(C\\nc) + (shift)$);\n\\coordinate (H\\nc) at ($(D\\nc) + (shift)$);\n% Front side\n\\draw[GreenLine,fill=green!08,line width=0.5pt] (A\\nc) -- (B\\nc) -- (C\\nc) --(D\\nc) -- cycle;\n% Top side\n\\draw[GreenLine,fill=green!20,line width=0.5pt] (D\\nc) -- (H\\nc) -- (G\\nc) -- (C\\nc);\n% Left\n\\draw[GreenLine,fill=green!15] (A\\nc) -- (E\\nc) -- (H\\nc)--(D\\nc)--cycle;\n\\draw[] (E\\nc) -- (H\\nc);\n\\draw[GreenLine,line width=0.75pt](A\\nc)--(B\\nc)--(C\\nc)--(D\\nc)--(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--(H\\nc);\n}\n%%%\n\\newcommand\\SmallCube[4]{\n\\def\\nc{#1}\n\\def\\depth{#2}\n\\def\\width{#3}\n\\def\\height{#4}\n\\coordinate (A\\nc) at (0, 0);\n\\coordinate (B\\nc) at (\\width, 0);\n\\coordinate (C\\nc) at (\\width, \\height);\n\\coordinate (D\\nc) at (0, \\height);\n\\coordinate (shift) at (-0.7*\\depth, \\depth);\n\\coordinate (E\\nc) at ($(A\\nc) + (shift)$);\n\\coordinate (F\\nc) at ($(B\\nc) + (shift)$);\n\\coordinate (G\\nc) at ($(C\\nc) + (shift)$);\n\\coordinate (H\\nc) at ($(D\\nc) + (shift)$);\n\\draw[RedLine,fill=red!08,line width=0.5pt,fill opacity=0.7] (A\\nc) -- (B\\nc) -- (C\\nc) -- (D\\nc) -- cycle;\n\\draw[RedLine,fill=red!20,line width=0.5pt,fill opacity=0.7] (D\\nc) -- (H\\nc) -- (G\\nc) -- (C\\nc);\n\\draw[RedLine,fill=red!15,fill opacity=0.7] (A\\nc) -- (E\\nc) -- (H\\nc)--(D\\nc)--cycle;\n\\draw[] (E\\nc) -- (H\\nc);\n}\n%%%%%%%%%%%%%%%%%%%%%\n%%4 column\n%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}\n%big cube\n\\begin{scope}\n\\FillCube{4VD}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.10,0.4)},line width=0.5pt]\n\\SmallCube{4MD}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{4VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{13} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{13} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(0,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{4VG}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.18,0.55)}]\n\\SmallCube{4MG}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n\\def\\nc{4VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%\n%%5 column\n%%%%\n%%small cube\n\\begin{scope}[shift={(4.15,0)}]\n%big cube\n\\begin{scope}\n\\FillCube{5VD}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.10,1.25)}]\n\\SmallCube{5MD}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{5VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{13} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{13} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(4.15,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{5VG}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.08,0.28)}]\n\\SmallCube{5MG}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{5VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%3 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-3.75,-0.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{3VD}{1.5}{2.33}{3}\n\\end{scope}\n%%small cube-down\n\\begin{scope}[shift={(-0.10,0.45)}]\n\\SmallCube{3MDI}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\end{scope}\n%%small cube - up\n\\begin{scope}[shift={(-0.12,2.23)}]\n\\SmallCube{3MDII}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1,pos=0.4]{27} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{27} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(-3.75,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{3VG}{1.5}{2.33}{3}\n\\end{scope}\n%%small cube-down\n\\begin{scope}[shift={(-0.42,0.75)}]\n\\SmallCube{3MGI}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VG}\n\\draw[GreenLine,line width=0.75pt](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n%%small cube-up\n\\begin{scope}[shift={(-0.06,0.18)}]\n\\SmallCube{3MGII}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%2 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-6.8,-1)}]\n%big cube\n\\begin{scope}\n\\FillCube{2VD}{2}{1.3}{3.8}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.2,2.5)}]\n\\SmallCube{2MD}{0.4}{1.3}{1}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{5}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{5}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{2VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{48} (B\\nc)--\n(C\\nc)--(D\\nc)--node[pos=0.6,right,text=black,text opacity=1]{55} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[pos=0.26,right,text=black,text opacity=1]{55} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(-6.8,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{2VG}{2}{1.3}{3.8}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.1,0.5)}]\n\\SmallCube{2MG}{0.4}{1.3}{1}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{5}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{5}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{2VG}\n\\draw[LineG](A\\nc)--node[above,text=black]{48} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%1 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-9.0,-1.2)}]\n%big cube\n\\begin{scope}\n\\FillCube{1VD}{2}{0.2}{4.55}\n\\end{scope}\n%%small cube=down\n\\begin{scope}[shift={(-0.25,0.5)}]\n\\SmallCube{1MDI}{0.8}{0.15}{1.7}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--\n(C\\nc)--(D\\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{1VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{3} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--node[below left,text=black]{224}(E\\nc)--\nnode[left,text=black,text opacity=1]{224}(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)-- (H\\nc);\n\\end{scope}\n%%small cube=up\n\\begin{scope}[shift={(-0.75,3.4)}]\n\\SmallCube{1MDII}{0.8}{0.15}{1.7}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--\n(C\\nc)--(D\\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{1VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{3} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--node[below left,text=black]{224}(E\\nc)--\nnode[left,text=black,text opacity=1]{224}(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)-- (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%\n\\begin{scope}[shift={(8.15,0)}]\n\\begin{scope}\n\\FillCube{6VD}{0.8}{2.0}{2}\n\\path(A6VD)--node[below]{128}(B6VD);\n\\path(A6VD)--node[right]{13}(D6VD);\n\\path(D6VD)--node[right]{13}(H6VD);\n\\end{scope}\n%up\n\\begin{scope}[shift={(0,3.5)}]\n\\FillCube{6VG}{0.8}{2.0}{2}\n\\path(A6VG)--node[below]{128}(B6VG);\n\\end{scope}\n\\end{scope}\n\n\\newcommand\\Boxx[3]{\n\\node[draw,LineG,fill=green!10,rectangle,minimum width=7mm,minimum height=#2](#1){};\n\\node[below=2pt of #1]{#3};\n}\n\\begin{scope}[shift={(11.7,1.0)}]\n \\Boxx{B1D}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(11.7,5.25)}]\n \\Boxx{B1G}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(13.5,1.0)}]\n \\Boxx{B2D}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(13.5,5.25)}]\n \\Boxx{B2G}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(15.0,1.0)}]\n \\Boxx{B3}{19mm}{1000}\n\\end{scope}\n%%%\n\\node[right=3pt of B1VD,align=center]{Stride\\\\ of 4};\n\\node[right=3pt of B2VD,align=center]{Max\\\\ pooling};\n\\node[right=3pt of B3VD,align=center]{Max\\\\ pooling};\n\\node[below=3pt of B6VD,align=center]{Max\\\\ pooling};\n%\n\\coordinate(1C2)at($(A2VD)!0.4!(D2VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 1MDI)--(1C2);\n}\n\\coordinate(2C2)at($(E2VG)!0.2!(H2VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 1MDII)--(2C2);\n}\n%3\n\\coordinate(1C3)at($(A3VD)!0.55!(H3VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 2MD)--(1C3);\n}\n\\coordinate(2C3)at($(A3MGI)!0.35!(D3MGI)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 2MG)--(2C3);\n}\n%4\n\\coordinate(1C4)at($(A4VG)!0.15!(D4VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MGI)--(1C4);\n}\n\\coordinate(2C4)at($(G4MD)!0.15!(H4MD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MGII)--(2C4);\n}\n\\coordinate(3C4)at($(A4MG)!0.5!(C4MG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MDII)--(3C4);\n}\n\\coordinate(3C4)at($(A4VD)!0.12!(D4VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MDI)--(3C4);\n}\n%5\n\\coordinate(1C5)at($(A5MG)!0.82!(H5MG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 4MG)--(1C5);\n}\n\\coordinate(2C5)at($(A5VD)!0.52!(C5VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 4MD)--(2C5);\n}\n%6\n\\coordinate(1C6)at($(A6VG)!0.52!(C6VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 5MG)--(1C6);\n}\n\\coordinate(1C6)at($(D6VD)!0.3!(B6VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 5MD)--(1C6);\n}\n%\n\\draw[LineA]($(B6VD)!0.52!(C6VD)$)coordinate(X1)--\nnode[below]{dense}(X1-|B1D.north west);\n\\draw[LineA](B1D)--node[below]{dense}(B2D);\n\\draw[LineA](B2D)--(B3);\n%\n\\draw[LineA]($(B6VG)!0.52!(C6VG)$)coordinate(X1)--(X1-|B1G.north west);\n\\draw[LineA]($(B6VG)!0.52!(C6VG)$)--(B1D);\n\\draw[LineA]($(B6VD)!0.52!(C6VD)$)--(B1G);\n\\draw[LineA](B1D)--(B2G);\n\\draw[LineA](B1G)--(B2D);\n\\draw[LineA](B2G)--node[right]{dense}(B3);\n\\draw[LineA]($(B1G.north east)!0.7!(B1G.south east)$)--($(B2G.north west)!0.7!(B2G.south west)$);\n\\end{tikzpicture}\n```\n**Convolutional Neural Network Architecture**: AlexNet demonstrated that deep neural networks could automatically learn effective features from images, dramatically outperforming traditional computer vision methods. This breakthrough showed that with sufficient data and computing power, neural networks could achieve remarkable accuracy in image recognition tasks.\n:::\n\nDeep learning subsequently entered an era of extraordinary scale. By the late 2010s, companies like Google, Facebook, and OpenAI trained neural networks thousands of times larger than AlexNet. These massive models, often called \"foundation models\"[^fn-intro-foundation-models], expanded deep learning capabilities to new domains.\n\n[^fn-intro-foundation-models]: **Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundation\" for many different applications through fine-tuning, like GPT for language tasks or CLIP for vision tasks. The term was coined by Stanford's AI researchers in 2021 to capture how these models became the basis for building more specific AI systems. From a systems perspective, foundation models' size (10-100GB for inference, 350GB+ for training) creates deployment challenges—organizations must often choose between accuracy (deploying the full model requiring expensive GPU servers) and feasibility (using distilled versions that fit on less expensive hardware). This trade-off drives the emergence of model-as-a-service architectures where companies like OpenAI provide API access rather than distributing models, shifting infrastructure costs to centralized providers.\n\nGPT-3, released in 2020 [@brown2020language], marked a milestone in demonstrating what scale could achieve. With 175 billion parameters requiring approximately 350GB to store (800GB+ for full training infrastructure), it represented a 1,000x scale increase from earlier neural networks like BERT-Large[^fn-bert-large] (340 million parameters). Training consumed approximately 314 zettaFLOPs[^fn-zettaflops] of computation across 1,024 V100 GPUs[^fn-v100-gpus] over several weeks, with training costs estimated at $4.6 million. The model demonstrated remarkable emergent abilities that appeared only at scale: writing human-like text, engaging in sophisticated conversation, and writing functional computer code. Subsequent models have grown substantially larger, but the systems engineering challenges GPT-3 introduced, coordinating thousands of accelerators, managing massive memory requirements, and serving models at scale, remain representative of modern infrastructure needs.\n\n[^fn-bert-large]: **BERT-Large**: A transformer-based language model developed by Google in 2018 with 340 million parameters, representing the previous generation of large language models before the GPT era. BERT (Bidirectional Encoder Representations from Transformers) was revolutionary for understanding context in both directions of a sentence, but GPT-3's 175 billion parameters dwarfed it by over 500x, marking the transition to truly large-scale language models.\n\n[^fn-zettaflops]: **ZettaFLOPs**: A measure of computational performance equal to one sextillion (10^21) floating-point operations per second. Training GPT-3 required approximately 3.14 × 10^23 FLOPS (roughly 314 zettaFLOPs), which would theoretically take 355 years on a single V100 GPU. This massive computational requirement illustrates why modern AI training requires distributed systems with thousands of GPUs working in parallel.\n\n[^fn-v100-gpus]: **V100 GPUs**: NVIDIA's data center graphics processing units designed specifically for AI training, featuring 32GB of high-bandwidth memory (HBM2) and 125 TFLOPS of mixed-precision deep learning performance. Each V100 cost approximately $8,000-$10,000 (2020 pricing), making the 1,024 GPUs used for GPT-3 training worth roughly $8-10 million in hardware alone, highlighting the enormous infrastructure investment required for cutting-edge AI research.\n\nA key insight emerged: larger neural networks trained on more data became capable of solving increasingly complex tasks. This scale introduced significant systems challenges[^fn-training-challenges]. Efficiently training large models requires thousands of parallel GPUs, storing and serving models hundreds of gigabytes in size, and handling massive training datasets.\n\n[^fn-training-challenges]: **Large-Scale Training Challenges**: Training GPT-3 required approximately 3,640 petaflop-days. At $2-3 per GPU-hour on cloud platforms (2020 pricing), this translates to approximately $4.6M in compute costs alone (Lambda Labs estimate), excluding data preprocessing, experimentation, and failed training runs [@li2020estimating]. Rule of thumb: total project cost is typically 3-5x raw compute cost due to experimentation overhead, making the full GPT-3 development cost approximately $15-20M. Modern foundation models can consume 100+ terabytes of training data and require specialized distributed training techniques to coordinate thousands of accelerators across multiple data centers.\n\nThe 2012 deep learning revolution built upon neural network research dating to the 1950s. The story begins with Frank Rosenblatt's Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. Though limited to linearly separable problems, as Minsky and Papert's 1969 book \"Perceptrons\" [@minsky1969perceptrons] demonstrated, it introduced the core concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation [@rumelhart1986learning] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using specialized neural networks designed for image processing [@lecun1989backpropagation][^fn-cnn].\n\n[^fn-cnn]: **Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing images, inspired by how the human visual system works. The \"convolutional\" part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene. From a systems perspective, CNNs' parameter sharing reduces model size 10-100x compared to fully-connected networks processing the same images—a CNN might use 5-10 million parameters where a fully-connected network would need 500 million. This dramatic reduction makes CNNs deployable on mobile devices: MobileNetV2 achieves 70% ImageNet accuracy in just 14MB (3.5M parameters), enabling on-device image recognition that would be impossible with fully-connected networks requiring gigabytes of storage and memory.\n\nThese networks largely stagnated through the 1990s and 2000s not because the ideas were incorrect, but because they preceded necessary technological developments. The field lacked three important ingredients: sufficient data to train complex networks, enough computational power to process this data, and the technical innovations needed to train very deep networks effectively.\n\nDeep learning's potential required the convergence of the three AI Triad components we will explore: sufficient data to train complex networks, enough computational power to process this data, and algorithmic breakthroughs needed to train very deep networks effectively. This extended development period explains why the 2012 ImageNet breakthrough represented the culmination of accumulated research rather than a sudden revolution. This evolution established machine learning systems engineering as a discipline bridging theoretical advancements with practical implementation, operating within the interconnected framework the AI Triad represents.\n\n{{< margin-video \"https://www.youtube.com/watch?v=FwFduRA_L6Q&ab_channel=YannLeCun\" \"Convolutional Network Demo from 1989\" \"Yann LeCun\" >}}\n\nThis evolution reveals a crucial insight: as AI progressed from symbolic reasoning to statistical learning and deep learning, applications became increasingly ambitious and complex. However, this growth introduced challenges extending beyond algorithms, necessitating engineering entire systems capable of deploying and sustaining AI at scale. Understanding how these modern ML systems operate in practice requires examining their lifecycle characteristics and deployment patterns, which distinguish them fundamentally from traditional software systems.\n\n## Understanding ML System Lifecycle and Deployment {#sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0}\n\nHaving traced AI's evolution from symbolic systems through statistical learning to deep learning, we can now explore how these modern ML systems operate in practice. Understanding the ML lifecycle and deployment landscape is important because these factors shape every engineering decision we make.\n\n### The ML Development Lifecycle {#sec-introduction-ml-development-lifecycle-05d8}\n\nML systems fundamentally differ from traditional software in their development and operational lifecycle. Traditional software follows predictable patterns where developers write explicit instructions that execute deterministically[^fn-deterministic]. These systems build on decades of established practices: version control maintains precise code histories, continuous integration pipelines[^fn-ci-cd] automate testing, and static analysis tools measure quality. This mature infrastructure enables reliable software development following well-defined engineering principles.\n\n[^fn-deterministic]: **Deterministic Execution**: Traditional software produces the same output every time given the same input, like a calculator that always returns 4 when adding 2+2. This predictability makes testing straightforward—you can verify correct behavior by checking that specific inputs produce expected outputs. ML systems, by contrast, are probabilistic: the same model might produce slightly different predictions due to randomness in inference or changes in underlying data patterns.\n\n[^fn-ci-cd]: **Continuous Integration/Continuous Deployment (CI/CD)**: Automated systems that continuously test code changes and deploy them to production. When developers commit code, CI/CD pipelines automatically run tests, check for errors, and if everything passes, deploy the changes to users. For traditional software, this works reliably; for ML systems, it's more complex because you must also validate data quality, model performance, and prediction distribution—not just code correctness.\n\nMachine learning systems depart from this paradigm. While traditional systems execute explicit programming logic, ML systems derive their behavior from data patterns discovered through training. This shift from code to data as the primary behavior driver introduces complexities that existing software engineering practices cannot address. These challenges require specialized workflows that @sec-ai-workflow addresses.\n\n@fig-ml_lifecycle_overview illustrates how ML systems operate in continuous cycles rather than traditional software's linear progression from design through deployment.\n\n::: {#fig-ml_lifecycle_overview fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{\n  Box/.style={inner xsep=2pt,\n  draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    anchor=west,\n    text width=20mm,align=flush center,\n    minimum width=20mm, minimum height=8mm\n  },\n Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},\n  Text/.style={inner sep=4pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor!70,\n    font=\\fontsize{8pt}{9}\\selectfont\\usefont{T1}{phv}{m}{n},\n    align=flush center,\n    minimum width=7mm, minimum height=5mm\n  },\n}\n\n\\node[Box](B1){ Data\\\\ Preparation};\n\\node[Box,node distance=15mm,right=of B1,fill=RedL,draw=RedLine](B2){Model\\\\ Evaluation};\n\\node[Box,node distance=32mm,right=of B2,fill=VioletL,draw=VioletLine](B3){Model \\\\ Deployment};\n\\node[Box,node distance=9mm,above=of $(B1)!0.5!(B2)$,\nfill=BackColor!60!yellow!90,draw=BackLine](GB){Model\\\\ Training};\n\\node[Box,node distance=9mm,below left=1.1 and 0 of B1.south west,\nfill=BlueL,draw=BlueLine](DB1){Data\\\\ Collection};\n\\node[Box,node distance=9mm,below right=1.1 and 0 of B3.south east,\nfill=OrangeL,draw=OrangeLine](DB2){Model \\\\Monitoring};\n\\draw[Line](B2)--node[Text,pos=0.5]{Meets\\\\ Requirements}(B3);\n\\draw[Line](B2)--++(270:1.2)-|node[Text,pos=0.25]{Needs\\\\ Improvement}(B1);\n\\draw[Line](DB2)--node[Text,pos=0.25]{Performance\\\\Degrades}(DB1);\n\\draw[Line](DB1)|-(B1);\n\\draw[Line](B1)|-(GB);\n\\draw[Line](GB)-|(B2);\n\\draw[Line](B3)-|(DB2);\n\\end{tikzpicture}\n```\n**ML System Lifecycle**: Continuous iteration defines successful machine learning systems, requiring feedback loops to refine models and address performance degradation across data collection, model training, evaluation, and deployment. This cyclical process contrasts with traditional software development and emphasizes the importance of ongoing monitoring and adaptation to maintain system reliability and accuracy in dynamic environments.\n:::\n\nThe data-dependent nature of ML systems creates dynamic lifecycles requiring continuous monitoring and adaptation. Unlike source code that changes only through developer modifications, data reflects real-world dynamics. Distribution shifts can silently alter system behavior without any code changes. Traditional tools designed for deterministic code-based systems prove insufficient for managing such data-dependent systems: version control excels at tracking discrete code changes but struggles with large, evolving datasets; testing frameworks designed for deterministic outputs require adaptation for probabilistic predictions. These challenges require specialized practices: @sec-data-engineering addresses data versioning and quality management, while @sec-ml-operations covers monitoring approaches that handle probabilistic behaviors rather than deterministic outputs.\n\nIn production, lifecycle stages create either virtuous or vicious cycles. Virtuous cycles emerge when high-quality data enables effective learning, robust infrastructure supports efficient processing, and well-engineered systems facilitate better data collection. Vicious cycles occur when poor data quality undermines learning, inadequate infrastructure hampers processing, and system limitations prevent data collection improvements—with each problem compounding the others.\n\n### The Deployment Spectrum {#sec-introduction-deployment-spectrum-06a1}\n\nManaging machine learning systems' complexity varies across different deployment environments, each presenting unique constraints and opportunities that shape lifecycle decisions.\n\nAt one end of the spectrum, cloud-based ML systems run in massive data centers[^fn-data-centers]. These systems, including large language models and recommendation engines, process petabytes of data while serving millions of users simultaneously. They leverage virtually unlimited computing resources but manage enormous operational complexity and costs. The architectural approaches for building such large-scale systems are covered in @sec-ml-systems and @sec-ai-acceleration.\n\n[^fn-data-centers]: **Data Centers**: Massive facilities housing thousands of servers, often consuming 100-300 megawatts of power, equivalent to a small city. Google operates over 20 data centers globally, each one costing $1-2 billion to build. These facilities maintain temperatures of exactly 80°F (27°C) with backup power systems that can run for days, enabling the reliable operation of AI services used by billions of people worldwide.\n\nAt the other end, TinyML systems run on microcontrollers[^fn-microcontrollers] and embedded devices, performing ML tasks with severe memory, computing power, and energy consumption constraints. Smart home devices like Alexa or Google Assistant must recognize voice commands using less power than LED bulbs, while sensors must detect anomalies on battery power for months or years. The specialized techniques for deploying ML on such constrained devices are explored in @sec-efficient-ai and @sec-model-optimizations.\n\n[^fn-microcontrollers]: **Microcontrollers**: Tiny computers-on-a-chip costing under $1 each, with just kilobytes of memory, about 1/millionth the memory of a smartphone. Popular chips like the Arduino Uno have only 32KB of storage and 2KB of RAM, yet can run simple AI models that classify sensor data, recognize voice commands, or detect movement patterns while consuming less power than a digital watch.\n\nBetween these extremes lies a rich variety of ML systems adapted for different contexts. Edge ML systems bring computation closer to data sources, reducing latency[^fn-latency] and bandwidth requirements while managing local computing resources. Mobile ML systems must balance sophisticated capabilities with severe constraints: modern smartphones typically have 4-12GB RAM, ARM processors operating at 1.5-3 GHz, and power budgets of 2-5 watts that must be shared across all system functions. For example, running a state-of-the-art image classification model on a smartphone might consume 100-500mW and complete inference in 10-100ms, compared to cloud servers that can use 200+ watts but deliver results in under 1ms. Enterprise ML systems often operate within specific business constraints, focusing on particular tasks while integrating with existing infrastructure. Some organizations employ hybrid approaches, distributing ML capabilities across multiple tiers to balance various requirements.\n\n[^fn-latency]: **Latency**: The time delay between when a request is made and when a response is received. In ML systems, this is critical: autonomous vehicles need <10ms latency for safety decisions, while voice assistants target <100ms for natural conversation. For comparison, sending data to a distant cloud server typically adds 50-100ms, which is why edge computing became essential for real-time AI applications.\n\n### How Deployment Shapes the Lifecycle {#sec-introduction-deployment-shapes-lifecycle-3531}\n\nThe deployment spectrum we've outlined represents more than just different hardware configurations. Each deployment environment creates an interplay of requirements, constraints, and trade-offs that impact every stage of the ML lifecycle, from initial data collection through continuous operation and evolution.\n\nPerformance requirements often drive initial architectural decisions. Latency-sensitive applications, like autonomous vehicles or real-time fraud detection, might require edge or embedded architectures despite their resource constraints. Conversely, applications requiring massive computational power for training, such as large language models, naturally gravitate toward centralized cloud architectures. However, raw performance is just one consideration in a complex decision space.\n\nResource management varies dramatically across architectures and directly impacts lifecycle stages. Cloud systems must optimize for cost efficiency at scale, balancing expensive GPU clusters, storage systems, and network bandwidth. This affects training strategies (how often to retrain models), data retention policies (what historical data to keep), and serving architectures (how to distribute inference load). Edge systems face fixed resource limits that constrain model complexity and update frequency. Mobile and embedded systems operate under the strictest constraints, where every byte of memory and milliwatt of power matters, forcing aggressive model compression[^fn-model-compression] and careful scheduling of training updates.\n\n[^fn-model-compression]: **Model Compression**: Techniques for reducing a model's size and computational requirements while preserving accuracy. Common approaches include quantization (using 8-bit integers instead of 32-bit floats, reducing model size by 4x), pruning (removing connections with minimal impact, potentially achieving 90% sparsity), and knowledge distillation (training a small \"student\" model to mimic a large \"teacher\" model). These techniques can shrink a 500MB model to 50MB while losing only 1-2% accuracy, making deployment on smartphones and embedded devices feasible.\n\nOperational complexity increases with system distribution, creating cascading effects throughout the lifecycle. While centralized cloud architectures benefit from mature deployment tools and managed services, edge and hybrid systems must handle distributed system management complexity. This manifests across all lifecycle stages: data collection requires coordination across distributed sensors with varying connectivity; version control must track models deployed across thousands of edge devices; evaluation needs to account for varying hardware capabilities; deployment must handle staged rollouts with rollback capabilities; and monitoring must aggregate signals from geographically distributed systems. The systematic approaches to operational excellence, including incident response and debugging methodologies for production ML systems, are thoroughly addressed in @sec-ml-operations.\n\nData considerations introduce competing pressures that reshape lifecycle workflows. Privacy requirements or data sovereignty regulations might push toward edge or embedded architectures where data stays local, fundamentally changing data collection and training strategies—perhaps requiring federated learning[^fn-federated-learning] approaches where models train on distributed data without centralization. Yet the need for large-scale training data might favor cloud approaches with centralized data aggregation. The velocity and volume of data also influence architectural choices: real-time sensor data might require edge processing to manage bandwidth during collection, while batch analytics might be better suited to cloud processing with periodic model updates.\n\n[^fn-federated-learning]: **Federated Learning**: A training approach where the model learns from data distributed across many devices without centralizing the data. For example, your smartphone's keyboard learns your typing patterns locally and only shares model updates (not your actual messages) with the cloud. This technique, pioneered by Google in 2016, enables privacy-preserving ML by keeping sensitive data on-device while still benefiting from collective learning across millions of users.\n\nEvolution and maintenance requirements must be considered from the initial design. Cloud architectures offer flexibility for system evolution with easy model updates and A/B testing[^fn-ab-testing], but can incur significant ongoing costs. Edge and embedded systems might be harder to update (requiring over-the-air updates[^fn-ota-updates] with careful bandwidth management), but could offer lower operational overhead. The continuous cycle of ML systems—collect data, train models, evaluate performance, deploy updates, monitor behavior—becomes particularly challenging in distributed architectures, where updating models and maintaining system health requires careful orchestration across multiple tiers.\n\n[^fn-ab-testing]: **A/B Testing**: A method of comparing two versions of a system by showing version A to some users and version B to others, then measuring which performs better. In ML systems, this might mean deploying a new model to 5% of users while keeping 95% on the old model, comparing metrics like accuracy or user engagement before fully rolling out the new version. This gradual rollout strategy helps catch problems before they affect all users.\n\n[^fn-ota-updates]: **Over-the-Air (OTA) Updates**: Wireless software updates delivered remotely to devices, like how your smartphone installs new apps without physical connection. For ML systems on embedded devices or vehicles, OTA updates enable deploying improved models to thousands or millions of devices without manual intervention. However, updating a 500MB neural network over cellular networks to a fleet of vehicles requires careful bandwidth management and rollback capabilities if updates fail.\n\nThese trade-offs are rarely simple binary choices. Modern ML systems often adopt hybrid approaches, balancing these considerations based on specific use cases and constraints. For instance, an autonomous vehicle might perform real-time perception and control at the edge for latency reasons, while uploading data to the cloud for model improvement and downloading updated models periodically. A voice assistant might do wake-word detection on-device to preserve privacy and reduce latency, but send full speech to the cloud for complex natural language processing.\n\nThe key insight is understanding how deployment decisions ripple through the entire system lifecycle. A choice to deploy on embedded devices doesn't just constrain model size, it affects data collection strategies (what sensors are feasible), training approaches (whether to use federated learning), evaluation metrics (accuracy vs. latency vs. power), deployment mechanisms (over-the-air updates), and monitoring capabilities (what telemetry can be collected). These interconnected decisions demonstrate the AI Triad framework in practice, where constraints in one component create cascading effects throughout the system.\n\nWith this understanding of how ML systems operate across their lifecycle and deployment spectrum, we can now examine concrete examples that illustrate these principles in action. The case studies that follow demonstrate how different deployment choices create distinct engineering challenges and solutions across the system lifecycle.\n\n## Case Studies in Real-World ML Systems {#sec-introduction-case-studies-realworld-ml-systems-a2ba}\n\nHaving established the AI Triad framework, lifecycle stages, and deployment spectrum, we can now examine these principles operating in real-world systems. Rather than surveying multiple systems superficially, we focus on one representative case study, autonomous vehicles, that illustrates the spectrum of ML systems engineering challenges across all three components, multiple lifecycle stages, and complex deployment constraints.\n\n### Case Study: Autonomous Vehicles {#sec-introduction-case-study-autonomous-vehicles-6f86}\n\n[Waymo](https://waymo.com/), a subsidiary of Alphabet Inc., stands at the forefront of autonomous vehicle technology, representing one of the most ambitious applications of machine learning systems to date. Evolving from the Google Self-Driving Car Project initiated in 2009, Waymo's approach to autonomous driving exemplifies how ML systems can span the entire spectrum from embedded systems to cloud infrastructure. This case study demonstrates the practical implementation of complex ML systems in a safety-critical, real-world environment, integrating real-time decision-making with long-term learning and adaptation.\n\n#### Data Considerations {#sec-introduction-data-considerations-bdc4}\n\nThe data ecosystem underpinning Waymo's technology is vast and dynamic. Each vehicle serves as a roving data center, its sensor suite, which comprises LiDAR[^fn-lidar], radar[^fn-radar], and high-resolution cameras, generating approximately one terabyte of data per hour of driving. This real-world data is complemented by an even more extensive simulated dataset, with Waymo's vehicles having traversed over 20 billion miles in simulation and more than 20 million miles on public roads. The challenge lies not just in the volume of data, but in its heterogeneity and the need for real-time processing. Waymo must handle both structured (e.g., GPS coordinates) and unstructured data (e.g., camera images) simultaneously. The data pipeline spans from edge processing on the vehicle itself to massive cloud-based storage and processing systems. Sophisticated data cleaning and validation processes are necessary, given the safety-critical nature of the application. The representation of the vehicle's environment in a form amenable to machine learning presents significant challenges, requiring complex preprocessing to convert raw sensor data into meaningful features that capture the dynamics of traffic scenarios.\n\n[^fn-lidar]: **LiDAR (Light Detection and Ranging)**: A sensor that uses laser pulses to measure distances, creating detailed 3D maps of surroundings by measuring how long light takes to bounce back from objects. A spinning LiDAR sensor might emit millions of laser pulses per second, detecting objects up to 200+ meters away with centimeter-level precision. While highly accurate, LiDAR sensors can cost $75,000+ (though prices are dropping) and struggle in heavy rain or fog where water droplets scatter the laser light.\n\n[^fn-radar]: **Radar (Radio Detection and Ranging)**: A sensor that uses radio waves to detect objects and measure their distance and velocity. Unlike LiDAR, radar works well in rain, fog, and darkness, making it essential for all-weather autonomous driving. Automotive radar operates at 77 GHz frequency, detecting vehicles up to 250 meters away and measuring their speed with high accuracy—critical for safely navigating highways. Modern vehicles use multiple radar units costing $150-300 each.\n\n#### Algorithmic Considerations {#sec-introduction-algorithmic-considerations-b99f}\n\nWaymo's ML stack represents a sophisticated ensemble of algorithms tailored to the multifaceted challenge of autonomous driving. The perception system employs specialized neural networks to process visual data for object detection and tracking. Prediction models, needed for anticipating the behavior of other road users, use neural networks that can understand patterns over time[^fn-rnn] in road user behavior. Building such complex multi-model systems requires the architectural patterns from @sec-dnn-architectures and the framework infrastructure covered in @sec-ai-frameworks. Waymo has developed custom ML models like VectorNet for predicting vehicle trajectories. The planning and decision-making systems may incorporate learning-from-experience techniques to handle complex traffic scenarios.\n\n[^fn-rnn]: **Sequential Neural Networks**: Neural network architectures designed to process data that occurs in sequences over time, such as predicting where a pedestrian will move next based on their previous movements. These networks maintain a form of \"memory\" of previous inputs to inform current decisions.\n\n#### Infrastructure Considerations {#sec-introduction-infrastructure-considerations-248a}\n\nThe computing infrastructure supporting Waymo's autonomous vehicles epitomizes the challenges of deploying ML systems across the full spectrum from edge to cloud. Each vehicle is equipped with a custom-designed compute platform capable of processing sensor data and making decisions in real-time, often leveraging specialized hardware like GPUs or tensor processing units (TPUs)[^fn-tpu]. This edge computing is complemented by extensive use of cloud infrastructure, leveraging the power of Google's data centers for training models, running large-scale simulations, and performing fleet-wide learning. Such systems demand specialized hardware architectures (@sec-ai-acceleration) and edge-cloud coordination strategies (@sec-ml-systems) to handle real-time processing at scale. The connectivity between these tiers is critical, with vehicles requiring reliable, high-bandwidth communication for real-time updates and data uploading. Waymo's infrastructure must be designed for robustness and fault tolerance, ensuring safe operation even in the face of hardware failures or network disruptions. The scale of Waymo's operation presents significant challenges in data management, model deployment, and system monitoring across a geographically distributed fleet of vehicles.\n\n[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom AI accelerator chip designed specifically for neural network operations, named after \"tensors\" (multi-dimensional arrays used in deep learning). First revealed in 2016, TPUs can perform matrix multiplications up to 15-30x faster than contemporary GPUs for AI workloads while using less power. A single TPU v4 pod can provide 1.1 exaflops of computing power—roughly equivalent to 10,000 high-end GPUs—enabling training of massive language models in days rather than months.\n\n#### Future Implications {#sec-introduction-future-implications-c2f2}\n\nWaymo's impact extends beyond technological advancement, potentially revolutionizing transportation, urban planning, and numerous aspects of daily life. The launch of Waymo One, a commercial ride-hailing service using autonomous vehicles in Phoenix, Arizona, represents a significant milestone in the practical deployment of AI systems in safety-critical applications. Waymo's progress has broader implications for the development of robust, real-world AI systems, driving innovations in sensor technology, edge computing, and AI safety that have applications far beyond the automotive industry. However, it also raises important questions about liability, ethics, and the interaction between AI systems and human society. As Waymo continues to expand its operations and explore applications in trucking and last-mile delivery, it serves as an important test bed for advanced ML systems, driving progress in areas such as continual learning, robust perception, and human-AI interaction. The Waymo case study underscores both the tremendous potential of ML systems to transform industries and the complex challenges involved in deploying AI in the real world.\n\n### Contrasting Deployment Scenarios {#sec-introduction-contrasting-deployment-scenarios-653a}\n\nWhile Waymo illustrates the full complexity of hybrid edge-cloud ML systems, other deployment scenarios present different constraint profiles. [FarmBeats](https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/), a Microsoft Research project for agricultural IoT, operates at the opposite end of the spectrum—severely resource-constrained edge deployments in remote locations with limited connectivity. FarmBeats demonstrates how ML systems engineering adapts to constraints: simpler models that can run on low-power microcontrollers, innovative connectivity solutions using TV white spaces, and local processing that minimizes data transmission. The challenges include maintaining sensor reliability in harsh conditions, validating data quality with limited human oversight, and updating models on devices that may be offline for extended periods.\n\nConversely, [AlphaFold](https://deepmind.google/technologies/alphafold/) [@jumper2021highly] represents purely cloud-based scientific ML where computational resources are essentially unlimited but accuracy is paramount. AlphaFold's protein structure prediction required training on 128 TPUv3 cores for weeks, processing hundreds of millions of protein sequences from multiple databases. The systems challenges differ markedly from Waymo or FarmBeats: managing massive training datasets (the Protein Data Bank contains over 180,000 structures), coordinating distributed training across specialized hardware, and validating predictions against experimental ground truth. Unlike Waymo's latency constraints or FarmBeats' power constraints, AlphaFold prioritizes computational throughput to explore vast search spaces—training costs exceeded $100,000 but enabled scientific breakthroughs.\n\nThese three systems—Waymo (hybrid, latency-critical), FarmBeats (edge, resource-constrained), and AlphaFold (cloud, compute-intensive)—illustrate how deployment environment shapes every engineering decision. The fundamental three-component framework applies to all, but the specific constraints and optimization priorities differ dramatically. Understanding this deployment diversity is essential for ML systems engineers, as the same algorithmic insight may require entirely different system implementations depending on operational context.\n\nWith concrete examples established, we can now examine the challenges that emerge across different deployment scenarios and lifecycle stages.\n\n\\medskip\n## Core Engineering Challenges in ML Systems {#sec-introduction-core-engineering-challenges-ml-systems-6482}\n\nThe Waymo case study and comparative deployment scenarios reveal how the AI Triad framework creates interdependent challenges across data, algorithms, and infrastructure. We've already established how ML systems differ from traditional software in their failure patterns and performance degradation. Now we can examine the specific challenge categories that emerge from this difference.\n\n### Data Challenges {#sec-introduction-data-challenges-2b0d}\n\nThe foundation of any ML system is its data, and managing this data introduces several core challenges that can silently degrade system performance. Data quality emerges as the primary concern: real-world data is often messy, incomplete, and inconsistent. Waymo's sensor suite must contend with environmental interference (rain obscuring cameras, LiDAR reflections from wet surfaces), sensor degradation over time, and data synchronization across multiple sensors capturing information at different rates. Unlike traditional software where input validation can catch malformed data, ML systems must handle ambiguity and uncertainty inherent in real-world observations.\n\nScale represents another critical dimension. Waymo generates approximately one terabyte per vehicle per hour—managing this data volume requires sophisticated infrastructure for collection, storage, processing, and efficient access during training. The challenge isn't just storing petabytes of data, but maintaining data quality metadata, version control for datasets, and efficient retrieval for model training. As systems scale to thousands of vehicles across multiple cities, these data management challenges compound exponentially.\n\nPerhaps most serious is data drift[^fn-drift], the gradual change in data patterns over time that silently degrades model performance. Waymo's models encounter new traffic patterns, road configurations, weather conditions, and driving behaviors that weren't present in training data. A model trained primarily on Phoenix driving might perform poorly when deployed in New York due to distribution shift: denser traffic, more aggressive drivers, different road layouts. Unlike traditional software where specifications remain constant, ML systems must adapt as the world they model evolves.\n\n[^fn-drift]: **Data Drift**: The gradual change in the statistical properties of input data over time, which can degrade model performance if not properly monitored and addressed through retraining or model updates.\n\nThis adaptation requirement introduces an important constraint that is often overlooked. While ML systems can generalize to unseen situations through learned statistical patterns, once trained, the model's learned behavior becomes fixed. The model cannot modify its understanding during deployment; it can only apply the patterns it learned during training. When distribution shift occurs, the model follows these outdated learned patterns just as deterministic code follows outdated rules. If construction zones triple in frequency, or new vehicle types appear regularly, the model's fixed responses may prove no more appropriate than hardcoded logic written for a different operational context. The advantage of ML emerges not from runtime adaptation but from the capacity to retrain with new data, a process requiring deliberate engineering intervention.\n\nDistribution shift manifests through multiple pathways. Seasonal variations affect sensor performance through changing sun angles and precipitation patterns. Infrastructure modifications alter road layouts. Urban growth evolves traffic patterns. Each shift can degrade specific model components: pedestrian detection accuracy may decline in winter conditions, while lane following confidence may decrease on newly repaved roads. Detecting these shifts requires continuous monitoring of input distributions and model performance across operational contexts.\n\nThe systematic approaches to managing these data challenges (quality assurance, versioning, drift detection, and remediation strategies) are covered in @sec-data-engineering. The key insight is that data challenges in ML systems are continuous and dynamic, requiring ongoing engineering attention rather than one-time solutions.\n\n### Model Challenges {#sec-introduction-model-challenges-eef4}\n\nCreating and maintaining the ML models themselves presents another set of challenges. Modern ML models, particularly in deep learning, can be complex. Consider a language model like GPT-3, which has hundreds of billions of parameters that need to be optimized through training processes[^fn-backprop]. This complexity creates practical challenges: these models require enormous computing power to train and run, making it difficult to deploy them in situations with limited resources, like on mobile phones or IoT devices.\n\n[^fn-backprop]: **Backpropagation**: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers.\n\nTraining these models effectively is itself a significant challenge. Unlike traditional programming where we write explicit instructions, ML models learn from examples. This learning process involves many architectural and hyperparameter choices: How should we structure the model? How long should we train it? How can we tell if it's learning the right patterns rather than memorizing training data? Making these decisions often requires both technical expertise and considerable trial and error.\n\nModern practice increasingly relies on transfer learning—reusing models developed for one task as starting points for related tasks. Rather than training a new image recognition model from scratch, practitioners might start with a model pre-trained on millions of images and adapt it to their specific domain (say, medical imaging or agricultural monitoring). This approach dramatically reduces both the training data and computation required, but introduces new challenges around ensuring the pre-trained model's biases don't transfer to the new application. These training challenges—transfer learning, distributed training, and bias mitigation—require systematic approaches that @sec-ai-training explores, building on the framework infrastructure from @sec-ai-frameworks.\n\nA particularly important challenge is ensuring that models work well in real-world conditions beyond their training data. This generalization gap, the difference between training performance and real-world performance, represents a central challenge in machine learning. A model might achieve 99% accuracy on its training data but only 75% accuracy in production due to subtle distribution differences. For important applications like autonomous vehicles or medical diagnosis systems, understanding and minimizing this gap becomes necessary for safe deployment.\n\n### System Challenges {#sec-introduction-system-challenges-0dc0}\n\nGetting ML systems to work reliably in the real world introduces its own set of challenges. Unlike traditional software that follows fixed rules, ML systems need to handle uncertainty and variability in their inputs and outputs. They also typically need both training systems (for learning from data) and serving systems (for making predictions), each with different requirements and constraints.\n\nConsider a company building a speech recognition system. They need infrastructure to collect and store audio data, systems to train models on this data, and then separate systems to actually process users' speech in real-time. Each part of this pipeline needs to work reliably and efficiently, and all the parts need to work together seamlessly. The engineering principles for building such robust data pipelines are covered in @sec-data-engineering, while the operational practices for maintaining these systems in production are explored in @sec-ml-operations.\n\nThese systems also need constant monitoring and updating. How do we know if the system is working correctly? How do we update models without interrupting service? How do we handle errors or unexpected inputs? These operational challenges become particularly complex when ML systems are serving millions of users.\n\n### Ethical Considerations {#sec-introduction-ethical-considerations-d6a5}\n\nAs ML systems become more prevalent in our daily lives, their broader impacts on society become increasingly important to consider. One major concern is fairness, as ML systems can sometimes learn to make decisions that discriminate against certain groups of people. This often happens unintentionally, as the systems pick up biases present in their training data. For example, a job application screening system might inadvertently learn to favor certain demographics if those groups were historically more likely to be hired. Detecting and mitigating such biases requires careful auditing of both training data and model behavior across different demographic groups.\n\nAnother important consideration is transparency and interpretability. Many modern ML models, particularly deep learning models with millions or billions of parameters, function as black boxes—systems where we can observe inputs and outputs but struggle to understand the internal reasoning. Like a radio that receives signals and produces sound without most users understanding the electronics inside, these models make predictions through complex mathematical transformations that resist human interpretation. A deep neural network might correctly diagnose a medical condition from an X-ray, but explaining why it reached that diagnosis—which visual features it considered most important—remains challenging. This opacity becomes particularly problematic when ML systems make consequential decisions affecting people's lives in domains like healthcare, criminal justice, or financial services, where stakeholders reasonably expect explanations for decisions that impact them.\n\nPrivacy is also a major concern. ML systems often need large amounts of data to work effectively, but this data might contain sensitive personal information. How do we balance the need for data with the need to protect individual privacy? How do we ensure that models do not inadvertently memorize and reveal private information through inference attacks[^fn-inference]? These challenges are not merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment. Addressing these concerns requires integrated approaches: fairness and bias detection, privacy-preserving techniques, inference attack mitigation, and system resilience under adversarial conditions.\n\n[^fn-inference]: **Inference Attack**: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training.\n\n### Understanding Challenge Interconnections {#sec-introduction-understanding-challenge-interconnections-3d30}\n\nAs the Waymo case study illustrates, challenges cascade and compound across the AI Triad. Data quality issues (sensor noise, distribution shift) degrade model performance. Model complexity constraints (latency budgets, power limits) force architectural compromises that may affect fairness (simpler models might show more bias). System-level failures (over-the-air update problems) can prevent deployment of improved models that address ethical concerns.\n\nThis interdependency explains why ML systems engineering requires holistic thinking that considers the AI Triad components together rather than optimizing them independently. A decision to use a larger model for better accuracy creates ripple effects: more training data required, longer training times, higher serving costs, increased latency, and potentially more pronounced biases if the training data isn't carefully curated. Successfully navigating these trade-offs requires understanding how choices in one dimension affect others.\n\nThe challenge landscape also explains why many research models fail to reach production. Academic ML often focuses on maximizing accuracy on benchmark datasets, potentially ignoring practical constraints like inference latency, training costs, data privacy, or operational monitoring. Production ML systems must balance accuracy against deployment feasibility, operational costs, ethical considerations, and long-term maintainability. This gap between research priorities and production realities motivates this book's emphasis on systems engineering rather than pure algorithmic innovation.\n\nThese interconnected challenges, spanning data quality and model complexity to infrastructure scalability and ethical considerations, distinguish ML systems from traditional software engineering. The transition from algorithmic innovation to systems integration challenges, combined with the unique operational characteristics we've examined, establishes the need for a distinct engineering discipline. We call this emerging field AI Engineering.\n\n## Defining AI Engineering {#sec-introduction-defining-ai-engineering-b812}\n\nHaving explored the historical evolution, lifecycle characteristics, practical applications, and core challenges of machine learning systems, we can now formally establish the discipline that addresses these systems-level concerns.\n\n::: {.callout-definition title=\"AI Engineering\"}\n***AI Engineering*** is the engineering discipline focused on the _systems-level integration_ of machine learning _algorithms_, _data_, and _computational infrastructure_ to build and operate production systems that are _reliable_, _efficient_, and _scalable_.\n:::\n\nAs we've traced through AI's history, a fundamental transformation has occurred. While AI once encompassed symbolic reasoning, expert systems, and rule-based approaches, learning-based methods now dominate the field. When organizations build AI today, they build machine learning systems. Netflix's recommendation engine processes billions of viewing events to train models serving millions of subscribers. Waymo's autonomous vehicles run dozens of neural networks processing sensor data in real time. Training GPT-4 required coordinating thousands of GPUs across data centers, consuming megawatts of power. Modern AI is overwhelmingly machine learning: systems whose capabilities emerge from learning patterns in data.\n\nThis convergence makes \"AI Engineering\" the natural name for the discipline, even though this text focuses specifically on machine learning systems as its subject matter. The term reflects how AI is actually built and deployed in practice today.\n\nAI Engineering encompasses the complete lifecycle of building production intelligent systems. A breakthrough algorithm requires efficient data collection and processing, distributed computation across hundreds or thousands of machines, reliable service to users with strict latency requirements, and continuous monitoring and updating based on real-world performance. The discipline addresses fundamental challenges at every level: designing efficient algorithms for specialized hardware, optimizing data pipelines that process petabytes daily, implementing distributed training across thousands of GPUs, deploying models that serve millions of concurrent users, and maintaining systems whose behavior evolves as data distributions shift. Energy efficiency is not an afterthought but a first-class constraint alongside accuracy and latency. The physics of memory bandwidth limitations, the breakdown of Dennard scaling, and the energy costs of data movement shape every architectural decision from chip design to data center deployment.\n\nThis emergence of AI Engineering as a distinct discipline mirrors how Computer Engineering emerged in the late 1960s and early 1970s.[^fn-computer-engineering] As computing systems grew more complex, neither Electrical Engineering nor Computer Science alone could address the integrated challenges of building reliable computers. Computer Engineering emerged as a complete discipline bridging both fields. Today, AI Engineering faces similar challenges at the intersection of algorithms, infrastructure, and operational practices. While Computer Science advances machine learning algorithms and Electrical Engineering develops specialized AI hardware, neither discipline fully encompasses the systems-level integration, deployment strategies, and operational practices required to build production AI systems at scale.\n\n[^fn-computer-engineering]: The first accredited computer engineering degree program in the United States was established at Case Western Reserve University in 1971, marking the formalization of Computer Engineering as a distinct academic discipline.\n\nWith AI Engineering now formally defined as the discipline, the remainder of this text discusses the practice of building and operating machine learning systems. We use \"ML systems engineering\" throughout to describe this practice, the work of designing, deploying, and maintaining the machine learning systems that constitute modern AI. These terms refer to the same discipline: AI Engineering is what we call it, ML systems engineering is what we do.\n\nHaving established AI Engineering as a discipline, we can now organize its practice into a coherent framework that addresses the challenges we've identified systematically.\n\n## Organizing ML Systems Engineering: The Five-Pillar Framework {#sec-introduction-organizing-ml-systems-engineering-fivepillar-framework-524d}\n\nThe challenges we've explored, from silent performance degradation and data drift to model complexity and ethical concerns, reveal why ML systems engineering has emerged as a distinct discipline. The unique failure patterns we discussed earlier exemplify the need for specialized approaches: traditional software engineering practices cannot address systems that degrade quietly rather than failing obviously. These challenges cannot be addressed through algorithmic innovation alone; they require systematic engineering practices that span the entire system lifecycle from initial data collection through continuous operation and evolution.\n\nThis work organizes ML systems engineering around five interconnected disciplines that directly address the challenge categories we have identified. These pillars, illustrated in @fig-pillars, represent the core engineering capabilities required to bridge the gap between research prototypes and production systems capable of operating reliably at scale.\n\n![**ML System Lifecycle**: Machine learning systems engineering encompasses five interconnected disciplines that address the real-world challenges of building, deploying, and maintaining AI systems at scale. Each pillar represents critical engineering capabilities needed to bridge the gap between research prototypes and production systems.](images/png/book_pillars.png){#fig-pillars}\n\n### The Five Engineering Disciplines {#sec-introduction-five-engineering-disciplines-6eee}\n\nThe five-pillar framework shown in @fig-pillars emerged directly from the systems challenges that distinguish ML from traditional software. Each pillar addresses specific challenge categories while recognizing their interdependencies.\n\nAlternative organizational frameworks exist. One could organize by system component (data, model, infrastructure) or by lifecycle phase (development, deployment, operation). We chose the five-pillar structure because it aligns with how engineering teams are typically organized in industry, with specialized roles for data engineering, training infrastructure, deployment, operations, and responsible AI practices. Notably, the Ethics pillar ensures that responsible engineering is treated as an explicit discipline rather than distributed implicitly across other areas, where it might be overlooked under deadline pressure.\n\n**Data Engineering** (@sec-data-engineering) addresses the data-related challenges we identified: quality assurance, scale management, drift detection, and distribution shift. This pillar encompasses building robust data pipelines that ensure quality, handle massive scale, maintain privacy, and provide the infrastructure upon which all ML systems depend. For systems like Waymo, this means managing terabytes of sensor data per vehicle, validating data quality in real-time, detecting distribution shifts across different cities and weather conditions, and maintaining data lineage for debugging and compliance. The techniques covered include data versioning, quality monitoring, drift detection algorithms, and privacy-preserving data processing.\n\n**Training Systems** (@sec-ai-training) tackles the model-related challenges around complexity and scale. This pillar covers developing training systems that can manage large datasets and complex models while optimizing computational resource utilization across distributed environments. Modern foundation models require coordinating thousands of GPUs, implementing parallelization strategies, managing training failures and restarts, and balancing training costs against model quality. The chapter explores distributed training architectures, optimization algorithms, hyperparameter tuning at scale, and the frameworks that make large-scale training practical.\n\n**Deployment Infrastructure** (@sec-ml-operations) addresses system-related challenges around the training-serving divide and operational complexity. This pillar encompasses building reliable deployment infrastructure that can serve models at scale, handle failures gracefully, and adapt to evolving requirements in production environments. Deployment spans the full spectrum from cloud services handling millions of requests per second to edge devices operating under severe latency and power constraints. The techniques include model serving architectures, edge deployment optimization, A/B testing frameworks, and staged rollout strategies that minimize risk while enabling rapid iteration.\n\n**Operations and Monitoring** (@sec-ml-operations, @sec-benchmarking-ai) directly addresses the silent performance degradation patterns we identified as distinctive to ML systems. This pillar covers creating monitoring and maintenance systems that ensure continued performance, enable early issue detection, and support safe system updates in production. Unlike traditional software monitoring focused on infrastructure metrics, ML operations requires the four-dimensional monitoring we discussed: infrastructure health, model performance, data quality, and business impact. The chapter explores metrics design, alerting strategies, incident response procedures, debugging techniques for production ML systems, and continuous evaluation approaches that catch degradation before it impacts users.\n\n**Ethics and Governance** addresses the ethical and societal challenges around fairness, transparency, privacy, and safety. This pillar implements responsible AI practices throughout the system lifecycle rather than treating ethics as an afterthought. For safety-critical systems like autonomous vehicles, this includes formal verification methods, scenario-based testing, bias detection and mitigation, privacy-preserving learning techniques, and explainability approaches that support debugging and certification. The relevant chapters cover both technical methods (differential privacy, fairness metrics, interpretability techniques) and organizational practices (ethics review boards, incident response protocols, stakeholder engagement).\n\n### Connecting Components, Lifecycle, and Disciplines {#sec-introduction-connecting-components-lifecycle-disciplines-388b}\n\nThe five pillars emerge naturally from the AI Triad framework and lifecycle stages we established earlier. Each AI Triad component maps to specific pillars: Data Engineering handles the data component's full lifecycle; Training Systems and Deployment Infrastructure address how algorithms interact with infrastructure during different lifecycle phases; Operations bridges all components by monitoring their interactions; Ethics & Governance cuts across all components, ensuring responsible practices throughout.\n\nThe challenge categories we identified find their solutions within specific pillars: Data challenges → Data Engineering. Model challenges → Training Systems. System challenges → Deployment Infrastructure and Operations. Ethical challenges → Ethics & Governance. As we established with the AI Triad framework, these pillars must coordinate rather than operate in isolation.\n\nThis structure reflects how AI evolved from algorithm-centric research to systems-centric engineering, shifting focus from \"can we make this algorithm work?\" to \"can we build systems that reliably deploy, operate, and maintain these algorithms at scale?\" The five pillars represent the engineering capabilities required to answer \"yes.\"\n\n### Future Directions in ML Systems Engineering {#sec-introduction-future-directions-ml-systems-engineering-db3b}\n\nWhile these five pillars provide a stable framework for ML systems engineering, the field continues evolving. Understanding current trends helps anticipate how the core challenges and trade-offs will manifest in future systems.\n\nApplication-level innovation increasingly features agentic systems that move beyond reactive prediction to autonomous action. Systems that can plan, reason, and execute complex tasks introduce new requirements for decision-making frameworks and safety constraints. These advances don't eliminate the five pillars but increase their importance: autonomous systems that can take consequential actions require even more rigorous data quality, more reliable deployment infrastructure, more comprehensive monitoring, and stronger ethical safeguards.\n\nSystem architecture evolution addresses sustainability and efficiency concerns that have become critical as models scale. Innovation in model compression, efficient training techniques, and specialized hardware stems from both environmental and economic pressures. Future architectures must balance the pursuit of more powerful models against growing resource constraints. These efficiency innovations primarily impact Training Systems and Deployment Infrastructure pillars, introducing new techniques like quantization, pruning, and neural architecture search that optimize for multiple objectives simultaneously.\n\nInfrastructure advances continue reshaping deployment possibilities. Specialized AI accelerators are emerging across the spectrum from powerful data center chips to efficient edge processors. This heterogeneous computing landscape enables dynamic model distribution across tiers based on capabilities and conditions, blurring traditional boundaries between cloud, edge, and embedded systems. These infrastructure innovations affect how all five pillars operate—new hardware enables new algorithms, which require new training approaches, which demand new monitoring strategies.\n\nDemocratization of AI technology is making ML systems more accessible to developers and organizations of all sizes. Cloud providers offer pre-trained models and automated ML platforms that reduce the expertise barrier for deploying AI solutions. This accessibility trend doesn't diminish the importance of systems engineering—if anything, it increases demand for robust, reliable systems that can operate without constant expert oversight. The five pillars become even more critical as ML systems proliferate into domains beyond traditional tech companies.\n\nThese trends share a common theme: they create ML systems that are more capable and widespread, but also more complex to engineer reliably. The five-pillar framework provides the foundation for navigating this landscape, though specific techniques within each pillar will continue advancing.\n\n### The Nature of Systems Knowledge {#sec-introduction-nature-systems-knowledge-1c67}\n\nMachine learning systems engineering differs epistemologically from purely theoretical computer science disciplines. While fields like algorithms, complexity theory, or formal verification build knowledge through mathematical proofs and rigorous derivations, ML systems engineering is a practice, a craft learned through building, deploying, and maintaining systems at scale. This distinction becomes apparent in topics like MLOps, where you'll encounter fewer theorems and more battle-tested patterns that have emerged from production experience. The knowledge here isn't about proving optimal solutions exist but about recognizing which approaches work reliably under real-world constraints.\n\nThis practical orientation reflects ML systems engineering's nature as a systems discipline. Like other engineering fields—civil, electrical, mechanical—the core challenge lies in managing complexity and trade-offs rather than deriving closed-form solutions. You'll learn to reason about latency versus accuracy trade-offs, to recognize when data quality issues will undermine even sophisticated models, to anticipate how infrastructure choices propagate through entire system architectures. This systems thinking develops through experience with concrete scenarios, debugging production failures, and understanding why certain design patterns persist across different applications.\n\nThe implication for learning is significant: mastery comes through building intuition about patterns, understanding trade-off spaces, and recognizing how different system components interact. When you read about monitoring strategies or deployment architectures, the goal isn't memorizing specific configurations but developing judgment about which approaches suit which contexts. This book provides the frameworks, principles, and representative examples, but expertise ultimately develops through applying these concepts to real problems, making mistakes, and building the pattern recognition that distinguishes experienced systems engineers from those who only understand individual components.\n\n## The Structure of This Textbook {#sec-introduction-structure}\n\nThis textbook organizes around three imperatives: build, optimize, and deploy. The structure progresses from foundational concepts through model development to production deployment, following a key pedagogical principle of establishing context and process before theory. @tbl-vol1-structure summarizes the four-part organization.\n\n+--------------------+--------------------------------+------------------------------------------+\n| **Part**           | **Theme**                      | **Key Chapters**                         |\n+:===================+:===============================+:=========================================+\n| **I: Foundations** | Context: ML systems landscape  | Introduction, ML Systems,                |\n|                    |                                | Workflow, Data Engineering               |\n+--------------------+--------------------------------+------------------------------------------+\n| **II: Build**      | Theory: Model fundamentals     | Deep Learning Primer, DNN Architectures, |\n|                    |                                | Frameworks, Training                     |\n+--------------------+--------------------------------+------------------------------------------+\n| **III: Optimize**  | Efficiency: Performance tuning | Efficient AI, Optimizations,             |\n|                    |                                | Hardware Acceleration, Benchmarking      |\n+--------------------+--------------------------------+------------------------------------------+\n| **IV: Deploy**     | Production: Real-world systems | Serving, ML Operations,                  |\n|                    |                                | Responsible Engineering, Conclusion      |\n+--------------------+--------------------------------+------------------------------------------+\n\n: **Volume I Structure**: The four parts progress from understanding the ML landscape through building and optimizing models to deploying production systems. {#tbl-vol1-structure}\n\nPart I establishes context by surveying the ML systems landscape. The Introduction develops the engineering revolution in AI and the frameworks that organize this discipline. ML Systems examines what distinguishes ML systems from traditional software, introducing unique failure patterns and lifecycle stages. Workflow presents the end-to-end process from problem formulation through deployment, providing the conceptual map that guides subsequent learning. Data Engineering addresses data collection, processing, and management, establishing that data infrastructure precedes and enables model development.\n\nPart II builds theoretical foundations and practical skills for model development. The Deep Learning Primer provides algorithmic foundations, while DNN Architectures extends these to specific network designs. AI Frameworks examines the software infrastructure from TensorFlow and PyTorch to specialized tools. AI Training develops training systems for complex models and large datasets.\n\nPart III addresses optimization for production deployment. Efficient AI introduces techniques for reducing computational requirements while maintaining quality. Optimizations covers compression techniques including quantization, pruning, and knowledge distillation. Hardware Acceleration examines specialized hardware from GPUs to custom ASICs. Benchmarking establishes methodologies for measuring and comparing system performance.\n\nPart IV ensures optimized systems operate reliably in production. Serving covers infrastructure for delivering predictions at scale. ML Operations encompasses practices from monitoring and deployment to incident response. Responsible Engineering addresses ethical considerations and governance. The Conclusion synthesizes the complete methodology.\n\nFor detailed guidance on reading paths, learning outcomes, prerequisites, and how to maximize your experience with this textbook, refer to the [About](../../frontmatter/about/about.qmd) section.\n\nThis introduction has established the conceptual foundation for everything that follows. The chapter began by examining the relationship between artificial intelligence as vision and machine learning as methodology, then defined machine learning systems as the artifacts that engineers build: integrated computing systems comprising data, algorithms, and infrastructure. Through the Bitter Lesson and AI's historical evolution, the chapter demonstrated why systems engineering has become fundamental to AI progress and how learning-based approaches came to dominate the field. This context enabled a formal definition of AI Engineering as a distinct discipline, following the pattern of Computer Engineering's emergence, establishing it as the field dedicated to building reliable, efficient, and scalable machine learning systems across all computational platforms.\n\nYet this broad vision raises immediate questions. If ML systems differ fundamentally from traditional software, what makes them different? Why do they fail in ways that conventional engineering intuitions cannot anticipate? The next chapter, @sec-ml-systems, examines these questions systematically, revealing the characteristics that distinguish ML systems from their traditional counterparts and establishing why specialized engineering approaches are necessary.\n\nWelcome to AI Engineering.\n","srcMarkdownNoYaml":"\n\n# Introduction {#sec-introduction}\n::: {layout-narrow}\n::: {.column-margin}\n_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting a roadmap of a book's chapters on machine learning systems, set on a crisp, clean white background. The image features a winding road traveling through various symbolic landmarks. Each landmark represents a chapter topic: Introduction, ML Systems, Deep Learning, AI Workflow, Data Engineering, AI Frameworks, AI Training, Efficient AI, Model Optimizations, AI Acceleration, Benchmarking AI, On-Device Learning, Embedded AIOps, Security & Privacy, Responsible AI, Sustainable AI, AI for Good, Robust AI, Generative AI. The style is clean, modern, and flat, suitable for a technical book, with each landmark clearly labeled with its chapter title._\n:::\n\n\\noindent\n![](images/png/cover_introduction.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_Why must we master the engineering principles that govern systems capable of learning, adapting, and operating at massive scale?_\n\nMachine learning represents the most significant transformation in computing since programmable computers, enabling systems whose behavior emerges from data rather than explicit instructions. This transformation requires new engineering foundations because traditional software engineering principles cannot address systems that learn and adapt based on experience. Every major technological challenge, from climate modeling and medical diagnosis to autonomous transportation, requires systems that process vast amounts of data and operate reliably despite uncertainty. Understanding ML systems engineering determines our ability to solve complex problems that exceed human cognitive capacity. This discipline provides the foundation for building systems that scale across deployment environments, from massive data centers to resource-constrained edge devices, establishing the technical groundwork for technological progress in the 21st century.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain machine learning systems as integrated architectures comprising data, algorithms, and computational infrastructure (AI Triad framework)\n\n- Distinguish ML systems from traditional software through their unique failure patterns and silent performance degradation characteristics\n\n- Trace the historical evolution of AI from symbolic reasoning through expert systems and statistical learning to modern deep learning architectures\n\n- Explain why general computational methods that leverage scale consistently outperform hand-crafted knowledge encoding approaches (the Bitter Lesson), establishing systems engineering as central to AI advancement\n\n- Describe how the ML development lifecycle differs from traditional software development through its continuous iteration and data-dependent adaptation\n\n- Identify the core engineering challenges in ML systems across data quality, model complexity, infrastructure scalability, and ethical considerations\n\n- Explain how the five-pillar framework (Data Engineering, Training Systems, Deployment Infrastructure, Operations, Ethics) organizes ML systems engineering to address data, training, deployment, operations, and ethics challenges\n\n- Analyze how deployment context (cloud, edge, mobile, embedded) influences engineering decisions across data collection, model design, infrastructure requirements, and operational practices\n\n- Evaluate whether a given problem is appropriate for ML-based solutions by considering data availability, success criteria measurability, tolerance for probabilistic behavior, and maintenance requirements\n\n:::\n\n## The Engineering Revolution in Artificial Intelligence {#sec-introduction-engineering-revolution-artificial-intelligence-a3eb}\n\nEngineering practice today stands at an inflection point comparable to the most transformative periods in technological history. The Industrial Revolution established mechanical engineering as a discipline for managing physical forces, while the Digital Revolution formalized computational engineering to handle algorithmic complexity. Today, artificial intelligence systems require a new engineering paradigm for systems that exhibit learned behaviors, autonomous adaptation, and operational scales that exceed conventional software engineering methods.\n\nThis shift reconceptualizes the nature of engineered systems. Traditional deterministic software operates according to explicitly programmed instructions, yielding predictable outputs for given inputs. Machine learning systems, by contrast, are probabilistic architectures whose behaviors emerge from statistical patterns extracted from training data. This transformation introduces engineering challenges that define ML systems engineering: ensuring reliability in systems whose behaviors are learned rather than programmed, achieving scalability for systems processing petabyte-scale[^fn-petabyte-scale] datasets while serving billions of concurrent users, and maintaining robustness when operational data distributions diverge from training distributions.\n\nThis textbook organizes around three foundational imperatives that address these challenges systematically. First, we must build the components of ML systems from data pipelines through model architectures, establishing the infrastructure and workflows that enable machine learning. Second, we must optimize these systems for efficiency, performance, and deployment constraints, ensuring they can operate effectively under real-world resource limitations. Third, we must operate them reliably in production environments, maintaining performance and adapting to changing conditions over time.\n\nThese challenges establish the theoretical and practical foundations of ML systems engineering as a distinct academic discipline. This chapter provides the conceptual foundation for understanding both the historical evolution that created this field and the engineering principles that differentiate machine learning systems from traditional software architectures. The analysis synthesizes perspectives from computer science, systems engineering, and statistical learning theory to establish a framework for the systematic study of intelligent systems.\n\nOur investigation begins with the relationship between artificial intelligence as a research objective and machine learning as the computational methodology for achieving intelligent behavior. We then establish what constitutes a machine learning system, the integrated computing systems comprising data, algorithms, and infrastructure that this discipline builds. Through historical analysis, we trace the evolution of AI paradigms from symbolic reasoning systems through statistical learning approaches to contemporary deep learning architectures, demonstrating how each transition required new engineering solutions. This progression illuminates Sutton's \"bitter lesson\" of AI research: that domain-general computational methods ultimately supersede hand-crafted knowledge representations, positioning systems engineering as central to AI advancement.\n\nThis historical and technical foundation enables us to formally define this discipline. Following the pattern established by Computer Engineering's emergence from Electrical Engineering and Computer Science, we establish it as a field focused on building reliable, efficient, and scalable machine learning systems across computational platforms. This formal definition addresses both the nomenclature used in practice and the technical scope of what practitioners actually build.\n\nBuilding upon this foundation, we introduce the theoretical frameworks that structure the analysis of ML systems throughout this text. The AI Triad provides a conceptual model for understanding the interdependencies among data, algorithms, and computational infrastructure. We examine the machine learning system lifecycle, contrasting it with traditional software development methodologies to highlight the unique phases of problem formulation, data curation, model development, validation, deployment, and continuous maintenance that characterize ML system engineering.\n\nThese theoretical frameworks are substantiated through examination of representative deployment scenarios that demonstrate the diversity of engineering requirements across application domains. From autonomous vehicles operating under stringent latency constraints at the network edge to recommendation systems serving billions of users through cloud infrastructure, these case studies illustrate how deployment context shapes system architecture and engineering trade-offs.\n\nThe analysis culminates by identifying the core challenges that establish ML systems engineering as both a necessary and complex discipline: silent performance degradation patterns that require specialized monitoring approaches, data quality issues and distribution shifts that compromise model validity, requirements for model robustness and interpretability in high-stakes applications, infrastructure scalability demands that exceed conventional distributed systems, and ethical considerations that impose new categories of system requirements. These challenges provide the foundation for the five-pillar organizational framework that structures this text, partitioning ML systems engineering into interconnected sub-disciplines that enable the development of robust, scalable, and responsible artificial intelligence systems.\n\nThis chapter establishes the theoretical foundation for Part I: Systems Foundations, introducing the principles that underlie all subsequent analysis of ML systems engineering. The conceptual frameworks introduced here provide the analytical tools that will be refined and applied throughout subsequent chapters, culminating in a methodology for engineering systems capable of reliably delivering artificial intelligence capabilities in production environments.\n\n## From Artificial Intelligence Vision to Machine Learning Practice {#sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a}\n\nHaving established AI's transformative impact across society, a question emerges: How do we actually create these intelligent capabilities? Understanding the relationship between Artificial Intelligence and Machine Learning provides the key to answering this question and is central to everything that follows.\n\nAI represents the broad goal of creating systems that can perform tasks requiring human-like intelligence: recognizing images, understanding language, making decisions, and solving problems. AI is the what, the vision of intelligent machines that can learn, reason, and adapt.\n\nMachine Learning (ML) represents the methodological approach and practical discipline for creating systems that demonstrate intelligent behavior. Rather than implementing intelligence through predetermined rules, machine learning provides the computational techniques to automatically discover patterns in data through mathematical processes. This methodology transforms AI's theoretical insights into functioning systems.\n\nConsider the evolution of chess-playing systems as an example of this shift. The AI goal remains constant: \"Create a system that can play chess like a human.\" However, the approaches differ:\n\n- **Symbolic AI Approach (Pre-ML)**: Program the computer with all chess rules and hand-craft strategies like \"control the center\" and \"protect the king.\" This requires expert programmers to explicitly encode thousands of chess principles, creating brittle systems that struggle with novel positions.\n\n- **Machine Learning Approach**: Have the computer analyze millions of chess games to learn winning strategies automatically from data. Rather than programming specific moves, the system discovers patterns that lead to victory through statistical analysis of game outcomes.\n\nThis transformation illustrates why ML has become the dominant approach: In rule-based systems, humans translate domain expertise directly into code. In ML systems, humans curate training data, design learning architectures, and define success metrics, allowing the system to extract its own operational logic from examples. Data-driven systems can adapt to situations that programmers never anticipated, while rule-based systems remain constrained by their original programming.\n\nMachine learning systems acquire recognition capabilities through processes that parallel human learning patterns. Object recognition develops through exposure to numerous examples, while natural language processing systems acquire linguistic capabilities through extensive textual analysis. These learning approaches operationalize theories of intelligence developed in AI research, building on mathematical foundations that we establish systematically throughout this text.\n\nThe distinction between AI as research vision and ML as engineering methodology carries significant implications for system design. Modern ML's data-driven approach requires infrastructure capable of collecting, processing, and learning from data at massive scale. Machine learning emerged as a practical approach to artificial intelligence through extensive research and major paradigm shifts[^fn-paradigm-shift], transforming theoretical principles about intelligence into functioning systems that form the algorithmic foundation of today's intelligent capabilities.\n\n[^fn-paradigm-shift]: **Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 [@kuhn1962structure] to describe major changes in scientific approach. In AI, the key paradigm shift was moving from symbolic reasoning (encoding human knowledge as rules) to statistical learning (discovering patterns from data). This shift had profound systems implications: rule-based systems scaled with programmer effort, requiring manual encoding of each new rule. Data-driven ML scales with compute and data infrastructure—achieving better performance by adding more GPUs and training data rather than more programmers. This transformation made systems engineering critical: success now depends on building infrastructure to collect massive datasets, train billion-parameter models, and serve predictions at scale, rather than encoding expert knowledge.\n\n[^fn-petabyte-scale]: **Petabyte-Scale Data**: One petabyte equals 1,000 terabytes or roughly 1 million gigabytes—enough to store 13.3 years of HD video or the entire written works of humanity 50 times over. Modern ML systems routinely process petabyte-scale datasets: Meta processes over 4 petabytes of data daily for its recommendation systems, while Google's search index contains hundreds of petabytes of web content. Managing this scale requires distributed storage systems (like HDFS or S3) that shard data across thousands of servers, parallel processing frameworks (like Apache Spark) that coordinate computation across clusters, and sophisticated data engineering pipelines that can validate, transform, and serve data at rates exceeding 100 GB/s. The engineering challenge isn't just storage capacity, but the bandwidth, fault tolerance, and consistency guarantees needed to make petabyte datasets useful for training and inference.\n\n::: {.callout-definition title=\"Key Definitions\"}\n***Artificial Intelligence (AI)*** is the field of computer science focused on creating systems that perform tasks requiring human-like _intelligence_, including _learning_, _reasoning_, and _adaptation_.\n\n***Machine Learning (ML)*** is the approach to AI that enables systems to automatically learn _patterns_ and make _decisions_ from _data_ rather than following explicit programmed rules.\n:::\n\nThe evolution from rule-based AI to data-driven ML represents one of the most significant shifts in computing history. This transformation explains why ML systems engineering has emerged as a discipline: the path to intelligent systems now runs through the engineering challenge of building systems that can effectively learn from data at massive scale.\n\n## Defining ML Systems {#sec-introduction-defining-ml-systems-bf7d}\n\nBefore exploring how we arrived at modern machine learning systems, we must first establish what we mean by an \"ML system.\" This definition provides the conceptual framework for understanding both the historical evolution and contemporary challenges that follow.\n\nNo universally accepted definition of machine learning systems exists, reflecting the field's rapid evolution and multidisciplinary nature. However, building on our understanding that modern ML relies on data-driven approaches at scale, this textbook adopts a perspective that encompasses the entire ecosystem in which algorithms operate:\n\n:::{.callout-definition title=\"Machine Learning System\"}\n***Machine Learning Systems*** are integrated computing systems comprising three interdependent components: _data_ that guides behavior, _algorithms_ that learn patterns, and _computational infrastructure_ that enables both _training_ and _inference_.\n:::\n\nAs illustrated in @fig-ai-triad, the core of any machine learning system consists of three interrelated components that form a triangular dependency: Models/Algorithms, Data, and Computing Infrastructure. Each element shapes the possibilities of the others. The model architecture dictates both the computational demands for training and inference, as well as the volume and structure of data required for effective learning. The data's scale and complexity influence what infrastructure is needed for storage and processing, while determining which model architectures are feasible. The infrastructure capabilities establish practical limits on both model scale and data processing capacity, creating a framework within which the other components must operate.\n\n::: {#fig-ai-triad fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.8}{%\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{\n Line/.style={line width=0.35pt,black!50,text=black},\n ALineA/.style={violet!80!black!50,line width=3pt,shorten <=2pt,shorten >=2pt,\n  {Triangle[width=1.1*6pt,length=0.8*6pt]}-{Triangle[width=1.1*6pt,length=0.8*6pt]}},\nLineD/.style={line width=0.75pt,black!50,text=black,dashed,dash pattern=on 5pt off 3pt},\nCircle/.style={inner xsep=2pt,\n  circle,\n    draw=BrownLine,\n    line width=0.75pt,\n    fill=BrownL!40,\n    minimum size=16mm\n  },\n circles/.pic={\n\\pgfkeys{/channel/.cd, #1}\n\\node[circle,draw=\\channelcolor,line width=\\Linewidth,fill=\\channelcolor!10,\nminimum size=2.5mm](\\picname){};\n        }\n}\n\\tikzset {\npics/cloud/.style = {\n        code = {\n\\colorlet{red}{RedLine}\n\\begin{scope}[local bounding box=CLO,scale=0.5, every node/.append style={transform shape}]\n\\draw[red,fill=white,line width=0.9pt](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)\nto[out=360,in=30,distance=9](1.68,0.42);\n\\draw[red,fill=white,line width=0.9pt](0,0)to[out=170,in=180,distance=11](0.1,0.61)\nto[out=90,in=105,distance=17](1.07,0.71)\nto[out=20,in=75,distance=7](1.48,0.36)\nto[out=350,in=0,distance=7](1.48,0)--(0,0);\n\\draw[red,fill=white,line width=0.9pt](0.27,0.71)to[bend left=25](0.49,0.96);\n\n\\end{scope}\n    }\n  }\n}\n%streaming\n\\tikzset{%\n LineST/.style={-{Circle[\\channelcolor,fill=RedLine,length=4pt]},draw=\\channelcolor,line width=\\Linewidth,rounded corners},\n ellipseST/.style={fill=\\channelcolor,ellipse,minimum width = 2.5mm, inner sep=2pt, minimum height =1.5mm},\n BoxST/.style={line width=\\Linewidth,fill=white,draw=\\channelcolor,rectangle,minimum width=56,\n minimum height=16,rounded corners=1.2pt},\n pics/streaming/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=STREAMING,scale=\\scalefac, every node/.append style={transform shape}]\n\\node[BoxST,minimum width=44,minimum height=48](\\picname-RE1){};\n\\foreach \\i/\\j in{1/north,2/center,3/south}{\n\\node[BoxST](\\picname-GR\\i)at(\\picname-RE1.\\j){};\n\\node[ellipseST]at($(\\picname-GR\\i.west)!0.2!(\\picname-GR\\i.east)$){};\n\\node[ellipseST]at($(\\picname-GR\\i.west)!0.4!(\\picname-GR\\i.east)$){};\n}\n\\draw[LineST](\\picname-GR3)--++(2,0)coordinate(\\picname-C4);\n\\draw[LineST](\\picname-GR3.320)--++(0,-0.7)--++(0.8,0)coordinate(\\picname-C5);\n\\draw[LineST](\\picname-GR3.220)--++(0,-0.7)--++(-0.8,0)coordinate(\\picname-C6);\n\\draw[LineST](\\picname-GR3)--++(-2,0)coordinate(\\picname-C7);\n \\end{scope}\n     }\n  }\n}\n%data\n\\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,\nminimum width=25mm,minimum height=11mm,line width=\\Linewidth,node distance=-0.15},\npics/data/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=STREAMING,scale=\\scalefac, every node/.append style={transform shape}]\n\\node[mycylinder,fill=\\channelcolor!50] (A) {};\n\\node[mycylinder, above=of A,fill=\\channelcolor!30] (B) {};\n\\node[mycylinder, above=of B,fill=\\channelcolor!10] (C) {};\n \\end{scope}\n     }\n  }\n}\n\\pgfkeys{\n  /channel/.cd,\n  channelcolor/.store in=\\channelcolor,\n  drawchannelcolor/.store in=\\drawchannelcolor,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  channelcolor=BrownLine,\n  drawchannelcolor=BrownLine,\n  scalefac=1,\n  Linewidth=0.5pt,\n  picname=C\n}\n\\node[Circle](MO){};\n\\node[Circle,below left=1 and 2.5 of MO,draw=GreenLine,fill=GreenL!40,](IN){};\n\\node[Circle,below right=1 and 2.5 of MO,draw=OrangeLine,fill=OrangeL!40,](DA){};\n\\draw[ALineA](MO)--(IN);\n\\draw[ALineA](MO)--(DA);\n\\draw[ALineA](DA)--(IN);\n\\node[below=2pt of MO]{Model};\n\\node[below=2pt of IN]{Infra};\n\\node[below=2pt of DA]{Data};\n%%\n\\begin{scope}[local bounding box=CIRCLE1,shift={($(MO)+(0.04,-0.24)$)},\nscale=0.55, every node/.append style={transform shape}]\n%1 column\n\\foreach \\j in {1,2,3} {\n  \\pgfmathsetmacro{\\y}{(1.5-\\j)*0.43 + 0.7}\n  \\pic at (-0.8,\\y) {circles={channelcolor=RedLine,picname=1CD\\j}};\n}\n%2 column\n\\foreach \\i in {1,...,4} {\n  \\pgfmathsetmacro{\\y}{(2-\\i)*0.43+0.7}\n  \\pic at (0,\\y) {circles={channelcolor=RedLine, picname=2CD\\i}};\n}\n%3 column\n\\foreach \\j in {1,2} {\n  \\pgfmathsetmacro{\\y}{(1-\\j)*0.43 + 0.7}\n  \\pic at (0.8,\\y) {circles={channelcolor=RedLine,picname=3CD\\j}};\n}\n\\foreach \\i in {1,2,3}{\n  \\foreach \\j in {1,2,3,4}{\n\\draw[Line](1CD\\i)--(2CD\\j);\n}}\n\\foreach \\i in {1,2,3,4}{\n  \\foreach \\j in {1,2}{\n\\draw[Line](2CD\\i)--(3CD\\j);\n}}\n\\end{scope}\n%\n\\pic[shift={(-0.4,-0.08)}] at (IN) {cloud};\n%\n\\pic[shift={(-0.05,-0.13)}] at  (IN){streaming={scalefac=0.25,picname=2,channelcolor=RedLine, Linewidth=0.65pt}};\n%\n\\pic[shift={(0,-0.3)}] at  (DA){data={scalefac=0.3,picname=1,channelcolor=green!70!black, Linewidth=0.4pt}};\n\\end{tikzpicture}}\n```\n**Component Interdependencies**: Machine learning system performance relies on the coordinated interaction of models, data, and computing infrastructure; limitations in any one component constrain the capabilities of the others. Effective system design requires balancing these interdependencies to optimize overall performance and feasibility.\n:::\n\nEach component serves a distinct but interconnected purpose:\n\n- **Algorithms**: Mathematical models and methods that learn patterns from data to make predictions or decisions\n\n- **Data**: Processes and infrastructure for collecting, storing, processing, managing, and serving data for both training and inference\n\n- **Computing**: Hardware and software infrastructure that enables training, serving, and operation of models at scale\n\nAs the triangle illustrates, no single element can function in isolation. Algorithms require data and computing resources, large datasets require algorithms and infrastructure to be useful, and infrastructure requires algorithms and data to serve any purpose.\n\nThis triangular dependency means that advancing any single component in isolation provides limited benefit. Improved algorithms cannot realize their potential without sufficient data and computational capacity. Larger datasets become burdensome without algorithms capable of extracting meaningful patterns and infrastructure capable of processing them efficiently. More powerful hardware accelerates computation but cannot compensate for poor data quality or unsuitable algorithmic approaches. Machine learning systems demand careful orchestration of all three components, with each constraining and enabling the others.\n\nThese interdependencies become clear when examining breakthrough moments in AI history. The 2012 AlexNet[^fn-alexnet-breakthrough] breakthrough illustrates the principle of hardware-software co-design that defines modern ML systems engineering. This deep learning revolution succeeded because the algorithmic innovation (convolutional neural networks) matched the hardware capability (parallel GPU architectures), graphics processing units originally designed for gaming but repurposed for AI computations, providing 10-100x speedups over traditional CPUs for machine learning tasks. Convolutional operations are inherently parallel, making them naturally suited to GPU's thousands of parallel cores. This co-design approach continues to shape ML system development across the industry.\n\n[^fn-alexnet-breakthrough]: **AlexNet**: A breakthrough deep learning model created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that won the 2012 ImageNet competition by a massive margin, reducing top-5 error rates from 26.2% to 15.3%. This was the \"ImageNet moment\" that proved deep learning could outperform traditional computer vision approaches and sparked the modern AI revolution. AlexNet demonstrated that with enough data (1.2 million images), computing power (two GPUs for 6 days), and clever engineering (dropout, data augmentation), neural networks could achieve superhuman performance on complex visual tasks.\n\nWith this three-component framework established, we must understand a fundamental difference that distinguishes ML systems from traditional software: how failures manifest across the AI Triad's components.\n\n## How ML Systems Differ from Traditional Software {#sec-introduction-ml-systems-differ-traditional-software-4370}\n\nThe AI Triad framework reveals what ML systems comprise: data that guides behavior, algorithms that extract patterns, and infrastructure that enables learning and inference. However, understanding these components alone does not capture what makes ML systems engineering fundamentally different from traditional software engineering. The critical distinction lies in how these systems fail.\n\nTraditional software exhibits explicit failure modes. When code breaks, applications crash, error messages propagate, and monitoring systems trigger alerts. This immediate feedback enables rapid diagnosis and remediation. The system operates correctly or fails observably. Machine learning systems operate under a fundamentally different paradigm: they can continue functioning while their performance degrades silently without triggering conventional error detection mechanisms. The algorithms continue executing, the infrastructure maintains prediction serving, yet the learned behavior becomes progressively less accurate or contextually relevant.\n\nConsider how an autonomous vehicle's perception system illustrates this distinction. Traditional automotive software exhibits binary operational states: the engine control unit either manages fuel injection correctly or triggers diagnostic warnings. The failure mode remains observable through standard monitoring. An ML-based perception system presents a qualitatively different challenge: the system's accuracy in detecting pedestrians might decline from 95% to 85% over several months due to seasonal changes, as different lighting conditions, clothing patterns, or weather phenomena underrepresented in training data affect model performance. The vehicle continues operating, successfully detecting most pedestrians, yet the degraded performance creates safety risks that become apparent only through systematic monitoring of edge cases and comprehensive evaluation. Conventional error logging and alerting mechanisms remain silent while the system becomes measurably less safe.\n\nThe magnitude of this degradation matters profoundly in safety-critical contexts. For autonomous vehicles, even 95% accuracy may be inadequate: safety-critical systems typically require 99.9% or higher reliability. The 10% degradation from 95% to 85% is especially concerning because failures concentrate in edge cases where detection was already marginal, precisely the scenarios where human safety is most at risk.\n\nThis silent degradation manifests across all three AI Triad components. The data distribution shifts as the world changes: user behavior evolves, seasonal patterns emerge, new edge cases appear. The algorithms continue making predictions based on outdated learned patterns, unaware that their training distribution no longer matches operational reality. The infrastructure faithfully serves these increasingly inaccurate predictions at scale, amplifying the problem. A recommendation system experiencing this degradation might decline from 85% accuracy to 60% over six months as user preferences evolve and training data becomes stale. The system continues generating recommendations, users receive results, the infrastructure reports healthy uptime metrics, yet business value silently erodes. This degradation often stems from training-serving skew, where features computed differently between training and serving pipelines cause model performance to degrade despite unchanged code, which is an infrastructure issue that manifests as algorithmic failure.\n\nThis fundamental difference in failure modes distinguishes ML systems from traditional software in ways that demand new engineering practices. Traditional software development focuses on eliminating bugs and ensuring deterministic behavior. ML systems engineering must additionally address probabilistic behaviors, evolving data distributions, and performance degradation that occurs without code changes. The monitoring systems must track not just infrastructure health but also model performance, data quality, and prediction distributions. The deployment practices must enable continuous model updates as data distributions shift. The entire system lifecycle, from data collection through model training to inference serving, must be designed with silent degradation in mind.\n\nThis operational reality establishes why ML systems developed in research settings require specialized engineering practices to reach production deployment. The unique lifecycle and monitoring requirements that ML systems demand stem directly from this failure characteristic, establishing the fundamental motivation for ML systems engineering as a distinct discipline.\n\nUnderstanding how ML systems fail differently raises an important question: given the three components of the AI Triad (data, algorithms, and infrastructure), which should we prioritize to advance AI capabilities? Should we invest in better algorithms, larger datasets, or more powerful computing infrastructure? The answer to this question reveals why systems engineering has become central to AI progress.\n\n## The Bitter Lesson: Why Systems Engineering Matters {#sec-introduction-bitter-lesson-systems-engineering-matters-dede}\n\nThe single biggest lesson from 70 years of AI research is that systems that can leverage massive computation ultimately win. This is why systems engineering, not just algorithmic cleverness, has become the bottleneck for progress in AI.\n\nThe evolution from symbolic AI through statistical learning to deep learning raises a fundamental question for system builders: Should we focus on developing more sophisticated algorithms, curating better datasets, or building more powerful infrastructure?\n\nThe answer to this question shapes how we approach building AI systems and reveals why systems engineering has emerged as a discipline.\n\nHistory provides a consistent answer. Across decades of AI research, the greatest breakthroughs have not come from better encoding of human knowledge or more algorithmic techniques, but from finding ways to leverage greater computational resources more effectively. This pattern, articulated by reinforcement learning pioneer Richard Sutton[^fn-richard-sutton] in his 2019 essay \"The Bitter Lesson\" [@sutton2019bitter], suggests that systems engineering has become the determinant of AI success.\n\n[^fn-richard-sutton]: **Richard Sutton**: A pioneering AI researcher who transformed how machines learn through reinforcement learning—teaching AI systems to learn from trial and error, like how you learned to ride a bike through practice rather than instruction manuals. At the University of Alberta, Sutton co-authored the foundational textbook \"Reinforcement Learning: An Introduction\" and developed key algorithms (TD-learning, policy gradients) that power everything from AlphaGo to modern robotics. He received the 2024 ACM Turing Award (computing's highest honor, often called the \"Nobel Prize of Computing\") shared with Andrew Barto for their decades of foundational contributions to how AI systems learn and adapt. His \"Bitter Lesson\" essay distills 70 years of AI history into one profound insight: general methods leveraging computation consistently beat approaches that encode human expertise.\n\nSutton observed that approaches emphasizing human expertise and domain knowledge, while providing short-term improvements, are consistently surpassed by general methods that can leverage massive computational resources. He writes: \"The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.\"\n\nThis principle finds validation across AI breakthroughs. In chess, IBM's Deep Blue defeated world champion Garry Kasparov in 1997 [@campbell2002deep] not by encoding chess strategies, but through brute-force search evaluating millions of positions per second. In Go, DeepMind's AlphaGo [@silver2016mastering] achieved superhuman performance by learning from self-play rather than studying centuries of human Go wisdom. In computer vision, convolutional neural networks that learn features directly from data have surpassed decades of hand-crafted feature engineering. In speech recognition, end-to-end deep learning systems have outperformed approaches built on detailed models of human phonetics and linguistics.\n\nThe \"bitter\" aspect of this lesson is that our intuition misleads us. We naturally assume that encoding human expertise should be the path to artificial intelligence. Yet repeatedly, systems that leverage computation to learn from data outperform systems that rely on human knowledge, given sufficient scale. This pattern has held across symbolic AI, statistical learning, and deep learning eras, a consistency we will examine in detail when we trace AI's historical evolution in the next section.\n\nConsider modern language models like GPT-4 or image generation systems like DALL-E. Their capabilities emerge not from linguistic or artistic theories encoded by humans, but from training general-purpose neural networks on vast amounts of data using enormous computational resources. Training large language models consumes substantial energy: estimates for GPT-3-scale training suggest several thousand MWh based on hardware specifications and reported training duration [@patterson2021carbon], equivalent to hundreds of U.S. homes for a year. Serving models to millions of users requires data centers consuming megawatts of continuous power. The engineering challenge is building systems that can manage this scale: collecting and processing petabytes of training data, coordinating training across thousands of GPUs, serving models to millions of users with millisecond latency, and continuously updating systems based on real-world performance.\n\nThese scale requirements reveal a fundamental engineering reality: building systems capable of training on petabytes of data and serving millions of users requires expertise in distributed systems, data engineering, and hardware optimization that goes far beyond algorithmic innovation. The computational infrastructure needed to realize modern AI capabilities has become the primary engineering challenge, from managing data movement between storage and processing units[^fn-memory-bandwidth] to coordinating thousands of processors and optimizing for both performance and energy efficiency. We explore these hardware constraints quantitatively in @sec-ai-acceleration, where students will have the prerequisite background to analyze memory bandwidth limitations and their implications for system design.\n\n[^fn-thermal-power-constraints]: **Thermal and Power Constraints**: The physical limits imposed by heat generation and power consumption in computing hardware. Modern GPUs consume 300-700W each (equivalent to 3-7 hair dryers running continuously) and generate enormous heat that must be removed via sophisticated cooling systems. A single AI training cluster with 1,000 GPUs consumes 300-700 kW of power just for computation, plus 30-50% more for cooling, totaling ~1MW—equivalent to powering 750 homes. Data centers hit thermal density limits: you can only pack so many hot chips together before cooling becomes impossible or prohibitively expensive. These constraints drive hardware design choices (chip architectures optimized for performance-per-watt), infrastructure decisions (liquid cooling vs. air cooling), and economic trade-offs (power costs can exceed hardware costs over 3-year lifespans). Power/thermal management explains many ML system architecture decisions, from edge deployment to model compression.\n\n[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be transferred between memory and processors, measured in GB/s (gigabytes per second). Modern GPUs like the H100 provide ~3TB/s memory bandwidth, while CPUs typically provide 100-200 GB/s. This seemingly large number becomes the bottleneck for ML workloads: a transformer model with 70 billion parameters requires 140GB just to store weights, taking 47ms to load at 3TB/s before any computation begins. The bandwidth constraint explains why ML accelerators focus on higher bandwidth memory (HBM) rather than just faster compute units. For comparison, arithmetic operations are relatively cheap: a GPU can perform trillions of multiply-add operations in the time it takes to move 1GB from memory, creating a fundamental tension where processors spend more time waiting for data than computing.\n\n[^fn-distributed-systems]: **Distributed Systems**: Computing systems where components run on multiple networked machines and coordinate through message passing. Modern ML training exemplifies distributed systems complexity: training GPT-3 required coordinating 1,024 V100 GPUs across multiple data centers, each processing different data batches while synchronizing gradient updates. Key challenges include fault tolerance (handling machine failures mid-training), network bottlenecks (all-reduce operations can consume 40%+ of total training time), and consistency (ensuring all nodes use the same model weights). Unlike traditional distributed systems focused on serving requests, ML distributed systems must coordinate massive data movement and maintain numerical precision across thousands of nodes, making consensus algorithms and load balancing far more complex.\n\n[^fn-amdahls-law]: **Amdahl's Law**: Formulated by computer architect Gene Amdahl in 1967, this law quantifies the theoretical speedup of a program when only part of it can be parallelized. The speedup is limited by the sequential portion: if P is the fraction that can be parallelized, maximum speedup = 1/(1-P). In ML systems, this explains why memory bandwidth and data movement often become the primary bottlenecks rather than compute capacity. We develop this concept with worked examples in @sec-ai-acceleration.\n\nSutton's bitter lesson helps explain the motivation for this book. If AI progress depends on our ability to scale computation effectively, then understanding how to build, deploy, and maintain these computational systems becomes the most important skill for AI practitioners. ML systems engineering has become important because creating modern systems requires coordinating thousands of GPUs across multiple data centers, processing petabytes of text data, and serving resulting models to millions of users with millisecond latency requirements. This challenge demands expertise in distributed systems[^fn-distributed-systems], data engineering, hardware optimization, and operational practices that represent an entirely new engineering discipline.\n\nThe convergence of these systems-level challenges suggests that no existing discipline addresses what modern AI requires. While Computer Science advances ML algorithms and Electrical Engineering develops specialized AI hardware, neither discipline alone provides the engineering principles needed to deploy, optimize, and sustain ML systems at scale. This gap requires a new engineering discipline. But to understand why this discipline has emerged now and what form it takes, we must first trace the evolution of AI itself, from early symbolic systems to modern machine learning.\n\n## Historical Evolution of AI Paradigms {#sec-introduction-historical-evolution-ai-paradigms-796e}\n\nThe systems-centric perspective we've established through the Bitter Lesson didn't emerge overnight. It developed through decades of AI research where each major transition revealed new insights about the relationship between algorithms, data, and computational infrastructure. Tracing this evolution helps us understand not just technological progress, but the shifts in approach that explain today's emphasis on scalable systems.\n\nUnderstanding why this transition to systems-focused ML is happening now requires recognizing the convergence of three factors in the last decade:\n\n1. **Massive Datasets**: The internet age created unprecedented data volumes through web content, social media, sensor networks, and digital transactions. Public datasets like ImageNet (millions of labeled images) and Common Crawl (billions of web pages) provide the raw material for learning complex patterns.\n\n2. **Algorithmic Breakthroughs**: Deep learning proved remarkably effective across diverse domains, from computer vision to natural language processing. Techniques like transformers, attention mechanisms, and transfer learning enabled models to learn generalizable representations from data.\n\n3. **Hardware Acceleration**: Graphics Processing Units (GPUs) originally designed for gaming provided 10-100x speedups for machine learning computations. Cloud computing infrastructure made this computational power accessible without massive capital investments.\n\nThis convergence explains why we've moved from theoretical models to large-scale deployed systems requiring a new engineering discipline. Each factor amplified the others: bigger datasets demanded more computation, better algorithms justified larger datasets, and faster hardware enabled more algorithms. This convergence transformed AI from an academic curiosity to a production technology requiring robust engineering practices.\n\nThe evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt [@rosenblatt1957perceptron], an early computational learning algorithm. Computer labs in 1965 contained room-sized mainframes[^fn-mainframes] running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, though groundbreaking for their time, differed substantially from today's machine learning systems that detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997 [@campbell2002deep]. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023 [@openai2023gpt4], demonstrating the dramatic evolution and increasing complexity of AI systems over the decades.\n\n[^fn-early]: **Perceptron**: One of the first computational learning algorithms (1957), simple enough to implement in hardware with minimal memory—1950s mainframes could only store thousands of weights, not millions. This hardware constraint shaped early AI research toward simple, interpretable models. The Perceptron's limitation to linearly separable problems wasn't just algorithmic—multi-layer networks (which could solve non-linear problems) were proposed in the 1960s but remained computationally intractable until the 1980s when memory became cheaper and CPUs faster. This 20-year gap between algorithmic insight and practical implementation foreshadowed a pattern in AI: breakthrough algorithms often wait decades for hardware to catch up, explaining why ML systems engineering focuses on co-designing algorithms with available infrastructure.\n\n[^fn-mainframes]: **Mainframes**: Room-sized computers that dominated the 1960s-70s, typically costing millions of dollars and requiring dedicated cooling systems. IBM's System/360 mainframe from 1964 weighed up to 20,000 pounds and had 8KB-1MB of memory depending on model, about 1/millionth the memory of a modern smartphone, yet represented the cutting edge of computing power that enabled early AI research.\n\n[^fn-eliza]: **ELIZA**: Created by MIT's Joseph Weizenbaum in 1966 [@weizenbaum1966eliza], ELIZA was one of the first chatbots that could simulate human conversation by pattern matching and substitution. From a systems perspective, ELIZA ran on 256KB mainframes using simple pattern matching—no learning, no data storage, no training phase. This computational simplicity allowed real-time interaction on 1960s hardware but resulted in brittleness that motivated the shift to data-driven ML. Modern chatbots like GPT-3 require vastly more infrastructure (350GB model parameters when uncompressed, $4.6M training cost estimate, GPU servers for inference) but handle conversations ELIZA couldn't—illustrating the systems trade-off: rule-based systems are computationally cheap but brittle, while ML systems are infrastructure-intensive but flexible. Ironically, Weizenbaum was horrified when people formed emotional attachments to his simple program, leading him to become a critic of AI.\n\n::: {#fig-ai-timeline fig-env=\"figure\" fig-pos=\"t!\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\definecolor{bluegraph}{RGB}{0,102,204}\n    \\pgfmathsetlengthmacro\\MajorTickLength{\n      \\pgfkeysvalueof{/pgfplots/major tick length} * 1.5\n    }\n\\tikzset{%\n   textt/.style={line width=0.5pt,draw=bluegraph,text width=26mm,align=flush center,\n                        font=\\usefont{T1}{phv}{m}{n}\\footnotesize,fill=cyan!7},\n   Line/.style={line width=0.85pt,draw=bluegraph,dash pattern=on 3pt off 2pt,\n   {Circle[bluegraph,length=4.5pt]}-   }\n}\n\n\\begin{axis}[clip=false,\n  axis line style={thick},\n  axis lines*=left,\n  axis on top,\n  width=18cm,\n  height=20cm,\n  xmin=1950,\n  xmax=2023,\n  ymin=0.000000,\n  ymax=0.00033,\n  xtick={1950,1960,1970,1980,1990,2000,2010,2020},\n  extra x ticks={1955,1965,1975,1985,1995,2005,2015},\n  extra x tick labels={},\n  xticklabels={1950,1960,1970,1980,1990,2000,2010,2020},\n  ytick={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},\n  yticklabels={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},\n  grid=none,\n    tick label style={/pgf/number format/assume math mode=true},\n    xticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n},\n   yticklabel style={\n  font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n  /pgf/number format/fixed,\n  /pgf/number format/fixed zerofill,\n  /pgf/number format/precision=5\n},\nscaled y ticks=false,\ntick style = {line width=1.0pt},\ntick align = outside,\nmajor tick length=\\MajorTickLength,\n]\n\\fill[fill=BrownL!70](axis cs:1974,0)rectangle(axis cs:1980,0.00031)\n        node[above,align=center,xshift=-7mm]{1st AI \\\\ Winter};\n\\fill[fill=BrownL!70](axis cs:1987,0)rectangle(axis cs:1993,0.00031)\n        node[above,align=center,xshift=-7mm]{2nd AI \\\\ Winter};\n\\addplot[line width=2pt,color=RedLine,smooth,samples=100] coordinates {\n(1950,0.0000006281)\n(1951,0.0000000683)\n(1952,0.0000003056)\n(1953,0.0000002927)\n(1954,0.0000004296)\n(1955,0.0000004593)\n(1956,0.0000016705)\n(1957,0.0000006570)\n(1958,0.0000021902)\n(1959,0.0000032832)\n(1960,0.0000126863)\n(1961,0.0000063721)\n(1962,0.0000240680)\n(1963,0.0000141502)\n(1964,0.0000111442)\n(1965,0.0000143832)\n(1966,0.0000147726)\n(1967,0.0000169539)\n(1968,0.0000167880)\n(1969,0.0000175559)\n(1970,0.0000155680)\n(1971,0.0000206809)\n(1972,0.0000223804)\n(1973,0.0000218203)\n(1974,0.0000256138)\n(1975,0.0000282924)\n(1976,0.0000247784)\n(1977,0.0000404966)\n(1978,0.0000358032)\n(1979,0.0000436903)\n(1980,0.0000472788)\n(1981,0.0000561471)\n(1982,0.0000767864)\n(1983,0.0001064465)\n(1984,0.0001592212)\n(1985,0.0002133700)\n(1986,0.0002559067)\n(1987,0.0002608470)\n(1988,0.0002623321)\n(1989,0.0002358150)\n(1990,0.0002301105)\n(1991,0.0002051343)\n(1992,0.0001789229)\n(1993,0.0001560935)\n(1994,0.0001508219)\n(1995,0.0001401406)\n(1996,0.0001169577)\n(1997,0.0001150365)\n(1998,0.0001051385)\n(1999,0.0000981740)\n(2000,0.0001010236)\n(2001,0.0000976966)\n(2002,0.0001038084)\n(2003,0.0000980004)\n(2004,0.0000989412)\n(2005,0.0000977251)\n(2006,0.0000899964)\n(2007,0.0000864005)\n(2008,0.0000911872)\n(2009,0.0000852932)\n(2010,0.0000822649)\n(2011,0.0000913442)\n(2012,0.0001104912)\n(2013,0.0001023061)\n(2014,0.0001022477)\n(2015,0.0000919719)\n(2016,0.0001134797)\n(2017,0.0001384348)\n(2018,0.0002057324)\n(2019,0.0002328642)\n}\nnode[left,pos=1,align=center,black]{Last year of\\\\ date: 2019};\n\n\\node[textt,text width=20mm](1950)at(axis cs:1957,0.00014){\\textcolor{red}{1950}\\\\\nAlan Turing publishes \\textbf{``Computing Machinery and Intelligence''} in the journal \\textit{Mind}.};\n\\node[red,align=center,above=2mm of 1950]{Milestones\\\\ in AI};\n\\draw[Line] (axis cs:1950,0) -- (1950.235);\n%\n\\node[textt,text width=19mm](1956)at(axis cs:1958,0.00007){\\textcolor{red}{Summer 1956}\\\\\n\\textbf{Dartmouth Workshop} A formative conference organized by AI pioneer John McCarthy.};\n\\draw[Line] (axis cs:1956,0) -- (1956.255);\n%\n\\node[textt](1957)at(axis cs:1969,0.00022){\\textcolor{red}{1957}\\\\\n\\textbf{Cornell psychologist Frank Rosenblatt invents the perceptron}, a system that paves the way for\nmodern neural networks\n(see \"The Turbulent Past and Uncertain Future of Artificial Intelligence,\" p. 26).};\n\\draw[Line] (axis cs:1957,0) -- ++(0mm,17mm)-|(1957.248);\n%\n\\node[textt,text width=21mm](1966)at(axis cs:1972,0.00012){\\textcolor{red}{1966}\\\\\n\\textbf{ELIZA chatbot} An early example of natural-language programming created by\nMIT professor Joseph Weizenbaum.};\n\\draw[Line] (axis cs:1966,0) -- ++(0mm,17mm)-|(1966);\n%\n\\node[textt,text width=20mm](1979)at(axis cs:1985,0.00012){\\textcolor{red}{1979}\\\\\nHans Moravec builds the \\textbf{Stanford Cart}, one of the first autonomous vehicles.};\n\\draw[Line] (axis cs:1979,0) -- ++(0mm,17mm)-|(1979.245);\n%\n\\node[textt,text width=21mm](1981)at(axis cs:1990,0.00006){\\textcolor{red}{1981}\\\\\nJapanese \\textbf{Fifth-Generation Computer Systems} project begins. The infusion of\nresearch funding helps end first \"AI winter.\"};\n\\draw[Line] (axis cs:1981,0) -- ++(0mm,10mm)-|(1981);\n%\n\\node[textt,text width=15mm](1997)at(axis cs:2001,0.00007){\\textcolor{red}{1997}\\\\\n\\textbf{IBM's Deep Blue} beats world chess champion Garry Kasparov};\n\\draw[Line] (axis cs:1997,0) -- ++(0mm,10mm)-|(1997);\n%\n\\node[textt,text width=15mm](2011)at(axis cs:2014,0.00003){\\textcolor{red}{2011}\\\\\n\\textbf{IBM's Watson} wins at Jeopardy!};\n\\draw[Line] (axis cs:2011,0) -- (2011);\n%\n\\node[textt,text width=19mm](2005)at(axis cs:2012,0.00009){\\textcolor{red}{2005}\\\\\n\\textbf{DARPA Grand Challenge} Stanford wins the agency's second driverless-car\ncompetition by driving 212 kilometers on an unrehearsed trail};\n\\draw[Line] (axis cs:2005,0) -- (2005);\n%\n\\node[textt,text width=30mm](2020)at(axis cs:2010,0.00017){\\textcolor{red}{2020}\\\\\n\\textbf{OpenAI introduces GPT-3}. The enormously powerful natural-language model\nlater causes an outcry when it begins spouting bigoted remarks};\n\\draw[Line] (axis cs:2020,0) |- (2020);\n%\n\\draw[Line,solid,-] (axis cs:1991,0.0002) --++(50:35mm)\nnode[bluegraph,above,align=center,text width=30mm]{Percent of U.S.-published books\nin Google's database that mention artificial intelligence};\n\\end{axis}\n\\end{tikzpicture}\n```\n**AI Development Timeline**: Early AI research focused on symbolic reasoning and rule-based systems, while modern AI leverages data-driven approaches like neural networks to achieve increasingly complex tasks. This progression exposes a shift from hand-coded intelligence to learned intelligence, marked by milestones such as the perceptron, deep blue, and large language models like GPT-3.\n:::\n\nExamining this timeline reveals several distinct eras of development, each building upon the lessons of its predecessors while addressing limitations that prevented earlier approaches from achieving their promise.\n\n### Symbolic AI Era {#sec-introduction-symbolic-ai-era-9d27}\n\nThe story of machine learning begins at the historic Dartmouth Conference[^fn-dartmouth-conference] in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term \"artificial intelligence\" [@mccarthy1956dartmouth]. Their approach assumed that intelligence could be reduced to symbol manipulation. Daniel Bobrow's STUDENT system from 1964 [@bobrow1964student] exemplifies this era by solving algebra word problems through natural language understanding.\n\n[^fn-dartmouth-conference]: **Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was officially born. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, it was the first time researchers gathered specifically to discuss \"artificial intelligence,\" a term McCarthy coined for the proposal. The ambitious goal was to make machines \"simulate every aspect of learning or any other feature of intelligence.\" From a systems perspective, participants fundamentally underestimated resource requirements—they assumed AI would fit on 1950s hardware (64KB memory maximum, kilohertz to low megahertz processors). Reality required 1,000,000x more resources: modern language models use 350GB memory and exaflops of training compute. This million-fold miscalculation of scale requirements helps explain why early symbolic AI failed: researchers focused on algorithmic cleverness while ignoring infrastructure constraints. The lesson: AI progress requires both algorithmic innovation AND systems engineering to provide necessary computational resources.\n\n::: {.callout-example title=\"STUDENT (1964)\"}\n```\nProblem: \"If the number of customers Tom gets is twice the\nsquare of 20% of the number of advertisements he runs, and\nthe number of advertisements is 45, what is the number of\ncustomers Tom gets?\"\n\nSTUDENT would:\n\n1. Parse the English text\n2. Convert it to algebraic equations\n3. Solve the equation: n = 2(0.2 × 45)²\n4. Provide the answer: 162 customers\n```\n:::\n\nEarly AI like STUDENT suffered from a limitation: they could only handle inputs that exactly matched their pre-programmed patterns and rules. This \"brittleness\"[^fn-brittleness] meant that while these solutions could appear intelligent when handling very specific cases they were designed for, they would break down completely when faced with even minor variations or real-world complexity. This limitation drove the evolution toward statistical approaches that we'll examine in the next section.\n\n[^fn-brittleness]: **Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle. This contrasts with human intelligence, which can adapt and make reasonable guesses even in unfamiliar situations. From a systems perspective, brittleness made deployment infeasible beyond controlled lab conditions—each new edge case required programmer intervention, creating unsustainable operational overhead. A speech recognition system encountering a new accent would fail rather than degrade gracefully, requiring system updates rather than continuous operation. ML's ability to generalize enables real-world deployment despite unpredictable inputs, shifting the challenge from explicit rule programming to infrastructure for collecting training data and continuously updating models as new patterns emerge.\n\n### Expert Systems Era {#sec-introduction-expert-systems-era-c7dd}\n\nRecognizing the limitations of symbolic AI, researchers by the mid-1970s acknowledged that general AI was overly ambitious and shifted their focus to capturing human expert knowledge in specific, well-defined domains. MYCIN [@shortliffe1976mycin], developed at Stanford, emerged as one of the first large-scale expert systems designed to diagnose blood infections.\n\n::: {.callout-example title=\"MYCIN (1976)\"}\n```\nRule Example from MYCIN:\nIF\n  The infection is primary-bacteremia\n  The site of the culture is one of the sterile sites\n  The suspected portal of entry is the gastrointestinal tract\nTHEN\n  Found suggestive evidence (0.7) that infection is bacteroid\n```\n:::\n\nMYCIN represented a major advance in medical AI with 600 expert rules for diagnosing blood infections, yet it revealed key challenges persisting in contemporary ML. Getting domain knowledge from human experts and converting it into precise rules proved time-consuming and difficult, as doctors often couldn't explain exactly how they made decisions. MYCIN struggled with uncertain or incomplete information, unlike human doctors who could make educated guesses. Maintaining and updating the rule base became more complex as MYCIN grew, as adding new rules frequently conflicted with existing ones, while medical knowledge itself continued to evolve. Knowledge capture, uncertainty handling, and maintenance remain concerns in modern machine learning, addressed through different technical approaches.\n\n### Statistical Learning Era {#sec-introduction-statistical-learning-era-8116}\n\nThese challenges with knowledge capture and system maintenance drove researchers toward a different approach. The 1990s marked a transformation in artificial intelligence as the field shifted from hand-coded rules toward statistical learning approaches.\n\nThree converging factors made statistical methods possible and powerful. First, the digital revolution meant massive amounts of data were available to train algorithms. Second, Moore's Law [@moore1965cramming][^fn-mooreslaw] delivered the computational power needed to process this data effectively. Third, researchers developed new algorithms like Support Vector Machines and improved neural networks that could learn patterns from data rather than following pre-programmed rules.\n\nThis combination transformed AI development: rather than encoding human knowledge directly, machines could discover patterns automatically from examples, creating more robust and adaptable systems.\n\n[^fn-mooreslaw]: **Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. Moore's Law enabled ML by providing approximately 1,000x more transistor density from 2000-2020, making previously impossible algorithms practical—neural networks proposed in the 1980s became viable only after 2010. However, slowing Moore's Law (transistor doubling now takes 3-4 years) drives innovation in specialized accelerators (TPUs provide 15-30x gains over GPUs through custom ML hardware) and algorithmic efficiency (techniques like quantization and pruning reduce compute requirements 4-10x). The systems lesson: when general hardware improvements slow, specialized hardware and efficient algorithms become critical.\n\nEmail spam filtering evolution illustrates this transformation. Early rule-based systems used explicit patterns but exhibited the same brittleness we saw with symbolic AI systems, proving easily circumvented. Statistical systems took a different approach: if the word 'viagra' appears in 90% of spam emails but only 1% of normal emails, we can use this pattern to identify spam. Rather than writing explicit rules, statistical systems learn these patterns automatically from thousands of example emails, making them adaptable to new spam techniques. The mathematical foundation relies on Bayes' theorem to calculate the probability that an email is spam given specific words: $P(\\text{spam}|\\text{word}) = P(\\text{word}|\\text{spam}) \\times P(\\text{spam}) / P(\\text{word})$. For emails with multiple words, we combine these probabilities across the entire message assuming conditional independence of words given the class (spam or not spam), which allows efficient computation despite the simplifying assumption that words don't depend on each other.\n\n::: {.callout-example title=\"Early Spam Detection Systems\"}\n```\nRule-based (1980s):\nIF contains(\"viagra\") OR contains(\"winner\") THEN spam\n\nStatistical (1990s):\nP(spam|word) = (frequency in spam emails) / (total frequency)\n\nCombined using Naive Bayes:\nP(spam|email) ∝ P(spam) × ∏ P(word|spam)\n```\n:::\n\nStatistical approaches introduced three concepts that remain central to AI development. First, the quality and quantity of training data became as important as the algorithms themselves. AI could only learn patterns that were present in its training examples. Second, rigorous evaluation methods became necessary to measure AI performance, leading to metrics that could measure success and compare different approaches. Third, a tension exists between precision (being right when making a prediction) and recall (catching all the cases we should find), forcing designers to make explicit trade-offs based on their application's needs. These challenges require systematic approaches: @sec-data-engineering covers data quality and drift detection, while @sec-benchmarking-ai addresses evaluation metrics and precision-recall trade-offs. Spam filters might tolerate some spam to avoid blocking important emails, while medical diagnosis systems prioritize catching every potential case despite increased false alarms.\n\n@tbl-ai-evolution-strengths summarizes the evolutionary journey of AI approaches, highlighting key strengths and capabilities emerging with each paradigm. Moving from left to right reveals important trends. Before examining shallow and deep learning, understanding trade-offs between existing approaches provides important context.\n\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Aspect**             | **Symbolic AI**          | **Expert Systems**       | **Statistical Learning** | **Shallow / Deep Learning**  |\n+:=======================+:=========================+:=========================+:=========================+:=============================+\n| **Key Strength**       | Logical reasoning        | Domain expertise         | Versatility              | Pattern recognition          |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Best Use Case**      | Well-defined, rule-based | Specific domain problems | Various structured data  | Complex, unstructured data   |\n|                        | problems                 |                          | problems                 | problems                     |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Data Handling**      | Minimal data needed      | Domain knowledge-based   | Moderate data required   | Large-scale data processing  |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Adaptability**       | Fixed rules              | Domain-specific          | Adaptable to various     | Highly adaptable to diverse  |\n|                        |                          | adaptability             | domains                  | tasks                        |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n| **Problem Complexity** | Simple, logic-based      | Complicated, domain-     | Complex, structured      | Highly complex, unstructured |\n|                        |                          | specific                 |                          |                              |\n+------------------------+--------------------------+--------------------------+--------------------------+------------------------------+\n\n: **AI Paradigm Evolution**: Shifting from symbolic AI to statistical approaches transformed machine learning by prioritizing data quantity and quality, enabling rigorous performance evaluation, and necessitating explicit trade-offs between precision and recall to optimize system behavior for specific applications. The table outlines how each paradigm addressed these challenges, revealing a progression towards data-driven systems capable of handling complex, real-world problems. {#tbl-ai-evolution-strengths}\n\nThis analysis bridges early approaches with recent developments in shallow and deep learning. It explains why certain approaches gained prominence in different eras and how each paradigm built upon predecessors while addressing their limitations. Earlier approaches continue to influence modern AI techniques, particularly in foundation model development.\n\nThese core concepts that emerged from statistical learning (data quality, evaluation metrics, and precision-recall trade-offs) became the foundation for all subsequent developments in machine learning.\n\n### Shallow Learning Era {#sec-introduction-shallow-learning-era-2500}\n\nBuilding on these statistical foundations, the 2000s marked a significant period in machine learning history known as the \"shallow learning\" era. The term \"shallow\" refers to architectural depth: shallow learning typically employed one or two processing levels, contrasting with deep learning's multiple hierarchical layers that emerged later.\n\nDuring this time, several algorithms dominated the machine learning landscape. Each brought unique strengths to different problems: Decision trees[^fn-decision-trees] provided interpretable results by making choices much like a flowchart. K-nearest neighbors made predictions by finding similar examples in past data, like asking your most experienced neighbors for advice. Linear and logistic regression offered straightforward, interpretable models that worked well for many real-world problems. Support Vector Machines[^fn-svms] (SVMs) excelled at finding complex boundaries between categories using the \"kernel trick\"[^fn-kernel-trick]. This technique transforms complex patterns by projecting data into higher dimensions where linear separation becomes possible. These algorithms formed the foundation of practical machine learning.\n\n[^fn-decision-trees]: **Decision Trees**: A machine learning algorithm that makes predictions by following a series of yes/no questions, much like a flowchart. Popularized in the 1980s, decision trees are highly interpretable—you can trace exactly why the algorithm made each decision. From a systems perspective, decision trees require minimal memory and compute compared to neural networks: a typical decision tree model might be 1-10MB versus 100MB-10GB for deep learning models, with inference taking microseconds on a single CPU core. This makes them ideal for resource-constrained deployments where model size matters more than maximum accuracy—embedded systems, mobile devices, or scenarios requiring real-time decisions with minimal latency. They remain widely used in medical diagnosis and loan approval where regulations require explainability.\n\n[^fn-svms]: **Support Vector Machines (SVMs)**: A powerful machine learning algorithm developed by Vladimir Vapnik in the 1990s that finds the optimal boundary between different categories of data. SVMs were the dominant technique for many classification problems before deep learning emerged, winning numerous machine learning competitions. From a systems perspective, SVMs excel with small datasets (thousands of examples vs millions needed for deep learning), requiring less training infrastructure—a high-end workstation can train SVMs that would require GPU clusters for equivalent deep learning models. However, SVMs don't scale well beyond ~100K data points due to O(n²) to O(n³) training complexity, limiting their use for massive modern datasets. They remain deployed in text classification, bioinformatics, and scenarios where data is limited but accuracy is crucial.\n\n[^fn-kernel-trick]: **Kernel Trick**: A mathematical technique that allows algorithms like SVMs to find complex, non-linear patterns by transforming data into higher-dimensional spaces where linear separation becomes possible. For example, data points that form a circle in 2D space can be projected into 3D space where they become linearly separable. From a systems view, the kernel trick trades memory for computation efficiency: precomputing kernel matrices requires O(n²) memory, limiting SVMs to datasets under ~100K points on typical hardware (a 100K×100K matrix with 8-byte entries requires 80GB RAM). This memory constraint explains why deep learning, despite requiring more computation, scales better to massive datasets—neural networks' memory requirements grow linearly with data size, not quadratically.\n\nA typical computer vision solution from 2005 exemplifies this approach:\n\n::: {.callout-example title=\"Traditional Computer Vision Pipeline\"}\n```\n1. Manual Feature Extraction\n  - SIFT (Scale-Invariant Feature Transform)\n  - HOG (Histogram of Oriented Gradients)\n  - Gabor filters\n2. Feature Selection/Engineering\n3. \"Shallow\" Learning Model (e.g., SVM)\n4. Post-processing\n```\n:::\n\nThis era's hybrid approach combined human-engineered features with statistical learning. They had strong mathematical foundations (researchers could prove why they worked). They performed well even with limited data. They were computationally efficient. They produced reliable, reproducible results.\n\nThe Viola-Jones algorithm [@viola2001rapidobject][^fn-viola-jones] (2001) exemplifies this era, achieving real-time face detection using simple rectangular features and cascaded classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade.\n\n[^fn-viola-jones]: **Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in real-time by using simple rectangular patterns (like comparing the brightness of eye regions versus cheek regions) and making decisions in stages, filtering out non-faces quickly and spending more computation only on promising candidates. The algorithm achieved real-time performance (24 fps) on 2001 hardware by computing features in <0.001ms using integral images—a clever preprocessing technique that enables constant-time rectangle sum computation. This efficiency enabled embedded camera deployment in consumer devices (digital cameras, phones), demonstrating how algorithm-hardware co-design enables new applications. The cascade approach reduced computation 10-100x by rejecting easy negatives early, making real-time vision feasible on CPUs that would be 1000x slower than modern GPUs.\n\n[^fn-cascade]: **Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quickly rejecting obvious non-matches and passing promising candidates to the next, more sophisticated stage. This approach is similar to how security screening works at airports with multiple checkpoints of increasing thoroughness. From a systems perspective, cascades achieve 10-100x computational savings by focusing expensive computation only on promising candidates—early stages might reject 95% of inputs with 1% of total computation. This compute-saving pattern appears throughout edge ML systems where power budgets matter: modern mobile face detection uses neural network cascades that process most frames with tiny networks (<1MB), escalating to larger networks (>10MB) only for ambiguous cases, enabling continuous face detection on milliwatt power budgets.\n\n### Deep Learning Era {#sec-introduction-deep-learning-era-f6c0}\n\nWhile Support Vector Machines excelled at finding complex category boundaries through mathematical transformations, deep learning adopted a different approach inspired by brain architecture. Rather than relying on human-engineered features, deep learning employs layers of simple computational units inspired by brain neurons, with each layer transforming input data into increasingly abstract representations. While @sec-dl-primer establishes the mathematical foundations of neural networks, @sec-dnn-architectures explores the detailed architectures that enable this layered learning approach.\n\nIn image processing, this layered approach works systematically. The first layer detects simple edges and contrasts, subsequent layers combine these into basic shapes and textures, higher layers recognize specific features like whiskers and ears, and final layers assemble these into concepts like \"cat.\"\n\nUnlike shallow learning methods requiring carefully engineered features, deep learning networks automatically discover useful features from raw data. This layered approach to learning, building from simple patterns to complex concepts, defines \"deep\" learning and proves effective for complex, real-world data like images, speech, and text.\n\nAlexNet, shown in @fig-alexnet, achieved a breakthrough in the 2012 ImageNet[^fn-intro-imagenet] competition that transformed machine learning through a perfect alignment of algorithmic innovation and hardware capability. The network required two NVIDIA GTX 580 GPUs, with 60 million parameters trained on 1.2 million images. Training consumed approximately 1,287 GPU-hours over 6 days, achieving 15.3% top-5 error rate compared to 26.2% for second place, a 42% relative improvement. The parallel architecture of GPUs proved naturally suited to the matrix operations underlying neural network computation, enabling 10-100x speedups over CPU implementations and reducing training time from months to days. This demonstrated that specialized hardware could unlock previously intractable algorithms when algorithm and hardware capabilities were aligned [@krizhevsky2012imagenet].\n\n[^fn-intro-imagenet]: **ImageNet**: A massive visual database containing over 14 million labeled images across 21,841 categories (full dataset), created by Stanford's Fei-Fei Li starting in 2009 [@deng2009imagenet]. The annual ImageNet challenge became the Olympics of computer vision, driving breakthrough after breakthrough in image recognition until neural networks became so good they essentially solved the competition. From a systems perspective, ImageNet's ~150GB size (2009) was manageable on single-server storage systems. Modern vision datasets like LAION-5B (5 billion image-text pairs, ~240TB of images) require distributed storage infrastructure and parallel data loading pipelines during training. This 1000x growth in dataset size drove innovations in distributed data engineering—systems must now shard datasets across dozens of storage nodes and coordinate parallel data loading to keep thousands of GPUs fed with training examples.\n\nThe success of AlexNet wasn't just a technical achievement; it was a watershed moment that demonstrated the practical viability of deep learning. This breakthrough required both algorithmic innovation and systems engineering advances. The achievement wasn't just algorithmic, it was enabled by framework infrastructure like Theano that could orchestrate GPU parallelism, handle automatic differentiation at scale, and manage the complex computational workflows that deep learning demands. Without these framework foundations, the algorithmic insights would have remained computationally intractable.\n\nThis pattern of requiring both algorithmic and systems breakthroughs has defined every major AI advance since. Modern frameworks represent infrastructure that transforms algorithmic possibilities into practical realities. Automatic differentiation (autograd) systems represent perhaps the most important innovation that makes modern deep learning possible, handling gradient computation automatically and enabling the complex architectures we use today. Understanding this framework-centric perspective (that major AI capabilities emerge from the intersection of algorithms and systems engineering) is important for building robust, scalable machine learning systems. This single result triggered an explosion of research and applications in deep learning that continues to this day. The infrastructure requirements that enabled this breakthrough represent the convergence of algorithmic innovation with systems engineering that this book explores.\n\n::: {#fig-alexnet fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\clip (-11.2,-2) rectangle (15.5,5.45);\n%\\draw[red](-11.2,-1.7) rectangle (15.5,5.45);\n\\tikzset{%\n LineD/.style={line width=0.7pt,black!50,dashed,dash pattern=on 3pt off 2pt},\n  LineG/.style={line width=0.75pt,GreenLine},\n  LineR/.style={line width=0.75pt,RedLine},\n  LineA/.style={line width=0.75pt,BrownLine,-latex,text=black}\n}\n\\newcommand\\FillCube[4]{\n\\def\\depth{#2}\n\\def\\width{#3}\n\\def\\height{#4}\n\\def\\nc{#1}\n% Lower front left corner\n\\coordinate (A\\nc) at (0, 0);\n% Donji prednji desni\n\\coordinate (B\\nc) at (\\width, 0);\n% Upper front right\n\\coordinate (C\\nc) at (\\width, \\height);\n% Upper front left\n\\coordinate (D\\nc) at (0, \\height);\n% Pomak u \"dubinu\"\n\\coordinate (shift) at (-0.7*\\depth, \\depth);\n% Last points (moved)\n\\coordinate (E\\nc) at ($(A\\nc) + (shift)$);\n\\coordinate (F\\nc) at ($(B\\nc) + (shift)$);\n\\coordinate (G\\nc) at ($(C\\nc) + (shift)$);\n\\coordinate (H\\nc) at ($(D\\nc) + (shift)$);\n% Front side\n\\draw[GreenLine,fill=green!08,line width=0.5pt] (A\\nc) -- (B\\nc) -- (C\\nc) --(D\\nc) -- cycle;\n% Top side\n\\draw[GreenLine,fill=green!20,line width=0.5pt] (D\\nc) -- (H\\nc) -- (G\\nc) -- (C\\nc);\n% Left\n\\draw[GreenLine,fill=green!15] (A\\nc) -- (E\\nc) -- (H\\nc)--(D\\nc)--cycle;\n\\draw[] (E\\nc) -- (H\\nc);\n\\draw[GreenLine,line width=0.75pt](A\\nc)--(B\\nc)--(C\\nc)--(D\\nc)--(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--(H\\nc);\n}\n%%%\n\\newcommand\\SmallCube[4]{\n\\def\\nc{#1}\n\\def\\depth{#2}\n\\def\\width{#3}\n\\def\\height{#4}\n\\coordinate (A\\nc) at (0, 0);\n\\coordinate (B\\nc) at (\\width, 0);\n\\coordinate (C\\nc) at (\\width, \\height);\n\\coordinate (D\\nc) at (0, \\height);\n\\coordinate (shift) at (-0.7*\\depth, \\depth);\n\\coordinate (E\\nc) at ($(A\\nc) + (shift)$);\n\\coordinate (F\\nc) at ($(B\\nc) + (shift)$);\n\\coordinate (G\\nc) at ($(C\\nc) + (shift)$);\n\\coordinate (H\\nc) at ($(D\\nc) + (shift)$);\n\\draw[RedLine,fill=red!08,line width=0.5pt,fill opacity=0.7] (A\\nc) -- (B\\nc) -- (C\\nc) -- (D\\nc) -- cycle;\n\\draw[RedLine,fill=red!20,line width=0.5pt,fill opacity=0.7] (D\\nc) -- (H\\nc) -- (G\\nc) -- (C\\nc);\n\\draw[RedLine,fill=red!15,fill opacity=0.7] (A\\nc) -- (E\\nc) -- (H\\nc)--(D\\nc)--cycle;\n\\draw[] (E\\nc) -- (H\\nc);\n}\n%%%%%%%%%%%%%%%%%%%%%\n%%4 column\n%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}\n%big cube\n\\begin{scope}\n\\FillCube{4VD}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.10,0.4)},line width=0.5pt]\n\\SmallCube{4MD}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{4VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{13} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{13} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(0,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{4VG}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.18,0.55)}]\n\\SmallCube{4MG}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n\\def\\nc{4VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%\n%%5 column\n%%%%\n%%small cube\n\\begin{scope}[shift={(4.15,0)}]\n%big cube\n\\begin{scope}\n\\FillCube{5VD}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.10,1.25)}]\n\\SmallCube{5MD}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{5VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{13} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{13} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(4.15,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{5VG}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.08,0.28)}]\n\\SmallCube{5MG}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{5VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%3 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-3.75,-0.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{3VD}{1.5}{2.33}{3}\n\\end{scope}\n%%small cube-down\n\\begin{scope}[shift={(-0.10,0.45)}]\n\\SmallCube{3MDI}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\end{scope}\n%%small cube - up\n\\begin{scope}[shift={(-0.12,2.23)}]\n\\SmallCube{3MDII}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1,pos=0.4]{27} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{27} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(-3.75,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{3VG}{1.5}{2.33}{3}\n\\end{scope}\n%%small cube-down\n\\begin{scope}[shift={(-0.42,0.75)}]\n\\SmallCube{3MGI}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VG}\n\\draw[GreenLine,line width=0.75pt](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n%%small cube-up\n\\begin{scope}[shift={(-0.06,0.18)}]\n\\SmallCube{3MGII}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%2 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-6.8,-1)}]\n%big cube\n\\begin{scope}\n\\FillCube{2VD}{2}{1.3}{3.8}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.2,2.5)}]\n\\SmallCube{2MD}{0.4}{1.3}{1}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{5}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{5}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{2VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{48} (B\\nc)--\n(C\\nc)--(D\\nc)--node[pos=0.6,right,text=black,text opacity=1]{55} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[pos=0.26,right,text=black,text opacity=1]{55} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(-6.8,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{2VG}{2}{1.3}{3.8}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.1,0.5)}]\n\\SmallCube{2MG}{0.4}{1.3}{1}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{5}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{5}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{2VG}\n\\draw[LineG](A\\nc)--node[above,text=black]{48} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%1 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-9.0,-1.2)}]\n%big cube\n\\begin{scope}\n\\FillCube{1VD}{2}{0.2}{4.55}\n\\end{scope}\n%%small cube=down\n\\begin{scope}[shift={(-0.25,0.5)}]\n\\SmallCube{1MDI}{0.8}{0.15}{1.7}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--\n(C\\nc)--(D\\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{1VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{3} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--node[below left,text=black]{224}(E\\nc)--\nnode[left,text=black,text opacity=1]{224}(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)-- (H\\nc);\n\\end{scope}\n%%small cube=up\n\\begin{scope}[shift={(-0.75,3.4)}]\n\\SmallCube{1MDII}{0.8}{0.15}{1.7}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--\n(C\\nc)--(D\\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{1VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{3} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--node[below left,text=black]{224}(E\\nc)--\nnode[left,text=black,text opacity=1]{224}(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)-- (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%\n\\begin{scope}[shift={(8.15,0)}]\n\\begin{scope}\n\\FillCube{6VD}{0.8}{2.0}{2}\n\\path(A6VD)--node[below]{128}(B6VD);\n\\path(A6VD)--node[right]{13}(D6VD);\n\\path(D6VD)--node[right]{13}(H6VD);\n\\end{scope}\n%up\n\\begin{scope}[shift={(0,3.5)}]\n\\FillCube{6VG}{0.8}{2.0}{2}\n\\path(A6VG)--node[below]{128}(B6VG);\n\\end{scope}\n\\end{scope}\n\n\\newcommand\\Boxx[3]{\n\\node[draw,LineG,fill=green!10,rectangle,minimum width=7mm,minimum height=#2](#1){};\n\\node[below=2pt of #1]{#3};\n}\n\\begin{scope}[shift={(11.7,1.0)}]\n \\Boxx{B1D}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(11.7,5.25)}]\n \\Boxx{B1G}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(13.5,1.0)}]\n \\Boxx{B2D}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(13.5,5.25)}]\n \\Boxx{B2G}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(15.0,1.0)}]\n \\Boxx{B3}{19mm}{1000}\n\\end{scope}\n%%%\n\\node[right=3pt of B1VD,align=center]{Stride\\\\ of 4};\n\\node[right=3pt of B2VD,align=center]{Max\\\\ pooling};\n\\node[right=3pt of B3VD,align=center]{Max\\\\ pooling};\n\\node[below=3pt of B6VD,align=center]{Max\\\\ pooling};\n%\n\\coordinate(1C2)at($(A2VD)!0.4!(D2VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 1MDI)--(1C2);\n}\n\\coordinate(2C2)at($(E2VG)!0.2!(H2VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 1MDII)--(2C2);\n}\n%3\n\\coordinate(1C3)at($(A3VD)!0.55!(H3VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 2MD)--(1C3);\n}\n\\coordinate(2C3)at($(A3MGI)!0.35!(D3MGI)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 2MG)--(2C3);\n}\n%4\n\\coordinate(1C4)at($(A4VG)!0.15!(D4VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MGI)--(1C4);\n}\n\\coordinate(2C4)at($(G4MD)!0.15!(H4MD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MGII)--(2C4);\n}\n\\coordinate(3C4)at($(A4MG)!0.5!(C4MG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MDII)--(3C4);\n}\n\\coordinate(3C4)at($(A4VD)!0.12!(D4VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MDI)--(3C4);\n}\n%5\n\\coordinate(1C5)at($(A5MG)!0.82!(H5MG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 4MG)--(1C5);\n}\n\\coordinate(2C5)at($(A5VD)!0.52!(C5VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 4MD)--(2C5);\n}\n%6\n\\coordinate(1C6)at($(A6VG)!0.52!(C6VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 5MG)--(1C6);\n}\n\\coordinate(1C6)at($(D6VD)!0.3!(B6VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 5MD)--(1C6);\n}\n%\n\\draw[LineA]($(B6VD)!0.52!(C6VD)$)coordinate(X1)--\nnode[below]{dense}(X1-|B1D.north west);\n\\draw[LineA](B1D)--node[below]{dense}(B2D);\n\\draw[LineA](B2D)--(B3);\n%\n\\draw[LineA]($(B6VG)!0.52!(C6VG)$)coordinate(X1)--(X1-|B1G.north west);\n\\draw[LineA]($(B6VG)!0.52!(C6VG)$)--(B1D);\n\\draw[LineA]($(B6VD)!0.52!(C6VD)$)--(B1G);\n\\draw[LineA](B1D)--(B2G);\n\\draw[LineA](B1G)--(B2D);\n\\draw[LineA](B2G)--node[right]{dense}(B3);\n\\draw[LineA]($(B1G.north east)!0.7!(B1G.south east)$)--($(B2G.north west)!0.7!(B2G.south west)$);\n\\end{tikzpicture}\n```\n**Convolutional Neural Network Architecture**: AlexNet demonstrated that deep neural networks could automatically learn effective features from images, dramatically outperforming traditional computer vision methods. This breakthrough showed that with sufficient data and computing power, neural networks could achieve remarkable accuracy in image recognition tasks.\n:::\n\nDeep learning subsequently entered an era of extraordinary scale. By the late 2010s, companies like Google, Facebook, and OpenAI trained neural networks thousands of times larger than AlexNet. These massive models, often called \"foundation models\"[^fn-intro-foundation-models], expanded deep learning capabilities to new domains.\n\n[^fn-intro-foundation-models]: **Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundation\" for many different applications through fine-tuning, like GPT for language tasks or CLIP for vision tasks. The term was coined by Stanford's AI researchers in 2021 to capture how these models became the basis for building more specific AI systems. From a systems perspective, foundation models' size (10-100GB for inference, 350GB+ for training) creates deployment challenges—organizations must often choose between accuracy (deploying the full model requiring expensive GPU servers) and feasibility (using distilled versions that fit on less expensive hardware). This trade-off drives the emergence of model-as-a-service architectures where companies like OpenAI provide API access rather than distributing models, shifting infrastructure costs to centralized providers.\n\nGPT-3, released in 2020 [@brown2020language], marked a milestone in demonstrating what scale could achieve. With 175 billion parameters requiring approximately 350GB to store (800GB+ for full training infrastructure), it represented a 1,000x scale increase from earlier neural networks like BERT-Large[^fn-bert-large] (340 million parameters). Training consumed approximately 314 zettaFLOPs[^fn-zettaflops] of computation across 1,024 V100 GPUs[^fn-v100-gpus] over several weeks, with training costs estimated at $4.6 million. The model demonstrated remarkable emergent abilities that appeared only at scale: writing human-like text, engaging in sophisticated conversation, and writing functional computer code. Subsequent models have grown substantially larger, but the systems engineering challenges GPT-3 introduced, coordinating thousands of accelerators, managing massive memory requirements, and serving models at scale, remain representative of modern infrastructure needs.\n\n[^fn-bert-large]: **BERT-Large**: A transformer-based language model developed by Google in 2018 with 340 million parameters, representing the previous generation of large language models before the GPT era. BERT (Bidirectional Encoder Representations from Transformers) was revolutionary for understanding context in both directions of a sentence, but GPT-3's 175 billion parameters dwarfed it by over 500x, marking the transition to truly large-scale language models.\n\n[^fn-zettaflops]: **ZettaFLOPs**: A measure of computational performance equal to one sextillion (10^21) floating-point operations per second. Training GPT-3 required approximately 3.14 × 10^23 FLOPS (roughly 314 zettaFLOPs), which would theoretically take 355 years on a single V100 GPU. This massive computational requirement illustrates why modern AI training requires distributed systems with thousands of GPUs working in parallel.\n\n[^fn-v100-gpus]: **V100 GPUs**: NVIDIA's data center graphics processing units designed specifically for AI training, featuring 32GB of high-bandwidth memory (HBM2) and 125 TFLOPS of mixed-precision deep learning performance. Each V100 cost approximately $8,000-$10,000 (2020 pricing), making the 1,024 GPUs used for GPT-3 training worth roughly $8-10 million in hardware alone, highlighting the enormous infrastructure investment required for cutting-edge AI research.\n\nA key insight emerged: larger neural networks trained on more data became capable of solving increasingly complex tasks. This scale introduced significant systems challenges[^fn-training-challenges]. Efficiently training large models requires thousands of parallel GPUs, storing and serving models hundreds of gigabytes in size, and handling massive training datasets.\n\n[^fn-training-challenges]: **Large-Scale Training Challenges**: Training GPT-3 required approximately 3,640 petaflop-days. At $2-3 per GPU-hour on cloud platforms (2020 pricing), this translates to approximately $4.6M in compute costs alone (Lambda Labs estimate), excluding data preprocessing, experimentation, and failed training runs [@li2020estimating]. Rule of thumb: total project cost is typically 3-5x raw compute cost due to experimentation overhead, making the full GPT-3 development cost approximately $15-20M. Modern foundation models can consume 100+ terabytes of training data and require specialized distributed training techniques to coordinate thousands of accelerators across multiple data centers.\n\nThe 2012 deep learning revolution built upon neural network research dating to the 1950s. The story begins with Frank Rosenblatt's Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. Though limited to linearly separable problems, as Minsky and Papert's 1969 book \"Perceptrons\" [@minsky1969perceptrons] demonstrated, it introduced the core concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation [@rumelhart1986learning] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using specialized neural networks designed for image processing [@lecun1989backpropagation][^fn-cnn].\n\n[^fn-cnn]: **Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing images, inspired by how the human visual system works. The \"convolutional\" part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene. From a systems perspective, CNNs' parameter sharing reduces model size 10-100x compared to fully-connected networks processing the same images—a CNN might use 5-10 million parameters where a fully-connected network would need 500 million. This dramatic reduction makes CNNs deployable on mobile devices: MobileNetV2 achieves 70% ImageNet accuracy in just 14MB (3.5M parameters), enabling on-device image recognition that would be impossible with fully-connected networks requiring gigabytes of storage and memory.\n\nThese networks largely stagnated through the 1990s and 2000s not because the ideas were incorrect, but because they preceded necessary technological developments. The field lacked three important ingredients: sufficient data to train complex networks, enough computational power to process this data, and the technical innovations needed to train very deep networks effectively.\n\nDeep learning's potential required the convergence of the three AI Triad components we will explore: sufficient data to train complex networks, enough computational power to process this data, and algorithmic breakthroughs needed to train very deep networks effectively. This extended development period explains why the 2012 ImageNet breakthrough represented the culmination of accumulated research rather than a sudden revolution. This evolution established machine learning systems engineering as a discipline bridging theoretical advancements with practical implementation, operating within the interconnected framework the AI Triad represents.\n\n{{< margin-video \"https://www.youtube.com/watch?v=FwFduRA_L6Q&ab_channel=YannLeCun\" \"Convolutional Network Demo from 1989\" \"Yann LeCun\" >}}\n\nThis evolution reveals a crucial insight: as AI progressed from symbolic reasoning to statistical learning and deep learning, applications became increasingly ambitious and complex. However, this growth introduced challenges extending beyond algorithms, necessitating engineering entire systems capable of deploying and sustaining AI at scale. Understanding how these modern ML systems operate in practice requires examining their lifecycle characteristics and deployment patterns, which distinguish them fundamentally from traditional software systems.\n\n## Understanding ML System Lifecycle and Deployment {#sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0}\n\nHaving traced AI's evolution from symbolic systems through statistical learning to deep learning, we can now explore how these modern ML systems operate in practice. Understanding the ML lifecycle and deployment landscape is important because these factors shape every engineering decision we make.\n\n### The ML Development Lifecycle {#sec-introduction-ml-development-lifecycle-05d8}\n\nML systems fundamentally differ from traditional software in their development and operational lifecycle. Traditional software follows predictable patterns where developers write explicit instructions that execute deterministically[^fn-deterministic]. These systems build on decades of established practices: version control maintains precise code histories, continuous integration pipelines[^fn-ci-cd] automate testing, and static analysis tools measure quality. This mature infrastructure enables reliable software development following well-defined engineering principles.\n\n[^fn-deterministic]: **Deterministic Execution**: Traditional software produces the same output every time given the same input, like a calculator that always returns 4 when adding 2+2. This predictability makes testing straightforward—you can verify correct behavior by checking that specific inputs produce expected outputs. ML systems, by contrast, are probabilistic: the same model might produce slightly different predictions due to randomness in inference or changes in underlying data patterns.\n\n[^fn-ci-cd]: **Continuous Integration/Continuous Deployment (CI/CD)**: Automated systems that continuously test code changes and deploy them to production. When developers commit code, CI/CD pipelines automatically run tests, check for errors, and if everything passes, deploy the changes to users. For traditional software, this works reliably; for ML systems, it's more complex because you must also validate data quality, model performance, and prediction distribution—not just code correctness.\n\nMachine learning systems depart from this paradigm. While traditional systems execute explicit programming logic, ML systems derive their behavior from data patterns discovered through training. This shift from code to data as the primary behavior driver introduces complexities that existing software engineering practices cannot address. These challenges require specialized workflows that @sec-ai-workflow addresses.\n\n@fig-ml_lifecycle_overview illustrates how ML systems operate in continuous cycles rather than traditional software's linear progression from design through deployment.\n\n::: {#fig-ml_lifecycle_overview fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{\n  Box/.style={inner xsep=2pt,\n  draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    anchor=west,\n    text width=20mm,align=flush center,\n    minimum width=20mm, minimum height=8mm\n  },\n Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},\n  Text/.style={inner sep=4pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor!70,\n    font=\\fontsize{8pt}{9}\\selectfont\\usefont{T1}{phv}{m}{n},\n    align=flush center,\n    minimum width=7mm, minimum height=5mm\n  },\n}\n\n\\node[Box](B1){ Data\\\\ Preparation};\n\\node[Box,node distance=15mm,right=of B1,fill=RedL,draw=RedLine](B2){Model\\\\ Evaluation};\n\\node[Box,node distance=32mm,right=of B2,fill=VioletL,draw=VioletLine](B3){Model \\\\ Deployment};\n\\node[Box,node distance=9mm,above=of $(B1)!0.5!(B2)$,\nfill=BackColor!60!yellow!90,draw=BackLine](GB){Model\\\\ Training};\n\\node[Box,node distance=9mm,below left=1.1 and 0 of B1.south west,\nfill=BlueL,draw=BlueLine](DB1){Data\\\\ Collection};\n\\node[Box,node distance=9mm,below right=1.1 and 0 of B3.south east,\nfill=OrangeL,draw=OrangeLine](DB2){Model \\\\Monitoring};\n\\draw[Line](B2)--node[Text,pos=0.5]{Meets\\\\ Requirements}(B3);\n\\draw[Line](B2)--++(270:1.2)-|node[Text,pos=0.25]{Needs\\\\ Improvement}(B1);\n\\draw[Line](DB2)--node[Text,pos=0.25]{Performance\\\\Degrades}(DB1);\n\\draw[Line](DB1)|-(B1);\n\\draw[Line](B1)|-(GB);\n\\draw[Line](GB)-|(B2);\n\\draw[Line](B3)-|(DB2);\n\\end{tikzpicture}\n```\n**ML System Lifecycle**: Continuous iteration defines successful machine learning systems, requiring feedback loops to refine models and address performance degradation across data collection, model training, evaluation, and deployment. This cyclical process contrasts with traditional software development and emphasizes the importance of ongoing monitoring and adaptation to maintain system reliability and accuracy in dynamic environments.\n:::\n\nThe data-dependent nature of ML systems creates dynamic lifecycles requiring continuous monitoring and adaptation. Unlike source code that changes only through developer modifications, data reflects real-world dynamics. Distribution shifts can silently alter system behavior without any code changes. Traditional tools designed for deterministic code-based systems prove insufficient for managing such data-dependent systems: version control excels at tracking discrete code changes but struggles with large, evolving datasets; testing frameworks designed for deterministic outputs require adaptation for probabilistic predictions. These challenges require specialized practices: @sec-data-engineering addresses data versioning and quality management, while @sec-ml-operations covers monitoring approaches that handle probabilistic behaviors rather than deterministic outputs.\n\nIn production, lifecycle stages create either virtuous or vicious cycles. Virtuous cycles emerge when high-quality data enables effective learning, robust infrastructure supports efficient processing, and well-engineered systems facilitate better data collection. Vicious cycles occur when poor data quality undermines learning, inadequate infrastructure hampers processing, and system limitations prevent data collection improvements—with each problem compounding the others.\n\n### The Deployment Spectrum {#sec-introduction-deployment-spectrum-06a1}\n\nManaging machine learning systems' complexity varies across different deployment environments, each presenting unique constraints and opportunities that shape lifecycle decisions.\n\nAt one end of the spectrum, cloud-based ML systems run in massive data centers[^fn-data-centers]. These systems, including large language models and recommendation engines, process petabytes of data while serving millions of users simultaneously. They leverage virtually unlimited computing resources but manage enormous operational complexity and costs. The architectural approaches for building such large-scale systems are covered in @sec-ml-systems and @sec-ai-acceleration.\n\n[^fn-data-centers]: **Data Centers**: Massive facilities housing thousands of servers, often consuming 100-300 megawatts of power, equivalent to a small city. Google operates over 20 data centers globally, each one costing $1-2 billion to build. These facilities maintain temperatures of exactly 80°F (27°C) with backup power systems that can run for days, enabling the reliable operation of AI services used by billions of people worldwide.\n\nAt the other end, TinyML systems run on microcontrollers[^fn-microcontrollers] and embedded devices, performing ML tasks with severe memory, computing power, and energy consumption constraints. Smart home devices like Alexa or Google Assistant must recognize voice commands using less power than LED bulbs, while sensors must detect anomalies on battery power for months or years. The specialized techniques for deploying ML on such constrained devices are explored in @sec-efficient-ai and @sec-model-optimizations.\n\n[^fn-microcontrollers]: **Microcontrollers**: Tiny computers-on-a-chip costing under $1 each, with just kilobytes of memory, about 1/millionth the memory of a smartphone. Popular chips like the Arduino Uno have only 32KB of storage and 2KB of RAM, yet can run simple AI models that classify sensor data, recognize voice commands, or detect movement patterns while consuming less power than a digital watch.\n\nBetween these extremes lies a rich variety of ML systems adapted for different contexts. Edge ML systems bring computation closer to data sources, reducing latency[^fn-latency] and bandwidth requirements while managing local computing resources. Mobile ML systems must balance sophisticated capabilities with severe constraints: modern smartphones typically have 4-12GB RAM, ARM processors operating at 1.5-3 GHz, and power budgets of 2-5 watts that must be shared across all system functions. For example, running a state-of-the-art image classification model on a smartphone might consume 100-500mW and complete inference in 10-100ms, compared to cloud servers that can use 200+ watts but deliver results in under 1ms. Enterprise ML systems often operate within specific business constraints, focusing on particular tasks while integrating with existing infrastructure. Some organizations employ hybrid approaches, distributing ML capabilities across multiple tiers to balance various requirements.\n\n[^fn-latency]: **Latency**: The time delay between when a request is made and when a response is received. In ML systems, this is critical: autonomous vehicles need <10ms latency for safety decisions, while voice assistants target <100ms for natural conversation. For comparison, sending data to a distant cloud server typically adds 50-100ms, which is why edge computing became essential for real-time AI applications.\n\n### How Deployment Shapes the Lifecycle {#sec-introduction-deployment-shapes-lifecycle-3531}\n\nThe deployment spectrum we've outlined represents more than just different hardware configurations. Each deployment environment creates an interplay of requirements, constraints, and trade-offs that impact every stage of the ML lifecycle, from initial data collection through continuous operation and evolution.\n\nPerformance requirements often drive initial architectural decisions. Latency-sensitive applications, like autonomous vehicles or real-time fraud detection, might require edge or embedded architectures despite their resource constraints. Conversely, applications requiring massive computational power for training, such as large language models, naturally gravitate toward centralized cloud architectures. However, raw performance is just one consideration in a complex decision space.\n\nResource management varies dramatically across architectures and directly impacts lifecycle stages. Cloud systems must optimize for cost efficiency at scale, balancing expensive GPU clusters, storage systems, and network bandwidth. This affects training strategies (how often to retrain models), data retention policies (what historical data to keep), and serving architectures (how to distribute inference load). Edge systems face fixed resource limits that constrain model complexity and update frequency. Mobile and embedded systems operate under the strictest constraints, where every byte of memory and milliwatt of power matters, forcing aggressive model compression[^fn-model-compression] and careful scheduling of training updates.\n\n[^fn-model-compression]: **Model Compression**: Techniques for reducing a model's size and computational requirements while preserving accuracy. Common approaches include quantization (using 8-bit integers instead of 32-bit floats, reducing model size by 4x), pruning (removing connections with minimal impact, potentially achieving 90% sparsity), and knowledge distillation (training a small \"student\" model to mimic a large \"teacher\" model). These techniques can shrink a 500MB model to 50MB while losing only 1-2% accuracy, making deployment on smartphones and embedded devices feasible.\n\nOperational complexity increases with system distribution, creating cascading effects throughout the lifecycle. While centralized cloud architectures benefit from mature deployment tools and managed services, edge and hybrid systems must handle distributed system management complexity. This manifests across all lifecycle stages: data collection requires coordination across distributed sensors with varying connectivity; version control must track models deployed across thousands of edge devices; evaluation needs to account for varying hardware capabilities; deployment must handle staged rollouts with rollback capabilities; and monitoring must aggregate signals from geographically distributed systems. The systematic approaches to operational excellence, including incident response and debugging methodologies for production ML systems, are thoroughly addressed in @sec-ml-operations.\n\nData considerations introduce competing pressures that reshape lifecycle workflows. Privacy requirements or data sovereignty regulations might push toward edge or embedded architectures where data stays local, fundamentally changing data collection and training strategies—perhaps requiring federated learning[^fn-federated-learning] approaches where models train on distributed data without centralization. Yet the need for large-scale training data might favor cloud approaches with centralized data aggregation. The velocity and volume of data also influence architectural choices: real-time sensor data might require edge processing to manage bandwidth during collection, while batch analytics might be better suited to cloud processing with periodic model updates.\n\n[^fn-federated-learning]: **Federated Learning**: A training approach where the model learns from data distributed across many devices without centralizing the data. For example, your smartphone's keyboard learns your typing patterns locally and only shares model updates (not your actual messages) with the cloud. This technique, pioneered by Google in 2016, enables privacy-preserving ML by keeping sensitive data on-device while still benefiting from collective learning across millions of users.\n\nEvolution and maintenance requirements must be considered from the initial design. Cloud architectures offer flexibility for system evolution with easy model updates and A/B testing[^fn-ab-testing], but can incur significant ongoing costs. Edge and embedded systems might be harder to update (requiring over-the-air updates[^fn-ota-updates] with careful bandwidth management), but could offer lower operational overhead. The continuous cycle of ML systems—collect data, train models, evaluate performance, deploy updates, monitor behavior—becomes particularly challenging in distributed architectures, where updating models and maintaining system health requires careful orchestration across multiple tiers.\n\n[^fn-ab-testing]: **A/B Testing**: A method of comparing two versions of a system by showing version A to some users and version B to others, then measuring which performs better. In ML systems, this might mean deploying a new model to 5% of users while keeping 95% on the old model, comparing metrics like accuracy or user engagement before fully rolling out the new version. This gradual rollout strategy helps catch problems before they affect all users.\n\n[^fn-ota-updates]: **Over-the-Air (OTA) Updates**: Wireless software updates delivered remotely to devices, like how your smartphone installs new apps without physical connection. For ML systems on embedded devices or vehicles, OTA updates enable deploying improved models to thousands or millions of devices without manual intervention. However, updating a 500MB neural network over cellular networks to a fleet of vehicles requires careful bandwidth management and rollback capabilities if updates fail.\n\nThese trade-offs are rarely simple binary choices. Modern ML systems often adopt hybrid approaches, balancing these considerations based on specific use cases and constraints. For instance, an autonomous vehicle might perform real-time perception and control at the edge for latency reasons, while uploading data to the cloud for model improvement and downloading updated models periodically. A voice assistant might do wake-word detection on-device to preserve privacy and reduce latency, but send full speech to the cloud for complex natural language processing.\n\nThe key insight is understanding how deployment decisions ripple through the entire system lifecycle. A choice to deploy on embedded devices doesn't just constrain model size, it affects data collection strategies (what sensors are feasible), training approaches (whether to use federated learning), evaluation metrics (accuracy vs. latency vs. power), deployment mechanisms (over-the-air updates), and monitoring capabilities (what telemetry can be collected). These interconnected decisions demonstrate the AI Triad framework in practice, where constraints in one component create cascading effects throughout the system.\n\nWith this understanding of how ML systems operate across their lifecycle and deployment spectrum, we can now examine concrete examples that illustrate these principles in action. The case studies that follow demonstrate how different deployment choices create distinct engineering challenges and solutions across the system lifecycle.\n\n## Case Studies in Real-World ML Systems {#sec-introduction-case-studies-realworld-ml-systems-a2ba}\n\nHaving established the AI Triad framework, lifecycle stages, and deployment spectrum, we can now examine these principles operating in real-world systems. Rather than surveying multiple systems superficially, we focus on one representative case study, autonomous vehicles, that illustrates the spectrum of ML systems engineering challenges across all three components, multiple lifecycle stages, and complex deployment constraints.\n\n### Case Study: Autonomous Vehicles {#sec-introduction-case-study-autonomous-vehicles-6f86}\n\n[Waymo](https://waymo.com/), a subsidiary of Alphabet Inc., stands at the forefront of autonomous vehicle technology, representing one of the most ambitious applications of machine learning systems to date. Evolving from the Google Self-Driving Car Project initiated in 2009, Waymo's approach to autonomous driving exemplifies how ML systems can span the entire spectrum from embedded systems to cloud infrastructure. This case study demonstrates the practical implementation of complex ML systems in a safety-critical, real-world environment, integrating real-time decision-making with long-term learning and adaptation.\n\n#### Data Considerations {#sec-introduction-data-considerations-bdc4}\n\nThe data ecosystem underpinning Waymo's technology is vast and dynamic. Each vehicle serves as a roving data center, its sensor suite, which comprises LiDAR[^fn-lidar], radar[^fn-radar], and high-resolution cameras, generating approximately one terabyte of data per hour of driving. This real-world data is complemented by an even more extensive simulated dataset, with Waymo's vehicles having traversed over 20 billion miles in simulation and more than 20 million miles on public roads. The challenge lies not just in the volume of data, but in its heterogeneity and the need for real-time processing. Waymo must handle both structured (e.g., GPS coordinates) and unstructured data (e.g., camera images) simultaneously. The data pipeline spans from edge processing on the vehicle itself to massive cloud-based storage and processing systems. Sophisticated data cleaning and validation processes are necessary, given the safety-critical nature of the application. The representation of the vehicle's environment in a form amenable to machine learning presents significant challenges, requiring complex preprocessing to convert raw sensor data into meaningful features that capture the dynamics of traffic scenarios.\n\n[^fn-lidar]: **LiDAR (Light Detection and Ranging)**: A sensor that uses laser pulses to measure distances, creating detailed 3D maps of surroundings by measuring how long light takes to bounce back from objects. A spinning LiDAR sensor might emit millions of laser pulses per second, detecting objects up to 200+ meters away with centimeter-level precision. While highly accurate, LiDAR sensors can cost $75,000+ (though prices are dropping) and struggle in heavy rain or fog where water droplets scatter the laser light.\n\n[^fn-radar]: **Radar (Radio Detection and Ranging)**: A sensor that uses radio waves to detect objects and measure their distance and velocity. Unlike LiDAR, radar works well in rain, fog, and darkness, making it essential for all-weather autonomous driving. Automotive radar operates at 77 GHz frequency, detecting vehicles up to 250 meters away and measuring their speed with high accuracy—critical for safely navigating highways. Modern vehicles use multiple radar units costing $150-300 each.\n\n#### Algorithmic Considerations {#sec-introduction-algorithmic-considerations-b99f}\n\nWaymo's ML stack represents a sophisticated ensemble of algorithms tailored to the multifaceted challenge of autonomous driving. The perception system employs specialized neural networks to process visual data for object detection and tracking. Prediction models, needed for anticipating the behavior of other road users, use neural networks that can understand patterns over time[^fn-rnn] in road user behavior. Building such complex multi-model systems requires the architectural patterns from @sec-dnn-architectures and the framework infrastructure covered in @sec-ai-frameworks. Waymo has developed custom ML models like VectorNet for predicting vehicle trajectories. The planning and decision-making systems may incorporate learning-from-experience techniques to handle complex traffic scenarios.\n\n[^fn-rnn]: **Sequential Neural Networks**: Neural network architectures designed to process data that occurs in sequences over time, such as predicting where a pedestrian will move next based on their previous movements. These networks maintain a form of \"memory\" of previous inputs to inform current decisions.\n\n#### Infrastructure Considerations {#sec-introduction-infrastructure-considerations-248a}\n\nThe computing infrastructure supporting Waymo's autonomous vehicles epitomizes the challenges of deploying ML systems across the full spectrum from edge to cloud. Each vehicle is equipped with a custom-designed compute platform capable of processing sensor data and making decisions in real-time, often leveraging specialized hardware like GPUs or tensor processing units (TPUs)[^fn-tpu]. This edge computing is complemented by extensive use of cloud infrastructure, leveraging the power of Google's data centers for training models, running large-scale simulations, and performing fleet-wide learning. Such systems demand specialized hardware architectures (@sec-ai-acceleration) and edge-cloud coordination strategies (@sec-ml-systems) to handle real-time processing at scale. The connectivity between these tiers is critical, with vehicles requiring reliable, high-bandwidth communication for real-time updates and data uploading. Waymo's infrastructure must be designed for robustness and fault tolerance, ensuring safe operation even in the face of hardware failures or network disruptions. The scale of Waymo's operation presents significant challenges in data management, model deployment, and system monitoring across a geographically distributed fleet of vehicles.\n\n[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom AI accelerator chip designed specifically for neural network operations, named after \"tensors\" (multi-dimensional arrays used in deep learning). First revealed in 2016, TPUs can perform matrix multiplications up to 15-30x faster than contemporary GPUs for AI workloads while using less power. A single TPU v4 pod can provide 1.1 exaflops of computing power—roughly equivalent to 10,000 high-end GPUs—enabling training of massive language models in days rather than months.\n\n#### Future Implications {#sec-introduction-future-implications-c2f2}\n\nWaymo's impact extends beyond technological advancement, potentially revolutionizing transportation, urban planning, and numerous aspects of daily life. The launch of Waymo One, a commercial ride-hailing service using autonomous vehicles in Phoenix, Arizona, represents a significant milestone in the practical deployment of AI systems in safety-critical applications. Waymo's progress has broader implications for the development of robust, real-world AI systems, driving innovations in sensor technology, edge computing, and AI safety that have applications far beyond the automotive industry. However, it also raises important questions about liability, ethics, and the interaction between AI systems and human society. As Waymo continues to expand its operations and explore applications in trucking and last-mile delivery, it serves as an important test bed for advanced ML systems, driving progress in areas such as continual learning, robust perception, and human-AI interaction. The Waymo case study underscores both the tremendous potential of ML systems to transform industries and the complex challenges involved in deploying AI in the real world.\n\n### Contrasting Deployment Scenarios {#sec-introduction-contrasting-deployment-scenarios-653a}\n\nWhile Waymo illustrates the full complexity of hybrid edge-cloud ML systems, other deployment scenarios present different constraint profiles. [FarmBeats](https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/), a Microsoft Research project for agricultural IoT, operates at the opposite end of the spectrum—severely resource-constrained edge deployments in remote locations with limited connectivity. FarmBeats demonstrates how ML systems engineering adapts to constraints: simpler models that can run on low-power microcontrollers, innovative connectivity solutions using TV white spaces, and local processing that minimizes data transmission. The challenges include maintaining sensor reliability in harsh conditions, validating data quality with limited human oversight, and updating models on devices that may be offline for extended periods.\n\nConversely, [AlphaFold](https://deepmind.google/technologies/alphafold/) [@jumper2021highly] represents purely cloud-based scientific ML where computational resources are essentially unlimited but accuracy is paramount. AlphaFold's protein structure prediction required training on 128 TPUv3 cores for weeks, processing hundreds of millions of protein sequences from multiple databases. The systems challenges differ markedly from Waymo or FarmBeats: managing massive training datasets (the Protein Data Bank contains over 180,000 structures), coordinating distributed training across specialized hardware, and validating predictions against experimental ground truth. Unlike Waymo's latency constraints or FarmBeats' power constraints, AlphaFold prioritizes computational throughput to explore vast search spaces—training costs exceeded $100,000 but enabled scientific breakthroughs.\n\nThese three systems—Waymo (hybrid, latency-critical), FarmBeats (edge, resource-constrained), and AlphaFold (cloud, compute-intensive)—illustrate how deployment environment shapes every engineering decision. The fundamental three-component framework applies to all, but the specific constraints and optimization priorities differ dramatically. Understanding this deployment diversity is essential for ML systems engineers, as the same algorithmic insight may require entirely different system implementations depending on operational context.\n\nWith concrete examples established, we can now examine the challenges that emerge across different deployment scenarios and lifecycle stages.\n\n\\medskip\n## Core Engineering Challenges in ML Systems {#sec-introduction-core-engineering-challenges-ml-systems-6482}\n\nThe Waymo case study and comparative deployment scenarios reveal how the AI Triad framework creates interdependent challenges across data, algorithms, and infrastructure. We've already established how ML systems differ from traditional software in their failure patterns and performance degradation. Now we can examine the specific challenge categories that emerge from this difference.\n\n### Data Challenges {#sec-introduction-data-challenges-2b0d}\n\nThe foundation of any ML system is its data, and managing this data introduces several core challenges that can silently degrade system performance. Data quality emerges as the primary concern: real-world data is often messy, incomplete, and inconsistent. Waymo's sensor suite must contend with environmental interference (rain obscuring cameras, LiDAR reflections from wet surfaces), sensor degradation over time, and data synchronization across multiple sensors capturing information at different rates. Unlike traditional software where input validation can catch malformed data, ML systems must handle ambiguity and uncertainty inherent in real-world observations.\n\nScale represents another critical dimension. Waymo generates approximately one terabyte per vehicle per hour—managing this data volume requires sophisticated infrastructure for collection, storage, processing, and efficient access during training. The challenge isn't just storing petabytes of data, but maintaining data quality metadata, version control for datasets, and efficient retrieval for model training. As systems scale to thousands of vehicles across multiple cities, these data management challenges compound exponentially.\n\nPerhaps most serious is data drift[^fn-drift], the gradual change in data patterns over time that silently degrades model performance. Waymo's models encounter new traffic patterns, road configurations, weather conditions, and driving behaviors that weren't present in training data. A model trained primarily on Phoenix driving might perform poorly when deployed in New York due to distribution shift: denser traffic, more aggressive drivers, different road layouts. Unlike traditional software where specifications remain constant, ML systems must adapt as the world they model evolves.\n\n[^fn-drift]: **Data Drift**: The gradual change in the statistical properties of input data over time, which can degrade model performance if not properly monitored and addressed through retraining or model updates.\n\nThis adaptation requirement introduces an important constraint that is often overlooked. While ML systems can generalize to unseen situations through learned statistical patterns, once trained, the model's learned behavior becomes fixed. The model cannot modify its understanding during deployment; it can only apply the patterns it learned during training. When distribution shift occurs, the model follows these outdated learned patterns just as deterministic code follows outdated rules. If construction zones triple in frequency, or new vehicle types appear regularly, the model's fixed responses may prove no more appropriate than hardcoded logic written for a different operational context. The advantage of ML emerges not from runtime adaptation but from the capacity to retrain with new data, a process requiring deliberate engineering intervention.\n\nDistribution shift manifests through multiple pathways. Seasonal variations affect sensor performance through changing sun angles and precipitation patterns. Infrastructure modifications alter road layouts. Urban growth evolves traffic patterns. Each shift can degrade specific model components: pedestrian detection accuracy may decline in winter conditions, while lane following confidence may decrease on newly repaved roads. Detecting these shifts requires continuous monitoring of input distributions and model performance across operational contexts.\n\nThe systematic approaches to managing these data challenges (quality assurance, versioning, drift detection, and remediation strategies) are covered in @sec-data-engineering. The key insight is that data challenges in ML systems are continuous and dynamic, requiring ongoing engineering attention rather than one-time solutions.\n\n### Model Challenges {#sec-introduction-model-challenges-eef4}\n\nCreating and maintaining the ML models themselves presents another set of challenges. Modern ML models, particularly in deep learning, can be complex. Consider a language model like GPT-3, which has hundreds of billions of parameters that need to be optimized through training processes[^fn-backprop]. This complexity creates practical challenges: these models require enormous computing power to train and run, making it difficult to deploy them in situations with limited resources, like on mobile phones or IoT devices.\n\n[^fn-backprop]: **Backpropagation**: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers.\n\nTraining these models effectively is itself a significant challenge. Unlike traditional programming where we write explicit instructions, ML models learn from examples. This learning process involves many architectural and hyperparameter choices: How should we structure the model? How long should we train it? How can we tell if it's learning the right patterns rather than memorizing training data? Making these decisions often requires both technical expertise and considerable trial and error.\n\nModern practice increasingly relies on transfer learning—reusing models developed for one task as starting points for related tasks. Rather than training a new image recognition model from scratch, practitioners might start with a model pre-trained on millions of images and adapt it to their specific domain (say, medical imaging or agricultural monitoring). This approach dramatically reduces both the training data and computation required, but introduces new challenges around ensuring the pre-trained model's biases don't transfer to the new application. These training challenges—transfer learning, distributed training, and bias mitigation—require systematic approaches that @sec-ai-training explores, building on the framework infrastructure from @sec-ai-frameworks.\n\nA particularly important challenge is ensuring that models work well in real-world conditions beyond their training data. This generalization gap, the difference between training performance and real-world performance, represents a central challenge in machine learning. A model might achieve 99% accuracy on its training data but only 75% accuracy in production due to subtle distribution differences. For important applications like autonomous vehicles or medical diagnosis systems, understanding and minimizing this gap becomes necessary for safe deployment.\n\n### System Challenges {#sec-introduction-system-challenges-0dc0}\n\nGetting ML systems to work reliably in the real world introduces its own set of challenges. Unlike traditional software that follows fixed rules, ML systems need to handle uncertainty and variability in their inputs and outputs. They also typically need both training systems (for learning from data) and serving systems (for making predictions), each with different requirements and constraints.\n\nConsider a company building a speech recognition system. They need infrastructure to collect and store audio data, systems to train models on this data, and then separate systems to actually process users' speech in real-time. Each part of this pipeline needs to work reliably and efficiently, and all the parts need to work together seamlessly. The engineering principles for building such robust data pipelines are covered in @sec-data-engineering, while the operational practices for maintaining these systems in production are explored in @sec-ml-operations.\n\nThese systems also need constant monitoring and updating. How do we know if the system is working correctly? How do we update models without interrupting service? How do we handle errors or unexpected inputs? These operational challenges become particularly complex when ML systems are serving millions of users.\n\n### Ethical Considerations {#sec-introduction-ethical-considerations-d6a5}\n\nAs ML systems become more prevalent in our daily lives, their broader impacts on society become increasingly important to consider. One major concern is fairness, as ML systems can sometimes learn to make decisions that discriminate against certain groups of people. This often happens unintentionally, as the systems pick up biases present in their training data. For example, a job application screening system might inadvertently learn to favor certain demographics if those groups were historically more likely to be hired. Detecting and mitigating such biases requires careful auditing of both training data and model behavior across different demographic groups.\n\nAnother important consideration is transparency and interpretability. Many modern ML models, particularly deep learning models with millions or billions of parameters, function as black boxes—systems where we can observe inputs and outputs but struggle to understand the internal reasoning. Like a radio that receives signals and produces sound without most users understanding the electronics inside, these models make predictions through complex mathematical transformations that resist human interpretation. A deep neural network might correctly diagnose a medical condition from an X-ray, but explaining why it reached that diagnosis—which visual features it considered most important—remains challenging. This opacity becomes particularly problematic when ML systems make consequential decisions affecting people's lives in domains like healthcare, criminal justice, or financial services, where stakeholders reasonably expect explanations for decisions that impact them.\n\nPrivacy is also a major concern. ML systems often need large amounts of data to work effectively, but this data might contain sensitive personal information. How do we balance the need for data with the need to protect individual privacy? How do we ensure that models do not inadvertently memorize and reveal private information through inference attacks[^fn-inference]? These challenges are not merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment. Addressing these concerns requires integrated approaches: fairness and bias detection, privacy-preserving techniques, inference attack mitigation, and system resilience under adversarial conditions.\n\n[^fn-inference]: **Inference Attack**: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training.\n\n### Understanding Challenge Interconnections {#sec-introduction-understanding-challenge-interconnections-3d30}\n\nAs the Waymo case study illustrates, challenges cascade and compound across the AI Triad. Data quality issues (sensor noise, distribution shift) degrade model performance. Model complexity constraints (latency budgets, power limits) force architectural compromises that may affect fairness (simpler models might show more bias). System-level failures (over-the-air update problems) can prevent deployment of improved models that address ethical concerns.\n\nThis interdependency explains why ML systems engineering requires holistic thinking that considers the AI Triad components together rather than optimizing them independently. A decision to use a larger model for better accuracy creates ripple effects: more training data required, longer training times, higher serving costs, increased latency, and potentially more pronounced biases if the training data isn't carefully curated. Successfully navigating these trade-offs requires understanding how choices in one dimension affect others.\n\nThe challenge landscape also explains why many research models fail to reach production. Academic ML often focuses on maximizing accuracy on benchmark datasets, potentially ignoring practical constraints like inference latency, training costs, data privacy, or operational monitoring. Production ML systems must balance accuracy against deployment feasibility, operational costs, ethical considerations, and long-term maintainability. This gap between research priorities and production realities motivates this book's emphasis on systems engineering rather than pure algorithmic innovation.\n\nThese interconnected challenges, spanning data quality and model complexity to infrastructure scalability and ethical considerations, distinguish ML systems from traditional software engineering. The transition from algorithmic innovation to systems integration challenges, combined with the unique operational characteristics we've examined, establishes the need for a distinct engineering discipline. We call this emerging field AI Engineering.\n\n## Defining AI Engineering {#sec-introduction-defining-ai-engineering-b812}\n\nHaving explored the historical evolution, lifecycle characteristics, practical applications, and core challenges of machine learning systems, we can now formally establish the discipline that addresses these systems-level concerns.\n\n::: {.callout-definition title=\"AI Engineering\"}\n***AI Engineering*** is the engineering discipline focused on the _systems-level integration_ of machine learning _algorithms_, _data_, and _computational infrastructure_ to build and operate production systems that are _reliable_, _efficient_, and _scalable_.\n:::\n\nAs we've traced through AI's history, a fundamental transformation has occurred. While AI once encompassed symbolic reasoning, expert systems, and rule-based approaches, learning-based methods now dominate the field. When organizations build AI today, they build machine learning systems. Netflix's recommendation engine processes billions of viewing events to train models serving millions of subscribers. Waymo's autonomous vehicles run dozens of neural networks processing sensor data in real time. Training GPT-4 required coordinating thousands of GPUs across data centers, consuming megawatts of power. Modern AI is overwhelmingly machine learning: systems whose capabilities emerge from learning patterns in data.\n\nThis convergence makes \"AI Engineering\" the natural name for the discipline, even though this text focuses specifically on machine learning systems as its subject matter. The term reflects how AI is actually built and deployed in practice today.\n\nAI Engineering encompasses the complete lifecycle of building production intelligent systems. A breakthrough algorithm requires efficient data collection and processing, distributed computation across hundreds or thousands of machines, reliable service to users with strict latency requirements, and continuous monitoring and updating based on real-world performance. The discipline addresses fundamental challenges at every level: designing efficient algorithms for specialized hardware, optimizing data pipelines that process petabytes daily, implementing distributed training across thousands of GPUs, deploying models that serve millions of concurrent users, and maintaining systems whose behavior evolves as data distributions shift. Energy efficiency is not an afterthought but a first-class constraint alongside accuracy and latency. The physics of memory bandwidth limitations, the breakdown of Dennard scaling, and the energy costs of data movement shape every architectural decision from chip design to data center deployment.\n\nThis emergence of AI Engineering as a distinct discipline mirrors how Computer Engineering emerged in the late 1960s and early 1970s.[^fn-computer-engineering] As computing systems grew more complex, neither Electrical Engineering nor Computer Science alone could address the integrated challenges of building reliable computers. Computer Engineering emerged as a complete discipline bridging both fields. Today, AI Engineering faces similar challenges at the intersection of algorithms, infrastructure, and operational practices. While Computer Science advances machine learning algorithms and Electrical Engineering develops specialized AI hardware, neither discipline fully encompasses the systems-level integration, deployment strategies, and operational practices required to build production AI systems at scale.\n\n[^fn-computer-engineering]: The first accredited computer engineering degree program in the United States was established at Case Western Reserve University in 1971, marking the formalization of Computer Engineering as a distinct academic discipline.\n\nWith AI Engineering now formally defined as the discipline, the remainder of this text discusses the practice of building and operating machine learning systems. We use \"ML systems engineering\" throughout to describe this practice, the work of designing, deploying, and maintaining the machine learning systems that constitute modern AI. These terms refer to the same discipline: AI Engineering is what we call it, ML systems engineering is what we do.\n\nHaving established AI Engineering as a discipline, we can now organize its practice into a coherent framework that addresses the challenges we've identified systematically.\n\n## Organizing ML Systems Engineering: The Five-Pillar Framework {#sec-introduction-organizing-ml-systems-engineering-fivepillar-framework-524d}\n\nThe challenges we've explored, from silent performance degradation and data drift to model complexity and ethical concerns, reveal why ML systems engineering has emerged as a distinct discipline. The unique failure patterns we discussed earlier exemplify the need for specialized approaches: traditional software engineering practices cannot address systems that degrade quietly rather than failing obviously. These challenges cannot be addressed through algorithmic innovation alone; they require systematic engineering practices that span the entire system lifecycle from initial data collection through continuous operation and evolution.\n\nThis work organizes ML systems engineering around five interconnected disciplines that directly address the challenge categories we have identified. These pillars, illustrated in @fig-pillars, represent the core engineering capabilities required to bridge the gap between research prototypes and production systems capable of operating reliably at scale.\n\n![**ML System Lifecycle**: Machine learning systems engineering encompasses five interconnected disciplines that address the real-world challenges of building, deploying, and maintaining AI systems at scale. Each pillar represents critical engineering capabilities needed to bridge the gap between research prototypes and production systems.](images/png/book_pillars.png){#fig-pillars}\n\n### The Five Engineering Disciplines {#sec-introduction-five-engineering-disciplines-6eee}\n\nThe five-pillar framework shown in @fig-pillars emerged directly from the systems challenges that distinguish ML from traditional software. Each pillar addresses specific challenge categories while recognizing their interdependencies.\n\nAlternative organizational frameworks exist. One could organize by system component (data, model, infrastructure) or by lifecycle phase (development, deployment, operation). We chose the five-pillar structure because it aligns with how engineering teams are typically organized in industry, with specialized roles for data engineering, training infrastructure, deployment, operations, and responsible AI practices. Notably, the Ethics pillar ensures that responsible engineering is treated as an explicit discipline rather than distributed implicitly across other areas, where it might be overlooked under deadline pressure.\n\n**Data Engineering** (@sec-data-engineering) addresses the data-related challenges we identified: quality assurance, scale management, drift detection, and distribution shift. This pillar encompasses building robust data pipelines that ensure quality, handle massive scale, maintain privacy, and provide the infrastructure upon which all ML systems depend. For systems like Waymo, this means managing terabytes of sensor data per vehicle, validating data quality in real-time, detecting distribution shifts across different cities and weather conditions, and maintaining data lineage for debugging and compliance. The techniques covered include data versioning, quality monitoring, drift detection algorithms, and privacy-preserving data processing.\n\n**Training Systems** (@sec-ai-training) tackles the model-related challenges around complexity and scale. This pillar covers developing training systems that can manage large datasets and complex models while optimizing computational resource utilization across distributed environments. Modern foundation models require coordinating thousands of GPUs, implementing parallelization strategies, managing training failures and restarts, and balancing training costs against model quality. The chapter explores distributed training architectures, optimization algorithms, hyperparameter tuning at scale, and the frameworks that make large-scale training practical.\n\n**Deployment Infrastructure** (@sec-ml-operations) addresses system-related challenges around the training-serving divide and operational complexity. This pillar encompasses building reliable deployment infrastructure that can serve models at scale, handle failures gracefully, and adapt to evolving requirements in production environments. Deployment spans the full spectrum from cloud services handling millions of requests per second to edge devices operating under severe latency and power constraints. The techniques include model serving architectures, edge deployment optimization, A/B testing frameworks, and staged rollout strategies that minimize risk while enabling rapid iteration.\n\n**Operations and Monitoring** (@sec-ml-operations, @sec-benchmarking-ai) directly addresses the silent performance degradation patterns we identified as distinctive to ML systems. This pillar covers creating monitoring and maintenance systems that ensure continued performance, enable early issue detection, and support safe system updates in production. Unlike traditional software monitoring focused on infrastructure metrics, ML operations requires the four-dimensional monitoring we discussed: infrastructure health, model performance, data quality, and business impact. The chapter explores metrics design, alerting strategies, incident response procedures, debugging techniques for production ML systems, and continuous evaluation approaches that catch degradation before it impacts users.\n\n**Ethics and Governance** addresses the ethical and societal challenges around fairness, transparency, privacy, and safety. This pillar implements responsible AI practices throughout the system lifecycle rather than treating ethics as an afterthought. For safety-critical systems like autonomous vehicles, this includes formal verification methods, scenario-based testing, bias detection and mitigation, privacy-preserving learning techniques, and explainability approaches that support debugging and certification. The relevant chapters cover both technical methods (differential privacy, fairness metrics, interpretability techniques) and organizational practices (ethics review boards, incident response protocols, stakeholder engagement).\n\n### Connecting Components, Lifecycle, and Disciplines {#sec-introduction-connecting-components-lifecycle-disciplines-388b}\n\nThe five pillars emerge naturally from the AI Triad framework and lifecycle stages we established earlier. Each AI Triad component maps to specific pillars: Data Engineering handles the data component's full lifecycle; Training Systems and Deployment Infrastructure address how algorithms interact with infrastructure during different lifecycle phases; Operations bridges all components by monitoring their interactions; Ethics & Governance cuts across all components, ensuring responsible practices throughout.\n\nThe challenge categories we identified find their solutions within specific pillars: Data challenges → Data Engineering. Model challenges → Training Systems. System challenges → Deployment Infrastructure and Operations. Ethical challenges → Ethics & Governance. As we established with the AI Triad framework, these pillars must coordinate rather than operate in isolation.\n\nThis structure reflects how AI evolved from algorithm-centric research to systems-centric engineering, shifting focus from \"can we make this algorithm work?\" to \"can we build systems that reliably deploy, operate, and maintain these algorithms at scale?\" The five pillars represent the engineering capabilities required to answer \"yes.\"\n\n### Future Directions in ML Systems Engineering {#sec-introduction-future-directions-ml-systems-engineering-db3b}\n\nWhile these five pillars provide a stable framework for ML systems engineering, the field continues evolving. Understanding current trends helps anticipate how the core challenges and trade-offs will manifest in future systems.\n\nApplication-level innovation increasingly features agentic systems that move beyond reactive prediction to autonomous action. Systems that can plan, reason, and execute complex tasks introduce new requirements for decision-making frameworks and safety constraints. These advances don't eliminate the five pillars but increase their importance: autonomous systems that can take consequential actions require even more rigorous data quality, more reliable deployment infrastructure, more comprehensive monitoring, and stronger ethical safeguards.\n\nSystem architecture evolution addresses sustainability and efficiency concerns that have become critical as models scale. Innovation in model compression, efficient training techniques, and specialized hardware stems from both environmental and economic pressures. Future architectures must balance the pursuit of more powerful models against growing resource constraints. These efficiency innovations primarily impact Training Systems and Deployment Infrastructure pillars, introducing new techniques like quantization, pruning, and neural architecture search that optimize for multiple objectives simultaneously.\n\nInfrastructure advances continue reshaping deployment possibilities. Specialized AI accelerators are emerging across the spectrum from powerful data center chips to efficient edge processors. This heterogeneous computing landscape enables dynamic model distribution across tiers based on capabilities and conditions, blurring traditional boundaries between cloud, edge, and embedded systems. These infrastructure innovations affect how all five pillars operate—new hardware enables new algorithms, which require new training approaches, which demand new monitoring strategies.\n\nDemocratization of AI technology is making ML systems more accessible to developers and organizations of all sizes. Cloud providers offer pre-trained models and automated ML platforms that reduce the expertise barrier for deploying AI solutions. This accessibility trend doesn't diminish the importance of systems engineering—if anything, it increases demand for robust, reliable systems that can operate without constant expert oversight. The five pillars become even more critical as ML systems proliferate into domains beyond traditional tech companies.\n\nThese trends share a common theme: they create ML systems that are more capable and widespread, but also more complex to engineer reliably. The five-pillar framework provides the foundation for navigating this landscape, though specific techniques within each pillar will continue advancing.\n\n### The Nature of Systems Knowledge {#sec-introduction-nature-systems-knowledge-1c67}\n\nMachine learning systems engineering differs epistemologically from purely theoretical computer science disciplines. While fields like algorithms, complexity theory, or formal verification build knowledge through mathematical proofs and rigorous derivations, ML systems engineering is a practice, a craft learned through building, deploying, and maintaining systems at scale. This distinction becomes apparent in topics like MLOps, where you'll encounter fewer theorems and more battle-tested patterns that have emerged from production experience. The knowledge here isn't about proving optimal solutions exist but about recognizing which approaches work reliably under real-world constraints.\n\nThis practical orientation reflects ML systems engineering's nature as a systems discipline. Like other engineering fields—civil, electrical, mechanical—the core challenge lies in managing complexity and trade-offs rather than deriving closed-form solutions. You'll learn to reason about latency versus accuracy trade-offs, to recognize when data quality issues will undermine even sophisticated models, to anticipate how infrastructure choices propagate through entire system architectures. This systems thinking develops through experience with concrete scenarios, debugging production failures, and understanding why certain design patterns persist across different applications.\n\nThe implication for learning is significant: mastery comes through building intuition about patterns, understanding trade-off spaces, and recognizing how different system components interact. When you read about monitoring strategies or deployment architectures, the goal isn't memorizing specific configurations but developing judgment about which approaches suit which contexts. This book provides the frameworks, principles, and representative examples, but expertise ultimately develops through applying these concepts to real problems, making mistakes, and building the pattern recognition that distinguishes experienced systems engineers from those who only understand individual components.\n\n## The Structure of This Textbook {#sec-introduction-structure}\n\nThis textbook organizes around three imperatives: build, optimize, and deploy. The structure progresses from foundational concepts through model development to production deployment, following a key pedagogical principle of establishing context and process before theory. @tbl-vol1-structure summarizes the four-part organization.\n\n+--------------------+--------------------------------+------------------------------------------+\n| **Part**           | **Theme**                      | **Key Chapters**                         |\n+:===================+:===============================+:=========================================+\n| **I: Foundations** | Context: ML systems landscape  | Introduction, ML Systems,                |\n|                    |                                | Workflow, Data Engineering               |\n+--------------------+--------------------------------+------------------------------------------+\n| **II: Build**      | Theory: Model fundamentals     | Deep Learning Primer, DNN Architectures, |\n|                    |                                | Frameworks, Training                     |\n+--------------------+--------------------------------+------------------------------------------+\n| **III: Optimize**  | Efficiency: Performance tuning | Efficient AI, Optimizations,             |\n|                    |                                | Hardware Acceleration, Benchmarking      |\n+--------------------+--------------------------------+------------------------------------------+\n| **IV: Deploy**     | Production: Real-world systems | Serving, ML Operations,                  |\n|                    |                                | Responsible Engineering, Conclusion      |\n+--------------------+--------------------------------+------------------------------------------+\n\n: **Volume I Structure**: The four parts progress from understanding the ML landscape through building and optimizing models to deploying production systems. {#tbl-vol1-structure}\n\nPart I establishes context by surveying the ML systems landscape. The Introduction develops the engineering revolution in AI and the frameworks that organize this discipline. ML Systems examines what distinguishes ML systems from traditional software, introducing unique failure patterns and lifecycle stages. Workflow presents the end-to-end process from problem formulation through deployment, providing the conceptual map that guides subsequent learning. Data Engineering addresses data collection, processing, and management, establishing that data infrastructure precedes and enables model development.\n\nPart II builds theoretical foundations and practical skills for model development. The Deep Learning Primer provides algorithmic foundations, while DNN Architectures extends these to specific network designs. AI Frameworks examines the software infrastructure from TensorFlow and PyTorch to specialized tools. AI Training develops training systems for complex models and large datasets.\n\nPart III addresses optimization for production deployment. Efficient AI introduces techniques for reducing computational requirements while maintaining quality. Optimizations covers compression techniques including quantization, pruning, and knowledge distillation. Hardware Acceleration examines specialized hardware from GPUs to custom ASICs. Benchmarking establishes methodologies for measuring and comparing system performance.\n\nPart IV ensures optimized systems operate reliably in production. Serving covers infrastructure for delivering predictions at scale. ML Operations encompasses practices from monitoring and deployment to incident response. Responsible Engineering addresses ethical considerations and governance. The Conclusion synthesizes the complete methodology.\n\nFor detailed guidance on reading paths, learning outcomes, prerequisites, and how to maximize your experience with this textbook, refer to the [About](../../frontmatter/about/about.qmd) section.\n\nThis introduction has established the conceptual foundation for everything that follows. The chapter began by examining the relationship between artificial intelligence as vision and machine learning as methodology, then defined machine learning systems as the artifacts that engineers build: integrated computing systems comprising data, algorithms, and infrastructure. Through the Bitter Lesson and AI's historical evolution, the chapter demonstrated why systems engineering has become fundamental to AI progress and how learning-based approaches came to dominate the field. This context enabled a formal definition of AI Engineering as a distinct discipline, following the pattern of Computer Engineering's emergence, establishing it as the field dedicated to building reliable, efficient, and scalable machine learning systems across all computational platforms.\n\nYet this broad vision raises immediate questions. If ML systems differ fundamentally from traditional software, what makes them different? Why do they fail in ways that conventional engineering intuitions cannot anticipate? The next chapter, @sec-ml-systems, examines these questions systematically, revealing the characteristics that distinguish ML systems from their traditional counterparts and establishing why specialized engineering approaches are necessary.\n\nWelcome to AI Engineering.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"introduction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","introduction.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3,"quiz":"introduction_quizzes.json","concepts":"introduction_concepts.yml","glossary":"introduction_glossary.json"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}