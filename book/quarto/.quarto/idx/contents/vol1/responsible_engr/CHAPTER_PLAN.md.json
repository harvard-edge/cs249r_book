{"title":"Responsible Systems Chapter Plan","markdown":{"headingText":"Responsible Systems Chapter Plan","containsRefs":false,"markdown":"\n**Location:** `book/quarto/contents/vol1/responsible_systems/responsible_systems.qmd`\n**Status:** Planning\n**Target Length:** 15-20 pages (approximately 4,000-6,000 words)\n\n---\n\n## Chapter Purpose\n\nThis chapter introduces the **engineering mindset** around responsible ML systems development. Unlike Volume II's deep technical treatments of fairness metrics, differential privacy, and sustainability measurement, this chapter focuses on:\n\n1. **Why** responsible engineering matters (not just ethics, but system reliability and real-world impact)\n2. **What questions** practitioners should ask before deployment\n3. **Awareness** of costs, impacts, and failure modes unique to ML systems\n\nThis is a foundational mindset chapter, not a technical methods chapter.\n\n---\n\n## Proposed Structure\n\n### Front Matter\n\n```yaml\n---\nbibliography: responsible_systems.bib\nquiz: responsible_systems_quizzes.json\nconcepts: responsible_systems_concepts.yml\nglossary: responsible_systems_glossary.json\ncrossrefs: responsible_systems_xrefs.json\n---\n```\n\n### Cover Image\n- Use existing `cover_responsible_ai.png` from vol2/responsible_ai/images/png/\n- Or create a new simpler cover for Vol 1\n\n---\n\n## Section Outline\n\n### 1. Purpose Statement (unnumbered)\n\n**Core question:** _Why does responsible engineering practice extend beyond technical correctness to encompass the broader impacts of ML systems on users, organizations, and society?_\n\n**Key points:**\n- ML systems affect real people in ways traditional software does not\n- Technical correctness does not guarantee beneficial outcomes\n- Responsible engineering is a professional obligation, not an optional consideration\n- This chapter establishes the mindset; Volume II provides advanced technical methods\n\n### 2. Learning Objectives\n\n```markdown\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain why ML systems require responsibility considerations beyond traditional software engineering practices\n\n- Identify the unique failure modes of ML systems that make responsible engineering essential\n\n- Apply a structured questioning framework before deploying ML systems to production\n\n- Recognize the resource costs (computational, financial, environmental) of ML system decisions\n\n- Describe the role of documentation, transparency, and monitoring in responsible ML practice\n\n- Distinguish between foundational responsible engineering (this chapter) and advanced technical methods (Volume II)\n\n:::\n```\n\n### 3. Beyond Technical Correctness\n\n**Opening example:** The Amazon hiring algorithm case (2019)\n- System was technically optimal (minimized prediction error)\n- System was ethically disastrous (discriminated against women)\n- Illustrates: you can be algorithmically sound while producing harmful outcomes\n\n**Key concepts:**\n- ML systems learn from historical data, which may encode historical biases\n- Optimization objectives may not align with societal values\n- Silent failures: systems degrade without raising alarms\n\n**Contrast with traditional software:**\n- Traditional software: fails loudly with errors\n- ML systems: fail silently with degraded predictions\n- This silent failure mode demands proactive responsibility\n\n### 4. The Responsible Engineering Mindset\n\n**Core message:** Responsibility is not a separate concern but integrated into every engineering decision.\n\n**Framework: Questions to Ask Before Deployment**\n\n| Phase | Questions |\n|-------|-----------|\n| **Data** | Where did this data come from? Who is represented? Who is missing? |\n| **Training** | What are we optimizing for? What might we be implicitly penalizing? |\n| **Evaluation** | Does performance hold across different user groups? What edge cases exist? |\n| **Deployment** | Who will this system affect? What happens when it fails? |\n| **Monitoring** | How will we detect problems? Who reviews system behavior? |\n\n**Callout box:** The \"Pre-Flight Checklist\" concept\n- Pilots use checklists before every flight\n- ML engineers should use checklists before every deployment\n- Not bureaucracy, but professional discipline\n\n### 5. Understanding System Impacts\n\n**Three dimensions of impact:**\n\n**5.1 Impact on Users**\n- Who uses this system?\n- What decisions does it influence?\n- What happens when predictions are wrong?\n- Example: Medical diagnosis vs. movie recommendations (different stakes)\n\n**5.2 Impact on Organizations**\n- Regulatory compliance requirements\n- Reputational risk from system failures\n- Legal liability for biased outcomes\n- Example: Credit scoring regulations, GDPR requirements\n\n**5.3 Impact on Society**\n- Aggregate effects of widely deployed systems\n- Feedback loops that amplify biases\n- Environmental costs of computation\n- Example: Recommendation systems shaping information consumption\n\n### 6. Resource Awareness\n\n**Core message:** Every ML decision has resource costs. Responsible engineers understand these costs.\n\n**6.1 Computational Costs**\n- Training large models consumes significant energy\n- The brain comparison: 20 watts vs. megawatts for comparable tasks\n- Not about guilt, but about informed decision-making\n\n**6.2 Financial Costs**\n- Cloud GPU costs can exceed $30,000/month for large models\n- Total cost of ownership includes training, inference, monitoring, updates\n- Efficiency optimizations have real economic value\n\n**6.3 Environmental Costs**\n- Data centers consume significant electricity and water\n- Carbon footprint of training runs\n- Efficiency as environmental responsibility\n\n**Key insight:** The efficiency techniques from @sec-efficient-ai and @sec-model-optimizations are not just performance optimizations but responsible engineering practices.\n\n### 7. Documentation and Transparency\n\n**Why documentation matters:**\n- Reproducibility: Can someone else understand and verify your work?\n- Auditability: Can you explain decisions if questioned?\n- Maintenance: Can future engineers understand the system?\n\n**Model Cards** (brief introduction)\n- Standardized documentation for ML models\n- What the model does, how it was trained, known limitations\n- Example template or simplified version\n\n**Data Documentation**\n- Data sources and collection methods\n- Known biases or limitations\n- Processing steps applied\n\n### 8. Monitoring for Responsibility\n\n**Beyond performance metrics:**\n- Traditional monitoring: latency, throughput, error rates\n- Responsible monitoring: fairness across groups, drift detection, unexpected behaviors\n\n**What to watch for:**\n- Performance degradation over time\n- Different performance across user segments\n- Unexpected patterns in predictions\n- User complaints and feedback\n\n**Connection to ops chapter:** Links to @sec-ml-operations for detailed monitoring implementation\n\n### 9. When Things Go Wrong\n\n**Incident response basics:**\n- Having a plan before you need it\n- Knowing when to roll back\n- Communication with stakeholders\n- Learning from failures\n\n**The importance of humility:**\n- No system is perfect\n- Planning for failure is not pessimism but professionalism\n- Continuous improvement mindset\n\n### 10. Conclusion\n\n**Summary of key principles:**\n1. Technical correctness is necessary but not sufficient\n2. Ask questions at every phase of development\n3. Understand the impacts on users, organizations, and society\n4. Be aware of resource costs\n5. Document thoroughly\n6. Monitor proactively\n7. Plan for when things go wrong\n\n**Bridge to Volume II:**\n- This chapter established the mindset\n- Volume II provides advanced technical methods: fairness metrics, differential privacy, adversarial robustness, sustainability measurement\n- The principles here remain constant; the techniques continue to evolve\n\n---\n\n## Visual Elements\n\n### Tables\n\n1. **Questions to Ask Framework** (Section 4)\n   - 5 rows (Data, Training, Evaluation, Deployment, Monitoring)\n   - 2 columns (Phase, Key Questions)\n\n2. **Impact Dimensions** (Section 5)\n   - Could be a simple 3-column layout\n   - Users | Organizations | Society\n\n3. **Documentation Checklist** (Section 7)\n   - Simple checklist format\n   - What to document at each phase\n\n### Figures\n\n**Minimal figures recommended.** This is a mindset chapter, not a technical methods chapter.\n\nPossible options:\n1. **Chapter cover image** - Reuse or adapt from vol2/responsible_ai\n2. **Simple conceptual diagram** - Could show the relationship between technical correctness and responsible outcomes (optional)\n\n### Callout Boxes\n\n1. **Definition: Responsible ML Engineering** - Opening definition\n2. **Pre-Flight Checklist** - The checklist analogy (Section 4)\n3. **The Amazon Hiring Case** - Motivating example (Section 3)\n4. **Model Cards** - Brief introduction (Section 7)\n\n---\n\n## References Needed\n\n### Primary Sources (to add to responsible_systems.bib)\n\n1. **Amazon hiring algorithm case**\n   - Dastin, J. (2018). Amazon scrapped secret AI recruiting tool that showed bias against women. Reuters.\n\n2. **Model Cards**\n   - Mitchell, M., et al. (2019). Model Cards for Model Reporting. FAT* '19.\n\n3. **Hidden Technical Debt in ML Systems**\n   - Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems. NeurIPS.\n\n4. **ML system failures**\n   - Various case studies of ML system failures in production\n\n5. **Energy consumption of AI**\n   - Strubell, E., et al. (2019). Energy and Policy Considerations for Deep Learning in NLP. ACL.\n\n### Cross-References to Other Chapters\n\n- @sec-efficient-ai - Efficiency techniques\n- @sec-model-optimizations - Optimization methods\n- @sec-ml-operations - MLOps and monitoring\n- @sec-benchmarking-ai - Measurement practices\n- @sec-ai-training - Training considerations\n- @sec-data-engineering - Data pipeline considerations\n\n---\n\n## Glossary Terms to Define\n\n1. **Responsible AI** - Engineering discipline integrating ethical considerations into ML system design\n2. **Model Card** - Standardized documentation describing an ML model's capabilities and limitations\n3. **Silent Failure** - System degradation without explicit error signals\n4. **Fairness** (brief) - Equitable treatment across user groups (detailed in Volume II)\n5. **Data Bias** - Systematic errors in data that lead to unfair outcomes\n6. **Total Cost of Ownership** - Comprehensive cost including training, inference, operations, and maintenance\n\n---\n\n## Quiz Questions (Draft)\n\n1. Why can a technically optimal ML system still produce harmful outcomes?\n   - Answer: Because optimization objectives may not align with societal values, and historical data may encode biases\n\n2. What is the key difference between how traditional software fails and how ML systems fail?\n   - Answer: Traditional software fails loudly with errors; ML systems fail silently with degraded predictions\n\n3. Name three dimensions of impact that responsible ML engineers should consider.\n   - Answer: Impact on users, impact on organizations, impact on society\n\n4. Why is documentation important for responsible ML engineering?\n   - Answer: Reproducibility, auditability, and maintainability\n\n5. What should ML engineers monitor beyond traditional performance metrics?\n   - Answer: Fairness across groups, drift detection, unexpected behaviors\n\n---\n\n## Implementation Notes\n\n### Style Guidelines\n- Maintain academic textbook tone\n- Avoid em-dashes and LLM-style writing patterns\n- Use \"do not\" instead of \"don't\"\n- Keep sentences direct and clear\n- No excessive superlatives or praise\n\n### Length Targets\n- Purpose: 150-200 words\n- Each major section: 400-600 words\n- Total: 4,000-6,000 words\n\n### Connection to Volume I Flow\n- Follows ops chapter (natural progression: deploy, then operate responsibly)\n- Precedes conclusion (responsibility is the capstone before synthesis)\n- Does NOT duplicate Volume II content\n\n---\n\n## Next Steps\n\n1. [ ] Review and approve this plan\n2. [ ] Create bibliography file with key references\n3. [ ] Write Purpose statement and Learning Objectives\n4. [ ] Draft main sections\n5. [ ] Create tables\n6. [ ] Add cross-references to other Vol 1 chapters\n7. [ ] Review for style consistency\n8. [ ] Add glossary terms\n9. [ ] Create quiz questions\n\n---\n\n## Open Questions\n\n1. **Chapter title:** Keep \"Responsible Systems\" or change to \"Responsible Engineering\" or \"Engineering Responsibility\"?\n\n2. **Depth of Amazon example:** How detailed should the opening case study be?\n\n3. **Model Cards section:** Include a simplified template or just describe the concept?\n\n4. **Environmental costs:** How much emphasis? (Currently one subsection)\n\n5. **Cover image:** Create new or reuse from Volume II?\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"CHAPTER_PLAN.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}