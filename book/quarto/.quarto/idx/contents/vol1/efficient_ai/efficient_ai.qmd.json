{"title":"Efficient AI","markdown":{"yaml":{"bibliography":"efficient_ai.bib","quiz":"efficient_ai_quizzes.json","concepts":"efficient_ai_concepts.yml","glossary":"efficient_ai_glossary.json"},"headingText":"Efficient AI","headingAttr":{"id":"sec-efficient-ai","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n::: {layout-narrow}\n::: {.column-margin}\n*DALL·E 3 Prompt: A conceptual illustration depicting efficiency in artificial intelligence using a shipyard analogy. The scene shows a bustling shipyard where containers represent bits or bytes of data. These containers are being moved around efficiently by cranes and vehicles, symbolizing the streamlined and rapid information processing in AI systems. The shipyard is meticulously organized, illustrating the concept of optimal performance within the constraints of limited resources. In the background, ships are docked, representing different platforms and scenarios where AI is applied. The atmosphere should convey advanced technology with an underlying theme of sustainability and wide applicability.*\n:::\n\n\\noindent\n![](images/png/cover_efficient_ai.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_What key trade-offs shape the pursuit of efficiency in machine learning systems, and why must engineers balance competing objectives?_\n\nMachine learning system efficiency requires balancing trade-offs across algorithmic complexity, computational resources, and data utilization. Improvements in one dimension often degrade performance in others, creating engineering tensions that demand systematic approaches. Understanding these interdependent relationships enables engineers to design systems achieving maximum performance within practical constraints of time, energy, and cost.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain the interdependencies between algorithmic efficiency, compute efficiency, and data efficiency, and how optimizing one dimension affects the others\n\n- Apply scaling law relationships to determine compute-optimal resource allocation between model size and training data for fixed computational budgets\n\n- Differentiate between resource-constrained, data-limited, and temporal scaling regimes, and determine which regime applies to a given system design scenario\n\n- Compare efficiency priorities across cloud, edge, mobile, and TinyML deployment contexts, and justify optimization strategies based on context-specific constraints\n\n- Explain how pruning, quantization, and knowledge distillation reduce model complexity while maintaining accuracy, recognizing these techniques are detailed in subsequent chapters\n\n- Critique scaling-based approaches by identifying breakdown conditions such as data saturation, infrastructure bottlenecks, and diminishing returns\n\n- Analyze fundamental trade-offs between efficiency dimensions, including algorithmic complexity vs. compute requirements, compute efficiency vs. real-time performance, and data efficiency vs. model generalization\n\n- Assess the environmental impact of ML system efficiency choices and evaluate how efficiency improvements affect equitable access to AI capabilities\n\n:::\n\n## The Efficiency Imperative {#sec-efficient-ai-efficiency-imperative-d65c}\n\nThe preceding chapters established how to build machine learning systems: the mathematics of neural network learning, the architectural patterns from MLPs through Transformers, the frameworks that implement these designs, and the training processes that optimize millions of parameters. A trained model, however, is not a deployed model. Training produces weights; deployment demands systems that run within memory budgets, latency constraints, and power envelopes that production environments impose. The gap between what we can train and what we can deploy defines the efficiency imperative.\n\nMachine learning efficiency has evolved from an afterthought to a fundamental discipline as models transitioned from simple statistical approaches to complex, resource-intensive architectures. The gap between theoretical capabilities and practical deployment has widened significantly, creating efficiency constraints that determine system feasibility and scalability.\n\nLarge-scale language models exemplify this challenge. GPT-3 required training costs estimated at $4.6 million (Lambda Labs estimate) and energy consumption of 1,287 MWh [@Patterson_et_al_2021]. The operational requirements, including memory footprints exceeding 700GB for inference (350GB for half-precision), create deployment barriers in resource-constrained environments. These constraints reveal a fundamental tension between model expressiveness and system practicality that demands rigorous analysis and optimization strategies.\n\nEfficiency research extends beyond resource optimization to encompass the theoretical foundations of learning system design. Engineers must understand how algorithmic complexity, computational architectures, and data utilization strategies interact to determine system viability. These interdependencies create multi-objective optimization problems where improvements in one dimension frequently degrade performance in others.\n\nThis chapter establishes the framework for analyzing efficiency in machine learning systems within Part III's performance engineering curriculum. The efficiency principles here inform the optimization techniques in @sec-model-optimizations, where quantization and pruning methods realize algorithmic efficiency goals, the hardware acceleration strategies in @sec-ai-acceleration that maximize compute efficiency, and the measurement methodologies in @sec-benchmarking-ai for validating efficiency improvements.\n\n## Defining System Efficiency {#sec-efficient-ai-defining-system-efficiency-a4b7}\n\nConsider building a photo search application for a smartphone. You face three competing pressures: the model must be small enough to fit in memory (an algorithmic challenge), it must run fast enough on the phone's processor without draining the battery (a compute challenge), and it must learn from a user's personal photos without requiring millions of examples (a data challenge). Efficient AI is the discipline of navigating these interconnected trade-offs.\n\nAddressing these efficiency challenges requires coordinated optimization across three interconnected dimensions that determine system viability.\n\n::: {.callout-definition title=\"Machine Learning System Efficiency\"}\n\n***Machine Learning System Efficiency*** is the optimization of ML systems to minimize _computational_, _memory_, and _energy_ demands while maintaining performance, achieved through improvements in _algorithms_, _hardware utilization_, and _data usage_.\n\n:::\n\nUnderstanding these interdependencies is necessary for designing systems that achieve maximum performance within practical constraints. Examining how the three dimensions interact in practice reveals how scaling laws expose these constraints.\n\n### Efficiency Interdependencies {#sec-efficient-ai-efficiency-interdependencies-5d69}\n\nThe three efficiency dimensions are deeply intertwined, creating a complex optimization landscape. Algorithmic efficiency reduces computational requirements through better algorithms and architectures, though it may increase development complexity or require specialized hardware. Compute efficiency maximizes hardware utilization through optimized implementations and specialized processors, though it may limit model expressiveness or require specific algorithmic approaches. Data efficiency enables learning with fewer examples through improved training procedures and data utilization, though it may require more sophisticated algorithms or additional computational resources.\n\nA concrete example illustrates these interconnections through the design of a photo search application for smartphones. The system must fit in 2GB memory (compute constraint), achieve acceptable accuracy with limited training data (data constraint), and complete searches within 50ms (algorithmic constraint). Optimization of any single dimension in isolation proves inadequate:\n\n**Algorithmic Efficiency** focuses on the model architecture. Using a compact vision-language model with 50 million parameters instead of a billion-parameter model reduces memory requirements from 4GB to 200MB and cuts inference time from 2 seconds to 100 milliseconds. However, accuracy decreases from 92% to 85%, necessitating careful evaluation of trade-off acceptability.\n\n**Compute Efficiency** addresses hardware utilization. The optimized model runs efficiently on smartphone processors, consuming only 10% battery per hour. Techniques like 8-bit quantization reduce computation while maintaining quality, and batch processing[^fn-batch-processing] handles multiple queries simultaneously. However, these optimizations necessitate algorithmic modifications to support reduced precision operations.\n\n**Data Efficiency** shapes how the model learns. Rather than requiring millions of labeled image-text pairs, the system leverages pre-trained foundation models and adapts using only thousands of user-specific examples. Continuous learning from user interactions provides implicit feedback without explicit labeling. This data efficiency necessitates more sophisticated algorithmic approaches and careful management of computational resources during adaptation.\n\nSynergy between these dimensions produces emergent benefits: the smaller model (algorithmic efficiency) enables on-device processing (compute efficiency), which facilitates learning from private user data (data efficiency) without transmitting personal images to remote servers. This integration provides enhanced performance and privacy protection, demonstrating how efficiency enables capabilities unattainable with less efficient approaches.\n\nThese interdependencies appear across all deployment contexts, from cloud systems with abundant resources to edge devices with severe constraints. As illustrated in @fig-interdependece, understanding these relationships provides the foundation for examining how scaling laws reveal fundamental efficiency limits.\n\n::: {#fig-interdependece fig-env=\"figure\" fig-pos=\"htb\"}\n\n```{.tikz}\n\\scalebox{0.7}{%\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n},scale=1.25,line width=0.75pt]\n\\def\\firstcircle{(0,0) circle (1.5cm)}\n\\def\\secondcircle{(300:2cm) circle (1.5cm)}\n\\def\\thirdcircle{(0:2cm) circle (1.5cm)}\n%\n    \\begin{scope}[shift={(3cm,-5cm)}, fill opacity=0.5]\n        \\fill[cyan] \\firstcircle;\n        \\fill[purple!70] \\secondcircle;\n        \\fill[orange] \\thirdcircle;\n    \\end{scope}\n\n\\begin{scope}[shift={(3cm,-5cm)}]\n    \\draw[draw=none] \\firstcircle node[black,left,align=center] {Algorithmic\\\\ Efficiency};\n    \\draw[draw=none] \\secondcircle node [black,below,align=center] {Data\\\\ Efficiency};\n    \\draw[draw=none] \\thirdcircle node [black,right,align=center] {Compute\\\\ Efficiency};\n\\end{scope}\n\\end{tikzpicture}}\n\n```\n: **Efficiency Interdependencies**: The three efficiency dimensions (algorithmic, compute, and data) overlap and influence one another, creating systemic trade-offs in machine learning systems. Optimizing for one efficiency dimension often requires careful consideration of its impact on the others, shaping overall system performance and resource utilization.\n:::\n\n[^fn-batch-processing]: **Batch Processing**: Processing multiple inputs together to amortize computational overhead and maximize GPU utilization. Mobile vision models achieve 3-5× speedup with batch size 8 vs. individual processing, but introduces 50-200ms latency as queries wait for batch completion—a classic throughput vs. latency trade-off in ML systems.\n\nWith this understanding of efficiency dimension interactions, we can examine why brute-force scaling alone cannot address real-world efficiency requirements. Scaling laws provide the quantitative framework for understanding these limitations.\n\n## AI Scaling Laws {#sec-efficient-ai-ai-scaling-laws-a043}\n\nMachine learning systems have followed a consistent pattern: increasing model scale through parameters, training data, and computational resources typically improves performance. This empirical observation has driven progress across natural language processing, computer vision, and speech recognition, where larger models trained on extensive datasets consistently achieve state-of-the-art results.\n\nThese scaling laws can be seen as the quantitative expression of Richard Sutton's \"Bitter Lesson\" from @sec-introduction: performance in machine learning is primarily driven by leveraging general methods at massive scale. The predictable power-law relationships show *how* computation, when scaled, yields better models.\n\nThis scaling trajectory raises critical questions about efficiency and sustainability. As computational demands grow exponentially and data requirements increase, questions emerge regarding when scaling costs outweigh performance benefits. Researchers have developed scaling laws[^fn-scaling-laws] that quantify how model performance relates to training resources, revealing why efficiency becomes increasingly important as systems expand in complexity.\n\n[^fn-scaling-laws]: **Scaling Laws**: Empirical relationships discovered by OpenAI showing that language model performance follows predictable power-law relationships with model size (N), dataset size (D), and compute budget (C). These laws enable researchers to predict performance and optimal resource allocation before expensive training runs.\n\nThis section introduces scaling laws, examines their manifestation across different dimensions, and analyzes their implications for system design, establishing why the multi-dimensional efficiency optimization framework is a fundamental requirement.\n\n### Empirical Evidence for Scaling Laws {#sec-efficient-ai-empirical-evidence-scaling-laws-0105}\n\nThe rapid evolution in AI capabilities over the past decade exemplifies this scaling trajectory. GPT-1 (2018) contained 117 million parameters and demonstrated basic sentence completion capabilities. GPT-2 (2019) scaled to 1.5 billion parameters and achieved coherent paragraph generation. GPT-3 (2020) expanded to 175 billion parameters and demonstrated sophisticated text generation across diverse domains. Each increase in model size brought dramatically improved capabilities, but at exponentially increasing costs.\n\nThis pattern extends beyond language models. In computer vision, doubling neural network size typically yields consistent accuracy gains when training data increases proportionally. AlexNet (2012) had 60 million parameters, VGG-16 (2014) scaled to 138 million, and large modern vision transformers can exceed 600 million parameters. Each generation achieved better image recognition accuracy, but required proportionally more computational resources and training data.\n\nThe scaling hypothesis underlies this progress: larger models possess increased capacity to capture intricate data patterns, facilitating improved accuracy and generalization. However, this scaling trajectory introduces critical resource constraints. Training GPT-3 required approximately 314 sextillion[^fn-sextillion] floating-point operations (314 followed by 21 zeros), equivalent to running a modern gaming PC continuously for over 350 years, at substantial financial and environmental costs.\n\n[^fn-sextillion]: **Sextillion**: A number with 21 zeros (10²¹), representing an almost incomprehensible scale. To put this in perspective, there are estimated 10²² to 10²⁴ stars in the observable universe, making GPT-3's training computation roughly 1/22nd of counting every star in the cosmos.\n\nThese resource demands reveal why understanding scaling laws is necessary for efficiency. @fig-compute-trends shows computational demands of training state-of-the-art models escalating at an unsustainable rate, growing faster than Moore's Law improvements in hardware.\n\n![**Model Training Compute Trends**: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: [@Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022.]](images/png/compute-trends.png){#fig-compute-trends}\n\nScaling laws provide a quantitative framework for understanding these trade-offs. They reveal that model performance exhibits predictable patterns as resources increase, following power-law relationships where performance improves consistently but with diminishing returns[^fn-diminishing-returns]. These laws show that optimal resource allocation requires coordinating model size, dataset size, and computational budget rather than scaling any single dimension in isolation.\n\n[^fn-diminishing-returns]: **Diminishing Returns**: Economic principle where each additional input yields progressively smaller output gains. In ML, doubling compute from 1 to 2 hours might improve accuracy by 5%, but doubling from 100 to 200 hours might improve it by only 0.5%.\n\n::: {.callout-note collapse=\"true\" title=\"Refresher: Transformer Computational Characteristics\"}\n\nRecall from @sec-dnn-architectures that transformers process sequences using self-attention mechanisms that compute relationships between all token pairs. This architecture's computational cost scales quadratically with sequence length, making resource allocation particularly critical for language models. The term \"FLOPs\" (floating-point operations) quantifies total computational work, while \"tokens\" represent the individual text units (typically subwords) that models process during training.\n:::\n\n### Compute-Optimal Resource Allocation {#sec-efficient-ai-computeoptimal-resource-allocation-541a}\n\nEmpirical studies of large language models (LLMs) reveal a key insight: for any fixed computational budget, there exists an optimal balance between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss.\n\n[^fn-tokens]: **Tokens**: Individual units of text that language models process, created by breaking text into subword pieces using algorithms like Byte-Pair Encoding (BPE). GPT-3 trained on 300 billion tokens while PaLM used 780 billion tokens, requiring text corpora equivalent to millions of books from web crawls and digitized literature.\n\n@fig-compute-optimal illustrates this principle through three related views. The left panel shows 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. The valleys in these curves identify the most efficient model size for each computational budget when training autoregressive[^fn-autoregressive] language models. The center and right panels reveal how the optimal number of parameters and tokens scales predictably as computational budgets increase, demonstrating the necessity for coordinated scaling to maximize resource utilization.\n\n[^fn-efficient-flops]: **FLOPs**: Floating-Point Operations, measuring computational work performed. Modern deep learning models require 10²²-10²⁴ FLOPs for training: GPT-3 used ~3.14 × 10²³ FLOPs (314 sextillion operations), equivalent to running a high-end gaming PC continuously for over 350 years.\n\n[^fn-transformer]: **Transformer**: Neural network architecture introduced by Vaswani et al. [@vaswani2017attention] that revolutionized NLP through self-attention mechanisms. Unlike sequential RNNs, transformers enable parallel processing during training, forming the foundation of modern large language models including GPT, BERT, T5, and their derivatives.\n\n[^fn-autoregressive]: **Autoregressive Models**: Language models that generate text by predicting each token based on all preceding tokens in the sequence. GPT-family models exemplify this approach, generating text left-to-right with causal attention masks to ensure each position only attends to previous positions.\n\n![**Optimal Compute Allocation**: For fixed computational budgets, language model performance depends on balancing model size and training data volume; the left panel maps training loss across parameter counts, identifying an efficiency sweet spot for each FLOP level. The center and right panels quantify how optimal parameter counts and token requirements scale predictably with increasing compute, demonstrating the need for coordinated scaling of both model and data to maximize resource utilization in large language models. Source: [@hoffmann2022training].](images/png/compute_optimal.png){#fig-compute-optimal}\n\n@kaplan2020scaling demonstrated that transformer-based language models scale predictably with three factors: the number of model parameters, the volume of the training dataset (measured in tokens), and the total computational budget (measured in floating-point operations). When these factors are augmented proportionally, models exhibit consistent performance improvements without requiring architectural modifications or task-specific tuning.\n\nThe practical manifestation of these patterns appears clearly in @fig-kaplan-scaling, which presents test loss curves for models spanning from $10^3$ to $10^9$ parameters. The figure reveals two key insights. First, larger models demonstrate superior sample efficiency, achieving target performance levels with fewer training tokens. Second, as computational resources increase, the optimal model size correspondingly grows, with loss decreasing predictably when compute is allocated efficiently.\n\n![**Scaling Laws & Compute Optimality**: Larger models consistently achieve better performance with increased training data and compute, but diminishing returns necessitate careful resource allocation during training. Optimal model size and training duration depend on the available compute budget, as evidenced by the convergence of loss curves at different parameter scales and training token counts. Source: [@kaplan2020scaling].](images/png/kaplan_scaling_data_compute.png){#fig-kaplan-scaling}\n\nThis theoretical scaling relationship defines optimal compute allocation: for a fixed budget, the relationship $D \\propto N^{0.74}$ [@hoffmann2022training] shows that dataset size $D$ and model size $N$ must grow in coordinated proportions. This represents a significant revision from earlier work by @kaplan2020scaling, which suggested $D \\propto N^{0.57}$, implying that scaling model size was more important than scaling data.\n\nThe Chinchilla study revealed that prior models were significantly undertrained. Chinchilla-70B, trained with 1.4 trillion tokens, outperformed the 280B parameter Gopher model across most benchmarks despite having 4x fewer parameters. This demonstrated that compute-optimal training requires substantially more data relative to model size than previously believed. The practical implication is that smaller models trained on more data often outperform larger models trained on less data, a finding with significant efficiency implications for practitioners operating under fixed compute budgets.\n\nThese scaling laws are empirical and continue to evolve as researchers explore different model families, data compositions, and training procedures.\n\nThese theoretical predictions assume perfect compute utilization, which becomes challenging in distributed training scenarios. Real-world implementations face communication overhead that scales unfavorably with system size, creating bandwidth bottlenecks that reduce effective utilization. Beyond 100 nodes, communication overhead can reduce expected performance gains by 20-40% depending on workload and interconnect, transforming predicted improvements into more modest real-world results.\n\n### Mathematical Foundations and Operational Regimes {#sec-efficient-ai-mathematical-foundations-operational-regimes-9afe}\n\nThe predictable patterns observed in scaling behavior can be expressed mathematically using power-law relationships, though understanding the intuition behind these patterns proves more important than precise mathematical formulation for most practitioners.\n\n::: {.callout-note collapse=\"true\" title=\"Formal Mathematical Formulation\"}\n\nFor readers interested in the formal mathematical framework, scaling laws can be expressed as power-law relationships. The general formulation is:\n\n$$\n\\mathcal{L}(N) = A N^{-\\alpha} + B\n$$\n\nwhere loss $\\mathcal{L}$ decreases as resource quantity $N$ increases, following a power-law decay with rate $\\alpha$, plus a baseline constant $B$. Here, $\\mathcal{L}(N)$ represents the loss achieved with resource quantity $N$, $A$ and $B$ are task-dependent constants, and $\\alpha$ is the scaling exponent that characterizes the rate of performance improvement. A larger value of $\\alpha$ signifies more efficient performance improvements with respect to scaling.\n\n:::\n\nThese theoretical predictions find strong empirical support across multiple model configurations. @fig-loss-vs-n-d shows that early-stopped test loss varies predictably with both dataset size and model size, and learning curves across configurations can be aligned through appropriate parameterization.\n\n#### Resource-Constrained Scaling Regimes {#sec-efficient-ai-resourceconstrained-scaling-regimes-062d}\n\nApplying scaling laws in practice requires recognizing three distinct resource allocation regimes that emerge from trade-offs between compute budget, data availability, and optimal resource allocation. These regimes provide practical guidance for system designers navigating resource constraints.\n\nCompute-limited regimes characterize scenarios where available computational resources restrict scaling potential despite abundant training data. Organizations with limited hardware budgets or strict training time constraints operate within this regime. The optimal strategy involves training smaller models for longer periods, maximizing utilization of available compute through extended training schedules rather than larger architectures. This approach proves particularly relevant for academic institutions, startups, or projects with constrained infrastructure access.\n\nData-limited regimes emerge when computational resources exceed what can be effectively utilized given dataset constraints. High-resource organizations working with specialized domains, proprietary datasets, or privacy-constrained data often encounter this regime. The optimal strategy involves training larger models for fewer optimization steps, leveraging model capacity to extract maximum information from limited training examples. This regime commonly appears in specialized applications like medical imaging or proprietary commercial datasets.\n\nOptimal regimes (Chinchilla Frontier) represent the balanced allocation of compute and data resources following compute-optimal scaling laws. This regime achieves maximum performance efficiency by scaling model size and training data proportionally, as demonstrated by DeepMind's Chinchilla model, which outperformed much larger models through optimal resource allocation [@hoffmann2022training]. Operating within this regime requires sophisticated resource planning but delivers superior performance per unit of computational investment.\n\nRecognizing these regimes enables practitioners to make informed decisions about resource allocation strategies, avoiding common inefficiencies such as over-parameterized models with insufficient training data or under-parameterized models that fail to utilize available computational resources effectively.\n\n::: {#fig-loss-vs-n-d fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\n\\definecolor{myblue}{RGB}{31,119,180}\n\\definecolor{myorange}{RGB}{255,127,14}\n\\definecolor{mygreen}{RGB}{44,160,44}\n\\definecolor{myred}{RGB}{214,39,40}\n\\definecolor{mypurple}{RGB}{148,103,189}\n\\definecolor{mybrown}{RGB}{140,86,75}\n\n\\tikzset{%\n    LineD/.style={line width=1.0pt,dashed,dash pattern=on 3pt off 2pt]}\n}\n\n\\pgfplotsset{myaxis/.style={\n  /pgf/number format/.cd,\n  1000 sep={},\n   legend style={at={(0.1,0.45)}, anchor=north},\n   legend cell align=left,\n   legend style={fill=BrownL!30,draw=BrownLine,row sep=-0.5pt,\n   font=\\fontsize{6pt}{6}\\selectfont\\usefont{T1}{phv}{m}{n}},\n   width=120mm,\n   height=67.2mm,\n   yticklabel style={xshift=1mm,font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n},\n   /pgf/number format/.cd, fixed, fixed zerofill, precision=1},\n   xticklabel style={font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n}},\n   ylabel style={font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n},align=center,yshift=-1.2mm},\n   xlabel style={font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n}},\n   tick align=outside,\n   major tick length=1mm,\n   title style={yshift=-4pt},\n   minor x tick  style={thin,black!60},\n   major tick  style={thin,black!60},\n   log basis y=10,\n   x tick label style={rotate=0, anchor=north,yshift=1pt},\n    }}\n\n\\begin{axis}[myaxis,\n  title={Loss vs Model and Dataset Size},\n  xmin=0.5e7,\n  xmax=4e10,\n  ymin=2.3, ymax=4.8,\n   ytick={2.5,3.0,3.5,4.0,4.5},\n  yticklabels={2.5,3,3.5,4.0,4.5},\n  xmode=log,\n  xtick={1e7,1e8,1e9,1e10},\n  xticklabels={10\\textsuperscript{7},10\\textsuperscript{8},10\\textsuperscript{9},10\\textsuperscript{10}},\n  xlabel={Tokens in Dataset},\n  ylabel={Loss},\n  grid=both,\n  major grid style={black!30},\n  minor grid style={draw=none},\n  minor x tick num=4,\n  xtick pos=left,\n   ytick pos=left,\n  cycle list={\n    {myblue,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {myorange,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {mygreen,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {myred,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {mypurple,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {mybrown,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {myblue},\n    {myorange},\n    {mygreen},\n    {myred},\n    {mypurple},\n    {mybrown}\n  }\n]\n%393.2K\n\\addplot+[] coordinates{\n(3.05e7,4.645)(3.05e7,4.48)(5.9e7,4.415)(1.14e8,4.34)(8.3e8,4.29)(2.3e10,4.28)\n};\n\\addlegendentry{393.2K}\n%2M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.25)(5.9e7,4.1)(1.14e8,3.93)(2.2e8,3.867)(4.3e8,3.837)(8.3e8,3.8)(2.3e10,3.77)\n};\n\\addlegendentry{3M}\n%25M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.25)(5.9e7,3.941)(1.14e8,3.735)(2.2e8,3.567)(4.3e8,3.415)(8.3e8,3.325)(2.3e10,3.27)\n};\n\\addlegendentry{25M}\n%85M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.21)(5.9e7,3.941)(1.14e8,3.69)(2.2e8,3.472)(4.3e8,3.31)(8.3e8,3.12)(1.61e9,3.04)(2.3e10,2.97)\n};\n\\addlegendentry{85M}\n%302M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.21)(5.9e7,3.941)(1.14e8,3.69)(2.2e8,3.46)(4.3e8,3.28)(8.3e8,3.01)(1.61e9,2.84)(2.3e10,2.62)\n};\n\\addlegendentry{302M}\n%708M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.31)(5.9e7,3.941)(1.14e8,3.69)(2.2e8,3.46)(4.3e8,3.28)(8.3e8,3.05)(1.61e9,2.80)(2.3e10,2.42)\n};\n\\addlegendentry{708M}\n%%%approximation\n%393.2K\n\\addplot+[LineD,smooth]coordinates{\n(1.5e7,4.595) (3.05e7,4.47) (5.9e7,4.395) (1.14e8,4.35) (8.3e8,4.3) (3e10,4.290)\n};\n%2M\n\\addplot+[LineD,smooth] coordinates{\n(1.5e7,4.46) (3.05e7,4.25) (5.9e7,4.08) (1.14e8,3.96) (2.2e8,3.867) (4.3e8,3.814) (8.3e8,3.789) (3e10,3.756)\n};\n%25M\n\\addplot+[LineD,smooth]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.95)(1.14e8,3.75)(2.2e8,3.58)(4.3e8,3.444)(8.3e8,3.345)(3e9,3.253)(3e10,3.213)};\n%85M\n\\addplot+[LineD,smooth,samples=200]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.93)(1.14e8,3.7)(2.2e8,3.499)(4.3e8,3.32)(8.3e8,3.17)\n(1.61e9,3.064)(5e9,2.955)(1e10,2.92)(3e10,2.913)};\n%30M\n\\addplot+[LineD,smooth,samples=200]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.93)(1.14e8,3.7)(2.2e8,3.467)(4.3e8,3.25)(8.3e8,3.054)\n(1.61e9,2.89)(4e9,2.73)(1e10,2.64)(3e10,2.59)};\n%708M\n\\addplot+[LineD,smooth,samples=200]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.93)(1.14e8,3.7)(2.2e8,3.456)(4.3e8,3.223)(8.3e8,3.013)\n(1.61e9,2.82)(4e9,2.61)(1e10,2.47)(3e10,2.39)};\n\\node[font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n},\nanchor=south,above=0pt,fill=white]at(axis description cs:0.1,0.45){Params};\n\\end{axis}\n\\end{tikzpicture}\n```\n: **Loss vs Model and Dataset Size**: Early-stopped test loss varies predictably with both dataset size and model size, highlighting the importance of balanced scaling for optimal performance under fixed compute budgets.\n:::\n\nScaling laws show that performance improvements follow predictable patterns that change depending on resource availability and exhibit distinct behaviors across different dimensions. Two important types of scaling regimes emerge: **data-driven regimes** that describe how performance changes with dataset size, and **temporal regimes** that describe when in the ML lifecycle we apply additional compute.\n\n#### Data-Limited Scaling Regimes {#sec-efficient-ai-datalimited-scaling-regimes-ba1d}\n\nThe relationship between generalization error and dataset size exhibits three distinct regimes, as shown in @fig-data-scaling-regimes. When limited examples are available, high generalization error results from inadequate statistical estimates. As data availability increases, generalization error decreases predictably as a function of dataset size, following a power-law relationship that provides the most practical benefit from data scaling. Eventually, performance reaches saturation, approaching a floor determined by inherent data limitations or model capacity, beyond which additional data yields negligible improvements.\n\n::: {#fig-data-scaling-regimes fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.7}{\n\\begin{tikzpicture}[line join=round,line cap=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\def\\hi{5.5}\n\\def\\wi{11}\n\\def\\hl{5/7*\\hi}\n\\draw[thick](0,-1)coordinate(O)--node[below=3pt]{Training Data Set Size (Log-Scale)}(\\wi,-1)coordinate(E);\n\\draw[thick](0,-1)--node[above=3pt,midway,sloped]{Generalization Error (Log-Scale)}(0,\\hi);\n%\n\\draw[dashed,violet,thick](0,0)--(\\wi,0);\n\\draw[dashed,red,thick](0,\\hl)--(\\wi,\\hl);\n%\n\\coordinate(A)at(3,-0.7);\n\\coordinate(A1)at(3,-1);\n\\coordinate(B)at(8,-0.7);\n\\coordinate(G1)at($(0,\\hl)+(0,-0.1)$);\n\\coordinate(G2)at($(\\wi,0)+(0,0.1)$);\n\\coordinate(GG1)at($(G1)+(1.5,0)$);\n\\coordinate(GG2)at($(G2)+(-1.5,0)$);\n\n\\path[thick](A)--++(90:\\hi)coordinate(LG1);\n\\path[thick](B)--++(90:\\hi)coordinate(LG2);\n\n\\draw[smooth,blue,line width=2pt](G1)--\nnode[above=2pt,align=center,text=black,pos=0.98]{Best Guess Error}(GG1)\nto[out=360,in=180](GG2)--\nnode[below=2pt,align=center,text=black,pos=0.1]{Irreducible Error}(G2);\n\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=0mm,\nline width=0.75pt,inner ysep=0mm,\nfill=magenta!05,fit=(O)(LG1)](BB){};\n\\node[above=1pt of BB.north,anchor=south,align=center]{Small Data\\\\ Region};\n%\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=0mm,\nline width=0.75pt,inner ysep=0mm,\nfill=green!10,fit=(A1)(LG2)](BB1){};\n\\node[above=1pt of BB1.north,anchor=south,align=center]{Power-Law\\\\ Region};\n\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=0mm,\nline width=0.75pt,inner ysep=0mm,\nfill=magenta!05,fit=(LG2)(E)](BB2){};\n\\node[above=1pt of BB2.north,anchor=south,align=center]{Irreducible Error\\\\ Region};\n%\n\\end{tikzpicture}}\n```\n: **Data Scaling Regimes**: The relationship between dataset size and generalization error follows distinct scaling regimes. Increasing dataset size initially reduces generalization error following a power-law relationship, but eventually plateaus at an irreducible error floor determined by inherent data limitations or model capacity [@hestness2017deep]. This behavior exposes diminishing returns from data scaling and informs practical decisions about data collection efforts in machine learning systems.\n:::\n\nThis three-regime pattern manifests across different resource dimensions beyond data alone. Operating within the power-law region provides the most reliable return on resource investment. Reaching this regime requires minimum resource thresholds, while maintaining operation within it demands careful allocation to avoid premature saturation.\n\n#### Temporal Scaling Regimes {#sec-efficient-ai-temporal-scaling-regimes-e118}\n\nWhile data-driven regimes characterize how performance varies with dataset size, a complementary perspective examines temporal allocation of compute resources within the ML lifecycle. Recent research has identified three distinct **temporal scaling regimes** characterizing different stages of model development and deployment.\n\n**Pre-training scaling** encompasses the traditional domain of scaling laws, characterizing how model performance improves with larger architectures, expanded datasets, and increased compute during initial training. Extensive study in foundation models has established clear power-law relationships between resources and capabilities.\n\n**Post-training scaling** characterizes improvements achieved after initial training through techniques including fine-tuning, prompt engineering, and task-specific adaptation. This regime has gained prominence with foundation models, where adaptation rather than retraining frequently provides the most efficient path to enhanced performance under moderate resource requirements.\n\n**Test-time scaling** characterizes how performance improvements result from additional compute allocation during inference without modifying model parameters. This encompasses methods including ensemble prediction, chain-of-thought prompting, and iterative refinement, enabling models to allocate additional processing time per input.\n\nThe relative compute allocation across these regimes reveals important efficiency insights:\n\n- **Pre-training**: Approximately $10^{24}$ FLOPs for frontier models (one-time investment)\n- **Post-training**: Approximately $10^{21}$ FLOPs (0.1% of pre-training compute)\n- **Inference per query**: Approximately $10^{12}$ FLOPs\n\nThe critical insight for efficiency planning is that aggregate inference compute can dominate total system cost. With 1 billion queries per day, daily inference compute reaches $10^{21}$ FLOPs, equaling the entire post-training budget. After one year, aggregate inference exceeds pre-training compute by 300x. This explains why inference efficiency becomes the dominant optimization target for high-volume deployments, while pre-training efficiency matters more for organizations that frequently retrain models.\n\n@fig-scaling-regimes shows these temporal regimes exhibit distinct characteristics in computational resource allocation for performance improvement. Pre-training demands massive resources while providing broad capabilities, post-training offers targeted enhancements under moderate requirements, and test-time scaling enables flexible performance-compute trade-offs adjustable per inference.\n\n::: {#fig-scaling-regimes fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.75}{\n\\begin{tikzpicture}[line join=round,line cap=round,font=\\small\\usefont{T1}{phv}{m}{n},yscale=0.8]\n\\tikzset{Line/.style={line width=2.5pt,RedLine},\nLineD/.style={Line,line width=0.75pt,dashed}\n}\n\\def\\hi{7.5}\n\\def\\wi{11}\n\\draw[thick](0,0)coordinate(O)--node[below=3pt]{Compute}(\\wi,0);\n\\draw[thick](0,0)--node[above=3pt,midway,sloped]{Intelligence}(0,\\hi)coordinate(Y);\n%\n\n\\coordinate(O)at(0.03,0.03);\n\\coordinate(T1)at(2,0.88);\n\\coordinate(T2)at(4.2,3.0);\n\\coordinate(T3)at(6,5.2);\n\\coordinate(T4)at(7.7,6.35);\n\\draw[Line](O)\nto (T1)\nto [out=30,in=210](T2)\nto [out=55,in=220](T3)\nto [out=40,in=210](T4);\n\\draw[Line,-latex](O)--++(23:3.6)node[below right,text=black]{Pre-training scaling};\n\\draw[blue,-latex,LineD](O)--++(23:7.0);\n%\n\\draw[Line,-latex](T2)--++(27:1.6)node[below right,text=black]{Post-training scaling};\n\\draw[-latex,LineD](T2)--++(27:4.0);\n\\draw[Line,-latex](T3)to [out=40,in=210]($(T4)+(0.15,0.09)$)\nnode[below right,text=black,align=center]{Test-time scaling\\\\ \"long thinking};\n\\draw[-latex,LineD](T4)--++(29:2.0);\n\\node[below right=of Y,align=center,font=\\normalsize\\usefont{T1}{phv}{m}{n}]{From one to three \\\\ scaling laws};\n\\end{tikzpicture}}\n```\n: **Temporal Scaling Regimes**: Different temporal scaling regimes offer distinct approaches to improving model performance with varying compute investments. Pre-training establishes broad capabilities through large-scale training from scratch, post-training refines existing models through additional training phases, and test-time scaling dynamically allocates compute during inference to enhance per-sample results. Understanding these regimes clarifies the trade-offs between upfront investment and flexible, on-demand resource allocation for optimal system performance.\n:::\n\nData-driven and temporal scaling regimes inform system design, revealing multiple paths to performance improvement beyond scaling training resources alone. For resource-constrained deployments, post-training and test-time scaling may provide more practical approaches than complete model retraining, while data-efficient techniques enable effective system operation within the power-law regime using smaller datasets.\n\n### Practical Applications in System Design {#sec-efficient-ai-practical-applications-system-design-5c97}\n\nScaling laws provide powerful insights for practical system design and resource planning. Consistent observation of power-law trends indicates that within well-defined operational regimes, model performance depends predominantly on scale rather than idiosyncratic architectural innovations. However, diminishing returns phenomena indicate that each additional improvement requires exponentially increased resources while delivering progressively smaller benefits.\n\nOpenAI's development of GPT-3 demonstrates this principle. Rather than conducting expensive architecture searches, the authors applied scaling laws derived from earlier experiments to determine optimal training dataset size and model parameter count [@brown2020language]. They scaled an established transformer architecture along the compute-optimal frontier to 175 billion parameters and approximately 300 billion tokens, enabling advance prediction of model performance and resource requirements. This methodology demonstrated the practical application of scaling laws in large-scale system planning.\n\nScaling laws serve multiple practical functions in system design. They enable practitioners to estimate returns on investment for different resource allocations during resource budgeting. Under fixed computational budgets, designers can utilize empirical scaling curves to determine optimal performance improvement strategies across model size, dataset expansion, or training duration.\n\nSystem designers can utilize scaling trends to identify when architectural changes yield significant improvements relative to gains achieved through scaling alone, thereby avoiding exhaustive architecture search. When a model family exhibits favorable scaling behavior, scaling the existing architecture may prove more effective than transitioning to more complex but unvalidated designs.\n\nIn edge and embedded environments with constrained resource budgets, understanding performance degradation under model scaling enables designers to select smaller configurations delivering acceptable accuracy within deployment constraints. By quantifying scale-performance trade-offs, scaling laws identify when brute-force scaling becomes inefficient and indicate the necessity for alternative approaches including model compression, efficient knowledge transfer, sparsity techniques, and hardware-aware design.\n\nScaling laws also function as diagnostic instruments. Performance plateaus despite increased resources may indicate dimensional saturation—such as inadequate data relative to model size—or inefficient computational resource utilization. This diagnostic capability renders scaling laws both predictive and prescriptive, facilitating systematic bottleneck identification and resolution.\n\n### Sustainability and Cost Implications {#sec-efficient-ai-sustainability-cost-implications-0473}\n\nScaling laws illuminate pathways to performance enhancement while revealing rapidly escalating resource demands. As models expand, training and deployment resource requirements grow disproportionately, creating tension between performance gains through scaling and system efficiency.\n\nTraining large-scale models necessitates substantial processing power, typically requiring distributed infrastructures[^fn-distributed-infrastructure] comprising hundreds or thousands of accelerators. State-of-the-art language model training may require tens of thousands of GPU-days, consuming millions of kilowatt-hours of electricity. These distributed training systems introduce additional complexity around communication overhead, synchronization, and scaling efficiency, as detailed in @sec-ai-training. Energy demands have outpaced Moore's Law improvements, raising critical questions about long-term sustainability.\n\n[^fn-distributed-infrastructure]: **Distributed Infrastructure**: Computing systems that spread ML workloads across multiple machines connected by high-speed networks. OpenAI's GPT-4 training likely used thousands of NVIDIA A100 GPUs connected via InfiniBand, requiring careful orchestration to avoid communication bottlenecks.\n\nLarge models require extensive, high-quality, diverse datasets to achieve their full potential. Data collection, cleansing, and labeling processes consume considerable time and resources. As models approach saturation of available high-quality data, particularly in natural language processing, additional performance gains through data scaling become increasingly difficult to achieve. This reality underscores data efficiency as a necessary complement to brute-force scaling approaches.\n\nThe financial and environmental implications compound these challenges. Training runs for large foundation models can incur millions of dollars in computational expenses, and associated carbon footprints[^fn-carbon-emissions] have garnered increasing scrutiny. These costs limit accessibility to cutting-edge research and exacerbate disparities in access to advanced AI systems. These democratization challenges introduced by efficiency barriers connect directly to broader accessibility and sustainability goals that responsible ML practitioners must consider.\n\n[^fn-carbon-emissions]: **Carbon Emissions**: Training GPT-3 generated approximately 502 tons of CO₂ equivalent, comparable to annual emissions of 123 gasoline-powered vehicles. Modern ML practices increasingly incorporate carbon tracking using tools like CodeCarbon and the ML CO2 Impact calculator.\n\nThese trade-offs demonstrate that scaling laws provide valuable frameworks for understanding performance growth but do not constitute unencumbered paths to improvement. Each incremental performance gain requires evaluation against corresponding resource requirements. As systems approach practical scaling limits, emphasis must transition from scaling alone to efficient scaling—a comprehensive approach balancing performance, cost, energy consumption, and environmental impact.\n\n### Scaling Law Breakdown Conditions {#sec-efficient-ai-scaling-law-breakdown-conditions-1f8c}\n\nScaling laws exhibit remarkable consistency within specific operational regimes but possess inherent limitations. As systems expand, they inevitably encounter boundaries where underlying assumptions of smooth, predictable scaling cease to hold. These breakdown points expose critical inefficiencies and emphasize the necessity for refined system design approaches.\n\nFor scaling laws to remain valid, model size, dataset size, and computational budget must be augmented in coordinated fashion. Over-investment in one dimension while maintaining others constant often results in suboptimal outcomes. For example, increasing model size without expanding training datasets may induce overfitting, while increasing computational resources without model redesign may lead to inefficient utilization [@hoffmann2022training].\n\nLarge-scale models require carefully tuned training schedules and learning rates to fully utilize available resources. When compute is insufficiently allocated due to premature stopping, batch size misalignment, or ineffective parallelism, models may fail to reach performance potential despite significant infrastructure investment.\n\nScaling laws presuppose continued performance improvement with sufficient training data. However, in numerous domains, availability of high-quality, human-annotated data is finite. As models consume increasingly large datasets, they reach points of diminishing marginal utility where additional data contributes minimal new information. Beyond this threshold, larger models may exhibit memorization rather than generalization.\n\nAs models grow, they demand greater memory bandwidth[^fn-memory-bandwidth], interconnect capacity, and I/O throughput. These hardware limitations become increasingly challenging even with specialized accelerators. Distributing trillion-parameter models across clusters necessitates meticulous management of data parallelism, communication overhead, and fault tolerance.\n\n[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be read from or written to memory, measured in GB/s. NVIDIA H100 provides 3.35 TB/s memory bandwidth vs. typical DDR5 RAM's 51 GB/s, a 65× difference critical for handling large model parameters.\n\nAt extreme scales, models may approach limits of what can be learned from training distributions. Performance on benchmarks may continue improving, but these improvements may no longer reflect meaningful gains in generalization or understanding. Models may become increasingly brittle, susceptible to adversarial examples, or prone to generating plausible but inaccurate outputs.\n\n@tbl-scaling-breakdown synthesizes the primary causes of scaling failure, outlining typical breakdown types, underlying causes, and representative scenarios as a reference for anticipating inefficiencies and guiding balanced system design.\n\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Dimension Scaled**   | **Type of Breakdown**   | **Underlying Cause**                           | **Example Scenario**                          |\n+:=======================+:========================+:===============================================+:==============================================+\n| **Model Size**         | Overfitting             | Model capacity exceeds available data          | Billion-parameter model on limited dataset    |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Data Volume**        | Diminishing Returns     | Saturation of new or diverse information       | Scaling web text beyond useful threshold      |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Compute Budget**     | Underutilized Resources | Insufficient training steps or inefficient use | Large model with truncated training duration  |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Imbalanced Scaling** | Inefficiency            | Uncoordinated increase in model/data/compute   | Doubling model size without more data or time |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **All Dimensions**     | Semantic Saturation     | Exhaustion of learnable patterns in the domain | No further gains despite scaling all inputs   |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n\n: **Scaling Breakdown Types**: Unbalanced scaling across model size, data volume, and compute resources leads to specific failure modes, such as overfitting or diminishing returns, impacting system performance and efficiency. The table categorizes these breakdowns, identifies their root causes, and provides representative scenarios to guide more effective system design and resource allocation. {#tbl-scaling-breakdown}\n\nThese breakdown points demonstrate that scaling laws describe empirical regularities under specific conditions that become increasingly difficult to maintain at scale. As machine learning systems continue evolving, discerning where and why scaling ceases to be effective becomes necessary, driving development of strategies that enhance performance without relying solely on scale.\n\n### Integrating Efficiency with Scaling {#sec-efficient-ai-integrating-efficiency-scaling-a513}\n\nThe limitations exposed by scaling laws (data saturation, infrastructure bottlenecks, and diminishing returns) demonstrate that brute-force scaling alone cannot deliver sustainable AI systems. These constraints necessitate a shift from expanding scale to achieving greater efficiency with reduced resources.\n\nThis transition requires coordinated optimization across three interconnected dimensions: **algorithmic efficiency** addresses computational intensity through better model design, **compute efficiency** maximizes hardware utilization to translate algorithmic improvements into practical gains, and **data efficiency** extracts maximum information from limited examples as high-quality data becomes scarce. Together, these dimensions provide systematic approaches to achieving performance goals that scaling alone cannot sustainably deliver, while addressing broader concerns about equitable access to AI capabilities and environmental impact.\n\nHaving examined how scaling laws reveal fundamental constraints, we now turn to the efficiency framework that provides concrete strategies for operating effectively within these constraints. The following section details how the three efficiency dimensions work together to enable sustainable, accessible machine learning systems.\n\n## The Efficiency Framework {#sec-efficient-ai-efficiency-framework-c0de}\n\nThe constraint identified through scaling laws (that continued progress requires systematic efficiency optimization) motivates three complementary efficiency dimensions. Each dimension addresses a specific limitation: algorithmic efficiency tackles computational intensity, compute efficiency addresses hardware utilization gaps, and data efficiency solves the data saturation problem.\n\nTogether, these three dimensions provide a systematic framework for addressing the constraints that scaling laws reveal. Targeted optimizations across algorithmic design, hardware utilization, and data usage can achieve what brute-force scaling cannot: sustainable, accessible, high-performance AI systems.\n\n### Multi-Dimensional Efficiency Synergies {#sec-efficient-ai-multidimensional-efficiency-synergies-ea04}\n\nOptimal performance requires coordinated optimization across multiple dimensions. No single resource—whether model parameters, training data, or compute budget—can be scaled indefinitely to achieve efficiency. Modern techniques demonstrate the potential: 10-100x gains in algorithmic efficiency through optimized architectures, 5-50x improvements in hardware utilization through specialized processors, and 10-1000x reductions in data requirements through advanced learning methods.\n\nThe power of this framework emerges from interconnections between dimensions, as depicted in @fig-evolution-efficiency. Algorithmic innovations often enable better hardware utilization, while hardware advances unlock new algorithmic possibilities. Data-efficient techniques reduce computational requirements, while compute-efficient methods enable training on larger datasets. Understanding these synergies is essential for building practical ML systems.\n\n::: {#fig-evolution-efficiency fig-env=\"figure\" fig-pos=\"htb\"}\n\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n},node distance=2mm]\n\\tikzset{\n  Box/.style={inner xsep=1pt,\n    draw=none,\n    fill=#1,\n    anchor=west,\n    text width=27mm,align=center,\n    minimum width=27mm, minimum height=10mm\n  },\n  Box/.default=red\n}\n\\definecolor{col1}{RGB}{128, 179, 255}\n\\definecolor{col2}{RGB}{255, 255, 128}\n\\definecolor{col3}{RGB}{204, 255, 204}\n\\definecolor{col4}{RGB}{230, 179, 255}\n\\definecolor{col5}{RGB}{255, 153, 204}\n\\definecolor{col6}{RGB}{245, 82, 102}\n\\definecolor{col7}{RGB}{255, 102, 102}\n\n\\node[Box={col1}](B1){Algorithmic\\\\ Efficiency};\n\\node[Box={col1},right=of B1](B2){Deep\\\\ Learning Era};\n\\node[Box={col1},right=of B2](B3){Modern\\\\ Efficiency};\n\\node[Box={col2},right=of B3](B4){General-Purpose\\\\ Computing};\n\\node[Box={col2},right=of B4](B5){Accelerated\\\\ Computing};\n\\node[Box={col2},right=of B5](B6){Sustainable Computing};\n\\node[Box={col3},right=of B6](B7){Data\\\\ Scarcity};\n\\node[Box={col3},right=of B7](B8){Big\\\\ Data Era};\n\\node[Box={col3},right=of B8](B9){ Data-Centric AI};\n%%%%\n\\node[Box={col1},above=of B2,minimum width=87mm,\n text width=85mm](GB1){Algorithmic Efficiency};\n\\node[Box={col2},above=of B5,minimum width=87mm,\ntext width=85mm](GB5){Compute Efficiency};\n\\node[Box={col3},above=of B8,minimum width=87mm,\ntext width=85mm](GB8){Data Efficiency};\n%%\n\\foreach \\x in{1,2,...,9}\n\\draw[dashed,thick,-latex](B\\x)--++(270:5.5);\n\n\\path[red]([yshift=-8mm]B1.south west)coordinate(P)-|coordinate(K)(B9.south east);\n\\draw[line width=2pt,-latex](P)--(K)--++(0:3mm);\n\n\\node[Box={col1!50},below=2 of B1](BB1){1980};\n\\node[Box={col1!50},below=2 of B2](BB2){2010};\n\\node[Box={col1!50},below=2 of B3](BB3){2023};\n\\node[Box={col2!70},below=2 of B4](BB4){1980};\n\\node[Box={col2!70},below=2 of B5](BB5){2010};\n\\node[Box={col2!70},below=2 of B6](BB6){2023};\n\\node[Box={col3!70},below=2 of B7](BB7){1980};\n\\node[Box={col3!50},below=2 of B8](BB8){2010};\n\\node[Box={col3!50},below=2 of B9](BB9){2023};\n%%%%%\n\\node[Box={col4!50},below= of BB1](BBB1){2010};\n\\node[Box={col4!50},below= of BB2](BBB2){2022};\n\\node[Box={col4!50},below= of BB3](BBB3){Future};\n%\n\\node[Box={col5!50},below= of BB4](BBB4){2010};\n\\node[Box={col5!50},below= of BB5](BBB5){2022};\n\\node[Box={col5!50},below= of BB6](BBB6){Future};\n%\n\\node[Box={col7!50},below= of BB7](BBB7){2010};\n\\node[Box={col7!50},below= of BB8](BBB8){2022};\n\\node[Box={col7!50},below= of BB9](BBB9){Future};\n\\end{tikzpicture}\n```\n: **Historical Efficiency Trends**: Algorithmic, computational, and data efficiency have each contributed to substantial gains in AI capabilities, though at different rates and with diminishing returns. Understanding these historical trends clarifies the interplay between these efficiency dimensions and informs strategies for scaling machine learning systems in data-limited environments.\n:::\n\nThe specific priorities vary across deployment environments. Cloud systems with abundant resources prioritize scalability and throughput, while edge devices face severe memory and power constraints. Mobile applications must balance performance with battery life, and TinyML deployments demand extreme resource efficiency. Understanding these context-specific patterns enables designers to make informed decisions about which efficiency dimensions to prioritize and how to address inevitable trade-offs between them.\n\n### Achieving Algorithmic Efficiency {#sec-efficient-ai-achieving-algorithmic-efficiency-ef15}\n\nAlgorithmic efficiency achieves maximum performance per unit of computation through optimized model architectures and training procedures. Modern techniques achieve 10-100x improvements in computational requirements while maintaining or improving accuracy, providing the most direct path to practical AI deployment.\n\n#### Efficiency Metrics for Compression {#sec-efficient-ai-efficiency-metrics-compression}\n\nEvaluating compression techniques requires multiple complementary metrics that capture different aspects of efficiency gains.\n\n**Compression Ratio** measures the reduction in model size:\n$$CR = \\frac{\\text{Original Size}}{\\text{Compressed Size}} = \\frac{N_{params} \\times b_{orig}}{N_{remaining} \\times b_{compressed} + \\text{Overhead}}$$\nwhere overhead includes index storage for sparse representations.\n\n**Theoretical vs. Actual Speedup** represents a critical distinction in system analysis. FLOPs reduction does not equal wall-clock speedup:\n$$\\text{Actual Speedup} = \\frac{T_{original}}{T_{compressed}} \\neq \\frac{\\text{FLOPs}_{original}}{\\text{FLOPs}_{compressed}}$$\n\nThis gap exists because memory bandwidth may limit performance rather than compute, sparse operations require indexing overhead, reduced precision may not map to hardware acceleration, and kernel launch overhead becomes proportionally larger for smaller operations.\n\n**Structured Pruning Advantage**: Structured pruning (removing entire filters or channels) achieves speedups closer to theoretical because it preserves dense tensor operations. Unstructured pruning requires sparse matrix formats that may not accelerate on standard hardware.\n\n**Accuracy-Efficiency Pareto Frontier**: The optimal compression strategy lies on the Pareto frontier, the set of solutions where no technique achieves BOTH higher accuracy AND higher efficiency. Different techniques occupy different positions on this frontier, as shown in the comparison table below.\n\n+---------------------------+------------+----------------+---------------+\n| Technique                 | Typical CR | Actual Speedup | Accuracy Loss |\n+:==========================+:===========+:===============+:==============+\n| INT8 quantization         | 4x         | 2-4x           | <1%           |\n+---------------------------+------------+----------------+---------------+\n| 50% structured pruning    | 2x         | 1.5-2x         | <2%           |\n+---------------------------+------------+----------------+---------------+\n| 90% unstructured pruning  | 10x        | 1.2-1.5x       | 1-3%          |\n+---------------------------+------------+----------------+---------------+\n| Knowledge distillation    | 4-10x      | 4-10x          | 1-5%          |\n+---------------------------+------------+----------------+---------------+\n\n: **Compression Technique Trade-offs**: Different compression approaches offer varying trade-offs between compression ratio, actual speedup, and accuracy loss, enabling practitioners to select techniques appropriate for their deployment constraints. {#tbl-compression-tradeoffs}\n\n#### Why Neural Networks Are Compressible {#sec-efficient-ai-why-networks-compressible}\n\nThe foundation for compression improvements lies in a key observation: most neural networks are dramatically overparameterized. Trained neural networks exhibit a characteristic property where weight magnitude distributions follow heavy-tailed patterns. Most weights cluster near zero while a small fraction carry the majority of information.\n\nMathematically, if we sort weights by absolute magnitude $|w_i|$, typically 80-90% of weights contribute less than 10% to the output activation. This redundancy arises from overparameterization during training, where networks learn in a high-dimensional space but converge to solutions lying in lower-dimensional manifolds.\n\n**Sensitivity Analysis**: The sensitivity of each weight to network output can be approximated by the first-order Taylor expansion of the loss function:\n$$\\Delta L \\approx \\sum_i g_i \\cdot \\Delta w_i$$\nwhere $g_i = \\partial L / \\partial w_i$ is the gradient. For converged networks, $g_i \\approx 0$, leading to the simpler heuristic of removing weights with smallest $|w_i|$. This is the basis of magnitude-based pruning pioneered in Deep Compression [@han2015deep].\n\nThe lottery ticket hypothesis reveals that networks contain sparse subnetworks that achieve comparable accuracy when trained in isolation [@frankle2019lottery]. The size of these \"winning tickets\" varies significantly by architecture: 3-5% of original parameters for deep vision models on ImageNet, 10-30% for smaller networks on CIFAR, and larger fractions for language models where attention patterns require more parameters. This discovery transforms compression into a principled approach: large models serve as initialization strategies for finding efficient architectures.\n\n+----------------------+-----------------------+-------------------+--------------+\n| Pruning Type         | Speedup               | Hardware Support  | Accuracy     |\n+:=====================+:======================+:==================+:=============+\n| Unstructured         | Theoretical only      | Requires sparse   | Best         |\n|                      |                       | formats           |              |\n+----------------------+-----------------------+-------------------+--------------+\n| Structured (filter)  | Real 2-4x             | Standard dense    | Moderate     |\n|                      |                       | ops               |              |\n+----------------------+-----------------------+-------------------+--------------+\n| N:M (e.g., 2:4)      | Real 2x               | Ampere Sparse     | Good         |\n|                      |                       | Tensor Cores      |              |\n+----------------------+-----------------------+-------------------+--------------+\n\n: **Structured vs. Unstructured Pruning**: Different pruning approaches offer varying trade-offs between theoretical speedup, hardware support requirements, and accuracy preservation. {#tbl-pruning-types}\n\n#### Model Compression Fundamentals {#sec-efficient-ai-model-compression-fundamentals-bcc3}\n\nThree major approaches dominate modern algorithmic efficiency, each targeting different aspects of model inefficiency:\n\n**Model Compression** systematically removes redundant components from neural networks. Pruning techniques achieve 2-4x inference speedup with 1-3% accuracy loss by removing unnecessary weights and structures. Research demonstrates that ResNet-50 can be reduced to 20% of original parameters while maintaining 99% of ImageNet accuracy [@gholami2021survey]. The specific pruning algorithms—including magnitude-based selection, structured vs. unstructured approaches, and layer-wise sensitivity analysis—are covered in detail in @sec-model-optimizations.\n\n**Precision Optimization** reduces computational requirements through quantization, which maps high-precision floating-point values to lower-precision representations. For uniform affine quantization, the mapping is defined as:\n$$q = \\text{round}\\left(\\frac{r}{s}\\right) + z$$\nwhere $r$ is the real-valued input, $s$ is the scale factor, $z$ is the zero-point offset, and $q$ is the quantized integer. Dequantization recovers an approximation: $\\hat{r} = s \\cdot (q - z)$. The quantization error $\\epsilon = r - \\hat{r}$ is bounded by $|\\epsilon| \\leq s/2$ for rounding, and the scale factor for a range $[r_{min}, r_{max}]$ mapped to $b$ bits is:\n$$s = \\frac{r_{max} - r_{min}}{2^b - 1}$$\n\nNeural networks demonstrate inherent robustness to this bounded quantization error for several reasons: the error is bounded and approximately uniform, normalization layers (BatchNorm and LayerNorm) provide implicit regularization, many ReLU activations are zero and require no precision, and overparameterization provides redundant capacity to absorb noise. INT8 quantization achieves 4x memory reduction and 2-4x inference speedup while typically maintaining 98-99% of FP32 accuracy [@Jacob_et_al_2018]. Modern techniques range from simple post-training quantization to sophisticated quantization-aware training. The specific quantization algorithms, calibration methods, and training procedures are detailed in @sec-model-optimizations.\n\n+----------------+------+----------------+-----------------+\n| Scheme         | Bits | Memory Savings | Accuracy Impact |\n+:===============+:=====+:===============+:================+\n| FP32 to FP16   | 16   | 2x             | Negligible      |\n+----------------+------+----------------+-----------------+\n| FP32 to INT8   | 8    | 4x             | 0.1-1% loss     |\n+----------------+------+----------------+-----------------+\n| FP32 to INT4   | 4    | 8x             | 1-5% loss       |\n+----------------+------+----------------+-----------------+\n\n: **Quantization Schemes and Trade-offs**: Lower precision representations provide greater memory savings but with increasing accuracy impact. {#tbl-quantization-schemes}\n\n**Knowledge Transfer** distills capabilities from large teacher models into efficient student models. The core mechanism uses a combined loss function:\n$$L = \\alpha \\cdot L_{CE}(y, p_s) + (1 - \\alpha) \\cdot T^2 \\cdot L_{KL}(p_t^{(T)}, p_s^{(T)})$$\nwhere $L_{CE}$ is the cross-entropy loss with true labels, $L_{KL}$ is the Kullback-Leibler divergence between teacher and student outputs, and $T$ is the temperature parameter that softens the probability distributions. The $T^2$ factor compensates for the reduced gradient magnitudes when using high temperatures. Higher temperatures reveal the teacher's learned relationships between classes, including which incorrect classes are \"close\" to the correct answer, providing richer training signal than hard labels alone. Knowledge distillation[^fn-knowledge-distillation] achieves 40-60% parameter reduction while retaining 95-97% of original performance, addressing both computational efficiency and data efficiency by requiring fewer training examples. The specific distillation algorithms, loss functions, and training procedures are covered in @sec-model-optimizations.\n\n[^fn-knowledge-distillation]: **Knowledge Distillation**: Technique where a large \"teacher\" model transfers knowledge to a smaller \"student\" model by training the student to mimic the teacher's output probabilities. DistilBERT achieves ~97% of BERT's performance on GLUE benchmark with 40% fewer parameters and 60% faster inference through distillation.\n\n#### Hardware-Algorithm Co-Design {#sec-efficient-ai-hardwarealgorithm-codesign-67e8}\n\nAlgorithmic optimizations alone are insufficient; their practical benefits depend on hardware-software co-design. Optimization techniques must be tailored to target hardware characteristics (memory bandwidth, compute capabilities, and precision support) to achieve real-world speedups.\n\n**Discrete Efficiency Steps**: Hardware provides acceleration in discrete steps aligned with specific bit-widths. INT8 quantization achieves 2.3x speedup on NVIDIA V100 GPUs with tensor core support, and INT4 provides additional gains on Ampere architecture, but arbitrary bit-widths like INT6 may provide no benefit because hardware lacks native support. This creates discrete efficiency steps rather than a continuous improvement curve.\n\n**Operational Intensity**: The ratio of compute operations to memory bytes transferred (FLOPs/byte) determines whether a workload is compute-bound or memory-bound. This metric is critical for optimization decisions:\n$$\\text{Operational Intensity} = \\frac{\\text{FLOPs}}{\\text{Bytes Transferred}}$$\nModern accelerators require high operational intensity (often 50-200 FLOPs/byte) to achieve peak utilization. Most neural network layers fall below this threshold, making them memory-bound and explaining why memory bandwidth optimization often provides larger gains than raw compute speedup.\n\n**Compute-Bound vs. Memory-Bound**: For memory-bound operations (most activations, normalization, small batch inference), reducing memory traffic through operator fusion and caching provides larger gains than faster arithmetic. For compute-bound operations (large matrix multiplications in attention and FFN layers), exploiting tensor cores and reduced-precision arithmetic maximizes throughput.\n\nThe roofline model in @sec-ai-acceleration provides the quantitative framework for these decisions, enabling systematic analysis of whether a given workload benefits more from compute or memory optimizations.\n\n#### Architectural Innovation for Efficiency {#sec-efficient-ai-architectural-innovation-efficiency-7dd9}\n\nModern efficiency requires architectures designed for resource constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs can deliver high performance through architectural innovations rather than scaling up existing designs. Each architecture embodies a distinct efficiency principle:\n\n**Depthwise Separable Convolutions** (MobileNet): Traditional convolutions apply $D_K \\times D_K$ filters across all input channels simultaneously. Depthwise separable convolutions factor this into two operations: depthwise convolution (one $D_K \\times D_K$ filter per channel) followed by pointwise convolution (1x1 filters across channels). This achieves approximately 8-9x FLOP reduction for 3x3 kernels on typical channel counts, enabling mobile deployment while maintaining accuracy. The specific derivation of this reduction factor is provided in @sec-model-optimizations.\n\n**Compound Scaling** (EfficientNet): Rather than scaling depth, width, or resolution independently, compound scaling increases all three dimensions together using learned coefficients. This balanced scaling achieves better accuracy-efficiency trade-offs than arbitrary scaling, with EfficientNet-B7 achieving 84.3% ImageNet accuracy while using 8.4x fewer FLOPs than the best prior models.\n\n**Fire Modules** (SqueezeNet): Fire modules first \"squeeze\" channels to a smaller count using 1x1 convolutions, then \"expand\" using a mix of 1x1 and 3x3 convolutions. This squeeze-expand pattern reduces parameters while maintaining receptive field size.\n\n[^fn-mobilenet]: **MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achieving ~50x fewer parameters than traditional models. MobileNet-v1 has only 4.2M parameters vs. VGG-16's ~138M, enabling deployment on smartphones with <100MB memory.\n\n[^fn-efficientnet]: **EfficientNet**: Architecture achieving state-of-the-art accuracy with superior parameter efficiency. EfficientNet-B7 achieves 84.3% ImageNet top-1 accuracy (84.4% in some reports) with 66M parameters, compared to ResNet-152's 77.0% accuracy with approximately 60M parameters.\n\n[^fn-squeezenet]: **SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50x fewer parameters (1.25M vs. 60M). Demonstrated that clever architecture design can dramatically reduce model size without sacrificing performance.\n\nDifferent deployment contexts require different efficiency trade-offs. Cloud inference prioritizes throughput and can tolerate higher memory usage, favoring parallel-friendly operations. Edge deployment prioritizes latency and memory efficiency, requiring architectures that minimize memory access. Mobile deployment constrains energy usage, demanding architectures optimized for energy-efficient operations.\n\n#### Parameter-Efficient Adaptation {#sec-efficient-ai-parameterefficient-adaptation-1bce}\n\nParameter-efficient fine-tuning[^fn-param-efficient] techniques demonstrate how the three efficiency dimensions work together. These methods update less than 1% of model parameters while achieving full fine-tuning performance, addressing all three efficiency pillars: algorithmic efficiency through reduced parameter updates, compute efficiency through lower memory requirements and faster training, and data efficiency by leveraging pre-trained representations that require fewer task-specific examples.\n\n[^fn-param-efficient]: **Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model parameters while achieving full fine-tuning performance. Reduces memory requirements from gigabytes to megabytes for large model adaptation.\n\n**Low-Rank Adaptation (LoRA)** exemplifies the mathematical insight behind parameter-efficient methods. Instead of updating a weight matrix $W \\in \\mathbb{R}^{d \\times k}$ directly, LoRA learns a low-rank decomposition of the update:\n$$W' = W + BA$$\nwhere $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ with rank $r \\ll \\min(d, k)$. This reduces trainable parameters from $d \\times k$ to $r \\times (d + k)$, a reduction factor of $\\frac{dk}{r(d+k)}$. For a 175B parameter model with $r = 8$, this enables approximately 10,000x fewer trainable parameters.\n\nThe key insight is that task-specific updates have low intrinsic dimensionality: while pre-training requires the full parameter space to capture general knowledge, adaptation to specific tasks operates in a much smaller subspace. This explains why LoRA works for fine-tuning but not for training from scratch.\n\nThe practical impact is transformative: fine-tuning GPT-3 traditionally requires storing gradients for 175 billion parameters, consuming over 700GB of GPU memory. LoRA reduces this to under 10GB by learning low-rank decompositions of weight updates, enabling efficient adaptation on single consumer GPUs while requiring only hundreds of examples rather than thousands for effective adaptation.\n\nAs @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-efficient-alexnet]-level performance on ImageNet[^fn-efficient-imagenet] classification decreased by approximately $44\\times$ between 2012 and 2019. This improvement, which halved every 16 months, outpaced hardware efficiency gains of Moore's Law[^fn-efficient-moores-law], demonstrating the role of algorithmic advancements in driving efficiency [@Hernandez_et_al_2020].\n\n[^fn-efficient-alexnet]: **AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with 15.3% error rate, nearly halving the previous best of 26.2%. Used 60M parameters, two GPUs, and launched the deep learning revolution.\n\n[^fn-efficient-imagenet]: **ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove computer vision breakthroughs from 2010-2017.\n\n[^fn-efficient-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every ~2 years. Traditional Moore's Law predicted ~2x transistor density every 18-24 months, though this rate has slowed significantly since ~2015, while AI algorithmic efficiency improved 44x in 7 years (2012-2019).\n\n::: {#fig-algo-efficiency fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{axis}[\n   axis line style={draw=none},\n  width=17cm,\n  height=10cm,\n  date coordinates in=x,\n  table/col sep=comma,\n  xticklabel=\\year,\n  xtick={2013-01-01,2014-01-01,2015-01-01,2016-01-01,2017-01-01,2018-01-01,2019-01-01,2020-01-01},\n  x tick label style={rotate=0, anchor=north},\n  xmax=2020-1-31,\n  ytick={0,5,...,50},\n  ymin=0, ymax=50,\n  ylabel={Training Efficiency Factor},\n  title={44$\\times$ less compute required to get to AlexNet performance 7 years later (linear scale)},\n  enlargelimits=0.05,\n  grid=both,\n  major grid style={black!60},\n  nodes near coords align=right,\n        tick label style={/pgf/number format/assume math mode=true},\n        ticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n]\n\n\\addplot[RedLine,\n  only marks,\n  mark size=2pt,\n] table[x=Date, y=Y,  col sep=comma, meta=Model] {\nModel,Y,Date\nAlexNet, 1.17, 2012-06-01\nGoogLeNet, 4.5, 2014-09-19\nMobileNet\\_v1, 11.2, 2017-04-17\nShuffleNet, 20.8, 2017-07-03\nShuffleNet_v2, 24.85, 2018-06-29\nEfficientNet, 44.5, 2019-06-07\n};\n\n \\addplot[%above\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=2pt,xshift=1mm,\n  font=\\scriptsize\\usefont{T1}{phv}{m}{n}, anchor=south},\n] table[meta=Model, x=Date, y=Y, col sep=comma] {\nModel,Y,Date\nAlexNet, 1, 2012-06-01\nGoogLeNet, 4.3, 2014-09-17\nSqueezenet\\_v1\\_1,3.8,2016-02-25\n};\n\n \\addplot[%left\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={xshift=-1pt,\n  font=\\scriptsize\\usefont{T1}{phv}{m}{n}, anchor=east},\n] table[meta=Model, x=Date, y=Y, col sep=comma] {\nModel,Y,Date\nShuffleNet\\_v1 1x, 21, 2017-07-03\nEfficientNet-b0, 44, 2019-05-28\nVGG-11,0.83,2014-09-04\nResNet-18,2.88,2015-12-11\n};\n \\addplot[%right\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={xshift=1pt,\n  font=\\scriptsize\\usefont{T1}{phv}{m}{n}, anchor=west},\n] table[meta=Model, x=Date, y=Y, col sep=comma] {\nModel,Y,Date\nMobileNet\\_v2,13.3,2018-01-11\nDenseNet121,3.3,2016-09-25\nMobileNet\\_v1, 11.2, 2017-04-17\nShuffleNet\\_v2\\_1\\_5x,17.4,2018-06-29\nShuffleNet\\_v2, 24.85, 2018-06-29\n};\n\n\\addplot[draw=red,  only marks,\n  color=blue,\n  mark=*,  mark size=2pt,\n] table[\n  x=Date,\n  y=Y,\n  col sep=comma\n] {\nModel,Y,Date\nVGG-11,0.83,2014-09-04\nResNet-18,2.88,2015-12-11\nResNet-34,2.38,2015-12-11\nWide_ResNet\\_50,1.0,2016-05-22\nSqueezenet\\_v1\\_1,3.8,2016-02-25\nDenseNet121,3.3,2016-09-25\nResNext\\_50,2.5,2016-09-15\nMobileNet\\_v2,13.3,2018-01-11\nShuffleNet\\_v2\\_1\\_5x,17.4,2018-06-29\n};\n%\n\\coordinate (DL) at (axis description cs:-0.002,0.065);\n\\coordinate (GD) at (axis description cs:0.904,0.945);\n\\draw[black,dashed,thick](DL)to[out=3,in=248,distance=185](GD);\n\\end{axis}\n\\end{tikzpicture}\n```\n: **Algorithmic Efficiency Progress**: Neural network training compute requirements decreased 44× between 2012 and 2019, outpacing hardware improvements and demonstrating the significant impact of algorithmic advancements on model efficiency. Innovations in model architecture and optimization techniques can drive substantial gains in AI system sustainability via this halving of compute every 16 months. Source: [@Hernandez_et_al_2020].\n:::\n\nThe evolution of algorithmic efficiency, from basic compression to hardware-aware optimization and parameter-efficient adaptation, demonstrates the centrality of these techniques to machine learning progress. As the field advances, algorithmic efficiency will remain central to designing systems that are high-performing, scalable, and sustainable.\n\n### Compute Efficiency {#sec-efficient-ai-compute-efficiency-745c}\n\nCompute efficiency focuses on the effective use of hardware and computational resources to train and deploy machine learning models. It encompasses strategies for reducing energy consumption, optimizing processing speed, and leveraging hardware capabilities to achieve scalable and sustainable system performance. While this chapter focuses on efficiency principles and trade-offs, the detailed technical implementation of hardware acceleration—including GPU architectures, TPU (Tensor Processing Unit) design, memory systems, and custom accelerators—is covered in @sec-ai-acceleration.\n\n#### From CPUs to AI Accelerators {#sec-efficient-ai-cpus-ai-accelerators-a8d7}\n\nCompute efficiency's evolution reveals why specialized hardware became essential. In the early days of machine learning, Central Processing Units (CPUs) shaped what was possible. CPUs excel at sequential processing and complex decision-making but have limited parallelism, typically 4-16 cores optimized for diverse tasks rather than the repetitive matrix operations that dominate machine learning. Training times for models were measured in days or weeks, as even relatively small datasets pushed hardware boundaries.\n\nThis CPU-constrained era ended as deep learning models like AlexNet and ResNet[^fn-efficient-resnet] demonstrated the potential of neural networks, quickly surpassing traditional CPU capabilities. As shown in @fig-comp_efficiency, this marked the beginning of exponential growth in compute usage. OpenAI's analysis reveals that compute used in AI training increased approximately 300,000 times from 2012 to 2019, doubling approximately every 3.4 months during this period, a rate far exceeding Moore's Law [@Amodei_et_al_2018].\n\n[^fn-efficient-resnet]: **ResNet**: Residual Network architecture by He et al. [@he2016deep] enabling training of very deep networks (152+ layers) through skip connections. Won ImageNet 2015 with 3.6% error rate, surpassing human-level performance for the first time.\n\n::: {#fig-comp_efficiency fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{axis}[\n   axis line style={draw=none},\n   /pgf/number format/.cd,\n   tick label style={/pgf/number format/assume math mode=true},\n   ticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n  1000 sep={},\n  title={AlexNet to AlphaGo Zero: 300,000$\\times$ increase in compute},\n  xlabel={},\n  ylabel={Petaflop/s-days},\n  xmajorgrids,\n  ymajorgrids,\n  ymin=0.1e-4, ymax=1e4,\n  ymode=log,\n  log basis y=10,\n  ytick={1e-4,1e-2,1e0,1e2,1e4},\n  yticklabels={1e-4,1e-2,1e0,1e2,1e4},\n  xtick={2012,2013,2014,2015,2016,2017,2018},\n  xmin=2011.4,  xmax=2018.5,\n  grid=both,\n  width=13cm,\n  height=9cm,\n  yticklabel style={\n  /pgf/number format/.cd,\n  sci,\n  sci generic={mantissa e exponent},\n  precision=0\n},\n]\n\\addplot+[only marks, mark=*, mark size=1.5pt,\nmark options={fill=red}, color=red]\ntable[x=Date,y=Y, col sep=comma] {\nDate,Y,Model\n  2012.405,5.66e-3,AlexNet\n  2012.495,2.1e-3,Dropout\n  2013.855,5.8e-3,Visualizing and Understanding Conv Nets\n  2013.96,2.6e-5,DQN\n  2014.69,9.3e-2,Seq2Seq\n  2014.67,9.5e-2,VGG\n  2014.7,1.77e-2,GoogleNet\n  2015.92,2.54e-1,DeepSpeech2\n  2015.93,1.14e-1,ResNets\n  2016.72,8.2e1,Neural Machine Translation\n  2016.76,5.33e0,Xception\n  2016.83,3.3e1,Neural Architecture Search\n  2017.6,7.2e0,TI7 Dota 1v1\n  2017.92,4.3e2,AlphaZero\n  2017.79,1.9e3,AlphaGoZero\n};\n%\n\\addplot[%right\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=0pt,\n  align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=west},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2015.92,2.54e-1,DeepSpeech2\n2014.69,9.3e-2,Seq2Seq\n2014.7,1.77e-2,GoogleNet\n2013.855,5.8e-3,Visualizing and Understanding Conv Nets\n2013.96,2.6e-5,DQN\n};\n\\addplot[%left\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=0pt,\n  align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=east},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2016.72,8.2e1,Neural Machine Translation\n2016.83,3.3e1,Neural Architecture Search\n2017.92,4.3e2,AlphaZero\n2017.79,1.9e3,AlphaGoZero\n};\n%\n\\addplot[%below\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=0pt,\n  text width=25mm, align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=north},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2012.495,2.1e-3,Dropout\n2015.93,1.14e-1,ResNets\n2016.76,5.33e0,Xception\n2017.6,7.2e0,TI7 Dota 1v1\n};\n%\n \\addplot[%above\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=1pt,\n  text width=25mm, align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=south},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2012.405,5.66e-3,AlexNet\n2014.67,9.5e-2,VGG\n};\n%\n\\coordinate (DL) at (axis description cs:-0.02,0.025);\n\\coordinate (GD) at (axis description cs:1.02,0.888);\n\\coordinate (SR) at (axis description cs:0.5,-0.10);\n\\end{axis}\n\\draw[dashed](DL)--(GD);\n \\node[below=0 of SR,text width=125mm,font=\\fontsize{7pt}{9}\\selectfont\\usefont{T1}{phv}{m}{n}]{%\n The total amount of compute, in petaflop/s-days, used to train selected results that are\n relatively well known, used a lot of compute for their time, and gave enough information\n to estimate the compute used.};\n\\end{tikzpicture}\n\n```\n: **AI Training Compute Growth**: AI training experienced a 300,000-fold increase in computational requirements from 2012 to 2019, exceeding the growth rate predicted by Moore's Law and driving demand for specialized hardware [@Amodei_et_al_2018]. This exponential growth underscores the increasing complexity of AI models and the need for efficient computing infrastructure to support continued progress.\n:::\n\nThis rapid growth was driven by adoption of Graphics Processing Units (GPUs), which offered unparalleled parallel processing capabilities. While CPUs might have 16 cores, modern high-end GPUs like the NVIDIA H100 contain over 16,000 CUDA cores[^fn-cuda-cores]. Specialized hardware accelerators such as Google's Tensor Processing Units (TPUs) further revolutionized compute efficiency by designing chips specifically for machine learning workloads, optimizing for specific data types and operations most common in neural networks.\n\n[^fn-cuda-cores]: **CUDA Cores**: NVIDIA's parallel processing units optimized for floating-point operations. Unlike CPU cores (designed for complex sequential tasks), CUDA cores are simpler and work together, enabling a single H100 GPU to perform 16,896 parallel operations simultaneously for massive speedup in matrix computations.\n\n#### Sustainable Computing and Energy Awareness {#sec-efficient-ai-sustainable-computing-energy-awareness-d77a}\n\nAs systems scale further, compute efficiency has become closely tied to sustainability. Training state-of-the-art large language models requires massive computational resources, leading to increased attention on environmental impact. The projected electricity usage of data centers, shown in @fig-datacenter-energy-usage, highlights this concern. Between 2010 and 2030, electricity consumption is expected to rise sharply, particularly under worst-case scenarios where it could exceed 8,000 TWh by 2030 [@jones2018much].\n\n::: {#fig-datacenter-energy-usage fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{axis}[\n  axis line style={draw=none},\n  width=16cm,\n  height=10cm,\n  table/col sep=comma,\n  x tick label style={rotate=0, anchor=north},\n  xmin=2009.5,xmax=2030,\n  ymin=250, ymax=8300,\n  ytick={2000,4000,6000,8000},\n  ylabel={Electricity Usage (TWh)},\n  xlabel={Year},\n   legend style={at={(0.15,0.9)}, anchor=north},\n   legend cell align=left,\n   legend style={fill=BrownL!40,draw=BrownLine,row sep=1.85pt,\n   font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n  grid=both,\n  minor tick num=1,\n  major grid style={black!80},\n  minor grid style={black!40},\n    /pgf/number format/.cd,\n  1000 sep={},\n  nodes near coords align=right,\n        tick label style={/pgf/number format/assume math mode=true},\n        ticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n    cycle multi list={\n     red,blue,green\\nextlist\n     solid\\nextlist\n     mark=o,mark=none,mark=triangle,mark=none,mark=,mark=none\n     },\n]\n\\addplot+[mark=*,line width=2pt,\nred] table[x=Date,y=Y, col sep=comma] {\nY,Date\n500, 2010\n510, 2012\n520, 2014\n540, 2016\n560, 2018\n580, 2020\n600, 2022\n630, 2024\n660, 2026\n690, 2028\n700, 2030\n};\n\\addplot+[mark=triangle*, mark size=3pt,cyan!90!black,\nline width=2pt] table[x=Date,y=Y, col sep=comma] {\nY,Date\n500, 2010\n550, 2012\n600, 2014\n680, 2016\n760, 2018\n860, 2020\n1000, 2022\n1200, 2024\n1600, 2026\n2000, 2028\n2967, 2030\n};\n\\addplot+[mark=square*,line width=2pt, mark size=2.5pt,\ngreen!70!black] table[x=Date,y=Y, col sep=comma] {\nY,Date\n500, 2010\n600, 2012\n750, 2014\n1000, 2016\n1250, 2018\n1600, 2020\n2200, 2022\n3000, 2024\n4500, 2026\n6000, 2028\n7933, 2030\n};\n \\legend{Best, Expected, Worst}\n\\coordinate (legend) at (axis description cs:0.15,0.92);\n\\end{axis}\n\\node[fill=white,above=2pt of legend,anchor=center]{\\small\\bfseries Scenario};\n\\end{tikzpicture}\n```\n: **Data Center Energy Projections**: Between 2010 and 2030, data center electricity usage is projected to increase sharply, particularly under worst-case scenarios where consumption could exceed 8,000 TWh by 2030 [@jones2018much]. This projection underscores the critical need for improved energy efficiency in AI systems.\n:::\n\nThis dramatic growth underscores urgency for compute efficiency, as even large data centers face energy constraints due to limitations in electrical grid capacity. Efficiency improvements alone may not guarantee environmental benefits due to a phenomenon known as Jevons Paradox.\n\nConsider the invention of the fuel-efficient car. While each car uses less gas per mile, the lower cost of driving encourages people to drive more often and live further from work. The result can be an *increase* in total gasoline consumption. This is Jevons Paradox: efficiency gains can be offset by increased consumption. In AI, this means making models 10x more efficient might lead to a 100x increase in their use, resulting in a net negative environmental impact if not managed carefully.\n\nAddressing these challenges requires optimizing hardware utilization and minimizing energy consumption in both cloud and edge contexts while being mindful of potential rebound effects from increased deployment.\n\nKey trends include adoption of energy-aware scheduling and resource allocation techniques that distribute workloads efficiently across available hardware [@Patterson_et_al_2021]. Researchers are also developing methods to dynamically adjust precision levels during training and inference, using lower precision operations (e.g., mixed-precision training) to reduce power consumption without sacrificing accuracy.\n\nDistributed systems achieve compute efficiency by splitting workloads across multiple machines. Techniques such as model parallelism[^fn-efficient-model-parallelism] and data parallelism[^fn-efficient-data-parallelism] allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput while minimizing idle time.\n\n[^fn-efficient-model-parallelism]: **Model Parallelism**: Distributing model components across multiple processors due to memory constraints. GPT-3 (175B parameters) requires 350GB memory, exceeding A100's 40GB capacity by 9×, necessitating tensor parallelism where each transformer layer splits across 8-16 GPUs with all-gather communication for activation synchronization.\n\n[^fn-efficient-data-parallelism]: **Data Parallelism**: Training method where the same model runs on multiple processors with different data batches. GPT-3 training used data parallelism across thousands of GPUs, processing multiple text sequences simultaneously.\n\nAt the edge, compute efficiency addresses growing demand for real-time processing in energy-constrained environments. Innovations such as hardware-aware model optimization, lightweight inference engines, and adaptive computing architectures enable highly efficient edge systems critical for applications like autonomous vehicles and smart home devices.\n\n#### Production Deployment Patterns {#sec-efficient-ai-production-deployment-patterns-208a}\n\nReal-world efficiency optimization demonstrates practical impact across deployment contexts. Production systems routinely achieve 5-10x efficiency gains through coordinated application of optimization techniques while maintaining 95%+ of original model performance.\n\nMobile applications achieve 4-7x model size reduction and 3-5x latency improvements through combined quantization, pruning, and distillation, enabling real-time inference on mid-range devices. Modern mobile AI systems distribute workloads across specialized processors (NPU for ultra-low power inference, GPU for parallel compute, CPU for control logic) based on power, performance, and real-time constraints.\n\nAutonomous vehicle systems optimize for safety-critical <10ms latency requirements through hardware-aware architectural design and mixed-precision quantization, processing multiple high-bandwidth sensor streams within strict power and thermal constraints.\n\nCloud serving infrastructure reduces costs by 70-80% through systematic optimization combining dynamic batching, quantization, and knowledge distillation, serving 4-5x more requests at comparable quality levels.\n\nEdge IoT deployments achieve month-long battery life through extreme model compression and duty-cycle optimization, operating on milliwatt power budgets while maintaining acceptable accuracy for practical applications.\n\nThese efficiency gains emerge from systematic optimization strategies that coordinate multiple techniques rather than applying individual optimizations in isolation. The specific optimization sequences, technique combinations, and engineering practices that enable these production results are detailed in @sec-model-optimizations.\n\nCompute efficiency complements algorithmic and data efficiency. Compact models reduce computational requirements, while efficient data pipelines streamline hardware usage. The evolution of compute efficiency (from early reliance on CPUs through specialized accelerators to sustainable computing practices) remains central to building scalable, accessible, and environmentally responsible machine learning systems.\n\n### Data Efficiency {#sec-efficient-ai-data-efficiency-a3ad}\n\nData efficiency focuses on optimizing the amount and quality of data required to train machine learning models effectively. Data efficiency has emerged as a pivotal dimension, driven by rising costs of data collection, storage, and processing, as well as the limits of available high-quality data.\n\n#### Maximizing Learning from Limited Data {#sec-efficient-ai-maximizing-learning-limited-data-2885}\n\nIn early machine learning, data efficiency was not a primary focus, as datasets were relatively small and manageable. The challenge was often acquiring enough labeled data to train models effectively. Researchers relied on curated datasets such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], using feature selection and dimensionality reduction techniques like principal component analysis (PCA)[^fn-pca] to extract maximum value from limited data.\n\n[^fn-uci]: **UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine, one of the most widely-used resources for machine learning datasets. Contains over 600 datasets and has been cited in thousands of research papers.\n\n[^fn-pca]: **Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearson in 1901, identifies the most important directions of variation in data. Reduces computational complexity while preserving 90%+ of data variance in many applications.\n\nThe advent of deep learning in the 2010s transformed data's role. Models like AlexNet and GPT-3 demonstrated that larger datasets often led to better performance, marking the beginning of the \"big data\" era. However, this reliance introduced inefficiencies. Data collection became costly and time-consuming, requiring vast amounts of labeled data for supervised learning.\n\nResearchers developed techniques enhancing data efficiency even as datasets grew. Transfer learning[^fn-efficient-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing task-specific data needs [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] artificially expanded datasets by creating new variations of existing samples. Active learning[^fn-active-learning] prioritized labeling only the most informative data points [@Settles_2009].\n\n[^fn-efficient-transfer-learning]: **Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for specific tasks. ImageNet pre-trained models can achieve high accuracy on new vision tasks with <1000 labeled examples vs. millions needed from scratch.\n\n[^fn-data-augmentation]: **Data Augmentation**: Artificially expanding datasets through transformations like rotations, crops, or noise. Can improve model performance by 5-15% and reduce overfitting, especially when labeled data is scarce.\n\n[^fn-active-learning]: **Active Learning**: Iteratively selecting the most informative samples for labeling to maximize learning efficiency. Can achieve target performance with 50-90% less labeled data compared to random sampling.\n\nAs systems continue growing in scale, inefficiencies of large datasets have become apparent. Data-centric AI[^fn-data-centric-ai] has emerged as a key paradigm, emphasizing data quality over quantity. This approach focuses on enhancing preprocessing, removing redundancy, and improving labeling efficiency. Research shows that careful curation and filtering can achieve comparable or superior performance while using only a fraction of original data volume [@penedo2024fineweb].\n\n[^fn-data-centric-ai]: **Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by Andrew Ng in 2021. Focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains.\n\nSeveral techniques support this transition. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures training to progress from simple to complex examples, improving learning efficiency.\n\n[^fn-self-supervised]: **Self-Supervised Learning**: Training method where models create their own labels from input data structure, like predicting masked words in BERT. Enables learning from billions of unlabeled examples.\n\n[^fn-curriculum-learning]: **Curriculum Learning**: Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education. Can improve convergence speed by 25-50% and final model performance.\n\nData efficiency is particularly important in foundation models[^fn-efficient-foundation-models]. As these models grow in scale and capability, they approach limits of available high-quality training data, especially for language tasks, as shown in @fig-running-out-of-human-data. This scarcity drives innovation in data processing and curation techniques.\n\n[^fn-efficient-foundation-models]: **Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks. Term coined by Stanford HAI in 2021, includes models like GPT-3, BERT, and DALL-E.\n\n![**Dataset Growth**: Foundation models are increasingly trained on vast datasets, reflecting the growing stock of human-generated text. This trend underscores the challenge of data scarcity in maintaining model performance as scale increases. Source: @villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024.](images/png/running_out_of_data.png){#fig-running-out-of-human-data}\n\nEvidence for data quality's impact appears across different deployment scales. In TinyML[^fn-tinyml] applications, datasets like Wake Vision demonstrate how performance critically depends on careful data curation [@banbury2024wakevisiontailoreddataset]. At larger scales, research on language models trained on web-scale datasets shows that intelligent filtering and selection strategies significantly improve performance on downstream tasks [@penedo2024fineweb]. @sec-benchmarking-ai establishes rigorous methodologies for measuring these data quality improvements.\n\n[^fn-tinyml]: **TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW power consumption. Enables AI in IoT devices, wearables, and sensors where traditional ML deployment is impossible.\n\nThis modern era of data efficiency represents a shift in how systems approach data utilization. By focusing on quality over quantity and developing sophisticated techniques for data selection and processing, the field is moving toward more sustainable and effective approaches to model training and deployment. Data efficiency is integral to scalable systems, impacting both model and compute efficiency. Smaller, higher-quality datasets reduce training times and computational demands while enabling better generalization. These principles also complement privacy-preserving techniques, where minimizing data requirements enhances both efficiency and user privacy protection.\n\n## Real-World Efficiency Strategies {#sec-efficient-ai-realworld-efficiency-strategies-8387}\n\nHaving explored each efficiency dimension individually and their interconnections, we examine how these dimensions manifest across different deployment contexts. The efficiency of machine learning systems emerges from understanding relationships between algorithmic, compute, and data efficiency in specific operational environments.\n\n### Context-Specific Efficiency Requirements {#sec-efficient-ai-contextspecific-efficiency-requirements-47e6}\n\nThe specific priorities and trade-offs vary dramatically across deployment environments. As our opening examples illustrated, these range from cloud systems with abundant resources to edge devices with severe memory and power constraints. @tbl-deployment-efficiency-priorities maps how these constraints translate into efficiency optimization priorities.\n\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **Deployment Context** | **Primary Constraints**              | **Efficiency Priorities**                       | **Representative Applications**                                     |\n+:=======================+:=====================================+:================================================+:====================================================================+\n| **Cloud**              | Cost at scale, energy consumption    | Throughput, scalability, operational efficiency | Large language model APIs, recommendation engines, video processing |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **Edge**               | Latency, local compute capacity,     | Real-time performance, power efficiency         | Autonomous vehicles, industrial automation, smart cameras           |\n|                        | connectivity                         |                                                 |                                                                     |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **Mobile**             | Battery life, memory, thermal limits | Energy efficiency, model size, responsiveness   | Voice assistants, photo enhancement, augmented reality              |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **TinyML**             | Extreme power/memory constraints     | Ultra-low power, minimal model size             | IoT sensors, wearables, environmental monitoring                    |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n\n: **Efficiency Optimization Priorities by Deployment Context**: Each environment demands different trade-offs between algorithmic, compute, and data optimization strategies based on unique constraints. Cloud systems prioritize scalability, edge deployments focus on real-time performance, mobile applications balance performance with battery life, and TinyML demands extreme resource efficiency. {#tbl-deployment-efficiency-priorities}\n\nUnderstanding these context-specific patterns enables designers to make informed decisions about which efficiency dimensions to prioritize and how to navigate inevitable trade-offs.\n\n### Scalability and Sustainability {#sec-efficient-ai-scalability-sustainability-4d30}\n\nSystem efficiency serves as a driver of environmental sustainability. When systems are optimized for efficiency, they can be deployed at scale while minimizing environmental footprint. This relationship creates a positive feedback loop, as shown in @fig-virtuous-efficiency-cycle.\n\n::: {#fig-virtuous-efficiency-cycle fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.7}{%\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\def\\ra{40mm}\n\\draw (90: 0.5*\\ra) node[yshift=-2pt](EF){Efficiency};\n\\draw (210: 0.5*\\ra) node(SC){Scalability};\n\\draw (330: 0.5*\\ra) node(SU){Sustainability};\n\\node[right=of EF]{};\n\n\\draw[-{Triangle[width=18pt,length=8pt]}, line width=10pt,violet!60] (340:0.5*\\ra)\narc[radius=0.5*\\ra, start angle=-20, end angle= 67];\n\n\\draw[-{Triangle[width=18pt,length=8pt]}, line width=10pt,cyan!80!black!90] (113:0.5*\\ra)\narc[radius=0.5*\\ra, start angle=113, end angle= 200];\n\n\\draw[-{Triangle[width=18pt,length=8pt]}, line width=10pt,orange!70] (220:0.5*\\ra)\narc[radius=0.5*\\ra, start angle=220, end angle= 320];\n\\end{tikzpicture}}\n```\n: **Efficiency and Sustainability Feedback Loop**: Optimized machine learning systems achieve greater scalability, which in turn incentivizes sustainable design practices and further efficiency improvements, creating a reinforcing feedback loop for long-term impact.\n:::\n\nEfficient systems are inherently scalable. Reducing resource demands through lightweight models, targeted datasets, and optimized compute utilization allows systems to deploy broadly. When efficient systems scale, they amplify their contribution to sustainability by reducing overall energy consumption and computational waste. Sustainability reinforces the need for efficiency, creating a feedback loop that strengthens the entire system.\n\n## Efficiency Trade-offs and Challenges {#sec-efficient-ai-efficiency-tradeoffs-challenges-946d}\n\nThe three efficiency dimensions can work synergistically under favorable conditions, but real-world systems often face scenarios where improving one dimension degrades another. The same resource constraints that make efficiency necessary force difficult choices: reducing model size may sacrifice accuracy, optimizing for real-time performance may increase energy consumption, and curating smaller datasets may limit generalization.\n\n### Fundamental Sources of Efficiency Trade-offs {#sec-efficient-ai-fundamental-sources-efficiency-tradeoffs-d16f}\n\nThese tensions manifest in various ways across machine learning systems. Understanding their root causes is essential for addressing design challenges. Each efficiency dimension influences the others, creating a dynamic interplay that shapes system performance.\n\n#### Algorithmic Efficiency vs. Compute Requirements {#sec-efficient-ai-algorithmic-efficiency-vs-compute-requirements-83a7}\n\nAlgorithmic efficiency focuses on designing compact models that minimize computational and memory demands. By reducing model size or complexity, deployment on resource-limited devices becomes feasible. Overly simplifying a model can reduce accuracy, especially for complex tasks. To compensate for this loss, additional computational resources may be required during training or deployment, placing strain on compute efficiency.\n\n#### Compute Efficiency vs. Real-Time Needs {#sec-efficient-ai-compute-efficiency-vs-realtime-needs-a269}\n\nCompute efficiency aims to minimize resources required for training and inference, reducing energy consumption, processing time, and memory use. In scenarios requiring real-time responsiveness (autonomous vehicles, augmented reality), compute efficiency becomes harder to maintain. @fig-efficiency-vs-latency illustrates this challenge: real-time systems often require high-performance hardware to process data instantly, conflicting with energy efficiency goals or increasing system costs.\n\n::: {#fig-efficiency-vs-latency fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{%\n    Line/.style={line width=1.0pt,violet!50,text=black},\n    stop/.style = {regular polygon, regular polygon sides=8,\n      draw=red, double, double distance=1.0mm,  thick,\n      fill=red, font=\\usefont{T1}{phv}{m}{n}\\Huge\\bfseries, text=white,\n      inner sep=0mm},\n    warning/.style = {regular polygon, regular polygon sides=3,line width=1.5pt,\n      draw=red,\n      fill=white, font=\\Huge\\bfseries, text=black,\n      inner ysep=3pt, node contents={!}},\n pics/car/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n            \\begin{scope}[shift={(0,0)},rotate=0,scale=\\scalefac,, every node/.append style={transform shape}]\n                %\n                \\draw[\\drawchannelcolor,line width=\\Linewidth,x=1mm,y=1mm,yscale=-1,xscale=-1,fill=\\channelcolor!50] (59.9429,0.0029) .. controls\n                (58.2798,0.0161) and (56.5224,0.0709) .. (54.6592,0.1699) .. controls (51.8698,0.3182) and (49.2785,0.7036) ..\n                (46.8955,1.2407) .. controls (46.9004,1.2391) and (46.9067,1.2365) .. (46.9116,1.2349) .. controls\n                (35.0588,3.3135) and (25.0020,10.1030) .. (25.0020,10.1030) -- (24.1113,10.1660) .. controls\n                (22.2803,10.1061) and (21.6259,10.2123) .. (17.5122,11.0391) .. controls (15.2265,11.1391) and\n                (13.1653,11.4703) .. (11.3730,11.9180) .. controls (11.2904,11.9383) and (11.2097,11.9609) ..\n                (11.1284,11.9824) .. controls (8.6666,12.6223) and (6.7447,13.4848) .. (5.6074,14.3101) ..\n                controls (2.5699,14.9763) and (0.3984,16.7520) .. (0.3984,16.7520) .. controls (-0.1586,17.2949) and\n                (0.0797,17.2023) .. (0.0044,17.6191) .. controls (-0.0709,18.0360) and (0.7119,21.0322) .. (0.7119,21.0322) ..\n                controls (0.7119,21.0322) and (0.0821,22.9131) .. (0.5215,23.0918) .. controls (0.9609,23.2703) and (1.0903,23.4957) ..\n                (1.4604,24.4233) .. controls (-0.8220,25.6494) and (0.4983,26.3315) .. (1.5059,26.9150) .. controls\n                (2.5136,27.4983) and (5.1650,28.1973) .. (6.5098,27.9229) .. controls (6.4949,27.8726) and\n                (6.4886,27.8209) .. (6.4746,27.7705) -- (8.3862,26.9062) -- (23.4346,26.2646) -- (25.2979,27.3164) ..\n                controls (25.3045,27.3313) and (25.3242,27.3955) .. (25.3242,27.3955) .. controls (25.3242,27.3955)\n                and (25.5918,27.6023) .. (26.2236,27.4849) .. controls (27.8013,27.0856) and (67.5264,26.7188) ..\n                (67.5264,26.7188) .. controls (67.5264,26.7188) and (71.0655,26.7059) .. (72.3955,27.2095) ..\n                controls (72.9263,27.4105) and (73.2239,27.3453) .. (73.4019,27.1245) .. controls (73.7709,27.0085)\n                and (75.1701,26.5817) .. (75.4629,26.5400) .. controls (75.7840,26.4940) and (90.4210,25.8970) ..\n                (90.3750,25.8970) .. controls (90.3293,25.8970) and (92.2559,26.6777) .. (92.2559,26.6777) ..\n                controls (92.2559,26.6777) and (92.3225,26.6082) .. (92.3320,26.5986) .. controls (92.5830,26.6361)\n                and (92.9367,26.6106) .. (93.4336,26.4961) .. controls (95.4068,26.0414) and (96.8291,25.3066) ..\n                (96.8291,25.3066) .. controls (96.8291,25.3066) and (98.1069,23.5919) .. (98.3862,22.9688) ..\n                controls (98.6655,22.3454) and (98.4976,22.1118) .. (98.4976,22.1118) .. controls (98.4976,22.1118)\n                and (98.8375,20.8511) .. (99.2549,19.8252) .. controls (99.6719,18.8000) and (99.6148,18.6385) ..\n                (98.9854,18.0322) .. controls (98.2215,17.0284) and (97.8547,14.8710) .. (98.0010,13.9409) ..\n                controls (98.0616,13.5558) and (98.0431,13.1384) .. (98.0083,12.7661) .. controls (98.0515,11.7298)\n                and (97.7331,10.8516) .. (97.4692,10.3418) .. controls (97.3419,9.9538) and (97.2028,9.5918) ..\n                (97.0620,9.4497) .. controls (96.6727,9.0568) and (97.2353,8.9554) .. (97.7930,8.6543) ..\n                controls (98.3509,8.3530) and (97.8727,8.0535) .. (97.5088,8.0420) .. controls (97.1451,8.0305)\n                and (96.4688,7.9805) .. (96.4688,7.9805) .. controls (95.4388,7.9064) and (92.8843,6.7387) ..\n                (85.3447,4.1309) -- (85.3271,4.1133) .. controls (85.3259,4.1146) and (85.3240,4.1207) ..\n                (85.3228,4.1221) .. controls (85.3044,4.1157) and (85.2943,4.1123) .. (85.2759,4.1060) .. controls\n                (78.6238,1.8073) and (71.5847,-0.0896) .. (59.9429,0.0029) -- cycle;\n%\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!50!gray,line width=2*\\Linewidth] (-2,-2.55) coordinate (w1)  circle (0.8);\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!99!gray,line width=2*\\Linewidth] (-2,-2.55) coordinate (w1)  circle (0.25);\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!50!gray,line width=2*\\Linewidth] (-8,-2.55) coordinate (w2)  circle (0.8);\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!99!gray,line width=2*\\Linewidth] (-8,-2.55) coordinate (w2)  circle (0.25);\n                \\draw[\\drawchannelcolor,fill=\\channelcolor!20,line width=\\Linewidth] (-9.8,-0.85) -- (-2.52,-0.99)\n                 to[out=150,in=345](-4.5,-0.16) to[out=170,in=7](-7.5,-0.12)to[out=190,in=17](-9.8,-0.85);\n                \\draw[\\drawchannelcolor,line width=2*\\Linewidth](-5,-0.96)--(-5,-0.1);\n                \\draw[\\drawchannelcolor,line width=2*\\Linewidth](-8,-0.9)--(-8,-0.22);\n            \\end{scope}\n   }\n  }\n}\n\n\\tikzset{%\n pics/danger/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=DANGER1,shift={(0,0)},rotate=0,scale=\\scalefac,every node/.append style={transform shape}]\n\\node[red]{$\\triangle$};\n\\node[yshift=0.125ex, scale=0.5]{!};\n\\end{scope}\n   }\n  }\n}\n\n\\pgfkeys{\n  /channel/.cd,\n  channelcolor/.store in=\\channelcolor,\n  drawchannelcolor/.store in=\\drawchannelcolor,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  channelcolor=BrownLine,\n  drawchannelcolor=BrownLine,\n  scalefac=1,\n  Linewidth=1.6pt,\n  picname=C\n}\n% #1 number of teeth\n% #2 radius intern\n% #3 radius extern\n% #4 angle from start to end of the first arc\n% #5 angle to decale the second arc from the first\n% #6 inner radius to cut off\n\\newcommand{\\gear}[6]{%\n  (0:#2)\n  \\foreach \\i [evaluate=\\i as \\n using {\\i-1)*360/#1}] in {1,...,#1}{%\n    arc (\\n:\\n+#4:#2) {[rounded corners=1.5pt] -- (\\n+#4+#5:#3)\n    arc (\\n+#4+#5:\\n+360/#1-#5:#3)} --  (\\n+360/#1:#2)\n  }%\n  (0,0) circle[radius=#6];\n}\n\\begin{scope}[local bounding box=CAR1,shift={($(0,0)+(1,1.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){car={scalefac=0.5,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine,drawchannelcolor=black!70}};\n \\end{scope}\n  \\node[below=of CAR1]{120 km/h};\n\n\\begin{scope}[local bounding box=WAY1,shift={($(CAR1)+(0.5,-0.7)$)},scale=1, every node/.append style={transform shape}]\n\\draw[draw=none,fill=brown!60](1.4,0)--(1.9,-0.35)to[out=330,in=30](1.5,-0.8)to[out=210,in=150,distance=9](1.5,-1.3)--(2.3,-1.9)\nto[out=330,in=30,distance=9](2.25,-2.37)--(-0.8,-3.75)--(1.6,-3.75)to[out=10,in=250,distance=5] (3.45,-2.5)\nto[out=55,in=310,distance=9](3.4,-1.9)to[out=140,in=340](2.32,-1.3)to[out=160,in=220](1.92,-0.8)\nto[out=35,in=330](2.1,-0.35)--(1.4,0);\n\\draw[white,line width=1.5pt](2.0,-0.35)to[out=330,in=30](1.72,-0.8)\nto[out=210,in=150,distance=9](1.9,-1.3)--(2.9,-1.9)to[out=330,in=30,distance=9](2.7,-2.5)--(0.45,-3.75);\n \\end{scope}\n  \\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=2mm,inner ysep=5mm,minimum height=64mm,\nyshift=2.5mm,fill=BackColor!30,fit=(CAR1)(WAY1),line width=1pt](BB2){};\n\\node[below=4pt of  BB2.north,inner sep=0pt,anchor=north]{\\textbf{Driving Simulator}};\n%gears\n\\begin{scope}[local bounding box=GEAR,shift={($(BB2)+(8.5,0.55)$)},\nscale=4.5,every node/.append style={transform shape}]\n\\colorlet{black}{brown!50!black}\n\\fill[draw=none,fill=black,even odd rule,xshift=-2mm]coordinate(GE1)\\gear{10}{0.23}{0.28}{10}{2}{0.1};\n\\fill[draw=none,fill=black,even odd rule,xshift=1.9mm,yshift=-3.0mm]coordinate(GE2)\\gear{10}{0.18}{0.22}{10}{2}{0.08};\n\\end{scope}\n   \\scoped[on background layer]\n\\node[draw=BrownLine,inner xsep=8,inner ysep=8,yshift=1.5mm,\nminimum width=55mm,minimum height=64mm,\n           fill=BrownL!50,fit=(GE1)(GE2),line width=1.0pt](BB1){};\n\\node[above=6pt of BB1.south,align=center]{Latency = 100 ms};\n\\node[below=2pt of BB1.north,align=center]{\\textbf{XYZ Simulator}\\\\ (e.g. GNSS)};\n%%\n\\node[below=5pt of BB2.south,align=center](DS){120 km/h = 3.33 m/s\\\\ 1 s: 33.33 m\\\\\n\\textbf{1 ms: 0.033 m}};\n\\node[below=5pt of BB1.south,align=center](DS1){\\vphantom{120 km/h = 3.33 m/s}\\\\\n\\vphantom{1 s: 33.33 m}\\\\ \\textbf{100 ms: 3.33 m}};\n\\draw[Line,-latex]([yshift=2mm]DS.south east)--([yshift=2mm]DS1.south west);\n\\draw[Line,-latex](BB2)--node[above]{Vehicle}node[below]{Dynamics}(BB1);\n\\draw[Line,latex-](BB2.west)--++(-1,0)--++(0,-5)--node[below=2mm,scale=0.23,stop,pos=0.44](STOP){\\textbf{STOP}}++(16.5,0)|-\n(BB1.east);\n\n\\begin{scope}[local bounding box=DANGER,shift={($(BB2.north)+(0,0.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){danger={scalefac=4.0,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine,drawchannelcolor=black!70}}node[left=6mm,red]{\\large Obstacle ahead};\n \\end{scope}\n\\node[anchor=west,red] at ($(STOP.east) + (0.2,0)$) {Avoid Collision};\n%%\n\\begin{scope}[local bounding box=CAR2,shift={($(BB1.east)+(6,1.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){car={scalefac=0.5,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine,drawchannelcolor=black!70}};\n \\end{scope}\n \\begin{scope}[local bounding box=CAR3,shift={($(CAR2.south)+(4,-3.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){car={scalefac=0.5,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine!30!,drawchannelcolor=black!10}};\n \\end{scope}\n \\node[above=18pt of CAR2,align=center]{Where were you in\\\\ a real life scenario?};\n \\draw[Line,latex-latex](CAR2)--node[left]{Uncertainty = 3.33 m}(CAR3);\n \\node[above=13pt of $(BB2.north east)!0.5!(BB1.north west)$]{\\large Why latency matters?};\n\\end{tikzpicture}\n```\n: **Real-Time System Constraints**: Autonomous vehicles demand careful balance between computational efficiency and low latency. Increasing processing power to reduce delay can conflict with energy and cost limitations, yet sacrificing latency compromises safety by increasing reaction time and braking distance.\n:::\n\n#### Data Efficiency vs. Model Generalization {#sec-efficient-ai-data-efficiency-vs-model-generalization-044a}\n\nData efficiency seeks to minimize the amount of data required to train a model without sacrificing performance. By curating smaller, high-quality datasets, training becomes faster and less resource-intensive. Ideally, this reinforces both algorithmic and compute efficiency. However, reducing dataset size can limit diversity, making it harder for models to generalize to unseen scenarios. To address this, additional compute resources or model complexity may be required, creating tension between data efficiency and broader system goals.\n\n### Recurring Trade-off Patterns in Practice {#sec-efficient-ai-recurring-tradeoff-patterns-practice-c205}\n\nThe trade-offs between efficiency dimensions become particularly evident when examining specific scenarios. Complex models with millions or billions of parameters can achieve higher accuracy by capturing intricate patterns, but require significant computational power and memory. A recommendation system in a cloud data center might use a highly complex model for better recommendations, but at the cost of higher energy consumption and operating costs. On resource-constrained devices like smartphones or autonomous vehicles, compact models may operate efficiently but require more sophisticated data preprocessing or training procedures to compensate for reduced capacity.\n\nEnergy efficiency and real-time performance often pull systems in opposite directions. Real-time systems like autonomous vehicles or augmented reality applications rely on high-performance hardware to process large volumes of data quickly, but this typically increases energy consumption. An autonomous vehicle must process sensor data from cameras, LiDAR, and radar in real time to make navigation decisions, requiring specialized accelerators that consume significant energy. In edge deployments with battery power or limited energy sources, this trade-off becomes even more critical.\n\nLarger datasets generally provide greater diversity and coverage, enabling models to capture subtle patterns and reduce overfitting risk. However, computational and memory demands of training on large datasets can be substantial. In resource-constrained environments like TinyML deployments, an IoT device monitoring environmental conditions might need a model that generalizes well across varying conditions, but collecting extensive datasets may be impractical due to storage and computational limitations. Smaller, carefully curated datasets or synthetic data may be used to reduce computational strain, but this risks missing key edge cases.\n\nThese trade-offs are not merely academic concerns but practical realities that shape system design decisions across all deployment contexts.\n\n## Strategic Trade-off Management {#sec-efficient-ai-strategic-tradeoff-management-0ac8}\n\nThe trade-offs inherent in machine learning system design require thoughtful strategies to navigate effectively. Achieving the right balance involves difficult decisions heavily influenced by specific goals and constraints of the deployment environment. Designers can adopt a range of strategies that address unique requirements of different contexts.\n\n### Environment-Driven Efficiency Priorities {#sec-efficient-ai-environmentdriven-efficiency-priorities-4057}\n\nEfficiency goals are rarely universal. The specific demands of an application or deployment scenario heavily influence which dimension—algorithmic, compute, or data—takes precedence. Prioritizing the right dimensions based on context is the first step in effectively managing trade-offs.\n\nIn Mobile ML deployments, battery life is often the primary constraint, placing a premium on compute efficiency. Energy consumption must be minimized to preserve operational time, so lightweight models are prioritized even if it means sacrificing some accuracy or requiring additional data preprocessing.\n\nIn Cloud ML systems, scalability and throughput are paramount. These systems must process large volumes of data and serve millions of users simultaneously. While compute resources are more abundant, energy efficiency and operational costs remain important. Algorithmic efficiency plays a critical role in ensuring systems can scale without overwhelming infrastructure.\n\nEdge ML systems present different priorities. Autonomous vehicles or real-time monitoring systems require low-latency processing for safe and reliable operation, making real-time performance and compute efficiency paramount, often at the expense of energy consumption. However, hardware constraints mean these systems must still carefully manage energy and computational resources.\n\n**TinyML** deployments demand extreme efficiency due to severe hardware and energy limitations. Algorithmic and data efficiency are top priorities, with models highly compact and capable of operating on microcontrollers with minimal memory and compute power, while training relies on small, carefully curated datasets.\n\n### Dynamic Resource Allocation at Inference {#sec-efficient-ai-dynamic-resource-allocation-inference-d6bc}\n\nSystem adaptability can be enhanced through dynamic resource allocation during inference. This approach recognizes that resource needs may fluctuate even within specific deployment contexts. By adjusting computational effort at inference time, systems can fine-tune performance to meet immediate demands.\n\nFor example, a cloud-based video analysis system might process standard streams with a streamlined model to maintain high throughput, but when a critical event is detected, dynamically allocate more resources to a complex model for higher precision. Similarly, mobile voice assistants might use lightweight models for routine commands to conserve battery, but temporarily activate resource-intensive models for complex queries.\n\nImplementing test-time compute introduces new challenges. Dynamic resource allocation requires sophisticated monitoring and control mechanisms. There are diminishing returns—increasing compute beyond certain thresholds may not yield significant performance improvements. The ability to dynamically increase compute can also create disparities in access to high-performance AI, raising equity concerns. Despite these challenges, test-time compute offers a valuable strategy for enhancing system adaptability.\n\n### End-to-End Co-Design and Automated Optimization {#sec-efficient-ai-endtoend-codesign-automated-optimization-1220}\n\nEfficient machine learning systems are rarely the product of isolated optimizations. Achieving balance across efficiency dimensions requires an end-to-end co-design perspective, where each system component is designed in tandem with others. This holistic approach aligns model architectures, hardware platforms, and data pipelines to work seamlessly together.\n\nCo-design becomes essential in resource-constrained environments. Models must align precisely with hardware capabilities—8-bit models require hardware support for efficient integer operations, while pruned models benefit from sparse tensor operations. Edge accelerators often optimize specific operations like convolutions, influencing model architecture choices. Detailed hardware architecture considerations are covered comprehensively in @sec-ai-acceleration.\n\n**Automation and optimization tools** help manage the complexity of navigating trade-offs. Automated machine learning (AutoML)[^fn-automl] enables exploration of different model architectures and hyperparameter configurations. Building on the systematic approach to ML workflows introduced in @sec-ai-workflow, AutoML tools automate many efficiency optimization decisions that traditionally required extensive manual tuning.\n\n[^fn-automl]: **AutoML**: Automated machine learning that systematically searches through model architectures, hyperparameters, and data preprocessing options. Google's AutoML achieved 84.3% ImageNet accuracy vs. human experts' 78.5%, while reducing development time from months to hours.\n\nNeural architecture search (NAS)[^fn-nas] takes automation further by designing model architectures tailored to specific hardware or deployment scenarios, evaluating a wide range of architectural possibilities to maximize performance while minimizing computational demands.\n\n[^fn-nas]: **Neural Architecture Search (NAS)**: Automated method for discovering optimal neural network architectures. EfficientNet-B7, discovered via NAS, achieved 84.3% ImageNet accuracy with 37M parameters vs. hand-designed ResNeXt-101's 80.9% with 84M parameters.\n\nData efficiency also benefits from automation. Tools that automate dataset curation, augmentation, and active learning reduce training dataset size without sacrificing performance, prioritizing high-value data points to speed up training and reduce computational overhead [@settles2009active]. @sec-ai-frameworks explores how modern ML frameworks incorporate these automation capabilities.\n\n### Measuring and Monitoring Efficiency Trade-offs {#sec-efficient-ai-measuring-monitoring-efficiency-tradeoffs-fd5b}\n\nBeyond technical automation lies the broader challenge of systematic evaluation. Efficiency optimization necessitates a structured approach assessing trade-offs that extends beyond purely technical considerations. As systems transition from research to production, success criteria must encompass algorithmic performance, economic viability, and operational sustainability.\n\nCosts associated with efficiency improvements manifest across engineering effort (research, experimentation, integration), balanced against ongoing operational expenses of running less efficient systems. Benefits span multiple domains—beyond direct cost reductions, efficient systems often enable qualitatively new capabilities like real-time processing in resource-constrained environments or deployment to edge devices.\n\nThis evaluation framework must be complemented by ongoing assessment mechanisms. The dynamic nature of ML systems in production necessitates continuous monitoring of efficiency characteristics. As models evolve, data distributions shift, and infrastructure changes, efficiency properties can degrade. Real-time monitoring enables rapid detection of efficiency regressions, while historical analysis provides insight into longer-term trends, revealing whether efficiency improvements are sustainable under changing conditions.\n\n## Engineering Principles for Efficient AI {#sec-efficient-ai-engineering-principles-efficient-ai-1206}\n\nDesigning an efficient machine learning system requires a holistic approach. True efficiency emerges when the entire system is considered as a whole, ensuring trade-offs are balanced across all stages of the ML pipeline from data collection to deployment. This end-to-end perspective transforms system design.\n\n### Holistic Pipeline Optimization {#sec-efficient-ai-holistic-pipeline-optimization-5bcc}\n\nEfficiency is achieved not through isolated optimizations but by considering the entire pipeline as a unified whole. Each stage—data collection, model training, hardware deployment, and inference—contributes to overall system efficiency. Decisions at one stage ripple through the rest, influencing performance, resource use, and scalability.\n\nData collection and preprocessing are starting points. @sec-data-engineering provides comprehensive coverage of how data pipeline design decisions cascade through the entire system. Curating smaller, high-quality datasets can reduce computational costs during training while simplifying model design. However, insufficient data diversity may affect generalization, necessitating compensatory measures.\n\nModel training is another critical stage. Architecture choice, optimization techniques, and hyperparameters must consider deployment hardware constraints. A model designed for high-performance cloud systems may emphasize accuracy and scalability, while models for edge devices must balance accuracy with size and energy efficiency.\n\nDeployment and inference demand precise hardware alignment. Each platform offers distinct capabilities—GPUs excel at parallel matrix operations, TPUs optimize specific neural network computations, and microcontrollers provide energy-efficient processing. A smartphone speech recognition system might leverage an NPU's dedicated convolution units for millisecond-level inference at low power, while an autonomous vehicle's FPGA processes multiple sensor streams with microsecond-level latency.\n\nAn end-to-end perspective ensures trade-offs are addressed holistically rather than shifting inefficiencies between pipeline stages. This systems thinking approach becomes particularly critical when deploying to resource-constrained environments such as edge devices and embedded systems.\n\n### Lifecycle and Environment Considerations {#sec-efficient-ai-lifecycle-environment-considerations-3abc}\n\nEfficiency needs differ significantly depending on lifecycle stage and deployment environment—from research prototypes to production systems, from high-performance cloud to resource-constrained edge.\n\nIn research, the primary focus is often model performance, with efficiency taking a secondary role. Prototypes are trained using abundant compute resources, enabling exploration of large architectures and extensive hyperparameter tuning. Production systems must prioritize efficiency to operate within practical constraints, often involving significant optimization like model pruning, quantization, or retraining. Production also requires continuous monitoring of efficiency metrics and operational frameworks for managing trade-offs at scale—comprehensive production efficiency management strategies are detailed in @sec-ml-operations.\n\nCloud-based systems handle massive workloads with relatively abundant resources, though energy efficiency and operational costs remain critical. The ML systems design principles covered in @sec-ml-systems provide architectural foundations for building scalable, efficiency-optimized cloud deployments. In contrast, edge and mobile systems operate under strict constraints detailed in our efficiency framework, demanding solutions prioritizing efficiency over raw performance.\n\nSome systems like recommendation engines require frequent retraining to remain effective, depending heavily on data efficiency with actively labeled datasets and sampling strategies. Other systems like embedded models in medical devices require long-term stability with minimal updates. Reliability requirements in critical applications significantly influence efficiency optimization strategies, as robust systems often require additional computational overhead for validation, redundancy, and fail-safe mechanisms.\n\n## Societal and Ethical Implications {#sec-efficient-ai-societal-ethical-implications-d0e5}\n\nWhile efficiency in machine learning is often framed as a technical challenge, it is also deeply tied to broader questions about AI systems' purpose and impact. Designing efficient systems involves navigating not only practical trade-offs but also complex ethical and philosophical considerations that responsible ML practitioners must address.\n\n### Equity and Access {#sec-efficient-ai-equity-access-c38d}\n\nEfficiency has the potential to reduce costs, improve scalability, and expand accessibility. However, resources needed to achieve efficiency—advanced hardware, curated datasets, state-of-the-art optimization techniques—are often concentrated in well-funded organizations, creating inequities in who can leverage efficiency gains.\n\nTraining costs for state-of-the-art models like GPT-4 and Gemini Ultra require tens to hundreds of millions of dollars worth of compute [@perrault2024artificial]. Research by [OECD.AI](https://oecd.ai/en/) indicates that 90% of global AI computing capacity is centralized in only five countries [@oecd_ai_2021]. Academic institutions often lack hardware needed to replicate state-of-the-art results, stifling innovation in underfunded sectors. Energy-efficient compute technologies like accelerators for TinyML or Mobile ML present promising avenues for democratization. By enabling powerful processing on low-cost, low-power devices, these technologies allow organizations without high-end infrastructure access to build impactful systems.\n\nData efficiency is essential where high-quality datasets are scarce, but achieving it is unequally distributed. NLP for low-resource languages suffers from lack of sufficient training data, leading to significant performance gaps. Efforts like the Masakhane project building open-source datasets for African languages show how collaborative initiatives can address this, though scaling globally requires greater investment. Democratizing data efficiency requires more open sharing of pre-trained models and datasets. Initiatives like Hugging Face's open access to transformers or Meta's No Language Left Behind aim to make state-of-the-art NLP models available worldwide, reducing barriers for data-scarce regions.\n\nAlgorithmic efficiency contributes to democratizing ML by enabling advanced capabilities on low-cost, resource-constrained devices. AI-powered diagnostic tools on smartphones are transforming healthcare in remote areas, while low-power TinyML models enable environmental monitoring in regions without reliable electricity.\n\nTechnologies like [TensorFlow Lite](https://ai.google.dev/edge/litert) and [PyTorch Mobile](https://pytorch.org/mobile/home/) allow developers to deploy lightweight models on everyday devices, expanding access in resource-constrained settings. Open-source efforts to share pre-optimized models like MobileNet or EfficientNet play a critical role by allowing under-resourced organizations to deploy state-of-the-art solutions.\n\n### Balancing Innovation with Efficiency Demands {#sec-efficient-ai-balancing-innovation-efficiency-demands-7a44}\n\nThe pursuit of efficiency often brings tension between optimizing for what is known and exploring what is new. Equity concerns are intensified by this tension: resource concentration in well-funded organizations enables expensive exploratory research, while resource-constrained institutions must focus on incremental improvements.\n\nEfficiency often favors established techniques proven to work well. Optimizing neural networks through pruning, quantization, or distillation typically refines existing architectures rather than developing entirely new ones. Consider the shift from traditional ML to deep learning: early neural network research in the 1990s-2000s required significant resources and often failed to outperform simpler methods, yet researchers persisted, eventually leading to breakthroughs defining modern AI.\n\nPioneering research often requires significant resources. Large language models like GPT-4 or PaLM are not inherently efficient—their training consumes enormous compute and energy. Yet these models have opened entirely new possibilities, prompting advancements that eventually lead to more efficient systems like smaller fine-tuned versions.\n\nThis reliance on resource-intensive innovation raises questions about who gets to participate. Well-funded organizations can afford to explore new frontiers, while smaller institutions may be constrained to incremental improvements prioritizing efficiency over novelty.\n\nEfficiency-focused design often requires adhering to strict constraints like reducing model size or latency. While constraints can drive ingenuity, they can also limit exploration scope. However, the drive for efficiency can positively impact innovation—constraints force creative thinking, leading to new methods maximizing performance within tight resource budgets. Techniques like NAS and attention mechanisms arose partly from the need to balance performance and efficiency.\n\nOrganizations and researchers must recognize when to prioritize efficiency and when to embrace experimentation risks. Applied systems for real-world deployment may demand strict efficiency, while exploratory research labs can focus on pushing boundaries. The relationship between innovation and efficiency is not adversarial but complementary—efficient systems create foundations for scalable applications, while resource-intensive experimentation drives breakthroughs redefining what's possible.\n\n### Optimization Limits {#sec-efficient-ai-optimization-limits-20f0}\n\nThe tensions between equity, innovation, and efficiency ultimately stem from a fundamental characteristic of optimization: diminishing returns. Optimization is central to building efficient ML systems, but it is not infinite. As systems become more refined, each additional improvement requires exponentially more effort, time, or resources while delivering increasingly smaller benefits.\n\nThe No Free Lunch (NFL) theorems[^fn-nfl-theorems] for optimization illustrate inherent limitations. According to NFL theorems, no single optimization algorithm can outperform all others across every possible problem, implying optimization technique effectiveness is highly problem-specific [@wolpert1997no].\n\n[^fn-nfl-theorems]: **No Free Lunch (NFL) Theorems**: Mathematical proof by Wolpert and Macready (1997) showing that averaged over all possible optimization problems, every algorithm performs equally well. In ML context, no universal optimization technique exists—methods must be tailored to specific problem domains.\n\nFor example, compressing an ML model can initially reduce memory and compute requirements significantly with minimal accuracy loss. However, as compression progresses, maintaining performance becomes increasingly challenging. Achieving additional gains may necessitate sophisticated techniques like hardware-specific optimizations or extensive retraining, increasing complexity and cost. These costs extend beyond financial investment to include time, expertise, iterative testing, and potential trade-offs in robustness and generalizability.\n\nThe NFL theorems highlight that no universal optimization solution exists, emphasizing need to balance efficiency pursuits with practical considerations. Over-optimization risks wasted resources and reduced adaptability, complicating future updates. Identifying when a system is \"good enough\" ensures resources are allocated effectively.\n\nSimilarly, optimizing datasets for training efficiency may initially save resources, but excessively reducing dataset size risks compromising diversity and weakening generalization. Pushing hardware to performance limits may improve metrics like latency, yet associated reliability concerns and engineering costs can outweigh gains.\n\nUnderstanding optimization limits is essential for creating systems balancing efficiency with practicality and sustainability. This perspective helps avoid over-optimization and ensures resources are invested in areas with meaningful returns.\n\n#### Moore's Law Case Study {#sec-efficient-ai-moores-law-case-study-5767}\n\nOne of the most insightful examples of optimization limits appears in Moore's Law and the economic curve underlying it. While Moore's Law is celebrated as a predictor of exponential computational power growth, its success relied on intricate economic balance. The relationship between integration and cost provides a compelling analogy for diminishing returns in ML optimization.\n\n@fig-moores-law-plot shows relative manufacturing cost per component as the number of components in an integrated circuit increases. Initially, as more components are packed onto a chip, cost per component decreases due to economies of scale—higher integration reduces need for packaging and interconnects. Moving from hundreds to thousands of components drastically reduced costs and improved performance [@moore2021cramming].\n\n::: {#fig-moores-law-plot fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.8}{\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{loglogaxis}\n[width=84mm,\ntick label style={/pgf/number format/assume math mode=true},\nxlabel=Number of components per integrated circuit,\nylabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\nxlabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\nylabel=Relative manufacturing cost/component,\ntick label style={font=\\footnotesize},\nclip=false,\nminor tick style={draw=none},\nmajor tick style={draw=black},\nxmin=1.0e0,\nxmax=1.0e5,\nymax=1.0e5,\nymin=1.0e0,\n]\n\\draw[VioletLine,line width=1.5pt,smooth] (axis cs:2.4, 29000)\nto [out=315,in=250,distance=26]node[above=5pt,pos=0.45]{1962}(axis cs:31, 29600);\n%\n\\draw[RedLine,line width=1.5pt,smooth] (axis cs:2.4,5800)\nto [out=320,in=255,distance=55]node[above=5pt,pos=0.45]{1965}(axis cs:198, 6700);\n%\n\\draw[BlueLine,line width=1.5pt,smooth] (axis cs:2.4,1400)\nto [out=320,in=250,distance=85]node[above=5pt,pos=0.45]{1970}(axis cs:24800.4, 1200);\n\\end{loglogaxis}\n\\end{tikzpicture}}\n```\n: **Moore's Law Economics**: Declining per-component manufacturing costs initially drove exponential growth in integrated circuit complexity, but diminishing returns eventually limited further cost reductions. This relationship mirrors optimization challenges in machine learning, where increasing model complexity yields diminishing gains in performance relative to computational expense. Source: [@moore2021cramming].\n:::\n\nHowever, as integration continues, the curve begins to rise. Components packed closer together face reliability issues like increased heat dissipation and signal interference. Addressing these requires more sophisticated manufacturing techniques—advanced lithography, error correction, improved materials—increasing complexity and cost. This U-shaped curve captures the fundamental trade-off: early improvements yield substantial benefits, but beyond a certain point, each additional gain comes at greater cost.\n\nThe dynamics mirror ML optimization challenges. Compressing a deep learning model to reduce size and energy consumption follows a similar trajectory. Initial optimizations like pruning redundant parameters or reducing precision often lead to significant savings with minimal accuracy impact. However, as compression progresses, performance losses become harder to recover. Techniques like quantization or hardware-specific tuning can restore some performance, but these add complexity and cost.\n\nSimilarly, in data efficiency, reducing training dataset size often improves computational efficiency initially. Yet as datasets shrink further, they may lose diversity, compromising generalization. Addressing this often involves synthetic data or sophisticated augmentation, demanding additional engineering effort.\n\nThe Moore's Law plot serves as a visual reminder that optimization is not infinite. The cost-benefit balance is always context-dependent, and the point of diminishing returns varies based on system goals and constraints. ML practitioners, like semiconductor engineers, must identify when further optimization ceases to provide meaningful benefits. Over-optimization can lead to wasted resources, reduced adaptability, and systems overly specialized to initial conditions.\n\n## Fallacies and Pitfalls {#sec-efficient-ai-fallacies-pitfalls-f804}\n\nEfficiency in AI systems involves complex trade-offs between multiple competing objectives that often pull in different directions. The mathematical elegance of scaling laws can create false confidence about predictable optimization paths, while diverse deployment context requirements create misconceptions about universal efficiency strategies.\n\n**Fallacy:** _Efficiency optimizations always improve system performance across all metrics._\n\nThis misconception leads teams to apply efficiency techniques without understanding trade-offs and side effects. Optimizing for computational efficiency might degrade accuracy, improving memory efficiency could increase latency, and reducing model size often requires more complex training procedures. Efficiency gains in one dimension frequently create costs in others that may be unacceptable for specific scenarios. Effective efficiency optimization requires careful analysis of which metrics matter most and acceptance that some performance aspects will necessarily be sacrificed.\n\n**Pitfall:** _Assuming scaling laws predict efficiency requirements linearly across all model sizes._\n\nTeams often extrapolate efficiency requirements based on scaling law relationships without considering breakdown points where these laws no longer apply. Scaling laws provide useful guidance for moderate increases, but fail to account for emergent behaviors, architectural constraints, and infrastructure limitations appearing at extreme scales. Applying scaling law predictions beyond validated ranges can lead to wildly inaccurate resource estimates and deployment failures. Successful efficiency planning requires understanding both utility and limits of scaling law frameworks.\n\n**Fallacy:** _Edge deployment efficiency requirements are simply scaled-down versions of cloud requirements._\n\nThis belief assumes edge deployment is merely cloud deployment with smaller models and less computation. Edge environments introduce qualitatively different constraints including real-time processing requirements, power consumption limits, thermal management needs, and connectivity variability. Optimization strategies working in cloud environments often fail catastrophically in edge contexts. Edge efficiency requires different approaches prioritizing predictable performance, energy efficiency, and robust operation under varying conditions.\n\n**Pitfall:** _Focusing on algorithmic efficiency while ignoring system-level efficiency factors._\n\nMany practitioners optimize algorithmic complexity metrics like FLOPs or parameter counts without considering how improvements translate to actual system performance. Real system efficiency depends on memory access patterns, data movement costs, hardware utilization characteristics, and software stack overhead that may not correlate with theoretical complexity metrics. A model with fewer parameters might still perform worse due to irregular memory access patterns or poor hardware mapping. Comprehensive efficiency optimization requires measuring and optimizing actual system performance rather than relying solely on algorithmic complexity indicators.\n\n## Summary {#sec-efficient-ai-summary-66bb}\n\nEfficiency has emerged as a design principle that transforms how we approach machine learning systems, moving beyond simple performance optimization toward comprehensive resource stewardship. This chapter revealed how scaling laws provide empirical insights into relationships between model performance and computational resources, establishing efficiency as a strategic advantage enabling broader accessibility, sustainability, and innovation. The interdependencies between algorithmic, compute, and data efficiency create a complex landscape where decisions in one dimension cascade throughout the entire system, requiring a holistic perspective balancing trade-offs across the complete ML pipeline.\n\nThe practical challenges of designing efficient systems highlight the importance of context-aware decision making, where deployment environments shape efficiency priorities. Cloud systems leverage abundant resources for scalability and throughput, while edge deployments optimize for real-time performance within strict power constraints, and TinyML applications push the boundaries of what's achievable with minimal resources. These diverse requirements demand sophisticated strategies including end-to-end co-design, automated optimization tools, and careful prioritization based on operational constraints. The emergence of scaling law breakdowns and tension between innovation and efficiency underscore that optimal system design requires addressing not just technical trade-offs but broader considerations of equity, sustainability, and long-term impact.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Efficiency is a strategic enabler that democratizes access to AI capabilities across diverse deployment contexts\n* Scaling laws provide predictive frameworks for resource allocation, but their limits reveal opportunities for architectural innovation\n* Trade-offs between algorithmic, compute, and data efficiency are interconnected and context-dependent, requiring holistic optimization strategies\n* Automation tools and end-to-end co-design approaches can transform efficiency constraints into opportunities for system synergy\n:::\n\nHaving established the three-pillar efficiency framework and explored scaling laws as the quantitative foundation for resource allocation, the following chapters provide the specific engineering techniques to achieve efficiency in each dimension. @sec-model-optimizations focuses on algorithmic efficiency through systematic approaches to reducing model complexity while preserving performance. The chapter covers quantization techniques that reduce numerical precision, pruning methods that eliminate redundant parameters, and knowledge distillation approaches that transfer capabilities from large models to smaller ones.\n\n@sec-ai-acceleration addresses compute efficiency by exploring how specialized hardware and optimized software implementations maximize performance per unit of computational resource. Topics include GPU optimization, AI accelerator architectures, and system-level optimizations that improve throughput and reduce latency. @sec-benchmarking-ai provides the measurement methodologies essential for quantifying efficiency gains across all three dimensions, covering performance evaluation frameworks, energy measurement techniques, and comparative analysis methods.\n\nThis progression from principles to specific techniques to measurement methodologies reflects the systematic engineering approach necessary for achieving real-world efficiency in machine learning systems. Each subsequent chapter builds upon the foundational understanding established here, creating a comprehensive toolkit for performance engineering that addresses the complex, interconnected trade-offs that define efficient AI system design.\n\nThese efficiency principles establish the foundation for the specific optimization techniques explored in @sec-model-optimizations, where detailed algorithms for quantization, pruning, and knowledge distillation provide concrete tools for achieving the efficiency goals outlined here. As machine learning systems continue scaling in complexity and reach, the principles of efficient design will remain essential for creating systems that are not only performant but also sustainable, accessible, and aligned with broader societal goals of responsible AI development.\n","srcMarkdownNoYaml":"\n\n# Efficient AI {#sec-efficient-ai}\n\n::: {layout-narrow}\n::: {.column-margin}\n*DALL·E 3 Prompt: A conceptual illustration depicting efficiency in artificial intelligence using a shipyard analogy. The scene shows a bustling shipyard where containers represent bits or bytes of data. These containers are being moved around efficiently by cranes and vehicles, symbolizing the streamlined and rapid information processing in AI systems. The shipyard is meticulously organized, illustrating the concept of optimal performance within the constraints of limited resources. In the background, ships are docked, representing different platforms and scenarios where AI is applied. The atmosphere should convey advanced technology with an underlying theme of sustainability and wide applicability.*\n:::\n\n\\noindent\n![](images/png/cover_efficient_ai.png)\n\n:::\n\n## Purpose {.unnumbered}\n\n_What key trade-offs shape the pursuit of efficiency in machine learning systems, and why must engineers balance competing objectives?_\n\nMachine learning system efficiency requires balancing trade-offs across algorithmic complexity, computational resources, and data utilization. Improvements in one dimension often degrade performance in others, creating engineering tensions that demand systematic approaches. Understanding these interdependent relationships enables engineers to design systems achieving maximum performance within practical constraints of time, energy, and cost.\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain the interdependencies between algorithmic efficiency, compute efficiency, and data efficiency, and how optimizing one dimension affects the others\n\n- Apply scaling law relationships to determine compute-optimal resource allocation between model size and training data for fixed computational budgets\n\n- Differentiate between resource-constrained, data-limited, and temporal scaling regimes, and determine which regime applies to a given system design scenario\n\n- Compare efficiency priorities across cloud, edge, mobile, and TinyML deployment contexts, and justify optimization strategies based on context-specific constraints\n\n- Explain how pruning, quantization, and knowledge distillation reduce model complexity while maintaining accuracy, recognizing these techniques are detailed in subsequent chapters\n\n- Critique scaling-based approaches by identifying breakdown conditions such as data saturation, infrastructure bottlenecks, and diminishing returns\n\n- Analyze fundamental trade-offs between efficiency dimensions, including algorithmic complexity vs. compute requirements, compute efficiency vs. real-time performance, and data efficiency vs. model generalization\n\n- Assess the environmental impact of ML system efficiency choices and evaluate how efficiency improvements affect equitable access to AI capabilities\n\n:::\n\n## The Efficiency Imperative {#sec-efficient-ai-efficiency-imperative-d65c}\n\nThe preceding chapters established how to build machine learning systems: the mathematics of neural network learning, the architectural patterns from MLPs through Transformers, the frameworks that implement these designs, and the training processes that optimize millions of parameters. A trained model, however, is not a deployed model. Training produces weights; deployment demands systems that run within memory budgets, latency constraints, and power envelopes that production environments impose. The gap between what we can train and what we can deploy defines the efficiency imperative.\n\nMachine learning efficiency has evolved from an afterthought to a fundamental discipline as models transitioned from simple statistical approaches to complex, resource-intensive architectures. The gap between theoretical capabilities and practical deployment has widened significantly, creating efficiency constraints that determine system feasibility and scalability.\n\nLarge-scale language models exemplify this challenge. GPT-3 required training costs estimated at $4.6 million (Lambda Labs estimate) and energy consumption of 1,287 MWh [@Patterson_et_al_2021]. The operational requirements, including memory footprints exceeding 700GB for inference (350GB for half-precision), create deployment barriers in resource-constrained environments. These constraints reveal a fundamental tension between model expressiveness and system practicality that demands rigorous analysis and optimization strategies.\n\nEfficiency research extends beyond resource optimization to encompass the theoretical foundations of learning system design. Engineers must understand how algorithmic complexity, computational architectures, and data utilization strategies interact to determine system viability. These interdependencies create multi-objective optimization problems where improvements in one dimension frequently degrade performance in others.\n\nThis chapter establishes the framework for analyzing efficiency in machine learning systems within Part III's performance engineering curriculum. The efficiency principles here inform the optimization techniques in @sec-model-optimizations, where quantization and pruning methods realize algorithmic efficiency goals, the hardware acceleration strategies in @sec-ai-acceleration that maximize compute efficiency, and the measurement methodologies in @sec-benchmarking-ai for validating efficiency improvements.\n\n## Defining System Efficiency {#sec-efficient-ai-defining-system-efficiency-a4b7}\n\nConsider building a photo search application for a smartphone. You face three competing pressures: the model must be small enough to fit in memory (an algorithmic challenge), it must run fast enough on the phone's processor without draining the battery (a compute challenge), and it must learn from a user's personal photos without requiring millions of examples (a data challenge). Efficient AI is the discipline of navigating these interconnected trade-offs.\n\nAddressing these efficiency challenges requires coordinated optimization across three interconnected dimensions that determine system viability.\n\n::: {.callout-definition title=\"Machine Learning System Efficiency\"}\n\n***Machine Learning System Efficiency*** is the optimization of ML systems to minimize _computational_, _memory_, and _energy_ demands while maintaining performance, achieved through improvements in _algorithms_, _hardware utilization_, and _data usage_.\n\n:::\n\nUnderstanding these interdependencies is necessary for designing systems that achieve maximum performance within practical constraints. Examining how the three dimensions interact in practice reveals how scaling laws expose these constraints.\n\n### Efficiency Interdependencies {#sec-efficient-ai-efficiency-interdependencies-5d69}\n\nThe three efficiency dimensions are deeply intertwined, creating a complex optimization landscape. Algorithmic efficiency reduces computational requirements through better algorithms and architectures, though it may increase development complexity or require specialized hardware. Compute efficiency maximizes hardware utilization through optimized implementations and specialized processors, though it may limit model expressiveness or require specific algorithmic approaches. Data efficiency enables learning with fewer examples through improved training procedures and data utilization, though it may require more sophisticated algorithms or additional computational resources.\n\nA concrete example illustrates these interconnections through the design of a photo search application for smartphones. The system must fit in 2GB memory (compute constraint), achieve acceptable accuracy with limited training data (data constraint), and complete searches within 50ms (algorithmic constraint). Optimization of any single dimension in isolation proves inadequate:\n\n**Algorithmic Efficiency** focuses on the model architecture. Using a compact vision-language model with 50 million parameters instead of a billion-parameter model reduces memory requirements from 4GB to 200MB and cuts inference time from 2 seconds to 100 milliseconds. However, accuracy decreases from 92% to 85%, necessitating careful evaluation of trade-off acceptability.\n\n**Compute Efficiency** addresses hardware utilization. The optimized model runs efficiently on smartphone processors, consuming only 10% battery per hour. Techniques like 8-bit quantization reduce computation while maintaining quality, and batch processing[^fn-batch-processing] handles multiple queries simultaneously. However, these optimizations necessitate algorithmic modifications to support reduced precision operations.\n\n**Data Efficiency** shapes how the model learns. Rather than requiring millions of labeled image-text pairs, the system leverages pre-trained foundation models and adapts using only thousands of user-specific examples. Continuous learning from user interactions provides implicit feedback without explicit labeling. This data efficiency necessitates more sophisticated algorithmic approaches and careful management of computational resources during adaptation.\n\nSynergy between these dimensions produces emergent benefits: the smaller model (algorithmic efficiency) enables on-device processing (compute efficiency), which facilitates learning from private user data (data efficiency) without transmitting personal images to remote servers. This integration provides enhanced performance and privacy protection, demonstrating how efficiency enables capabilities unattainable with less efficient approaches.\n\nThese interdependencies appear across all deployment contexts, from cloud systems with abundant resources to edge devices with severe constraints. As illustrated in @fig-interdependece, understanding these relationships provides the foundation for examining how scaling laws reveal fundamental efficiency limits.\n\n::: {#fig-interdependece fig-env=\"figure\" fig-pos=\"htb\"}\n\n```{.tikz}\n\\scalebox{0.7}{%\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n},scale=1.25,line width=0.75pt]\n\\def\\firstcircle{(0,0) circle (1.5cm)}\n\\def\\secondcircle{(300:2cm) circle (1.5cm)}\n\\def\\thirdcircle{(0:2cm) circle (1.5cm)}\n%\n    \\begin{scope}[shift={(3cm,-5cm)}, fill opacity=0.5]\n        \\fill[cyan] \\firstcircle;\n        \\fill[purple!70] \\secondcircle;\n        \\fill[orange] \\thirdcircle;\n    \\end{scope}\n\n\\begin{scope}[shift={(3cm,-5cm)}]\n    \\draw[draw=none] \\firstcircle node[black,left,align=center] {Algorithmic\\\\ Efficiency};\n    \\draw[draw=none] \\secondcircle node [black,below,align=center] {Data\\\\ Efficiency};\n    \\draw[draw=none] \\thirdcircle node [black,right,align=center] {Compute\\\\ Efficiency};\n\\end{scope}\n\\end{tikzpicture}}\n\n```\n: **Efficiency Interdependencies**: The three efficiency dimensions (algorithmic, compute, and data) overlap and influence one another, creating systemic trade-offs in machine learning systems. Optimizing for one efficiency dimension often requires careful consideration of its impact on the others, shaping overall system performance and resource utilization.\n:::\n\n[^fn-batch-processing]: **Batch Processing**: Processing multiple inputs together to amortize computational overhead and maximize GPU utilization. Mobile vision models achieve 3-5× speedup with batch size 8 vs. individual processing, but introduces 50-200ms latency as queries wait for batch completion—a classic throughput vs. latency trade-off in ML systems.\n\nWith this understanding of efficiency dimension interactions, we can examine why brute-force scaling alone cannot address real-world efficiency requirements. Scaling laws provide the quantitative framework for understanding these limitations.\n\n## AI Scaling Laws {#sec-efficient-ai-ai-scaling-laws-a043}\n\nMachine learning systems have followed a consistent pattern: increasing model scale through parameters, training data, and computational resources typically improves performance. This empirical observation has driven progress across natural language processing, computer vision, and speech recognition, where larger models trained on extensive datasets consistently achieve state-of-the-art results.\n\nThese scaling laws can be seen as the quantitative expression of Richard Sutton's \"Bitter Lesson\" from @sec-introduction: performance in machine learning is primarily driven by leveraging general methods at massive scale. The predictable power-law relationships show *how* computation, when scaled, yields better models.\n\nThis scaling trajectory raises critical questions about efficiency and sustainability. As computational demands grow exponentially and data requirements increase, questions emerge regarding when scaling costs outweigh performance benefits. Researchers have developed scaling laws[^fn-scaling-laws] that quantify how model performance relates to training resources, revealing why efficiency becomes increasingly important as systems expand in complexity.\n\n[^fn-scaling-laws]: **Scaling Laws**: Empirical relationships discovered by OpenAI showing that language model performance follows predictable power-law relationships with model size (N), dataset size (D), and compute budget (C). These laws enable researchers to predict performance and optimal resource allocation before expensive training runs.\n\nThis section introduces scaling laws, examines their manifestation across different dimensions, and analyzes their implications for system design, establishing why the multi-dimensional efficiency optimization framework is a fundamental requirement.\n\n### Empirical Evidence for Scaling Laws {#sec-efficient-ai-empirical-evidence-scaling-laws-0105}\n\nThe rapid evolution in AI capabilities over the past decade exemplifies this scaling trajectory. GPT-1 (2018) contained 117 million parameters and demonstrated basic sentence completion capabilities. GPT-2 (2019) scaled to 1.5 billion parameters and achieved coherent paragraph generation. GPT-3 (2020) expanded to 175 billion parameters and demonstrated sophisticated text generation across diverse domains. Each increase in model size brought dramatically improved capabilities, but at exponentially increasing costs.\n\nThis pattern extends beyond language models. In computer vision, doubling neural network size typically yields consistent accuracy gains when training data increases proportionally. AlexNet (2012) had 60 million parameters, VGG-16 (2014) scaled to 138 million, and large modern vision transformers can exceed 600 million parameters. Each generation achieved better image recognition accuracy, but required proportionally more computational resources and training data.\n\nThe scaling hypothesis underlies this progress: larger models possess increased capacity to capture intricate data patterns, facilitating improved accuracy and generalization. However, this scaling trajectory introduces critical resource constraints. Training GPT-3 required approximately 314 sextillion[^fn-sextillion] floating-point operations (314 followed by 21 zeros), equivalent to running a modern gaming PC continuously for over 350 years, at substantial financial and environmental costs.\n\n[^fn-sextillion]: **Sextillion**: A number with 21 zeros (10²¹), representing an almost incomprehensible scale. To put this in perspective, there are estimated 10²² to 10²⁴ stars in the observable universe, making GPT-3's training computation roughly 1/22nd of counting every star in the cosmos.\n\nThese resource demands reveal why understanding scaling laws is necessary for efficiency. @fig-compute-trends shows computational demands of training state-of-the-art models escalating at an unsustainable rate, growing faster than Moore's Law improvements in hardware.\n\n![**Model Training Compute Trends**: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: [@Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022.]](images/png/compute-trends.png){#fig-compute-trends}\n\nScaling laws provide a quantitative framework for understanding these trade-offs. They reveal that model performance exhibits predictable patterns as resources increase, following power-law relationships where performance improves consistently but with diminishing returns[^fn-diminishing-returns]. These laws show that optimal resource allocation requires coordinating model size, dataset size, and computational budget rather than scaling any single dimension in isolation.\n\n[^fn-diminishing-returns]: **Diminishing Returns**: Economic principle where each additional input yields progressively smaller output gains. In ML, doubling compute from 1 to 2 hours might improve accuracy by 5%, but doubling from 100 to 200 hours might improve it by only 0.5%.\n\n::: {.callout-note collapse=\"true\" title=\"Refresher: Transformer Computational Characteristics\"}\n\nRecall from @sec-dnn-architectures that transformers process sequences using self-attention mechanisms that compute relationships between all token pairs. This architecture's computational cost scales quadratically with sequence length, making resource allocation particularly critical for language models. The term \"FLOPs\" (floating-point operations) quantifies total computational work, while \"tokens\" represent the individual text units (typically subwords) that models process during training.\n:::\n\n### Compute-Optimal Resource Allocation {#sec-efficient-ai-computeoptimal-resource-allocation-541a}\n\nEmpirical studies of large language models (LLMs) reveal a key insight: for any fixed computational budget, there exists an optimal balance between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss.\n\n[^fn-tokens]: **Tokens**: Individual units of text that language models process, created by breaking text into subword pieces using algorithms like Byte-Pair Encoding (BPE). GPT-3 trained on 300 billion tokens while PaLM used 780 billion tokens, requiring text corpora equivalent to millions of books from web crawls and digitized literature.\n\n@fig-compute-optimal illustrates this principle through three related views. The left panel shows 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. The valleys in these curves identify the most efficient model size for each computational budget when training autoregressive[^fn-autoregressive] language models. The center and right panels reveal how the optimal number of parameters and tokens scales predictably as computational budgets increase, demonstrating the necessity for coordinated scaling to maximize resource utilization.\n\n[^fn-efficient-flops]: **FLOPs**: Floating-Point Operations, measuring computational work performed. Modern deep learning models require 10²²-10²⁴ FLOPs for training: GPT-3 used ~3.14 × 10²³ FLOPs (314 sextillion operations), equivalent to running a high-end gaming PC continuously for over 350 years.\n\n[^fn-transformer]: **Transformer**: Neural network architecture introduced by Vaswani et al. [@vaswani2017attention] that revolutionized NLP through self-attention mechanisms. Unlike sequential RNNs, transformers enable parallel processing during training, forming the foundation of modern large language models including GPT, BERT, T5, and their derivatives.\n\n[^fn-autoregressive]: **Autoregressive Models**: Language models that generate text by predicting each token based on all preceding tokens in the sequence. GPT-family models exemplify this approach, generating text left-to-right with causal attention masks to ensure each position only attends to previous positions.\n\n![**Optimal Compute Allocation**: For fixed computational budgets, language model performance depends on balancing model size and training data volume; the left panel maps training loss across parameter counts, identifying an efficiency sweet spot for each FLOP level. The center and right panels quantify how optimal parameter counts and token requirements scale predictably with increasing compute, demonstrating the need for coordinated scaling of both model and data to maximize resource utilization in large language models. Source: [@hoffmann2022training].](images/png/compute_optimal.png){#fig-compute-optimal}\n\n@kaplan2020scaling demonstrated that transformer-based language models scale predictably with three factors: the number of model parameters, the volume of the training dataset (measured in tokens), and the total computational budget (measured in floating-point operations). When these factors are augmented proportionally, models exhibit consistent performance improvements without requiring architectural modifications or task-specific tuning.\n\nThe practical manifestation of these patterns appears clearly in @fig-kaplan-scaling, which presents test loss curves for models spanning from $10^3$ to $10^9$ parameters. The figure reveals two key insights. First, larger models demonstrate superior sample efficiency, achieving target performance levels with fewer training tokens. Second, as computational resources increase, the optimal model size correspondingly grows, with loss decreasing predictably when compute is allocated efficiently.\n\n![**Scaling Laws & Compute Optimality**: Larger models consistently achieve better performance with increased training data and compute, but diminishing returns necessitate careful resource allocation during training. Optimal model size and training duration depend on the available compute budget, as evidenced by the convergence of loss curves at different parameter scales and training token counts. Source: [@kaplan2020scaling].](images/png/kaplan_scaling_data_compute.png){#fig-kaplan-scaling}\n\nThis theoretical scaling relationship defines optimal compute allocation: for a fixed budget, the relationship $D \\propto N^{0.74}$ [@hoffmann2022training] shows that dataset size $D$ and model size $N$ must grow in coordinated proportions. This represents a significant revision from earlier work by @kaplan2020scaling, which suggested $D \\propto N^{0.57}$, implying that scaling model size was more important than scaling data.\n\nThe Chinchilla study revealed that prior models were significantly undertrained. Chinchilla-70B, trained with 1.4 trillion tokens, outperformed the 280B parameter Gopher model across most benchmarks despite having 4x fewer parameters. This demonstrated that compute-optimal training requires substantially more data relative to model size than previously believed. The practical implication is that smaller models trained on more data often outperform larger models trained on less data, a finding with significant efficiency implications for practitioners operating under fixed compute budgets.\n\nThese scaling laws are empirical and continue to evolve as researchers explore different model families, data compositions, and training procedures.\n\nThese theoretical predictions assume perfect compute utilization, which becomes challenging in distributed training scenarios. Real-world implementations face communication overhead that scales unfavorably with system size, creating bandwidth bottlenecks that reduce effective utilization. Beyond 100 nodes, communication overhead can reduce expected performance gains by 20-40% depending on workload and interconnect, transforming predicted improvements into more modest real-world results.\n\n### Mathematical Foundations and Operational Regimes {#sec-efficient-ai-mathematical-foundations-operational-regimes-9afe}\n\nThe predictable patterns observed in scaling behavior can be expressed mathematically using power-law relationships, though understanding the intuition behind these patterns proves more important than precise mathematical formulation for most practitioners.\n\n::: {.callout-note collapse=\"true\" title=\"Formal Mathematical Formulation\"}\n\nFor readers interested in the formal mathematical framework, scaling laws can be expressed as power-law relationships. The general formulation is:\n\n$$\n\\mathcal{L}(N) = A N^{-\\alpha} + B\n$$\n\nwhere loss $\\mathcal{L}$ decreases as resource quantity $N$ increases, following a power-law decay with rate $\\alpha$, plus a baseline constant $B$. Here, $\\mathcal{L}(N)$ represents the loss achieved with resource quantity $N$, $A$ and $B$ are task-dependent constants, and $\\alpha$ is the scaling exponent that characterizes the rate of performance improvement. A larger value of $\\alpha$ signifies more efficient performance improvements with respect to scaling.\n\n:::\n\nThese theoretical predictions find strong empirical support across multiple model configurations. @fig-loss-vs-n-d shows that early-stopped test loss varies predictably with both dataset size and model size, and learning curves across configurations can be aligned through appropriate parameterization.\n\n#### Resource-Constrained Scaling Regimes {#sec-efficient-ai-resourceconstrained-scaling-regimes-062d}\n\nApplying scaling laws in practice requires recognizing three distinct resource allocation regimes that emerge from trade-offs between compute budget, data availability, and optimal resource allocation. These regimes provide practical guidance for system designers navigating resource constraints.\n\nCompute-limited regimes characterize scenarios where available computational resources restrict scaling potential despite abundant training data. Organizations with limited hardware budgets or strict training time constraints operate within this regime. The optimal strategy involves training smaller models for longer periods, maximizing utilization of available compute through extended training schedules rather than larger architectures. This approach proves particularly relevant for academic institutions, startups, or projects with constrained infrastructure access.\n\nData-limited regimes emerge when computational resources exceed what can be effectively utilized given dataset constraints. High-resource organizations working with specialized domains, proprietary datasets, or privacy-constrained data often encounter this regime. The optimal strategy involves training larger models for fewer optimization steps, leveraging model capacity to extract maximum information from limited training examples. This regime commonly appears in specialized applications like medical imaging or proprietary commercial datasets.\n\nOptimal regimes (Chinchilla Frontier) represent the balanced allocation of compute and data resources following compute-optimal scaling laws. This regime achieves maximum performance efficiency by scaling model size and training data proportionally, as demonstrated by DeepMind's Chinchilla model, which outperformed much larger models through optimal resource allocation [@hoffmann2022training]. Operating within this regime requires sophisticated resource planning but delivers superior performance per unit of computational investment.\n\nRecognizing these regimes enables practitioners to make informed decisions about resource allocation strategies, avoiding common inefficiencies such as over-parameterized models with insufficient training data or under-parameterized models that fail to utilize available computational resources effectively.\n\n::: {#fig-loss-vs-n-d fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\n\\definecolor{myblue}{RGB}{31,119,180}\n\\definecolor{myorange}{RGB}{255,127,14}\n\\definecolor{mygreen}{RGB}{44,160,44}\n\\definecolor{myred}{RGB}{214,39,40}\n\\definecolor{mypurple}{RGB}{148,103,189}\n\\definecolor{mybrown}{RGB}{140,86,75}\n\n\\tikzset{%\n    LineD/.style={line width=1.0pt,dashed,dash pattern=on 3pt off 2pt]}\n}\n\n\\pgfplotsset{myaxis/.style={\n  /pgf/number format/.cd,\n  1000 sep={},\n   legend style={at={(0.1,0.45)}, anchor=north},\n   legend cell align=left,\n   legend style={fill=BrownL!30,draw=BrownLine,row sep=-0.5pt,\n   font=\\fontsize{6pt}{6}\\selectfont\\usefont{T1}{phv}{m}{n}},\n   width=120mm,\n   height=67.2mm,\n   yticklabel style={xshift=1mm,font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n},\n   /pgf/number format/.cd, fixed, fixed zerofill, precision=1},\n   xticklabel style={font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n}},\n   ylabel style={font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n},align=center,yshift=-1.2mm},\n   xlabel style={font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n}},\n   tick align=outside,\n   major tick length=1mm,\n   title style={yshift=-4pt},\n   minor x tick  style={thin,black!60},\n   major tick  style={thin,black!60},\n   log basis y=10,\n   x tick label style={rotate=0, anchor=north,yshift=1pt},\n    }}\n\n\\begin{axis}[myaxis,\n  title={Loss vs Model and Dataset Size},\n  xmin=0.5e7,\n  xmax=4e10,\n  ymin=2.3, ymax=4.8,\n   ytick={2.5,3.0,3.5,4.0,4.5},\n  yticklabels={2.5,3,3.5,4.0,4.5},\n  xmode=log,\n  xtick={1e7,1e8,1e9,1e10},\n  xticklabels={10\\textsuperscript{7},10\\textsuperscript{8},10\\textsuperscript{9},10\\textsuperscript{10}},\n  xlabel={Tokens in Dataset},\n  ylabel={Loss},\n  grid=both,\n  major grid style={black!30},\n  minor grid style={draw=none},\n  minor x tick num=4,\n  xtick pos=left,\n   ytick pos=left,\n  cycle list={\n    {myblue,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {myorange,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {mygreen,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {myred,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {mypurple,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {mybrown,mark=*,only marks,mark options={line width=1pt},mark size=1.75pt},\n    {myblue},\n    {myorange},\n    {mygreen},\n    {myred},\n    {mypurple},\n    {mybrown}\n  }\n]\n%393.2K\n\\addplot+[] coordinates{\n(3.05e7,4.645)(3.05e7,4.48)(5.9e7,4.415)(1.14e8,4.34)(8.3e8,4.29)(2.3e10,4.28)\n};\n\\addlegendentry{393.2K}\n%2M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.25)(5.9e7,4.1)(1.14e8,3.93)(2.2e8,3.867)(4.3e8,3.837)(8.3e8,3.8)(2.3e10,3.77)\n};\n\\addlegendentry{3M}\n%25M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.25)(5.9e7,3.941)(1.14e8,3.735)(2.2e8,3.567)(4.3e8,3.415)(8.3e8,3.325)(2.3e10,3.27)\n};\n\\addlegendentry{25M}\n%85M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.21)(5.9e7,3.941)(1.14e8,3.69)(2.2e8,3.472)(4.3e8,3.31)(8.3e8,3.12)(1.61e9,3.04)(2.3e10,2.97)\n};\n\\addlegendentry{85M}\n%302M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.21)(5.9e7,3.941)(1.14e8,3.69)(2.2e8,3.46)(4.3e8,3.28)(8.3e8,3.01)(1.61e9,2.84)(2.3e10,2.62)\n};\n\\addlegendentry{302M}\n%708M\n\\addplot+[]\ncoordinates{\n(3.05e7,4.31)(5.9e7,3.941)(1.14e8,3.69)(2.2e8,3.46)(4.3e8,3.28)(8.3e8,3.05)(1.61e9,2.80)(2.3e10,2.42)\n};\n\\addlegendentry{708M}\n%%%approximation\n%393.2K\n\\addplot+[LineD,smooth]coordinates{\n(1.5e7,4.595) (3.05e7,4.47) (5.9e7,4.395) (1.14e8,4.35) (8.3e8,4.3) (3e10,4.290)\n};\n%2M\n\\addplot+[LineD,smooth] coordinates{\n(1.5e7,4.46) (3.05e7,4.25) (5.9e7,4.08) (1.14e8,3.96) (2.2e8,3.867) (4.3e8,3.814) (8.3e8,3.789) (3e10,3.756)\n};\n%25M\n\\addplot+[LineD,smooth]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.95)(1.14e8,3.75)(2.2e8,3.58)(4.3e8,3.444)(8.3e8,3.345)(3e9,3.253)(3e10,3.213)};\n%85M\n\\addplot+[LineD,smooth,samples=200]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.93)(1.14e8,3.7)(2.2e8,3.499)(4.3e8,3.32)(8.3e8,3.17)\n(1.61e9,3.064)(5e9,2.955)(1e10,2.92)(3e10,2.913)};\n%30M\n\\addplot+[LineD,smooth,samples=200]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.93)(1.14e8,3.7)(2.2e8,3.467)(4.3e8,3.25)(8.3e8,3.054)\n(1.61e9,2.89)(4e9,2.73)(1e10,2.64)(3e10,2.59)};\n%708M\n\\addplot+[LineD,smooth,samples=200]  coordinates{\n(1.5e7,4.42) (3.05e7,4.17)(5.9e7,3.93)(1.14e8,3.7)(2.2e8,3.456)(4.3e8,3.223)(8.3e8,3.013)\n(1.61e9,2.82)(4e9,2.61)(1e10,2.47)(3e10,2.39)};\n\\node[font=\\fontsize{7pt}{7}\\selectfont\\usefont{T1}{phv}{m}{n},\nanchor=south,above=0pt,fill=white]at(axis description cs:0.1,0.45){Params};\n\\end{axis}\n\\end{tikzpicture}\n```\n: **Loss vs Model and Dataset Size**: Early-stopped test loss varies predictably with both dataset size and model size, highlighting the importance of balanced scaling for optimal performance under fixed compute budgets.\n:::\n\nScaling laws show that performance improvements follow predictable patterns that change depending on resource availability and exhibit distinct behaviors across different dimensions. Two important types of scaling regimes emerge: **data-driven regimes** that describe how performance changes with dataset size, and **temporal regimes** that describe when in the ML lifecycle we apply additional compute.\n\n#### Data-Limited Scaling Regimes {#sec-efficient-ai-datalimited-scaling-regimes-ba1d}\n\nThe relationship between generalization error and dataset size exhibits three distinct regimes, as shown in @fig-data-scaling-regimes. When limited examples are available, high generalization error results from inadequate statistical estimates. As data availability increases, generalization error decreases predictably as a function of dataset size, following a power-law relationship that provides the most practical benefit from data scaling. Eventually, performance reaches saturation, approaching a floor determined by inherent data limitations or model capacity, beyond which additional data yields negligible improvements.\n\n::: {#fig-data-scaling-regimes fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.7}{\n\\begin{tikzpicture}[line join=round,line cap=round,font=\\small\\usefont{T1}{phv}{m}{n}]\n\\def\\hi{5.5}\n\\def\\wi{11}\n\\def\\hl{5/7*\\hi}\n\\draw[thick](0,-1)coordinate(O)--node[below=3pt]{Training Data Set Size (Log-Scale)}(\\wi,-1)coordinate(E);\n\\draw[thick](0,-1)--node[above=3pt,midway,sloped]{Generalization Error (Log-Scale)}(0,\\hi);\n%\n\\draw[dashed,violet,thick](0,0)--(\\wi,0);\n\\draw[dashed,red,thick](0,\\hl)--(\\wi,\\hl);\n%\n\\coordinate(A)at(3,-0.7);\n\\coordinate(A1)at(3,-1);\n\\coordinate(B)at(8,-0.7);\n\\coordinate(G1)at($(0,\\hl)+(0,-0.1)$);\n\\coordinate(G2)at($(\\wi,0)+(0,0.1)$);\n\\coordinate(GG1)at($(G1)+(1.5,0)$);\n\\coordinate(GG2)at($(G2)+(-1.5,0)$);\n\n\\path[thick](A)--++(90:\\hi)coordinate(LG1);\n\\path[thick](B)--++(90:\\hi)coordinate(LG2);\n\n\\draw[smooth,blue,line width=2pt](G1)--\nnode[above=2pt,align=center,text=black,pos=0.98]{Best Guess Error}(GG1)\nto[out=360,in=180](GG2)--\nnode[below=2pt,align=center,text=black,pos=0.1]{Irreducible Error}(G2);\n\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=0mm,\nline width=0.75pt,inner ysep=0mm,\nfill=magenta!05,fit=(O)(LG1)](BB){};\n\\node[above=1pt of BB.north,anchor=south,align=center]{Small Data\\\\ Region};\n%\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=0mm,\nline width=0.75pt,inner ysep=0mm,\nfill=green!10,fit=(A1)(LG2)](BB1){};\n\\node[above=1pt of BB1.north,anchor=south,align=center]{Power-Law\\\\ Region};\n\n\\scoped[on background layer]\n\\node[draw=none,inner xsep=0mm,\nline width=0.75pt,inner ysep=0mm,\nfill=magenta!05,fit=(LG2)(E)](BB2){};\n\\node[above=1pt of BB2.north,anchor=south,align=center]{Irreducible Error\\\\ Region};\n%\n\\end{tikzpicture}}\n```\n: **Data Scaling Regimes**: The relationship between dataset size and generalization error follows distinct scaling regimes. Increasing dataset size initially reduces generalization error following a power-law relationship, but eventually plateaus at an irreducible error floor determined by inherent data limitations or model capacity [@hestness2017deep]. This behavior exposes diminishing returns from data scaling and informs practical decisions about data collection efforts in machine learning systems.\n:::\n\nThis three-regime pattern manifests across different resource dimensions beyond data alone. Operating within the power-law region provides the most reliable return on resource investment. Reaching this regime requires minimum resource thresholds, while maintaining operation within it demands careful allocation to avoid premature saturation.\n\n#### Temporal Scaling Regimes {#sec-efficient-ai-temporal-scaling-regimes-e118}\n\nWhile data-driven regimes characterize how performance varies with dataset size, a complementary perspective examines temporal allocation of compute resources within the ML lifecycle. Recent research has identified three distinct **temporal scaling regimes** characterizing different stages of model development and deployment.\n\n**Pre-training scaling** encompasses the traditional domain of scaling laws, characterizing how model performance improves with larger architectures, expanded datasets, and increased compute during initial training. Extensive study in foundation models has established clear power-law relationships between resources and capabilities.\n\n**Post-training scaling** characterizes improvements achieved after initial training through techniques including fine-tuning, prompt engineering, and task-specific adaptation. This regime has gained prominence with foundation models, where adaptation rather than retraining frequently provides the most efficient path to enhanced performance under moderate resource requirements.\n\n**Test-time scaling** characterizes how performance improvements result from additional compute allocation during inference without modifying model parameters. This encompasses methods including ensemble prediction, chain-of-thought prompting, and iterative refinement, enabling models to allocate additional processing time per input.\n\nThe relative compute allocation across these regimes reveals important efficiency insights:\n\n- **Pre-training**: Approximately $10^{24}$ FLOPs for frontier models (one-time investment)\n- **Post-training**: Approximately $10^{21}$ FLOPs (0.1% of pre-training compute)\n- **Inference per query**: Approximately $10^{12}$ FLOPs\n\nThe critical insight for efficiency planning is that aggregate inference compute can dominate total system cost. With 1 billion queries per day, daily inference compute reaches $10^{21}$ FLOPs, equaling the entire post-training budget. After one year, aggregate inference exceeds pre-training compute by 300x. This explains why inference efficiency becomes the dominant optimization target for high-volume deployments, while pre-training efficiency matters more for organizations that frequently retrain models.\n\n@fig-scaling-regimes shows these temporal regimes exhibit distinct characteristics in computational resource allocation for performance improvement. Pre-training demands massive resources while providing broad capabilities, post-training offers targeted enhancements under moderate requirements, and test-time scaling enables flexible performance-compute trade-offs adjustable per inference.\n\n::: {#fig-scaling-regimes fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.75}{\n\\begin{tikzpicture}[line join=round,line cap=round,font=\\small\\usefont{T1}{phv}{m}{n},yscale=0.8]\n\\tikzset{Line/.style={line width=2.5pt,RedLine},\nLineD/.style={Line,line width=0.75pt,dashed}\n}\n\\def\\hi{7.5}\n\\def\\wi{11}\n\\draw[thick](0,0)coordinate(O)--node[below=3pt]{Compute}(\\wi,0);\n\\draw[thick](0,0)--node[above=3pt,midway,sloped]{Intelligence}(0,\\hi)coordinate(Y);\n%\n\n\\coordinate(O)at(0.03,0.03);\n\\coordinate(T1)at(2,0.88);\n\\coordinate(T2)at(4.2,3.0);\n\\coordinate(T3)at(6,5.2);\n\\coordinate(T4)at(7.7,6.35);\n\\draw[Line](O)\nto (T1)\nto [out=30,in=210](T2)\nto [out=55,in=220](T3)\nto [out=40,in=210](T4);\n\\draw[Line,-latex](O)--++(23:3.6)node[below right,text=black]{Pre-training scaling};\n\\draw[blue,-latex,LineD](O)--++(23:7.0);\n%\n\\draw[Line,-latex](T2)--++(27:1.6)node[below right,text=black]{Post-training scaling};\n\\draw[-latex,LineD](T2)--++(27:4.0);\n\\draw[Line,-latex](T3)to [out=40,in=210]($(T4)+(0.15,0.09)$)\nnode[below right,text=black,align=center]{Test-time scaling\\\\ \"long thinking};\n\\draw[-latex,LineD](T4)--++(29:2.0);\n\\node[below right=of Y,align=center,font=\\normalsize\\usefont{T1}{phv}{m}{n}]{From one to three \\\\ scaling laws};\n\\end{tikzpicture}}\n```\n: **Temporal Scaling Regimes**: Different temporal scaling regimes offer distinct approaches to improving model performance with varying compute investments. Pre-training establishes broad capabilities through large-scale training from scratch, post-training refines existing models through additional training phases, and test-time scaling dynamically allocates compute during inference to enhance per-sample results. Understanding these regimes clarifies the trade-offs between upfront investment and flexible, on-demand resource allocation for optimal system performance.\n:::\n\nData-driven and temporal scaling regimes inform system design, revealing multiple paths to performance improvement beyond scaling training resources alone. For resource-constrained deployments, post-training and test-time scaling may provide more practical approaches than complete model retraining, while data-efficient techniques enable effective system operation within the power-law regime using smaller datasets.\n\n### Practical Applications in System Design {#sec-efficient-ai-practical-applications-system-design-5c97}\n\nScaling laws provide powerful insights for practical system design and resource planning. Consistent observation of power-law trends indicates that within well-defined operational regimes, model performance depends predominantly on scale rather than idiosyncratic architectural innovations. However, diminishing returns phenomena indicate that each additional improvement requires exponentially increased resources while delivering progressively smaller benefits.\n\nOpenAI's development of GPT-3 demonstrates this principle. Rather than conducting expensive architecture searches, the authors applied scaling laws derived from earlier experiments to determine optimal training dataset size and model parameter count [@brown2020language]. They scaled an established transformer architecture along the compute-optimal frontier to 175 billion parameters and approximately 300 billion tokens, enabling advance prediction of model performance and resource requirements. This methodology demonstrated the practical application of scaling laws in large-scale system planning.\n\nScaling laws serve multiple practical functions in system design. They enable practitioners to estimate returns on investment for different resource allocations during resource budgeting. Under fixed computational budgets, designers can utilize empirical scaling curves to determine optimal performance improvement strategies across model size, dataset expansion, or training duration.\n\nSystem designers can utilize scaling trends to identify when architectural changes yield significant improvements relative to gains achieved through scaling alone, thereby avoiding exhaustive architecture search. When a model family exhibits favorable scaling behavior, scaling the existing architecture may prove more effective than transitioning to more complex but unvalidated designs.\n\nIn edge and embedded environments with constrained resource budgets, understanding performance degradation under model scaling enables designers to select smaller configurations delivering acceptable accuracy within deployment constraints. By quantifying scale-performance trade-offs, scaling laws identify when brute-force scaling becomes inefficient and indicate the necessity for alternative approaches including model compression, efficient knowledge transfer, sparsity techniques, and hardware-aware design.\n\nScaling laws also function as diagnostic instruments. Performance plateaus despite increased resources may indicate dimensional saturation—such as inadequate data relative to model size—or inefficient computational resource utilization. This diagnostic capability renders scaling laws both predictive and prescriptive, facilitating systematic bottleneck identification and resolution.\n\n### Sustainability and Cost Implications {#sec-efficient-ai-sustainability-cost-implications-0473}\n\nScaling laws illuminate pathways to performance enhancement while revealing rapidly escalating resource demands. As models expand, training and deployment resource requirements grow disproportionately, creating tension between performance gains through scaling and system efficiency.\n\nTraining large-scale models necessitates substantial processing power, typically requiring distributed infrastructures[^fn-distributed-infrastructure] comprising hundreds or thousands of accelerators. State-of-the-art language model training may require tens of thousands of GPU-days, consuming millions of kilowatt-hours of electricity. These distributed training systems introduce additional complexity around communication overhead, synchronization, and scaling efficiency, as detailed in @sec-ai-training. Energy demands have outpaced Moore's Law improvements, raising critical questions about long-term sustainability.\n\n[^fn-distributed-infrastructure]: **Distributed Infrastructure**: Computing systems that spread ML workloads across multiple machines connected by high-speed networks. OpenAI's GPT-4 training likely used thousands of NVIDIA A100 GPUs connected via InfiniBand, requiring careful orchestration to avoid communication bottlenecks.\n\nLarge models require extensive, high-quality, diverse datasets to achieve their full potential. Data collection, cleansing, and labeling processes consume considerable time and resources. As models approach saturation of available high-quality data, particularly in natural language processing, additional performance gains through data scaling become increasingly difficult to achieve. This reality underscores data efficiency as a necessary complement to brute-force scaling approaches.\n\nThe financial and environmental implications compound these challenges. Training runs for large foundation models can incur millions of dollars in computational expenses, and associated carbon footprints[^fn-carbon-emissions] have garnered increasing scrutiny. These costs limit accessibility to cutting-edge research and exacerbate disparities in access to advanced AI systems. These democratization challenges introduced by efficiency barriers connect directly to broader accessibility and sustainability goals that responsible ML practitioners must consider.\n\n[^fn-carbon-emissions]: **Carbon Emissions**: Training GPT-3 generated approximately 502 tons of CO₂ equivalent, comparable to annual emissions of 123 gasoline-powered vehicles. Modern ML practices increasingly incorporate carbon tracking using tools like CodeCarbon and the ML CO2 Impact calculator.\n\nThese trade-offs demonstrate that scaling laws provide valuable frameworks for understanding performance growth but do not constitute unencumbered paths to improvement. Each incremental performance gain requires evaluation against corresponding resource requirements. As systems approach practical scaling limits, emphasis must transition from scaling alone to efficient scaling—a comprehensive approach balancing performance, cost, energy consumption, and environmental impact.\n\n### Scaling Law Breakdown Conditions {#sec-efficient-ai-scaling-law-breakdown-conditions-1f8c}\n\nScaling laws exhibit remarkable consistency within specific operational regimes but possess inherent limitations. As systems expand, they inevitably encounter boundaries where underlying assumptions of smooth, predictable scaling cease to hold. These breakdown points expose critical inefficiencies and emphasize the necessity for refined system design approaches.\n\nFor scaling laws to remain valid, model size, dataset size, and computational budget must be augmented in coordinated fashion. Over-investment in one dimension while maintaining others constant often results in suboptimal outcomes. For example, increasing model size without expanding training datasets may induce overfitting, while increasing computational resources without model redesign may lead to inefficient utilization [@hoffmann2022training].\n\nLarge-scale models require carefully tuned training schedules and learning rates to fully utilize available resources. When compute is insufficiently allocated due to premature stopping, batch size misalignment, or ineffective parallelism, models may fail to reach performance potential despite significant infrastructure investment.\n\nScaling laws presuppose continued performance improvement with sufficient training data. However, in numerous domains, availability of high-quality, human-annotated data is finite. As models consume increasingly large datasets, they reach points of diminishing marginal utility where additional data contributes minimal new information. Beyond this threshold, larger models may exhibit memorization rather than generalization.\n\nAs models grow, they demand greater memory bandwidth[^fn-memory-bandwidth], interconnect capacity, and I/O throughput. These hardware limitations become increasingly challenging even with specialized accelerators. Distributing trillion-parameter models across clusters necessitates meticulous management of data parallelism, communication overhead, and fault tolerance.\n\n[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be read from or written to memory, measured in GB/s. NVIDIA H100 provides 3.35 TB/s memory bandwidth vs. typical DDR5 RAM's 51 GB/s, a 65× difference critical for handling large model parameters.\n\nAt extreme scales, models may approach limits of what can be learned from training distributions. Performance on benchmarks may continue improving, but these improvements may no longer reflect meaningful gains in generalization or understanding. Models may become increasingly brittle, susceptible to adversarial examples, or prone to generating plausible but inaccurate outputs.\n\n@tbl-scaling-breakdown synthesizes the primary causes of scaling failure, outlining typical breakdown types, underlying causes, and representative scenarios as a reference for anticipating inefficiencies and guiding balanced system design.\n\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Dimension Scaled**   | **Type of Breakdown**   | **Underlying Cause**                           | **Example Scenario**                          |\n+:=======================+:========================+:===============================================+:==============================================+\n| **Model Size**         | Overfitting             | Model capacity exceeds available data          | Billion-parameter model on limited dataset    |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Data Volume**        | Diminishing Returns     | Saturation of new or diverse information       | Scaling web text beyond useful threshold      |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Compute Budget**     | Underutilized Resources | Insufficient training steps or inefficient use | Large model with truncated training duration  |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **Imbalanced Scaling** | Inefficiency            | Uncoordinated increase in model/data/compute   | Doubling model size without more data or time |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n| **All Dimensions**     | Semantic Saturation     | Exhaustion of learnable patterns in the domain | No further gains despite scaling all inputs   |\n+------------------------+-------------------------+------------------------------------------------+-----------------------------------------------+\n\n: **Scaling Breakdown Types**: Unbalanced scaling across model size, data volume, and compute resources leads to specific failure modes, such as overfitting or diminishing returns, impacting system performance and efficiency. The table categorizes these breakdowns, identifies their root causes, and provides representative scenarios to guide more effective system design and resource allocation. {#tbl-scaling-breakdown}\n\nThese breakdown points demonstrate that scaling laws describe empirical regularities under specific conditions that become increasingly difficult to maintain at scale. As machine learning systems continue evolving, discerning where and why scaling ceases to be effective becomes necessary, driving development of strategies that enhance performance without relying solely on scale.\n\n### Integrating Efficiency with Scaling {#sec-efficient-ai-integrating-efficiency-scaling-a513}\n\nThe limitations exposed by scaling laws (data saturation, infrastructure bottlenecks, and diminishing returns) demonstrate that brute-force scaling alone cannot deliver sustainable AI systems. These constraints necessitate a shift from expanding scale to achieving greater efficiency with reduced resources.\n\nThis transition requires coordinated optimization across three interconnected dimensions: **algorithmic efficiency** addresses computational intensity through better model design, **compute efficiency** maximizes hardware utilization to translate algorithmic improvements into practical gains, and **data efficiency** extracts maximum information from limited examples as high-quality data becomes scarce. Together, these dimensions provide systematic approaches to achieving performance goals that scaling alone cannot sustainably deliver, while addressing broader concerns about equitable access to AI capabilities and environmental impact.\n\nHaving examined how scaling laws reveal fundamental constraints, we now turn to the efficiency framework that provides concrete strategies for operating effectively within these constraints. The following section details how the three efficiency dimensions work together to enable sustainable, accessible machine learning systems.\n\n## The Efficiency Framework {#sec-efficient-ai-efficiency-framework-c0de}\n\nThe constraint identified through scaling laws (that continued progress requires systematic efficiency optimization) motivates three complementary efficiency dimensions. Each dimension addresses a specific limitation: algorithmic efficiency tackles computational intensity, compute efficiency addresses hardware utilization gaps, and data efficiency solves the data saturation problem.\n\nTogether, these three dimensions provide a systematic framework for addressing the constraints that scaling laws reveal. Targeted optimizations across algorithmic design, hardware utilization, and data usage can achieve what brute-force scaling cannot: sustainable, accessible, high-performance AI systems.\n\n### Multi-Dimensional Efficiency Synergies {#sec-efficient-ai-multidimensional-efficiency-synergies-ea04}\n\nOptimal performance requires coordinated optimization across multiple dimensions. No single resource—whether model parameters, training data, or compute budget—can be scaled indefinitely to achieve efficiency. Modern techniques demonstrate the potential: 10-100x gains in algorithmic efficiency through optimized architectures, 5-50x improvements in hardware utilization through specialized processors, and 10-1000x reductions in data requirements through advanced learning methods.\n\nThe power of this framework emerges from interconnections between dimensions, as depicted in @fig-evolution-efficiency. Algorithmic innovations often enable better hardware utilization, while hardware advances unlock new algorithmic possibilities. Data-efficient techniques reduce computational requirements, while compute-efficient methods enable training on larger datasets. Understanding these synergies is essential for building practical ML systems.\n\n::: {#fig-evolution-efficiency fig-env=\"figure\" fig-pos=\"htb\"}\n\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n},node distance=2mm]\n\\tikzset{\n  Box/.style={inner xsep=1pt,\n    draw=none,\n    fill=#1,\n    anchor=west,\n    text width=27mm,align=center,\n    minimum width=27mm, minimum height=10mm\n  },\n  Box/.default=red\n}\n\\definecolor{col1}{RGB}{128, 179, 255}\n\\definecolor{col2}{RGB}{255, 255, 128}\n\\definecolor{col3}{RGB}{204, 255, 204}\n\\definecolor{col4}{RGB}{230, 179, 255}\n\\definecolor{col5}{RGB}{255, 153, 204}\n\\definecolor{col6}{RGB}{245, 82, 102}\n\\definecolor{col7}{RGB}{255, 102, 102}\n\n\\node[Box={col1}](B1){Algorithmic\\\\ Efficiency};\n\\node[Box={col1},right=of B1](B2){Deep\\\\ Learning Era};\n\\node[Box={col1},right=of B2](B3){Modern\\\\ Efficiency};\n\\node[Box={col2},right=of B3](B4){General-Purpose\\\\ Computing};\n\\node[Box={col2},right=of B4](B5){Accelerated\\\\ Computing};\n\\node[Box={col2},right=of B5](B6){Sustainable Computing};\n\\node[Box={col3},right=of B6](B7){Data\\\\ Scarcity};\n\\node[Box={col3},right=of B7](B8){Big\\\\ Data Era};\n\\node[Box={col3},right=of B8](B9){ Data-Centric AI};\n%%%%\n\\node[Box={col1},above=of B2,minimum width=87mm,\n text width=85mm](GB1){Algorithmic Efficiency};\n\\node[Box={col2},above=of B5,minimum width=87mm,\ntext width=85mm](GB5){Compute Efficiency};\n\\node[Box={col3},above=of B8,minimum width=87mm,\ntext width=85mm](GB8){Data Efficiency};\n%%\n\\foreach \\x in{1,2,...,9}\n\\draw[dashed,thick,-latex](B\\x)--++(270:5.5);\n\n\\path[red]([yshift=-8mm]B1.south west)coordinate(P)-|coordinate(K)(B9.south east);\n\\draw[line width=2pt,-latex](P)--(K)--++(0:3mm);\n\n\\node[Box={col1!50},below=2 of B1](BB1){1980};\n\\node[Box={col1!50},below=2 of B2](BB2){2010};\n\\node[Box={col1!50},below=2 of B3](BB3){2023};\n\\node[Box={col2!70},below=2 of B4](BB4){1980};\n\\node[Box={col2!70},below=2 of B5](BB5){2010};\n\\node[Box={col2!70},below=2 of B6](BB6){2023};\n\\node[Box={col3!70},below=2 of B7](BB7){1980};\n\\node[Box={col3!50},below=2 of B8](BB8){2010};\n\\node[Box={col3!50},below=2 of B9](BB9){2023};\n%%%%%\n\\node[Box={col4!50},below= of BB1](BBB1){2010};\n\\node[Box={col4!50},below= of BB2](BBB2){2022};\n\\node[Box={col4!50},below= of BB3](BBB3){Future};\n%\n\\node[Box={col5!50},below= of BB4](BBB4){2010};\n\\node[Box={col5!50},below= of BB5](BBB5){2022};\n\\node[Box={col5!50},below= of BB6](BBB6){Future};\n%\n\\node[Box={col7!50},below= of BB7](BBB7){2010};\n\\node[Box={col7!50},below= of BB8](BBB8){2022};\n\\node[Box={col7!50},below= of BB9](BBB9){Future};\n\\end{tikzpicture}\n```\n: **Historical Efficiency Trends**: Algorithmic, computational, and data efficiency have each contributed to substantial gains in AI capabilities, though at different rates and with diminishing returns. Understanding these historical trends clarifies the interplay between these efficiency dimensions and informs strategies for scaling machine learning systems in data-limited environments.\n:::\n\nThe specific priorities vary across deployment environments. Cloud systems with abundant resources prioritize scalability and throughput, while edge devices face severe memory and power constraints. Mobile applications must balance performance with battery life, and TinyML deployments demand extreme resource efficiency. Understanding these context-specific patterns enables designers to make informed decisions about which efficiency dimensions to prioritize and how to address inevitable trade-offs between them.\n\n### Achieving Algorithmic Efficiency {#sec-efficient-ai-achieving-algorithmic-efficiency-ef15}\n\nAlgorithmic efficiency achieves maximum performance per unit of computation through optimized model architectures and training procedures. Modern techniques achieve 10-100x improvements in computational requirements while maintaining or improving accuracy, providing the most direct path to practical AI deployment.\n\n#### Efficiency Metrics for Compression {#sec-efficient-ai-efficiency-metrics-compression}\n\nEvaluating compression techniques requires multiple complementary metrics that capture different aspects of efficiency gains.\n\n**Compression Ratio** measures the reduction in model size:\n$$CR = \\frac{\\text{Original Size}}{\\text{Compressed Size}} = \\frac{N_{params} \\times b_{orig}}{N_{remaining} \\times b_{compressed} + \\text{Overhead}}$$\nwhere overhead includes index storage for sparse representations.\n\n**Theoretical vs. Actual Speedup** represents a critical distinction in system analysis. FLOPs reduction does not equal wall-clock speedup:\n$$\\text{Actual Speedup} = \\frac{T_{original}}{T_{compressed}} \\neq \\frac{\\text{FLOPs}_{original}}{\\text{FLOPs}_{compressed}}$$\n\nThis gap exists because memory bandwidth may limit performance rather than compute, sparse operations require indexing overhead, reduced precision may not map to hardware acceleration, and kernel launch overhead becomes proportionally larger for smaller operations.\n\n**Structured Pruning Advantage**: Structured pruning (removing entire filters or channels) achieves speedups closer to theoretical because it preserves dense tensor operations. Unstructured pruning requires sparse matrix formats that may not accelerate on standard hardware.\n\n**Accuracy-Efficiency Pareto Frontier**: The optimal compression strategy lies on the Pareto frontier, the set of solutions where no technique achieves BOTH higher accuracy AND higher efficiency. Different techniques occupy different positions on this frontier, as shown in the comparison table below.\n\n+---------------------------+------------+----------------+---------------+\n| Technique                 | Typical CR | Actual Speedup | Accuracy Loss |\n+:==========================+:===========+:===============+:==============+\n| INT8 quantization         | 4x         | 2-4x           | <1%           |\n+---------------------------+------------+----------------+---------------+\n| 50% structured pruning    | 2x         | 1.5-2x         | <2%           |\n+---------------------------+------------+----------------+---------------+\n| 90% unstructured pruning  | 10x        | 1.2-1.5x       | 1-3%          |\n+---------------------------+------------+----------------+---------------+\n| Knowledge distillation    | 4-10x      | 4-10x          | 1-5%          |\n+---------------------------+------------+----------------+---------------+\n\n: **Compression Technique Trade-offs**: Different compression approaches offer varying trade-offs between compression ratio, actual speedup, and accuracy loss, enabling practitioners to select techniques appropriate for their deployment constraints. {#tbl-compression-tradeoffs}\n\n#### Why Neural Networks Are Compressible {#sec-efficient-ai-why-networks-compressible}\n\nThe foundation for compression improvements lies in a key observation: most neural networks are dramatically overparameterized. Trained neural networks exhibit a characteristic property where weight magnitude distributions follow heavy-tailed patterns. Most weights cluster near zero while a small fraction carry the majority of information.\n\nMathematically, if we sort weights by absolute magnitude $|w_i|$, typically 80-90% of weights contribute less than 10% to the output activation. This redundancy arises from overparameterization during training, where networks learn in a high-dimensional space but converge to solutions lying in lower-dimensional manifolds.\n\n**Sensitivity Analysis**: The sensitivity of each weight to network output can be approximated by the first-order Taylor expansion of the loss function:\n$$\\Delta L \\approx \\sum_i g_i \\cdot \\Delta w_i$$\nwhere $g_i = \\partial L / \\partial w_i$ is the gradient. For converged networks, $g_i \\approx 0$, leading to the simpler heuristic of removing weights with smallest $|w_i|$. This is the basis of magnitude-based pruning pioneered in Deep Compression [@han2015deep].\n\nThe lottery ticket hypothesis reveals that networks contain sparse subnetworks that achieve comparable accuracy when trained in isolation [@frankle2019lottery]. The size of these \"winning tickets\" varies significantly by architecture: 3-5% of original parameters for deep vision models on ImageNet, 10-30% for smaller networks on CIFAR, and larger fractions for language models where attention patterns require more parameters. This discovery transforms compression into a principled approach: large models serve as initialization strategies for finding efficient architectures.\n\n+----------------------+-----------------------+-------------------+--------------+\n| Pruning Type         | Speedup               | Hardware Support  | Accuracy     |\n+:=====================+:======================+:==================+:=============+\n| Unstructured         | Theoretical only      | Requires sparse   | Best         |\n|                      |                       | formats           |              |\n+----------------------+-----------------------+-------------------+--------------+\n| Structured (filter)  | Real 2-4x             | Standard dense    | Moderate     |\n|                      |                       | ops               |              |\n+----------------------+-----------------------+-------------------+--------------+\n| N:M (e.g., 2:4)      | Real 2x               | Ampere Sparse     | Good         |\n|                      |                       | Tensor Cores      |              |\n+----------------------+-----------------------+-------------------+--------------+\n\n: **Structured vs. Unstructured Pruning**: Different pruning approaches offer varying trade-offs between theoretical speedup, hardware support requirements, and accuracy preservation. {#tbl-pruning-types}\n\n#### Model Compression Fundamentals {#sec-efficient-ai-model-compression-fundamentals-bcc3}\n\nThree major approaches dominate modern algorithmic efficiency, each targeting different aspects of model inefficiency:\n\n**Model Compression** systematically removes redundant components from neural networks. Pruning techniques achieve 2-4x inference speedup with 1-3% accuracy loss by removing unnecessary weights and structures. Research demonstrates that ResNet-50 can be reduced to 20% of original parameters while maintaining 99% of ImageNet accuracy [@gholami2021survey]. The specific pruning algorithms—including magnitude-based selection, structured vs. unstructured approaches, and layer-wise sensitivity analysis—are covered in detail in @sec-model-optimizations.\n\n**Precision Optimization** reduces computational requirements through quantization, which maps high-precision floating-point values to lower-precision representations. For uniform affine quantization, the mapping is defined as:\n$$q = \\text{round}\\left(\\frac{r}{s}\\right) + z$$\nwhere $r$ is the real-valued input, $s$ is the scale factor, $z$ is the zero-point offset, and $q$ is the quantized integer. Dequantization recovers an approximation: $\\hat{r} = s \\cdot (q - z)$. The quantization error $\\epsilon = r - \\hat{r}$ is bounded by $|\\epsilon| \\leq s/2$ for rounding, and the scale factor for a range $[r_{min}, r_{max}]$ mapped to $b$ bits is:\n$$s = \\frac{r_{max} - r_{min}}{2^b - 1}$$\n\nNeural networks demonstrate inherent robustness to this bounded quantization error for several reasons: the error is bounded and approximately uniform, normalization layers (BatchNorm and LayerNorm) provide implicit regularization, many ReLU activations are zero and require no precision, and overparameterization provides redundant capacity to absorb noise. INT8 quantization achieves 4x memory reduction and 2-4x inference speedup while typically maintaining 98-99% of FP32 accuracy [@Jacob_et_al_2018]. Modern techniques range from simple post-training quantization to sophisticated quantization-aware training. The specific quantization algorithms, calibration methods, and training procedures are detailed in @sec-model-optimizations.\n\n+----------------+------+----------------+-----------------+\n| Scheme         | Bits | Memory Savings | Accuracy Impact |\n+:===============+:=====+:===============+:================+\n| FP32 to FP16   | 16   | 2x             | Negligible      |\n+----------------+------+----------------+-----------------+\n| FP32 to INT8   | 8    | 4x             | 0.1-1% loss     |\n+----------------+------+----------------+-----------------+\n| FP32 to INT4   | 4    | 8x             | 1-5% loss       |\n+----------------+------+----------------+-----------------+\n\n: **Quantization Schemes and Trade-offs**: Lower precision representations provide greater memory savings but with increasing accuracy impact. {#tbl-quantization-schemes}\n\n**Knowledge Transfer** distills capabilities from large teacher models into efficient student models. The core mechanism uses a combined loss function:\n$$L = \\alpha \\cdot L_{CE}(y, p_s) + (1 - \\alpha) \\cdot T^2 \\cdot L_{KL}(p_t^{(T)}, p_s^{(T)})$$\nwhere $L_{CE}$ is the cross-entropy loss with true labels, $L_{KL}$ is the Kullback-Leibler divergence between teacher and student outputs, and $T$ is the temperature parameter that softens the probability distributions. The $T^2$ factor compensates for the reduced gradient magnitudes when using high temperatures. Higher temperatures reveal the teacher's learned relationships between classes, including which incorrect classes are \"close\" to the correct answer, providing richer training signal than hard labels alone. Knowledge distillation[^fn-knowledge-distillation] achieves 40-60% parameter reduction while retaining 95-97% of original performance, addressing both computational efficiency and data efficiency by requiring fewer training examples. The specific distillation algorithms, loss functions, and training procedures are covered in @sec-model-optimizations.\n\n[^fn-knowledge-distillation]: **Knowledge Distillation**: Technique where a large \"teacher\" model transfers knowledge to a smaller \"student\" model by training the student to mimic the teacher's output probabilities. DistilBERT achieves ~97% of BERT's performance on GLUE benchmark with 40% fewer parameters and 60% faster inference through distillation.\n\n#### Hardware-Algorithm Co-Design {#sec-efficient-ai-hardwarealgorithm-codesign-67e8}\n\nAlgorithmic optimizations alone are insufficient; their practical benefits depend on hardware-software co-design. Optimization techniques must be tailored to target hardware characteristics (memory bandwidth, compute capabilities, and precision support) to achieve real-world speedups.\n\n**Discrete Efficiency Steps**: Hardware provides acceleration in discrete steps aligned with specific bit-widths. INT8 quantization achieves 2.3x speedup on NVIDIA V100 GPUs with tensor core support, and INT4 provides additional gains on Ampere architecture, but arbitrary bit-widths like INT6 may provide no benefit because hardware lacks native support. This creates discrete efficiency steps rather than a continuous improvement curve.\n\n**Operational Intensity**: The ratio of compute operations to memory bytes transferred (FLOPs/byte) determines whether a workload is compute-bound or memory-bound. This metric is critical for optimization decisions:\n$$\\text{Operational Intensity} = \\frac{\\text{FLOPs}}{\\text{Bytes Transferred}}$$\nModern accelerators require high operational intensity (often 50-200 FLOPs/byte) to achieve peak utilization. Most neural network layers fall below this threshold, making them memory-bound and explaining why memory bandwidth optimization often provides larger gains than raw compute speedup.\n\n**Compute-Bound vs. Memory-Bound**: For memory-bound operations (most activations, normalization, small batch inference), reducing memory traffic through operator fusion and caching provides larger gains than faster arithmetic. For compute-bound operations (large matrix multiplications in attention and FFN layers), exploiting tensor cores and reduced-precision arithmetic maximizes throughput.\n\nThe roofline model in @sec-ai-acceleration provides the quantitative framework for these decisions, enabling systematic analysis of whether a given workload benefits more from compute or memory optimizations.\n\n#### Architectural Innovation for Efficiency {#sec-efficient-ai-architectural-innovation-efficiency-7dd9}\n\nModern efficiency requires architectures designed for resource constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs can deliver high performance through architectural innovations rather than scaling up existing designs. Each architecture embodies a distinct efficiency principle:\n\n**Depthwise Separable Convolutions** (MobileNet): Traditional convolutions apply $D_K \\times D_K$ filters across all input channels simultaneously. Depthwise separable convolutions factor this into two operations: depthwise convolution (one $D_K \\times D_K$ filter per channel) followed by pointwise convolution (1x1 filters across channels). This achieves approximately 8-9x FLOP reduction for 3x3 kernels on typical channel counts, enabling mobile deployment while maintaining accuracy. The specific derivation of this reduction factor is provided in @sec-model-optimizations.\n\n**Compound Scaling** (EfficientNet): Rather than scaling depth, width, or resolution independently, compound scaling increases all three dimensions together using learned coefficients. This balanced scaling achieves better accuracy-efficiency trade-offs than arbitrary scaling, with EfficientNet-B7 achieving 84.3% ImageNet accuracy while using 8.4x fewer FLOPs than the best prior models.\n\n**Fire Modules** (SqueezeNet): Fire modules first \"squeeze\" channels to a smaller count using 1x1 convolutions, then \"expand\" using a mix of 1x1 and 3x3 convolutions. This squeeze-expand pattern reduces parameters while maintaining receptive field size.\n\n[^fn-mobilenet]: **MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achieving ~50x fewer parameters than traditional models. MobileNet-v1 has only 4.2M parameters vs. VGG-16's ~138M, enabling deployment on smartphones with <100MB memory.\n\n[^fn-efficientnet]: **EfficientNet**: Architecture achieving state-of-the-art accuracy with superior parameter efficiency. EfficientNet-B7 achieves 84.3% ImageNet top-1 accuracy (84.4% in some reports) with 66M parameters, compared to ResNet-152's 77.0% accuracy with approximately 60M parameters.\n\n[^fn-squeezenet]: **SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50x fewer parameters (1.25M vs. 60M). Demonstrated that clever architecture design can dramatically reduce model size without sacrificing performance.\n\nDifferent deployment contexts require different efficiency trade-offs. Cloud inference prioritizes throughput and can tolerate higher memory usage, favoring parallel-friendly operations. Edge deployment prioritizes latency and memory efficiency, requiring architectures that minimize memory access. Mobile deployment constrains energy usage, demanding architectures optimized for energy-efficient operations.\n\n#### Parameter-Efficient Adaptation {#sec-efficient-ai-parameterefficient-adaptation-1bce}\n\nParameter-efficient fine-tuning[^fn-param-efficient] techniques demonstrate how the three efficiency dimensions work together. These methods update less than 1% of model parameters while achieving full fine-tuning performance, addressing all three efficiency pillars: algorithmic efficiency through reduced parameter updates, compute efficiency through lower memory requirements and faster training, and data efficiency by leveraging pre-trained representations that require fewer task-specific examples.\n\n[^fn-param-efficient]: **Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model parameters while achieving full fine-tuning performance. Reduces memory requirements from gigabytes to megabytes for large model adaptation.\n\n**Low-Rank Adaptation (LoRA)** exemplifies the mathematical insight behind parameter-efficient methods. Instead of updating a weight matrix $W \\in \\mathbb{R}^{d \\times k}$ directly, LoRA learns a low-rank decomposition of the update:\n$$W' = W + BA$$\nwhere $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ with rank $r \\ll \\min(d, k)$. This reduces trainable parameters from $d \\times k$ to $r \\times (d + k)$, a reduction factor of $\\frac{dk}{r(d+k)}$. For a 175B parameter model with $r = 8$, this enables approximately 10,000x fewer trainable parameters.\n\nThe key insight is that task-specific updates have low intrinsic dimensionality: while pre-training requires the full parameter space to capture general knowledge, adaptation to specific tasks operates in a much smaller subspace. This explains why LoRA works for fine-tuning but not for training from scratch.\n\nThe practical impact is transformative: fine-tuning GPT-3 traditionally requires storing gradients for 175 billion parameters, consuming over 700GB of GPU memory. LoRA reduces this to under 10GB by learning low-rank decompositions of weight updates, enabling efficient adaptation on single consumer GPUs while requiring only hundreds of examples rather than thousands for effective adaptation.\n\nAs @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-efficient-alexnet]-level performance on ImageNet[^fn-efficient-imagenet] classification decreased by approximately $44\\times$ between 2012 and 2019. This improvement, which halved every 16 months, outpaced hardware efficiency gains of Moore's Law[^fn-efficient-moores-law], demonstrating the role of algorithmic advancements in driving efficiency [@Hernandez_et_al_2020].\n\n[^fn-efficient-alexnet]: **AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with 15.3% error rate, nearly halving the previous best of 26.2%. Used 60M parameters, two GPUs, and launched the deep learning revolution.\n\n[^fn-efficient-imagenet]: **ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove computer vision breakthroughs from 2010-2017.\n\n[^fn-efficient-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every ~2 years. Traditional Moore's Law predicted ~2x transistor density every 18-24 months, though this rate has slowed significantly since ~2015, while AI algorithmic efficiency improved 44x in 7 years (2012-2019).\n\n::: {#fig-algo-efficiency fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{axis}[\n   axis line style={draw=none},\n  width=17cm,\n  height=10cm,\n  date coordinates in=x,\n  table/col sep=comma,\n  xticklabel=\\year,\n  xtick={2013-01-01,2014-01-01,2015-01-01,2016-01-01,2017-01-01,2018-01-01,2019-01-01,2020-01-01},\n  x tick label style={rotate=0, anchor=north},\n  xmax=2020-1-31,\n  ytick={0,5,...,50},\n  ymin=0, ymax=50,\n  ylabel={Training Efficiency Factor},\n  title={44$\\times$ less compute required to get to AlexNet performance 7 years later (linear scale)},\n  enlargelimits=0.05,\n  grid=both,\n  major grid style={black!60},\n  nodes near coords align=right,\n        tick label style={/pgf/number format/assume math mode=true},\n        ticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n]\n\n\\addplot[RedLine,\n  only marks,\n  mark size=2pt,\n] table[x=Date, y=Y,  col sep=comma, meta=Model] {\nModel,Y,Date\nAlexNet, 1.17, 2012-06-01\nGoogLeNet, 4.5, 2014-09-19\nMobileNet\\_v1, 11.2, 2017-04-17\nShuffleNet, 20.8, 2017-07-03\nShuffleNet_v2, 24.85, 2018-06-29\nEfficientNet, 44.5, 2019-06-07\n};\n\n \\addplot[%above\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=2pt,xshift=1mm,\n  font=\\scriptsize\\usefont{T1}{phv}{m}{n}, anchor=south},\n] table[meta=Model, x=Date, y=Y, col sep=comma] {\nModel,Y,Date\nAlexNet, 1, 2012-06-01\nGoogLeNet, 4.3, 2014-09-17\nSqueezenet\\_v1\\_1,3.8,2016-02-25\n};\n\n \\addplot[%left\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={xshift=-1pt,\n  font=\\scriptsize\\usefont{T1}{phv}{m}{n}, anchor=east},\n] table[meta=Model, x=Date, y=Y, col sep=comma] {\nModel,Y,Date\nShuffleNet\\_v1 1x, 21, 2017-07-03\nEfficientNet-b0, 44, 2019-05-28\nVGG-11,0.83,2014-09-04\nResNet-18,2.88,2015-12-11\n};\n \\addplot[%right\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={xshift=1pt,\n  font=\\scriptsize\\usefont{T1}{phv}{m}{n}, anchor=west},\n] table[meta=Model, x=Date, y=Y, col sep=comma] {\nModel,Y,Date\nMobileNet\\_v2,13.3,2018-01-11\nDenseNet121,3.3,2016-09-25\nMobileNet\\_v1, 11.2, 2017-04-17\nShuffleNet\\_v2\\_1\\_5x,17.4,2018-06-29\nShuffleNet\\_v2, 24.85, 2018-06-29\n};\n\n\\addplot[draw=red,  only marks,\n  color=blue,\n  mark=*,  mark size=2pt,\n] table[\n  x=Date,\n  y=Y,\n  col sep=comma\n] {\nModel,Y,Date\nVGG-11,0.83,2014-09-04\nResNet-18,2.88,2015-12-11\nResNet-34,2.38,2015-12-11\nWide_ResNet\\_50,1.0,2016-05-22\nSqueezenet\\_v1\\_1,3.8,2016-02-25\nDenseNet121,3.3,2016-09-25\nResNext\\_50,2.5,2016-09-15\nMobileNet\\_v2,13.3,2018-01-11\nShuffleNet\\_v2\\_1\\_5x,17.4,2018-06-29\n};\n%\n\\coordinate (DL) at (axis description cs:-0.002,0.065);\n\\coordinate (GD) at (axis description cs:0.904,0.945);\n\\draw[black,dashed,thick](DL)to[out=3,in=248,distance=185](GD);\n\\end{axis}\n\\end{tikzpicture}\n```\n: **Algorithmic Efficiency Progress**: Neural network training compute requirements decreased 44× between 2012 and 2019, outpacing hardware improvements and demonstrating the significant impact of algorithmic advancements on model efficiency. Innovations in model architecture and optimization techniques can drive substantial gains in AI system sustainability via this halving of compute every 16 months. Source: [@Hernandez_et_al_2020].\n:::\n\nThe evolution of algorithmic efficiency, from basic compression to hardware-aware optimization and parameter-efficient adaptation, demonstrates the centrality of these techniques to machine learning progress. As the field advances, algorithmic efficiency will remain central to designing systems that are high-performing, scalable, and sustainable.\n\n### Compute Efficiency {#sec-efficient-ai-compute-efficiency-745c}\n\nCompute efficiency focuses on the effective use of hardware and computational resources to train and deploy machine learning models. It encompasses strategies for reducing energy consumption, optimizing processing speed, and leveraging hardware capabilities to achieve scalable and sustainable system performance. While this chapter focuses on efficiency principles and trade-offs, the detailed technical implementation of hardware acceleration—including GPU architectures, TPU (Tensor Processing Unit) design, memory systems, and custom accelerators—is covered in @sec-ai-acceleration.\n\n#### From CPUs to AI Accelerators {#sec-efficient-ai-cpus-ai-accelerators-a8d7}\n\nCompute efficiency's evolution reveals why specialized hardware became essential. In the early days of machine learning, Central Processing Units (CPUs) shaped what was possible. CPUs excel at sequential processing and complex decision-making but have limited parallelism, typically 4-16 cores optimized for diverse tasks rather than the repetitive matrix operations that dominate machine learning. Training times for models were measured in days or weeks, as even relatively small datasets pushed hardware boundaries.\n\nThis CPU-constrained era ended as deep learning models like AlexNet and ResNet[^fn-efficient-resnet] demonstrated the potential of neural networks, quickly surpassing traditional CPU capabilities. As shown in @fig-comp_efficiency, this marked the beginning of exponential growth in compute usage. OpenAI's analysis reveals that compute used in AI training increased approximately 300,000 times from 2012 to 2019, doubling approximately every 3.4 months during this period, a rate far exceeding Moore's Law [@Amodei_et_al_2018].\n\n[^fn-efficient-resnet]: **ResNet**: Residual Network architecture by He et al. [@he2016deep] enabling training of very deep networks (152+ layers) through skip connections. Won ImageNet 2015 with 3.6% error rate, surpassing human-level performance for the first time.\n\n::: {#fig-comp_efficiency fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{axis}[\n   axis line style={draw=none},\n   /pgf/number format/.cd,\n   tick label style={/pgf/number format/assume math mode=true},\n   ticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n  1000 sep={},\n  title={AlexNet to AlphaGo Zero: 300,000$\\times$ increase in compute},\n  xlabel={},\n  ylabel={Petaflop/s-days},\n  xmajorgrids,\n  ymajorgrids,\n  ymin=0.1e-4, ymax=1e4,\n  ymode=log,\n  log basis y=10,\n  ytick={1e-4,1e-2,1e0,1e2,1e4},\n  yticklabels={1e-4,1e-2,1e0,1e2,1e4},\n  xtick={2012,2013,2014,2015,2016,2017,2018},\n  xmin=2011.4,  xmax=2018.5,\n  grid=both,\n  width=13cm,\n  height=9cm,\n  yticklabel style={\n  /pgf/number format/.cd,\n  sci,\n  sci generic={mantissa e exponent},\n  precision=0\n},\n]\n\\addplot+[only marks, mark=*, mark size=1.5pt,\nmark options={fill=red}, color=red]\ntable[x=Date,y=Y, col sep=comma] {\nDate,Y,Model\n  2012.405,5.66e-3,AlexNet\n  2012.495,2.1e-3,Dropout\n  2013.855,5.8e-3,Visualizing and Understanding Conv Nets\n  2013.96,2.6e-5,DQN\n  2014.69,9.3e-2,Seq2Seq\n  2014.67,9.5e-2,VGG\n  2014.7,1.77e-2,GoogleNet\n  2015.92,2.54e-1,DeepSpeech2\n  2015.93,1.14e-1,ResNets\n  2016.72,8.2e1,Neural Machine Translation\n  2016.76,5.33e0,Xception\n  2016.83,3.3e1,Neural Architecture Search\n  2017.6,7.2e0,TI7 Dota 1v1\n  2017.92,4.3e2,AlphaZero\n  2017.79,1.9e3,AlphaGoZero\n};\n%\n\\addplot[%right\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=0pt,\n  align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=west},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2015.92,2.54e-1,DeepSpeech2\n2014.69,9.3e-2,Seq2Seq\n2014.7,1.77e-2,GoogleNet\n2013.855,5.8e-3,Visualizing and Understanding Conv Nets\n2013.96,2.6e-5,DQN\n};\n\\addplot[%left\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=0pt,\n  align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=east},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2016.72,8.2e1,Neural Machine Translation\n2016.83,3.3e1,Neural Architecture Search\n2017.92,4.3e2,AlphaZero\n2017.79,1.9e3,AlphaGoZero\n};\n%\n\\addplot[%below\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=0pt,\n  text width=25mm, align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=north},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2012.495,2.1e-3,Dropout\n2015.93,1.14e-1,ResNets\n2016.76,5.33e0,Xception\n2017.6,7.2e0,TI7 Dota 1v1\n};\n%\n \\addplot[%above\n  only marks,\n  nodes near coords,\n  point meta=explicit symbolic,\n  every node near coord/.append style={yshift=1pt,\n  text width=25mm, align=flush center,\n  font=\\fontsize{6pt}{8}\\selectfont\\usefont{T1}{phv}{m}{n}, anchor=south},\n] table[\n  meta=Model,\n  x=Date,\n  y=Y,\n  col sep=comma\n] {%\nDate,Y,Model\n2012.405,5.66e-3,AlexNet\n2014.67,9.5e-2,VGG\n};\n%\n\\coordinate (DL) at (axis description cs:-0.02,0.025);\n\\coordinate (GD) at (axis description cs:1.02,0.888);\n\\coordinate (SR) at (axis description cs:0.5,-0.10);\n\\end{axis}\n\\draw[dashed](DL)--(GD);\n \\node[below=0 of SR,text width=125mm,font=\\fontsize{7pt}{9}\\selectfont\\usefont{T1}{phv}{m}{n}]{%\n The total amount of compute, in petaflop/s-days, used to train selected results that are\n relatively well known, used a lot of compute for their time, and gave enough information\n to estimate the compute used.};\n\\end{tikzpicture}\n\n```\n: **AI Training Compute Growth**: AI training experienced a 300,000-fold increase in computational requirements from 2012 to 2019, exceeding the growth rate predicted by Moore's Law and driving demand for specialized hardware [@Amodei_et_al_2018]. This exponential growth underscores the increasing complexity of AI models and the need for efficient computing infrastructure to support continued progress.\n:::\n\nThis rapid growth was driven by adoption of Graphics Processing Units (GPUs), which offered unparalleled parallel processing capabilities. While CPUs might have 16 cores, modern high-end GPUs like the NVIDIA H100 contain over 16,000 CUDA cores[^fn-cuda-cores]. Specialized hardware accelerators such as Google's Tensor Processing Units (TPUs) further revolutionized compute efficiency by designing chips specifically for machine learning workloads, optimizing for specific data types and operations most common in neural networks.\n\n[^fn-cuda-cores]: **CUDA Cores**: NVIDIA's parallel processing units optimized for floating-point operations. Unlike CPU cores (designed for complex sequential tasks), CUDA cores are simpler and work together, enabling a single H100 GPU to perform 16,896 parallel operations simultaneously for massive speedup in matrix computations.\n\n#### Sustainable Computing and Energy Awareness {#sec-efficient-ai-sustainable-computing-energy-awareness-d77a}\n\nAs systems scale further, compute efficiency has become closely tied to sustainability. Training state-of-the-art large language models requires massive computational resources, leading to increased attention on environmental impact. The projected electricity usage of data centers, shown in @fig-datacenter-energy-usage, highlights this concern. Between 2010 and 2030, electricity consumption is expected to rise sharply, particularly under worst-case scenarios where it could exceed 8,000 TWh by 2030 [@jones2018much].\n\n::: {#fig-datacenter-energy-usage fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{axis}[\n  axis line style={draw=none},\n  width=16cm,\n  height=10cm,\n  table/col sep=comma,\n  x tick label style={rotate=0, anchor=north},\n  xmin=2009.5,xmax=2030,\n  ymin=250, ymax=8300,\n  ytick={2000,4000,6000,8000},\n  ylabel={Electricity Usage (TWh)},\n  xlabel={Year},\n   legend style={at={(0.15,0.9)}, anchor=north},\n   legend cell align=left,\n   legend style={fill=BrownL!40,draw=BrownLine,row sep=1.85pt,\n   font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n  grid=both,\n  minor tick num=1,\n  major grid style={black!80},\n  minor grid style={black!40},\n    /pgf/number format/.cd,\n  1000 sep={},\n  nodes near coords align=right,\n        tick label style={/pgf/number format/assume math mode=true},\n        ticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\n    cycle multi list={\n     red,blue,green\\nextlist\n     solid\\nextlist\n     mark=o,mark=none,mark=triangle,mark=none,mark=,mark=none\n     },\n]\n\\addplot+[mark=*,line width=2pt,\nred] table[x=Date,y=Y, col sep=comma] {\nY,Date\n500, 2010\n510, 2012\n520, 2014\n540, 2016\n560, 2018\n580, 2020\n600, 2022\n630, 2024\n660, 2026\n690, 2028\n700, 2030\n};\n\\addplot+[mark=triangle*, mark size=3pt,cyan!90!black,\nline width=2pt] table[x=Date,y=Y, col sep=comma] {\nY,Date\n500, 2010\n550, 2012\n600, 2014\n680, 2016\n760, 2018\n860, 2020\n1000, 2022\n1200, 2024\n1600, 2026\n2000, 2028\n2967, 2030\n};\n\\addplot+[mark=square*,line width=2pt, mark size=2.5pt,\ngreen!70!black] table[x=Date,y=Y, col sep=comma] {\nY,Date\n500, 2010\n600, 2012\n750, 2014\n1000, 2016\n1250, 2018\n1600, 2020\n2200, 2022\n3000, 2024\n4500, 2026\n6000, 2028\n7933, 2030\n};\n \\legend{Best, Expected, Worst}\n\\coordinate (legend) at (axis description cs:0.15,0.92);\n\\end{axis}\n\\node[fill=white,above=2pt of legend,anchor=center]{\\small\\bfseries Scenario};\n\\end{tikzpicture}\n```\n: **Data Center Energy Projections**: Between 2010 and 2030, data center electricity usage is projected to increase sharply, particularly under worst-case scenarios where consumption could exceed 8,000 TWh by 2030 [@jones2018much]. This projection underscores the critical need for improved energy efficiency in AI systems.\n:::\n\nThis dramatic growth underscores urgency for compute efficiency, as even large data centers face energy constraints due to limitations in electrical grid capacity. Efficiency improvements alone may not guarantee environmental benefits due to a phenomenon known as Jevons Paradox.\n\nConsider the invention of the fuel-efficient car. While each car uses less gas per mile, the lower cost of driving encourages people to drive more often and live further from work. The result can be an *increase* in total gasoline consumption. This is Jevons Paradox: efficiency gains can be offset by increased consumption. In AI, this means making models 10x more efficient might lead to a 100x increase in their use, resulting in a net negative environmental impact if not managed carefully.\n\nAddressing these challenges requires optimizing hardware utilization and minimizing energy consumption in both cloud and edge contexts while being mindful of potential rebound effects from increased deployment.\n\nKey trends include adoption of energy-aware scheduling and resource allocation techniques that distribute workloads efficiently across available hardware [@Patterson_et_al_2021]. Researchers are also developing methods to dynamically adjust precision levels during training and inference, using lower precision operations (e.g., mixed-precision training) to reduce power consumption without sacrificing accuracy.\n\nDistributed systems achieve compute efficiency by splitting workloads across multiple machines. Techniques such as model parallelism[^fn-efficient-model-parallelism] and data parallelism[^fn-efficient-data-parallelism] allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput while minimizing idle time.\n\n[^fn-efficient-model-parallelism]: **Model Parallelism**: Distributing model components across multiple processors due to memory constraints. GPT-3 (175B parameters) requires 350GB memory, exceeding A100's 40GB capacity by 9×, necessitating tensor parallelism where each transformer layer splits across 8-16 GPUs with all-gather communication for activation synchronization.\n\n[^fn-efficient-data-parallelism]: **Data Parallelism**: Training method where the same model runs on multiple processors with different data batches. GPT-3 training used data parallelism across thousands of GPUs, processing multiple text sequences simultaneously.\n\nAt the edge, compute efficiency addresses growing demand for real-time processing in energy-constrained environments. Innovations such as hardware-aware model optimization, lightweight inference engines, and adaptive computing architectures enable highly efficient edge systems critical for applications like autonomous vehicles and smart home devices.\n\n#### Production Deployment Patterns {#sec-efficient-ai-production-deployment-patterns-208a}\n\nReal-world efficiency optimization demonstrates practical impact across deployment contexts. Production systems routinely achieve 5-10x efficiency gains through coordinated application of optimization techniques while maintaining 95%+ of original model performance.\n\nMobile applications achieve 4-7x model size reduction and 3-5x latency improvements through combined quantization, pruning, and distillation, enabling real-time inference on mid-range devices. Modern mobile AI systems distribute workloads across specialized processors (NPU for ultra-low power inference, GPU for parallel compute, CPU for control logic) based on power, performance, and real-time constraints.\n\nAutonomous vehicle systems optimize for safety-critical <10ms latency requirements through hardware-aware architectural design and mixed-precision quantization, processing multiple high-bandwidth sensor streams within strict power and thermal constraints.\n\nCloud serving infrastructure reduces costs by 70-80% through systematic optimization combining dynamic batching, quantization, and knowledge distillation, serving 4-5x more requests at comparable quality levels.\n\nEdge IoT deployments achieve month-long battery life through extreme model compression and duty-cycle optimization, operating on milliwatt power budgets while maintaining acceptable accuracy for practical applications.\n\nThese efficiency gains emerge from systematic optimization strategies that coordinate multiple techniques rather than applying individual optimizations in isolation. The specific optimization sequences, technique combinations, and engineering practices that enable these production results are detailed in @sec-model-optimizations.\n\nCompute efficiency complements algorithmic and data efficiency. Compact models reduce computational requirements, while efficient data pipelines streamline hardware usage. The evolution of compute efficiency (from early reliance on CPUs through specialized accelerators to sustainable computing practices) remains central to building scalable, accessible, and environmentally responsible machine learning systems.\n\n### Data Efficiency {#sec-efficient-ai-data-efficiency-a3ad}\n\nData efficiency focuses on optimizing the amount and quality of data required to train machine learning models effectively. Data efficiency has emerged as a pivotal dimension, driven by rising costs of data collection, storage, and processing, as well as the limits of available high-quality data.\n\n#### Maximizing Learning from Limited Data {#sec-efficient-ai-maximizing-learning-limited-data-2885}\n\nIn early machine learning, data efficiency was not a primary focus, as datasets were relatively small and manageable. The challenge was often acquiring enough labeled data to train models effectively. Researchers relied on curated datasets such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], using feature selection and dimensionality reduction techniques like principal component analysis (PCA)[^fn-pca] to extract maximum value from limited data.\n\n[^fn-uci]: **UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine, one of the most widely-used resources for machine learning datasets. Contains over 600 datasets and has been cited in thousands of research papers.\n\n[^fn-pca]: **Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearson in 1901, identifies the most important directions of variation in data. Reduces computational complexity while preserving 90%+ of data variance in many applications.\n\nThe advent of deep learning in the 2010s transformed data's role. Models like AlexNet and GPT-3 demonstrated that larger datasets often led to better performance, marking the beginning of the \"big data\" era. However, this reliance introduced inefficiencies. Data collection became costly and time-consuming, requiring vast amounts of labeled data for supervised learning.\n\nResearchers developed techniques enhancing data efficiency even as datasets grew. Transfer learning[^fn-efficient-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing task-specific data needs [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] artificially expanded datasets by creating new variations of existing samples. Active learning[^fn-active-learning] prioritized labeling only the most informative data points [@Settles_2009].\n\n[^fn-efficient-transfer-learning]: **Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for specific tasks. ImageNet pre-trained models can achieve high accuracy on new vision tasks with <1000 labeled examples vs. millions needed from scratch.\n\n[^fn-data-augmentation]: **Data Augmentation**: Artificially expanding datasets through transformations like rotations, crops, or noise. Can improve model performance by 5-15% and reduce overfitting, especially when labeled data is scarce.\n\n[^fn-active-learning]: **Active Learning**: Iteratively selecting the most informative samples for labeling to maximize learning efficiency. Can achieve target performance with 50-90% less labeled data compared to random sampling.\n\nAs systems continue growing in scale, inefficiencies of large datasets have become apparent. Data-centric AI[^fn-data-centric-ai] has emerged as a key paradigm, emphasizing data quality over quantity. This approach focuses on enhancing preprocessing, removing redundancy, and improving labeling efficiency. Research shows that careful curation and filtering can achieve comparable or superior performance while using only a fraction of original data volume [@penedo2024fineweb].\n\n[^fn-data-centric-ai]: **Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by Andrew Ng in 2021. Focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains.\n\nSeveral techniques support this transition. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures training to progress from simple to complex examples, improving learning efficiency.\n\n[^fn-self-supervised]: **Self-Supervised Learning**: Training method where models create their own labels from input data structure, like predicting masked words in BERT. Enables learning from billions of unlabeled examples.\n\n[^fn-curriculum-learning]: **Curriculum Learning**: Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education. Can improve convergence speed by 25-50% and final model performance.\n\nData efficiency is particularly important in foundation models[^fn-efficient-foundation-models]. As these models grow in scale and capability, they approach limits of available high-quality training data, especially for language tasks, as shown in @fig-running-out-of-human-data. This scarcity drives innovation in data processing and curation techniques.\n\n[^fn-efficient-foundation-models]: **Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks. Term coined by Stanford HAI in 2021, includes models like GPT-3, BERT, and DALL-E.\n\n![**Dataset Growth**: Foundation models are increasingly trained on vast datasets, reflecting the growing stock of human-generated text. This trend underscores the challenge of data scarcity in maintaining model performance as scale increases. Source: @villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024.](images/png/running_out_of_data.png){#fig-running-out-of-human-data}\n\nEvidence for data quality's impact appears across different deployment scales. In TinyML[^fn-tinyml] applications, datasets like Wake Vision demonstrate how performance critically depends on careful data curation [@banbury2024wakevisiontailoreddataset]. At larger scales, research on language models trained on web-scale datasets shows that intelligent filtering and selection strategies significantly improve performance on downstream tasks [@penedo2024fineweb]. @sec-benchmarking-ai establishes rigorous methodologies for measuring these data quality improvements.\n\n[^fn-tinyml]: **TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW power consumption. Enables AI in IoT devices, wearables, and sensors where traditional ML deployment is impossible.\n\nThis modern era of data efficiency represents a shift in how systems approach data utilization. By focusing on quality over quantity and developing sophisticated techniques for data selection and processing, the field is moving toward more sustainable and effective approaches to model training and deployment. Data efficiency is integral to scalable systems, impacting both model and compute efficiency. Smaller, higher-quality datasets reduce training times and computational demands while enabling better generalization. These principles also complement privacy-preserving techniques, where minimizing data requirements enhances both efficiency and user privacy protection.\n\n## Real-World Efficiency Strategies {#sec-efficient-ai-realworld-efficiency-strategies-8387}\n\nHaving explored each efficiency dimension individually and their interconnections, we examine how these dimensions manifest across different deployment contexts. The efficiency of machine learning systems emerges from understanding relationships between algorithmic, compute, and data efficiency in specific operational environments.\n\n### Context-Specific Efficiency Requirements {#sec-efficient-ai-contextspecific-efficiency-requirements-47e6}\n\nThe specific priorities and trade-offs vary dramatically across deployment environments. As our opening examples illustrated, these range from cloud systems with abundant resources to edge devices with severe memory and power constraints. @tbl-deployment-efficiency-priorities maps how these constraints translate into efficiency optimization priorities.\n\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **Deployment Context** | **Primary Constraints**              | **Efficiency Priorities**                       | **Representative Applications**                                     |\n+:=======================+:=====================================+:================================================+:====================================================================+\n| **Cloud**              | Cost at scale, energy consumption    | Throughput, scalability, operational efficiency | Large language model APIs, recommendation engines, video processing |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **Edge**               | Latency, local compute capacity,     | Real-time performance, power efficiency         | Autonomous vehicles, industrial automation, smart cameras           |\n|                        | connectivity                         |                                                 |                                                                     |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **Mobile**             | Battery life, memory, thermal limits | Energy efficiency, model size, responsiveness   | Voice assistants, photo enhancement, augmented reality              |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n| **TinyML**             | Extreme power/memory constraints     | Ultra-low power, minimal model size             | IoT sensors, wearables, environmental monitoring                    |\n+------------------------+--------------------------------------+-------------------------------------------------+---------------------------------------------------------------------+\n\n: **Efficiency Optimization Priorities by Deployment Context**: Each environment demands different trade-offs between algorithmic, compute, and data optimization strategies based on unique constraints. Cloud systems prioritize scalability, edge deployments focus on real-time performance, mobile applications balance performance with battery life, and TinyML demands extreme resource efficiency. {#tbl-deployment-efficiency-priorities}\n\nUnderstanding these context-specific patterns enables designers to make informed decisions about which efficiency dimensions to prioritize and how to navigate inevitable trade-offs.\n\n### Scalability and Sustainability {#sec-efficient-ai-scalability-sustainability-4d30}\n\nSystem efficiency serves as a driver of environmental sustainability. When systems are optimized for efficiency, they can be deployed at scale while minimizing environmental footprint. This relationship creates a positive feedback loop, as shown in @fig-virtuous-efficiency-cycle.\n\n::: {#fig-virtuous-efficiency-cycle fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.7}{%\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\def\\ra{40mm}\n\\draw (90: 0.5*\\ra) node[yshift=-2pt](EF){Efficiency};\n\\draw (210: 0.5*\\ra) node(SC){Scalability};\n\\draw (330: 0.5*\\ra) node(SU){Sustainability};\n\\node[right=of EF]{};\n\n\\draw[-{Triangle[width=18pt,length=8pt]}, line width=10pt,violet!60] (340:0.5*\\ra)\narc[radius=0.5*\\ra, start angle=-20, end angle= 67];\n\n\\draw[-{Triangle[width=18pt,length=8pt]}, line width=10pt,cyan!80!black!90] (113:0.5*\\ra)\narc[radius=0.5*\\ra, start angle=113, end angle= 200];\n\n\\draw[-{Triangle[width=18pt,length=8pt]}, line width=10pt,orange!70] (220:0.5*\\ra)\narc[radius=0.5*\\ra, start angle=220, end angle= 320];\n\\end{tikzpicture}}\n```\n: **Efficiency and Sustainability Feedback Loop**: Optimized machine learning systems achieve greater scalability, which in turn incentivizes sustainable design practices and further efficiency improvements, creating a reinforcing feedback loop for long-term impact.\n:::\n\nEfficient systems are inherently scalable. Reducing resource demands through lightweight models, targeted datasets, and optimized compute utilization allows systems to deploy broadly. When efficient systems scale, they amplify their contribution to sustainability by reducing overall energy consumption and computational waste. Sustainability reinforces the need for efficiency, creating a feedback loop that strengthens the entire system.\n\n## Efficiency Trade-offs and Challenges {#sec-efficient-ai-efficiency-tradeoffs-challenges-946d}\n\nThe three efficiency dimensions can work synergistically under favorable conditions, but real-world systems often face scenarios where improving one dimension degrades another. The same resource constraints that make efficiency necessary force difficult choices: reducing model size may sacrifice accuracy, optimizing for real-time performance may increase energy consumption, and curating smaller datasets may limit generalization.\n\n### Fundamental Sources of Efficiency Trade-offs {#sec-efficient-ai-fundamental-sources-efficiency-tradeoffs-d16f}\n\nThese tensions manifest in various ways across machine learning systems. Understanding their root causes is essential for addressing design challenges. Each efficiency dimension influences the others, creating a dynamic interplay that shapes system performance.\n\n#### Algorithmic Efficiency vs. Compute Requirements {#sec-efficient-ai-algorithmic-efficiency-vs-compute-requirements-83a7}\n\nAlgorithmic efficiency focuses on designing compact models that minimize computational and memory demands. By reducing model size or complexity, deployment on resource-limited devices becomes feasible. Overly simplifying a model can reduce accuracy, especially for complex tasks. To compensate for this loss, additional computational resources may be required during training or deployment, placing strain on compute efficiency.\n\n#### Compute Efficiency vs. Real-Time Needs {#sec-efficient-ai-compute-efficiency-vs-realtime-needs-a269}\n\nCompute efficiency aims to minimize resources required for training and inference, reducing energy consumption, processing time, and memory use. In scenarios requiring real-time responsiveness (autonomous vehicles, augmented reality), compute efficiency becomes harder to maintain. @fig-efficiency-vs-latency illustrates this challenge: real-time systems often require high-performance hardware to process data instantly, conflicting with energy efficiency goals or increasing system costs.\n\n::: {#fig-efficiency-vs-latency fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{%\n    Line/.style={line width=1.0pt,violet!50,text=black},\n    stop/.style = {regular polygon, regular polygon sides=8,\n      draw=red, double, double distance=1.0mm,  thick,\n      fill=red, font=\\usefont{T1}{phv}{m}{n}\\Huge\\bfseries, text=white,\n      inner sep=0mm},\n    warning/.style = {regular polygon, regular polygon sides=3,line width=1.5pt,\n      draw=red,\n      fill=white, font=\\Huge\\bfseries, text=black,\n      inner ysep=3pt, node contents={!}},\n pics/car/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n            \\begin{scope}[shift={(0,0)},rotate=0,scale=\\scalefac,, every node/.append style={transform shape}]\n                %\n                \\draw[\\drawchannelcolor,line width=\\Linewidth,x=1mm,y=1mm,yscale=-1,xscale=-1,fill=\\channelcolor!50] (59.9429,0.0029) .. controls\n                (58.2798,0.0161) and (56.5224,0.0709) .. (54.6592,0.1699) .. controls (51.8698,0.3182) and (49.2785,0.7036) ..\n                (46.8955,1.2407) .. controls (46.9004,1.2391) and (46.9067,1.2365) .. (46.9116,1.2349) .. controls\n                (35.0588,3.3135) and (25.0020,10.1030) .. (25.0020,10.1030) -- (24.1113,10.1660) .. controls\n                (22.2803,10.1061) and (21.6259,10.2123) .. (17.5122,11.0391) .. controls (15.2265,11.1391) and\n                (13.1653,11.4703) .. (11.3730,11.9180) .. controls (11.2904,11.9383) and (11.2097,11.9609) ..\n                (11.1284,11.9824) .. controls (8.6666,12.6223) and (6.7447,13.4848) .. (5.6074,14.3101) ..\n                controls (2.5699,14.9763) and (0.3984,16.7520) .. (0.3984,16.7520) .. controls (-0.1586,17.2949) and\n                (0.0797,17.2023) .. (0.0044,17.6191) .. controls (-0.0709,18.0360) and (0.7119,21.0322) .. (0.7119,21.0322) ..\n                controls (0.7119,21.0322) and (0.0821,22.9131) .. (0.5215,23.0918) .. controls (0.9609,23.2703) and (1.0903,23.4957) ..\n                (1.4604,24.4233) .. controls (-0.8220,25.6494) and (0.4983,26.3315) .. (1.5059,26.9150) .. controls\n                (2.5136,27.4983) and (5.1650,28.1973) .. (6.5098,27.9229) .. controls (6.4949,27.8726) and\n                (6.4886,27.8209) .. (6.4746,27.7705) -- (8.3862,26.9062) -- (23.4346,26.2646) -- (25.2979,27.3164) ..\n                controls (25.3045,27.3313) and (25.3242,27.3955) .. (25.3242,27.3955) .. controls (25.3242,27.3955)\n                and (25.5918,27.6023) .. (26.2236,27.4849) .. controls (27.8013,27.0856) and (67.5264,26.7188) ..\n                (67.5264,26.7188) .. controls (67.5264,26.7188) and (71.0655,26.7059) .. (72.3955,27.2095) ..\n                controls (72.9263,27.4105) and (73.2239,27.3453) .. (73.4019,27.1245) .. controls (73.7709,27.0085)\n                and (75.1701,26.5817) .. (75.4629,26.5400) .. controls (75.7840,26.4940) and (90.4210,25.8970) ..\n                (90.3750,25.8970) .. controls (90.3293,25.8970) and (92.2559,26.6777) .. (92.2559,26.6777) ..\n                controls (92.2559,26.6777) and (92.3225,26.6082) .. (92.3320,26.5986) .. controls (92.5830,26.6361)\n                and (92.9367,26.6106) .. (93.4336,26.4961) .. controls (95.4068,26.0414) and (96.8291,25.3066) ..\n                (96.8291,25.3066) .. controls (96.8291,25.3066) and (98.1069,23.5919) .. (98.3862,22.9688) ..\n                controls (98.6655,22.3454) and (98.4976,22.1118) .. (98.4976,22.1118) .. controls (98.4976,22.1118)\n                and (98.8375,20.8511) .. (99.2549,19.8252) .. controls (99.6719,18.8000) and (99.6148,18.6385) ..\n                (98.9854,18.0322) .. controls (98.2215,17.0284) and (97.8547,14.8710) .. (98.0010,13.9409) ..\n                controls (98.0616,13.5558) and (98.0431,13.1384) .. (98.0083,12.7661) .. controls (98.0515,11.7298)\n                and (97.7331,10.8516) .. (97.4692,10.3418) .. controls (97.3419,9.9538) and (97.2028,9.5918) ..\n                (97.0620,9.4497) .. controls (96.6727,9.0568) and (97.2353,8.9554) .. (97.7930,8.6543) ..\n                controls (98.3509,8.3530) and (97.8727,8.0535) .. (97.5088,8.0420) .. controls (97.1451,8.0305)\n                and (96.4688,7.9805) .. (96.4688,7.9805) .. controls (95.4388,7.9064) and (92.8843,6.7387) ..\n                (85.3447,4.1309) -- (85.3271,4.1133) .. controls (85.3259,4.1146) and (85.3240,4.1207) ..\n                (85.3228,4.1221) .. controls (85.3044,4.1157) and (85.2943,4.1123) .. (85.2759,4.1060) .. controls\n                (78.6238,1.8073) and (71.5847,-0.0896) .. (59.9429,0.0029) -- cycle;\n%\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!50!gray,line width=2*\\Linewidth] (-2,-2.55) coordinate (w1)  circle (0.8);\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!99!gray,line width=2*\\Linewidth] (-2,-2.55) coordinate (w1)  circle (0.25);\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!50!gray,line width=2*\\Linewidth] (-8,-2.55) coordinate (w2)  circle (0.8);\n                \\draw [\\drawchannelcolor,fill=\\channelcolor!99!gray,line width=2*\\Linewidth] (-8,-2.55) coordinate (w2)  circle (0.25);\n                \\draw[\\drawchannelcolor,fill=\\channelcolor!20,line width=\\Linewidth] (-9.8,-0.85) -- (-2.52,-0.99)\n                 to[out=150,in=345](-4.5,-0.16) to[out=170,in=7](-7.5,-0.12)to[out=190,in=17](-9.8,-0.85);\n                \\draw[\\drawchannelcolor,line width=2*\\Linewidth](-5,-0.96)--(-5,-0.1);\n                \\draw[\\drawchannelcolor,line width=2*\\Linewidth](-8,-0.9)--(-8,-0.22);\n            \\end{scope}\n   }\n  }\n}\n\n\\tikzset{%\n pics/danger/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=DANGER1,shift={(0,0)},rotate=0,scale=\\scalefac,every node/.append style={transform shape}]\n\\node[red]{$\\triangle$};\n\\node[yshift=0.125ex, scale=0.5]{!};\n\\end{scope}\n   }\n  }\n}\n\n\\pgfkeys{\n  /channel/.cd,\n  channelcolor/.store in=\\channelcolor,\n  drawchannelcolor/.store in=\\drawchannelcolor,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  channelcolor=BrownLine,\n  drawchannelcolor=BrownLine,\n  scalefac=1,\n  Linewidth=1.6pt,\n  picname=C\n}\n% #1 number of teeth\n% #2 radius intern\n% #3 radius extern\n% #4 angle from start to end of the first arc\n% #5 angle to decale the second arc from the first\n% #6 inner radius to cut off\n\\newcommand{\\gear}[6]{%\n  (0:#2)\n  \\foreach \\i [evaluate=\\i as \\n using {\\i-1)*360/#1}] in {1,...,#1}{%\n    arc (\\n:\\n+#4:#2) {[rounded corners=1.5pt] -- (\\n+#4+#5:#3)\n    arc (\\n+#4+#5:\\n+360/#1-#5:#3)} --  (\\n+360/#1:#2)\n  }%\n  (0,0) circle[radius=#6];\n}\n\\begin{scope}[local bounding box=CAR1,shift={($(0,0)+(1,1.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){car={scalefac=0.5,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine,drawchannelcolor=black!70}};\n \\end{scope}\n  \\node[below=of CAR1]{120 km/h};\n\n\\begin{scope}[local bounding box=WAY1,shift={($(CAR1)+(0.5,-0.7)$)},scale=1, every node/.append style={transform shape}]\n\\draw[draw=none,fill=brown!60](1.4,0)--(1.9,-0.35)to[out=330,in=30](1.5,-0.8)to[out=210,in=150,distance=9](1.5,-1.3)--(2.3,-1.9)\nto[out=330,in=30,distance=9](2.25,-2.37)--(-0.8,-3.75)--(1.6,-3.75)to[out=10,in=250,distance=5] (3.45,-2.5)\nto[out=55,in=310,distance=9](3.4,-1.9)to[out=140,in=340](2.32,-1.3)to[out=160,in=220](1.92,-0.8)\nto[out=35,in=330](2.1,-0.35)--(1.4,0);\n\\draw[white,line width=1.5pt](2.0,-0.35)to[out=330,in=30](1.72,-0.8)\nto[out=210,in=150,distance=9](1.9,-1.3)--(2.9,-1.9)to[out=330,in=30,distance=9](2.7,-2.5)--(0.45,-3.75);\n \\end{scope}\n  \\scoped[on background layer]\n\\node[draw=BackLine,inner xsep=2mm,inner ysep=5mm,minimum height=64mm,\nyshift=2.5mm,fill=BackColor!30,fit=(CAR1)(WAY1),line width=1pt](BB2){};\n\\node[below=4pt of  BB2.north,inner sep=0pt,anchor=north]{\\textbf{Driving Simulator}};\n%gears\n\\begin{scope}[local bounding box=GEAR,shift={($(BB2)+(8.5,0.55)$)},\nscale=4.5,every node/.append style={transform shape}]\n\\colorlet{black}{brown!50!black}\n\\fill[draw=none,fill=black,even odd rule,xshift=-2mm]coordinate(GE1)\\gear{10}{0.23}{0.28}{10}{2}{0.1};\n\\fill[draw=none,fill=black,even odd rule,xshift=1.9mm,yshift=-3.0mm]coordinate(GE2)\\gear{10}{0.18}{0.22}{10}{2}{0.08};\n\\end{scope}\n   \\scoped[on background layer]\n\\node[draw=BrownLine,inner xsep=8,inner ysep=8,yshift=1.5mm,\nminimum width=55mm,minimum height=64mm,\n           fill=BrownL!50,fit=(GE1)(GE2),line width=1.0pt](BB1){};\n\\node[above=6pt of BB1.south,align=center]{Latency = 100 ms};\n\\node[below=2pt of BB1.north,align=center]{\\textbf{XYZ Simulator}\\\\ (e.g. GNSS)};\n%%\n\\node[below=5pt of BB2.south,align=center](DS){120 km/h = 3.33 m/s\\\\ 1 s: 33.33 m\\\\\n\\textbf{1 ms: 0.033 m}};\n\\node[below=5pt of BB1.south,align=center](DS1){\\vphantom{120 km/h = 3.33 m/s}\\\\\n\\vphantom{1 s: 33.33 m}\\\\ \\textbf{100 ms: 3.33 m}};\n\\draw[Line,-latex]([yshift=2mm]DS.south east)--([yshift=2mm]DS1.south west);\n\\draw[Line,-latex](BB2)--node[above]{Vehicle}node[below]{Dynamics}(BB1);\n\\draw[Line,latex-](BB2.west)--++(-1,0)--++(0,-5)--node[below=2mm,scale=0.23,stop,pos=0.44](STOP){\\textbf{STOP}}++(16.5,0)|-\n(BB1.east);\n\n\\begin{scope}[local bounding box=DANGER,shift={($(BB2.north)+(0,0.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){danger={scalefac=4.0,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine,drawchannelcolor=black!70}}node[left=6mm,red]{\\large Obstacle ahead};\n \\end{scope}\n\\node[anchor=west,red] at ($(STOP.east) + (0.2,0)$) {Avoid Collision};\n%%\n\\begin{scope}[local bounding box=CAR2,shift={($(BB1.east)+(6,1.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){car={scalefac=0.5,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine,drawchannelcolor=black!70}};\n \\end{scope}\n \\begin{scope}[local bounding box=CAR3,shift={($(CAR2.south)+(4,-3.5)$)},scale=0.8, every node/.append style={transform shape}]\n\\pic[shift={(0,0)}] at  (0,0){car={scalefac=0.5,picname=1,Linewidth=0.7pt,\n channelcolor=BlueLine!30!,drawchannelcolor=black!10}};\n \\end{scope}\n \\node[above=18pt of CAR2,align=center]{Where were you in\\\\ a real life scenario?};\n \\draw[Line,latex-latex](CAR2)--node[left]{Uncertainty = 3.33 m}(CAR3);\n \\node[above=13pt of $(BB2.north east)!0.5!(BB1.north west)$]{\\large Why latency matters?};\n\\end{tikzpicture}\n```\n: **Real-Time System Constraints**: Autonomous vehicles demand careful balance between computational efficiency and low latency. Increasing processing power to reduce delay can conflict with energy and cost limitations, yet sacrificing latency compromises safety by increasing reaction time and braking distance.\n:::\n\n#### Data Efficiency vs. Model Generalization {#sec-efficient-ai-data-efficiency-vs-model-generalization-044a}\n\nData efficiency seeks to minimize the amount of data required to train a model without sacrificing performance. By curating smaller, high-quality datasets, training becomes faster and less resource-intensive. Ideally, this reinforces both algorithmic and compute efficiency. However, reducing dataset size can limit diversity, making it harder for models to generalize to unseen scenarios. To address this, additional compute resources or model complexity may be required, creating tension between data efficiency and broader system goals.\n\n### Recurring Trade-off Patterns in Practice {#sec-efficient-ai-recurring-tradeoff-patterns-practice-c205}\n\nThe trade-offs between efficiency dimensions become particularly evident when examining specific scenarios. Complex models with millions or billions of parameters can achieve higher accuracy by capturing intricate patterns, but require significant computational power and memory. A recommendation system in a cloud data center might use a highly complex model for better recommendations, but at the cost of higher energy consumption and operating costs. On resource-constrained devices like smartphones or autonomous vehicles, compact models may operate efficiently but require more sophisticated data preprocessing or training procedures to compensate for reduced capacity.\n\nEnergy efficiency and real-time performance often pull systems in opposite directions. Real-time systems like autonomous vehicles or augmented reality applications rely on high-performance hardware to process large volumes of data quickly, but this typically increases energy consumption. An autonomous vehicle must process sensor data from cameras, LiDAR, and radar in real time to make navigation decisions, requiring specialized accelerators that consume significant energy. In edge deployments with battery power or limited energy sources, this trade-off becomes even more critical.\n\nLarger datasets generally provide greater diversity and coverage, enabling models to capture subtle patterns and reduce overfitting risk. However, computational and memory demands of training on large datasets can be substantial. In resource-constrained environments like TinyML deployments, an IoT device monitoring environmental conditions might need a model that generalizes well across varying conditions, but collecting extensive datasets may be impractical due to storage and computational limitations. Smaller, carefully curated datasets or synthetic data may be used to reduce computational strain, but this risks missing key edge cases.\n\nThese trade-offs are not merely academic concerns but practical realities that shape system design decisions across all deployment contexts.\n\n## Strategic Trade-off Management {#sec-efficient-ai-strategic-tradeoff-management-0ac8}\n\nThe trade-offs inherent in machine learning system design require thoughtful strategies to navigate effectively. Achieving the right balance involves difficult decisions heavily influenced by specific goals and constraints of the deployment environment. Designers can adopt a range of strategies that address unique requirements of different contexts.\n\n### Environment-Driven Efficiency Priorities {#sec-efficient-ai-environmentdriven-efficiency-priorities-4057}\n\nEfficiency goals are rarely universal. The specific demands of an application or deployment scenario heavily influence which dimension—algorithmic, compute, or data—takes precedence. Prioritizing the right dimensions based on context is the first step in effectively managing trade-offs.\n\nIn Mobile ML deployments, battery life is often the primary constraint, placing a premium on compute efficiency. Energy consumption must be minimized to preserve operational time, so lightweight models are prioritized even if it means sacrificing some accuracy or requiring additional data preprocessing.\n\nIn Cloud ML systems, scalability and throughput are paramount. These systems must process large volumes of data and serve millions of users simultaneously. While compute resources are more abundant, energy efficiency and operational costs remain important. Algorithmic efficiency plays a critical role in ensuring systems can scale without overwhelming infrastructure.\n\nEdge ML systems present different priorities. Autonomous vehicles or real-time monitoring systems require low-latency processing for safe and reliable operation, making real-time performance and compute efficiency paramount, often at the expense of energy consumption. However, hardware constraints mean these systems must still carefully manage energy and computational resources.\n\n**TinyML** deployments demand extreme efficiency due to severe hardware and energy limitations. Algorithmic and data efficiency are top priorities, with models highly compact and capable of operating on microcontrollers with minimal memory and compute power, while training relies on small, carefully curated datasets.\n\n### Dynamic Resource Allocation at Inference {#sec-efficient-ai-dynamic-resource-allocation-inference-d6bc}\n\nSystem adaptability can be enhanced through dynamic resource allocation during inference. This approach recognizes that resource needs may fluctuate even within specific deployment contexts. By adjusting computational effort at inference time, systems can fine-tune performance to meet immediate demands.\n\nFor example, a cloud-based video analysis system might process standard streams with a streamlined model to maintain high throughput, but when a critical event is detected, dynamically allocate more resources to a complex model for higher precision. Similarly, mobile voice assistants might use lightweight models for routine commands to conserve battery, but temporarily activate resource-intensive models for complex queries.\n\nImplementing test-time compute introduces new challenges. Dynamic resource allocation requires sophisticated monitoring and control mechanisms. There are diminishing returns—increasing compute beyond certain thresholds may not yield significant performance improvements. The ability to dynamically increase compute can also create disparities in access to high-performance AI, raising equity concerns. Despite these challenges, test-time compute offers a valuable strategy for enhancing system adaptability.\n\n### End-to-End Co-Design and Automated Optimization {#sec-efficient-ai-endtoend-codesign-automated-optimization-1220}\n\nEfficient machine learning systems are rarely the product of isolated optimizations. Achieving balance across efficiency dimensions requires an end-to-end co-design perspective, where each system component is designed in tandem with others. This holistic approach aligns model architectures, hardware platforms, and data pipelines to work seamlessly together.\n\nCo-design becomes essential in resource-constrained environments. Models must align precisely with hardware capabilities—8-bit models require hardware support for efficient integer operations, while pruned models benefit from sparse tensor operations. Edge accelerators often optimize specific operations like convolutions, influencing model architecture choices. Detailed hardware architecture considerations are covered comprehensively in @sec-ai-acceleration.\n\n**Automation and optimization tools** help manage the complexity of navigating trade-offs. Automated machine learning (AutoML)[^fn-automl] enables exploration of different model architectures and hyperparameter configurations. Building on the systematic approach to ML workflows introduced in @sec-ai-workflow, AutoML tools automate many efficiency optimization decisions that traditionally required extensive manual tuning.\n\n[^fn-automl]: **AutoML**: Automated machine learning that systematically searches through model architectures, hyperparameters, and data preprocessing options. Google's AutoML achieved 84.3% ImageNet accuracy vs. human experts' 78.5%, while reducing development time from months to hours.\n\nNeural architecture search (NAS)[^fn-nas] takes automation further by designing model architectures tailored to specific hardware or deployment scenarios, evaluating a wide range of architectural possibilities to maximize performance while minimizing computational demands.\n\n[^fn-nas]: **Neural Architecture Search (NAS)**: Automated method for discovering optimal neural network architectures. EfficientNet-B7, discovered via NAS, achieved 84.3% ImageNet accuracy with 37M parameters vs. hand-designed ResNeXt-101's 80.9% with 84M parameters.\n\nData efficiency also benefits from automation. Tools that automate dataset curation, augmentation, and active learning reduce training dataset size without sacrificing performance, prioritizing high-value data points to speed up training and reduce computational overhead [@settles2009active]. @sec-ai-frameworks explores how modern ML frameworks incorporate these automation capabilities.\n\n### Measuring and Monitoring Efficiency Trade-offs {#sec-efficient-ai-measuring-monitoring-efficiency-tradeoffs-fd5b}\n\nBeyond technical automation lies the broader challenge of systematic evaluation. Efficiency optimization necessitates a structured approach assessing trade-offs that extends beyond purely technical considerations. As systems transition from research to production, success criteria must encompass algorithmic performance, economic viability, and operational sustainability.\n\nCosts associated with efficiency improvements manifest across engineering effort (research, experimentation, integration), balanced against ongoing operational expenses of running less efficient systems. Benefits span multiple domains—beyond direct cost reductions, efficient systems often enable qualitatively new capabilities like real-time processing in resource-constrained environments or deployment to edge devices.\n\nThis evaluation framework must be complemented by ongoing assessment mechanisms. The dynamic nature of ML systems in production necessitates continuous monitoring of efficiency characteristics. As models evolve, data distributions shift, and infrastructure changes, efficiency properties can degrade. Real-time monitoring enables rapid detection of efficiency regressions, while historical analysis provides insight into longer-term trends, revealing whether efficiency improvements are sustainable under changing conditions.\n\n## Engineering Principles for Efficient AI {#sec-efficient-ai-engineering-principles-efficient-ai-1206}\n\nDesigning an efficient machine learning system requires a holistic approach. True efficiency emerges when the entire system is considered as a whole, ensuring trade-offs are balanced across all stages of the ML pipeline from data collection to deployment. This end-to-end perspective transforms system design.\n\n### Holistic Pipeline Optimization {#sec-efficient-ai-holistic-pipeline-optimization-5bcc}\n\nEfficiency is achieved not through isolated optimizations but by considering the entire pipeline as a unified whole. Each stage—data collection, model training, hardware deployment, and inference—contributes to overall system efficiency. Decisions at one stage ripple through the rest, influencing performance, resource use, and scalability.\n\nData collection and preprocessing are starting points. @sec-data-engineering provides comprehensive coverage of how data pipeline design decisions cascade through the entire system. Curating smaller, high-quality datasets can reduce computational costs during training while simplifying model design. However, insufficient data diversity may affect generalization, necessitating compensatory measures.\n\nModel training is another critical stage. Architecture choice, optimization techniques, and hyperparameters must consider deployment hardware constraints. A model designed for high-performance cloud systems may emphasize accuracy and scalability, while models for edge devices must balance accuracy with size and energy efficiency.\n\nDeployment and inference demand precise hardware alignment. Each platform offers distinct capabilities—GPUs excel at parallel matrix operations, TPUs optimize specific neural network computations, and microcontrollers provide energy-efficient processing. A smartphone speech recognition system might leverage an NPU's dedicated convolution units for millisecond-level inference at low power, while an autonomous vehicle's FPGA processes multiple sensor streams with microsecond-level latency.\n\nAn end-to-end perspective ensures trade-offs are addressed holistically rather than shifting inefficiencies between pipeline stages. This systems thinking approach becomes particularly critical when deploying to resource-constrained environments such as edge devices and embedded systems.\n\n### Lifecycle and Environment Considerations {#sec-efficient-ai-lifecycle-environment-considerations-3abc}\n\nEfficiency needs differ significantly depending on lifecycle stage and deployment environment—from research prototypes to production systems, from high-performance cloud to resource-constrained edge.\n\nIn research, the primary focus is often model performance, with efficiency taking a secondary role. Prototypes are trained using abundant compute resources, enabling exploration of large architectures and extensive hyperparameter tuning. Production systems must prioritize efficiency to operate within practical constraints, often involving significant optimization like model pruning, quantization, or retraining. Production also requires continuous monitoring of efficiency metrics and operational frameworks for managing trade-offs at scale—comprehensive production efficiency management strategies are detailed in @sec-ml-operations.\n\nCloud-based systems handle massive workloads with relatively abundant resources, though energy efficiency and operational costs remain critical. The ML systems design principles covered in @sec-ml-systems provide architectural foundations for building scalable, efficiency-optimized cloud deployments. In contrast, edge and mobile systems operate under strict constraints detailed in our efficiency framework, demanding solutions prioritizing efficiency over raw performance.\n\nSome systems like recommendation engines require frequent retraining to remain effective, depending heavily on data efficiency with actively labeled datasets and sampling strategies. Other systems like embedded models in medical devices require long-term stability with minimal updates. Reliability requirements in critical applications significantly influence efficiency optimization strategies, as robust systems often require additional computational overhead for validation, redundancy, and fail-safe mechanisms.\n\n## Societal and Ethical Implications {#sec-efficient-ai-societal-ethical-implications-d0e5}\n\nWhile efficiency in machine learning is often framed as a technical challenge, it is also deeply tied to broader questions about AI systems' purpose and impact. Designing efficient systems involves navigating not only practical trade-offs but also complex ethical and philosophical considerations that responsible ML practitioners must address.\n\n### Equity and Access {#sec-efficient-ai-equity-access-c38d}\n\nEfficiency has the potential to reduce costs, improve scalability, and expand accessibility. However, resources needed to achieve efficiency—advanced hardware, curated datasets, state-of-the-art optimization techniques—are often concentrated in well-funded organizations, creating inequities in who can leverage efficiency gains.\n\nTraining costs for state-of-the-art models like GPT-4 and Gemini Ultra require tens to hundreds of millions of dollars worth of compute [@perrault2024artificial]. Research by [OECD.AI](https://oecd.ai/en/) indicates that 90% of global AI computing capacity is centralized in only five countries [@oecd_ai_2021]. Academic institutions often lack hardware needed to replicate state-of-the-art results, stifling innovation in underfunded sectors. Energy-efficient compute technologies like accelerators for TinyML or Mobile ML present promising avenues for democratization. By enabling powerful processing on low-cost, low-power devices, these technologies allow organizations without high-end infrastructure access to build impactful systems.\n\nData efficiency is essential where high-quality datasets are scarce, but achieving it is unequally distributed. NLP for low-resource languages suffers from lack of sufficient training data, leading to significant performance gaps. Efforts like the Masakhane project building open-source datasets for African languages show how collaborative initiatives can address this, though scaling globally requires greater investment. Democratizing data efficiency requires more open sharing of pre-trained models and datasets. Initiatives like Hugging Face's open access to transformers or Meta's No Language Left Behind aim to make state-of-the-art NLP models available worldwide, reducing barriers for data-scarce regions.\n\nAlgorithmic efficiency contributes to democratizing ML by enabling advanced capabilities on low-cost, resource-constrained devices. AI-powered diagnostic tools on smartphones are transforming healthcare in remote areas, while low-power TinyML models enable environmental monitoring in regions without reliable electricity.\n\nTechnologies like [TensorFlow Lite](https://ai.google.dev/edge/litert) and [PyTorch Mobile](https://pytorch.org/mobile/home/) allow developers to deploy lightweight models on everyday devices, expanding access in resource-constrained settings. Open-source efforts to share pre-optimized models like MobileNet or EfficientNet play a critical role by allowing under-resourced organizations to deploy state-of-the-art solutions.\n\n### Balancing Innovation with Efficiency Demands {#sec-efficient-ai-balancing-innovation-efficiency-demands-7a44}\n\nThe pursuit of efficiency often brings tension between optimizing for what is known and exploring what is new. Equity concerns are intensified by this tension: resource concentration in well-funded organizations enables expensive exploratory research, while resource-constrained institutions must focus on incremental improvements.\n\nEfficiency often favors established techniques proven to work well. Optimizing neural networks through pruning, quantization, or distillation typically refines existing architectures rather than developing entirely new ones. Consider the shift from traditional ML to deep learning: early neural network research in the 1990s-2000s required significant resources and often failed to outperform simpler methods, yet researchers persisted, eventually leading to breakthroughs defining modern AI.\n\nPioneering research often requires significant resources. Large language models like GPT-4 or PaLM are not inherently efficient—their training consumes enormous compute and energy. Yet these models have opened entirely new possibilities, prompting advancements that eventually lead to more efficient systems like smaller fine-tuned versions.\n\nThis reliance on resource-intensive innovation raises questions about who gets to participate. Well-funded organizations can afford to explore new frontiers, while smaller institutions may be constrained to incremental improvements prioritizing efficiency over novelty.\n\nEfficiency-focused design often requires adhering to strict constraints like reducing model size or latency. While constraints can drive ingenuity, they can also limit exploration scope. However, the drive for efficiency can positively impact innovation—constraints force creative thinking, leading to new methods maximizing performance within tight resource budgets. Techniques like NAS and attention mechanisms arose partly from the need to balance performance and efficiency.\n\nOrganizations and researchers must recognize when to prioritize efficiency and when to embrace experimentation risks. Applied systems for real-world deployment may demand strict efficiency, while exploratory research labs can focus on pushing boundaries. The relationship between innovation and efficiency is not adversarial but complementary—efficient systems create foundations for scalable applications, while resource-intensive experimentation drives breakthroughs redefining what's possible.\n\n### Optimization Limits {#sec-efficient-ai-optimization-limits-20f0}\n\nThe tensions between equity, innovation, and efficiency ultimately stem from a fundamental characteristic of optimization: diminishing returns. Optimization is central to building efficient ML systems, but it is not infinite. As systems become more refined, each additional improvement requires exponentially more effort, time, or resources while delivering increasingly smaller benefits.\n\nThe No Free Lunch (NFL) theorems[^fn-nfl-theorems] for optimization illustrate inherent limitations. According to NFL theorems, no single optimization algorithm can outperform all others across every possible problem, implying optimization technique effectiveness is highly problem-specific [@wolpert1997no].\n\n[^fn-nfl-theorems]: **No Free Lunch (NFL) Theorems**: Mathematical proof by Wolpert and Macready (1997) showing that averaged over all possible optimization problems, every algorithm performs equally well. In ML context, no universal optimization technique exists—methods must be tailored to specific problem domains.\n\nFor example, compressing an ML model can initially reduce memory and compute requirements significantly with minimal accuracy loss. However, as compression progresses, maintaining performance becomes increasingly challenging. Achieving additional gains may necessitate sophisticated techniques like hardware-specific optimizations or extensive retraining, increasing complexity and cost. These costs extend beyond financial investment to include time, expertise, iterative testing, and potential trade-offs in robustness and generalizability.\n\nThe NFL theorems highlight that no universal optimization solution exists, emphasizing need to balance efficiency pursuits with practical considerations. Over-optimization risks wasted resources and reduced adaptability, complicating future updates. Identifying when a system is \"good enough\" ensures resources are allocated effectively.\n\nSimilarly, optimizing datasets for training efficiency may initially save resources, but excessively reducing dataset size risks compromising diversity and weakening generalization. Pushing hardware to performance limits may improve metrics like latency, yet associated reliability concerns and engineering costs can outweigh gains.\n\nUnderstanding optimization limits is essential for creating systems balancing efficiency with practicality and sustainability. This perspective helps avoid over-optimization and ensures resources are invested in areas with meaningful returns.\n\n#### Moore's Law Case Study {#sec-efficient-ai-moores-law-case-study-5767}\n\nOne of the most insightful examples of optimization limits appears in Moore's Law and the economic curve underlying it. While Moore's Law is celebrated as a predictor of exponential computational power growth, its success relied on intricate economic balance. The relationship between integration and cost provides a compelling analogy for diminishing returns in ML optimization.\n\n@fig-moores-law-plot shows relative manufacturing cost per component as the number of components in an integrated circuit increases. Initially, as more components are packed onto a chip, cost per component decreases due to economies of scale—higher integration reduces need for packaging and interconnects. Moving from hundreds to thousands of components drastically reduced costs and improved performance [@moore2021cramming].\n\n::: {#fig-moores-law-plot fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.8}{\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\begin{loglogaxis}\n[width=84mm,\ntick label style={/pgf/number format/assume math mode=true},\nxlabel=Number of components per integrated circuit,\nylabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\nxlabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n}},\nylabel=Relative manufacturing cost/component,\ntick label style={font=\\footnotesize},\nclip=false,\nminor tick style={draw=none},\nmajor tick style={draw=black},\nxmin=1.0e0,\nxmax=1.0e5,\nymax=1.0e5,\nymin=1.0e0,\n]\n\\draw[VioletLine,line width=1.5pt,smooth] (axis cs:2.4, 29000)\nto [out=315,in=250,distance=26]node[above=5pt,pos=0.45]{1962}(axis cs:31, 29600);\n%\n\\draw[RedLine,line width=1.5pt,smooth] (axis cs:2.4,5800)\nto [out=320,in=255,distance=55]node[above=5pt,pos=0.45]{1965}(axis cs:198, 6700);\n%\n\\draw[BlueLine,line width=1.5pt,smooth] (axis cs:2.4,1400)\nto [out=320,in=250,distance=85]node[above=5pt,pos=0.45]{1970}(axis cs:24800.4, 1200);\n\\end{loglogaxis}\n\\end{tikzpicture}}\n```\n: **Moore's Law Economics**: Declining per-component manufacturing costs initially drove exponential growth in integrated circuit complexity, but diminishing returns eventually limited further cost reductions. This relationship mirrors optimization challenges in machine learning, where increasing model complexity yields diminishing gains in performance relative to computational expense. Source: [@moore2021cramming].\n:::\n\nHowever, as integration continues, the curve begins to rise. Components packed closer together face reliability issues like increased heat dissipation and signal interference. Addressing these requires more sophisticated manufacturing techniques—advanced lithography, error correction, improved materials—increasing complexity and cost. This U-shaped curve captures the fundamental trade-off: early improvements yield substantial benefits, but beyond a certain point, each additional gain comes at greater cost.\n\nThe dynamics mirror ML optimization challenges. Compressing a deep learning model to reduce size and energy consumption follows a similar trajectory. Initial optimizations like pruning redundant parameters or reducing precision often lead to significant savings with minimal accuracy impact. However, as compression progresses, performance losses become harder to recover. Techniques like quantization or hardware-specific tuning can restore some performance, but these add complexity and cost.\n\nSimilarly, in data efficiency, reducing training dataset size often improves computational efficiency initially. Yet as datasets shrink further, they may lose diversity, compromising generalization. Addressing this often involves synthetic data or sophisticated augmentation, demanding additional engineering effort.\n\nThe Moore's Law plot serves as a visual reminder that optimization is not infinite. The cost-benefit balance is always context-dependent, and the point of diminishing returns varies based on system goals and constraints. ML practitioners, like semiconductor engineers, must identify when further optimization ceases to provide meaningful benefits. Over-optimization can lead to wasted resources, reduced adaptability, and systems overly specialized to initial conditions.\n\n## Fallacies and Pitfalls {#sec-efficient-ai-fallacies-pitfalls-f804}\n\nEfficiency in AI systems involves complex trade-offs between multiple competing objectives that often pull in different directions. The mathematical elegance of scaling laws can create false confidence about predictable optimization paths, while diverse deployment context requirements create misconceptions about universal efficiency strategies.\n\n**Fallacy:** _Efficiency optimizations always improve system performance across all metrics._\n\nThis misconception leads teams to apply efficiency techniques without understanding trade-offs and side effects. Optimizing for computational efficiency might degrade accuracy, improving memory efficiency could increase latency, and reducing model size often requires more complex training procedures. Efficiency gains in one dimension frequently create costs in others that may be unacceptable for specific scenarios. Effective efficiency optimization requires careful analysis of which metrics matter most and acceptance that some performance aspects will necessarily be sacrificed.\n\n**Pitfall:** _Assuming scaling laws predict efficiency requirements linearly across all model sizes._\n\nTeams often extrapolate efficiency requirements based on scaling law relationships without considering breakdown points where these laws no longer apply. Scaling laws provide useful guidance for moderate increases, but fail to account for emergent behaviors, architectural constraints, and infrastructure limitations appearing at extreme scales. Applying scaling law predictions beyond validated ranges can lead to wildly inaccurate resource estimates and deployment failures. Successful efficiency planning requires understanding both utility and limits of scaling law frameworks.\n\n**Fallacy:** _Edge deployment efficiency requirements are simply scaled-down versions of cloud requirements._\n\nThis belief assumes edge deployment is merely cloud deployment with smaller models and less computation. Edge environments introduce qualitatively different constraints including real-time processing requirements, power consumption limits, thermal management needs, and connectivity variability. Optimization strategies working in cloud environments often fail catastrophically in edge contexts. Edge efficiency requires different approaches prioritizing predictable performance, energy efficiency, and robust operation under varying conditions.\n\n**Pitfall:** _Focusing on algorithmic efficiency while ignoring system-level efficiency factors._\n\nMany practitioners optimize algorithmic complexity metrics like FLOPs or parameter counts without considering how improvements translate to actual system performance. Real system efficiency depends on memory access patterns, data movement costs, hardware utilization characteristics, and software stack overhead that may not correlate with theoretical complexity metrics. A model with fewer parameters might still perform worse due to irregular memory access patterns or poor hardware mapping. Comprehensive efficiency optimization requires measuring and optimizing actual system performance rather than relying solely on algorithmic complexity indicators.\n\n## Summary {#sec-efficient-ai-summary-66bb}\n\nEfficiency has emerged as a design principle that transforms how we approach machine learning systems, moving beyond simple performance optimization toward comprehensive resource stewardship. This chapter revealed how scaling laws provide empirical insights into relationships between model performance and computational resources, establishing efficiency as a strategic advantage enabling broader accessibility, sustainability, and innovation. The interdependencies between algorithmic, compute, and data efficiency create a complex landscape where decisions in one dimension cascade throughout the entire system, requiring a holistic perspective balancing trade-offs across the complete ML pipeline.\n\nThe practical challenges of designing efficient systems highlight the importance of context-aware decision making, where deployment environments shape efficiency priorities. Cloud systems leverage abundant resources for scalability and throughput, while edge deployments optimize for real-time performance within strict power constraints, and TinyML applications push the boundaries of what's achievable with minimal resources. These diverse requirements demand sophisticated strategies including end-to-end co-design, automated optimization tools, and careful prioritization based on operational constraints. The emergence of scaling law breakdowns and tension between innovation and efficiency underscore that optimal system design requires addressing not just technical trade-offs but broader considerations of equity, sustainability, and long-term impact.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Efficiency is a strategic enabler that democratizes access to AI capabilities across diverse deployment contexts\n* Scaling laws provide predictive frameworks for resource allocation, but their limits reveal opportunities for architectural innovation\n* Trade-offs between algorithmic, compute, and data efficiency are interconnected and context-dependent, requiring holistic optimization strategies\n* Automation tools and end-to-end co-design approaches can transform efficiency constraints into opportunities for system synergy\n:::\n\nHaving established the three-pillar efficiency framework and explored scaling laws as the quantitative foundation for resource allocation, the following chapters provide the specific engineering techniques to achieve efficiency in each dimension. @sec-model-optimizations focuses on algorithmic efficiency through systematic approaches to reducing model complexity while preserving performance. The chapter covers quantization techniques that reduce numerical precision, pruning methods that eliminate redundant parameters, and knowledge distillation approaches that transfer capabilities from large models to smaller ones.\n\n@sec-ai-acceleration addresses compute efficiency by exploring how specialized hardware and optimized software implementations maximize performance per unit of computational resource. Topics include GPU optimization, AI accelerator architectures, and system-level optimizations that improve throughput and reduce latency. @sec-benchmarking-ai provides the measurement methodologies essential for quantifying efficiency gains across all three dimensions, covering performance evaluation frameworks, energy measurement techniques, and comparative analysis methods.\n\nThis progression from principles to specific techniques to measurement methodologies reflects the systematic engineering approach necessary for achieving real-world efficiency in machine learning systems. Each subsequent chapter builds upon the foundational understanding established here, creating a comprehensive toolkit for performance engineering that addresses the complex, interconnected trade-offs that define efficient AI system design.\n\nThese efficiency principles establish the foundation for the specific optimization techniques explored in @sec-model-optimizations, where detailed algorithms for quantization, pruning, and knowledge distillation provide concrete tools for achieving the efficiency goals outlined here. As machine learning systems continue scaling in complexity and reach, the principles of efficient design will remain essential for creating systems that are not only performant but also sustainable, accessible, and aligned with broader societal goals of responsible AI development.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"efficient_ai.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","efficient_ai.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3,"quiz":"efficient_ai_quizzes.json","concepts":"efficient_ai_concepts.yml","glossary":"efficient_ai_glossary.json"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}