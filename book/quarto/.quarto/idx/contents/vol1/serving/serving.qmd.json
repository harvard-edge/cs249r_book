{"title":"Serving","markdown":{"yaml":{"bibliography":"serving.bib","quiz":"serving_quizzes.json","concepts":"serving_concepts.yml","glossary":"serving_glossary.json"},"headingText":"Serving","headingAttr":{"id":"sec-serving","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n::: {layout-narrow}\n::: {.column-margin}\n*DALLÂ·E 3 Prompt: An illustration of a machine learning serving system, depicted as a high-speed assembly line in a modern factory. On one end, raw data (images, text documents, sensor readings) enters on conveyor belts. In the center, a glowing neural network processes inputs through preprocessing stations, a central inference engine, and postprocessing units. Workers monitor latency gauges and throughput meters. Some requests wait in small batches while others stream through individually. The scene conveys speed, precision, and the transformation from raw input to useful predictions, with visible clocks emphasizing the time-critical nature of serving.*\n:::\n\n\\noindent\n![](images/png/cover_serving.png){width=80%}\n\n:::\n\n## Purpose {.unnumbered}\n\n_How do the system requirements for serving trained models differ from training them, and what principles govern the design of responsive prediction systems?_\n\nTraining and serving are fundamentally different computational paradigms requiring distinct system designs. Training optimizes throughput over days or weeks of computation while serving inverts this priority, optimizing latency per request under strict time constraints measured in milliseconds. A common misconception is that faster hardware automatically means faster serving, but in practice preprocessing and postprocessing often dominate latency: production systems report preprocessing consuming 60-70% of total request time when inference runs on optimized accelerators. Understanding where latency actually comes from requires mastering queuing theory fundamentals that explain why systems degrade nonlinearly under load, recognizing how traffic patterns (Poisson arrivals for servers, streaming for autonomous vehicles, single-user for mobile) determine batching strategy, and connecting serving decisions to prior chapters on quantization, hardware acceleration, and benchmarking. The principles established here for single-machine serving provide the foundation for understanding when and why scaling to multiple machines becomes necessary\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain how serving systems invert training priorities by optimizing for per-request latency and percentile metrics rather than aggregate throughput\n\n- Decompose request latency into preprocessing, inference, and postprocessing phases to identify optimization bottlenecks\n\n- Apply queuing theory (Little's Law, M/M/1 model) to predict latency under load, analyze the utilization-latency relationship, and perform capacity planning to meet SLOs\n\n- Identify and prevent training-serving skew through consistent preprocessing implementations, statistical validation, and production monitoring\n\n- Select appropriate batching strategies (dynamic batching, continuous batching, no batching) based on traffic patterns and deployment contexts\n\n- Evaluate tradeoffs in runtime selection (framework-native, ONNX, TensorRT), precision configuration (FP32, FP16, INT8), and model loading strategies to meet deployment constraints\n\n- Calculate cost per inference across compute options (GPU vs CPU) and apply capacity planning principles to provision infrastructure\n\n:::\n\n## From Training to Production {#sec-serving-training-to-production}\n\nThe journey from a trained model to a production system crosses a fundamental boundary. Previous chapters established how to train models efficiently (@sec-ai-training), optimize them for deployment (@sec-model-optimizations), and accelerate their execution on specialized hardware (@sec-ai-acceleration). Serving introduces a new dimension that transforms every prior decision: real-time responsiveness under unpredictable load. Where training optimizes for samples processed per hour over days of computation, serving must deliver predictions within milliseconds while handling request patterns that no training process could anticipate.\n\nThis inversion of priorities has profound consequences. The benchmarking techniques from @sec-benchmarking-ai now target percentile latencies rather than aggregate throughput. The quantization methods from @sec-model-optimizations must be validated not just for accuracy preservation but for calibration with production traffic. The hardware acceleration from @sec-ai-acceleration must be configured for single-request responsiveness rather than batch throughput. Understanding these connections enables practitioners to apply earlier optimizations correctly in the serving context.\n\nThe serving systems examined here focus on single-machine deployment, establishing the foundational principles that govern all inference systems. When these single-machine foundations prove insufficient, @sec-inference-at-scale examines distributed serving patterns including load balancing across nodes and model sharding for models exceeding single-machine memory.\n\n::: {.callout-note title=\"Lighthouse Example: Serving a ResNet-50 Image Classifier\"}\n\nThis chapter uses **serving a ResNet-50 image classification model** as a consistent reference point to ground abstract concepts in concrete reality. ResNet-50 represents an ideal teaching example because it:\n\n- **Spans common deployment scenarios**: Used in everything from mobile apps to cloud APIs\n- **Has well-documented performance characteristics**: 25.6M parameters, ~4 GFLOPS per inference, extensively benchmarked\n- **Exhibits all key serving challenges**: Preprocessing overhead (image decoding, resizing), batching tradeoffs, memory management\n- **Represents production ML systems**: Image classification remains one of the most widely deployed ML applications\n\n**Key ResNet-50 Serving Specifications:**\n\n- **Parameters**: 25.6 million (98MB FP32, 49MB FP16, 25MB INT8)\n- **Input**: 224Ã—224Ã—3 RGB images (150KB typical JPEG, 588KB uncompressed tensor)\n- **Inference Time**: ~5ms on V100 GPU (batch=1), ~1ms per image at batch=32\n- **Preprocessing**: JPEG decode (~3ms), resize (~1ms), normalize (~0.5ms)\n- **Memory Footprint**: ~400MB GPU memory including activations\n\n**ðŸ”„ ResNet-50 Example Markers** appear at strategic points where this specific model illuminates the concept under discussion. Each example provides quantitative specifications and concrete implementation decisions encountered when serving this model.\n\n:::\n\n## Serving System Fundamentals {#sec-serving-fundamentals}\n\nServing systems occupy a unique position in the machine learning lifecycle, operating under constraints that differ fundamentally from both training and batch processing. Before examining specific optimization techniques, we must establish the conceptual framework that distinguishes these systems from other computational workloads.\n\n::: {.callout-definition title=\"Model Serving\"}\n\n***Model Serving*** is the process of exposing trained machine learning models for _real-time prediction_, requiring systems that transform raw inputs into useful outputs while meeting _latency constraints_, maintaining _consistency_ with training behavior, and achieving _cost-effective resource utilization_.\n\n:::\n\nThe defining characteristic of serving systems is their request-driven nature. Unlike training, where the system controls when and how data flows through the model, serving systems must respond to external requests that arrive unpredictably. This fundamental difference shapes every design decision, from memory management to error handling.\n\n### Static vs Dynamic Inference {#sec-serving-static-dynamic}\n\nThe first architectural decision in any serving system is whether predictions happen before or during user requests [@google2024staticdynamic]. This choice fundamentally shapes system design, cost structure, and capability boundaries.\n\n**Static inference** (also called offline or batch inference) pre-computes predictions for anticipated inputs and stores them for retrieval. Consider a recommendation system that generates predictions for all user-item pairs nightly. When a user requests recommendations, the system retrieves pre-computed results from a lookup table rather than running inference. This approach eliminates inference latency entirely since results already exist, enables quality verification before deployment, and dramatically reduces serving costs. However, static inference cannot handle novel inputs that were not anticipated during the batch computation and introduces hours or days of latency when models update.\n\n**Dynamic inference** (also called online or real-time inference) computes predictions on demand when requests arrive. This handles any input, including rare edge cases and novel combinations, and immediately reflects model updates. The cost is strict latency requirements that may force simpler models and more intensive monitoring infrastructure.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Static vs Dynamic Tradeoffs\"}\n\nFor our ResNet-50 image classifier, consider two deployment scenarios:\n\n**Static approach**: A photo organization app pre-classifies all images in a user's library overnight. With 10,000 photos and 5ms inference each, batch processing takes ~50 seconds total. Users see instant classification when browsing their library.\n\n**Dynamic approach**: A content moderation API must classify user-uploaded images in real-time. Each image requires the full preprocessingâ†’inferenceâ†’postprocessing pipeline, with a 100ms latency budget to meet user expectations.\n\nMost production image classification systems use a **hybrid approach**: frequently requested images (popular products, known memes) are pre-classified and cached, while novel uploads trigger dynamic inference. This reduces average latency while maintaining flexibility.\n\n:::\n\nMost production systems combine both approaches. Common queries hit a cache populated by batch inference while uncommon requests trigger dynamic computation. Understanding this spectrum is essential because it determines which subsequent optimization strategies apply. Static inference optimizes for throughput during batch computation and storage efficiency for serving. Dynamic inference optimizes for per-request latency under concurrent load, which requires understanding where time goes within each request.\n\n### The Load Balancer Layer {#sec-serving-load-balancer}\n\nProduction serving systems never expose model servers directly to clients. A load balancer sits between clients and model replicas, providing essential functions that affect request distribution, health monitoring, and deployment strategies.\n\n**Request Distribution**\n\nCommon algorithms for distributing requests across model replicas include:\n\n- **Round-robin**: Simple, even distribution that ignores server load\n- **Least-connections**: Routes to server with fewest active requests; effective for variable latency workloads\n- **Weighted**: Assigns proportional traffic based on server capacity\n- **Consistent hashing**: Routes similar requests to same servers for cache efficiency\n\nFor latency-sensitive ML serving, least-connections often outperforms round-robin because it naturally routes away from slow replicas.\n\n**Health Checking**\n\nLoad balancers continuously probe replica health:\n\n```yaml\n# Typical health check configuration\nhealth_check:\n  interval: 5s\n  timeout: 2s\n  unhealthy_threshold: 3  # Mark unhealthy after 3 failures\n  healthy_threshold: 2    # Mark healthy after 2 successes\n  path: /health           # HTTP endpoint to probe\n```\n\nFor ML systems, health checks must verify not just process liveness but model readiness (weights loaded, warmup complete). A server that responds to health checks but has not loaded the model will produce errors or high latency.\n\n**Connection Management**\n\nLoad balancers manage the connection lifecycle:\n\n- Maintain connection pools to replicas (avoiding TCP handshake per request)\n- Implement retry on failure (with exponential backoff)\n- Circuit breakers: Stop routing to replicas that fail repeatedly\n\n**Gradual Rollout**\n\nLoad balancers enable safe deployment through traffic splitting:\n\n```\nTraffic split during canary deployment:\n  stable-v1: 95%\n  canary-v2: 5%\n\nAfter validation:\n  stable-v1: 50%\n  canary-v2: 50%\n\nFinal promotion:\n  stable-v2: 100%\n```\n\n**Impact on Queuing Analysis**: The load balancer's behavior affects the queuing dynamics analyzed in @sec-serving-queuing. When capacity planning considers \"the server,\" it really means the combined behavior of multiple replicas behind a load balancer.\n\n## The Latency Budget {#sec-serving-latency-budget}\n\nFor dynamic inference systems, the fundamental difference from training lies in optimization objectives. Training optimizes throughput: maximizing samples processed per hour over days of computation. A training job that processes 1000 samples per second is successful regardless of how long any individual sample takes. Serving inverts this priority, optimizing latency per request under strict time constraints. A serving system with 1000ms per-request latency has failed, even if it achieves excellent throughput.\n\nThis mindset shift has concrete implications for system design [@gujarati2020serving]. The metrics that matter change from aggregate throughput to latency distributions. Mean latency tells you little about user experience; p50, p95, and p99 latencies reveal how the system performs across the full range of requests. If your mean latency is 50ms but p99 is 2 seconds, one in a hundred users waits 40 times longer than average. For consumer-facing applications, these tail latencies often determine user satisfaction and retention.[^fn-tail-latency]\n\n[^fn-tail-latency]: **Tail Latency Impact**: Research at Google found that users are more sensitive to latency variance than mean latency. A 100ms increase in p99 latency can reduce revenue by 1% for e-commerce applications. This is why service level objectives (SLOs) typically specify percentile targets rather than averages.\n\n::: {.callout-definition title=\"Latency Budget\"}\n\n***Latency Budget*** is the maximum time allowed for a serving request to complete, decomposed into allocations for _preprocessing_, _inference_, and _postprocessing_ phases. Effective latency budgeting requires understanding where time is consumed and allocating resources accordingly.\n\n:::\n\nEvery serving request decomposes into three phases, each consuming part of the latency budget:\n\n1. **Preprocessing**: Transform raw input (image bytes, text strings) into model-ready tensors\n2. **Inference**: Execute the model computation\n3. **Postprocessing**: Transform model outputs into user-facing responses\n\nA common misconception is that faster hardware automatically means faster serving. In practice, preprocessing and postprocessing often dominate total latency. Studies of production systems show preprocessing consuming 60-70% of total request time when inference runs on optimized accelerators [@nvidia2024tritontutorial]. Optimizing only the inference phase yields diminishing returns when the surrounding pipeline remains bottlenecked on CPU operations.\n\n### Latency Distribution Analysis {#sec-serving-latency-analysis}\n\nUnderstanding where time goes requires instrumenting each phase independently. Consider what happens when our ResNet-50 classifier receives a JPEG image:\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Latency Budget Breakdown\"}\n\nA typical serving request for our ResNet-50 classifier shows the following latency distribution:\n\n| Phase | Operation | Time | Percentage |\n|-------|-----------|------|------------|\n| Preprocessing | JPEG decode | 3.0ms | 30% |\n| Preprocessing | Resize to 224Ã—224 | 1.0ms | 10% |\n| Preprocessing | Normalize (mean/std) | 0.5ms | 5% |\n| Data Transfer | CPUâ†’GPU copy | 0.5ms | 5% |\n| **Inference** | **ResNet-50 forward pass** | **5.0ms** | **50%** |\n| Postprocessing | Softmax + top-5 | 0.1ms | ~0% |\n| **Total** | | **10.1ms** | **100%** |\n\nKey insight: **Preprocessing consumes 45% of latency** despite model inference being the computationally intensive phase. With TensorRT optimization reducing inference to 2ms, preprocessing would dominate at 75%.\n\n:::\n\nThis breakdown reveals why naive optimization efforts often fail. Engineers focus on model optimization (quantization, pruning) because that is where ML expertise applies, but the actual bottleneck is image decoding running on CPU. Moving preprocessing to GPU can reduce total latency by 6x in some pipelines by eliminating CPU-GPU data transfers between stages [@nvidia2024tritontutorial].\n\n**The Killer Microseconds Problem**\n\nBarroso, Patterson, and colleagues identified a critical gap in how systems handle latency at different time scales [@barroso2017attack]. Modern systems efficiently handle nanosecond-scale events (CPU cache access, DRAM reads) through hardware mechanisms like out-of-order execution, and millisecond-scale events (disk I/O, network calls) through software techniques like threading and asynchronous I/O. But microsecond-scale events fall into an uncomfortable middle ground where neither approach works well.\n\nML serving lives squarely in this microsecond regime. Individual inference calls complete in 1-10ms, but the surrounding operations (serialization, memory allocation, network stack processing, encryption) each add microseconds that compound into significant overhead. Google's analysis found that a significant fraction (often 20% or more) of datacenter CPU cycles are consumed by this \"datacenter tax\" rather than useful computation. For serving systems, this means:\n\n- A 2Î¼s network fabric can become 100Î¼s end-to-end through software overhead\n- Context switching costs (5-10Î¼s) can exceed the inference time for small models\n- Memory allocation patterns in preprocessing can add unpredictable microsecond delays\n\nThese overheads explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization (memory pooling, zero-copy data paths, kernel bypass) matters as much as model optimization.\n\nThe latency budget framework provides a systematic approach to optimization. First, measure each phase to identify the true bottleneck. Then allocate engineering effort proportionally to where time is actually spent. Finally, consider architectural changes (GPU preprocessing, batching strategies) that can shift work between phases.\n\n### Resolution and Input Size Tradeoffs {#sec-serving-resolution}\n\nInput resolution dramatically affects both preprocessing and inference latency, but the relationship differs depending on whether the system is compute-bound or memory-bound. Understanding this distinction (covered in depth in @sec-ai-acceleration) is essential for making informed resolution decisions.\n\nFor compute-bound models, throughput scales inversely with resolution squared, as shown in @eq-resolution-throughput:\n\n$$\\frac{T(r_2)}{T(r_1)} = \\left(\\frac{r_1}{r_2}\\right)^2$$ {#eq-resolution-throughput}\n\nDoubling resolution from 224 to 448 theoretically yields 4Ã— slowdown (measured: 3.6Ã— due to fixed overhead amortization). However, at high resolutions, models transition from compute-bound to memory-bound as activation tensors exceed cache capacity. @tbl-resolution-bottleneck illustrates this transition for ResNet-50:\n\n| Resolution | Activation Size | Arith. Intensity | Bottleneck |\n|:-----------|:----------------|:-----------------|:-----------|\n| 224Ã—224 | 12.5MB | 290 FLOPS/byte | Compute |\n| 384Ã—384 | 36.8MB | 168 FLOPS/byte | Transitional |\n| 512Ã—512 | 65.5MB | 95 FLOPS/byte | Memory BW |\n| 640Ã—640 | 102.4MB | 61 FLOPS/byte | Memory BW |\n\n: **Resolution and Compute Bottleneck**: ResNet-50 arithmetic intensity decreases with resolution as activation sizes grow. The V100 crossover point (~125 FLOPS/byte) occurs around 384Ã—384, above which memory bandwidth limits throughput more than compute capacity. {#tbl-resolution-bottleneck}\n\n**Deployment-Specific Resolution Decisions**\n\nDifferent deployment contexts have fundamentally different resolution requirements:\n\n- **Mobile applications**: Lower resolution (224Ã—224) acceptable for object detection in camera viewfinders; latency and battery life dominate\n- **Medical imaging**: High resolution (512Ã—512+) required for diagnostic accuracy; latency requirements relaxed\n- **Autonomous vehicles**: Multiple resolutions for different tasks (low-res for detection, high-res crops for recognition)\n- **Cloud APIs**: Resolution often set by client upload; service must handle range gracefully\n\n**Adaptive Resolution**\n\nProduction systems can select resolution dynamically based on content:\n\n1. Run lightweight classifier at 128Ã—128 to categorize content type\n2. Select task-appropriate resolution: documents at 512Ã—512, landscapes at 224Ã—224, faces at 384Ã—384\n3. Achieve 1.4Ã— throughput improvement with 99.2% accuracy retention versus fixed high resolution\n\nThis pattern trades preprocessing cost (running the lightweight classifier) for inference savings on the main model.\n\n## Queuing Fundamentals {#sec-serving-queuing}\n\nThe latency budget framework explains where time goes within a single request, but production systems must handle many concurrent requests competing for finite resources. Understanding why latency degrades under load requires queuing theory, the mathematical framework that explains how requests wait for service in any system with finite capacity. These principles apply universally, from web servers to ML inference, and they explain the counterintuitive behavior that causes well-provisioned systems to suddenly violate latency SLOs when load increases modestly.\n\n### Little's Law {#sec-serving-littles-law}\n\nThe most fundamental result in queuing theory is Little's Law (@eq-littles-law), which relates three quantities in any stable system:\n\n$$L = \\lambda \\cdot W$$ {#eq-littles-law}\n\nwhere $L$ is the average number of requests in the system, $\\lambda$ is the arrival rate (requests per second), and $W$ is the average time each request spends in the system. This relationship holds regardless of arrival distribution, service time distribution, or scheduling policy.\n\nLittle's Law has immediate practical implications. If your inference service averages 10ms per request ($W = 0.01$s) and you observe 50 concurrent requests in the system on average ($L = 50$), then your arrival rate must be $\\lambda = L/W = 5000$ requests per second. Conversely, if you need to limit concurrent requests to 10 (perhaps due to GPU memory constraints), and your service time is 10ms, you can sustain at most 1000 requests per second.\n\n### The Utilization-Latency Relationship {#sec-serving-utilization-latency}\n\nFor a system with Poisson arrivals and exponential service times (the M/M/1 queue model), the average time in system follows:\n\n$$W = \\frac{1}{\\mu - \\lambda} = \\frac{\\text{service time}}{1 - \\rho}$$ {#eq-mm1-wait}\n\nwhere $\\mu$ is the service rate (requests per second the server can handle), and $\\rho = \\lambda/\\mu$ is the utilization (fraction of time the server is busy).\n\nThis equation reveals why serving systems exhibit nonlinear behavior. At 50% utilization, average wait time is $2\\times$ the service time. At 80% utilization, it is $5\\times$. At 90% utilization, it is $10\\times$. The relationship is hyperbolic: small increases in load near capacity cause disproportionate latency increases.\n\n| Utilization ($\\rho$) | Wait Time Multiple | Example (5ms service) |\n|:---------------------|:-------------------|:----------------------|\n| 50% | 2.0Ã— | 10ms |\n| 70% | 3.3Ã— | 17ms |\n| 80% | 5.0Ã— | 25ms |\n| 90% | 10.0Ã— | 50ms |\n| 95% | 20.0Ã— | 100ms |\n\n: **Utilization-Latency Relationship**: Average wait time as a multiple of service time for an M/M/1 queue. The nonlinear relationship explains why systems that perform well at moderate load can suddenly violate SLOs when traffic increases. {#tbl-utilization-latency}\n\nThe M/M/1 model assumes exponentially distributed service times, but ML inference typically has near-constant service time for fixed batch sizes, making the M/D/1 (deterministic service) model more accurate in practice. We use M/M/1 here because it yields closed-form solutions and produces conservative estimates. For M/D/1 queues, average wait time is approximately half of M/M/1 at the same utilization, which matters for capacity planning: M/M/1 analysis will slightly over-provision, erring on the side of meeting SLOs rather than violating them.[^fn-queuing-models]\n\n[^fn-queuing-models]: **Queuing Model Selection**: For deeper treatment of M/M/c (multi-server) and M/G/1 (general service distribution) queues applicable to ML serving, see Harchol-Balter's *Performance Modeling and Design of Computer Systems* [@harchol2013performance], particularly chapters on server farms and scheduling policies.\n\n### Multi-Server Queuing Reality {#sec-serving-multi-server-queuing}\n\nProduction serving systems deploy multiple replicas behind a load balancer, making M/M/c (c servers) the more applicable model than M/M/1.\n\n**Key insight**: For c servers with utilization $\\rho$ per server, the probability of queuing drops exponentially with c, and average wait time is approximately:\n\n$$W_{M/M/c} \\approx \\frac{W_{M/M/1}}{c} \\quad \\text{(for low utilization)}$$\n\n**Practical Implication**: Four servers at 70% utilization each provide far better tail latency than one server at 70%, despite handling the same total load.\n\n| Configuration | Total Capacity | Avg Wait | p99 Wait |\n|:--------------|:---------------|:---------|:---------|\n| 1 server @ 70% | 1x | 2.3x service | 7x service |\n| 4 servers @ 70% each | 4x | 0.6x service | 2x service |\n\n: **Multi-Server Queuing Benefits**: Horizontal scaling improves tail latency even when adding capacity, because the queuing dynamics fundamentally improve with multiple servers. {#tbl-multi-server-queuing}\n\nThis explains why horizontal scaling improves tail latency even when adding capacity: the queuing dynamics fundamentally improve with multiple servers.\n\n**When M/M/1 Analysis Applies**: Despite multi-server reality, M/M/1 analysis remains useful for understanding fundamental queuing dynamics, worst-case analysis (what if load concentrates on one server?), and capacity planning for individual server sizing.\n\n### Tail Latency {#sec-serving-tail-latency}\n\nProduction SLOs typically specify percentile targets (p95, p99) rather than averages because tail latency determines user experience for the slowest requests [@dean2013tail]. For an M/M/1 queue, the p99 latency follows:\n\n$$W_{p99} \\approx \\text{service time} \\cdot \\left(1 + \\frac{\\ln(100 \\cdot \\rho)}{1 - \\rho}\\right)$$ {#eq-p99-latency}\n\nAt 70% utilization, p99 latency is approximately $12\\times$ the service time, while average latency is only $3.3\\times$. This explains why systems that seem healthy (low average latency) can have unacceptable tail latency: the average hides the experience of the unluckiest requests.\n\n**The Tail at Scale Problem**\n\nDean and Barroso's foundational analysis reveals why tail latency becomes critical in distributed systems [@dean2013tail]. Consider a service where individual servers have p50 latency of 10ms and p99 latency of 50ms (a 5x tail ratio, typical for well-tuned systems). For a single server, only 1% of requests experience the 50ms tail. But when a user request fans out to 100 servers in parallel and waits for all responses:\n\n$$P(\\text{any server slow}) = 1 - (1 - 0.01)^{100} = 0.63$$\n\nSixty-three percent of user requests experience the slow tail. This fan-out amplification is why even well-behaved individual servers create unacceptable tail latency at scale. At 1000 servers, the probability rises to 99.996%, meaning virtually every request hits at least one slow server.\n\n**The Tail at Scale Solution**: Techniques like request hedging (sending redundant requests after a timeout), backup requests, and load balancing away from slow servers directly address this amplification effect. As we will see in @sec-inference-at-scale, these tail-tolerant techniques become essential when scaling to distributed inference systems where fan-out magnifies individual server variance.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Capacity Planning Worked Example\"}\n\nConsider designing a ResNet-50 serving system with these requirements:\n\n- **Target p99 latency**: 50ms\n- **Peak expected traffic**: 5,000 requests per second\n- **Service time** (TensorRT FP16): 5ms\n\n**Step 1: Find safe utilization**\n\nUsing @eq-p99-latency, we need $W_{p99} \\leq 50$ms with 5ms service time. Solving for $\\rho$:\n\n$$5\\text{ms} \\cdot \\left(1 + \\frac{\\ln(100 \\cdot \\rho)}{1 - \\rho}\\right) \\leq 50\\text{ms}$$\n\nThis yields $\\rho \\leq 0.72$ (72% maximum utilization).\n\n**Step 2: Calculate required service rate**\n\n$$\\mu_{\\text{required}} = \\frac{\\lambda}{\\rho_{\\text{safe}}} = \\frac{5000}{0.72} = 6944 \\text{ requests/second}$$\n\n**Step 3: Determine GPU count**\n\nSingle V100 throughput at batch=16: 1,143 images/second\n\n$$\\text{GPUs needed} = \\frac{6944}{1143} = 6.1 \\rightarrow 7 \\text{ GPUs}$$\n\n**Step 4: Add headroom for variance**\n\nProduction systems add 30% headroom for traffic spikes and variance:\n\n$$\\text{Final count} = 7 \\times 1.3 = 9.1 \\rightarrow 10 \\text{ GPUs}$$\n\n**Step 5: Verify fault tolerance (N+1 redundancy)**\n\nThe 30% headroom addresses traffic variance, but production systems also need fault tolerance. With 10 GPUs, losing one leaves 9 GPUs handling 5,000 QPS:\n\n$$\\text{Utilization after failure} = \\frac{5000 / 1143}{9} = 48.6\\%$$\n\nThis remains well below the 72% safe utilization threshold, confirming N+1 redundancy is satisfied. For stricter fault tolerance requirements, N+2 redundancy (tolerating two simultaneous failures) would require 11-12 GPUs.\n\n**Result**: Provision 10 V100 GPUs to serve 5,000 QPS at 50ms p99 latency with N+1 fault tolerance.\n\n:::\n\nThe queuing analysis explains the capacity planning approach mentioned in @sec-serving-capacity-planning and connects to the MLPerf Server scenario from @sec-benchmarking-ai, which measures throughput only for requests meeting the latency SLO. A system achieving 10,000 QPS but violating the SLO on 5% of requests reports only 9,500 valid QPS.\n\n### Tail-Tolerant Techniques {#sec-serving-tail-tolerant}\n\nRather than eliminating all sources of latency variability (often impractical), production systems employ techniques that tolerate variability while still meeting SLOs [@dean2013tail; @dean2012rapid]. These techniques treat latency variance as a fact of life and design around it.\n\n**Hedged Requests**\n\nWhen a request has not completed within the expected time, send a duplicate request to another server. The client uses whichever response arrives first and cancels the other. For ML serving, this means maintaining multiple model replicas and routing slow requests to alternative replicas. The overhead is modest: if you hedge at the 95th percentile, only 5% of requests generate duplicates, increasing load by just 5% while dramatically reducing tail latency.\n\n**Cancellation Complexity**: A critical implementation detail is that CUDA kernels cannot be interrupted mid-execution. When a hedged request completes, the duplicate must be cancelled, but if inference has already begun on the GPU, cancellation approaches include: (1) checking a cancellation flag before launching inference, (2) accepting wasted compute for the in-flight kernel, or (3) using request prioritization to deprioritize the duplicate. Since hedging typically applies only to the slowest 5% of requests, the overhead from occasional wasted compute remains acceptable.\n\n**Tied Requests**\n\nSend the request to multiple servers simultaneously, but include a tag allowing servers to cancel execution once another server begins processing. This eliminates the delay of waiting to detect a slow response before hedging. For inference servers with significant startup overhead (model loading, memory allocation), tied requests ensure at least one server begins immediately.\n\n**Canary Requests**\n\nFor requests that fan out to many backends, first send the request to a small subset (1-2 servers). If these return within expected time, send to the remainder. If the canary is slow, the system can take corrective action (retry elsewhere, use cached results) before committing to the full fan-out. This prevents a single slow backend from stalling an entire distributed inference request.\n\n**Graceful Degradation**\n\nWhen load exceeds capacity, return approximate results rather than timing out. For classification, return cached predictions for similar inputs. For generative models, return shorter outputs. For ensemble systems, return predictions from a subset of models. This maintains responsiveness at the cost of some accuracy, which users often prefer to outright failures.\n\n**Admission Control**\n\nWhen traffic exceeds capacity, accepting all requests guarantees SLO violations for everyone. Admission control proactively rejects requests when queue depth exceeds a threshold, returning immediate 503 responses rather than accepting requests that will timeout. This sacrifices throughput to protect latency for admitted requests.\n\n**Setting the Threshold**: A practical starting point is 2-3x service time multiplied by the number of workers. For a system with 4 workers and 10ms service time, this yields a queue depth threshold of 80-120 requests. Adaptive admission control adjusts thresholds based on observed p99 latency, tightening when latency increases above target and relaxing when latency remains healthy.\n\n**Retry Storm Prevention**: A subtle failure mode occurs when all replicas are overloaded simultaneously. If the load balancer retries rejected requests at other replicas (also overloaded), retry traffic amplifies the overload. Coordinated load shedding addresses this by sharing load information across replicas, enabling system-wide decisions about which requests to accept. When global load exceeds capacity, replicas collectively reject the same fraction of requests rather than each rejecting independently and triggering retries.\n\nThese techniques become essential at scale when fan-out amplification makes individual server tail latency visible to users. Single-machine serving systems can implement hedged and tied requests across GPU streams or model replicas. The queuing analysis here assumes FIFO processing, but production systems often implement priority scheduling (deadline-aware, shortest-job-first) to further reduce tail latency for heterogeneous workloads [@harchol2013performance].\n\n## Training-Serving Skew {#sec-serving-skew}\n\nThe queuing theory and latency analysis from previous sections assume the model produces correct predictions. However, one of the most insidious problems in production ML systems threatens this assumption: training-serving skew, the phenomenon where a model behaves differently in production than during training, despite using identical weights [@sculley2015hidden]. The model has not changed, but something in the preprocessing pipeline produces different inputs in production than during training, causing silent accuracy degradation that may go undetected for weeks or months.\n\n::: {.callout-definition title=\"Training-Serving Skew\"}\n\n***Training-Serving Skew*** occurs when the _preprocessing logic_ or _data characteristics_ differ between training and serving environments, causing models to receive inputs that do not match their training distribution and resulting in degraded prediction quality.\n\n:::\n\nThis problem is particularly dangerous because traditional software testing often fails to catch it. The serving system returns predictions, the model runs without errors, and all health checks pass. Only careful monitoring of prediction quality reveals that something is wrong.\n\n### Sources of Skew {#sec-serving-skew-sources}\n\nUnderstanding the common causes of training-serving skew helps practitioners design systems that avoid these pitfalls.\n\n**Numerical precision differences** arise from different implementations of the same mathematical operations. Training uses float64 mean normalization while serving uses float32, producing different normalized values. A model achieving 0.87 AUC in validation may drop to 0.78 in production purely from these inconsistencies.\n\n**Library version mismatches** introduce subtle behavioral differences. Training uses NumPy 1.24 while serving uses TensorFlow ops with different rounding behavior. Image resizing with different interpolation defaults produces subtly different pixel values that accumulate through the network.\n\n**Missing value handling** diverges when training and serving encounter null values differently. Training fills nulls with column means computed over the training set while serving uses zeros or different defaults. Features present in training become missing in production due to upstream service failures.\n\n**Time and ordering effects** cause misalignment in temporal features. Training uses UTC timestamps while serving uses local time, causing off-by-one-day errors in date features. Tokenizers trained on specific vocabulary versions encounter unknown tokens differently than during training.\n\n**Environment differences** create systemic discrepancies [@polyzotis2017data]. Training runs in analytical environments (notebooks, data lakes) while serving runs in production microservices. Re-implementing feature engineering in different frameworks (Spark for training, pandas for serving) creates two codebases that must produce identical outputs.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Image Preprocessing Skew\"}\n\nFor ResNet-50 serving, common sources of skew include:\n\n**Resize interpolation**: Training uses PIL.BILINEAR while OpenCV defaults to cv2.INTER_LINEAR. These produce pixel-level differences that can shift accuracy by 0.5-1%.\n\n**Color space handling**: JPEG loading in different libraries may produce BGR vs RGB ordering. If the model trained on RGB but serves BGR inputs, predictions are essentially random.\n\n**Normalization constants**: ImageNet normalization uses specific mean/std values. Using `mean=[0.5, 0.5, 0.5]` instead of `mean=[0.485, 0.456, 0.406]` shifts inputs out of the training distribution.\n\n**Prevention**: The safest approach is to export the exact preprocessing code used during training and run it identically in serving, or use a framework like NVIDIA DALI that guarantees identical operations.\n\n:::\n\n### Prevention Strategies {#sec-serving-skew-prevention}\n\nThe fundamental solution is using identical preprocessing code for training and serving. This sounds obvious but proves difficult in practice. Training pipelines optimize for batch processing while serving pipelines optimize for single-request latency. Feature stores address this by centralizing feature computation, ensuring both training and serving retrieve features from the same source [@orr2021managing].\n\nWhen identical code is impossible, rigorous testing catches skew before deployment. Statistical comparison between training feature distributions and serving feature distributions reveals discrepancies. Shadow deployment runs the candidate model on live traffic alongside the baseline, surfacing skew that synthetic tests miss.\n\nMonitoring in production detects skew that emerges over time. Feature drift detection compares serving distributions to training baselines using statistical distance measures [@breck2019data]. When drift exceeds thresholds, alerts trigger investigation before accuracy degrades noticeably.\n\n@tbl-skew-prevention summarizes strategies for preventing and detecting training-serving skew.\n\n+-------------------------------+----------------------------------+---------------------------------+\n| **Strategy**                  | **Approach**                     | **Tradeoffs**                   |\n+:==============================+:=================================+:================================+\n| **Shared preprocessing code** | Use identical code for training  | May require runtime translation |\n|                               | and serving                      | (e.g., Python to C++)           |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Feature stores**            | Centralize feature computation   | Adds infrastructure complexity  |\n|                               | for both environments            |                                 |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Statistical validation**    | Compare feature distributions    | Catches drift but not all skew  |\n|                               | between training and serving     | types                           |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Shadow deployment**         | Run new model alongside baseline | Doubles serving cost during     |\n|                               | on live traffic                  | validation                      |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Continuous monitoring**     | Track prediction distributions   | Reactive rather than preventive |\n|                               | and accuracy metrics             |                                 |\n+-------------------------------+----------------------------------+---------------------------------+\n\n: **Training-Serving Skew Prevention**: Strategies range from prevention (shared code, feature stores) to detection (statistical validation, monitoring), each with distinct operational tradeoffs. {#tbl-skew-prevention}\n\n## Model Loading and Initialization {#sec-serving-model-loading}\n\nWith preprocessing pipelines designed to avoid training-serving skew, the next challenge is getting the model ready to serve. Before processing any request, models must load from storage into memory and prepare for inference [@romero2021infaas]. This initialization phase creates the cold start problem: the first request after deployment or scaling experiences dramatically higher latency. Understanding cold start dynamics is essential for designing systems that meet latency requirements from the moment they begin serving traffic.\n\n### Cold Start Anatomy {#sec-serving-cold-start}\n\nCold start latency compounds from multiple sources, each adding to the time between deployment and serving readiness:\n\n1. **Weight loading**: Reading model parameters from disk or network storage\n2. **Graph compilation**: Just-in-time compilation of operations for the specific hardware\n3. **Memory allocation**: Reserving GPU memory for activations and intermediate values\n4. **Warmup execution**: Initial inferences that populate caches and trigger lazy initialization\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Cold Start Timeline\"}\n\nLoading ResNet-50 for production serving involves the following cold start phases:\n\n| Phase | Duration | Notes |\n|-------|----------|-------|\n| Weight loading (SSD) | 0.5s | 98MB FP32 weights from local storage |\n| Weight loading (S3) | 3-5s | Network latency dominates for cloud storage |\n| CUDA context | 0.3-0.5s | CUDA 11+ lazy loading significantly reduced this |\n| TensorRT compilation | 15-30s | Converts PyTorch model to optimized engine |\n| Warmup (10 inferences) | 0.2s | Triggers remaining lazy initialization |\n| **Total (local, optimized)** | **~1.5s** | With pre-compiled TensorRT engine, warm container |\n| **Total (cloud, first deploy)** | **~35s** | Including compilation from cold state |\n\n**Key insight**: Pre-compiling models and storing the optimized engine eliminates the 30-second compilation phase on subsequent deployments. Production systems use CUDA MPS (Multi-Process Service) or pre-warmed container pools to amortize CUDA initialization costs across requests.\n\n:::\n\nWithout warmup, the first real request triggers compilation and memory allocation mid-inference, often causing timeout failures. A request that normally takes 5ms might require 500ms during cold start, violating SLOs and degrading user experience.\n\n### Loading Strategies {#sec-serving-loading-strategies}\n\nDifferent loading strategies trade off cold start duration against serving performance and memory efficiency.\n\n**Full loading** reads the entire model into memory before serving begins. This maximizes inference speed since all weights are immediately available, but extends cold start duration and limits model size to available memory. Full loading is appropriate when cold start latency is acceptable and models comfortably fit in memory.\n\n**Memory mapping** maps model files directly into the address space, loading pages on demand as accessed. This reduces cold start time since inference can begin before the full model loads, but causes unpredictable latency as pages fault in during initial requests. Memory mapping works well for infrequently accessed model components but can cause latency spikes if critical weights are not preloaded.\n\n**Lazy initialization** defers compilation and allocation until first use. This minimizes startup time but shifts latency to the first request. Production systems often combine lazy initialization with synthetic warmup requests to trigger initialization before real traffic arrives.\n\n### Model Caching Infrastructure {#sec-serving-model-caching}\n\nProduction systems cache model weights at the infrastructure level to reduce cold start for common deployment scenarios:\n\n**Container Image Embedding**: Bundle model weights directly in the container image. This produces a single deployment artifact and eliminates network fetches at startup, but creates large images (often 10-50GB) that slow container pulls and consume registry storage. Best for models that rarely update.\n\n**Shared Filesystem**: Mount a network filesystem (EFS, GCS FUSE) containing model weights. Multiple replicas share cached weights, and updates propagate immediately without redeployment. However, network latency affects cold start, and filesystem availability becomes a critical dependency. Best for organizations with many models and frequent updates.\n\n**Node-Local SSD Cache**: Pre-populate local SSDs on inference nodes with frequently-used models. Provides fast loading (500MB/s+ for NVMe) without network dependency, but requires cache management to handle model updates and capacity limits. Best for high-traffic models where cold start latency is critical.\n\nThe choice depends on model update frequency: infrequent updates favor container embedding, frequent updates favor shared filesystem, and performance-critical deployments benefit from local caching with background refresh.\n\n### Multi-Model Serving {#sec-serving-multi-model}\n\nProduction systems often serve multiple models from a single machine, whether different model versions for A/B testing, ensemble components, or entirely different models sharing infrastructure. GPU memory becomes the limiting resource, requiring careful management strategies.\n\nStrategies for multi-model serving include:\n\n- **Time-multiplexing**: Load one model at a time, swapping based on request routing\n- **Memory sharing**: Models share GPU memory, limiting concurrent execution but enabling more models\n- **Model virtualization**: Frameworks like Triton manage model lifecycle, loading and unloading based on traffic patterns [@nvidia2024triton]\n\nThe choice depends on request patterns. If models receive traffic evenly, concurrent loading works. If traffic is bursty and model-specific, time-multiplexing with intelligent preloading reduces average latency while maximizing GPU utilization.\n\n**Multi-Stream Execution**\n\nWhen multiple models (or multiple instances of the same model) must run concurrently on a single GPU, the hardware must partition resources between them. NVIDIA's Multi-Instance GPU (MIG) technology enables hardware-level isolation, dividing an A100 into up to 7 independent GPU instances, each with dedicated memory and compute resources. MIG is available on A100, A30, H100, and newer data center GPUs. For older GPUs (V100, T4), CUDA stream scheduling provides time-multiplexed sharing without hardware isolation.\n\nMIG provides stronger isolation guarantees but reduces flexibility: once partitioned, GPU slices cannot be dynamically resized. CUDA streams offer flexibility but no performance isolation. The choice depends on whether consistent latency (MIG) or maximum utilization (shared streams) is the priority. We examine full implementation details for multi-model orchestration across GPUs in @sec-inference-at-scale, where distributed resource management becomes critical for production-scale deployments.\n\n## Batching for Serving {#sec-serving-batching}\n\nOnce models are loaded and ready to serve, the next optimization opportunity lies in how requests are grouped for processing. Batching fundamentally differs between training and serving [@crankshaw2017clipper]. Training batches maximize throughput, processing hundreds or thousands of samples together with no concern for individual sample latency. Serving batches must balance throughput against individual request latency, typically processing single digits of requests together while ensuring no request waits too long.\n\n::: {.callout-definition title=\"Dynamic Batching\"}\n\n***Dynamic Batching*** is a serving strategy that collects incoming requests within a _time window_ and processes them together, trading individual request latency for improved _throughput_ and _hardware utilization_. The window size and maximum batch size parameters control this tradeoff.\n\n:::\n\n### Why Batching Helps {#sec-serving-batching-why}\n\nModern accelerators achieve peak efficiency only at sufficient batch sizes [@shen2019nexus]. A single inference request leaves most compute units idle because GPUs are designed for parallel execution across thousands of threads. Batching amortizes fixed costs (kernel launch overhead, weight loading from memory) across multiple requests and enables parallel execution across the batch dimension.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Batching Efficiency\"}\n\nThe throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates the power of batching:\n\n| Batch Size | Inference Time* | Per-Image Compute | Throughput | GPU Util. |\n|:-----------|:----------------|:------------------|:-----------|:----------|\n| 1 | 5.0ms | 5.0ms | 200 img/s | 15% |\n| 4 | 7.2ms | 1.8ms | 556 img/s | 42% |\n| 8 | 9.1ms | 1.1ms | 879 img/s | 65% |\n| 16 | 14.0ms | 0.9ms | 1,143 img/s | 85% |\n| 32 | 25.0ms | 0.8ms | 1,280 img/s | 95% |\n\n*Times shown are pure inference time, excluding queue wait. User-perceived latency includes batching window wait (see @sec-serving-traffic-patterns).\n\n**Key insight**: Batch size 32 achieves 6.4Ã— higher throughput than batch size 1. However, user-perceived latency includes both queue wait and inference time. With a 10ms batching window and 25ms inference, total latency reaches 35ms versus 5ms at batch size 1.\n\n:::\n\nHowever, batching forces requests to wait. A request arriving just after a batch closes must wait for the current batch to complete plus full processing of its own batch. This waiting time directly adds to user-perceived latency, creating the fundamental tradeoff that serving system designers must navigate.\n\n### Static vs Dynamic Batching {#sec-serving-batching-types}\n\n**Static batching** waits for a fixed batch size before processing. Simple to implement but problematic in practice: during low traffic, requests wait indefinitely for a full batch. During high traffic, large batches increase per-request latency.\n\n**Dynamic batching** collects requests within a time window, processing whatever has arrived when the window closes [@olston2017tensorflow]. This bounds maximum wait time regardless of traffic level. The window size represents a direct tradeoff: shorter windows reduce latency but sacrifice throughput; longer windows improve throughput but increase latency.\n\nTypical configurations use windows of 5-50ms with maximum batch sizes of 8-32 for latency-sensitive applications. The optimal configuration depends on request arrival patterns, model characteristics, and latency requirements.\n\n### Continuous Batching {#sec-serving-continuous-batching}\n\nAutoregressive models like language models generate outputs token by token, creating a batching challenge that differs fundamentally from single-pass models like ResNet-50. Traditional batching processes all sequences in a batch for all generation steps, wasting compute when sequences complete at different times [@yu2022orca]. If one sequence in a batch of 8 finishes after 10 tokens while others need 100 tokens, 87.5% of the compute for that sequence slot is wasted. This inefficiency becomes critical as language models dominate production inference workloads.\n\nContinuous batching (also called iteration-level batching) addresses this waste by allowing new requests to join a batch between generation steps and completed sequences to exit [@kwon2023vllm]. Rather than forming static batches that persist for the entire generation process, the system dynamically manages batch composition at each decoding iteration.\n\nThe mechanism works as follows: when a sequence generates its end-of-sequence token, its slot becomes immediately available. A waiting request can fill that slot for the next iteration rather than waiting for the entire batch to complete. Similarly, the system can add new requests to available slots without interrupting ongoing generation. This dynamic approach maintains high GPU utilization even when sequence lengths vary dramatically.\n\nSystems implementing continuous batching, such as vLLM and TensorRT-LLM, achieve 2-4Ã— higher throughput than traditional static batching [@agrawal2024sarathi]. The improvement comes from two sources: eliminating wasted compute on completed sequences and reducing average wait time for new requests. For production language model serving where response lengths vary from single tokens to thousands, continuous batching has become essential for cost-effective deployment.\n\nMemory management adds complexity to continuous batching. As sequences enter and exit the batch, the key-value cache that stores attention context must be dynamically allocated and freed. PagedAttention, introduced in vLLM, applies operating system paging concepts to manage this memory efficiently, avoiding fragmentation that would otherwise limit batch capacity [@kwon2023vllm]. These techniques represent the intersection of classical systems engineering with modern ML serving challenges.\n\n::: {.callout-note title=\"LLM Serving: Beyond the Fundamentals\"}\n\nLanguage model serving introduces challenges beyond the batching and memory principles established here. The key-value cache that stores attention context scales with sequence length and batch size, often exceeding the model weights themselves in memory consumption. Techniques like speculative decoding use small draft models to propose multiple tokens that the target model verifies in parallel, achieving 2-3Ã— latency reduction for interactive applications. Weight-only quantization (INT4 weights with FP16 activations) proves more effective than activation quantization for memory-bandwidth-bound LLM inference.\n\nThese LLM-specific optimizations build directly on the foundations this chapter establishes: queuing theory governs request scheduling, batching tradeoffs determine throughput-latency curves, and precision selection follows the same accuracy-efficiency principles. The serving fundamentals apply universally; LLM serving adds domain-specific techniques atop this foundation.\n\n:::\n\n### When Not to Batch {#sec-serving-no-batch}\n\nSome scenarios require single-request processing:\n\n- **Ultra-low latency requirements**: When p99 latency must stay under 10ms, any batching delay is unacceptable\n- **Highly variable request sizes**: When inputs vary dramatically in size, batching creates padding overhead that wastes compute\n- **Memory constraints**: When models already consume most GPU memory, batch activations may cause out-of-memory errors\n\n### Session Affinity Constraints {#sec-serving-session-affinity}\n\nWhen requests from the same user or session should route to the same replica, batching becomes constrained. Session affinity (also called sticky sessions) matters for three main reasons:\n\n**KV-Cache Reuse**: For conversational AI, the key-value cache from previous turns dramatically speeds up multi-turn conversations. Routing a follow-up request to a different replica forfeits this cached context, increasing latency by 2-5x for long conversations.\n\n**User-Specific Models**: Some systems serve personalized models or adapters per user. Routing requests to the replica that has already loaded that user's adapter avoids repeated loading overhead.\n\n**Stateful Preprocessing**: When preprocessing maintains state (tokenizer caches, session-specific normalization), routing to a different replica requires rebuilding this state.\n\nThe tension with batching is clear: strict affinity constrains which requests can be batched together, potentially reducing batch sizes and GPU utilization. Production systems often implement \"soft affinity\" where requests prefer their assigned replica but can overflow to others when that replica is overloaded. This preserves most affinity benefits while maintaining load balance.\n\n### Traffic Patterns and Batching Strategy {#sec-serving-traffic-patterns}\n\nThe optimal batching strategy depends critically on how requests arrive. Different deployment contexts exhibit fundamentally different arrival patterns, each requiring distinct batching approaches. The MLPerf inference benchmark codifies these patterns into four scenarios that directly map to real-world deployments (see @sec-benchmarking-ai for MLPerf details).\n\n**Server Traffic (Poisson Arrivals)**\n\nCloud APIs and web services typically receive requests following a Poisson process, where arrivals are independent and uniformly distributed over time. For Poisson arrivals with rate $\\lambda$ and batching window $T$, the expected batch size follows @eq-poisson-batch:\n\n$$E[\\text{batch size}] = \\lambda \\cdot T$$ {#eq-poisson-batch}\n\nThe variance equals the mean (a property of Poisson distributions), so batch sizes fluctuate significantly at moderate traffic. With $\\lambda = 200$ requests/second and $T = 10$ms, expected batch size is 2, but 16% of windows will have zero requests (wasted compute cycles) while others may have 4 or more.\n\nThe optimal batching window balances waiting cost against throughput benefit. As shown in @eq-optimal-window:\n\n$$T_{\\text{optimal}} = \\min\\left(L - S, \\sqrt{\\frac{S}{\\lambda}}\\right)$$ {#eq-optimal-window}\n\nwhere $L$ is the latency SLO and $S$ is the service time. A counterintuitive result emerges from this equation: as traffic increases, the optimal window decreases while achieved batch sizes still grow. @tbl-traffic-adaptive demonstrates this relationship across traffic levels.\n\n| Arrival Rate | Optimal Window | Avg Batch Size | p99 Latency |\n|:-------------|:---------------|:---------------|:------------|\n| 100 QPS | 20ms | 2.0 | 45ms |\n| 500 QPS | 8ms | 4.0 | 42ms |\n| 1,000 QPS | 5ms | 5.0 | 38ms |\n| 5,000 QPS | 2ms | 10.0 | 35ms |\n\n: **Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time. {#tbl-traffic-adaptive}\n\n**Streaming Traffic (Correlated Arrivals)**\n\nAutonomous vehicles, video analytics, and robotics systems receive inputs from multiple synchronized sensors. Rather than independent arrivals, frames from all cameras for a given timestamp must be processed together as a batch.\n\n::: {.callout-note title=\"ðŸ”„ Multi-Camera Autonomous Vehicle Serving\"}\n\nConsider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial fusion:\n\n**Timeline for processing frame set N:**\n\n| Time | Event |\n|:-----|:------|\n| T = 0ms | Cameras begin capturing frame N |\n| T = 8ms | Camera 1 frame arrives |\n| T = 10ms | Cameras 2-5 frames arrive |\n| T = 15ms | Camera 6 arrives (jitter) |\n| T = 15ms | Batch inference begins (6 images) |\n| T = 25ms | Inference complete |\n| T = 32ms | Result ready for planning module |\n\n**Key constraints:**\n\n- Hard deadline: 33ms per frame set (real-time requirement)\n- Batch size: Fixed at 6 (one per camera)\n- Synchronization budget: 12ms of 33ms total (36% for jitter tolerance)\n- Timeout policy: If camera frame not received by T+20ms, use previous frame\n\nUnlike Poisson traffic where dynamic batching optimizes throughput, streaming traffic requires synchronization policies that handle sensor jitter while meeting hard deadlines.\n\n:::\n\n**Single-User Traffic (Sequential Arrivals)**\n\nMobile and embedded applications serve one user at a time, with requests arriving only after the previous result is consumed. Batch size is always 1, eliminating batching optimization entirely but raising different challenges.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Mobile Single-User Serving (Pixel 6 NPU)\"}\n\n| Phase | Duration | Notes |\n|:------|:---------|:------|\n| Camera buffer read | 8ms | System API overhead |\n| JPEG decode (CPU) | 15ms | Single-threaded |\n| Resize + Normalize | 5ms | CPU preprocessing |\n| NPU inference | 12ms | 82% NPU utilization |\n| Post-process + UI | 5ms | Result rendering |\n| **Total** | **45ms** | Perceived as \"instant\" |\n\n**Critical insight**: Even at batch size 1, the mobile NPU achieves 82% utilization because its compute capacity matches single-image workloads. This differs fundamentally from datacenter GPUs, which achieve only 15% utilization at batch size 1 because their massive parallelism requires larger batches to saturate.\n\nMobile serving optimization focuses on preprocessing efficiency and power management rather than batching strategies.\n\n:::\n\n@tbl-traffic-patterns-summary maps MLPerf scenarios to deployment contexts and appropriate batching strategies.\n\n+------------------+---------------------+------------------+---------------------------+\n| **MLPerf**       | **Deployment**      | **Batch**        | **Optimization**          |\n| **Scenario**     | **Context**         | **Strategy**     | **Focus**                 |\n+:=================+:====================+:=================+:==========================+\n| **Server**       | Cloud APIs,         | Dynamic batching | Window tuning,            |\n|                  | web services        | with timeout     | utilization-latency curve |\n+------------------+---------------------+------------------+---------------------------+\n| **MultiStream**  | Autonomous driving, | Synchronized     | Jitter handling,          |\n|                  | video analytics     | sensor fusion    | deadline guarantees       |\n+------------------+---------------------+------------------+---------------------------+\n| **SingleStream** | Mobile apps,        | No batching      | Preprocessing,            |\n|                  | embedded devices    | (batch=1)        | power efficiency          |\n+------------------+---------------------+------------------+---------------------------+\n| **Offline**      | Batch processing,   | Maximum batch    | Throughput,               |\n|                  | data pipelines      | size             | hardware utilization      |\n+------------------+---------------------+------------------+---------------------------+\n\n: **Traffic Patterns and Batching Strategies**: The MLPerf inference scenarios map to distinct deployment contexts, each requiring different batching approaches and optimization priorities. {#tbl-traffic-patterns-summary}\n\n## Postprocessing {#sec-serving-postprocessing}\n\nBatching determines how inputs flow through inference, but the journey does not end when the model produces output tensors. Model outputs are arrays of floating-point numbers. Users need predictions: labels, probabilities, generated text, structured data. Postprocessing bridges this gap, transforming raw model outputs into responses that applications can consume. While often overlooked in system design, postprocessing significantly impacts both latency and the usefulness of predictions.\n\n### From Logits to Predictions {#sec-serving-logits}\n\nClassification models output logits or probabilities across classes. Converting these to predictions involves several potential steps:\n\n- **Argmax selection**: Choosing the highest-probability class\n- **Thresholding**: Applying confidence thresholds before returning predictions\n- **Top-k extraction**: Returning multiple high-probability classes with scores\n- **Calibration**: Adjusting raw probabilities to better reflect true likelihoods\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Postprocessing Pipeline\"}\n\nFor ResNet-50 image classification, typical postprocessing includes:\n\n```python\n# Raw model output: logits tensor of shape (batch_size, 1000)\nprobs = torch.softmax(logits, dim=-1)  # 0.05ms\ntop5_probs, top5_indices = probs.topk(5)  # 0.02ms\nlabels = [IMAGENET_CLASSES[i] for i in top5_indices]  # 0.01ms\n\n# Response formatting\nresponse = {\n    \"predictions\": [\n        {\"label\": label, \"confidence\": float(prob)}\n        for label, prob in zip(labels, top5_probs)\n    ],\n    \"model_version\": \"resnet50-v2.1\",\n    \"inference_time_ms\": 5.2,\n}\n```\n\n**Total postprocessing time**: ~0.1ms (negligible compared to preprocessing and inference)\n\n:::\n\nEach step adds latency but improves response utility. Calibration in particular can add significant computation but is essential when downstream systems make decisions based on confidence scores.\n\n### Generation and Decoding {#sec-serving-decoding}\n\nGenerative models require decoding strategies that trade off quality, diversity, and latency. The choice of decoding strategy can dramatically affect both output quality and computational cost.\n\n**Greedy decoding** selects the highest-probability token at each step. Fast but often produces repetitive, low-quality outputs because it cannot recover from early mistakes.\n\n**Beam search** maintains multiple candidate sequences, selecting the highest-scoring complete sequence. Produces higher-quality outputs but multiplies computation by the beam width.\n\n**Sampling** with temperature, top-k, and top-p parameters introduces randomness for diversity [@holtzman2020curious]. Temperature scales logits before softmax; top-k limits sampling to the k highest-probability tokens; top-p (nucleus sampling) limits sampling to tokens comprising probability mass p.\n\nThe choice involves latency tradeoffs [@meister2020beam]. Beam search with width 5 takes roughly 5Ã— the compute of greedy decoding. Sampling adds minimal overhead but requires careful parameter tuning to balance quality and coherence.\n\n### Output Formatting and Streaming {#sec-serving-output-format}\n\nProduction systems rarely return raw predictions. Outputs must conform to API contracts, often requiring:\n\n- JSON serialization with specific schema\n- Confidence score formatting and thresholding\n- Error handling for edge cases (no confident prediction, out-of-distribution input)\n- Metadata attachment (model version, inference time, feature attributions)\n\nStreaming responses for generative models add complexity. Rather than waiting for complete generation, systems return tokens as they are produced. This improves perceived latency (users see output beginning quickly) but requires infrastructure support for chunked responses and client-side incremental rendering.\n\n## Inference Runtime Selection {#sec-serving-runtimes}\n\nThe preprocessing, inference, and postprocessing pipeline we have examined runs on an inference runtime that significantly impacts latency, throughput, and operational complexity. Choosing appropriately requires understanding the tradeoffs between framework-native serving, general-purpose optimization, and specialized inference engines. This decision often determines whether a model can meet its latency SLOs.\n\n### Framework-Native Serving {#sec-serving-framework-native}\n\nPyTorch and TensorFlow models can serve directly using their native runtimes. This approach maximizes compatibility (any model that trains will serve) and simplifies the deployment pipeline (no export or conversion step). However, framework runtimes include training functionality that adds overhead, and default execution paths may not exploit hardware-specific optimizations.\n\nTorchScript and TensorFlow SavedModel formats enable ahead-of-time compilation and graph optimization, improving over eager execution while maintaining framework compatibility. These formats represent the first step toward deployment optimization without abandoning the familiar framework ecosystem.\n\n### General-Purpose Optimization {#sec-serving-onnx}\n\nONNX Runtime provides a hardware-agnostic optimization layer [@onnxruntime2024]. Models export to ONNX format, then ONNX Runtime applies graph optimizations and selects execution providers for the target hardware. This enables single-format deployment across CPUs, GPUs, and specialized accelerators.\n\n### Specialized Inference Engines {#sec-serving-specialized}\n\nTensorRT (NVIDIA GPUs), OpenVINO (Intel hardware), and similar engines optimize specifically for their target hardware [@nvidia2024tensorrt; @chen2018tvm]. They apply aggressive optimizations: layer fusion, precision calibration, kernel auto-tuning. These typically achieve 2-5Ã— speedup over framework-native serving but require explicit export and may not support all operations.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Runtime Comparison\"}\n\nPerformance comparison for ResNet-50 inference on V100 GPU (batch size 1):\n\n| Runtime | Latency | Speedup | Notes |\n|---------|---------|---------|-------|\n| PyTorch (eager) | 8.5ms | 1.0Ã— | Baseline, no optimization |\n| TorchScript | 6.2ms | 1.4Ã— | JIT compilation |\n| ONNX Runtime | 5.1ms | 1.7Ã— | Cross-platform |\n| TensorRT FP32 | 2.8ms | 3.0Ã— | NVIDIA-specific |\n| TensorRT FP16 | 1.4ms | 6.1Ã— | Tensor Core acceleration |\n| TensorRT INT8 | 0.9ms | 9.4Ã— | Requires calibration |\n\n**Key insight**: The 9.4Ã— speedup from TensorRT INT8 comes at the cost of: (1) quantization calibration data, (2) potential accuracy loss (<1% for ResNet-50), and (3) NVIDIA-specific deployment.\n\n:::\n\nThe optimization-compatibility tradeoff is fundamental. More aggressive optimization yields better performance but increases deployment complexity and may introduce numerical differences from training. The choice depends on latency requirements, deployment constraints, and available engineering resources.\n\n### Runtime Configuration {#sec-serving-runtime-config}\n\nBeyond runtime selection, configuration choices impact serving performance:\n\n- **Thread pools**: Controlling parallelism for CPU inference\n- **Memory allocation**: Pre-allocating buffers vs dynamic allocation\n- **Execution providers**: Selecting and prioritizing hardware backends\n- **Graph optimization level**: Trading compilation time for runtime performance\n\nProduction deployments typically require experimentation to find optimal configurations for specific models and hardware combinations. A systematic approach tests key parameters and measures their impact on latency distributions.\n\n### Precision Selection for Serving {#sec-serving-precision}\n\nNumerical precision directly trades accuracy for throughput, connecting to the quantization techniques covered in @sec-model-optimizations. While @sec-model-optimizations focuses on training-time quantization, serving introduces additional considerations: calibration requirements, layer sensitivity, and dynamic precision selection.\n\n**Precision-Throughput Relationship**\n\nFor memory-bandwidth-bound operations, reducing precision proportionally increases throughput by reducing data movement. The theoretical maximum speedup from precision reduction follows @eq-precision-throughput:\n\n$$\\frac{\\text{Throughput}_{\\text{INT8}}}{\\text{Throughput}_{\\text{FP32}}} = \\frac{32}{8} = 4\\times \\text{ (theoretical maximum)}$$ {#eq-precision-throughput}\n\nIn practice, GPU compute pipelines and Tensor Core alignment requirements limit achieved speedup to 2.5-3.5Ã— for INT8 versus FP32. Tensor Cores require specific alignment: INT8 operations need tensor dimensions divisible by 16, while FP16 requires divisibility by 8 (see @sec-ai-acceleration for Tensor Core architecture details).\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Precision Tradeoffs on V100\"}\n\n| Precision | Latency | Memory | Accuracy | Tensor Core Util. | Calibration |\n|:----------|:--------|:-------|:---------|:------------------|:------------|\n| FP32 | 2.8ms | 98MB | 76.13% | 0% | None |\n| FP16 | 1.4ms | 49MB | 76.13% | 85% | None |\n| INT8 (PTQ) | 0.9ms | 25MB | 75.80% | 92% | 1,000 samples |\n| INT8 (QAT) | 0.9ms | 25MB | 76.05% | 92% | Full retraining |\n\n**Key observations:**\n\n- INT8 achieves 3.1Ã— speedup but loses 0.33% accuracy with post-training quantization (PTQ)\n- Quantization-aware training (QAT) recovers most accuracy but requires retraining\n- FP16 provides 2Ã— speedup with no accuracy loss for most models\n\n:::\n\n**Layer Sensitivity**\n\nNot all layers tolerate reduced precision equally. Quantization error for a layer scales with weight magnitude and gradient sensitivity, as captured by @eq-quant-error:\n\n$$\\epsilon_{\\text{quant}} \\propto \\alpha \\cdot \\|W\\|_2 \\cdot 2^{-b}$$ {#eq-quant-error}\n\nwhere $\\alpha$ is a layer-specific sensitivity coefficient, $\\|W\\|_2$ is the weight L2 norm, and $b$ is the bit width. This explains observed patterns:\n\n- **First convolutional layers** (high gradients, large $\\alpha$): Precision-sensitive, often kept at FP16\n- **Middle layers** (stable gradients, low $\\alpha$): Tolerate INT8 well\n- **Final classification layers** (small weights but high task sensitivity): Benefit from FP16+\n\n**Calibration Requirements**\n\nPost-training quantization requires a calibration dataset to determine optimal scale factors for INT8 conversion. Production experience shows that calibration data must be representative of actual serving traffic, not just training data. Using ImageNet validation images to calibrate a model serving wildlife camera images resulted in 3.2% accuracy degradation in one production system.\n\n**Dynamic Precision Selection**\n\nAdvanced serving systems select precision per request based on runtime conditions:\n\n- If ahead of latency SLO, use higher precision for better accuracy\n- For low-confidence INT8 results, recompute at FP16\n- Different customer tiers may receive different precision levels\n\nThis pattern enables adaptive quality-latency tradeoffs while maximizing throughput during normal operation.\n\n## Cost and Capacity Planning {#sec-serving-cost}\n\nThe runtime and precision choices examined in previous sections determine per-inference performance, but production deployment requires translating these choices into infrastructure decisions. Serving costs scale with request volume, unlike training costs that scale with dataset size and model complexity [@zhang2019mark]. Understanding cost structure enables informed infrastructure decisions that balance performance requirements against budget constraints.\n\n### Cost Per Inference {#sec-serving-cost-per-inference}\n\nTotal serving cost decomposes into several components:\n\n- **Compute**: GPU/CPU time per inference\n- **Memory**: Accelerator memory required to hold model and activations\n- **Data transfer**: Network bandwidth for request/response payloads\n- **Orchestration overhead**: Container runtime, load balancing, monitoring\n\nFor GPU inference, compute time dominates when utilization is high. When utilization is low, memory cost dominates because the GPU is reserved but idle. This creates a utilization imperative: serving infrastructure should maximize GPU utilization through batching, multi-model serving, or right-sized instance selection.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Cost Analysis\"}\n\nConsider serving ResNet-50 on AWS infrastructure:\n\n| Instance Type | Cost/Hour | Throughput | Cost per 1M Images |\n|---------------|-----------|------------|-------------------|\n| c5.xlarge (CPU) | $0.17 | 50 img/s | $0.94 |\n| g4dn.xlarge (T4 GPU) | $0.53 | 400 img/s | $0.37 |\n| p3.2xlarge (V100 GPU) | $3.06 | 1,200 img/s | $0.71 |\n\n**Key insight**: The T4 GPU instance achieves the lowest cost per inference despite higher hourly cost, because GPU throughput dramatically exceeds CPU throughput. The V100 is only cost-effective at very high sustained traffic where its higher throughput justifies the 6Ã— price increase.\n\n:::\n\n### GPU vs CPU Economics {#sec-serving-gpu-cpu}\n\nGPUs provide orders-of-magnitude speedup for parallel operations but cost significantly more per hour [@wu2019machine]. The crossover point depends on model characteristics and latency requirements.\n\nCPU inference makes economic sense when:\n\n- Models are small (few parameters, simple operations)\n- Latency requirements are relaxed (hundreds of milliseconds acceptable)\n- Request volume is low or highly variable\n- Models use operations that do not parallelize well\n\nGPU inference makes economic sense when:\n\n- Models are large with parallel-friendly operations\n- Latency requirements are strict (tens of milliseconds)\n- Request volume is high and consistent\n- Batching can achieve high utilization\n\n**Scaling Responsiveness**: Beyond steady-state costs, startup time affects scaling economics. CPU instances typically start in 30-60 seconds while GPU instances take 2-5 minutes (including driver initialization, model loading, and warmup). For variable traffic patterns, this startup latency can be more important than cost per inference. If traffic spikes arrive faster than GPU instances can scale, latency SLOs will be violated despite having sufficient eventual capacity.\n\nThis asymmetry suggests different scaling strategies: CPU instances enable reactive scaling (responding to current demand) while GPU instances often require predictive scaling (provisioning based on anticipated demand). For bursty workloads, a hybrid approach uses always-on GPU capacity for baseline load plus CPU overflow capacity for spikes, trading higher per-inference cost during spikes for better responsiveness.\n\n### Capacity Planning {#sec-serving-capacity-planning}\n\nThe GPU versus CPU decision establishes the cost per inference, but determining how much infrastructure to provision requires combining cost analysis with the queuing theory foundations from @sec-serving-queuing. Capacity planning translates latency requirements and traffic projections into infrastructure specifications. Key inputs include:\n\n- **Traffic patterns**: Peak request rate, daily/weekly cycles, growth projections\n- **Latency SLOs**: p50, p95, p99 targets\n- **Model characteristics**: Inference time distribution at various batch sizes\n\nFrom these inputs, queuing theory determines required capacity [@harchol2013performance]. The equations developed in @sec-serving-queuing provide the mathematical foundation: @eq-mm1-wait shows how latency scales with utilization, while @eq-p99-latency enables calculating p99 latency for capacity planning.\n\nThe relationship between utilization and latency is nonlinear (@tbl-utilization-latency): at 70% utilization, p99 latency is approximately 12Ã— service time; at 90% utilization, it reaches 33Ã— service time. This nonlinearity explains why systems that seem healthy (low average latency) can suddenly violate SLOs when traffic increases modestly.\n\nThe worked example in @sec-serving-queuing demonstrates the complete capacity planning process: starting from a 50ms p99 SLO and 5,000 QPS target, deriving the safe utilization threshold (72%), calculating required service rate (6,944 QPS), and determining GPU count with headroom (10 V100s). Production systems typically provision for peak load plus 30% headroom, using auto-scaling to reduce costs during low-traffic periods while maintaining latency guarantees during peaks.\n\n## Fallacies and Pitfalls {#sec-serving-fallacies-pitfalls}\n\nThe principles established throughout this chapter provide a systematic framework for designing serving systems that meet latency requirements while maximizing efficiency. However, practitioners frequently encounter misconceptions that lead to suboptimal designs or production failures. These fallacies and pitfalls emerge from the fundamental differences between training and serving that make intuitions from one domain misleading in the other.\n\n**Fallacy:** _Faster model inference automatically means faster end-to-end serving._\n\nThis misconception leads teams to focus optimization efforts exclusively on model inference while ignoring preprocessing and postprocessing overhead. As demonstrated in @sec-serving-latency-budget, preprocessing often consumes 45-70% of total latency when inference runs on optimized accelerators. A team that reduces inference time from 5ms to 2ms through quantization achieves only 15% improvement in total latency if preprocessing remains at 8ms. Effective optimization requires profiling the complete request path and allocating engineering effort proportionally to where time is actually spent.\n\n**Pitfall:** _Running serving infrastructure at high utilization to maximize cost efficiency._\n\nThe nonlinear relationship between utilization and latency (@eq-mm1-wait) makes high utilization dangerous for latency-sensitive systems. At 90% utilization, average wait time reaches 10Ã— service time, and p99 latency becomes unacceptable for most SLOs. Teams that provision for average load rather than peak load find their systems violate latency guarantees precisely when traffic increases, the moment when reliable performance matters most. Production systems typically target 60-70% utilization at peak load to maintain latency headroom.\n\n**Fallacy:** _Training accuracy guarantees serving accuracy._\n\nTraining-serving skew (@sec-serving-skew) causes models to behave differently in production despite using identical weights. Differences in preprocessing libraries, numerical precision, feature computation timing, or input distribution silently degrade accuracy without triggering obvious errors. A model achieving 95% accuracy in training evaluation might drop to 90% in production due to subtle preprocessing differences that shift inputs outside the training distribution. Prevention requires either identical preprocessing code paths or rigorous statistical monitoring of input distributions.\n\n**Pitfall:** _Using average latency to evaluate serving system performance._\n\nAverage latency hides the experience of the slowest requests, which often determine user satisfaction and SLO compliance. A system with 10ms average latency might have 200ms p99 latency, meaning 1% of users experience 20Ã— worse performance. At scale with fan-out amplification (@sec-serving-tail-latency), even rare slow responses become visible to most users. Production SLOs specify percentile targets (p95, p99) precisely because averages mask unacceptable tail behavior.\n\n**Fallacy:** _Larger batch sizes always improve throughput._\n\nWhile batching amortizes fixed costs and improves GPU utilization, it also increases per-request latency and can cause memory exhaustion. Beyond a certain point, larger batches provide diminishing throughput returns while latency continues to grow linearly. Additionally, batch sizes that exceed GPU memory cause out-of-memory failures, and highly variable input sizes create padding overhead that wastes compute. The optimal batch size depends on latency SLOs, memory constraints, and traffic patterns, not just throughput maximization.\n\n**Pitfall:** _Calibrating quantized models with training data rather than production traffic._\n\nPost-training quantization requires calibration data to determine optimal scale factors, but using training data assumes production inputs match the training distribution. When production traffic differs (different image sources, lighting conditions, user behavior), calibration on training data produces suboptimal scale factors that degrade accuracy. One production system experienced 3.2% accuracy loss when calibrating with ImageNet validation images but serving wildlife camera images. Effective calibration requires representative samples of actual serving traffic.\n\n**Fallacy:** _Cold start latency only matters for the first request._\n\nCold start affects any request that arrives after a period of inactivity, after model updates, or when auto-scaling adds new instances. In systems with bursty traffic or multiple model versions, cold starts can affect a significant fraction of requests. A model requiring 500ms to load impacts not just the first user but every user who triggers a scale-up event or model reload. Production systems mitigate cold start through model preloading, keep-alive mechanisms, and gradual traffic shifting during deployments.\n\n## Summary {#sec-serving-summary}\n\nServing represents the critical transition from model development to production deployment, where the optimization priorities that governed training must be fundamentally inverted. Throughout this chapter, we have seen how the shift from throughput maximization to latency minimization transforms every system design decision. The queuing theory foundations established here reveal why this inversion is not merely a change in metrics but a change in the governing mathematics: the nonlinear relationship between utilization and latency means that systems behaving well at moderate load can suddenly violate SLOs when traffic increases modestly. Little's Law and the M/M/1 wait time equations provide the quantitative foundation for capacity planning that replaces intuition-based provisioning with engineering rigor.\n\nOur exploration of the latency budget demonstrates that effective serving optimization requires understanding the complete request path rather than focusing exclusively on model inference. Preprocessing often consumes 45-70% of total latency when inference runs on optimized accelerators, yet engineering effort frequently targets the wrong bottleneck. The microsecond-scale overheads identified by Barroso, Patterson, and colleagues explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization matters as much as model optimization. Training-serving skew represents another dimension of this complexity, silently degrading accuracy when preprocessing logic differs between training and production environments in ways that traditional testing cannot detect.\n\nThe traffic pattern analysis reveals how deployment context fundamentally shapes batching strategy and system design. Server workloads with Poisson arrivals optimize dynamic batching windows, autonomous vehicles with streaming sensor data require synchronized batch formation, and mobile applications with single-user patterns eliminate batching entirely. The MLPerf scenarios codify these patterns for standardized benchmarking, connecting the serving principles established here to the measurement frameworks explored in @sec-benchmarking-ai. Precision selection and runtime optimization extend the quantization techniques from @sec-model-optimizations and Tensor Core capabilities from @sec-ai-acceleration into the serving domain, where calibration with representative production traffic becomes essential.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Serving inverts training priorities: latency per request matters more than aggregate throughput, requiring fundamentally different system design\n* Queuing theory provides the mathematical foundation for capacity planning, with the utilization-latency relationship explaining why systems degrade nonlinearly under load\n* Preprocessing often dominates total latency (45-70%), making pipeline optimization as important as model optimization\n* Traffic patterns (Poisson, streaming, single-user) determine optimal batching strategy, directly mapping to MLPerf inference scenarios\n* Precision selection and resolution tradeoffs connect serving decisions to optimization techniques from earlier chapters\n* Training-serving skew silently degrades accuracy and requires identical preprocessing code or rigorous monitoring to prevent\n:::\n\nThe serving foundations established in this chapter provide the infrastructure for the operational deployment strategies explored in @sec-ml-operations. While this chapter focused on the mechanics of transforming requests into predictions efficiently, production environments introduce additional complexities of monitoring, versioning, and continuous validation that characterize real-world ML system deployment. The single-machine serving principles developed here also prepare practitioners for understanding when and why scaling to multiple machines becomes necessary, with distributed serving patterns including load balancing across nodes and model sharding for models exceeding single-machine memory covered in @sec-inference-at-scale.\n\n<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->\n::: { .quiz-end }\n:::\n","srcMarkdownNoYaml":"\n\n# Serving {#sec-serving}\n\n::: {layout-narrow}\n::: {.column-margin}\n*DALLÂ·E 3 Prompt: An illustration of a machine learning serving system, depicted as a high-speed assembly line in a modern factory. On one end, raw data (images, text documents, sensor readings) enters on conveyor belts. In the center, a glowing neural network processes inputs through preprocessing stations, a central inference engine, and postprocessing units. Workers monitor latency gauges and throughput meters. Some requests wait in small batches while others stream through individually. The scene conveys speed, precision, and the transformation from raw input to useful predictions, with visible clocks emphasizing the time-critical nature of serving.*\n:::\n\n\\noindent\n![](images/png/cover_serving.png){width=80%}\n\n:::\n\n## Purpose {.unnumbered}\n\n_How do the system requirements for serving trained models differ from training them, and what principles govern the design of responsive prediction systems?_\n\nTraining and serving are fundamentally different computational paradigms requiring distinct system designs. Training optimizes throughput over days or weeks of computation while serving inverts this priority, optimizing latency per request under strict time constraints measured in milliseconds. A common misconception is that faster hardware automatically means faster serving, but in practice preprocessing and postprocessing often dominate latency: production systems report preprocessing consuming 60-70% of total request time when inference runs on optimized accelerators. Understanding where latency actually comes from requires mastering queuing theory fundamentals that explain why systems degrade nonlinearly under load, recognizing how traffic patterns (Poisson arrivals for servers, streaming for autonomous vehicles, single-user for mobile) determine batching strategy, and connecting serving decisions to prior chapters on quantization, hardware acceleration, and benchmarking. The principles established here for single-machine serving provide the foundation for understanding when and why scaling to multiple machines becomes necessary\n\n::: {.callout-tip title=\"Learning Objectives\"}\n\n- Explain how serving systems invert training priorities by optimizing for per-request latency and percentile metrics rather than aggregate throughput\n\n- Decompose request latency into preprocessing, inference, and postprocessing phases to identify optimization bottlenecks\n\n- Apply queuing theory (Little's Law, M/M/1 model) to predict latency under load, analyze the utilization-latency relationship, and perform capacity planning to meet SLOs\n\n- Identify and prevent training-serving skew through consistent preprocessing implementations, statistical validation, and production monitoring\n\n- Select appropriate batching strategies (dynamic batching, continuous batching, no batching) based on traffic patterns and deployment contexts\n\n- Evaluate tradeoffs in runtime selection (framework-native, ONNX, TensorRT), precision configuration (FP32, FP16, INT8), and model loading strategies to meet deployment constraints\n\n- Calculate cost per inference across compute options (GPU vs CPU) and apply capacity planning principles to provision infrastructure\n\n:::\n\n## From Training to Production {#sec-serving-training-to-production}\n\nThe journey from a trained model to a production system crosses a fundamental boundary. Previous chapters established how to train models efficiently (@sec-ai-training), optimize them for deployment (@sec-model-optimizations), and accelerate their execution on specialized hardware (@sec-ai-acceleration). Serving introduces a new dimension that transforms every prior decision: real-time responsiveness under unpredictable load. Where training optimizes for samples processed per hour over days of computation, serving must deliver predictions within milliseconds while handling request patterns that no training process could anticipate.\n\nThis inversion of priorities has profound consequences. The benchmarking techniques from @sec-benchmarking-ai now target percentile latencies rather than aggregate throughput. The quantization methods from @sec-model-optimizations must be validated not just for accuracy preservation but for calibration with production traffic. The hardware acceleration from @sec-ai-acceleration must be configured for single-request responsiveness rather than batch throughput. Understanding these connections enables practitioners to apply earlier optimizations correctly in the serving context.\n\nThe serving systems examined here focus on single-machine deployment, establishing the foundational principles that govern all inference systems. When these single-machine foundations prove insufficient, @sec-inference-at-scale examines distributed serving patterns including load balancing across nodes and model sharding for models exceeding single-machine memory.\n\n::: {.callout-note title=\"Lighthouse Example: Serving a ResNet-50 Image Classifier\"}\n\nThis chapter uses **serving a ResNet-50 image classification model** as a consistent reference point to ground abstract concepts in concrete reality. ResNet-50 represents an ideal teaching example because it:\n\n- **Spans common deployment scenarios**: Used in everything from mobile apps to cloud APIs\n- **Has well-documented performance characteristics**: 25.6M parameters, ~4 GFLOPS per inference, extensively benchmarked\n- **Exhibits all key serving challenges**: Preprocessing overhead (image decoding, resizing), batching tradeoffs, memory management\n- **Represents production ML systems**: Image classification remains one of the most widely deployed ML applications\n\n**Key ResNet-50 Serving Specifications:**\n\n- **Parameters**: 25.6 million (98MB FP32, 49MB FP16, 25MB INT8)\n- **Input**: 224Ã—224Ã—3 RGB images (150KB typical JPEG, 588KB uncompressed tensor)\n- **Inference Time**: ~5ms on V100 GPU (batch=1), ~1ms per image at batch=32\n- **Preprocessing**: JPEG decode (~3ms), resize (~1ms), normalize (~0.5ms)\n- **Memory Footprint**: ~400MB GPU memory including activations\n\n**ðŸ”„ ResNet-50 Example Markers** appear at strategic points where this specific model illuminates the concept under discussion. Each example provides quantitative specifications and concrete implementation decisions encountered when serving this model.\n\n:::\n\n## Serving System Fundamentals {#sec-serving-fundamentals}\n\nServing systems occupy a unique position in the machine learning lifecycle, operating under constraints that differ fundamentally from both training and batch processing. Before examining specific optimization techniques, we must establish the conceptual framework that distinguishes these systems from other computational workloads.\n\n::: {.callout-definition title=\"Model Serving\"}\n\n***Model Serving*** is the process of exposing trained machine learning models for _real-time prediction_, requiring systems that transform raw inputs into useful outputs while meeting _latency constraints_, maintaining _consistency_ with training behavior, and achieving _cost-effective resource utilization_.\n\n:::\n\nThe defining characteristic of serving systems is their request-driven nature. Unlike training, where the system controls when and how data flows through the model, serving systems must respond to external requests that arrive unpredictably. This fundamental difference shapes every design decision, from memory management to error handling.\n\n### Static vs Dynamic Inference {#sec-serving-static-dynamic}\n\nThe first architectural decision in any serving system is whether predictions happen before or during user requests [@google2024staticdynamic]. This choice fundamentally shapes system design, cost structure, and capability boundaries.\n\n**Static inference** (also called offline or batch inference) pre-computes predictions for anticipated inputs and stores them for retrieval. Consider a recommendation system that generates predictions for all user-item pairs nightly. When a user requests recommendations, the system retrieves pre-computed results from a lookup table rather than running inference. This approach eliminates inference latency entirely since results already exist, enables quality verification before deployment, and dramatically reduces serving costs. However, static inference cannot handle novel inputs that were not anticipated during the batch computation and introduces hours or days of latency when models update.\n\n**Dynamic inference** (also called online or real-time inference) computes predictions on demand when requests arrive. This handles any input, including rare edge cases and novel combinations, and immediately reflects model updates. The cost is strict latency requirements that may force simpler models and more intensive monitoring infrastructure.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Static vs Dynamic Tradeoffs\"}\n\nFor our ResNet-50 image classifier, consider two deployment scenarios:\n\n**Static approach**: A photo organization app pre-classifies all images in a user's library overnight. With 10,000 photos and 5ms inference each, batch processing takes ~50 seconds total. Users see instant classification when browsing their library.\n\n**Dynamic approach**: A content moderation API must classify user-uploaded images in real-time. Each image requires the full preprocessingâ†’inferenceâ†’postprocessing pipeline, with a 100ms latency budget to meet user expectations.\n\nMost production image classification systems use a **hybrid approach**: frequently requested images (popular products, known memes) are pre-classified and cached, while novel uploads trigger dynamic inference. This reduces average latency while maintaining flexibility.\n\n:::\n\nMost production systems combine both approaches. Common queries hit a cache populated by batch inference while uncommon requests trigger dynamic computation. Understanding this spectrum is essential because it determines which subsequent optimization strategies apply. Static inference optimizes for throughput during batch computation and storage efficiency for serving. Dynamic inference optimizes for per-request latency under concurrent load, which requires understanding where time goes within each request.\n\n### The Load Balancer Layer {#sec-serving-load-balancer}\n\nProduction serving systems never expose model servers directly to clients. A load balancer sits between clients and model replicas, providing essential functions that affect request distribution, health monitoring, and deployment strategies.\n\n**Request Distribution**\n\nCommon algorithms for distributing requests across model replicas include:\n\n- **Round-robin**: Simple, even distribution that ignores server load\n- **Least-connections**: Routes to server with fewest active requests; effective for variable latency workloads\n- **Weighted**: Assigns proportional traffic based on server capacity\n- **Consistent hashing**: Routes similar requests to same servers for cache efficiency\n\nFor latency-sensitive ML serving, least-connections often outperforms round-robin because it naturally routes away from slow replicas.\n\n**Health Checking**\n\nLoad balancers continuously probe replica health:\n\n```yaml\n# Typical health check configuration\nhealth_check:\n  interval: 5s\n  timeout: 2s\n  unhealthy_threshold: 3  # Mark unhealthy after 3 failures\n  healthy_threshold: 2    # Mark healthy after 2 successes\n  path: /health           # HTTP endpoint to probe\n```\n\nFor ML systems, health checks must verify not just process liveness but model readiness (weights loaded, warmup complete). A server that responds to health checks but has not loaded the model will produce errors or high latency.\n\n**Connection Management**\n\nLoad balancers manage the connection lifecycle:\n\n- Maintain connection pools to replicas (avoiding TCP handshake per request)\n- Implement retry on failure (with exponential backoff)\n- Circuit breakers: Stop routing to replicas that fail repeatedly\n\n**Gradual Rollout**\n\nLoad balancers enable safe deployment through traffic splitting:\n\n```\nTraffic split during canary deployment:\n  stable-v1: 95%\n  canary-v2: 5%\n\nAfter validation:\n  stable-v1: 50%\n  canary-v2: 50%\n\nFinal promotion:\n  stable-v2: 100%\n```\n\n**Impact on Queuing Analysis**: The load balancer's behavior affects the queuing dynamics analyzed in @sec-serving-queuing. When capacity planning considers \"the server,\" it really means the combined behavior of multiple replicas behind a load balancer.\n\n## The Latency Budget {#sec-serving-latency-budget}\n\nFor dynamic inference systems, the fundamental difference from training lies in optimization objectives. Training optimizes throughput: maximizing samples processed per hour over days of computation. A training job that processes 1000 samples per second is successful regardless of how long any individual sample takes. Serving inverts this priority, optimizing latency per request under strict time constraints. A serving system with 1000ms per-request latency has failed, even if it achieves excellent throughput.\n\nThis mindset shift has concrete implications for system design [@gujarati2020serving]. The metrics that matter change from aggregate throughput to latency distributions. Mean latency tells you little about user experience; p50, p95, and p99 latencies reveal how the system performs across the full range of requests. If your mean latency is 50ms but p99 is 2 seconds, one in a hundred users waits 40 times longer than average. For consumer-facing applications, these tail latencies often determine user satisfaction and retention.[^fn-tail-latency]\n\n[^fn-tail-latency]: **Tail Latency Impact**: Research at Google found that users are more sensitive to latency variance than mean latency. A 100ms increase in p99 latency can reduce revenue by 1% for e-commerce applications. This is why service level objectives (SLOs) typically specify percentile targets rather than averages.\n\n::: {.callout-definition title=\"Latency Budget\"}\n\n***Latency Budget*** is the maximum time allowed for a serving request to complete, decomposed into allocations for _preprocessing_, _inference_, and _postprocessing_ phases. Effective latency budgeting requires understanding where time is consumed and allocating resources accordingly.\n\n:::\n\nEvery serving request decomposes into three phases, each consuming part of the latency budget:\n\n1. **Preprocessing**: Transform raw input (image bytes, text strings) into model-ready tensors\n2. **Inference**: Execute the model computation\n3. **Postprocessing**: Transform model outputs into user-facing responses\n\nA common misconception is that faster hardware automatically means faster serving. In practice, preprocessing and postprocessing often dominate total latency. Studies of production systems show preprocessing consuming 60-70% of total request time when inference runs on optimized accelerators [@nvidia2024tritontutorial]. Optimizing only the inference phase yields diminishing returns when the surrounding pipeline remains bottlenecked on CPU operations.\n\n### Latency Distribution Analysis {#sec-serving-latency-analysis}\n\nUnderstanding where time goes requires instrumenting each phase independently. Consider what happens when our ResNet-50 classifier receives a JPEG image:\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Latency Budget Breakdown\"}\n\nA typical serving request for our ResNet-50 classifier shows the following latency distribution:\n\n| Phase | Operation | Time | Percentage |\n|-------|-----------|------|------------|\n| Preprocessing | JPEG decode | 3.0ms | 30% |\n| Preprocessing | Resize to 224Ã—224 | 1.0ms | 10% |\n| Preprocessing | Normalize (mean/std) | 0.5ms | 5% |\n| Data Transfer | CPUâ†’GPU copy | 0.5ms | 5% |\n| **Inference** | **ResNet-50 forward pass** | **5.0ms** | **50%** |\n| Postprocessing | Softmax + top-5 | 0.1ms | ~0% |\n| **Total** | | **10.1ms** | **100%** |\n\nKey insight: **Preprocessing consumes 45% of latency** despite model inference being the computationally intensive phase. With TensorRT optimization reducing inference to 2ms, preprocessing would dominate at 75%.\n\n:::\n\nThis breakdown reveals why naive optimization efforts often fail. Engineers focus on model optimization (quantization, pruning) because that is where ML expertise applies, but the actual bottleneck is image decoding running on CPU. Moving preprocessing to GPU can reduce total latency by 6x in some pipelines by eliminating CPU-GPU data transfers between stages [@nvidia2024tritontutorial].\n\n**The Killer Microseconds Problem**\n\nBarroso, Patterson, and colleagues identified a critical gap in how systems handle latency at different time scales [@barroso2017attack]. Modern systems efficiently handle nanosecond-scale events (CPU cache access, DRAM reads) through hardware mechanisms like out-of-order execution, and millisecond-scale events (disk I/O, network calls) through software techniques like threading and asynchronous I/O. But microsecond-scale events fall into an uncomfortable middle ground where neither approach works well.\n\nML serving lives squarely in this microsecond regime. Individual inference calls complete in 1-10ms, but the surrounding operations (serialization, memory allocation, network stack processing, encryption) each add microseconds that compound into significant overhead. Google's analysis found that a significant fraction (often 20% or more) of datacenter CPU cycles are consumed by this \"datacenter tax\" rather than useful computation. For serving systems, this means:\n\n- A 2Î¼s network fabric can become 100Î¼s end-to-end through software overhead\n- Context switching costs (5-10Î¼s) can exceed the inference time for small models\n- Memory allocation patterns in preprocessing can add unpredictable microsecond delays\n\nThese overheads explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization (memory pooling, zero-copy data paths, kernel bypass) matters as much as model optimization.\n\nThe latency budget framework provides a systematic approach to optimization. First, measure each phase to identify the true bottleneck. Then allocate engineering effort proportionally to where time is actually spent. Finally, consider architectural changes (GPU preprocessing, batching strategies) that can shift work between phases.\n\n### Resolution and Input Size Tradeoffs {#sec-serving-resolution}\n\nInput resolution dramatically affects both preprocessing and inference latency, but the relationship differs depending on whether the system is compute-bound or memory-bound. Understanding this distinction (covered in depth in @sec-ai-acceleration) is essential for making informed resolution decisions.\n\nFor compute-bound models, throughput scales inversely with resolution squared, as shown in @eq-resolution-throughput:\n\n$$\\frac{T(r_2)}{T(r_1)} = \\left(\\frac{r_1}{r_2}\\right)^2$$ {#eq-resolution-throughput}\n\nDoubling resolution from 224 to 448 theoretically yields 4Ã— slowdown (measured: 3.6Ã— due to fixed overhead amortization). However, at high resolutions, models transition from compute-bound to memory-bound as activation tensors exceed cache capacity. @tbl-resolution-bottleneck illustrates this transition for ResNet-50:\n\n| Resolution | Activation Size | Arith. Intensity | Bottleneck |\n|:-----------|:----------------|:-----------------|:-----------|\n| 224Ã—224 | 12.5MB | 290 FLOPS/byte | Compute |\n| 384Ã—384 | 36.8MB | 168 FLOPS/byte | Transitional |\n| 512Ã—512 | 65.5MB | 95 FLOPS/byte | Memory BW |\n| 640Ã—640 | 102.4MB | 61 FLOPS/byte | Memory BW |\n\n: **Resolution and Compute Bottleneck**: ResNet-50 arithmetic intensity decreases with resolution as activation sizes grow. The V100 crossover point (~125 FLOPS/byte) occurs around 384Ã—384, above which memory bandwidth limits throughput more than compute capacity. {#tbl-resolution-bottleneck}\n\n**Deployment-Specific Resolution Decisions**\n\nDifferent deployment contexts have fundamentally different resolution requirements:\n\n- **Mobile applications**: Lower resolution (224Ã—224) acceptable for object detection in camera viewfinders; latency and battery life dominate\n- **Medical imaging**: High resolution (512Ã—512+) required for diagnostic accuracy; latency requirements relaxed\n- **Autonomous vehicles**: Multiple resolutions for different tasks (low-res for detection, high-res crops for recognition)\n- **Cloud APIs**: Resolution often set by client upload; service must handle range gracefully\n\n**Adaptive Resolution**\n\nProduction systems can select resolution dynamically based on content:\n\n1. Run lightweight classifier at 128Ã—128 to categorize content type\n2. Select task-appropriate resolution: documents at 512Ã—512, landscapes at 224Ã—224, faces at 384Ã—384\n3. Achieve 1.4Ã— throughput improvement with 99.2% accuracy retention versus fixed high resolution\n\nThis pattern trades preprocessing cost (running the lightweight classifier) for inference savings on the main model.\n\n## Queuing Fundamentals {#sec-serving-queuing}\n\nThe latency budget framework explains where time goes within a single request, but production systems must handle many concurrent requests competing for finite resources. Understanding why latency degrades under load requires queuing theory, the mathematical framework that explains how requests wait for service in any system with finite capacity. These principles apply universally, from web servers to ML inference, and they explain the counterintuitive behavior that causes well-provisioned systems to suddenly violate latency SLOs when load increases modestly.\n\n### Little's Law {#sec-serving-littles-law}\n\nThe most fundamental result in queuing theory is Little's Law (@eq-littles-law), which relates three quantities in any stable system:\n\n$$L = \\lambda \\cdot W$$ {#eq-littles-law}\n\nwhere $L$ is the average number of requests in the system, $\\lambda$ is the arrival rate (requests per second), and $W$ is the average time each request spends in the system. This relationship holds regardless of arrival distribution, service time distribution, or scheduling policy.\n\nLittle's Law has immediate practical implications. If your inference service averages 10ms per request ($W = 0.01$s) and you observe 50 concurrent requests in the system on average ($L = 50$), then your arrival rate must be $\\lambda = L/W = 5000$ requests per second. Conversely, if you need to limit concurrent requests to 10 (perhaps due to GPU memory constraints), and your service time is 10ms, you can sustain at most 1000 requests per second.\n\n### The Utilization-Latency Relationship {#sec-serving-utilization-latency}\n\nFor a system with Poisson arrivals and exponential service times (the M/M/1 queue model), the average time in system follows:\n\n$$W = \\frac{1}{\\mu - \\lambda} = \\frac{\\text{service time}}{1 - \\rho}$$ {#eq-mm1-wait}\n\nwhere $\\mu$ is the service rate (requests per second the server can handle), and $\\rho = \\lambda/\\mu$ is the utilization (fraction of time the server is busy).\n\nThis equation reveals why serving systems exhibit nonlinear behavior. At 50% utilization, average wait time is $2\\times$ the service time. At 80% utilization, it is $5\\times$. At 90% utilization, it is $10\\times$. The relationship is hyperbolic: small increases in load near capacity cause disproportionate latency increases.\n\n| Utilization ($\\rho$) | Wait Time Multiple | Example (5ms service) |\n|:---------------------|:-------------------|:----------------------|\n| 50% | 2.0Ã— | 10ms |\n| 70% | 3.3Ã— | 17ms |\n| 80% | 5.0Ã— | 25ms |\n| 90% | 10.0Ã— | 50ms |\n| 95% | 20.0Ã— | 100ms |\n\n: **Utilization-Latency Relationship**: Average wait time as a multiple of service time for an M/M/1 queue. The nonlinear relationship explains why systems that perform well at moderate load can suddenly violate SLOs when traffic increases. {#tbl-utilization-latency}\n\nThe M/M/1 model assumes exponentially distributed service times, but ML inference typically has near-constant service time for fixed batch sizes, making the M/D/1 (deterministic service) model more accurate in practice. We use M/M/1 here because it yields closed-form solutions and produces conservative estimates. For M/D/1 queues, average wait time is approximately half of M/M/1 at the same utilization, which matters for capacity planning: M/M/1 analysis will slightly over-provision, erring on the side of meeting SLOs rather than violating them.[^fn-queuing-models]\n\n[^fn-queuing-models]: **Queuing Model Selection**: For deeper treatment of M/M/c (multi-server) and M/G/1 (general service distribution) queues applicable to ML serving, see Harchol-Balter's *Performance Modeling and Design of Computer Systems* [@harchol2013performance], particularly chapters on server farms and scheduling policies.\n\n### Multi-Server Queuing Reality {#sec-serving-multi-server-queuing}\n\nProduction serving systems deploy multiple replicas behind a load balancer, making M/M/c (c servers) the more applicable model than M/M/1.\n\n**Key insight**: For c servers with utilization $\\rho$ per server, the probability of queuing drops exponentially with c, and average wait time is approximately:\n\n$$W_{M/M/c} \\approx \\frac{W_{M/M/1}}{c} \\quad \\text{(for low utilization)}$$\n\n**Practical Implication**: Four servers at 70% utilization each provide far better tail latency than one server at 70%, despite handling the same total load.\n\n| Configuration | Total Capacity | Avg Wait | p99 Wait |\n|:--------------|:---------------|:---------|:---------|\n| 1 server @ 70% | 1x | 2.3x service | 7x service |\n| 4 servers @ 70% each | 4x | 0.6x service | 2x service |\n\n: **Multi-Server Queuing Benefits**: Horizontal scaling improves tail latency even when adding capacity, because the queuing dynamics fundamentally improve with multiple servers. {#tbl-multi-server-queuing}\n\nThis explains why horizontal scaling improves tail latency even when adding capacity: the queuing dynamics fundamentally improve with multiple servers.\n\n**When M/M/1 Analysis Applies**: Despite multi-server reality, M/M/1 analysis remains useful for understanding fundamental queuing dynamics, worst-case analysis (what if load concentrates on one server?), and capacity planning for individual server sizing.\n\n### Tail Latency {#sec-serving-tail-latency}\n\nProduction SLOs typically specify percentile targets (p95, p99) rather than averages because tail latency determines user experience for the slowest requests [@dean2013tail]. For an M/M/1 queue, the p99 latency follows:\n\n$$W_{p99} \\approx \\text{service time} \\cdot \\left(1 + \\frac{\\ln(100 \\cdot \\rho)}{1 - \\rho}\\right)$$ {#eq-p99-latency}\n\nAt 70% utilization, p99 latency is approximately $12\\times$ the service time, while average latency is only $3.3\\times$. This explains why systems that seem healthy (low average latency) can have unacceptable tail latency: the average hides the experience of the unluckiest requests.\n\n**The Tail at Scale Problem**\n\nDean and Barroso's foundational analysis reveals why tail latency becomes critical in distributed systems [@dean2013tail]. Consider a service where individual servers have p50 latency of 10ms and p99 latency of 50ms (a 5x tail ratio, typical for well-tuned systems). For a single server, only 1% of requests experience the 50ms tail. But when a user request fans out to 100 servers in parallel and waits for all responses:\n\n$$P(\\text{any server slow}) = 1 - (1 - 0.01)^{100} = 0.63$$\n\nSixty-three percent of user requests experience the slow tail. This fan-out amplification is why even well-behaved individual servers create unacceptable tail latency at scale. At 1000 servers, the probability rises to 99.996%, meaning virtually every request hits at least one slow server.\n\n**The Tail at Scale Solution**: Techniques like request hedging (sending redundant requests after a timeout), backup requests, and load balancing away from slow servers directly address this amplification effect. As we will see in @sec-inference-at-scale, these tail-tolerant techniques become essential when scaling to distributed inference systems where fan-out magnifies individual server variance.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Capacity Planning Worked Example\"}\n\nConsider designing a ResNet-50 serving system with these requirements:\n\n- **Target p99 latency**: 50ms\n- **Peak expected traffic**: 5,000 requests per second\n- **Service time** (TensorRT FP16): 5ms\n\n**Step 1: Find safe utilization**\n\nUsing @eq-p99-latency, we need $W_{p99} \\leq 50$ms with 5ms service time. Solving for $\\rho$:\n\n$$5\\text{ms} \\cdot \\left(1 + \\frac{\\ln(100 \\cdot \\rho)}{1 - \\rho}\\right) \\leq 50\\text{ms}$$\n\nThis yields $\\rho \\leq 0.72$ (72% maximum utilization).\n\n**Step 2: Calculate required service rate**\n\n$$\\mu_{\\text{required}} = \\frac{\\lambda}{\\rho_{\\text{safe}}} = \\frac{5000}{0.72} = 6944 \\text{ requests/second}$$\n\n**Step 3: Determine GPU count**\n\nSingle V100 throughput at batch=16: 1,143 images/second\n\n$$\\text{GPUs needed} = \\frac{6944}{1143} = 6.1 \\rightarrow 7 \\text{ GPUs}$$\n\n**Step 4: Add headroom for variance**\n\nProduction systems add 30% headroom for traffic spikes and variance:\n\n$$\\text{Final count} = 7 \\times 1.3 = 9.1 \\rightarrow 10 \\text{ GPUs}$$\n\n**Step 5: Verify fault tolerance (N+1 redundancy)**\n\nThe 30% headroom addresses traffic variance, but production systems also need fault tolerance. With 10 GPUs, losing one leaves 9 GPUs handling 5,000 QPS:\n\n$$\\text{Utilization after failure} = \\frac{5000 / 1143}{9} = 48.6\\%$$\n\nThis remains well below the 72% safe utilization threshold, confirming N+1 redundancy is satisfied. For stricter fault tolerance requirements, N+2 redundancy (tolerating two simultaneous failures) would require 11-12 GPUs.\n\n**Result**: Provision 10 V100 GPUs to serve 5,000 QPS at 50ms p99 latency with N+1 fault tolerance.\n\n:::\n\nThe queuing analysis explains the capacity planning approach mentioned in @sec-serving-capacity-planning and connects to the MLPerf Server scenario from @sec-benchmarking-ai, which measures throughput only for requests meeting the latency SLO. A system achieving 10,000 QPS but violating the SLO on 5% of requests reports only 9,500 valid QPS.\n\n### Tail-Tolerant Techniques {#sec-serving-tail-tolerant}\n\nRather than eliminating all sources of latency variability (often impractical), production systems employ techniques that tolerate variability while still meeting SLOs [@dean2013tail; @dean2012rapid]. These techniques treat latency variance as a fact of life and design around it.\n\n**Hedged Requests**\n\nWhen a request has not completed within the expected time, send a duplicate request to another server. The client uses whichever response arrives first and cancels the other. For ML serving, this means maintaining multiple model replicas and routing slow requests to alternative replicas. The overhead is modest: if you hedge at the 95th percentile, only 5% of requests generate duplicates, increasing load by just 5% while dramatically reducing tail latency.\n\n**Cancellation Complexity**: A critical implementation detail is that CUDA kernels cannot be interrupted mid-execution. When a hedged request completes, the duplicate must be cancelled, but if inference has already begun on the GPU, cancellation approaches include: (1) checking a cancellation flag before launching inference, (2) accepting wasted compute for the in-flight kernel, or (3) using request prioritization to deprioritize the duplicate. Since hedging typically applies only to the slowest 5% of requests, the overhead from occasional wasted compute remains acceptable.\n\n**Tied Requests**\n\nSend the request to multiple servers simultaneously, but include a tag allowing servers to cancel execution once another server begins processing. This eliminates the delay of waiting to detect a slow response before hedging. For inference servers with significant startup overhead (model loading, memory allocation), tied requests ensure at least one server begins immediately.\n\n**Canary Requests**\n\nFor requests that fan out to many backends, first send the request to a small subset (1-2 servers). If these return within expected time, send to the remainder. If the canary is slow, the system can take corrective action (retry elsewhere, use cached results) before committing to the full fan-out. This prevents a single slow backend from stalling an entire distributed inference request.\n\n**Graceful Degradation**\n\nWhen load exceeds capacity, return approximate results rather than timing out. For classification, return cached predictions for similar inputs. For generative models, return shorter outputs. For ensemble systems, return predictions from a subset of models. This maintains responsiveness at the cost of some accuracy, which users often prefer to outright failures.\n\n**Admission Control**\n\nWhen traffic exceeds capacity, accepting all requests guarantees SLO violations for everyone. Admission control proactively rejects requests when queue depth exceeds a threshold, returning immediate 503 responses rather than accepting requests that will timeout. This sacrifices throughput to protect latency for admitted requests.\n\n**Setting the Threshold**: A practical starting point is 2-3x service time multiplied by the number of workers. For a system with 4 workers and 10ms service time, this yields a queue depth threshold of 80-120 requests. Adaptive admission control adjusts thresholds based on observed p99 latency, tightening when latency increases above target and relaxing when latency remains healthy.\n\n**Retry Storm Prevention**: A subtle failure mode occurs when all replicas are overloaded simultaneously. If the load balancer retries rejected requests at other replicas (also overloaded), retry traffic amplifies the overload. Coordinated load shedding addresses this by sharing load information across replicas, enabling system-wide decisions about which requests to accept. When global load exceeds capacity, replicas collectively reject the same fraction of requests rather than each rejecting independently and triggering retries.\n\nThese techniques become essential at scale when fan-out amplification makes individual server tail latency visible to users. Single-machine serving systems can implement hedged and tied requests across GPU streams or model replicas. The queuing analysis here assumes FIFO processing, but production systems often implement priority scheduling (deadline-aware, shortest-job-first) to further reduce tail latency for heterogeneous workloads [@harchol2013performance].\n\n## Training-Serving Skew {#sec-serving-skew}\n\nThe queuing theory and latency analysis from previous sections assume the model produces correct predictions. However, one of the most insidious problems in production ML systems threatens this assumption: training-serving skew, the phenomenon where a model behaves differently in production than during training, despite using identical weights [@sculley2015hidden]. The model has not changed, but something in the preprocessing pipeline produces different inputs in production than during training, causing silent accuracy degradation that may go undetected for weeks or months.\n\n::: {.callout-definition title=\"Training-Serving Skew\"}\n\n***Training-Serving Skew*** occurs when the _preprocessing logic_ or _data characteristics_ differ between training and serving environments, causing models to receive inputs that do not match their training distribution and resulting in degraded prediction quality.\n\n:::\n\nThis problem is particularly dangerous because traditional software testing often fails to catch it. The serving system returns predictions, the model runs without errors, and all health checks pass. Only careful monitoring of prediction quality reveals that something is wrong.\n\n### Sources of Skew {#sec-serving-skew-sources}\n\nUnderstanding the common causes of training-serving skew helps practitioners design systems that avoid these pitfalls.\n\n**Numerical precision differences** arise from different implementations of the same mathematical operations. Training uses float64 mean normalization while serving uses float32, producing different normalized values. A model achieving 0.87 AUC in validation may drop to 0.78 in production purely from these inconsistencies.\n\n**Library version mismatches** introduce subtle behavioral differences. Training uses NumPy 1.24 while serving uses TensorFlow ops with different rounding behavior. Image resizing with different interpolation defaults produces subtly different pixel values that accumulate through the network.\n\n**Missing value handling** diverges when training and serving encounter null values differently. Training fills nulls with column means computed over the training set while serving uses zeros or different defaults. Features present in training become missing in production due to upstream service failures.\n\n**Time and ordering effects** cause misalignment in temporal features. Training uses UTC timestamps while serving uses local time, causing off-by-one-day errors in date features. Tokenizers trained on specific vocabulary versions encounter unknown tokens differently than during training.\n\n**Environment differences** create systemic discrepancies [@polyzotis2017data]. Training runs in analytical environments (notebooks, data lakes) while serving runs in production microservices. Re-implementing feature engineering in different frameworks (Spark for training, pandas for serving) creates two codebases that must produce identical outputs.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Image Preprocessing Skew\"}\n\nFor ResNet-50 serving, common sources of skew include:\n\n**Resize interpolation**: Training uses PIL.BILINEAR while OpenCV defaults to cv2.INTER_LINEAR. These produce pixel-level differences that can shift accuracy by 0.5-1%.\n\n**Color space handling**: JPEG loading in different libraries may produce BGR vs RGB ordering. If the model trained on RGB but serves BGR inputs, predictions are essentially random.\n\n**Normalization constants**: ImageNet normalization uses specific mean/std values. Using `mean=[0.5, 0.5, 0.5]` instead of `mean=[0.485, 0.456, 0.406]` shifts inputs out of the training distribution.\n\n**Prevention**: The safest approach is to export the exact preprocessing code used during training and run it identically in serving, or use a framework like NVIDIA DALI that guarantees identical operations.\n\n:::\n\n### Prevention Strategies {#sec-serving-skew-prevention}\n\nThe fundamental solution is using identical preprocessing code for training and serving. This sounds obvious but proves difficult in practice. Training pipelines optimize for batch processing while serving pipelines optimize for single-request latency. Feature stores address this by centralizing feature computation, ensuring both training and serving retrieve features from the same source [@orr2021managing].\n\nWhen identical code is impossible, rigorous testing catches skew before deployment. Statistical comparison between training feature distributions and serving feature distributions reveals discrepancies. Shadow deployment runs the candidate model on live traffic alongside the baseline, surfacing skew that synthetic tests miss.\n\nMonitoring in production detects skew that emerges over time. Feature drift detection compares serving distributions to training baselines using statistical distance measures [@breck2019data]. When drift exceeds thresholds, alerts trigger investigation before accuracy degrades noticeably.\n\n@tbl-skew-prevention summarizes strategies for preventing and detecting training-serving skew.\n\n+-------------------------------+----------------------------------+---------------------------------+\n| **Strategy**                  | **Approach**                     | **Tradeoffs**                   |\n+:==============================+:=================================+:================================+\n| **Shared preprocessing code** | Use identical code for training  | May require runtime translation |\n|                               | and serving                      | (e.g., Python to C++)           |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Feature stores**            | Centralize feature computation   | Adds infrastructure complexity  |\n|                               | for both environments            |                                 |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Statistical validation**    | Compare feature distributions    | Catches drift but not all skew  |\n|                               | between training and serving     | types                           |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Shadow deployment**         | Run new model alongside baseline | Doubles serving cost during     |\n|                               | on live traffic                  | validation                      |\n+-------------------------------+----------------------------------+---------------------------------+\n| **Continuous monitoring**     | Track prediction distributions   | Reactive rather than preventive |\n|                               | and accuracy metrics             |                                 |\n+-------------------------------+----------------------------------+---------------------------------+\n\n: **Training-Serving Skew Prevention**: Strategies range from prevention (shared code, feature stores) to detection (statistical validation, monitoring), each with distinct operational tradeoffs. {#tbl-skew-prevention}\n\n## Model Loading and Initialization {#sec-serving-model-loading}\n\nWith preprocessing pipelines designed to avoid training-serving skew, the next challenge is getting the model ready to serve. Before processing any request, models must load from storage into memory and prepare for inference [@romero2021infaas]. This initialization phase creates the cold start problem: the first request after deployment or scaling experiences dramatically higher latency. Understanding cold start dynamics is essential for designing systems that meet latency requirements from the moment they begin serving traffic.\n\n### Cold Start Anatomy {#sec-serving-cold-start}\n\nCold start latency compounds from multiple sources, each adding to the time between deployment and serving readiness:\n\n1. **Weight loading**: Reading model parameters from disk or network storage\n2. **Graph compilation**: Just-in-time compilation of operations for the specific hardware\n3. **Memory allocation**: Reserving GPU memory for activations and intermediate values\n4. **Warmup execution**: Initial inferences that populate caches and trigger lazy initialization\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Cold Start Timeline\"}\n\nLoading ResNet-50 for production serving involves the following cold start phases:\n\n| Phase | Duration | Notes |\n|-------|----------|-------|\n| Weight loading (SSD) | 0.5s | 98MB FP32 weights from local storage |\n| Weight loading (S3) | 3-5s | Network latency dominates for cloud storage |\n| CUDA context | 0.3-0.5s | CUDA 11+ lazy loading significantly reduced this |\n| TensorRT compilation | 15-30s | Converts PyTorch model to optimized engine |\n| Warmup (10 inferences) | 0.2s | Triggers remaining lazy initialization |\n| **Total (local, optimized)** | **~1.5s** | With pre-compiled TensorRT engine, warm container |\n| **Total (cloud, first deploy)** | **~35s** | Including compilation from cold state |\n\n**Key insight**: Pre-compiling models and storing the optimized engine eliminates the 30-second compilation phase on subsequent deployments. Production systems use CUDA MPS (Multi-Process Service) or pre-warmed container pools to amortize CUDA initialization costs across requests.\n\n:::\n\nWithout warmup, the first real request triggers compilation and memory allocation mid-inference, often causing timeout failures. A request that normally takes 5ms might require 500ms during cold start, violating SLOs and degrading user experience.\n\n### Loading Strategies {#sec-serving-loading-strategies}\n\nDifferent loading strategies trade off cold start duration against serving performance and memory efficiency.\n\n**Full loading** reads the entire model into memory before serving begins. This maximizes inference speed since all weights are immediately available, but extends cold start duration and limits model size to available memory. Full loading is appropriate when cold start latency is acceptable and models comfortably fit in memory.\n\n**Memory mapping** maps model files directly into the address space, loading pages on demand as accessed. This reduces cold start time since inference can begin before the full model loads, but causes unpredictable latency as pages fault in during initial requests. Memory mapping works well for infrequently accessed model components but can cause latency spikes if critical weights are not preloaded.\n\n**Lazy initialization** defers compilation and allocation until first use. This minimizes startup time but shifts latency to the first request. Production systems often combine lazy initialization with synthetic warmup requests to trigger initialization before real traffic arrives.\n\n### Model Caching Infrastructure {#sec-serving-model-caching}\n\nProduction systems cache model weights at the infrastructure level to reduce cold start for common deployment scenarios:\n\n**Container Image Embedding**: Bundle model weights directly in the container image. This produces a single deployment artifact and eliminates network fetches at startup, but creates large images (often 10-50GB) that slow container pulls and consume registry storage. Best for models that rarely update.\n\n**Shared Filesystem**: Mount a network filesystem (EFS, GCS FUSE) containing model weights. Multiple replicas share cached weights, and updates propagate immediately without redeployment. However, network latency affects cold start, and filesystem availability becomes a critical dependency. Best for organizations with many models and frequent updates.\n\n**Node-Local SSD Cache**: Pre-populate local SSDs on inference nodes with frequently-used models. Provides fast loading (500MB/s+ for NVMe) without network dependency, but requires cache management to handle model updates and capacity limits. Best for high-traffic models where cold start latency is critical.\n\nThe choice depends on model update frequency: infrequent updates favor container embedding, frequent updates favor shared filesystem, and performance-critical deployments benefit from local caching with background refresh.\n\n### Multi-Model Serving {#sec-serving-multi-model}\n\nProduction systems often serve multiple models from a single machine, whether different model versions for A/B testing, ensemble components, or entirely different models sharing infrastructure. GPU memory becomes the limiting resource, requiring careful management strategies.\n\nStrategies for multi-model serving include:\n\n- **Time-multiplexing**: Load one model at a time, swapping based on request routing\n- **Memory sharing**: Models share GPU memory, limiting concurrent execution but enabling more models\n- **Model virtualization**: Frameworks like Triton manage model lifecycle, loading and unloading based on traffic patterns [@nvidia2024triton]\n\nThe choice depends on request patterns. If models receive traffic evenly, concurrent loading works. If traffic is bursty and model-specific, time-multiplexing with intelligent preloading reduces average latency while maximizing GPU utilization.\n\n**Multi-Stream Execution**\n\nWhen multiple models (or multiple instances of the same model) must run concurrently on a single GPU, the hardware must partition resources between them. NVIDIA's Multi-Instance GPU (MIG) technology enables hardware-level isolation, dividing an A100 into up to 7 independent GPU instances, each with dedicated memory and compute resources. MIG is available on A100, A30, H100, and newer data center GPUs. For older GPUs (V100, T4), CUDA stream scheduling provides time-multiplexed sharing without hardware isolation.\n\nMIG provides stronger isolation guarantees but reduces flexibility: once partitioned, GPU slices cannot be dynamically resized. CUDA streams offer flexibility but no performance isolation. The choice depends on whether consistent latency (MIG) or maximum utilization (shared streams) is the priority. We examine full implementation details for multi-model orchestration across GPUs in @sec-inference-at-scale, where distributed resource management becomes critical for production-scale deployments.\n\n## Batching for Serving {#sec-serving-batching}\n\nOnce models are loaded and ready to serve, the next optimization opportunity lies in how requests are grouped for processing. Batching fundamentally differs between training and serving [@crankshaw2017clipper]. Training batches maximize throughput, processing hundreds or thousands of samples together with no concern for individual sample latency. Serving batches must balance throughput against individual request latency, typically processing single digits of requests together while ensuring no request waits too long.\n\n::: {.callout-definition title=\"Dynamic Batching\"}\n\n***Dynamic Batching*** is a serving strategy that collects incoming requests within a _time window_ and processes them together, trading individual request latency for improved _throughput_ and _hardware utilization_. The window size and maximum batch size parameters control this tradeoff.\n\n:::\n\n### Why Batching Helps {#sec-serving-batching-why}\n\nModern accelerators achieve peak efficiency only at sufficient batch sizes [@shen2019nexus]. A single inference request leaves most compute units idle because GPUs are designed for parallel execution across thousands of threads. Batching amortizes fixed costs (kernel launch overhead, weight loading from memory) across multiple requests and enables parallel execution across the batch dimension.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Batching Efficiency\"}\n\nThe throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates the power of batching:\n\n| Batch Size | Inference Time* | Per-Image Compute | Throughput | GPU Util. |\n|:-----------|:----------------|:------------------|:-----------|:----------|\n| 1 | 5.0ms | 5.0ms | 200 img/s | 15% |\n| 4 | 7.2ms | 1.8ms | 556 img/s | 42% |\n| 8 | 9.1ms | 1.1ms | 879 img/s | 65% |\n| 16 | 14.0ms | 0.9ms | 1,143 img/s | 85% |\n| 32 | 25.0ms | 0.8ms | 1,280 img/s | 95% |\n\n*Times shown are pure inference time, excluding queue wait. User-perceived latency includes batching window wait (see @sec-serving-traffic-patterns).\n\n**Key insight**: Batch size 32 achieves 6.4Ã— higher throughput than batch size 1. However, user-perceived latency includes both queue wait and inference time. With a 10ms batching window and 25ms inference, total latency reaches 35ms versus 5ms at batch size 1.\n\n:::\n\nHowever, batching forces requests to wait. A request arriving just after a batch closes must wait for the current batch to complete plus full processing of its own batch. This waiting time directly adds to user-perceived latency, creating the fundamental tradeoff that serving system designers must navigate.\n\n### Static vs Dynamic Batching {#sec-serving-batching-types}\n\n**Static batching** waits for a fixed batch size before processing. Simple to implement but problematic in practice: during low traffic, requests wait indefinitely for a full batch. During high traffic, large batches increase per-request latency.\n\n**Dynamic batching** collects requests within a time window, processing whatever has arrived when the window closes [@olston2017tensorflow]. This bounds maximum wait time regardless of traffic level. The window size represents a direct tradeoff: shorter windows reduce latency but sacrifice throughput; longer windows improve throughput but increase latency.\n\nTypical configurations use windows of 5-50ms with maximum batch sizes of 8-32 for latency-sensitive applications. The optimal configuration depends on request arrival patterns, model characteristics, and latency requirements.\n\n### Continuous Batching {#sec-serving-continuous-batching}\n\nAutoregressive models like language models generate outputs token by token, creating a batching challenge that differs fundamentally from single-pass models like ResNet-50. Traditional batching processes all sequences in a batch for all generation steps, wasting compute when sequences complete at different times [@yu2022orca]. If one sequence in a batch of 8 finishes after 10 tokens while others need 100 tokens, 87.5% of the compute for that sequence slot is wasted. This inefficiency becomes critical as language models dominate production inference workloads.\n\nContinuous batching (also called iteration-level batching) addresses this waste by allowing new requests to join a batch between generation steps and completed sequences to exit [@kwon2023vllm]. Rather than forming static batches that persist for the entire generation process, the system dynamically manages batch composition at each decoding iteration.\n\nThe mechanism works as follows: when a sequence generates its end-of-sequence token, its slot becomes immediately available. A waiting request can fill that slot for the next iteration rather than waiting for the entire batch to complete. Similarly, the system can add new requests to available slots without interrupting ongoing generation. This dynamic approach maintains high GPU utilization even when sequence lengths vary dramatically.\n\nSystems implementing continuous batching, such as vLLM and TensorRT-LLM, achieve 2-4Ã— higher throughput than traditional static batching [@agrawal2024sarathi]. The improvement comes from two sources: eliminating wasted compute on completed sequences and reducing average wait time for new requests. For production language model serving where response lengths vary from single tokens to thousands, continuous batching has become essential for cost-effective deployment.\n\nMemory management adds complexity to continuous batching. As sequences enter and exit the batch, the key-value cache that stores attention context must be dynamically allocated and freed. PagedAttention, introduced in vLLM, applies operating system paging concepts to manage this memory efficiently, avoiding fragmentation that would otherwise limit batch capacity [@kwon2023vllm]. These techniques represent the intersection of classical systems engineering with modern ML serving challenges.\n\n::: {.callout-note title=\"LLM Serving: Beyond the Fundamentals\"}\n\nLanguage model serving introduces challenges beyond the batching and memory principles established here. The key-value cache that stores attention context scales with sequence length and batch size, often exceeding the model weights themselves in memory consumption. Techniques like speculative decoding use small draft models to propose multiple tokens that the target model verifies in parallel, achieving 2-3Ã— latency reduction for interactive applications. Weight-only quantization (INT4 weights with FP16 activations) proves more effective than activation quantization for memory-bandwidth-bound LLM inference.\n\nThese LLM-specific optimizations build directly on the foundations this chapter establishes: queuing theory governs request scheduling, batching tradeoffs determine throughput-latency curves, and precision selection follows the same accuracy-efficiency principles. The serving fundamentals apply universally; LLM serving adds domain-specific techniques atop this foundation.\n\n:::\n\n### When Not to Batch {#sec-serving-no-batch}\n\nSome scenarios require single-request processing:\n\n- **Ultra-low latency requirements**: When p99 latency must stay under 10ms, any batching delay is unacceptable\n- **Highly variable request sizes**: When inputs vary dramatically in size, batching creates padding overhead that wastes compute\n- **Memory constraints**: When models already consume most GPU memory, batch activations may cause out-of-memory errors\n\n### Session Affinity Constraints {#sec-serving-session-affinity}\n\nWhen requests from the same user or session should route to the same replica, batching becomes constrained. Session affinity (also called sticky sessions) matters for three main reasons:\n\n**KV-Cache Reuse**: For conversational AI, the key-value cache from previous turns dramatically speeds up multi-turn conversations. Routing a follow-up request to a different replica forfeits this cached context, increasing latency by 2-5x for long conversations.\n\n**User-Specific Models**: Some systems serve personalized models or adapters per user. Routing requests to the replica that has already loaded that user's adapter avoids repeated loading overhead.\n\n**Stateful Preprocessing**: When preprocessing maintains state (tokenizer caches, session-specific normalization), routing to a different replica requires rebuilding this state.\n\nThe tension with batching is clear: strict affinity constrains which requests can be batched together, potentially reducing batch sizes and GPU utilization. Production systems often implement \"soft affinity\" where requests prefer their assigned replica but can overflow to others when that replica is overloaded. This preserves most affinity benefits while maintaining load balance.\n\n### Traffic Patterns and Batching Strategy {#sec-serving-traffic-patterns}\n\nThe optimal batching strategy depends critically on how requests arrive. Different deployment contexts exhibit fundamentally different arrival patterns, each requiring distinct batching approaches. The MLPerf inference benchmark codifies these patterns into four scenarios that directly map to real-world deployments (see @sec-benchmarking-ai for MLPerf details).\n\n**Server Traffic (Poisson Arrivals)**\n\nCloud APIs and web services typically receive requests following a Poisson process, where arrivals are independent and uniformly distributed over time. For Poisson arrivals with rate $\\lambda$ and batching window $T$, the expected batch size follows @eq-poisson-batch:\n\n$$E[\\text{batch size}] = \\lambda \\cdot T$$ {#eq-poisson-batch}\n\nThe variance equals the mean (a property of Poisson distributions), so batch sizes fluctuate significantly at moderate traffic. With $\\lambda = 200$ requests/second and $T = 10$ms, expected batch size is 2, but 16% of windows will have zero requests (wasted compute cycles) while others may have 4 or more.\n\nThe optimal batching window balances waiting cost against throughput benefit. As shown in @eq-optimal-window:\n\n$$T_{\\text{optimal}} = \\min\\left(L - S, \\sqrt{\\frac{S}{\\lambda}}\\right)$$ {#eq-optimal-window}\n\nwhere $L$ is the latency SLO and $S$ is the service time. A counterintuitive result emerges from this equation: as traffic increases, the optimal window decreases while achieved batch sizes still grow. @tbl-traffic-adaptive demonstrates this relationship across traffic levels.\n\n| Arrival Rate | Optimal Window | Avg Batch Size | p99 Latency |\n|:-------------|:---------------|:---------------|:------------|\n| 100 QPS | 20ms | 2.0 | 45ms |\n| 500 QPS | 8ms | 4.0 | 42ms |\n| 1,000 QPS | 5ms | 5.0 | 38ms |\n| 5,000 QPS | 2ms | 10.0 | 35ms |\n\n: **Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time. {#tbl-traffic-adaptive}\n\n**Streaming Traffic (Correlated Arrivals)**\n\nAutonomous vehicles, video analytics, and robotics systems receive inputs from multiple synchronized sensors. Rather than independent arrivals, frames from all cameras for a given timestamp must be processed together as a batch.\n\n::: {.callout-note title=\"ðŸ”„ Multi-Camera Autonomous Vehicle Serving\"}\n\nConsider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial fusion:\n\n**Timeline for processing frame set N:**\n\n| Time | Event |\n|:-----|:------|\n| T = 0ms | Cameras begin capturing frame N |\n| T = 8ms | Camera 1 frame arrives |\n| T = 10ms | Cameras 2-5 frames arrive |\n| T = 15ms | Camera 6 arrives (jitter) |\n| T = 15ms | Batch inference begins (6 images) |\n| T = 25ms | Inference complete |\n| T = 32ms | Result ready for planning module |\n\n**Key constraints:**\n\n- Hard deadline: 33ms per frame set (real-time requirement)\n- Batch size: Fixed at 6 (one per camera)\n- Synchronization budget: 12ms of 33ms total (36% for jitter tolerance)\n- Timeout policy: If camera frame not received by T+20ms, use previous frame\n\nUnlike Poisson traffic where dynamic batching optimizes throughput, streaming traffic requires synchronization policies that handle sensor jitter while meeting hard deadlines.\n\n:::\n\n**Single-User Traffic (Sequential Arrivals)**\n\nMobile and embedded applications serve one user at a time, with requests arriving only after the previous result is consumed. Batch size is always 1, eliminating batching optimization entirely but raising different challenges.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Mobile Single-User Serving (Pixel 6 NPU)\"}\n\n| Phase | Duration | Notes |\n|:------|:---------|:------|\n| Camera buffer read | 8ms | System API overhead |\n| JPEG decode (CPU) | 15ms | Single-threaded |\n| Resize + Normalize | 5ms | CPU preprocessing |\n| NPU inference | 12ms | 82% NPU utilization |\n| Post-process + UI | 5ms | Result rendering |\n| **Total** | **45ms** | Perceived as \"instant\" |\n\n**Critical insight**: Even at batch size 1, the mobile NPU achieves 82% utilization because its compute capacity matches single-image workloads. This differs fundamentally from datacenter GPUs, which achieve only 15% utilization at batch size 1 because their massive parallelism requires larger batches to saturate.\n\nMobile serving optimization focuses on preprocessing efficiency and power management rather than batching strategies.\n\n:::\n\n@tbl-traffic-patterns-summary maps MLPerf scenarios to deployment contexts and appropriate batching strategies.\n\n+------------------+---------------------+------------------+---------------------------+\n| **MLPerf**       | **Deployment**      | **Batch**        | **Optimization**          |\n| **Scenario**     | **Context**         | **Strategy**     | **Focus**                 |\n+:=================+:====================+:=================+:==========================+\n| **Server**       | Cloud APIs,         | Dynamic batching | Window tuning,            |\n|                  | web services        | with timeout     | utilization-latency curve |\n+------------------+---------------------+------------------+---------------------------+\n| **MultiStream**  | Autonomous driving, | Synchronized     | Jitter handling,          |\n|                  | video analytics     | sensor fusion    | deadline guarantees       |\n+------------------+---------------------+------------------+---------------------------+\n| **SingleStream** | Mobile apps,        | No batching      | Preprocessing,            |\n|                  | embedded devices    | (batch=1)        | power efficiency          |\n+------------------+---------------------+------------------+---------------------------+\n| **Offline**      | Batch processing,   | Maximum batch    | Throughput,               |\n|                  | data pipelines      | size             | hardware utilization      |\n+------------------+---------------------+------------------+---------------------------+\n\n: **Traffic Patterns and Batching Strategies**: The MLPerf inference scenarios map to distinct deployment contexts, each requiring different batching approaches and optimization priorities. {#tbl-traffic-patterns-summary}\n\n## Postprocessing {#sec-serving-postprocessing}\n\nBatching determines how inputs flow through inference, but the journey does not end when the model produces output tensors. Model outputs are arrays of floating-point numbers. Users need predictions: labels, probabilities, generated text, structured data. Postprocessing bridges this gap, transforming raw model outputs into responses that applications can consume. While often overlooked in system design, postprocessing significantly impacts both latency and the usefulness of predictions.\n\n### From Logits to Predictions {#sec-serving-logits}\n\nClassification models output logits or probabilities across classes. Converting these to predictions involves several potential steps:\n\n- **Argmax selection**: Choosing the highest-probability class\n- **Thresholding**: Applying confidence thresholds before returning predictions\n- **Top-k extraction**: Returning multiple high-probability classes with scores\n- **Calibration**: Adjusting raw probabilities to better reflect true likelihoods\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Postprocessing Pipeline\"}\n\nFor ResNet-50 image classification, typical postprocessing includes:\n\n```python\n# Raw model output: logits tensor of shape (batch_size, 1000)\nprobs = torch.softmax(logits, dim=-1)  # 0.05ms\ntop5_probs, top5_indices = probs.topk(5)  # 0.02ms\nlabels = [IMAGENET_CLASSES[i] for i in top5_indices]  # 0.01ms\n\n# Response formatting\nresponse = {\n    \"predictions\": [\n        {\"label\": label, \"confidence\": float(prob)}\n        for label, prob in zip(labels, top5_probs)\n    ],\n    \"model_version\": \"resnet50-v2.1\",\n    \"inference_time_ms\": 5.2,\n}\n```\n\n**Total postprocessing time**: ~0.1ms (negligible compared to preprocessing and inference)\n\n:::\n\nEach step adds latency but improves response utility. Calibration in particular can add significant computation but is essential when downstream systems make decisions based on confidence scores.\n\n### Generation and Decoding {#sec-serving-decoding}\n\nGenerative models require decoding strategies that trade off quality, diversity, and latency. The choice of decoding strategy can dramatically affect both output quality and computational cost.\n\n**Greedy decoding** selects the highest-probability token at each step. Fast but often produces repetitive, low-quality outputs because it cannot recover from early mistakes.\n\n**Beam search** maintains multiple candidate sequences, selecting the highest-scoring complete sequence. Produces higher-quality outputs but multiplies computation by the beam width.\n\n**Sampling** with temperature, top-k, and top-p parameters introduces randomness for diversity [@holtzman2020curious]. Temperature scales logits before softmax; top-k limits sampling to the k highest-probability tokens; top-p (nucleus sampling) limits sampling to tokens comprising probability mass p.\n\nThe choice involves latency tradeoffs [@meister2020beam]. Beam search with width 5 takes roughly 5Ã— the compute of greedy decoding. Sampling adds minimal overhead but requires careful parameter tuning to balance quality and coherence.\n\n### Output Formatting and Streaming {#sec-serving-output-format}\n\nProduction systems rarely return raw predictions. Outputs must conform to API contracts, often requiring:\n\n- JSON serialization with specific schema\n- Confidence score formatting and thresholding\n- Error handling for edge cases (no confident prediction, out-of-distribution input)\n- Metadata attachment (model version, inference time, feature attributions)\n\nStreaming responses for generative models add complexity. Rather than waiting for complete generation, systems return tokens as they are produced. This improves perceived latency (users see output beginning quickly) but requires infrastructure support for chunked responses and client-side incremental rendering.\n\n## Inference Runtime Selection {#sec-serving-runtimes}\n\nThe preprocessing, inference, and postprocessing pipeline we have examined runs on an inference runtime that significantly impacts latency, throughput, and operational complexity. Choosing appropriately requires understanding the tradeoffs between framework-native serving, general-purpose optimization, and specialized inference engines. This decision often determines whether a model can meet its latency SLOs.\n\n### Framework-Native Serving {#sec-serving-framework-native}\n\nPyTorch and TensorFlow models can serve directly using their native runtimes. This approach maximizes compatibility (any model that trains will serve) and simplifies the deployment pipeline (no export or conversion step). However, framework runtimes include training functionality that adds overhead, and default execution paths may not exploit hardware-specific optimizations.\n\nTorchScript and TensorFlow SavedModel formats enable ahead-of-time compilation and graph optimization, improving over eager execution while maintaining framework compatibility. These formats represent the first step toward deployment optimization without abandoning the familiar framework ecosystem.\n\n### General-Purpose Optimization {#sec-serving-onnx}\n\nONNX Runtime provides a hardware-agnostic optimization layer [@onnxruntime2024]. Models export to ONNX format, then ONNX Runtime applies graph optimizations and selects execution providers for the target hardware. This enables single-format deployment across CPUs, GPUs, and specialized accelerators.\n\n### Specialized Inference Engines {#sec-serving-specialized}\n\nTensorRT (NVIDIA GPUs), OpenVINO (Intel hardware), and similar engines optimize specifically for their target hardware [@nvidia2024tensorrt; @chen2018tvm]. They apply aggressive optimizations: layer fusion, precision calibration, kernel auto-tuning. These typically achieve 2-5Ã— speedup over framework-native serving but require explicit export and may not support all operations.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Runtime Comparison\"}\n\nPerformance comparison for ResNet-50 inference on V100 GPU (batch size 1):\n\n| Runtime | Latency | Speedup | Notes |\n|---------|---------|---------|-------|\n| PyTorch (eager) | 8.5ms | 1.0Ã— | Baseline, no optimization |\n| TorchScript | 6.2ms | 1.4Ã— | JIT compilation |\n| ONNX Runtime | 5.1ms | 1.7Ã— | Cross-platform |\n| TensorRT FP32 | 2.8ms | 3.0Ã— | NVIDIA-specific |\n| TensorRT FP16 | 1.4ms | 6.1Ã— | Tensor Core acceleration |\n| TensorRT INT8 | 0.9ms | 9.4Ã— | Requires calibration |\n\n**Key insight**: The 9.4Ã— speedup from TensorRT INT8 comes at the cost of: (1) quantization calibration data, (2) potential accuracy loss (<1% for ResNet-50), and (3) NVIDIA-specific deployment.\n\n:::\n\nThe optimization-compatibility tradeoff is fundamental. More aggressive optimization yields better performance but increases deployment complexity and may introduce numerical differences from training. The choice depends on latency requirements, deployment constraints, and available engineering resources.\n\n### Runtime Configuration {#sec-serving-runtime-config}\n\nBeyond runtime selection, configuration choices impact serving performance:\n\n- **Thread pools**: Controlling parallelism for CPU inference\n- **Memory allocation**: Pre-allocating buffers vs dynamic allocation\n- **Execution providers**: Selecting and prioritizing hardware backends\n- **Graph optimization level**: Trading compilation time for runtime performance\n\nProduction deployments typically require experimentation to find optimal configurations for specific models and hardware combinations. A systematic approach tests key parameters and measures their impact on latency distributions.\n\n### Precision Selection for Serving {#sec-serving-precision}\n\nNumerical precision directly trades accuracy for throughput, connecting to the quantization techniques covered in @sec-model-optimizations. While @sec-model-optimizations focuses on training-time quantization, serving introduces additional considerations: calibration requirements, layer sensitivity, and dynamic precision selection.\n\n**Precision-Throughput Relationship**\n\nFor memory-bandwidth-bound operations, reducing precision proportionally increases throughput by reducing data movement. The theoretical maximum speedup from precision reduction follows @eq-precision-throughput:\n\n$$\\frac{\\text{Throughput}_{\\text{INT8}}}{\\text{Throughput}_{\\text{FP32}}} = \\frac{32}{8} = 4\\times \\text{ (theoretical maximum)}$$ {#eq-precision-throughput}\n\nIn practice, GPU compute pipelines and Tensor Core alignment requirements limit achieved speedup to 2.5-3.5Ã— for INT8 versus FP32. Tensor Cores require specific alignment: INT8 operations need tensor dimensions divisible by 16, while FP16 requires divisibility by 8 (see @sec-ai-acceleration for Tensor Core architecture details).\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Precision Tradeoffs on V100\"}\n\n| Precision | Latency | Memory | Accuracy | Tensor Core Util. | Calibration |\n|:----------|:--------|:-------|:---------|:------------------|:------------|\n| FP32 | 2.8ms | 98MB | 76.13% | 0% | None |\n| FP16 | 1.4ms | 49MB | 76.13% | 85% | None |\n| INT8 (PTQ) | 0.9ms | 25MB | 75.80% | 92% | 1,000 samples |\n| INT8 (QAT) | 0.9ms | 25MB | 76.05% | 92% | Full retraining |\n\n**Key observations:**\n\n- INT8 achieves 3.1Ã— speedup but loses 0.33% accuracy with post-training quantization (PTQ)\n- Quantization-aware training (QAT) recovers most accuracy but requires retraining\n- FP16 provides 2Ã— speedup with no accuracy loss for most models\n\n:::\n\n**Layer Sensitivity**\n\nNot all layers tolerate reduced precision equally. Quantization error for a layer scales with weight magnitude and gradient sensitivity, as captured by @eq-quant-error:\n\n$$\\epsilon_{\\text{quant}} \\propto \\alpha \\cdot \\|W\\|_2 \\cdot 2^{-b}$$ {#eq-quant-error}\n\nwhere $\\alpha$ is a layer-specific sensitivity coefficient, $\\|W\\|_2$ is the weight L2 norm, and $b$ is the bit width. This explains observed patterns:\n\n- **First convolutional layers** (high gradients, large $\\alpha$): Precision-sensitive, often kept at FP16\n- **Middle layers** (stable gradients, low $\\alpha$): Tolerate INT8 well\n- **Final classification layers** (small weights but high task sensitivity): Benefit from FP16+\n\n**Calibration Requirements**\n\nPost-training quantization requires a calibration dataset to determine optimal scale factors for INT8 conversion. Production experience shows that calibration data must be representative of actual serving traffic, not just training data. Using ImageNet validation images to calibrate a model serving wildlife camera images resulted in 3.2% accuracy degradation in one production system.\n\n**Dynamic Precision Selection**\n\nAdvanced serving systems select precision per request based on runtime conditions:\n\n- If ahead of latency SLO, use higher precision for better accuracy\n- For low-confidence INT8 results, recompute at FP16\n- Different customer tiers may receive different precision levels\n\nThis pattern enables adaptive quality-latency tradeoffs while maximizing throughput during normal operation.\n\n## Cost and Capacity Planning {#sec-serving-cost}\n\nThe runtime and precision choices examined in previous sections determine per-inference performance, but production deployment requires translating these choices into infrastructure decisions. Serving costs scale with request volume, unlike training costs that scale with dataset size and model complexity [@zhang2019mark]. Understanding cost structure enables informed infrastructure decisions that balance performance requirements against budget constraints.\n\n### Cost Per Inference {#sec-serving-cost-per-inference}\n\nTotal serving cost decomposes into several components:\n\n- **Compute**: GPU/CPU time per inference\n- **Memory**: Accelerator memory required to hold model and activations\n- **Data transfer**: Network bandwidth for request/response payloads\n- **Orchestration overhead**: Container runtime, load balancing, monitoring\n\nFor GPU inference, compute time dominates when utilization is high. When utilization is low, memory cost dominates because the GPU is reserved but idle. This creates a utilization imperative: serving infrastructure should maximize GPU utilization through batching, multi-model serving, or right-sized instance selection.\n\n::: {.callout-note title=\"ðŸ”„ ResNet-50: Cost Analysis\"}\n\nConsider serving ResNet-50 on AWS infrastructure:\n\n| Instance Type | Cost/Hour | Throughput | Cost per 1M Images |\n|---------------|-----------|------------|-------------------|\n| c5.xlarge (CPU) | $0.17 | 50 img/s | $0.94 |\n| g4dn.xlarge (T4 GPU) | $0.53 | 400 img/s | $0.37 |\n| p3.2xlarge (V100 GPU) | $3.06 | 1,200 img/s | $0.71 |\n\n**Key insight**: The T4 GPU instance achieves the lowest cost per inference despite higher hourly cost, because GPU throughput dramatically exceeds CPU throughput. The V100 is only cost-effective at very high sustained traffic where its higher throughput justifies the 6Ã— price increase.\n\n:::\n\n### GPU vs CPU Economics {#sec-serving-gpu-cpu}\n\nGPUs provide orders-of-magnitude speedup for parallel operations but cost significantly more per hour [@wu2019machine]. The crossover point depends on model characteristics and latency requirements.\n\nCPU inference makes economic sense when:\n\n- Models are small (few parameters, simple operations)\n- Latency requirements are relaxed (hundreds of milliseconds acceptable)\n- Request volume is low or highly variable\n- Models use operations that do not parallelize well\n\nGPU inference makes economic sense when:\n\n- Models are large with parallel-friendly operations\n- Latency requirements are strict (tens of milliseconds)\n- Request volume is high and consistent\n- Batching can achieve high utilization\n\n**Scaling Responsiveness**: Beyond steady-state costs, startup time affects scaling economics. CPU instances typically start in 30-60 seconds while GPU instances take 2-5 minutes (including driver initialization, model loading, and warmup). For variable traffic patterns, this startup latency can be more important than cost per inference. If traffic spikes arrive faster than GPU instances can scale, latency SLOs will be violated despite having sufficient eventual capacity.\n\nThis asymmetry suggests different scaling strategies: CPU instances enable reactive scaling (responding to current demand) while GPU instances often require predictive scaling (provisioning based on anticipated demand). For bursty workloads, a hybrid approach uses always-on GPU capacity for baseline load plus CPU overflow capacity for spikes, trading higher per-inference cost during spikes for better responsiveness.\n\n### Capacity Planning {#sec-serving-capacity-planning}\n\nThe GPU versus CPU decision establishes the cost per inference, but determining how much infrastructure to provision requires combining cost analysis with the queuing theory foundations from @sec-serving-queuing. Capacity planning translates latency requirements and traffic projections into infrastructure specifications. Key inputs include:\n\n- **Traffic patterns**: Peak request rate, daily/weekly cycles, growth projections\n- **Latency SLOs**: p50, p95, p99 targets\n- **Model characteristics**: Inference time distribution at various batch sizes\n\nFrom these inputs, queuing theory determines required capacity [@harchol2013performance]. The equations developed in @sec-serving-queuing provide the mathematical foundation: @eq-mm1-wait shows how latency scales with utilization, while @eq-p99-latency enables calculating p99 latency for capacity planning.\n\nThe relationship between utilization and latency is nonlinear (@tbl-utilization-latency): at 70% utilization, p99 latency is approximately 12Ã— service time; at 90% utilization, it reaches 33Ã— service time. This nonlinearity explains why systems that seem healthy (low average latency) can suddenly violate SLOs when traffic increases modestly.\n\nThe worked example in @sec-serving-queuing demonstrates the complete capacity planning process: starting from a 50ms p99 SLO and 5,000 QPS target, deriving the safe utilization threshold (72%), calculating required service rate (6,944 QPS), and determining GPU count with headroom (10 V100s). Production systems typically provision for peak load plus 30% headroom, using auto-scaling to reduce costs during low-traffic periods while maintaining latency guarantees during peaks.\n\n## Fallacies and Pitfalls {#sec-serving-fallacies-pitfalls}\n\nThe principles established throughout this chapter provide a systematic framework for designing serving systems that meet latency requirements while maximizing efficiency. However, practitioners frequently encounter misconceptions that lead to suboptimal designs or production failures. These fallacies and pitfalls emerge from the fundamental differences between training and serving that make intuitions from one domain misleading in the other.\n\n**Fallacy:** _Faster model inference automatically means faster end-to-end serving._\n\nThis misconception leads teams to focus optimization efforts exclusively on model inference while ignoring preprocessing and postprocessing overhead. As demonstrated in @sec-serving-latency-budget, preprocessing often consumes 45-70% of total latency when inference runs on optimized accelerators. A team that reduces inference time from 5ms to 2ms through quantization achieves only 15% improvement in total latency if preprocessing remains at 8ms. Effective optimization requires profiling the complete request path and allocating engineering effort proportionally to where time is actually spent.\n\n**Pitfall:** _Running serving infrastructure at high utilization to maximize cost efficiency._\n\nThe nonlinear relationship between utilization and latency (@eq-mm1-wait) makes high utilization dangerous for latency-sensitive systems. At 90% utilization, average wait time reaches 10Ã— service time, and p99 latency becomes unacceptable for most SLOs. Teams that provision for average load rather than peak load find their systems violate latency guarantees precisely when traffic increases, the moment when reliable performance matters most. Production systems typically target 60-70% utilization at peak load to maintain latency headroom.\n\n**Fallacy:** _Training accuracy guarantees serving accuracy._\n\nTraining-serving skew (@sec-serving-skew) causes models to behave differently in production despite using identical weights. Differences in preprocessing libraries, numerical precision, feature computation timing, or input distribution silently degrade accuracy without triggering obvious errors. A model achieving 95% accuracy in training evaluation might drop to 90% in production due to subtle preprocessing differences that shift inputs outside the training distribution. Prevention requires either identical preprocessing code paths or rigorous statistical monitoring of input distributions.\n\n**Pitfall:** _Using average latency to evaluate serving system performance._\n\nAverage latency hides the experience of the slowest requests, which often determine user satisfaction and SLO compliance. A system with 10ms average latency might have 200ms p99 latency, meaning 1% of users experience 20Ã— worse performance. At scale with fan-out amplification (@sec-serving-tail-latency), even rare slow responses become visible to most users. Production SLOs specify percentile targets (p95, p99) precisely because averages mask unacceptable tail behavior.\n\n**Fallacy:** _Larger batch sizes always improve throughput._\n\nWhile batching amortizes fixed costs and improves GPU utilization, it also increases per-request latency and can cause memory exhaustion. Beyond a certain point, larger batches provide diminishing throughput returns while latency continues to grow linearly. Additionally, batch sizes that exceed GPU memory cause out-of-memory failures, and highly variable input sizes create padding overhead that wastes compute. The optimal batch size depends on latency SLOs, memory constraints, and traffic patterns, not just throughput maximization.\n\n**Pitfall:** _Calibrating quantized models with training data rather than production traffic._\n\nPost-training quantization requires calibration data to determine optimal scale factors, but using training data assumes production inputs match the training distribution. When production traffic differs (different image sources, lighting conditions, user behavior), calibration on training data produces suboptimal scale factors that degrade accuracy. One production system experienced 3.2% accuracy loss when calibrating with ImageNet validation images but serving wildlife camera images. Effective calibration requires representative samples of actual serving traffic.\n\n**Fallacy:** _Cold start latency only matters for the first request._\n\nCold start affects any request that arrives after a period of inactivity, after model updates, or when auto-scaling adds new instances. In systems with bursty traffic or multiple model versions, cold starts can affect a significant fraction of requests. A model requiring 500ms to load impacts not just the first user but every user who triggers a scale-up event or model reload. Production systems mitigate cold start through model preloading, keep-alive mechanisms, and gradual traffic shifting during deployments.\n\n## Summary {#sec-serving-summary}\n\nServing represents the critical transition from model development to production deployment, where the optimization priorities that governed training must be fundamentally inverted. Throughout this chapter, we have seen how the shift from throughput maximization to latency minimization transforms every system design decision. The queuing theory foundations established here reveal why this inversion is not merely a change in metrics but a change in the governing mathematics: the nonlinear relationship between utilization and latency means that systems behaving well at moderate load can suddenly violate SLOs when traffic increases modestly. Little's Law and the M/M/1 wait time equations provide the quantitative foundation for capacity planning that replaces intuition-based provisioning with engineering rigor.\n\nOur exploration of the latency budget demonstrates that effective serving optimization requires understanding the complete request path rather than focusing exclusively on model inference. Preprocessing often consumes 45-70% of total latency when inference runs on optimized accelerators, yet engineering effort frequently targets the wrong bottleneck. The microsecond-scale overheads identified by Barroso, Patterson, and colleagues explain why serving latency often exceeds the sum of its measured parts, and why system-level optimization matters as much as model optimization. Training-serving skew represents another dimension of this complexity, silently degrading accuracy when preprocessing logic differs between training and production environments in ways that traditional testing cannot detect.\n\nThe traffic pattern analysis reveals how deployment context fundamentally shapes batching strategy and system design. Server workloads with Poisson arrivals optimize dynamic batching windows, autonomous vehicles with streaming sensor data require synchronized batch formation, and mobile applications with single-user patterns eliminate batching entirely. The MLPerf scenarios codify these patterns for standardized benchmarking, connecting the serving principles established here to the measurement frameworks explored in @sec-benchmarking-ai. Precision selection and runtime optimization extend the quantization techniques from @sec-model-optimizations and Tensor Core capabilities from @sec-ai-acceleration into the serving domain, where calibration with representative production traffic becomes essential.\n\n::: {.callout-important title=\"Key Takeaways\"}\n* Serving inverts training priorities: latency per request matters more than aggregate throughput, requiring fundamentally different system design\n* Queuing theory provides the mathematical foundation for capacity planning, with the utilization-latency relationship explaining why systems degrade nonlinearly under load\n* Preprocessing often dominates total latency (45-70%), making pipeline optimization as important as model optimization\n* Traffic patterns (Poisson, streaming, single-user) determine optimal batching strategy, directly mapping to MLPerf inference scenarios\n* Precision selection and resolution tradeoffs connect serving decisions to optimization techniques from earlier chapters\n* Training-serving skew silently degrades accuracy and requires identical preprocessing code or rigorous monitoring to prevent\n:::\n\nThe serving foundations established in this chapter provide the infrastructure for the operational deployment strategies explored in @sec-ml-operations. While this chapter focused on the mechanics of transforming requests into predictions efficiently, production environments introduce additional complexities of monitoring, versioning, and continuous validation that characterize real-world ML system deployment. The single-machine serving principles developed here also prepare practitioners for understanding when and why scaling to multiple machines becomes necessary, with distributed serving patterns including load balancing across nodes and model sharding for models exceeding single-machine memory covered in @sec-inference-at-scale.\n\n<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->\n::: { .quiz-end }\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../filters/sidenote.lua","../../../filters/inject_parts.lua","../../../filters/inject_quizzes.lua","pandoc-ext/diagram","mlsysbook-ext/custom-numbered-blocks"],"title-prefix":"","reference-location":"margin","highlight-style":"../../../assets/styles/custom-code.theme","toc":true,"toc-depth":4,"number-sections":false,"include-in-header":{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap\" rel=\"stylesheet\">\n<link rel=\"manifest\" href=\"/site.webmanifest\">\n<link rel=\"apple-touch-icon\" href=\"/assets/images/icons/favicon.png\">\n<meta name=\"theme-color\" content=\"#A51C30\">\n\n<script type=\"module\"  src=\"/tools/scripts/socratiQ/bundle.js\" defer></script>\n<script src=\"/assets/scripts/sidebar-auto-collapse.js\" defer></script>\n<script src=\"/assets/scripts/version-link.js\" defer></script>\n<script src=\"/assets/scripts/subscribe-modal.js\" defer></script>\n"},"citeproc":true,"output-file":"serving.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","bibliography":["../../../contents/vol1/introduction/introduction.bib","../../../contents/vol1/responsible_engr/responsible_engr.bib","../../../contents/vol1/benchmarking/benchmarking.bib","../../../contents/vol1/data_engineering/data_engineering.bib","../../../contents/vol1/dl_primer/dl_primer.bib","../../../contents/vol1/dnn_architectures/dnn_architectures.bib","../../../contents/vol1/efficient_ai/efficient_ai.bib","../../../contents/vol1/ml_systems/ml_systems.bib","../../../contents/vol1/frameworks/frameworks.bib","../../../contents/vol1/hw_acceleration/hw_acceleration.bib","../../../contents/vol1/ops/ops.bib","../../../contents/vol1/optimizations/optimizations.bib","../../../contents/vol1/training/training.bib","../../../contents/vol1/workflow/workflow.bib","../../../contents/vol1/serving/serving.bib","../../../contents/vol1/conclusion/conclusion.bib","../../../contents/vol2/ops_scale/ops_scale.bib","../../../contents/vol2/edge_intelligence/edge_intelligence.bib","../../../contents/vol2/privacy_security/privacy_security.bib","../../../contents/vol2/responsible_ai/responsible_ai.bib","../../../contents/vol2/robust_ai/robust_ai.bib","../../../contents/vol2/sustainable_ai/sustainable_ai.bib","../../../contents/vol2/ai_for_good/ai_for_good.bib","../../../contents/vol2/frontiers/frontiers.bib","serving.bib"],"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","key":"vid","latex-env":"vid","reference-prefix":"Video"}]},"filter-metadata":{"quiz-config":{"file-pattern":"*_quizzes.json","scan-directory":"../../../contents/vol1","auto-discover-pdf":false},"part-summaries":{"file":"../../../contents/parts/summaries.yml","enabled":true},"mlsysbook-ext/custom-numbered-blocks":{"icon-path":"../../../assets/images/icons/callouts","icon-format":"png","groups":{"quiz-question":{"colors":["F0F0F8","5B4B8A"],"collapse":true},"quiz-answer":{"colors":["E8F2EA","4a7c59"],"collapse":true},"resource-slides":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-videos":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"resource-exercises":{"colors":["E0F2F1","20B2AA"],"collapse":true,"numbered":false},"chapter-connection":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-forward":{"colors":["FDF2F7","A51C30"],"boxstyle":"foldbox.simple","collapse":true,"numbered":false},"chapter-recall":{"colors":["FFF4E6","C06014"],"collapse":true,"numbered":false},"code-listing":{"colors":["F2F4F8","D1D7E0"],"collapse":false,"numbered":false},"definition":{"colors":["F0F4F8","1B4F72"],"collapse":false,"numbered":false},"example":{"colors":["F0F8F6","148F77"],"collapse":false,"numbered":false},"colab-interactive":{"colors":["FFF5E6","FF6B35"],"collapse":false,"numbered":false}},"classes":{"callout-quiz-question":{"label":"Self-Check: Question","group":"quiz-question"},"callout-quiz-answer":{"label":"Self-Check: Answer","group":"quiz-answer"},"callout-resource-slides":{"label":"Listing","group":"resource-slides"},"callout-resource-videos":{"label":"Videos","group":"resource-videos"},"callout-resource-exercises":{"label":"Exercises","group":"resource-exercises"},"callout-chapter-connection":{"label":"Related Topics","group":"chapter-connection"},"callout-code":{"label":"Code","group":"code-listing"},"callout-definition":{"label":"Definition","group":"definition"},"callout-example":{"label":"Example","group":"example"},"callout-colab":{"label":"Interactive Colab","group":"colab-interactive"}}}},"diagram":{"engine":{"dot":false,"mermaid":false,"asymptote":false,"tikz":{"execpath":"lualatex","output-format":"svg","header-includes":["\\usepackage{tikz}","\\usepackage{pgfplots}","\\usepackage{pgf-pie}","\\usepackage{amsmath}","\\usepackage{amssymb}","\\usepackage{xcolor}","\\pgfplotsset{compat=1.9}","\\usepgfplotslibrary{fillbetween}","\\usetikzlibrary{angles}","\\usetikzlibrary{arrows.meta}","\\usetikzlibrary{arrows}","\\usetikzlibrary{backgrounds}","\\usetikzlibrary{bending}","\\usetikzlibrary{calc}","\\usetikzlibrary{shadows.blur}","\\usetikzlibrary{fit}","\\usetikzlibrary{intersections}","\\usetikzlibrary{positioning}","\\usetikzlibrary{shapes.geometric}","\\usetikzlibrary{shapes}","\\usetikzlibrary{quotes}","\\usetikzlibrary{decorations.pathmorphing}","\\usetikzlibrary{decorations.markings}","\\usetikzlibrary{matrix}","\\usepgfplotslibrary{dateplot}","\\usepgfplotslibrary{polar}","\\definecolor{Brown}{rgb}{0.65, 0.16, 0.16}","\\definecolor{BrownL}{rgb}{0.6, 0.4, 0.2}","\\definecolor{BrownLine}{rgb}{0.5, 0.3, 0.1}","\\definecolor{BackColor}{RGB}{255,255,229}","\\definecolor{BackLine}{RGB}{181,181,72}","\\definecolor{BlueD}{RGB}{62,100,125}","\\definecolor{BlueL}{RGB}{209,243,255}","\\definecolor{BlueLine}{RGB}{34,148,189}","\\definecolor{BrownL}{RGB}{233,222,220}","\\definecolor{BrownLine}{RGB}{143,120,116}","\\definecolor{Green}{rgb}{0.0, 0.5, 0.0}","\\definecolor{GreenD}{RGB}{40,117,40}","\\definecolor{GreenL}{RGB}{219,253,166}","\\definecolor{GreenLine}{RGB}{73,89,56}","\\definecolor{OliveL}{RGB}{230,227,191}","\\definecolor{OliveLine}{RGB}{173,166,10}","\\definecolor{OrangeL}{RGB}{250,212,175}","\\definecolor{OrangeLine}{RGB}{255,127,76}","\\definecolor{RedL}{RGB}{253,226,240}","\\definecolor{RedLine}{RGB}{201,20,110}","\\definecolor{Sepia}{rgb}{0.44, 0.26, 0.08}","\\definecolor{TextColor}{RGB}{224,224,224}","\\definecolor{VioletL}{RGB}{247,180,247}","\\definecolor{VioletL2}{RGB}{243,243,255}","\\definecolor{VioletLine}{RGB}{34,125,189}","\\definecolor{VioletLine2}{RGB}{169,136,229}"]}}},"editor":{"render-on-save":true},"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"comments":{"hypothesis":{"theme":"clean","openSidebar":false}},"lightbox":true,"mermaid":{"theme":"default"},"theme":{"light":["default","../../../assets/styles/style.scss"],"dark":["default","../../../assets/styles/style.scss","../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"pagetitle":"ML Systems Textbook","code-block-bg":true,"code-copy":true,"citation-location":"margin","sidenote":true,"anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"toc-expand":true,"toc-title":"On this page","number-depth":3,"quiz":"serving_quizzes.json","concepts":"serving_concepts.yml","glossary":"serving_glossary.json"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}