\begin{theindex}

  \item A/B Testing
    \subitem randomization unit, 28
    \subitem statistical model validation, 28
  \item Abstraction Debt
    \subitem missing ML interfaces, 13
  \item Ad Hoc MLOps
    \subitem manual workflows, 52
  \item Alert Fatigue
    \subitem operational risk, 46
  \item Artifact Versioning
    \subitem reproducibility requirement, 5
  \item Autoscaling
    \subitem ML workloads, 34

  \indexspace

  \item Blue-Green Deployment
    \subitem zero-downtime updates, 26
  \item Boundary Erosion
    \subitem dissolution of modularity, 10
  \item Bulkhead Pattern
    \subitem resource isolation, 54
  \item Byzantine Fault Tolerance
    \subitem ML-specific manifestations, 54

  \indexspace

  \item CACHE Principle
    \subitem change propagation, 10
  \item Canary Deployment
    \subitem risk-controlled rollout, 26
  \item Canary Testing
    \subitem gradual rollout, 25
  \item Centralized MLOps Team
    \subitem organizational pattern, 54
  \item CI/CD Pipelines
    \subitem ML-specific adaptations, 18
  \item Circuit Breaker Pattern
    \subitem cascade failure prevention, 43
  \item ClinAIOps
    \subitem continuous therapeutic monitoring, 58
    \subitem healthcare ML operations, 58
    \subitem MLOps comparison, 60
  \item Clinician-AI Loop
    \subitem professional oversight, 59
  \item Cohort-Based Monitoring
    \subitem subpopulation tracking, 39
  \item Compliance
    \subitem governance objective, 47
  \item Concept Drift
    \subitem changing relationships, 37
  \item Configuration Debt
    \subitem Facebook case study, 14
    \subitem parameter sprawl, 12
  \item Consistency Imperative
    \subitem training-serving parity, 5
  \item Containerization
    \subitem ML deployment, 34
  \item Continuous Therapeutic Monitoring
    \subitem healthcare AI, 58
  \item Continuous Training
    \subitem retraining decision framework, 21
  \item Controlled Deployment
    \subitem risk mitigation, 25
  \item Correction Cascades
    \subitem sequential dependencies, 11
    \subitem Zillow case study, 13
  \item Cost-Aware Automation
    \subitem retraining decisions, 6
  \item Counterfactual Analysis
    \subitem prediction debugging, 45
  \item Counterfactual Evaluation
    \subitem holdout groups, 39
  \item Cross-Functional Collaboration
    \subitem ML teams, 48
  \item CUPED
    \subitem variance reduction, 28

  \indexspace

  \item Data Dependency Debt
    \subitem unstable inputs, 12
  \item Data Drift, 37
  \item Data Freshness
    \subitem staleness detection, 41
  \item Data Management
    \subitem MLOps lifecycle, 14
    \subitem technical debt prevention, 14
  \item Data Pipelines
    \subitem automated workflows, 15
  \item Data Quality
    \subitem proactive monitoring, 39
  \item Data Quality Monitoring
    \subitem input validation, 39
  \item Data Versioning
    \subitem DVC tool, 15
    \subitem reproducibility, 17
  \item Data-Model Interface
    \subitem feature consistency, 3
  \item Dead Code
    \subitem experimental ML paths, 13
  \item Debugging
    \subitem canary deployment issues, 27
  \item Debugging Checklist
    \subitem runbook steps, 45
  \item Debugging Decision Tree
    \subitem systematic diagnosis, 44
  \item Degradation Equation
    \subitem operational implementation, 3
  \item Delayed Feedback
    \subitem A/B testing challenges, 28
  \item DevOps
    \subitem MLOps extension, 4
  \item DVC (Data Version Control)
    \subitem dataset versioning, 15

  \indexspace

  \item Edge AI
    \subitem deployment patterns, 33
    \subitem Oura Ring case study, 56
  \item Entanglement
    \subitem ML system coupling, 8
  \item Environment Parity
    \subitem production deployment, 3
  \item Experiment Tracking
    \subitem collaborative ML development, 48
  \item Explainability
    \subitem governance requirement, 47

  \indexspace

  \item F1 Score
    \subitem model evaluation, 25
  \item Fairness
    \subitem governance objective, 47
  \item Fairness Monitoring
    \subitem governance requirement, 47
  \item Feature Attribution
    \subitem debugging techniques, 44
  \item Feature Consistency
    \subitem training-serving alignment, 3
  \item Feature Contracts
    \subitem schema specification, 49
  \item Feature Selection
    \subitem automation, 21
  \item Feature Store
    \subitem training-serving consistency, 16
    \subitem Uber case study, 17
    \subitem Uber Michelangelo origin, 15
  \item Federated MLOps
    \subitem embedded engineers, 54
  \item Feedback Loops
    \subitem self-reinforcing bias, 12
    \subitem YouTube case study, 13

  \indexspace

  \item Glue Code
    \subitem integration overhead, 12
  \item GPU Utilization
    \subitem monitoring patterns, 35
  \item Guardrail Metrics
    \subitem A/B test constraints, 28

  \indexspace

  \item Holdout Test Set
    \subitem performance evaluation, 25
  \item Hyperparameter Tuning
    \subitem automated optimization, 21
  \item Hypertension
    \subitem ClinAIOps case study, 60

  \indexspace

  \item Immutable Artifacts
    \subitem model registration, 18
  \item Incident Response
    \subitem ML-specific challenges, 43
    \subitem structured processes, 43
  \item Inference Serving
    \subitem production infrastructure, 30
  \item Infrastructure as Code (IaC)
    \subitem ML systems, 34
  \item Interface Dependencies
    \subitem ML system challenges, 12
  \item Interleaving Experiments
    \subitem Netflix methodology, 39

  \indexspace

  \item Jupyter Notebooks
    \subitem production risks, 20

  \indexspace

  \item Kubernetes
    \subitem ML orchestration, 34
  \item Kullback-Leibler Divergence
    \subitem distribution comparison, 40
  \item KV-Cache
    \subitem LLM serving bottleneck, 36

  \indexspace

  \item Latency Budget
    \subitem end-to-end analysis, 30
  \item LIME
    \subitem local explanations, 47
  \item Lineage Tracking
    \subitem model provenance, 17
  \item Load Balancing
    \subitem ML serving, 32

  \indexspace

  \item Maturity Levels
    \subitem evolutionary stages, 52
  \item Michelangelo
    \subitem Uber ML platform, 17
  \item ML Team Roles
    \subitem organizational structure, 48
  \item ML Test Score
    \subitem production readiness, 51
  \item MLOps
    \subitem DevOps comparison, 4
    \subitem production deployment challenges, 1
  \item MLOps Anti-Patterns
    \subitem organizational failures, 54
  \item MLOps Fallacies
    \subitem automated retraining sufficiency, 62
    \subitem automatic consistency, 62
    \subitem DevOps equivalence, 62
  \item MLOps Pitfalls
    \subitem monitoring overconfidence, 63
    \subitem one-time deployment, 62
    \subitem organizational neglect, 62
  \item Model Debugging
    \subitem probabilistic failures, 44
    \subitem root cause analysis, 44
  \item Model Deployment
    \subitem packaging and serving, 26
  \item Model Governance
    \subitem proactive policies, 47
    \subitem transparency and compliance, 47
  \item Model Monitoring
    \subitem drift detection, 36
    \subitem Netflix case study, 39
    \subitem performance tracking, 36
  \item Model Optimization
    \subitem format conversion, 29
    \subitem framework comparison, 29
  \item Model Registry
    \subitem centralized artifact management, 29
    \subitem etymology, 18
    \subitem version management, 29
    \subitem version promotion, 18
  \item Model Validation
    \subitem candidate selection, 25
    \subitem pre-deployment assessment, 25
  \item Model Versioning
    \subitem artifact tracking, 17
  \item Model-Infrastructure Interface
    \subitem environment parity, 3

  \indexspace

  \item Near-Online Serving
    \subitem hybrid latency, 30
  \item Netflix
    \subitem ML monitoring at scale, 39
  \item Neural Architecture Search
    \subitem automated design, 21
  \item Notebook to Production
    \subitem handoff challenges, 49
  \item Novelty Effects
    \subitem A/B testing bias, 28

  \indexspace

  \item Observability
    \subitem system state inference, 36
  \item Offline Serving
    \subitem batch inference, 30
  \item On-Call Practices
    \subitem ML systems, 45
  \item On-Call Rotation
    \subitem ML-specific requirements, 45
  \item Online Learning
    \subitem incremental updates, 22
  \item Online Serving
    \subitem real-time inference, 30
  \item ONNX
    \subitem model interchange format, 29
  \item Operational Maturity
    \subitem system integration, 52
  \item Operational Mismatch
    \subitem ML vs traditional software, 2
  \item Oura Ring
    \subitem wearable ML case study, 56
  \item Over-the-Air (OTA) Updates
    \subitem model distribution, 33

  \indexspace

  \item Patient-AI Loop
    \subitem real-time monitoring, 59
  \item Patient-Clinician Loop
    \subitem shared decision-making, 59
  \item Photoplethysmography (PPG)
    \subitem wearable sensing, 60
  \item Pipeline Debt
    \subitem workflow fragmentation, 12
  \item Point-in-Time Correctness
    \subitem feature retrieval, 17
  \item Population Stability Index (PSI)
    \subitem drift quantification, 40
  \item Postmortem Documentation
    \subitem incident analysis, 44
  \item Production Readiness Review
    \subitem handoff criteria, 49
  \item Production-Monitoring Interface
    \subitem feedback loop, 4

  \indexspace

  \item Quantization
    \subitem serving optimization, 30

  \indexspace

  \item Reproducibility
    \subitem versioning principle, 5
  \item Request Batching
    \subitem throughput optimization, 32
  \item Retraining
    \subitem scheduled vs triggered, 21
  \item Retraining Cadence
    \subitem drift-triggered decisions, 4
  \item Retraining Economics
    \subitem cost-benefit optimization, 22
  \item Rollback Strategies
    \subitem safety mechanisms, 27

  \indexspace

  \item Scheduled Retraining
    \subitem fixed interval updates, 22
  \item Schema Validation
    \subitem structural data checks, 40
  \item Separation of Concerns
    \subitem MLOps layers, 5
  \item Service Level Agreement (SLA)
    \subitem ML systems, 30
  \item Service Level Objective (SLO)
    \subitem latency and error rates, 30
  \item Severity Classification
    \subitem incident prioritization, 43
  \item Shadow Deployment
    \subitem etymology, 26
    \subitem production validation, 26
  \item SHAP
    \subitem model interpretability, 47
  \item Shift Handoffs
    \subitem context transfer, 46
  \item Silent Failures
    \subitem gradual degradation, 5
  \item Slice Analysis
    \subitem subpopulation debugging, 44
  \item Stakeholder Communication
    \subitem business alignment, 49
    \subitem technical translation, 49
  \item Staleness Cost
    \subitem accuracy decay model, 23
  \item Statistical Process Control
    \subitem anomaly detection, 39

  \indexspace

  \item Technical Debt
    \subitem assessment rubric, 51
    \subitem ML system complexity, 8
    \subitem ML-specific drivers, 8
  \item Telemetry
    \subitem ML system observability, 4
  \item Tiered Escalation
    \subitem incident response, 46
  \item TinyML
    \subitem operational constraints, 33
  \item Tool-First Anti-Pattern
    \subitem fragmented responsibilities, 54
  \item Training Pipelines
    \subitem automated ML workflows, 20
  \item Training-Serving Skew
    \subitem diagnosis and prevention, 16
    \subitem silent accuracy degradation, 16
  \item Transparency
    \subitem governance objective, 47
  \item Triggered Retraining
    \subitem drift-based updates, 22

  \indexspace

  \item Undeclared Consumers
    \subitem hidden model dependencies, 12
    \subitem Tesla case study, 13

  \indexspace

  \item Verification Gap
    \subitem MLOps closure, 3

  \indexspace

  \item Workflow Orchestration
    \subitem pipeline automation, 15

\end{theindex}
