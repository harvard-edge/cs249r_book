% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Advanced Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Advanced Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol2.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.225\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Advanced}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Advanced Machine Learning Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume II)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Advanced\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~II}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume II}\label{welcome-to-volume-ii}
\addcontentsline{toc}{chapter}{Welcome to Volume II}

\markboth{Welcome to Volume II}{Welcome to Volume II}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume II extends the foundations into production-scale systems through
five parts:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations of Scale} --- Master the algorithms of
  scale. Learn how to coordinate computation across thousands of devices
  using Data, Tensor, and Pipeline parallelism, and understand the
  collective communication primitives and fault tolerance mechanisms
  that synchronize them.
\item
  \textbf{Part II: Building the Machine Learning Fleet} --- Build the
  physical computer. Architect the datacenter infrastructure,
  accelerators, and high-performance storage systems required to support
  distributed workloads at scale.
\item
  \textbf{Part III: Deployment at Scale} --- Serve the world. Navigate
  the shift from training to inference, push intelligence to the edge,
  and manage the operational lifecycle of production fleets.
\item
  \textbf{Part IV: Production Concerns} --- Harden the system. Address
  the non-functional requirements of privacy, security, robustness, and
  environmental sustainability in large-scale operations.
\item
  \textbf{Part V: Responsible AI at Scale} --- Shape the future. Explore
  responsible AI governance, AI for social good, and the emerging
  frontiers of AGI systems.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Foundational or equivalent} background in single-machine ML
  systems
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability
\item
  Familiarity with distributed systems concepts (networking,
  parallelism) is helpful for advanced topics
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Compute}\label{sec-compute}

\marginnote{\begin{footnotesize}

\emph{Gemini Pro 3 Prompt: A sweeping architectural visualization of a
modern AI datacenter infrastructure. The scene reveals a massive
facility with rows of GPU clusters arranged in pods, connected by
high-bandwidth networking fabric depicted as glowing fiber optic
pathways. Cooling systems appear as flowing blue currents between server
racks. The visualization includes multiple layers: physical
infrastructure at the bottom with power and cooling, compute
infrastructure in the middle with thousands of interconnected
accelerators, and orchestration software at the top represented as an
abstract control plane. Visual elements include resource managers
allocating workloads, capacity graphs showing utilization, and
geographic connections to other datacenters. The color scheme uses
industrial grays and silvers with accent colors of electric blue for
networking and amber for active computation. Photorealistic technical
illustration style suitable for infrastructure engineering
documentation. Rendered in the style of Nanobanana.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/infrastructure/images/png/cover_infrastructure.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does infrastructure---not algorithms---increasingly determine
who can participate in advancing machine learning?}

The algorithms for training large models are published in papers anyone
can read. The datasets are increasingly public or synthetically
generated. The frameworks are open source. Yet only a handful of
organizations can actually train frontier models, and that bottleneck is
infrastructure: the ability to acquire thousands of accelerators, power
them, cool them, connect them with sufficient bandwidth, and keep them
running reliably for months. This concentration reflects a shift in what
limits machine learning progress. When algorithms were the scarce
resource, a clever idea in a garage could change the field. Now that
scale dominates, the constraint is who can build and operate the
physical systems that make scale possible. The economics are
unforgiving: a 10,000-GPU cluster represents hundreds of millions in
capital, megawatts of continuous power draw, and operational complexity
that compounds with every added node. Infrastructure has become the moat
that separates organizations that can pursue ambitious goals from those
that can only follow where others lead.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, toptitle=1mm, coltitle=black, opacityback=0, colback=white, colframe=quarto-callout-tip-color-frame, left=2mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, leftrule=.75mm, titlerule=0mm, opacitybacktitle=0.6, colbacktitle=quarto-callout-tip-color!10!white, bottomtitle=1mm, breakable, arc=.35mm, bottomrule=.15mm, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  Calculate datacenter power requirements using PUE and thermal
  dissipation metrics
\item
  Evaluate cooling architectures for high-density GPU clusters using
  thermal capacity constraints
\item
  Apply the roofline model to identify compute-bound versus memory-bound
  workloads
\item
  Analyze accelerator selection by matching workload characteristics to
  hardware capabilities
\item
  Compute total cost of ownership by combining amortized CapEx and OpEx
\item
  Evaluate the trade-offs between GPU, TPU, and custom ASIC
  architectures for specific workloads
\end{itemize}

\end{tcolorbox}

\section{The Warehouse-Scale
Computer}\label{sec-compute-warehousescale-computer-384e}

We are now building the \textbf{Physical Layer} of the \textbf{Systems
Sandwich} (\textbf{?@sec-vol2-introduction}). This layer provides the
raw capabilities, FLOPS, Watts, and Bandwidth, that the
\textbf{Operational Layer} (Part I) demands. The physics of cooling and
power density here set the hard limits on how large our logical clusters
can scale.

The datacenter can be understood as a computer that executes learning
workloads. If datasets are the programs driving ML computation, then
infrastructure provides the execution engine.

We must move beyond viewing the datacenter as mere housing for servers
and instead understand it as a carefully engineered system where power,
cooling, and interconnects determine what computations are possible.

\subsection{Physical Infrastructure
Fundamentals}\label{sec-compute-physical-infrastructure-fundamentals-4d8a}

ML datacenters differ from traditional cloud facilities in three
critical dimensions. First, power density per rack reaches 5 to 10 times
higher than conventional levels. Second, cooling requirements demand
liquid rather than air-based solutions
(Figure~\ref{fig-cooling-topology} contrasts traditional air-cooled
architectures with direct-to-chip liquid systems capable of managing
700W per GPU). Third, physical layout must optimize for high-bandwidth
interconnects rather than flexible networking.

\subsubsection{Power Delivery and
Distribution}\label{sec-compute-power-delivery-distribution-90e2}

A single NVIDIA DGX H100 system consumes 10.2 kW at peak load. A rack
containing four such systems requires over 40 kW, compared to 5 to 10 kW
for traditional server racks. This power density\sidenote{\textbf{Power
density}: The power consumption per unit of datacenter floor space,
typically measured in kW per rack or kW per square meter. Traditional
enterprise datacenters design for 5-10 kW per rack. Modern GPU clusters
require 40-100+ kW per rack, demanding specialized power distribution
and cooling infrastructure. Power density directly limits how many GPUs
can be deployed in existing facilities. } changes power infrastructure
design.

\textbf{Utility and Backup Power.} Production ML facilities require
redundant power feeds, typically N+1 or 2N
configurations\sidenote{\textbf{N+1 and 2N redundancy}: Power redundancy
configurations where N is the capacity required to serve the load. N+1
provides one backup component (e.g., 3 UPS units where 2 are required),
allowing single-failure tolerance. 2N provides complete duplication
(e.g., 4 UPS units), allowing maintenance on one path while maintaining
failure protection on the other. ML training runs spanning weeks justify
2N redundancy to prevent catastrophic job loss. } where N represents the
load requirement. Uninterruptible power supplies bridge the gap during
utility failures, though the massive power draw of GPU clusters limits
battery backup duration to minutes rather than hours. Diesel generators
provide extended backup, while automatic transfer switches complete
failover within 10 to 15 seconds.

\textbf{Power Distribution Architecture.} Modern ML datacenters use a
tiered distribution model:

\begin{longtable}[]{@{}lrl@{}}
\toprule\noalign{}
\textbf{Distribution Level} & \textbf{Typical Voltage} &
\textbf{Purpose} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Utility feed} & 13.8-69 kV & Grid connection \\
\textbf{Substation transformer} & 480V (US) & Building distribution \\
\textbf{PDU (Power Distribution} & 208V & Rack-level distribution \\
\textbf{Unit)} & & \\
\textbf{Server PSU} & 12V DC & Component-level power \\
\end{longtable}

Figure~\ref{fig-power-hierarchy} traces power from grid connection at
13.8 kV through four voltage step-downs to the 12V DC rails that feed
individual GPU components:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/1a448f3c49428f22f02cab67b1a5929ec0bb2ee0.pdf}}

}

\caption{\label{fig-power-hierarchy}\textbf{Datacenter Power
Distribution}: Hierarchical power delivery for ML infrastructure.
Voltage steps down from utility-level 13.8 kV through 480V building
distribution to 208V rack distribution and finally 12V DC at the
component level. The 2N redundancy pattern at UPS and PDU tiers ensures
training jobs survive single-component failures during multi-week runs.
Each DGX H100 system draws 10.2 kW, requiring robust power
infrastructure to support racks exceeding 40 kW total load.}

\end{figure}%

\textbf{Power Usage Effectiveness.} The PUE
metric\sidenote{\textbf{Power Usage Effectiveness (PUE)}: An
industry-standard metric where values closer to 1.0 indicate greater
efficiency. A PUE of 2.0 means half the power goes to overhead (cooling,
lighting, power distribution), while 1.1 means only 10\% goes to
overhead. Google's most efficient datacenters achieve PUE of 1.06, while
typical enterprise facilities operate at 1.5-2.0. }, developed by The
Green Grid consortium in 2007 (\citeproc{ref-thegreengrid2007pue}{Grid
2007}), quantifies datacenter energy efficiency:

Every watt spent on cooling is a watt not spent on computation, an
overhead we call the \emph{cooling tax}. The following worked example
quantifies this tax for a realistic GPU cluster.

\phantomsection\label{callout-notebookux2a-1.1}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Cooling Tax}
\phantomsection\label{callout-notebook*-1.1}
\textbf{Problem}: You operate a \textbf{10 MW} GPU cluster. Electricity
costs \textbf{\$0.10/kWh}. How much money do you waste with inefficient
cooling (PUE 1.5) vs.~efficient liquid cooling (PUE 1.1)?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Compute Power}: 10 MW.
\item
  \textbf{Scenario A (Air Cooled, PUE 1.5)}: Total Power =
  \(10 \times 1.5 = 15 \text{ MW}\). Cooling uses 5 MW.
\item
  \textbf{Scenario B (Liquid Cooled, PUE 1.1)}: Total Power =
  \(10 \times 1.1 = 11 \text{ MW}\). Cooling uses 1 MW.
\item
  \textbf{Difference}: \(15 - 11 = 4 \text{ MW}\) of wasted power.
\item
  \textbf{Annual Cost}:
  \(4 \text{ MW} \times 24 \text{ h/day} \times 365 \text{ days} \times \$0.10/\text{kWh} = \mathbf{\$3.5 \text{ Million/year}}\).
\end{enumerate}

\textbf{The Systems Conclusion}: PUE is not just an environmental
metric; it is a massive OpEx lever. Improving PUE from 1.5 to 1.1 saves
enough money to buy \textasciitilde100 H100 GPUs every year.

\end{fbx}

A PUE of 1.0 represents perfect efficiency where all power goes to
computing. Traditional datacenters achieve PUE values of 1.5 to 2.0,
while hyperscale facilities target 1.1 to 1.2. ML datacenters face a
challenge. The extreme heat density of GPU clusters increases cooling
overhead, pushing PUE higher unless advanced cooling technologies are
deployed.

\subsubsection{Cooling Systems at
Scale}\label{sec-compute-cooling-systems-scale-2ccb}

At extreme rack densities, cooling ceases to be a facilities concern and
becomes a fundamental bound on computation, what we might call \emph{the
thermodynamic limit of intelligence}.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Thermodynamic Limit of Intelligence}
\phantomsection\label{callout-perspective*-1.2}
\textbf{The Hardest Constraint:} We often think of intelligence as
abstract information processing, but in the warehouse-scale computer, it
is a \textbf{heat engine}. Training a model is thermodynamically
equivalent to reversing entropy by organizing weights. By the Second Law
of Thermodynamics, this requires generating massive amounts of waste
heat. The density of intelligence
(\(\text{Tokens}/\text{sec}/\text{m}^2\)) is strictly limited by the
density of heat removal (\(\text{Watts}/\text{m}^2\)). \textbf{Power
density is not just a facility spec; it is the physical speed limit of
learning.}

\end{fbx}

Heat dissipation represents the primary constraint on ML cluster
density. An H100 GPU generates 700W of thermal output from a surface
area smaller than a dinner plate, producing heat flux comparable to a
nuclear reactor's fuel rod surface.

\textbf{Air Cooling Limitations.} Traditional air cooling becomes
impractical above 30 to 40 kW per rack. The physics are straightforward:
air's low heat capacity of approximately 1 kJ/kg-K requires massive
airflow to remove heat. A 40 kW rack requires roughly 10,000 CFM of
airflow, creating acoustic levels exceeding 80 dB and significant fan
power overhead.

\textbf{Hot Aisle/Cold Aisle Containment.} This architectural pattern
separates cold supply air from hot exhaust air using physical barriers.
Cold air enters through raised floor vents or overhead ducts, passes
through servers front-to-back, and exhausts into a contained hot aisle.
Containment improves cooling efficiency by preventing mixing, but cannot
solve the fundamental heat density challenge of modern GPU clusters.
Examine Figure~\ref{fig-cooling-topology} to see how containment manages
airflow separation in panel A, while panel B illustrates the
direct-to-chip liquid loops that bypass air entirely.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0c3edef3d954c866622325e86f31b1476b1606c4.pdf}}

}

\caption{\label{fig-cooling-topology}\textbf{Datacenter Cooling
Architectures}: Comparison of traditional hot-aisle/cold-aisle air
cooling versus modern direct-to-chip liquid cooling. The air cooling
diagram shows airflow management separating intake and exhaust streams,
while the liquid cooling diagram illustrates coolant loops directly
contacting high-power components to manage extreme heat density.}

\end{figure}%

\textbf{Direct-to-Chip Liquid Cooling.} Liquid cooling addresses heat
density through water's superior heat capacity of 4.2 kJ/kg-K, roughly
four times that of air. Cold plates mounted directly on GPUs and CPUs
transfer heat to circulating coolant, which flows to facility-level heat
exchangers. This approach enables rack densities exceeding 100 kW while
reducing cooling power consumption by 30 to 40 percent compared to air
cooling. The following note highlights how rapidly this technology has
become essential:

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, toptitle=1mm, coltitle=black, opacityback=0, colback=white, colframe=quarto-callout-note-color-frame, left=2mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Liquid Cooling Adoption}, leftrule=.75mm, titlerule=0mm, opacitybacktitle=0.6, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, breakable, arc=.35mm, bottomrule=.15mm, toprule=.15mm]

As of 2024, liquid cooling has transitioned from specialty option to
requirement for large-scale ML clusters. NVIDIA's GB200 NVL72 systems
require liquid cooling with no air-cooled option available. Facilities
planning for next-generation hardware must include liquid cooling
infrastructure from initial design.

\end{tcolorbox}

\textbf{Immersion Cooling.} The most aggressive thermal solution
submerges entire servers in dielectric fluid. Single-phase immersion
uses non-conductive oils that remain liquid, while two-phase systems use
fluids that boil at low temperatures, leveraging latent heat of
vaporization for efficient heat transfer. Immersion enables rack
densities exceeding 200 kW but requires specialized maintenance
procedures and component compatibility.

\subsubsection{Physical Layout
Optimization}\label{sec-compute-physical-layout-optimization-54f7}

ML cluster performance depends critically on physical topology. Unlike
web serving workloads where any server can handle any request,
distributed training requires specific communication patterns between
specific nodes. This dependency exists because, at warehouse scale, the
network fabric functions as the computer's system bus, making physical
distance a first-order performance constraint. We call this the
\emph{gradient bus perspective}.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Gradient Bus Perspective}
\phantomsection\label{callout-perspective*-1.3}
\textbf{The Network is the Bus:} In a warehouse-scale computer, the
InfiniBand/Ethernet fabric is not networking in the traditional sense;
it is the \textbf{System Bus} (like PCIe or NVLink) writ large. Packet
loss equals data corruption, latency equals memory access time, and
bandwidth equals memory throughput.

When we optimize physical layout, we are placing memory closer to
compute to minimize the speed-of-light latency of the Gradient Bus.

\end{fbx}

\textbf{Rack Density Considerations.} Higher density reduces cable
lengths and switch hops but concentrates power and cooling requirements.
Production deployments balance these factors based on workload
characteristics:

\begin{longtable}[]{@{}lrl@{}}
\toprule\noalign{}
\textbf{Workload Type} & \textbf{Typical Density} & \textbf{Limiting
Factor} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{LLM training} & 80-120 kW/rack & Cooling capacity \\
\textbf{Recommendation} & 30-50 kW/rack & CPU/memory balance \\
\textbf{inference} & & \\
\textbf{Vision training} & 60-80 kW/rack & Network bandwidth \\
\end{longtable}

\textbf{Cable Management.} High-bandwidth interconnects like InfiniBand
use copper cables for distances under 3 meters and fiber optics beyond.
Cable routing must maintain bend radius requirements (typically 10 times
cable diameter) while enabling airflow for any air-cooled components.
Active optical cables simplify routing but add latency and power
consumption compared to passive copper.

\subsection{Compute Infrastructure
Design}\label{sec-compute-compute-infrastructure-design-1353}

ML clusters combine multiple node types, each optimized for different
phases of the training and inference pipeline. Understanding these roles
clarifies infrastructure design decisions.

\subsubsection{GPU Cluster
Architectures}\label{sec-compute-gpu-cluster-architectures-e70f}

Modern GPU clusters are built from dense multi-GPU nodes connected via
high-bandwidth fabrics. Two reference architectures dominate production
deployments.

\textbf{DGX-Style Dense Nodes.} NVIDIA's DGX systems package 8 GPUs with
NVLink\sidenote{\textbf{NVLink}: NVIDIA's proprietary high-bandwidth
interconnect for GPU-to-GPU communication, providing 900 GB/s
bidirectional bandwidth in NVLink 4.0 (H100), compared to 64 GB/s for
PCIe Gen5. NVLink enables efficient tensor parallelism by allowing GPUs
to share memory across the interconnect with near-local-memory latency.
} interconnects, high-bandwidth networking, and substantial local
storage in a single chassis. The DGX H100 provides 8 H100 GPUs with 640
GB total HBM3\sidenote{\textbf{High Bandwidth Memory (HBM)}: A
3D-stacked DRAM technology that places memory dies vertically atop the
GPU die, connected via thousands of through-silicon vias (TSVs). HBM3
provides 3.35 TB/s bandwidth per GPU compared to 400 GB/s for DDR5. This
bandwidth is essential for memory-bound ML workloads where data
movement, not computation, limits performance. } memory, an NVSwitch
fabric enabling 900 GB/s GPU-to-GPU bandwidth, 8 ports at 400 Gbps for
InfiniBand or Ethernet, 2 Intel Xeon CPUs for preprocessing, and 30 TB
of NVMe storage for dataset staging. This dense packaging illustrates
\emph{the need for scale-up} when training trillion-parameter models.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, toptitle=1mm, coltitle=black, opacityback=0, colback=white, colframe=quarto-callout-note-color-frame, left=2mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Archetype A: The Need for Scale-Up}, leftrule=.75mm, titlerule=0mm, opacitybacktitle=0.6, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, breakable, arc=.35mm, bottomrule=.15mm, toprule=.15mm]

\textbf{Archetype A (The Trillion-Parameter LLM)} critically depends on
architectures like the DGX. The 900 GB/s NVLink bandwidth is not a
luxury but a requirement for \textbf{Tensor Parallelism}. When a single
model layer is split across 8 GPUs, every forward and backward pass
requires synchronizing activations. Standard PCIe Gen5 (64 GB/s) would
bottleneck these operations, stalling the training of Archetype A
models.

\end{tcolorbox}

This integrated design simplifies deployment but limits flexibility.
Each DGX H100 costs approximately \$300,000 (pricing reflects 2024
market conditions and fluctuates significantly based on supply, demand,
and generation transitions). This cost makes component-level upgrades
economically impractical.

\textbf{HGX Baseboard Designs.} For organizations building custom
infrastructure, NVIDIA's HGX baseboards provide the GPU and interconnect
components for integration into custom server designs. Cloud providers
and large enterprises use HGX to optimize for their specific power,
cooling, and networking requirements while maintaining compatibility
with NVIDIA's software stack.

\textbf{PCIe vs.~NVLink Configurations.} The choice between PCIe and
NVLink connectivity involves fundamental trade-offs that
Figure~\ref{fig-interconnect-topology} visualizes by contrasting
host-centric PCIe trees with peer-to-peer NVLink meshes:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/4e21e5de91157e87de0600857dd35c20fceb532d.pdf}}

}

\caption{\label{fig-interconnect-topology}\textbf{Node Interconnect
Topologies}: Contrast between PCIe-based commodity servers and
NVLink-based dense nodes. The PCIe topology shows CPU-centric
communication with higher latency and shared bandwidth bottlenecks,
while the NVLink topology demonstrates a high-bandwidth mesh allowing
direct peer-to-peer GPU communication essential for tensor parallelism.}

\end{figure}%

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interconnect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{PCIe Gen5 x16} & 64 GB/s & \textasciitilde1 microsecond &
Inference, small models \\
\textbf{NVLink 4.0} & 900 GB/s & \textasciitilde0.5 microsecond & Large
model training \\
\textbf{(bidirectional)} & & & \\
\end{longtable}

For models requiring tensor parallelism across GPUs, NVLink's
14\(\times\) bandwidth advantage directly translates to training
throughput. \textbf{?@sec-distributed-training-systems} examines how
tensor parallelism splits individual layers across GPUs, creating the
tight synchronization requirements that make NVLink essential.
PCIe-based systems suffice for data-parallel workloads where gradient
synchronization occurs less frequently.

\subsubsection{CPU Infrastructure
Roles}\label{sec-compute-cpu-infrastructure-roles-e47f}

While GPUs dominate ML computation, CPUs perform essential supporting
functions that bottleneck overall system performance if
under-provisioned.

\textbf{Preprocessing and Data Preparation.} Training data pipelines
involve decompression, augmentation, tokenization, and batching. These
operations execute on CPUs, which must supply data fast enough to keep
GPUs utilized. A common rule of thumb allocates 4 to 8 CPU cores per GPU
for training workloads, though data-intensive pipelines handling video
or large images require more.

\textbf{Feature Serving for Recommendation Systems.} Recommendation
models present a distinct infrastructure pattern. These systems combine
deep learning components with massive embedding tables that may exceed 1
TB. The embedding lookups are memory-bound CPU operations, while neural
network components benefit from GPU acceleration. Production
recommendation systems often use CPU-heavy nodes for embedding serving
alongside GPU nodes for model computation, connected via low-latency
networks.

\textbf{Control Plane and Orchestration.} Cluster management, job
scheduling, and monitoring run on dedicated CPU nodes separate from the
training cluster. This isolation prevents resource contention and
enables management operations even when the training cluster is fully
utilized.

\subsubsection{Hybrid
Architectures}\label{sec-compute-hybrid-architectures-1266}

Real production systems rarely use homogeneous hardware throughout.
Workload-aware placement matches job characteristics to appropriate
resources.

\textbf{Embedding Table Placement.} For recommendation systems with
embedding tables exceeding GPU memory, hybrid architectures place
embeddings in CPU DRAM while compute-intensive layers execute on GPUs.
Facebook's DLRM architecture (\citeproc{ref-naumov2019dlrm}{Naumov et
al. 2019}) pioneered this pattern, with embeddings distributed across
CPU nodes communicating with GPU nodes via high-bandwidth networks.

\textbf{Heterogeneous Scheduling.} Modern orchestration systems support
mixed node types within a single cluster. Kubernetes with GPU support
and Slurm with Generic Resource Scheduling enable jobs to request
specific hardware combinations. A training job might request 64 GPU
nodes for model computation plus 16 high-memory CPU nodes for embedding
tables as a coordinated allocation.

\subsection{Accelerator Selection by Workload
Type}\label{sec-compute-accelerator-selection-workload-type-93ab}

Having examined how different compute resources combine within a
cluster, we now turn to a more fundamental question: which accelerator
technologies should comprise that infrastructure? The accelerator
landscape has expanded beyond NVIDIA GPUs to include Google TPUs, custom
ASICs, and emerging architectures. Selection requires matching
accelerator characteristics to workload requirements.

\subsubsection{NVIDIA GPU
Ecosystem}\label{sec-compute-nvidia-gpu-ecosystem-a3a5}

NVIDIA maintains market dominance through integrated hardware-software
offerings. Understanding the architecture evolution clarifies capability
differences.

\textbf{Architecture Progression.} Each generation brings substantial
improvements in compute density and memory bandwidth:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{GPU}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{FP16} \textbf{Tensor} \textbf{TFLOPS}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{HBM} \textbf{Capacity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory} \textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{TDP}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{A100} & 312 & 80 GB HBM2e & 2.0 TB/s & 400W \\
\textbf{H100} & 989 & 80 GB HBM3 & 3.35 TB/s & 700W \\
\textbf{B100}* & \textasciitilde1,800 & 192 GB HBM3e & 8.0 TB/s &
\textasciitilde700W \\
\end{longtable}

*B100 specifications are preliminary estimates based on NVIDIA
announcements. Verify against official specifications for production
planning.

The TFLOPS figures above assume reduced-precision formats, which raises
a natural question: which floating-point format should training use? The
answer increasingly points to \emph{the case for bfloat16}.

\phantomsection\label{callout-perspectiveux2a-1.4}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Case for Bfloat16}
\phantomsection\label{callout-perspective*-1.4}
Deep learning training is resilient to low precision but sensitive to
dynamic range. Standard FP16 (IEEE 754) uses 5 bits for the exponent and
10 bits for the mantissa. This limited exponent range often causes
gradients to underflow to zero or overflow to infinity during training,
requiring complex loss scaling techniques to maintain stability.

\textbf{Bfloat16 (Brain Floating Point)}, developed by Google,
reallocates bits to use 8 bits for the exponent (matching FP32) and 7
bits for the mantissa. This matches the dynamic range of FP32, meaning
numbers that fit in FP32 also fit in BF16. This eliminates the need for
loss scaling and stabilizes training for large transformers. The
trade-off is lower precision from fewer mantissa bits, but neural
networks typically do not need high precision for individual weights,
only for aggregate accumulation (which is usually done in FP32).
Virtually all modern large language models, including GPT-4, PaLM, and
Llama, are trained using BF16.

\end{fbx}

The H100 delivers approximately 3\(\times\) the tensor TFLOPS of A100 at
1.75\(\times\) the power. To derive the efficiency improvement, A100
achieves 312 TF / 400 W = 0.78 TF/W, while H100 achieves 989 TF / 700 W
= 1.41 TF/W, yielding approximately 80\% improvement in FLOPS/watt.
Memory bandwidth increases proportionally, maintaining the
compute-to-memory ratio critical for transformer models.

\textbf{Tensor Core Utilization.} Tensor Cores\sidenote{\textbf{Tensor
Cores}: Specialized matrix-multiply-accumulate units introduced in
NVIDIA Volta (2017) that perform 4\$\times\$4 matrix operations in a
single clock cycle. Unlike general-purpose CUDA cores, Tensor Cores are
optimized for the fused multiply-add pattern \(D = A \times B + C\) that
dominates neural network computation. H100 Tensor Cores support FP8,
FP16, BF16, TF32, and INT8 formats. } accelerate matrix operations but
require specific data layouts and sizes for full utilization. Dimensions
should be multiples of 8 for FP16 or 16 for INT8 for optimal
performance. Underutilized Tensor Cores represent the most common source
of poor GPU efficiency in production, with many workloads achieving only
30 to 50 percent of theoretical peak FLOPS.

\textbf{NVLink Topology.} Within a node, NVSwitch provides
full-bandwidth connectivity between all GPUs. Across nodes, NVLink
Network, available in H100 and later, extends high-bandwidth
connectivity, though at reduced bandwidth compared to intra-node links.
\textbf{?@sec-communication-collective-operations} examines how
topology-aware job placement exploits these bandwidth asymmetries to
minimize communication overhead in multi-node training.

\subsubsection{Google TPU
Infrastructure}\label{sec-compute-google-tpu-infrastructure-f7cb}

Google's Tensor Processing Units offer an alternative architecture
optimized for matrix operations with a distinct programming model.

\textbf{TPU Pod Architecture.} TPUs connect via proprietary Inter-Chip
Interconnect\sidenote{\textbf{Inter-Chip Interconnect (ICI)}: Google's
proprietary chip-to-chip communication fabric integrated directly into
TPU silicon. ICI provides 6 links per chip at 100 GB/s each, enabling
the torus topology without external switches. This integration reduces
latency and power compared to discrete networking but limits flexibility
in topology configuration. } forming 2D or 3D
torus\sidenote{\textbf{Torus topology}: A network topology where nodes
connect in a ring structure along each dimension, with the last node
wrapping around to connect to the first. A 3D torus creates a cube-like
structure where each node connects to 6 neighbors. This topology
provides consistent latency for nearest-neighbor communication patterns
common in model parallelism, though AllReduce operations require
O(sqrt(N)) hops compared to O(log(N)) for fat-tree. } topologies. A TPU
v4 pod contains 4,096 chips with 1.1 exaFLOPS of aggregate compute.
Unlike GPU clusters where networking is separate from compute nodes, TPU
pods integrate the interconnect directly into the chip design.

\subsubsection{The Systolic Array
Advantage}\label{sec-compute-systolic-array-advantage-bbf6}

To understand why TPUs achieve such high efficiency for matrix
operations, we must examine their distinctive hardware design. The
defining feature of the TPU architecture is the \textbf{systolic array}.
Unlike CPUs or GPUs that function as general-purpose instruction
processors, a systolic array is a specialized grid of arithmetic units
designed for massive matrix multiplication.

The name ``systolic'' refers to the way data flows through the chip in
rhythmic waves, analogous to blood pumped by a heart. The architecture
employs a weight-stationary design where weights from matrix \(B\) are
loaded into the array and held stationary in local registers. Data from
matrix \(A\) flows in from the left, and partial sums flow down.

This design drastically reduces energy consumption. In a standard
architecture, every operation requires reading operands from registers
or memory, incurring high energy cost. In a systolic array, operands are
passed directly to the next neighbor unit, incurring very low energy
cost. A single memory access effectively amortizes over hundreds of
operations. This architecture explains why TPUs achieve extremely high
FLOPS/watt for dense matrix operations but struggle with sparse or
irregular computations that break the rhythmic data flow.

\textbf{TPU Slices and Multislice.} Users allocate TPU slices,
contiguous subsets of a pod. Multislice training connects multiple
slices via datacenter network for jobs exceeding single-slice capacity.
The programming model using JAX with pjit abstracts the physical
topology, enabling code portability across slice sizes.

\textbf{TPU vs.~GPU Trade-offs.} TPUs excel for large-scale training
with regular computation patterns:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Factor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TPU Advantage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{GPU Advantage}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Large transformer} & Optimized matrix units, & Broader operator
support \\
\textbf{training} & integrated interconnect & \\
\textbf{Custom operations} & Limited flexibility & CUDA extensibility \\
\textbf{Software ecosystem} & JAX-centric & PyTorch, TensorFlow, many
frameworks \\
\textbf{Availability} & Google Cloud only & Multiple cloud and
on-premise options \\
\end{longtable}

\subsubsection{Custom ASICs and Specialized
Accelerators}\label{sec-compute-custom-asics-specialized-accelerators-034b}

The ML accelerator landscape continues to diversify as organizations
optimize for specific workloads.

\textbf{Inference-Optimized Accelerators.} Training and inference
present different requirements. Training needs high-precision
arithmetic, large memory for activations and optimizer state, and high
interconnect bandwidth. Inference prioritizes low latency, high
throughput, and power efficiency. Recent analysis by Ma and Patterson
(\citeproc{ref-ma2024challenges}{Ma and Patterson 2024}) further refines
this distinction by separating the \emph{prefill} phase (compute-bound
processing of input tokens) from the \emph{decode} phase
(memory-bandwidth-bound generation of output tokens). The decode phase,
being autoregressive, requires reading the entire model weight set for
every token generated, making memory bandwidth, not FLOPS, the primary
determinant of performance. This ``memory wall'' drives the design of
inference-specialized chips that sacrifice raw compute density for
massive memory bandwidth and capacity, optimizing for the unique
token-by-token access pattern of large language models. Accelerators
like Google's TPU Inference chips and AWS Inferentia optimize for these
characteristics, achieving 2--4\(\times\) better performance per watt
than training-focused hardware for appropriate workloads.

\textbf{Emerging Architectures.} Several companies offer alternative
approaches. Cerebras WSE uses wafer-scale
integration\sidenote{\textbf{Wafer-scale integration}: Building an
entire processor on a full silicon wafer (850 cm\textsuperscript{2})
rather than dicing it into individual chips. Cerebras's WSE-2 contains
850,000 cores and 40 GB of on-chip SRAM, eliminating off-chip memory
bandwidth bottlenecks. The approach requires novel solutions for defect
tolerance and power delivery, as traditional packaging assumes perfect
dies. } to place an entire ML accelerator on a single silicon wafer,
eliminating chip-to-chip communication for models that fit on-chip.
Graphcore IPU employs a Bulk Synchronous Parallel\sidenote{\textbf{Bulk
Synchronous Parallel (BSP)}: An execution model where computation
proceeds in supersteps: parallel computation, global communication, then
barrier synchronization. Graphcore's IPU implements BSP at the hardware
level, with all 1,472 cores executing the same phase simultaneously.
This deterministic execution simplifies debugging but requires all
operations to complete within time bounds. } execution model with
distributed on-chip memory targeting sparse and dynamic workloads.
SambaNova provides reconfigurable dataflow architecture for enterprise
AI applications.

These alternatives find niches where their architectural trade-offs
align with workload requirements, though NVIDIA and Google maintain
dominant market positions for general ML training.

\subsection{Quantitative Infrastructure
Analysis}\label{sec-compute-quantitative-infrastructure-analysis-aabb}

Effective infrastructure decisions require quantitative comparison
across accelerator options and workload types.

\textbf{FLOPS per Watt Comparison.} Energy efficiency varies
significantly across accelerator types and precision levels:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{FP16 TFLOPS}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{TDP (Watts)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{TFLOPS/Watt}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NVIDIA H100 SXM} & 989 & 700 & 1.41 \\
\textbf{NVIDIA H100 PCIe} & 756 & 350 & 2.16 \\
\textbf{Google TPU v5p} & 459 & 250-400* & 1.1-1.8 \\
\textbf{AWS Trainium} & 210 & 150 (estimated) & 1.40 \\
\end{longtable}

*TPU power varies significantly by deployment configuration and is not
officially published. Direct TFLOPS/Watt comparisons across
architectures are problematic because utilization profiles differ. These
figures should be treated as approximate.

The PCIe variant's higher efficiency reflects reduced interconnect
power, acceptable for inference but limiting for distributed training.

\textbf{Memory Bandwidth Utilization.} Different model types exhibit
distinct memory access patterns. LLM training is memory-bound for
attention computation, achieving 70 to 85 percent bandwidth utilization.
CNN training is compute-bound for convolutions, reaching 30 to 50
percent bandwidth utilization. Recommendation inference is memory-bound
for embeddings, often exceeding available bandwidth. Understanding these
patterns guides accelerator selection. Memory-bound workloads benefit
from HBM3's bandwidth improvements, while compute-bound workloads
prioritize FLOPS per dollar.

\textbf{The Roofline Model.} The roofline
model\sidenote{\textbf{Roofline model}: A visual performance model
developed at Berkeley that plots achievable FLOPS against arithmetic
intensity (FLOPS per byte of memory traffic). The ``roofline'' consists
of a sloped region (memory-bound, limited by bandwidth) and a flat
region (compute-bound, limited by peak FLOPS). The intersection point,
called the ridge point, indicates where workloads transition between
these regimes. This model helps identify whether to optimize for compute
or memory access. } (\citeproc{ref-williams2009roofline}{Williams,
Waterman, and Patterson 2009}) provides a systematic framework for
understanding whether workloads are compute-bound or memory-bound.
Achievable performance is limited by the minimum of peak compute and
memory bandwidth:

The roofline model hinges on a single metric, \emph{arithmetic
intensity}, that determines which resource constrains a given workload.

\phantomsection\label{callout-definitionux2a-1.5}
\begin{fbx}{callout-definition}{Definition:}{Arithmetic Intensity}
\phantomsection\label{callout-definition*-1.5}
\textbf{Arithmetic Intensity} is the ratio of floating-point operations
(FLOPs) performed to bytes of memory accessed (Bytes) during a
computation. It determines whether a workload is \emph{compute-bound}
(limited by processor speed) or \emph{memory-bound} (limited by
bandwidth).

\end{fbx}

When general-purpose processors cannot deliver sufficient arithmetic
intensity, architects turn to \emph{domain-specific architectures
(DSAs)} that trade generality for efficiency.

\phantomsection\label{callout-definitionux2a-1.6}
\begin{fbx}{callout-definition}{Definition:}{Domain-Specific Architecture (DSA)}
\phantomsection\label{callout-definition*-1.6}
\textbf{\emph{Domain-Specific Architecture (DSA)}} is a class of
processors tailored to a specific domain of workloads (like deep
learning) rather than general-purpose computation. DSAs (such as TPUs)
trade flexibility for performance per watt by optimizing memory
hierarchies and arithmetic units specifically for the domain's dominant
operations (e.g., matrix multiplication).

\end{fbx}

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/e760ae7d636e471ae31ea28e26b3c6208179898b.pdf}}

}

\caption{\label{fig-roofline-model}\textbf{Roofline Performance Model}:
A visual representation of performance limits plotting achievable FLOPS
against arithmetic intensity (FLOPS/Byte). The slanted ``roof''
represents the memory-bound region where bandwidth constrains
performance, while the flat ``roof'' represents the compute-bound region
limited by peak processor throughput. Operational points for LLMs
typically fall under the slanted roof, indicating memory bandwidth
dependence.}

\end{figure}%

\[
\text{Achievable FLOPS} = \min\left(\text{Peak Compute}, \text{Memory Bandwidth} \times \text{Arithmetic Intensity}\right)
\]

Arithmetic intensity measures FLOPS per byte of memory traffic.
Figure~\ref{fig-roofline-model} reveals that workloads fall into two
distinct regimes: the sloped region where memory bandwidth limits
performance (LLM attention at 50-100 FLOP/byte) and the flat region
where peak compute limits performance (convolutions at 200+ FLOP/byte).
The ``ridge point'' where these limits intersect determines which
workloads benefit from each resource:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak Compute} \textbf{(TF FP16)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory BW} \textbf{(TB/s)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Ridge Point} \textbf{(FLOP/byte)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{H100 SXM} & 989 & 3.35 & 295 \\
\textbf{A100 80GB} & 312 & 2.0 & 153 \\
\textbf{TPU v4} & 275 & 1.2 & 229 \\
\end{longtable}

Most LLM training operates at 50 to 100 FLOP/byte arithmetic intensity,
well below the ridge point, making these workloads memory-bound. At 75
FLOP/byte on H100, achievable performance is \(3.4 \times 75 = 255\) TF,
only 26 percent of peak compute. This explains why production training
achieves 30 to 50 percent of theoretical FLOPS. The bottleneck is memory
bandwidth, not compute capacity.

CNN training with large batch sizes operates near 200 FLOP/byte,
approaching the ridge point where both resources limit performance.
Recommendation inference with random embedding lookups operates at
extremely low arithmetic intensity of 1 to 10 FLOP/byte, memory-bound
regardless of accelerator choice.

These roofline gaps motivate a practical efficiency metric, \emph{Model
FLOPs Utilization (MFU)}, that measures what fraction of theoretical
peak a workload actually achieves.

\phantomsection\label{callout-notebookux2a-1.7}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Model FLOPs Utilization (MFU)}
\phantomsection\label{callout-notebook*-1.7}
\textbf{The Problem}: You buy an H100 GPU advertised at 989 TFLOPS
(FP16). Your training logs show it's only processing data at a rate
equivalent to 350 TFLOPS. Where did the performance go?

\textbf{Definition}:
\[ \text{MFU} = \frac{\text{Achieved FLOPS}}{\text{Peak Theoretical FLOPS}} \]

\textbf{Calculation for Transformer Training}: For a model with \(P\)
parameters trained on \(D\) tokens in \(T\) seconds:
\[ \text{Achieved FLOPS} \approx \frac{6 \cdot P \cdot D}{T} \]
\emph{(Factor of 6 accounts for forward + backward pass operations per
parameter)}.

\textbf{Worked Example}:

\begin{itemize}
\tightlist
\item
  \textbf{Model}: 70B parameters (\(70 \times 10^9\))
\item
  \textbf{Data}: 100B tokens
\item
  \textbf{Time}: 14 days (\(1.2 \times 10^6\) seconds) on 512 H100s.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Total FLOPs Required}:
  \(6 \cdot (70 \times 10^9) \cdot (100 \times 10^9) = 4.2 \times 10^{22}\)
  FLOPs.
\item
  \textbf{Achieved System Throughput}:
  \(\frac{4.2 \times 10^{22}}{1.2 \times 10^6} = 3.5 \times 10^{16}\)
  FLOPS (\(35\) PFLOPS).
\item
  \textbf{Per-GPU Throughput}:
  \(35 \text{ PFLOPS} / 512 = 68 \text{ TFLOPS}\).
\item
  \textbf{MFU}: \(68 / 989 \approx \mathbf{6.8\%}\).
\end{enumerate}

\textbf{Conclusion}: This low MFU indicates severe bottlenecks: likely
memory bandwidth (waiting for weights) or communication overhead
(waiting for AllReduce). Good MFU for LLMs is 40-50\%.

\end{fbx}

\textbf{Cost per PFLOP.} Infrastructure economics depend on utilization
and workload fit:

\[
\text{Effective Cost per PFLOP} = \frac{\text{Hardware Cost} + \text{3-year OpEx}}{\text{Peak PFLOPS} \times \text{Average Utilization} \times 3 \text{ years}}
\]

For a DGX H100 at \$300,000 with \$50,000 annual power and cooling
costs, achieving 50 percent average utilization yields an effective cost
of approximately \$0.35 per PFLOP-hour. Cloud instances at \$30 per hour
for equivalent hardware cost \$0.15 per PFLOP-hour at 100 percent
utilization, but on-premises becomes favorable above 40 percent
sustained utilization over three years. However, this utilization
assumption warrants careful examination:

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, toptitle=1mm, coltitle=black, opacityback=0, colback=white, colframe=quarto-callout-warning-color-frame, left=2mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Utilization Reality}, leftrule=.75mm, titlerule=0mm, opacitybacktitle=0.6, colbacktitle=quarto-callout-warning-color!10!white, bottomtitle=1mm, breakable, arc=.35mm, bottomrule=.15mm, toprule=.15mm]

Quoted peak FLOPS numbers assume perfect utilization. Production
training jobs typically achieve 30 to 50 percent of peak
(\citeproc{ref-mattson2020mlperf}{Mattson et al. 2020}) due to
communication overhead, data pipeline stalls, and suboptimal kernel
efficiency. Infrastructure planning must account for realistic
utilization rates rather than theoretical peaks.

\end{tcolorbox}

This infrastructure foundation enables the distributed training
strategies examined in subsequent chapters.
\textbf{?@sec-distributed-training-systems} builds on these physical
capabilities to develop data, tensor, and pipeline parallelism
strategies, while \textbf{?@sec-communication-collective-operations}
analyzes the collective operations that these networks must support. The
physical constraints examined here, particularly power delivery, cooling
capacity, and interconnect topology, ultimately determine what scale of
training is achievable.

\section{Total Cost of Ownership
Analysis}\label{sec-compute-total-cost-ownership-analysis-0be4}

Understanding the complete financial picture of ML infrastructure
requires moving beyond simple hardware acquisition costs to
comprehensive Total Cost of Ownership (TCO) analysis. This section
provides quantitative frameworks for evaluating infrastructure
investments, comparing deployment strategies, and optimizing long-term
operational efficiency.

\subsection{Capital Expenditure
Components}\label{sec-compute-capital-expenditure-components-2d8e}

Capital expenditure\sidenote{\textbf{CapEx vs OpEx}: Capital expenditure
(CapEx) covers upfront asset purchases (hardware, construction) that are
depreciated over time, while operational expenditure (OpEx) covers
ongoing costs (power, staff, cloud fees) that are expensed immediately.
Cloud computing shifts costs from CapEx to OpEx, which affects financial
planning, tax treatment, and budget approval processes differently
across organizations. } encompasses all upfront investments required to
establish ML infrastructure. These costs are typically amortized over 3
to 5 years, though the rapid pace of GPU advancement often compresses
effective useful life to shorter periods.

\subsubsection{Hardware Costs}\label{sec-compute-hardware-costs-5bef}

GPU and accelerator acquisition represents the dominant CapEx component
for ML infrastructure. Current market pricing reflects both performance
capabilities and supply constraints.

\begin{longtable}[]{@{}lrrr@{}}
\toprule\noalign{}
\textbf{System} & \textbf{Base Cost} & \textbf{Memory Config} &
\textbf{Cost per PFLOP} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{DGX H100} & \textasciitilde\$300,000 & 640 GB HBM3 &
\textasciitilde\$75,000 \\
\textbf{DGX B100}* & \textasciitilde\$450,000 & 1.4 TB HBM3e &
\textasciitilde\$25,000 \\
\textbf{HGX H100 (8-way)} & \textasciitilde\$250,000 & 640 GB HBM3 &
\textasciitilde\$62,500 \\
\textbf{TPU v5p Pod} & Variable & 95 GB HBM & \textasciitilde\$30,000 \\
\end{longtable}

*B100 pricing is estimated. All prices reflect approximate 2024 market
conditions and should be verified for current planning. GPU pricing
fluctuates 20 to 40 percent based on supply constraints and generation
transitions.

Server and storage costs add substantial overhead beyond accelerators. A
complete DGX H100 deployment requires NVMe storage at \$15,000 to
\$30,000 per node, high-speed networking cards at \$8,000 to \$15,000,
and rack infrastructure at \$5,000 to \$10,000. Storage architecture for
large-scale training demands parallel file systems capable of sustaining
the I/O bandwidth required by hundreds of GPUs, with enterprise
solutions like Lustre or GPFS adding \$500 to \$1,000 per terabyte of
high-performance capacity.

Networking equipment costs scale superlinearly with cluster size due to
the hierarchical nature of high-bandwidth fabrics. A 256-GPU cluster
using InfiniBand HDR requires approximately \$800,000 to \$1,200,000 in
networking equipment.

\[C_{\text{network}} = N_{\text{switches}} \cdot P_{\text{switch}} + N_{\text{cables}} \cdot P_{\text{cable}} + N_{\text{adapters}} \cdot P_{\text{adapter}}\]

For a 256-GPU deployment with 2:1 oversubscription:

\[C_{\text{network}} \approx 32 \times \$15,000 + 512 \times \$800 + 256 \times \$3,000 \approx \$1,660,000\]

Refresh cycle planning significantly impacts TCO calculations. GPU
generations advance every 2 to 3 years with typical performance
improvements of 2--3\(\times\) per generation. Organizations must
balance the benefits of newer hardware against the disruption costs of
migration. A common strategy employs staggered refresh cycles, replacing
25 to 33\% of infrastructure annually to maintain competitive capability
while avoiding wholesale replacement costs.

\subsubsection{Facility Costs}\label{sec-compute-facility-costs-f185}

Datacenter construction costs range from \$7 to \$12 million per
megawatt of IT capacity for purpose-built facilities. ML workloads, with
their high power density requirements (30 to 50 kW per rack versus 5 to
10 kW for traditional compute), demand specialized cooling
infrastructure that increases construction costs by 20 to 40\%.

Power infrastructure represents a substantial portion of facility
investment. Electrical distribution systems including transformers,
switchgear, uninterruptible power supplies (UPS), and power distribution
units (PDUs) typically cost \$2 to \$4 million per megawatt. Redundancy
requirements (N+1 or 2N configurations) can double these costs for
mission-critical deployments.

Cooling systems for high-density ML infrastructure increasingly require
liquid cooling solutions. Direct-to-chip liquid cooling adds \$50,000 to
\$100,000 per rack in capital costs but enables the power densities
required for modern GPU configurations. The DGX H100 systems referenced
in our datacenter architecture discussion require liquid cooling for
sustained operation, representing a non-optional facility cost.

\subsection{Operational Expenditure
Components}\label{sec-compute-operational-expenditure-components-3e68}

Operational expenditure (OpEx) captures ongoing costs that accumulate
throughout infrastructure lifetime. For ML systems, power costs and
specialized staffing dominate this category.

\subsubsection{Power Costs}\label{sec-compute-power-costs-de49}

Electricity represents the largest operational cost for ML
infrastructure. Power costs vary dramatically by geography, with
industrial rates ranging from \$0.04/kWh in regions with abundant
hydroelectric power to \$0.20/kWh in constrained markets. The following
worked example demonstrates how these costs accumulate for a typical
system:

\phantomsection\label{callout-notebookux2a-1.8}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Annual Power Costs}
\phantomsection\label{callout-notebook*-1.8}
\textbf{Scenario}: Calculating the electricity bill for a single DGX
H100 system. \textbf{Parameters}:

\begin{itemize}
\tightlist
\item
  \textbf{Power}: 10.2 kW (max load)
\item
  \textbf{PUE}: 1.15 (efficient liquid cooling)
\item
  \textbf{Utilization}: 80\% (high sustained load)
\item
  \textbf{Rate}: \$0.08/kWh (industrial rate)
\end{itemize}

\textbf{Calculation}:
\[ \text{Annual Cost} = P_{\text{system}} \times \text{PUE} \times H_{\text{hours}} \times R_{\text{rate}} \times U_{\text{util}} \]
\[ = 10.2 \times 1.15 \times 8760 \times 0.08 \times 0.8 \]
\[ \approx \mathbf{\$6,575 \text{ per year}} \]

\textbf{Total 3-Year OpEx}: \(\approx \$20,000\) (just for power).
Start-up CapEx is \textasciitilde\$300k.

\end{fbx}

Electricity pricing models significantly impact operational costs.
Time-of-use pricing creates opportunities for training workload
scheduling during off-peak hours (typically nights and weekends),
potentially reducing power costs by 20 to 40\%. Demand charges, which
price peak power consumption, incentivize workload smoothing to avoid
utilization spikes.

Renewable energy considerations extend beyond environmental
responsibility to economic optimization. Power Purchase Agreements
(PPAs) for renewable energy often provide long-term price stability,
hedging against electricity market volatility. Many organizations target
100\% renewable energy matching through a combination of on-site
generation, PPAs, and Renewable Energy Certificates (RECs).
\textbf{?@sec-sustainable-ai} develops the comprehensive framework for
quantifying environmental impact, including lifecycle carbon assessment
and the geographic optimization strategies that can reduce emissions by
50 to 80\% through thoughtful infrastructure placement.

\subsubsection{Staffing and
Operations}\label{sec-compute-staffing-operations-69aa}

ML infrastructure requires specialized operational expertise across
multiple domains. Staffing costs often represent 15 to 25\% of total
operational expenditure for well-run facilities.

Hardware operations teams manage physical infrastructure including
installation, maintenance, and failure response. For clusters of 500+
GPUs, dedicated hardware technicians are essential, with typical ratios
of 1 technician per 200 to 400 GPUs depending on hardware heterogeneity
and SLA requirements.

Software platform teams maintain the scheduling systems, container
infrastructure, and ML frameworks that enable productive use of hardware
resources. These roles command premium compensation due to the
specialized intersection of systems engineering and ML expertise
required.

Utilization monitoring represents both a staffing function and a key
lever for TCO optimization. Continuous monitoring of GPU utilization,
memory bandwidth, and job efficiency enables identification of
optimization opportunities. Organizations achieving 70\%+ sustained GPU
utilization versus the more common 30 to 50\% effectively halve their
per-computation infrastructure costs.

\subsection{Build vs.~Buy
Analysis}\label{sec-compute-build-vs-buy-analysis-44c2}

The fundamental infrastructure decision is whether to operate private
infrastructure or consume cloud capacity. This choice involves complex
trade-offs that depend on workload characteristics, scale, and
organizational capabilities.

\subsubsection{Cloud vs.~On-Premises
Trade-offs}\label{sec-compute-cloud-vs-onpremises-tradeoffs-9be4}

Cloud computing offers compelling advantages for specific use cases.
Variable workloads with unpredictable demand benefit from cloud
elasticity, avoiding stranded capacity during low-demand periods.
Experimentation and research phases, where hardware requirements remain
uncertain, benefit from the ability to test different configurations
without capital commitment. Geographic distribution requirements for
inference serving often favor cloud deployment due to the substantial
investment required for multi-region presence.

On-premises infrastructure wins economically under sustained high
utilization. The break-even analysis requires comparing amortized CapEx
plus OpEx against equivalent cloud costs:

\[\text{Break-even utilization} = \frac{C_{\text{cloud}} \times H_{\text{annual}}}{\frac{C_{\text{capex}}}{Y_{\text{amortization}}} + C_{\text{opex}}}\]

Consider a DGX H100 system with \$300,000 CapEx, 3-year amortization,
and \$25,000 annual OpEx (power, maintenance, proportional staff). Cloud
equivalent (8x H100 instance at \textasciitilde\$25/hour):

\[\text{Break-even} = \frac{25 \times 8,760}{\frac{300,000}{3} + 25,000} = \frac{219,000}{125,000} \approx 1.75\]

This calculation suggests on-premises becomes favorable when utilization
exceeds approximately 57\% (1/1.75). In practice, organizations report
break-even utilization thresholds of 40 to 60\% depending on specific
cloud pricing and operational efficiency.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\textbf{Factor} & \textbf{Favors Cloud} & \textbf{Favors On-Premises} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Utilization} & \textless40\% average & \textgreater60\%
sustained \\
\textbf{Workload pattern} & Variable, bursty & Steady, predictable \\
\textbf{Data volume} & Moderate & Petabyte-scale \\
\textbf{Time horizon} & \textless2 years & \textgreater3 years \\
\textbf{Team capability} & Limited ops staff & Strong infrastructure \\
\end{longtable}

Hybrid strategies combine cloud burst capacity with on-premises baseline
infrastructure. Organizations maintain on-premises systems sized for
typical load (e.g., 60th percentile demand) while using cloud for peak
periods. This approach captures most on-premises economic benefits while
retaining cloud flexibility.

\subsubsection{Reserved Capacity vs.~Spot
Instances}\label{sec-compute-reserved-capacity-vs-spot-instances-8c44}

Cloud providers offer commitment discount programs that substantially
reduce effective pricing. Reserved instances with 1-year commitments
typically offer 30 to 40\% discounts, while 3-year commitments reach 50
to 60\% discounts relative to on-demand pricing. These discounts shift
cloud economics but introduce utilization risk similar to on-premises
ownership.

Spot instance strategies enable dramatic cost reduction (60 to 80\%
below on-demand) for fault-tolerant training workloads. Effective spot
utilization requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Checkpoint integration}: Training frameworks must save state
  frequently enough that spot interruption costs remain acceptable.
  Modern distributed training checkpoints every 10 to 30 minutes,
  limiting maximum lost computation.
\item
  \textbf{Fallback mechanisms}: Automated job migration to alternative
  instance types or regions when spot capacity becomes unavailable.
\item
  \textbf{Heterogeneous training}: Frameworks capable of operating
  across mixed instance types to maximize spot availability.
\end{enumerate}

The effective spot discount must account for interruption overhead:

\[C_{\text{effective}} = C_{\text{spot}} \times (1 + R_{\text{interrupt}} \times T_{\text{recovery}})\]

where \(R_{\text{interrupt}}\) is the hourly interruption rate and
\(T_{\text{recovery}}\) is recovery time as a fraction of checkpoint
interval. With 5\% hourly interruption rate and 10-minute recovery on
30-minute checkpoints:

\[C_{\text{effective}} = 0.30 \times C_{\text{ondemand}} \times (1 + 0.05 \times 0.33) \approx 0.31 \times C_{\text{ondemand}}\]

Even accounting for interruption overhead, spot instances provide
compelling economics for training workloads with proper checkpoint
infrastructure.

\subsection{Comprehensive TCO
Model}\label{sec-compute-comprehensive-tco-model-e13d}

A complete TCO model integrates capital and operational components
across the infrastructure lifetime:

\[\text{TCO} = \sum_{t=1}^{Y} \frac{C_{\text{capex}}^{(t)} + C_{\text{opex}}^{(t)}}{(1+r)^t}\]

where \(r\) is the discount rate reflecting cost of capital. For a
256-GPU cluster over 4 years:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year 1}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year 2}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year 3}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year 4}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Hardware CapEx} & \$9,600,000 & \$0 & \$0 & \$3,200,000 \\
\textbf{Network CapEx} & \$1,660,000 & \$0 & \$0 & \$0 \\
\textbf{Power (at 70\% util)} & \$1,690,000 & \$1,690,000 & \$1,690,000
& \$1,690,000 \\
\textbf{Maintenance} & \$480,000 & \$576,000 & \$691,000 & \$829,000 \\
\textbf{Staff (allocated)} & \$800,000 & \$840,000 & \$882,000 &
\$926,000 \\
\textbf{Annual Total} & \$14,230,000 & \$3,106,000 & \$3,263,000 &
\$6,645,000 \\
\end{longtable}

Figure~\ref{fig-tco-breakdown} reveals the cost structure's evolution
over a four-year lifecycle: hardware CapEx dominates Year 1, but
accumulated operational expenses (power and staffing) eventually exceed
the initial capital investment, making sustained utilization the
critical economic lever:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/9adc94f3ef168615107c0d8b6dc85e7beb7d81ca.pdf}}

}

\caption{\label{fig-tco-breakdown}\textbf{Total Cost of Ownership
Breakdown}. Analysis of infrastructure costs over a 4-year lifecycle.
While hardware CapEx is the largest initial outlay, operational costs
(Power and Staffing) accumulate to exceed hardware costs over the
system's life. High utilization is key to amortizing these fixed and
ongoing costs.}

\end{figure}%

The NPV at 8\% discount rate equals approximately \$24.1 million,
yielding a 4-year cost per GPU-hour of \$4.30 at 70\% utilization. This
compares favorably to cloud A100 pricing of
\(3-4/hour only when accounting for the H100's 3\)\times\$ performance
advantage, yielding effective cost per computation approximately 40\%
below cloud alternatives at this utilization level.

Power cost sensitivity analysis reveals the importance of electricity
pricing in deployment decisions. A \$0.04/kWh difference in electricity
rates shifts the 4-year TCO by approximately \$2.7 million for a 256-GPU
cluster, potentially changing the optimal deployment strategy.
Organizations with access to low-cost renewable energy enjoy structural
cost advantages that compound over multi-year infrastructure
investments.

\section{Case Studies}\label{sec-compute-case-studies-8da7}

The infrastructure patterns examined in previous sections combine in
different configurations depending on workload characteristics and
organizational constraints. Four production deployments illustrate how
datacenter architecture, networking, and resource management decisions
interact to enable distinct ML workloads. Each case study represents a
different point in the design space: GPU-centric dense training,
TPU-based transformer optimization, hybrid CPU-GPU recommendation
serving, and custom silicon for domain-specific acceleration.

\subsection{NVIDIA DGX SuperPOD
Architecture}\label{sec-compute-nvidia-dgx-superpod-architecture-d2cc}

The DGX SuperPOD represents NVIDIA's reference architecture for
large-scale training, combining the dense GPU packaging of DGX systems
with purpose-built networking. While \textbf{?@sec-networking} examines
the SuperPOD's networking topology, this case study addresses the
complete system architecture including physical deployment, management
infrastructure, and operational characteristics.

\subsubsection{Physical Layout and Cooling
Integration}\label{sec-compute-physical-layout-cooling-integration-7ac8}

A production SuperPOD deployment with 512 DGX H100 systems (4096 GPUs)
occupies approximately 2000 square meters of datacenter floor space. The
layout follows a pod-based organization where groups of 32 DGX systems
share common power and cooling infrastructure. Each pod dissipates over
300 kW, requiring direct liquid cooling loops with facility-level heat
exchangers.

The cooling architecture uses a closed-loop system with water
temperature maintained at 35 to 45C entering the cold plates. Unlike
traditional datacenter cooling that targets low air temperatures,
warm-water cooling improves efficiency by enabling free cooling in
moderate climates. Heat removed from GPU cold plates transfers to
building cooling towers without mechanical refrigeration for ambient
temperatures below 25C.

Power distribution follows the N+1 redundancy model at the pod level,
with each DGX system receiving dual power feeds. A complete SuperPOD
installation requires 5 to 7 MW of utility power including cooling
overhead, corresponding to PUE values of 1.2 to 1.3 for liquid-cooled
deployments.

\subsubsection{Management Plane
Architecture}\label{sec-compute-management-plane-architecture-adeb}

SuperPOD management integrates multiple control systems spanning
hardware, networking, and workload orchestration. Base Controller
Manager (BCM) provides hardware-level management including firmware
updates, health monitoring, and out-of-band access. The Unified Fabric
Manager coordinates InfiniBand network configuration, adaptive routing
policies, and link health monitoring.

At the workload level, SuperPOD deployments typically integrate with
either Slurm or Kubernetes for job scheduling. The NVIDIA GPU Operator
handles GPU driver installation, monitoring integration, and device
plugin management for Kubernetes environments. Slurm configurations use
GRES scheduling with topology-aware placement to ensure jobs receive
contiguous GPU allocations that minimize inter-node communication.

Storage integration varies by deployment, but reference architectures
include NVIDIA's GPUDirect Storage for direct data paths between NVMe
storage and GPU memory. A typical SuperPOD includes 30 to 50 PB of
high-performance storage providing 200+ GB/s aggregate throughput,
staging training data close to compute.

\subsection{Google TPU Pod
Infrastructure}\label{sec-compute-google-tpu-pod-infrastructure-4a00}

Google's TPU pods represent an alternative architectural philosophy:
vertically integrated accelerators designed specifically for transformer
training, with interconnect capabilities built into the chip rather than
added as external networking.

\subsubsection{TPU v4 Pod
Architecture}\label{sec-compute-tpu-v4-pod-architecture-f7f0}

A TPU v4 pod contains 4096 TPU chips arranged in a 3D torus topology.
Each chip provides approximately 275 TFLOPS of bfloat16 compute with 32
GB of HBM2e memory, yielding aggregate pod capacity of 1.1 exaFLOPS and
128 TB of memory. The power envelope for a complete pod is approximately
4 to 5 MW, competitive with GPU-based systems at similar compute
density.

The physical packaging differs from GPU systems. TPU chips mount in
trays of 4, with trays assembled into racks of 64 chips each. Sixty-four
racks form the complete pod, arranged in a cube topology that matches
the 3D torus interconnect structure. Cooling uses rear-door heat
exchangers with facility water, maintaining chip temperatures below 85C
under sustained load.

\subsubsection{Inter-Chip Interconnect
Topology}\label{sec-compute-interchip-interconnect-topology-9533}

The ICI (Inter-Chip Interconnect) fabric provides direct chip-to-chip
connectivity without external switches. Each TPU v4 chip has six ICI
links at 100 GB/s each, enabling 3D torus connectivity:

\[
\text{Bisection Bandwidth} = 2 \times \sqrt[3]{N} \times B_{\text{link}} \times N/2
\]

For N=4096 chips with 100 GB/s links, the torus bisection bandwidth
reaches approximately 32 TB/s. While lower than fat-tree alternatives,
the consistent latency characteristics of torus topology benefit the
regular communication patterns of transformer training.

The topology choice optimizes for AllReduce patterns where each chip
communicates with neighbors rather than arbitrary endpoints. For a model
using 3D parallelism with 4 tensor-parallel chips, 16 pipeline stages,
and 64-way data parallelism, the workload maps naturally onto a 4x16x64
slice of the pod topology.

\subsubsection{Software Stack
Integration}\label{sec-compute-software-stack-integration-cf4e}

TPU software centers on JAX and XLA\sidenote{\textbf{XLA (Accelerated
Linear Algebra)}: A domain-specific compiler for linear algebra
operations that optimizes entire computation graphs rather than
individual operations. XLA performs operator fusion (combining multiple
ops into single kernels), buffer reuse optimization, and automatic
layout transformation. Originally developed for TPUs, XLA now supports
GPUs and CPUs, enabling framework-agnostic optimization of ML
computations. }, with pjit (partitioned JIT compilation) managing
distributed execution. XLA compiles high-level model descriptions to
TPU-specific operations, automatically inserting communication
collectives based on partition specifications. This approach differs
from the explicit communication programming required for GPU clusters.

Multislice training extends beyond single pods by connecting multiple
TPU slices via datacenter network. A PaLM-scale training run might
utilize four TPU v4 pods (16,384 chips) with cross-slice communication
at lower bandwidth than intra-slice ICI. The software stack handles this
hierarchy transparently, using different collective algorithms for
intra-slice versus inter-slice operations.

\subsection{Meta Recommendation
Infrastructure}\label{sec-compute-meta-recommendation-infrastructure-ff88}

Meta's recommendation systems illustrate infrastructure optimized for a
different workload pattern: models combining massive embedding tables
with relatively modest dense computation. This architecture serves
billions of daily recommendation queries across products including
Facebook Feed, Instagram, and Reels. Unlike the compute-bound LLM
workloads examined above, recommendation models hit a \emph{memory
capacity wall} where embedding tables exceed any single accelerator's
memory.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, toptitle=1mm, coltitle=black, opacityback=0, colback=white, colframe=quarto-callout-note-color-frame, left=2mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Archetype B: Memory Capacity Wall}, leftrule=.75mm, titlerule=0mm, opacitybacktitle=0.6, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, breakable, arc=.35mm, bottomrule=.15mm, toprule=.15mm]

\textbf{Archetype B (The Global Real-Time Recommendation Engine)}
defines this architectural split. Unlike Archetype A (which is
compute-bound), Archetype B is \textbf{memory-capacity bound}. The
embedding tables representing billions of users and items can exceed
10TB. Since this cannot fit in GPU HBM, the system must adopt a hybrid
design: storing embeddings in massive CPU DRAM tiers while using GPUs
only for the dense compute layers.

\end{tcolorbox}

\subsubsection{CPU-GPU Hybrid
Architecture}\label{sec-compute-cpugpu-hybrid-architecture-6f4a}

Recommendation models like DLRM\sidenote{\textbf{DLRM (Deep Learning
Recommendation Model)}: Meta's open-source recommendation model
architecture that became the reference design for industry
recommendation systems. DLRM processes both dense features (through
MLPs) and sparse features (through embedding tables), combining them via
a factorization machine-inspired interaction layer. The architecture
explicitly acknowledges the embedding-memory bottleneck, making it a
natural fit for hybrid CPU-GPU deployment. } (Deep Learning
Recommendation Model) partition naturally between embedding operations
and dense neural network computation. Embedding tables for production
systems can exceed 10 TB, far exceeding GPU memory capacity. The hybrid
architecture addresses this by placing embeddings in CPU DRAM while
dense layers execute on GPUs.

A production recommendation training node combines multiple CPUs
totaling 2 to 4 TB of DRAM with 8 GPUs for dense computation. The CPUs
handle embedding lookups, concatenation, and feature preprocessing.
Resulting feature vectors transfer to GPUs via PCIe for the dense
forward and backward passes. Gradient updates for embeddings return to
CPU memory via the same path.

This architecture requires careful balancing. The ratio of embedding
lookups to dense computation determines optimal CPU-to-GPU allocation.
For Meta's workloads, approximately 4:1 CPU socket to GPU ratios provide
balanced utilization, though this varies by model architecture.

\subsubsection{Embedding Table Serving at
Scale}\label{sec-compute-embedding-table-serving-scale-e014}

Inference architecture differs from training by emphasizing latency over
throughput. Production serving distributes embedding tables across a
fleet of CPU-based servers using consistent hashing for shard
assignment. A single recommendation query may access hundreds of
embedding shards, requiring parallel lookups that complete within the 50
to 100 ms latency budget.

The embedding serving tier operates separately from the dense model
serving tier. This separation enables independent scaling: embedding
servers scale with table size and query rate, while dense model servers
scale with compute requirements. Cross-tier communication uses
low-latency RPC, typically completing in under 5 ms for local datacenter
deployments.

Feature stores cache frequently accessed embeddings and precomputed
features, reducing embedding server load for popular items. A tiered
caching architecture places hot embeddings in GPU memory (microsecond
access), warm embeddings in CPU DRAM (sub-millisecond), and cold
embeddings in distributed storage (milliseconds). Cache hit rates above
90\% are typical for recommendation workloads due to power-law
popularity distributions of items.

\subsubsection{Training and Serving
Coordination}\label{sec-compute-training-serving-coordination-3cf1}

The separation between training and serving infrastructure creates
coordination challenges for model updates. Meta's approach uses a staged
rollout pipeline: models train on dedicated GPU clusters, export to
serving format, deploy to staging clusters for validation, then
gradually roll out to production serving. The complete pipeline from
training completion to full production deployment spans hours to days
depending on model criticality.

Training clusters optimize for throughput using large batch sizes and
aggressive gradient accumulation. Serving clusters optimize for latency
using quantized models, batched inference, and result caching. The
different optimization targets justify separate infrastructure rather
than shared clusters.

\subsection{Tesla Dojo for Vision
Training}\label{sec-compute-tesla-dojo-vision-training-3369}

Tesla's Dojo system represents the custom silicon approach to ML
infrastructure: building purpose-designed chips and packaging for a
specific workload rather than using general-purpose accelerators.

\subsubsection{Custom Silicon
Architecture}\label{sec-compute-custom-silicon-architecture-9d2a}

The Dojo D1 chip provides 1024 custom-designed cores in a 645
mm\textsuperscript{2} die. Each core combines an 8-wide vector unit,
64-bit scalar unit, and 1.25 MB of SRAM, yielding approximately 22.6
TFLOPS of BF16\sidenote{\textbf{BF16 (Brain Floating Point)}: A 16-bit
floating point format with the same 8-bit exponent as FP32 but only 7
mantissa bits (versus 23 in FP32). Developed by Google for TPUs, BF16
matches FP32's dynamic range, avoiding the overflow/underflow issues of
FP16 that require loss scaling. BF16 has become the default training
precision for transformers, offering 2\(\times\) memory savings with
minimal accuracy impact. } compute per chip. The design optimizes for
convolutional and attention operations typical of vision models, with
dataflow execution patterns that minimize memory traffic.

Twenty-five D1 chips mount on a single training tile, connected via a 2D
mesh interconnect providing 4 TB/s aggregate bandwidth. Six tiles
combine into a system tray, and multiple trays assemble into a complete
ExaPOD delivering over 1 exaFLOP of aggregate compute. The modular
architecture enables deployments from single tiles (0.5 PFLOPS) to
multi-ExaPOD installations.

\subsubsection{Wafer-Scale
Considerations}\label{sec-compute-waferscale-considerations-0768}

While Dojo uses conventional chip packaging, the architecture addresses
similar challenges to wafer-scale integration: maximizing on-chip
bandwidth while managing thermal and yield constraints. The 2D mesh
topology within each tile provides nearest-neighbor bandwidth of 18 GB/s
between chips, avoiding the bottlenecks of hierarchical topologies for
spatially-local operations common in vision processing.

Power density presents the primary challenge: a fully populated system
tray dissipates over 100 kW in a compact form factor. Tesla's thermal
solution uses direct liquid cooling with custom manifolds delivering
coolant to each training tile. The aggressive cooling enables sustained
operation at power densities exceeding traditional datacenter limits.

Yield management for custom silicon requires careful attention. Unlike
commodity GPU purchases where defective units return to the vendor,
custom chip production creates internal yield loss. Dojo's design
includes redundant cores and interconnect paths, enabling graceful
degradation when manufacturing defects occur. Production testing
identifies defective units, and the software stack maps computation
around unavailable resources.

\subsubsection{Training Video Data at
Scale}\label{sec-compute-training-video-data-scale-4e1a}

Dojo's primary workload is training vision models on Tesla's fleet data:
over 1 million video clips per day from vehicles worldwide. The data
pipeline presents distinct challenges from text or image training. Video
requires decompression, temporal alignment, sensor calibration, and
often 3D scene reconstruction before training.

The preprocessing pipeline runs on CPU clusters adjacent to Dojo
compute, staging prepared batches to high-speed storage. Storage
bandwidth of 10+ GB/s per training tile ensures compute utilization
despite the data-intensive nature of video processing. The complete
system integrates 10 PB of flash storage providing over 100 GB/s
aggregate throughput.

This infrastructure supports auto-labeling workflows where preliminary
models identify scenarios of interest in raw video, generating training
data for improved models. The closed-loop between deployment, data
collection, and training enables rapid iteration cycles measured in days
rather than weeks.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

These case studies demonstrate that production ML infrastructure defies
one-size-fits-all solutions. DGX SuperPOD optimizes for flexible
general-purpose training with emphasis on GPU density and high-bandwidth
networking. TPU pods sacrifice flexibility for vertical integration that
excels at transformer workloads. Meta's hybrid architecture addresses
the embedding-heavy patterns unique to recommendation systems. Tesla's
Dojo pursues custom silicon for domain-specific acceleration where scale
justifies development costs. The choice among these approaches depends
on workload characteristics, scale requirements, and organizational
capabilities rather than any universal optimum. Understanding these
trade-offs enables informed infrastructure decisions as models and
training requirements continue to evolve.
\textbf{?@sec-distributed-training-systems} provides the implementation
details for the parallelism strategies that leverage these
infrastructure platforms, while
\textbf{?@sec-communication-collective-operations} examines the
collective operations and algorithms that govern network-level
performance.

\section{Fallacies and
Pitfalls}\label{sec-compute-fallacies-pitfalls-383c}

Infrastructure for distributed ML systems involves counterintuitive
interactions between compute, networking, power, and cooling that lead
to costly miscalculations. These fallacies and pitfalls capture errors
that waste millions in infrastructure investment, delay projects by
months, or cause production systems to achieve only 30 to 50\% of
planned capacity.

\textbf{Fallacy:} \textbf{\emph{More GPUs always means faster
training.}}

Engineers assume training time scales linearly with GPU count. In
production, communication overhead dominates beyond modest cluster
sizes. Amdahl's Law establishes the hard limit: gradient synchronization
is inherently sequential since all gradients must be collected before
any update proceeds. For a 175B parameter model (350 GB gradients) on 64
GPUs with 400 Gbps InfiniBand, AllReduce requires 14 seconds while
compute takes 1 second, yielding 6.7\% efficiency. Organizations
frequently discover that 512-GPU clusters train slower than optimized
128-GPU deployments. At \$30 per hour per H100, a 512-GPU cluster
achieving 25\% efficiency wastes \$11,520 per hour in idle capacity.

\textbf{Fallacy:} \textbf{\emph{Peak FLOPS determines training
throughput.}}

Procurement teams select accelerators by comparing peak FLOPS: H100
delivers 989 TF of FP16 compute. In production workloads, memory
bandwidth limits performance. The roofline model in
Section~\ref{sec-compute-warehousescale-computer-384e} shows that
arithmetic intensity below the ridge point (\textasciitilde290 FLOP/byte
for H100) makes accelerators memory-bound. Most LLM training operates at
50 to 100 FLOP/byte, achieving only 168 to 335 TF effective throughput
(17 to 34\% of peak). Organizations that compare accelerators by peak
FLOPS alone make purchasing decisions that cost 1.5 to 2x more per
actual training FLOP delivered. For attention-dominated workloads,
lower-FLOPS accelerators with higher memory bandwidth often outperform
higher-FLOPS alternatives.

\textbf{Fallacy:} \textbf{\emph{All ML infrastructure should be
GPU-based.}}

Teams assume GPUs optimize every ML workload. At scale, hybrid
architectures deliver superior economics for embedding-heavy workloads.
Recommendation systems, which constitute 80 to 90\% of inference volume
at Meta and Google, require random access to terabytes of embedding
tables that cannot fit in GPU HBM. These memory-bound lookups perform
poorly on GPUs optimized for dense compute. Meta's production
infrastructure uses CPU clusters for embedding lookups while GPU
clusters process dense neural network layers, achieving 3x better cost
efficiency than GPU-only deployments. Organizations that deploy GPU-only
infrastructure for diverse workloads waste 40 to 60\% of capacity on
tasks CPUs handle more efficiently.

\textbf{Pitfall:} \textbf{\emph{Ignoring power and cooling constraints
during infrastructure planning.}}

Teams plan GPU purchases based on compute requirements without verifying
datacenter capacity. Power and cooling represent hard physical limits
that cannot be resolved through software optimization. A single rack of
4 DGX H100 systems requires 40 kW of power and generates equivalent
thermal load, compared to 5 to 10 kW for traditional server racks.
Organizations discover their facility cannot support the power density
only after hardware arrives. Thermal throttling causes GPUs to reduce
clock speeds when cooling is inadequate. A cluster designed for 100\%
utilization may achieve only 70\% sustained throughput due to thermal
constraints, costing \$9,000 per hour in lost productivity for a
1000-GPU cluster at \$30 per hour per H100.

\textbf{Pitfall:} \textbf{\emph{Underestimating network requirements for
distributed training.}}

Operators calculate network bandwidth but ignore latency, topology, and
software overhead. As shown in
\textbf{?@sec-networking-networking-largescale-ml-adca}, a cluster with
400 Gbps InfiniBand achieves only 300 Gbps effective throughput due to
NCCL protocol overhead, suboptimal job placement, and contention from
concurrent jobs. Topology choice critically impacts performance:
fat-tree topologies provide full bisection bandwidth at significant
switch cost (\$800K to \$1.2M for 256 GPUs), while rail-optimized
topologies reduce hardware cost but constrain job placement. NCCL
introduces 5 to 20 microseconds of software latency per collective
operation; for small message sizes common in pipeline parallelism, this
overhead dominates transfer time. The crossover where bandwidth
dominates latency occurs at approximately 250 KB message size.

\textbf{Fallacy:} \textbf{\emph{Cloud computing costs scale linearly
with usage.}}

Organizations assume cloud expenses grow proportionally with compute
consumption. In production, data egress fees and storage costs create
nonlinear scaling. Training a 70B parameter model generates 200 to 400
GB of checkpoints every few hours; with 5 checkpoint retention, storage
reaches 1 to 2 TB per experiment. Cloud storage at \$0.023 per GB-month
costs \$23 to \$46 per month per experiment, but egress fees for
downloading checkpoints cost \$0.09 per GB (\$18 to \$36 per full
checkpoint download). Organizations running 100 concurrent experiments
accumulate \$2,300 to \$4,600 per month in storage costs alone. The
break-even point where on-premises infrastructure becomes more
economical occurs at 40 to 60\% sustained utilization. Teams that
migrate to cloud assuming linear scaling discover total costs 1.5 to
2.5x higher than projected.

\textbf{Pitfall:} \textbf{\emph{Underestimating operational complexity
of distributed schedulers.}}

Teams deploy Kubernetes or Slurm expecting resource management to be
solved. In production, achieving 60\%+ GPU utilization requires
continuous tuning of gang scheduling policies, preemption strategies,
and job bin-packing algorithms. Naive first-come-first-served scheduling
fragments nodes: with 8-GPU nodes and 6-GPU jobs, each job wastes 2 GPUs
per node, reducing effective capacity to 75\%. The CAP theorem
established in \textbf{?@sec-vol2-introduction} forces schedulers to
trade off consistency versus availability. At 4096 GPUs with 99.9\%
annual reliability per GPU, clusters experience approximately 4 failures
per year, requiring robust fault tolerance mechanisms. Organizations
that treat scheduling as solved discover their \$50M GPU cluster
achieves only 35 to 45\% utilization versus the 65 to 75\% possible with
dedicated tuning, wasting \$5 to \$9M annually.

\section{Summary}\label{sec-compute-summary-fb77}

Compute infrastructure represents the physical foundation upon which all
distributed training and serving systems operate. The constraints
examined in this chapter, from power delivery and cooling capacity to
accelerator selection, ultimately determine what scale of ML systems an
organization can build and operate effectively.

We examined how power density and cooling represent hard physical limits
that constrain cluster design independent of budget. The transition from
air-cooled 10 kW racks to liquid-cooled 100 kW racks is not just an
upgrade but a fundamental architectural shift required by modern
accelerators.

We analyzed the trade-offs between different accelerator architectures,
GPUs, TPUs, and custom ASICs, and how they map to specific workloads.
The distinction between training-optimized (high precision, bandwidth)
and inference-optimized (low latency, efficiency) hardware is driving
specialized fleet designs.

Finally, we established that Total Cost of Ownership extends far beyond
hardware acquisition. Power consumption, cooling infrastructure, and
operational overhead often exceed the initial cost of GPUs over a
three-year lifecycle. The core lessons from this chapter can be
summarized as follows:

\phantomsection\label{callout-takeawaysux2a-1.9}
\begin{fbx}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.9}

\begin{itemize}
\tightlist
\item
  Power density and cooling capacity represent hard physical limits that
  constrain cluster design independent of budget
\item
  Hybrid CPU-GPU architectures outperform GPU-only configurations for
  recommendation systems and other embedding-heavy workloads
\item
  Total cost of ownership must include power, cooling, operations, and
  realistic utilization rates rather than theoretical peak performance
\item
  Accelerator selection requires matching hardware characteristics
  (memory bandwidth vs.~FLOPS) to workload bottlenecks (decode
  vs.~prefill)
\end{itemize}

\end{fbx}

With the compute nodes defined, powered, cooled, and racked, we have
built the individual engines of the fleet. But these engines cannot
operate in isolation. Distributed training requires them to function as
a single, coordinated machine.

The next chapter, \textbf{?@sec-networking}, examines the high-bandwidth
networking fabrics, InfiniBand, RoCE, and specialized topologies, that
bind these nodes together into a unified supercomputer.

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-thegreengrid2007pue}
Grid, The Green. 2007. {``Green Grid Data Center Power Efficiency
Metrics: PUE and DCIE.''} The Green Grid.

\bibitem[\citeproctext]{ref-ma2024challenges}
Ma, Xiaoyu, and David Patterson. 2024. {``Challenges and Research
Directions for Large Language Model Inference Hardware.''} \emph{arXiv
Preprint arXiv:2601.05047}.

\bibitem[\citeproctext]{ref-mattson2020mlperf}
Mattson, Peter, Christine Cheng, Cody Coleman, Greg Diamos, Paulius
Micikevicius, David Patterson, Hanlin Tang, et al. 2020. {``MLPerf
Training Benchmark.''} In \emph{Proceedings of Machine Learning and
Systems}, 2:336--49.
\url{https://proceedings.mlsys.org/paper/2020/hash/02522a2b2726fb0a03bb19f2d8d9524d-Abstract.html}.

\bibitem[\citeproctext]{ref-naumov2019dlrm}
Naumov, Maxim, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, et al. 2019. {``Deep
Learning Recommendation Model for Personalization and Recommendation
Systems.''} \emph{arXiv Preprint arXiv:1906.00091}, May.
\url{http://arxiv.org/abs/1906.00091v1}.

\bibitem[\citeproctext]{ref-williams2009roofline}
Williams, Samuel, Andrew Waterman, and David Patterson. 2009.
{``Roofline: An Insightful Visual Performance Model for Multicore
Architectures.''} \emph{Communications of the ACM} 52 (4): 65--76.
\url{https://doi.org/10.1145/1498765.1498785}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
