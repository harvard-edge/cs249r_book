% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Advanced Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Advanced Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol2.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.225\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Advanced}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Advanced Machine Learning Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume II)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Advanced\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~II}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume II}\label{welcome-to-volume-ii}
\addcontentsline{toc}{chapter}{Welcome to Volume II}

\markboth{Welcome to Volume II}{Welcome to Volume II}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume II extends the foundations into production-scale systems through
five parts:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations of Scale} --- Master the algorithms of
  scale. Learn how to coordinate computation across thousands of devices
  using Data, Tensor, and Pipeline parallelism, and understand the
  collective communication primitives and fault tolerance mechanisms
  that synchronize them.
\item
  \textbf{Part II: Building the Machine Learning Fleet} --- Build the
  physical computer. Architect the datacenter infrastructure,
  accelerators, and high-performance storage systems required to support
  distributed workloads at scale.
\item
  \textbf{Part III: Deployment at Scale} --- Serve the world. Navigate
  the shift from training to inference, push intelligence to the edge,
  and manage the operational lifecycle of production fleets.
\item
  \textbf{Part IV: Production Concerns} --- Harden the system. Address
  the non-functional requirements of privacy, security, robustness, and
  environmental sustainability in large-scale operations.
\item
  \textbf{Part V: Responsible AI at Scale} --- Shape the future. Explore
  responsible AI governance, AI for social good, and the emerging
  frontiers of AGI systems.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Foundational or equivalent} background in single-machine ML
  systems
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability
\item
  Familiarity with distributed systems concepts (networking,
  parallelism) is helpful for advanced topics
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{---}\label{section}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{sec-vol2-introduction}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/introduction/images/png/cover_introduction.png}}

\textbf{?@fig-intro-cover} captures the three interconnected imperatives
that define Volume II: scale, distribution, and governance.

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why do the engineering principles that work on single machines
break down at production scale?}

Everything changes when ML systems grow beyond what one machine can
handle. Communication between machines becomes more expensive than
computation within them. Hardware failures transition from rare
exceptions to routine events that systems must absorb without
interruption. Optimization strategies that improved single-GPU training
actively harm distributed efficiency. Serving architectures that worked
for thousands of users collapse under millions. The techniques that made
you successful at small scale become the obstacles preventing success at
large scale. This discontinuity explains why organizations that master
single-machine ML often struggle with production deployment: the
intuitions developed in controlled environments mislead when applied to
distributed systems where coordination costs dominate, failures are
inevitable, and decisions made by one component propagate through
thousands of others. Scale is not more of the same---it is fundamentally
different engineering terrain requiring different principles, different
architectures, and different ways of thinking about what makes systems
work.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, bottomrule=.15mm, titlerule=0mm, opacityback=0, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, breakable, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, toprule=.15mm, colback=white, rightrule=.15mm, toptitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, arc=.35mm, left=2mm, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  Explain why communication costs dominate computation as ML systems
  scale from single machines to distributed clusters
\item
  Analyze how ML compute requirements have grown 10-million-fold from
  AlexNet to GPT-4, identifying infrastructure implications
\item
  Compare synchronous versus asynchronous distributed training using the
  CAP theorem to evaluate consistency-availability trade-offs
\item
  Differentiate datacenter and edge distribution challenges in terms of
  connectivity, heterogeneity, and privacy constraints
\item
  Classify ML-specific security threats and explain why production scale
  amplifies attacker incentives
\item
  Apply the AI Triad framework to analyze how optimizing data,
  algorithms, or infrastructure in isolation creates system-wide
  bottlenecks
\item
  Explain why routine hardware failures and network partitions require
  fault tolerance as a core design principle at scale
\end{itemize}

\end{tcolorbox}

\section{The Scale
Transformation}\label{sec-vol2-introduction-scale-transformation}

The history of machine learning is defined by scale. Each major
capability leap has come not from algorithmic breakthroughs alone, but
from the ability to apply computation at previously impossible scales.
Compute requirements have evolved over the past decade in ways that make
systems engineering central to AI advancement. Three qualitative changes
emerge at production scale: communication dominance, routine failure,
and governance requirements that accompany societal impact.

Compute requirements have grown exponentially. AlexNet (2012) trained on
two GTX 580 GPUs for approximately 5-6 days
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}). BERT (2018) required 64 TPU\sidenote{\textbf{Tensor Processing
Unit (TPU)}: Google's custom-designed ASIC optimized specifically for
neural network computation. Unlike general-purpose GPUs, TPUs implement
a systolic array architecture that excels at the matrix multiplications
dominating deep learning workloads. TPU v4 chips deliver approximately
275 TFLOPS of bfloat16 performance at 175 watts, roughly 2.5× the
performance per watt of contemporary GPUs for transformer training. The
latest TPU v5p chips are interconnected via Inter-Core Interconnect
(ICI) providing 4,800 Gb/s of bandwidth per chip, enabling tight
coupling for distributed training across TPU pods containing thousands
of chips. While GPUs offer broader flexibility for varied workloads,
TPUs demonstrate how hardware-software co-design enables significant
efficiency gains for specific computational patterns. } chips for 4
days, roughly 6,144 chip hours (\citeproc{ref-devlin2018bert}{Devlin et
al. 2018}).

GPT-3 (2020) consumed an estimated \(3\.14 \\\\\\\\times 10^\{23\}\)
FLOPS during training, requiring thousands of V100 GPUs running for
weeks (\citeproc{ref-brown2020language}{Brown et al. 2020}). PaLM (2022)
trained on 6,144 TPU v4 chips for roughly 60 days, consuming
approximately 10²⁴ FLOPS (\citeproc{ref-chowdhery2022palm}{Chowdhery et
al. 2022}). GPT-4 (2023) reportedly trained on approximately 25,000 A100
GPUs over 90-100 days (\citeproc{ref-openai2023gpt4}{OpenAI et al.
2023}). This progression represents approximately a 10-million-fold
increase in training compute over a single decade, from roughly 10¹⁸
FLOPS for AlexNet to 10²⁵ FLOPS for GPT-4
(\citeproc{ref-sevilla2022compute}{Sevilla et al. 2022};
\citeproc{ref-amodei2018ai}{Amodei and Hernandez 2018}). The following
table summarizes this evolution:

\phantomsection\label{callout-exampleux2a-2.1}
\begin{fbx}{callout-example}{Example:}{Training Compute Evolution}
\phantomsection\label{callout-example*-2.1}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{GPUs/TPUs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Training Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Estimated FLOPS}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{AlexNet} & 2012 & 2 GPUs & 5-6 days & \textasciitilde10¹⁸ \\
\textbf{BERT-Large} & 2018 & 64 TPUs & 4 days & \textasciitilde10²⁰ \\
\textbf{GPT-3} & 2020 & \textasciitilde1000 GPUs & \textasciitilde30
days & \textasciitilde10²³ \\
\textbf{PaLM} & 2022 & 6144 TPUs & \textasciitilde60 days &
\textasciitilde10²⁴ \\
\textbf{GPT-4} & 2023 & \textasciitilde25000 GPUs & \textasciitilde100
days & \textasciitilde10²⁵ \\
\end{longtable}

\end{fbx}

Throughout this volume, recurring \emph{lighthouse examples} ground
abstract principles in concrete, quantifiable workloads. The following
callout introduces these examples and explains how they evolve from
their Volume I counterparts.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, bottomrule=.15mm, titlerule=0mm, opacityback=0, colframe=quarto-callout-note-color-frame, bottomtitle=1mm, breakable, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using Lighthouse Examples}, toprule=.15mm, colback=white, rightrule=.15mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, arc=.35mm, left=2mm, leftrule=.75mm]

Volume II extends the \textbf{Lighthouse Examples} from Volume I to the
regime of massive scale. We revisit our canonical workloads, but now
examining their behavior when distributed across thousands of devices:

\begin{itemize}
\tightlist
\item
  \textbf{GPT-4 / Llama-3 (Language Lighthouse)}: The evolution of
  GPT-2. We move from single-GPU memory bounds to multi-node
  \textbf{Model Parallelism} and \textbf{Pipeline Parallelism}.
\item
  \textbf{DLRM at Scale (Recommendation Lighthouse)}: We move from
  fitting embedding tables in memory to \textbf{Embedding Sharding}
  across hundreds of nodes, creating massive all-to-all communication
  bottlenecks.
\item
  \textbf{Federated MobileNet (Edge Lighthouse)}: We move from
  single-device inference to \textbf{Federated Learning} across billions
  of devices, introducing privacy and straggler challenges.
\end{itemize}

\textbf{Systems Perspectives} continue to appear as sidebars, now
focusing on the physics of data centers, network topology, and
distributed consistency (CAP theorem).

\end{tcolorbox}

This exponential growth has transformed ML from a discipline where
algorithms dominate to one where systems engineering determines success.
A sophisticated algorithm that cannot scale often provides less
practical value than a simpler algorithm deployed efficiently across
scalable infrastructure.

The transition from single-machine to distributed training introduces
qualitative changes in system behavior. The unit of compute is no longer
a single server, but a \textbf{Machine Learning Fleet}---a massive,
interconnected distributed system that must act as a single coherent
engine.

\phantomsection\label{callout-definitionux2a-2.2}
\begin{fbx}{callout-definition}{Definition:}{The Machine Learning Fleet}
\phantomsection\label{callout-definition*-2.2}
\textbf{\emph{The Machine Learning Fleet}} is a distributed system of
thousands of interconnected accelerators (GPUs/TPUs), storage arrays,
and network fabrics designed to operate as a single coherent computer.
Unlike traditional clusters that manage independent jobs, the Fleet
coordinates synchronous states across all nodes, where a failure in any
single component can stall the entire system.

\end{fbx}

As systems scale beyond a single node, a fundamental physical constraint
emerges: the \emph{bisection bandwidth wall}, which limits how fast data
can cross the network midpoint. This constraint explains why networking,
not just compute, often determines the ``speed'' of your model.

On a single GPU, training proceeds deterministically: the same code,
data, and random seed produce identical results. At the scale of
thousands of GPUs, new phenomena emerge. Network partitions can split
clusters into groups that train independently, causing model
divergence\sidenote{\textbf{Network Partition in Distributed Training}:
When network failures isolate groups of workers, each group may continue
training independently with different gradient updates, causing models
to diverge. In synchronous distributed training, partitions typically
halt progress until connectivity restores. In asynchronous training,
partitions can cause split brain scenarios where different model
versions evolve in parallel. Production systems implement partition
detection through heartbeat mechanisms and quorum protocols. Google's
approach with TPU pods uses dedicated high-bandwidth interconnects (ICI
providing 4,800 Gb/s per chip for TPU v5p) to minimize partition
probability, while software-level protocols detect and recover from rare
partitions within minutes rather than allowing extended divergent
training. }. Stragglers (workers that process data slower than peers due
to hardware variation or thermal throttling) can bottleneck entire
training runs\sidenote{\textbf{The Straggler Problem}: In synchronous
distributed training, all workers must complete their computation before
proceeding to the next iteration. If one worker runs 10\% slower due to
thermal throttling, memory pressure, or background processes, the entire
cluster waits, reducing effective utilization. At 1,000 workers, the
probability of at least one straggler per iteration approaches
certainty. Solutions include backup workers that redundantly compute
slow partitions, gradient coding that tolerates missing results, bounded
staleness that allows proceeding with partial results, and careful
hardware provisioning that minimizes variation. Meta's research found
that stragglers can reduce large-scale training throughput by 20-30\%
without mitigation strategies. }. Hardware failures that occur once per
machine-year become daily events when operating 10,000 machines. Systems
must checkpoint frequently enough that losing a day's progress becomes
acceptable rather than catastrophic\sidenote{\textbf{Hardware Failure
Rates at Scale}: Individual server components fail infrequently.
Enterprise SSDs show annual failure rates of 0.5-2\%
(\citeproc{ref-pinheiro2007failure}{Pinheiro, Weber, and Barroso 2007});
GPUs fail at roughly 1-2\% annually under typical datacenter conditions,
though rates can exceed 9\% under intensive AI training workloads;
memory DIMMs fail at 1-2\% per year
(\citeproc{ref-schroeder2009dram}{Schroeder, Pinheiro, and Weber 2009}).
These rates seem manageable until multiplied by scale. A cluster of
10,000 GPUs with 2\% annual failure rate experiences 200 GPU failures
per year, or roughly one every two days. A training run lasting 100 days
will statistically encounter 50 or more GPU failures. Production systems
implement automatic detection, workload migration, and checkpoint-based
recovery to handle failures as routine events rather than emergencies.
Meta reported that during LLaMA 3 training on 16,384 H100 GPUs, they
experienced hardware failures roughly every three hours, requiring
automated recovery systems to maintain over 90\% effective training time
(\citeproc{ref-dubey2024llama3}{Grattafiori et al. 2024}). }.

These scale-induced challenges drive infrastructure investment by the
largest AI organizations. Meta's Research SuperCluster (RSC) contains
16,000 NVIDIA A100 GPUs connected by 200 Gb/s
InfiniBand\sidenote{\textbf{InfiniBand}: A high-performance, low-latency
networking technology purpose-built for data centers and supercomputing.
While standard Ethernet reaches 100-400 Gb/s with 1-5 microsecond
latencies, InfiniBand achieves 200-400 Gb/s with sub-microsecond
latencies through features including Remote Direct Memory Access (RDMA),
which bypasses the operating system to transfer data directly between
application memory on different machines. For distributed ML training,
InfiniBand's RDMA support enables NVIDIA Collective Communications
Library (NCCL) to achieve near-optimal gradient synchronization
bandwidth. The 200 Gb/s HDR InfiniBand links common in 2024 AI clusters
deliver approximately 25 GB/s of usable bandwidth, while the newer 400
Gb/s NDR generation reaches 50 GB/s. These speeds determine whether
training becomes communication-bound or compute-bound for large models.
} networking (\citeproc{ref-meta2022rsc}{AI 2022}). Google's TPU v4 pods
contain 4,096 chips with 1.1 exaFLOPS\sidenote{\textbf{ExaFLOPS}: One
quintillion (10\^{}18) floating-point operations per second. For
context, a single NVIDIA H100 GPU delivers approximately 1,979 TFLOPS
(teraFLOPS, or 10\^{}12 FLOPS) of FP8 compute; achieving 1 exaFLOPS thus
requires roughly 500 such GPUs operating in parallel with perfect
efficiency. Google's TPU v4 pods reaching 1.1 exaFLOPS represent one of
the first single-system installations to cross this threshold. The scale
progression from megaFLOPS (10\^{}6, 1980s workstations) through
gigaFLOPS (10\^{}9, 1990s servers), teraFLOPS (10\^{}12, 2000s GPUs),
petaFLOPS (10\^{}15, 2010s supercomputers), to exaFLOPS (10\^{}18, 2020s
AI clusters) illustrates the exponential growth enabling modern ML
capabilities. } of aggregate compute capacity. Microsoft's Azure AI
infrastructure spans multiple datacenters with tens of thousands of GPUs
dedicated to AI workloads. Frontier AI capabilities require frontier
infrastructure.

\section{AI Scaling Laws}\label{sec-vol2-intro-ai-scaling-laws-a043}

The infrastructure investments described in the preceding section did
not arise from arbitrary organizational ambitions. They emerged from an
empirical discovery: machine learning performance follows predictable
mathematical relationships with scale. Understanding these scaling laws
explains why distributed systems have become essential and reveals the
constraints that shape their design.

Machine learning systems have followed a consistent pattern: increasing
model scale through parameters, training data, and computational
resources typically improves performance. Progress across natural
language processing, computer vision, and speech recognition has
confirmed this empirical observation, with larger models trained on
extensive datasets consistently achieving state-of-the-art results.

Scaling laws quantify the ``Bitter Lesson'' articulated by Rich Sutton:
performance in machine learning is primarily driven by leveraging
general methods at massive scale rather than encoding human knowledge
into algorithms. The predictable power-law relationships show \emph{how}
computation, when scaled, yields better models.

As computational demands grow exponentially and data requirements
increase, questions emerge about efficiency and sustainability: when do
scaling costs outweigh performance benefits? Researchers have developed
scaling laws that quantify how model performance relates to training
resources, revealing why efficiency becomes increasingly important as
systems expand in complexity.

\phantomsection\label{callout-definitionux2a-2.3}
\begin{fbx}{callout-definition}{Definition:}{Scaling Laws}
\phantomsection\label{callout-definition*-2.3}
\textbf{Scaling Laws} refer to empirical relationships discovered by
OpenAI showing that language model performance follows predictable
\emph{power-law relationships} with \emph{model size (N)}, \emph{dataset
size (D)}, and \emph{compute budget (C)}. These laws enable researchers
to predict performance and optimal resource allocation before expensive
training runs.

\end{fbx}

Understanding why efficiency becomes increasingly important as systems
expand in complexity requires examining how scaling laws manifest across
different dimensions. We analyze their implications for system design,
establishing why the multi-dimensional efficiency optimization framework
is essential at production scale.

\subsection{Empirical Evidence for Scaling
Laws}\label{sec-vol2-intro-empirical-evidence-scaling-laws-0105}

The rapid evolution in AI capabilities over the past decade exemplifies
this scaling trajectory. GPT-1 (2018) contained 117 million parameters
and demonstrated basic sentence completion capabilities. GPT-2 (2019)
scaled to 1.5 billion parameters and achieved coherent paragraph
generation. GPT-3 (2020) expanded to 175 billion parameters and
demonstrated sophisticated text generation across diverse domains. Each
increase in model size brought dramatically improved capabilities, but
at exponentially increasing costs.

This pattern extends beyond language models. In computer vision,
doubling neural network size typically yields consistent accuracy gains
when training data increases proportionally. AlexNet (2012) had 60
million parameters, VGG-16 (2014) scaled to 138 million, and large
modern vision transformers can exceed 600 million parameters. Each
generation achieved better image recognition accuracy but required
proportionally more computational resources and training data.

The scaling hypothesis underlies this progress. Larger models possess
increased capacity to capture intricate data patterns, facilitating
improved accuracy and generalization. However, this scaling trajectory
introduces critical resource constraints. Training GPT-3 required
approximately 314 sextillion\sidenote{\textbf{Sextillion}: A number with
21 zeros (10²¹), representing an almost incomprehensible scale. To put
this in perspective, there are estimated 10²² to 10²⁴ stars in the
observable universe, making GPT-3's training computation roughly 1/22nd
of counting every star in the cosmos. } floating-point operations (314
followed by 21 zeros), equivalent to running a modern gaming PC
continuously for hundreds of years at substantial financial and
environmental costs.

These resource demands reveal why understanding scaling laws is
necessary for efficiency. Figure~\ref{fig-compute-trends} traces how
computational demands of training state-of-the-art models have escalated
at an unsustainable rate, growing faster than Moore's Law improvements
in hardware.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/introduction/images/png/compute-trends.png}}

}

\caption{\label{fig-compute-trends}\textbf{Model Training Compute
Trends}: Training compute has grown exponentially, accelerating
dramatically in the deep learning era. Between 2012 and 2019, compute
requirements doubled approximately every 3.4 months, far exceeding
Moore's Law (\textasciitilde2 years). This trajectory explains why
efficiency has become a strategic imperative rather than an optional
optimization. Source:
(\citeproc{ref-Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022}{\textbf{Sevilla\_Heim\_Ho\_Besiroglu\_Hobbhahn\_Villalobos\_2022?}})}

\end{figure}%

Scaling laws provide a quantitative framework for understanding these
trade-offs. They reveal that model performance exhibits predictable
patterns as resources increase, following power-law relationships where
performance improves consistently but with diminishing
returns\sidenote{\textbf{Diminishing Returns}: Economic principle where
each additional input yields progressively smaller output gains. In ML,
doubling compute from 1 to 2 hours might improve accuracy by 5\%, but
doubling from 100 to 200 hours might improve it by only 0.5\%. }. These
laws show that optimal resource allocation calls for coordinating model
size, dataset size, and computational budget rather than scaling any
single dimension in isolation.

Before examining how these workloads stress distributed infrastructure,
we review the computational characteristics that drive their resource
demands.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, bottomrule=.15mm, titlerule=0mm, opacityback=0, colframe=quarto-callout-note-color-frame, bottomtitle=1mm, breakable, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Transformer Compute Refresher}, toprule=.15mm, colback=white, rightrule=.15mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, arc=.35mm, left=2mm, leftrule=.75mm]

Transformers process sequences using self-attention mechanisms that
compute relationships between all token pairs. This architecture's
computational cost scales quadratically with sequence length (\(O(n^2)\)
where \(n\) is sequence length), making resource allocation particularly
critical for language models. The term ``FLOPs'' (floating-point
operations) quantifies total computational work, while ``tokens''
represent the individual text units (typically subwords) that models
process during training.

\end{tcolorbox}

\subsection{Compute-Optimal Resource
Allocation}\label{sec-vol2-intro-computeoptimal-resource-allocation-541a}

Empirical studies of large language models (LLMs) reveal a key insight.
For any fixed computational budget, there exists an optimal balance
between model size and dataset size (measured in
tokens\sidenote{\textbf{Tokens}: Individual units of text that language
models process, created by breaking text into subword pieces using
algorithms like Byte-Pair Encoding (BPE). GPT-3 trained on 300 billion
tokens while PaLM used 780 billion tokens, requiring text corpora
equivalent to millions of books from web crawls and digitized
literature. }) that minimizes training loss.

This principle emerges clearly in Figure~\ref{fig-compute-optimal}
through three related views. The left panel shows `IsoFLOP curves' where
each curve corresponds to a constant number of floating-point operations
(FLOPs\sidenote{\textbf{FLOPs (Floating-Point Operations)}: Measure of
computational work independent of hardware. GPT-3 required
\(3\.14 \\\\\\\\times 10^\{23\}\) FLOPs---equivalent to a gaming PC
running 350 years. Chinchilla scaling laws suggest optimal FLOP
allocation between model size and training tokens. FLOPs/second (FLOPS)
measures hardware throughput; A100 delivers 312 FP16 Tensor TFLOPS. })
during transformer\sidenote{\textbf{Transformer}: Neural network
architecture introduced by Vaswani et al.
(\citeproc{ref-vaswani2017attention}{Vaswani et al. 2025}) that
revolutionized NLP through self-attention mechanisms. Unlike sequential
RNNs, transformers enable parallel processing during training, forming
the foundation of modern large language models including GPT, BERT, T5,
and their derivatives. } training. The valleys in these curves identify
the most efficient model size for each computational budget when
training autoregressive\sidenote{\textbf{Autoregressive Models}:
Language models that generate text by predicting each token based on all
preceding tokens in the sequence. GPT-family models exemplify this
approach, generating text left-to-right with causal attention masks to
ensure each position only attends to previous positions. } language
models. The center and right panels reveal how the optimal number of
parameters and tokens scales predictably as computational budgets
increase, demonstrating the necessity for coordinated scaling to
maximize resource utilization.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/introduction/images/png/compute_optimal.png}}

}

\caption{\label{fig-compute-optimal}\textbf{Optimal Compute Allocation}:
For fixed computational budgets, language model performance depends on
balancing model size and training data volume; the left panel maps
training loss across parameter counts, identifying an efficiency sweet
spot for each FLOP level. The center and right panels quantify how
optimal parameter counts and token requirements scale predictably with
increasing compute, demonstrating the need for coordinated scaling of
both model and data to maximize resource utilization in large language
models. Source: (\citeproc{ref-hoffmann2022training}{Hoffmann et al.
2022}).}

\end{figure}%

Kaplan et al. (\citeproc{ref-kaplan2020scaling}{2020}) demonstrated that
transformer-based language models scale predictably with three factors:
the number of model parameters, the volume of the training dataset
(measured in tokens), and the total computational budget (measured in
floating-point operations). When these factors are augmented
proportionally, models exhibit consistent performance improvements
without requiring architectural modifications or task-specific tuning.

Figure~\ref{fig-kaplan-scaling} presents test loss curves for models
spanning from \(10^3\) to \(10^9\) parameters, revealing two key
insights. First, larger models demonstrate superior sample efficiency,
achieving target performance levels with fewer training tokens. Second,
as computational resources increase, the optimal model size
correspondingly grows, with loss decreasing predictably when compute is
allocated efficiently.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/introduction/images/png/kaplan_scaling_data_compute.png}}

}

\caption{\label{fig-kaplan-scaling}\textbf{Scaling Laws \& Compute
Optimality}: Larger models consistently achieve better performance with
increased training data and compute, but diminishing returns necessitate
careful resource allocation during training. Optimal model size and
training duration depend on the available compute budget, as evidenced
by the convergence of loss curves at different parameter scales and
training token counts. Source: (\citeproc{ref-kaplan2020scaling}{Kaplan
et al. 2020}).}

\end{figure}%

Optimal compute allocation follows from the theoretical scaling
relationship \(D
able N^{0.74}\) (\citeproc{ref-hoffmann2022training}{Hoffmann et al.
2022}), which shows that dataset size \(D\) and model size \(N\) must
grow in coordinated proportions for a fixed budget. As model size
increases, the dataset should grow at roughly three-quarters the rate to
maintain compute-optimal efficiency.

These theoretical predictions assume perfect compute utilization, which
becomes challenging in distributed training scenarios. Real-world
implementations face communication overhead that scales unfavorably with
system size, creating bandwidth bottlenecks that reduce effective
utilization. Beyond 100 nodes, communication overhead can reduce
expected performance gains by 20-40\% depending on workload and
interconnect
(\citeproc{ref-narayanan2021efficient}{\textbf{narayanan2021efficient?}}).

\subsection{Mathematical Foundations and Operational
Regimes}\label{sec-vol2-intro-mathematical-foundations-operational-regimes-9afe}

The predictable patterns observed in scaling behavior can be expressed
mathematically using power-law relationships, though understanding the
intuition behind these patterns proves more important than precise
mathematical formulation for most practitioners.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, bottomrule=.15mm, titlerule=0mm, opacityback=0, colframe=quarto-callout-note-color-frame, bottomtitle=1mm, breakable, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Formal Mathematical Formulation}, toprule=.15mm, colback=white, rightrule=.15mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, arc=.35mm, left=2mm, leftrule=.75mm]

For readers interested in the formal mathematical framework, scaling
laws can be expressed as power-law relationships. The general
formulation is:

\[
\mathcal{L}(N) = A N^{-\alpha} + B
\]

where loss \(\mathcal{L}\) decreases as resource quantity \(N\)
increases, following a power-law decay with rate \(\alpha\), plus a
baseline constant \(B\). Here, \(\mathcal{L}(N)\) represents the loss
achieved with resource quantity \(N\), \(A\) and \(B\) are
task-dependent constants, and \(\alpha\) is the scaling exponent that
characterizes the rate of performance improvement. A larger value of
\(\alpha\) signifies more efficient performance improvements with
respect to scaling.

\end{tcolorbox}

These theoretical predictions find strong empirical support across
multiple model configurations. Figure~\ref{fig-loss-vs-n-d} demonstrates
how early-stopped test loss varies predictably with both dataset size
and model size, revealing that learning curves across configurations can
be aligned through appropriate parameterization.

\subsubsection{Resource-Constrained Scaling
Regimes}\label{sec-vol2-intro-resourceconstrained-scaling-regimes-062d}

Applying scaling laws in practice calls for recognizing three distinct
resource allocation regimes that emerge from trade-offs between compute
budget, data availability, and optimal resource allocation. These
regimes provide practical guidance for system designers navigating
resource constraints.

Compute-limited regimes characterize scenarios where available
computational resources restrict scaling potential despite abundant
training data. Organizations with limited hardware budgets or strict
training time constraints operate within this regime. The optimal
strategy involves training smaller models for longer periods, maximizing
utilization of available compute through extended training schedules
rather than larger architectures. This approach proves particularly
relevant for academic institutions, startups, and projects with
constrained infrastructure access.

Data-limited regimes emerge when computational resources exceed what can
be effectively utilized given dataset constraints. High-resource
organizations working with specialized domains, proprietary datasets, or
privacy-constrained data often encounter this regime. The optimal
strategy involves training larger models for fewer optimization steps,
leveraging model capacity to extract maximum information from limited
training examples. This regime commonly appears in specialized
applications like medical imaging or proprietary commercial datasets.

Optimal regimes (Chinchilla Frontier) represent the balanced allocation
of compute and data resources following compute-optimal scaling laws.
This regime achieves maximum performance efficiency by scaling model
size and training data proportionally, as demonstrated by DeepMind's
Chinchilla model, which outperformed much larger models through optimal
resource allocation (\citeproc{ref-hoffmann2022training}{Hoffmann et al.
2022}). Operating within this regime requires sophisticated resource
planning but delivers superior performance per unit of computational
investment.

Recognizing these regimes enables practitioners to make informed
decisions about resource allocation strategies, avoiding common
inefficiencies such as over-parameterized models with insufficient
training data or under-parameterized models that fail to utilize
available computational resources effectively.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/550529b396b866e7e8781962ecaf5b420a8c9079.pdf}}

}

\caption{\label{fig-loss-vs-n-d}: \textbf{Loss vs Model and Dataset
Size}: Early-stopped test loss varies predictably with both dataset size
(10\^{}7 to 10\^{}10 tokens) and model size (393K to 708M parameters).
The curves show that larger models achieve lower loss at any given
dataset size, but all models eventually plateau as data becomes the
limiting factor. This demonstrates the importance of balanced scaling:
doubling model size without proportionally increasing data yields
diminishing returns.}

\end{figure}%

Scaling laws show that performance improvements follow predictable
patterns that change depending on resource availability and exhibit
distinct behaviors across different dimensions. Two important types of
scaling regimes emerge: \textbf{data-driven regimes} that describe how
performance changes with dataset size, and \textbf{temporal regimes}
that describe when in the ML lifecycle we apply additional compute.

\subsubsection{Data-Limited Scaling
Regimes}\label{sec-vol2-intro-datalimited-scaling-regimes-ba1d}

The relationship between generalization error and dataset size exhibits
three distinct regimes. When limited examples are available, high
generalization error results from inadequate statistical estimates. As
data availability increases, generalization error decreases predictably
as a function of dataset size, following a power-law relationship that
provides the most practical benefit from data scaling. Eventually,
performance reaches saturation, approaching a floor determined by
inherent data limitations or model capacity, beyond which additional
data yields negligible improvements.
Figure~\ref{fig-data-scaling-regimes} maps these transitions across the
three regimes.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8b59336ddb04017dc62ea48a8c24921cbc85fa43.pdf}}

}

\caption{\label{fig-data-scaling-regimes}: \textbf{Data Scaling
Regimes}: The relationship between dataset size and generalization error
follows distinct scaling regimes. Increasing dataset size initially
reduces generalization error following a power-law relationship, but
eventually plateaus at an irreducible error floor determined by inherent
data limitations or model capacity
(\citeproc{ref-hestness2017deep}{\textbf{hestness2017deep?}}). This
behavior exposes diminishing returns from data scaling and informs
practical decisions about data collection efforts in machine learning
systems.}

\end{figure}%

This three-regime pattern manifests across different resource dimensions
beyond data alone. Operating within the power-law region provides the
most reliable return on resource investment. Reaching this regime
requires minimum resource thresholds, while maintaining operation within
it demands careful allocation to avoid premature saturation.

\subsubsection{Temporal Scaling
Regimes}\label{sec-vol2-intro-temporal-scaling-regimes-e118}

The data-driven regimes just described characterize performance across
dataset sizes, revealing where scaling becomes inefficient. A
complementary perspective asks: when during the ML lifecycle should we
invest computational resources? Rather than focusing on how much data,
this temporal lens examines whether to invest in pre-training,
post-training adaptation, or inference-time computation. Recent research
has identified three distinct \textbf{temporal scaling regimes} that
reveal additional optimization opportunities beyond data scaling alone.

\textbf{Pre-training scaling} encompasses the traditional domain of
scaling laws, characterizing how model performance improves with larger
architectures, expanded datasets, and increased compute during initial
training. Extensive study in foundation models has established clear
power-law relationships between resources and capabilities.

\textbf{Post-training scaling} characterizes improvements achieved after
initial training through techniques including fine-tuning, prompt
engineering, and task-specific adaptation. This regime has gained
prominence with foundation models, where adaptation rather than
retraining frequently provides the most efficient path to enhanced
performance under moderate resource requirements.

\textbf{Test-time scaling} characterizes how performance improvements
result from additional compute allocation during inference without
modifying model parameters. This encompasses methods including ensemble
prediction, chain-of-thought prompting, and iterative refinement,
enabling models to allocate additional processing time per input.

These temporal regimes exhibit distinct characteristics in computational
resource allocation for performance improvement.
Figure~\ref{fig-scaling-regimes} illustrates how pre-training demands
massive resources while providing broad capabilities, post-training
offers targeted enhancements under moderate requirements, and test-time
scaling enables flexible performance-compute trade-offs adjustable per
inference.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/56d7e9fc9d849e98a6d33b86a1e357a7d1d6a6fd.pdf}}

}

\caption{\label{fig-scaling-regimes}: \textbf{Temporal Scaling Regimes}:
Different temporal scaling regimes offer distinct approaches to
improving model performance with varying compute investments.
Pre-training establishes broad capabilities through large-scale training
from scratch, post-training refines existing models through additional
training phases, and test-time scaling dynamically allocates compute
during inference to enhance per-sample results. Understanding these
regimes clarifies the trade-offs between upfront investment and
flexible, on-demand resource allocation for optimal system performance.}

\end{figure}%

Data-driven and temporal scaling regimes inform system design by
revealing multiple paths to performance improvement beyond scaling
training resources alone. For resource-constrained deployments,
post-training and test-time scaling may provide more practical
approaches than complete model retraining, while data-efficient
techniques enable effective system operation within the power-law regime
using smaller datasets.

\subsection{Practical Applications in System
Design}\label{sec-vol2-intro-practical-applications-system-design-5c97}

Understanding scaling laws informs practical system design and resource
planning. Consistent observation of power-law trends indicates that
within well-defined operational regimes, model performance depends
predominantly on scale rather than idiosyncratic architectural
innovations. However, diminishing returns phenomena indicate that each
additional improvement demands exponentially increased resources while
delivering progressively smaller benefits.

OpenAI's development of GPT-3 demonstrates this principle. Rather than
conducting expensive architecture searches, the authors applied scaling
laws derived from earlier experiments to determine optimal training
dataset size and model parameter count
(\citeproc{ref-brown2020language}{Brown et al. 2020}). They scaled an
established transformer architecture along the compute-optimal frontier
to 175 billion parameters and approximately 300 billion tokens, enabling
advance prediction of model performance and resource requirements.

Scaling laws serve multiple practical functions in system design. They
enable practitioners to estimate returns on investment for different
resource allocations during resource budgeting. Under fixed
computational budgets, designers can utilize empirical scaling curves to
determine optimal performance improvement strategies across model size,
dataset expansion, or training duration.

System designers can utilize scaling trends to identify when
architectural changes yield significant improvements relative to gains
achieved through scaling alone, thereby avoiding exhaustive architecture
search. When a model family exhibits favorable scaling behavior, scaling
the existing architecture may prove more effective than transitioning to
more complex but unvalidated designs.

In edge and embedded environments with constrained resource budgets,
understanding performance degradation under model scaling enables
designers to select smaller configurations delivering acceptable
accuracy within deployment constraints. By quantifying scale-performance
trade-offs, scaling laws identify when brute-force scaling becomes
inefficient and indicate the necessity for alternative approaches
including model compression, efficient knowledge transfer, sparsity
techniques, and hardware-aware design.

Scaling laws also function as diagnostic instruments. Performance
plateaus despite increased resources may indicate dimensional
saturation---such as inadequate data relative to model size---or
inefficient computational resource utilization. This diagnostic
capability renders scaling laws both predictive and prescriptive,
facilitating systematic bottleneck identification and resolution.

\subsection{Sustainability and Cost
Implications}\label{sec-vol2-intro-sustainability-cost-implications-0473}

Scaling laws illuminate pathways to performance enhancement while
revealing rapidly escalating resource demands. As models expand,
training and deployment resource requirements grow disproportionately,
creating tension between performance gains through scaling and system
efficiency.

Training large-scale models necessitates substantial processing power,
typically requiring distributed
infrastructures\sidenote{\textbf{Distributed Infrastructure}: Computing
systems spreading ML workloads across machines connected by high-speed
networks (InfiniBand 400Gb/s, NVLink 900GB/s). GPT-4 training reportedly
used 10,000+ GPUs coordinated across clusters. Communication overhead
(AllReduce synchronization) can consume 30-60\% of training time, making
network topology and parallelism strategy critical design decisions. }
comprising hundreds or thousands of accelerators. State-of-the-art
language model training may require tens of thousands of GPU-days,
consuming millions of kilowatt-hours of electricity.
\textbf{?@sec-distributed-training} examines how these distributed
training systems introduce additional complexity around communication
overhead, synchronization, and scaling efficiency.

Large models require extensive, high-quality, diverse datasets to
achieve their full potential. Data collection, cleansing, and labeling
processes consume considerable time and resources. As models approach
saturation of available high-quality data, particularly in natural
language processing, additional performance gains through data scaling
become increasingly difficult to achieve. This reality underscores data
efficiency as a necessary complement to brute-force scaling approaches.

The financial and environmental implications compound these challenges.
Training runs for large foundation models can incur millions of dollars
in computational expenses. Associated carbon
footprints\sidenote{\textbf{Carbon Emissions}: ML carbon footprint
depends on training duration, hardware efficiency, and grid carbon
intensity. GPT-3 training emitted \textasciitilde552 tons
CO₂---equivalent to 120 cars annually. Grid location matters 10\$
imes\$: training in Quebec (hydropower) vs.~Poland (coal) differs
dramatically. Tools like CodeCarbon and ML CO2 Impact enable
carbon-aware ML development. } have garnered increasing scrutiny.
Published estimates suggest that training large language models can emit
on the order of \(10^2\) to \(10^3\) tons of CO\(_2\) equivalent, though
estimates vary widely with assumptions about hardware, utilization, and
electricity mix. These costs limit accessibility to cutting-edge
research and exacerbate disparities in access to advanced AI systems.

While scaling laws provide valuable frameworks for understanding
performance growth, they do not constitute unencumbered paths to
improvement. Each incremental performance gain demands evaluation
against corresponding resource requirements. As systems approach
practical scaling limits, emphasis must transition from scaling alone to
efficient scaling---a comprehensive approach balancing performance,
cost, energy consumption, and environmental impact.

\subsection{Scaling Law Breakdown
Conditions}\label{sec-vol2-intro-scaling-law-breakdown-conditions-1f8c}

Scaling laws exhibit remarkable consistency within specific operational
regimes but possess inherent limitations. As systems expand, they
inevitably encounter boundaries where underlying assumptions of smooth,
predictable scaling cease to hold. These breakdown points expose
critical inefficiencies and emphasize the necessity for refined system
design approaches.

For scaling laws to remain valid, model size, dataset size, and
computational budget must be augmented in coordinated fashion.
Over-investment in one dimension while maintaining others constant often
results in suboptimal outcomes. Increasing model size without expanding
training datasets may induce overfitting. Increasing computational
resources without model redesign may lead to inefficient utilization
(\citeproc{ref-hoffmann2022training}{Hoffmann et al. 2022}).

Large-scale models require carefully tuned training schedules and
learning rates to fully utilize available resources. When compute is
insufficiently allocated due to premature stopping, batch size
misalignment, or ineffective parallelism, models may fail to reach
performance potential despite significant infrastructure investment.

Scaling laws presuppose continued performance improvement with
sufficient training data. However, in numerous domains, availability of
high-quality, human-annotated data is finite. As models consume
increasingly large datasets, they reach points of diminishing marginal
utility where additional data contributes minimal new information.
Beyond this threshold, larger models may exhibit memorization rather
than generalization.

As models grow, they demand greater memory
bandwidth\sidenote{\textbf{Memory Bandwidth}: The rate at which data can
be read from or written to memory, measured in GB/s. In representative
systems, datacenter accelerators can provide bandwidth on the order of
TB/s, while general-purpose CPU memory subsystems are often on the order
of tens to low hundreds of GB/s. This gap is a key reason why large
models can be memory-bound. }, interconnect capacity, and I/O
throughput. These hardware limitations become increasingly challenging
even with specialized accelerators. Distributing trillion-parameter
models across clusters necessitates meticulous management of data
parallelism, communication overhead, and fault tolerance.

At extreme scales, models may approach limits of what can be learned
from training distributions. Performance on benchmarks may continue
improving, but these improvements may no longer reflect meaningful gains
in generalization or understanding. Models may become increasingly
brittle, susceptible to adversarial examples, or prone to generating
plausible but inaccurate outputs.

Table~\ref{tbl-scaling-breakdown} categorizes the primary causes of
scaling failure, mapping each breakdown type to its underlying cause and
providing representative scenarios that guide practitioners in
anticipating inefficiencies and designing balanced systems.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Scaling Breakdown Types}: Unbalanced scaling across
model size, data volume, and compute resources leads to specific failure
modes, such as overfitting or diminishing returns, impacting system
performance and efficiency. The table categorizes these breakdowns,
identifies their root causes, and provides representative scenarios to
guide more effective system design and resource
allocation.}\label{tbl-scaling-breakdown}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dimension Scaled}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Type of Breakdown}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Underlying Cause}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Scenario}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dimension Scaled}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Type of Breakdown}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Underlying Cause}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Scenario}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Model Size} & Overfitting & Model capacity exceeds available
data & Billion-parameter model on limited dataset \\
\textbf{Data Volume} & Diminishing Returns & Saturation of new or
diverse information & Scaling web text beyond useful threshold \\
\textbf{Compute Budget} & Underutilized Resources & Insufficient
training steps or inefficient use & Large model with truncated training
duration \\
\textbf{Imbalanced Scaling} & Inefficiency & Uncoordinated increase in
model/data/compute & Doubling model size without more data or time \\
\textbf{All Dimensions} & Semantic Saturation & Exhaustion of learnable
patterns in the domain & No further gains despite scaling all inputs \\
\end{longtable}

These breakdown points demonstrate that scaling laws describe empirical
regularities under specific conditions that become increasingly
difficult to maintain at scale. As machine learning systems continue
evolving, discerning where and why scaling ceases to be effective
becomes necessary, driving development of strategies that enhance
performance without relying solely on scale.

\subsection{Integrating Efficiency with
Scaling}\label{sec-vol2-intro-integrating-efficiency-scaling-a513}

Scaling laws reveal the walls; efficiency engineering builds the paths
around them. Data saturation, infrastructure bottlenecks, and
diminishing returns set hard limits on what brute-force scaling can
achieve. But these same constraints point toward solutions: if we cannot
always add more data, we must extract more value from existing data. If
compute becomes the bottleneck, we must use compute more effectively. If
larger models become impractical, we must make smaller models smarter.

Three interconnected dimensions address the specific limitations that
scaling analysis revealed, working together to achieve what scaling
alone cannot. The efficiency framework that structures the remainder of
this chapter builds on this insight.

\section{Real-World Efficiency
Strategies}\label{sec-vol2-intro-realworld-efficiency-strategies-8387}

The scaling constraints identified above manifest differently depending
on deployment context. A cloud training cluster faces different
bottlenecks than an edge inference device, and the efficiency strategies
appropriate for each diverge accordingly. This section examines how
algorithmic efficiency (model architecture and complexity), compute
efficiency (hardware utilization and throughput), and data efficiency
(training data requirements and quality) interact within specific
operational environments. \textbf{?@sec-inference-at-scale} develops the
infrastructure considerations for model serving in these environments,
covering latency optimization, batching strategies, and runtime
selection.

\subsection{Context-Specific Efficiency
Requirements}\label{sec-vol2-intro-contextspecific-efficiency-requirements-47e6}

The specific priorities and trade-offs vary dramatically across
deployment environments. As our opening examples illustrated, these
range from cloud systems with abundant resources to edge devices with
severe memory and power constraints.
Table~\ref{tbl-deployment-efficiency-priorities} maps each deployment
context to its primary constraints, efficiency priorities, and
representative applications, providing a systematic framework for
matching optimization strategies to operational requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Efficiency Optimization Priorities by Deployment
Context}: Each environment demands different trade-offs between
algorithmic, compute, and data optimization strategies based on unique
constraints. Cloud systems prioritize scalability, edge deployments
focus on real-time performance, mobile applications balance performance
with battery life, and TinyML demands extreme resource
efficiency.}\label{tbl-deployment-efficiency-priorities}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraints}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Efficiency Priorities}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Representative Applications}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraints}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Efficiency Priorities}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Representative Applications}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Cloud} & Cost at scale, energy consumption & Throughput,
scalability, operational efficiency & Large language model APIs,
recommendation engines, video processing \\
\textbf{Edge} & Latency, local compute capacity, connectivity &
Real-time performance, power efficiency & Autonomous vehicles,
industrial automation, smart cameras \\
\textbf{Mobile} & Battery life, memory, thermal limits & Energy
efficiency, model size, responsiveness & Voice assistants, photo
enhancement, augmented reality \\
\textbf{TinyML} & Extreme power/memory constraints & Ultra-low power,
minimal model size & IoT sensors, wearables, environmental monitoring \\
\end{longtable}

Understanding these context-specific patterns enables designers to make
informed decisions about which efficiency dimensions to prioritize and
how to address inevitable trade-offs.

\subsection{Scalability and
Sustainability}\label{sec-vol2-intro-scalability-sustainability-4d30}

System efficiency serves as a driver of environmental sustainability.
When systems are optimized for efficiency, they can be deployed at scale
while minimizing environmental footprint.
Figure~\ref{fig-virtuous-efficiency-cycle} illustrates this positive
feedback loop. Efficiency enables scalability. Scalability amplifies
impact. Sustainable practices reinforce the commitment to efficiency,
creating a self-reinforcing cycle that drives long-term system
improvement.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/59f01c66ef13b3741aa168801d76138d5f15f043.pdf}}

}

\caption{\label{fig-virtuous-efficiency-cycle}: \textbf{Efficiency and
Sustainability Feedback Loop}: The virtuous cycle connecting efficiency,
scalability, and sustainability. Efficient systems require fewer
resources per operation, enabling broader deployment (scalability).
Scaled deployment amplifies the impact of efficiency gains, making
sustainable design economically viable. Each improvement reinforces the
others, creating a self-strengthening cycle that drives long-term
progress in ML system design.}

\end{figure}%

Efficient systems are inherently scalable. Reducing resource demands
through lightweight models, targeted datasets, and optimized compute
utilization allows systems to deploy broadly. When efficient systems
scale, they amplify their contribution to sustainability by reducing
overall energy consumption and computational waste. Sustainability
reinforces the need for efficiency, creating a feedback loop that
strengthens the entire system.

\section{Efficiency Trade-offs and
Challenges}\label{sec-vol2-intro-efficiency-tradeoffs-challenges-946d}

The three efficiency dimensions can work synergistically under favorable
conditions, but real-world systems often face scenarios where improving
one dimension degrades another. The same resource constraints that make
efficiency necessary force difficult choices. Reducing model size may
sacrifice accuracy. Optimizing for real-time performance may increase
energy consumption. Curating smaller datasets may limit generalization.

\subsection{Fundamental Sources of Efficiency
Trade-offs}\label{sec-vol2-intro-fundamental-sources-efficiency-tradeoffs-d16f}

These tensions manifest in various ways across machine learning systems.
Understanding their root causes is essential for addressing design
challenges. Each efficiency dimension influences the others, creating a
dynamic interplay that shapes system performance.

\subsubsection{Algorithmic Efficiency vs.~Compute
Requirements}\label{sec-vol2-intro-algorithmic-efficiency-vs-compute-requirements-83a7}

Algorithmic efficiency focuses on designing compact models that minimize
computational and memory demands. By reducing model size or complexity,
deployment on resource-limited devices becomes feasible. Overly
simplifying a model can reduce accuracy, especially for complex tasks.
To compensate for this loss, additional computational resources may be
required during training or deployment, placing strain on compute
efficiency.

\subsubsection{Compute Efficiency vs.~Real-Time
Needs}\label{sec-vol2-intro-compute-efficiency-vs-realtime-needs-a269}

Compute efficiency aims to minimize resources required for training and
inference, reducing energy consumption, processing time, and memory use.
In scenarios requiring real-time responsiveness (autonomous vehicles,
augmented reality), compute efficiency becomes harder to maintain.
Real-time systems often require high-performance hardware to process
data instantly, conflicting with energy efficiency goals or increasing
system costs. Figure~\ref{fig-efficiency-vs-latency} quantifies this
tension: at 120 km/h, a 100ms processing delay translates to 3.33 meters
of uncertainty in vehicle position, directly impacting safety margins
and braking distance calculations.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/333ea02d84ae167e2b7f2168d6a52dfb4afb043c.pdf}}

}

\caption{\label{fig-efficiency-vs-latency}: \textbf{Real-Time System
Constraints}: Latency directly impacts safety in autonomous systems. At
120 km/h (33.3 m/s), a vehicle travels 3.33 meters during 100ms of
processing delay, creating position uncertainty that compounds braking
distance calculations. This quantifies the tension between compute
efficiency (lower power, lower cost) and real-time performance (faster
processing, higher power). Safety-critical systems cannot simply
optimize for efficiency; they must meet hard latency bounds.}

\end{figure}%

\subsubsection{Data Efficiency vs.~Model
Generalization}\label{sec-vol2-intro-data-efficiency-vs-model-generalization-044a}

Data efficiency seeks to minimize the amount of data required to train a
model without sacrificing performance. By curating smaller, high-quality
datasets, training becomes faster and less resource-intensive. Ideally,
this reinforces both algorithmic and compute efficiency. However,
reducing dataset size can limit diversity, making it harder for models
to generalize to unseen scenarios. To address this, additional compute
resources or model complexity may be required, creating tension between
data efficiency and broader system goals.

\subsection{Recurring Trade-off Patterns in
Practice}\label{sec-vol2-intro-recurring-tradeoff-patterns-practice-c205}

The trade-offs between efficiency dimensions become particularly evident
when examining specific scenarios. Complex models with millions or
billions of parameters can achieve higher accuracy by capturing
intricate patterns, but require significant computational power and
memory. A recommendation system in a cloud data center might use a
highly complex model for better recommendations, but at the cost of
higher energy consumption and operating costs. On resource-constrained
devices like smartphones or autonomous vehicles, compact models may
operate efficiently but require more sophisticated data preprocessing or
training procedures to compensate for reduced capacity.

Energy efficiency and real-time performance often pull systems in
opposite directions. Real-time systems like autonomous vehicles or
augmented reality applications rely on high-performance hardware to
process large volumes of data quickly, but this typically increases
energy consumption. An autonomous vehicle must process sensor data from
cameras, LiDAR, and radar in real time to make navigation decisions,
requiring specialized accelerators that consume significant energy. In
edge deployments with battery power or limited energy sources, this
trade-off becomes even more critical.

Larger datasets generally provide greater diversity and coverage,
enabling models to capture subtle patterns and reduce overfitting risk.
However, computational and memory demands of training on large datasets
can be substantial. In resource-constrained environments like TinyML
deployments, an IoT device monitoring environmental conditions might
need a model that generalizes well across varying conditions, but
collecting extensive datasets may be impractical due to storage and
computational limitations. Smaller, carefully curated datasets or
synthetic data may be used to reduce computational strain, but this
risks missing key edge cases.

These trade-offs are not merely academic concerns but practical
realities that shape system design decisions across all deployment
contexts.

\section{Strategic Trade-off
Management}\label{sec-vol2-intro-strategic-tradeoff-management-0ac8}

The trade-offs inherent in machine learning system design require
thoughtful strategies. Achieving the right balance involves difficult
decisions heavily influenced by specific goals and constraints of the
deployment environment. Designers can adopt a range of strategies that
address unique requirements of different contexts.

\subsection{Environment-Driven Efficiency
Priorities}\label{sec-vol2-intro-environmentdriven-efficiency-priorities-4057}

Efficiency goals are rarely universal. The specific demands of an
application or deployment scenario heavily influence which
dimension---algorithmic, compute, or data---takes precedence.
Prioritizing the right dimensions based on context is the first step in
effectively managing trade-offs.

In Mobile ML deployments, battery life is often the primary constraint,
placing a premium on compute efficiency. Energy consumption must be
minimized to preserve operational time, so lightweight models are
prioritized even if it means sacrificing some accuracy or requiring
additional data preprocessing.

In Cloud ML systems, scalability and throughput are paramount. These
systems must process large volumes of data and serve millions of users
simultaneously. While compute resources are more abundant, energy
efficiency and operational costs remain important. Algorithmic
efficiency plays a critical role in ensuring systems can scale without
overwhelming infrastructure.

Edge ML systems present different priorities. Autonomous vehicles or
real-time monitoring systems require low-latency processing for safe and
reliable operation, making real-time performance and compute efficiency
paramount, often at the expense of energy consumption. However, hardware
constraints mean these systems must still carefully manage energy and
computational resources.

TinyML deployments demand extreme efficiency due to severe hardware and
energy limitations. Algorithmic and data efficiency are top priorities,
with models highly compact and capable of operating on microcontrollers
with minimal memory and compute power, while training relies on small,
carefully curated datasets.

\subsection{Dynamic Resource Allocation at
Inference}\label{sec-vol2-intro-dynamic-resource-allocation-inference-d6bc}

System adaptability can be enhanced through dynamic resource allocation
during inference. This approach recognizes that resource needs may
fluctuate even within specific deployment contexts. By adjusting
computational effort at inference time, systems can fine-tune
performance to meet immediate demands.

For example, a cloud-based video analysis system might process standard
streams with a streamlined model to maintain high throughput, but when a
critical event is detected, dynamically allocate more resources to a
complex model for higher precision. Similarly, mobile voice assistants
might use lightweight models for routine commands to conserve battery,
but temporarily activate resource-intensive models for complex queries.

Implementing test-time compute introduces new challenges. Dynamic
resource allocation requires sophisticated monitoring and control
mechanisms. There are diminishing returns---increasing compute beyond
certain thresholds may not yield significant performance improvements.
The ability to dynamically increase compute can also create disparities
in access to high-performance AI, raising equity concerns. Despite these
challenges, test-time compute offers a valuable strategy for enhancing
system adaptability.

\subsection{End-to-End Co-Design and Automated
Optimization}\label{sec-vol2-intro-endtoend-codesign-automated-optimization-1220}

Efficient machine learning systems are rarely the product of isolated
optimizations. Achieving balance across efficiency dimensions requires
an end-to-end co-design perspective where each system component is
designed in tandem with others. This holistic approach aligns model
architectures, hardware platforms, and data pipelines to work seamlessly
together.

Co-design becomes essential in resource-constrained environments. Models
must align precisely with hardware capabilities. 8-bit models require
hardware support for efficient integer operations, while pruned models
benefit from sparse tensor operations. Edge accelerators often optimize
specific operations like convolutions, influencing model architecture
choices. \textbf{?@sec-infrastructure} develops these hardware
architecture considerations comprehensively for distributed systems,
including cluster topologies, accelerator selection, and network fabric
design.

\textbf{Automation and optimization tools} help manage the complexity of
these trade-offs. Automated machine learning
(AutoML)\sidenote{\textbf{AutoML}: Automated ML pipeline optimization
covering feature engineering, model selection, and hyperparameter
tuning. Google's AutoML Vision achieved 84.3\% ImageNet accuracy
vs.~experts' 78.5\% with minimal manual intervention. Tools like
Auto-sklearn, H2O AutoML, and Amazon SageMaker Autopilot democratize ML,
though understanding the search space remains crucial for practitioners.
} enables exploration of different model architectures and
hyperparameter configurations. At production scale, AutoML tools extend
to distributed hyperparameter search across GPU clusters, automating
many efficiency optimization decisions that traditionally required
extensive manual tuning.

Neural architecture search (NAS)\sidenote{\textbf{Neural Architecture
Search (NAS)}: Automated architecture discovery through reinforcement
learning, evolution, or gradient-based methods. Early NAS required 2,000
GPU-days; weight-sharing (DARTS, 2019) reduced this to 1-4 GPU-days.
NAS-discovered EfficientNet surpasses hand-designed architectures,
demonstrating that search over discrete architecture spaces is tractable
and often superior to human expertise. } takes automation further by
designing model architectures tailored to specific hardware or
deployment scenarios, evaluating a wide range of architectural
possibilities to maximize performance while minimizing computational
demands.

Data efficiency also benefits from automation. Tools that automate
dataset curation, augmentation, and active learning reduce training
dataset size without sacrificing performance, prioritizing high-value
data points to speed up training and reduce computational overhead
(\citeproc{ref-settles2009active}{\textbf{settles2009active?}}). Modern
ML frameworks incorporate these automation capabilities through built-in
distributed data loading, augmentation pipelines, and experiment
tracking that scale across cluster deployments.

\subsection{Measuring and Monitoring Efficiency
Trade-offs}\label{sec-vol2-intro-measuring-monitoring-efficiency-tradeoffs-fd5b}

Beyond technical automation lies the broader challenge of systematic
evaluation. Efficiency optimization necessitates a structured approach
assessing trade-offs that extends beyond purely technical
considerations. As systems transition from research to production,
success criteria must encompass algorithmic performance, economic
viability, and operational sustainability.

Costs associated with efficiency improvements manifest across
engineering effort (research, experimentation, integration), balanced
against ongoing operational expenses of running less efficient systems.
Benefits span multiple domains---beyond direct cost reductions,
efficient systems often enable qualitatively new capabilities like
real-time processing in resource-constrained environments or deployment
to edge devices.

This evaluation framework must be complemented by ongoing assessment
mechanisms. The dynamic nature of ML systems in production necessitates
continuous monitoring of efficiency characteristics. As models evolve,
data distributions shift, and infrastructure changes, efficiency
properties can degrade. Real-time monitoring enables rapid detection of
efficiency regressions, while historical analysis provides insight into
longer-term trends, revealing whether efficiency improvements are
sustainable under changing conditions.

\section{Engineering Principles for Efficient
AI}\label{sec-vol2-intro-engineering-principles-efficient-ai-1206}

Designing an efficient machine learning system requires a holistic
approach. True efficiency emerges when the entire system is considered
as a whole, ensuring trade-offs are balanced across all stages of the ML
pipeline from data collection to deployment. This end-to-end perspective
transforms system design.

\subsection{Holistic Pipeline
Optimization}\label{sec-vol2-intro-holistic-pipeline-optimization-5bcc}

Efficiency is achieved not through isolated optimizations but by
considering the entire pipeline as a unified whole. Each stage (data
collection, model training, hardware deployment, and inference)
contributes to overall system efficiency. Decisions at one stage ripple
through the rest, influencing performance, resource use, and
scalability.

Data collection and preprocessing are starting points. Data pipeline
design decisions cascade through the entire system, from storage
architecture through feature engineering to training-serving
consistency. At distributed scale, these decisions determine whether
data ingestion can keep pace with accelerator throughput.
\textbf{?@sec-storage} examines how storage hierarchies and data formats
must be designed to sustain petabyte-scale datasets. Curating smaller,
high-quality datasets can reduce computational costs during training
while simplifying model design. However, insufficient data diversity may
affect generalization, necessitating compensatory measures.

Model training is another critical stage. Architecture choice,
optimization techniques, and hyperparameters must consider deployment
hardware constraints. A model designed for high-performance cloud
systems may emphasize accuracy and scalability, while models for edge
devices must balance accuracy with size and energy efficiency.

Deployment and inference demand precise hardware alignment. Each
platform offers distinct capabilities---GPUs excel at parallel matrix
operations, TPUs optimize specific neural network computations, and
microcontrollers provide energy-efficient processing. A smartphone
speech recognition system might leverage an NPU's dedicated convolution
units for millisecond-level inference at low power, while an autonomous
vehicle's FPGA processes multiple sensor streams with microsecond-level
latency.

An end-to-end perspective ensures trade-offs are addressed holistically
rather than shifting inefficiencies between pipeline stages. This
systems thinking approach becomes particularly critical when deploying
to resource-constrained environments, as explored in this chapter.

\subsection{Lifecycle and Environment
Considerations}\label{sec-vol2-intro-lifecycle-environment-considerations-3abc}

Efficiency needs differ significantly depending on lifecycle stage and
deployment environment, from research prototypes to production systems
and from high-performance cloud to resource-constrained edge.

In research, the primary focus is often model performance, with
efficiency taking a secondary role. Prototypes are trained using
abundant compute resources, enabling exploration of large architectures
and extensive hyperparameter tuning. Production systems must prioritize
efficiency to operate within practical constraints, often involving
significant optimization like model pruning, quantization, or
retraining. \textbf{?@sec-ops-scale} develops comprehensive production
efficiency management strategies for distributed deployments, covering
monitoring frameworks, deployment automation, and operational trade-off
management at organizational scale.

Cloud-based systems handle massive workloads with relatively abundant
resources, though energy efficiency and operational costs remain
critical. The infrastructure principles developed throughout this volume
provide architectural foundations for building scalable,
efficiency-optimized distributed deployments. In contrast, edge and
mobile systems operate under strict constraints detailed in our
efficiency framework, demanding solutions prioritizing efficiency over
raw performance. \textbf{?@sec-edge-intelligence} examines how federated
learning and edge-cloud coordination address these constraints.

Some systems like recommendation engines require frequent retraining to
remain effective, depending heavily on data efficiency with actively
labeled datasets and sampling strategies. Other systems like embedded
models in medical devices require long-term stability with minimal
updates. Reliability requirements in critical applications significantly
influence efficiency optimization strategies.

\subsection{Optimization
Limits}\label{sec-vol2-intro-optimization-limits-20f0}

The tensions between equity, innovation, and efficiency ultimately stem
from a core characteristic of optimization: diminishing returns.
Optimization is central to building efficient ML systems but is not
infinite. As systems become more refined, each additional improvement
requires exponentially more effort, time, or resources while delivering
increasingly smaller benefits.

The No Free Lunch (NFL) theorems\sidenote{\textbf{No Free Lunch (NFL)
Theorems}: Mathematical proof by Wolpert and Macready (1997) showing
that averaged over all possible optimization problems, every algorithm
performs equally well. In ML context, no universal optimization
technique exists---methods must be tailored to specific problem domains.
} for optimization illustrate inherent limitations. According to NFL
theorems, no single optimization algorithm can outperform all others
across every possible problem, implying optimization technique
effectiveness is highly problem-specific
(\citeproc{ref-wolpert1997no}{\textbf{wolpert1997no?}}).

Compressing an ML model can initially reduce memory and compute
requirements significantly with minimal accuracy loss. However, as
compression progresses, maintaining performance becomes increasingly
challenging. Achieving additional gains may necessitate sophisticated
techniques like hardware-specific optimizations or extensive retraining,
increasing complexity and cost. These costs extend beyond financial
investment to include time, expertise, iterative testing, and potential
trade-offs in robustness and generalizability.

The NFL theorems highlight that no universal optimization solution
exists, emphasizing need to balance efficiency pursuits with practical
considerations. Over-optimization risks wasted resources and reduced
adaptability, complicating future updates. Identifying when a system is
``good enough'' ensures resources are allocated effectively.

Similarly, optimizing datasets for training efficiency may initially
save resources, but excessively reducing dataset size risks compromising
diversity and weakening generalization. Pushing hardware to performance
limits may improve metrics like latency, yet associated reliability
concerns and engineering costs can outweigh gains.

Understanding optimization limits is essential for creating systems
balancing efficiency with practicality and sustainability. This
perspective helps avoid over-optimization and ensures resources are
invested in areas with meaningful returns.

\subsubsection{Moore's Law Case
Study}\label{sec-vol2-intro-moores-law-case-study-5767}

One of the most insightful examples of optimization limits appears in
Moore's Law and the economic curve underlying it. While Moore's Law is
celebrated as a predictor of exponential computational power growth, its
success relied on intricate economic balance, one that ultimately
collapsed with \emph{the end of Dennard scaling}. The relationship
between integration and cost provides a compelling analogy for
diminishing returns in ML optimization.

\phantomsection\label{callout-perspectiveux2a-2.4}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The End of Dennard Scaling}
\phantomsection\label{callout-perspective*-2.4}
\textbf{The Hard Wall}: For decades, Moore's Law worked in tandem with
\textbf{Dennard Scaling}, which allowed transistor frequency to increase
as they got smaller without increasing power density. Around 2005, this
``free lunch'' ended due to leakage current and thermal limits.
Computers could no longer get faster just by making transistors smaller;
they had to get more efficient through \textbf{parallelism} and
\textbf{architectural specialization}. This historical pivot in computer
architecture is now repeating in AI: we have hit the power wall of
brute-force scaling, and the next decade of progress will be defined not
by who can build the largest model, but by who can build the most
efficient one. Efficiency is no longer an optimization; it is the new
scaling law.

\end{fbx}

Relative manufacturing cost per component decreases initially as the
number of components in an integrated circuit increases. Higher
integration reduces need for packaging and interconnects through
economies of scale. Moving from hundreds to thousands of components
drastically reduced costs and improved performance
(\citeproc{ref-moore2021cramming}{\textbf{moore2021cramming?}}).
Figure~\ref{fig-moores-law-plot} reveals the U-shaped cost curve across
three eras (1962, 1965, 1970), showing how the inflection point shifted
rightward over time as manufacturing technology improved, yet the
fundamental pattern of diminishing returns persisted.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a65838d58152976ed81f1c96b2180f9390afa757.pdf}}

}

\caption{\label{fig-moores-law-plot}: \textbf{Moore's Law Economics}:
Declining per-component manufacturing costs initially drove exponential
growth in integrated circuit complexity, but diminishing returns
eventually limited further cost reductions. This relationship mirrors
optimization challenges in machine learning, where increasing model
complexity yields diminishing gains in performance relative to
computational expense.
(\citeproc{ref-moore2021cramming}{\textbf{moore2021cramming?}}).}

\end{figure}%

However, as integration continues, the curve begins to rise. Components
packed closer together face reliability issues like increased heat
dissipation and signal interference. Addressing these requires more
sophisticated manufacturing techniques such as advanced lithography,
error correction, and improved materials, increasing complexity and
cost. This U-shaped curve captures the fundamental trade-off: early
improvements yield substantial benefits, but beyond a certain point,
each additional gain comes at greater cost.

The dynamics mirror ML optimization challenges. Compressing a deep
learning model to reduce size and energy consumption follows a similar
trajectory. Initial optimizations like pruning redundant parameters or
reducing precision often lead to significant savings with minimal
accuracy impact. However, as compression progresses, performance losses
become harder to recover. Techniques like quantization or
hardware-specific tuning can restore some performance, but these add
complexity and cost.

Similarly, in data efficiency, reducing training dataset size often
improves computational efficiency initially. Yet as datasets shrink
further, they may lose diversity, compromising generalization.
Addressing this often involves synthetic data or sophisticated
augmentation, demanding additional engineering effort.

This parallel between semiconductor scaling and ML optimization is more
than analogy. The Moore's Law plot serves as a visual reminder that
optimization is not infinite. The cost-benefit balance is always
context-dependent, and the point of diminishing returns varies based on
system goals and constraints. ML practitioners, like semiconductor
engineers, must identify when further optimization ceases to provide
meaningful benefits. Over-optimization can lead to wasted resources,
reduced adaptability, and systems overly specialized to initial
conditions.

\section{Fallacies and
Pitfalls}\label{sec-vol2-intro-fallacies-pitfalls-f804}

Efficiency optimization involves counterintuitive trade-offs where
improvements in one dimension degrade others. The mathematical elegance
of scaling laws creates false confidence in predictable optimization,
while practitioners assume techniques transfer across deployment
contexts. These fallacies waste resources on misguided optimizations and
lead to production failures when systems encounter real-world
constraints.

\textbf{Fallacy:} \textbf{\emph{Efficiency optimizations always improve
system performance across all metrics.}}

Engineers assume efficiency techniques provide universal benefits. In
production, each optimization introduces specific trade-offs that
constrain other dimensions. INT8 quantization achieves 4\$ imes\$ memory
reduction and 1.5--2\$ imes\$ inference speedup on GPUs with tensor core
support (up to 3--4\$ imes\$ under optimal conditions), but typically
incurs 1--2\% accuracy loss. Structured pruning guarantees 2--4\$ imes\$
speedup on standard hardware but sacrifices 1--3\% accuracy. Knowledge
distillation enables 2--4\$ imes\$ compression (DistilBERT achieves 97\%
of BERT's performance with 40\% fewer parameters) but demands expensive
teacher model training and careful temperature tuning. A team optimizing
mobile inference through aggressive INT4 quantization might achieve 8\$
imes\$ memory reduction but suffer 5--8\% accuracy degradation that
renders the model unusable, wasting weeks of optimization effort on a
technique inappropriate for their accuracy requirements.

\textbf{Pitfall:} \textbf{\emph{Assuming scaling laws predict efficiency
requirements linearly across all model sizes.}}

Teams extrapolate resource requirements using power-law relationships:
\(\mathcal{L}(N) = A N^{-\alpha} + B\), where loss decreases predictably
with model size. This works within validated ranges but fails at
boundaries.
Section~\ref{sec-vol2-intro-scaling-law-breakdown-conditions-1f8c}
identifies breakdowns: data quality degradation when exhausting
high-quality sources, architectural saturation where model capacity
exceeds task complexity, and hardware constraints where communication
overhead dominates computation. A team training 100B-parameter models by
extrapolating from 10B-parameter experiments might predict 3\$ imes\$
improvement but achieve only 1.3\$ imes\$ due to communication overhead
consuming 40\% of compute time at that scale. Production systems
designed assuming linear scaling have experienced 2--3\$ imes\$ cost
overruns and missed deployment deadlines when empirical performance
deviated from power-law predictions beyond validated thresholds.

\textbf{Fallacy:} \textbf{\emph{Edge deployment efficiency requirements
are simply scaled-down versions of cloud requirements.}}

This belief treats edge systems as resource-constrained cloud systems.
Edge devices face qualitatively different constraints that necessitate
distinct optimization strategies.
Section~\ref{sec-vol2-intro-contextspecific-efficiency-requirements-47e6}
demonstrates that autonomous vehicles at 120 km/h convert every 100ms
processing delay into 3.33 meters of positional uncertainty, directly
impacting safety margins. Edge systems operate under 5--15 W power
budgets (versus kilowatt-scale cloud deployments), demanding
energy-proportional architectures where unused components power down.
Thermal constraints limit sustained performance: a smartphone throttles
to 60\% peak throughput after 30 seconds of intensive inference. A team
deploying a cloud-optimized model that achieves 95\% accuracy at 50ms
latency might find it unusable on edge devices where thermal throttling
increases latency to 200ms and drains battery in 45 minutes instead of
the 8-hour requirement.

\textbf{Pitfall:} \textbf{\emph{Focusing on algorithmic efficiency while
ignoring system-level efficiency factors.}}

Engineers optimize FLOPs and parameter counts assuming these metrics
predict deployment performance. Real efficiency depends on hardware
characteristics that algorithmic complexity ignores. INT8 quantization
provides 2--4\$ imes\$ speedup on NVIDIA Turing GPUs with tensor core
support but negligible benefit on CPUs lacking integer SIMD
instructions. Unstructured pruning achieves 80\% sparsity but delivers
no speedup on dense hardware, while structured pruning at 50\% sparsity
guarantees 2\$ imes\$ speedup. Memory-bound operations see throughput
determined by bandwidth (A100: 2 TB/s HBM2e) rather than compute (312
TFLOPS). A model reduced from 10B to 3B parameters (70\% FLOPs
reduction) might achieve only 20\% latency improvement because memory
bandwidth bottlenecks dominate and the pruning pattern lacks
hardware-friendly structure.

\section{Why Scale Changes
Everything}\label{sec-vol2-introduction-why-scale-changes}

Scale is not just ``more of the same.'' Systems that work perfectly at
modest scale exhibit qualitatively different behaviors at production
scale. Understanding scaling laws and efficiency constraints sets the
\emph{requirements}. But fulfilling them requires a fundamental shift in
architecture. This is why we move from single machines to the Machine
Learning Fleet.

\subsection{ML Workloads Are Unique}\label{ml-workloads-are-unique}

Why do we need specialized infrastructure for machine learning? Why not
simply use established distributed systems like Apache Spark or Hadoop?
The answer lies in the fundamental characteristics of ML training
workloads.

Traditional distributed data processing optimizes for
\textbf{throughput} on largely independent tasks. A MapReduce job
processes terabytes of logs where each record is handled separately. If
one node fails, only its task needs re-execution.

Distributed ML training, by contrast, is \textbf{iterative, stateful,
and tightly coupled}.

\begin{itemize}
\tightlist
\item
  \textbf{Iterative}: The system repeats the same computation
  (forward/backward pass) millions of times.
\item
  \textbf{Stateful}: All workers must maintain and synchronize a shared
  state (the model parameters).
\item
  \textbf{Tightly Coupled}: Progress is often blocked until \emph{all}
  workers complete their update.
\end{itemize}

This unique profile creates different engineering constraints. While a
web server optimizes for request latency and a database for transaction
consistency, an ML cluster must optimize for \textbf{all-to-all
bandwidth}. A 10\% performance drop on one node doesn't just reduce
total throughput by a fraction; in synchronous training, it slows down
the entire cluster. This distinct workload profile dictates the design
of the specialized ``ML Fleets'' we examine in Part II.

\subsection{Communication Becomes
Dominant}\label{communication-becomes-dominant}

At small scale, computation dominates. Training a model on a single GPU
spends most time performing matrix multiplications. Communication
overhead (moving data between CPU and GPU memory) represents a small
fraction of total time.

At large scale, communication often dominates. Distributed training
requires synchronizing gradients across workers after each batch. For a
model with 175 billion parameters using 32-bit gradients, each
synchronization must transfer 700GB of data (175 billion parameters × 4
bytes per parameter). Using the ring all-reduce
algorithm\sidenote{\textbf{Ring All-Reduce}: An efficient collective
communication algorithm for gradient synchronization that achieves
near-optimal bandwidth utilization regardless of worker count. For large
models, this synchronization can consume a significant fraction of
training time. \textbf{?@sec-communication} examines AllReduce
algorithms and their trade-offs. } across 1,000 workers connected by 200
Gb/s InfiniBand (25 GB/s), theoretical completion time for the full
synchronization approaches 56 seconds
(\(2 \times 700 \text{ GB} / 25 \text{ GB/s}\)). Practical
implementations achieve 60-80\% of theoretical bandwidth, making
communication consume a significant fraction of total training time even
with optimized networks.

This ratio explains why distributed training systems optimize
communication aggressively. Gradient
compression\sidenote{\textbf{Gradient Compression}: Techniques that
reduce gradient data volume during distributed training, including
sparsification and quantization, achieving 10--100\$ imes\$ compression
with minimal accuracy impact. \textbf{?@sec-communication} examines
compression algorithms and their trade-offs. } reduces transfer volume
by 10\$ imes\$ to 100\$ imes\$ at the cost of some accuracy
(\citeproc{ref-lin2018deep}{Lin et al. 2018}). Overlapping communication
with computation hides transfer latency during the next batch's forward
pass. Hierarchical aggregation reduces cross-rack traffic by combining
gradients locally first. These optimizations, unnecessary at small
scale, become essential at production scale.
\textbf{?@sec-communication} examines these techniques in depth,
developing the quantitative framework for reasoning about when
communication becomes the bottleneck and how to address it.

The balance between transfer time and compute time is captured by the
\emph{communication-computation ratio}, which we now define formally.

\phantomsection\label{callout-definitionux2a-2.5}
\begin{fbx}{callout-definition}{Definition:}{Communication-Computation Ratio}
\phantomsection\label{callout-definition*-2.5}
\textbf{\emph{Communication-Computation Ratio}} describes the relative
time spent transferring data versus performing computation in
distributed systems. A ratio of 1:1 means equal time on each; higher
ratios indicate communication-bound workloads. Modern distributed
training systems typically achieve ratios between 1:3 and 1:1, making
communication optimization critical for efficiency. The ratio depends on
model size (larger models have more gradients to synchronize), batch
size (larger batches amortize communication over more computation), and
network bandwidth (faster networks reduce communication time).

\end{fbx}

Communication optimization addresses one scale-induced challenge, but
the machines generating and consuming that communication present their
own transformation. As systems grow from dozens to thousands of
components, the probability model for hardware reliability changes
fundamentally.

\subsection{Failure Becomes Routine}\label{failure-becomes-routine}

At small scale, failure is exceptional. A well-maintained server might
run for years without hardware issues. Software bugs, once fixed, stay
fixed. Administrators can manually investigate and remediate problems.

At large scale, failure becomes statistical certainty. With 10,000 GPUs,
multiple failures occur weekly. With 100,000 concurrent user sessions,
software edge cases that occur once in a million times happen hundreds
of times daily. Manual intervention becomes impossible; systems must
self-heal. This transition requires architectural changes from the
beginning. Small-scale systems optimize for the common case and handle
failures through manual recovery, while large-scale systems embed
failure handling into their core design:

\begin{itemize}
\tightlist
\item
  \textbf{Checkpointing}\sidenote{\textbf{Checkpointing in Distributed
  Training}: Periodically saving model state to persistent storage,
  enabling recovery from failures. For frontier models, checkpoints
  reach terabytes, making checkpoint management a systems challenge.
  \textbf{?@sec-fault-tolerance} examines checkpointing strategies. }:
  Saving model state frequently enough that losing hours of progress is
  acceptable when failures occur
\item
  \textbf{Redundancy}: Running extra workers that can absorb failed
  workers' tasks without restart
\item
  \textbf{Isolation}: Containing failures so that one component's crash
  does not cascade through the system
\item
  \textbf{Detection}: Monitoring that identifies failures within seconds
\item
  \textbf{Recovery}: Automated procedures that restore service without
  human intervention
\end{itemize}

\textbf{?@sec-fault-tolerance} develops these principles systematically,
showing how to design training systems that treat failure as expected
rather than exceptional.

\subsection{Heterogeneity Emerges}\label{heterogeneity-emerges}

At small scale, systems are homogeneous. A single GPU training job runs
on one type of hardware with one software configuration. Behavior is
predictable and reproducible.

At large scale, heterogeneity becomes unavoidable. A fleet of 10,000
GPUs contains multiple hardware generations purchased over years.
Different racks have different thermal characteristics affecting clock
speeds. Software updates roll out gradually, creating version skew.
Network paths vary in latency and bandwidth.

This heterogeneity creates engineering challenges absent at small scale.
Load balancing must account for hardware capability differences.
Gradient aggregation must handle workers completing at different rates.
Inference routing must direct requests to servers with appropriate model
versions. Testing must verify behavior across configuration variants
that grow combinatorially.

Managing heterogeneity is necessary but not sufficient. Even with
perfect load balancing and configuration management, a deeper challenge
remains: distribution itself introduces fundamental constraints from the
physics of information transfer. These constraints exist regardless of
scale, though they become inescapable as systems grow beyond single
machines.

\section{Why Distribution is
Hard}\label{sec-vol2-introduction-why-distribution-hard}

Scale forces distribution: no single machine provides the thousands of
GPUs that frontier training requires, no single datacenter serves global
user bases with acceptable latency, and no centralized system can
collect the distributed data that edge devices generate. Coordinating
computation across physically separated machines connected by
finite-bandwidth, non-zero-latency networks creates constraints that no
amount of engineering can eliminate. These constraints manifest in three
forms: the impossibility theorems that bound what distributed systems
can guarantee, the coordination overhead that taxes every
synchronization point, and the amplified complexity when distribution
extends beyond the datacenter to edge devices.

\subsection{The CAP Theorem Reality}\label{the-cap-theorem-reality}

The CAP theorem\sidenote{\textbf{CAP Theorem}: Proven by Eric Brewer and
formalized by Gilbert and Lynch
(\citeproc{ref-gilbert2002brewer}{Gilbert and Lynch 2002}), the CAP
theorem states that a distributed data store cannot simultaneously
provide more than two of Consistency (every read receives the most
recent write), Availability (every request receives a non-error
response), and Partition tolerance (the system continues operating
despite network partitions). Since partitions are unavoidable in
distributed systems, the practical choice is between CP (consistent but
potentially unavailable during partitions) and AP (available but
potentially inconsistent). Distributed ML systems make different
choices. Synchronous training is CP (training halts during partitions to
maintain model consistency), while asynchronous training is AP (training
continues with potentially stale gradients). Understanding this
trade-off informs architecture decisions throughout distributed ML
systems. } establishes that distributed systems can provide at most two
of three properties: consistency (all nodes see the same data),
availability (every request receives a response), and partition
tolerance (the system continues operating despite network failures).
Since network partitions can always occur, practical systems must choose
between consistency and availability during partitions.

Distributed ML systems make different CAP trade-offs depending on their
requirements. Synchronous distributed training chooses consistency: all
workers see the same model state, but training halts if any worker
becomes unreachable. Asynchronous training chooses availability:
training continues even with stragglers or failures, but workers may
operate on slightly stale model versions
(\citeproc{ref-dean2012large}{Dean et al. 2012}). Federated learning
often chooses availability with eventual consistency: edge devices train
locally and periodically synchronize, accepting temporary inconsistency
for continuous operation.

\subsection{Coordination Overhead}\label{coordination-overhead}

Beyond the impossibility theorems, distributed systems pay a practical
tax on every operation that demands agreement across machines. Every
synchronization point introduces latency. Every consensus protocol
necessitates network round-trips. Every distributed lock limits
parallelism.

Consider the overhead of distributed training synchronization. Each
training iteration requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Forward pass computation (parallelizable)
\item
  Loss computation (local to each worker)
\item
  Backward pass computation (parallelizable)
\item
  Gradient aggregation (requires network communication)
\item
  Parameter update (can parallelize with next iteration)
\end{enumerate}

Steps 1-3 and 5 parallelize efficiently. Step 4, gradient aggregation,
requires global coordination. Even with optimized AllReduce algorithms
and high-bandwidth networks, this coordination can consume a substantial
fraction of total training time for large models
(\citeproc{ref-shoeybi2019megatron}{Shoeybi et al. 2019}). This overhead
is fundamental: no algorithm can aggregate globally distributed values
without communication proportional to the data volume.

\subsection{Edge Distribution
Complexity}\label{edge-distribution-complexity}

The coordination challenges discussed so far assume datacenter
distribution, where all machines run in managed facilities with reliable
power, cooling, and networking. Administrators can access any machine
for diagnosis and repair. These assumptions fail entirely when
distribution extends to edge devices.

Edge distribution amplifies every challenge. Billions of smartphones,
IoT devices, and embedded systems operate in uncontrolled environments
with unreliable connectivity, limited power, and heterogeneous
capabilities. Google's Gboard keyboard runs on over 1 billion Android
devices, each potentially participating in federated
learning\sidenote{\textbf{Federated Learning}: A distributed machine
learning approach where models train across many decentralized devices
holding local data samples, without exchanging the raw data. Instead of
collecting user data to a central server, federated learning sends the
model to devices, trains locally, and aggregates only the model updates
(gradients or weight differences). This preserves data privacy while
enabling learning from distributed sources. Challenges include handling
heterogeneous device capabilities, intermittent connectivity, and
ensuring convergence despite non-uniform data distributions. Federated
learning is covered in detail in \textbf{?@sec-edge-intelligence}. } to
improve predictions (\citeproc{ref-hard2018federated}{Hard et al.
2018}). This deployment context introduces unique constraints:

\begin{itemize}
\tightlist
\item
  \textbf{Intermittent connectivity}: Devices may be reachable only when
  on WiFi and charging
\item
  \textbf{Heterogeneous hardware}: Model must run efficiently across
  devices spanning a 100\$ imes\$ performance range
\item
  \textbf{Privacy requirements}: Raw data cannot leave devices,
  requiring on-device processing
\item
  \textbf{Update complexity}: Pushing model updates to billions of
  devices takes weeks
\item
  \textbf{Monitoring limitations}: Cannot install arbitrary diagnostics
  on user devices
\end{itemize}

These constraints require architectural approaches that differ from
datacenter ML in essential ways. Federated learning aggregates model
updates without collecting data. On-device inference optimizes for
varied hardware capabilities. Differential
privacy\sidenote{\textbf{Differential Privacy}: A mathematical framework
providing provable privacy guarantees by ensuring query results are
nearly identical whether or not any individual's data is included.
\textbf{?@sec-security-privacy} examines differential privacy
implementation and trade-offs. } (which adds calibrated noise to protect
individual data points while preserving aggregate statistical
properties) provides mathematical guarantees about information leakage
(\citeproc{ref-dwork2014algorithmic}{Dwork and Roth 2014}). These
techniques become essential for edge deployment.

Distribution multiplies engineering complexity. When ML systems operate
at the scale of billions of users across distributed infrastructure,
their impact on society demands consideration beyond engineering
excellence.

\section{Why Governance Matters at
Scale}\label{sec-vol2-introduction-why-governance-matters}

Scale and distribution do not just create engineering challenges; they
amplify impact. Systems serving billions of users at millisecond
latencies affect society in ways that demand governance beyond technical
excellence. A bug in a small system affects few users; a bug in a system
serving billions affects society. This amplification creates governance
requirements that small-scale systems can ignore.

\subsection{Security Threats
Intensify}\label{security-threats-intensify}

ML systems face unique security threats beyond traditional software
vulnerabilities\sidenote{\textbf{ML-Specific Security Threats}:
Traditional software security focuses on preventing unauthorized code
execution and data access. ML systems face additional threats that
exploit the learned behavior of models. Data poisoning attacks inject
malicious training examples that cause targeted misbehavior. Researchers
demonstrated that controlling 0.1\% of training data can implant
backdoors that cause misclassification on specific inputs. Model
inversion attacks reconstruct training data from model access. Facial
recognition models can leak enough information to reconstruct
recognizable images of training individuals. Adversarial reprogramming
hijacks models to perform unintended tasks through specially crafted
inputs. These threats require defenses beyond traditional security
including differential privacy, certified robustness, and continuous
monitoring of model behavior in production. }. Model extraction attacks
can steal proprietary models through query access. Researchers
demonstrated extracting functionally equivalent copies of production ML
models using only API access (\citeproc{ref-tramer2016stealing}{Tramèr
et al. 2016}). Membership inference attacks can determine whether
specific data was used in training, creating privacy violations from
seemingly innocuous model access
(\citeproc{ref-shokri2017membership}{Shokri et al. 2017}). Adversarial
examples can cause misclassification with perturbations imperceptible to
humans, demonstrated against production systems including Tesla
Autopilot and content moderation systems
(\citeproc{ref-goodfellow2014explaining}{Goodfellow, Shlens, and Szegedy
2014}).

At production scale, these threats become economically attractive to
attackers. A model serving millions of users represents substantial
intellectual property worth stealing. A model making consequential
decisions (loans, hiring, content moderation) offers high-value
manipulation targets. A model processing sensitive data (health records,
financial information) provides valuable inference targets.

Defending against these threats demands systematic approaches including
access controls that limit query rates and patterns, output perturbation
that provides differential privacy guarantees, adversarial training that
improves robustness to perturbations, and monitoring that detects
anomalous query patterns indicative of attacks. These defenses impose
overhead unnecessary for small-scale systems but essential for
production deployment.

\subsection{Regulatory Requirements
Emerge}\label{regulatory-requirements-emerge}

Systems operating at scale attract regulatory attention. The European
Union's General Data Protection Regulation (GDPR) imposes obligations
for systems processing EU residents' data, including the right to
explanation for automated decisions (\citeproc{ref-gdpr2016}{European
Parliament and Council of the European Union 2016}). The EU AI Act
establishes risk-based requirements for AI systems, with high-risk
applications (healthcare, employment, law enforcement) requiring
conformity assessments, human oversight, and accuracy documentation
(\citeproc{ref-euaiact2024}{Parliament and European Union 2024}).
Similar regulations exist or are developing in jurisdictions worldwide.

Meeting these regulatory requirements demands technical capabilities:

\begin{itemize}
\tightlist
\item
  \textbf{Audit trails}: Recording inputs, outputs, and model versions
  for every decision
\item
  \textbf{Explanation generation}: Producing human-interpretable
  justifications for model outputs
\item
  \textbf{Consent management}: Tracking and honoring user preferences
  for data usage
\item
  \textbf{Data deletion}: Removing specific users' data from training
  sets and retraining affected models
\item
  \textbf{Bias testing}: Evaluating model performance across protected
  demographic groups
\end{itemize}

These capabilities impose engineering costs absent for unregulated
systems but mandatory for production deployment in regulated contexts.
Regulatory compliance represents only one dimension of governance; the
broader question concerns how systems affecting billions of lives should
be built and operated.

\subsection{Societal Impact Demands
Responsibility}\label{societal-impact-demands-responsibility}

Beyond legal compliance, systems affecting billions of users carry
ethical obligations. Recommendation algorithms shape public discourse.
Researchers have documented how engagement-optimizing systems can
amplify misinformation and polarization
(\citeproc{ref-ribeiro2020auditing}{Ribeiro et al. 2020}). Hiring
algorithms affect employment opportunities. Amazon discontinued an AI
recruiting tool that exhibited bias against women
(\citeproc{ref-dastin2018amazon}{Dastin 2022}). Content moderation
systems determine what speech is visible. Errors can suppress legitimate
expression or fail to remove harmful content. Responsible engineering
practices must address these impacts:

\begin{itemize}
\tightlist
\item
  \textbf{Fairness evaluation}: Testing for disparate impact across
  demographic groups before deployment
\item
  \textbf{Impact assessment}: Analyzing potential harms before launching
  new capabilities
\item
  \textbf{Human oversight}: Maintaining human review for high-stakes
  decisions
\item
  \textbf{Incident response}: Processes for rapidly addressing
  identified harms
\item
  \textbf{Transparency}: Documentation of system capabilities,
  limitations, and decision factors
\end{itemize}

These obligations converge under the umbrella of \emph{Responsible AI},
which we define as follows.

\phantomsection\label{callout-definitionux2a-2.6}
\begin{fbx}{callout-definition}{Definition:}{Responsible AI}
\phantomsection\label{callout-definition*-2.6}
\textbf{\emph{Responsible AI}} encompasses the practices, frameworks,
and technical approaches that ensure ML systems operate in ways that are
fair, transparent, accountable, and beneficial to society. Responsible
AI addresses the ethical implications of automated decision making, the
potential for algorithmic bias, the environmental impact of large-scale
computation, and the governance structures needed to maintain human
oversight over consequential systems. Unlike traditional software
quality assurance focused on correctness and performance, responsible AI
evaluates systems against societal values and human rights
considerations.

\end{fbx}

Governance challenges cannot be addressed in isolation from the
infrastructure and distribution decisions that precede them. A security
vulnerability in gradient aggregation affects both training efficiency
and model confidentiality. A storage architecture decision shapes both
compliance capabilities and inference latency. The communication
bottlenecks that dominate large-scale training also determine what audit
logging is feasible without degrading throughput. Scale, distribution,
and governance form an integrated system where decisions in one domain
cascade through others.

\section{Foundational
Concepts}\label{sec-vol2-introduction-foundational-concepts}

To reason systematically about these interconnections, we need
organizing frameworks that operate at different levels of analysis. The
AI Triad reveals component interdependencies at the system level. The
Five-Pillar Framework organizes the engineering discipline itself,
distinguishing data engineering from model development from deployment
infrastructure. The Six Principles guide individual design decisions.
Together, these frameworks provide a multi-scale lens: system-level for
understanding trade-offs, discipline-level for organizing expertise, and
decision-level for guiding implementation choices.

Figure~\ref{fig-systems-sandwich} organizes the complexity of Volume II
into \textbf{The Systems Sandwich}, a three-layer framework where
engineering decisions at the bottom constrain possibilities at the top.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/25da2c7605a56b1c0b684a239a948bcea7bfc1ed.pdf}}

}

\caption{\label{fig-systems-sandwich}\textbf{The Systems Sandwich}. The
organizing framework for Volume II. We build from the \textbf{Physical
Layer} (constraints of atoms and energy) up to the \textbf{Operational
Layer} (distributed logic and algorithms) and finally to the
\textbf{Societal Layer} (human impact and governance). Engineering
decisions at the bottom constrain possibilities at the top.}

\end{figure}%

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The Physical Layer (The Warehouse-Scale Computer)}: This is
  the foundation. It deals with the hard constraints of physics---power
  density, cooling, interconnect bandwidth, and silicon limits. This is
  where we build the ``Machine Learning Fleet.''
\item
  \textbf{The Operational Layer (Distribution)}: This is the logic of
  scale. It covers how we distribute training (algorithms), serve
  predictions (inference), and manage state across thousands of
  unreliable nodes. This bridges the physical hardware with the software
  application.
\item
  \textbf{The Societal Layer (The Control Plane)}: This is the objective
  function of the system. It ensures the system operates safely,
  legally, and ethically. It includes security, privacy, and responsible
  AI as stability constraints.
\end{enumerate}

This layered progression structures the textbook's five Parts, each
corresponding to a different tier of the system stack, as
Figure~\ref{fig-vol2-roadmap} illustrates.

\textbf{The AI Triad} provides the organizing framework for
understanding ML systems. Every machine learning system comprises three
interdependent components: data that guides behavior, algorithms that
learn patterns, and computational infrastructure that enables both
training and inference. These components exist in dynamic tension.
Larger models require more data and compute. Larger datasets enable more
sophisticated models but demand storage and processing infrastructure.

At production scale, these interdependencies intensify. The
10\textsuperscript{25} FLOPS required for GPT-4 training demanded
infrastructure architecture that enabled efficient gradient
synchronization across 25,000 GPUs. The terabytes of training data
required distributed storage systems with access patterns optimized for
ML workloads. The 175 billion parameters required partitioning
strategies that balance computation against communication overhead.
Understanding these interdependencies helps you predict when a change to
one Triad component will cascade through the others.

Figure~\ref{fig-vol2-ai-triad} visualizes these dependencies between
data, algorithms, and infrastructure, revealing the optimization
landscape that ML systems engineers must address.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/13739273e30234735ee5e5a2c2f8512408fdd4c1.pdf}}

}

\caption{\label{fig-vol2-ai-triad}\textbf{The AI Triad at Scale}: The
three interdependent components of every ML system. At production scale,
each component's requirements intensify: data pipelines must handle
petabytes with consistent quality; algorithms demand
10\textsuperscript{25} FLOPS for frontier training; and infrastructure
must coordinate thousands of accelerators while maintaining fault
tolerance. Changes to any vertex cascade through the others, creating
the multi-dimensional optimization challenge that defines ML systems
engineering.}

\end{figure}%

\textbf{The Five-Pillar Framework} structures the ML systems engineering
discipline into interconnected domains: data engineering establishes
pipelines for collecting, processing, and serving training and inference
data; model development encompasses architecture design, training
procedures, and validation methodologies; optimization techniques
compress and accelerate models for deployment constraints; deployment
infrastructure spans cloud platforms, edge devices, and embedded
systems; and operations practices ensure systems remain reliable,
secure, and effective throughout their lifecycle. This textbook extends
each pillar to production scale, where the engineering challenges
multiply.

\textbf{The Six Systems Engineering Principles} provide guidance for
design decisions across all five pillars:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Measure Everything}: At scale, measurement itself becomes a
  systems challenge requiring distributed monitoring infrastructure
\item
  \emph{Design for 10\$ imes\$ Scale}: Production deployment reveals
  whether 10\$ imes\$ design was adequate or optimistic
\item
  \emph{Optimize the Bottleneck}: Scale shifts bottlenecks from compute
  to communication to coordination
\item
  \emph{Plan for Failure}: At scale, failure is not exceptional but
  routine
\item
  \emph{Design Cost-Consciously}: Scale makes efficiency improvements
  worth millions of dollars
\item
  \emph{Co-Design for Hardware}: Distributed hardware introduces network
  topology and storage hierarchy as co-design considerations
\end{enumerate}

These frameworks assume familiarity with single-machine ML systems: how
models are trained, optimized, and deployed on individual devices. This
textbook teaches you to scale, distribute, and govern them across
distributed infrastructure.

\section{Three Systems
Archetypes}\label{sec-vol2-introduction-archetypes}

To bridge abstract principles and concrete engineering, this textbook
employs a longitudinal narrative strategy. Rather than using isolated
examples in each chapter, we trace the engineering evolution of three
distinct system archetypes. These archetypes represent the fundamental
constraint regimes of modern ML systems: throughput-bound,
latency-bound, and power/privacy-bound. By revisiting these same three
systems across different chapters, you will see how the physics of
distribution manifests differently depending on the primary constraint.

\subsection{Archetype A: The Scaled Lighthouse (GPT-4 /
Llama-3)}\label{archetype-a-the-scaled-lighthouse-gpt-4-llama-3}

\begin{itemize}
\tightlist
\item
  \textbf{The System}: A generative foundation model (like GPT-4 or
  Gemini) trained on internet-scale text and served via API.
\item
  \textbf{The Constraint}: \textbf{Throughput}. Training requires
  ExaFLOPS of compute; serving requires massive memory bandwidth.
\item
  \textbf{Key Challenges}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Distributed Training}: Requires 3D parallelism (Data + Tensor
    + Pipeline) to fit in memory and scale to thousands of GPUs.
  \item
    \emph{Infrastructure}: Demands high-bandwidth interconnects
    (InfiniBand/NVLink) and burst-buffer storage for checkpointing.
  \item
    \emph{Ops}: Failures are catastrophic to training progress;
    checkpoint/restore is the critical loop.
  \item
    \emph{Sustainability}: Energy consumption per training run is the
    dominant metric.
  \end{itemize}
\end{itemize}

\subsection{Archetype B: The Scaled Lighthouse (DLRM at
Scale)}\label{archetype-b-the-scaled-lighthouse-dlrm-at-scale}

\begin{itemize}
\tightlist
\item
  \textbf{The System}: A personalized feed (like TikTok or Instagram)
  serving billions of users with sub-second freshness.
\item
  \textbf{The Constraint}: \textbf{Latency \& Volume}. Must process
  millions of queries per second (QPS) with \textless100ms tail latency.
\item
  \textbf{Key Challenges}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Inference}: Throughput is high, but latency budget is strict.
    Requires sophisticated batching and caching.
  \item
    \emph{Data}: Feature stores must handle massive read rates with
    point-in-time correctness.
  \item
    \emph{Ops}: A/B testing and continuous deployment are constant;
    ``model staleness'' allows for rapid performance degradation.
  \item
    \emph{Distribution}: Embedding tables (10TB+) exceed single-machine
    memory, requiring specialized parameter servers or embedding
    sharding.
  \end{itemize}
\end{itemize}

\subsection{Archetype C: The Scaled Lighthouse (Federated
MobileNet)}\label{archetype-c-the-scaled-lighthouse-federated-mobilenet}

\begin{itemize}
\tightlist
\item
  \textbf{The System}: A fleet of wearable devices detecting cardiac
  anomalies locally, using federated learning for improvement.
\item
  \textbf{The Constraint}: \textbf{Power \& Privacy}. Compute budget is
  milliwatts; raw data cannot leave the device.
\item
  \textbf{Key Challenges}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Edge Intelligence}: Models must be aggressively quantized
    (int8/int4) to fit on microcontrollers.
  \item
    \emph{Training}: Federated Learning coordinates updates across
    millions of unreliable devices without centralizing data.
  \item
    \emph{Communication}: Bandwidth is scarce and intermittent.
  \item
    \emph{Privacy}: Differential privacy is not optional; it is a core
    requirement for regulatory compliance.
  \end{itemize}
\end{itemize}

Throughout this volume, we use Archetype A to explain distributed
training protocols, Archetype B to derive inference load-balancing
theorems, and Archetype C to motivate edge security architectures. This
approach ensures that you understand not just how a technique works, but
why it is the right choice for a specific set of constraints.

\section{The Structure of This
Textbook}\label{sec-vol2-introduction-structure}

This textbook organizes around the three imperatives, progressing from
algorithmic foundations through physical infrastructure to governance
practices. We adopt this ``Logic First'' pedagogical approach because
the communication patterns and synchronization requirements of
distributed algorithms (Part I) fundamentally determine the design
constraints of the physical supercomputers (Part II) required to run
them. One cannot effectively architect a datacenter without
understanding the traffic patterns of the workloads it must support.

Examine \textbf{?@tbl-vol2-structure} for the complete five-part
organization, which maps the progression from foundational algorithms to
societal governance.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0d86b3dd38b5e99545ee00f3eac7422098c3db08.pdf}}

}

\caption{\label{fig-vol2-roadmap}\textbf{Volume 2 Roadmap}. The textbook
structure follows the system lifecycle. \textbf{Part I} establishes the
mathematical and physical foundations of scale. \textbf{Part II} applies
these to build the distributed training ``fleet''. \textbf{Part III}
covers the deployment of trained models to global users. \textbf{Part
IV} addresses the operational hardening required for production.
\textbf{Part V} elevates to the governance layer, ensuring systems are
responsible and beneficial.}

\end{figure}%

\textbf{Part} \textbar{} \textbf{Theme} \textbar{} \textbf{Key Chapters}
\textbar{}

\textbar:---------\textbar:----------\textbar:-----------------\textbar{}

\textbf{I: Foundations of Scale} \textbar{} \textbf{Scale}: Algorithmic
and software foundations \textbar{} Distributed Training, Communication,
Fault Tolerance \textbar{}

\textbf{II: Building the Machine Learning Fleet} \textbar{}
\textbf{Build}: The Warehouse-Scale Computer \textbar{} Compute
Infrastructure, Networking, Storage, Orchestration \textbar{}

\textbf{III: Deployment at Scale} \textbar{} \textbf{Deploy}: Serving
predictions to millions of users \textbar{} Inference at Scale, Edge
Intelligence, ML Operations at Scale \textbar{}

\textbf{IV: Production Concerns} \textbar{} \textbf{Operate}: Running
systems safely and sustainably \textbar{} Privacy \& Security, Robust
AI, Sustainable AI \textbar{}

\textbf{V: Responsible AI at Scale} \textbar{} \textbf{Govern}: The
Control Plane \textbar{} Responsible AI, AI for Good, Frontiers
\textbar{}

: \textbf{Volume II Organization}: The five parts progress from
algorithmic foundations through physical infrastructure and deployment
to production concerns and responsible governance. Each Part addresses a
different system layer, enabling mastery of one level before advancing
to the next. \{\#tbl-vol2-structure\}

\subsection{Part I: Foundations of
Scale}\label{part-i-foundations-of-scale}

The scale transformation we examined demands fundamentally different
software architectures. Training a model across thousands of GPUs
requires partitioning computations, synchronizing states, and recovering
from inevitable failures. Part I establishes these algorithmic
foundations, creating the logical system that must run upon physical
hardware.

\textbf{Distributed Training} develops techniques for training models
across devices and machines. Data parallelism\sidenote{\textbf{Data
Parallelism}: A distributed training strategy where each worker
processes different data batches while maintaining synchronized model
copies. \textbf{?@sec-distributed-training} examines data parallelism
implementation in detail. }, model parallelism\sidenote{\textbf{Model
Parallelism}: Distributes model parameters across multiple devices,
enabling training of models too large for single-device memory.
\textbf{?@sec-distributed-training} examines tensor parallelism and
other model partitioning strategies. }, and pipeline
parallelism\sidenote{\textbf{Pipeline Parallelism}: Partitions the model
by layers across devices, with computation flowing through stages.
\textbf{?@sec-distributed-training} examines pipeline scheduling and its
trade-offs. } each address different constraints. You will understand
when each applies, how they combine, and what synchronization and
consistency they require.

\textbf{Communication} analyzes the collective operations that
coordinate distributed training. AllReduce,
AllGather\sidenote{\textbf{AllGather}: A collective operation where each
worker contributes data and receives the concatenation of all
contributions, essential for model parallelism.
\textbf{?@sec-communication} examines collective operations in detail.
}, and other primitives dominate training communication. You will
understand algorithms, topologies, and optimization techniques that
minimize communication overhead and maximize bandwidth utilization.

\textbf{Fault Tolerance} ensures distributed systems continue operating
despite failures. At production scale, failures occur daily.
Checkpointing, redundancy, and recovery procedures enable training to
continue despite inevitable component failures.

\subsection{Part II: Building the Machine Learning
Fleet}\label{part-ii-building-the-machine-learning-fleet}

With the algorithmic foundations established, we turn to the physical
reality. The communication and reliability requirements defined in Part
I must be satisfied by concrete hardware. The terabytes of gradient
synchronization and the checkpointing demands require a massive,
interconnected supercomputer (a Warehouse-Scale Computer where the
network is the system bus and power density is the thermodynamic speed
limit). Part II examines the Fleet that executes modern ML workloads.

\textbf{Compute Infrastructure} examines the hardware units at the heart
of the fleet: GPU clusters with NVLink interconnects, high-density power
and cooling architectures, and the accelerator selection trade-offs that
determine cost and performance. We frame the datacenter not as a
building, but as the execution engine for our data-compiler.

\textbf{Cluster Networking} extends to the \textbf{Gradient Bus}---the
fabric that binds these nodes together. You will explore InfiniBand and
RoCE networks, packet spraying, and congestion control algorithms like
DCQCN that prevent head-of-line blocking.

\textbf{Storage Systems} addresses the AI Triad's data component at
scale. Training datasets for frontier models exceed any single storage
system's capacity; feature stores must serve the real-time lookups that
inference demands; artifact management tracks the thousands of model
versions that production systems generate.

\textbf{Orchestration} builds the brain of the fleet. You will examine
gang scheduling algorithms, bin-packing strategies, and the differences
between HPC schedulers (Slurm) and cloud-native orchestrators
(Kubernetes) that manage resources for thousands of concurrent jobs.

\subsection{Part III: Deployment at
Scale}\label{part-iii-deployment-at-scale}

Training produces models; deployment delivers value to users. The edge
distribution complexity we examined, where billions of heterogeneous
devices operate in uncontrolled environments, requires techniques that
extend far beyond datacenter serving.

\textbf{Inference at Scale} examines serving systems that deliver
predictions with low latency and high throughput. Request routing, load
balancing, autoscaling, and geographic distribution enable production
inference to meet demanding performance requirements.

\textbf{Edge Intelligence} extends ML to resource-constrained devices at
the network edge. Model compression, runtime optimization, and
edge-cloud coordination enable deployment where centralized inference is
infeasible.

\textbf{ML Operations at Scale} encompasses practices that maintain
large ML systems in production. Monitoring, debugging, deployment
pipelines, and incident response adapt for ML-specific requirements at
production scale.

\subsection{Part IV: Production
Concerns}\label{part-iv-production-concerns}

The security threats and regulatory requirements we examined create
operational challenges that require systematic approaches. At production
scale, the economic incentives for attacks, the regulatory scrutiny, and
the environmental impact all intensify.

\textbf{Privacy and Security} addresses threats specific to ML systems.
Model extraction, membership inference, adversarial examples, and data
poisoning require defenses including differential privacy, secure
computation, and adversarial training.

\textbf{Robust AI} ensures reliable operation under uncertainty.
Distribution shift, out-of-distribution inputs, and novel situations
require systems that recognize the limits of their competence and
respond appropriately.

\textbf{Sustainable AI} addresses environmental impact. Efficient
algorithms, appropriate model sizing, and renewable energy sourcing
minimize the environmental footprint of large-scale ML.

\subsection{Part V: Responsible AI at
Scale}\label{part-v-responsible-ai-at-scale}

When recommendation algorithms shape public discourse and hiring
algorithms affect employment opportunities, governance practices must
transcend technical excellence. Technical excellence is insufficient for
systems affecting human lives at scale.

\textbf{Responsible AI} addresses fairness, transparency, and
accountability. We frame this not as ``ethics vs.~engineering,'' but as
the Control Plane of the system. Fairness is a stability constraint;
transparency is observability. These are the objective functions that
keep the fleet from optimizing for the wrong target.

\textbf{AI for Good} demonstrates how ML systems address societal
challenges. Applications in healthcare, climate, education, and
accessibility illustrate how systems engineering principles enable
beneficial impact.

\textbf{AGI Systems} examines emerging directions including foundation
models, compound AI systems, and novel computing paradigms.
Understanding these trajectories prepares you for the Era of Compound
Capability, where the orchestration layer becomes the new compute
frontier.

This progression from algorithmic logic through physical hardware to
societal governance reflects how production ML systems are actually
built: software requirements determine what hardware is necessary, and
hardware capabilities enable the services that governance must oversee.
For detailed guidance on reading paths, prerequisite knowledge, and
navigation strategies, refer to the
\href{../../frontmatter/about/about.qmd}{About} section.

\section{The Journey Ahead}\label{sec-vol2-introduction-journey-ahead}

Having mapped the territory from algorithmic foundations through
governance practices, consider what mastering this material means for
your professional growth. The six systems engineering principles provide
a vision for building ML systems that matter. This textbook extends that
vision to the scale at which most consequential ML systems operate.

The transition from building systems that work to building systems that
scale, distribute, and govern responsibly represents significant
professional growth. The ML systems that will define this era require
precisely these capabilities: foundation models serving hundreds of
millions of users, edge deployments spanning billions of devices, and AI
systems making consequential decisions about human lives.

Throughout this volume, you will learn to partition computation across
thousands of accelerators, architect the infrastructure that supports
them, and design inference systems that serve billions of predictions
responsibly.

The engineering challenges are substantial, and so is the impact of
addressing them correctly.

The path forward begins with the physical foundations of scale.
\textbf{?@sec-infrastructure} examines datacenter infrastructure, from
power delivery and cooling to GPU cluster architectures and network
fabric design. \textbf{?@sec-storage} addresses the storage hierarchy
that must sustain petabyte-scale datasets and terabyte checkpoints. Once
we have built the metal and the memory, we will then explore the
distributed logic that coordinates these resources into a single global
engine.

Let us begin.

\section{Summary}\label{sec-vol2-intro-summary-66bb}

This volume opened with a fundamental challenge: the engineering
principles that enable success on single machines become the obstacles
that prevent success at scale. We have moved from the controlled
environment of the laboratory to the chaotic reality of the
\textbf{Machine Learning Fleet}, where communication costs dominate
computation, failures are a statistical certainty, and societal impact
demands rigorous governance.

This transformation requires reframing ML development as a
multi-dimensional optimization problem. We explored the \textbf{AI
Scaling Laws}, which provide the ``physics'' of deep learning by
predicting how performance improves with model size, data volume, and
compute budget. Yet, we also identified the breakdown points where these
laws encounter the hard walls of data saturation and hardware
bottlenecks. Efficiency engineering---spanning algorithmic, compute, and
data dimensions---provides the paths around these walls.

The progression of this textbook follows the three imperatives of scale,
distribution, and governance. We build from the logical foundations of
distributed training up to the physical infrastructure of the
datacenter, eventually reaching the control plane of responsible AI.
Mastering these interdependencies distinguishes those who build
prototypes from those who build the systems that define the modern era.

\phantomsection\label{callout-takeawaysux2a-2.7}
\begin{fbx}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-2.7}

\begin{itemize}
\tightlist
\item
  \textbf{Scale creates qualitative change}: Techniques that work for 8
  GPUs may fail at 8,000 GPUs due to network congestion, straggler
  effects, and coordination overhead. Scale is not ``more of the
  same''---it is fundamentally different engineering terrain.
\item
  \textbf{Communication dominates computation at scale}: The physics of
  data movement determines the ``speed'' of frontier models. As models
  scale, network bandwidth replaces FLOPs as the primary system
  bottleneck.
\item
  \textbf{Failure is a design constraint, not an edge case}: In a
  10,000-GPU cluster, hardware fails every few hours. Production systems
  must be designed to absorb failures continuously without human
  intervention.
\item
  \textbf{The Scaling Laws define the ROI}: Power-law relationships
  enable researchers to predict performance and resource requirements
  before starting expensive training runs, identifying the
  ``Compute-Optimal'' frontier.
\item
  \textbf{Efficiency is the new scaling law}: When brute-force scaling
  hits the power or data wall, success depends on multi-dimensional
  optimization across model architecture, hardware selection, and data
  quality.
\item
  \textbf{The CAP theorem limits distribution}: Distributed ML systems
  must choose between consistency (halts on partition) and availability
  (accepts staleness). Synchronous training is CP; asynchronous training
  is AP.
\end{itemize}

\end{fbx}

Throughout this volume, we trace these imperatives across three distinct
system archetypes, each representing a different constraint regime.

\phantomsection\label{callout-lighthouseux2a-2.8}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Volume II Archetypes across the Spectrum}
\phantomsection\label{callout-lighthouse*-2.8}
The challenges of scale manifest differently depending on whether a
system is throughput-bound, latency-bound, or resource-constrained:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Scaling Challenge}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Archetype A (GPT-4)} & Throughput & 3D Parallelism;
high-bandwidth interconnects (NVLink) \\
\textbf{Archetype B (RecSys)} & Latency \& Volume & Embedding sharding;
\(O(N^2)\) AllToAll contention \\
\textbf{Archetype C (Fed. MobileNet)} & Power \& Privacy & Federated
updates on unreliable, heterogeneous edge devices \\
\end{longtable}

The common thread: \textbf{scale is not a single problem but a series of
trade-offs} tailored to the system's objective function.

\end{fbx}

The transition from single-node to fleet-scale systems represents a
qualitative shift in engineering practice, not merely a quantitative
increase in resources. The principles established in Volume I---measure
everything, optimize the bottleneck, design for failure---remain
essential, but their application changes fundamentally when the
``system'' spans thousands of interconnected nodes. Network topology
becomes as important as memory hierarchy, distributed consensus replaces
local synchronization, and failure becomes a statistical certainty
rather than an exceptional event.

\phantomsection\label{callout-chapter-connectionux2a-2.9}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Requirements to Logic}
\phantomsection\label{callout-chapter-connection*-2.9}
We have established the \emph{requirements} of scale: we know why we
must distribute and what the costs will be. But how do we actually split
the math across thousands of independent processors?

In \textbf{Distributed Training}
(\textbf{?@sec-distributed-training-systems}), we explore the logic of
partitioning. We move from the ``Why'' of scale to the ``How,''
examining the specific algorithms that allow a single massive model to
live and learn across an entire cluster of GPUs.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-meta2022rsc}
AI, Meta. 2022. {``Building the Most Powerful AI Supercomputer: Meta's
AI Research SuperCluster.''} Meta AI Blog.
\url{https://ai.meta.com/blog/ai-rsc/}.

\bibitem[\citeproctext]{ref-amodei2018ai}
Amodei, Dario, and Danny Hernandez. 2018. {``AI and Compute.''}
\emph{OpenAI Blog}. \url{https://openai.com/research/ai-and-compute}.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} \emph{Advances in Neural Information
Processing Systems} 33 (May): 1877--1901.
\url{http://arxiv.org/abs/2005.14165v4}.

\bibitem[\citeproctext]{ref-chowdhery2022palm}
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, et al. 2022. {``PaLM: Scaling
Language Modeling with Pathways.''} \emph{arXiv Preprint
arXiv:2204.02311}, April. \url{http://arxiv.org/abs/2204.02311v5}.

\bibitem[\citeproctext]{ref-dastin2018amazon}
Dastin, Jeffrey. 2022. {``Amazon Scraps Secret AI Recruiting Tool That
Showed Bias Against Women.''} In \emph{Ethics of Data and Analytics},
296--99. Auerbach Publications.
\url{https://doi.org/10.1201/9781003278290-44}.

\bibitem[\citeproctext]{ref-dean2012large}
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen 0010, Matthieu Devin,
Quoc V. Le, Mark Z. Mao, et al. 2012. {``Large Scale Distributed Deep
Networks.''} In \emph{Advances in Neural Information Processing
Systems}, 25:1232--40.
\url{https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html}.

\bibitem[\citeproctext]{ref-devlin2018bert}
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
{``BERT: Pre-Training of Deep Bidirectional Transformers for Language
Understanding.''} \emph{arXiv Preprint arXiv:1810.04805}, October.
\url{http://arxiv.org/abs/1810.04805v2}.

\bibitem[\citeproctext]{ref-dwork2014algorithmic}
Dwork, Cynthia, and Aaron Roth. 2014. {``The Algorithmic Foundations of
Differential Privacy.''} \emph{Foundations and Trends\textregistered{}
in Theoretical Computer Science}, Foundations and trends in theoretical
computer science, 9 (3-4): 211--487.
\url{https://doi.org/10.1561/0400000042}.

\bibitem[\citeproctext]{ref-gdpr2016}
European Parliament, and Council of the European Union. 2016.
{``Regulation (EU) 2016/679 of the European Parliament and of the
Council on the Protection of Natural Persons with Regard to the
Processing of Personal Data and on the Free Movement of Such Data
(General Data Protection Regulation).''} \emph{Official Journal of the
European Union}. \url{https://eur-lex.europa.eu/eli/reg/2016/679/oj}.

\bibitem[\citeproctext]{ref-gilbert2002brewer}
Gilbert, Seth, and Nancy Lynch. 2002. {``Brewer's Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web
Services.''} \emph{ACM SIGACT News} 33 (2): 51--59.
\url{https://doi.org/10.1145/564585.564601}.

\bibitem[\citeproctext]{ref-goodfellow2014explaining}
Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. 2014.
{``Explaining and Harnessing Adversarial Examples.''} \emph{arXiv
Preprint arXiv:1412.6572}, December.
\url{http://arxiv.org/abs/1412.6572v3}.

\bibitem[\citeproctext]{ref-dubey2024llama3}
Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. 2024. {``The
Llama 3 Herd of Models.''} \emph{arXiv Preprint arXiv:2407.21783}, July.
\url{http://arxiv.org/abs/2407.21783v3}.

\bibitem[\citeproctext]{ref-hard2018federated}
Hard, Andrew, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise
Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel
Ramage. 2018. {``Federated Learning for Mobile Keyboard Prediction.''}
\emph{arXiv Preprint arXiv:1811.03604}, November.
\url{http://arxiv.org/abs/1811.03604v2}.

\bibitem[\citeproctext]{ref-hoffmann2022training}
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022.
{``Training Compute-Optimal Large Language Models''} 35 (March):
30016--30. \url{http://arxiv.org/abs/2203.15556v1}.

\bibitem[\citeproctext]{ref-kaplan2020scaling}
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin
Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario
Amodei. 2020. {``Scaling Laws for Neural Language Models.''} \emph{ArXiv
Preprint} abs/2001.08361 (January).
\url{http://arxiv.org/abs/2001.08361v1}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-lin2018deep}
Lin, Yujun, Song Han, Huizi Mao, Yu Wang, and William J Dally. 2018.
{``Deep Gradient Compression: Reducing the Communication Bandwidth for
Distributed Training.''} In \emph{International Conference on Learning
Representations}.

\bibitem[\citeproctext]{ref-openai2023gpt4}
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
Akkaya, Florencia Leoni Aleman, et al. 2023. {``GPT-4 Technical
Report,''} March. \url{http://arxiv.org/abs/2303.08774v6}.

\bibitem[\citeproctext]{ref-euaiact2024}
Parliament, European, and Council of the European Union. 2024.
{``Regulation (EU) 2024/1689 of the European Parliament and of the
Council Laying down Harmonised Rules on Artificial Intelligence (AI
Act).''} \emph{Official Journal of the European Union}.

\bibitem[\citeproctext]{ref-pinheiro2007failure}
Pinheiro, Eduardo, Wolf-Dietrich Weber, and Luiz André Barroso. 2007.
{``Failure Trends in a Large Disk Drive Population.''} In \emph{5th
USENIX Conference on File and Storage Technologies (FAST)}, 17--28.
\url{http://www.usenix.org/events/fast07/tech/pinheiro.html}.

\bibitem[\citeproctext]{ref-ribeiro2020auditing}
Ribeiro, Manoel Horta, Raphael Ottoni, Robert West, Virgílio A. F.
Almeida, and Jr. Meira Wagner. 2020. {``Auditing Radicalization Pathways
on YouTube.''} In \emph{Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency}, 131--41. ACM.
\url{https://doi.org/10.1145/3351095.3372879}.

\bibitem[\citeproctext]{ref-schroeder2009dram}
Schroeder, Bianca, Eduardo Pinheiro, and Wolf-Dietrich Weber. 2009.
{``DRAM Errors in the Wild: A Large-Scale Field Study.''} \emph{ACM
SIGMETRICS Performance Evaluation Review} 37 (1): 193--204.
\url{https://doi.org/10.1145/2492101.1555372}.

\bibitem[\citeproctext]{ref-sevilla2022compute}
Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius
Hobbhahn, and Pablo Villalobos. 2022. {``Compute Trends Across Three
Eras of Machine Learning.''} In \emph{2022 International Joint
Conference on Neural Networks (IJCNN)}, 1--8. IEEE.
\url{https://doi.org/10.1109/ijcnn55064.2022.9891914}.

\bibitem[\citeproctext]{ref-shoeybi2019megatron}
Shoeybi, Mohammad, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
Casper, and Bryan Catanzaro. 2019. {``Megatron-LM: Training
Multi-Billion Parameter Language Models Using Model Parallelism.''}
\emph{arXiv Preprint arXiv:1909.08053}, September.
\url{http://arxiv.org/abs/1909.08053v4}.

\bibitem[\citeproctext]{ref-shokri2017membership}
Shokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
2017. {``Membership Inference Attacks Against Machine Learning
Models.''} In \emph{2017 IEEE Symposium on Security and Privacy (SP)},
3--18. IEEE; IEEE. \url{https://doi.org/10.1109/sp.2017.41}.

\bibitem[\citeproctext]{ref-tramer2016stealing}
Tramèr, Florian, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas
Ristenpart. 2016. {``Stealing Machine Learning Models via Prediction
APIs.''} In \emph{25th USENIX Security Symposium (USENIX Security 16)},
601--18.

\bibitem[\citeproctext]{ref-vaswani2017attention}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N.Gomez, Lukasz Kaiser, and Illia Polosukhin. 2025.
{``Attention Is All You Need.''} Shenzhen Medical Academy of Research;
Translation. \url{https://doi.org/10.65215/pc26a033}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
