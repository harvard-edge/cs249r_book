% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{multirow}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Introduction}\label{sec-introduction}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration
depicting a roadmap of a book's chapters on machine learning systems,
set on a crisp, clean white background. The image features a winding
road traveling through various symbolic landmarks. Each landmark
represents a chapter topic: Introduction, ML Systems, Deep Learning, AI
Workflow, Data Engineering, AI Frameworks, AI Training, Efficient AI,
Model Optimizations, AI Acceleration, Benchmarking AI, On-Device
Learning, Embedded AIOps, Security \& Privacy, Responsible AI,
Sustainable AI, AI for Good, Robust AI, Generative AI. The style is
clean, modern, and flat, suitable for a technical book, with each
landmark clearly labeled with its chapter title.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/introduction/images/png/cover_introduction.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does building machine learning systems require engineering
principles fundamentally different from those governing traditional
software?}

When a traditional program misbehaves, engineers trace the bug to
specific lines of code. When a machine learning system misbehaves, there
is often no bug to find: the code executes correctly, but the learned
behavior is wrong. This distinction captures why ML systems demand their
own engineering discipline. In traditional software, programmers write
explicit logic that computers execute faithfully. In machine learning,
programmers write optimization procedures that extract operational logic
from data, producing systems whose behavior emerges from statistical
patterns rather than hand-crafted rules. This shift from
instruction-centric to data-centric computing transforms every aspect of
system design: debugging moves from code inspection to data analysis,
testing shifts from coverage metrics to distribution validation, and
deployment requires continuous monitoring of learned behaviors that can
silently degrade as the world changes. The engineering principles that
made traditional software reliable---version control, unit testing,
reproducible builds---remain necessary but are no longer sufficient when
the system's behavior is defined by data it has seen rather than code
someone wrote.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-tip-color!10!white, leftrule=.75mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, colframe=quarto-callout-tip-color-frame, toptitle=1mm, opacityback=0, left=2mm, opacitybacktitle=0.6, bottomtitle=1mm, titlerule=0mm, arc=.35mm, breakable, toprule=.15mm, bottomrule=.15mm, rightrule=.15mm, colback=white]

\begin{itemize}
\item
  Explain the AI Triad framework (Data, Algorithm, Machine) and apply
  the DAM Taxonomy to diagnose bottlenecks in ML systems
\item
  Distinguish ML systems from traditional software based on silent
  performance degradation patterns, and apply the Degradation Equation
  to reason about drift
\item
  Trace AI's evolution from symbolic reasoning through statistical
  learning to modern deep learning
\item
  Explain why computational scale consistently outperforms encoded
  expertise in AI advancement (the Bitter Lesson)
\item
  Describe the three dimensions of efficiency (algorithmic, compute,
  data) that enable sustainable AI systems
\item
  Characterize the deployment spectrum from cloud to TinyML and the
  constraints each environment imposes
\item
  Describe the ML development lifecycle and its continuous,
  data-dependent iteration cycles
\item
  Identify core engineering challenges spanning data quality, model
  complexity, infrastructure, and ethics
\item
  Apply the five-pillar framework to organize ML systems engineering
  practices
\end{itemize}

\end{tcolorbox}

\section{The AI Moment}\label{sec-introduction-ai-moment-d1fc}

Artificial intelligence has moved from research laboratories to the
fabric of daily life. When you ask your phone a question, an AI system
converts your speech to text, interprets your intent, and generates a
response. When you scroll through social media, AI systems decide which
posts appear and in what order. When you apply for a loan, AI systems
assess your creditworthiness. When you drive a modern car, AI systems
monitor lane position, detect pedestrians, and adjust cruise control.
These are not future possibilities---they are present realities
affecting billions of people daily.

The scale of this transformation is staggering. ChatGPT reached 100
million users in two months, making it the fastest-growing consumer
application at the time of its launch. YouTube's recommendation
algorithm influences over a billion hours of video watched daily. Google
processes 8.5 billion searches per day, each one triggering multiple AI
systems for ranking, spell-checking, and knowledge extraction.
Autonomous vehicles collectively drive millions of miles weekly, each
mile generating terabytes of sensor data processed by onboard AI
systems. The global AI market, valued at approximately \$150 billion in
2023, is projected to exceed \$1 trillion by 2030.

Yet beneath these user-facing applications lies an engineering challenge
that most users never see. When ChatGPT generates a response, thousands
of GPUs\sidenote{\textbf{GPU (Graphics Processing Unit)}: Originally
designed for rendering video game graphics, GPUs excel at performing
thousands of simple calculations simultaneously. A modern data center
GPU like NVIDIA's H100 can perform approximately 2,000 trillion
floating-point operations per second for FP16 Tensor Core operations (or
nearly 4,000 TFLOPS for FP8), compared to roughly 1 trillion for a
high-end CPU. This massive parallelism aligns naturally with neural
network computations, which consist of matrix multiplications across
millions of parameters. The GPU's rise as the dominant AI accelerator
represents a systems engineering insight: matching algorithm structure
to hardware capabilities. } coordinate across data centers.

When your spam filter catches a phishing attempt, it draws on models
trained on billions of emails. When a Tesla avoids a collision, dozens
of neural networks\sidenote{\textbf{Neural Network}: Named for its
inspiration from biological neurons, from Greek \emph{neuron} (nerve,
sinew). Warren McCulloch and Walter Pitts introduced the computational
model in 1943 (\citeproc{ref-mcculloch1943logical}{McCulloch and Pitts
1943}), abstracting the brain's interconnected nerve cells into
mathematical functions. The biological metaphor persists in terminology:
neurons, synapses (connections), and activation (firing). Despite the
name, modern neural networks bear little resemblance to actual brain
architecture; they are better understood as differentiable function
approximators organized in layers. } process camera, radar, and
ultrasonic data in milliseconds. The AI systems that seem effortless to
use require extraordinary engineering to build, deploy, and maintain.

This textbook addresses that engineering challenge. Before we can
understand \emph{how} to build these systems, we must understand
\emph{what} they are. That understanding begins with a fundamental
distinction: the difference between artificial intelligence as a vision
and machine learning as a methodology.

\section{From Artificial Intelligence Vision to Machine Learning
Practice}\label{sec-introduction-artificial-intelligence-vision-machine-learning-practice-feba}

AI represents the broad goal of creating systems that perform tasks
requiring human-like intelligence: recognizing images, understanding
language, making decisions, and solving problems. AI is the vision of
intelligent machines that learn, reason, and adapt.

Machine Learning (ML) provides the methodological approach for creating
systems that demonstrate intelligent behavior. Rather than implementing
intelligence through predetermined rules, machine learning provides
computational techniques that automatically discover patterns in data
through mathematical processes, transforming AI's theoretical insights
into functioning systems.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Key Definitions}
\phantomsection\label{callout-definition*-1.1}
\textbf{\emph{Artificial Intelligence (AI)}} is the field of computer
science focused on creating systems that perform tasks requiring
human-like \emph{intelligence}, including \emph{learning},
\emph{reasoning}, and \emph{adaptation}.

\textbf{\emph{Machine Learning (ML)}} is the approach to AI that enables
systems to automatically learn \emph{patterns} and make \emph{decisions}
from \emph{data} rather than following explicit programmed rules.

\end{fbx}

Consider the evolution of chess-playing systems as an example of this
shift. The AI goal remains constant: create a system that can play chess
like a human. However, the approaches differ:

\begin{itemize}
\item
  \textbf{Symbolic AI Approach (Pre-ML)}: Program the computer with all
  chess rules and hand-craft strategies like ``control the center'' and
  ``protect the king.'' This requires expert programmers to explicitly
  encode thousands of chess principles, creating brittle systems that
  struggle with novel positions.
\item
  \textbf{Machine Learning Approach}: Have the computer analyze millions
  of chess games to learn winning strategies automatically from data.
  Rather than programming specific moves, the system discovers patterns
  that lead to victory through statistical analysis of game outcomes.
\end{itemize}

\phantomsection\label{callout-exampleux2a-1.2}
\begin{fbx}{callout-example}{Example: }{Rule-Based vs. Learned: Chatbots Then and Now}
\phantomsection\label{callout-example*-1.2}
\textbf{ELIZA (1966)} used explicit pattern-matching scripts
(\citeproc{ref-weizenbaum1966eliza}{Weizenbaum 1966}):

\begin{verbatim}
keyword: MOTHER
  decomp: * my mother *
  reasmb: Tell me more about your family.

keyword: FEEL
  decomp: * i feel *
  reasmb: Why do you feel (3)?
\end{verbatim}

\textbf{Modern LLMs} have no such rules---behavior emerges from
training:

\begin{verbatim}
User: I feel anxious about my presentation.
Assistant: It's natural to feel that way. Let's break down...
\end{verbatim}

Same goal, fundamentally different engineering.

\end{fbx}

This transformation illustrates why ML has become the dominant approach.
In rule-based systems, humans translate domain expertise directly into
code. In ML systems, humans curate training data, design learning
architectures, and define success metrics, allowing the system to
extract its own operational logic from examples. Data-driven systems
adapt to situations that programmers never anticipated; rule-based
systems remain constrained by their original programming.

Machine learning systems acquire capabilities much as humans do: through
exposure to examples. Object recognition develops from viewing thousands
of images; natural language processing emerges from analyzing vast
quantities of text. These learning approaches operationalize theories of
intelligence developed in AI research, building on mathematical
foundations established throughout this text.

The distinction between AI as research vision and ML as engineering
methodology has profound consequences for system design. Rule-based AI
systems scaled with programmer effort, requiring manual encoding of each
new capability. Data-driven ML systems scale through computational and
data infrastructure, achieving improved performance by expanding
training datasets and computational resources rather than through
additional programming effort. This
transformation\sidenote{\textbf{Paradigm Shift}: From Greek
\emph{paradeigma} (pattern, model, example), the term was popularized by
philosopher Thomas Kuhn in \emph{The Structure of Scientific
Revolutions} (\citeproc{ref-kuhn1962structure}{Kuhn 1962}) to describe
fundamental changes in scientific worldviews. In AI, this refers to the
transition from expert systems (encoding knowledge) to machine learning
(learning from data), and subsequently to deep learning (representation
learning). } elevated systems engineering to a central role: advancement
now depends on building infrastructure capable of collecting massive
datasets, training models with billions of parameters, and serving
predictions at scale.

The shift from rule-based to data-driven systems constitutes a
fundamental paradigm shift in how we think about computing itself.

\phantomsection\label{quiz-question-sec-introduction-artificial-intelligence-vision-machine-learning-practice-feba}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.1}{}
\phantomsection\label{quiz-question-sec-introduction-artificial-intelligence-vision-machine-learning-practice-feba}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the relationship between
  Artificial Intelligence (AI) and Machine Learning (ML)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    AI is a subset of ML focused on data-driven techniques.
  \item
    ML is a practical implementation of AI using rule-based systems.
  \item
    AI and ML are completely independent fields.
  \item
    ML is a subset of AI focused on data-driven techniques.
  \end{enumerate}
\item
  Explain why machine learning has become the dominant approach in
  achieving AI goals.
\item
  Order the following steps in the evolution from symbolic AI to machine
  learning: (1) Encoding human knowledge as rules, (2) Discovering
  patterns from data, (3) Scaling with compute and data infrastructure.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-artificial-intelligence-vision-machine-learning-practice-feba]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{The Data-Centric Paradigm
Shift}\label{sec-introduction-datacentric-paradigm-shift-254a}

The transition from rule-based AI to machine learning represents a
fundamental reconception of computing. Andrej Karpathy formalized this
distinction as the shift from \textbf{Software 1.0} to \textbf{Software
2.0} (\citeproc{ref-karpathy2017software}{Karpathy 2017}), a framing
that captures why ML systems require entirely new engineering
approaches. Table~\ref{tbl-software-1-vs-2} summarizes this paradigm
shift.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2088}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3626}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4176}}@{}}
\caption{\textbf{The Paradigm Shift from Software 1.0 to Software 2.0}:
In Software 2.0, the ``programmer'' does not write the logic; they
curate the dataset that the optimization process uses to write the
logic. Debugging therefore moves upstream from code to
data.}\label{tbl-software-1-vs-2}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Software 1.0 (Traditional)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Software 2.0 (Machine Learning)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Software 1.0 (Traditional)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Software 2.0 (Machine Learning)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Source Code} & C++, Python, Java & Training Data + Labels \\
\textbf{Compiler} & GCC, LLVM & Training Loop (SGD) \\
\textbf{Logic} & Explicit (Hand-coded) & Implicit (Learned) \\
\textbf{Failure Mode} & Loud (Crash, Exception) & Silent (Metric
Degradation) \\
\textbf{Debugging} & Trace execution path & Inspect data distribution \\
\end{longtable}

This table reveals the fundamental shift: \textbf{Data is Source Code}.
In traditional software, a programmer writes explicit logic
(\texttt{if\ x\ \textgreater{}\ 0\ then\ y}). In machine learning, the
programmer writes the \emph{optimization meta-logic} (the training
algorithm), but the actual operational logic is ``compiled'' from the
training dataset through stochastic gradient
descent\sidenote{\textbf{Stochastic Gradient Descent (SGD)}: The name
reveals the method: ``stochastic'' derives from Greek
\emph{stochastikos} (able to guess or aim at a target), reflecting the
algorithm's use of random sampling; ``gradient'' comes from Latin
\emph{gradiens} (stepping), describing the incremental steps down the
error surface. Rather than computing gradients over the entire dataset
(computationally prohibitive for large datasets), SGD estimates
gradients from small random batches, typically 32-256 samples. This
stochasticity introduces noise that paradoxically helps optimization
escape poor local minima. Modern variants like Adam combine SGD with
momentum and adaptive learning rates. } and related optimization
methods. The dataset serves as source code, the training pipeline as
compiler, and the model weights as binary executable.

From a systems perspective, this represents a transition from
\emph{instruction-centric} to \emph{data-centric} computing:

\begin{itemize}
\item
  \textbf{Instruction-centric computing} (traditional): Systems
  optimized for the efficient execution of hand-crafted logic. The
  programmer's job is to write correct instructions.
\item
  \textbf{Data-centric computing} (ML): Systems optimized for the
  efficient ingestion of data and the iterative refinement of model
  parameters. The programmer's job is to curate correct data.
\end{itemize}

Debugging an ML system means debugging the \emph{data}, not the Python
scripts. Version control must track \emph{datasets}, not just git
commits. Testing must validate data distributions, not just code paths.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-note-color!10!white, leftrule=.75mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{The Verification Gap}, colframe=quarto-callout-note-color-frame, toptitle=1mm, opacityback=0, left=2mm, opacitybacktitle=0.6, bottomtitle=1mm, titlerule=0mm, arc=.35mm, breakable, toprule=.15mm, bottomrule=.15mm, rightrule=.15mm, colback=white]

\textbf{Why we can't just `test' ML systems}: In Software 1.0, logic is
discrete. We can write unit tests that cover edge cases because the
input space is often enumerable or partitionable.

In Software 2.0, the input space is \textbf{continuous and
high-dimensional} (e.g., all possible images). It is mathematically
impossible to verify correctness for every input.

\[ \text{Verification Gap} = \text{Total Input Space} - \text{Test Set Coverage} \approx \infty \]

This gap means we must rely on \textbf{statistical monitoring} in
production (MLOps) rather than pre-deployment verification alone. We
trade \emph{guaranteed correctness} for \emph{statistical reliability}.

\end{tcolorbox}

This data-centric paradigm requires rethinking the entire computing
stack. The shift from instruction-centric to data-centric computing did
not happen overnight; it emerged through seven decades of paradigm
transitions, each overcoming the bottlenecks of its predecessor.
Understanding this history reveals why systems engineering has become
central to AI progress.

\section{Historical Evolution of AI
Paradigms}\label{sec-introduction-historical-evolution-ai-paradigms-3be8}

Tracing AI's evolution reveals a progression of bottlenecks, each
overcome by systems innovations that expanded what was computationally
possible. Early systems, such as the
Perceptron\sidenote{\textbf{Perceptron}: Frank Rosenblatt coined this
term in 1957 by combining ``perceive'' with the suffix ``-tron'' (from
Greek, meaning instrument), literally an ``instrument for perceiving.''
One of the first computational learning algorithms
(\citeproc{ref-rosenblatt1957perceptron}{Rosenblatt 1957}), it was
simple enough to implement in hardware with minimal memory. The
Perceptron's limitation to linearly separable problems was not just
algorithmic: multi-layer networks (which could solve non-linear
problems) were proposed in the 1960s but remained computationally
intractable until the 1980s. } (1957) and ELIZA\sidenote{\textbf{ELIZA}:
Created by MIT's Joseph Weizenbaum in 1966
(\citeproc{ref-weizenbaum1966eliza}{Weizenbaum 1966}), ELIZA simulated
conversation via pattern matching. From a systems perspective, it was
computationally cheap (running on 256KB mainframes) but brittle---it had
no learning capability and no memory of past interactions. } (1966),
were limited by manual logic and the constraints of
mainframes\sidenote{\textbf{Mainframes}: Room-sized computers that
dominated the 1960s-70s. IBM's System/360 (1964) weighed up to 20,000
pounds with \textasciitilde1MB of memory, yet represented the cutting
edge that enabled early AI research. }, resulting in brittleness.
Subsequent eras were limited by manual knowledge entry, creating
scalability issues. Modern systems face the new bottleneck of
computational throughput.

Figure~\ref{fig-ai-timeline} traces this evolution. Each era represents
an engineering paradigm shift attempting to overcome the limitations of
the previous approach.

\begin{figure}[t!]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/7a77c0c3cc19fa71346eb24454e7ab72a65ea895.pdf}}

}

\caption{\label{fig-ai-timeline}\textbf{AI Development Timeline}: Early
AI research focused on symbolic reasoning and rule-based systems, while
modern AI leverages data-driven approaches like neural networks to
achieve increasingly complex tasks. This progression exposes a shift
from hand-coded intelligence to learned intelligence, marked by
milestones such as the perceptron, deep blue, and large language models
like GPT-3.}

\end{figure}%

\subsection{Symbolic AI Era: The Logic
Bottleneck}\label{sec-introduction-symbolic-ai-era-logic-bottleneck-74c2}

The first era of AI engineering (1950s--1970s) attempted to reduce
intelligence to symbolic manipulation. Researchers at the 1956 Dartmouth
Conference\sidenote{\textbf{Dartmouth Conference (1956)}: The summer
workshop where John McCarthy coined ``artificial intelligence,''
deliberately choosing \emph{artificial} (from Latin \emph{artificium},
meaning craft or skill made by art) to distinguish machine cognition
from natural intelligence. McCarthy later admitted the term was chosen
partly for marketing appeal. Participants severely underestimated the
systems challenge, assuming AI could run on 1950s hardware (64 KB
memory), focusing on algorithmic logic while ignoring the physical
constraints of storage and compute. } hypothesized that if they could
formalize the rules of logic, machines could ``think.'' Daniel Bobrow's
STUDENT system (\citeproc{ref-bobrow1964student}{Bobrow 1964}) (1964)
exemplifies this approach.

\phantomsection\label{callout-exampleux2a-1.3}
\begin{fbx}{callout-example}{Example: }{STUDENT (1964)}
\phantomsection\label{callout-example*-1.3}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Problem: "If the number of customers Tom gets is twice the}
\NormalTok{square of 20\% of the number of advertisements he runs, and}
\NormalTok{the number of advertisements is 45, what is the number of}
\NormalTok{customers Tom gets?"}

\NormalTok{STUDENT would:}

\NormalTok{1. Parse the English text}
\NormalTok{2. Convert it to algebraic equations}
\NormalTok{3. Solve the equation: n = 2(0.2 x 45)\^{}2}
\NormalTok{4. Provide the answer: 162 customers}
\end{Highlighting}
\end{Shaded}

\end{fbx}

While impressive in demonstrations, these systems were operationally
\textbf{brittle}. They relied on manually coded rules for every possible
state. A minor variation in input phrasing (e.g., ``Tom's client
count'') would cause system failure. The engineering lesson was that
explicit logic cannot scale to handle real-world ambiguity; the
complexity of the ``rule base'' grows exponentially until it becomes
unmaintainable.

\subsection{Expert Systems Era: The Knowledge
Bottleneck}\label{sec-introduction-expert-systems-era-knowledge-bottleneck-1d9d}

In the 1980s, engineers pivoted from general logic to capturing deep
domain expertise. MYCIN (\citeproc{ref-shortliffe1976mycin}{Shortliffe
et al. 1975}) (1976), designed to diagnose blood infections, encoded
approximately 600 rules derived from interviews with medical experts.

\phantomsection\label{callout-exampleux2a-1.4}
\begin{fbx}{callout-example}{Example: }{MYCIN (1976)}
\phantomsection\label{callout-example*-1.4}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rule Example from MYCIN:}
\NormalTok{IF}
\NormalTok{  The infection is primary{-}bacteremia}
\NormalTok{  The site of the culture is one of the sterile sites}
\NormalTok{  The suspected portal of entry is the gastrointestinal tract}
\NormalTok{THEN}
\NormalTok{  Found suggestive evidence (0.7) that infection is bacteroid}
\end{Highlighting}
\end{Shaded}

\end{fbx}

MYCIN outperformed junior doctors in specific tests but revealed the
\textbf{Knowledge Acquisition Bottleneck}. Extracting implicit intuition
from human experts and formalizing it into IF-THEN rules proved slow,
error-prone, and contradictory. Maintaining a system with thousands of
conflicting rules became an intractable systems engineering problem.
This failure demonstrated that scalable AI required systems to learn
rules from data, rather than having them manually injected by engineers.

\subsection{Statistical Learning Era: The Feature Engineering
Bottleneck}\label{sec-introduction-statistical-learning-era-feature-engineering-bottleneck-eb1f}

The 1990s marked the shift to probabilistic systems. Instead of
hard-coded logic, systems estimated probabilities from data
(\(P(Y|X)\)). This transition was driven by the availability of digital
data and the ``unreasonable
effectiveness''\sidenote{\textbf{Unreasonable Effectiveness of Data}: A
concept popularized by Google researchers Halevy, Norvig, and Pereira
(\citeproc{ref-halevy2009unreasonable}{Halevy, Norvig, and Pereira
2009}), noting that simple algorithms with massive data often outperform
complex algorithms with limited data. } of large datasets.

Spam filtering illustrates this shift. Rather than maintaining lists of
forbidden words, statistical filters learned the probability that a word
implies spam based on millions of examples.

\phantomsection\label{callout-exampleux2a-1.5}
\begin{fbx}{callout-example}{Example: }{Early Spam Detection Systems}
\phantomsection\label{callout-example*-1.5}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rule{-}based (1980s):}
\NormalTok{IF contains("viagra") OR contains("winner") THEN spam}

\NormalTok{Statistical (1990s):}
\NormalTok{P(spam|word) = (frequency in spam emails) / (total frequency)}

\NormalTok{Combined using Naive Bayes:}
\NormalTok{P(spam|email) \textasciitilde{} P(spam) x product of P(word|spam)}
\end{Highlighting}
\end{Shaded}

\end{fbx}

However, this era faced the \textbf{Feature Engineering Bottleneck}.
Algorithms like Support Vector Machines (SVMs) could learn robustly, but
only \emph{after} humans converted raw data into structured
``features.'' The system's performance was bounded by human ingenuity in
preprocessing, not by the data itself.

\phantomsection\label{callout-exampleux2a-1.6}
\begin{fbx}{callout-example}{Example: }{Traditional Computer Vision Pipeline}
\phantomsection\label{callout-example*-1.6}

\begin{verbatim}
1. Manual Feature Extraction
   - SIFT (Scale-Invariant Feature Transform)
   - HOG (Histogram of Oriented Gradients)
   - Gabor filters
2. Feature Selection/Engineering
3. "Shallow" Learning Model (e.g., SVM)
4. Post-processing
\end{verbatim}

\end{fbx}

This hybrid approach combined human-engineered features with statistical
learning. The Viola-Jones algorithm
(\citeproc{ref-viola2001rapidobject}{Viola and Jones,
n.d.})\sidenote{\textbf{Viola-Jones Algorithm}: A groundbreaking
computer vision algorithm that detected faces in real-time by using
simple rectangular patterns (comparing brightness of eye regions versus
cheek regions) and making decisions in stages, filtering out non-faces
quickly. The cascade approach reduced computation 10-100x by rejecting
easy negatives early, making real-time vision feasible on CPUs. This
compute-saving pattern appears throughout edge ML systems where power
budgets matter. } (2001) exemplifies this era, achieving real-time face
detection using simple rectangular features and cascaded classifiers.
This algorithm powered digital camera face detection for nearly a
decade---demonstrating that well-engineered features could enable
practical applications, but only within narrow domains where experts
could hand-craft the right representations.

Table~\ref{tbl-ai-evolution-strengths} summarizes these paradigm shifts,
highlighting the bottlenecks that defined each era.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0962}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1683}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2212}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{AI Paradigm Evolution}: Each era is defined by the
systems bottleneck that constrained it. Deep learning (far right)
overcame the Feature Engineering bottleneck but introduced new
infrastructure challenges, necessitating modern ML systems
engineering.}\label{tbl-ai-evolution-strengths}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symbolic AI}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Expert Systems}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Statistical Learning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symbolic AI}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Expert Systems}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Statistical Learning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Key Strength} & Logical reasoning & Domain expertise &
Versatility & Pattern recognition \\
\textbf{Bottleneck} & \textbf{Brittleness}(Rules break) &
\textbf{Knowledge Entry}(Experts are scarce) & \textbf{Feature
Engineering}(Manual preprocessing) & \textbf{Compute \& Data
Scale}(Infrastructure cost) \\
\textbf{Data Handling} & Minimal data needed & Domain knowledge-based &
Moderate data required & Massive data processing \\
\end{longtable}

\subsection{Deep Learning Era: The Infrastructure
Bottleneck}\label{sec-introduction-deep-learning-era-infrastructure-bottleneck-490a}

Deep learning (2012--Present) removed the human feature engineering
requirement. Neural networks learn representations directly from raw
data (pixels, audio waveforms), enabling ``end-to-end'' learning.

This shift was unlocked not by new algorithms (CNNs existed in the
1980s), but by \textbf{Systems Co-design}. The 2012 AlexNet breakthrough
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}), illustrated in Figure~\ref{fig-alexnet}, occurred because
algorithmic structure (parallel matrix operations) matched hardware
capabilities (GPUs).

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/89946785f056ace8b5a9cf4b0b21cff6870b92c4.pdf}}

}

\caption{\label{fig-alexnet}\textbf{AlexNet Convolutional Neural Network
Architecture}: The network that launched the deep learning revolution at
ImageNet 2012. With 60 million parameters trained on 1.2 million images
across two GTX 580 GPUs, AlexNet achieved 15.3\% top-5 error compared to
26.2\% for the second-place entry, a 42\% relative improvement. The
architecture progresses from convolutional layers (green blocks showing
spatial feature extraction) through fully connected layers (dense
connections), demonstrating that deep networks could automatically learn
effective visual features without hand-crafted engineering.}

\end{figure}%

This shift effectively traded the \textbf{Feature Engineering
Bottleneck} for a new \textbf{Compute Bottleneck}. Models like GPT-3
(\citeproc{ref-brown2020language}{Brown et al. 2020}) (175 billion
parameters) required:

\begin{itemize}
\tightlist
\item
  \textbf{Compute Scale}: 314 zettaFLOPs (\(10^{21}\) operations).
\item
  \textbf{Data Scale}: Approximately 500 billion tokens
  (\textasciitilde700GB) from filtered web text, books, and Wikipedia.
\item
  \textbf{Infrastructure Scale}: 1,024 GPUs running for weeks.
\end{itemize}

The primary engineering challenge shifted from ``how do we describe a
cat's ear?'' to ``how do we coordinate 1,000 GPUs without failure?''

This progression through four paradigms, each defined by its bottleneck
and each overcome by systems innovation, raises a strategic question.
Given the three components of the AI Triad (Data, Algorithm, Machine),
which should we prioritize to advance AI capabilities? Should
organizations invest in better algorithms, larger datasets, or more
powerful machines? Seven decades of AI history reveal a consistent
pattern: each paradigm transition was enabled by systems innovations
that overcame the bottleneck of the previous era. One of AI's leading
researchers has crystallized this insight into a principle that has
guided the field's most successful practitioners.

\phantomsection\label{quiz-question-sec-introduction-historical-evolution-ai-paradigms-3be8}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.2}{}
\phantomsection\label{quiz-question-sec-introduction-historical-evolution-ai-paradigms-3be8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following factors did NOT contribute to the transition
  towards a systems-focused approach in AI?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Massive datasets from the internet age
  \item
    The development of symbolic AI in the 1950s
  \item
    Increased availability of low-cost GPUs
  \item
    Algorithmic breakthroughs in deep learning
  \end{enumerate}
\item
  Explain how the convergence of massive datasets, algorithmic
  breakthroughs, and hardware acceleration has transformed AI from an
  academic curiosity to a production technology.
\item
  True or False: The systems-centric approach in AI emerged because
  early AI systems were too complex and required simplification.
\item
  The introduction of \_\_\_\_ by OpenAI in 2020 demonstrated the
  increasing complexity and capability of AI systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-historical-evolution-ai-paradigms-3be8]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{The Bitter Lesson: Why Systems Engineering
Matters}\label{sec-introduction-bitter-lesson-systems-engineering-matters-46a1}

Richard Sutton's 2019 essay ``The Bitter Lesson'' formalizes the
historical pattern we just traced
(\citeproc{ref-sutton2019bitter}{Sutton 2019}). Looking back at seven
decades of research, Sutton observed that general methods which can
leverage increasing computation consistently outperform approaches that
encode human expertise. He writes: ``The biggest lesson that can be read
from 70 years of AI research is that general methods that leverage
computation are ultimately the most effective, and by a large margin.''

Table~\ref{tbl-ai-evolution-performance} provides quantitative
validation of this principle. The shift from expert systems to
statistical learning to deep learning has dramatically improved
performance on representative tasks, with each transition enabled by
increased computational scale rather than cleverer encoding of human
knowledge.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2303}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2171}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1711}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1447}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2171}}@{}}
\caption{\textbf{AI Performance Evolution Across Paradigms}: Each
paradigm transition correlates with increased computational scale rather
than algorithmic sophistication. Performance improved from amateur-level
expert systems (2000 Elo) to superhuman foundation models (86.4\% MMLU),
while computational requirements grew from single CPUs to 25,000 A100
GPU-days. Training time initially increased (hours to days) but later
decreased as distributed systems enabled
parallelization.}\label{tbl-ai-evolution-performance}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Representative Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Performance}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Computational Resources}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Representative Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Performance}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Computational Resources}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Expert Systems (1980s)} & Hand-crafted rules & Chess (Elo
rating) & \textasciitilde2000 Elo (amateur) & Minimal (rule
evaluation) \\
\textbf{Statistical ML (1990s-2000s)} & Feature engineering + learning &
ImageNet top-5 accuracy & 50--60\% & Hours on single CPU \\
\textbf{Deep Learning (2012)} & End-to-end neural networks & ImageNet
top-5 accuracy & 84.6\% (AlexNet) & 6 days on 2 GPUs \\
\textbf{Modern Deep Learning (2020+)} & Large-scale transformers &
ImageNet top-5 accuracy & 90.0\%+ (ViT) & Hours on distributed
systems \\
\textbf{Modern Deep Learning (2023)} & Foundation models & MMLU
benchmark & 86.4\% (GPT-4) & Estimated 25,000 A100 GPU-days \\
\end{longtable}

The table reveals three insights. Performance improvements correlate
with computational scale, not algorithmic sophistication alone. Training
time initially increased (hours to days) but then decreased (back to
hours) as distributed systems enabled parallelization. The most dramatic
improvements occurred at paradigm transitions (expert systems →
statistical learning, statistical learning → deep learning) when new
approaches unlocked the ability to leverage more computation
effectively. This pattern validates Sutton's observation: progress comes
from finding ways to use more compute, not from encoding more human
knowledge.

This principle finds further validation across AI breakthroughs. In
chess, IBM's Deep Blue defeated world champion Garry Kasparov in 1997
(\citeproc{ref-campbell2002deep}{Campbell, Hoane, and Hsu 2002}) not by
encoding chess strategies, but through brute-force search evaluating
millions of positions per second. In Go, DeepMind's
AlphaGo\sidenote{\textbf{AlphaGo}: DeepMind's Go-playing system that
defeated world champion Lee Sedol in 2016. From a systems perspective,
AlphaGo demonstrated the power of combining neural networks with tree
search at unprecedented scale: the match version used 1,920 CPUs and 280
GPUs for inference alone. Its successor AlphaGo Zero eliminated human
game data entirely, learning solely through self-play using 64 GPU
workers and 19 CPU parameter servers for training. The generalized
successor AlphaZero extended this approach to chess and shogi using
5,000 TPUs. This progression from human-data-dependent to fully
self-supervised learning, enabled by massive compute infrastructure,
exemplifies the Bitter Lesson in practice. In computer vision,
convolutional neural networks that learn features directly from data
have surpassed decades of hand-crafted feature engineering. In speech
recognition, end-to-end deep learning systems have outperformed
approaches built on detailed models of human phonetics and linguistics.
} (\citeproc{ref-silver2016mastering}{Silver et al. 2016}) achieved
superhuman performance by learning from self-play rather than studying
centuries of human Go wisdom.

The ``bitter'' aspect is that our intuition misleads us. We naturally
assume that encoding human expertise should be the path to artificial
intelligence. Yet repeatedly, systems that leverage computation to learn
from data outperform systems that rely on human knowledge given
sufficient scale. This pattern has held across symbolic AI, statistical
learning, and deep learning eras.

Consider modern language models like GPT-4 or image generation systems
like DALL-E. Their capabilities emerge not from linguistic or artistic
theories encoded by humans, but from training general-purpose neural
networks on vast amounts of data using substantial computational
resources. Estimates for models at GPT-3's scale suggest thousands of
megawatt-hours of energy based on hardware specifications and reported
training duration (\citeproc{ref-patterson2021carbon}{Patterson et al.
2021}). Serving models to millions of users requires data centers with
significant continuous power demand. The engineering challenge is
building systems that can manage this scale: collecting and processing
large training datasets, coordinating training across many accelerators,
serving models to many users with tight latency requirements, and
continuously updating systems based on real-world performance.

These scale requirements expose a fundamental engineering reality:
building systems capable of training on petabytes of data and serving
millions of users requires expertise in distributed systems, data
engineering, and hardware optimization that goes far beyond algorithmic
innovation. The computational infrastructure needed to realize modern AI
capabilities has become the primary engineering challenge, from managing
data movement between storage and processing
units\sidenote{\textbf{Memory Bandwidth}: The rate at which data can be
transferred between memory and processors. Many ML workloads are
constrained by data movement rather than arithmetic throughput, a
constraint that motivates specialized memory architectures in
accelerators. We develop quantitative analysis of memory bandwidth and
its implications for system design in \textbf{?@sec-ai-acceleration}. }
to coordinating thousands of processors and optimizing for both
performance and energy efficiency. We explore these hardware constraints
quantitatively in \textbf{?@sec-ai-acceleration}, where students will
have the prerequisite background to analyze memory bandwidth limitations
and their implications for system design.

Sutton's bitter lesson explains the motivation for ML systems
engineering. If AI progress depends on our ability to scale computation
effectively, then understanding how to build, deploy, and maintain these
computational systems becomes essential for AI practitioners. Creating
modern systems requires coordinating thousands of GPUs across multiple
data centers, processing petabytes of text data, and serving resulting
models to millions of users with millisecond latency requirements. This
challenge demands expertise in distributed
systems\sidenote{\textbf{Distributed Systems}: Computing systems where
components run on multiple networked machines and coordinate through
message passing. Modern large-scale training often requires distributed
computation, which introduces challenges in fault tolerance, network
bottlenecks, and consistency. This book develops awareness of these
constraints, and the companion book covers implementation details and
case studies. }, data engineering, hardware optimization, and
operational practices that represent an entirely new engineering
discipline.

The convergence of these systems-level challenges suggests that no
existing discipline addresses what modern AI requires. While Computer
Science advances ML algorithms and Electrical Engineering develops
specialized AI hardware, neither discipline alone provides the
engineering principles needed to deploy, optimize, and sustain ML
systems at scale. This gap has given rise to a new engineering
discipline.

\phantomsection\label{quiz-question-sec-introduction-bitter-lesson-systems-engineering-matters-46a1}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.3}{}
\phantomsection\label{quiz-question-sec-introduction-bitter-lesson-systems-engineering-matters-46a1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary lesson from 70 years of AI research according to
  Richard Sutton's `Bitter Lesson'?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Leveraging massive computational resources
  \item
    Curating better datasets
  \item
    Developing more sophisticated algorithms
  \item
    Encoding human expertise into AI systems
  \end{enumerate}
\item
  Explain why systems engineering has become more critical than
  algorithmic development in modern AI systems.
\item
  True or False: The primary constraint in modern ML systems is compute
  capacity rather than memory bandwidth.
\item
  Which factor is NOT a primary challenge in scaling modern AI systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Thermal and power constraints
  \item
    Memory bandwidth limitations
  \item
    Data center coordination
  \item
    Algorithmic complexity
  \end{enumerate}
\item
  In a production system, how might you address the memory bandwidth
  bottleneck when deploying large-scale ML models?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-bitter-lesson-systems-engineering-matters-46a1]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Defining AI
Engineering}\label{sec-introduction-defining-ai-engineering-19ce}

The Bitter Lesson establishes that computational scale drives AI
progress. The preceding discussion reveals that realizing this scale
requires coordinating data infrastructure, training systems, and
deployment platforms, challenges that span traditional disciplinary
boundaries. These observations define the scope of an emerging
discipline:

\phantomsection\label{callout-definitionux2a-1.7}
\begin{fbx}{callout-definition}{Definition: }{AI Engineering}
\phantomsection\label{callout-definition*-1.7}
\textbf{AI Engineering} is the discipline of building \emph{efficient},
\emph{reliable}, \emph{safe}, and \emph{robust} intelligent systems that
operate in the \emph{real world}, not just models in isolation.

\end{fbx}

AI Engineering encompasses the complete lifecycle of production
intelligent systems. A breakthrough algorithm requires efficient data
collection and processing, distributed computation across hundreds or
thousands of machines, reliable service to users with strict latency
requirements, and continuous monitoring and updating based on real-world
performance. The discipline addresses fundamental challenges at every
level: designing efficient algorithms for specialized hardware,
optimizing data pipelines that process petabytes daily, implementing
distributed training across thousands of GPUs, deploying models that
serve millions of concurrent users, and maintaining systems whose
behavior evolves as data distributions shift.

This emergence of AI Engineering as a distinct discipline mirrors how
Computer Engineering emerged in the late 1960s and early
1970s.\sidenote{\textbf{Computer Engineering Origins}: Case Western
Reserve University established the first accredited US computer
engineering program in 1971, formally bridging electrical engineering
and computer science. ML systems engineering follows this tradition,
combining algorithmic expertise with hardware understanding. } As
computing systems grew more complex, neither Electrical Engineering nor
Computer Science alone could address the integrated challenges of
building reliable computers. Computer Engineering emerged as a complete
discipline bridging both fields. Today, AI Engineering faces similar
challenges at the intersection of algorithms, infrastructure, and
operational practices.

With AI Engineering defined and its governing equation established, we
use ``ML systems engineering'' throughout this text to describe the
practice: the work of designing, deploying, and maintaining the machine
learning systems that constitute modern AI.

\phantomsection\label{callout-perspectiveux2a-1.8}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Iron Law of ML Systems}
\phantomsection\label{callout-perspective*-1.8}
To reason about ML systems as engineers, we need more than qualitative
descriptions; we need a quantitative framework that connects every layer
of the stack. Just as classical mechanics is governed by Newton's laws,
and processor performance is governed by the Iron Law of Processor
Performance, machine learning system performance is governed by the
\textbf{Iron Law of ML Systems}:

\[
\text{Time}_{\text{total}} = \underbrace{ \frac{\text{Data}}{\text{Bandwidth}} }_{\text{The Data Term}} + \underbrace{ \frac{\text{Ops}}{\text{Throughput}} \times \frac{1}{\text{Utilization}} }_{\text{The Compute Term}} + \underbrace{ \text{Latency}_{\text{fixed}} }_{\text{The Overhead Term}}
\]

This equation is the mathematical spine of this book. It decomposes the
total time required for any ML task---whether training a model for weeks
or serving an inference in milliseconds---into three fundamental terms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The Data Term} (\(\frac{\text{Data}}{\text{Bandwidth}}\)): The
  physical cost of moving bits. Whether loading terabytes from cloud
  storage or fetching weights from HBM, performance is often limited by
  I/O physics. We address this in \textbf{Part I: Foundations}.
\item
  \textbf{The Compute Term}
  (\(\frac{\text{Ops}}{\text{Throughput}} \times \frac{1}{\text{Utilization}}\)):
  The cost of arithmetic. ``Throughput'' is the hardware's peak
  capacity, while ``Utilization'' is the efficiency of the software and
  compiler. We address this in \textbf{Part II: Build} and \textbf{Part
  III: Optimize}.
\item
  \textbf{The Overhead Term} (\(\text{Latency}_{\text{fixed}}\)): The
  irreducible ``tax'' of system orchestration, networking, and
  serialization. This term often dominates in real-time deployment. We
  address this in \textbf{Part IV: Deploy}.
\end{enumerate}

Throughout this volume, every optimization technique we study---from
pruning to kernel fusion---is simply a method for reducing one of these
variables.

\end{fbx}

If scale is the ultimate lever for performance, it is also the ultimate
consumer of resources. The Bitter Lesson teaches that scale works, but
the Iron Law teaches us how to afford it. This tension between scaling
and sustainability shapes the engineering principles that follow.

The Iron Law provides more than a diagnostic framework; it organizes the
entire discipline. Each term in the equation corresponds to a
fundamental engineering imperative. The Data Term demands that we
\emph{build} robust data pipelines and infrastructure. The Compute Term
requires that we \emph{optimize} algorithms and hardware utilization for
efficiency. The Overhead Term necessitates that we \emph{deploy} and
\emph{operate} systems reliably in production. These three imperatives
structure this textbook: Part I and II address building, Part III
addresses optimization, and Part IV addresses deployment and operations.

Before exploring these imperatives in detail, we must establish what we
mean by ``ML systems.'' The term appears throughout discussions of AI
progress, but its precise meaning and how it differs from traditional
software requires clarification.

\phantomsection\label{quiz-question-sec-introduction-defining-ai-engineering-19ce}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.4}{}
\phantomsection\label{quiz-question-sec-introduction-defining-ai-engineering-19ce}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary focus of AI Engineering as a discipline?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Developing new AI algorithms
  \item
    Improving symbolic reasoning techniques
  \item
    Building reliable, efficient, and scalable ML systems
  \item
    Enhancing user interfaces for AI applications
  \end{enumerate}
\item
  Explain how the historical shift from symbolic systems to
  learning-based approaches has influenced the emergence of AI
  Engineering.
\item
  Which of the following best describes the role of AI Engineering in
  modern AI systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Focusing solely on algorithmic development
  \item
    Developing symbolic AI techniques
  \item
    Designing user-friendly AI interfaces
  \item
    Integrating and optimizing systems for real-world deployment
  \end{enumerate}
\item
  In a production system, how might AI Engineering address the challenge
  of energy efficiency while maintaining performance?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-defining-ai-engineering-19ce]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Defining ML
Systems}\label{sec-introduction-defining-ml-systems-d4af}

Rather than beginning with an abstract definition, consider a system you
likely interact with daily.

\subsection{A Concrete Example: Email Spam
Filtering}\label{sec-introduction-concrete-example-email-spam-filtering-adbe}

Consider the spam filter protecting your inbox. Every day, it processes
millions of emails, deciding in milliseconds which messages deserve your
attention and which should be quarantined. Gmail alone processes
approximately 300 billion emails annually, with spam comprising roughly
50\% of all email traffic (\citeproc{ref-statista2024email}{Statista
2024}). Production spam filters typically target accuracy above 99.9\%
while processing each email in under 50 ms to avoid noticeable delays.

This deceptively simple task reveals what distinguishes machine learning
systems from traditional software:

The data challenge arises because the filter trains on millions of
labeled examples, constantly adapting as spammers evolve their tactics.
Traditional software would require programmers to manually encode rules
for every spam pattern. The ML approach learns patterns automatically
from data, adapting to new spam techniques without programmer
intervention.

The algorithmic challenge requires the system to generalize from
training examples to recognize spam it has never seen before. It
balances precision against recall, avoiding false positives that hide
legitimate emails while catching actual spam. This probabilistic
decision-making differs from deterministic software logic in fundamental
ways.

The infrastructure challenge means servers must process billions of
emails daily, storing models that encode learned patterns, updating
those models as spam evolves, and serving predictions with sub-100 ms
latency. The system must scale horizontally across data centers while
maintaining consistency.

This spam filter demonstrates three interconnected concerns that appear
in every machine learning system: obtaining and managing training data
at scale, implementing algorithms that learn and generalize effectively,
and building infrastructure that supports both training and real-time
prediction. No traditional software system exhibits all three of these
characteristics simultaneously.

\subsection{Formalizing the
Definition}\label{sec-introduction-formalizing-definition-729e}

We define a machine learning system as follows:

\phantomsection\label{callout-definitionux2a-1.9}
\begin{fbx}{callout-definition}{Definition: }{Machine Learning Systems}
\phantomsection\label{callout-definition*-1.9}
\textbf{Machine Learning Systems} refer to integrated computing systems
comprising three interdependent components: \emph{data} that guides
behavior, \emph{algorithms} that learn patterns, and \emph{machines}
that enable training and inference.

\end{fbx}

These three components form the \textbf{AI Triad}: \textbf{D}ata,
\textbf{A}lgorithm, and \textbf{M}achine. We formalize this interaction
as the \textbf{DAM Taxonomy}, a diagnostic framework for identifying
bottlenecks and understanding system constraints.
Table~\ref{tbl-dam-taxonomy} defines each component.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3672}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4141}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0781}}@{}}
\caption{\textbf{The DAM Taxonomy}: Every ML system comprises these
three interdependent components. When performance stalls, ask:
\emph{``Where is the flow blocked? Check the
DAM.''}}\label{tbl-dam-taxonomy}\tabularnewline
\toprule\noalign{}
\endfirsthead
\endhead
\bottomrule\noalign{}
\endlastfoot
\multirow{2}{=}{Component :============== \textbf{Data}} &
\multirow{2}{=}{Definition
:============================================= Information that guides
system behavior} & \multirow{2}{=}{Role in System
:=================================================== \emph{The Fuel}:
Defines what the system learns} & \\
& & & \\
\textbf{Algorithm} & Mathematical structures that learn patterns &
\emph{The Blueprint}: Defines how patterns are captured & \\
\textbf{Machine} & Hardware and software infrastructure &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4922} + 2\tabcolsep}@{}}{%
\emph{The Engine}: Defines how fast and where computation happens} \\
\end{longtable}

Think of it like a rocket: \textbf{Data} is the fuel that powers the
journey, \textbf{Algorithm} is the blueprint that defines the flight
path, and \textbf{Machine} is the engine that makes it all move. Without
fuel, the engine sits idle. Without a blueprint, the fuel burns
aimlessly. Without an engine, fuel and blueprints remain theoretical. ML
systems engineering is the discipline of keeping all three in balance.

The DAM Taxonomy serves as a diagnostic tool throughout this text. Scale
in ML systems is the relentless pursuit of the \emph{moving bottleneck}:
alleviating a constraint in one component often shifts the limitation to
another. Upgrading to faster GPUs (Machine) might reveal that storage
cannot feed data fast enough (Data). Collecting a massive dataset (Data)
might reveal that the model lacks capacity to learn from it (Algorithm).
Switching to a larger model (Algorithm) might exceed available memory
(Machine). Understanding these dynamics is central to ML systems
engineering. Part III formalizes this diagnostic approach with the DAM ×
Bottleneck matrix (\textbf{?@sec-benchmarking-ai}).

Figure~\ref{fig-ai-triad} illustrates the triangular dependency among
Data, Algorithm, and Machine. Each element shapes the possibilities of
the others. The algorithm dictates both the computational demands for
training and inference, as well as the volume and structure of data
required for effective learning. The data's scale and complexity
influence what machines are needed for storage and processing while
determining which algorithms are feasible. The machine capabilities
establish practical limits on both model scale and data processing
capacity, creating a framework within which the other components must
operate.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0152fa5b8f01f8a2b0b9263d82435c6d1f0dfc5b.pdf}}

}

\caption{\label{fig-ai-triad}\textbf{Component Interdependencies}:
Machine learning system performance relies on the coordinated
interaction of models, data, and machines; limitations in any one
component constrain the capabilities of the others. Effective system
design requires balancing these interdependencies to optimize overall
performance and feasibility.}

\end{figure}%

Each component of the AI Triad serves a distinct but interconnected
purpose:

\begin{itemize}
\item
  \textbf{Data}: Processes and systems for collecting, storing,
  processing, managing, and serving data for both training and inference
\item
  \textbf{Algorithm}\sidenote{\textbf{Algorithm}: Derived from the
  Latinized name \emph{Algoritmi}, honoring the 9th-century Persian
  mathematician Muhammad ibn Musa al-Khwarizmi, whose treatise on
  arithmetic introduced Hindu-Arabic numerals to Europe. The term
  originally meant ``computation with Arabic numerals'' before evolving
  to mean any step-by-step computational procedure. This etymology
  reminds us that systematic computation has roots stretching back over
  a millennium. }: Mathematical models and methods that learn patterns
  from data to make predictions or decisions
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Machine}: Hardware and software systems that enable training,
  serving, and operation of models at scale
\end{itemize}

\phantomsection\label{callout-perspectiveux2a-1.10}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Samples per Dollar}
\phantomsection\label{callout-perspective*-1.10}
\textbf{The Systems View}: While researchers optimize for
\emph{accuracy}, systems engineers optimize for \textbf{Samples per
Dollar}. This metric unifies the three components of the AI Triad into a
single constraint equation:

\[ \text{Cost} \propto \frac{\text{Model Size} \times \text{Dataset Size}}{\text{Hardware Efficiency}} \]

\begin{itemize}
\tightlist
\item
  \textbf{Data}: Improving data quality (cleaning, filtering) increases
  the ``learning value'' of each sample, effectively reducing the
  numerator.
\item
  \textbf{Algorithm}: More efficient architectures (like Transformers vs
  RNNs) improve the rate at which samples translate to accuracy.
\item
  \textbf{Machine}: Specialized hardware (GPUs/TPUs) increases the
  denominator, allowing more samples to be processed for the same cost.
\end{itemize}

Systems engineering is the art of balancing this equation. A 10\% gain
in hardware efficiency allows for a 10\% larger dataset, which might
yield a 1\% gain in accuracy. The engineer's job is to determine if that
trade-off is economically viable.

\end{fbx}

As the triangle illustrates, no single element can function in
isolation. Algorithms require data and machines to run on, large
datasets require algorithms and machines to be useful, and machines
require algorithms and data to serve any purpose.

This triangular dependency means that advancing any single component in
isolation provides limited benefit. Improved algorithms cannot realize
their potential without sufficient data and computational capacity.
Larger datasets become burdensome without algorithms capable of
extracting meaningful patterns and machines capable of processing them
efficiently. More powerful machines accelerate computation but cannot
compensate for poor data quality or unsuitable algorithmic approaches.
Machine learning systems demand orchestration of all three AI Triad
components, with each constraining and enabling the others.

\subsection{Lighthouse Archetypes: The Systems
Detectives}\label{sec-introduction-lighthouse-archetypes-systems-detectives-a216}

To ground the abstract interdependencies of the Iron Law in concrete
practice, this textbook employs five recurring \textbf{Lighthouse
Archetypes}. We do not use these models merely as examples; they are
\textbf{Systems Detectives}---canonical workloads that we will use in
every chapter to ``interrogate'' the Silicon Contract.

Each archetype represents a distinct extreme of the Iron Law. For
instance, \textbf{ResNet-50} allows us to investigate the
\textbf{Compute Term} in its purest form, while \textbf{GPT-2/Llama}
acts as our primary probe for \textbf{Memory Bandwidth} bottlenecks. By
following these same workloads from data engineering through to edge
deployment, you will see how a single architectural choice propagates
physical and economic constraints across the entire system.

Table~\ref{tbl-lighthouse-examples} summarizes the quantitative
characteristics that make each archetype a unique diagnostic tool for ML
systems engineering.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1679}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1241}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1241}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1971}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1825}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1752}}@{}}
\caption{\textbf{Lighthouse Archetype Specifications}: These workloads
recur throughout subsequent chapters, acting as ``Detectives'' that
reveal how different systems principles (e.g., quantization, batching,
or pruning) affect performance differently across various architectural
constraints.}\label{tbl-lighthouse-examples}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arithmetic Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Target}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arithmetic Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Target}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} & 25.6M & 102 MB & High (compute-bound) & Throughput
& Cloud / Edge \\
\textbf{GPT-2 / Llama} & 1.5B - 70B & 6 GB - 140 GB & Low (memory-bound)
& Memory bandwidth & Cloud \\
\textbf{MobileNetV2} & 3.5M & 14 MB & Medium & Latency, power &
Mobile \\
\textbf{DLRM} & 25B & 100 GB+ & Very low (memory-bound) & Memory
capacity & Cloud (distributed) \\
\textbf{Keyword Spotting} & 200K & 800 KB & Low & Power, memory & TinyML
/ MCU \\
\end{longtable}

Each archetype manifests different constraints within the AI Triad,
ensuring that the principles developed throughout this text are tested
against the diversity of real-world systems engineering challenges.
Later in this chapter, we complement these technical workloads with
three deployment case studies, Waymo, FarmBeats, and AlphaFold, that
illustrate how the same core challenges manifest in production systems
under radically different constraints.

The AI Triad's interdependencies become concrete in breakthrough
moments. The 2012 AlexNet victory
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017})---reducing ImageNet\sidenote{\textbf{ImageNet}: A dataset of over
14 million labeled images organized into 22,000 categories, created at
Stanford and Princeton starting in 2006. The associated ImageNet Large
Scale Visual Recognition Challenge (ILSVRC), held from 2010 to 2017,
became the definitive benchmark for computer vision systems. ImageNet's
scale (1.2 million training images across 1,000 categories in the
competition subset) demanded systems engineering solutions: distributed
data loading, GPU memory optimization, and efficient preprocessing
pipelines. The benchmark methodology established here influenced all
subsequent ML systems evaluation. We examine this watershed moment in
detail when tracing AI's historical evolution
(Section~\ref{sec-introduction-deep-learning-era-infrastructure-bottleneck-490a}),
but the lesson is immediate: coordinating all three AI Triad components
enables capabilities that any single component cannot achieve alone. }
(\citeproc{ref-deng2009imagenet}{Deng et al. 2009}) top-5 error from
26.2\% to 15.3\%---occurred not through algorithmic novelty alone, but
because convolutional neural networks' parallel matrix operations
aligned perfectly with GPU hardware capabilities.

However, this interdependence creates new vulnerabilities. When any AI
Triad component degrades---data distributions shift, model assumptions
break, or machines fail---the entire system can fail in ways that
traditional software never experiences. Understanding \emph{how} these
failures differ from conventional software bugs establishes why ML
systems engineering has emerged as a distinct discipline.

\phantomsection\label{quiz-question-sec-introduction-defining-ml-systems-d4af}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.5}{}
\phantomsection\label{quiz-question-sec-introduction-defining-ml-systems-d4af}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes a machine learning system?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    A computing system that integrates Data, Algorithm, and Machine (the
    DAM).
  \item
    A standalone algorithm that processes data.
  \item
    A software application that uses pre-defined rules to make
    decisions.
  \item
    A data storage system optimized for large datasets.
  \end{enumerate}
\item
  True or False: In a machine learning system, the model architecture
  does not influence the computational demands for training and
  inference.
\item
  In the context of ML systems, what role does computing infrastructure
  play?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It solely stores and retrieves data.
  \item
    It provides the necessary resources for both training and inference.
  \item
    It is only responsible for serving the model predictions.
  \item
    It determines the model architecture to be used.
  \end{enumerate}
\item
  Consider a scenario where an ML system's data component is limited by
  storage capacity. How might this affect the other components of the
  system?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-defining-ml-systems-d4af]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{How ML Systems Differ from Traditional
Software}\label{sec-introduction-ml-systems-differ-traditional-software-7ea8}

The AI Triad (Data, Algorithm, Machine) reveals what ML systems
comprise: data that guides behavior, algorithms that extract patterns,
and machines that enable learning and
inference\sidenote{\textbf{Inference}: From Latin \emph{inferre} (to
bring in, to conclude), combining \emph{in-} (into) and \emph{ferre} (to
carry, to bear). In logic, inference means deriving conclusions from
premises. In ML, the term describes using a trained model to make
predictions on new data, ``carrying'' learned patterns into novel
situations. The training/inference distinction parallels the traditional
compile-time/run-time split in software, with inference being the
deployment phase where learned knowledge is applied. }. Understanding
these components alone does not capture how ML systems engineering
differs from traditional software engineering. The critical distinction
lies in how these systems fail.

Traditional software exhibits explicit failure modes. When code breaks,
applications crash, error messages propagate, and monitoring systems
trigger alerts. This immediate feedback enables rapid diagnosis and
remediation. The system operates correctly or fails observably. Machine
learning systems operate under a different paradigm. They can continue
functioning while their performance degrades silently without triggering
conventional error detection mechanisms. The algorithms continue
executing, the machines maintain prediction serving, yet the learned
behavior becomes progressively less accurate or contextually relevant.

Consider how an autonomous vehicle's perception system illustrates this
distinction. Traditional automotive software exhibits binary operational
states: the engine control unit either manages fuel injection correctly
or triggers diagnostic warnings. The failure mode remains observable
through standard monitoring. An ML-based perception system presents a
different challenge: the system's accuracy in detecting pedestrians
might decline from 95\% to 85\% over several months due to seasonal
changes, as different lighting conditions, clothing patterns, or weather
phenomena underrepresented in training data affect model performance.
The vehicle continues operating, successfully detecting most
pedestrians, yet the degraded performance creates safety risks that
become apparent only through systematic monitoring of edge cases and
comprehensive evaluation. Conventional error logging and alerting
mechanisms remain silent while the system becomes measurably less safe.

The magnitude of this degradation matters profoundly in safety-critical
contexts. For autonomous vehicles, even 95\% accuracy may be inadequate:
safety-critical systems typically require 99.9\% or higher reliability.
The 10\% degradation from 95\% to 85\% is especially concerning because
failures concentrate in edge cases where detection was already marginal,
precisely the scenarios where human safety is most at risk.

This silent degradation manifests across all three AI Triad components.
The data distribution shifts as the world changes: user behavior
evolves, seasonal patterns emerge, new edge cases appear. The algorithms
continue making predictions based on outdated learned patterns, unaware
that their training distribution no longer matches operational reality.
The machines faithfully serve these increasingly inaccurate predictions
at scale, amplifying the problem.

\subsection{The Degradation
Equation}\label{sec-introduction-degradation-equation-e87e}

This phenomenon demands a quantitative framework. Just as Patterson and
Hennessy's Iron Law decomposed CPU performance into fundamental
components, we can decompose ML system degradation into its constituent
factors. The \textbf{Degradation Equation} captures how model
performance evolves over time:

\[ \text{Accuracy}(t) \approx \text{Accuracy}_0 - \lambda \cdot D(P_t \| P_0) \]

where:

\begin{itemize}
\tightlist
\item
  \(\text{Accuracy}_0\): Initial accuracy at deployment
\item
  \(D(P_t \| P_0)\): Statistical divergence between current data
  distribution \(P_t\) and training distribution \(P_0\)
\item
  \(\lambda\): Model sensitivity to distribution shift
  (architecture-dependent)
\end{itemize}

This equation reveals three engineering levers for managing degradation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Improve initial accuracy} (\(\text{Accuracy}_0\)): Better
  training, more data, superior architectures---but this only shifts the
  curve, not its slope
\item
  \textbf{Reduce distribution sensitivity} (\(\lambda\)): Robust
  training techniques, domain adaptation, broader training
  distributions---this flattens the degradation curve
\item
  \textbf{Monitor and respond to drift} (\(D\)): Continuous measurement
  of distribution divergence enables proactive retraining before
  accuracy falls below acceptable thresholds
\end{enumerate}

The practical implication is profound: \textbf{knowing when to retrain
is as important as knowing how to train}. A system that retrains when
\(D(P_t \| P_0) > \tau\) for some threshold \(\tau\) maintains accuracy
within bounds. A system without drift monitoring operates blind to its
own degradation.

This framework distinguishes ML systems engineering from traditional
software engineering at the deepest level. Traditional systems have no
equivalent equation because they don't drift---a function that computed
correctly yesterday computes correctly today. ML systems require
continuous investment in monitoring infrastructure that traditional
software never needed, and the Degradation Equation quantifies why.

Consider a recommendation system experiencing this degradation: it might
decline from 85\% to 60\% accuracy over six months as user preferences
evolve and training data becomes stale. The system continues generating
recommendations, users receive results, the machines report healthy
uptime metrics, yet business value silently erodes. This degradation
often stems from training-serving skew, where features computed
differently between training and serving pipelines cause model
performance to degrade despite unchanged code. This is a machine issue
that manifests as algorithmic failure.

This difference in failure modes distinguishes ML systems from
traditional software in ways that demand new engineering practices.
Traditional software development focuses on eliminating bugs and
ensuring deterministic behavior. ML systems engineering must
additionally address probabilistic behaviors, evolving data
distributions, and performance degradation that occurs without code
changes. The monitoring systems must track not just infrastructure
health but also model performance, data quality, and prediction
distributions. The deployment practices must enable continuous model
updates as data distributions shift. The entire system lifecycle, from
data collection through model training to inference serving, must be
designed with silent degradation in mind.

ML systems developed in research settings require specialized
engineering practices to reach production deployment. The unique
lifecycle and monitoring requirements stem directly from these failure
characteristics, establishing the core motivation for ML systems
engineering as a distinct discipline.

The Bitter Lesson established that computational scale drives AI
progress. However, as we saw in the previous section, scale introduces
new reliability challenges through silent degradation. Beyond these
operational risks, scale also confronts hard physical and economic
limits. Training modern foundation models consumes millions of dollars
in compute and gigawatt-hours of energy. This creates a fundamental
tension: if we must scale to advance AI, but resources are finite, how
do we sustain this progress?

The answer lies in efficiency: achieving more capability with less
resource expenditure.

\phantomsection\label{quiz-question-sec-introduction-ml-systems-differ-traditional-software-7ea8}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.6}{}
\phantomsection\label{quiz-question-sec-introduction-ml-systems-differ-traditional-software-7ea8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the fundamental difference in failure modes between
  traditional software and ML systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Traditional software crashes visibly while ML systems can degrade
    silently without triggering alerts.
  \item
    Traditional software requires more monitoring than ML systems.
  \item
    ML systems always fail faster than traditional software.
  \item
    Traditional software cannot handle errors while ML systems have
    built-in error recovery.
  \end{enumerate}
\item
  Explain how the concept of `silent performance degradation'
  differentiates machine learning systems from traditional software
  systems.
\item
  True or False: ML systems can maintain optimal performance without
  specialized monitoring approaches beyond traditional software metrics.
\item
  Why do ML systems require different monitoring approaches compared to
  traditional software systems?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-ml-systems-differ-traditional-software-7ea8]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{The Efficiency
Framework}\label{sec-introduction-efficiency-framework-4196}

The Bitter Lesson establishes that scale drives AI progress, but it
creates a paradox: if advancing AI requires ever-larger datasets and
compute budgets, how can anyone but the most resource-rich organizations
participate? And even those organizations face physical limits: data
center power constraints, memory bandwidth bottlenecks, and the
diminishing returns of adding more parameters.

Training GPT-4-class models reportedly consumed over 25,000 A100
GPU-days, representing millions of dollars in compute costs and
substantial environmental impact. Many research institutions and
companies simply cannot afford to compete through brute-force scaling.
This reality motivates a complementary approach: rather than asking
``how much more compute can we apply?'' we must also ask ``how
efficiently can we use the compute we have?''

This question defines the efficiency framework. Three complementary
dimensions address the limitations that pure scaling cannot overcome:

\begin{itemize}
\item
  \textbf{Algorithmic efficiency} reduces computational requirements
  through better model design and training procedures. Techniques
  include model compression (pruning, quantization, knowledge
  distillation), efficient architectures designed for resource
  constraints, and neural architecture search that automates the
  discovery of efficient designs.
\item
  \textbf{Compute efficiency} maximizes hardware utilization by aligning
  algorithms with processor capabilities. This dimension encompasses the
  evolution from general-purpose CPUs to specialized accelerators (GPUs,
  TPUs, custom ASICs) and the hardware-software co-design principles
  that determine whether theoretical speedups translate to real-world
  gains.
\item
  \textbf{Data efficiency} extracts more learning signal from limited
  examples, reducing the data requirements that otherwise constrain
  model development. Transfer learning, data augmentation, active
  learning, and self-supervised approaches all contribute to training
  effective models with less labeled data.
\end{itemize}

Together, these dimensions enable sustainable, accessible AI systems
that pure scaling cannot deliver. Figure~\ref{fig-evolution-efficiency}
traces how these three pillars have co-evolved historically, each
progressing through distinct eras at different rates. Data efficiency
has undergone the most dramatic conceptual shift, moving from
scarcity-driven feature engineering to abundance-driven curation.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/45d4f3114f6960aad4f74cd7434a142678a96a0e.pdf}}

}

\caption{\label{fig-evolution-efficiency}\textbf{Historical Efficiency
Trends}: Algorithmic, computational, and data efficiency have each
contributed to substantial gains in AI capabilities, though at different
rates and through distinct eras. Understanding these parallel
trajectories contextualizes the quantitative gains discussed below.}

\end{figure}%

\subsection{Quantitative Evidence for Efficiency
Gains}\label{sec-introduction-quantitative-evidence-efficiency-gains-13ae}

The magnitude of efficiency improvements is substantial and measurable.
Between 2012 and 2019, computational resources needed to train a neural
network to achieve AlexNet-level performance on ImageNet classification
decreased by approximately \(44\times\)
(\citeproc{ref-Hernandez_et_al_2020}{Hernandez, Brown, et al. 2020}).
This improvement, which halved every 16 months, outpaced hardware
efficiency gains predicted by Moore's Law\sidenote{\textbf{Moore's Law}:
Intel co-founder Gordon Moore's 1965 observation that transistor density
on integrated circuits doubles approximately every two years,
historically corresponding to similar improvements in cost-performance
ratios. For ML systems, Moore's Law matters because AI compute demand
has grown far faster: training compute doubled every 3.4 months from
2012-2019, roughly 6x faster than Moore's Law. This gap explains why
specialized AI accelerators (GPUs, TPUs, custom ASICs) became necessary,
and why algorithmic efficiency gains are essential to keep AI
development economically viable. }, demonstrating that algorithmic
innovation drives efficiency as much as hardware advances.

Simultaneously, the compute used in AI training increased approximately
\(300{,}000\times\) from 2012 to 2019, doubling every 3.4 months
(\citeproc{ref-Amodei_et_al_2018}{Amodei, Hernandez, et al. 2018}). This
exponential growth far exceeds Moore's Law and explains why efficiency
optimization is not optional---without it, only the most resource-rich
organizations could participate in AI development. These measurements
emerge from rigorous empirical methodology that tracked training compute
across hundreds of published models; \textbf{?@sec-benchmarking-ai}
develops the measurement frameworks that enable such systematic analysis
of ML system performance.

Figure~\ref{fig-algo-efficiency} visualizes the algorithmic efficiency
trajectory, tracing how innovations from AlexNet through EfficientNet
achieved the 44× improvement.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/42c577fae09fde06f8555750582310668a6e86af.pdf}}

}

\caption{\label{fig-algo-efficiency}: \textbf{Algorithmic Efficiency
Progress}: Neural network training compute requirements decreased
\(44\times\) between 2012 and 2019, outpacing hardware improvements.
This halving of compute every 16 months demonstrates that algorithmic
innovation drives efficiency as much as hardware advances. Source:
(\citeproc{ref-Hernandez_et_al_2020}{Hernandez, Brown, et al. 2020}).}

\end{figure}%

Meanwhile, Figure~\ref{fig-comp_efficiency} shows the countervailing
trend: the exponential growth in compute demand that makes efficiency
optimization essential rather than optional.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8c549866e66746166abe9b10d809fea300f5f447.pdf}}

}

\caption{\label{fig-comp_efficiency}: \textbf{AI Training Compute
Growth}: AI training experienced a 300,000-fold increase in
computational requirements from 2012 to 2019, doubling every 3.4
months---far exceeding Moore's Law
(\citeproc{ref-Amodei_et_al_2018}{Amodei, Hernandez, et al. 2018}). This
exponential growth drives demand for specialized hardware and efficiency
optimization.}

\end{figure}%

Taken together, these two figures on efficiency gains and compute growth
reveal a seeming contradiction that defines the economics of modern AI
development.

\phantomsection\label{callout-perspectiveux2a-1.11}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Efficiency Paradox}
\phantomsection\label{callout-perspective*-1.11}
These two figures reveal a paradox central to ML systems engineering:
\textbf{algorithmic efficiency improved 44× while compute usage grew
300,000×}. Efficiency gains enabled larger experiments, which demanded
more compute, which motivated further efficiency research. This feedback
loop---where efficiency enables scale, and scale demands
efficiency---defines the modern AI engineering landscape. Understanding
this dynamic is essential for making informed decisions about where to
invest optimization effort.

\end{fbx}

The specific techniques for achieving these gains---pruning algorithms,
quantization strategies, knowledge distillation, neural architecture
search, hardware-aware optimization, and efficient training
procedures---are developed systematically in
\textbf{?@sec-model-compression} (algorithmic techniques) and
\textbf{?@sec-ai-acceleration} (hardware foundations).
\textbf{?@sec-data-engineering-ml} addresses data efficiency through
pipeline design and quality optimization.

\subsection{Deployment Context Shapes
Priorities}\label{sec-introduction-deployment-context-shapes-priorities-d8e7}

While the efficiency framework applies universally, the specific
priorities vary dramatically across deployment environments, as shown in
Table~\ref{tbl-efficiency-priorities}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2800}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4900}}@{}}
\caption{\textbf{Efficiency Priorities by Deployment Context}: Different
deployment environments create different bottlenecks, requiring
different optimization strategies. Cloud systems optimize for throughput
and cost; edge systems optimize for memory and power; TinyML systems
require extreme efficiency across all
dimensions.}\label{tbl-efficiency-priorities}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Environment}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Efficiency Focus}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Environment}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Efficiency Focus}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Cloud training} & Cost, throughput & Distributed efficiency,
hardware utilization \\
\textbf{Cloud inference} & Latency, cost per query & Batching, model
serving optimization \\
\textbf{Edge devices} & Memory, power & Model compression,
quantization \\
\textbf{Mobile} & Battery, thermal & Energy-efficient inference \\
\textbf{TinyML} & KB-scale memory, mW power & Extreme compression,
specialized architectures \\
\end{longtable}

Cloud systems with abundant resources prioritize scalability and
throughput, while edge devices face severe memory and power constraints.
Mobile applications must balance performance with battery life, and
TinyML deployments demand extreme resource efficiency. Understanding
these context-specific patterns enables designers to make informed
decisions about which efficiency dimensions to prioritize and how to
address inevitable trade-offs between them.

With efficiency principles established, we turn to how these
optimizations integrate with the broader operational context. Efficiency
techniques do not exist in isolation; they must be applied within the
continuous cycles of data collection, training, deployment, and
monitoring that characterize production ML systems.

\section{Understanding ML System Lifecycle and
Deployment}\label{sec-introduction-understanding-ml-system-lifecycle-deployment-ea09}

The evolution from symbolic reasoning to data-driven learning changed
not only what we build but how we build and operate AI systems. Modern
ML systems operate through specialized lifecycles and deployment
patterns that shape every engineering decision.

\subsection{The ML Development
Lifecycle}\label{sec-introduction-ml-development-lifecycle-f595}

ML systems fundamentally differ from traditional software in their
development and operational lifecycle. Traditional software follows
predictable patterns where developers write explicit instructions that
execute deterministically\sidenote{\textbf{Deterministic Execution}:
Traditional software produces the same output every time given the same
input, like a calculator that always returns 4 when adding 2+2. This
predictability makes testing straightforward---you can verify correct
behavior by checking that specific inputs produce expected outputs. ML
systems, by contrast, are probabilistic: the same model might produce
slightly different predictions due to randomness in inference or changes
in underlying data patterns. }. These systems build on decades of
established practices: version control maintains precise code histories,
continuous integration pipelines\sidenote{\textbf{Continuous
Integration/Continuous Deployment (CI/CD)}: Automated systems that
continuously test code changes and deploy them to production. When
developers commit code, CI/CD pipelines automatically run tests, check
for errors, and if everything passes, deploy the changes to users. For
traditional software, this works reliably; for ML systems, it's more
complex because you must also validate data quality, model performance,
and prediction distribution---not just code correctness. } automate
testing, and static analysis tools measure quality. This mature
infrastructure enables reliable software development following
well-defined engineering principles.

Machine learning systems depart from this paradigm. While traditional
systems execute explicit programming logic, ML systems derive their
behavior from data patterns discovered through training. This shift from
code to data as the primary behavior driver introduces complexities that
existing software engineering practices cannot address. We address these
challenges and specialized workflows in
\textbf{?@sec-ai-development-workflow}.

Unlike traditional software's linear progression from design through
deployment, ML systems operate in continuous cycles.
Figure~\ref{fig-ml_lifecycle_overview} illustrates this iterative
pattern, where performance degradation triggers data collection, which
feeds model training, evaluation, and redeployment.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/23195f3febdd96d15d53652241cb631110bc287e.pdf}}

}

\caption{\label{fig-ml_lifecycle_overview}\textbf{ML System Lifecycle}:
Continuous iteration defines successful machine learning systems,
requiring feedback loops to refine models and address performance
degradation across data collection, model training, evaluation, and
deployment. This cyclical process contrasts with traditional software
development and emphasizes the importance of ongoing monitoring and
adaptation to maintain system reliability and accuracy in dynamic
environments.}

\end{figure}%

The data-dependent nature of ML systems creates dynamic lifecycles
requiring continuous monitoring and adaptation. Unlike source code that
changes only through developer modifications, data reflects real-world
dynamics. Distribution shifts can silently alter system behavior without
any code changes. Traditional tools designed for deterministic
code-based systems prove insufficient for managing such data-dependent
systems. Version control excels at tracking discrete code changes but
struggles with large, evolving datasets. Testing frameworks designed for
deterministic outputs require adaptation for probabilistic predictions.
We address data versioning and quality management in
\textbf{?@sec-data-engineering-ml} and monitoring approaches that handle
probabilistic behaviors in
\textbf{?@sec-machine-learning-operations-mlops}.

In production, lifecycle stages create either virtuous or vicious
cycles. Virtuous cycles emerge when high-quality data enables effective
learning, robust infrastructure supports efficient processing, and
well-engineered systems facilitate better data collection. Vicious
cycles occur when poor data quality undermines learning, inadequate
infrastructure hampers processing, and system limitations prevent data
collection improvements---with each problem compounding the others.

\subsection{The Deployment
Spectrum}\label{sec-introduction-deployment-spectrum-e890}

While the lifecycle stages apply universally to ML systems, their
specific implementation varies dramatically based on deployment
environment. Understanding this deployment spectrum, from the most
powerful data centers to the most constrained embedded devices,
establishes the range of engineering challenges that shape how each
lifecycle stage is realized in practice.

At one end of the spectrum, cloud-based ML systems run in massive data
centers\sidenote{\textbf{Data Centers}: Massive facilities housing
thousands of servers, often consuming 100--300 megawatts of power,
equivalent to a small city. Google operates over 20 data centers
globally, each one costing \$1--2 billion to build. These facilities
maintain temperatures of exactly 80 °F (27 °C) with backup power systems
that can run for days, enabling the reliable operation of AI services
used by billions of people worldwide. }. These systems, including large
language models and recommendation engines, process petabytes of data
while serving millions of users simultaneously. They leverage virtually
unlimited computing resources but manage enormous operational complexity
and costs. \textbf{?@sec-ml-system-architecture} examines the
architectural patterns for building such large-scale systems, while
\textbf{?@sec-ai-acceleration} explores the hardware foundations that
make this scale economically viable.

At the other end, TinyML systems run on
microcontrollers\sidenote{\textbf{Microcontrollers}: Tiny
computers-on-a-chip costing under \$1 each, with just kilobytes of
memory, about 1/millionth the memory of a smartphone. Popular chips like
the Arduino Uno have only 32KB of storage and 2KB of RAM, yet can run
simple AI models that classify sensor data, recognize voice commands, or
detect movement patterns while consuming less power than a digital
watch. } and embedded devices, performing ML tasks with severe memory,
computing power, and energy consumption constraints. Smart home devices
like Alexa or Google Assistant must recognize voice commands using less
power than LED bulbs, while sensors must detect anomalies on battery
power for months or years. The efficiency framework developed earlier in
this chapter (Section~\ref{sec-introduction-efficiency-framework-4196})
introduces the principles underlying constrained deployment, while
\textbf{?@sec-model-compression} provides the specific techniques
(quantization, pruning, distillation) that make TinyML feasible.

Between these extremes lies a rich variety of ML systems adapted for
different contexts. Edge ML systems bring computation closer to data
sources, reducing latency\sidenote{\textbf{Latency}: From Latin
\emph{latere} (to lie hidden, to lurk), the same root as ``latent.'' The
term captures how delay ``hides'' between request and response. In ML
systems, latency is critical: autonomous vehicles need less than 10 ms
latency for safety decisions, while voice assistants target less than
100 ms for natural conversation. Sending data to a distant cloud server
typically adds 50-100 ms of network latency alone, which is why edge
computing became essential for real-time AI applications. } and
bandwidth requirements while managing local computing resources. Mobile
ML systems must balance sophisticated capabilities with severe
constraints. Modern smartphones typically have 4--12 GB RAM, ARM
processors operating at 1.5--3 GHz, and power budgets of 2--5 W that
must be shared across all system functions. For example, running a
state-of-the-art image classification model on a smartphone might
consume 100--500 mW and complete inference in 10--100 ms, compared to
cloud servers that can use 200+ W but deliver results in under 1 ms.
Enterprise ML systems often operate within specific business
constraints, focusing on particular tasks while integrating with
existing infrastructure. Some organizations employ hybrid approaches,
distributing ML capabilities across multiple tiers to balance various
requirements.

\subsection{How Deployment Shapes the
Lifecycle}\label{sec-introduction-deployment-shapes-lifecycle-720f}

The deployment spectrum represents more than different hardware
configurations. Each deployment environment creates an interplay of
requirements, constraints, and trade-offs that impacts every stage of
the ML lifecycle, from initial data collection through continuous
operation and evolution.

Performance requirements often drive initial architectural decisions.
Latency-sensitive applications, like autonomous vehicles or real-time
fraud detection, might require edge or embedded architectures despite
their resource constraints. Conversely, applications requiring massive
computational power for training, such as large language models,
naturally gravitate toward centralized cloud architectures. However, raw
performance is just one consideration in a complex decision space.

Resource management varies dramatically across architectures and
directly impacts lifecycle stages. Cloud systems must optimize for cost
efficiency at scale, balancing expensive GPU clusters, storage systems,
and network bandwidth. This affects training strategies (how often to
retrain models), data retention policies (what historical data to keep),
and serving architectures (how to distribute inference load). Edge
systems face fixed resource limits that constrain model complexity and
update frequency. Mobile and embedded systems operate under the
strictest constraints, where every byte of memory and milliwatt of power
matters, forcing aggressive model compression\sidenote{\textbf{Model
Compression}: Techniques for reducing a model's size and computational
requirements while preserving accuracy, including quantization, pruning,
and knowledge distillation. These methods enable deployment on
resource-constrained devices and are covered systematically in
\textbf{?@sec-model-compression}. } and careful scheduling of training
updates.

Operational complexity increases with system distribution, creating
cascading effects throughout the lifecycle. While centralized cloud
architectures benefit from mature deployment tools and managed services,
edge and hybrid systems must handle distributed system management
complexity. This manifests across all lifecycle stages: data collection
requires coordination across distributed sensors with varying
connectivity; version control must track models deployed across
thousands of edge devices; evaluation needs to account for varying
hardware capabilities; deployment must handle staged rollouts with
rollback capabilities; and monitoring must aggregate signals from
geographically distributed systems. The systematic approaches to
operational excellence, including incident response and debugging
methodologies for production ML systems, are thoroughly addressed in
\textbf{?@sec-machine-learning-operations-mlops}.

Data considerations introduce competing pressures that reshape lifecycle
workflows. Privacy requirements or data sovereignty regulations might
push toward edge or embedded architectures where data stays local,
fundamentally changing data collection and training strategies---perhaps
requiring federated learning\sidenote{\textbf{Federated Learning}: A
training approach where models learn from data distributed across many
devices without centralizing the raw data. This technique enables
privacy-preserving ML by keeping sensitive data on-device while still
benefiting from collective learning. } approaches where models train on
distributed data without centralization. Yet the need for large-scale
training data might favor cloud approaches with centralized data
aggregation. The velocity and volume of data also influence
architectural choices: real-time sensor data might require edge
processing to manage bandwidth during collection, while batch analytics
might be better suited to cloud processing with periodic model updates.

Evolution and maintenance requirements must be considered from the
initial design. Cloud architectures offer flexibility for system
evolution with easy model updates and A/B testing\sidenote{\textbf{A/B
Testing}: A method of comparing two versions of a system by showing
version A to some users and version B to others, then measuring which
performs better. In ML systems, this might mean deploying a new model to
5\% of users while keeping 95\% on the old model, comparing metrics like
accuracy or user engagement before fully rolling out the new version.
This gradual rollout strategy helps catch problems before they affect
all users. }, but can incur significant ongoing costs. Edge and embedded
systems might be harder to update (requiring over-the-air
updates\sidenote{\textbf{Over-the-Air (OTA) Updates}: Wireless software
updates delivered remotely to devices, like how your smartphone installs
new apps without physical connection. For ML systems on embedded devices
or vehicles, OTA updates enable deploying improved models to thousands
or millions of devices without manual intervention. However, updating a
500MB neural network over cellular networks to a fleet of vehicles
requires careful bandwidth management and rollback capabilities if
updates fail. } with careful bandwidth management), but could offer
lower operational overhead. The continuous cycle of ML systems---collect
data, train models, evaluate performance, deploy updates, monitor
behavior---becomes particularly challenging in distributed
architectures, where updating models and maintaining system health
requires careful orchestration across multiple tiers.

These trade-offs are rarely simple binary choices. Modern ML systems
often adopt hybrid approaches, balancing these considerations based on
specific use cases and constraints. For instance, an autonomous vehicle
might perform real-time perception and control at the edge for latency
reasons, while uploading data to the cloud for model improvement and
downloading updated models periodically. A voice assistant might do
wake-word detection on-device to preserve privacy and reduce latency,
but send full speech to the cloud for complex natural language
processing.

The key insight is understanding how deployment decisions ripple through
the entire system lifecycle. A choice to deploy on embedded devices
doesn't just constrain model size, it affects data collection strategies
(what sensors are feasible), training approaches (whether to use
federated learning), evaluation metrics (accuracy vs.~latency
vs.~power), deployment mechanisms (over-the-air updates), and monitoring
capabilities (what telemetry can be collected). These interconnected
decisions demonstrate the DAM in practice, where constraints in one
component create cascading effects throughout the system.

To make these abstract trade-offs concrete, we examine three production
systems that represent the extremes of the deployment spectrum. Each
system faces the same fundamental challenges---data quality, model
complexity, and machine scale---but the constraints of their deployment
environments force radically different engineering solutions.

\phantomsection\label{quiz-question-sec-introduction-understanding-ml-system-lifecycle-deployment-ea09}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.7}{}
\phantomsection\label{quiz-question-sec-introduction-understanding-ml-system-lifecycle-deployment-ea09}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is a key difference between the lifecycle of ML systems and
  traditional software systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems require continuous monitoring and adaptation.
  \item
    Traditional software systems rely on data for behavior.
  \item
    ML systems have a linear development process.
  \item
    Traditional software systems are probabilistic in nature.
  \end{enumerate}
\item
  How does the deployment environment influence the ML system lifecycle?
\item
  Order the following stages of the ML system lifecycle as depicted in
  the section: (1) Model Training, (2) Model Deployment, (3) Model
  Evaluation, (4) Data Collection, (5) Model Monitoring, (6) Data
  Preparation.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-understanding-ml-system-lifecycle-deployment-ea09]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Case Studies in Deployment
Extremes}\label{sec-introduction-case-studies-deployment-extremes-d3dd}

To understand how engineering principles apply across the ML landscape,
we examine three systems representing the extremes of the deployment
spectrum: \textbf{Waymo} (high-stakes hybrid), \textbf{FarmBeats}
(resource-constrained edge), and \textbf{AlphaFold}
(\citeproc{ref-jumper2021highly}{Jumper et al. 2021}) (compute-intensive
cloud). These systems complement the Lighthouse Examples introduced
earlier by illustrating how the same core challenges---data quality,
model complexity, and infrastructure scale---manifest under radically
different constraints. Where the Lighthouse Examples provide recurring
technical workloads for quantitative analysis throughout subsequent
chapters, these case studies establish the diversity of real-world
deployment contexts that ML systems engineers must navigate.

Rather than examining each system in isolation, we analyze them through
the lens of the AI Triad. The same data drift phenomenon that affects
Waymo's perception models in changing weather also affects FarmBeats'
crop disease detection across growing seasons, though the engineering
responses differ dramatically based on machine constraints.

\subsection{Core Engineering Challenges: The AI Triad in
Practice}\label{sec-introduction-core-engineering-challenges-ai-triad-practice-2ab9}

The interdependencies of the AI Triad create specific challenge
categories that define the daily work of an ML systems engineer. By
examining our deployment extremes, we can see these challenges in their
most rigorous forms.

\subsubsection{Data Challenges: Quality, Scale, and
Drift}\label{sec-introduction-data-challenges-quality-scale-drift-6ad5}

\textbf{Data Quality and Heterogeneity} present the first hurdle.
Real-world data is often noisy and inconsistent. Waymo's autonomous
vehicles serve as roving data centers, processing between 1 and 19
terabytes of data per hour across their sensor suite, including
LiDAR\sidenote{\textbf{LiDAR (Light Detection and Ranging)}: A remote
sensing method that uses light in the form of a pulsed laser to measure
ranges (variable distances) to the Earth. },
radar\sidenote{\textbf{Radar (Radio Detection and Ranging)}: A detection
system that uses radio waves to determine the range, angle, or velocity
of objects. }, and cameras. Engineers must solve for sensor
interference, such as rain obscuring cameras, and temporal misalignment
across asynchronous data streams.

\textbf{Scale and Infrastructure} requirements compound these
challenges. Managing the sheer volume of data requires sophisticated
pipelines. While FarmBeats operates under severe constraints---running
inference on models under 500KB transmitted over TV white-space
bandwidth measured in kilobits per second---AlphaFold occupies the
opposite extreme, requiring access to the entire Protein Data Bank
containing over 180,000 experimentally determined structures to predict
configurations for more than 200 million proteins. The challenge lies in
maintaining version control and low-latency access for training across
this vast range of scales.

\textbf{Data Drift} creates an ongoing operational burden. The gradual
change in data patterns over time silently degrades performance. A model
trained on Phoenix's sun-drenched roads may fail in New York's
snowstorms due to distribution shift\sidenote{\textbf{Data Drift}:
Gradual change in input data statistical properties over time.
Production systems at Google reportedly retrain 25\%+ of models monthly
to mitigate this; continuous monitoring is essential for reliability. }.
Detecting these shifts requires continuous monitoring of input
statistics before they manifest as system failures.

\subsubsection{Model Challenges: Complexity and
Generalization}\label{sec-introduction-model-challenges-complexity-generalization-b5cc}

Modern models achieve high performance by scaling parameters, but this
scaling introduces significant implementation costs.

\textbf{Computational Intensity} defines the upper bound of capability.
Training a foundation model like GPT-3 (175B parameters) required an
estimated 314 zettaFLOPs of compute. Even smaller scientific models like
AlphaFold required training on 128 TPUv3 cores for weeks. Systems
engineers must optimize for ``FLOPs per watt'' to make these models
economically and environmentally viable.

\textbf{The Generalization Gap} remains the central algorithmic risk. A
model might achieve 99\% accuracy on benchmarks but only 75\% in the
real world. In safety-critical systems like autonomous driving,
minimizing this gap is a life-or-death requirement. Techniques like
transfer learning and adversarial testing are used to ensure models
remain robust across the long tail of edge cases.

\subsubsection{System Challenges: Latency and
Scale}\label{sec-introduction-system-challenges-latency-scale-6c24}

Getting models to work reliably in the real world requires managing the
``training-serving divide,'' the gap between the flexible environment
where models are born and the rigid environment where they operate.

\textbf{Latency vs.~Throughput} trade-offs dictate architecture. An
autonomous vehicle perception system requires \textless10ms latency for
safety, forcing computation to the \textbf{edge}. Conversely, a protein
folding simulation prioritizes throughput, running for days in the
\textbf{cloud} to explore vast search spaces.

\textbf{Hybrid Coordination} adds complexity. Modern systems often use
tiered architectures. A voice assistant performs wake-word detection
locally (TinyML) to preserve privacy and reduce latency, but offloads
complex natural language processing to massive GPU clusters in the
cloud.

\subsubsection{Ethical and Governance
Considerations}\label{sec-introduction-ethical-governance-considerations-884c}

As systems scale, their impact on society becomes a first-class
engineering concern.

\textbf{Fairness and Bias} must be managed proactively. Models can
unintentionally learn societal biases present in their training data.
Responsible engineering requires systematic auditing of performance
across demographic subgroups to ensure equitable outcomes.

\textbf{Transparency and Privacy} requirements constrain design. Many
deep networks function as ``black boxes.'' In domains like healthcare or
finance, stakeholders require interpretability. Systems must also be
resilient against inference attacks\sidenote{\textbf{Inference Attack}:
A privacy attack extracting sensitive training data information through
model queries. Membership inference determines if specific records were
in the training set, motivating defenses like differential privacy. }
that attempt to extract sensitive training data from model predictions.

\subsection{Understanding Challenge
Interconnections}\label{sec-introduction-understanding-challenge-interconnections-199c}

These case studies illustrate how challenges cascade across the AI
Triad. A decision to use a larger model for better accuracy (Algorithm)
increases the demand for high-bandwidth training machines (Machine) and
more diverse datasets (Data), while potentially increasing inference
latency beyond acceptable limits for edge deployment. The Waymo example
embodies this tension: safety demands high-accuracy perception models,
but the \textless10ms latency requirement prohibits offloading
computation to the cloud, forcing engineers to balance model complexity
against on-vehicle compute constraints.

Managing these cascading trade-offs, quantifying them, optimizing across
them, and monitoring their evolution in production, is the core work of
AI Engineering.

\phantomsection\label{quiz-question-sec-introduction-case-studies-deployment-extremes-d3dd}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.8}{}
\phantomsection\label{quiz-question-sec-introduction-case-studies-deployment-extremes-d3dd}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the primary challenge Waymo
  faces with its data pipeline?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Limited data storage capacity
  \item
    Heterogeneity and real-time processing of data
  \item
    High cost of data transmission
  \item
    Insufficient data collection from sensors
  \end{enumerate}
\item
  Explain how Waymo's use of both edge and cloud computing supports its
  autonomous vehicle operations.
\item
  Waymo's perception system employs specialized \_\_\_\_ to process
  visual data for object detection and tracking.
\item
  True or False: Waymo's infrastructure is designed to prioritize
  computational throughput over latency.
\item
  In a production system like Waymo, what trade-offs might engineers
  consider between sensor accuracy and cost?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-case-studies-deployment-extremes-d3dd]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Organizing ML Systems Engineering: The Five-Pillar
Framework}\label{sec-introduction-organizing-ml-systems-engineering-fivepillar-framework-c452}

The challenges we have explored, from silent performance degradation and
data drift to model complexity and ethical concerns, reveal why ML
systems engineering has emerged as a distinct discipline. Traditional
software engineering practices cannot address systems that degrade
quietly rather than failing obviously. These challenges require
systematic engineering practices spanning the entire system lifecycle
from initial data collection through continuous operation and evolution.

This work organizes ML systems engineering around five interconnected
disciplines that directly address the challenge categories we have
identified. Figure~\ref{fig-pillars} visualizes these pillars, which
represent the core engineering capabilities required to bridge the gap
between research prototypes and production systems capable of operating
reliably at scale. While these pillars organize the \emph{practice} of
ML engineering, they are supported by the foundational technical
imperatives of \textbf{Performance Optimization} and \textbf{Hardware
Acceleration} (covered in Part III), which provide the efficiency
required to make large-scale training and deployment economically and
physically viable.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/introduction/images/png/book_pillars.png}}

}

\caption{\label{fig-pillars}\textbf{Five-Pillar Framework for ML Systems
Engineering}: Machine learning systems engineering encompasses five
interconnected disciplines: Data Engineering, Training Systems,
Deployment Infrastructure, Operations and Monitoring, and Ethics and
Governance. Each pillar addresses specific challenge categories (data
quality, model complexity, serving requirements, silent degradation, and
responsible deployment) while recognizing their interdependencies. This
framework bridges the gap between research prototypes and production
systems capable of operating reliably at scale.}

\end{figure}%

\subsection{The Five Engineering
Disciplines}\label{sec-introduction-five-engineering-disciplines-6ef9}

The five-pillar framework emerged directly from the systems challenges
that distinguish ML from traditional software. Each pillar addresses
specific challenge categories while recognizing their interdependencies.

Alternative organizational frameworks exist. One could organize by
system component (data, model, infrastructure) or by lifecycle phase
(development, deployment, operation). We chose the five-pillar structure
because it aligns with how engineering teams are typically organized in
industry, with specialized roles for data engineering, training
infrastructure, deployment, operations, and responsible AI practices.
Notably, the Ethics pillar ensures that responsible engineering is
treated as an explicit discipline rather than distributed implicitly
across other areas, where it might be overlooked under deadline
pressure.

Data Engineering (\textbf{?@sec-data-engineering-ml}) addresses the
data-related challenges we identified: quality assurance, scale
management, drift detection, and distribution shift. This pillar
encompasses building robust data pipelines that ensure quality, handle
massive scale, maintain privacy, and provide the infrastructure upon
which all ML systems depend. For systems like Waymo, this means managing
terabytes of sensor data per vehicle, validating data quality in
real-time, detecting distribution shifts across different cities and
weather conditions, and maintaining data lineage for debugging and
compliance. The techniques covered include data versioning, quality
monitoring, drift detection algorithms, and privacy-preserving data
processing.

Training Systems (\textbf{?@sec-ai-training}) tackles the model-related
challenges around complexity and scale. This pillar covers developing
training systems that can manage large datasets and complex models while
optimizing computational resource utilization across distributed
environments. Modern foundation models require coordinating thousands of
GPUs, implementing parallelization strategies, managing training
failures and restarts, and balancing training costs against model
quality. The chapter explores distributed training architectures,
optimization algorithms, hyperparameter tuning at scale, and the
frameworks that make large-scale training practical.

Deployment Infrastructure (\textbf{?@sec-benchmarking-ai},
\textbf{?@sec-machine-learning-operations-mlops}) addresses
system-related challenges around the training-serving divide and
operational complexity. This pillar encompasses measuring and optimizing
inference performance across deployment tiers, from cloud to edge
devices. \textbf{?@sec-benchmarking-ai} covers inference metrics,
latency analysis, and the MLPerf scenarios that characterize different
deployment contexts. \textbf{?@sec-machine-learning-operations-mlops}
covers A/B testing, staged rollouts, and operational playbooks for
production systems.

Operations and Monitoring
(\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-benchmarking-ai}) directly addresses the silent
performance degradation patterns we identified as distinctive to ML
systems. This pillar covers creating monitoring and maintenance systems
that ensure continued performance, enable early issue detection, and
support safe system updates in production. Unlike traditional software
monitoring focused on infrastructure metrics, ML operations requires the
four-dimensional monitoring we discussed: infrastructure health, model
performance, data quality, and business impact. The chapter explores
metrics design, alerting strategies, incident response procedures,
debugging techniques for production ML systems, and continuous
evaluation approaches that catch degradation before it impacts users.

Ethics and Governance (\textbf{?@sec-responsible-engineering}) addresses
the ethical and societal challenges around fairness, transparency,
privacy, and safety. This pillar implements responsible engineering
practices throughout the system lifecycle rather than treating them as
an afterthought. This book introduces core methods and workflows, and
the companion book extends these ideas to governance and deployment at
scale.

\subsection{Connecting Components, Lifecycle, and
Disciplines}\label{sec-introduction-connecting-components-lifecycle-disciplines-75cf}

The five pillars emerge naturally from the AI Triad and lifecycle stages
we established earlier. Each AI Triad component maps to specific
pillars: Data Engineering handles the data component's full lifecycle;
Training Systems and Deployment Infrastructure address how algorithms
interact with machines during different lifecycle phases; Operations
bridges all components by monitoring their interactions; Ethics \&
Governance cuts across all components, ensuring responsible practices
throughout.

The challenge categories we identified find their solutions within
specific pillars: Data challenges → Data Engineering. Algorithm
challenges → Training Systems. Machine challenges → Deployment
Infrastructure and Operations. Ethical challenges → Ethics \&
Governance. As we established with the AI Triad, these pillars must
coordinate rather than operate in isolation.

This structure reflects how AI evolved from algorithm-centric research
to systems-centric engineering, shifting focus from ``can we make this
algorithm work?'' to ``can we build systems that reliably deploy,
operate, and maintain these algorithms at scale?'' The five pillars
represent the engineering capabilities required to answer ``yes.''

\subsection{Future Directions in ML Systems
Engineering}\label{sec-introduction-future-directions-ml-systems-engineering-cb49}

While these five pillars provide a stable framework for ML systems
engineering, the field continues evolving. Understanding current trends
helps anticipate how the core challenges and trade-offs will manifest in
future systems.

Application-level innovation increasingly features agentic systems that
move beyond reactive prediction to autonomous action. Systems that can
plan, reason, and execute complex tasks introduce new requirements for
decision-making frameworks and safety constraints. These advances don't
eliminate the five pillars but increase their importance: autonomous
systems that can take consequential actions require even more rigorous
data quality, more reliable deployment infrastructure, more
comprehensive monitoring, and stronger ethical safeguards.

System architecture evolution addresses sustainability and efficiency
concerns that have become critical as models scale. Innovation in model
compression, efficient training techniques, and specialized hardware
stems from both environmental and economic pressures. Future
architectures must balance the pursuit of more powerful models against
growing resource constraints. These efficiency innovations primarily
impact Training Systems and Deployment Infrastructure pillars,
introducing new techniques like quantization, pruning, and neural
architecture search that optimize for multiple objectives
simultaneously.

Infrastructure advances continue reshaping deployment possibilities.
Specialized AI accelerators are emerging across the spectrum from
powerful data center chips to efficient edge processors. This
heterogeneous computing landscape enables dynamic model distribution
across tiers based on capabilities and conditions, blurring traditional
boundaries between cloud, edge, and embedded systems. These
infrastructure innovations affect how all five pillars operate---new
hardware enables new algorithms, which require new training approaches,
which demand new monitoring strategies.

Democratization of AI technology is making ML systems more accessible to
developers and organizations of all sizes. Cloud providers offer
pre-trained models and automated ML platforms that reduce the expertise
barrier for deploying AI solutions. This accessibility trend doesn't
diminish the importance of systems engineering---if anything, it
increases demand for robust, reliable systems that can operate without
constant expert oversight. The five pillars become even more critical as
ML systems proliferate into domains beyond traditional tech companies.

These trends share a common theme: they create ML systems that are more
capable and widespread but also more complex to engineer reliably. The
five-pillar framework provides the foundation for addressing this
landscape, and while specific techniques within each pillar will
continue advancing, the underlying systems thinking required to navigate
them remains constant.

What does it mean to develop this systems thinking? The knowledge
required for ML systems engineering differs fundamentally from the
knowledge developed in theoretical computer science courses.

\subsection{The Nature of Systems
Knowledge}\label{sec-introduction-nature-systems-knowledge-2e9b}

Machine learning systems engineering differs from purely theoretical
computer science disciplines. While fields like algorithms, complexity
theory, or formal verification build knowledge through mathematical
proofs and rigorous derivations, ML systems engineering is a practice, a
craft learned through building, deploying, and maintaining systems at
scale. This distinction becomes apparent in topics like MLOps, where
you'll encounter fewer theorems and more battle-tested patterns that
have emerged from production experience. The knowledge here is not about
proving optimal solutions exist but about recognizing which approaches
work reliably under real-world constraints.

This practical orientation reflects ML systems engineering's nature as a
systems discipline. Like other engineering fields---civil, electrical,
mechanical---the core challenge lies in managing complexity and
trade-offs rather than deriving closed-form solutions. You'll learn to
reason about latency versus accuracy trade-offs, to recognize when data
quality issues will undermine even sophisticated models, to anticipate
how infrastructure choices propagate through entire system
architectures. This systems thinking develops through experience with
concrete scenarios, debugging production failures, and understanding why
certain design patterns persist across different applications.

The implication for learning is significant: mastery comes through
building intuition about patterns, understanding trade-off spaces, and
recognizing how different system components interact. When you read
about monitoring strategies or deployment architectures, the goal isn't
memorizing specific configurations but developing judgment about which
approaches suit which contexts. This book provides the frameworks,
principles, and representative examples, but expertise ultimately
develops through applying these concepts to real problems, making
mistakes, and building the pattern recognition that distinguishes
experienced systems engineers from those who only understand individual
components.

Building this intuition requires understanding what works, but equally
important is recognizing what does not. Experienced practitioners have
accumulated hard-won knowledge about common misconceptions, intuitions
that seem reasonable but lead to failed systems. These fallacies and
pitfalls, distilled from industry experience, help newcomers avoid
mistakes that have derailed countless ML projects.

\section{Fallacies and
Pitfalls}\label{sec-introduction-fallacies-pitfalls-230d}

The following fallacies and pitfalls emerge from intuitions developed in
adjacent fields, such as mathematics, traditional software engineering,
and academic research, that do not transfer cleanly to machine learning
systems.

\textbf{Fallacy:} \emph{Better algorithms automatically produce better
systems.}

Engineers assume algorithmic sophistication drives system performance.
This assumption ignores the Iron Law: novel architectures often increase
Total Operations (\(Ops\)) or model size (\(D\)) non-linearly. If
hardware Peak Performance (\(P\)) or Bandwidth (\(B\)) remain constant,
the ``better'' algorithm disproportionately increases Latency (\(L\)). A
state-of-the-art Vision Transformer achieves 1-2\% higher accuracy than
ResNet-50 on ImageNet but requires 4× the FLOPs and 3× the memory
bandwidth (\citeproc{ref-dosovitskiy2020image}{Dosovitskiy et al.
2021}). Per the Iron Law, this accuracy gain comes at a steep systems
cost.

In production, latency penalties destroy value. Google's search
experiments demonstrated that 100ms of added latency reduced revenue by
0.6\% (\citeproc{ref-brutlag2009speed}{Brutlag 2009}). Amazon found that
every 100ms of latency cost 1\% in sales
(\citeproc{ref-linden2006marissa}{Linden 2006}). A model that is 1\%
more accurate but forces \(L > L_{SLA}\) (e.g., 200ms) has effectively
zero utility. EfficientNet-B0 achieves comparable accuracy to ResNet-50
with one-tenth the FLOPs (\citeproc{ref-tan2019efficientnet}{Tan and Le
2019}), illustrating that ``better'' in a systems context means
optimizing \(Ops\)/Accuracy, not just Accuracy.

The Bitter Lesson
(Section~\ref{sec-introduction-bitter-lesson-systems-engineering-matters-46a1})
reinforces one aspect: Rich Sutton's analysis shows that leveraging
compute scale consistently outperforms hand-crafted algorithmic
complexity (\citeproc{ref-sutton2019bitter}{Sutton 2019}). But scale
requires infrastructure. Google's analysis of ML system failures found
that only 5\% of production code is the model itself; the remaining 95\%
is data pipelines, serving infrastructure, and monitoring
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). A
well-engineered system with a simpler model and robust infrastructure
consistently outperforms a state-of-the-art architecture lacking
monitoring. Organizations that allocate 80\% of resources to algorithm
research and 20\% to systems engineering repeatedly miss deployment
deadlines.

\textbf{Pitfall:} \emph{Treating ML systems as traditional software that
happens to include a model.}

Engineers apply traditional software testing and deployment practices to
ML systems. In production, these systems fail in fundamentally different
ways
(Section~\ref{sec-introduction-ml-systems-differ-traditional-software-7ea8}).
Traditional software bugs produce stack traces within milliseconds and
get fixed within hours; ML systems silently degrade for 3-6 months
before accuracy drops become noticeable, with mean time to detection
exceeding 90 days. A/B tests in conventional software show clear winner
signals within 2-3 days; ML system comparisons require 3-4 weeks to
detect 2-3\% accuracy differences across diverse subpopulations. Unit
tests verify 100\% of code paths in traditional systems; ML systems
require monitoring infrastructure that catches the 5-10\% of predictions
where models hallucinate or produce nonsensical outputs. Teams that
deploy ML systems using only CI/CD pipelines without drift detection
experience silent failures affecting 15-25\% of predictions before
intervention.

\textbf{Fallacy:} \emph{High accuracy on
benchmark\sidenote{\textbf{Benchmark}: Originally a surveyor's term from
the 1830s, referring to marks chiseled into stone walls or posts to
serve as reference points for elevation measurements. A surveyor's
leveling rod would rest on a ``bench'' (bracket) at the mark. In
computing, the term was adopted in the 1970s to describe standardized
tests for comparing system performance. ML benchmarks like ImageNet or
GLUE serve the same purpose: providing fixed reference points against
which to measure progress, though as this fallacy warns, they can
diverge significantly from real-world performance. } datasets indicates
production readiness.}

Engineers assume benchmark performance predicts production accuracy. In
deployment, distribution shift and operational differences cause
substantial degradation. A sentiment analysis model achieving 94\%
accuracy on curated test data drops to 78-82\% accuracy in production as
users employ slang, emojis, and context absent from benchmarks. The
deployment spectrum
(Section~\ref{sec-introduction-deployment-spectrum-e890}) shows that
cloud, edge, and mobile environments each introduce unique constraints:
network latency adds 50-200ms overhead, mobile quantization reduces
accuracy by 2-5\%, and edge devices lack the memory for ensemble
techniques that boosted benchmark scores. Production systems require
failure mode analysis across demographic subgroups where performance may
vary by 10-15 percentage points, monitoring infrastructure to detect
drift, and validation protocols that match actual operating conditions
rather than idealized test sets.

\textbf{Pitfall:} \emph{Optimizing individual components without
considering system interactions.}

Engineers optimize models for inference latency in isolation. In
production, system-wide effects dominate performance. A team reduces
model inference time from 45ms to 15ms through quantization, expecting
proportional end-to-end improvement. However, preprocessing consumes
60ms and postprocessing adds 25ms, so total latency decreases only from
130ms to 100ms---a 23\% improvement rather than the expected 67\%. The
DAM framework and the ``Samples per Dollar'' perspective show that data,
algorithms, and machines form interdependent systems where optimizing
one component often shifts bottlenecks rather than eliminating them.
Switching to a more accurate model that requires 3x more preprocessing
can increase total cost by 40\% while improving accuracy by only 2\%. A
cheaper inference solution that crashes 0.5\% of the time costs more
than premium infrastructure: at 1M requests per day, 5,000 failures
requiring 30-second retries plus customer support at \$5 per incident
costs \$25,000 daily versus \$8,000 for reliable infrastructure. Teams
that optimize components independently waste 30-50\% of engineering
effort on changes that fail to improve end-to-end metrics.

\textbf{Fallacy:} \emph{ML systems can be deployed once and left to run
indefinitely.}

Engineers assume deployed ML systems maintain performance like
traditional software. In production, accuracy degrades as the world
changes around static models. A recommendation system trained on 2019
user behavior dropped from 82\% to 68\% accuracy within 6 months of 2020
deployment as pandemic behavior shifted purchasing patterns. The ML
development lifecycle
(Section~\ref{sec-introduction-ml-development-lifecycle-f595}) shows
continuous monitoring and retraining as operational requirements, not
optional enhancements. Fraud detection models degrade 5-10\% per quarter
as attackers adapt to detection patterns. Natural language systems
experience 3-8\% annual accuracy decline from vocabulary drift and
evolving slang. Without monitoring infrastructure, systems appear to run
successfully while silently failing on 20-30\% of requests.
Organizations that treat deployment as a one-time event rather than
ongoing operation discover failures only through customer complaints
months after degradation begins.

\textbf{Pitfall:} \emph{Assuming that ML expertise alone is sufficient
for ML systems engineering.}

Organizations hire ML researchers and expect production-ready systems.
In reality, the five engineering disciplines
(Section~\ref{sec-introduction-five-engineering-disciplines-6ef9})
require integrated expertise across ML algorithms, software engineering,
systems design, and operations. A team of PhD researchers with 95\%
benchmark accuracy struggled for 8 months to deploy a model because they
lacked experience with API design, database optimization, and monitoring
infrastructure, ultimately shipping a system that served 3 requests per
second instead of the required 100. Conversely, experienced software
engineers without ML understanding built technically sound
infrastructure that inadvertently introduced preprocessing bugs causing
12\% accuracy degradation that went undetected for 4 months. Industry
surveys show that 60-70\% of ML projects fail due to insufficient
systems engineering expertise, not algorithmic limitations. Effective
teams combine ML researchers, software engineers, and operations
specialists rather than expecting any single role to master all required
skills.

\section{The Structure of This
Textbook}\label{sec-introduction-structure-textbook-654a}

This textbook organizes around three imperatives: build, optimize, and
deploy. The structure progresses from foundational concepts through
model development to production deployment, following a key pedagogical
principle of establishing context and process before theory.
Table~\ref{tbl-vol1-structure} outlines this four-part organization,
which moves systematically from understanding the ML landscape through
building and optimizing models to deploying production systems.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1469}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2308}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.6014}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0070}}@{}}
\caption{\textbf{Volume I Organization}: The four parts follow a
pedagogical progression from context (Foundations) through theory
(Build) to practice (Optimize, Deploy). Part I establishes ML systems
context before Part II introduces deep learning principles. Part III
addresses efficiency constraints required for production viability. Part
IV covers deployment and operation of reliable systems at
scale.}\label{tbl-vol1-structure}\tabularnewline
\toprule\noalign{}
\endfirsthead
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Part} :------------------- \textbf{I: Foundations} &
\textbf{Theme} :------------------------------- Context: ML systems
landscape &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.6084} + 2\tabcolsep}@{}}{%
\textbf{Key Chapters}
:-------------------------------------------------------------------------------------
Chapter~\ref{sec-introduction}, \textbf{?@sec-ml-system-architecture},
\textbf{?@sec-ai-development-workflow},
\textbf{?@sec-data-engineering-ml}} \\
\textbf{II: Build} & Theory: Model fundamentals &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.6084} + 2\tabcolsep}@{}}{%
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-ai-training} \textbar{}} \\
\textbf{III: Optimize} & Efficiency: Performance tuning &
\textbf{?@sec-data-efficiency}, \textbf{?@sec-model-compression},
\textbf{?@sec-ai-acceleration}, \textbf{?@sec-benchmarking-ai} & \\
\textbf{IV: Deploy} & Production: Real-world systems &
\textbf{?@sec-model-serving-systems},
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-responsible-engineering}, \textbf{?@sec-conclusion} & \\
\end{longtable}

Part I establishes context by surveying the ML systems landscape.
Chapter~\ref{sec-introduction} develops the engineering revolution in AI
and the frameworks that organize this discipline.
\textbf{?@sec-ml-system-architecture} examines what distinguishes ML
systems from traditional software, introducing unique failure patterns
and lifecycle stages. \textbf{?@sec-ai-development-workflow} presents
the end-to-end process from problem formulation through deployment,
providing the conceptual map that guides subsequent learning.
\textbf{?@sec-data-engineering-ml} addresses data collection,
processing, and management, establishing that data infrastructure
precedes and enables model development.

Part II builds theoretical foundations and practical skills for model
development. \textbf{?@sec-deep-learning-systems-foundations} provides
algorithmic foundations, while \textbf{?@sec-dnn-architectures} extends
these to specific network designs. Both chapters reference the five
\textbf{Lighthouse Examples} introduced above---ResNet-50, GPT-2/Llama,
MobileNet, DLRM, and Keyword Spotting---to anchor abstract concepts in
concrete workloads. \textbf{?@sec-ai-frameworks} examines the software
infrastructure from TensorFlow and PyTorch to specialized tools.
\textbf{?@sec-ai-training} develops training systems for complex models
and large datasets.

Part III addresses optimization for production deployment.
\textbf{?@sec-data-efficiency} introduces techniques for reducing
computational requirements while maintaining quality.
\textbf{?@sec-model-compression} covers compression techniques including
quantization, pruning, and knowledge distillation.
\textbf{?@sec-ai-acceleration} examines specialized hardware from GPUs
to custom ASICs. \textbf{?@sec-benchmarking-ai} establishes
methodologies for measuring and comparing system performance.

Part IV ensures optimized systems operate reliably in production.
\textbf{?@sec-model-serving-systems} covers infrastructure for
delivering predictions with low latency.
\textbf{?@sec-machine-learning-operations-mlops} encompasses practices
from monitoring and deployment to incident response.
\textbf{?@sec-responsible-engineering} addresses ethical considerations
and governance. \textbf{?@sec-conclusion} synthesizes the complete
methodology and prepares you for the transition from single-node mastery
to fleet-scale orchestration.

For detailed guidance on reading paths, learning outcomes,
prerequisites, and how to maximize your experience with this textbook,
refer to the \href{../../frontmatter/about/about.qmd}{About} section.

\begin{tcolorbox}[enhanced jigsaw, colbacktitle=quarto-callout-important-color!10!white, leftrule=.75mm, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, colframe=quarto-callout-important-color-frame, toptitle=1mm, opacityback=0, left=2mm, opacitybacktitle=0.6, bottomtitle=1mm, titlerule=0mm, arc=.35mm, breakable, toprule=.15mm, bottomrule=.15mm, rightrule=.15mm, colback=white]

\begin{itemize}
\item
  \textbf{The AI Triad (DAM) governs all ML systems}: Data, Algorithm,
  and Machine are interdependent. Optimizing one component in isolation
  shifts bottlenecks rather than eliminating them. When performance
  stalls, analysis should examine all three components of the DAM
  framework.
\item
  \textbf{ML systems fail silently rather than explicitly}: Unlike
  traditional software that crashes on errors, ML systems degrade
  gradually as data distributions drift. A model can maintain 100\%
  uptime while accuracy drops 15\% undetected.
\item
  \textbf{System balance constrains performance}: The slowest component
  limits the system. Reducing model inference from 45ms to 15ms yields
  only 23\% end-to-end improvement when preprocessing (60ms) and
  postprocessing (25ms) dominate total latency.
\item
  \textbf{The Bitter Lesson applies}: Scale and compute outperform
  hand-crafted features. Systems that leverage general methods with more
  computation consistently outperform specialized approaches long-term.
\item
  \textbf{Five pillars require integration}: ML systems engineering
  encompasses Data Engineering, Training Systems, Deployment
  Infrastructure, Operations and Monitoring, and Ethics and Governance.
  Teams lacking expertise in any pillar face elevated project failure
  rates.
\end{itemize}

\end{tcolorbox}

This introduction has established the conceptual foundation for
everything that follows. The chapter began by examining the relationship
between artificial intelligence as vision and machine learning as
methodology, then defined machine learning systems as the artifacts that
engineers build: integrated computing systems comprising data,
algorithms, and machines---the AI Triad formalized as the DAM Taxonomy.
Two fundamental principles provide the conceptual backbone for reasoning
about these systems. The \textbf{Principle of System Balance} reveals
that performance is limited by the slowest component, whether compute,
memory bandwidth, or I/O. The \textbf{Degradation Equation} captures how
model performance evolves as data distributions shift, a phenomenon
unique to ML systems that traditional software engineering never
confronts. Through the Bitter Lesson and AI's historical evolution, the
chapter demonstrated why systems engineering has become central to AI
progress and how learning-based approaches came to dominate the field.
This context enabled a formal definition of AI Engineering as a distinct
discipline, following the pattern of Computer Engineering's emergence
and establishing it as the field dedicated to building reliable,
efficient, and scalable machine learning systems across all
computational platforms. The five Lighthouse Examples introduced here
(ResNet-50, GPT-2/Llama, MobileNet, DLRM, and Keyword Spotting) will
serve as recurring touchstones throughout subsequent chapters, grounding
abstract principles in the concrete engineering challenges of real
workloads.

If ML systems differ from traditional software, what makes them
different? Why do they fail in ways that conventional engineering
intuitions cannot anticipate? \textbf{?@sec-ml-system-architecture}
examines these questions systematically, revealing the characteristics
that distinguish ML systems from their traditional counterparts and
establishing why specialized engineering approaches are necessary.

Welcome to AI Engineering.

\section{Self-Check Answers}\label{self-check-answers}

\phantomsection\label{quiz-answer-sec-introduction-artificial-intelligence-vision-machine-learning-practice-feba}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.1}{}
\phantomsection\label{quiz-answer-sec-introduction-artificial-intelligence-vision-machine-learning-practice-feba}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the relationship between
  Artificial Intelligence (AI) and Machine Learning (ML)?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    AI is a subset of ML focused on data-driven techniques.
  \item
    ML is a practical implementation of AI using rule-based systems.
  \item
    AI and ML are completely independent fields.
  \item
    ML is a subset of AI focused on data-driven techniques.
  \end{enumerate}

  \emph{Answer}: The correct answer is D. ML is a subset of AI focused
  on data-driven techniques. AI is the broader goal of creating
  intelligent systems, while ML provides the methods to achieve this
  through learning from data.

  \emph{Learning Objective}: Understand the hierarchical relationship
  between AI and ML.
\item
  \textbf{Explain why machine learning has become the dominant approach
  in achieving AI goals.}

  \emph{Answer}: Machine learning has become dominant because it allows
  systems to automatically discover patterns from data, making them
  adaptable to new situations without the need for explicit programming.
  For example, ML systems can learn to play chess by analyzing game data
  rather than relying on pre-programmed strategies. This adaptability is
  crucial for handling complex, real-world scenarios where rule-based
  systems fall short.

  \emph{Learning Objective}: Analyze the advantages of ML over
  traditional rule-based AI approaches.
\item
  \textbf{Order the following steps in the evolution from symbolic AI to
  machine learning: (1) Encoding human knowledge as rules, (2)
  Discovering patterns from data, (3) Scaling with compute and data
  infrastructure.}

  \emph{Answer}: The correct order is: (1) Encoding human knowledge as
  rules, (2) Discovering patterns from data, (3) Scaling with compute
  and data infrastructure. Initially, AI relied on manually encoded
  rules, but the shift to ML allowed systems to learn from data. This
  evolution further required scaling with infrastructure to handle large
  datasets and complex models.

  \emph{Learning Objective}: Understand the historical progression and
  scaling implications of AI methodologies.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-artificial-intelligence-vision-machine-learning-practice-feba]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-historical-evolution-ai-paradigms-3be8}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.2}{}
\phantomsection\label{quiz-answer-sec-introduction-historical-evolution-ai-paradigms-3be8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following factors did NOT contribute to the
  transition towards a systems-focused approach in AI?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Massive datasets from the internet age
  \item
    The development of symbolic AI in the 1950s
  \item
    Increased availability of low-cost GPUs
  \item
    Algorithmic breakthroughs in deep learning
  \end{enumerate}

  \emph{Answer}: The correct answer is B. The development of symbolic AI
  in the 1950s. While symbolic AI was foundational, the transition to
  systems-focused AI was driven by more recent factors like data,
  algorithms, and hardware improvements.

  \emph{Learning Objective}: Understand the key factors that influenced
  the shift towards a systems-centric approach in AI.
\item
  \textbf{Explain how the convergence of massive datasets, algorithmic
  breakthroughs, and hardware acceleration has transformed AI from an
  academic curiosity to a production technology.}

  \emph{Answer}: The convergence allowed AI to scale effectively:
  massive datasets provided the raw material for learning, algorithmic
  breakthroughs like deep learning enabled models to learn from this
  data, and hardware acceleration made it feasible to train and deploy
  these models at scale. This transformation made AI practical for
  real-world applications, requiring robust engineering practices.

  \emph{Learning Objective}: Analyze the interplay of data, algorithms,
  and hardware in transforming AI into a practical technology.
\item
  \textbf{True or False: The systems-centric approach in AI emerged
  because early AI systems were too complex and required
  simplification.}

  \emph{Answer}: False. The systems-centric approach emerged due to the
  need to handle massive datasets, leverage algorithmic breakthroughs,
  and utilize hardware acceleration, not because early systems were too
  complex.

  \emph{Learning Objective}: Correct misconceptions about the reasons
  for the shift to systems-centric AI.
\item
  \textbf{The introduction of \_\_\_\_ by OpenAI in 2020 demonstrated
  the increasing complexity and capability of AI systems.}

  \emph{Answer}: GPT-3. This model exemplified the scale and capability
  of modern AI systems, requiring significant computational resources
  and showcasing emergent abilities.

  \emph{Learning Objective}: Recall key milestones that illustrate the
  evolution and scaling of AI systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-historical-evolution-ai-paradigms-3be8]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-bitter-lesson-systems-engineering-matters-46a1}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.3}{}
\phantomsection\label{quiz-answer-sec-introduction-bitter-lesson-systems-engineering-matters-46a1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary lesson from 70 years of AI research
  according to Richard Sutton's `Bitter Lesson'?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Leveraging massive computational resources
  \item
    Curating better datasets
  \item
    Developing more sophisticated algorithms
  \item
    Encoding human expertise into AI systems
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Leveraging massive
  computational resources. This is correct because Sutton's `Bitter
  Lesson' emphasizes that general methods using computation are the most
  effective.

  \emph{Learning Objective}: Understand the core insight from the
  `Bitter Lesson' regarding the role of computation in AI success.
\item
  \textbf{Explain why systems engineering has become more critical than
  algorithmic development in modern AI systems.}

  \emph{Answer}: Systems engineering is critical because leveraging
  massive computational resources has proven more effective than
  algorithmic improvements. For example, systems like AlphaGo achieve
  success through computation rather than human expertise. This is
  important because effective scaling of computation determines AI
  progress.

  \emph{Learning Objective}: Analyze the shift in focus from algorithmic
  development to systems engineering in AI.
\item
  \textbf{True or False: The primary constraint in modern ML systems is
  compute capacity rather than memory bandwidth.}

  \emph{Answer}: False. This is false because the primary constraint is
  memory bandwidth, which limits data movement and thus system
  performance.

  \emph{Learning Objective}: Understand the technical constraints in
  modern ML systems, specifically the role of memory bandwidth.
\item
  \textbf{Which factor is NOT a primary challenge in scaling modern AI
  systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Thermal and power constraints
  \item
    Memory bandwidth limitations
  \item
    Data center coordination
  \item
    Algorithmic complexity
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Algorithmic complexity. This
  is correct because the primary challenges are related to
  infrastructure, not the complexity of algorithms.

  \emph{Learning Objective}: Identify the main challenges in scaling AI
  systems from a systems engineering perspective.
\item
  \textbf{In a production system, how might you address the memory
  bandwidth bottleneck when deploying large-scale ML models?}

  \emph{Answer}: To address memory bandwidth bottlenecks, one could use
  high-bandwidth memory (HBM), optimize data movement, or employ
  near-data processing. For example, using specialized accelerators that
  co-locate compute and storage can reduce data transfer times. This is
  important to improve system efficiency and performance.

  \emph{Learning Objective}: Apply knowledge of memory bandwidth
  constraints to practical ML system deployment scenarios.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-bitter-lesson-systems-engineering-matters-46a1]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-defining-ai-engineering-19ce}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.4}{}
\phantomsection\label{quiz-answer-sec-introduction-defining-ai-engineering-19ce}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary focus of AI Engineering as a discipline?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Developing new AI algorithms
  \item
    Improving symbolic reasoning techniques
  \item
    Building reliable, efficient, and scalable ML systems
  \item
    Enhancing user interfaces for AI applications
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Building reliable, efficient,
  and scalable ML systems. AI Engineering focuses on the entire
  lifecycle of ML systems, emphasizing reliability, efficiency, and
  scalability.

  \emph{Learning Objective}: Understand the core focus of AI Engineering
  as a discipline.
\item
  \textbf{Explain how the historical shift from symbolic systems to
  learning-based approaches has influenced the emergence of AI
  Engineering.}

  \emph{Answer}: The shift from symbolic systems to learning-based
  approaches has led to the dominance of machine learning in AI,
  necessitating a focus on systems-level challenges. This has resulted
  in the emergence of AI Engineering, which addresses the lifecycle of
  building and maintaining scalable and efficient ML systems. This is
  important because it reflects the practical realities of deploying AI
  in production environments.

  \emph{Learning Objective}: Analyze the historical context that led to
  the development of AI Engineering.
\item
  \textbf{Which of the following best describes the role of AI
  Engineering in modern AI systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Focusing solely on algorithmic development
  \item
    Developing symbolic AI techniques
  \item
    Designing user-friendly AI interfaces
  \item
    Integrating and optimizing systems for real-world deployment
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Integrating and optimizing
  systems for real-world deployment. AI Engineering involves the
  systems-level integration and optimization necessary for deploying AI
  systems effectively.

  \emph{Learning Objective}: Identify the role of AI Engineering in the
  deployment of AI systems.
\item
  \textbf{In a production system, how might AI Engineering address the
  challenge of energy efficiency while maintaining performance?}

  \emph{Answer}: AI Engineering addresses energy efficiency by
  optimizing data pipelines, utilizing specialized hardware, and
  designing algorithms that reduce computational overhead. For example,
  deploying models on energy-efficient GPUs can maintain performance
  while minimizing power consumption. This is important because energy
  costs are a significant factor in large-scale AI deployments.

  \emph{Learning Objective}: Evaluate how AI Engineering balances energy
  efficiency with system performance.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-defining-ai-engineering-19ce]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-defining-ml-systems-d4af}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.5}{}
\phantomsection\label{quiz-answer-sec-introduction-defining-ml-systems-d4af}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes a machine learning
  system?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    A computing system that integrates Data, Algorithm, and Machine (the
    DAM).
  \item
    A standalone algorithm that processes data.
  \item
    A software application that uses pre-defined rules to make
    decisions.
  \item
    A data storage system optimized for large datasets.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. A computing system that
  integrates Data, Algorithm, and Machine (the DAM). This is correct
  because an ML system encompasses the entire ecosystem where algorithms
  operate, including data, learning algorithms, and machines. The DAM
  Taxonomy formalizes these three interdependent components. Options B,
  C, and D describe only parts of an ML system or unrelated concepts.

  \emph{Learning Objective}: Understand the comprehensive definition of
  a machine learning system.
\item
  \textbf{True or False: In a machine learning system, the model
  architecture does not influence the computational demands for training
  and inference.}

  \emph{Answer}: False. This is false because the model architecture
  directly dictates the computational demands for both training and
  inference, influencing the required infrastructure.

  \emph{Learning Objective}: Recognize the interdependencies between
  model architecture and computing infrastructure in ML systems.
\item
  \textbf{In the context of ML systems, what role does computing
  infrastructure play?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It solely stores and retrieves data.
  \item
    It provides the necessary resources for both training and inference.
  \item
    It is only responsible for serving the model predictions.
  \item
    It determines the model architecture to be used.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. It provides the necessary
  resources for both training and inference. This is correct because
  computing infrastructure enables the operation of models at scale,
  supporting both the learning process and the application of learned
  knowledge. Options A, C, and D are incorrect as they describe
  incomplete or unrelated functions.

  \emph{Learning Objective}: Identify the role of computing
  infrastructure in the operation of ML systems.
\item
  \textbf{Consider a scenario where an ML system's data component is
  limited by storage capacity. How might this affect the other
  components of the system?}

  \emph{Answer}: If the data component is limited by storage capacity,
  it may restrict the volume and variety of data available for training,
  potentially leading to less effective learning algorithms.
  Additionally, the computing infrastructure may be underutilized if it
  cannot process larger datasets. This interdependency emphasizes the
  need for balanced system design to optimize overall performance.

  \emph{Learning Objective}: Analyze the interdependencies between data,
  algorithms, and computing infrastructure in ML systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-defining-ml-systems-d4af]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-ml-systems-differ-traditional-software-7ea8}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.6}{}
\phantomsection\label{quiz-answer-sec-introduction-ml-systems-differ-traditional-software-7ea8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the fundamental difference in failure modes between
  traditional software and ML systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Traditional software crashes visibly while ML systems can degrade
    silently without triggering alerts.
  \item
    Traditional software requires more monitoring than ML systems.
  \item
    ML systems always fail faster than traditional software.
  \item
    Traditional software cannot handle errors while ML systems have
    built-in error recovery.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Traditional software crashes
  visibly while ML systems can degrade silently without triggering
  alerts. This is correct because traditional software exhibits explicit
  failure modes with error messages and alerts, while ML systems can
  continue operating with declining performance due to data distribution
  shifts without triggering conventional error detection mechanisms.
  Options B, C, and D misrepresent the actual differences in failure
  characteristics.

  \emph{Learning Objective}: Distinguish between explicit failure modes
  in traditional software and silent degradation in ML systems.
\item
  \textbf{Explain how the concept of `silent performance degradation'
  differentiates machine learning systems from traditional software
  systems.}

  \emph{Answer}: Silent performance degradation in ML systems refers to
  the phenomenon where a system continues to operate without obvious
  errors, but its performance gradually declines. Unlike traditional
  software, which fails visibly, ML systems may degrade due to changes
  in data distribution or model drift, requiring careful monitoring to
  detect and address these issues. This is important because it
  highlights the need for specialized operational practices in ML system
  deployment.

  \emph{Learning Objective}: Understand the concept of silent
  performance degradation and its implications for ML system operation.
\item
  \textbf{True or False: ML systems can maintain optimal performance
  without specialized monitoring approaches beyond traditional software
  metrics.}

  \emph{Answer}: False. ML systems require specialized monitoring beyond
  traditional software metrics because they can degrade silently due to
  data distribution changes, model drift, or environmental shifts
  without triggering conventional error detection. Unlike traditional
  software that fails visibly, ML systems may continue operating with
  declining performance, necessitating comprehensive monitoring of model
  behavior, data quality, and prediction patterns.

  \emph{Learning Objective}: Understand why ML systems require
  specialized monitoring approaches beyond traditional software metrics.
\item
  \textbf{Why do ML systems require different monitoring approaches
  compared to traditional software systems?}

  \emph{Answer}: ML systems require monitoring of infrastructure health,
  model performance, data quality, and prediction distributions, whereas
  traditional software monitoring focuses primarily on infrastructure
  metrics like uptime and latency. This is necessary because ML systems
  can degrade due to data distribution shifts, model drift, or
  environmental changes without any code changes or infrastructure
  failures. For example, a recommendation system's accuracy might
  decline as user preferences evolve, requiring monitoring beyond
  traditional infrastructure metrics. This comprehensive monitoring
  enables early detection of performance degradation before it impacts
  users.

  \emph{Learning Objective}: Analyze the unique monitoring requirements
  that distinguish ML systems from traditional software engineering
  practices.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-ml-systems-differ-traditional-software-7ea8]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-understanding-ml-system-lifecycle-deployment-ea09}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.7}{}
\phantomsection\label{quiz-answer-sec-introduction-understanding-ml-system-lifecycle-deployment-ea09}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is a key difference between the lifecycle of ML systems
  and traditional software systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems require continuous monitoring and adaptation.
  \item
    Traditional software systems rely on data for behavior.
  \item
    ML systems have a linear development process.
  \item
    Traditional software systems are probabilistic in nature.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. ML systems require continuous
  monitoring and adaptation. This is because ML systems are data-driven
  and must adjust to changes in data patterns, unlike traditional
  software which follows deterministic logic.

  \emph{Learning Objective}: Understand the fundamental differences in
  lifecycle between ML systems and traditional software.
\item
  \textbf{How does the deployment environment influence the ML system
  lifecycle?}

  \emph{Answer}: The deployment environment dictates resource
  availability, operational complexity, and data management strategies.
  For example, cloud deployments offer scalability but incur high costs,
  while edge deployments reduce latency but face resource constraints.
  These factors influence data collection, model training, and update
  strategies, impacting the entire lifecycle.

  \emph{Learning Objective}: Analyze how different deployment
  environments affect the ML system lifecycle.
\item
  \textbf{Order the following stages of the ML system lifecycle as
  depicted in the section: (1) Model Training, (2) Model Deployment, (3)
  Model Evaluation, (4) Data Collection, (5) Model Monitoring, (6) Data
  Preparation.}

  \emph{Answer}: The correct order is: (4) Data Collection, (6) Data
  Preparation, (1) Model Training, (3) Model Evaluation, (2) Model
  Deployment, (5) Model Monitoring. This order reflects the iterative
  nature of ML systems, emphasizing continuous feedback and adaptation.

  \emph{Learning Objective}: Reinforce understanding of the cyclical
  nature of the ML system lifecycle.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-understanding-ml-system-lifecycle-deployment-ea09]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-case-studies-deployment-extremes-d3dd}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.8}{}
\phantomsection\label{quiz-answer-sec-introduction-case-studies-deployment-extremes-d3dd}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the primary challenge
  Waymo faces with its data pipeline?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Limited data storage capacity
  \item
    Heterogeneity and real-time processing of data
  \item
    High cost of data transmission
  \item
    Insufficient data collection from sensors
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Heterogeneity and real-time
  processing of data. Waymo's data pipeline must handle both structured
  and unstructured data in real-time, which requires sophisticated
  processing due to the safety-critical nature of autonomous driving.

  \emph{Learning Objective}: Understand the data challenges faced by ML
  systems in real-world applications like autonomous vehicles.
\item
  \textbf{Explain how Waymo's use of both edge and cloud computing
  supports its autonomous vehicle operations.}

  \emph{Answer}: Waymo uses edge computing to process sensor data and
  make real-time decisions on the vehicle, while cloud computing
  supports large-scale model training and simulation. This combination
  allows for real-time responsiveness and extensive learning
  capabilities. For example, edge computing enables immediate reaction
  to road conditions, whereas cloud computing facilitates continuous
  improvement of driving models. This is important because it balances
  the need for immediate action with the capacity for long-term
  learning.

  \emph{Learning Objective}: Analyze the role of edge and cloud
  computing in supporting complex ML systems in real-time applications.
\item
  \textbf{Waymo's perception system employs specialized \_\_\_\_ to
  process visual data for object detection and tracking.}

  \emph{Answer}: neural networks. Waymo uses neural networks to
  interpret visual data from its sensors, which is crucial for detecting
  and tracking objects in the vehicle's environment.

  \emph{Learning Objective}: Recall the specific technologies used in ML
  systems for perception tasks.
\item
  \textbf{True or False: Waymo's infrastructure is designed to
  prioritize computational throughput over latency.}

  \emph{Answer}: False. Waymo's infrastructure prioritizes latency to
  ensure real-time decision-making capability in safety-critical
  environments like autonomous driving.

  \emph{Learning Objective}: Understand the infrastructure priorities
  for real-time ML systems in safety-critical applications.
\item
  \textbf{In a production system like Waymo, what trade-offs might
  engineers consider between sensor accuracy and cost?}

  \emph{Answer}: Engineers must balance sensor accuracy with cost to
  ensure that the system remains economically viable while maintaining
  safety. For instance, LiDAR provides high accuracy but is expensive,
  so engineers might use a combination of LiDAR and cheaper sensors like
  radar to achieve a cost-effective solution. This trade-off is crucial
  because it affects both the system's performance and its scalability.

  \emph{Learning Objective}: Evaluate the trade-offs involved in
  choosing sensor technologies for ML systems in autonomous vehicles.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-case-studies-deployment-extremes-d3dd]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-Amodei_et_al_2018}
Amodei, Dario, Danny Hernandez, et al. 2018. {``AI and Compute.''}
\emph{OpenAI Blog}. \url{https://openai.com/research/ai-and-compute}.

\bibitem[\citeproctext]{ref-bobrow1964student}
Bobrow, Daniel G. 1964. {``Natural Language Input for a Computer Problem
Solving System.''} \emph{PhD Thesis, MIT}.
\url{https://dspace.mit.edu/handle/1721.1/12962}.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} Edited by Hugo Larochelle, Marc'Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin.
\emph{Advances in Neural Information Processing Systems} 33 (May):
1877--1901. \url{https://doi.org/10.48550/arxiv.2005.14165}.

\bibitem[\citeproctext]{ref-brutlag2009speed}
Brutlag, Jake. 2009. {``Speed Matters for Google Web Search.''} Google
Research Blog. \url{https://research.google/blog/speed-matters/}.

\bibitem[\citeproctext]{ref-campbell2002deep}
Campbell, Murray, Jr. Hoane A.Joseph, and Feng-hsiung Hsu. 2002. {``Deep
Blue.''} \emph{Artificial Intelligence} 134 (1-2): 57--83.
\url{https://doi.org/10.1016/s0004-3702(01)00129-1}.

\bibitem[\citeproctext]{ref-deng2009imagenet}
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
2009. {``ImageNet: A Large-Scale Hierarchical Image Database.''} In
\emph{2009 IEEE Conference on Computer Vision and Pattern Recognition},
248--55. Ieee; IEEE. \url{https://doi.org/10.1109/cvpr.2009.5206848}.

\bibitem[\citeproctext]{ref-dosovitskiy2020image}
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.
2021. {``An Image Is Worth 16x16 Words: Transformers for Image
Recognition at Scale.''} \emph{International Conference on Learning
Representations (ICLR)}. \url{https://arxiv.org/abs/2010.11929}.

\bibitem[\citeproctext]{ref-halevy2009unreasonable}
Halevy, Alon, Peter Norvig, and Fernando Pereira. 2009. {``The
Unreasonable Effectiveness of Data.''} \emph{IEEE Intelligent Systems}
24 (2): 8--12. \url{https://doi.org/10.1109/MIS.2009.36}.

\bibitem[\citeproctext]{ref-Hernandez_et_al_2020}
Hernandez, Danny, Tom B. Brown, et al. 2020. {``Measuring the
Algorithmic Efficiency of Neural Networks.''} \emph{OpenAI Blog}.
\url{https://openai.com/research/ai-and-efficiency}.

\bibitem[\citeproctext]{ref-jumper2021highly}
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael
Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.
{``Highly Accurate Protein Structure Prediction with AlphaFold.''}
\emph{Nature} 596 (7873): 583--89.
\url{https://doi.org/10.1038/s41586-021-03819-2}.

\bibitem[\citeproctext]{ref-karpathy2017software}
Karpathy, Andrej. 2017. {``Software 2.0.''} \emph{Medium}.
\url{https://karpathy.medium.com/software-2-0-a64152b37c35}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-kuhn1962structure}
Kuhn, Thomas S. 1962. \emph{The Structure of Scientific Revolutions}.
Chicago: University of Chicago Press.

\bibitem[\citeproctext]{ref-linden2006marissa}
Linden, Greg. 2006. {``Marissa Mayer at Web 2.0.''} Greg Linden's Blog.
\url{https://web.archive.org/web/20090803010539/glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html}.

\bibitem[\citeproctext]{ref-mcculloch1943logical}
McCulloch, Warren S., and Walter Pitts. 1943. {``A Logical Calculus of
the Ideas Immanent in Nervous Activity.''} \emph{Bulletin of
Mathematical Biophysics} 5: 115--33.
\url{https://doi.org/10.1007/BF02478259}.

\bibitem[\citeproctext]{ref-patterson2021carbon}
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel
Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.
{``Carbon Emissions and Large Neural Network Training.''} \emph{arXiv
Preprint arXiv:2104.10350}, April.
\url{http://arxiv.org/abs/2104.10350v3}.

\bibitem[\citeproctext]{ref-rosenblatt1957perceptron}
Rosenblatt, Frank. 1957. {``The Perceptron: A Perceiving and Recognizing
Automaton.''} 85-460-1. Cornell Aeronautical Laboratory.

\bibitem[\citeproctext]{ref-sculley2015hidden}
Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,
Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and
Dan Dennison. 2021. {``Technical Debt in Machine Learning Systems.''} In
\emph{Technical Debt in Practice}, 28:177--92. The MIT Press.
\url{https://doi.org/10.7551/mitpress/12440.003.0011}.

\bibitem[\citeproctext]{ref-shortliffe1976mycin}
Shortliffe, Edward H., Randall Davis, Stanton G. Axline, Bruce G.
Buchanan, C.Cordell Green, and Stanley N. Cohen. 1975. {``Computer-Based
Consultations in Clinical Therapeutics: Explanation and Rule Acquisition
Capabilities of the MYCIN System.''} \emph{Computers and Biomedical
Research} 8 (4): 303--20.
\url{https://doi.org/10.1016/0010-4809(75)90009-9}.

\bibitem[\citeproctext]{ref-silver2016mastering}
Silver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,
George van den Driessche, Julian Schrittwieser, et al. 2016.
{``Mastering the Game of Go with Deep Neural Networks and Tree
Search.''} \emph{Nature} 529 (7587): 484--89.
\url{https://doi.org/10.1038/nature16961}.

\bibitem[\citeproctext]{ref-statista2024email}
Statista. 2024. {``Number of Sent and Received Emails Per Day Worldwide
from 2017 to 2026.''}
\textless https://www.statista.com/statistics/456500/daily-number-of-e-mails-worldwide/\textgreater.

\bibitem[\citeproctext]{ref-sutton2019bitter}
Sutton, Richard S. 2019. {``The Bitter Lesson.''}
\url{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}.

\bibitem[\citeproctext]{ref-tan2019efficientnet}
Tan, Mingxing, and Quoc V Le. 2019. {``EfficientNet: Rethinking Model
Scaling for Convolutional Neural Networks.''} In \emph{International
Conference on Machine Learning (ICML)}, 6105--14.

\bibitem[\citeproctext]{ref-viola2001rapidobject}
Viola, P., and M. Jones. n.d. {``Rapid Object Detection Using a Boosted
Cascade of Simple Features.''} In \emph{Proceedings of the 2001 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition.
CVPR 2001}, 1:I-511-I-518. IEEE Comput. Soc.
\url{https://doi.org/10.1109/cvpr.2001.990517}.

\bibitem[\citeproctext]{ref-weizenbaum1966eliza}
Weizenbaum, Joseph. 1966. {``ELIZA---a Computer Program for the Study of
Natural Language Communication Between Man and Machine.''}
\emph{Communications of the ACM} 9 (1): 36--45.
\url{https://doi.org/10.1145/365153.365168}.

\end{CSLReferences}


\backmatter


\end{document}
