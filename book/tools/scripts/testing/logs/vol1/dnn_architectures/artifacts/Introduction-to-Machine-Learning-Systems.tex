% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% NOTE: TikZ colors (BlueLine, GreenLine, RedLine, OrangeLine, etc.) are defined
% in the YAML config files under format > pdf > tikz > include-headers.
% Only colors specific to LaTeX packages (not TikZ) are defined here.

% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
\titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
\titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
% Using height= instead of width= ensures consistent header heights across all icons
% regardless of aspect ratio
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[height=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-chapter-connection-color1}{HTML}{EFF6FF}
\definecolor{callout-chapter-connection-color2}{HTML}{1E3A5F}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4A7C59}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 2, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 2, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 2, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{Network Architectures}\label{sec-dnn-architectures}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: A visually striking rectangular image
illustrating the interplay between deep learning algorithms like CNNs,
RNNs, and Attention Networks, interconnected with machine learning
systems. The composition features neural network diagrams blending
seamlessly with representations of computational systems such as
processors, graphs, and data streams. Bright neon tones contrast against
a dark futuristic background, symbolizing cutting-edge technology and
intricate system complexity.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/dnn_architectures/images/png/cover_dl_arch.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why is choosing a neural network architecture an infrastructure
commitment rather than a modeling decision?}

The mathematical operators---matrix multiplications, activation
functions, gradient computations---are the vocabulary of neural
networks. Architecture determines how you assemble that vocabulary into
structures optimized for specific data types and deployment constraints.
When you select a Transformer over a CNN, you are not choosing a
syntax---you are \emph{signing a contract with physics}. A convolutional
layer exploits spatial locality to reduce parameters by orders of
magnitude; a Transformer's attention mechanism enables long-range
dependencies but demands memory that scales quadratically with sequence
length; a recurrent architecture handles variable sequences but creates
sequential dependencies that prevent parallelization. These choices
cascade throughout the system stack: they determine whether a model fits
in mobile device memory, whether training completes in days or months,
whether inference meets latency requirements, and whether deployment is
economically viable. Architecture selection cannot be revised later
without rebuilding the entire system. The architecture is not what your
model does---it is what your hardware must do, and the physics of that
hardware determines what is computationally possible.

\begin{tcolorbox}[enhanced jigsaw, colback=white, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, breakable, leftrule=.75mm, bottomtitle=1mm, coltitle=black, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, arc=.35mm, titlerule=0mm, bottomrule=.15mm, opacityback=0]

\begin{itemize}
\tightlist
\item
  Distinguish the computational characteristics of major neural network
  architectures (MLPs, CNNs, RNNs, Transformers)
\item
  Explain how inductive biases enable architectures to exploit structure
  in different data types
\item
  Analyze computational complexity and memory scaling behaviors across
  architectural families
\item
  Apply the architecture selection framework to match data
  characteristics with appropriate designs
\item
  Evaluate hardware mapping efficiency for different architectural
  primitives and patterns
\item
  Assess system-level deployment constraints including latency,
  bandwidth, and parallelization requirements
\item
  Critique common architectural selection fallacies using systems
  engineering principles
\end{itemize}

\end{tcolorbox}

\section{Architectural Principles and Engineering
Trade-offs}\label{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e}

Every neural network architecture answers one central question:
\emph{how should we structure computation to match the structure in our
data?} Images have spatial locality, language has sequential
dependencies, and tabular records have no inherent structure at all. The
architecture encodes assumptions about these patterns directly into the
computational graph, and those assumptions determine everything from
parameter count to hardware utilization to deployment feasibility.
Architecture selection is therefore a systems engineering problem, not
merely a modeling decision.

\textbf{?@sec-deep-learning-systems-foundations} established the
mathematical operators, matrix multiplication, activation functions, and
gradient computation, that form the ``verbs'' of neural networks. This
chapter examines how these operators assemble into
\textbf{architectures}: specialized structures optimized for specific
data types and computational constraints. As defined in the
\textbf{Silicon Contract}, every architecture makes an implicit
agreement with hardware, trading computational patterns for efficiency
on particular problem classes.

The structural assumptions that each architecture encodes are known as
\textbf{inductive biases}, and they serve as the \emph{unifying concept}
for this entire chapter. A CNN's inductive bias is spatial locality:
nearby pixels matter more than distant ones. A Transformer's inductive
bias is that any element may attend to any other, enabling flexible
long-range relationships at the cost of quadratic memory scaling. These
biases are not incidental design choices; they are the mechanism through
which architectures achieve efficiency by restricting the space of
functions they can represent. We formalize this framework in
Section~\ref{sec-dnn-architectures-unified-framework-inductive-biases-257d},
after examining how each architecture's bias manifests in practice.

Machine learning systems face a fundamental engineering trade-off:
\textbf{representational power versus computational efficiency}. In the
context of the \textbf{Iron Law of ML Systems}
(\textbf{?@sec-silicon-contract}), architectural choice is the primary
determinant of the \textbf{Ops} term. A Transformer's attention
mechanism enables global relationships but scales as \(O(N^2)\)
operations; a CNN exploits spatial locality to reduce operations to
\(O(N)\). Navigating this trade-off, choosing the right inductive biases
for your data while setting a manageable ``Ops budget,'' defines the
practice of neural architecture selection.

Five architectural families define modern neural computation, each
optimized for different data characteristics:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1633}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2959}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3061}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2347}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Core Innovation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MLPs} & Tabular/Unstructured & Dense connectivity & Memory
bandwidth \\
\textbf{CNNs} & Spatial (images) & Local filters + weight sharing &
Compute throughput \\
\textbf{RNNs} & Sequential (time series) & Recurrent state & Sequential
dependencies \\
\textbf{Transformers} & Relational (language) & Dynamic attention &
Memory capacity (N²) \\
\textbf{DLRM} & Categorical (recommendations) & Embedding tables &
Memory capacity (TB+) \\
\end{longtable}

Each architectural choice creates distinct computational signatures that
propagate through every level of the implementation stack. Before
examining each architecture, we introduce five \textbf{Lighthouse
Models} that will serve as concrete reference points throughout this
book.

\subsection{Lighthouse Architectures: Recurring Systems
Examples}\label{sec-dnn-architectures-lighthouse-architectures-recurring-systems-examples-7d9c}

Throughout this book, we will use five specific model architectures as
recurring \textbf{Lighthouse Models}. These serve as consistent
reference points to ground abstract concepts in concrete systems
reality. We introduce them here with both their qualitative roles and
quantitative characteristics, then define their architectures in detail
within their respective sections. These examples are concrete
implementations of the \textbf{Workload Archetypes} (Compute Beast,
Bandwidth Hog, etc.) introduced in
\textbf{?@sec-ml-system-architecture}.

To understand \emph{why} these specific models were chosen, it is
helpful to look at the history of model evolution through the lens of
the \textbf{Efficiency Frontier} (Figure~\ref{fig-efficiency-frontier}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/dnn_architectures/dnn_architectures_files/figure-pdf/fig-efficiency-frontier-output-1.pdf}}

}

\caption{\label{fig-efficiency-frontier}\textbf{The Efficiency
Frontier}: ImageNet Top-1 Accuracy vs.~Computational Cost (GFLOPs). The
dashed `Pareto Frontier' represents the optimal trade-off between
representational power and computational efficiency. Notice the
progression from dense CNNs (blue) to efficient Mobile architectures
(green) that minimized compute, and finally to Transformers (red) that
push the accuracy boundary at significantly higher computational costs.}

\end{figure}%

These models serve as more than convenient examples; they form a set of
\emph{canonical workloads} for understanding system constraints.

\phantomsection\label{callout-perspectiveux2a-1.1}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{Canonical Workloads}
\phantomsection\label{callout-perspective*-1.1}
In computer architecture, the \emph{MIPS} processor is often used to
teach pipelining, not because it is the fastest chip today, but because
it is the clearest embodiment of RISC principles. Similarly, this book
uses \emph{ResNet-50}, \emph{GPT-2}, and \emph{DLRM} as
\textbf{canonical workloads}.

We choose these specific models because they isolate distinct system
bottlenecks:

\begin{itemize}
\tightlist
\item
  \emph{ResNet-50} isolates \textbf{Compute} (Dense Matrix Math).
\item
  \emph{GPT-2} isolates \textbf{Memory Bandwidth} (Data Movement).
\item
  \emph{DLRM} isolates \textbf{Memory Capacity} (Random Access).
\end{itemize}

By studying these ``Lighthouses,'' you learn engineering principles
(roofline analysis, arithmetic intensity, memory hierarchies) that
remain valid even as the specific ``State of the Art'' model
architectures evolve.

\end{fbxSimple}

\subsubsection{Lighthouse Roster: Model
Biographies}\label{sec-dnn-architectures-lighthouse-roster-model-biographies-a763}

Before using these models as engineering benchmarks, it is helpful to
understand their historical context and \emph{why} they became
standards.

\textbf{ResNet-50 (Microsoft Research, 2015)} The Residual Network
(ResNet) (\citeproc{ref-he2016deep}{He et al. 2016}) solved the
``vanishing gradient'' problem that prevented training very deep
networks. By introducing ``skip connections'' that allow gradients to
flow unimpeded, it enabled networks of 50, 100, or even 1000 layers. It
won the ImageNet 2015 competition and became the standard ``backbone''
for computer vision. From a systems perspective, it is a highly regular,
compute-intensive workload composed almost entirely of dense
convolutions, making it the ideal test for GPU floating-point
throughput.

\textbf{GPT-2 (OpenAI, 2019)} Generative Pre-trained Transformer 2
(GPT-2) demonstrated that scaling up a simple architecture (the
Transformer Decoder) on massive datasets could produce coherent text
generation. Unlike BERT (which processes text bidirectionally), GPT-2
generates text sequentially (autoregressively), creating a unique memory
bandwidth bottleneck where the entire model must be loaded to generate
just one token. It serves as our archetype for modern Large Language
Models (LLMs) like Llama and ChatGPT.

\textbf{DLRM (Meta, 2019)} The Deep Learning Recommendation Model (DLRM)
was open-sourced by Meta to expose a workload that differs fundamentally
from CNNs and Transformers. While vision and language models are
compute-heavy, recommendation systems are memory-heavy. They must look
up user and item preferences in massive ``embedding tables'' that can
reach terabytes in size. DLRM is the standard benchmark for memory
capacity and sparse memory access patterns in the data center.

\textbf{MobileNet (Google, 2017)} MobileNet
(\citeproc{ref-howard2017mobilenets}{Howard et al. 2017}) challenged the
trend of ever-larger models by prioritizing efficiency. It introduced
``Depthwise Separable Convolutions,'' an architectural innovation that
reduced computational cost (FLOPs) by 8-9x with minimal accuracy loss.
It proved that model architecture could be co-designed with hardware
constraints, becoming the standard for running vision models on
smartphones and embedded devices where battery life and latency are
critical.

\textbf{Keyword Spotting (KWS) (Google/Arm, 2017)} Keyword Spotting
models (like those detecting ``Hey Siri'' or ``Ok Google'') represent
the extreme end of efficiency. Designed to run on ``always-on''
microcontrollers with kilobyte-scale memory and milliwatt power budgets,
these models (often Depthwise Separable CNNs) exemplify the constraints
of TinyML. They force engineers to count every byte and cycle, driving
innovations in extreme quantization (int8/int4) and specialized
hardware.

\subsubsection{The Arithmetic Intensity
Spectrum}\label{sec-dnn-architectures-understanding-arithmetic-intensity-ade5}

The quantitative characteristics in
Table~\ref{tbl-lighthouse-comparison} expose a critical engineering
constraint established in
\textbf{?@sec-deep-learning-systems-foundations}: \textbf{arithmetic
intensity}. As we saw, this ratio of operations performed per byte of
data moved determines whether a workload is compute-bound or
memory-bound.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0851}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0585}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.2394}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1702}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.2128}}@{}}
\caption{\textbf{Lighthouse Model Comparison}: Quantitative
characteristics and pedagogical roles of the five canonical workloads.
Parameters and memory represent model weights at FP32 precision. FLOPs
measured per single inference. The bottleneck column indicates the
primary system constraint each model reveals: compute-bound models like
ResNet stress arithmetic throughput, while bandwidth-bound models like
GPT-2 stress memory transfer
rates.}\label{tbl-lighthouse-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Params}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{FLOPs/Inf}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Role in Textbook}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Params}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{FLOPs/Inf}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Role in Textbook}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} & Vision & 25.6M & 4.1 GFLOPs & 102 MB & Compute &
Parallelism, quantization, and batching \\
\textbf{GPT-2 XL} & Language & 1.5B & 3.0 GFLOPs/token & 6.0 GB & Mem.
Bandwidth & Autoregressive generation and KV caching \\
\textbf{DLRM} & Recommender & 25B & Low & 100 GB & Mem. Capacity &
Embedding tables and scale-out systems \\
\textbf{MobileNetV2} & Edge Vision & 3.5M & 300 MFLOPs & 14 MB & Latency
& Depthwise convolutions and efficiency \\
\textbf{KWS (DS-CNN)} & Audio & 200K & 20 MFLOPs & 800 KB & Power &
Extreme quantization and always-on ops \\
\end{longtable}

Architecture selection is ultimately an engineering trade-off between
\textbf{Math} (\(Ops\)) and \textbf{Memory Movement} (\(D\)). By
comparing our Lighthouses, we can see how architectural choices shift a
model's position on the intensity spectrum:

\begin{itemize}
\tightlist
\item
  \textbf{ResNet-50 (Compute-Bound)}: High intensity (\(\approx 100\)
  \(Ops/Byte\)). Convolutional layers reuse each weight many times
  across the spatial dimensions of an image. Its performance is limited
  by how fast the hardware can do math.
\item
  \textbf{GPT-2 (Bandwidth-Bound)}: Low intensity (\(\approx 1\)
  \(Ops/Byte\)). Autoregressive transformers must load massive weights
  from memory just to perform a single token's math. Its performance is
  limited by how fast memory can move bits.
\item
  \textbf{MobileNet (Memory-Bound on GPUs)}: Low intensity
  (\(\approx 10\) \(Ops/Byte\)). MobileNet reduces total \(Ops\) through
  depthwise separable convolutions, but it moves more data relative to
  that work. It fits mobile hardware perfectly but often ``starves''
  high-end GPUs optimized for dense math.
\end{itemize}

This spectrum determines whether you need a faster processor or faster
memory to improve performance. The Roofline Model, which we examine in
detail in \textbf{?@sec-ai-acceleration}, provides the analytical
framework for quantifying these limits on specific hardware.

\phantomsection\label{callout-checkpointux2a-1.2}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{Arithmetic Intensity and Architecture}
\phantomsection\label{callout-checkpoint*-1.2}

Match the architectural choice to its systems implication:

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Weight Reuse (CNNs)}: Increases arithmetic intensity by using
  the same weights across many inputs.
\item[$\square$]
  \textbf{Large Embedding Tables (DLRM)}: Decreases arithmetic intensity
  by requiring massive data movement for minimal computation.
\item[$\square$]
  \textbf{Sequential Attention (GPT)}: Decreases arithmetic intensity by
  loading weights per-token rather than per-batch.
\end{itemize}

\end{fbxSimple}

With these quantitative reference points established, we now examine
each architectural family in detail, starting with the foundational
Multi-Layer Perceptron, the architecture that established the
computational patterns underlying all modern neural networks. From
there, we progress through increasingly specialized designs: CNNs that
exploit spatial structure, RNNs that capture temporal dependencies,
Transformers that enable dynamic attention, and finally DLRM that
handles massive categorical features. Each architecture represents a
different answer to the same fundamental question: \emph{how} should we
structure computation to match the patterns in our data?

\section{Multi-Layer Perceptrons: Dense Pattern
Processing}\label{sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-bc11}

Multi-Layer Perceptrons\sidenote{\textbf{Perceptron}: Coined by Frank
Rosenblatt at Cornell in 1957, combining ``perception'' with the suffix
``-tron'' (electronic device), following the naming convention of era
machines like cyclotron and magnetron. Rosenblatt described the
perceptron as ``the first machine capable of having an original idea,''
building on McCulloch and Pitts' 1943 neuron model to create what he
envisioned as a computational model for biological perception and
memory. } (MLPs) represent the fully-connected architectures introduced
in \textbf{?@sec-deep-learning-systems-foundations}, now examined
through the lens of architectural choice and systems trade-offs.

MLPs embody an inductive bias: \textbf{they assume no prior structure in
the data, allowing any input to relate to any output}. This
architectural choice enables maximum flexibility by treating all input
relationships as equally plausible, making MLPs versatile but
computationally intensive compared to specialized alternatives. Their
computational power was established theoretically by the Universal
Approximation Theorem (UAT)\sidenote{\textbf{Universal Approximation
Theorem}: Proven independently by Cybenko (1989) and Hornik (1989), this
result showed that neural networks could theoretically learn any
function, a discovery that reinvigorated interest in neural networks
after the ``AI Winter'' of the 1980s and established mathematical
foundations for modern deep learning. }
(\citeproc{ref-cybenko1989approximation}{Cybenko 1989};
\citeproc{ref-hornik1989multilayer}{Hornik, Stinchcombe, and White
1989}), which we encountered as a footnote in
\textbf{?@sec-deep-learning-systems-foundations}. This theorem states
that a sufficiently large MLP with non-linear activation functions can
approximate any continuous function on a compact domain, given suitable
weights and biases. The following definition formalizes the
\emph{multi-layer perceptron} as an architectural concept.

\phantomsection\label{callout-definitionux2a-1.3}
\begin{fbxSimple}{callout-definition}{Definition:}{Multi-Layer Perceptrons}
\phantomsection\label{callout-definition*-1.3}
\textbf{\emph{Multi-Layer Perceptrons}} are the architectural embodiment
of \textbf{Global Connectivity}. By connecting every input to every
output, they maximize \textbf{Expressivity} (Universal Approximation)
but sacrifice \textbf{Locality}, resulting in \textbf{\(O(N^2)\)}
parameter scaling that makes them efficient only for low-dimensional or
semantic data.

\end{fbxSimple}

In practice, the UAT explains \emph{why} MLPs succeed across diverse
tasks while revealing the gap between theoretical capability and
practical implementation. The theorem guarantees that \emph{some} MLP
can approximate any function, yet provides no guidance on requisite
network size or weight determination. While MLPs can theoretically solve
any pattern recognition problem, achieving this capability may require
impractically large networks or extensive computation. This theoretical
power drives the selection of MLPs for tabular data, recommendation
systems, and problems where input relationships are unknown. At the same
time, these practical limitations motivated the development of
specialized architectures that exploit data structure for computational
efficiency, as detailed in
Section~\ref{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e}.

This tension between theoretical capability and practical feasibility
reveals a fundamental distinction that shapes all architecture selection
decisions.

\subsubsection{The Learnability Gap: Why Universal Approximation Is Not
Enough}\label{sec-dnn-architectures-learnability-gap-universal-approximation-enough-0624}

The Universal Approximation Theorem establishes that a single hidden
layer MLP with sufficient width can approximate any continuous function
to arbitrary accuracy. This sounds definitive: if MLPs are universal,
\emph{why} do we need specialized architectures like CNNs, RNNs, or
Transformers? The answer lies in a critical distinction between
\emph{what} a network \emph{can represent} and \emph{what} it \emph{can
learn}.

\textbf{Representation capacity} refers to the functions an architecture
can express given unlimited resources; the UAT guarantees MLPs have
universal representation capacity. This capacity is particularly
effective because of the \textbf{Manifold
Hypothesis}\sidenote{\textbf{Manifold Hypothesis}: The assumption that
high-dimensional real-world data (like images) actually lies on a
low-dimensional surface (manifold) embedded within that space. For
example, a 256x256 image lives in a 65,536-dimensional space, but the
subset of ``valid cat images'' occupies a tiny, structured region within
it. Deep learning works because layers progressively unfold this
crumpled manifold to make it linearly separable. }, which suggests that
high-dimensional data actually occupies a much simpler structure.
\textbf{Learnability} refers to whether gradient descent can find good
weights given finite training samples and computational budgets. A
function may be representable yet practically unlearnable.

This distinction resolves what appears to be a paradox: if MLPs are
universal approximators, \emph{why} has architectural innovation
(ResNets, Transformers) driven deep learning progress? Specialized
architectures improve learnability by embedding inductive biases that
match data structure, even when doing so restricts representational
capacity.

Three factors create the learnability gap:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sample complexity}: The UAT provides no bounds on training
  examples needed. For 28×28 images, an MLP treats 784 pixels
  independently, requiring exponentially many samples to learn spatial
  correlations. A CNN embeds locality bias, drastically reducing sample
  requirements. Mathematically, sample complexity can scale as
  \(O(e^d)\) for MLPs but \(O(\text{poly}(d))\) for architectures
  matching data structure.
\item
  \textbf{Parameter efficiency}: The UAT guarantees \emph{some} width
  suffices, but provides no constructive bounds. Required width can be
  exponential in input dimension: approximating
  \(\sin(x_1) + \cdots + \sin(x_d)\) may require \(O(e^d)\) MLP neurons
  versus \(O(d)\) for architectures processing dimensions independently.
\item
  \textbf{Optimization difficulty}: Even when optimal weights exist,
  gradient descent may not find them. MLP loss surfaces exhibit complex
  topology without the regularizing effect of architectural constraints.
  Specialized architectures reduce the search space, introducing
  symmetries that gradient descent exploits.
\end{enumerate}

The classic \emph{MNIST} handwritten digit benchmark illustrates this
gap between \emph{representation and learnability} concretely.

\phantomsection\label{callout-exampleux2a-1.4}
\begin{fbxSimple}{callout-example}{Example:}{MNIST: Representation vs Learnability}
\phantomsection\label{callout-example*-1.4}
Consider classifying 28\(\times\) 28 MNIST digits (784 input pixels, 10
output classes).

\textbf{MLP Approach}:

\begin{itemize}
\tightlist
\item
  Architecture: 784 → 4096 → 4096 → 10
\item
  Parameters:
  \((784 \times 4096) + (4096 \times 4096) + (4096 \times 10) \approx\)
  20M parameters
\item
  Training: 60,000 examples (standard MNIST training set)
\item
  Test Accuracy: \textasciitilde97-98\%
\item
  Rationale: Treats every pixel independently. Must learn all spatial
  correlations from data alone. No prior knowledge about spatial
  structure.
\end{itemize}

\textbf{CNN Approach}:

\begin{itemize}
\tightlist
\item
  Architecture: Conv(32, 3×3) → Pool → Conv(64, 3×3) → Pool → FC(128) →
  10
\item
  Parameters:
  \((3 \times 3 \times 32) + (3 \times 3 \times 32 \times 64) + (64 \times 7 \times 7 \times 128) + (128 \times 10) \approx\)
  421K parameters
\item
  Training: 60,000 examples (same data)
\item
  Test Accuracy: \textasciitilde99\%+
\item
  Rationale: Embeds locality bias (nearby pixels are related) and
  translation invariance (digit patterns are meaningful regardless of
  position). These structural assumptions reduce parameter count and
  improve generalization.
\end{itemize}

\textbf{Comparison}:

\begin{itemize}
\tightlist
\item
  Parameter Efficiency: CNN uses 47× fewer parameters
\item
  Sample Efficiency: CNN achieves better accuracy with the same training
  data
\item
  Systems Implications: CNN requires 47× less memory, trains faster, and
  runs faster at inference
\end{itemize}

Both architectures can \emph{represent} the digit classification
function (UAT guarantees this for MLPs; CNNs have similar or greater
representational capacity). The difference is \emph{learnability}: the
CNN's inductive bias matches the spatial structure of images, enabling
efficient learning with limited data and compute.

\end{fbxSimple}

\textbf{Why Specialized Architectures Exist}: The learnability gap
motivates the core design principle: embed inductive biases that match
data structure. CNNs embed locality for spatial data; RNNs embed
sequential dependencies for temporal data; Transformers embed learned
attention for relational data. Each sacrifices theoretical generality
for practical learnability.

\textbf{No Free Lunch} (\citeproc{ref-wolpert1996lack}{Wolpert 1996}):
The bias that helps one task may hurt another. CNN's translation
invariance aids image classification but hurts tasks where absolute
position matters. Architecture selection fundamentally matches inductive
bias to data structure.

These theoretical insights translate directly into engineering
decisions. Appropriate inductive biases reduce parameter counts
(enabling edge deployment), accelerate convergence (reducing training
costs), and produce structured computation patterns that map efficiently
to specialized hardware (\textbf{?@sec-ai-acceleration}). A
20M-parameter MLP infeasible for edge deployment becomes a
421K-parameter CNN that fits comfortably, a 47x reduction achieved by
matching architecture to data structure.

The MNIST handwritten digit recognition challenge\sidenote{\textbf{MNIST
Dataset}: Created by Yann LeCun, Corinna Cortes, and Chris Burges in
1998 (\citeproc{ref-lecun1998gradient}{Lecun et al. 1998}) from NIST's
database of handwritten digits, MNIST's 60,000 training images became
the ``fruit fly'' of machine learning research. Despite human-level
accuracy of 99.77\% being achieved by various models, MNIST remains
valuable for education because its simplicity allows students to focus
on architectural concepts without data complexity distractions. }
provides a concrete example: an MLP transforms a \(28\times 28\) pixel
image into a digit classification. Understanding how MLPs accomplish
this task requires examining the specific pattern processing
requirements that dense architectures address.

\subsection{Pattern Processing
Needs}\label{sec-dnn-architectures-pattern-processing-needs-3c2a}

Deep learning models frequently encounter problems where any input
feature may influence any output without inherent constraints. Financial
market analysis exemplifies this challenge: any economic indicator may
affect any market outcome. Similarly, in natural language processing,
word meaning may depend on any other word in the sentence. These
scenarios demand an architectural pattern capable of learning arbitrary
relationships across all input features.

Dense pattern processing provides three essential properties:
unrestricted feature interactions, where each output can depend on any
combination of inputs; learned feature importance, where the system
determines which connections matter rather than relying on prescribed
relationships; and adaptive representation, where the network reshapes
internal representations based on the data.

The MNIST digit recognition task illustrates this uncertainty: while
humans might focus on specific parts of digits (loops in `6' or
crossings in `8'), the pixel combinations critical for classification
remain indeterminate. A `7' written with a serif may share pixel
patterns with a `2', and variations in handwriting mean discriminative
features may appear anywhere in the image. This uncertainty about
feature relationships requires a dense processing approach where every
pixel can potentially influence the classification decision. Such
unrestricted connectivity leads directly to the mathematical foundation
of MLPs.

\subsection{Algorithmic
Structure}\label{sec-dnn-architectures-algorithmic-structure-99f2}

The pattern processing needs established above demand an architecture
capable of relating any input to any output. MLPs solve this with
complete connectivity between all nodes. This connectivity requirement
manifests through a series of fully-connected layers, where each neuron
connects to every neuron in adjacent layers, the ``dense'' connectivity
pattern introduced in \textbf{?@sec-deep-learning-systems-foundations}.

This architectural principle translates the dense connectivity pattern
into matrix multiplication operations\sidenote{\textbf{GEMM (General
Matrix Multiply)}: A fundamental operation underlying many neural
network layers. GEMM performs (C = \alpha AB + \beta C) and has been
optimized for decades. In many dense models, a large fraction of runtime
is spent in GEMM-like kernels, and well-tuned libraries can approach
peak throughput on suitable workloads. This is why matrix-kernel
efficiency often dominates end-to-end performance in practice. },
establishing the mathematical foundation that makes MLPs computationally
tractable. Figure~\ref{fig-mlp} illustrates how each layer transforms
its input through the fundamental operation introduced in
\textbf{?@sec-deep-learning-systems-foundations}:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/bafda02f800fc90751036e4f4b74ee48f5ab8d9d.pdf}}

}

\caption{\label{fig-mlp}\textbf{Multi-Layer Perceptron Architecture}:
Three fully-connected layers where every neuron connects to all neurons
in adjacent layers. The highlighted neuron receives weighted
contributions from all inputs, illustrating the dense \(O(N \times M)\)
connectivity pattern implemented through matrix multiplications. For
MNIST classification, a 784-dimensional input connects to 100 hidden
neurons through a \(784 \times 100\) weight matrix, requiring 78,400
multiply-accumulate operations per sample. Adapted from
(\citeproc{ref-reagen2017deep}{Reagen et al. 2017}).}

\end{figure}%

\[
\mathbf{h}^{(l)} = f\big(\mathbf{h}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{b}^{(l)}\big)
\]

Recall that \(\mathbf{h}^{(l)}\) represents the layer \(l\) output
(activation vector), \(\mathbf{h}^{(l-1)}\) represents the input from
the previous layer, \(\mathbf{W}^{(l)}\) denotes the weight matrix for
layer \(l\), \(\mathbf{b}^{(l)}\) denotes the bias vector, and
\(f(\cdot)\) denotes the activation function (such as ReLU, as detailed
in \textbf{?@sec-deep-learning-systems-foundations}). This layer-wise
transformation, while conceptually simple, creates computational
patterns whose efficiency depends critically on how we organize these
operations for different problem structures.

The dimensions of these operations reveal the computational scale of
dense pattern processing. The input vector
\(\mathbf{h}^{(0)} \in \mathbb{R}^{d_{\text{in}}}\) (treated as a row
vector in this formulation) represents all potential input features.
Weight matrices
\(\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}}\)
capture all possible input-output relationships. The output vector
\(\mathbf{h}^{(l)} \in \mathbb{R}^{d_{\text{out}}}\) produces
transformed representations. The following example illustrates this
computation concretely.

\phantomsection\label{callout-exampleux2a-1.5}
\begin{fbxSimple}{callout-example}{Example:}{Concrete Computation Example}
\phantomsection\label{callout-example*-1.5}
Consider a simplified 4-pixel image processed by a 3-neuron hidden
layer:

\textbf{Input}: \(\mathbf{h}^{(0)} = [0.8, 0.2, 0.9, 0.1]\) (4 pixel
intensities)

\textbf{Weight matrix}:
\(\mathbf{W}^{(1)} = \begin{bmatrix} 0.5 & 0.1 & -0.2 \\ -0.3 & 0.8 & 0.4 \\ 0.2 & -0.4 & 0.6 \\ 0.7 & 0.3 & -0.1 \end{bmatrix}\)
(4×3 matrix)

\textbf{Computation}: \begin{gather*}
\mathbf{z}^{(1)} = \mathbf{h}^{(0)}\mathbf{W}^{(1)} = \begin{bmatrix} 0.5×0.8 + (-0.3)×0.2 + 0.2×0.9 + 0.7×0.1 \\ 0.1×0.8 + 0.8×0.2 + (-0.4)×0.9 + 0.3×0.1 \\ (-0.2)×0.8 + 0.4×0.2 + 0.6×0.9 + (-0.1)×0.1 \end{bmatrix}
\\
= \begin{bmatrix} 0.65 \\ -0.17 \\ 0.47 \end{bmatrix}
\end{gather*} \textbf{After ReLU}:
\(\mathbf{h}^{(1)} = [0.65, 0, 0.47]\) (negative values zeroed)

Each hidden neuron combines ALL input pixels with different weights,
demonstrating unrestricted feature interaction.

\end{fbxSimple}

The MNIST example demonstrates the practical scale of these operations.
Each 784-dimensional input (\(28\times 28\) pixels) connects to every
neuron in the first hidden layer. A hidden layer with 100 neurons
requires a \(784\times 100\) weight matrix, where each weight represents
a learnable relationship between an input pixel and a hidden feature.

This algorithmic structure enables arbitrary feature relationships while
creating specific computational patterns that computer systems must
accommodate.

\subsection{Architectural
Characteristics}\label{sec-dnn-architectures-architectural-characteristics-35d9}

Dense connectivity provides the universal approximation capability
established earlier but introduces computational redundancy. While the
theoretical power of MLPs enables modeling of any continuous function
given sufficient width, this flexibility requires numerous parameters to
learn relatively simple patterns. Every input feature influences every
output, yielding maximum expressiveness at the cost of maximum
computational expense.

These trade-offs motivate optimization techniques that reduce
computational demands while preserving model capability. Strategies
including pruning and quantization are examined in
\textbf{?@sec-model-compression}, with \textbf{?@sec-ai-acceleration}
exploring hardware-specific implementations that exploit regular matrix
operation structure. The architectural foundations established here
determine which optimization approaches work most effectively for dense
connectivity patterns.

\subsection{Computational
Mapping}\label{sec-dnn-architectures-computational-mapping-cf30}

The mathematical representation of dense matrix multiplication maps to
specific computational patterns that systems must handle.
Listing~\ref{lst-mlp_layer_matrix} demonstrates how this mapping
progresses from mathematical abstraction to computational reality.

\begin{codelisting}

\caption{\label{lst-mlp_layer_matrix}\textbf{Dense Layer Abstraction}:
Framework-level matrix operations hide O(N x M) multiply-accumulate
complexity behind a single function call, enabling hardware-optimized
BLAS libraries to achieve 80-95\% of peak throughput.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mlp\_layer\_matrix(X, W, b):}
    \CommentTok{"""MLP forward pass using framework{-}level matrix operations."""}
    \CommentTok{\# X: input matrix (batch\_size x num\_inputs)}
    \CommentTok{\# W: weight matrix (num\_inputs x num\_outputs)}
    \CommentTok{\# b: bias vector (num\_outputs)}

    \CommentTok{\# Single GEMM call: frameworks dispatch to optimized BLAS/cuBLAS}
    \CommentTok{\# For MNIST: 784 x 100 = 78,400 MACs per sample}
\NormalTok{    H }\OperatorTok{=}\NormalTok{ activation(matmul(X, W) }\OperatorTok{+}\NormalTok{ b)}
    \ControlFlowTok{return}\NormalTok{ H}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The function mlp\_layer\_matrix directly mirrors the mathematical
equation, employing high-level matrix operations (\texttt{matmul}) to
express the computation in a single line while abstracting the
underlying complexity. This implementation style characterizes deep
learning frameworks, where optimized libraries manage the actual
computation.

To understand the system implications of this architecture, we must look
``under the hood'' of the high-level framework call. The elegant
one-line matrix multiplication \texttt{output\ =\ matmul(X,\ W)} is,
from the hardware's perspective, a series of nested loops that expose
the true computational demands on the system. This translation from
logical model to physical execution reveals critical patterns that
determine memory access, parallelization strategies, and hardware
utilization.

The second implementation in Listing~\ref{lst-mlp_layer_compute} exposes
the actual computational pattern through nested loops, revealing what
really happens when we compute a layer's output: we process each sample
in the batch, computing each output neuron by accumulating weighted
contributions from all inputs.

\begin{codelisting}

\caption{\label{lst-mlp_layer_compute}\textbf{Dense Layer Computation}:
Nested loops reveal O(batch x outputs x inputs) complexity where each
output neuron requires num\_inputs multiply-accumulate operations,
explaining why MNIST classification demands 78,400 MACs per layer.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mlp\_layer\_compute(X, W, b):}
    \CommentTok{"""Explicit loop structure exposing MLP computational patterns."""}
    \CommentTok{\# Loop 1: Process each sample independently (parallelizable)}
    \ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):}
        \CommentTok{\# Loop 2: Compute each output neuron}
        \ControlFlowTok{for}\NormalTok{ out }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_outputs):}
\NormalTok{            Z[batch, out] }\OperatorTok{=}\NormalTok{ b[out]  }\CommentTok{\# Initialize with bias}

            \CommentTok{\# Loop 3: Accumulate weighted inputs (innermost loop)}
            \CommentTok{\# This is the MAC operation: result += input * weight}
            \ControlFlowTok{for}\NormalTok{ in\_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_inputs):}
\NormalTok{                Z[batch, out] }\OperatorTok{+=}\NormalTok{ X[batch, in\_] }\OperatorTok{*}\NormalTok{ W[in\_, out]}
            \CommentTok{\# Total per output: num\_inputs MACs +}
            \CommentTok{\# num\_inputs memory reads}

\NormalTok{    H }\OperatorTok{=}\NormalTok{ activation(Z)  }\CommentTok{\# Element{-}wise nonlinearity}
    \ControlFlowTok{return}\NormalTok{ H}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This translation from mathematical abstraction to concrete computation
exposes how dense matrix multiplication decomposes into nested loops of
simpler operations. The outer loop processes each sample in the batch,
while the middle loop computes values for each output neuron. Within the
innermost loop, the system performs repeated multiply-accumulate
operations\sidenote{\textbf{Multiply-Accumulate (MAC)}: The atomic
operation in neural networks: multiply two values and add to a running
sum. Modern accelerators measure performance in MACs per second:
datacenter class accelerators can sustain on the order of (10\^{}\{14\})
to (10\^{}\{15\}) MAC/s on dense kernels (e.g., NVIDIA H100 at
\textasciitilde1 PFLOPS), while mobile class chips are often on the
order of (10\^{}\{12\}) to (10\^{}\{13\}) MAC/s. At the hardware level,
the energy cost of the arithmetic itself is typically in the picojoule
range per MAC, while moving data to and from off chip memory can cost
orders of magnitude more, which is why data movement is often the
dominant systems concern. }, combining each input with its corresponding
weight.

In the MNIST example, each output neuron requires 784
multiply-accumulate operations and at least 1,568 memory accesses (784
for inputs, 784 for weights). While actual implementations use
optimizations through libraries like BLAS\sidenote{\textbf{Basic Linear
Algebra Subprograms (BLAS)}: Developed in the 1970s as a standard for
basic vector and matrix operations, BLAS became the foundation for
virtually all scientific computing. Modern implementations like Intel
MKL and OpenBLAS can achieve 80-95\% of theoretical peak performance on
well-optimized workloads, making them necessary for neural network
efficiency. } or cuBLAS, these patterns drive key system design
decisions. The hardware architectures that accelerate these matrix
operations, including GPU tensor cores\sidenote{\textbf{Tensor Cores}:
Specialized matrix units in NVIDIA GPUs (Volta+) accelerating
mixed-precision GEMM operations. A100 Tensor Cores deliver 312 TFLOPS
for FP16 (or 624 TOPS for INT8) compared to 19.5 TFLOPS for FP32 on CUDA
cores. Note that this comparison spans both precision (FP16 vs FP32) and
execution unit (Tensor Core vs CUDA Core); TF32 mode on Tensor Cores
achieves 156 TFLOPS. Tensor Cores require specific matrix dimensions
(multiples of 8/16) and data layouts; frameworks like cuBLAS
automatically tile operations to maximize utilization. } and specialized
AI accelerators, are covered in \textbf{?@sec-ai-acceleration}.

\subsection{System
Implications}\label{sec-dnn-architectures-system-implications-0550}

Neural network architectures exhibit distinct system-level
characteristics across three core dimensions: memory requirements,
computation needs, and data movement. Analyzing these dimensions
consistently reveals both commonalities and architecture-specific
optimizations across the families examined in this chapter. The
following analysis builds directly on neural network computation
patterns, memory systems, and system scaling discussed in
\textbf{?@sec-deep-learning-systems-foundations}.

\textbf{Memory Requirements.} For dense pattern processing, the memory
requirements stem from storing and accessing weights, inputs, and
intermediate results. In our MNIST example, connecting our
784-dimensional input layer to a hidden layer of 100 neurons requires
78,400 weight parameters. Each forward pass must access all these
weights, along with input data and intermediate results. The all-to-all
connectivity pattern means there's no inherent locality in these
accesses; every output needs every input and its corresponding weights.

These memory access patterns allow optimization through careful data
organization and reuse. Modern processors handle these dense access
patterns through specialized approaches: CPUs leverage their cache
hierarchy for data reuse, while GPUs employ memory architectures
designed for high-bandwidth access to large parameter matrices.
Frameworks abstract these optimizations through high-performance matrix
operations (as detailed in our earlier analysis).

\textbf{Computation Needs.} The core computation revolves around
multiply-accumulate operations arranged in nested loops. Each output
value requires as many multiply-accumulates as there are inputs. For
MNIST, this requires 784 multiply-accumulates per output neuron. With
100 neurons in the hidden layer, 78,400 multiply-accumulates are
performed for a single input image. While these operations are simple,
their volume and arrangement create specific demands on processing
resources.

This computational structure allows specific optimization strategies in
modern hardware. The dense matrix multiplication pattern parallelizes
across multiple processing units, with each handling different subsets
of neurons. Modern hardware accelerators use specialized matrix
multiplication units, while software frameworks automatically convert
these operations into optimized BLAS (Basic Linear Algebra Subprograms)
calls. CPUs and GPUs can both exploit cache locality by carefully tiling
the computation to maximize data reuse, though their specific approaches
differ based on their architectural strengths.

\textbf{Data Movement.} The all-to-all connectivity pattern in MLPs
creates significant data movement requirements. Each multiply-accumulate
operation needs three pieces of data: an input value, a weight value,
and the running sum. For our MNIST example layer, computing a single
output value requires moving 784 inputs and 784 weights to wherever the
computation occurs. This movement pattern repeats for each of the 100
output neurons, creating large data transfer demands between memory and
compute units.

The predictable data movement patterns allow strategic data staging and
transfer optimizations. Different architectures address this challenge
through various mechanisms; CPUs use prefetching and multi-level caches,
while GPUs employ high-bandwidth memory systems and latency hiding
through massive threading. Software frameworks orchestrate these data
movements through memory management systems that reduce redundant
transfers and increase data reuse.

This analysis shows that while dense connectivity provides universal
approximation capabilities, it creates significant inefficiencies when
data exhibits inherent structure. In practice, standalone MLPs are rare
in production systems, yet MLP layers appear as components inside nearly
every lighthouse architecture: the feed-forward blocks within GPT-2's
Transformer layers, the dense interaction layers in DLRM, and the
classification heads in ResNet-50 and MobileNet (see
Table~\ref{tbl-lighthouse-comparison}). The mismatch between dense
connectivity's assumptions and structured data motivated the development
of specialized approaches that exploit structural patterns for
computational gain.

\section{CNNs: Spatial Pattern
Processing}\label{sec-dnn-architectures-cnns-spatial-pattern-processing-5b8d}

The MLP's assumption that all input features interact equally with all
outputs proves particularly costly for spatially structured data like
images. Building on the computational complexity considerations outlined
in
Section~\ref{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e},
this mismatch motivated architectural patterns that exploit inherent
data structure.

\textbf{Convolutional}\sidenote{\textbf{Convolution}: From Latin
\emph{convolvere} meaning ``to roll together,'' the mathematical
operation appeared in 18th-century work by D'Alembert and Laplace on
series expansions. The term ``convolution'' was first applied to this
integral in 1934 by mathematician Aurel Wintner. In signal processing,
convolution describes how a system's impulse response ``rolls together''
with an input signal, a metaphor that extends naturally to the sliding
filter operations in neural networks. } \textbf{Neural Networks} emerged
as the solution to this challenge
(\citeproc{ref-lecun1998gradient}{Lecun et al. 1998};
\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}), embodying a specific inductive bias: they assume spatial
locality and translation invariance, where nearby pixels are related and
patterns can appear anywhere.

This architectural assumption produces two key innovations that enhance
efficiency for spatially structured data. Parameter sharing allows the
same feature detector to be applied across different spatial positions,
reducing parameters from millions to thousands while improving
generalization. Local connectivity restricts connections to spatially
adjacent regions, reflecting the insight that spatial proximity
correlates with feature relevance. Together, these innovations define
\emph{convolutional neural networks} as an architectural family.

\phantomsection\label{callout-definitionux2a-1.6}
\begin{fbxSimple}{callout-definition}{Definition:}{Convolutional Neural Networks}
\phantomsection\label{callout-definition*-1.6}
\textbf{\emph{Convolutional Neural Networks}} are architectures defined
by \textbf{Translation Equivariance}. They exploit \textbf{Spatial
Locality} through weight sharing, decoupling parameter count from input
size (\(O(1)\) scaling) to enable efficient processing of
high-dimensional grid data (images) at the cost of \textbf{Long-Range
Context}.

\end{fbxSimple}

These architectural innovations represent a trade-off in deep learning
design: sacrificing the theoretical generality of MLPs for practical
efficiency gains when data exhibits known structure. While MLPs treat
each input element independently, CNNs exploit spatial relationships to
achieve computational savings and improved performance on vision tasks.

\subsection{Pattern Processing
Needs}\label{sec-dnn-architectures-pattern-processing-needs-30b4}

Spatial pattern processing addresses scenarios where the relationship
between data points depends on their relative positions or proximity.
Consider processing a natural image: a pixel's relationship with its
neighbors is important for detecting edges, textures, and shapes. These
local patterns then combine hierarchically to form more complex
features: edges form shapes, shapes form objects, and objects form
scenes.

This hierarchical spatial pattern processing appears across many
domains. In computer vision, local pixel patterns form edges and
textures that combine into recognizable objects. Speech processing
relies on patterns across nearby time segments to identify phonemes and
words. Sensor networks analyze correlations between physically proximate
sensors to understand environmental patterns. Medical imaging depends on
recognizing tissue patterns that indicate biological structures. This
hierarchical approach succeeds not because it mimics the brain, but
because it mirrors the compositional structure of the data itself.

Focusing on image processing to illustrate these principles, if we want
to detect a cat in an image, certain spatial patterns must be
recognized: the triangular shape of ears, the round contours of the
face, the texture of fur. Importantly, these patterns maintain their
meaning regardless of where they appear in the image. A cat is still a
cat whether it appears in the top-left or bottom-right corner. This
indicates two key requirements for spatial pattern processing: the
ability to detect local patterns and the ability to recognize these
patterns regardless of their position\sidenote{\textbf{ImageNet
Revolution}: AlexNet's dramatic victory in the 2012 ImageNet challenge
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}) reduced top-5 error from 26.2\% (the runner-up) to 15.3\%, a 10.9
percentage point improvement that sparked the deep learning renaissance.
ImageNet's 14+ million labeled images across more than 21,000 categories
(with the ILSVRC competition subset using 1,000 classes) provided the
scale needed to train deep CNNs, proving that ``big data + big compute +
big models'' could achieve unprecedented performance. }.
Figure~\ref{fig-cnn-spatial-processing} shows convolutional neural
networks achieving this through hierarchical feature extraction, where
simple patterns compose into increasingly complex representations at
successive layers.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/554ab21ec191003f32b42d179c320ec4344796dc.pdf}}

}

\caption{\label{fig-cnn-spatial-processing}\textbf{Spatial Feature
Extraction}: Convolutional neural networks identify patterns independent
of their location in an image by applying learnable filters across the
input, enabling robust object recognition. These filters detect local
features, and their repeated application across the image creates
translation invariance, the ability to recognize a pattern regardless of
its position.}

\end{figure}%

Figure~\ref{fig-cnn-spatial-processing} illustrates how the CNN
architecture introduced earlier in this chapter implements these spatial
processing principles in practice. As pioneered by Yann
LeCun\sidenote{\textbf{Yann LeCun and CNNs}: LeCun's 1989 LeNet
architecture was inspired by Hubel and Wiesel's discovery of simple and
complex cells in cat visual cortex
(\citeproc{ref-hubel1962receptive}{Hubel and Wiesel 1962}). LeNet-5
achieved approximately 0.9\% error rate on MNIST in 1998 and was
deployed by banks to read millions of checks daily, among the first
large-scale commercial applications of neural networks. } and Y. LeCun
et al. (\citeproc{ref-lecun1989backpropagation}{1989}), the key
innovations that make this possible are parameter
sharing\sidenote{\textbf{Parameter Sharing}: CNNs reuse the same filter
weights across spatial positions, reducing parameters substantially. A
CNN processing \(224\times224\) images might use \(3\times3\) filters
with only 9 parameters per channel, versus an equivalent MLP requiring
50,176 parameters per neuron, a \$\sim\(5,575\)\times\$ reduction per
neuron enabling practical computer vision. }, local connectivity, and
translation invariance\sidenote{\textbf{Translation Invariance}: CNNs
detect features regardless of spatial position. A cat's ear is
recognized whether in the top-left or bottom-right corner. This property
emerges from convolution's sliding window design and is important for
computer vision, where objects appear at arbitrary locations in images.
}.

\subsection{Algorithmic
Structure}\label{sec-dnn-architectures-algorithmic-structure-deb9}

The core operation in a CNN can be expressed mathematically as:

\[
\mathbf{H}^{(l)}_{i,j,k} = f\left(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k\right)
\]

This equation describes how CNNs process spatial data.
\(\mathbf{H}^{(l)}_{i,j,k}\) is the output at spatial position \((i,j)\)
in channel \(k\) of layer \(l\). The triple sum iterates over the filter
dimensions: \((di,dj)\) scans the spatial filter size, and \(c\) covers
input channels. \(\mathbf{W}^{(l)}_{di,dj,c,k}\) represents the filter
weights, capturing local spatial patterns. Unlike MLPs that connect all
inputs to outputs, CNNs only connect local spatial neighborhoods.

Breaking down the notation further, \((i,j)\) corresponds to spatial
positions, \(k\) indexes output channels, \(c\) indexes input channels,
and \((di,dj)\) spans the local receptive
field\sidenote{\textbf{Receptive Field}: The region of the input that
influences a particular output neuron. In CNNs, receptive fields grow
with depth. A neuron in layer 3 might ``see'' a 7×7 region even with 3×3
filters, due to stacking. Understanding receptive field size is
important for ensuring networks can capture features at the right scale
for the task. }. Unlike the dense matrix multiplication of MLPs, this
operation:

Convolutional layers process local neighborhoods (typically
\(3 \times 3\) or \(5 \times 5\)), reuses the same weights at each
spatial position, and maintains spatial structure in its output.

To illustrate this process concretely, consider the MNIST digit
classification task with \(28 \times 28\) grayscale images. Each
convolutional layer applies a set of filters (e.g., \(3 \times 3\)) that
slide across the image, computing local weighted sums. If we use 32
filters with padding to preserve dimensions, the layer produces a
\(28 \times 28 \times 32\) output, where each spatial position contains
32 different feature measurements of its local neighborhood. This
contrasts sharply with the Multi-Layer Perceptron (MLP) approach, where
the entire image is flattened into a 784-dimensional vector before
processing.

This algorithmic structure directly implements the requirements for
spatial pattern processing, creating distinct computational patterns
that influence system design. Unlike MLPs, convolutional networks
preserve spatial locality, using the hierarchical feature extraction
principles established above. These properties drive architectural
optimizations in AI accelerators, where operations such as data reuse,
tiling, and parallel filter computation are important for performance.

\subsubsection{Translation Equivariance: Preserving Spatial
Structure}\label{sec-dnn-architectures-translation-equivariance-preserving-spatial-structure-11d2}

The mathematical property of \textbf{translation equivariance} is
central to understanding why CNNs work effectively for spatial data.
Group theory provides the mathematical language for symmetries and
transformations: convolution implements equivariance, meaning that
shifting the input shifts the output feature map correspondingly. The
implications for learning efficiency and systems design warrant deeper
examination. We approach this topic in four stages: first, we
distinguish equivariance from invariance and explain why this
distinction matters; second, we develop the mathematical formulation
with concrete examples; third, we explore the group theory perspective
that generalizes these concepts; and finally, we examine the systems
implications that make equivariance a practical concern for deployment.

Equivariance and invariance are related but distinct concepts that
determine how architectures handle transformations. Equivariance means
that transforming the input produces the same transformation in the
output:

\[
f(T(\mathbf{x})) = T(f(\mathbf{x}))
\]

For CNNs with translation \(T_v\) (shift by vector \(v\)), if the input
shifts by 5 pixels right, the feature maps also shift by 5 pixels right.
Position information is preserved through the transformation.
Invariance, by contrast, means transforming the input does not change
the output:

\[
f(T(\mathbf{x})) = f(\mathbf{x})
\]

Global average pooling over an entire feature map exhibits translation
invariance: shifting the input does not change the averaged output.
Position information is discarded.

Equivariance matters for learning because it preserves information
needed for structured representations. Consider spatial relationships: a
feature detector responding to an eye at position \((x, y)\) will
respond to the same eye at position \((x+5, y)\), but the response moves
to reflect the new position. The network can learn spatial relationships
like ``eye above nose'' that matter for face detection. Full invariance
would lose this relational information, leaving only ``eye and nose both
present somewhere,'' which proves insufficient for many tasks.

Object detection illustrates why equivariance is essential for
localization. Detection outputs bounding boxes like ``car at
\((100, 200)\) with size \(50 \times 80\)'', requiring equivariant
layers to track position through the network while invariant final
layers determine class. This architectural choice matches task
structure: equivariance for localization, invariance for classification.

Equivariance also supports hierarchical composition. Early layers detect
edges equivariantly at all positions, middle layers combine edges into
shapes while maintaining equivariance, and final layers may use partial
invariance through pooling for classification. This hierarchy works
precisely because intermediate features maintain spatial structure for
composition. The following note provides the mathematical formalization
of this property.

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{Equivariance Formalism}
\phantomsection\label{callout-perspective*-1.7}
\textbf{Mathematical Formulation}: For a convolutional layer with filter
\(\mathbf{w}\) and input \(\mathbf{x}\):

\[
(f * \mathbf{w})[i, j] = \sum_{m,n} \mathbf{w}[m, n] \cdot \mathbf{x}[i + m, j + n]
\]

Applying translation \(T_v\) (shift by \(v = (v_1, v_2)\)) to the input:

\[
(T_v \mathbf{x})[i, j] = \mathbf{x}[i - v_1, j - v_2]
\]

The convolution of the translated input becomes:

\[
(f * \mathbf{w})[T_v \mathbf{x}][i, j] = \sum_{m,n} \mathbf{w}[m, n] \cdot \mathbf{x}[(i - v_1) + m, (j - v_2) + n]
\]

\[
= \sum_{m,n} \mathbf{w}[m, n] \cdot \mathbf{x}[(i + m) - v_1, (j + n) - v_2]
\]

\[
= (f * \mathbf{w})[\mathbf{x}][i - v_1, j - v_2] = T_v((f * \mathbf{w})[\mathbf{x}])[i, j]
\]

This proves translation equivariance:
\(f(T_v \mathbf{x}) = T_v(f(\mathbf{x}))\).

\end{fbxSimple}

A concrete example illustrates these properties. Consider detecting
whisker patterns in a cat image where the cat face appears at position
\((50, 50)\). An equivariant convolutional layer applies a
\(3 \times 3\) filter to detect whisker textures, producing whisker
features at position \((50, 50)\) in the feature map. If the input
shifts so the cat face appears at \((55, 55)\), the whisker features
shift correspondingly to position \((55, 55)\) in the feature map. The
feature position tracks the input position, preserving spatial
information.

An invariant global pooling layer behaves entirely differently. Average
pooling over the entire spatial dimensions produces a scalar output
(say, average whisker strength of \(0.8\)) with no position information.
Whether the cat face appears at \((50, 50)\) or \((55, 55)\), the output
remains \(0.8\). The layer ignores spatial position entirely.

The equivariant layers preserve where features occur, enabling the
network to learn that ``whiskers near mouth'' and ``ears above eyes''
matter for cat classification. Invariant final layers discard absolute
position for classification. The following example demonstrates this
behavior concretely with a simple edge detector.

\phantomsection\label{callout-exampleux2a-1.8}
\begin{fbxSimple}{callout-example}{Example:}{Equivariance: Feature Detection}
\phantomsection\label{callout-example*-1.8}
Consider a \(7 \times 7\) image with a vertical edge at column 3:

\[
\mathbf{x} = \begin{bmatrix}
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0
\end{bmatrix}
\]

Vertical edge detector filter:

\[
\mathbf{w} = \begin{bmatrix}
-1 & 0 & 1 \\
-1 & 0 & 1 \\
-1 & 0 & 1
\end{bmatrix}
\]

\textbf{Convolving original image}:

Output feature map has strong activation at column 3 (where edge is):

\[
f(\mathbf{x}) = \begin{bmatrix}
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0
\end{bmatrix}
\]

\textbf{Shifted input} (edge moved to column 5):

\[
T_2 \mathbf{x} = \begin{bmatrix}
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0
\end{bmatrix}
\]

\textbf{Convolving shifted image}:

\[
f(T_2 \mathbf{x}) = \begin{bmatrix}
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0
\end{bmatrix} = T_2(f(\mathbf{x}))
\]

The feature activation shifts by the same amount as the input,
demonstrating equivariance. The network knows the edge is at column 5 in
the shifted image, not just that an edge exists somewhere.

\end{fbxSimple}

Equivariance carries systems implications that extend beyond
mathematical elegance. Parameter efficiency is the most immediate
benefit: equivariance through parameter sharing produces dramatic
reductions in model size. Consider processing a \(224 \times 224\) RGB
image. An MLP would require each hidden neuron to connect to all
\(224 \times 224 \times 3 = 150,528\) input pixels. A CNN with a
\(3 \times 3\) filter needs only \(3 \times 3 \times 3 = 27\) parameters
per filter, reused across all \(224 \times 224\) positions. This
represents approximately \(5,575\times\) fewer parameters per feature
detector, and the memory savings enable larger models and bigger batches
on fixed hardware.

The computational structure created by equivariance proves equally
valuable for systems optimization. The sliding window pattern applies
the same operation at every spatial position, creating regular
computation that hardware can exploit. Input pixels are used by multiple
filter positions, enabling im2col optimizations that restructure data
for efficient matrix operations. The resulting computation is inherently
SIMD-friendly, as modern GPUs can execute identical instructions across
spatial positions simultaneously. This structural regularity explains
why TPUs and AI accelerators include specialized units for convolution:
the operation maps efficiently to silicon precisely because equivariance
creates predictable, parallelizable patterns.

Equivariance also improves sample efficiency in ways that benefit the
entire training pipeline. When a network learns an edge detector at one
position, equivariance ensures that same detector works at all positions
automatically. Training no longer requires examples with edges at every
possible location, providing a form of built-in data augmentation. The
systems benefits cascade: less training data means reduced storage
requirements, faster training, and lower bandwidth consumption during
data loading.

From a group theory perspective, convolution's equivariance to
translations represents one instance of a general principle. The
translation group \((\mathbb{R}^2, +)\) consists of all 2D translations,
closed under composition (translating by \(v\) then \(u\) equals
translating by \(v + u\)). Convolution is equivariant to this group.
Recent research extends this framework to other symmetry groups. Cohen
\& Welling (\citeproc{ref-cohen2016group}{Cohen and Welling 2016})
developed Group-Equivariant CNNs that handle rotations and reflections
by constructing filters equivariant to rotation groups. This allows
learning rotation-invariant features for tasks like satellite imagery or
medical imaging where orientation does not determine meaning.

The mathematical framework generalizes cleanly: for group \(G\) acting
on input space \(X\) and output space \(Y\), a function \(f: X \to Y\)
is \(G\)-equivariant if:

\[
f(g \cdot \mathbf{x}) = g \cdot f(\mathbf{x}) \quad \forall g \in G, \mathbf{x} \in X
\]

Standard CNNs are translation-equivariant, while rotation-equivariant
networks extend this to rotation groups. The architectural principle
generalizes: embed symmetries of your data as equivariances in your
architecture. For systems engineering, this means that identifying data
symmetries directly informs architecture choice, that more constrained
architectures with stronger symmetries often produce smaller models, and
that specialized equivariances may require custom operations like
rotation convolutions that need either hardware support or efficient
software implementations.

In practice, perfect equivariance is often sacrificed for computational
efficiency or training stability. Asymmetric padding at image boundaries
breaks perfect translation equivariance, as does strided downsampling,
which introduces quantization where a one-pixel shift in input produces
a non-integer shift in output. Batch normalization, when computing
statistics per position in some implementations, also breaks
equivariance. Modern networks accept these deviations as necessary
trade-offs, and the slight loss of theoretical purity rarely impacts
practical performance.

Different tasks impose different requirements on where equivariance
should be maintained versus where invariance should be introduced. Image
classification needs only the final class label to be invariant;
intermediate layers benefit from staying equivariant to preserve spatial
information for hierarchical feature learning. Object detection requires
equivariance throughout the network because bounding box coordinates
must track object positions. Semantic segmentation demands full
equivariance to the output layer since per-pixel labels must align with
input positions. Image generation similarly requires equivariance to
maintain spatial structure in the output. The architectural decision of
where to introduce invariance through pooling or global averaging versus
maintaining equivariance reflects these task requirements and directly
shapes network design.

These task-specific requirements illustrate a deeper principle: the
choice of convolution reflects fundamental assumptions about
\emph{inductive bias} in neural architecture design. By restricting
connectivity to local neighborhoods and sharing parameters across
spatial positions, CNNs encode prior knowledge about the structure of
visual data: that important features are local and
translation-invariant. This architectural constraint reduces the
hypothesis space\sidenote{\textbf{Hypothesis Space}: The set of all
possible functions a model can represent given its architecture and
parameters. MLPs have a larger hypothesis space than CNNs for images,
but CNNs' constrained space contains better solutions for visual tasks,
demonstrating that architectural constraints often improve rather than
limit performance. Recent work has extended these principles to other
symmetry groups, developing Group-Equivariant CNNs that handle rotations
and reflections (\citeproc{ref-cohen2016group}{Cohen and Welling 2016}).
} that the network must search, enabling more efficient learning from
limited data compared to fully connected networks.

\phantomsection\label{callout-checkpointux2a-1.9}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{Spatial Inductive Bias}
\phantomsection\label{callout-checkpoint*-1.9}

CNNs succeed because they match the structure of image data. Verify you
understand how:

\begin{itemize}
\tightlist
\item[$\square$]
  Can you explain \textbf{Parameter Sharing}: how using the same filter
  across the image reduces parameter count by orders of magnitude
  compared to MLPs?
\item[$\square$]
  Do you understand \textbf{Translation Equivariance}: why shifting the
  input image results in a corresponding shift in the feature map?
\item[$\square$]
  Can you calculate why a conv layer is typically \textbf{Compute-Bound}
  (high arithmetic intensity) compared to other layers?
\end{itemize}

\end{fbxSimple}

This concept of architectural constraints guiding learning deserves a
formal definition.

\phantomsection\label{callout-definitionux2a-1.10}
\begin{fbxSimple}{callout-definition}{Definition:}{Inductive Bias}
\phantomsection\label{callout-definition*-1.10}
\textbf{\emph{Inductive Bias}} is the set of \textbf{Structural
Assumptions} encoded into a model to constrain the \textbf{Hypothesis
Space}. It trades \textbf{Generality} for \textbf{Sample Efficiency},
enabling learning from finite data by prioritizing solutions that align
with the physical properties of the domain (locality, invariance,
causality).

\end{fbxSimple}

CNNs naturally implement hierarchical representation learning through
their layered structure. Early layers detect low-level features like
edges and textures with small receptive fields, while deeper layers
combine these into increasingly complex patterns with larger receptive
fields. This hierarchical organization enables CNNs to build
compositional representations: complex objects are represented as
compositions of simpler parts. The mathematical foundation for this
emerges from the fact that stacking convolutional layers creates a
tree-like dependency structure, where each deep neuron depends on an
exponentially large set of input pixels, enabling efficient
representation of hierarchical patterns.

\subsubsection{Architectural
Characteristics}\label{sec-dnn-architectures-architectural-characteristics-80b9}

Parameter sharing dramatically reduces complexity compared to MLPs by
reusing the same filters across spatial locations. This sharing embodies
the assumption that useful features (such as edges or textures) can
appear anywhere in an image, making the same feature detector valuable
across all spatial positions.

These architectural properties make CNNs highly amenable to
systems-level analysis, and one model in particular has become the
standard reference point for \emph{compute-bound} vision workloads: the
\textbf{ResNet-50} architecture.

\phantomsection\label{callout-lighthouseux2a-1.11}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{ResNet-50 (Vision Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.11}

\textbf{Why it matters:} ResNet-50 is the gold standard benchmark for
\textbf{compute-bound} vision workloads. Its architecture consists
almost entirely of dense convolutional layers, making it highly regular
and efficient on GPUs. Unlike MobileNet (latency-bound) or Transformers
(memory-bound), ResNet-50's performance is typically limited by raw
floating-point throughput (FLOPs), making it the ideal lighthouse for
explaining data parallelism, quantization, and batching strategies.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1103}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3015}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5882}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Value}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Parameters} & 25.6 million & 102 MB model size at FP32; fits
comfortably in GPU memory. \\
\textbf{FLOPs/Image} & 4.1 GFLOPs (224×224) & Dominated by 3×3
convolutions (\textasciitilde90\% of compute). \\
\textbf{Constraint} & Compute Bound & Limited by raw FLOPs, not memory
bandwidth. \\
\textbf{Bottleneck} & FP Throughput & Benefits maximally from
specialized Matrix Units (Tensor Cores). \\
\textbf{Profile} & High Arithmetic Intensity & High ratio of
math-to-memory operations (\textasciitilde100 ops/byte). \\
\end{longtable}

\end{fbxSimple}

At the opposite end of the compute-efficiency spectrum, MobileNet
demonstrates how architectural choices can dramatically reduce
computational requirements.

\phantomsection\label{callout-lighthouseux2a-1.12}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{MobileNet (Efficiency Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.12}

\textbf{Why it matters:} MobileNet represents
\textbf{latency-constrained} edge workloads. By replacing dense
convolutions with \textbf{Depthwise Separable Convolutions}, it reduces
FLOPs by 8-9x with minimal accuracy loss. This architecture is critical
for understanding \textbf{efficiency trade-offs}: it trades channel
mixing capacity for speed, making it the standard baseline for mobile
apps, embedded vision, and neural architecture search (NAS).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1007}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2483}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6510}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Value}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Parameters} & 3.5 million & 14 MB at FP32; 7× smaller than
ResNet-50. \\
\textbf{FLOPs/Image} & 300 MFLOPs & 14× fewer than ResNet-50 for similar
accuracy. \\
\textbf{Constraint} & Latency Bound & Single-image inference speed is
the priority. \\
\textbf{Bottleneck} & Overhead / Serial Ops & Kernel launch overhead
often dominates actual compute. \\
\textbf{Profile} & Low Arithmetic Intensity & Memory access and control
logic matter more than raw FLOPs. \\
\end{longtable}

\end{fbxSimple}

The arithmetic intensity differences between these two architectures
expose a common misconception: that \emph{FLOPs equal speed}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Misconception: FLOPs = Speed}, rightrule=.15mm, opacitybacktitle=0.6, colframe=quarto-callout-warning-color-frame, breakable, leftrule=.75mm, bottomtitle=1mm, coltitle=black, left=2mm, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm, arc=.35mm, titlerule=0mm, bottomrule=.15mm, opacityback=0]

\textbf{Misconception:} ``MobileNet has 7× fewer FLOPs than ResNet-50,
so it must run 7× faster.''

\textbf{Reality:} On high-end GPUs, MobileNet often runs \emph{slower}
than ResNet-50 despite using far fewer operations. MobileNet's depthwise
separable convolutions have low arithmetic intensity: they move more
data relative to computation. GPUs optimized for dense matrix operations
(high arithmetic intensity) cannot saturate their compute units on
MobileNet's memory-bound kernels. FLOPs measure \emph{work}; throughput
depends on how well that work \emph{maps to hardware}. This is why
MobileNet excels on mobile CPUs (where memory bandwidth matches compute)
but underperforms on datacenter GPUs (where compute far exceeds
bandwidth).

\end{tcolorbox}

The architectural efficiency of CNNs allows further optimization through
specialized techniques like depthwise separable convolutions and
pruning, detailed in \textbf{?@sec-model-compression}. These
optimization strategies build on spatial locality principles, with
\textbf{?@sec-ai-acceleration} detailing how modern processors exploit
convolution's inherent data reuse patterns.

Figure~\ref{fig-cnn} visualizes the core convolution operation: a small
filter slides over the input image to generate a feature
map\sidenote{\textbf{Feature Map}: The output of applying a
convolutional filter to an input, representing detected features at
different spatial locations. A 64-filter layer produces 64 feature maps,
each highlighting different patterns like edges, textures, or shapes.
Feature maps become more abstract (detecting objects, faces) in deeper
layers compared to early layers (detecting edges, colors). }, capturing
local structures while maintaining translation invariance. For an
interactive visual exploration of convolutional networks, the CNN
Explainer (\citeproc{ref-cnn_explainer}{Z. J. Wang et al. 2021}) project
provides an insightful demonstration of how these networks are
constructed.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/cbe748cd47966cad8c6ef83c6fff7b11de6c097d.pdf}}

}

\caption{\label{fig-cnn}\textbf{Convolution Operation}: A \(3 \times 3\)
filter slides across the input, computing local dot products at each
position. The highlighted purple region shows the receptive field
producing one output value, requiring only 9 multiply-accumulate
operations compared to 784 for an equivalent MLP connection. This
parameter sharing reduces memory by \(5{,}000\times\) while encoding
translation equivariance: the same edge detector works at any image
location.}

\end{figure}%

\subsection{Computational
Mapping}\label{sec-dnn-architectures-computational-mapping-824c}

Convolution operations create computational patterns different from MLP
dense matrix multiplication. This translation from mathematical
operations to implementation details reveals distinct computational
characteristics.

The first implementation in Listing~\ref{lst-conv_layer_spatial} uses
high-level convolution operations to express the computation concisely,
typical of deep learning frameworks where optimized libraries handle the
underlying complexity.

\begin{codelisting}

\caption{\label{lst-conv_layer_spatial}\textbf{Convolutional Layer
Abstraction}: Framework-level convolution operations hide the complexity
of sliding window computations, enabling hardware-optimized
implementations that exploit spatial locality and parameter sharing.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ conv\_layer\_spatial(}\BuiltInTok{input}\NormalTok{, kernel, bias):}
    \CommentTok{"""Framework{-}level convolution.}

\CommentTok{    Single call dispatches to optimized kernel.}
\CommentTok{    """}
    \CommentTok{\# Convolution applies shared weights across all positions}
    \CommentTok{\# For a 3x3 kernel on 28x28 input:}
    \CommentTok{\# 9 MACs per position x 676 positions}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ convolution(}\BuiltInTok{input}\NormalTok{, kernel) }\OperatorTok{+}\NormalTok{ bias}
    \ControlFlowTok{return}\NormalTok{ activation(}
\NormalTok{        output}
\NormalTok{    )  }\CommentTok{\# Element{-}wise nonlinearity (e.g., ReLU)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The bridge between the logical model and physical execution becomes
critical for understanding CNN system requirements. While the high-level
convolution operation appears as a simple sliding window computation,
the hardware must orchestrate complex data movement patterns and exploit
spatial locality for efficiency.

Listing~\ref{lst-conv_layer_compute} reveals the actual computational
pattern: seven nested loops that process each spatial position, applying
the same filter weights to local regions of the input. This structure
exposes the true nature of convolution's computational demands and the
optimization opportunities it creates.

\begin{codelisting}

\caption{\label{lst-conv_layer_compute}\textbf{Convolutional Layer
Computation}: Seven nested loops expose O(batch x height x width x
output\_channels x kernel\_h x kernel\_w x input\_channels) complexity.
For a 3x3 kernel on 28x28 MNIST images with 32 filters, this performs 28
x 28 x 32 x 9 = 225,792 MACs per sample, a 3.5x reduction from
equivalent MLP connectivity while preserving spatial locality.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ conv\_layer\_compute(}\BuiltInTok{input}\NormalTok{, kernel, bias):}
    \CommentTok{\# Loop 1: Process each image in batch}
    \ControlFlowTok{for}\NormalTok{ image }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):}

        \CommentTok{\# Loop 2\&3: Move across image spatially}
        \ControlFlowTok{for}\NormalTok{ y }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(height):}
            \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(width):}

                \CommentTok{\# Loop 4: Compute each output feature}
                \ControlFlowTok{for}\NormalTok{ out\_channel }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_output\_channels):}
\NormalTok{                    result }\OperatorTok{=}\NormalTok{ bias[out\_channel]}

                    \CommentTok{\# Loop 5\&6: Move across kernel window}
                    \ControlFlowTok{for}\NormalTok{ ky }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(kernel\_height):}
                        \ControlFlowTok{for}\NormalTok{ kx }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(kernel\_width):}

                            \CommentTok{\# Loop 7: Process each input feature}
                            \ControlFlowTok{for}\NormalTok{ in\_channel }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}
\NormalTok{                                num\_input\_channels}
\NormalTok{                            ):}
                                \CommentTok{\# Get input value from}
                                \CommentTok{\# correct window position}
\NormalTok{                                in\_y }\OperatorTok{=}\NormalTok{ y }\OperatorTok{+}\NormalTok{ ky}
\NormalTok{                                in\_x }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ kx}
                                \CommentTok{\# Perform multiply{-}accumulate}
\NormalTok{                                result }\OperatorTok{+=}\NormalTok{ (}
                                    \BuiltInTok{input}\NormalTok{[}
\NormalTok{                                        image, in\_y, in\_x, in\_channel}
\NormalTok{                                    ]}
                                    \OperatorTok{*}\NormalTok{ kernel[}
\NormalTok{                                        ky,}
\NormalTok{                                        kx,}
\NormalTok{                                        in\_channel,}
\NormalTok{                                        out\_channel,}
\NormalTok{                                    ]}
\NormalTok{                                )}

                    \CommentTok{\# Store result for this output position}
\NormalTok{                    output[image, y, x, out\_channel] }\OperatorTok{=}\NormalTok{ result}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The seven nested loops reveal different aspects of the computation:

The loop structure divides into three groups. The outer loops manage
position, determining which image and where in the image. The middle
loop handles output features, computing different learned patterns. The
inner loops perform the actual convolution, sliding the kernel window
across the input.

Examining this process in detail, the outer two loops (\texttt{for\ y}
and \texttt{for\ x}) traverse each spatial position in the output
feature map (for the MNIST example, this traverses all \(28\times 28\)
positions). At each position, values are computed for each output
channel (\texttt{for\ k} loop), representing different learned features
or patterns: the 32 different feature detectors.

The inner three loops implement the actual convolution operation at each
position. For each output value, we process a local \(3\times 3\) region
of the input (the \texttt{dy} and \texttt{dx} loops) across all input
channels (\texttt{for\ c} loop). This creates a sliding window effect,
where the same \(3\times 3\) filter moves across the image, performing
multiply-accumulates between the filter weights and the local input
values. Unlike the MLP's global connectivity, this local processing
pattern means each output value depends only on a small neighborhood of
the input.

For our MNIST example with \(3\times 3\) filters and 32 output channels,
each output position requires only 9 multiply-accumulate operations per
input channel, compared to the 784 operations needed in our MLP layer.
This operation must be repeated for every spatial position
\((28\times 28)\) and every output channel (32).

While using fewer operations per output, the spatial structure creates
different patterns of memory access and computation that systems must
handle. These patterns influence system design, creating both challenges
and opportunities for optimization. Understanding these system-level
implications reveals why CNNs dominate computer vision despite their
apparent simplicity.

\subsection{System
Implications}\label{sec-dnn-architectures-system-implications-6b63}

CNNs exhibit distinctive system-level patterns that differ significantly
from MLP dense connectivity across all three analysis dimensions.

\textbf{Memory Requirements.} For convolutional layers, memory
requirements center around two key components: filter weights and
feature maps. Unlike MLPs that require storing full connection matrices,
CNNs use small, reusable filters. For a typical CNN processing
\(224 \times 224\) ImageNet images, a convolutional layer with 64
filters of size \(3 \times 3\) requires storing only 576 weight
parameters (\(3 \times 3 \times 64\)), dramatically less than the
millions of weights needed for equivalent fully-connected processing.
The system must store feature maps for all spatial positions, creating a
different memory demand. A \(224 \times 224\) input with 64 output
channels requires storing 3.2 million activation values
(\(224 \times 224 \times 64\)).

These memory access patterns suggest opportunities for optimization
through weight reuse and careful feature map management. Processors
optimize these spatial patterns by caching filter weights for reuse
across positions while streaming feature map data. Frameworks implement
spatial optimizations through specialized memory layouts that enable
filter reuse and spatial locality in feature map access. CPUs and GPUs
approach this differently. CPUs use their cache hierarchy to keep
frequently used filters resident, while GPUs employ specialized memory
architectures designed for the spatial access patterns of image
processing. The detailed architecture design principles for these
specialized processors are covered in \textbf{?@sec-ai-acceleration}.

\textbf{Computation Needs.} The core computation in CNNs involves
repeatedly applying small filters across spatial positions. Each output
value requires a local multiply-accumulate operation over the filter
region. For ImageNet processing with \(3 \times 3\) filters and 64
output channels, computing one spatial position involves 576
multiply-accumulates (\(3 \times 3 \times 64\)), and this must be
repeated for all 50,176 spatial positions (\(224 \times 224\)). While
each individual computation involves fewer operations than an MLP layer,
the total computational load remains large due to spatial repetition.

This computational pattern presents different optimization opportunities
than MLPs. The regular, repeated nature of convolution operations
enables efficient hardware utilization through structured parallelism.
Modern processors exploit this pattern in various ways. CPUs leverage
SIMD instructions\sidenote{\textbf{SIMD (Single Instruction, Multiple
Data)}: CPU instructions that perform the same operation on multiple
data elements simultaneously. Modern x86 processors support AVX-512,
enabling 16 single-precision operations per instruction, a 16x speedup
over scalar code. SIMD is important for efficient neural network
inference on CPUs, especially for edge deployment. Deep learning
frameworks further optimize this through specialized convolution
algorithms that transform the computation to better match hardware
capabilities. } to process multiple filter positions simultaneously,
while GPUs parallelize computation across spatial positions and
channels. The model optimization techniques that further reduce these
computational demands, including specialized convolution optimizations
and sparsity patterns, are detailed in \textbf{?@sec-model-compression}.

\textbf{Data Movement.} The sliding window pattern of convolutions
creates a distinctive data movement profile. Unlike MLPs where each
weight is used once per forward pass, CNN filter weights are reused many
times as the filter slides across spatial positions. For ImageNet
processing, each \(3 \times 3\) filter weight is reused 50,176 times
(once for each position in the \(224 \times 224\) feature map). This
creates a different challenge: the system must stream input features
through the computation unit while keeping filter weights stable.

The predictable spatial access pattern enables strategic data movement
optimizations. Different architectures handle this movement pattern
through specialized mechanisms. CPUs maintain frequently used filter
weights in cache while streaming through input features. GPUs employ
memory architectures optimized for spatial locality and provide hardware
support for efficient sliding window operations. Deep learning
frameworks orchestrate these movements by organizing computations to
maximize filter weight reuse and minimize redundant feature map
accesses.

\subsection{Efficient Architectures: Keyword
Spotting}\label{sec-dnn-architectures-efficient-architectures-keyword-spotting-0625}

The system implications discussed above assume standard CNN
architectures with full convolutions. However, standard CNNs achieve
high accuracy at a computational cost
(\(O(N \times K^2 \times C_{in} \times C_{out})\)) that is often
prohibitive for always-on edge devices. Keyword Spotting (KWS)
applications, such as wake-word detection (``Hey Siri''), must operate
on microcontrollers with only kilobytes of memory and milliwatt power
budgets.

To bridge this gap, efficient architectures like \textbf{Depthwise
Separable CNNs (DS-CNN)} decompose the standard convolution into two
cheaper operations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Depthwise Convolution:} Filters apply to each input channel
  independently (\(K \times K \times C_{in}\) parameters).
\item
  \textbf{Pointwise Convolution:} A \(1 \times 1\) convolution projects
  channels to the output dimension
  (\(1 \times 1 \times C_{in} \times C_{out}\) parameters).
\end{enumerate}

This decomposition reduces parameter count and FLOPs by a factor of
roughly \(1/K^2\), making real-time audio processing feasible on tiny
hardware. \emph{KWS} thus serves as the chapter's \emph{TinyML
lighthouse}, illustrating power-constrained design at its most extreme.

\phantomsection\label{callout-lighthouseux2a-1.13}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{KWS (TinyML Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.13}
\textbf{Why it matters:} Keyword Spotting models (like DS-CNN) represent
the \textbf{power-constrained} end of the spectrum. Used in always-on
applications like \textbf{Smart Doorbells} (which often pair KWS with
Wake Vision), these models must run on microcontrollers with milliwatt
power budgets.

KWS forces engineers to count every byte and cycle. It is the lighthouse
for \textbf{extreme quantization} (int8/int4) and specialized
architectural primitives (Depthwise Separable Convolutions) that trade
theoretical representational power for maximum efficiency per watt.

\end{fbxSimple}

\section{RNNs: Sequential Pattern
Processing}\label{sec-dnn-architectures-rnns-sequential-pattern-processing-f804}

CNNs demonstrate how architectural constraints can transform
computational challenges into efficiency gains for spatially structured
data. Yet their core assumption, that nearby elements are most relevant,
fails when patterns depend on temporal order rather than spatial
proximity. While CNNs excel at recognizing "what" is present in data
through shared feature detectors, they cannot capture "when" events
occur or how they relate across time. This limitation manifests in
domains such as natural language processing, where word meaning depends
on sentential context, and time-series analysis, where future values
depend on historical patterns.

Sequential data presents a challenge distinct from spatial processing:
patterns can span arbitrary temporal distances, rendering fixed-size
kernels ineffective. While spatial convolution leverages the principle
that nearby pixels are typically related, temporal relationships operate
differently. Important connections may span hundreds or thousands of
time steps with no correlation to proximity. Traditional feedforward
architectures, including CNNs, process each input independently and
cannot maintain the temporal context necessary for these long-range
dependencies.

\textbf{Recurrent}\sidenote{\textbf{Recurrent}: From Latin
\emph{recurrere} meaning ``to run back'' or ``return,'' combining
\emph{re-} (back) and \emph{currere} (to run). The term captures the
architecture's defining feature: information ``runs back'' through time
via connections that loop output back to input. This circular flow
creates the memory mechanism that distinguishes RNNs from feedforward
networks, though the same looping structure creates the sequential
dependencies that limit parallelization. } \textbf{Neural Networks}
address this architectural limitation
(\citeproc{ref-elman1990finding}{Elman 1990};
\citeproc{ref-hochreiter1997long}{Hochreiter and Schmidhuber 1997}) by
embodying a temporal inductive bias: they assume sequential dependence,
where the order of information matters and the past influences the
present.

This architectural assumption guides the introduction of memory as a
component of the computational model. Rather than processing inputs in
isolation, RNNs maintain an internal state that propagates information
from previous time steps, allowing the network to condition its current
output on historical context. This architecture embodies another
trade-off: while CNNs sacrifice theoretical generality for spatial
efficiency, \emph{recurrent neural networks} introduce computational
dependencies that challenge parallel execution in exchange for temporal
processing capabilities.

\phantomsection\label{callout-definitionux2a-1.14}
\begin{fbxSimple}{callout-definition}{Definition:}{Recurrent Neural Networks}
\phantomsection\label{callout-definition*-1.14}
\textbf{\emph{Recurrent Neural Networks}} are architectures defined by
\textbf{Sequential State}. They compress variable-length histories into
a fixed-size vector via recurrence (\(h_t = f(h_{t-1}, x_t)\)), trading
\textbf{Parallelism} for \textbf{Inference Memory Efficiency}---only the
current hidden state is needed to generate the next token (\(O(1)\)
vs.~attention's \(O(N)\) KV cache)---but suffering from \textbf{Gradient
Instability} over long horizons.

\end{fbxSimple}

\subsection{Pattern Processing
Needs}\label{sec-dnn-architectures-pattern-processing-needs-241d}

Sequential pattern processing addresses scenarios where current input
interpretation depends on preceding information. In natural language
processing, word meaning often depends heavily on previous words in the
sentence. Context determines interpretation, as evidenced by the varying
meanings of words based on surrounding terms. Similarly, in speech
recognition, phoneme interpretation depends on surrounding sounds, while
financial forecasting requires understanding historical data patterns.

The challenge in sequential processing lies in maintaining and updating
relevant context over time. Human text comprehension does not restart
with each word; rather, a running understanding evolves as new
information arrives. Similarly, time-series data processing encounters
patterns spanning different timescales, from immediate dependencies to
long-term trends. An effective sequential architecture must therefore
maintain state over time while updating it in response to new inputs.

These requirements translate into specific architectural demands: the
system must maintain internal state to capture temporal context, update
this state based on new inputs, and learn which historical information
is relevant for current predictions. Unlike MLPs and CNNs, which process
fixed-size inputs, sequential processing must accommodate
variable-length sequences while maintaining computational efficiency.

\subsection{Algorithmic
Structure}\label{sec-dnn-architectures-algorithmic-structure-9dea}

The pattern processing requirements above demand an architecture that
maintains and updates state over time. RNNs address this through
recurrent connections, distinguishing them from MLPs and CNNs. Rather
than merely mapping inputs to outputs, RNNs maintain an internal state
updated at each time step, creating a memory mechanism that propagates
information forward in time. This temporal dependency modeling
capability was first explored by Elman
(\citeproc{ref-elman1990finding}{1990}), who demonstrated RNN capacity
to identify structure in time-dependent data. Basic RNNs suffer from the
vanishing gradient problem\sidenote{\textbf{Vanishing Gradient Problem}:
During backpropagation through time, gradients shrink exponentially as
they propagate backward through RNN layers. When recurrent weights have
magnitude \textless{} 1, gradients multiply by values \textless{} 1 at
each time step, vanishing after 5-10 steps and preventing learning of
long-term dependencies---a key limitation solved by LSTMs and attention
mechanisms. }, constraining their ability to learn long-term
dependencies.

The core operation in a basic RNN can be expressed mathematically as: \[
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\] where \(\mathbf{h}_t\) denotes the hidden state at time \(t\),
\(\mathbf{x}_t\) denotes the input at time \(t\), \(\mathbf{W}_{hh}\)
contains the recurrent weights, and \(\mathbf{W}_{xh}\) contains the
input weights. Figure~\ref{fig-rnn} visualizes the unfolded network
structure, making explicit the temporal dependencies that this
recurrence creates.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0f60957e288d0417698bba5fb6968c63aa643d75.pdf}}

}

\caption{\label{fig-rnn}\textbf{Recurrent Neural Network Unfolding}:
Left panel shows the compact recurrent loop; right panel unfolds this
across time steps. Three weight matrices are shared across all steps:
\(W_{hx}\) (input-to-hidden), \(W_{hh}\) (hidden-to-hidden), and
\(W_{yh}\) (hidden-to-output). This weight sharing keeps parameter count
constant at \(O(h^2)\) regardless of sequence length. For a
128-dimensional hidden state, each time step requires 16,384 MACs for
recurrent connections plus 12,800 for input projection.}

\end{figure}%

In word sequence processing, each word may be represented as a
100-dimensional vector (\(\mathbf{x}_t\)), with a hidden state of 128
dimensions (\(\mathbf{h}_t\)). At each time step, the network combines
the current input with its previous state to update its sequential
understanding, establishing a memory mechanism capable of capturing
patterns across time steps.

This recurrent structure fulfills sequential processing requirements
through connections that maintain internal state and propagate
information forward in time. Rather than processing all inputs
independently, RNNs process sequential data by iteratively updating a
hidden state based on the current input and the previous hidden state.
This architecture suits tasks including language modeling, speech
recognition, and time-series forecasting.

RNNs implement a recursive algorithm where each time step's function
call depends on the result of the previous call. Analogous to recursive
functions that maintain state through the call stack, RNNs maintain
state through their hidden vectors. The mathematical formula
\(\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)\) directly parallels
recursive function definitions where
\texttt{f(n)\ =\ g(f(n-1),\ input(n))}. This correspondence explains RNN
capacity to handle variable-length sequences: just as recursive
algorithms process lists of arbitrary length by applying the same
function recursively, RNNs process sequences of any length by applying
the same recurrent computation.

\subsubsection{Efficiency and
Optimization}\label{sec-dnn-architectures-efficiency-optimization-6e2e}

Sequential processing creates computational bottlenecks but produces
unique efficiency characteristics for memory usage. RNNs achieve
constant memory overhead for hidden state storage regardless of sequence
length, making them extremely memory-efficient for long sequences. While
Transformers require O(n²) memory for sequence length n, RNNs maintain
fixed memory usage, allowing processing of sequences thousands of steps
long on modest hardware.

RNNs exhibit computational redundancy that optimization techniques can
exploit. The recurrent weight matrix often contains connections with
minimal contribution to temporal dependencies, allowing significant
compression through methods covered in \textbf{?@sec-model-compression}.

Sequential processing introduces unique challenges for low-precision
computation, as errors can accumulate through time. The quantization
strategies that address these challenges are examined in
\textbf{?@sec-model-compression}.

\subsection{Computational
Mapping}\label{sec-dnn-architectures-computational-mapping-701d}

RNN sequential processing creates computational patterns different from
both MLPs and CNNs, extending the architectural diversity discussed in
Section~\ref{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e}.
This implementation approach shows temporal dependencies translating
into specific computational requirements.

Listing~\ref{lst-rnn_layer_step} demonstrates the operation using
high-level matrix operations found in deep learning frameworks. The
function handles a single time step, taking the current input
\texttt{x\_t} and previous hidden state \texttt{h\_prev}, along with two
weight matrices: \texttt{W\_hh} for hidden-to-hidden connections and
\texttt{W\_xh} for input-to-hidden connections. Through matrix
multiplication operations (\texttt{matmul}), it merges the previous
state and current input to generate the next hidden state.

\begin{codelisting}

\caption{\label{lst-rnn_layer_step}\textbf{RNN Layer Abstraction}:
Framework-level implementation combining two matrix multiplications
(h\_prev x W\_hh and x\_t x W\_xh) per time step. For 128-dimensional
hidden state and 100-dimensional input, each step requires 16,384 +
12,800 = 29,184 MACs, but sequential dependencies prevent
parallelization across time.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rnn\_layer\_step(x\_t, h\_prev, W\_hh, W\_xh, b):}
    \CommentTok{\# x\_t: input at time t (batch\_size × input\_dim)}
    \CommentTok{\# h\_prev: previous hidden state (batch\_size × hidden\_dim)}
    \CommentTok{\# W\_hh: recurrent weights (hidden\_dim × hidden\_dim)}
    \CommentTok{\# W\_xh: input weights (input\_dim × hidden\_dim)}
\NormalTok{    h\_t }\OperatorTok{=}\NormalTok{ activation(matmul(h\_prev, W\_hh) }\OperatorTok{+}\NormalTok{ matmul(x\_t, W\_xh) }\OperatorTok{+}\NormalTok{ b)}
    \ControlFlowTok{return}\NormalTok{ h\_t}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Understanding RNN system implications requires examining how the elegant
mathematical abstraction translates into hardware execution patterns.
The simple recurrence relation
\texttt{h\_t\ =\ tanh(W\_hh\ h\_\{t-1\}\ +\ W\_xh\ x\_t\ +\ b)} conceals
a computational structure that creates unique challenges: sequential
dependencies that prevent parallelization, memory access patterns that
differ from feedforward networks, and state management requirements that
affect system design.

The detailed implementation in Listing~\ref{lst-rnn_layer_compute}
reveals the computational reality beneath the mathematical abstraction.
Its nested loop structure exposes how sequential processing creates both
limitations and opportunities in system optimization.

\begin{codelisting}

\caption{\label{lst-rnn_layer_compute}\textbf{RNN Layer Computation}:
Nested loops expose the sequential dependency structure. Loop 1 enables
batch parallelism, but Loops 2-3 must complete before Loop 4's
activation, and crucially, each time step depends on the previous step's
output, creating O(T) sequential depth that prevents GPU parallelization
across the time dimension.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rnn\_layer\_compute(x\_t, h\_prev, W\_hh, W\_xh, b):}
    \CommentTok{\# Initialize next hidden state}
\NormalTok{    h\_t }\OperatorTok{=}\NormalTok{ np.zeros\_like(h\_prev)}

    \CommentTok{\# Loop 1: Process each sequence in the batch}
    \ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):}
        \CommentTok{\# Loop 2: Compute recurrent contribution}
        \CommentTok{\# (h\_prev × W\_hh)}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(hidden\_dim):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(hidden\_dim):}
\NormalTok{                h\_t[batch, i] }\OperatorTok{+=}\NormalTok{ h\_prev[batch, j] }\OperatorTok{*}\NormalTok{ W\_hh[j, i]}

        \CommentTok{\# Loop 3: Compute input contribution (x\_t × W\_xh)}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(hidden\_dim):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(input\_dim):}
\NormalTok{                h\_t[batch, i] }\OperatorTok{+=}\NormalTok{ x\_t[batch, j] }\OperatorTok{*}\NormalTok{ W\_xh[j, i]}

        \CommentTok{\# Loop 4: Add bias and apply activation}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(hidden\_dim):}
\NormalTok{            h\_t[batch, i] }\OperatorTok{=}\NormalTok{ activation(h\_t[batch, i] }\OperatorTok{+}\NormalTok{ b[i])}

    \ControlFlowTok{return}\NormalTok{ h\_t}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The nested loops in \texttt{rnn\_layer\_compute} expose the core
computational pattern of RNNs. Loop 1 processes each sequence in the
batch independently, allowing for batch-level parallelism. Within each
batch item, Loop 2 computes how the previous hidden state influences the
next state through the recurrent weights \texttt{W\_hh}. Loop 3 then
incorporates new information from the current input through the input
weights \texttt{W\_xh}. Finally, Loop 4 adds biases and applies the
activation function to produce the new hidden state.

For a sequence processing task with input dimension 100 and hidden state
dimension 128, each time step requires two matrix multiplications: one
\(128\times 128\) for the recurrent connection and one \(100\times 128\)
for the input projection. While individual time steps can process in
parallel across batch elements, the time steps themselves must execute
sequentially, producing a computational pattern with fundamentally
different parallelization characteristics than MLPs or CNNs.

\subsection{System
Implications}\label{sec-dnn-architectures-system-implications-18c3}

Following the analytical framework established for MLPs, RNNs exhibit
distinctive patterns in memory requirements, computation needs, and data
movement that differ significantly from both dense and spatial
processing architectures.

\textbf{Memory Requirements.} RNNs require storing two sets of weights
(input-to-hidden and hidden-to-hidden) along with the hidden state. For
the example with input dimension 100 and hidden state dimension 128,
this requires storing 12,800 weights for input projection
\((100\times 128)\) and 16,384 weights for recurrent connections
\((128\times 128)\). Unlike CNNs where weights are reused across spatial
positions, RNN weights are reused across time steps. The system must
maintain the hidden state, which constitutes a key factor in memory
usage and access patterns.

These memory access patterns create a different profile from MLPs and
CNNs. Processors optimize sequential patterns by maintaining weight
matrices in cache while streaming through temporal elements. Frameworks
optimize temporal processing by batching sequences and managing hidden
state storage between time steps. CPUs and GPUs approach this through
different strategies; CPUs leverage their cache hierarchy for weight
reuse; meanwhile, GPUs use specialized memory architectures designed for
maintaining state across sequential operations. The specialized hardware
optimizations for sequential processing, including memory banking and
pipeline architectures, are detailed in \textbf{?@sec-ai-acceleration}.

\textbf{Computation Needs.} The core computation in RNNs involves
repeatedly applying weight matrices across time steps. For each time
step, we perform two matrix multiplications: one with the input weights
and one with the recurrent weights. In our example, processing a single
time step requires 12,800 multiply-accumulates for the input projection
\((100\times 128)\) and 16,384 multiply-accumulates for the recurrent
connection \((128\times 128)\).

This computational pattern differs from both MLPs and CNNs in a
fundamental respect: while batch elements parallelize freely, time steps
cannot overlap because each depends on the previous step's hidden state.
The resulting tension between sequential algorithmic dependencies and
the parallel execution capabilities of modern hardware defines the
central systems challenge of recurrent architectures.

Processors address sequential constraints through specialized
approaches. CPUs pipeline operations within time steps while maintaining
temporal ordering. GPUs batch multiple sequences together to maintain
high throughput despite sequential dependencies. Software frameworks
optimize this further by techniques like sequence packing and unrolling
computations across multiple time steps when possible, enabling more
efficient utilization of parallel processing resources while respecting
the sequential constraints inherent in recurrent architectures.

\textbf{Data Movement.} The sequential processing in RNNs creates a
distinctive data movement pattern that differs from both MLPs and CNNs.
While MLPs need each weight only once per forward pass and CNNs reuse
weights across spatial positions, RNNs reuse their weights across time
steps while requiring careful management of the hidden state data flow.

For our example with a 128-dimensional hidden state, each time step
must: load the previous hidden state (128 values), access both weight
matrices (29,184 total weights from both input and recurrent
connections), and store the new hidden state (128 values). This pattern
repeats for every element in the sequence. Unlike CNNs where we can
predict and prefetch data based on spatial patterns, RNN data movement
is driven by temporal dependencies.

Different architectures handle this sequential data movement through
specialized mechanisms. CPUs maintain weight matrices in cache while
streaming through sequence elements and managing hidden state updates.
GPUs employ memory architectures optimized for maintaining state
information across sequential operations while processing multiple
sequences in parallel. Deep learning frameworks orchestrate these
movements by managing data transfers between time steps and optimizing
batch operations.

While RNNs established concepts for sequential processing, their
architectural constraints create bottlenecks: sequential dependencies
prevent parallelization across time steps, fixed-capacity hidden states
create information bottlenecks for long sequences, and temporal
proximity assumptions break down when important relationships span
distant positions. These limitations motivated the development of
attention mechanisms, which eliminate sequential processing constraints
through dynamic, content-dependent connectivity. The following section
examines how attention mechanisms address each of these RNN limitations
while introducing new computational challenges. This extensive treatment
reflects attention mechanisms' dominance in modern ML systems and their
fundamental reimagining of sequential pattern processing.

\section{Attention Mechanisms: Dynamic Pattern
Processing}\label{sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-22df}

These RNN bottlenecks become concrete with a simple example. Consider
the sentence ``The cat, which was sitting by the window overlooking the
garden, was sleeping.'' Here, ``cat'' and ``sleeping'' are separated by
multiple intervening words, yet they form the core subject-predicate
relationship. An RNN would process all intervening elements
sequentially, potentially losing this connection in its fixed-capacity
hidden state. What if, instead of processing tokens in order, the
architecture could directly compute the relevance between any two
positions regardless of distance?

\textbf{Attention mechanisms} emerged as the answer to this question
(\citeproc{ref-bahdanau2014neural}{Bahdanau, Cho, and Bengio 2014}) by
introducing dynamic connectivity patterns that adapt based on input
content. Rather than processing elements in predetermined order with
fixed relationships, attention mechanisms compute the relevance between
all pairs of elements and weight their interactions accordingly,
replacing structural constraints with learned, data-dependent processing
patterns.

\phantomsection\label{callout-definitionux2a-1.15}
\begin{fbxSimple}{callout-definition}{Definition:}{Attention Mechanisms}
\phantomsection\label{callout-definition*-1.15}
\textbf{\emph{Attention Mechanisms}} are \textbf{Content-Addressable
Memory} systems. They replace fixed structural connectivity with
dynamic, data-dependent routing (\(softmax(QK^T)V\)), allowing
information to flow between any two tokens in constant \textbf{Depth}
(one layer, vs.~\(O(N)\) sequential steps in RNNs), at the cost of
\textbf{\(O(N^2)\)} compute and memory per layer.

\end{fbxSimple}

While attention mechanisms were initially used as components within
recurrent architectures, the
\textbf{Transformer}\sidenote{\textbf{Transformer}: Named by co-author
Jakob Uszkoreit, who simply ``liked the sound of the word.'' The paper's
title ``Attention Is All You Need'' references the Beatles song ``All
You Need Is Love.'' Despite the casual naming, the architecture proved
transformative: cited over 173,000 times as of early 2025, it became the
foundation for GPT, BERT, and virtually all modern large language
models. } architecture (\citeproc{ref-vaswani2017attention}{Vaswani et
al. 2025}) demonstrated that attention alone could entirely replace
sequential processing, creating a new architectural paradigm. This
paradigm deserves a formal definition.

\phantomsection\label{callout-definitionux2a-1.16}
\begin{fbxSimple}{callout-definition}{Definition:}{Transformers}
\phantomsection\label{callout-definition*-1.16}
\textbf{\emph{Transformers}} are the architectural paradigm of
\textbf{Parallel Sequence Processing}. By eliminating recurrence in
favor of global self-attention, they decouple \textbf{Sequence Length}
from \textbf{Compute Depth}, enabling massive parallelization (training
efficiency) at the cost of quadratic memory scaling (inference
bottleneck).

\end{fbxSimple}

\subsection{Pattern Processing
Needs}\label{sec-dnn-architectures-pattern-processing-needs-4b64}

Dynamic pattern processing addresses scenarios where relationships
between elements are not fixed by architecture but instead emerge from
content. Language translation exemplifies this challenge: when
translating ``the bank by the river,'' understanding ``bank'' requires
attending to ``river,'' but in ``the bank approved the loan,'' the
important relationship is with ``approved'' and ``loan.'' Unlike RNNs
that process information sequentially or CNNs that use fixed spatial
patterns, an architecture is required that can dynamically determine
which relationships matter.

Expanding beyond language, this requirement for dynamic processing
appears across many domains. In protein structure prediction,
interactions between amino acids depend on their chemical properties and
spatial arrangements. In graph analysis, node relationships vary based
on graph structure and node features, typically modeled by Graph
Convolutional Networks (GCNs) (\citeproc{ref-kipf2016semi}{Kipf and
Welling 2017}). In document analysis, connections between different
sections depend on semantic content rather than just proximity.

Synthesizing these requirements, dynamic processing demands specific
capabilities: the system must compute relationships between all pairs of
elements, weigh these relationships based on content, and use the
resulting weights to selectively combine information. Unlike previous
architectures with fixed connectivity patterns, dynamic processing
requires the flexibility to modify its computation graph based on the
input itself. These capabilities lead to the attention mechanism, which
serves as the foundation for the Transformer architecture examined in
detail in the following sections.
Figure~\ref{fig-transformer-attention-visualized} shows attention
enabling this dynamic information flow.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/629b38e86e8d9f3f306761893f1a4fd38f4ea0ba.pdf}}

}

\caption{\label{fig-transformer-attention-visualized}\textbf{Attention
Weights Visualization}: Attention head (layer 4, head 2) resolving the
pronoun ``they'' in the sentence. Line thickness indicates attention
weight magnitude: ``student'', ``The'', and ``finish'' receive equally
strong attention (bold connections), demonstrating that attention learns
to link pronouns with their referents across arbitrary distances. This
dynamic routing replaces RNN sequential processing with \(O(1)\)
information flow depth, enabling parallel computation across all 12
positions simultaneously.}

\end{figure}%

\subsection{Basic Attention
Mechanism}\label{sec-dnn-architectures-basic-attention-mechanism-1961}

Attention mechanisms shift from fixed architectural connections to
dynamic, content-based interactions between sequence elements. The
following subsections develop the mathematical foundations of attention,
examining how query-key-value operations enable flexible pattern
processing and analyzing the computational requirements that make
attention both powerful and demanding.

\subsubsection{Algorithmic
Structure}\label{sec-dnn-architectures-algorithmic-structure-8468}

The pattern processing needs described above require computing
relationships dynamically based on content. Attention mechanisms achieve
this by computing weighted connections between elements based on their
content (\citeproc{ref-bahdanau2014neural}{Bahdanau, Cho, and Bengio
2014}), processing relationships that emerge from the data itself rather
than being fixed by architecture. At the core of an attention mechanism
lies an operation that can be expressed mathematically as:

\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}
\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\]

This equation shows scaled dot-product attention. \(\mathbf{Q}\)
(queries) and \(\mathbf{K}\) (keys) are matrix-multiplied to compute
similarity scores, divided by \(\sqrt{d_k}\) (key dimension) for
numerical stability, then normalized with
softmax\sidenote{\textbf{Softmax Function}: Named as a ``soft'' version
of the maximum function, softmax approximates argmax while remaining
differentiable. The mathematical form traces back to physicist Ludwig
Boltzmann's 1868 work on statistical mechanics (hence also called the
``Boltzmann distribution''). Defined as
\(\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}\), it converts
logits to probabilities while amplifying differences between inputs. }
to get attention weights. These weights are applied to \(\mathbf{V}\)
(values) to produce the output. The result is a weighted combination
where each position receives information from all relevant positions
based on content similarity.

In this equation, \(\mathbf{Q}\) (queries), \(\mathbf{K}\) (keys), and
\(\mathbf{V}\) (values)\sidenote{\textbf{Query-Key-Value Attention}:
Inspired by information retrieval systems where queries search through
keys to retrieve values. In neural attention, queries and keys compute
similarity scores (like a search engine matching queries to documents),
while values contain the actual information to retrieve---a design that
enables flexible, content-based information access. } represent learned
projections of the input. For a sequence of length \(N\) with dimension
\(d\), this operation creates an \(N\times N\) attention matrix,
determining how each position should attend to all others.

The attention operation involves several key steps. First, it computes
query, key, and value projections for each position in the sequence.
Next, as Figure~\ref{fig-attention} illustrates, it generates an
\(N\times N\) attention matrix through query-key interactions. Finally,
it uses these attention weights to combine value vectors, producing the
output.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/d85ba7c784657c268455f9ae8af55b194eb4b717.pdf}}

}

\caption{\label{fig-attention}\textbf{Query-Key-Value Attention
Mechanism}: For a 6-token sequence, queries (cyan) match against keys
(red) to produce a \(6 \times 6\) attention matrix with \(O(N^2)\)
entries. Color intensity indicates attention weight: darker cells show
stronger relationships. Each output position aggregates information from
all values (green) weighted by its attention row. The matrix structure
reveals both the computational pattern (36 similarity computations) and
the memory bottleneck (storing \(N^2\) attention weights). Source:
Transformer Explainer (\citeproc{ref-transformer_explainer}{A. Cho et
al. 2025}).}

\end{figure}%

Unlike the fixed weight matrices found in previous architectures,
attention weights are computed dynamically for each input.
Figure~\ref{fig-attention-weightcalc} demonstrates this dynamic
computation, showing how the model adapts its processing based on the
specific content.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/bbad563b00833b8dfd848e68912149faad1f280c.pdf}}

}

\caption{\label{fig-attention-weightcalc}\textbf{QKV Projection
Computation}: The embedding matrix \((6 \times 768)\) multiplies with
QKV weight matrices \((768 \times 2304)\) plus bias to produce combined
projections \((6 \times 2304)\). The 2304 output dimension contains
concatenated query, key, and value projections (each 768-dimensional).
This single batched matrix multiplication, requiring
\(6 \times 768 \times 2304 = 10.6\) million MACs, replaces three
separate projection operations for efficiency. Source: Transformer
Explainer (\citeproc{ref-transformer_explainer}{A. Cho et al. 2025}).}

\end{figure}%

\subsubsection{Computational
Mapping}\label{sec-dnn-architectures-computational-mapping-13cd}

Attention mechanisms create computational patterns that differ
significantly from previous architectures.
Listing~\ref{lst-attention_layer_compute} reveals how dynamic
connectivity translates into specific computational requirements,
exposing the nested loops that implement pairwise attention scoring.

\begin{codelisting}

\caption{\label{lst-attention_layer_compute}\textbf{Attention
Computation}: Two implementations showing the same O(N\^{}2 x d)
complexity. The matrix form (top) uses optimized GEMM, while the nested
loops (bottom) expose the quadratic pairwise comparisons: for sequence
length 512 and dimension 64, computing attention scores requires 512 x
512 x 64 = 16.8 million MACs per attention head, plus another 16.8M for
value aggregation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ attention\_layer\_matrix(Q, K, V):}
    \CommentTok{\# Q, K, V: (batch\_size × seq\_len × d\_model)}
\NormalTok{    scores }\OperatorTok{=}\NormalTok{ matmul(Q, K.transpose(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{)) }\OperatorTok{/}\NormalTok{ sqrt(}
\NormalTok{        d\_k}
\NormalTok{    )  }\CommentTok{\# Compute attention scores}
\NormalTok{    weights }\OperatorTok{=}\NormalTok{ softmax(scores)  }\CommentTok{\# Normalize scores}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ matmul(weights, V)  }\CommentTok{\# Combine values}
    \ControlFlowTok{return}\NormalTok{ output}


\CommentTok{\# Core computational pattern}
\KeywordTok{def}\NormalTok{ attention\_layer\_compute(Q, K, V):}
    \CommentTok{\# Initialize outputs}
\NormalTok{    scores }\OperatorTok{=}\NormalTok{ np.zeros((batch\_size, seq\_len, seq\_len))}
\NormalTok{    outputs }\OperatorTok{=}\NormalTok{ np.zeros\_like(V)}

    \CommentTok{\# Loop 1: Process each sequence in batch}
    \ControlFlowTok{for}\NormalTok{ b }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):}
        \CommentTok{\# Loop 2: Compute attention for each query position}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_len):}
            \CommentTok{\# Loop 3: Compare with each key position}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_len):}
                \CommentTok{\# Compute attention score}
                \ControlFlowTok{for}\NormalTok{ d }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(d\_model):}
\NormalTok{                    scores[b, i, j] }\OperatorTok{+=}\NormalTok{ Q[b, i, d] }\OperatorTok{*}\NormalTok{ K[b, j, d]}
\NormalTok{                scores[b, i, j] }\OperatorTok{/=}\NormalTok{ sqrt(d\_k)}

        \CommentTok{\# Apply softmax to scores}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_len):}
\NormalTok{            scores[b, i] }\OperatorTok{=}\NormalTok{ softmax(scores[b, i])}

        \CommentTok{\# Loop 4: Combine values using attention weights}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_len):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_len):}
                \ControlFlowTok{for}\NormalTok{ d }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(d\_model):}
\NormalTok{                    outputs[b, i, d] }\OperatorTok{+=}\NormalTok{ scores[b, i, j] }\OperatorTok{*}\NormalTok{ V[b, j, d]}

    \ControlFlowTok{return}\NormalTok{ outputs}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The translation from attention's mathematical elegance to hardware
execution reveals the computational price of dynamic connectivity. While
the attention equation
\texttt{Attention(Q,K,V)\ =\ softmax(QK\^{}T/√d\_k)V} appears as a
straightforward matrix operation, the physical implementation requires
orchestrating quadratic numbers of pairwise computations that create
different system demands than previous architectures.

The nested loops in \texttt{attention\_layer\_compute} expose
attention's true computational signature. The first loop processes each
sequence in the batch independently. The second and third loops compute
attention scores between all pairs of positions, creating the quadratic
computation pattern that makes attention both powerful and
computationally demanding. The fourth loop uses these attention weights
to combine values from all positions, completing the dynamic
connectivity pattern that defines attention mechanisms.

\subsubsection{System
Implications}\label{sec-dnn-architectures-system-implications-05a3}

Attention mechanisms exhibit distinctive system-level patterns that
differ from previous architectures through their dynamic connectivity
requirements.

\textbf{Memory Requirements.} Attention mechanisms require storage for
attention weights, key-query-value projections, and intermediate feature
representations. For a sequence length \(N\) and dimension d, each
attention layer must store an \(N\times N\) attention weight matrix for
each sequence in the batch, three sets of projection matrices for
queries, keys, and values (each sized \(d\times d\)), and input and
output feature maps of size \(N\times d\). The dynamic generation of
attention weights for every input creates a memory access pattern where
intermediate attention weights become a significant factor in memory
usage, producing a \emph{quadratic bottleneck} that defines modern
Transformer scaling limits. The following calculation illustrates how
quickly this bottleneck manifests at scale.

\phantomsection\label{callout-notebookux2a-1.17}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Quadratic Bottleneck}
\phantomsection\label{callout-notebook*-1.17}

\textbf{Problem}: Calculate the memory required for the attention matrix
of a single layer with sequence length \(N=100\,000\) (context window).

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Matrix Size}: The attention score matrix (\(QK^T\)) has
  dimensions \(N \times N\).
\item
  \textbf{Elements}: \(100\,000 \times 100\,000 = 1e10\) elements.
\item
  \textbf{Memory}: At FP16 (2 bytes/element):
  \(1e10 \times 2 \text{ bytes} =\) \textbf{20 GB}.
\end{enumerate}

\textbf{The Systems Conclusion}: A single layer's attention matrix
consumes \textbf{20 GB} of HBM. A 32-layer model would require
\textbf{640 GB} just for transient attention scores, far exceeding any
single GPU's capacity. This \textbf{Memory Wall} collision forces the
use of:

\begin{itemize}
\tightlist
\item
  \textbf{FlashAttention} (tiling to avoid materializing the full
  matrix).
\item
  \textbf{Sparse Attention} (computing only a subset of scores).
\end{itemize}

\end{fbxSimple}

\textbf{Computation Needs.} Attention computation divides into two main
phases: generating attention weights and applying them to values. For
each attention layer, the system performs many multiply-accumulate
operations across multiple computational stages. The query-key
interactions alone require \(N\times N\times d\) multiply-accumulates,
with an equal number needed for applying attention weights to values.
Additional computations are required for the projection matrices and
softmax operations. This computational pattern differs from previous
architectures due to its quadratic scaling with sequence length and the
need to perform fresh computations for each input.

\textbf{Data Movement.} Data movement in attention mechanisms presents
distinct challenges. Each attention operation involves projecting and
moving query, key, and value vectors for each position, storing and
accessing the full attention weight matrix, and coordinating value
vector movement during the weighted combination phase. Intermediate
attention weights become a major factor in system bandwidth
requirements. Unlike the more predictable access patterns of CNNs or the
sequential access of RNNs, attention operations require frequent
movement of dynamically computed weights across the memory hierarchy.

These distinctive memory, computation, and data movement characteristics
shape system design and optimization decisions. A natural question
follows: if attention provides such powerful dynamic connectivity, could
it replace other architectural components entirely?

\phantomsection\label{callout-checkpointux2a-1.18}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{The Quadratic Bottleneck}
\phantomsection\label{callout-checkpoint*-1.18}

Modern AI scaling is defined by the cost of Attention. Verify your
intuition:

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Complexity}: Do you understand why doubling the sequence
  length quadruples the memory required for the Attention Matrix?
\item[$\square$]
  \textbf{Implication}: This \(O(N^2)\) cost is the primary motivator
  for systems optimizations like \textbf{FlashAttention}
  (\textbf{?@sec-ai-training}) and \textbf{KV-Caching}
  (\textbf{?@sec-model-serving-systems}).
\end{itemize}

\end{fbxSimple}

\subsection{Transformers: Attention-Only
Architecture}\label{sec-dnn-architectures-transformers-attentiononly-architecture-1b56}

While attention mechanisms introduced the concept of dynamic pattern
processing, they were initially applied as additions to existing
architectures, particularly RNNs for sequence-to-sequence tasks. This
hybrid approach still suffered from the fundamental limitations of
recurrent architectures: sequential processing constraints that
prevented efficient parallelization and difficulties with very long
sequences. The breakthrough insight was recognizing that attention
mechanisms alone could replace both convolutional and recurrent
processing entirely.

Transformers, introduced in the landmark "Attention is All You Need"
paper\sidenote{\textbf{``Attention is All You Need''}: This 2017 paper
by Google researchers eliminated recurrence entirely, showing that
attention mechanisms alone could achieve state-of-the-art results. The
title itself became a rallying cry, and within 5 years,
transformer-based models achieved breakthrough performance in language
(GPT, BERT), vision (ViT), and beyond
(\citeproc{ref-radford2018improving}{Radford et al. 2018};
\citeproc{ref-devlin2018bert}{Devlin et al. 2018};
\citeproc{ref-dosovitskiy2021image}{Dosovitskiy et al. 2021}). This
paper marked a historical turning point in deep learning, demonstrating
that the sequential processing that defined RNNs and LSTMs was no longer
necessary; attention mechanisms could capture both short and long-range
dependencies through parallel computation. While the basic attention
mechanism allows for content-based weighting of information from a
source sequence, Transformers extend this idea by applying attention
within a single sequence, enabling each element to attend to all other
elements including itself. } by Vaswani et al.
(\citeproc{ref-vaswani2017attention}{2025}), embody a revolutionary
inductive bias: \textbf{they assume no prior structure but allow the
model to learn all pairwise relationships dynamically based on content}.
This architectural assumption represents the culmination of the
architectural evolution detailed in
Section~\ref{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e}
by eliminating all structural constraints in favor of pure
content-dependent processing. Rather than adding attention to RNNs,
Transformers built the entire architecture around attention mechanisms,
introducing self-attention as the primary computational pattern. This
architectural decision traded the parameter efficiency of CNNs and the
sequential coherence of RNNs for maximum flexibility and
parallelizability.

The architectural evolution reaches its current culmination with
Transformers, which eliminate structural constraints in favor of pure
content-dependent processing. The progression moves from MLPs that
connect everything, to CNNs that connect locally, to RNNs that connect
sequentially, to Transformers that connect dynamically based on learned
content relationships. Each iteration refined the balance between
flexibility and efficiency.

\subsubsection{Algorithmic
Structure}\label{sec-dnn-architectures-algorithmic-structure-60ef}

The key innovation in Transformers lies in their use of self-attention
layers. In the self-attention mechanism used by Transformers, the Query,
Key, and Value vectors are all derived from the same input sequence.
This is the key distinction from earlier attention mechanisms where the
query might come from a decoder while the keys and values came from an
encoder. By making all components self-referential, self-attention
allows the model to weigh the importance of different positions within
the same sequence when encoding each position. For instance, in
processing the sentence ``The animal didn't cross the street because it
was too wide,'' self-attention allows the model to link ``it'' with
``street,'' capturing long-range dependencies that are challenging for
traditional sequential models.

The self-attention mechanism can be expressed mathematically in a form
similar to the basic attention mechanism: \[
\text{SelfAttention}(\mathbf{X}) = \text{softmax}
\left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}
\]

Here, \(\mathbf{X}\) is the input sequence, and \(\mathbf{W_Q}\),
\(\mathbf{W_K}\), and \(\mathbf{W_V}\) are learned weight matrices for
queries, keys, and values respectively. This formulation highlights how
self-attention derives all its components from the same input, creating
a dynamic, content-dependent processing pattern.

Building on this foundation, Transformers employ multi-head attention,
which extends the self-attention mechanism by running multiple attention
functions in parallel. Each ``head'' involves a separate set of
query/key/value projections that can focus on different aspects of the
input, allowing the model to jointly attend to information from
different representation subspaces. This multi-head structure provides
the model with a richer representational capability, enabling it to
capture various types of relationships within the data simultaneously.

The mathematical formulation for multi-head attention is: \[
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O
\] where each attention head is computed as: \[
\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
\]

A critical component in both self-attention and multi-head attention is
the scaling factor \(\sqrt{d_k}\), which serves an important
mathematical purpose. This factor prevents the dot products from growing
too large, which would push the softmax function into regions with
extremely small gradients. For queries and keys of dimension \(d_k\),
their dot product has variance \(d_k\), so dividing by \(\sqrt{d_k}\)
normalizes the variance to 1, maintaining stable gradients and enabling
effective learning.\sidenote{\textbf{Attention Scaling}: Division by
√d\_k prevents softmax saturation in attention computation. Without
scaling, dot products grow with dimension (E{[}q·k{]} = 0, Var{[}q·k{]}
= d\_k), pushing softmax toward one-hot extremes with vanishing
gradients. This mathematical insight from Vaswani et al.~(2017) enables
stable training of transformers from BERT-base (d\_k=64) to GPT-4
(d\_k=128+). }

Beyond the mathematical mechanics, attention mechanisms can be
understood conceptually as implementing a form of content-addressable
memory system. Like hash tables that retrieve values based on key
matching, attention computes similarity between a query and all
available keys, then retrieves a weighted combination of corresponding
values. The dot product similarity \texttt{Q·K} functions like a hash
function that measures how well each key matches the query. The softmax
normalization ensures the weights sum to 1, implementing a probabilistic
retrieval mechanism. This connection explains why attention proves
effective for tasks requiring flexible information retrieval: it
provides a differentiable approximation to database lookup operations.

From an information-theoretic perspective, attention mechanisms
implement optimal information aggregation under uncertainty. The
attention weights represent uncertainty about which parts of the input
contain relevant information for the current processing step. The
softmax operation implements a maximum entropy principle: among all
possible ways to distribute attention across input positions, softmax
selects the distribution with maximum entropy subject to the constraint
that similarity scores determine relative importance
(\citeproc{ref-cover2006elements}{Cover and Thomas 2001}).

\subsubsection{Efficiency and
Optimization}\label{sec-dnn-architectures-efficiency-optimization-f79c}

Attention mechanisms exhibit significant redundancy, with many heads
learning similar patterns. This architectural property creates
opportunities for optimization through pruning and factorization
techniques covered in \textbf{?@sec-model-compression}.

The softmax operation and quadratic attention scores create sensitivity
to reduced precision, requiring specialized quantization approaches
covered in \textbf{?@sec-model-compression}.

The quadratic scaling with sequence length creates efficiency
limitations that motivate architectural modifications. Sparse attention
patterns and linear attention approximations that address these
limitations are examined in \textbf{?@sec-model-compression}.

Despite these computational costs, attention's effectiveness stems from
its information-theoretic properties. This information-theoretic
interpretation reveals why attention is so effective for selective
processing. The mechanism automatically balances two competing
objectives: focusing on the most relevant information (minimizing
entropy) while maintaining sufficient breadth to avoid missing important
details (maximizing entropy). The attention pattern emerges as the
optimal trade-off between these objectives, explaining why transformers
can effectively handle long sequences and complex dependencies.

Self-attention learns dynamic activation patterns across the input
sequence. Unlike CNNs which apply fixed filters or RNNs which use fixed
recurrence patterns, attention learns which elements should activate
together based on their content. This creates a form of adaptive
connectivity where the effective network topology changes for each
input. Recent research has shown that attention heads in trained models
often specialize in detecting specific linguistic or semantic patterns
(\citeproc{ref-clark2019what}{Clark et al. 2019}), suggesting that the
mechanism naturally discovers interpretable structural regularities in
data.

The Transformer architecture leverages this self-attention mechanism
within a broader structure that typically includes feed-forward layers,
layer normalization, and residual connections.
Figure~\ref{fig-transformer} illustrates this complete architecture,
showing how these components combine to process input sequences in
parallel while capturing complex dependencies without sequential
computation. Transformers have demonstrated significant effectiveness
across a wide range of tasks, from natural language processing to
computer vision, transforming deep learning architectures across
domains.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/6577827145793c63db2677f3808b14a8056349e7.pdf}}

}

\caption{\label{fig-transformer}\textbf{Transformer Architecture
(Encoder-Decoder)}: Complete architecture from Vaswani et al.~The
encoder (left, repeated \(N\) times) consists of multi-head attention
followed by feed-forward layers, each with residual connections (arrows
bypassing blocks) and layer normalization. The decoder (right) adds
masked attention to prevent attending to future tokens during
autoregressive generation. Positional encodings (sine waves)
(\citeproc{ref-su2024roformer}{Su et al. 2024}) inject sequence order
information absent from the permutation-invariant attention operation.
This design enables training parallelism across all positions while the
decoder maintains autoregressive causality during inference. Source:
Vaswani et al. (\citeproc{ref-vaswani2017attention}{Vaswani et al.
2025}).}

\end{figure}%

\subsubsection{Computational
Mapping}\label{sec-dnn-architectures-computational-mapping-aea3}

While Transformer self-attention builds upon the basic attention
mechanism, it introduces distinct computational patterns that set it
apart. Listing~\ref{lst-self_attention_layer} presents a typical
implementation, showing how self-attention derives queries, keys, and
values from the same input sequence:

\begin{codelisting}

\caption{\label{lst-self_attention_layer}\textbf{Self-Attention and
Multi-Head Attention}: Self-attention (top) derives Q, K, V from the
same input X through three projections, then computes attention as
before. Multi-head attention (bottom) runs h parallel attention heads
with dimension d\_k = d\_model/h, then concatenates and projects. For
GPT-2 (768-dim, 12 heads), each head operates on 64 dimensions, reducing
per-head attention memory while enabling diverse relationship patterns.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ self\_attention\_layer(X, W\_Q, W\_K, W\_V, d\_k):}
    \CommentTok{\# X: input tensor (batch\_size × seq\_len × d\_model)}
    \CommentTok{\# W\_Q, W\_K, W\_V: weight matrices (d\_model × d\_k)}

\NormalTok{    Q }\OperatorTok{=}\NormalTok{ matmul(X, W\_Q)}
\NormalTok{    K }\OperatorTok{=}\NormalTok{ matmul(X, W\_K)}
\NormalTok{    V }\OperatorTok{=}\NormalTok{ matmul(X, W\_V)}

\NormalTok{    scores }\OperatorTok{=}\NormalTok{ matmul(Q, K.transpose(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{)) }\OperatorTok{/}\NormalTok{ sqrt(d\_k)}
\NormalTok{    attention\_weights }\OperatorTok{=}\NormalTok{ softmax(scores, dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ matmul(attention\_weights, V)}

    \ControlFlowTok{return}\NormalTok{ output}


\KeywordTok{def}\NormalTok{ multi\_head\_attention(X, W\_Q, W\_K, W\_V, W\_O, num\_heads, d\_k):}
\NormalTok{    outputs }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_heads):}
\NormalTok{        head\_output }\OperatorTok{=}\NormalTok{ self\_attention\_layer(}
\NormalTok{            X, W\_Q[i], W\_K[i], W\_V[i], d\_k}
\NormalTok{        )}
\NormalTok{        outputs.append(head\_output)}

\NormalTok{    concat\_output }\OperatorTok{=}\NormalTok{ torch.cat(outputs, dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)}
\NormalTok{    final\_output }\OperatorTok{=}\NormalTok{ matmul(concat\_output, W\_O)}

    \ControlFlowTok{return}\NormalTok{ final\_output}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{System
Implications}\label{sec-dnn-architectures-system-implications-77ac}

Where ResNet-50 serves as the lighthouse for compute-bound workloads,
language models reveal a fundamentally different bottleneck.
Autoregressive decoding turns Transformer inference into a
\emph{memory-bandwidth-bound} problem, making these models the ideal
reference for understanding data movement constraints.

\phantomsection\label{callout-lighthouseux2a-1.19}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{GPT-2 / Llama (Language Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.19}

\textbf{Why it matters:} LLMs like GPT-2 and Llama exemplify
\textbf{memory-bandwidth-bound} workloads. During the autoregressive
generation phase (token-by-token decoding), the system must load all
model weights from HBM to compute just one token. Because the ratio of
math to data movement is extremely low, the processor remains mostly
idle, waiting for data. This lighthouse grounds discussions on
\textbf{KV caching}, \textbf{low-precision weights} (quantization), and
the critical role of \textbf{memory bandwidth} in LLM performance.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0959}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2192}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6849}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Value}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Parameters} & 1.5 Billion & 6.0 GB at FP32, 3.0 GB at FP16; must
load all weights/token. \\
\textbf{KV Cache} & Grows with sequence length & 2 × layers × heads ×
seq\_len × head\_dim bytes per sequence. \\
\textbf{Constraint} & Memory Bandwidth & Limited by how fast data moves
from HBM to chip. \\
\textbf{Bottleneck} & Weight Loading & Parameters are loaded repeatedly
for every single token. \\
\textbf{Profile} & Low Arithmetic Intensity & Operations wait for data;
Tensor Cores are underutilized. \\
\end{longtable}

\end{fbxSimple}

This implementation reveals key computational characteristics. First,
self-attention enables parallel processing across all positions in the
sequence, as the matrix multiplications for \texttt{Q}, \texttt{K}, and
\texttt{V} execute simultaneously for all positions. Unlike recurrent
architectures that process inputs sequentially, this parallelism maps
efficiently to modern hardware.

Second, the attention score computation produces a matrix of size
\texttt{(seq\_len\ x\ seq\_len)}, creating quadratic complexity with
respect to sequence length. This quadratic relationship becomes a
significant bottleneck for long sequences, spurring research into more
efficient attention mechanisms.

Third, multi-head attention runs multiple self-attention operations in
parallel, each with its own learned projections. The computational load
increases linearly with the number of heads, but the model gains richer
representational power by capturing different types of relationships
simultaneously.

Fourth, the core computations are dominated by large matrix
multiplications involving matrices of sizes \((N\times d)\),
\((d\times d)\), and \((N\times N)\) for sequence length \(N\) and
embedding dimension \(d\). These operations map well to specialized
hardware like GPUs but contribute substantially to overall computational
cost.

Fifth, self-attention generates memory-intensive intermediate results.
The attention weights matrix \((N\times N)\) and per-head intermediate
results create large memory requirements, especially for long sequences,
requiring careful memory management for deployment on constrained
devices.

The parallel nature of Transformers makes them well-suited for modern
hardware, but quadratic complexity with sequence length constrains
long-sequence processing. Much research has therefore focused on
optimization techniques such as sparse attention patterns and low-rank
approximations. Each technique presents its own trade-offs between
computational efficiency and model expressiveness, a balance that must
be considered in practical applications. As shown in
Figure~\ref{fig-context-explosion}, this architectural capability has
driven an exponential growth in context windows, enabling models to
reason over entire books or codebases in a single pass.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/dnn_architectures/dnn_architectures_files/figure-pdf/fig-context-explosion-output-1.pdf}}

}

\caption{\label{fig-context-explosion}\textbf{The Context Explosion}:
Maximum supported context window (tokens) over time (Log Scale). The
transition from `Standard' windows (512--2k tokens) to `Massive' windows
(1M+ tokens) represents a fundamental shift in how ML systems handle
long-range dependencies, increasingly favoring in-context reasoning over
traditional retrieval-based approaches.}

\end{figure}%

This examination of four architectural families (MLPs, CNNs, RNNs, and
Transformers) reveals both their individual characteristics and their
collective evolution. Each addresses distinct data types: spatial,
sequential, and relational patterns. While CNNs and Transformers
dominate academic attention, industrial AI workloads are driven by a
fundamentally different architecture class. One critical paradigm
remains that differs fundamentally from all four: \textbf{sparse
architectures} for recommendation systems. These systems power content
feeds, product suggestions, and ad ranking for billions of users, yet
receive less academic attention than their media-processing counterparts
despite consuming the majority of industrial AI compute cycles.

\subsection{Sparse Architectures: Recommendation
Systems}\label{sec-dnn-architectures-sparse-architectures-recommendation-systems-5d19}

While CNNs and Transformers dominate media processing (vision, language,
audio), a different class of architecture consumes the majority of AI
cycles in industrial applications: recommendation systems (RecSys).
These systems power content feeds, product suggestions, and ad ranking
for billions of users. Unlike media models which are typically
\textbf{compute-bound} (limited by FLOPs), recommendation models are
uniquely \textbf{memory-capacity-bound} and
\textbf{memory-bandwidth-bound} due to their reliance on massive
embedding tables.

The core challenge in RecSys is handling high-cardinality categorical
features. A model might need to process User IDs (billions of unique
users) and Item IDs (millions of videos or products). We cannot input
these raw IDs directly into a neural network; instead, we map each ID to
a dense vector called an \textbf{embedding}\sidenote{\textbf{Embedding}:
From the mathematical concept of embedding one space into another, the
term entered ML via word2vec (2013) and related work. Just as a
topological embedding maps a space into a higher-dimensional one while
preserving structure, neural embeddings map discrete tokens (words, user
IDs) into continuous vector spaces where semantic relationships become
geometric ones: similar items cluster together, and relationships like
``king - man + woman = queen'' emerge as vector arithmetic. }
(\citeproc{ref-mikolov2013efficient}{Mikolov et al. 2013}).

\textbf{The Deep Learning Recommendation Model (DLRM).} The
\textbf{DLRM} architecture (\citeproc{ref-naumov2019deep}{Naumov et al.
2019}) standardizes this pattern, combining two distinct computational
regimes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Dense Features (Bottom MLP):} Continuous features (like user
  age, time of day) are processed by a standard Multi-Layer Perceptron
  (MLP). This component is compute-intensive but memory-light.
\item
  \textbf{Sparse Features (Embedding Tables):} Categorical features
  (User ID, Item ID) are looked up in massive embedding tables. A table
  for 1 billion users with 128-dimensional vectors requires
  \(10^9 \times 128 \times 4\) bytes ≈ 512 GB of memory. This component
  is memory-intensive but compute-light (just a memory copy).
\item
  \textbf{Interaction Layer:} The dense vectors from the MLP and the
  sparse vectors from embeddings are combined (typically via dot
  products) to capture interactions between user and item features.
\item
  \textbf{Top MLP:} The combined features are processed by another MLP
  to produce a final probability (e.g., click-through rate).
\end{enumerate}

This combination of dense and sparse computation makes \emph{DLRM} the
chapter's \emph{recommendation lighthouse}, exemplifying
memory-capacity-bound workloads.

\phantomsection\label{callout-lighthouseux2a-1.20}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{DLRM (Recommendation Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.20}

\textbf{Why it matters:} DLRM exemplifies \textbf{memory-capacity-bound}
workloads. Its massive embedding tables often exceed the memory of a
single GPU, forcing \textbf{model parallelism} (sharding tables across
devices). The interaction layer requires \textbf{all-to-all
communication}, stressing network bandwidth. This contrasts sharply with
CNNs (compute-bound) and Transformers (memory-bandwidth-bound),
requiring different hardware optimizations.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1653}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3223}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5124}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Value}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Embedding Tables} & 25 Billion & Entries × embedding\_dim × 4
bytes; dominates total model size. \\
\textbf{Model Size} & 100 GB (FP32) & Requires distributed memory (model
parallelism) to fit. \\
\textbf{Constraint} & Memory Capacity & Model size \textgreater{} Single
GPU Memory. \\
\textbf{Bottleneck} & Network Bandwidth & ``All-to-All'' communication
required to gather embeddings. \\
\textbf{Profile} & Mixed (Sparse/Dense) & Combines memory-heavy lookups
with compute-heavy MLPs. \\
\end{longtable}

\end{fbxSimple}

\textbf{System Implications: The Memory Wall.} DLRM creates a unique
systems challenge: \textbf{the model is too big to fit on a single GPU}.
While a ResNet-50 (102 MB) or even GPT-3 (350 GB at FP16 precision, or
approximately 700 GB at FP32) might fit on a single node, industrial
recommendation models can reach terabytes or petabytes in size due to
the embedding tables.

This forces a specific parallelization strategy called \textbf{Model
Parallelism} (specifically, \textbf{Embedding Sharding}):

\begin{itemize}
\tightlist
\item
  The embedding tables are split (sharded) across hundreds of GPUs.
\item
  The dense MLPs are replicated on every GPU (\textbf{Data
  Parallelism}).
\item
  During the forward pass, each GPU processes a batch of users. It looks
  up local embeddings but must request remote embeddings from other GPUs
  over the network.
\end{itemize}

The result is an \textbf{All-to-All} communication pattern: every GPU
must exchange data with every other GPU to gather the necessary
embedding vectors for its batch. Consequently, DLRM performance is often
limited not by GPU compute speed (FLOPs) but by \textbf{network
bandwidth} and \textbf{memory bandwidth}. Optimizing these systems
requires high-speed interconnects (such as NVLink or InfiniBand) and
specialized embedding caches, fundamentally different optimizations than
those used for CNNs or Transformers.

While we have examined each architecture family independently---MLPs
establishing fundamental patterns, CNNs exploiting spatial locality,
RNNs capturing temporal dependencies, Transformers enabling parallel
sequence processing, and DLRMs handling sparse categorical
data---understanding their historical relationships reveals how modern
designs evolved through systematic refinement of computational
primitives. A natural question emerges: beneath this diversity, what
computational foundations do these architectures share? Understanding
shared building blocks explains why certain optimizations transfer
across architectures (matrix tiling benefits all dense operations) while
others remain architecture-specific (FlashAttention targets transformer
attention patterns). It also reveals evolutionary relationships, showing
how each architecture innovated by recombining and refining existing
primitives rather than starting from scratch.

\section{Architectural
Evolution}\label{sec-dnn-architectures-architectural-building-blocks-aa3a}

Deep learning architectures, while presented as distinct families in the
preceding sections, are better understood as compositions of design
patterns that evolved over time. Modern neural networks combine and
iterate on core computational patterns that emerged through decades of
research (\citeproc{ref-lecun2015deep}{Yann LeCun, Bengio, and Hinton
2015}). The simple perceptron
(\citeproc{ref-rosenblatt1958perceptron}{Rosenblatt 1958}) evolved into
multi-layer networks (\citeproc{ref-rumelhart1986learning}{Rumelhart,
Hinton, and Williams 1986}), which subsequently spawned specialized
patterns for spatial and sequential processing. Each advancement
preserved useful elements from predecessors while introducing new
computational primitives; contemporary architectures such as
Transformers represent carefully engineered combinations of these
building blocks.

Table~\ref{tbl-dl-evolution} traces this evolution from early dense
matrix operations optimized for CPUs through the current era of
attention mechanisms requiring flexible accelerators and high-bandwidth
memory. Each architectural era introduced new computational primitives
that drove corresponding hardware innovations: CNNs motivated GPU
adoption, RNNs demanded sophisticated memory hierarchies, and
Transformers now require high-bandwidth memory systems to handle their
quadratic attention patterns.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2155}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2241}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3793}}@{}}
\caption{\textbf{Deep Learning Evolution}: Neural network architectures
have progressed from simple, fully connected layers to complex models
leveraging specialized hardware and addressing sequential data
dependencies. Architectural eras map to key computational primitives and
corresponding system-level optimizations, revealing a historical trend
toward increased parallelism and memory bandwidth
requirements.}\label{tbl-dl-evolution}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dominant Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Primitives}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Focus}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dominant Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Primitives}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Focus}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Early NN} & MLP & Dense Matrix Ops & CPU optimization \\
\textbf{CNN Revolution} & CNN & Convolutions & GPU acceleration \\
\textbf{Sequence Modeling} & RNN & Sequential Ops & Memory
hierarchies \\
\textbf{Attention Era} & Transformer & Attention, Dynamic Compute &
Flexible accelerators, High-bandwidth memory \\
\end{longtable}

The timing of architectural breakthroughs correlates directly with
computational capability growth. LeNet-5
(\citeproc{ref-lecun1998gradient}{Lecun et al. 1998}) trained on CPUs
with networks small enough to fit in megabytes of memory. AlexNet
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}) required GPU parallelism: its 60 million parameters and billions
of floating-point operations per image were infeasible on CPUs of that
era, but mapped naturally to GPU architectures designed for graphics
workloads with similar parallel structure. GoogLeNet/Inception
(\citeproc{ref-szegedy2015going}{Szegedy et al. 2015}) introduced
multi-scale feature extraction through parallel convolutions,
demonstrating how architectural innovations could improve efficiency.
ResNet's 152-layer depth (\citeproc{ref-he2016deep}{He et al. 2016})
became trainable only after batch normalization and skip connections
solved gradient flow at scale, exploiting the 12-16 GB memory capacity
of Pascal-era GPUs. Transformers
(\citeproc{ref-vaswani2017attention}{Vaswani et al. 2025}) became
practical precisely when GPU memory bandwidth crossed \textasciitilde900
GB/s (P100) and on-chip SRAM exceeded 20 MB---thresholds that made
quadratic attention matrices feasible for sequences of 512 tokens.
Earlier GPUs with \textasciitilde300 GB/s bandwidth would have made even
short attention layers severely memory-bound. This pattern continues:
each architectural innovation exploits newly available computational
resources while pushing against the limits of existing systems. Model
sizes have grown approximately 10× annually while hardware capabilities
improve roughly 2× per generation, creating persistent tension between
architectural ambition and system constraints.

The following subsections trace how these primitives evolved and
combined to produce increasingly capable neural network architectures.

\subsection{Evolution from Perceptron to Multi-Layer
Networks}\label{sec-dnn-architectures-evolution-perceptron-multilayer-networks-d06e}

While
Section~\ref{sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-bc11}
examined MLPs as dense pattern processors, the systems engineering
legacy of MLPs extends far beyond their direct applications. The
computational patterns established by early MLPs shaped the entire
trajectory of deep learning hardware and software.

MLPs introduced the GEMM-dominated computation profile that led GPU
vendors to develop tensor cores. The backpropagation
algorithm's\sidenote{\textbf{Backpropagation Algorithm}: While the chain
rule was known since the 1600s, Rumelhart, Hinton, and Williams
(\citeproc{ref-rumelhart1986learning}{Rumelhart, Hinton, and Williams
1986}) showed how to efficiently apply it to train multi-layer networks.
This ``learning by error propagation'' algorithm made deep networks
practical and remains virtually unchanged in modern systems. } memory
access patterns, with its alternating forward and backward passes
storing intermediate activations, influenced accelerator memory
hierarchies. The batch processing paradigm pioneered for MLP training
established the datacenter-scale throughput optimization that defines
modern ML infrastructure. These foundational patterns (dense matrix
operations, gradient-based optimization, batch-oriented processing)
appear in every architecture examined in this chapter, even when
obscured by domain-specific terminology.

The MLP paradigm also established the fundamental trade-off between
model expressiveness and computational cost that every subsequent
architecture navigates. Dense connectivity requires \(O(n^2)\)
parameters and operations for layers of width \(n\), setting a baseline
against which specialized architectures demonstrate efficiency gains.
CNNs achieve spatial processing with \(O(k^2)\) parameters per location
(where \(k\) is kernel size), transformers trade parameter efficiency
for dynamic computation with \(O(n^2)\) attention complexity, and sparse
architectures like DLRM exploit embedding lookups to handle categorical
dimensions that would explode dense layer sizes. Each innovation
represents a different strategy for escaping the dense connectivity
baseline that MLPs established.

\subsection{Evolution from Dense to Spatial
Processing}\label{sec-dnn-architectures-evolution-dense-spatial-processing-2761}

The development of CNNs marked an architectural innovation, specifically
the realization that we could specialize the dense connectivity of MLPs
for spatial patterns. While retaining the core concept of layer-wise
processing, CNNs introduced several building blocks that would influence
all future architectures.

The first key innovation was the concept of parameter sharing. Unlike
MLPs where each connection had its own weight, CNNs showed how the same
parameters could be reused across different parts of the input. This not
only made the networks more efficient but introduced the powerful idea
that architectural structure could encode useful priors about the data
(\citeproc{ref-lecun1998gradient}{Lecun et al. 1998}).

Yet parameter sharing alone could not solve the challenges of training
deep networks. As practitioners attempted to build deeper CNNs for more
complex tasks, they encountered a fundamental barrier.

\subsubsection{Gradient Flow and the Depth
Problem}\label{sec-dnn-architectures-gradient-flow-depth-problem-e644}

Before examining the architectural innovations that enabled training
very deep networks, we must understand the fundamental challenge that
depth creates: the gradient flow problem. This mathematical constraint
has profound implications for architecture design and explains why
certain architectural patterns became essential rather than optional.

\textbf{The Problem of Depth.} Backpropagation through \(L\) layers
applies the chain rule repeatedly. For a deep network with layers
\(f_1, f_2, \ldots, f_L\), the gradient of the loss \(\mathcal{L}\) with
respect to the weights in layer 1 is:

\[
\frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial a_L} \cdot \frac{\partial a_L}{\partial z_L} \cdot \frac{\partial z_L}{\partial a_{L-1}} \cdot \ldots \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1}
\]

where \(z_l\) represents the pre-activation and \(a_l = \sigma(z_l)\)
the post-activation output of layer \(l\). The gradient becomes a
product of \(L\) terms, each depending on the activation function
derivative \(\sigma'(z_l)\).

Vanishing gradients create a silent training failure in deep
architectures. For sigmoid activation functions, the derivative is
\(\sigma'(z) = \sigma(z)(1 - \sigma(z))\), with maximum value
\(\sigma'(0) = 0.25\). Through \(L\) layers, the gradient magnitude is
multiplied by approximately \((0.25)^L\). With such extreme attenuation,
early layers receive infinitesimal gradient signals. Weight updates
become negligible, effectively preventing these layers from training.

Exploding gradients are the catastrophic counterpart to vanishing
gradients. If activation function derivatives exceed 1, gradients grow
exponentially through the layers. Consider a network where each layer's
Jacobian has eigenvalues around 1.5. This exponential growth causes
numerical overflow (NaN values), extreme parameter updates, and training
divergence. Unlike vanishing gradients which silently prevent learning,
exploding gradients cause immediate training failure.

\textbf{Quantitative Analysis: Plain Deep Networks.} Consider training a
50-layer convolutional network on CIFAR-10 without architectural
interventions. Even with ReLU activations, which have derivative 1 for
positive inputs, gradient magnitudes vary dramatically across depth.
Near the output at layer 50, gradient norms measure approximately
\(\|\nabla_{W_{50}} \mathcal{L}\| \approx 0.1\). By the middle of the
network at layer 25, this has decayed to
\(\|\nabla_{W_{25}} \mathcal{L}\| \approx 0.001\). At the earliest
layer, gradients have effectively vanished to
\(\|\nabla_{W_1} \mathcal{L}\| \approx 10^{-8}\).

The training behavior reflects this gradient distribution. After 50
epochs, training loss starts at 2.3 (random chance) and improves only to
1.8, while test accuracy reaches just 45\% compared to 60\% or better
for shallow networks. The network barely learns, significantly
underperforming shallow counterparts despite its greater theoretical
capacity.

This ``degradation problem'' is not overfitting. Deeper networks train
worse than shallow ones, contradicting the intuition that more layers
should provide more representational capacity.

\textbf{Why ReLU Helps But Is Not Sufficient.} ReLU activation
(\(\text{ReLU}(z) = \max(0, z)\)) has derivative:

\[
\text{ReLU}'(z) = \begin{cases}
1 & \text{if } z > 0 \\
0 & \text{if } z \leq 0
\end{cases}
\]

Through active paths (\(z > 0\)), the derivative equals 1, avoiding
gradient decay from the activation function. This represents significant
improvement over sigmoid, enabling training of networks with 10-20
layers.

However, ReLU introduces a different problem: dead neurons. When
\(z \leq 0\), the gradient is exactly zero, permanently blocking
gradient flow through that path. A poorly initialized neuron or large
gradient update can push a ReLU unit into the negative regime across all
training examples, causing it to ``die'' and never recover. ReLU does
not solve gradient flow issues arising from weight matrices themselves.
If weight matrices have eigenvalues far from 1, gradients still vanish
or explode regardless of activation function.

\textbf{The Residual Solution.} ResNet blocks introduce skip connections
that fundamentally change gradient flow. A residual block computes:

\[
y = \mathcal{F}(x) + x
\]

where \(\mathcal{F}(x)\) represents the residual function (typically two
convolutional layers with batch normalization and ReLU) and \(x\) is the
identity skip connection.

During backpropagation, the gradient flows through this addition:

\[
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial (\mathcal{F}(x) + x)}{\partial x}
\]

Applying the chain rule:

\[
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \left(\frac{\partial \mathcal{F}(x)}{\partial x} + 1\right) = \frac{\partial \mathcal{L}}{\partial y} \cdot \mathcal{F}'(x) + \frac{\partial \mathcal{L}}{\partial y}
\]

This equation reveals the critical insight: the gradient has two paths:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Residual path}:
  \(\frac{\partial \mathcal{L}}{\partial y} \cdot \mathcal{F}'(x)\) (can
  vanish if \(\mathcal{F}'(x) \to 0\))
\item
  \textbf{Identity path}: \(\frac{\partial \mathcal{L}}{\partial y}\)
  (always flows unimpeded)
\end{enumerate}

The identity term ensures that even if the residual function produces
vanishing gradients, the gradient signal
\(\frac{\partial \mathcal{L}}{\partial y}\) flows directly to earlier
layers.

\textbf{Gradient Flow Through Multiple Residual Blocks.} Through \(L\)
residual blocks, the gradient becomes:

\[
\frac{\partial \mathcal{L}}{\partial x_0} = \frac{\partial \mathcal{L}}{\partial x_L} \cdot \prod_{l=1}^{L} \left(\mathcal{F}'_l(x_l) + 1\right)
\]

Each factor \((\mathcal{F}'_l + 1)\) has expectation at least 1
(assuming \(\mathcal{F}'_l\) is non-negative on average). Unlike plain
networks where gradients multiply factors potentially less than 1,
ResNets multiply factors that maintain or increase gradient magnitude.
This mathematical property allows training of networks with 100+ layers.

\textbf{Analysis and Validation.} Consider each layer as a function
\(f_l: \mathbb{R}^d \to \mathbb{R}^d\) with Jacobian
\(J_l = \frac{\partial f_l}{\partial x_l}\). Through \(L\) layers:

\[
\frac{\partial \mathcal{L}}{\partial x_0} = \frac{\partial \mathcal{L}}{\partial x_L} \cdot J_L \cdot J_{L-1} \cdot \ldots \cdot J_1
\]

For plain networks, the product of Jacobians causes problems. When
eigenvalues fall below 1, gradients vanish exponentially through depth.
When eigenvalues exceed 1, gradients explode exponentially. Achieving
perfect eigenvalues of exactly 1 proves extremely difficult in practice,
leaving plain networks trapped between vanishing and exploding
gradients.

For ResNets with \(f_l(x) = \mathcal{F}_l(x) + x\), the Jacobian
becomes:

\[
J_l = \mathcal{F}'_l + I
\]

where \(I\) is the identity matrix. The identity ensures that the
Jacobian has eigenvalues at least 1, providing a ``gradient highway''
that prevents vanishing gradients regardless of what happens in the
residual function \(\mathcal{F}_l\).

\textbf{Empirical Validation: 50-Layer Comparison.} Training identical
50-layer networks on CIFAR-10 with and without residual connections
demonstrates the practical impact. A plain 50-layer network with
convolutional layers, ReLU activations, and no skip connections begins
training at loss 2.3 (random initialization) and improves only to 1.8
after 50 epochs, achieving just 45\% test accuracy. The gradient norms
tell the story: at layer 50 near the output, gradients measure \(0.1\),
but at layer 1, they have vanished to \(10^{-8}\). Training stagnates
because early layers barely update.

ResNet-50, with identical depth but organized into residual blocks,
produces a completely different outcome. Starting from the same random
initialization with loss 2.3, it reaches 0.05 after 50 epochs (near
perfect training fit), achieving 93\% test accuracy on CIFAR-10. The
critical difference appears in gradient flow: at layer 1, gradient norms
measure \(0.01\), four orders of magnitude larger than the plain
network, while layer 50 maintains the same \(0.1\) magnitude. All layers
train effectively because gradients propagate.

The difference is stark: the plain network fails to train despite having
identical representational capacity, while ResNet trains successfully.
The architectural difference is skip connections; the mathematical
difference is gradient flow.

While skip connections solve gradient flow, they introduce system-level
costs. Memory overhead increases because skip connections require
storing the input to each residual block for the addition operation
during the forward pass and for backpropagation. For a ResNet-50 with
batch size 32 processing \(224 \times 224\) RGB images, this adds
approximately 20\% memory overhead compared to a plain network. The
computational cost of the addition operation
(\(y = \mathcal{F}(x) + x\)) is computationally trivial, adding
negligible compute time. The primary cost is the residual function
\(\mathcal{F}(x)\) itself.

Better gradient flow accelerates convergence and reduces total training
time. ResNet-50 typically converges in 90 epochs on ImageNet, while
plain 50-layer networks may not converge at all. The per-epoch cost
increases by approximately 10\% due to memory overhead, but total
training time decreases dramatically because the network actually
learns.

These empirical results establish a systems constraint: depth requires
architectural support for gradient flow. The relationship is
quantitative. Networks with fewer than 20 layers can train without skip
connections, as demonstrated by architectures like VGG-16
(\citeproc{ref-simonyan2014very}{Simonyan and Zisserman 2014}). Between
20 and 100 layers, skip connections become necessary, which is why
ResNet-50 and ResNet-101 incorporate them. Beyond 100 layers, skip
connections alone prove insufficient; architectures like ResNet-v2 with
pre-activation require skip connections plus careful normalization to
maintain trainability.

This constraint shapes architecture selection: if the task benefits from
depth (and empirically, most vision and language tasks do), the
architecture must incorporate mechanisms to maintain gradient flow. Skip
connections became not just an optimization but a necessity.

Perhaps even more influential was the introduction of skip connections
through ResNets\sidenote{\textbf{ResNet}: Short for ``Residual
Network,'' named for its key insight: instead of learning full
transformations \(\mathcal{H}(\mathbf{x})\), layers learn
\emph{residual} functions
\(\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}\), then
add the identity back. From mathematics, a ``residual'' is what remains
after subtracting an approximation. This reframing solved the
``degradation problem'' (deeper networks performing worse), enabling
training of 1000+ layer networks and winning ImageNet 2015. }
(\citeproc{ref-he2016deep}{He et al. 2016}). Originally designed to help
train very deep CNNs, skip connections have become a building block that
appears in virtually every modern architecture. They showed how direct
paths through the network could aid gradient flow and information
propagation, a concept now central to Transformer designs. DenseNet
(\citeproc{ref-huang2017densely}{Huang et al. 2017}) explored an
alternative approach, connecting each layer to all subsequent layers
through dense connectivity patterns, further demonstrating the
importance of gradient pathways in deep architectures.

The gradient flow improvements from skip connections solved one
fundamental training challenge, but revealed another: controlling
activation distributions across layers. Even with skip connections
ensuring gradient flow, poorly conditioned activations can destabilize
training. The following analysis of normalization techniques provides
essential foundations for understanding why modern architectures
universally include these components.

\subsubsection{Normalization Layers: Mathematical
Foundations}\label{sec-dnn-architectures-normalization-layers-mathematical-foundations-0fb7}

While skip connections provide direct gradient pathways, normalization
layers address the complementary problem of controlling the scale and
distribution of activations throughout the network. CNNs introduced
batch normalization (\citeproc{ref-ioffe2015batch}{Ioffe and Szegedy
2015}), which has since evolved into variants essential for modern
architectures. Understanding the mathematics of normalization reveals
why these layers are not merely optimization tricks but fundamental
components enabling deep network training.

\textbf{Batch Normalization: Definition and Formulation.} Batch
normalization normalizes activations across the batch dimension during
training. For a mini-batch \(\mathcal{B} = \{x_1, \ldots, x_m\}\) of
activations at a particular layer, the transformation proceeds in two
stages.

First, compute the batch statistics:

\[
\mu_{\mathcal{B}} = \frac{1}{m}\sum_{i=1}^{m} x_i \qquad \sigma_{\mathcal{B}}^2 = \frac{1}{m}\sum_{i=1}^{m} (x_i - \mu_{\mathcal{B}})^2
\]

Then normalize and apply learnable scale and shift. The normalization
step in Equation~\ref{eq-batchnorm-normalize} centers and scales
activations, while Equation~\ref{eq-batchnorm-transform} applies
learnable parameters that allow the network to recover the identity
transformation if optimal:

\begin{equation}\phantomsection\label{eq-batchnorm-normalize}{
\hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
}\end{equation}

\begin{equation}\phantomsection\label{eq-batchnorm-transform}{
y_i = \gamma \hat{x}_i + \beta
}\end{equation}

The parameters \(\gamma\) (scale) and \(\beta\) (shift) are learned
during training, while \(\epsilon\) (typically \(10^{-5}\)) prevents
division by zero. This formulation ensures the network can represent the
identity transformation if optimal (\(\gamma = \sigma_{\mathcal{B}}\),
\(\beta = \mu_{\mathcal{B}}\)), preserving representational capacity.

\textbf{Jacobian Conditioning: Why Normalization Helps Gradient Flow.}
The mathematical insight into why normalization aids training lies in
how it conditions the Jacobian matrix of the layer. Consider the
gradient of the normalized output with respect to the input:

\[
\frac{\partial \hat{x}_i}{\partial x_j} = \frac{1}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} \left( \delta_{ij} - \frac{1}{m} - \frac{(x_i - \mu_{\mathcal{B}})(x_j - \mu_{\mathcal{B}})}{m\sigma_{\mathcal{B}}^2} \right)
\]

where \(\delta_{ij}\) is the Kronecker delta. The critical observation
is that this Jacobian has bounded eigenvalues. Without normalization,
the Jacobian of a linear layer \(W\) can have eigenvalues spanning
orders of magnitude (empirically, 0.01 to 100 in deep networks). With
batch normalization, the effective Jacobian eigenvalues are constrained
to a much narrower range, typically within \([0.5, 2.0]\).

This constraint prevents both vanishing gradients (eigenvalues
\(\ll 1\)) and exploding gradients (eigenvalues \(\gg 1\)) through the
normalization layer itself. The quantitative impact on training
stability is substantial: without normalization, gradient norms can vary
by factors of \(10^4\) across layers, but with batch normalization,
gradient norms typically vary by only factors of 2 to 4 across layers.

This stability allows significantly higher learning rates. Networks with
batch normalization commonly train with learning rates 10 to 30 times
larger than unnormalized networks, directly accelerating convergence.

\textbf{Layer Normalization: Architecture Independence.} While batch
normalization proved transformative for CNNs, it introduced a
problematic dependency on batch statistics. This creates issues for
small batch sizes (noisy statistics), varying sequence lengths
(incompatible batch dimensions), and inference (requires running
mean/variance estimation). Layer normalization addresses these
limitations by normalizing across features rather than across the batch
(\citeproc{ref-ba2016layer}{Ba, Kiros, and Hinton 2016}).

For an input vector \(\mathbf{x} \in \mathbb{R}^H\) with \(H\) features:

\[
\mu_L = \frac{1}{H}\sum_{i=1}^{H} x_i \qquad \sigma_L^2 = \frac{1}{H}\sum_{i=1}^{H} (x_i - \mu_L)^2
\]

Equation~\ref{eq-layernorm} defines the complete layer normalization
operation, where \(\odot\) denotes element-wise multiplication:

\begin{equation}\phantomsection\label{eq-layernorm}{
\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}} \odot \boldsymbol{\gamma} + \boldsymbol{\beta}
}\end{equation}

where \(\odot\) denotes element-wise multiplication. Each sample is
normalized independently, making layer normalization invariant to batch
size and suitable for autoregressive models where future tokens must not
influence current computations.

This architectural difference explains why Transformers universally
adopt layer normalization: the self-attention mechanism processes
sequences of varying length, and autoregressive generation requires each
position to be normalized independently of batch composition.

\textbf{Comparative Analysis: When to Use Each Variant.} The choice
between normalization variants depends on the computational context:

Table~\ref{tbl-normalization-comparison} reveals the systems costs that
architects must consider when selecting normalization strategies. Memory
overhead increases because batch normalization maintains running
statistics (mean and variance) for each normalized feature, requiring
\(2 \times H\) additional parameters per layer during training, where
\(H\) is the feature dimension. For inference, these become fixed
constants. Layer normalization computes statistics on-the-fly, adding no
persistent memory but requiring temporary buffers.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2315}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2685}}@{}}
\caption{\textbf{Normalization Variant Comparison}: Different
normalization techniques trade off between computational efficiency,
batch size sensitivity, and architectural compatibility. RMSNorm
(\citeproc{ref-zhang2019root}{D. Wang et al. 2023}), used in LLaMA and
other efficient architectures, omits mean centering:
\(\text{RMSNorm}(\mathbf{x}) = \mathbf{x} / \sqrt{\frac{1}{H}\sum_i x_i^2 + \epsilon} \cdot \boldsymbol{\gamma}\).}\label{tbl-normalization-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristic}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{BatchNorm}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{LayerNorm}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{RMSNorm}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristic}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{BatchNorm}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{LayerNorm}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{RMSNorm}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Normalization Axis} & Batch dimension & Feature dimension &
Feature dimension \\
\textbf{Batch Size Dependency} & High (noisy for small batches) & None &
None \\
\textbf{Typical Use Case} & CNNs, vision models & Transformers, RNNs &
LLaMA, efficient Transformers \\
\textbf{Computation Cost} & Higher (mean + variance) & Higher (mean +
variance) & Lower (RMS only) \\
\textbf{Training/Inference} & Different (running stats) & Identical &
Identical \\
\textbf{Behavior} & & & \\
\end{longtable}

Batch size constraints emerge because batch normalization requires
sufficiently large batches for stable statistics. Empirically, batch
sizes below 16 degrade performance noticeably, and sizes below 8 can
cause training instability. This constraint impacts memory-limited
scenarios such as high-resolution images or very large models.

The computational cost of computing mean and variance adds
\(O(m \times H)\) operations per batch normalization layer for batch
size \(m\) and feature dimension \(H\). For layer normalization, the
cost is \(O(H)\) per sample. RMSNorm reduces this further by eliminating
the mean computation.

Operational differences between training versus inference require
explicit mode switching for batch normalization, which exhibits
different behavior between training (batch statistics) and inference
(running statistics). Incorrect mode handling is a common source of
training-serving skew. Layer normalization behaves identically in both
modes, simplifying deployment.

These innovations, including parameter sharing, skip connections, and
normalization, transcended their origins in spatial processing to become
essential building blocks in the deep learning toolkit. The same
gradient flow and activation stability challenges that drove their
development in CNNs reappear in every deep architecture.

\subsection{Evolution of Sequence
Processing}\label{sec-dnn-architectures-evolution-sequence-processing-d809}

While CNNs specialized MLPs for spatial patterns, sequence models
adapted neural networks for temporal dependencies. RNNs introduced the
concept of maintaining and updating state, a building block that
influenced how networks could process sequential information,
(\citeproc{ref-elman1990finding}{Elman 1990}).

The development of LSTMs\sidenote{\textbf{LSTM Origins}: Sepp Hochreiter
and Jürgen Schmidhuber invented LSTMs in 1997
(\citeproc{ref-hochreiter1997long}{Hochreiter and Schmidhuber 1997}) to
solve the ``vanishing gradient problem'' that plagued RNNs. Their gating
mechanism introduced a ``Constant Error Carousel'' that protects error
signals from decay as they propagate back through time---a breakthrough
that enabled sequence modeling and facilitated modern language models. }
and GRUs\sidenote{\textbf{Gated Recurrent Unit (GRU)}: Simplified
version of LSTM introduced by Cho et al.~(2014)
(\citeproc{ref-cho2014properties}{K. Cho et al. 2014}) with only 2 gates
instead of 3, reducing parameters by \textasciitilde25\% while
maintaining similar performance. GRUs became popular for their
computational efficiency and easier training, proving that architectural
simplification can sometimes improve rather than hurt performance. }
brought sophisticated gating mechanisms to neural networks
(\citeproc{ref-hochreiter1997long}{Hochreiter and Schmidhuber 1997};
\citeproc{ref-cho2014properties}{K. Cho et al. 2014}). These gates,
themselves small MLPs, showed how simple feedforward computations could
be composed to control information flow. This concept of using neural
networks to modulate other neural networks became a recurring pattern in
architecture design.

The evolution of these sequence models serves as a critical lesson in
managing state across time. Early RNNs hit a ``temporal barrier'' where
gradients vanished or exploded, revealing that simple recurrence was
insufficient for long-term dependencies. The introduction of gating in
LSTMs and GRUs represented an engineering solution: architectural
mechanisms acting as physical valves to protect and route signals. This
taught us that processing complex, variable-length history requires
explicit architectural support for signal propagation.

Perhaps most significantly, sequence models demonstrated the power of
adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs,
RNNs showed how networks could process variable-length inputs by reusing
weights over time. This insight, that architectural patterns could adapt
to input structure, laid groundwork for more flexible architectures.

Sequence models also popularized the concept of attention through
encoder-decoder architectures
(\citeproc{ref-bahdanau2014neural}{Bahdanau, Cho, and Bengio 2014}).
Initially introduced as an improvement to machine translation, attention
mechanisms showed how networks could learn to dynamically focus on
relevant information. This building block would later become the
foundation of Transformer architectures, which synthesize insights from
all previous architectural generations.

\subsection{Modern Architectures: Synthesis and
Unification}\label{sec-dnn-architectures-modern-architectures-synthesis-unification-c816}

The progression from perceptrons to attention mechanisms shows how each
architectural generation contributed building blocks that modern designs
combine and refine. Transformers, in particular, represent a
sophisticated synthesis of these fundamental components. Rather than
introducing entirely new patterns, they innovate through strategic
combination and refinement of existing components. The Transformer
architecture exemplifies this approach: at its core, MLP-style
feedforward networks process features between attention layers. The
attention mechanism itself builds on sequence model concepts while
eliminating recurrent connections, instead employing position embeddings
inspired by CNN intuitions. Skip connections inherited from ResNets
(Figure~\ref{fig-example-skip-connection}) enable gradient flow through
deep stacks, while layer normalization, evolved from CNN batch
normalization, stabilizes optimization (\citeproc{ref-ba2016layer}{Ba,
Kiros, and Hinton 2016}).

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/4d8c073e55d73006f0671dec582ed93512c8db37.pdf}}

}

\caption{\label{fig-example-skip-connection}\textbf{Residual Connection
Block}: The skip connection implements \(y = \mathcal{F}(x) + x\),
creating an identity shortcut that bypasses the weight layers. During
backpropagation, gradients flow through both the residual path (via
\(\mathcal{F}'(x)\)) and the identity path (constant gradient of 1),
ensuring gradients reach early layers even in 100+ layer networks. This
architectural pattern enabled ResNet-50 to achieve 93\% accuracy on
CIFAR-10 where equivalent plain networks stagnate at 45\%, and has
become a required component in all modern deep architectures including
Transformers.}

\end{figure}%

The transition to Transformers represents a fundamental engineering
shift from sequential to parallel state management. While RNNs processed
tokens one by one, creating a computational bottleneck that limited
hardware parallelism, Transformers introduced the ``Attention''
mechanism as an architectural solution to a systems constraint. By
replacing time-step dependencies with global, data-dependent routing, we
moved from \(O(n)\) sequential complexity to \(O(1)\) depth for
information flow, enabling full use of the massive parallel processing
power of modern accelerators.

This composition of building blocks creates emergent capabilities
exceeding the sum of individual components. The self-attention
mechanism, while building on previous attention concepts, allows novel
forms of dynamic pattern processing. The arrangement of these components
(attention followed by feedforward layers, with skip connections and
normalization) has proven sufficiently effective to become a template
for new architectures.

Recent innovations in vision and language models follow this pattern of
recombining building blocks. Vision Transformers\sidenote{\textbf{Vision
Transformers (ViTs)}: Google's October 2020 Vision Transformer paper
(presented at ICLR 2021) showed that pure transformers could match CNN
performance on ImageNet by treating image patches as ``words.'' ViTs
split a \(224\times 224\) image into \(16\times 16\) patches (196
``tokens''), proving that attention mechanisms could replace
convolutional inductive biases with sufficient data. } adapt the
Transformer architecture to images while maintaining its essential
components (\citeproc{ref-dosovitskiy2021image}{Dosovitskiy et al.
2021}). Large language models scale up these patterns while introducing
refinements like grouped-query attention or sliding window attention,
yet still rely on the core building blocks established through this
architectural evolution (\citeproc{ref-brown2020language}{Brown et al.
2020}). These modern architectural innovations demonstrate the
principles of efficient scaling covered in \textbf{?@sec-introduction},
while their practical implementation challenges and optimizations are
explored in \textbf{?@sec-model-compression}.

Transformers uniquely combine matrix multiplication with attention
mechanisms, resulting in random memory access and data movement patterns
distinct from sequential RNNs or strided CNNs.

Table~\ref{tbl-primitive-comparison} reveals how Transformers combine
elements from previous architectures while introducing new patterns.
They retain the core matrix multiplication operations common to all
architectures but introduce more complex memory access patterns with
their attention mechanism, blending the broadcast operations of MLPs
with the gather operations reminiscent of more dynamic architectures.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2368}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1316}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2368}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2105}}@{}}
\caption{\textbf{Primitive Utilization Across Architectures}:
Computational primitives vary across architectures, with Transformers
uniquely combining matrix multiplication with attention mechanisms.
Memory access patterns range from sequential (MLPs) to strided (CNNs) to
random (attention). Data movement patterns, including broadcast,
scatter, gather, and reduction, define information flow and often
dominate performance.}\label{tbl-primitive-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primitive Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLP}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{CNN}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{RNN}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Transformer}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primitive Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLP}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{CNN}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{RNN}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Transformer}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Computation} & Dense GEMM & Convolution & Sequential GEMM & GEMM
+ Attention \\
\textbf{Memory Access} & Sequential & Strided & Sequential + State &
Random (QKV) \\
\textbf{Data Movement} & Broadcast & Sliding window & Temporal broadcast
& Gather + Reduce \\
\textbf{Parallelism} & High & High & Low (time deps) & High
(positions) \\
\end{longtable}

This synthesis of primitives in Transformers shows modern architectures
innovating by recombining and refining existing building blocks from the
architectural progression established in
Section~\ref{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e},
rather than inventing entirely new computational paradigms. This
evolutionary process guides the development of future architectures and
helps design efficient systems to support them.

\section{System-Level Building
Blocks}\label{sec-dnn-architectures-systemlevel-building-blocks-41c5}

The architectural evolution reveals that despite their apparent
diversity, all neural networks rely on a small set of fundamental
computational primitives. These system-level building blocks---matrix
operations, memory access patterns, and data movement
structures---determine how architectures map to hardware and where
performance bottlenecks arise. Understanding these primitives is
essential for practitioners because it reveals which optimizations prove
most effective and why certain techniques transfer across architectures
while others remain architecture-specific.

Examining the deep learning architectures covered above reveals system
requirements that distill into primitives underpinning both hardware and
software implementations. These primitives represent operations that
cannot be decomposed further while maintaining their essential
characteristics.

\subsection{Core Computational
Primitives}\label{sec-dnn-architectures-core-computational-primitives-b853}

Three operations serve as the fundamental building blocks for all deep
learning computations: matrix multiplication, sliding window operations,
and dynamic computation. These operations are primitive because they
cannot be further decomposed without losing their essential
computational properties and efficiency characteristics.

Matrix multiplication represents the basic form of transforming sets of
features. Multiplying a matrix of inputs by a matrix of weights computes
weighted combinations, the core operation of neural networks. For
example, in our MNIST network, each 784-dimensional input vector
multiplies with a \(784\times 100\) weight matrix. This pattern appears
everywhere: MLPs use it directly for layer computations, CNNs reshape
convolutions into matrix multiplications
(Figure~\ref{fig-im2col-diagram} shows this transformation of a
\(3\times 3\) convolution into a matrix operation), and Transformers use
it extensively in their attention mechanisms.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/00ad8a907d5563fc6629d8deb08a5619c2ce51e1.pdf}}

}

\caption{\label{fig-im2col-diagram}\textbf{im2col Transformation}:
Converts convolution to GEMM by rearranging image patches into columns.
The input feature maps (cyan/orange grids, \(3 \times 3\)) are unfolded
so each sliding window position becomes a matrix column, while filter
kernels (green/yellow, \(2 \times 2\)) become rows. The resulting
\(4 \times 8\) matrix multiplication produces all output positions in
one operation. This transformation trades 2x memory overhead
(duplicating overlapping pixels) for 5-10x speedup by leveraging decades
of BLAS optimizations and enabling efficient GPU parallelization.}

\end{figure}%

\subsubsection{Computational Building
Blocks}\label{sec-dnn-architectures-computational-building-blocks-60fb}

Modern neural networks operate through three computational patterns that
appear across all architectures. These patterns explain how different
architectures achieve their computational goals and why certain hardware
optimizations are effective.

The detailed analysis of sparse computation patterns, including
structured and unstructured sparsity, hardware-aware optimization
strategies, and algorithm-hardware co-design principles, is addressed in
\textbf{?@sec-model-compression} and \textbf{?@sec-ai-acceleration}.

The im2col\sidenote{\textbf{im2col (Image to Column)}: A data layout
transformation that converts convolution operations into matrix
multiplications by unfolding image patches into columns. This approach
trades memory consumption (through data duplication) for computational
efficiency, enabling CNNs to leverage decades of GEMM optimizations and
achieving substantial speedups. } (image to column) technique
accomplishes matrix reshaping by unfolding overlapping image patches
into columns of a matrix (Figure~\ref{fig-im2col-diagram}). Each sliding
window position in the convolution becomes a column in the transformed
matrix, while the filter kernels are arranged as rows. This allows the
convolution operation to be expressed as a standard GEMM (General Matrix
Multiply) operation. The transformation trades memory consumption
(duplicating data where windows overlap) for computational efficiency,
enabling CNNs to leverage decades of BLAS optimizations and achieving
5-10x speedups on CPUs. In modern systems, these matrix multiplications
map to specific hardware and software implementations. Datacenter
accelerators can deliver on the order of hundreds of TFLOPS on
mixed-precision matrix operations, and software frameworks like PyTorch
and TensorFlow automatically map these high-level operations to
optimized matrix libraries (for example, NVIDIA cuBLAS and Intel oneMKL)
that exploit available hardware capabilities.

Sliding window operations compute local relationships by applying the
same operation to chunks of data. In CNNs processing MNIST images, a
\(3\times 3\) convolution filter slides across the \(28\times 28\)
input, requiring \(26\times 26\) windows of computation, assuming a
stride size of 1. Modern hardware accelerators implement this through
specialized memory access patterns and data buffering schemes that
optimize data reuse. For example, TPUs use systolic
arrays\sidenote{\textbf{Systolic Array}: A network of processing
elements that rhythmically compute and pass data through neighbors, like
a ``heartbeat'' of computation. Invented by H.T. Kung and Charles
Leiserson in 1978, systolic arrays achieve high throughput by
overlapping computation with data movement. A (128\times128) systolic
array contains 16,384 processing elements and can perform tens of
thousands of multiply-accumulate operations per cycle, depending on the
dataflow and precision. } where data flows systematically through
processing elements, allowing each input value to be reused across
multiple computations without repeatedly accessing off-chip memory.

Dynamic computation, where the operation itself depends on the input
data, emerged prominently with attention mechanisms but represents a
capability needed for adaptive processing. In Transformer attention,
each query dynamically determines its interaction weights with all keys;
for a sequence of length 512, 512 different weight patterns must be
computed on the fly. Unlike fixed patterns where the computation graph
is known in advance, dynamic computation requires runtime decisions.
This creates specific implementation challenges: hardware must provide
flexible data routing (modern GPUs employ dynamic scheduling) and
support variable computation patterns, while software frameworks require
efficient mechanisms for handling data-dependent execution paths
(PyTorch's dynamic computation graphs, TensorFlow's dynamic control
flow).

These primitives combine in sophisticated ways in modern architectures.
A Transformer layer processing a sequence of 512 tokens demonstrates
this clearly: it uses matrix multiplications for feature projections
(\(512\times 512\) operations implemented through tensor cores), may
employ sliding windows for efficient attention over long sequences
(using specialized memory access patterns for local regions), and
requires dynamic computation for attention weights (computing
\(512\times 512\) attention patterns at runtime). The way these
primitives interact creates specific demands on system design, ranging
from memory hierarchy organization to computation scheduling.

These building blocks explain why certain hardware features exist
(tensor cores for matrix multiplication) and why software frameworks
organize computations in particular ways (batching similar operations
together). Yet computational primitives tell only part of the story: the
way these operations access memory often determines real-world
performance more than the operations themselves.

\subsection{Memory Access
Primitives}\label{sec-dnn-architectures-memory-access-primitives-5e8b}

The efficiency of deep learning models depends heavily on memory access
and management. Memory access often constitutes the primary bottleneck
in modern ML systems: even a matrix multiplication unit capable of
thousands of operations per cycle will remain idle if data is not
available in time. Accessing data from DRAM typically requires hundreds
of cycles, while on-chip computation requires only a few.

\phantomsection\label{callout-notebookux2a-1.21}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Energy Cost of Data Movement}
\phantomsection\label{callout-notebook*-1.21}
A fundamental systems principle: \textbf{moving data costs more than
computing on it}. A floating-point multiply-accumulate operation
consumes approximately 1 picojoule (pJ), while fetching that same
operand from DRAM consumes 100--600 pJ depending on memory technology
and access pattern. This 100x energy ratio explains why memory access
patterns, not just FLOP counts, determine real-world efficiency.
Architectures with high data reuse (CNNs reusing filter weights across
spatial positions) achieve better energy efficiency than architectures
requiring fresh data movement for each operation (attention mechanisms
loading unique key-value pairs). This principle underlies many
optimization strategies: quantization reduces bits moved per value,
pruning eliminates unnecessary data movement, and tiling keeps working
sets in faster, lower-energy caches.

\end{fbxSimple}

Three memory access patterns dominate in deep learning architectures:
sequential access, strided access, and random access. Each pattern
creates different demands on the memory system and offers different
opportunities for optimization.

Sequential access is the simplest and most efficient pattern. Consider
an MLP performing matrix multiplication with a batch of MNIST images: it
needs to access both the \(784\times 100\) weight matrix and the input
vectors sequentially. This pattern maps well to modern memory systems;
DRAM can operate in burst mode for sequential reads (reaching on the
order of hundreds of GB/s in modern GPUs), and hardware prefetchers can
effectively predict and fetch upcoming data. Software frameworks
optimize for this by ensuring data is laid out contiguously in memory
and aligning data to cache line boundaries.

Strided access appears prominently in CNNs, where each output position
needs to access a window of input values at regular intervals. For a CNN
processing MNIST images with \(3\times 3\) filters, each output position
requires accessing 9 input values with a stride matching the input
width. While less efficient than sequential access, hardware supports
this through pattern-aware caching strategies and specialized memory
controllers. Software frameworks often transform these strided patterns
into sequential access through data layout reorganization, where the
im2col transformation in deep learning frameworks converts convolution's
strided access into efficient matrix multiplications.

Random access poses the greatest challenge for system efficiency. In a
Transformer processing a sequence of 512 tokens, each attention
operation potentially needs to access any position in the sequence,
creating unpredictable memory access patterns. Random access can
severely impact performance through cache misses (potentially causing
100+ cycle stalls per access) and unpredictable memory latencies.
Systems address this through large cache hierarchies (modern GPUs have
several MB of L2 cache) and sophisticated prefetching strategies, while
software frameworks employ techniques like attention pattern pruning to
reduce random access requirements.

Table~\ref{tbl-arch-complexity} quantifies how these different memory
access patterns contribute to the overall memory requirements of each
architecture, comparing MLPs, CNNs, RNNs, and Transformers across
parameter storage, activation storage, and scaling behavior.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1641}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3984}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1562}}@{}}
\caption{\textbf{Memory Access Complexity}: Different neural network
architectures exhibit varying memory access patterns and storage
requirements, impacting system performance and scalability. Parameter
storage scales with input dependency and model size, while activation
storage represents a significant runtime cost, particularly for
sequence-based models where RNNs offer a parameter efficiency advantage
when sequence length exceeds hidden state size
(\(n > h\)).}\label{tbl-arch-complexity}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Input Dependency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parameter Storage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Activation Storage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scaling Behavior}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Input Dependency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parameter Storage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Activation Storage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scaling Behavior}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MLP} & Linear & \(O(N \times W)\) & \(O(B \times W)\) &
Predictable \\
\textbf{CNN} & Constant & \(O(K \times C)\) &
\(O(B\times H_{\text{img}}\) \(\times W_{\text{img}})\) & Efficient \\
\textbf{RNN} & Linear & \(O(h^2)\) & \(O(B \times T \times h)\) &
Challenging \\
\textbf{Transformer} & Quadratic & \(O(N \times d)\) &
\(O(B \times N^2)\) & Problematic \\
\end{longtable}

Where:

\begin{itemize}
\tightlist
\item
  \(N\): Input or sequence size
\item
  \(W\): Layer width
\item
  \(B\): Batch size
\item
  \(K\): Kernel size
\item
  \(C\): Number of channels
\item
  \(H_{\text{img}}\): Height of input feature map (CNN)
\item
  \(W_{\text{img}}\): Width of input feature map (CNN)
\item
  \(h\): Hidden state size (RNN)
\item
  \(T\): Sequence length
\item
  \(d\): Model dimensionality
\end{itemize}

These memory access patterns complement the computational scaling
behaviors summarized in Table~\ref{tbl-computational-complexity},
completing the picture of each architecture's resource requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0608}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2395}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2700}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2205}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1483}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0608}}@{}}
\caption{\textbf{Computational Complexity Comparison}: Scaling behaviors
and resource requirements for major neural network architectures.
Variables: \(d\) = dimension, \(h\) = hidden size, \(k\) = kernel size,
\(c\)~=~channels, \(H,W\) = spatial dimensions, \(T\) = time steps,
\(n\) = sequence length, \(b\) = batch size. The bottleneck column
identifies the primary performance-limiting factor in typical
deployments.}\label{tbl-computational-complexity}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Forward Pass}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parallelization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Forward Pass}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parallelization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MLPs} & \(O(d_{\text{in}}\times\) \(d_{\text{out}})\) per~layer
& \(O(d_{\text{in}}\times\) \(d_{\text{out}})\) per~layer & \(O(d^2)\)
weights \(O(d\times b)\) activations & Excellent Matrix ops parallel &
Memory bandwidth \\
\textbf{CNNs} & \(O(k^2\times\) \(c_{\text{in}}\times\)
\(c_{\text{out}})\) per layer & \(O(H\times W\times\) \(k^2\times\)
\(c_{\text{in}}\times\) \(c_{\text{out}})\) & \(O(H\times W\times c)\)
features \(O(k^2\times c^2)\) weights & Good Spatial independence &
Memory bandwidth \\
\textbf{RNNs} & \(O(h^2+h\times d)\) total & \(O(T\times h^2)\) for
\(T\) time~steps & \(O(h)\) hidden state (constant) & Poor Sequential
deps & Sequential deps \\
\textbf{Transformers} & \(O(d^2)\) projections \(O(d^2\times h)\)
multi-head & \(O(n^2\times d+n\) \(\times d²)\) per layer & \(O(n^2)\)
attention \(O(n\times d)\) sequences & Excellent (positions) Limited by
memory & Memory (\(n^2\)) \\
\end{longtable}

The memory requirements scale differently with architectural choices.
The quadratic scaling of activation storage in Transformers, for
instance, highlights the need for large memory capacities and efficient
memory management in systems designed for Transformer-based workloads.
In contrast, CNNs exhibit more favorable memory scaling due to their
parameter sharing and localized processing. Together, these two tables
provide a comprehensive view of each architecture's resource profile,
informing system-level design decisions such as choosing memory
hierarchy configurations and developing memory optimization strategies.

The impact of these patterns becomes clear when we consider data reuse
opportunities. In CNNs, each input pixel participates in multiple
convolution windows (typically 9 times for a \(3 \times 3\) filter),
making effective data reuse necessary for performance. Modern GPUs
provide multi-level cache hierarchies (L1, L2, shared memory) to capture
this reuse, while software techniques like loop tiling ensure data
remains in cache once loaded.

Working set size, the amount of data needed simultaneously for
computation, varies dramatically across architectures. An MLP layer
processing MNIST images might need only a few hundred KB (weights plus
activations), while a Transformer processing long sequences can require
several MB just for storing attention patterns. These differences
directly influence hardware design choices, like the balance between
compute units and on-chip memory, and software optimizations like
activation checkpointing or attention approximation techniques.

Understanding these memory access patterns is essential as architectures
evolve. The shift from CNNs to Transformers, for instance, has driven
the development of hardware with larger on-chip memories and more
sophisticated caching strategies to handle increased working sets and
more dynamic access patterns. Future architectures will likely continue
to be shaped by their memory access characteristics as much as their
computational requirements.

\subsection{Data Movement
Primitives}\label{sec-dnn-architectures-data-movement-primitives-5b39}

Memory access patterns describe where data resides, but a complementary
question determines system performance: how does information flow
between components? Data movement primitives characterize these flows.
These patterns are key because data movement often consumes more time
and energy than computation itself, as moving data from off-chip memory
typically requires 100-1000\(\times\) more energy than performing a
floating-point operation.

Four data movement patterns are prevalent in deep learning
architectures: broadcast, scatter, gather, and reduction.
Figure~\ref{fig-collective-comm} visualizes these fundamental patterns
and their relationships, showing how data flows between processing
elements in each case. Broadcast operations send the same data to
multiple destinations simultaneously. In matrix multiplication with
batch size 32, each weight must be broadcast to process different inputs
in parallel. Modern hardware supports this through specialized
interconnects and hardware multicast capabilities, with bandwidth on the
order of hundreds of GB/s in high-end accelerator interconnects, while
some accelerators also use dedicated on-chip broadcast fabrics. Software
frameworks optimize broadcasts by restructuring computations (like
matrix tiling) to maximize data reuse.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/1f58d4c3327a91d998bc521a7af27815f2a96962.pdf}}

}

\caption{\label{fig-collective-comm}\textbf{Data Movement Primitives}:
Four fundamental patterns govern information flow in neural network
computation. \textbf{Broadcast} (top-left) replicates a single value to
all destinations, used when sharing weights across batch elements.
\textbf{Scatter} (top-right) distributes distinct elements to different
destinations, enabling work partitioning. \textbf{Gather} (bottom-left)
collects distributed values to a single location, as in attention
pooling. \textbf{Reduction} (bottom-right) combines multiple values
through aggregation (sum, max), appearing in gradient synchronization
and attention scoring. Moving data typically costs 100-1000x more energy
than computation, making these patterns critical optimization targets.}

\end{figure}%

Scatter operations distribute different elements to different
destinations. When parallelizing a \(512\times 512\) matrix
multiplication across accelerator cores, each core receives a subset of
the computation. This parallelization is important for performance but
challenging, as memory conflicts and load imbalance can reduce
efficiency substantially. Hardware provides flexible high-bandwidth
interconnects (often in the hundreds of GB/s class within a node), while
software frameworks employ sophisticated work distribution algorithms to
maintain high utilization.

Gather operations collect data from multiple sources. In Transformer
attention with sequence length 512, each query must gather information
from 512 different key-value pairs. These irregular access patterns are
challenging, random gathering can be \(10\times\) slower than sequential
access. Hardware supports this through high-bandwidth interconnects and
large caches, while software frameworks employ techniques like attention
pattern pruning to reduce gathering overhead.

Reduction operations combine multiple values into a single result
through operations like summation. When computing attention scores in
Transformers or layer outputs in MLPs, efficient reduction is essential.
Hardware implements tree-structured reduction networks (reducing latency
from \(O(n)\) to \(O(\log n)\)), while software frameworks use optimized
parallel reduction algorithms that can achieve near-theoretical peak
performance.

These patterns combine in sophisticated ways. A Transformer attention
operation with sequence length 512 and batch size 32 involves
broadcasting query vectors (\(512\times 64\) elements), gathering
relevant keys and values (\(512\times 512\times 64\) elements), and
reducing attention scores (\(512\times 512\) elements per sequence).

The evolution from CNNs to Transformers has increased reliance on gather
and reduction operations, driving hardware innovations like more
flexible interconnects and larger on-chip memories. As models grow (some
now exceeding 100 billion parameters\sidenote{\textbf{Parameter
Scaling}: Model sizes have grown from tens of millions of parameters
(early deep CNNs) to hundreds of billions and, in some cases,
trillion-scale parameter counts. Training at this scale requires
specialized distributed computing infrastructure and can consume
megawatt-scale power during training. }), efficient data movement
becomes increasingly critical, leading to innovations like near-memory
processing and sophisticated data flow optimizations.

\subsection{System Design
Impact}\label{sec-dnn-architectures-system-design-impact-500a}

The computational, memory access, and data movement primitives we've
explored form the foundational requirements that shape the design of
systems for deep learning. The way these primitives influence hardware
design, create common bottlenecks, and drive trade-offs is important for
developing efficient and effective ML systems.

One of the most significant impacts of these primitives on system design
is the push towards specialized hardware. The prevalence of matrix
multiplications and convolutions in deep learning has led to the
development of tensor processing units (TPUs)\sidenote{\textbf{Tensor
Processing Units}: Google's TPUs emerged from the need to run neural
networks at large scale. They exemplify domain-specific hardware that
uses systolic arrays and a tailored memory hierarchy to achieve large
gains in throughput per watt on targeted inference and training kernels.
These specialized units can perform many multiply-accumulate operations
in parallel, dramatically accelerating the core computations of neural
networks. } and tensor cores in GPUs, which are specifically designed to
perform these operations efficiently.

Memory systems have also been profoundly influenced by the demands of
deep learning primitives. The need to support both sequential and random
access patterns efficiently has driven the development of sophisticated
memory hierarchies. High-bandwidth memory (HBM)\sidenote{\textbf{High
Bandwidth Memory (HBM)}: Stacked DRAM technology that can provide
bandwidth on the order of TB/s, higher than many traditional graphics
memory and CPU memory subsystems (often on the order of hundreds of
GB/s). HBM enables the massive data movement required by modern AI
workloads; for very large models, parameter streaming through memory can
be TB-scale per forward pass, making bandwidth a central limiter. } has
become common in AI accelerators to support the massive data movement
requirements, especially for operations like attention mechanisms in
Transformers. On-chip memory hierarchies have grown in complexity, with
multiple levels of caching and scratchpad
memories\sidenote{\textbf{Scratchpad Memory}: Programmer-controlled
on-chip memory providing predictable, fast access without cache
management overhead. Unlike caches, scratchpads require explicit data
movement but enable precise control over memory allocation---critical
for neural network accelerators where memory access patterns are known
and performance must be deterministic. } to support the diverse working
set sizes of different neural network layers.

The data movement primitives have particularly influenced the design of
interconnects and on-chip networks. The need to support efficient
broadcasts, gathers, and reductions has led to the development of more
flexible and higher-bandwidth interconnects. Some AI chips now feature
specialized networks-on-chip designed to accelerate common data movement
patterns in neural networks.

The system implications of these primitives span hardware, software, and
performance considerations:

Examine Table~\ref{tbl-sys-design-implications} to see how architectural
primitives drive hardware and software optimization decisions. Despite
these advancements, several bottlenecks persist in deep learning models.
Memory bandwidth often remains a key limitation, particularly for models
with large working sets or those that require frequent random access.
The energy cost of data movement, especially between off-chip memory and
processing units, continues to be a significant concern. For large-scale
models, the communication overhead in distributed training can become a
bottleneck, limiting scaling efficiency.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2475}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2475}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2475}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2574}}@{}}
\caption{\textbf{Primitive-Hardware Co-Design}: Efficient machine
learning systems require tight integration between algorithmic
primitives and underlying hardware. Common primitives map to specific
hardware accelerations and software optimizations, each presenting
distinct implementation challenges. Specialized hardware such as tensor
cores addresses computational demands of matrix multiplication and
sliding windows, while software techniques like batching and dynamic
graph execution further enhance
performance.}\label{tbl-sys-design-implications}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primitive}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hardware Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Software Optimization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Challenges}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primitive}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hardware Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Software Optimization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Challenges}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Matrix Multiplication} & Tensor Cores & Batching, GEMM libraries
& Parallelization, precision \\
\textbf{Sliding Window} & Specialized datapaths & Data layout
optimization & Stride handling \\
\textbf{Dynamic Computation} & Flexible routing & Dynamic graph
execution & Load balancing \\
\textbf{Sequential Access} & Burst mode DRAM & Contiguous allocation &
Access latency \\
\textbf{Random Access} & Large caches & Memory-aware scheduling & Cache
misses \\
\textbf{Broadcast} & Specialized interconnects & Operation fusion &
Bandwidth \\
\textbf{Gather/Scatter} & High-bandwidth memory & Work distribution &
Load balancing \\
\end{longtable}

\subsubsection{Energy Consumption Analysis Across
Architectures}\label{sec-dnn-architectures-energy-consumption-analysis-across-architectures-1b1d}

Energy consumption patterns vary dramatically across neural network
architectures, with implications for both datacenter deployment and edge
computing scenarios. Each architectural pattern exhibits distinct energy
characteristics that inform deployment decisions and optimization
strategies.

Dense matrix operations in MLPs achieve excellent arithmetic
intensity\sidenote{\textbf{Arithmetic Intensity}: The ratio of
floating-point operations to memory accesses, measured in FLOPS per
byte. High arithmetic intensity (\textgreater10 FLOPS/byte) enables
efficient hardware utilization, while low intensity (\textless1
FLOPS/byte) makes workloads memory-bound. Attention mechanisms typically
have low arithmetic intensity, explaining their energy inefficiency. }
(computation per data movement) but consume significant absolute energy.
Each multiply-accumulate operation consumes approximately 4.6pJ, while
data movement from DRAM costs 640pJ per 32-bit value
(\citeproc{ref-Horowitz2014}{Horowitz 2014}). For typical MLP inference,
70-80\% of energy goes to data movement rather than computation, making
memory bandwidth optimization critical for energy efficiency.

Convolutional operations reduce energy consumption through data reuse
but exhibit variable efficiency depending on implementation.
Im2col-based convolution implementations trade memory for simplicity,
often doubling memory requirements and energy consumption. Direct
convolution implementations achieve 3-5x better energy efficiency by
eliminating redundant data movement, particularly for larger kernel
sizes.

Sequential processing in RNNs creates energy efficiency opportunities
through temporal data reuse. The constant memory footprint of RNN hidden
states allows aggressive caching strategies, reducing DRAM access energy
by 80-90\% for long sequences. The sequential dependencies limit
parallelization opportunities, often resulting in suboptimal hardware
utilization and higher energy per operation.

Attention mechanisms in Transformers exhibit the highest energy
consumption per operation due to quadratic scaling and complex data
movement patterns. Self-attention operations consume 2-3x more energy
per FLOP than standard matrix multiplication due to irregular memory
access patterns and the need to store attention matrices. This energy
cost scales quadratically with sequence length, making long-sequence
processing energy-prohibitive without architectural modifications.

System designers must navigate trade-offs in supporting different
primitives, each with unique characteristics that influence system
design and performance. For example, optimizing for the dense matrix
operations common in MLPs and CNNs might come at the cost of flexibility
needed for the more dynamic computations in attention mechanisms.
Supporting large working sets for Transformers might require sacrificing
energy efficiency.

Balancing these trade-offs requires consideration of the target
workloads and deployment scenarios. Understanding the nature of each
primitive guides the development of both hardware and software
optimizations in ML systems, allowing designers to make informed
decisions about system architecture and resource allocation.

The analysis of architectural patterns, computational primitives, and
system implications provides the conceptual foundation for architecture
selection. Yet practitioners face a practical challenge: how do
engineers systematically choose the right architecture for their
specific problem? The diversity of neural network architectures, each
optimized for different data patterns and computational constraints,
requires a structured approach. This selection process must consider not
only algorithmic performance but also deployment constraints covered in
\textbf{?@sec-ml-system-architecture} and operational efficiency
requirements detailed in
\textbf{?@sec-machine-learning-operations-mlops}.

\section{Architecture Selection
Framework}\label{sec-dnn-architectures-architecture-selection-framework-a442}

The preceding exploration of neural network architectures, from dense
MLPs to dynamic Transformers, demonstrates how each design embodies
specific assumptions about data structure and computational patterns.
MLPs assume arbitrary feature relationships, CNNs exploit spatial
locality, RNNs capture temporal dependencies, and Transformers model
complex relational patterns. For practitioners facing real-world
problems, a practical question emerges: how to systematically select the
appropriate architecture for a specific use case?

Successful architecture selection requires understanding principles
rather than following trends: matching data characteristics to
architectural strengths, evaluating computational constraints against
system capabilities, and balancing accuracy requirements with deployment
realities. The framework presented here draws upon the computational
patterns and system implications explored in the preceding sections,
integrating principles from \textbf{?@sec-data-selection} with practical
deployment considerations discussed in
\textbf{?@sec-machine-learning-operations-mlops}.

\subsection{Data-to-Architecture
Mapping}\label{sec-dnn-architectures-datatoarchitecture-mapping-fddb}

The first step in systematic architecture selection involves
understanding how different data types align with architectural
strengths. The architectural families introduced in
Section~\ref{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e}
provide the foundation: MLPs for tabular data with arbitrary
relationships, CNNs for spatial data with local patterns, RNNs for
sequential data with temporal dependencies, and Transformers for complex
relational data where any element might influence any other.

This alignment is not coincidental; it reflects fundamental
computational trade-offs. Architectures that match data characteristics
can leverage natural structure for efficiency, while mismatched
architectures must work against their design assumptions, leading to
poor performance or excessive resource consumption.

In practice, MLPs excel for financial modeling, medical measurements,
and structured prediction where feature relationships are unknown a
priori. CNNs dominate image recognition, 2D sensor processing, and
signal analysis where spatial locality matters. RNNs remain useful for
time series forecasting and simple sequential tasks where memory across
time is essential. Transformers have become the architecture of choice
for language understanding, machine translation, and complex reasoning
tasks (\citeproc{ref-wei2022chain}{Wei et al. 2022}) requiring
long-range dependencies.

Beyond data type matching, computational constraints often determine
final feasibility. Understanding the scaling behavior of each
architecture allows realistic resource planning and prevents costly
architectural mismatches during deployment.

\subsection{Computational Complexity
Considerations}\label{sec-dnn-architectures-computational-complexity-considerations-08da}

Architecture selection must account for computational and memory
trade-offs that determine deployment feasibility. Each architecture
exhibits distinct scaling behaviors that create different bottlenecks as
problem size increases, and understanding these patterns allows
realistic resource planning.

The computational profile of each architecture reflects its underlying
design philosophy. Dense architectures like MLPs prioritize
representational capacity through full connectivity, while structured
architectures like CNNs achieve efficiency through parameter sharing and
locality assumptions. Sequential architectures like RNNs trade
parallelization for memory efficiency, while attention-based
architectures like Transformers exchange memory for computational
flexibility. As Table~\ref{tbl-computational-complexity} showed earlier
alongside Table~\ref{tbl-arch-complexity}, examining these same
architectures from both computational scaling and memory access
perspectives reveals different optimization opportunities and system
design considerations.

\subsubsection{Scalability and Production
Considerations}\label{sec-dnn-architectures-scalability-production-considerations-68dd}

Production deployment introduces constraints beyond algorithmic
performance, including latency requirements, memory limitations, energy
budgets, and fault tolerance needs. Each architecture exhibits distinct
production characteristics that determine real-world feasibility.

MLPs and CNNs scale well across multiple devices through data
parallelism, achieving near-linear speedups with proper batch size
scaling. RNNs face parallelization challenges due to sequential
dependencies, requiring pipeline parallelism or other specialized
techniques. Transformers achieve excellent parallelization across
sequence positions but suffer from quadratic memory scaling that limits
batch sizes and effective utilization.

MLPs provide predictable latency proportional to layer size, making them
suitable for real-time applications with strict SLA requirements. CNNs
exhibit variable latency depending on implementation strategy and
hardware capabilities, with optimized implementations achieving
sub-millisecond inference. RNNs create latency dependencies on sequence
length, making them challenging for interactive applications.
Transformers provide excellent throughput for batch processing but
struggle with single-inference latency due to attention overhead.

Memory requirements vary significantly across architectures in
production environments. MLPs require fixed memory proportional to model
size, enabling straightforward capacity planning. CNNs need variable
memory for feature maps that scales with input resolution, requiring
dynamic memory management for variable-size inputs. RNNs maintain
constant memory for hidden states but may require unbounded memory for
very long sequences. Transformers face quadratic memory growth that
creates hard limits on sequence length in production.

Fault tolerance and recovery characteristics differ significantly
between architectures. MLPs and CNNs exhibit stateless computation that
allows straightforward checkpointing and recovery. RNNs maintain
temporal state that complicates distributed training and failure
recovery procedures. Transformers combine stateless computation with
massive memory requirements, making checkpoint sizes a practical concern
for large models.

Hardware mapping efficiency varies considerably across architectural
patterns. Modern MLPs achieve 80-90\% of peak hardware performance on
specialized tensor units. CNNs reach 60-75\% efficiency depending on
layer configuration and memory hierarchy design. RNNs typically achieve
30-50\% of peak performance due to sequential constraints and irregular
memory access patterns. Transformers achieve 70-85\% efficiency for
large batch sizes but drop significantly for small batches due to
attention overhead.

\subsubsection{Hardware Mapping and Optimization
Strategies}\label{sec-dnn-architectures-hardware-mapping-optimization-strategies-ec3b}

Different architectural patterns require distinct optimization
strategies for efficient hardware mapping. Understanding these patterns
allows systematic performance tuning and hardware selection decisions.

Dense matrix operations in MLPs map naturally to tensor processing units
and GPU tensor cores. These operations benefit from several key
optimizations: matrix tiling to fit cache hierarchies, mixed-precision
computation to double throughput, and operation fusion to reduce memory
traffic. Optimal tile sizes depend on cache hierarchy, typically
\(64 \times 64\) for L1 cache and \(256 \times 256\) for L2, while
tensor cores achieve peak efficiency with specific dimension multiples
such as \(16 \times 16\) blocks for Volta architecture.

CNNs benefit from specialized convolution algorithms and data layout
optimizations that differ significantly from dense matrix operations.
Im2col transformations convert convolutions to matrix multiplication but
double memory usage. Winograd algorithms\sidenote{\textbf{Winograd
Algorithms}: Fast convolution algorithms based on Shmuel Winograd's 1980
work on minimal multiplication complexity. For 3x3 convolutions,
Winograd reduces multiply operations from 9 to 4 per output (2.25x
reduction) by trading multiplications for additions, which cost less in
terms of both latency and energy. Modern deep learning frameworks like
cuDNN automatically select Winograd for appropriate layer
configurations, though numerical precision degradation at FP16 limits
applicability for mixed-precision training. } reduce arithmetic
complexity by 2.25\(\times\) for \(3 \times 3\) convolutions at the cost
of numerical stability. Direct convolution with custom kernels achieves
optimal memory efficiency but requires architecture-specific tuning.

RNNs require different optimization approaches due to their temporal
dependencies. Loop unrolling reduces control overhead but increases
memory usage. State vectorization allows SIMD operations across multiple
sequences. Wavefront parallelization exploits independence across
timesteps for bidirectional processing.

Transformer attention demands specialized optimizations that reduce
memory usage and complexity. Techniques such as
FlashAttention\sidenote{\textbf{FlashAttention}: An IO-aware attention
algorithm developed by Tri Dao et al.~(2022) that avoids materializing
the full N x N attention matrix in GPU HBM. By fusing attention
computation into a single kernel and tiling to fit in SRAM,
FlashAttention achieves 2-4x speedup and reduces memory from O(N\^{}2)
to O(N) for sequence length N. This enables training on sequences 4-16x
longer than standard attention implementations, making it foundational
for modern long-context language models. } and sparse attention
patterns, which can dramatically reduce resource requirements, are
examined in \textbf{?@sec-model-compression}.

Multi-Layer Perceptrons represent the most straightforward computational
pattern, with costs dominated by matrix multiplications. The dense
connectivity that enables MLPs to model arbitrary relationships comes at
the price of quadratic parameter growth with layer width. Each neuron
connects to every neuron in the previous layer, creating large parameter
counts that grow quadratically with network width. The computation is
dominated by matrix-vector products, which are highly optimized on
modern hardware. Matrix operations are inherently parallel and map
efficiently to GPU architectures, with each output neuron computed
independently. The optimization techniques for reducing these parameter
counts, including pruning and low-rank approximations specifically
targeting dense layers, are covered in \textbf{?@sec-model-compression}.

Convolutional Neural Networks achieve computational efficiency through
parameter sharing and spatial locality, but their costs scale with both
spatial dimensions and channel depth. The convolution operation's
computational intensity depends heavily on kernel size and feature map
resolution. Parameter sharing across spatial locations dramatically
reduces memory compared to equivalent MLPs, while computational cost
grows linearly with image resolution and quadratically with kernel size.
Feature map memory dominates usage and becomes prohibitive for
high-resolution inputs. Spatial independence enables parallel processing
across different spatial locations and channels, though memory bandwidth
often becomes the limiting factor.

Recurrent Neural Networks optimize for memory efficiency at the cost of
parallelization. Their sequential nature creates computational
bottlenecks but enables processing of variable-length sequences with
constant memory overhead. The hidden-to-hidden connections (\(h^2\)
term) dominate parameter count for large hidden states. Sequential
dependencies prevent parallel processing across time, making RNNs
inherently slower than feedforward alternatives. Their constant memory
usage for hidden state storage makes RNNs memory-efficient for long
sequences, with this efficiency coming at the cost of computational
speed.

Transformers achieve maximum flexibility through attention mechanisms
but pay a steep price in memory usage. Their quadratic scaling with
sequence length creates limits on the sequences they can process.
Parameter count scales with model dimension but remains independent of
sequence length. The \(n^2\) term from attention computation dominates
for long sequences, while the \(n \times d^2\) term from feed-forward
layers dominates for short sequences. Attention matrices create the
primary memory bottleneck, as each attention head must store pairwise
similarities between all sequence positions, leading to prohibitive
memory usage for long sequences. While parallelization is excellent
across sequence positions and attention heads, the quadratic memory
requirement often forces smaller batch sizes, limiting effective
parallelization.

These complexity patterns define optimal domains for each architecture.
MLPs excel when parameter efficiency is not critical, CNNs dominate for
moderate-resolution spatial data, RNNs remain viable for very long
sequences where memory is constrained, and Transformers excel for
complex relational tasks where their computational cost is justified
through superior performance. With these quantitative foundations
established, we can now formalize the architecture selection process
into a systematic decision framework.

\subsection{Decision
Framework}\label{sec-dnn-architectures-decision-framework-a889}

Effective architecture selection requires balancing multiple competing
factors: data characteristics, computational resources, performance
requirements, and deployment constraints. While data patterns provide
initial guidance and complexity analysis establishes feasibility bounds,
final architectural choices often involve nuanced trade-offs demanding
systematic evaluation.

The decision flowchart in Figure~\ref{fig-dnn-fm-framework} guides
systematic architecture selection by first matching data characteristics
to architectural strengths, then validating against practical
constraints. The process is inherently iterative: resource limitations
or performance gaps often necessitate reconsidering earlier choices,
ensuring consideration of all relevant factors while avoiding selection
based on novelty or perceived sophistication.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/7cea7f3c22d6c3f3ed19ec3c2ec0975ef042c977.pdf}}

}

\caption{\label{fig-dnn-fm-framework}\textbf{Architecture Selection
Decision Framework}: A systematic flowchart for choosing neural network
architectures based on data characteristics and deployment constraints.
The process begins with data type identification
(text/sequences/images/tabular) to select initial architecture
candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates
memory budget, computational cost, inference speed, accuracy targets,
and hardware compatibility.}

\end{figure}%

The framework applies through four key steps. First, data analysis:
pattern types in data provide the strongest initial signal. Spatial data
naturally aligns with CNNs, sequential data with RNNs. Second,
progressive constraint validation: each constraint check (memory,
computational budget, inference speed) acts as a filter. Failing any
constraint requires either scaling down the current architecture or
considering a fundamentally different approach.

Third, iterative trade-off handling when accuracy targets remain unmet:
additional model capacity may be needed, requiring a return to
constraint checking. If deployment hardware cannot support the chosen
architecture, reconsidering the entire architectural approach may be
necessary. Fourth, practitioners should anticipate multiple iterations,
as real projects typically cycle through this framework several times
before reaching an optimal balance between data fit, computational
feasibility, and deployment requirements.

This systematic approach prevents architecture selection based solely on
novelty or perceived sophistication, aligning choices with both problem
requirements and system capabilities.

The decision framework above provides practical guidance for
architecture selection, but a deeper question remains: what unifies
these diverse architectures at a theoretical level? Understanding the
common principles beneath their apparent diversity reveals why certain
architectures succeed for specific problems and how to reason about new
architectures as they emerge.

\section{Unified Framework: Inductive
Biases}\label{sec-dnn-architectures-unified-framework-inductive-biases-257d}

The architectural diversity explored throughout this chapter shares a
unified theoretical framework: each architecture embodies specific
inductive biases that constrain the hypothesis space and guide learning
toward solutions appropriate for different data types and problem
structures.

Different architectures form a hierarchy of decreasing inductive bias.
CNNs exhibit the strongest inductive biases through local connectivity,
parameter sharing, and translation equivariance. These constraints
dramatically reduce the parameter space while limiting flexibility to
spatial data with local structure. RNNs demonstrate moderate inductive
bias through sequential processing and shared temporal weights. The
hidden state mechanism assumes that past information influences current
processing, rendering them appropriate for temporal sequences.

MLPs maintain minimal architectural bias beyond layer-wise processing.
Dense connectivity allows modeling arbitrary relationships but requires
more data to learn structure that other architectures encode explicitly.
Transformers represent adaptive inductive bias through learned attention
patterns. The architecture can dynamically adjust its inductive bias
based on the data, combining flexibility with the ability to discover
relevant structural regularities.

All successful architectures implement forms of hierarchical
representation learning, but through different mechanisms. CNNs build
spatial hierarchies through progressive receptive field expansion,
applying the spatial pattern processing framework detailed in
Section~\ref{sec-dnn-architectures-cnns-spatial-pattern-processing-5b8d}.
RNNs build temporal hierarchies through hidden state evolution,
extending the sequential processing approach from
Section~\ref{sec-dnn-architectures-rnns-sequential-pattern-processing-f804}.
Transformers build content-dependent hierarchies through multi-head
attention, applying the dynamic pattern processing mechanisms described
in
Section~\ref{sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-22df}.

This hierarchical organization reflects a general principle: complex
patterns can be efficiently represented through composition of simpler
components. The success of deep learning stems from the finding that
gradient-based optimization can effectively learn these compositional
structures when provided with appropriate architectural inductive
biases.

The theoretical insights about representation learning have direct
implications for systems engineering. Hierarchical representations
require computational patterns that can efficiently compose lower-level
features into higher-level abstractions. This drives system design
decisions. Memory hierarchies must align with representational
hierarchies to minimize data movement costs. Parallelization strategies
must respect the dependency structure of hierarchical computation.
Hardware accelerators must efficiently support the matrix operations
that implement feature composition. Software frameworks must provide
abstractions that enable efficient hierarchical computation across
diverse architectures.

Understanding architectures as embodying different inductive biases
helps explain both their strengths and their systems requirements,
providing a principled foundation for architecture selection and system
optimization decisions.

With this theoretical foundation established, we can now demonstrate how
to apply these principles systematically. The following calculations
illustrate how the quantitative characteristics from
Table~\ref{tbl-lighthouse-comparison} translate into real design
constraints, beginning with the \emph{capacity wall} that recommendation
systems encounter.

\phantomsection\label{callout-notebookux2a-1.22}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Capacity Wall}
\phantomsection\label{callout-notebook*-1.22}
\textbf{Problem}: You are building a recommendation system for a store
with \textbf{100 Million items}. You use an embedding size of
\textbf{128}. How much memory do you need for just the item table?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Table Entries}: 100 Million.
\item
  \textbf{Vector Size}: 128 elements.
\item
  \textbf{Precision}: FP32 (4 bytes per element).
\item
  \textbf{Table Size}:
  \(100 \text{ Million} \times 128 \times 4 \text{ bytes} \approx \mathbf{51\.2 \text{ GB}}\).
\end{enumerate}

\textbf{The Systems Conclusion}: A single embedding table for one
feature (Items) already consumes \textbf{64\%} of an 80GB A100 GPU. If
you add a User table of the same size, the model \textbf{cannot fit} on
a single machine. DLRM is \textbf{Capacity-Bound}, necessitating the
``scale-out'' distributed memory systems discussed in
\textbf{?@sec-ai-training}.

\end{fbxSimple}

While DLRM workloads are capacity-bound, vision workloads face a
different constraint: sustained throughput for real-time processing.

\phantomsection\label{callout-notebookux2a-1.23}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Throughput Ceiling}
\phantomsection\label{callout-notebook*-1.23}
\textbf{Problem}: You need to process \textbf{30 frames per second} of
video with ResNet-50 for a real-time application. What sustained compute
throughput do you need?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Model Cost}: ResNet-50 requires \textasciitilde4 GFLOPs per
  224×224 image.
\item
  \textbf{Frame Rate}: 30 FPS required.
\item
  \textbf{Sustained Throughput}:
  \(30 \times 4 \text{ GFLOPs} = \mathbf{123 \text{ GFLOPs/sec}}\).
\end{enumerate}

\textbf{The Systems Conclusion}: A mid-range GPU delivering 10 TFLOPs
theoretical peak achieves \textasciitilde50--60\% utilization in
practice, yielding \textbf{5--6 TFLOPs effective}. For ResNet-50 at 30
FPS, you have \textbf{41x headroom}, easily achievable. But switch to an
object detection model at 100 GFLOPs per frame, and you need \textbf{3
TFLOPs sustained}, leaving only \textbf{2x headroom}. Add batch size
constraints or multi-stream processing, and you quickly approach the
compute ceiling. ResNet-50 is \textbf{Compute-Bound}, but with
comfortable margins on modern hardware.

\end{fbxSimple}

These back-of-the-napkin calculations demonstrate how constraints
manifest in practice. The following worked example shows how to apply
these principles systematically when selecting architectures for
production systems.

\section{Putting It All Together: Architecture Selection in
Practice}\label{sec-dnn-architectures-putting-together-architecture-selection-practice-052f}

This section synthesizes the chapter's concepts through a complete
architecture selection exercise. Rather than isolated examples, we walk
through the full decision process an ML systems engineer would follow,
using a \emph{real-time wildlife monitoring} scenario as the integrating
case study.

\phantomsection\label{callout-exampleux2a-1.24}
\begin{fbxSimple}{callout-example}{Example:}{Real-Time Wildlife Monitoring}
\phantomsection\label{callout-example*-1.24}
\textbf{Problem Statement:} Design an ML system to identify wildlife
species from camera trap images in a national park. The system must
process images locally (no cloud connectivity), operate on battery power
for 6 months, and achieve 90\%+ accuracy on 50 target species.

\subsubsection*{Step 1: Data
Characterization}\label{step-1-data-characterization}
\addcontentsline{toc}{subsubsection}{Step 1: Data Characterization}

The input is spatial data (images from camera traps, typically 1920×1080
resolution, downsampled to 224×224 for processing). The task requires
recognizing visual patterns (fur textures, body shapes, distinctive
markings) that are:

\begin{itemize}
\tightlist
\item
  \textbf{Spatially local}: Species identification relies on local
  features (ear shape, stripe patterns)
\item
  \textbf{Translation invariant}: A deer in the top-left is still a deer
  in the bottom-right
\item
  \textbf{Hierarchical}: Low-level edges combine into textures, then
  body parts, then whole animals
\end{itemize}

\textbf{Initial Architecture Candidate:} CNN (matches spatial locality
and translation invariance)

\subsubsection*{Step 2: Constraint
Analysis}\label{step-2-constraint-analysis}
\addcontentsline{toc}{subsubsection}{Step 2: Constraint Analysis}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1684}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3053}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5263}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Requirement}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Connectivity} & None (offline) & All inference must run
on-device \\
\textbf{Power} & \textasciitilde2W average (solar + battery) & Rules out
GPUs; must use low-power MCU or edge NPU \\
\textbf{Latency} & \textless500ms per detection & Allows batch size 1,
no real-time streaming \\
\textbf{Memory} & 512 MB RAM, 2 GB storage & Model must fit in
\textasciitilde100 MB after quantization \\
\textbf{Accuracy} & 90\%+ on 50 species & Requires sufficient model
capacity \\
\end{longtable}

\subsubsection*{Step 3: Architecture Evaluation Using Lighthouse
Benchmarks}\label{step-3-architecture-evaluation-using-lighthouse-benchmarks}
\addcontentsline{toc}{subsubsection}{Step 3: Architecture Evaluation
Using Lighthouse Benchmarks}

Compare candidates against our Lighthouse models:

\begin{itemize}
\item
  \textbf{ResNet-50} (25.6M params, 4.1 GFLOPs): Too large. At
  \textasciitilde102 MB FP32, leaves no room for OS and buffers. Power
  consumption would exceed budget.
\item
  \textbf{MobileNetV1} (4.2M params, 569 MFLOPs): Promising.
  \textasciitilde17 MB at FP32, \textasciitilde4 MB quantized to INT8.
  Power-efficient depthwise separable convolutions.
\item
  \textbf{KWS DS-CNN} (200K params, 20 MFLOPs): Too small. Designed for
  12-class audio, insufficient capacity for 50 visual species.
\end{itemize}

\textbf{Architecture Selection:} MobileNetV2 variant with width
multiplier 0.75

\begin{itemize}
\tightlist
\item
  \textbf{Parameters:} \textasciitilde2.2M (\textasciitilde9 MB FP32,
  \textasciitilde2.2 MB INT8)
\item
  \textbf{FLOPs:} \textasciitilde150 MFLOPs at 224×224
\item
  \textbf{Rationale:} Sufficient capacity for 50-class problem; fits
  memory budget with margin; depthwise separable convolutions are
  power-efficient
\end{itemize}

\subsubsection*{Step 4: Systems
Validation}\label{step-4-systems-validation}
\addcontentsline{toc}{subsubsection}{Step 4: Systems Validation}

\emph{Memory Check:} \[
\underbrace{\text{2.5 MB}}_{\text{Model}} + \underbrace{224 \times 224 \times 64 \times 4 \approx \text{12 MB}}_{\text{Activations}} + \underbrace{\text{50 MB}}_{\text{OS/Buffers}} = \text{65 MB} \ll \text{512 MB}~\checkmark
\]

\emph{Compute Check:} Target device: ARM Cortex-A53 @ 1.2 GHz with NEON
SIMD (\textasciitilde2 GOPS INT8)

\[\frac{150 \text{ MOPs}}{2 \text{ GOPS}} = 75 \text{ ms latency} \ll 500 \text{ ms target} \checkmark\]

\emph{Power Check:} Estimated inference power: \textasciitilde200 mW for
75 ms = 15 mJ per inference At 100 inferences/day: 1.5 J/day →
negligible vs.~sleep power budget ✓

\subsubsection*{Step 5: Risk Assessment}\label{step-5-risk-assessment}
\addcontentsline{toc}{subsubsection}{Step 5: Risk Assessment}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3162}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6838}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Risk}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mitigation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{90\% accuracy not achieved} & Train on augmented dataset;
consider EfficientNet-Lite if MobileNet insufficient \\
\textbf{Thermal throttling in enclosure} & Add passive heatsink; reduce
inference frequency in high-temperature conditions \\
\textbf{New species added post-deployment} & Reserve 10\% model
capacity; plan for OTA update mechanism \\
\end{longtable}

\textbf{Final Decision:} MobileNetV2 (0.75× width) with INT8
quantization, deployed on Cortex-A53 SoC with 512 MB RAM.

This architecture achieves the accuracy target while operating within
the 2W power envelope, processing images in \textless100ms, and leaving
sufficient memory headroom for system operations. The decision was
driven by matching the CNN inductive bias to spatial data
characteristics, then validating against hardware constraints using
quantitative analysis.

\end{fbxSimple}

This worked example demonstrates the systematic approach that transforms
architectural knowledge into practical engineering decisions. However,
theoretical understanding and even systematic methodology do not prevent
practitioners from making poor choices. The gap between knowing
principles and applying them correctly is where engineering experience
proves essential.

\section{Fallacies and
Pitfalls}\label{sec-dnn-architectures-fallacies-pitfalls-96b4}

Organizations regularly deploy transformers for problems where CNNs
would suffice at 10x lower cost, or select RNNs without accounting for
sequential bottlenecks that cripple throughput. These mistakes stem from
specific misconceptions about architecture performance that appear
plausible but collapse under systems analysis. Recognizing these
pitfalls transforms theoretical knowledge into practical engineering
judgment.

\paragraph*{\texorpdfstring{Fallacy: \emph{More complex architectures
always perform better than simpler
ones.}}{Fallacy: More complex architectures always perform better than simpler ones.}}\label{fallacy-more-complex-architectures-always-perform-better-than-simpler-ones.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{More complex
architectures always perform better than simpler ones.}}

Engineers assume that transformer-based models outperform simpler
architectures across all tasks. In production, architectural
sophistication must match problem complexity and available computational
resources. As demonstrated in
Section~\ref{sec-dnn-architectures-architectural-principles-engineering-tradeoffs-362e},
a CNN achieves 99\% accuracy on MNIST with 100K parameters while an MLP
requires 20M parameters for 98\% accuracy. The CNN uses 200x fewer
parameters and trains substantially faster. For problems with clear
structural patterns like spatial locality in images, CNNs exploit
inductive biases that MLPs cannot match efficiently. Transformers excel
at tasks requiring long-range dependencies but incur \(O(n^2)\) memory
costs for sequence length \(n\), making them infeasible for
resource-constrained deployments. Teams that default to transformers for
tabular data or small-image classification waste 5--10x computational
resources: a training job that could cost \$1,000 and complete in 2
hours becomes \$10,000 and 20 hours, with no accuracy benefit.

\paragraph*{\texorpdfstring{Pitfall: \emph{Selecting architectures based
solely on accuracy metrics without analyzing computational
requirements.}}{Pitfall: Selecting architectures based solely on accuracy metrics without analyzing computational requirements.}}\label{pitfall-selecting-architectures-based-solely-on-accuracy-metrics-without-analyzing-computational-requirements.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Selecting architectures
based solely on accuracy metrics without analyzing computational
requirements.}}

Many practitioners choose architectures from academic papers that report
state-of-the-art accuracy, ignoring the computational and memory
implications. A CNN achieves excellent image classification accuracy but
requires specialized memory access patterns for sliding window
operations. RNNs' sequential dependencies create serialization that
limits parallelization; as shown in
Section~\ref{sec-dnn-architectures-computational-complexity-considerations-08da},
RNNs achieve only 30--50\% of peak hardware performance compared to
80--90\% for MLPs due to sequential constraints. Transformers deliver
strong results but face quadratic memory scaling: a sequence length of
2048 requires 16× more memory than length 512. Production systems that
ignore these characteristics encounter deployment failures: models miss
latency SLAs (100ms target becomes 500ms reality, violating user-facing
contracts), exceed memory budgets (8GB estimation becomes 32GB
requirement, forcing expensive hardware upgrades), or achieve poor
hardware utilization (expected 80\% efficiency drops to 25\%, wasting
75\% of compute spend). These mismatches commonly add 2--6 months to
deployment timelines as teams scramble to redesign systems that looked
viable on paper.

\paragraph*{\texorpdfstring{Fallacy: \emph{Architecture performance
transfers uniformly across different hardware
platforms.}}{Fallacy: Architecture performance transfers uniformly across different hardware platforms.}}\label{fallacy-architecture-performance-transfers-uniformly-across-different-hardware-platforms.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Architecture performance
transfers uniformly across different hardware platforms.}}

This belief assumes benchmarks on high-end GPUs predict performance on
edge devices. In reality, hardware-architecture alignment determines
efficiency. As discussed in
Section~\ref{sec-dnn-architectures-systemlevel-building-blocks-41c5},
CNNs benefit from specialized matrix acceleration units achieving
80--95\% of peak throughput, while RNNs' irregular memory access
patterns yield only 30--50\% efficiency. A model optimized for GPU
matrix acceleration (exploiting \(16 \times 16\) matrix tile sizes) may
perform poorly on CPUs lacking specialized units or mobile processors
with limited SIMD width. Mobile deployment particularly exposes
mismatches: a transformer that runs at 50ms on an A100 GPU may require
2000ms on a mobile SoC due to lack of high-bandwidth memory and tensor
acceleration, a 40x slowdown that renders the model unusable for
interactive applications. Organizations that benchmark only on training
hardware discover these performance gaps late in development, often
requiring complete architecture redesigns that delay product launches by
quarters, not weeks.

\paragraph*{\texorpdfstring{Pitfall: \emph{Combining architectural
patterns without analyzing interaction effects at the system
level.}}{Pitfall: Combining architectural patterns without analyzing interaction effects at the system level.}}\label{pitfall-combining-architectural-patterns-without-analyzing-interaction-effects-at-the-system-level.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Combining architectural
patterns without analyzing interaction effects at the system level.}}

Engineers add attention mechanisms to CNNs or insert convolutional
layers into transformers expecting additive benefits. Each architectural
pattern creates distinct memory access characteristics: CNNs perform
sliding window operations with spatial locality, while attention
mechanisms require all-to-all communication with quadratic memory
requirements. Naive combinations create memory bandwidth conflicts:
attention layers may flush CNN feature maps from cache, eliminating
spatial locality benefits. A ResNet achieving 250 images/second can drop
to 80 images/second when attention layers disrupt the cache-optimized
convolution pipeline, a 3x throughput reduction that may require
tripling infrastructure costs to maintain the same capacity. Similarly,
adding recurrent connections to transformer encoders reintroduces
sequential dependencies that eliminate the parallelization advantages
transformers provide. Successful hybrid architectures require profiling
memory access patterns, cache behavior, and bandwidth utilization to
ensure components cooperate rather than conflict.

\paragraph*{\texorpdfstring{Pitfall: \emph{Optimizing architectural
decisions for training hardware without considering deployment
constraints.}}{Pitfall: Optimizing architectural decisions for training hardware without considering deployment constraints.}}\label{pitfall-optimizing-architectural-decisions-for-training-hardware-without-considering-deployment-constraints.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Optimizing architectural
decisions for training hardware without considering deployment
constraints.}}

Teams design architectures targeting high-end GPU clusters, then
discover deployment failures on edge devices or cloud instances with
different characteristics. An architecture exploiting 8x A100 GPUs with
80GB memory each (640GB total) cannot deploy to edge devices with 4GB
memory; the 160x memory reduction requires fundamental architectural
changes, not just quantization. As
Section~\ref{sec-dnn-architectures-decision-framework-a889} emphasizes,
effective architecture selection requires analyzing the full system
stack: compute infrastructure, compiler optimization capabilities,
target hardware characteristics, and operational constraints. A
500-layer ResNet trains efficiently on GPU clusters but compilation
takes 45 minutes and generates 8GB of optimized kernels, making
iterative development impractical. Edge deployment compounds these
issues: models must fit in 10-100MB storage, execute in 50-200ms
latency, and operate within 2-5W power budgets. Organizations that defer
deployment considerations to ``optimize later'' encounter fundamental
architectural mismatches requiring costly model redesigns.

With these cautionary notes in mind, we now synthesize the key concepts
that practitioners should carry forward.

\section{Summary}\label{sec-dnn-architectures-summary-e642}

Neural network architectures form specialized computational structures
tailored to process different types of data and solve distinct classes
of problems. Multi-Layer Perceptrons handle tabular data through dense
connections, convolutional networks exploit spatial locality in images,
and recurrent networks process sequential information. Each architecture
embodies specific assumptions about data structure and computational
patterns. Modern transformer architectures unify many of these concepts
through attention mechanisms that dynamically route information based on
relevance rather than fixed connectivity patterns.

Despite their apparent diversity, these architectures share fundamental
computational primitives that recur across different designs. Matrix
multiplication operations form the computational core, whether in dense
layers, convolutions, or attention mechanisms. Memory access patterns
vary significantly between architectures, with some requiring sliding
window operations for local processing while others demand global
information aggregation. The five \textbf{Lighthouse Models} (ResNet-50,
GPT-2/Llama, MobileNet, DLRM, and KWS) exemplify how different
architectures create distinct system bottlenecks (compute, memory
bandwidth, latency, memory capacity, and power), providing concrete
reference points for systems reasoning throughout this book. The
following key principles should guide your architecture decisions.

\phantomsection\label{callout-takeawaysux2a-1.25}
\begin{fbxSimple}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.25}

\begin{itemize}
\tightlist
\item
  \textbf{Match architecture to data structure}: MLPs for tabular, CNNs
  for spatial, RNNs for sequential, Transformers for relational, DLRM
  for high-cardinality categorical. Mismatched architectures incur
  10--100× greater computational cost.
\item
  \textbf{Characterize performance bottlenecks before optimization}:
  ResNet-50 is compute-bound (optimize FLOPS), GPT-2 is
  memory-bandwidth-bound (optimize data movement), DLRM is
  memory-capacity-bound (optimize distribution).
\item
  \textbf{Learnability outweighs representation}: Specialized
  architectures achieve superior performance not because MLPs lack
  representational power, but because inductive biases enable learning
  with 10--100× less data.
\item
  \textbf{Hardware constraints determine feasibility}: High accuracy
  becomes impractical if it exceeds memory budget, latency target, or
  power envelope. Validate hardware fit before optimizing accuracy.
\end{itemize}

\end{fbxSimple}

The Lighthouse Models introduced throughout this chapter provide
concrete reference points for systems reasoning: ResNet-50's
compute-bound convolutions, GPT-2's memory-bandwidth-bound attention,
MobileNet's latency-sensitive depthwise separables, DLRM's
memory-capacity-bound embeddings, and KWS's power-constrained inference.
The diversity of computational patterns established here explains why no
single optimization strategy suffices for all workloads: each
architecture creates distinct bottlenecks that demand targeted
solutions.

\phantomsection\label{callout-chapter-connectionux2a-1.26}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Blueprints to Construction}
\phantomsection\label{callout-chapter-connection*-1.26}
We have established the architectural blueprints for modern
intelligence, from the spatial efficiency of CNNs to the relational
power of Transformers. But a blueprint is not a building. We turn next
to \textbf{?@sec-ai-frameworks}, where we explore how deep learning
frameworks translate these abstract layers into executable code on
physical hardware.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-ba2016layer}
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. {``Layer
Normalization.''} \emph{arXiv Preprint arXiv:1607.06450}, July.
\url{http://arxiv.org/abs/1607.06450v1}.

\bibitem[\citeproctext]{ref-bahdanau2014neural}
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. {``Neural
Machine Translation by Jointly Learning to Align and Translate.''}
\emph{arXiv Preprint arXiv:1409.0473}, September.
\url{http://arxiv.org/abs/1409.0473v7}.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} Edited by Hugo Larochelle, Marc'Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin.
\emph{Advances in Neural Information Processing Systems} 33 (May):
1877--1901. \url{https://doi.org/10.48550/arxiv.2005.14165}.

\bibitem[\citeproctext]{ref-transformer_explainer}
Cho, Aeree, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J.
Wang, Seongmin Lee, Benjamin Hoover, and Duen Horng (Polo) Chau. 2025.
{``TRANSFORMER EXPLAINER: Interactive Learning of Text-Generative
Models.''} \emph{Proceedings of the AAAI Conference on Artificial
Intelligence} 39 (28): 29625--27.
\url{https://doi.org/10.1609/aaai.v39i28.35347}.

\bibitem[\citeproctext]{ref-cho2014properties}
Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua
Bengio. 2014. {``On the Properties of Neural Machine Translation:
Encoder-Decoder Approaches.''} In \emph{Eighth Workshop on Syntax,
Semantics and Structure in Statistical Translation (SSST-8)}, 103--11.
Association for Computational Linguistics.

\bibitem[\citeproctext]{ref-clark2019what}
Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.
2019. {``What Does BERT Look at? An Analysis of BERT's Attention.''} In
\emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and
Interpreting Neural Networks for NLP}, 276--86. Association for
Computational Linguistics. \url{https://doi.org/10.18653/v1/w19-4828}.

\bibitem[\citeproctext]{ref-cohen2016group}
Cohen, Taco, and Max Welling. 2016. {``Group Equivariant Convolutional
Networks.''} \emph{International Conference on Machine Learning},
2990--99.

\bibitem[\citeproctext]{ref-cover2006elements}
Cover, Thomas M., and Joy A. Thomas. 2001. \emph{Elements of Information
Theory}. 2nd ed. Wiley. \url{https://doi.org/10.1002/0471200611}.

\bibitem[\citeproctext]{ref-cybenko1989approximation}
Cybenko, G. 1989. {``Approximation by Superpositions of a Sigmoidal
Function.''} \emph{Mathematics of Control, Signals, and Systems} 2 (4):
303--14. \url{https://doi.org/10.1007/bf02551274}.

\bibitem[\citeproctext]{ref-devlin2018bert}
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
{``BERT: Pre-Training of Deep Bidirectional Transformers for Language
Understanding.''} \emph{arXiv Preprint arXiv:1810.04805}, October.
\url{http://arxiv.org/abs/1810.04805v2}.

\bibitem[\citeproctext]{ref-dosovitskiy2021image}
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.
2021. {``An Image Is Worth 16x16 Words: Transformers for Image
Recognition at Scale.''} \emph{International Conference on Learning
Representations}.

\bibitem[\citeproctext]{ref-elman1990finding}
Elman, Jeffrey L. 1990. {``Finding Structure in Time.''} \emph{Cognitive
Science} 14 (2): 179--211.
\url{https://doi.org/10.1207/s15516709cog1402/_1}.

\bibitem[\citeproctext]{ref-he2016deep}
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. {``Deep
Residual Learning for Image Recognition.''} In \emph{2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)}, 770--78.
IEEE. \url{https://doi.org/10.1109/cvpr.2016.90}.

\bibitem[\citeproctext]{ref-hochreiter1997long}
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. {``Long Short-Term
Memory.''} \emph{Neural Computation} 9 (8): 1735--80.
\url{https://doi.org/10.1162/neco.1997.9.8.1735}.

\bibitem[\citeproctext]{ref-hornik1989multilayer}
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989.
{``Multilayer Feedforward Networks Are Universal Approximators.''}
\emph{Neural Networks} 2 (5): 359--66.
\url{https://doi.org/10.1016/0893-6080(89)90020-8}.

\bibitem[\citeproctext]{ref-Horowitz2014}
Horowitz, Mark. 2014. {``1.1 Computing's Energy Problem (and What We Can
Do about It).''} In \emph{2014 IEEE International Solid-State Circuits
Conference Digest of Technical Papers (ISSCC)}, 10--14. IEEE.
\url{https://doi.org/10.1109/isscc.2014.6757323}.

\bibitem[\citeproctext]{ref-howard2017mobilenets}
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.
{``MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
Applications.''} \emph{ArXiv Preprint} abs/1704.04861 (April).
\url{http://arxiv.org/abs/1704.04861v1}.

\bibitem[\citeproctext]{ref-huang2017densely}
Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q.
Weinberger. 2017. {``Densely Connected Convolutional Networks.''} In
\emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)}, 2261--69. IEEE. \url{https://doi.org/10.1109/cvpr.2017.243}.

\bibitem[\citeproctext]{ref-hubel1962receptive}
Hubel, D. H., and T. N. Wiesel. 1962. {``Receptive Fields, Binocular
Interaction and Functional Architecture in the Cat's Visual Cortex.''}
\emph{The Journal of Physiology} 160 (1): 106--54.
\url{https://doi.org/10.1113/jphysiol.1962.sp006837}.

\bibitem[\citeproctext]{ref-ioffe2015batch}
Ioffe, Sergey, and Christian Szegedy. 2015. {``Batch Normalization:
Accelerating Deep Network Training by Reducing Internal Covariate
Shift.''} In \emph{Proceedings of the 32nd International Conference on
Machine Learning (ICML)}, 37:448--56.
\url{http://proceedings.mlr.press/v37/ioffe15.html}.

\bibitem[\citeproctext]{ref-kipf2016semi}
Kipf, Thomas N, and Max Welling. 2017. {``Semi-Supervised Classification
with Graph Convolutional Networks.''} In \emph{International Conference
on Learning Representations}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-lecun2015deep}
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. {``Deep
Learning.''} \emph{Nature} 521 (7553): 436--44.
\url{https://doi.org/10.1038/nature14539}.

\bibitem[\citeproctext]{ref-lecun1989backpropagation}
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W.
Hubbard, and L. D. Jackel. 1989. {``Backpropagation Applied to
Handwritten Zip Code Recognition.''} \emph{Neural Computation} 1 (4):
541--51. \url{https://doi.org/10.1162/neco.1989.1.4.541}.

\bibitem[\citeproctext]{ref-lecun1998gradient}
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. {``Gradient-Based
Learning Applied to Document Recognition.''} \emph{Proceedings of the
IEEE} 86 (11): 2278--2324. \url{https://doi.org/10.1109/5.726791}.

\bibitem[\citeproctext]{ref-mikolov2013efficient}
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
{``Efficient Estimation of Word Representations in Vector Space.''}
\emph{ICLR}, January. \url{http://arxiv.org/abs/1301.3781v3}.

\bibitem[\citeproctext]{ref-naumov2019deep}
Naumov, Maxim, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, et al. 2019. {``Deep
Learning Recommendation Model for Personalization and Recommendation
Systems.''} \emph{arXiv Preprint arXiv:1906.00091}, May.
\url{http://arxiv.org/abs/1906.00091v1}.

\bibitem[\citeproctext]{ref-radford2018improving}
Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
2018. {``Improving Language Understanding by Generative Pre-Training.''}

\bibitem[\citeproctext]{ref-reagen2017deep}
Reagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David
Brooks. 2017. \emph{Deep Learning for Computer Architects}. Springer
International Publishing.
\url{https://doi.org/10.1007/978-3-031-01756-8}.

\bibitem[\citeproctext]{ref-rosenblatt1958perceptron}
Rosenblatt, F. 1958. {``The Perceptron: A Probabilistic Model for
Information Storage and Organization in the Brain.''}
\emph{Psychological Review} 65 (6): 386--408.
\url{https://doi.org/10.1037/h0042519}.

\bibitem[\citeproctext]{ref-rumelhart1986learning}
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.
{``Learning Representations by Back-Propagating Errors.''} \emph{Nature}
323 (6088): 533--36. \url{https://doi.org/10.1038/323533a0}.

\bibitem[\citeproctext]{ref-simonyan2014very}
Simonyan, Karen, and Andrew Zisserman. 2014. {``Very Deep Convolutional
Networks for Large-Scale Image Recognition,''} September.
\url{http://arxiv.org/abs/1409.1556v6}.

\bibitem[\citeproctext]{ref-su2024roformer}
Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng
Liu. 2024. {``RoFormer: Enhanced Transformer with Rotary Position
Embedding.''} \emph{Neurocomputing} 568 (February): 127063.
\url{https://doi.org/10.1016/j.neucom.2023.127063}.

\bibitem[\citeproctext]{ref-szegedy2015going}
Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. 2015. {``Going Deeper with Convolutions.''} In \emph{2015
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
1--9. IEEE. \url{https://doi.org/10.1109/cvpr.2015.7298594}.

\bibitem[\citeproctext]{ref-vaswani2017attention}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N.Gomez, Lukasz Kaiser, and Illia Polosukhin. 2025.
{``Attention Is All You Need.''} Shenzhen Medical Academy of Research;
Translation. \url{https://doi.org/10.65215/ctdc8e75}.

\bibitem[\citeproctext]{ref-zhang2019root}
Wang, Di, Fengyuan Zhao, Rui Wang, Junwei Guo, Cihai Zhang, Huimin Liu,
Yongsheng Wang, Guohao Zong, Le Zhao, and Weihua Feng. 2023. {``A
Lightweight Convolutional Neural Network for Nicotine Prediction in
Tobacco by Near-Infrared Spectroscopy.''} \emph{Frontiers in Plant
Science} 14 (May): 1138693.
\url{https://doi.org/10.3389/fpls.2023.1138693}.

\bibitem[\citeproctext]{ref-cnn_explainer}
Wang, Zijie J., Robert Turko, Omar Shaikh, Haekyu Park, Nilaksh Das,
Fred Hohman, Minsuk Kahng, and Duen Horng Polo Chau. 2021. {``CNN
Explainer: Learning Convolutional Neural Networks with Interactive
Visualization.''} \emph{IEEE Transactions on Visualization and Computer
Graphics} 27 (2): 1396--406.
\url{https://doi.org/10.1109/tvcg.2020.3030418}.

\bibitem[\citeproctext]{ref-wei2022chain}
Wei, Jason, Xuezhi Wang 0002, Dale Schuurmans, Maarten Bosma, Brian
Ichter, Fei Xia 0002, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022.
{``Chain-of-Thought Prompting Elicits Reasoning in Large Language
Models.''} In \emph{Advances in Neural Information Processing Systems},
35:24824--37.\href{\%0A\%20\%20\%20\%20http://papers.nips.cc/paper/_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html\%0A\%20\%20}{http://papers.nips.cc/paper\textbackslash\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
}.

\bibitem[\citeproctext]{ref-wolpert1996lack}
Wolpert, David H. 1996. {``The Lack of a Priori Distinctions Between
Learning Algorithms.''} \emph{Neural Computation} 8 (7): 1341--90.
\url{https://doi.org/10.1162/neco.1996.8.7.1341}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
