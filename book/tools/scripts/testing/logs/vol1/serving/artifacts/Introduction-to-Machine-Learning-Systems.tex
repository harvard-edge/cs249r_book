% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% NOTE: TikZ colors (BlueLine, GreenLine, RedLine, OrangeLine, etc.) are defined
% in the YAML config files under format > pdf > tikz > include-headers.
% Only colors specific to LaTeX packages (not TikZ) are defined here.

% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
\titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
\titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
% Using height= instead of width= ensures consistent header heights across all icons
% regardless of aspect ratio
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[height=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4A7C59}
\definecolor{callout-chapter-connection-color1}{HTML}{EFF6FF}
\definecolor{callout-chapter-connection-color2}{HTML}{1E3A5F}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 2, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 2, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 2, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{Model Serving}\label{sec-model-serving-systems}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: An illustration of a machine learning serving
system, depicted as a high-speed assembly line in a modern factory. On
one end, raw data (images, text documents, sensor readings) enters on
conveyor belts. In the center, a glowing neural network processes inputs
through preprocessing stations, a central inference engine, and
postprocessing units. Workers monitor latency gauges and throughput
meters. Some requests wait in small batches while others stream through
individually. The scene conveys speed, precision, and the transformation
from raw input to useful predictions, with visible clocks emphasizing
the time-critical nature of serving.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/serving/images/png/cover_serving.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does serving invert every optimization priority that made
training successful?}

Benchmarks validated your optimizations under controlled conditions.
Serving reveals whether those optimizations survive contact with
production reality. Training and serving demand opposite physics.
Training maximizes throughput---large batches and long epochs where
latency spikes are absorbed invisibly. Serving demands
immediacy---individual requests answered in milliseconds, where a single
slow response is a \emph{broken product}. Training amortizes hardware
costs across billions of examples; serving pays a tax on every single
request, where inefficiency compounds into massive operational debt.
Training checkpoints and restarts; serving must never fail visibly. This
inversion is why models that train beautifully often serve poorly: the
batch-heavy architectures designed to saturate GPUs are fundamentally
ill-suited for the bursty, latency-critical reality of production.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, leftrule=.75mm, opacitybacktitle=0.6, bottomtitle=1mm, toprule=.15mm, arc=.35mm, colframe=quarto-callout-tip-color-frame, colback=white, titlerule=0mm, left=2mm, breakable, coltitle=black, rightrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, opacityback=0, toptitle=1mm]

\begin{itemize}
\tightlist
\item
  Explain the inversion from throughput optimization to latency
  minimization that distinguishes serving from training
\item
  Decompose request latency into preprocessing, inference, and
  postprocessing phases to identify bottlenecks
\item
  Apply queuing theory (Little's Law, M/M/1 models) and capacity
  planning to meet percentile latency SLOs
\item
  Identify sources of training-serving skew and select appropriate
  prevention strategies
\item
  Select batching and runtime strategies based on traffic patterns,
  latency constraints, and cost requirements
\item
  Evaluate deployment tradeoffs across precision, throughput, and
  infrastructure cost
\end{itemize}

\end{tcolorbox}

\section{The Serving
Paradigm}\label{sec-model-serving-systems-serving-paradigm-9634}

Serving marks the transition from model development to production
deployment, where the \textbf{Iron Law of ML Systems}
(\textbf{?@sec-silicon-contract}) undergoes a fundamental shift.
Training optimized for throughput, processing massive datasets over days
where the goal was samples per hour and latency spikes were absorbed
invisibly. Serving must deliver predictions within milliseconds under
unpredictable load. The \textbf{Latency term} (\(L_{lat}\)), the
irreducible overhead of request scheduling, network round-trips, and
system orchestration, becomes the dominant constraint rather than a
rounding error. This inversion, from throughput to \emph{latency}, from
controlled conditions to \emph{unpredictable traffic}, from offline to
\emph{real time}, defines the serving challenge.
\textbf{?@sec-benchmarking-ai} established techniques for measuring
performance under controlled conditions; serving faces traffic patterns
that no benchmark could anticipate. \textbf{?@sec-model-compression}
provided quantization methods that reduced model size; serving must
validate that those optimizations preserve accuracy under real traffic
distributions. This fundamental shift in constraints creates what we
call the \emph{serving inversion}.

\phantomsection\label{callout-perspectiveux2a-1.1}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Serving Inversion}
\phantomsection\label{callout-perspective*-1.1}

Applying the \textbf{DAM Taxonomy} reveals how deployment fundamentally
flips your engineering priorities:

\begin{itemize}
\tightlist
\item
  \textbf{Data (Information)}: In training, you maximize \textbf{Volume}
  (shuffling billions of samples). In serving, you maximize
  \textbf{Freshness} (processing one request \emph{right now}).
\item
  \textbf{Algorithm (Logic)}: In training, the math is \textbf{Mutable}
  (updating weights via backprop). In serving, the math is
  \textbf{Frozen} (fixed weights, forward pass only).
\item
  \textbf{Machine (Physics)}: In training, you maximize
  \textbf{Utilization} (keeping GPUs at 100\% to saturate throughput).
  In serving, you maximize \textbf{Headroom} (keeping GPUs at 50-70\% to
  survive traffic spikes).
\end{itemize}

\end{fbxSimple}

The consequences of ignoring this inversion become visceral during a
\emph{traffic spike} that pushes the system beyond what it was designed
to handle.

\phantomsection\label{callout-exampleux2a-1.2}
\begin{fbxSimple}{callout-example}{Example:}{The 'Black Friday' Traffic Spike}
\phantomsection\label{callout-example*-1.2}

\textbf{The Scenario}: An e-commerce recommendation system runs
comfortably at 50ms latency with 1,000 queries per second (QPS).

\textbf{The Event}: On Black Friday, traffic spikes 10x to 10,000 QPS.

\textbf{The Failure}: The system does not just slow down 10x; it
\textbf{collapses}. Latency hits 10 seconds, then requests start timing
out. The servers are 100\% utilized, but throughput drops to near zero.

\textbf{The Physics}: This is \textbf{Little's Law} and \textbf{Queueing
Theory} in action. As utilization approaches 100\%, queue lengths grow
exponentially, not linearly. The system spends more time managing the
queue (context switching, thrashing) than doing useful work.

\textbf{The Fix}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Load Shedding}: Reject excess requests immediately to keep the
  queue short.
\item
  \textbf{Autoscaling}: Spin up more replicas \emph{before} utilization
  hits the ``knee'' of the curve.
\item
  \textbf{Degradation}: Serve cached/dumber recommendations to reduce
  compute cost per query.
\end{enumerate}

\end{fbxSimple}

As shown in Figure~\ref{fig-tail-latency-explosion}, this physics
dictates \emph{why} production systems must run at relatively low
utilization (40-60\%) to guarantee stable tail latency (p99).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/serving/serving_files/figure-pdf/fig-tail-latency-explosion-output-1.pdf}}

}

\caption{\label{fig-tail-latency-explosion}\textbf{The Tail Latency
Explosion}: Request Latency vs.~System Utilization (\(\rho\)). While
median latency (Blue) remains stable, tail latency (Red, p99) explodes
exponentially once utilization passes the `Knee' at \textasciitilde70\%.
This physics-driven behavior dictates that production serving clusters
must maintain headroom (40-60\% utilization) to guarantee stable
response times and avoid queue collapse under stochastic load.}

\end{figure}%

Beyond the technical limits of latency, the economics of serving have
undergone a radical transformation. As models become more efficient and
hardware becomes more specialized, the cost of ``intelligence'' is
collapsing. This trend is visualized in
Figure~\ref{fig-intelligence-deflation}, which tracks the plummeting
price of token generation across model generations.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/serving/serving_files/figure-pdf/fig-intelligence-deflation-output-1.pdf}}

}

\caption{\label{fig-intelligence-deflation}\textbf{Intelligence
Deflation}: Cost per 1M tokens (USD) over time (Log Scale). The cost of
token generation has collapsed by multiple orders of magnitude
(2020--2025). Initially driven by OpenAI's GPT series, the market has
entered a phase of intense price competition with entrants like
Anthropic (Claude), Google (Gemini), and DeepSeek pushing costs below
\$0.10/million tokens. This hyper-deflation transforms the economics of
automated AI workflows.}

\end{figure}%

These priorities motivate a precise definition of the discipline this
chapter addresses.

\phantomsection\label{callout-definitionux2a-1.3}
\begin{fbxSimple}{callout-definition}{Definition:}{Model Serving}
\phantomsection\label{callout-definition*-1.3}
\textbf{\emph{Model Serving}} is the operational phase that inverts the
\textbf{Throughput} priority of training into a \textbf{Latency}
constraint. It requires a distinct architectural stack designed to
minimize the \textbf{Tail Latency} of individual inferences under
stochastic load, bounded by the \textbf{Service Level Objective (SLO)}.

\end{fbxSimple}

Serving systems must execute a complete inference pipeline under latency
constraints, not just the neural network computation.
Figure~\ref{fig-serving-inference-pipeline} illustrates this pipeline:
raw inputs flow through preprocessing (traditional computing), neural
network inference (deep learning), and postprocessing (traditional
computing) before producing final outputs. Each stage contributes to
total latency, and bottlenecks can occur anywhere in the pipeline.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/07306c9ad5e8f6b8f4d268f12b05976aa5e920f3.pdf}}

}

\caption{\label{fig-serving-inference-pipeline}\textbf{The Inference
Pipeline}: ML serving systems transform raw inputs into final outputs
through sequential stages: preprocessing, neural network computation,
and postprocessing. The neural network represents just one component;
preprocessing and postprocessing rely on traditional computing and often
dominate total latency in optimized systems.}

\end{figure}%

This chapter explains how to build systems that orchestrate this
pipeline efficiently. The chapter proceeds in three parts. \textbf{Part
1} establishes system fundamentals: serving architectures, server
anatomy, and the protocols that connect clients to models. \textbf{Part
2} follows the request lifecycle: where latency accumulates across
preprocessing, inference, and postprocessing, then how queuing dynamics
govern system behavior under load. \textbf{Part 3} addresses
optimization: model lifecycle management ensures models are ready to
serve, batching strategies maximize throughput, LLM-specific techniques
handle generative workloads, runtime selection tunes performance, and
economics translate these choices into infrastructure decisions.

\subsection{Static vs Dynamic
Inference}\label{sec-model-serving-systems-static-vs-dynamic-inference-e864}

The first architectural decision in any serving system is whether
predictions happen before or during user requests
(\citeproc{ref-google2024staticdynamic}{Google 2024}). This choice
shapes system design, cost structure, and capability boundaries.

\textbf{Static inference} (also called offline or batch inference)
pre-computes predictions for anticipated inputs and stores them for
retrieval. Consider a recommendation system that generates predictions
for all user-item pairs nightly. When a user requests recommendations,
the system retrieves pre-computed results from a lookup table rather
than running inference. This approach eliminates inference latency
entirely since results already exist, enables quality verification
before deployment, and reduces serving costs. However, static inference
cannot handle novel inputs that were not anticipated during the batch
computation and introduces hours or days of latency when models update.

\textbf{Dynamic inference} (also called online or real-time inference)
computes predictions on demand when requests arrive. This handles any
input, including rare edge cases and novel combinations, and immediately
reflects model updates. The cost is strict latency requirements that may
force simpler models and more intensive monitoring infrastructure.

For our ResNet-50 image classifier, consider two deployment scenarios. A
\textbf{static approach} suits a photo organization app that
pre-classifies all images in a user's library overnight. With 10,000
photos and 5ms inference each, batch processing takes \textasciitilde50
seconds total, and users see instant classification when browsing. A
\textbf{dynamic approach} suits a content moderation API that must
classify user-uploaded images in real-time, with each image requiring
the full preprocessing→inference→postprocessing pipeline and a 100ms
latency budget. Most production image classification systems use a
\textbf{hybrid approach}: frequently requested images (popular products,
known memes) are pre-classified and cached, while novel uploads trigger
dynamic inference.

The choice between static and dynamic serving has direct economic
implications. Stricter latency requirements directly translate into
higher infrastructure costs, creating a tradeoff that serving system
architects must navigate carefully.

\phantomsection\label{notebook-cost-latency}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Cost of Latency}
\phantomsection\label{notebook-cost-latency}
Latency constraints directly dictate infrastructure costs. Consider a
GPU server renting for \$4/hour.

\textbf{Scenario A (Low Latency):} Batch size 1.

\begin{itemize}
\tightlist
\item
  Latency: 5ms.
\item
  Throughput: 200 req/s.
\item
  Cost per million queries: \textbf{\$5.56}.
\end{itemize}

\textbf{Scenario B (High Throughput):} Batch size 8.

\begin{itemize}
\tightlist
\item
  Latency: 10ms (doubled due to batching overhead).
\item
  Throughput: 800 req/s (quadrupled due to parallel efficiency).
\item
  Cost per million queries: \textbf{\$1.39}.
\end{itemize}

\textbf{The Trade-off:} Reducing latency from 10ms to 5ms increases the
hardware bill by \textbf{300\%}. Engineers must quantify whether that
5ms speedup generates enough business value to justify the 4x cost
increase.

\end{fbxSimple}

Most production systems combine both approaches. Common queries hit a
cache populated by batch inference while uncommon requests trigger
dynamic computation. Understanding this spectrum is essential because it
determines which subsequent optimization strategies apply. Static
inference optimizes for throughput during batch computation and storage
efficiency for serving. Dynamic inference optimizes for per-request
latency under concurrent load, which requires understanding \emph{where}
time goes within each request.

The static-versus-dynamic decision is just the first of several
architectural choices that shape serving system design. Equally
important is \emph{where} the model executes, since deployment context
fundamentally constrains every subsequent optimization.

\subsection{The Spectrum of Serving
Architectures}\label{sec-model-serving-systems-spectrum-serving-architectures-8966}

While ``serving'' often implies a networked server processing API
requests, the architectural pattern varies fundamentally by deployment
environment. This spectrum matters for Volume I's focus on mastering the
ML node, since the same model may require radically different serving
strategies depending on \emph{where} it executes.

\textbf{1. Networked Serving (Cloud/Datacenter)}

The model runs as a standalone service (microservice). The primary
interface is the network (HTTP/gRPC). Optimization focuses on
\textbf{throughput} (batching) and \textbf{concurrency}.

\begin{itemize}
\tightlist
\item
  \emph{Key Constraint:} Network bandwidth and serialization cost.
\item
  \emph{Typical Hardware:} NVIDIA GPUs (V100, A100, H100), Google TPUs,
  AWS Inferentia.
\item
  \emph{Cold Start:} Seconds to minutes (container startup, model
  loading, warmup).
\end{itemize}

\textbf{2. Application-Embedded Serving (Mobile/Edge)}

The model runs within the user application process (e.g., a smartphone
app using CoreML or TensorFlow Lite). There is no ``server.'' The
interface is a function call. Optimization focuses on \textbf{energy}
and \textbf{responsiveness} (SingleStream).

\begin{itemize}
\tightlist
\item
  \emph{Key Advantage:} \textbf{Zero-Copy Inference}. When data moves
  through a system, each copy consumes CPU cycles and memory bandwidth.
  In cloud serving, a camera frame might be copied four times: from
  network buffer to application memory, then to a preprocessing buffer,
  then to GPU-accessible memory, and finally to GPU VRAM. Mobile NPUs
  can eliminate most of these copies by sharing memory directly with the
  camera hardware. The camera writes pixels into a buffer that the NPU
  reads directly, avoiding the CPU entirely. This reduces both latency
  (no copy operations) and energy (memory copies consume significant
  power). The mechanism requires hardware support: the camera, CPU, and
  NPU must share a unified memory architecture, which modern mobile SoCs
  like Apple's M-series and Qualcomm Snapdragon provide.
\item
  \emph{Typical Hardware:} Mobile NPUs (Apple Neural Engine, Qualcomm
  Hexagon), embedded GPUs (Jetson).
\item
  \emph{Cold Start:} Milliseconds (model already in app memory); first
  inference may trigger JIT compilation (100-500ms).
\item
  \emph{Power Budget:} 1-5W sustained, with thermal throttling after
  prolonged inference.
\end{itemize}

\textbf{3. Bare-Metal Serving (TinyML)}

The model is compiled into the firmware of a microcontroller. There is
no operating system or dynamic memory allocator. ``Serving'' is a tight
loop reading sensors and invoking the interpreter. Optimization focuses
on \textbf{static memory usage} (fitting in SRAM).

\begin{itemize}
\tightlist
\item
  \emph{Key Difference:} All memory is pre-allocated (Tensor Arena).
  Dynamic batching is impossible.
\item
  \emph{Typical Hardware:} ARM Cortex-M series, ESP32, specialized
  TinyML accelerators.
\item
  \emph{Cold Start:} Microseconds (model weights in flash, tensor arena
  pre-allocated).
\item
  \emph{Power Budget:} Microwatts to milliwatts; battery operation for
  months or years.
\end{itemize}

Table~\ref{tbl-serving-spectrum} summarizes \emph{how} these deployment
contexts shape serving system design:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{Serving Architecture Spectrum}: The deployment context
fundamentally shapes every aspect of serving system design. Cloud
systems optimize for throughput with dynamic batching; mobile systems
optimize for energy with fixed batch-1; TinyML systems operate under
extreme memory and power constraints with no dynamic
allocation.}\label{tbl-serving-spectrum}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristic}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cloud/Datacenter}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mobile/Edge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TinyML}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristic}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cloud/Datacenter}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mobile/Edge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TinyML}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Latency Target} & 10-100ms & 20-50ms & 1-100ms \\
\textbf{Batch Size} & 1-128 (dynamic) & 1 (fixed) & 1 (fixed) \\
\textbf{Memory} & 16-80GB VRAM & 2-8GB shared & 256KB-2MB SRAM \\
\textbf{Power} & 300-700W & 1-10W & 1-100mW \\
\textbf{Update Mechanism} & Container deploy & App store update &
Firmware OTA \\
\textbf{Failure Mode} & Retry/failover & Graceful degradation & Silent
or reset \\
\textbf{Monitoring} & Full telemetry & Limited analytics & Heartbeat
only \\
\end{longtable}

To make these architectural differences concrete, consider \emph{how} a
single model must adapt to each deployment context:

\phantomsection\label{perspective-resnet-serving}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{ResNet-50 Across the Serving Spectrum}
\phantomsection\label{perspective-resnet-serving}
The same ResNet-50 architecture requires dramatically different serving
strategies across deployment contexts:

\textbf{Cloud (V100 GPU):}

\begin{itemize}
\tightlist
\item
  Model format: TensorRT FP16 engine (49MB)
\item
  Inference: 1.4ms at batch-1, 14ms at batch-16
\item
  Throughput: 1,143 images/second (batched)
\item
  Memory: 2GB VRAM (model + activations for batch-32)
\end{itemize}

\textbf{Mobile (Pixel 6 NPU):}

\begin{itemize}
\tightlist
\item
  Model format: TensorFlow Lite INT8 (25MB)
\item
  Inference: 12ms at batch-1 (NPU), 45ms (CPU fallback)
\item
  Throughput: \textasciitilde80 images/second (single-stream)
\item
  Memory: 150MB peak (shared with app)
\item
  Energy: 0.8mJ per inference (NPU), 4.2mJ (CPU)
\end{itemize}

\textbf{TinyML (Cortex-M7):}

\begin{itemize}
\tightlist
\item
  Model format: Not feasible; ResNet-50 requires 98MB weights
\item
  Alternative: MobileNetV2-0.35 quantized to INT8 (1.4MB)
\item
  Inference: 120ms at batch-1
\item
  Throughput: \textasciitilde8 images/second
\item
  Memory: 320KB tensor arena (fits in 512KB SRAM)
\item
  Energy: 12mJ per inference
\end{itemize}

\textbf{Key insight}: The ``same model'' claim is misleading: each
deployment requires not just different optimization but often different
architectures entirely. TinyML serving cannot use ResNet-50; it requires
architectures designed for the constraints from the start.

\end{fbxSimple}

\subsection{The Load Balancer
Layer}\label{sec-model-serving-systems-load-balancer-layer-9c4d}

The preceding spectrum focused on \emph{how} deployment context shapes
serving constraints. For cloud and datacenter deployments, where
multiple replicas serve the same model, an additional infrastructure
layer is required: the load balancer. Production serving systems place
load balancers between clients and model servers, providing three
essential functions for serving infrastructure.

\textbf{Request Distribution} routes incoming requests to available
model replicas using algorithms like round-robin or least-connections.
For latency-sensitive ML serving, algorithms that route away from slow
or overloaded replicas improve tail latency.

\textbf{Health Monitoring} continuously verifies that replicas are ready
to serve, routing traffic away from unhealthy instances. For ML systems,
health checks must verify not just process liveness but model readiness,
confirming that weights are loaded and warmup is complete.

\textbf{Deployment Support} enables safe model updates by gradually
shifting traffic between versions.
\textbf{?@sec-machine-learning-operations-mlops} examines deployment
strategies including canary testing, blue-green deployments, and shadow
mode validation.

For single-machine serving with multiple model instances, such as
running several ONNX Runtime sessions, the framework and operating
system handle request queuing. The full complexity of load balancing
becomes essential when scaling to distributed inference systems, where
multiple machines serve the same model. The implementation details of
request distribution algorithms and multi-replica architectures belong
to that distributed context.

\textbf{Impact on Queuing Analysis}: When capacity planning considers
``the server'' in this chapter, it means the single machine's model
serving capacity. The queuing dynamics analyzed in
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
apply to understanding single-machine behavior and determining when
scaling to multiple machines becomes necessary.

While load balancers distribute requests across replicas, achieving
predictable latency also requires controlling what happens \emph{within}
each machine. The operating system environment introduces its own
sources of variability.

\subsection{Deterministic Latency and Resource
Isolation}\label{sec-model-serving-systems-deterministic-latency-resource-isolation-4d1c}

An inference server does not operate in isolation. On a single machine,
the operating system manages multiple competing processes (logging
agents, monitoring tools, and system interrupts) that can intermittently
steal CPU cycles from the inference pipeline. These ``noisy neighbors''
are a primary source of \textbf{latency jitter}, where the time required
to process identical requests varies significantly, causing the 99th
percentile (P99) latency to spike even when the hardware is
under-utilized.

To achieve deterministic performance on a single node, systems engineers
employ three primary isolation techniques:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{CPU Affinity (Pinning)}: Restricting the inference server's
  threads to specific physical CPU cores. This prevents the operating
  system from context-switching the server's processes, ensuring that
  the ``preprocessing'' stage of the pipeline always has immediate
  access to computational resources.
\item
  \textbf{Memory Locking (\texttt{mlock})}: Instructing the OS to lock
  the model weights and KV caches in physical RAM. This prevents the
  system from ``paging out'' model data to slow disk storage during
  periods of high memory pressure, ensuring consistent microsecond-scale
  access times.
\item
  \textbf{Interrupt Shielding}: Configuring the system to route network
  and storage interrupts to CPU cores not used by the inference runner.
  This ensures that a burst of incoming network traffic does not
  interrupt the GPU's command stream, protecting the ``heartbeat'' of
  the inference execution.
\end{enumerate}

These isolation principles transform a simple ``model script'' into a
\textbf{deterministic service}, a transition essential for
safety-critical applications like autonomous driving or real-time
industrial control.

\section{Serving System
Architecture}\label{sec-model-serving-systems-serving-system-architecture-4879}

The preceding section established the architectural spectrum from cloud
to TinyML and the infrastructure layers that route requests to models.
Building a high-performance serving system requires coordinating
multiple software components to minimize overhead and maximize hardware
utilization. This section examines the internal architecture of
inference servers and the protocols that connect them to clients.

\subsection{Internal Architecture and Request
Flow}\label{sec-model-serving-systems-anatomy-inference-server-f12e}

While model optimization focuses on the mathematical artifact, model
serving requires a specialized software architecture to manage
high-frequency request streams and hardware utilization. An inference
server\sidenote{\textbf{Inference Server}: The concept emerged from
Google's TensorFlow Serving (\citeproc{ref-olston2017tensorflow}{Olston
et al. 2017}), open-sourced February 2016, which pioneered the
separation of model logic from serving infrastructure. NVIDIA's Triton
(\citeproc{ref-nvidia2024triton}{Savard et al. 2024}), originally
TensorRT Inference Server with GA release in March 2019, extended this
to multi-framework support. These servers implement dynamic batching
that can improve GPU utilization by up to 70\% compared to naive
single-request serving. The architecture mirrors the separation of
concerns in traditional web servers like Apache (1995) and nginx (2004),
applying decades of distributed systems knowledge to ML deployment. }
(such as NVIDIA Triton, TensorFlow Serving, or TorchServe) is not a
simple wrapper around a model script; it is a high-performance scheduler
that manages concurrency, memory, and data movement.

The internal anatomy of these servers reveals \emph{how} they bridge the
gap between irregular user traffic and the highly regular,
batch-oriented requirements of accelerators.

\textbf{The Request Pipeline.} Every request traverses a multi-stage
pipeline designed to maximize hardware throughput while minimizing
latency overhead. Figure~\ref{fig-server-anatomy} visualizes this
internal flow.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b9aeb7b6f335f42ea57b926c64abe5f37aef26f0.pdf}}

}

\caption{\label{fig-server-anatomy}\textbf{Inference Server Anatomy}: A
modern inference server decouples network handling from accelerator
execution through a staged pipeline. Each stage isolates a concern, from
absorbing bursty traffic to forming efficient batches, so the hardware
accelerator stays highly utilized despite irregular arrival patterns.}

\end{figure}%

The server architecture serves three critical functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Concurrency Management}: Servers use asynchronous event loops
  or thread pools to handle thousands of concurrent client connections
  without blocking. This ensures that network I/O wait times do not idle
  the accelerator.
\item
  \textbf{Request Transformation}: The server handles the conversion of
  network payloads (JSON/Protobuf) into the specific tensor formats
  required by the optimized model runtime. Image tensors, for example,
  can be stored as NCHW (batch, channels, height, width) or NHWC (batch,
  height, width, channels). PyTorch and TensorRT prefer NCHW because it
  places channel data contiguously, enabling efficient convolution on
  GPUs. TensorFlow defaults to NHWC, which is more efficient on CPUs. A
  format mismatch between client and server silently corrupts inference:
  the model interprets pixel rows as color channels, producing garbage
  outputs without raising errors.
\item
  \textbf{Model Management}: Servers manage the lifecycle of models,
  including loading weights into VRAM, managing versioning, and ensuring
  that ``warm-up'' inferences are completed before exposing the model to
  live traffic.
\end{enumerate}

Of these components, the scheduler deserves special attention because it
embodies the fundamental serving tradeoff between throughput and
latency.

\subsection{The Scheduler: Where Throughput Meets
Latency}\label{sec-model-serving-systems-scheduler-throughput-meets-latency-d022}

The \textbf{Scheduler} is the ``brain'' of the inference server. It
implements the dynamic batching logic discussed in
Section~\ref{sec-model-serving-systems-throughput-optimization-18d1}.
The scheduler must decide: ``\emph{Should} I run this one request now to
minimize its latency, or wait 5 milliseconds for a second request to
arrive and process them together to maximize throughput?''

Systems designers use the \textbf{Batching Window} parameter to tune
this trade-off. A window of 0ms optimizes for pure latency (no
batching), while a window of 10-50ms is common for high-throughput cloud
services. This decision determines the ``duty cycle'' of the GPU, the
percentage of time the hardware is actually computing versus waiting for
work.

\subsection{Interface Protocols and
Serialization}\label{sec-model-serving-systems-interface-protocols-serialization-5510}

The mechanism used to transport data between client and server directly
affects the latency budget. While model inference is often highly
optimized, the cost of moving data into the model (serialization and
network protocol overhead) can become the dominant bottleneck,
especially for lightweight models where inference time is small.

\textbf{The Serialization Bottleneck.} Text-based formats like JSON are
ubiquitous but computationally expensive. Parsing a JSON object requires
reading every byte, validating syntax, and converting text
representations into machine-native types. For high-throughput systems,
this consumes CPU cycles that could otherwise be used for request
handling or preprocessing.

Binary formats like Protocol Buffers (Protobuf) or FlatBuffers reduce
this overhead by designing the wire format to map directly to in-memory
data structures. This enables ``zero-copy'' deserialization in optimal
cases, where the network buffer can be used directly without allocating
new memory.

\subsubsection{REST vs
gRPC}\label{sec-model-serving-systems-rest-vs-grpc-c7b7}

Two dominant paradigms define modern serving interfaces, each with
distinct system characteristics:

\textbf{REST (Representational State Transfer)} typically uses HTTP/1.1
and JSON. It is universally supported, human-readable, and stateless,
making it the default choice for public-facing APIs. However, standard
HTTP/1.1 requires a new TCP handshake for each request (unless
keep-alive is carefully tuned), and JSON serialization adds significant
latency for numerical data like tensors.

\textbf{gRPC (gRPC Remote Procedure Call)}\sidenote{\textbf{gRPC}:
Open-sourced by Google in February 2015, gRPC evolved from Stubby,
Google's internal RPC framework that had been handling tens of billions
of calls per second across their datacenters since approximately 2001.
The combination of HTTP/2 multiplexing and Protocol Buffers binary
serialization achieves roughly 10x lower serialization overhead than
REST/JSON, making it the de facto standard for latency-sensitive ML
inference APIs. } uses HTTP/2 and Protobuf. HTTP/2 enables multiplexing
multiple requests over a single persistent TCP connection, eliminating
handshake latency and allowing efficient binary streaming. Protobuf
provides strict type safety and efficient binary serialization, making
it the standard for internal service-to-service communication where
latency is critical.

The following example compares \emph{JSON vs Protobuf serialization}.

\phantomsection\label{callout-notebookux2a-1.6}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{JSON vs Protobuf Serialization}
\phantomsection\label{callout-notebook*-1.6}
Consider a request payload containing 1,000 floating point numbers
(e.g., an embedding vector).

\begin{itemize}
\tightlist
\item
  \textbf{JSON}: Uses \textasciitilde9 KB on the wire. Requires
  \textasciitilde50\(\mu\)s to parse.
\item
  \textbf{Protobuf}: Uses \textasciitilde4 KB on the wire. Requires
  \textasciitilde5\(\mu\)s to parse.
\end{itemize}

For a system processing 10,000 requests per second, switching to
Protobuf saves nearly half a core of CPU time just in serialization
overhead. This 10\(\times\) efficiency gain makes gRPC essential for
high-throughput internal microservices.

\end{fbxSimple}

\textbf{System Choice}: Use REST for public APIs to maximize developer
accessibility. Use gRPC for high-performance internal communication to
minimize the serialization tax.

The architectural components and protocols examined so far describe
\emph{how} serving systems are built. Understanding \emph{why} certain
configurations perform better requires analyzing what happens to
individual requests as they traverse these components.

\section{The Request
Lifecycle}\label{sec-model-serving-systems-request-lifecycle-d9c6}

With the serving architecture established, we now trace \emph{what}
happens to a single request as it flows through the system.
Understanding \emph{where} time goes within each request is essential
for effective optimization: you cannot improve what you do not measure.

\subsection{The Latency
Budget}\label{sec-model-serving-systems-latency-budget-ef40}

For dynamic inference systems, the fundamental difference from training
lies in optimization objectives. Training optimizes throughput:
maximizing samples processed per hour over days of computation. A
training job that processes 1000 samples per second is successful
regardless of how long any individual sample takes. Serving inverts this
priority, optimizing latency (introduced in
\textbf{?@sec-ml-system-architecture}) per request under strict time
constraints. A serving system with 1000ms per-request latency has
failed, even if it achieves excellent throughput.

This shift has concrete implications for system design
(\citeproc{ref-gujarati2020serving}{Gujarati et al. 2020}). The metrics
that matter change from aggregate throughput to latency distributions.
Mean latency tells you little about user experience; p50, p95, and p99
latencies reveal \emph{how} the system performs across the full range of
requests. If your mean latency is 50ms but p99 is 2 seconds, one in a
hundred users waits 40 times longer than average. For consumer-facing
applications, these tail latencies often determine user satisfaction and
retention.\sidenote{\textbf{Tail Latency Impact}: Research at Google and
Amazon in the mid-2000s established that users are more sensitive to
latency variance than mean latency. Industry experience suggests that
latency increases of 100ms can measurably impact user engagement and
conversion rates for e-commerce applications, though the magnitude
varies by context. This is why service level objectives (SLOs) typically
specify percentile targets rather than averages. }

Managing these percentile constraints requires decomposing the total
allowed response time into a \emph{latency budget} that allocates time
across each processing phase.

\phantomsection\label{callout-definitionux2a-1.7}
\begin{fbxSimple}{callout-definition}{Definition:}{Latency Budget}
\phantomsection\label{callout-definition*-1.7}
\textbf{\emph{Latency Budget}} is the \textbf{Time Capital} allocated to
a request, strictly bounded by the \textbf{End-to-End SLA}. It acts as a
zero-sum constraint system where any milliseconds consumed by
\textbf{Serialization} or \textbf{Network Overhead} directly reduce the
computational budget available for Model Inference.

\end{fbxSimple}

We can visualize this by examining a \emph{ResNet-50 latency budget
breakdown}.

\phantomsection\label{callout-notebookux2a-1.8}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50: Latency Budget Breakdown}
\phantomsection\label{callout-notebook*-1.8}

Serving is about optimizing the \textbf{Tail Latency} under load.

\textbf{The Physics of Latency}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Queuing Theory}: Can you explain why latency spikes
  non-linearly as utilization approaches 100\%? (Hint: The M/M/1 queue
  model).
\item[$\square$]
  \textbf{Batching Trade-offs}: Why does increasing batch size improve
  throughput (images/sec) but degrade latency (ms/request)?
\end{itemize}

\textbf{Optimization Targets}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{The Bottleneck}: In a highly optimized inference server, why
  does \textbf{Preprocessing} often consume more time than the model
  itself?
\end{itemize}

\end{fbxSimple}

Every serving request decomposes into three phases that each consume
part of the latency budget. Preprocessing transforms raw input such as
image bytes or text strings into model-ready tensors. Inference executes
the model computation. Postprocessing transforms model outputs into
user-facing responses.

A common misconception is that faster hardware automatically means
faster serving. In practice, preprocessing and postprocessing often
dominate total latency. Studies of production systems show preprocessing
consuming 60 to 70 percent of total request time when inference runs on
optimized accelerators (\citeproc{ref-nvidia2024tritontutorial}{NVIDIA
2025}). Optimizing only the inference phase yields diminishing returns
when the surrounding pipeline remains bottlenecked on CPU operations.

\subsection{Latency Distribution
Analysis}\label{sec-model-serving-systems-latency-distribution-analysis-b0f8}

Understanding \emph{where} time goes requires instrumenting each phase
independently. Consider \emph{what} happens when our ResNet-50
classifier receives a JPEG image:

\phantomsection\label{callout-notebookux2a-1.9}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50: Latency Budget Breakdown}
\phantomsection\label{callout-notebook*-1.9}
A typical serving request for our ResNet-50 classifier shows the
following latency distribution:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1895}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2737}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2737}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2632}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Percentage}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Preprocessing} & JPEG decode & 3.0ms & 30\% \\
\textbf{Preprocessing} & Resize to 224×224 & 1.0ms & 10\% \\
\textbf{Preprocessing} & Normalize (mean/std) & 0.5ms & 5\% \\
\textbf{Data Transfer} & CPU→GPU copy & 0.5ms & 5\% \\
\textbf{Inference} & \textbf{ResNet-50 forward pass} & \textbf{5.0ms} &
\textbf{50\%} \\
\textbf{Postprocessing} & Softmax + top-5 & 0.1ms &
\textasciitilde1\% \\
\textbf{Total} & & \textbf{10.1ms} & \textbf{100\%} \\
\end{longtable}

Key insight: \textbf{Preprocessing consumes 45\% of latency} despite
model inference being the computationally intensive phase. With TensorRT
optimization reducing inference to 2ms, preprocessing would dominate at
63\%.

\end{fbxSimple}

The ResNet example represents compute-bound inference where math
dominates. Recommendation systems reveal a different bottleneck profile
entirely.

\phantomsection\label{callout-lighthouseux2a-1.10}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Lighthouse Example: DLRM Serving}
\phantomsection\label{callout-lighthouse*-1.10}
\textbf{The Scenario}: Serving a Recommendation System (DLRM) with a
10ms P99 latency budget.

\textbf{The Contrast}: While ResNet-50 serving is limited by
\textbf{Math} (CNN ops), DLRM serving is strictly limited by
\textbf{I/O} and \textbf{Memory Capacity}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2022}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3146}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3258}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1573}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input Parsing} & Request parsing & 0.5ms & CPU \\
\textbf{Embedding Look} & \textbf{Fetch 100+ dense vectors} &
\textbf{6.0ms} & \textbf{Memory BW} \\
\textbf{Inference} & MLP forward pass & 1.5ms & Compute \\
\textbf{Postprocessing} & Ranking \& Filtering & 1.0ms & CPU \\
\textbf{Total} & & \textbf{9.0ms} & \\
\end{longtable}

\textbf{Key Systems Insight}: In DLRM, the ``Inference'' (MLP) is only
\textasciitilde15\% of the latency. The majority of time is spent in
\textbf{Embedding Lookups}---retrieving massive 128-dim vectors from
terabyte-scale tables. This is an \textbf{IO-bound} workload where
adding more GPUs doesn't help unless you also add more \textbf{Memory
Bandwidth} and \textbf{Capacity}.

\end{fbxSimple}

This breakdown reveals why straightforward optimization efforts often
fail. Engineers focus on model optimization (quantization, pruning)
because that is where ML expertise applies, but the actual bottleneck is
image decoding running on CPU. Adopting \emph{the quantitative approach
to serving} exposes these hidden bottlenecks before engineering effort
is misallocated.

\phantomsection\label{callout-notebookux2a-1.11}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Quantitative Approach to Serving}
\phantomsection\label{callout-notebook*-1.11}
\textbf{Amdahl's Law at Work}: Preprocessing (4.5ms) and data transfer
(0.5ms) consume 50\% of total latency. Optimizing the model 10× faster
(5ms → 0.5ms) yields only 1.8× end-to-end speedup (from 10.1ms to
5.6ms). This is why focusing exclusively on model optimization
(quantization, pruning) often disappoints: the bottleneck is elsewhere.

\textbf{DSA Efficiency}: General-purpose CPUs achieve only 1-2\% of peak
performance at batch-1 because instruction overhead dominates. DSAs like
TPUs and Tensor Cores replace complex logic with dense MAC arrays,
achieving 10-100× higher arithmetic intensity. This makes hardware
acceleration a requirement for economically viable serving.

\textbf{Engineering Implication}: Profile before optimizing. If
preprocessing dominates, GPU-accelerated pipelines (NVIDIA DALI) may
outperform model quantization.

\end{fbxSimple}

Moving preprocessing to GPU can reduce total latency by 6x in some
pipelines by eliminating CPU-GPU data transfers between stages
(\citeproc{ref-nvidia2024tritontutorial}{NVIDIA 2025}). Effective
optimization targets the largest time consumers first.

\textbf{The Serving Tax Bill}

Beyond the model execution itself, every request pays a ``tax'' to the
serving infrastructure. Table~\ref{tbl-serving-tax} quantifies these
overheads for a typical high-performance inference request (e.g.,
ResNet-50 classification).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2024}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1905}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3690}}@{}}
\caption{\textbf{The Serving Tax Bill}: A breakdown of non-inference
latency sources. While individual components like serialization seem
small (\(<1\) ms), they compound. In a 5ms inference service, this
``tax'' can easily consume 50\% of the latency budget. The primary
engineering goal is to drive these costs to zero through architectural
choices like gRPC and Zero-Copy data
paths.}\label{tbl-serving-tax}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tax Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scaling Behavior}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tax Evasion Strategy}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tax Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scaling Behavior}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tax Evasion Strategy}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Network I/O} & 1-5 ms & Linear with payload & Compression,
Region Colocation \\
\textbf{Serialization} & 50-500 \(\mu\)s & Linear with payload &
gRPC/Protobuf (vs JSON) \\
\textbf{Queuing} & 0.1-10 ms & Exponential w/ load & Dynamic Batching,
Autoscaling \\
\textbf{Dispatch} & 10-50 \(\mu\)s & Constant per batch & Kernel Fusion
(reduce launches) \\
\textbf{Data Copy} & 100-500 \(\mu\)s & Linear with tensor & Zero-Copy /
Shared Memory \\
\end{longtable}

\subsubsection{The Killer Microseconds
Problem}\label{sec-model-serving-systems-killer-microseconds-problem-bc00}

Barroso, Patterson, and colleagues identified a critical gap in
\emph{how} systems handle latency at different time scales
(\citeproc{ref-barroso2017attack}{Barroso et al. 2017}). Operations in
the microsecond range are too short for traditional OS scheduling (which
operates at millisecond granularity) yet too long to simply spin-wait
without wasting CPU cycles. This ``killer microseconds'' regime
dominates modern serving workloads, where individual operations complete
quickly but aggregate into significant overhead. The latency budget
framework provides a systematic approach to optimization. First, measure
each phase to identify the true bottleneck. Then allocate engineering
effort proportionally to \emph{where} time is actually spent. Finally,
consider architectural changes such as GPU preprocessing or batching
strategies that can shift work between phases.

\subsection{Resolution and Input Size
Tradeoffs}\label{sec-model-serving-systems-resolution-input-size-tradeoffs-155d}

Input resolution affects both preprocessing and inference latency, but
the relationship differs depending on whether the system is
compute-bound (limited by arithmetic throughput) or memory-bound
(limited by data movement). A compute-bound system slows proportionally
to increased computation; a memory-bound system may show minimal
slowdown if activation tensors still fit in fast memory.
\textbf{?@sec-ai-acceleration} covers this distinction in depth through
roofline model analysis; understanding it is essential for making
informed resolution decisions.

For compute-bound models, Equation~\ref{eq-resolution-throughput}
formalizes how throughput scales inversely with resolution squared:

\begin{equation}\phantomsection\label{eq-resolution-throughput}{\frac{T(r_2)}{T(r_1)} = \left(\frac{r_1}{r_2}\right)^2}\end{equation}

Doubling resolution from 224 to 448 theoretically yields 4x slowdown
(measured: 3.6x due to fixed overhead amortization). However, at high
resolutions, models transition from compute-bound to memory-bound as
activation tensors exceed cache capacity.
Table~\ref{tbl-resolution-bottleneck} quantifies this transition for
ResNet-50, showing how arithmetic intensity decreases with resolution:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2090}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2836}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2985}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2090}}@{}}
\caption{\textbf{Resolution and Compute Bottleneck}: ResNet-50
arithmetic intensity decreases with resolution as activation sizes grow.
For a V100 PCIe (15.7 TFLOPS FP32, 900 GB/s bandwidth), the ridge point
is approximately 16 FLOPS/byte. At 224x224, compute dominates; by
512x512, memory bandwidth becomes the limiting
factor.}\label{tbl-resolution-bottleneck}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Resolution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Activation Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arith. Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Resolution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Activation Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arith. Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{224×224} & 12.5MB & 85 FLOPS/byte & Compute \\
\textbf{384×384} & 36.8MB & 49 FLOPS/byte & Transitional \\
\textbf{512×512} & 65.5MB & 28 FLOPS/byte & Memory BW \\
\textbf{640×640} & 102.4MB & 18 FLOPS/byte & Memory BW \\
\end{longtable}

\subsubsection{Resolution Strategies in
Production}\label{sec-model-serving-systems-deploymentspecific-resolution-decisions-1d76}

Different deployment contexts have distinct resolution requirements.
Mobile applications often accept lower resolution such as 224×224 for
object detection in camera viewfinders, where latency and battery life
dominate. Medical imaging requires high resolution of 512×512 or higher
for diagnostic accuracy, with relaxed latency requirements. Autonomous
vehicles use multiple resolutions for different tasks, with low
resolution for detection and high resolution crops for recognition.
Cloud APIs typically receive resolution set by client upload and must
handle a range gracefully. This variability makes cloud APIs ideal
candidates for adaptive resolution strategies, where the system selects
resolution dynamically based on content characteristics.

\textbf{Adaptive Resolution.} Production systems can select resolution
dynamically based on content. One approach runs a lightweight classifier
at 128×128 to categorize content type, then selects task-appropriate
resolution with documents at 512×512, landscapes at 224×224, and faces
at 384×384. This achieves 1.4× throughput improvement with 99.2 percent
accuracy retention versus fixed high resolution. This pattern trades
preprocessing cost from running the lightweight classifier for inference
savings on the main model.

The latency analysis so far has focused on sequential processing: one
request completing before the next begins. The preprocessing, inference,
and postprocessing stages use different hardware resources. This
separation creates an opportunity to process multiple requests
simultaneously.

\subsection{Hardware Utilization and Request
Pipelining}\label{sec-model-serving-systems-utilization-request-pipelining-c61c}

The preceding analysis examined where time goes within individual
pipeline stages. Optimizing each stage in isolation, however, misses a
critical opportunity: the stages use different hardware resources. The
latency budget analysis in
Section~\ref{sec-model-serving-systems-latency-budget-ef40} reveals that
model inference is only one component of the request lifecycle. From a
hardware perspective, the primary goal of a serving system is to
maximize the \textbf{duty cycle} of the accelerator, the percentage of
time the GPU is performing useful computation.

In a serialized serving system, the hardware sits idle during network
I/O and CPU-based preprocessing. High-performance serving systems use
\textbf{Request Pipelining} to overlap these stages, ensuring the GPU is
fed a continuous stream of tensors.

\textbf{Overlapping I/O and Compute.}
Figure~\ref{fig-serving-pipeline-timing} contrasts serial execution with
pipelined execution. In the serial case (A), each request must complete
its entire lifecycle (Network \(\rightarrow\) CPU Preprocessing
\(\rightarrow\) GPU Inference \(\rightarrow\) Postprocessing) before the
next request begins. Even with a fast GPU, the system throughput is
limited by the slowest stage, and the GPU remains idle for more than
50\% of the time.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/351af79dcce52b65b4cb0ec037738fe50828861b.pdf}}

}

\caption{\label{fig-serving-pipeline-timing}\textbf{Request Pipelining}:
Pipelining hides latency by overlapping independent operations across
different hardware resources. In pipelined execution (B), the CPU
processes the next request's data while the GPU executes the current
request's inference. This increases the GPU duty cycle toward 100\%,
effectively doubling or tripling throughput on the same hardware without
changing the model.}

\end{figure}%

Pipelining is enabled by \textbf{Asynchronous I/O} and
\textbf{Concurrency Models}. Instead of waiting for a GPU kernel to
finish, the server's CPU thread submits the work to the GPU's command
queue and immediately begins preprocessing the next incoming request.

\textbf{The Systems Metric: Hardware Duty Cycle.} In the ``Quantitative
Approach'' to ML systems, we define the efficiency of a serving system
by its ability to saturate the bottleneck resource. For most ML systems,
this is the GPU's compute cores or memory bandwidth.

\[\text{System Efficiency} = \frac{\sum T_{\text{compute}}}{\text{Wall Clock Time} \times \text{Resource Count}}\]

If a ResNet-50 request takes 10ms total (5ms GPU, 5ms CPU), a serial
system achieves only 50\% efficiency. By pipelining just two requests,
efficiency approaches 100\% (assuming the CPU can keep up with the GPU).
If the CPU is too slow to feed the GPU, the system becomes
\textbf{CPU-bound}, and further model optimization provides zero
throughput gain, a direct application of Amdahl's Law (introduced in
\textbf{?@sec-ml-system-architecture}) to serving: if preprocessing
consumes 50\% of latency, maximum speedup is 2x regardless of how fast
the model runs.

\subsection{Postprocessing}\label{sec-model-serving-systems-postprocessing-3b24}

Preprocessing and inference produce raw tensors, but these
floating-point arrays carry no inherent meaning to applications or
users. The final phase of the request lifecycle, postprocessing,
transforms these tensors into actionable predictions: a 0.95 probability
becomes a confident ``dog'' label, a sequence of token IDs becomes
readable text, or a bounding box tensor becomes a highlighted region in
an image. While often overlooked in system design, postprocessing
significantly impacts both latency and the usefulness of predictions.

\subsubsection{From Logits to
Predictions}\label{sec-model-serving-systems-logits-predictions-09df}

Classification models output logits or probabilities across classes.
Converting these to predictions involves several potential steps
including argmax selection that chooses the highest-probability class,
thresholding that applies confidence thresholds before returning
predictions, top-k extraction that returns multiple high-probability
classes with scores, and calibration that adjusts raw probabilities to
better reflect true likelihoods.

For ResNet-50 image classification, typical postprocessing includes
transforming logits to probabilities, extracting top predictions, and
formatting responses. Listing~\ref{lst-resnet-postprocessing} shows a
complete postprocessing pipeline with timing annotations. Total
postprocessing time is approximately 0.1ms, negligible compared to
preprocessing and inference.

\begin{codelisting}

\caption{\label{lst-resnet-postprocessing}\textbf{ResNet-50
Postprocessing}: Transforms raw logits to calibrated probabilities,
extracts top-k predictions, and formats the API response.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Transform raw logits to calibrated probabilities}
\CommentTok{\# Input: logits tensor of shape (batch\_size, 1000) {-} one score per}
\CommentTok{\# ImageNet class}
\NormalTok{probs }\OperatorTok{=}\NormalTok{ torch.softmax(}
\NormalTok{    logits, dim}\OperatorTok{={-}}\DecValTok{1}
\NormalTok{)  }\CommentTok{\# Normalize to sum=1; \textasciitilde{}0.05ms on GPU}

\CommentTok{\# Extract top{-}5 predictions for multi{-}class response}
\CommentTok{\# topk returns (values, indices) sorted by probability}
\NormalTok{top5\_probs, top5\_indices }\OperatorTok{=}\NormalTok{ probs.topk(}\DecValTok{5}\NormalTok{)  }\CommentTok{\# \textasciitilde{}0.02ms; GPU operation}

\CommentTok{\# Map class indices to human{-}readable labels}
\CommentTok{\# IMAGENET\_CLASSES: list of 1000 class names from synset mapping}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    IMAGENET\_CLASSES[i] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ top5\_indices}
\NormalTok{]  }\CommentTok{\# \textasciitilde{}0.01ms; CPU lookup}

\CommentTok{\# Format response with predictions and metadata for API contract}
\NormalTok{response }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"predictions"}\NormalTok{: [}
\NormalTok{        \{}\StringTok{"label"}\NormalTok{: label, }\StringTok{"confidence"}\NormalTok{: }\BuiltInTok{float}\NormalTok{(prob)\}}
        \ControlFlowTok{for}\NormalTok{ label, prob }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(labels, top5\_probs)}
\NormalTok{    ],}
    \StringTok{"model\_version"}\NormalTok{: }\StringTok{"resnet50{-}v2.1"}\NormalTok{,  }\CommentTok{\# Client{-}side version tracking}
    \StringTok{"inference\_time\_ms"}\NormalTok{: }\FloatTok{5.2}\NormalTok{,  }\CommentTok{\# Observability for latency monitoring}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each step adds latency but improves response utility. Calibration in
particular can add significant computation but is essential when
downstream systems make decisions based on confidence scores.

\textbf{Output Formatting.} Production systems rarely return raw
predictions. Outputs must conform to API contracts, often requiring JSON
serialization with specific schema, confidence score formatting and
thresholding, error handling for edge cases such as no confident
prediction or out-of-distribution input, and metadata attachment
including model version, inference time, and feature attributions.

The latency budget analysis reveals \emph{where} time goes within a
single request. Production systems, however, do not process requests in
isolation: they must handle hundreds or thousands of concurrent requests
competing for finite resources. Understanding this concurrency requires
a different analytical framework.

\section{Queuing Theory and Tail
Latency}\label{sec-model-serving-systems-queuing-theory-tail-latency-29a6}

The request lifecycle analysis explains \emph{where} time goes within a
single request, but production systems must handle many concurrent
requests competing for finite resources. Explaining \emph{why} latency
degrades under load requires queuing theory, the mathematical framework
that explains \emph{how} requests wait for service in any system with
finite capacity. These principles apply to web servers and ML inference
alike, and explain the counterintuitive behavior that causes
well-provisioned systems to violate latency SLOs when load increases
modestly.

\subsection{Queuing
Fundamentals}\label{sec-model-serving-systems-queuing-fundamentals-10d3}

Serving engineers routinely face a concrete question: given a latency
SLO and an expected request rate, \emph{how} many GPUs must be
provisioned? Answering this question requires predicting \emph{how}
latency changes as load increases, which is precisely what queuing
theory provides. Two mathematical foundations govern serving system
behavior: Little's Law, which relates queue depth to throughput, and the
M/M/1 model, which predicts how latency degrades under load. Together,
they provide the quantitative framework for capacity planning.

\subsection{Little's
Law}\label{sec-model-serving-systems-littles-law-9352}

The most fundamental result in queuing theory is Little's
Law,\sidenote{\textbf{Little's Law}: Proven by John D.C. Little in 1961
(\citeproc{ref-little1961proof}{Little 1961}), this theorem establishes
that \(L = \lambda W\) holds for any stable queuing system regardless of
arrival patterns, service distributions, or scheduling policies. The
remarkable generality makes it one of the most useful results in
operations research. For serving systems, it enables capacity planning
from observable metrics: measuring queue depth and arrival rate directly
yields average latency without instrumenting individual requests. }
\sidenote{\textbf{Little's Law in the Coffee Shop}: Throughput
(\(\lambda\)) is the rate of arriving customers; Latency (\(W\)) is the
time to make one drink; Queue (\(L\)) is the number of people waiting.
If the barista takes 1 minute per drink (\(W=1\)) and customers arrive
every 30 seconds (\(\lambda=2\)), the queue (\(L\)) will grow
indefinitely unless more baristas are added. } which
Equation~\ref{eq-littles-law} expresses as a simple relationship between
three quantities in any stable system:

\begin{equation}\phantomsection\label{eq-littles-law}{L = \lambda \cdot W}\end{equation}

where \(L\) is the average number of requests in the system, \(\lambda\)
is the arrival rate (requests per second), and \(W\) is the average time
each request spends in the system. This relationship holds regardless of
arrival distribution, service time distribution, or scheduling policy.
The following notebook quantifies this capacity relationship using a
practical example of Little's Law.

\phantomsection\label{notebook-littles-law}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Little's Law}
\phantomsection\label{notebook-littles-law}

\textbf{The Capacity Physics}: How much memory do you need to serve
1,000 queries per second?

\textbf{The Law}: \(L = \lambda W\) (Concurrency = Throughput \(\times\)
Latency) (see \textbf{?@sec-system-foundations-littles-law-9c4c} for the
derivation).

\textbf{Scenario}:

\begin{itemize}
\tightlist
\item
  \textbf{Throughput Target (\(\lambda\))}: 1,000 requests/sec.
\item
  \textbf{Latency Target (\(W\))}: 50 ms (0.05 s).
\end{itemize}

\textbf{The Calculation}:
\[ L = 1\,000 \times 0\.05 = \mathbf{50 \text{ concurrent requests}} \]

\textbf{The Constraint}: Your server \emph{must} have enough RAM to hold
50 requests simultaneously (batch size + queue).

\begin{itemize}
\tightlist
\item
  If your GPU runs out of memory at Batch Size 32, you physically
  \textbf{cannot} hit 1,000 QPS at 50ms latency.
\item
  You must either reduce latency (\(W\)) or buy more memory (\(L\)).
\end{itemize}

\end{fbxSimple}

Little's Law has immediate practical implications. If your inference
service averages 10ms per request (\(W = 0.01\)s) and you observe 50
concurrent requests in the system on average (\(L = 50\)), then your
arrival rate must be \(\lambda = L/W = 5000\) requests per second.
Conversely, if you need to limit concurrent requests to 10 (perhaps due
to GPU memory constraints), and your service time is 10ms, you can
sustain at most 1000 requests per second.

\subsection{The Utilization-Latency
Relationship}\label{sec-model-serving-systems-utilizationlatency-relationship-a2f0}

Little's Law tells us what the system looks like on average, but it does
not reveal \emph{how} latency changes as load approaches capacity. To
answer the critical question of \emph{how} much spare capacity a serving
system needs, we turn to the M/M/1 queue model. For a system with
Poisson arrivals and exponential service times, the average time in
system follows:

\begin{equation}\phantomsection\label{eq-mm1-wait}{W = \frac{1}{\mu - \lambda} = \frac{\text{service time}}{1 - \rho}}\end{equation}

where \(\mu\) is the service rate (requests per second the server can
handle), and \(\rho = \lambda/\mu\) is the utilization (fraction of time
the server is busy).

This equation reveals why serving systems exhibit nonlinear behavior. At
50\% utilization, average \textbf{time in system} is \(2\times\) the
service time. At 80\% utilization, it is \(5\times\). At 90\%
utilization, it is \(10\times\). Small increases in load near capacity
cause disproportionate latency increases.

The M/M/1 model assumes exponentially distributed service times, but ML
inference typically has near-constant service time for fixed batch
sizes, making the M/D/1 (deterministic service) model more accurate in
practice. We use M/M/1 here because it yields closed-form solutions and
produces conservative estimates. Table~\ref{tbl-utilization-latency}
reveals how average \textbf{latency} grows rapidly as utilization
approaches 100\%. For M/D/1 queues, average wait time is approximately
half of M/M/1 at the same utilization, which matters for capacity
planning: M/M/1 analysis will slightly over-provision, erring on the
side of meeting SLOs rather than violating
them.\sidenote{\textbf{Kendall Notation}: The M/M/1 notation was
introduced by British statistician David Kendall in 1953 and follows the
pattern A/S/c (Arrivals/Service/servers). ``M'' stands for ``Markovian''
(memoryless, meaning exponential distributions), honoring Russian
mathematician Andrey Markov (1856-1922). ``D'' means deterministic. So
M/M/1 describes a single server with exponential arrivals and service
times, while M/D/1 has deterministic service. ML inference is closer to
M/D/1 since inference time is nearly constant, but M/M/1 yields
conservative estimates suitable for capacity planning. }

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3478}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2899}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3623}}@{}}
\caption{\textbf{Utilization-Latency Relationship}: Average
\textbf{latency} as a multiple of service time for an M/M/1 queue. At
50\% utilization, latency is 2x service time; at 90\%, it reaches 10x.
This nonlinear growth explains why systems that perform well at moderate
load suddenly violate SLOs when traffic increases: moving from 80\% to
90\% utilization doubles
latency.}\label{tbl-utilization-latency}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Utilization (\(\rho\))}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency Multiple}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Example (5ms service)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Utilization (\(\rho\))}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency Multiple}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Example (5ms service)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
50\% & 2.0× & 10ms \\
70\% & 3.3× & 17ms \\
80\% & 5.0× & 25ms \\
90\% & 10.0× & 50ms \\
95\% & 20.0× & 100ms \\
\end{longtable}

\subsection{Multi-Server
Considerations}\label{sec-model-serving-systems-multiserver-considerations-00fc}

The preceding analysis focuses on a single ML node (one machine serving
inference requests). This scope aligns with Volume I's focus on
mastering the fundamental unit of ML systems. Single-node queuing
dynamics are prerequisite to effective scaling: you cannot optimize a
distributed system without first understanding the behavior of its
components.

\textbf{When Single-Node Analysis Applies}: M/M/1 analysis remains the
foundation for:

\begin{itemize}
\tightlist
\item
  \textbf{Right-sizing individual nodes}: Determining whether a single
  GPU can meet latency SLOs at expected traffic
\item
  \textbf{Identifying the scaling trigger}: Calculating when traffic
  exceeds single-node capacity
\item
  \textbf{Cost-effective provisioning}: Avoiding premature scale-out
  that wastes resources
\end{itemize}

For traffic exceeding single-node capacity, production systems deploy
multiple replicas behind a load balancer. The M/M/c queuing model
extends M/M/1 to c parallel servers, showing that multiple replicas
dramatically improve tail latency: the probability of all servers being
simultaneously slow drops exponentially with server count. At c=4
replicas, p99 latency can be 3× lower than the single-server case at the
same total throughput.

\textbf{Scope Boundary}: This chapter establishes single-node serving
foundations. Distributed inference systems (model sharding across GPUs,
tensor parallelism, pipeline parallelism) introduce coordination
overhead and consistency challenges that require advanced scaling
principles.

\subsection{Tail
Latency}\label{sec-model-serving-systems-tail-latency-5376}

Production SLOs typically specify percentile targets (p95, p99) rather
than averages because tail latency determines user experience for the
slowest requests (\citeproc{ref-dean2013tail}{Dean and Barroso 2013}).
For an M/M/1 queue, the p99 latency follows:

\begin{equation}\phantomsection\label{eq-p99-latency}{W_{p99} \approx \text{service time} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right)}\end{equation}

At 70 percent utilization, p99 latency is approximately fifteen times
the service time, while average latency is only 3.3 times. This explains
\emph{why} systems that seem healthy with low average latency can have
unacceptable tail latency, since the average hides the experience of the
unluckiest requests.

\subsubsection{The Tail at Scale
Problem}\label{sec-model-serving-systems-tail-scale-problem-958d}

Dean and Barroso's analysis reveals \emph{why} tail latency becomes
critical as systems scale beyond single machines
(\citeproc{ref-dean2013tail}{Dean and Barroso 2013}). The key insight is
that when requests fan out to multiple servers, the probability of
experiencing at least one slow response grows rapidly with server count.
This ``tail at scale'' effect makes individual server tail latency
critical for overall system performance.

For single-machine serving, this principle has two implications. First,
tail latency on individual machines matters because it will compound
when systems eventually scale. Second, the tail-tolerant techniques
described below (hedging, graceful degradation) provide value even on
single machines and become essential at scale.

\textbf{Tail-Tolerant Techniques}: Request hedging sends redundant
requests after a timeout, accepting whichever response arrives first.
Backup requests and load balancing away from slow servers directly
address latency variance. These techniques apply to single-machine
serving with multiple GPU streams or model replicas, and become
essential when scaling to distributed inference systems.

With the queuing model and tail latency analysis established, we can now
apply these tools to a concrete capacity planning exercise.

We can formalize this through \emph{ResNet-50 capacity planning}.

\phantomsection\label{callout-notebookux2a-1.13}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50 Capacity Planning}
\phantomsection\label{callout-notebook*-1.13}
Consider designing a ResNet-50 serving system with these requirements:

\begin{itemize}
\tightlist
\item
  \textbf{Target p99 latency}: 50ms
\item
  \textbf{Peak expected traffic}: 5,000 requests per second
\item
  \textbf{Service time} (TensorRT FP16): 5ms
\end{itemize}

\subsubsection*{Step 1: Find Safe
Utilization}\label{step-1-find-safe-utilization}
\addcontentsline{toc}{subsubsection}{Step 1: Find Safe Utilization}

Applying Equation~\ref{eq-p99-latency} to constrain
\(W_{p99} \leq 50\)ms with 5ms service time and solving for \(\rho\):

\[5\text{ms} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right) \leq 50\text{ms}\]

This yields \(\rho \leq 0\.72\) (72\% maximum utilization).

\subsubsection*{Step 2: Calculate Required Service
Rate}\label{step-2-calculate-required-service-rate}
\addcontentsline{toc}{subsubsection}{Step 2: Calculate Required Service
Rate}

\[\mu_{\text{required}} = \frac{\lambda}{\rho_{\text{safe}}} = \frac{5\,000}{0\.72} = 6944 \text{ requests/second}\]

\subsubsection*{Step 3: Determine GPU
Count}\label{step-3-determine-gpu-count}
\addcontentsline{toc}{subsubsection}{Step 3: Determine GPU Count}

Single V100 throughput at batch=16: 1,143 images/second

\[\text{GPUs needed} = \frac{6944}{1\,143} = 6\.1 \rightarrow 7 \text{ GPUs}\]

\subsubsection*{Step 4: Add Headroom for
Variance}\label{step-4-add-headroom-for-variance}
\addcontentsline{toc}{subsubsection}{Step 4: Add Headroom for Variance}

Production systems add 30\% headroom for traffic spikes and variance:

\[\text{Final count} = 7 \times 1.3 = 9\.1 \rightarrow 10 \text{ GPUs}\]

\subsubsection*{Step 5: Verify Fault
Tolerance}\label{step-5-verify-fault-tolerance}
\addcontentsline{toc}{subsubsection}{Step 5: Verify Fault Tolerance}

The 30\% headroom addresses traffic variance, but production systems
also need fault tolerance. With 10 GPUs, losing one leaves 9 GPUs
handling 5,000 QPS:

\[\text{Utilization after failure} = \frac{5\,000 / 1\,143}{9} = 48\.6\%\]

This remains well below the 72\% safe utilization threshold, confirming
N+1 redundancy is satisfied. For stricter fault tolerance requirements,
N+2 redundancy (tolerating two simultaneous failures) would require
11-12 GPUs.

\textbf{Result}: Provision 10 V100 GPUs to serve 5,000 QPS at 50ms p99
latency with N+1 fault tolerance.

\end{fbxSimple}

The queuing analysis explains the capacity planning approach detailed in
Section~\ref{sec-model-serving-systems-capacity-planning-96a3} and
connects directly to the MLPerf Server scenario.
\textbf{?@sec-benchmarking-ai} explains how MLPerf measures throughput
only for requests meeting the latency SLO: a system achieving 10,000 QPS
but violating the SLO on 5\% of requests reports only 9,500 valid QPS.

\subsection{Tail-Tolerant
Techniques}\label{sec-model-serving-systems-tailtolerant-techniques-066e}

Rather than eliminating all sources of latency variability, which is
often impractical, production systems employ techniques that tolerate
variability while still meeting SLOs (\citeproc{ref-dean2013tail}{Dean
and Barroso 2013}; \citeproc{ref-dean2012rapid}{Dean 2012}). These
techniques treat latency variance as a given and design around it.

\textbf{Hedged Requests.} When a request has not completed within the
expected time, send a duplicate request to another
server.\sidenote{\textbf{Hedging}: Borrowed from finance, where
``hedging'' means reducing risk by making offsetting bets. The term
derives from the literal hedge (a boundary of shrubs) that protects a
garden. Financial hedging dates to the 1600s Dutch tulip markets.
Google's Jeff Dean introduced ``hedged requests'' in his influential
2013 ``Tail at Scale'' paper, applying the financial concept to
distributed systems: send redundant requests to protect against the risk
of slow responses. } The client uses whichever response arrives first
and cancels the other. For ML serving, this means maintaining multiple
model replicas and routing slow requests to alternative replicas. The
overhead is modest: if you hedge at the 95th percentile, only 5\% of
requests generate duplicates, increasing load by just 5\% while
dramatically reducing tail latency.

\textbf{Cancellation Complexity}: A critical implementation detail is
that CUDA kernels cannot be interrupted mid-execution. When a hedged
request completes, the duplicate must be cancelled, but if inference has
already begun on the GPU, cancellation approaches include checking a
cancellation flag before launching inference, accepting wasted compute
for the in-flight kernel, or using request prioritization to
deprioritize the duplicate. Since hedging typically applies only to the
slowest 5 percent of requests, the overhead from occasional wasted
compute remains acceptable.

\textbf{Tied Requests.} Send the request to multiple servers
simultaneously, but include a tag allowing servers to cancel execution
once another server begins processing. This eliminates the delay of
waiting to detect a slow response before hedging. For inference servers
with significant startup overhead from model loading and memory
allocation, tied requests ensure at least one server begins immediately.

\textbf{Canary Requests.} For requests that fan out to many backends,
first send the request to a small subset of 1 to 2
servers.\sidenote{\textbf{Canary}: From the practice of using canary
birds in coal mines from the early 1900s through the 1980s. Miners
brought caged canaries underground because the birds' high metabolic
rate made them sensitive to carbon monoxide and methane, dying before
gas concentrations became lethal to humans. In software, ``canary''
describes any small-scale test that detects problems before they affect
the full system, whether canary deployments, canary requests, or canary
tests. } If these return within expected time, send to the remainder. If
the canary is slow, the system can take corrective action by retrying
elsewhere or using cached results before committing to the full fan-out.
This prevents a single slow backend from stalling an entire distributed
inference request.

\textbf{Graceful Degradation.} When load exceeds capacity, return
approximate results rather than timing out. For classification, return
cached predictions for similar inputs. For generative models, return
shorter outputs. For ensemble systems, return predictions from a subset
of models. This maintains responsiveness at the cost of some accuracy,
which users often prefer to outright failures.

\textbf{Admission Control.} When traffic exceeds capacity, accepting all
requests can trigger widespread SLO violations. Admission control
proactively rejects requests when queue depth exceeds a threshold,
returning immediate 503 responses rather than accepting requests that
are likely to timeout. This sacrifices throughput to protect latency for
admitted requests.

\textbf{Setting the Threshold}: A practical starting point is 2 to 3
times service time multiplied by the number of workers. For a system
with 4 workers and 10ms service time, this yields a queue depth
threshold of 80 to 120 requests. Adaptive admission control adjusts
thresholds based on observed p99 latency, tightening when latency
increases above target and relaxing when latency remains healthy.

\textbf{Retry Storm Prevention}: A subtle failure mode occurs when all
replicas are overloaded simultaneously. If the load balancer retries
rejected requests at other replicas that are also overloaded, retry
traffic amplifies the overload. Coordinated load shedding addresses this
by sharing load information across replicas, enabling system-wide
decisions about which requests to accept. When global load exceeds
capacity, replicas collectively reject the same fraction of requests
rather than each rejecting independently and triggering retries.

These techniques become essential at scale when fan-out amplification
makes individual server tail latency visible to users. Single-machine
serving systems can implement hedged and tied requests across GPU
streams or model replicas. The queuing analysis here assumes FIFO
processing, but production systems often implement priority scheduling
such as deadline-aware or shortest-job-first approaches to further
reduce tail latency for heterogeneous workloads
(\citeproc{ref-harchol2013performance}{Harchol-Balter 2013}).

The tail-tolerant techniques examined in this section optimize the flow
of requests through a functioning serving system. The queuing analysis,
however, assumes a critical precondition: that models are loaded,
initialized, and producing correct predictions. In production, this
assumption fails regularly: during deployments, new instances must load
models from scratch; during scaling events, cold start latency affects
the first requests to new replicas; and when preprocessing pipelines
diverge from training, accuracy silently degrades. The next section
examines these lifecycle challenges that must be solved before queuing
optimization becomes relevant.

\section{Model Lifecycle
Management}\label{sec-model-serving-systems-model-lifecycle-management-ff2e}

The queuing analysis from previous sections assumes two prerequisites:
models are loaded and ready to process requests, and predictions match
what was validated during development. Production systems often violate
both assumptions. Cold start latency can exceed inference time by orders
of magnitude during scaling events. Subtle preprocessing differences
between training and serving pipelines cause accuracy degradation that
no amount of queuing optimization can address. This section examines the
challenges that threaten these foundational assumptions. Retraining
frequency spans multiple orders of magnitude: from hourly updates for
recommendation systems to annual updates for some embedded devices.

\subsection{Training-Serving
Skew}\label{sec-model-serving-systems-trainingserving-skew-7b99}

A model that performed well during validation may silently degrade when
deployed. This phenomenon, known as \textbf{training-serving skew},
represents one of the most subtle failure modes in production ML because
it is invisible to latency monitoring and exception tracking.

\phantomsection\label{callout-definitionux2a-1.14}
\begin{fbxSimple}{callout-definition}{Definition:}{Training-Serving Skew}
\phantomsection\label{callout-definition*-1.14}
\textbf{\emph{Training-Serving Skew}} is the \textbf{Distributional
Divergence} between the training and inference environments. It arises
when the function \(f_{train}(x)\) differs from \(f_{serve}(x)\) due to
inconsistent preprocessing logic or environmental state, violating the
\textbf{Consistency Imperative} and causing silent accuracy degradation.

\end{fbxSimple}

\textbf{?@sec-machine-learning-operations-mlops} provides comprehensive
coverage of skew diagnosis, monitoring, and organizational prevention
strategies. Here we focus on the \emph{serving-specific} manifestation:
\textbf{preprocessing divergence}. This occurs when the real-time
inference pipeline processes raw data differently than the batch
training pipeline, a common failure mode when training uses
Python/Pandas while serving uses C++/Java or optimized inference
servers. Unlike data drift (which
\textbf{?@sec-machine-learning-operations-mlops} addresses through
monitoring), preprocessing divergence is deterministic and preventable
through careful engineering.

\phantomsection\label{callout-exampleux2a-1.15}
\begin{fbxSimple}{callout-example}{Example:}{ResNet-50: Image Preprocessing Skew}
\phantomsection\label{callout-example*-1.15}
For ResNet-50 serving, common sources of skew include:

\textbf{Resize interpolation}: Training uses PIL.BILINEAR while OpenCV
defaults to cv2.INTER\_LINEAR. These produce pixel-level differences
that can shift accuracy by 0.5-1\%.

\textbf{Color space handling}: JPEG loading in different libraries may
produce BGR vs RGB ordering. If the model trained on RGB but serves BGR
inputs, predictions are essentially random.

\textbf{Normalization constants}: ImageNet normalization uses specific
mean/std values. Using \texttt{mean={[}0.5,\ 0.5,\ 0.5{]}} instead of
\texttt{mean={[}0.485,\ 0.456,\ 0.406{]}} shifts inputs out of the
training distribution.

\textbf{Prevention}: The safest approach is to export the exact
preprocessing code used during training and run it identically in
serving, or use a framework like NVIDIA DALI that can help standardize
preprocessing across training and serving environments.

\end{fbxSimple}

\subsection{Cold Start and Initialization
Dynamics}\label{sec-model-serving-systems-model-loading-initialization-cc5a}

With preprocessing pipelines designed to avoid training-serving skew,
the next challenge is getting models ready to serve. Before processing
any request, models must load from storage into memory and prepare for
inference (\citeproc{ref-romero2021infaas}{Romero et al. 2021}). This
initialization latency, known as \textbf{cold start}, affects system
responsiveness during deployments, scaling events, and recovery from
failures.

\phantomsection\label{callout-definitionux2a-1.16}
\begin{fbxSimple}{callout-definition}{Definition:}{Cold Start}
\phantomsection\label{callout-definition*-1.16}
\textbf{\emph{Cold Start}}---a metaphor borrowed from automotive
engineering where engines operate inefficiently until reaching thermal
equilibrium---is the \textbf{Initialization Latency} incurred when
instantiating a new model replica. It represents the fixed cost of
\textbf{State Hydration} (loading weights, compiling graphs) that
effectively blocks the system's ability to scale elastically in response
to traffic bursts. ML inference systems similarly suffer high latency
during their ``warm-up'' phase before they can serve traffic at peak
efficiency.

\end{fbxSimple}

Cold start dynamics determine whether systems meet latency requirements
from the moment they begin serving traffic. A \emph{cold start timeline}
for a representative model reveals where each phase contributes to total
initialization latency.

\textbf{Cold Start Anatomy.} Cold start latency compounds from multiple
sources, each adding to the time between deployment and serving
readiness. Weight loading reads model parameters from disk or network
storage. Graph compilation performs just-in-time compilation of
operations for the specific hardware. Memory allocation reserves GPU
memory for activations and intermediate values.
Warmup\sidenote{\textbf{Warmup}: The computing metaphor derives from
physical warming, where engines and machines perform better after
reaching operating temperature. In JIT-compiled systems like the JVM
(1990s), ``warmup'' specifically refers to the period when the runtime
gathers profiling data and compiles hot paths. For ML serving, warmup
serves a dual purpose: triggering lazy memory allocation and populating
CPU/GPU caches with frequently-accessed data, ensuring the first real
user request does not pay these one-time costs. } execution performs
initial inferences that populate caches and trigger lazy initialization.

\phantomsection\label{callout-notebookux2a-1.17}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50: Cold Start Timeline}
\phantomsection\label{callout-notebook*-1.17}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2897}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2523}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4579}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Duration}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Weight loading (SSD)} & 0.5s & 98MB FP32 weights from local
storage \\
\textbf{Weight loading (S3)} & 3-5s & Network latency dominates for
cloud storage \\
\textbf{CUDA context} & 0.3-0.5s & GPU driver initialization and memory
setup \\
\textbf{TensorRT compilation} & 15-30s & Converts PyTorch model to
optimized engine \\
\textbf{Warmup (10 inferences)} & 0.2s & Triggers remaining lazy
initialization \\
\textbf{Total (local, optimized)} & \textbf{\textasciitilde1.5s} & With
pre-compiled TensorRT engine, warm container \\
\textbf{Total (cloud, first deploy)} & \textbf{\textasciitilde35s} &
Including compilation from cold state \\
\end{longtable}

\textbf{Key insight}: Pre-compiling models and storing the optimized
engine eliminates the 30-second compilation phase on subsequent
deployments.

\textbf{CUDA Context}: Before any GPU operation, the CUDA runtime must
establish a \emph{context}: a data structure that tracks memory
allocations, loaded kernels, and device state. Creating a context
requires communicating with the GPU driver and allocating GPU memory for
internal bookkeeping. This one-time cost (0.3-0.5s) affects every new
process that uses the GPU. CUDA 11+ introduced lazy initialization that
defers some setup until first use, reducing apparent startup time but
shifting cost to the first inference.

\textbf{CUDA MPS (Multi-Process Service)}: Normally, each process
creates its own CUDA context, and the GPU time-slices between contexts.
MPS allows multiple processes to share a single context, eliminating
redundant initialization and enabling concurrent kernel execution. For
serving systems running multiple model replicas, MPS can reduce
aggregate cold start time and improve GPU utilization. The trade-off is
reduced isolation: a crash in one process can affect others sharing the
MPS server.

\end{fbxSimple}

Without warmup, the first real request triggers compilation and memory
allocation mid-inference, often causing timeout failures. A request that
normally takes 5ms might require 500ms during cold start, violating SLOs
and degrading user experience.

\subsection{Loading
Strategies}\label{sec-model-serving-systems-loading-strategies-eb38}

Different loading strategies trade off cold start duration against
serving performance and memory efficiency.

\textbf{Full loading} reads the entire model into memory before serving
begins. This maximizes inference speed since all weights are immediately
available, but extends cold start duration and limits model size to
available memory. Full loading is appropriate when cold start latency is
acceptable and models comfortably fit in memory.

\textbf{Memory mapping} maps model files directly into the address
space, loading pages on demand as accessed. This reduces cold start time
since inference can begin before the full model loads, but causes
unpredictable latency as pages fault in during initial requests. Memory
mapping works well for infrequently accessed model components but can
cause latency spikes if critical weights are not preloaded.

\textbf{Lazy initialization} defers compilation and allocation until
first use. This minimizes startup time but shifts latency to the first
request. Production systems often combine lazy initialization with
synthetic warmup requests to trigger initialization before real traffic
arrives.

\subsection{Model Caching
Infrastructure}\label{sec-model-serving-systems-model-caching-infrastructure-4f1a}

Production systems cache model weights at the infrastructure level to
reduce cold start for common deployment scenarios:

\textbf{Container Image Embedding}: Bundle model weights directly in the
container image. This produces a single deployment artifact and
eliminates network fetches at startup, but creates large images (often
10-50GB) that slow container pulls and consume registry storage. Best
for models that rarely update.

\textbf{Shared Filesystem}: Mount a network filesystem (EFS, GCS FUSE)
containing model weights. Multiple replicas share cached weights, and
updates propagate immediately without redeployment. Network latency
affects cold start, and filesystem availability becomes a critical
dependency. Best for organizations with many models and frequent
updates.

\textbf{Node-Local SSD Cache}: Pre-populate local SSDs on inference
nodes with frequently-used models. Provides fast loading (500MB/s+ for
NVMe) without network dependency, but requires cache management to
handle model updates and capacity limits. Best for high-traffic models
where cold start latency is critical.

The choice depends on model update frequency: infrequent updates favor
container embedding, frequent updates favor shared filesystem, and
performance-critical deployments benefit from local caching with
background refresh.

\subsection{Multi-Model
Serving}\label{sec-model-serving-systems-multimodel-serving-a9c1}

Production systems often serve multiple models from a single machine,
whether different model versions for A/B testing, ensemble components,
or entirely different models sharing infrastructure. GPU memory becomes
the limiting resource, requiring careful management strategies.

Strategies for multi-model serving include time-multiplexing that loads
one model at a time and swaps based on request routing, memory sharing
where models share GPU memory to limit concurrent execution but enable
more models, and model virtualization where frameworks like Triton
manage model lifecycle by loading and unloading based on traffic
patterns (\citeproc{ref-nvidia2024triton}{Savard et al. 2024}). The
choice depends on request patterns. If models receive traffic evenly,
concurrent loading works. If traffic is bursty and model-specific,
time-multiplexing with intelligent preloading reduces average latency
while maximizing GPU utilization.

\subsubsection{Multi-Stream
Execution}\label{sec-model-serving-systems-multistream-execution-1b1f}

When multiple models or multiple instances of the same model must run
concurrently on a single GPU, the hardware must partition resources
between them. NVIDIA's Multi-Instance GPU technology enables
hardware-level isolation, dividing an A100 into up to 7 independent GPU
instances, each with dedicated memory and compute resources. MIG is
available on A100, A30 (up to 4 instances), H100, H200, and newer data
center GPUs. For older GPUs such as V100 or T4, CUDA stream scheduling
provides time-multiplexed sharing without hardware isolation.

The choice depends on whether consistent latency with MIG or maximum
utilization with shared streams is the priority.

\subsubsection{Model Swapping and Host
Memory}\label{sec-model-serving-systems-model-swapping-host-memory-c54f}

When the aggregate size of all models exceeds GPU memory capacity, the
serving system must swap models between host memory (DRAM) and device
memory (VRAM) on demand. This introduces a new latency component
determined by the PCIe bus bandwidth.

For a 10 GB model on PCIe Gen4 x16 (32 GB/s theoretical bandwidth),
loading takes at least:
\[ T_{\text{load}} = \frac{10 \text{ GB}}{32 \text{ GB/s}} \approx 312 \text{ ms} \]

To mitigate this, systems use \textbf{Pinned Memory} (page-locked host
memory). By default, the operating system can move (``page'') any memory
region to disk when RAM is under pressure. This creates a problem for
GPU transfers: if the GPU's DMA (Direct Memory Access) engine begins
reading a memory region that gets paged out mid-transfer, the transfer
fails or stalls. To avoid this, the CPU must first copy data to a
temporary pinned buffer before the GPU can safely read it, adding both
latency and CPU overhead.

Pinning memory instructs the OS to keep that region permanently in
physical RAM. The GPU's DMA engine can then transfer data directly from
the pinned region at full PCIe bandwidth without CPU involvement. The
trade-off is that pinned memory reduces the RAM available for other
processes and cannot be reclaimed under memory pressure. For model
serving, the performance gain (2-3× faster transfers) typically
justifies pinning model weights and frequently-used input buffers, while
leaving less critical memory pageable.

The lifecycle management strategies examined so far ensure models are
ready to serve: loaded into memory, warmed up, and producing predictions
consistent with training. With these prerequisites satisfied, the
queuing dynamics from
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
become relevant. The next optimization opportunity lies in how requests
are grouped for processing, which directly affects both the throughput
and latency terms in our queuing equations.

\section{Throughput
Optimization}\label{sec-model-serving-systems-throughput-optimization-18d1}

With models loaded, initialized, and ready to serve, the next
optimization opportunity lies in how requests are grouped for
processing. Batching\sidenote{\textbf{Batch}: From Old French ``bache''
(a quantity baked at one time), the term entered computing in the 1950s
to describe jobs processed together without human interaction, as
contrasted with interactive computing. IBM's batch processing systems of
the 1960s would collect punch cards overnight and process them
sequentially. The ML usage preserves this core meaning: group samples
together for efficient processing, trading individual response time for
aggregate throughput. } differs fundamentally between training and
serving (\citeproc{ref-crankshaw2017clipper}{Crankshaw et al. 2017}).
Training batches maximize throughput, processing hundreds or thousands
of samples together with no concern for individual sample latency.
Serving batches must balance throughput against individual request
latency, typically processing single digits of requests together while
ensuring no request waits too long. This adaptive approach is called
\textbf{dynamic batching} because the system adjusts batch composition
in real time based on arriving requests.

\phantomsection\label{callout-definitionux2a-1.18}
\begin{fbxSimple}{callout-definition}{Definition:}{Dynamic Batching}
\phantomsection\label{callout-definition*-1.18}
\textbf{\emph{Dynamic Batching}} is the runtime optimization of trading
\textbf{Latency} for \textbf{Throughput} under stochastic arrival
patterns. By buffering requests into a \textbf{Batch Window}, the
scheduler amortizes fixed overheads (kernel launch, weight IO) across
multiple inputs, pushing the system away from the memory-bound regime.

\end{fbxSimple}

\subsection{Why Batching
Helps}\label{sec-model-serving-systems-batching-helps-f1dc}

Modern accelerators achieve peak efficiency only at sufficient batch
sizes (\citeproc{ref-shen2019nexus}{Shen et al. 2019}). A single
inference request leaves most compute units idle because GPUs are
designed for parallel execution across thousands of threads. Batching
amortizes fixed costs across multiple requests and enables parallel
execution across the batch dimension.

Two fixed costs dominate at small batch sizes. \textbf{Kernel launch
overhead}\sidenote{\textbf{Kernel}: From Old English ``cyrnel'' meaning
seed or grain, the essential core of something. In operating systems
(1960s), the kernel is the core that manages hardware resources. CUDA
borrowed this term around 2007 for GPU functions because they represent
the computational ``core'' of parallel algorithms. Unlike OS kernels
that run continuously, GPU kernels are discrete units of parallel work
launched by the CPU and executed across thousands of GPU threads
simultaneously. } is the time for the CPU to prepare and submit work to
the GPU. Each layer in a neural network typically requires a separate
kernel launch: the CPU must assemble kernel parameters, copy them to
GPU-accessible memory, and signal the GPU to begin execution. This
overhead is typically 5-20μs per kernel, independent of batch size.
ResNet-50 has approximately 50 layers, so kernel launch alone adds
250-1000μs per inference. At batch size 1, this overhead may exceed the
actual compute time; at batch size 32, the same overhead is amortized
across 32 images. \textbf{Weight loading} reads model parameters from
GPU memory (VRAM) to the compute units. At batch size 1, the GPU reads
all weights to process one image; at batch size 32, the same weight read
processes 32 images, achieving 32× better memory efficiency. Measuring
\emph{batching efficiency} on a concrete model quantifies how these
fixed costs amortize in practice.

\phantomsection\label{callout-notebookux2a-1.19}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50 Batching Efficiency}
\phantomsection\label{callout-notebook*-1.19}
The throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates
the power of batching:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1728}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2346}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2593}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1728}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1605}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Inference Time}*
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Per-Image Compute}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{GPU Util.}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 5.0ms & 5.0ms & 200 img/s & 15\% \\
4 & 7.2ms & 1.8ms & 556 img/s & 42\% \\
8 & 9.1ms & 1.1ms & 879 img/s & 65\% \\
16 & 14.0ms & 0.9ms & 1,143 img/s & 85\% \\
32 & 25.0ms & 0.8ms & 1,280 img/s & 95\% \\
\end{longtable}

*Times shown are pure inference time, excluding queue wait.
Section~\ref{sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b}
analyzes how user-perceived latency includes batching window wait.

\textbf{Key insight}: Batch size 32 achieves 6.4× higher throughput than
batch size 1. However, user-perceived latency includes both queue wait
and inference time. With a 10ms batching window and 25ms inference,
total latency reaches 35ms versus 5ms at batch size 1.

\end{fbxSimple}

The table reveals the throughput-latency tradeoff in stark terms: larger
batches dramatically improve hardware efficiency but increase
per-request latency. In practice, the optimal batch size depends on both
the latency Service Level Objective (SLO) and the arrival rate of
requests. The question facing every serving engineer is therefore
quantitative: given a specific latency budget, what is the largest batch
size that still meets the SLO? The following analysis shows how to find
\emph{the batching sweet spot}.

\phantomsection\label{callout-notebookux2a-1.20}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Batching Sweet Spot}
\phantomsection\label{callout-notebook*-1.20}
\textbf{Problem}: You are serving a ResNet-50 model. At batch=1, the GPU
is mostly idle (15\% utilization). You want to increase throughput to
save money, but you have a \textbf{20 ms} latency budget.

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Baseline (Batch 1)}: Inference = \textbf{5 ms}. Throughput =
  \textbf{200 img/s}.
\item
  \textbf{Optimized (Batch 8)}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Wait Time}: You set a \textbf{5 ms} batching window to
    collect requests.
  \item
    \textbf{Inference Time}: Batch 8 inference takes \textbf{9 ms}.
  \item
    \textbf{User Latency}:
    \(5 \text{ ms (wait)} + 9 \text{ ms (compute)} = \mathbf{14 \text{ ms}}\).
  \item
    \textbf{Throughput}:
    \(8 \text{ img} / 14 \text{ ms} \approx \mathbf{571 \text{ img/s}}\).
  \end{itemize}
\end{enumerate}

\textbf{The Systems Conclusion}: By accepting a \textbf{3× increase in
latency} (5ms → 14ms), you have achieved nearly \textbf{3× higher
throughput} on the same hardware. As long as 14ms is under your 20ms
budget, this is ``free'' capacity. This trade-off is the fundamental
lever of serving economics.

\end{fbxSimple}

Figure~\ref{fig-throughput-latency-knee} visualizes this trade-off,
showing the \textbf{``Knee''} of the curve. This is the optimal
operating point where throughput is maximized before latency spikes due
to queuing.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/serving/serving_files/figure-pdf/fig-throughput-latency-knee-output-1.pdf}}

}

\caption{\label{fig-throughput-latency-knee}\textbf{The
Throughput-Latency Knee.} Batch Size vs.~Throughput (Blue) and Latency
(Orange). Throughput increases with batch size as hardware utilization
improves, but eventually saturates. Latency remains relatively flat
(hidden by parallel resources) until the `Knee,' after which it spikes
linearly due to queuing. The optimal operating point lies just before
this spike.}

\end{figure}%

The efficiency gains from batching come at a cost: requests must wait
for the batch to form. This creates a fundamental tension between
throughput optimization (larger batches) and latency minimization
(immediate processing). The different batching strategies and their
tradeoffs govern how engineers tune this balance.

\subsection{Static vs Dynamic
Batching}\label{sec-model-serving-systems-static-vs-dynamic-batching-fd0a}

\textbf{Static batching} waits for a fixed batch size before processing.
Simple to implement but problematic in practice: during low traffic,
requests wait indefinitely for a full batch. During high traffic, large
batches increase per-request latency.

\textbf{Dynamic batching} collects requests within a time window,
processing whatever has arrived when the window closes
(\citeproc{ref-olston2017tensorflow}{Olston et al. 2017}). This bounds
maximum wait time regardless of traffic level. The window size
represents a direct tradeoff: shorter windows reduce latency but
sacrifice throughput; longer windows improve throughput but increase
latency.

Typical configurations use windows of 5-50ms with maximum batch sizes of
8-32 for latency-sensitive applications. The optimal configuration
depends on request arrival patterns, model characteristics, and latency
requirements.

\subsection{Dynamic Batching Latency-Throughput
Trade-offs}\label{sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d}

Dynamic batching introduces a fundamental tension between throughput
optimization and latency constraints. Understanding this tradeoff
quantitatively enables systematic configuration decisions rather than
trial-and-error tuning. To begin, consider \emph{why latency spikes
under load}:

\phantomsection\label{callout-notebookux2a-1.21}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Why Latency Spikes Under Load}
\phantomsection\label{callout-notebook*-1.21}
\textbf{Recall} from
Section~\ref{sec-model-serving-systems-littles-law-9352}: Little's Law
(\(L = \lambda W\)) governs all stable queues. When hardware is
saturated (throughput \(\lambda\) is maxed out), any increase in traffic
increases queue depth (\(L\)). Since \(\lambda\) cannot grow,
\textbf{latency (\(W\)) must grow linearly with queue depth}. This is
why \textbf{admission control} (rejecting requests when \(L\) exceeds a
threshold) is the only way to preserve latency during overload.

\end{fbxSimple}

Equation~\ref{eq-batching-latency} decomposes the total user-perceived
latency for a batched request into two components:

\begin{equation}\phantomsection\label{eq-batching-latency}{L_{\text{total}} = L_{\text{wait}} + L_{\text{compute}}(b)}\end{equation}

where \(L_{\text{wait}}\) is the time spent waiting in the batching
queue and \(L_{\text{compute}}(b)\) is the inference time for batch size
\(b\). The batching window \(T\) bounds wait time
(\(L_{\text{wait}} \leq T\)), while batch size affects compute time
through GPU utilization characteristics.

\subsubsection{Quantitative Analysis of
Batching}\label{sec-model-serving-systems-queue-waiting-time-analysis-8d5c}

For Poisson arrivals with rate \(\lambda\) and batching window \(T\),
requests arrive uniformly within the window. A request arriving at time
\(t\) within the window waits \(T - t\) for the batch to close.
Equation~\ref{eq-avg-wait} shows that the average wait time is simply
half the window:

\begin{equation}\phantomsection\label{eq-avg-wait}{E[L_{\text{wait}}] = \frac{T}{2}}\end{equation}

This simple relationship has direct implications. A 20ms batching window
adds 10ms average latency regardless of batch size achieved. If your
latency SLO is 50ms and inference takes 5ms, the batching window
consumes 20\% of your latency budget before any computation begins.

\textbf{Batch Size Distribution.} The number of requests collected
during window \(T\) follows a Poisson distribution with mean
\(\lambda T\). Equation~\ref{eq-batch-distribution} formalizes this
relationship:

\begin{equation}\phantomsection\label{eq-batch-distribution}{P(\text{batch size} = k) = \frac{(\lambda T)^k e^{-\lambda T}}{k!}}\end{equation}

Table~\ref{tbl-batch-variability} quantifies this variability, showing
how batch size fluctuates for different traffic levels with a fixed 10ms
window:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2162}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1892}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1486}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1892}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2568}}@{}}
\caption{\textbf{Batch Size Variability}: At low traffic, batching
windows frequently contain zero requests (wasted GPU cycles). At
moderate traffic, batch sizes fluctuate significantly around the mean.
High traffic provides more stable batching but still sees 13\% of
batches exceeding twice the mean
size.}\label{tbl-batch-variability}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Mean Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Std Dev}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch=0)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch≥2×mean)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Mean Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Std Dev}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch=0)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch≥2×mean)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{50 QPS} & 0.5 & 0.7 & 61\% & 1\% \\
\textbf{200 QPS} & 2.0 & 1.4 & 14\% & 9\% \\
\textbf{500 QPS} & 5.0 & 2.2 & 0.7\% & 12\% \\
\textbf{1000 QPS} & 10.0 & 3.2 & 0.005\% & 13\% \\
\end{longtable}

\subsubsection{Throughput Maximization
Strategy}\label{sec-model-serving-systems-throughput-maximization-strategy-27f5}

Throughput optimization requires maximizing the number of requests
processed per unit time. For a system with service time \(S(b)\) for
batch size \(b\), throughput follows Equation~\ref{eq-batch-throughput}:

\begin{equation}\phantomsection\label{eq-batch-throughput}{\text{Throughput}(b) = \frac{b}{T + S(b)}}\end{equation}

The numerator increases linearly with batch size while the denominator
increases sub-linearly (due to GPU parallelism). This creates an optimal
batch size that balances these competing effects.

For ResNet-50 on a V100 GPU, service time scales as
\(S(b) = 5\text{ms} + 0.6b\) (5ms fixed overhead plus 0.6ms per
additional image in the batch). With \(T = 10\)ms batching window:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1867}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2133}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2267}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1867}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1867}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Service Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Total Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Efficiency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 5.6ms & 15.6ms & 64 img/s & Low \\
4 & 7.4ms & 17.4ms & 230 img/s & Moderate \\
8 & 9.8ms & 19.8ms & 404 img/s & Good \\
16 & 14.6ms & 24.6ms & 650 img/s & High \\
32 & 24.2ms & 34.2ms & 935 img/s & Maximum \\
\end{longtable}

\begin{description}
\item[The throughput gains in \textbf{?@tbl-batching-throughput} trace
directly back to the Iron Law framework established in
\textbf{?@sec-ai-training-iron-law-training-performance-a53f}, where
batching amortizes the fixed overhead term.]
\textbf{Batching Throughput Analysis}: ResNet-50 throughput on V100 with
10ms batching window. Throughput increases 14.6x from batch size 1 to 32
(64 to 936 img/s), but total latency more than doubles (15.6ms to
34.2ms). The optimal configuration depends on whether the latency SLO or
throughput target is the binding constraint.
\{\#tbl-batching-throughput\}
\end{description}

\phantomsection\label{callout-notebookux2a-1.22}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Iron Law of Batching Efficiency}
\phantomsection\label{callout-notebook*-1.22}
\textbf{The Iron Law Connection:} In serving, we maximize throughput by
amortizing the \textbf{Latency Term} (\(L_{lat}\)).
\[ T = \frac{O}{R_{peak} \cdot \eta} + L_{lat} \]

\textbf{Deriving the Sweet Spot:}

\begin{itemize}
\tightlist
\item
  \textbf{Case 1 (Batch 1):} Overhead (5ms) \(\approx\) Compute (0.6ms).
  Efficiency \(\approx 10\%\). The GPU is mostly waiting.
\item
  \textbf{Case 2 (Batch 32):} Overhead (5ms) \(\ll\) Compute (19.2ms).
  Efficiency \(\approx 79\%\). The GPU is crunching numbers.
\end{itemize}

\textbf{The Golden Rule:} Increase batch size until the \textbf{Latency
Term} becomes negligible (\textless{} 10\% of total time). Beyond this
point, you gain minimal throughput but pay a linear latency penalty.

\end{fbxSimple}

\subsubsection{Latency-Constrained
Optimization}\label{sec-model-serving-systems-latencyconstrained-optimization-8f66}

When latency SLOs provide the binding constraint, the optimization
problem becomes finding the maximum batch size that meets the SLO. For
SLO \(L_{\text{SLO}}\) and average wait time \(T/2\),
Equation~\ref{eq-latency-constrained-batch} defines the maximum
allowable batch size:

\begin{equation}\phantomsection\label{eq-latency-constrained-batch}{b_{\text{max}} = \max\{b : \frac{T}{2} + S(b) \leq L_{\text{SLO}}\}}\end{equation}

Consider a 50ms p95 latency SLO for ResNet-50 serving:

\textbf{Scenario 1: Conservative window (T = 5ms)} - Average wait: 2.5ms
- Latency budget for inference: 47.5ms - Maximum batch size: 71 (but
typically capped at 32 for memory) - Achieved throughput:
\textasciitilde1,140 img/s (batch=32)

\textbf{Scenario 2: Aggressive window (T = 25ms)} - Average wait: 12.5ms
- Latency budget for inference: 37.5ms - Maximum batch size: 48 -
Achieved throughput: \textasciitilde1,280 img/s (batch=48)

The aggressive window achieves only 12\% higher throughput but increases
average latency by 10ms and p99 latency by 25ms. Examine
\textbf{?@tbl-batching-throughput}: for latency-sensitive applications,
the conservative window provides better user experience at modest
throughput cost.

\textbf{SLO Violation Analysis.} Batch size variability causes SLO
violations even when mean latency appears safe. The p99 latency includes
both worst-case wait time (full window) and worst-case batch size
(governed by Poisson tail). Equation~\ref{eq-p99-batch-latency} captures
this relationship:

\begin{equation}\phantomsection\label{eq-p99-batch-latency}{L_{p99} \approx T + S(b_{p99})}\end{equation}

where \(b_{p99}\) is the 99th percentile batch size. For
\(\lambda = 500\) QPS and \(T = 10\)ms:

\begin{itemize}
\tightlist
\item
  Mean batch size: 5
\item
  p99 batch size: 11 (from Poisson distribution)
\item
  Mean latency: \(5\text{ms} + 8\.0\text{ms} = 13\text{ms}\)
\item
  p99 latency: \(10\text{ms} + 11\.6\text{ms} = 21\.6\text{ms}\)
\end{itemize}

The p99 latency is 1.66× the mean, reflecting both wait time variance
and batch size variance. Systems that provision based on mean latency
will experience SLO violations.

\phantomsection\label{callout-perspectiveux2a-1.23}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{Practitioner's Perspective: The Latency-Throughput Trade-off}
\phantomsection\label{callout-perspective*-1.23}
In systems engineering interviews and architecture reviews, the most
common pitfall is discussing ``inference speed'' without specifying
\textbf{Batch Size}.

\begin{itemize}
\tightlist
\item
  \textbf{Batch-1 Regime}: Optimized for \textbf{Latency}. Relevant for
  real-time interaction (e.g., typing helpers, robotics). The bottleneck
  is usually Python overhead or memory bandwidth.
\item
  \textbf{Batch-32 Regime}: Optimized for \textbf{Throughput}. Relevant
  for offline processing or high-traffic services. The bottleneck is
  usually Compute (FLOPS).
\end{itemize}

\textbf{The Professional Response}: When asked ``how fast is this
model?'', always clarify: ``Are we optimizing for single-stream latency
(Batch 1) or maximum throughput (Batch N)?'' This distinction
demonstrates systems maturity.

\end{fbxSimple}

\subsubsection{Adaptive Batching
Windows}\label{sec-model-serving-systems-adaptive-batching-windows-c404}

Fixed batching windows waste latency budget during high traffic when
large batches form quickly. Listing~\ref{lst-adaptive-batching}
demonstrates how adaptive strategies adjust the window based on queue
depth.

\begin{codelisting}

\caption{\label{lst-adaptive-batching}\textbf{Adaptive Batching Window}:
Dynamically adjusts batch timeout based on queue depth and arrival rate,
reducing average latency by 27\% compared to fixed windows while
maintaining throughput.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ adaptive\_batching\_window(queue\_depth, arrival\_rate, slo\_ms):}
    \CommentTok{"""Compute optimal batching window.}

\CommentTok{    Based on current system state.}
\CommentTok{    """}
\NormalTok{    target\_batch\_size }\OperatorTok{=} \DecValTok{16}  \CommentTok{\# Optimal batch for GPU utilization}

    \CommentTok{\# Fast path: batch ready, close immediately to minimize latency}
    \ControlFlowTok{if}\NormalTok{ queue\_depth }\OperatorTok{\textgreater{}=}\NormalTok{ target\_batch\_size:}
        \ControlFlowTok{return} \DecValTok{0}

    \CommentTok{\# Compute maximum allowable wait from SLO constraint}
    \CommentTok{\# Reserve 30\% of latency budget for batching,}
    \CommentTok{\# remainder for inference}
\NormalTok{    max\_wait }\OperatorTok{=}\NormalTok{ slo\_ms }\OperatorTok{*} \FloatTok{0.3}

    \CommentTok{\# Estimate time to accumulate target batch at current arrival rate}
    \ControlFlowTok{if}\NormalTok{ arrival\_rate }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
\NormalTok{        requests\_needed }\OperatorTok{=}\NormalTok{ target\_batch\_size }\OperatorTok{{-}}\NormalTok{ queue\_depth}
\NormalTok{        estimated\_wait }\OperatorTok{=}\NormalTok{ requests\_needed }\OperatorTok{/}\NormalTok{ arrival\_rate}
        \CommentTok{\# Return minimum of estimated wait and SLO{-}constrained maximum}
        \ControlFlowTok{return} \BuiltInTok{min}\NormalTok{(estimated\_wait, max\_wait)}

    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        max\_wait  }\CommentTok{\# Low traffic: use full budget to accumulate batch}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This approach reduces average wait time during high traffic while
maintaining batch sizes. For traffic varying between 200-1000 QPS:

\begin{itemize}
\tightlist
\item
  Fixed window (10ms): Average latency 15ms, throughput 650 img/s
\item
  Adaptive window: Average latency 11ms (27\% reduction), throughput 680
  img/s (5\% improvement)
\end{itemize}

The interplay between window size and batch limits creates a space of
possible configurations, each representing a different balance between
throughput and latency.

\textbf{Throughput-Latency Pareto Frontier}

The batching configuration space forms a Pareto frontier where improving
throughput requires accepting higher latency.
Table~\ref{tbl-pareto-batching} traces this frontier across five
representative configurations:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1630}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1413}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1630}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1630}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1522}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2174}}@{}}
\caption{\textbf{Batching Pareto Frontier}: Each configuration
represents a different point on the throughput-latency trade-off curve.
Moving from 2ms to 50ms windows improves throughput by only 52\% while
increasing p99 latency by 5.4×. Diminishing returns make aggressive
batching costly for latency-sensitive
applications.}\label{tbl-pareto-batching}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Window (ms)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Max Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Configuration}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Window (ms)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Max Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Configuration}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 & 16 & 8ms & 18ms & 890 img/s & Ultra-low latency \\
5 & 32 & 10ms & 22ms & 1,140 img/s & Balanced \\
10 & 32 & 15ms & 35ms & 1,240 img/s & Moderate latency \\
20 & 64 & 23ms & 52ms & 1,310 img/s & Throughput-optimized \\
50 & 128 & 38ms & 98ms & 1,350 img/s & Maximum throughput \\
\end{longtable}

\subsubsection{Practical Configuration
Guidelines}\label{sec-model-serving-systems-practical-configuration-guidelines-9791}

Based on quantitative analysis, principled batching configuration
follows these guidelines. Start with the latency budget by allocating 20
to 30 percent of SLO to batching wait time. Estimate traffic using the
p95 arrival rate rather than average to account for traffic spikes.
Calculate the maximum window as
\(T_{\text{max}} = 0.3 \times L_{\text{SLO}}\). Determine the batch size
limit from GPU memory and p99 latency constraints. Monitor the actual
distribution since batch size variance indicates whether traffic
assumptions hold.

For ResNet-50 with 50ms SLO and 500 QPS traffic:

\begin{itemize}
\tightlist
\item
  Latency budget for batching: 15ms
\item
  Maximum window: 15ms
\item
  Expected batch size: 7.5
\item
  Maximum batch size: 32 (memory limit)
\item
  Configuration: \(T = 12\)ms, \(b_{\text{max}} = 32\)
\item
  Predicted p99 latency: 43ms (within SLO)
\item
  Predicted throughput: 1,180 img/s
\end{itemize}

\subsection{Continuous
Batching}\label{sec-model-serving-systems-continuous-batching-8bb6}

Autoregressive models like language models generate outputs token by
token, creating a batching challenge that differs from single-pass
models like ResNet-50. Traditional batching processes all sequences in a
batch for all generation steps, wasting compute when sequences complete
at different times (\citeproc{ref-yu2022orca}{Yu et al. 2022}). If one
sequence in a batch of 8 finishes after 10 tokens while others need 100
tokens, 87.5\% of the compute for that sequence slot is wasted. This
inefficiency matters as language models grow to dominate production
inference workloads.

Continuous batching (also called iteration-level batching) addresses
this waste by allowing new requests to join a batch between generation
steps and completed sequences to exit (\citeproc{ref-kwon2023vllm}{Kwon
et al. 2023}). Rather than forming static batches that persist for the
entire generation process, the system manages batch composition
dynamically at each decoding iteration.

The mechanism works as follows: when a sequence generates its
end-of-sequence token, its slot becomes immediately available. A waiting
request can fill that slot for the next iteration rather than waiting
for the entire batch to complete. Similarly, the system can add new
requests to available slots without interrupting ongoing generation.
This dynamic approach maintains high GPU utilization even when sequence
lengths vary dramatically.

Systems implementing continuous batching, such as vLLM and TensorRT-LLM,
achieve 2-4× higher throughput than traditional static batching
(\citeproc{ref-agrawal2024sarathi}{Agrawal et al. 2025}). The
improvement comes from two sources: eliminating wasted compute on
completed sequences and reducing average wait time for new requests. For
production language model serving where response lengths vary from
single tokens to thousands, continuous batching has become essential for
cost-effective deployment.

Memory management adds complexity to continuous batching. As sequences
enter and exit the batch, the key-value cache that stores attention
context must be dynamically allocated and freed. Consider what happens
when sequences of varying lengths share GPU memory: a 100-token sequence
completes and releases its cache, but a new 150-token sequence cannot
use that space because it needs a larger contiguous block. Over time,
small unusable gaps accumulate between allocated regions, eventually
preventing new sequences from starting even when total free memory
appears sufficient. This \emph{memory fragmentation} can waste 40 to 50
percent of available memory in naive implementations, severely limiting
the concurrent batch size that determines throughput.

\textbf{PagedAttention},\sidenote{\textbf{PagedAttention}: Introduced by
Kwon et al.~at SOSP 2023, this algorithm directly applies operating
system virtual memory concepts to GPU memory management for LLMs. Before
PagedAttention, researchers found that existing systems wasted 60-80\%
of KV cache memory due to fragmentation and over-reservation. By
borrowing paging and copy-on-write mechanisms from OS design,
PagedAttention reduces waste to under 4\%, enabling 2-4x higher
throughput on the same hardware. This technique has become the de facto
standard in production LLM serving systems. } introduced in vLLM, solves
this fragmentation problem by applying operating system virtual memory
concepts to GPU memory (\citeproc{ref-kwon2023vllm}{Kwon et al. 2023}).
Instead of allocating one contiguous block per sequence, PagedAttention
divides the KV cache into fixed-size \emph{pages} (typically 16 tokens
each). A sequence's cache consists of pointers to non-contiguous pages
scattered across GPU memory. When a sequence completes, its pages return
to a free list and can be reused by any new sequence, regardless of
length. This approach achieves near-zero fragmentation: vLLM reports
memory utilization above 95\% compared to 50-60\% for contiguous
allocation schemes. The overhead is modest (one pointer lookup per page
during attention computation), making PagedAttention the standard for
production LLM serving.

The batching and memory techniques covered here establish the foundation
for LLM serving, but several advanced topics warrant additional study:

\phantomsection\label{callout-perspectiveux2a-1.24}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{LLM Serving: Beyond the Fundamentals}
\phantomsection\label{callout-perspective*-1.24}
Language model serving introduces challenges beyond the batching and
memory principles established here. The key-value cache that stores
attention context scales with sequence length and batch size, often
exceeding the model weights themselves in memory consumption. Techniques
like speculative decoding use small draft models to propose multiple
tokens that the target model verifies in parallel, achieving 2-3×
latency reduction for interactive applications. Weight-only quantization
(INT4 weights with FP16 activations) proves more effective than
activation quantization for memory-bandwidth-bound LLM inference.

These LLM-specific optimizations build directly on the foundations this
chapter establishes: queuing theory governs request scheduling, batching
tradeoffs determine throughput-latency curves, and precision selection
follows the same accuracy-efficiency principles. The serving
fundamentals apply universally; LLM serving adds domain-specific
techniques atop this foundation. Advanced treatments provide detailed
coverage of KV cache optimization, including advanced techniques for
multi-tenant serving and distributed inference.

\end{fbxSimple}

While continuous batching represents the state of the art for LLM
serving, not all deployment scenarios benefit from batching at all. The
sophisticated techniques examined so far, from dynamic batching windows
to PagedAttention, optimize for high-throughput server workloads. These
techniques, however, introduce complexity and latency overhead that may
not be justified for all deployment contexts. A fundamental question
remains: \emph{when} does batching hurt rather than help?

\textbf{When Not to Batch.} Some scenarios require single-request
processing. Ultra-low latency requirements where p99 latency must stay
under 10ms make any batching delay unacceptable. Highly variable request
sizes where inputs vary dramatically in size cause batching to create
padding overhead that wastes compute. Memory constraints where models
already consume most GPU memory mean batch activations may cause
out-of-memory errors.

\subsection{Session Affinity
Constraints}\label{sec-model-serving-systems-session-affinity-constraints-8b1f}

When requests from the same user or session should route to the same
replica, batching becomes constrained. Session affinity, also called
sticky sessions, matters for three main reasons.

\textbf{KV-Cache Reuse}: For conversational AI, the key-value cache from
previous turns dramatically speeds up multi-turn conversations. Routing
a follow-up request to a different replica forfeits this cached context,
increasing latency by 2 to 5 times for long conversations.

\textbf{User-Specific Models}: Some systems serve personalized models or
adapters per user. Routing requests to the replica that has already
loaded that user's adapter avoids repeated loading overhead.

\textbf{Stateful Preprocessing}: When preprocessing maintains state
through tokenizer caches or session-specific normalization, routing to a
different replica requires rebuilding this state.

The tension with batching is clear since strict affinity constrains
which requests can be batched together, potentially reducing batch sizes
and GPU utilization. Production systems often implement soft affinity
where requests prefer their assigned replica but can overflow to others
when that replica is overloaded. This preserves most affinity benefits
while maintaining load balance.

\subsection{Traffic Patterns and Batching
Strategy}\label{sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b}

The optimal batching strategy depends critically on how requests arrive.
Different deployment contexts exhibit fundamentally different arrival
patterns, each requiring distinct batching approaches. The MLPerf
inference benchmark codifies these patterns into four scenarios that
directly map to real-world deployments, as
\textbf{?@sec-benchmarking-ai} explains in detail.

\textbf{Server Traffic (Poisson Arrivals).} Cloud APIs and web services
typically receive requests following a Poisson
process,\sidenote{\textbf{Poisson Process}: A stochastic model where
events occur continuously and independently at a constant average rate.
Named after French mathematician Simeon Denis Poisson (1781-1840), this
model accurately describes many real-world arrival patterns including
web requests and API calls. The key property for serving systems is that
inter-arrival times are exponentially distributed, meaning the
probability of long gaps between requests decays exponentially, which is
why batching windows can be tuned probabilistically. } where arrivals
are independent and uniformly distributed over time.
Equation~\ref{eq-poisson-batch} expresses the expected batch size for
Poisson arrivals with rate \(\lambda\) and batching window \(T\):

\begin{equation}\phantomsection\label{eq-poisson-batch}{E[\text{batch size}] = \lambda \cdot T}\end{equation}

The variance equals the mean (a property of Poisson distributions), so
batch sizes fluctuate significantly at moderate traffic. With
\(\lambda = 200\) requests/second and \(T = 10\)ms, expected batch size
is 2, but 16\% of windows will have zero requests (wasted compute
cycles) while others may have 4 or more.

The optimal batching window balances waiting cost against throughput
benefit. Equation~\ref{eq-optimal-window} defines this optimum:

\begin{equation}\phantomsection\label{eq-optimal-window}{T_{\text{optimal}} = \min\left(L - S, \sqrt{\frac{S}{\lambda}}\right)}\end{equation}

where \(L\) is the latency SLO and \(S\) is the service time. A perhaps
surprising result emerges from this equation: as traffic increases, the
optimal window decreases while achieved batch sizes still grow.
Table~\ref{tbl-traffic-adaptive} demonstrates this phenomenon across
four traffic levels.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2388}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2687}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2687}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2239}}@{}}
\caption{\textbf{Traffic-Adaptive Batching}: Higher traffic enables
shorter windows while still achieving larger batches. The optimal window
decreases even as batch sizes grow because more requests arrive per unit
time.}\label{tbl-traffic-adaptive}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Optimal Window}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Optimal Window}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{100 QPS} & 20ms & 2.0 & 45ms \\
\textbf{500 QPS} & 8ms & 4.0 & 42ms \\
\textbf{1,000 QPS} & 5ms & 5.0 & 38ms \\
\textbf{5,000 QPS} & 2ms & 10.0 & 35ms \\
\end{longtable}

\textbf{Streaming Traffic (Correlated Arrivals).} Autonomous vehicles,
video analytics, and robotics systems receive inputs from multiple
synchronized sensors. This scenario illustrates \emph{multi-camera
autonomous vehicle serving}.

\phantomsection\label{callout-notebookux2a-1.25}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Multi-Camera Autonomous Vehicle Serving}
\phantomsection\label{callout-notebook*-1.25}
Consider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial
fusion:

\textbf{Timeline for processing frame set N:}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
\textbf{Time} & \textbf{Event} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
T = 0ms & Cameras begin capturing frame N \\
T = 8ms & Camera 1 frame arrives \\
T = 10ms & Cameras 2-5 frames arrive \\
T = 15ms & Camera 6 arrives (jitter) \\
T = 15ms & Batch inference begins (6 images) \\
T = 25ms & Inference complete \\
T = 32ms & Result ready for planning module \\
\end{longtable}

\textbf{Key constraints:}

\begin{itemize}
\tightlist
\item
  Hard deadline: 33ms per frame set (real-time requirement)
\item
  Batch size: Fixed at 6 (one per camera)
\item
  Synchronization budget: 12ms of 33ms total (36\% for jitter tolerance)
\item
  Timeout policy: If camera frame not received by T+20ms, use previous
  frame
\end{itemize}

Unlike Poisson traffic where dynamic batching optimizes throughput,
streaming traffic requires synchronization policies that handle sensor
jitter while meeting hard deadlines.

\end{fbxSimple}

\textbf{Single-User Traffic (Sequential Arrivals).} Mobile and embedded
applications serve one user at a time, with requests arriving only after
the previous result is consumed. We can analyze these constraints in
\emph{ResNet-50 mobile serving}.

\phantomsection\label{callout-notebookux2a-1.26}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50: Mobile Serving}
\phantomsection\label{callout-notebook*-1.26}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2268}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2990}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2990}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1753}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Duration}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Energy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Camera buffer read} & 8ms & 0.08mJ & System API \\
\textbf{JPEG decode (CPU)} & 15ms & 1.5mJ & Single-threaded \\
\textbf{Resize + Normalize} & 5ms & 0.4mJ & CPU preprocessing \\
\textbf{NPU inference} & 12ms & 0.8mJ & 82\% utilization \\
\textbf{Post-process + UI} & 5ms & 0.2mJ & Result rendering \\
\textbf{Total} & \textbf{45ms} & \textbf{3.0mJ} & 22 FPS sustained \\
\end{longtable}

\textbf{Key metrics for ML node serving:}

\begin{itemize}
\tightlist
\item
  \textbf{Energy per inference}: 3.0mJ enables \textasciitilde9,000
  inferences per 10Wh battery (typical smartphone)
\item
  \textbf{Thermal budget}: At 3.0mJ/45ms = 67mW sustained, indefinite
  operation without throttling
\item
  \textbf{NPU vs CPU tradeoff}: CPU fallback uses 4.2mJ (1.4× energy) at
  85ms (1.9× latency)
\item
  \textbf{Memory footprint}: 150MB peak (model + activations), competing
  with app memory
\end{itemize}

\textbf{Critical insight}: Even at batch size 1, the mobile NPU achieves
82\% utilization because its compute capacity matches single-image
workloads. This differs from datacenter GPUs, which achieve only 15\%
utilization at batch size 1 because their massive parallelism requires
larger batches to saturate.

\end{fbxSimple}

\subsubsection{Mobile Serving
Constraints}\label{sec-model-serving-systems-mobile-serving-constraints-eb68}

Unlike cloud serving where cost dominates, mobile serving faces three
related constraints that shape optimization strategy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Energy Budget}: Each inference depletes battery. A photo app
  running continuous inference at 22 FPS drains 240mW, acceptable for
  active use but problematic for background processing. The optimization
  target shifts from throughput to energy-per-inference.
\item
  \textbf{Thermal Throttling}: Sustained high-power operation triggers
  thermal management. When the SoC reaches thermal limits (typically
  45°C junction), the OS reduces NPU frequency by 30-50\%, degrading
  both latency and throughput. Bursty workloads that allow cooling
  between bursts outperform sustained maximum throughput.
\item
  \textbf{Memory Constraints}: Mobile devices share limited RAM between
  applications. A model consuming 500MB may be evicted during background
  operation, requiring reload (cold start) that adds 200-500ms latency.
  Even a 150MB footprint becomes problematic when the model must coexist
  with other app components. Memory-efficient quantization directly
  improves user experience through faster model restoration, and
  memory-mapped model loading
  (Section~\ref{sec-model-serving-systems-loading-strategies-eb38})
  helps further by loading pages on demand rather than requiring the
  full model in memory.
\end{enumerate}

These constraints make mobile serving optimization fundamentally
different from cloud optimization. The goal is not maximum throughput
but \textbf{sustainable performance}, maintaining acceptable latency
without thermal throttling or excessive battery drain.

Table~\ref{tbl-traffic-patterns-summary} maps the four MLPerf scenarios
to their deployment contexts and optimal batching strategies, providing
a decision framework for serving system design.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2417}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Traffic Patterns and Batching Strategies}: The four
MLPerf inference scenarios map to distinct deployment contexts. Server
traffic (cloud APIs) uses dynamic batching with timeout; MultiStream
(autonomous driving) uses synchronized sensor fusion; SingleStream
(mobile) processes requests individually; Offline (batch processing)
maximizes batch size for
throughput.}\label{tbl-traffic-patterns-summary}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Focus}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Focus}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Server} & Cloud APIs, web services & Dynamic batching with
timeout & Window tuning, utilization-latency curve \\
\textbf{MultiStream} & Autonomous driving, video analytics &
Synchronized sensor fusion & Jitter handling, deadline guarantees \\
\textbf{SingleStream} & Mobile apps, embedded devices & No batching
(batch=1) & Preprocessing, power efficiency \\
\textbf{Offline} & Batch processing, data pipelines & Maximum batch size
& Throughput, hardware utilization \\
\end{longtable}

\section{LLM Serving}\label{sec-model-serving-systems-llm-serving-b8bf}

The traffic patterns and batching strategies examined in the previous
section share a common assumption: models produce a single output per
request, whether a classification label, a bounding box, or an embedding
vector. Large language models break this assumption, generating tokens
incrementally over hundreds or thousands of iterations and creating a
different latency profile. The p50, p95, and p99 metrics that govern
classification serving apply differently when a single request takes 2
to 3 seconds to complete but must feel responsive throughout. While the
foundational principles of queuing theory, batching tradeoffs, and
latency budgets apply universally, LLMs require additional metrics,
different optimization strategies, and unique memory management
techniques.

\subsection{Performance Metrics: TTFT and
TPOT}\label{sec-model-serving-systems-performance-metrics-ttft-tpot-b009}

Generative models produce a stream of tokens rather than a single output
tensor. This streaming nature requires dedicated \emph{LLM performance
metrics} that reflect the internal state transition from ``prefill''
(processing input) to ``decode'' (generating output). The two key
measures are \emph{Time to First Token (TTFT)} and \emph{Time Per Output
Token (TPOT)}, which capture responsiveness and fluidity respectively.

\phantomsection\label{callout-definitionux2a-1.27}
\begin{fbxSimple}{callout-definition}{Definition:}{LLM Performance Metrics}
\phantomsection\label{callout-definition*-1.27}
\textbf{\emph{Time to First Token (TTFT)}} measures latency from request
to first output token, governed by the compute-bound \textbf{Prefill
Phase} (processing the full prompt)
(\citeproc{ref-pope2023efficiently}{Kim 2025}). \textbf{Time Per Output
Token (TPOT)} measures latency of each subsequent token, governed by the
memory-bandwidth-bound \textbf{Decode Phase} (autoregressive KV cache
lookups). This decomposition isolates the distinct hardware bottlenecks
(\textbf{Compute} versus \textbf{Memory Bandwidth}), enabling targeted
optimization of each phase.

\end{fbxSimple}

These two metrics capture fundamentally different user experience
aspects. A fast TTFT provides immediate responsiveness (the system
starts answering quickly), while a fast TPOT provides fluid generation
(the answer streams smoothly). Production systems must optimize both,
typically with different techniques since they are governed by different
hardware constraints. Translating these metrics into concrete \emph{LLM
serving latency targets} grounds the discussion in production reality.

\phantomsection\label{callout-lighthouseux2a-1.28}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{LLM Serving Latency Targets}
\phantomsection\label{callout-lighthouse*-1.28}

A production-grade LLM service typically targets the following SLOs:

\begin{itemize}
\tightlist
\item
  \textbf{TTFT}: \textless{} 500 ms (for a 1000-token prompt)
\item
  \textbf{TPOT}: \textless{} 50 ms (equivalent to \textasciitilde20
  tokens/second, faster than human reading speed)
\item
  \textbf{Throughput}: \textgreater{} 1000 tokens/second aggregate
  across all users
\end{itemize}

\end{fbxSimple}

\subsection{Decoding
Strategies}\label{sec-model-serving-systems-decoding-strategies-afe8}

Generative models require decoding strategies that trade off quality,
diversity, and latency. The choice of decoding strategy can dramatically
affect both output quality and computational cost.

\textbf{Greedy decoding} selects the highest-probability token at each
step. Fast but often produces repetitive, low-quality outputs because it
cannot recover from early mistakes.

\textbf{Beam search} maintains multiple candidate sequences, selecting
the highest-scoring complete sequence. Produces higher-quality outputs
but multiplies computation by the beam width.

\textbf{Sampling} with temperature, top-k, and top-p parameters
introduces randomness for diversity
(\citeproc{ref-holtzman2020curious}{Holtzman et al. 2020}). Temperature
scales logits before softmax. Top-k limits sampling to the k
highest-probability tokens. Top-p, also called nucleus sampling, limits
sampling to tokens comprising probability mass p.

The choice presents latency tradeoffs
(\citeproc{ref-meister2020beam}{Meister, Cotterell, and Vieira 2020}).
Beam search with width 5 takes roughly 5× the compute of greedy
decoding. Sampling adds minimal overhead but requires careful parameter
tuning to balance quality and coherence.

\textbf{Streaming Responses.} Rather than waiting for complete
generation, production LLM systems return tokens as they are produced.
This improves perceived latency since users see output beginning
quickly, but requires infrastructure support for chunked HTTP responses
and client-side incremental rendering. Streaming changes the latency
profile: TTFT determines when output starts appearing, while TPOT
determines the perceived generation speed.

\subsection{Memory and KV
Cache}\label{sec-model-serving-systems-memory-kv-cache-d1ea}

Generative inference requires managing the \textbf{KV Cache}, a stateful
memory structure that grows with sequence length. Unlike traditional
models where memory usage is constant per batch, LLM memory usage is
dynamic:

\begin{itemize}
\tightlist
\item
  \textbf{State Accumulation}: Each generated token adds to the context
  window, consuming additional GPU memory.
\item
  \textbf{Fragmentation}: Variable-length sequences can lead to memory
  fragmentation if not managed explicitly.
\end{itemize}

The continuous batching and PagedAttention techniques covered in
Section~\ref{sec-model-serving-systems-continuous-batching-8bb6} address
these challenges. Advanced techniques including prefix caching and
speculative decoding are covered in specialized coverage of large-scale
systems.

The computational intensity of managing KV caches across concurrent
requests raises a broader question: \emph{what} is the energy cost of
each token generated? Translating these hardware demands into energy and
carbon metrics makes the environmental impact of LLM serving concrete.

\phantomsection\label{notebook-carbon-chat}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Carbon Cost of a Chat}
\phantomsection\label{notebook-carbon-chat}
\textbf{Joules per Token: The Green Metric}: As LLMs scale, energy
efficiency becomes a first-class operational metric alongside latency.
For an H100 GPU (700W TDP), we can quantify the energy footprint of
serving:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Throughput}: 114 concurrent requests × 7.5 tokens/sec/req
  \(\approx\) \textbf{855 tokens/sec}.
\item
  \textbf{Power}: 700W (GPU) + 300W (Host/Overhead) = \textbf{1000W}.
\item
  \textbf{Energy per Token}:

  \[\frac{1000 \text{ Joules/sec}}{855 \text{ tokens/sec}} \approx \mathbf{1\.17 \text{ Joules/token}}\]
\end{enumerate}

\textbf{The Systems Conclusion}: A typical 500-token response consumes
\(\approx \mathbf{585 \text{ Joules}}\).

\begin{itemize}
\tightlist
\item
  For comparison, charging a smartphone consumes \(\approx 40\,000\)
  Joules.
\item
  Boiling a cup of water consumes \(\approx 100\,000\) Joules.
\end{itemize}

\textbf{The Engineering Lever}: The primary way to reduce Joules/Token
is to \textbf{increase hardware utilization}. If the GPU sits at 10\%
utilization due to poor batching, the ``Idle Power'' is still
\textasciitilde300W, causing the energy-per-token to skyrocket to
\textbf{\textgreater10 Joules}. MLOps is not just about speed; it is
about sustainability through efficiency.

\end{fbxSimple}

\section{Inference Runtime
Selection}\label{sec-model-serving-systems-inference-runtime-selection-5eef}

The batching strategies and LLM-specific techniques examined in
preceding sections determine \emph{how} requests are grouped and
processed. These strategies, however, assume an underlying execution
engine that actually runs the model computations. The execution
environment directly affects whether the latency budgets established
earlier are achievable. The inference runtime, the software layer that
orchestrates tensor operations and manages hardware resources, can vary
by an order of magnitude in performance for identical models. Choosing
appropriately requires understanding the tradeoffs between
framework-native serving, general-purpose optimization, and specialized
inference engines.

\subsection{Runtime Ecosystem and
Configuration}\label{sec-model-serving-systems-frameworknative-serving-da62}

PyTorch and TensorFlow models can serve directly using their native
runtimes. This approach maximizes compatibility (any model that trains
will serve) and simplifies the deployment pipeline (no export or
conversion step). However, framework runtimes include training
functionality that adds overhead, and default execution paths may not
exploit hardware-specific optimizations.

TorchScript and TensorFlow SavedModel formats enable ahead-of-time
compilation and graph optimization, improving over eager execution while
maintaining framework compatibility. These formats represent the first
step toward deployment optimization without abandoning the familiar
framework ecosystem.

\textbf{General-Purpose Optimization.} ONNX Runtime provides a
hardware-agnostic optimization layer
(\citeproc{ref-onnxruntime2024}{Microsoft 2024}). Models export to ONNX
format, then ONNX Runtime applies graph optimizations and selects
execution providers for the target hardware. This enables single-format
deployment across CPUs, GPUs, and specialized accelerators.

\hyperref[sec-model-serving-systems-specialized-inference-engines-1924]{}
\textbf{Specialized Inference Engines.}
TensorRT\sidenote{\textbf{TensorRT}: NVIDIA's inference optimization SDK
that applies layer fusion, kernel auto-tuning, and precision calibration
to neural networks. Unlike framework-native runtimes that preserve
training-time graph structure, TensorRT rebuilds the computation graph
for the specific target GPU during a build phase. This GPU-specific
compilation means TensorRT engines are not portable across GPU
architectures, requiring separate builds for V100, A100, and H100
deployments. The build phase can take minutes but produces engines that
often achieve 2-5x speedup over PyTorch. } (NVIDIA GPUs), OpenVINO
(Intel hardware), and similar engines optimize specifically for their
target hardware (\citeproc{ref-nvidia2024tensorrt}{Khan, Yoon, and
Bhandarkar 2025}; \citeproc{ref-chen2018tvm}{0001 et al. 2018}). They
apply aggressive optimizations that framework-native runtimes cannot
safely perform:

\textbf{Layer fusion} combines multiple sequential operations into a
single GPU kernel. Consider a common pattern: convolution → batch
normalization → ReLU activation. Without fusion, this requires three
kernel launches, three round-trips to GPU memory (write conv output,
read for batchnorm, write batchnorm output, read for ReLU), and three
sets of intermediate tensors. Fusion combines all three into one kernel
that reads inputs once, computes the combined result in registers, and
writes final outputs once. This eliminates kernel launch overhead
(15-60μs saved per fusion) and reduces memory traffic by 2-3×. TensorRT
automatically detects and fuses common patterns; a typical ResNet-50
reduces from \textasciitilde50 kernels to \textasciitilde15 after
fusion.

\textbf{Kernel auto-tuning} selects the fastest algorithm for each
operation on the specific GPU. A single convolution can be implemented
using dozens of algorithms (direct, FFT-based, Winograd, various tiling
strategies), each optimal for different input sizes and GPU
architectures. Auto-tuning benchmarks each candidate and caches the
winner, trading compilation time for runtime performance.

These optimizations typically achieve 2-5x speedup over framework-native
serving but require explicit export and may not support all operations.
A \emph{runtime comparison} on a standard model quantifies these gains
across the optimization spectrum.

\phantomsection\label{callout-notebookux2a-1.30}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50: Runtime Comparison}
\phantomsection\label{callout-notebook*-1.30}
Performance comparison for ResNet-50 inference on V100 GPU (batch size
1):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1339}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3036}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3393}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2232}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Runtime}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Speedup}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
PyTorch (eager) & 8.5ms & 1.0× & Baseline, no optimization \\
TorchScript & 6.2ms & 1.4× & JIT compilation \\
ONNX Runtime & 5.1ms & 1.7× & Cross-platform \\
TensorRT FP32 & 2.8ms & 3.0× & NVIDIA-specific \\
TensorRT FP16 & 1.4ms & 6.1× & Tensor Core acceleration \\
TensorRT INT8 & 0.9ms & 9.4× & Requires calibration \\
\end{longtable}

\textbf{Key insight}: The 9.4× speedup from TensorRT INT8 comes at the
cost of: (1) quantization calibration data, (2) potential accuracy loss
(\textless1\% for ResNet-50), and (3) NVIDIA-specific deployment.

\end{fbxSimple}

The optimization-compatibility tradeoff is inherent. More aggressive
optimization yields better performance but increases deployment
complexity and may introduce numerical differences from training. The
choice depends on latency requirements, deployment constraints, and
available engineering resources.

\textbf{Runtime Configuration.} Beyond runtime selection, configuration
choices impact serving performance including thread pools that control
parallelism for CPU inference, memory allocation strategies that choose
between pre-allocating buffers versus dynamic allocation, execution
providers that select and prioritize hardware backends, and graph
optimization level that trades compilation time for runtime performance.
Production deployments require experimentation to find optimal
configurations for specific models and hardware combinations. A
systematic approach tests key parameters and measures their impact on
latency distributions.

\subsection{Precision Selection for
Serving}\label{sec-model-serving-systems-precision-selection-serving-55ba}

Numerical precision directly trades accuracy for throughput, connecting
to the quantization techniques covered in
\textbf{?@sec-model-compression}. While \textbf{?@sec-model-compression}
focuses on training-time quantization, serving introduces additional
considerations including calibration requirements, layer sensitivity,
and dynamic precision selection.

\textbf{Precision-Throughput Relationship.} For memory-bandwidth-bound
operations, reducing precision proportionally increases throughput by
reducing data movement. Equation~\ref{eq-precision-throughput}
quantifies the theoretical maximum speedup from precision reduction:

\begin{equation}\phantomsection\label{eq-precision-throughput}{\frac{\text{Throughput}_{\text{INT8}}}{\text{Throughput}_{\text{FP32}}} = \frac{32}{8} = 4\times \text{ (theoretical maximum)}}\end{equation}

In practice, GPU compute pipelines and Tensor Core alignment
requirements limit achieved speedup to 2.5-3.5x for INT8 versus FP32.
Tensor Cores require specific alignment: INT8 operations need tensor
dimensions divisible by 16, while FP16 requires divisibility by 8.
\textbf{?@sec-ai-acceleration} provides the detailed Tensor Core
architecture that explains these alignment constraints. The
\emph{precision tradeoffs} for a standard vision model illustrate how
these theoretical limits manifest in practice.

\phantomsection\label{callout-notebookux2a-1.31}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50: Precision Tradeoffs on V100}
\phantomsection\label{callout-notebook*-1.31}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1687}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1325}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1205}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1446}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2530}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1807}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Tensor Core Util.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calibration}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{FP32} & 2.8ms & 98MB & 76.13\% & 0\% & None \\
\textbf{FP16} & 1.4ms & 49MB & 76.13\% & 85\% & None \\
\textbf{INT8 (PTQ)} & 0.9ms & 25MB & 75.80\% & 92\% & 1,000 samples \\
\textbf{INT8 (QAT)} & 0.9ms & 25MB & 76.05\% & 92\% & Full retraining \\
\end{longtable}

\textbf{Key observations:}

\begin{itemize}
\tightlist
\item
  INT8 achieves 3.1× speedup but loses 0.33\% accuracy with
  post-training quantization (PTQ)
\item
  Quantization-aware training (QAT) recovers most accuracy but requires
  retraining
\item
  FP16 provides 2× speedup with no accuracy loss for most models
\end{itemize}

\end{fbxSimple}

\textbf{Layer Sensitivity.} Not all layers tolerate reduced precision
equally. Equation~\ref{eq-quant-error} captures how quantization error
for a layer scales with weight magnitude and gradient sensitivity:

\begin{equation}\phantomsection\label{eq-quant-error}{\epsilon_{\text{quant}} \propto \alpha \cdot \|W\|_2 \cdot 2^{-b}}\end{equation}

where \(\alpha\) is a layer-specific sensitivity coefficient,
\(\|W\|_2\) is the weight L2 norm, and \(b\) is the bit width. This
explains observed patterns where first convolutional layers with high
gradients and large sensitivity coefficients are precision-sensitive and
often kept at FP16, middle layers with stable gradients and low
sensitivity coefficients tolerate INT8 well, and final classification
layers with small weights but high task sensitivity benefit from FP16 or
higher precision.

\textbf{Calibration Requirements.} Post-training quantization requires a
calibration dataset to determine optimal scale factors for INT8
conversion. Production experience shows that calibration data must be
representative of actual serving traffic, not just training data. Using
ImageNet validation images to calibrate a model serving wildlife camera
images resulted in 3.2\% accuracy degradation in one production system.

\textbf{Dynamic Precision Selection.} Advanced serving systems select
precision per request based on runtime conditions. If the system is
ahead of latency SLO, it uses higher precision for better accuracy. For
low-confidence INT8 results, it recomputes at FP16. Different customer
tiers may receive different precision levels. This pattern enables
adaptive quality-latency tradeoffs while maximizing throughput during
normal operation.

The precision decision has direct infrastructure consequences: INT8
inference achieves roughly 3x higher throughput than FP32, meaning a
workload requiring 30 GPUs at FP32 needs only 10 at INT8. This 3x
reduction in hardware translates directly to a 3x reduction in operating
costs. The connection between model-level optimization and
infrastructure economics is why precision selection cannot be treated as
purely a model concern.

\section{Node-Level Optimization \&
Profiling}\label{sec-model-serving-systems-node-level-optimization-profiling}

Beyond selecting a runtime and precision, maximizing the efficiency of a
single ML node requires examining in detail \emph{how} the hardware
executes the model. This section explores optimizations that occur at
the boundaries of software and silicon: compiling the computation graph,
exploiting CPU capabilities when GPUs are absent, minimizing the time to
get bytes from disk to memory, and visualizing exactly \emph{where}
every microsecond goes.

\subsection{Runtime Graph
Compilation}\label{sec-model-serving-systems-runtime-graph-compilation}

We introduced inference engines like TensorRT in
Section~\ref{sec-model-serving-systems-inference-runtime-selection-5eef},
but \emph{how} do they achieve 2-5x speedups? The answer lies in
\textbf{Graph Compilation}. Unlike training, where the computation graph
is dynamic and mutable, serving graphs are static. This allows compilers
to perform aggressive optimizations that would be unsafe or too slow
during training.

\textbf{Operator Fusion}: The most potent optimization. As discussed in
\textbf{?@sec-ai-acceleration}, memory bandwidth often limits
performance more than compute. Fusion collapses multiple operations
(e.g., \texttt{Conv2D} -\textgreater{} \texttt{BiasAdd} -\textgreater{}
\texttt{ReLU}) into a single kernel launch. This keeps intermediate data
in the GPU's fast L1/L2 cache or registers, avoiding round-trips to
global memory (VRAM).

\textbf{Constant Folding}: Parts of the graph that depend only on model
weights (which are constant during serving) can be pre-computed at
compile time. For example, if a model contains
\texttt{x\ *\ (sqrt(2)\ /\ 2)}, the compiler replaces the division and
square root with a single multiplication by \texttt{0.707...}.

\textbf{Memory Planning}: Since the graph structure is known, the
compiler can pre-calculate the exact memory offsets for every tensor.
This leads to the fundamental architectural choice of \emph{JIT vs.~AOT
compilation}.

\phantomsection\label{callout-notebookux2a-1.32}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{JIT vs. AOT Compilation}
\phantomsection\label{callout-notebook*-1.32}

\begin{itemize}
\tightlist
\item
  \textbf{Just-In-Time (JIT)}: Compiles the graph the first time it is
  run (e.g., \texttt{torch.compile}).

  \begin{itemize}
  \tightlist
  \item
    \emph{Pros}: Optimizes for the specific input shapes seen at
    runtime.
  \item
    \emph{Cons}: First request pays a ``compilation penalty'' (latency
    spike).
  \end{itemize}
\item
  \textbf{Ahead-of-Time (AOT)}: Compiles the graph before deployment
  (e.g., \texttt{torch.export}, TensorRT \texttt{trtexec}).

  \begin{itemize}
  \tightlist
  \item
    \emph{Pros}: Zero compilation latency at startup; guarantees a fixed
    graph.
  \item
    \emph{Cons}: Must handle all dynamic shapes explicitly or compile
    multiple profiles.
  \end{itemize}
\end{itemize}

\end{fbxSimple}

\subsection{CPU Inference
Optimization}\label{sec-model-serving-systems-cpu-inference-optimization}

While GPUs dominate the narrative, CPUs remain the workhorse for a vast
number of inference workloads, particularly for smaller models,
latency-insensitive batch jobs, or cost-constrained environments.
Optimizing for the CPU requires a different mindset.

\textbf{SIMD and Vectorization}: Modern CPUs (Intel Xeon, AMD EPYC) pack
powerful vector units (AVX-512, AMX). Standard Python loops cannot use
these. Specialized runtimes like \textbf{OpenVINO} or \textbf{Intel
Extension for PyTorch (IPEX)} map neural network operators directly to
these vector instructions, achieving order-of-magnitude speedups over
vanilla implementations.

\textbf{Thread Pinning \& NUMA}: On multi-socket servers, accessing
memory attached to a different CPU socket (NUMA) adds significant
latency. Inference servers must be ``NUMA-aware,'' pinning threads to
specific cores and ensuring that memory allocations remain local to
those cores.

\textbf{The ``Small Batch'' Advantage}: CPUs often outperform GPUs at
batch size 1 for small models. The overhead of launching a GPU kernel
(\textasciitilde10\(\mu\)s) and transferring data
(\textasciitilde50\(\mu\)s) can exceed the compute time for a tiny dense
layer. For models under 50MB serving single requests, a well-optimized
CPU runtime often delivers lower latency than a GPU.

\subsection{Fast Model
Loading}\label{sec-model-serving-systems-fast-model-loading}

In autoscaling systems, the time to spin up a new node is critical. A
major component of ``Cold Start''
(Section~\ref{sec-model-serving-systems-model-loading-initialization-cc5a})
is simply reading the model weights from disk into memory.

\textbf{The Pickle Problem}: The standard PyTorch \texttt{torch.load()}
uses Python's \texttt{pickle} format. This is inefficient because it
requires the CPU to unpickle objects one by one, copy them into memory,
and then often copy them \emph{again} to the GPU.

\textbf{Zero-Copy with \texttt{mmap}}: Memory mapping allows the OS to
map a file directly into the process's virtual address space. The data
is effectively ``loaded'' only when accessed, and the OS handles the
transfer from disk to RAM efficiently.

\textbf{Safetensors}: A modern format designed specifically for fast
loading. It stores tensors as raw bytes with a minimal JSON header. This
allows for \textbf{zero-copy} loading: the raw bytes on disk are mapped
directly into the tensor's memory buffer.

\phantomsection\label{callout-exampleux2a-1.33}
\begin{fbxSimple}{callout-example}{Example:}{Loading Speed: Safetensors vs. Pickle}
\phantomsection\label{callout-example*-1.33}
Loading a 5GB Stable Diffusion model:

\begin{itemize}
\tightlist
\item
  \textbf{Pickle (\texttt{torch.load})}: \textasciitilde15 seconds. High
  CPU usage.
\item
  \textbf{Safetensors}: \textasciitilde0.5 seconds. Near-zero CPU usage.
\end{itemize}

By using \texttt{mmap} and formats like \texttt{safetensors}, loading
speed becomes limited only by the disk's read speed (e.g., 3GB/s for
NVMe), rather than CPU parsing overhead.

\end{fbxSimple}

\subsection{Profiling the Serving
Node}\label{sec-model-serving-systems-profiling-serving-node}

Optimization without measurement is guesswork. To truly master the ML
node, you must visualize the execution flow.

\textbf{The Timeline View}: Tools like \textbf{PyTorch Profiler} or
NVIDIA \textbf{Nsight Systems (nsys)} generate a timeline trace. This
visualization reveals the exact sequence of events on the CPU and GPU.

\textbf{What to Look For:} 1. \textbf{Gaps in the GPU Timeline}: If the
GPU bar has empty spaces, the GPU is idle. This usually means the GPU is
waiting for the CPU (preprocessing bottleneck) or disk (data loading).
2. \textbf{Kernel Launch Overhead}: If you see thousands of tiny slivers
on the GPU timeline, your model is launching too many small kernels.
This is a prime candidate for \textbf{Operator Fusion}. 3.
\textbf{Host-to-Device Transfers}: Look for \texttt{MemcpyHtoD} (Host to
Device) blocks. Are they overlapping with computation, or blocking it?

\phantomsection\label{callout-exampleux2a-1.34}
\begin{fbxSimple}{callout-example}{Example:}{The Profiling Loop}
\phantomsection\label{callout-example*-1.34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Capture}: Run a warmup, then capture a trace of 10-50
  requests.
\item
  \textbf{Visualize}: Open the trace in a viewer (Chrome Tracing,
  Nsight).
\item
  \textbf{Identify}: Find the largest gap or the longest block.
\item
  \textbf{Optimize}: Apply a specific fix (e.g., fusion, pinning).
\item
  \textbf{Verify}: Re-capture and confirm the gap is gone.
\end{enumerate}

\end{fbxSimple}

\textbf{Optimization Technique Impact Matrix}

To guide optimization efforts, Table~\ref{tbl-optimization-impact}
summarizes the key techniques available at the node level, their primary
targets, and expected returns.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2100}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1900}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2400}}@{}}
\caption{\textbf{Node-Level Optimization Impact}: A decision matrix for
selecting optimization techniques. High-impact techniques like
quantization often carry higher implementation costs (calibration data
requirements), while architectural changes like zero-copy loading offer
dramatic gains for specific metrics (startup time) with low
effort.}\label{tbl-optimization-impact}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Target Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Gain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implement. Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Target Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Gain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implement. Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Operator Fusion} & Latency \& Throughput & 2-5\(\times\) &
Medium (Compiler) & Memory-bound layers \\
\textbf{INT8 Quantization} & Throughput & 3-4\(\times\) & High
(Calibration) & Inference-heavy nodes \\
\textbf{Graph Compilation} & Latency & 1.5-3\(\times\) & Low (One-line)
& Static graph models \\
\textbf{Zero-Copy Loading} & Startup Time & 10-50\(\times\) & Low (File
format) & Autoscaling / Cold Start \\
\textbf{CPU Pinning} & Tail Latency (P99) & 20-50\% reduction & Low
(Config) & Latency-critical apps \\
\end{longtable}

This hierarchy of impact guides where to invest engineering effort. Use
the following checklist to prioritize your optimization strategy.

\phantomsection\label{callout-checkpointux2a-1.35}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{The Optimization Hierarchy}
\phantomsection\label{callout-checkpoint*-1.35}

Optimizing inference requires a layered approach.

\textbf{The Stack}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{System Level}: Have you minimized network round trips and
  serialization overhead? (gRPC, persistent connections).
\item[$\square$]
  \textbf{Application Level}: Are you batching requests effectively?
  (Dynamic batching).
\item[$\square$]
  \textbf{Model Level}: Is the model compiled for the target hardware?
  (TensorRT, ONNX Runtime).
\item[$\square$]
  \textbf{Kernel Level}: Are operations fused to minimize memory
  bandwidth?
\end{itemize}

\end{fbxSimple}

\section{Economics and Capacity
Planning}\label{sec-model-serving-systems-economics-capacity-planning-3e7e}

The runtime selection, precision tuning, and node-level optimizations
examined in the preceding sections collectively determine the
fundamental unit of serving physics: the performance per inference.
Production deployment requires translating these technical metrics into
infrastructure decisions. Serving costs scale with request volume,
unlike training costs that scale with dataset size and model complexity
(\citeproc{ref-zhang2019mark}{Zhang et al. 2019}). Cost structure
analysis enables decisions that balance performance requirements against
budget constraints.

\textbf{Cost Per Inference.} Total serving cost decomposes into several
components including compute time for GPU or CPU per inference, memory
for accelerator memory required to hold model and activations, data
transfer for network bandwidth for request and response payloads, and
orchestration overhead for container runtime, load balancing, and
monitoring. For GPU inference, compute time dominates when utilization
is high. When utilization is low, memory cost dominates because the GPU
is reserved but idle. We can apply this framework to a \emph{ResNet-50
cost analysis}.

\phantomsection\label{callout-notebookux2a-1.36}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{ResNet-50: Cost Analysis}
\phantomsection\label{callout-notebook*-1.36}
Consider serving ResNet-50 on AWS infrastructure (US-East region,
on-demand pricing as of 2024):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3378}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1757}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1892}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2973}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Instance Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost/Hour}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost per 1M Images}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{c5.xlarge (CPU)} & \$0.17 & 50 img/s & \$0.94 \\
\textbf{g4dn.xlarge (T4 GPU)} & \$0.53 & 400 img/s & \$0.37 \\
\textbf{p3.2xlarge (V100 GPU)} & \$3.06 & 1,200 img/s & \$0.71 \\
\end{longtable}

\textbf{Key insight}: The T4 GPU instance achieves the lowest cost per
inference despite higher hourly cost, because GPU throughput
dramatically exceeds CPU throughput. The V100 is only cost-effective at
very high sustained traffic where its higher throughput justifies the 6x
price increase. Note that cloud pricing varies by region and changes
over time; consult current pricing for production planning.

\end{fbxSimple}

\subsection{GPU vs CPU
Economics}\label{sec-model-serving-systems-gpu-vs-cpu-economics-eb06}

GPUs provide significant speedup for parallel operations but cost more
per hour (\citeproc{ref-wu2019machine}{Wu et al. 2019}). The crossover
point depends on model characteristics and latency requirements.

CPU inference makes economic sense when models are small with few
parameters and simple operations, latency requirements are relaxed with
hundreds of milliseconds acceptable, request volume is low or highly
variable, and models use operations that do not parallelize well. GPU
inference makes economic sense when models are large with
parallel-friendly operations, latency requirements are strict at tens of
milliseconds, request volume is high and consistent, and batching can
achieve high utilization.

\textbf{Scaling Responsiveness.} Beyond steady-state costs, startup time
affects scaling economics. CPU instances typically start in 30 to 60
seconds while GPU instances take 2 to 5 minutes including driver
initialization, model loading, and warmup. For variable traffic
patterns, this startup latency can be more important than cost per
inference. If traffic spikes arrive faster than GPU instances can scale,
latency SLOs will be violated despite having sufficient eventual
capacity.

This asymmetry suggests different scaling strategies where CPU instances
enable reactive scaling by responding to current demand while GPU
instances often require predictive scaling by provisioning based on
anticipated demand. For bursty workloads, a hybrid approach uses
always-on GPU capacity for baseline load plus CPU overflow capacity for
spikes, trading higher per-inference cost during spikes for better
responsiveness.

\subsection{Capacity
Planning}\label{sec-model-serving-systems-capacity-planning-96a3}

The GPU versus CPU decision establishes the cost per inference, but
determining how much infrastructure to provision requires combining cost
analysis with the queuing theory foundations from
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}.
Capacity planning translates latency requirements and traffic
projections into infrastructure specifications. Key inputs include
traffic patterns such as peak request rate, daily and weekly cycles, and
growth projections, latency SLOs including p50, p95, and p99 targets,
and model characteristics such as inference time distribution at various
batch sizes. From these inputs, queuing theory determines required
capacity (\citeproc{ref-harchol2013performance}{Harchol-Balter 2013}).
The equations developed in
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
provide the mathematical foundation where Equation~\ref{eq-mm1-wait}
shows how latency scales with utilization, while
Equation~\ref{eq-p99-latency} enables calculating p99 latency for
capacity planning.

The relationship between utilization and latency is nonlinear as shown
in the utilization-latency table. At 70 percent utilization, p99 latency
is approximately fifteen times service time. At 90 percent utilization,
it reaches approximately 46 times service time. This nonlinearity
explains why systems that seem healthy with low average latency can
suddenly violate SLOs when traffic increases modestly.

The worked example in
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
demonstrates the complete capacity planning process by starting from a
50ms p99 SLO and 5,000 QPS target, deriving the safe utilization
threshold of 72 percent, calculating required service rate of 6,944 QPS,
and determining GPU count with headroom of 10 V100s. Production systems
typically provision for peak load plus 30 percent headroom, using
auto-scaling to reduce costs during low-traffic periods while meeting
latency objectives during peaks.

\subsection{Production Case Study: Serving
Llama-3-8B}\label{sec-model-serving-systems-production-case-study-serving-llama38b-0499}

To apply the principles of latency budgeting, memory management, and
hardware efficiency, we analyze the production profile of a modern Large
Language Model (LLM) serving workload. This case study demonstrates how
physical constraints (memory bandwidth and PCIe capacity) translate
directly into service-level metrics and unit economics.

Figure~\ref{fig-kv-cache-growth} visualizes the memory pressure that
explains \emph{why} long-context serving is memory-bound even on H100s.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/serving/serving_files/figure-pdf/fig-kv-cache-growth-output-1.pdf}}

}

\caption{\label{fig-kv-cache-growth}\textbf{The KV-Cache Explosion}:
Memory usage vs.~Context Length for a 70B parameter model. The linear
growth of the Key-Value cache (storing attention history) quickly
consumes available GPU memory (red dashed line). For batch size 32
(purple), the system hits the `OOM Zone' at just 8k context length,
forcing a trade-off between batch size (throughput) and context window
(capability).}

\end{figure}%

The linear growth of the KV cache with sequence length forces a hard
trade-off: to support longer contexts (32k+), we must reduce batch size,
which in turn kills throughput efficiency.

\subsubsection{Workload
Profile}\label{sec-model-serving-systems-workload-profile-a380}

\begin{itemize}
\tightlist
\item
  \textbf{Model}: Llama-3-8B (quantized to 4-bit AWQ).
\item
  \textbf{Hardware}: 1\(\times\) NVIDIA H100 SXM5 GPU (80 GB HBM3, 3.35
  TB/s bandwidth).
\item
  \textbf{Request Characteristics}: 1,000-token input prompt (Prefill),
  256-token generated response (Decode).
\item
  \textbf{Target SLOs}: TTFT \(<\) 200 ms, TPOT \(<\) 20 ms.
\end{itemize}

\subsubsection{Latency
Deconstruction}\label{sec-model-serving-systems-latency-deconstruction-217e}

The end-to-end request latency is governed by the two-phase execution
model of autoregressive transformers.

\textbf{1. Prefill Phase (Time to First Token)}

The model processes the 1,000-token prompt in parallel. On an H100, this
compute-bound operation achieves approximately 10,000 tokens per second.
*
\(T_{\text{prefill}} = \frac{1000 \text{ tokens}}{10000 \text{ tokens/s}} = 100 \text{ ms}\).
* Accounting for 20 ms of system overhead (network ingress,
tokenization), the \textbf{TTFT is 120 ms}, comfortably within the 200
ms SLO.

\textbf{2. Decode Phase (Time Per Output Token)}

The model generates 256 tokens sequentially. This phase is
memory-bandwidth bound; the system must read the entire 3.5 GB weight
tensor from VRAM to generate a single token.

\phantomsection\label{callout-perspectiveux2a-1.37}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Physics of Token Generation}
\phantomsection\label{callout-perspective*-1.37}
Recall the \textbf{Energy-Movement Invariant} from
\textbf{?@sec-data-engineering-ml}: moving a bit is 100--1,000× more
expensive than computing on it. In the \textbf{Decode Phase}, this law
determines the physical ``cost per word.''

\textbf{The Memory Wall for Generative AI}: Because the decode phase has
an arithmetic intensity of \(\approx 1\) FLOP/byte (we must read every
weight just to generate one token), performance is strictly limited by
memory bandwidth (\(BW\)), not compute.

\[ T_{\text{token}} \approx \frac{\text{Model Size (Bytes)}}{\text{Memory Bandwidth (Bytes/s)}} \]

\textbf{The Engineering Implication}: Every time you generate a token,
you are paying a massive ``energy tax'' to move the model's logic from
HBM into compute registers. For Llama-3-8B (3.5 GB int4), an A100 80GB
(2.0 TB/s HBM2e) generates tokens at \(\approx 1.7\) ms/token. Adding
more \emph{compute cores} yields \textbf{zero} latency improvement; only
faster memory (Physics) or smaller models (Algorithm) can speed up
generation.

\end{fbxSimple}

\begin{itemize}
\tightlist
\item
  \(T_{\text{token}}\) ≈ 3.5 GB / 3.35 TB/s ≈ 1 ms (theoretical limit).
\item
  Accounting for kernel launch overhead and attention computation,
  realized \(T_{\text{token}}\) is approximately 10 ms.
\item
  Total decode time: 256 tokens × 10 ms/token = 2.56 seconds.
\item
  \textbf{TPOT is 10 ms}, well within the 20 ms ``fluidity'' SLO.
\end{itemize}

\subsubsection{Memory \&
Throughput}\label{sec-model-serving-systems-memory-throughput-63dd}

With 4-bit weights occupying 3.5 GB, the remaining \textasciitilde76 GB
of VRAM is available for the \textbf{KV Cache}. Using
\textbf{PagedAttention}, we can allocate this memory with near-zero
fragmentation.

\begin{itemize}
\tightlist
\item
  Each token requires approximately 0.5 MB of KV cache (32 layers × 4096
  dim × 2 vectors × 2-byte precision).
\item
  Total cache capacity ≈ 72 GB / 0.5 MB/token ≈ 144,000 tokens.
\item
  At 1,256 tokens per request (input + output), the GPU can handle a
  \textbf{concurrent batch size of \textasciitilde114 requests}.
\end{itemize}

\subsubsection{Unit
Economics}\label{sec-model-serving-systems-unit-economics-b685}

For an H100 SXM5 instance at approximately \$3.00 per hour (specialized
cloud providers; hyperscaler rates vary from \$2-13 per hour as of
2024):

\begin{itemize}
\tightlist
\item
  Total tokens per hour: 114 batch × (3600 s/hr / 2.68 s/req) × 1,256
  tokens/req ≈ 192 million tokens/hour.
\item
  \textbf{Cost per million tokens}: \$3.00 / 192 ≈ \textbf{\$0.016}.
\end{itemize}

This analysis highlights that for LLMs, \textbf{memory capacity} (the
size of the KV cache) is the primary determinant of throughput and cost,
while \textbf{memory bandwidth} is the primary determinant of latency.

This case study applies the core principles developed throughout this
chapter: latency budgets decompose into prefill and decode phases,
queuing theory governs batch sizing and capacity planning, and hardware
constraints in the form of memory bandwidth and capacity determine
achievable performance and cost. The quantitative framework established
here enables principled engineering decisions, but only when applied
correctly. Common misconceptions cause even experienced engineers to
misapply these principles in practice.

\section{Fallacies and
Pitfalls}\label{sec-model-serving-systems-fallacies-pitfalls-336b}

Serving inverts training priorities in unexpected ways. Intuitions from
batch processing fail under latency constraints and variable load,
causing wasted effort, violated SLOs, and silent accuracy degradation in
production.

\paragraph*{\texorpdfstring{Fallacy: \emph{Reducing model inference
latency proportionally reduces user-perceived
latency.}}{Fallacy: Reducing model inference latency proportionally reduces user-perceived latency.}}\label{fallacy-reducing-model-inference-latency-proportionally-reduces-user-perceived-latency.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Reducing model inference
latency proportionally reduces user-perceived latency.}}

Engineers optimize model inference and expect proportional improvement
in request latency, but serving systems introduce latency sources absent
from offline benchmarks. Under load, queuing delay dominates:
Equation~\ref{eq-mm1-wait} shows that at 80 percent utilization with 5ms
service time, average wait time is 20ms before inference even begins.
Reducing inference from 5ms to 2ms changes service time but also shifts
utilization from 80 percent to 32 percent, reducing queuing wait from
20ms to 2.4ms, a 10× queuing improvement that dwarfs the 3ms inference
gain. This nonlinear interaction between inference speed and queuing
behavior means the \emph{system-level} speedup (25ms → 4.4ms, or 5.7×)
far exceeds the \emph{model-level} speedup (5ms → 2ms, or 2.5×).
Conversely, teams that reduce inference by only 20 percent at high
utilization see negligible user-facing improvement because queuing still
dominates. Serving optimization requires analyzing the complete latency
budget, including serialization, queuing, preprocessing, and
postprocessing, under realistic load conditions rather than profiling
inference latency in isolation.

\paragraph*{\texorpdfstring{Pitfall: \emph{Running serving
infrastructure at high utilization to maximize cost
efficiency.}}{Pitfall: Running serving infrastructure at high utilization to maximize cost efficiency.}}\label{pitfall-running-serving-infrastructure-at-high-utilization-to-maximize-cost-efficiency.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Running serving
infrastructure at high utilization to maximize cost efficiency.}}

Teams target 90 percent utilization to minimize idle capacity. In
production, latency degrades nonlinearly as utilization approaches
capacity. Equation~\ref{eq-mm1-wait} shows that at 90 percent
utilization, average wait time reaches 10× service time. Moving from 70
percent to 90 percent utilization cuts infrastructure costs by 22
percent but triples average latency. For a 5ms inference service, p99
latency jumps from 25ms to 50ms. Systems provisioned for average load
violate SLOs precisely when traffic increases during business-critical
periods. Production systems targeting 60 to 70 percent utilization at
peak load maintain the latency headroom needed to absorb traffic spikes.

\paragraph*{\texorpdfstring{Fallacy: \emph{Training accuracy guarantees
serving
accuracy.}}{Fallacy: Training accuracy guarantees serving accuracy.}}\label{fallacy-training-accuracy-guarantees-serving-accuracy.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Training accuracy
guarantees serving accuracy.}}

Engineers assume identical model weights preserve validation set
performance. In production, preprocessing differences silently shift
inputs outside the training distribution.
Section~\ref{sec-model-serving-systems-trainingserving-skew-7b99} shows
\emph{how} training-serving skew causes accuracy degradation despite
identical weights: PIL versus OpenCV resize interpolation differs
subtly, float64 versus float32 normalization produces different values,
or feature computation timing changes. A model achieving 95 percent
validation accuracy drops to 90 percent in production from these
preprocessing mismatches. Standard monitoring checking exceptions and
latency violations fails to detect this silent degradation. Production
systems require either identical preprocessing code for training and
serving, or statistical monitoring comparing input distributions to
catch drift before accuracy degrades.

\paragraph*{\texorpdfstring{Pitfall: \emph{Using average latency to
evaluate serving system
performance.}}{Pitfall: Using average latency to evaluate serving system performance.}}\label{pitfall-using-average-latency-to-evaluate-serving-system-performance.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Using average latency to
evaluate serving system performance.}}

Engineers monitor average latency because it trends smoothly and is
simple to compute. In production, averages hide the slowest requests
that determine user satisfaction. A system with 10ms average latency
might have 200ms p99 latency, meaning 1 percent of users experience 20×
worse performance.
Section~\ref{sec-model-serving-systems-tail-latency-5376} explains how
queuing variability causes this divergence: at 70 percent utilization
with 5ms service time, average latency is 17ms but p99 reaches 75ms.
Production SLOs specify percentile targets (p95, p99) precisely because
averages mask tail behavior. Systems reporting only averages pass
monitoring checks while violating user experience standards.

\paragraph*{\texorpdfstring{Fallacy: \emph{Larger serving batches always
improve throughput without affecting latency
SLOs.}}{Fallacy: Larger serving batches always improve throughput without affecting latency SLOs.}}\label{fallacy-larger-serving-batches-always-improve-throughput-without-affecting-latency-slos.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Larger serving batches
always improve throughput without affecting latency SLOs.}}

Engineers maximize batch size assuming GPU saturation improves cost
efficiency under production load. In serving systems, however, batching
introduces a latency-throughput tradeoff governed by queuing dynamics
absent from offline benchmarks. Accumulating requests into larger
batches increases wait time for early arrivals: a batch window of 10ms
means the first request waits 10ms before inference begins, directly
adding to p99 latency. For ResNet-50 on V100, increasing batch size from
16 to 32 improves throughput only 12 percent while nearly doubling
per-batch inference time from 14ms to 25ms, and variable input sizes
within a batch create padding overhead that wastes 15 to 30 percent of
compute on padding tokens.
Section~\ref{sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d}
shows that for 50ms p99 targets, batch sizes above 32 routinely violate
SLOs because batch formation delay plus increased per-batch inference
time exceeds the latency budget. Serving batch optimization requires
jointly tuning batch size, batch timeout, and concurrency against
latency SLOs under realistic traffic patterns, not maximizing throughput
in isolation.

\paragraph*{\texorpdfstring{Pitfall: \emph{Calibrating quantized models
with training data rather than production
traffic.}}{Pitfall: Calibrating quantized models with training data rather than production traffic.}}\label{pitfall-calibrating-quantized-models-with-training-data-rather-than-production-traffic.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Calibrating quantized
models with training data rather than production traffic.}}

Teams calibrate with training data because it is readily available and
produced validation accuracy. In production, traffic distribution often
differs from training data, making calibration scale factors suboptimal.
Post-training quantization determines INT8 scale factors by measuring
activation ranges on calibration data, but this assumes production
inputs match the calibration distribution. One production system
experienced 3.2 percent accuracy loss serving wildlife camera images
after calibrating with ImageNet validation data.
\textbf{?@sec-model-compression} shows quantization error scales with
activation range: miscalibration amplifies errors precisely on
out-of-distribution inputs. Effective quantization requires calibrating
with representative samples of actual serving traffic.

\paragraph*{\texorpdfstring{Pitfall: \emph{Cold start latency only
matters for the first
request.}}{Pitfall: Cold start latency only matters for the first request.}}\label{pitfall-cold-start-latency-only-matters-for-the-first-request.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Cold start latency only
matters for the first request.}}

Engineers optimize steady-state latency assuming most requests hit warm
instances. In production, cold starts affect any request arriving after
inactivity, after model updates, or during auto-scaling. Systems with
bursty traffic experience cold starts on 10 to 30 percent of requests
during scale-up events. ResNet-50 with TensorRT requires 30 seconds for
compilation if the optimized engine is not cached; during a traffic
spike triggering 10 new instances, 300 seconds of user-facing latency is
added across the first requests to each instance.
Section~\ref{sec-model-serving-systems-model-loading-initialization-cc5a}
shows cold start compounds weight loading, CUDA context initialization,
and warmup. Systems ignoring cold start meet SLOs during steady state
but violate them during scale-up events and deployment windows.

\section{Summary}\label{sec-model-serving-systems-summary-9635}

Serving marks the transition from model development to production
deployment, where the optimization priorities that governed training
must be inverted. The shift from throughput maximization to latency
minimization transforms every system design decision. The queuing theory
foundations established here reveal \emph{why} this inversion is not
merely a change in metrics but a change in the governing mathematics:
the nonlinear relationship between utilization and latency means that
systems behaving well at moderate load can suddenly violate SLOs when
traffic increases modestly. Little's Law and the M/M/1 wait time
equations provide the quantitative foundation for capacity planning,
replacing intuition-based provisioning with engineering rigor.

Effective serving optimization requires understanding the complete
request path rather than focusing exclusively on model inference.
Interface protocols like gRPC and efficient serialization formats
minimize the ``tax'' of data movement, while preprocessing often
consumes 45 to 70 percent of total latency when inference runs on
optimized accelerators. The microsecond-scale overheads identified by
Barroso, Patterson, and colleagues explain \emph{why} serving latency
often exceeds the sum of its measured parts, and \emph{why} system-level
optimization matters as much as model optimization. Training-serving
skew represents another dimension of this complexity, silently degrading
accuracy when preprocessing logic differs between training and
production environments in ways that traditional testing cannot detect.

The traffic pattern analysis reveals \emph{how} deployment context
shapes batching strategy and system design. Server workloads with
Poisson arrivals optimize dynamic batching windows, autonomous vehicles
with streaming sensor data require synchronized batch formation, and
mobile applications with single-user patterns eliminate batching
entirely. The MLPerf scenarios codify these patterns for standardized
benchmarking, connecting the serving principles established here to the
measurement frameworks explored in \textbf{?@sec-benchmarking-ai}.
Precision selection and runtime optimization extend the quantization
techniques from \textbf{?@sec-model-compression} and Tensor Core
capabilities from \textbf{?@sec-ai-acceleration} into the serving
domain. Finally, the translation of these technical metrics into unit
economics, as shown by the Llama-3 case study, demonstrates \emph{how}
engineering decisions regarding batching, precision, and hardware
selection directly determine the financial viability of deployment.

\phantomsection\label{callout-takeawaysux2a-1.38}
\begin{fbxSimple}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.38}

\begin{itemize}
\tightlist
\item
  \textbf{Serving inverts training priorities}: Training optimizes
  throughput (samples/hour); serving optimizes latency (ms/request).
  Different objectives require different system designs.
\item
  \textbf{Queuing theory governs capacity planning}: At 80\%
  utilization, wait time is 5× service time; at 90\%, it reaches 10×.
  Small load increases cause disproportionate latency spikes.
\item
  \textbf{Preprocessing dominates optimized systems}: When model
  inference is fast (5ms), preprocessing (image decode, tokenization)
  consumes 45--70\% of total latency. Optimize the pipeline, not just
  the model.
\item
  \textbf{Batching strategy depends on traffic pattern}: Poisson
  arrivals (web APIs) use dynamic batching; streaming sensors use
  synchronized batches; mobile apps eliminate batching entirely.
\item
  \textbf{Training-serving skew can degrade accuracy undetected}:
  Different preprocessing between training and serving (e.g., resize
  interpolation, normalization order) shifts inputs outside the training
  distribution, causing accuracy degradation that conventional
  monitoring cannot detect. Use identical code paths.
\item
  \textbf{KV cache optimization is critical for large language models}:
  Generation is memory-bound. PagedAttention and continuous batching can
  improve throughput 2--4× over naive serving.
\end{itemize}

\end{fbxSimple}

The serving principles established here (queuing theory for capacity
planning, preprocessing optimization, batching strategy selection, and
training-serving skew prevention) form the foundation for building
production ML systems that meet real-world SLAs. Whether deploying a
recommendation system serving millions of users or a medical AI where
every millisecond affects patient outcomes, these principles translate
mathematical understanding into engineering decisions that determine
whether systems succeed or fail under load.

\phantomsection\label{callout-chapter-connectionux2a-1.39}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Node to Factory}
\phantomsection\label{callout-chapter-connection*-1.39}
We have optimized the single node for milliseconds, but a single node is
fragile. In \textbf{?@sec-machine-learning-operations-mlops}, we scale
our perspective from the single request to the full system lifecycle,
building the automated machinery that keeps production systems running
through crashes, model drift, and continuous updates.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-chen2018tvm}
0001, Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q.
Yan, Haichen Shen, Meghan Cowan, et al. 2018. {``TVM: An Automated
End-to-End Optimizing Compiler for Deep Learning.''} In \emph{OSDI},
578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-agrawal2024sarathi}
Agrawal, Arney, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun
Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee.
2025. {``Efficient LLM Inference via Chunked Prefills.''} \emph{ACM
SIGOPS Operating Systems Review} 59 (1): 9--16.
\url{https://doi.org/10.1145/3759441.3759444}.

\bibitem[\citeproctext]{ref-barroso2017attack}
Barroso, Luiz, Mike Marty, David Patterson, and Parthasarathy
Ranganathan. 2017. {``Attack of the Killer Microseconds.''}
\emph{Communications of the ACM} 60 (4): 48--54.
\url{https://doi.org/10.1145/3015146}.

\bibitem[\citeproctext]{ref-crankshaw2017clipper}
Crankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E.
Gonzalez, and Ion Stoica. 2017. {``Clipper: A Low-Latency Online
Prediction Serving System.''} In \emph{14th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 17)}, 613--27. USENIX
Association.

\bibitem[\citeproctext]{ref-dean2012rapid}
Dean, Jeffrey. 2012. {``Achieving Rapid Response Times in Large Online
Services.''} Berkeley AMPLab Cloud Seminar.
\url{https://research.google/pubs/pub44875/}.

\bibitem[\citeproctext]{ref-dean2013tail}
Dean, Jeffrey, and Luiz André Barroso. 2013. {``The Tail at Scale.''}
\emph{Communications of the ACM} 56 (2): 74--80.
\url{https://doi.org/10.1145/2408776.2408794}.

\bibitem[\citeproctext]{ref-google2024staticdynamic}
Google. 2024. {``Static Vs. Dynamic Inference.''} Google Machine
Learning Crash
Course.\href{\%0A\%20\%20\%20\%20https://developers.google.com/machine-learning/crash-course/production-ml-systems/static-vs-dynamic-inference\%0A\%20\%20}{https://developers.google.com/machine-learning/crash-course/production-ml-systems/static-vs-dynamic-inference
}.

\bibitem[\citeproctext]{ref-gujarati2020serving}
Gujarati, Arpan, Reza Karber, Safraz Musaev, Weiyang Liu, Anurag
Narayanan, Shi Quan Zhong, Mahmut Kandemir, Vyas Sekar, and Alexander
Zadorozhny. 2020. {``Serving DNNs Like Clockwork: Performance
Predictability from the Bottom Up.''} In \emph{14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 20)}, 443--62. USENIX
Association.

\bibitem[\citeproctext]{ref-harchol2013performance}
Harchol-Balter, Mor. 2013. \emph{Performance Modeling and Design of
Computer Systems: Queueing Theory in Action}. Cambridge University
Press. \url{https://doi.org/10.1017/cbo9781139226424}.

\bibitem[\citeproctext]{ref-holtzman2020curious}
Holtzman, Ari, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi 0001.
2020. {``The Curious Case of Neural Text Degeneration.''} In
\emph{ICLR}. \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[\citeproctext]{ref-nvidia2024tensorrt}
Khan, Zirak, Seung-Chul Yoon, and Suchendra M. Bhandarkar. 2025. {``Deep
Learning Model Compression and Hardware Acceleration for
High-Performance Foreign Material Detection on Poultry Meat Using NIR
Hyperspectral Imaging.''} \emph{Sensors} 25 (3): 970.
\url{https://doi.org/10.3390/s25030970}.

\bibitem[\citeproctext]{ref-pope2023efficiently}
Kim, Jicheon. 2025. {``A Multi-Chip-Module-Based Architecture Simulator
for Scaling Vision Transformer Inference.''} In \emph{2025 IEEE
International Conference on Consumer Electronics (ICCE)}, 5:1--3. IEEE.
\url{https://doi.org/10.1109/icce63647.2025.10930221}.

\bibitem[\citeproctext]{ref-kwon2023vllm}
Kwon, Woosuk, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. {``Efficient
Memory Management for Large Language Model Serving with
PagedAttention.''} In \emph{Proceedings of the 29th Symposium on
Operating Systems Principles}, 611--26. ACM; ACM.
\url{https://doi.org/10.1145/3600006.3613165}.

\bibitem[\citeproctext]{ref-little1961proof}
Little, John D. C. 1961. {``A Proof for the Queuing Formula:
\textless I\textgreater l\textless/i\textgreater{} =
\(\lambda\)\textless i\textgreater w\textless/i\textgreater{}.''}
\emph{Operations Research} 9 (3): 383--87.
\url{https://doi.org/10.1287/opre.9.3.383}.

\bibitem[\citeproctext]{ref-meister2020beam}
Meister, Clara, Ryan Cotterell, and Tim Vieira. 2020. {``If Beam Search
Is the Answer, What Was the Question?''} In \emph{Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP)}, 2173--85. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2020.emnlp-main.170}.

\bibitem[\citeproctext]{ref-onnxruntime2024}
Microsoft. 2024. {``ONNX Runtime: Cross-Platform Inference and Training
Machine-Learning Accelerator.''} GitHub.
\url{https://github.com/microsoft/onnxruntime}.

\bibitem[\citeproctext]{ref-nvidia2024tritontutorial}
NVIDIA. 2025. {``Productionizing GPU Inference on EKS with KServe and
NVIDIA Triton.''} \emph{American International Journal of Computer
Science and Technology} 7 (6).
\url{https://doi.org/10.63282/3117-5481/aijcst-v7i6p104}.

\bibitem[\citeproctext]{ref-olston2017tensorflow}
Olston, Christopher, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li
Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke.
2017. {``TensorFlow-Serving: Flexible, High-Performance ML Serving.''}
\emph{CoRR} abs/1712.06139 (December).
\url{http://arxiv.org/abs/1712.06139v2}.

\bibitem[\citeproctext]{ref-romero2021infaas}
Romero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos
Kozyrakis. 2021. {``INFaaS: Automated Model-Less Inference Serving.''}
In \emph{2021 USENIX Annual Technical Conference (USENIX ATC 21)},
397--411. USENIX Association.
\url{https://www.usenix.org/conference/atc21/presentation/romero}.

\bibitem[\citeproctext]{ref-nvidia2024triton}
Savard, Claire, Nicholas Manganelli, Burt Holzman, Lindsey Gray, Alexx
Perloff, Kevin Pedro, Kevin Stenson, and Keith Ulmer. 2024.
{``Optimizing High-Throughput Inference on Graph Neural Networks at
Shared Computing Facilities with the NVIDIA Triton Inference Server.''}
\emph{Computing and Software for Big Science} 8 (1): 14.
\url{https://doi.org/10.1007/s41781-024-00123-2}.

\bibitem[\citeproctext]{ref-shen2019nexus}
Shen, Haichen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong,
Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019.
{``Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video
Analysis.''} In \emph{Proceedings of the 27th ACM Symposium on Operating
Systems Principles}, 322--37. ACM; ACM.
\url{https://doi.org/10.1145/3341301.3359658}.

\bibitem[\citeproctext]{ref-wu2019machine}
Wu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury,
Marat Dukhan, Kim Hazelwood, et al. 2019. {``Machine Learning at
Facebook: Understanding Inference at the Edge.''} In \emph{2019 IEEE
International Symposium on High Performance Computer Architecture
(HPCA)}, 331--44. IEEE. \url{https://doi.org/10.1109/hpca.2019.00048}.

\bibitem[\citeproctext]{ref-yu2022orca}
Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and
Byung-Gon Chun. 2022. {``Orca: A Distributed Serving System for
Transformer-Based Generative Models.''} In \emph{16th USENIX Symposium
on Operating Systems Design and Implementation (OSDI 22)}, 521--38.
USENIX Association.
\url{https://www.usenix.org/conference/osdi22/presentation/yu}.

\bibitem[\citeproctext]{ref-zhang2019mark}
Zhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019.
{``MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine
Learning Inference Serving.''} In \emph{2019 USENIX Annual Technical
Conference (USENIX ATC 19)}, 1049--62. USENIX Association.
\url{https://www.usenix.org/conference/atc19/presentation/zhang-chengliang}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
