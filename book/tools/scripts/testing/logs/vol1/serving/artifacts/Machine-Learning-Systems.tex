% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Responsible Engineering}\label{sec-responsible-engineering}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: A conceptual illustration showing a hand cradling
a green seedling beneath a glowing white tree structure. Cosmic backdrop
with galaxy, network nodes, planet, and industrial structures with
smokestacks on the horizon, representing the balance between
technological progress and environmental responsibility.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/responsible_engr/images/png/cover_responsible_systems.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why can a machine learning system be simultaneously correct and
harmful?}

A model can achieve excellent accuracy, meet latency requirements,
maintain high availability, and still cause systematic harm. A loan
approval system that correctly predicts default risk may encode
historical discrimination. A content recommendation system that
accurately predicts engagement may amplify harmful content. A hiring
algorithm that reliably identifies candidates similar to past hires may
perpetuate workforce homogeneity. This paradox---technical success
coexisting with ethical failure---arises because standard ML metrics
measure prediction quality on the data provided, not whether that data
reflects the world as it should be or whether those predictions serve
all populations equitably. The system faithfully learns and reproduces
whatever patterns exist in its training distribution, including patterns
of historical injustice, underrepresentation, and societal bias.
Responsible engineering exists because correctness is insufficient:
building systems that work is an engineering achievement, but building
systems that work \emph{for everyone} without causing harm requires
additional constraints that technical metrics alone cannot capture.

\begin{tcolorbox}[enhanced jigsaw, colback=white, bottomrule=.15mm, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, opacityback=0, coltitle=black, toptitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, arc=.35mm, left=2mm, toprule=.15mm, bottomtitle=1mm, breakable, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, leftrule=.75mm, opacitybacktitle=0.6]

\begin{itemize}
\item
  Explain how ML systems fail silently through bias amplification and
  distribution shift, distinguishing these from traditional software
  failures.
\item
  Evaluate ML system performance using disaggregated metrics across
  demographic groups to detect disparities hidden in aggregate measures.
\item
  Apply pre-deployment assessment frameworks to identify potential
  harms, affected populations, and monitoring needs before production
  release.
\item
  Compute fairness metrics (demographic parity, equal opportunity,
  equalized odds) from confusion matrices and interpret disparities
  against established thresholds.
\item
  Analyze total cost of ownership for ML systems including training,
  inference, operational costs, and environmental impact over the system
  lifecycle.
\item
  Construct model documentation using standardized formats (model cards,
  datasheets) that specify intended use, evaluation results, and known
  limitations.
\end{itemize}

\end{tcolorbox}

\section{Introduction}\label{sec-responsible-engineering-introduction-3c46}

If \textbf{MLOps} (\textbf{?@sec-machine-learning-operations-mlops}) is
the control loop for \emph{reliability}, then \textbf{Responsible
Engineering} is the control loop for \emph{safety}. Where MLOps monitors
system health and triggers retraining when performance degrades,
responsible engineering monitors \emph{outcome quality} and triggers
intervention when systems cause harm. This distinction matters because a
model can achieve excellent accuracy and still cause systematic harm, a
paradox that standard ML metrics cannot detect.

Traditional software engineering assumes that bugs are local: a defect
in one module rarely corrupts unrelated functionality. Machine learning
systems violate this assumption. Data flows through shared
representations, causing problems in one component to propagate
unpredictably across the entire system. A biased training dataset does
not produce a localized bug; it corrupts every prediction the system
makes.

Engineering responsibility for ML systems extends in two directions.
First, systems must work correctly in the traditional sense: reliable,
performant, and maintainable. Second, systems must work responsibly:
fair across user groups, efficient in resource consumption, and
transparent in their decision processes. This chapter provides
frameworks for addressing both dimensions, organized around four themes:

\begin{itemize}
\tightlist
\item
  \textbf{The Responsibility Gap}
  (Section~\ref{sec-responsible-engineering-engineering-responsibility-gap-e69b}):
  Why technically correct systems produce harmful outcomes, and how to
  detect silent failures
\item
  \textbf{The Responsible Engineering Checklist}
  (Section~\ref{sec-responsible-engineering-responsible-engineering-checklist-844e}):
  Systematic processes for assessment, documentation, testing, and
  incident response
\item
  \textbf{Environmental and Cost Awareness}
  (Section~\ref{sec-responsible-engineering-environmental-cost-awareness-0f3e}):
  How efficiency optimization serves responsibility goals
\item
  \textbf{Fallacies and Pitfalls}
  (Section~\ref{sec-responsible-engineering-fallacies-pitfalls-61b9}):
  Common mistakes that undermine responsible engineering efforts
\end{itemize}

We begin with concrete failure cases that establish why engineers must
lead on responsibility, a conclusion that emerges from examining the
evidence.

\phantomsection\label{quiz-question-sec-responsible-engineering-introduction-3c46}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.1}{}
\phantomsection\label{quiz-question-sec-responsible-engineering-introduction-3c46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is responsible engineering particularly critical for machine
  learning systems compared to traditional software?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems are more expensive to develop.
  \item
    ML systems fail silently through biased outputs that appear normal.
  \item
    Traditional software does not require any testing.
  \item
    ML systems always produce deterministic results.
  \end{enumerate}
\item
  Why can responsibility not be delegated exclusively to ethics boards
  or legal departments in an ML project?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-responsible-engineering-introduction-3c46]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{The Engineering Responsibility
Gap}\label{sec-responsible-engineering-engineering-responsibility-gap-e69b}

Technical correctness and responsible outcomes are not equivalent. A
model can achieve state-of-the-art accuracy on benchmark datasets while
systematically disadvantaging specific user populations in production.
This gap between technical performance and responsible deployment
represents a central challenge in machine learning systems engineering,
one that existing testing methodologies were not designed to address.

Understanding \emph{how} this gap manifests in practice is essential
before discussing \emph{how} to prevent it. The cases below share a
common pattern: each system succeeded by every conventional engineering
metric while failing in ways that those metrics could not detect.
Examining these failures reveals the mechanisms that responsible
engineering must address.

\subsection{When Optimization Succeeds But Systems
Fail}\label{sec-responsible-engineering-optimization-succeeds-systems-fail-1a22}

The Amazon recruiting tool case illustrates this gap. In 2014, Amazon
developed an AI system to automate resume screening for technical
positions, training it on historical hiring data spanning ten years of
resumes submitted to the company. By 2015, the company discovered the
system exhibited gender bias in candidate ratings
(\citeproc{ref-dastin2018amazon}{Dastin 2022}).

The technical implementation was sound. The model successfully learned
patterns from historical data and optimized for the objective it was
given: identify candidates similar to those previously hired. However,
historical hiring patterns encoded gender bias. The system penalized
resumes containing the word ``women's,'' as in ``women's chess club
captain,'' and downgraded graduates of all-women's colleges.

The technical mechanism behind this outcome is straightforward. The
model learned token-level patterns from historical data. When most
previously successful hires were men, resumes containing language
associated with women's activities or institutions appeared
statistically less correlated with positive hiring decisions. The model
correctly identified these patterns in the training data but learned the
wrong lesson from correct pattern recognition.

Amazon attempted remediation by removing explicit gender indicators and
gendered terms from the training process. This intervention failed
because the model had learned proxy signals that correlated with
gender.\sidenote{\textbf{Proxy Variables}: From Latin \emph{procurator}
(agent, substitute), ``proxy'' entered statistics to denote variables
that stand in for unmeasurable quantities. In ML fairness, proxies are
features that correlate with protected attributes without directly
encoding them: ZIP codes correlate with race due to residential
segregation; first names correlate with gender and ethnicity; healthcare
utilization correlates with socioeconomic status. Removing protected
attributes from training data is insufficient because models learn these
correlations from remaining features. Systems implications: fairness
requires adversarial debiasing, fairness constraints during
optimization, or post-hoc threshold adjustment per group. } College
names revealed attendance at all-women's institutions. Activity
descriptions encoded gender-associated language patterns. Career gaps
suggested parental leave patterns that differed between genders. The
model reconstructed protected attributes from these proxies without ever
seeing gender labels directly.

The right intervention would have required multiple levels of change.
Separate evaluation of resume scores for male-associated versus
female-associated candidates would have revealed the disparity
quantitatively. Training with fairness constraints or adversarial
debiasing techniques could have prevented the model from learning
gender-correlated patterns. Human-in-the-loop review for borderline
cases would have provided a safeguard against systematic errors.
Tracking actual hiring outcomes by gender over time would have enabled
outcome monitoring beyond model metrics alone. Amazon eventually
scrapped the project after determining that sufficient remediation was
not feasible.

This case demonstrates how optimization objectives can diverge from
organizational values. The system optimized exactly what it was told to
optimize and found genuine statistical patterns in historical hiring
decisions. However, those patterns reflected biased historical practices
rather than job-relevant qualifications.

The COMPAS recidivism prediction system presents similar dynamics in
criminal justice.\sidenote{\textbf{COMPAS (Correctional Offender
Management Profiling for Alternative Sanctions)}: A proprietary risk
assessment tool developed by Northpointe (now Equivant) and used in
criminal justice systems across the United States. ProPublica's 2016
analysis of 7,000+ defendants in Broward County, Florida found the
system's overall accuracy was only 61\%, comparable to untrained humans.
The critical systems finding: while the algorithm was calibrated
(similar positive predictive values across races), its error rates
differed dramatically. Black defendants who did not recidivate were
incorrectly flagged as high risk at 44.9\% compared to 23.5\% for white
defendants. This case demonstrates why aggregate accuracy is
insufficient: a system can be ``calibrated'' while still producing
disparate outcomes. } The system assigns risk scores used in bail and
sentencing decisions. A 2016 ProPublica investigation found that Black
defendants were significantly more likely than white defendants to be
incorrectly labeled as high risk, while white defendants were more
likely to be incorrectly labeled as low risk
(\citeproc{ref-angwin2016machine}{Angwin et al. 2022}). The algorithm
used factors like criminal history and neighborhood characteristics that
correlate with race due to historical policing patterns.

Better testing would not catch these problems because they represent
failures of problem specification, where the technical objective
(minimizing prediction error on historical outcomes) diverges from the
desired social objective (making fair and accurate predictions across
demographic groups). These specification failures are difficult to
detect precisely because the systems continue functioning normally by
conventional engineering metrics.

\subsection{Silent Failure
Modes}\label{sec-responsible-engineering-silent-failure-modes-e219}

Traditional software fails loudly. A null pointer exception crashes the
program, a network timeout returns an error code. These visible failures
enable rapid detection and response. In contrast, ML systems fail
silently because degraded predictions look like normal
predictions.\sidenote{\textbf{Silent Failures}: Model degradation that
evades traditional monitoring by producing plausible but suboptimal
outputs. Recommendation systems may drift toward engagement-optimized
but low-value content. Fraud models may miss new attack patterns. Unlike
crashes or latency spikes, silent failures require business-metric
monitoring and human review to detect gradual performance decay. }

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Distribution Shift}
\phantomsection\label{callout-definition*-1.1}
\textbf{Distribution shift} occurs when the statistical properties of
production data diverge from the training data distribution. A model
trained on historical patterns encounters new patterns it has never
seen: new user behaviors, seasonal changes, competitor actions, or
demographic shifts. The model continues to produce confident
predictions, but those predictions become increasingly unreliable as the
gap between training and production distributions widens. Distribution
shift is the primary mechanism behind silent model degradation.

\end{fbx}

Distribution shift explains why models degrade over time. But there is a
second mechanism for silent failure that can occur even when the data
distribution is stable: misalignment between the metric the model
optimizes and the outcome the organization actually values. This
misalignment creates what we call the \emph{alignment gap}, where
optimizing a measurable proxy decouples the system from its intended
purpose.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Alignment Gap}
\phantomsection\label{callout-perspective*-1.2}
\textbf{The Problem}: A model optimizes a proxy metric (Clicks) because
the true metric (User Satisfaction) is unobservable. How much can they
diverge?

\textbf{The Physics}: Goodhart's Law states that optimizing a proxy
eventually decouples it from the goal. * \textbf{Initial State}:
Correlation(Clicks, Satisfaction) = 0.8. * \textbf{Optimization}: You
train a model to maximize Clicks. * \textbf{Result}: The model finds
``Clickbait,'' items with high clicks but low satisfaction. *
\textbf{Final State}: Correlation(Clicks, Satisfaction) drops to 0.2.

\textbf{The Quantification}:
\[ \text{Gap} = E[\text{Proxy}] - E[\text{True}] \] If the model
increases Clicks by 20\% but decreases Satisfaction by 5\%, the
\textbf{Alignment Gap} has widened.

\textbf{The Systems Conclusion}: You cannot optimize what you cannot
measure. If your true goal is unobservable, you must use
\textbf{Counterfactual Evaluation} (random holdouts) to periodically
re-calibrate your proxy.

\end{fbx}

Table~\ref{tbl-failure-modes} categorizes these distinct failure modes
by their detection time, spatial scope, and remediation requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2212}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1858}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1770}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1770}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2124}}@{}}
\caption{\textbf{ML System Failure Mode Taxonomy}: Different failure
modes require different detection strategies and remediation approaches.
Silent failures such as data quality issues, distribution shift, and
fairness violations demand proactive monitoring because they do not
trigger traditional alerts.}\label{tbl-failure-modes}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Failure Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Detection Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Spatial Scope}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Reversibility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Failure Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Detection Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Spatial Scope}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Reversibility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Crash} & Immediate & Complete & Immediate & Out of memory
error \\
\textbf{Performance} \textbf{Degradation} & Minutes & Complete & After
fix & Latency spike from resource contention \\
\textbf{Data Quality} & Hours--days & Partial & Requires data correction
& Corrupted inputs from upstream system \\
\textbf{Distribution Shift} & Days--weeks & Partial or all & Requires
retraining & Population change due to new user segment \\
\textbf{Fairness Violation} & Weeks--months & Subpopulation & Requires
redesign & Bias amplification in historical patterns \\
\end{longtable}

This taxonomy shows why traditional monitoring approaches prove
insufficient for ML systems. Crashes and performance degradation trigger
immediate alerts through existing infrastructure. Data quality issues,
distribution shifts, and fairness violations require specialized
detection mechanisms because the system continues operating normally
from a technical perspective while producing increasingly problematic
outputs.

YouTube's recommendation system illustrated this pattern at scale
(\citeproc{ref-ribeiro2020auditing}{M. H. Ribeiro et al. 2020}). The
system successfully optimized for watch time and discovered that
emotionally provocative content maximized engagement
metrics.\sidenote{\textbf{Goodhart's Law}: Named after British economist
Charles Goodhart, who articulated this principle in a 1975 Bank of
England paper on monetary policy. His original formulation: ``Any
observed statistical regularity will tend to collapse once pressure is
placed upon it for control purposes.'' Anthropologist Marilyn Strathern
later generalized it to the widely-cited form: ``When a measure becomes
a target, it ceases to be a good measure.'' The law explains why
optimizing for proxy metrics (watch time, clicks, engagement) eventually
decouples them from true objectives (user satisfaction, value). ML
systems are particularly susceptible because they optimize proxies at
scale and speed impossible for human decision-makers. } Over time, the
algorithm developed pathways that moved users toward increasingly
extreme content because each step increased the target metric. The
system worked exactly as designed while producing outcomes that
conflicted with organizational and societal values.

This behavior exemplifies a feedback loop characteristic of ML systems.
Users watch videos while the system observes engagement through watch
time and interactions. The algorithm updates recommendations based on
what maximized those metrics. Users see more emotionally charged
content, engagement increases because such content triggers stronger
reactions, and the system reinforces this pattern in the next iteration.
Each cycle amplifies small biases into large distributional shifts.

Detection requires monitoring the input distribution for drift caused by
the model's own outputs. When the system increasingly recommends extreme
content, the population of videos watched shifts over time even if
individual user preferences remain constant. Traditional monitoring
focused on prediction accuracy would miss this drift because the system
successfully predicts user engagement on the content it provides. The
problem is not prediction quality but the feedback loop between
predictions and the data distribution those predictions create.

YouTube has since implemented multiple interventions including diverse
objectives beyond watch time, exploration mechanisms that surface
content outside current model preferences, and explicit limits on
recommendation pathways toward certain content categories. These changes
illustrate that addressing feedback loops requires architectural
modifications, not just parameter tuning.

Distribution shift creates another silent failure mode where models
trained on one population perform differently on another population
without obvious indicators. Healthcare risk prediction algorithms
studied by Obermeyer et al.~used healthcare costs as a proxy for health
needs (\citeproc{ref-obermeyer2019dissecting}{Obermeyer et al. 2019}).
Because Black patients historically had less access to healthcare and
thus lower costs for similar conditions, the algorithm systematically
underestimated their health needs. The model showed strong overall
performance metrics while encoding racial bias in its predictions.

Silent failure modes create profound testing challenges. Traditional
software testing verifies deterministic behavior against specifications.
ML systems produce probabilistic outputs learned from data, making
correctness far more complex to define. Yet the failures examined above
share a troubling pattern: each organization possessed the technical
capability to prevent harm but lacked the systematic processes to apply
that capability.

These failures are not inevitable. The same engineering capabilities
that enabled the problems can prevent them when organizations commit to
systematic practice. The following cases demonstrate what responsible
engineering looks like when it succeeds.

\subsection{When Responsible Engineering
Succeeds}\label{sec-responsible-engineering-responsible-engineering-succeeds-29e0}

The preceding examples emphasize failure, but responsible engineering
also produces measurable successes that demonstrate both the feasibility
and business value of systematic responsibility practices.

\textbf{Microsoft's Facial Recognition Improvement.} Following the
Gender Shades findings, Microsoft invested in improving facial
recognition performance across demographic groups. The approach combined
technical and organizational interventions: targeted data collection to
address underrepresented populations, model architecture changes to
improve feature extraction for diverse skin tones, and systematic
disaggregated evaluation across all demographic intersections. By 2019,
Microsoft had reduced error rates for darker-skinned subjects by up to
20 times, bringing error rates below 2\% for all demographic groups
(\citeproc{ref-raji2019actionable}{Raji and Buolamwini 2019}). The
company published these improvements transparently, enabling external
verification. The business outcome: Microsoft's facial recognition API
maintained enterprise customer trust while competitors faced regulatory
scrutiny and contract cancellations.

\textbf{Twitter's Principled Feature Removal.} In 2020, users discovered
Twitter's automatic image cropping system exhibited racial bias in
choosing which faces to display in preview thumbnails. Twitter responded
with a responsible engineering approach: systematic analysis to
characterize the problem quantitatively, publication of results enabling
independent verification, and ultimately removal of the automatic
cropping feature entirely
(\citeproc{ref-twitter2021cropping}{Engineering 2021}). The company
determined that no technical solution could guarantee equitable outcomes
across all contexts. This decision prioritized user fairness over
engagement optimization and demonstrated that responsible engineering
sometimes means not shipping a feature.

\textbf{Apple's Differential Privacy Implementation.} Apple's deployment
of differential privacy in iOS represents responsible engineering at
scale.\sidenote{\textbf{Differential Privacy}: A mathematical framework
introduced by Dwork et al. (\citeproc{ref-dwork2006calibrating}{Dwork et
al. 2006}) that provides formal privacy guarantees. A mechanism
satisfies epsilon-differential privacy if the probability of any output
changes by at most a factor of e\^{}epsilon when any single individual's
data is added or removed. Smaller epsilon values provide stronger
privacy but reduce data utility. Apple's implementation uses local
differential privacy (noise added on-device before transmission) with
epsilon values ranging from 2 to 16 depending on the data type and use
case (for example, epsilon=2 for health type usage, epsilon=4 for emoji
suggestions, and epsilon=8-16 for Safari-related features). Systems
trade-offs: differential privacy adds 15-30\% computational overhead,
requires 10-100x more data to achieve equivalent statistical accuracy,
and demands careful privacy budget management across multiple queries. }
The system collects usage data for product improvement while providing
mathematical guarantees about individual privacy. The implementation
required substantial engineering investment: noise calibration to
balance utility against privacy, distributed computation to minimize
data exposure, and transparent documentation of privacy parameters. The
business value: Apple differentiated on privacy as a product feature,
enabling data collection that would otherwise face regulatory and
reputational barriers.

\textbf{Spotify's Algorithmic Transparency.} Spotify addressed
recommendation system concerns by implementing transparency features
showing users why songs were recommended and providing controls to
adjust algorithm behavior. This engineering investment served multiple
purposes: user trust through explainability, reduced filter bubble
effects through diversity injection, and regulatory compliance through
user control mechanisms. The approach demonstrates that responsibility
features can enhance rather than constrain product value.

These examples share common patterns. Technical interventions (improved
data, better evaluation, architectural changes) combined with
organizational commitments (transparency, willingness to remove
features, long-term investment). The business outcomes (maintained
customer trust, regulatory compliance, competitive differentiation)
demonstrate that responsible engineering creates value rather than
simply adding cost. The technical capabilities for responsible systems
exist; the question is whether organizations commit engineering
resources to apply them systematically. Such systematic application
requires understanding what makes ML testing fundamentally different
from traditional software verification.

\subsection{The Testing
Challenge}\label{sec-responsible-engineering-testing-challenge-77b0}

Traditional software testing verifies that systems behave correctly
because correctness has clear definitions. The function should return
the sum of its inputs, the database should maintain referential
integrity. These properties can be expressed as testable assertions.

Responsible ML properties resist simple formalization. Fairness has
multiple conflicting mathematical definitions that cannot all be
satisfied simultaneously. What counts as fair depends on context,
values, and tradeoffs that technical systems cannot resolve alone.
Individual fairness requires that similar individuals receive similar
treatment, while group fairness requires equitable outcomes across
demographic categories. These criteria can conflict, and choosing
between them requires value judgments beyond the scope of
optimization.\sidenote{\textbf{Fairness Tradeoffs}: Research has shown
that different mathematical definitions of fairness are often mutually
exclusive (\citeproc{ref-chouldechova2017fair}{Chouldechova 2017}).
Satisfying one criterion may require violating another. This is not a
technical problem to be solved but a design choice requiring explicit
stakeholder input. }

Responsible properties become testable when engineers work with
stakeholders to define criteria appropriate for specific applications.
The Gender Shades project\sidenote{\textbf{Gender Shades}: A landmark
2018 study by Joy Buolamwini and Timnit Gebru at MIT Media Lab that
audited commercial facial recognition systems from Microsoft, IBM, and
Face++. The name evokes both the demographic dimensions studied (gender,
skin shade) and the ``shades of gray'' in algorithmic accountability.
Using the Fitzpatrick skin type scale from dermatology, they created a
balanced benchmark (Pilot Parliaments Benchmark) with equal
representation across gender and skin tone. The study's methodology
became a template for algorithmic auditing, and its findings directly
prompted Microsoft and IBM to improve their systems. } demonstrated how
disaggregated evaluation across demographic categories reveals
disparities invisible in aggregate metrics
(\citeproc{ref-buolamwini2018gender}{Buolamwini and Gebru 2018a}).
Table~\ref{tbl-gender-shades-results} captures the dramatic error rate
differences commercial facial recognition systems showed across
demographic groups.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3733}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2800}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Gender Shades Facial Recognition Error Rates}:
Disaggregated evaluation reveals that aggregate accuracy metrics conceal
severe performance disparities. Systems that appear highly accurate
overall show error rates varying by more than 40× across demographic
groups. Source: Buolamwini and Gebru
(\citeproc{ref-buolamwini2018gender}{2018a}).}\label{tbl-gender-shades-results}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Demographic Group}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Error Rate (\%)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Relative Disparity}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Demographic Group}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Error Rate (\%)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Relative Disparity}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Light-skinned males} & 0.8 & Baseline (1.0×) \\
\textbf{Light-skinned females} & 7.1 & 8.9× higher \\
\textbf{Dark-skinned males} & 12.0 & 15.0× higher \\
\textbf{Dark-skinned females} & 34.7 & 43.4× higher \\
\end{longtable}

Disaggregated evaluation revealed what aggregate accuracy scores
concealed. Systems reporting high overall accuracy simultaneously
achieved error rates as low as 0.8\% for light-skinned males and as high
as 34.7\% for dark-skinned females (corresponding to accuracies of
99.2\% and 65.3\% respectively). The aggregate metric provided no
indication of this 43-fold disparity in error rates.

While no universal threshold defines acceptable disparity, engineering
teams should establish explicit bounds before deployment. Common
industry practices include error rate ratios below 1.25× between
demographic groups for high-stakes applications, false positive rate
differences under 5 percentage points for screening systems, and
selection rate ratios between 0.8 and 1.25 (the four-fifths rule from
employment discrimination law).\sidenote{\textbf{Four-Fifths Rule}: A
statistical guideline codified in the 1978 Uniform Guidelines on
Employee Selection Procedures, used by the EEOC, Department of Labor,
and Department of Justice. The rule states that a selection rate for any
protected group below 80\% of the highest group's rate constitutes prima
facie evidence of adverse impact. For example, if 60\% of male
applicants pass a screening test, at least 48\% of female applicants
must pass to satisfy the rule. The rule is a threshold for
investigation, not a definitive finding of discrimination. In ML
systems, implementing four-fifths monitoring requires tracking selection
rates by demographic group and alerting when ratios fall below 0.8. }
These thresholds are starting points for discussion with stakeholders,
not absolute standards. The key engineering discipline is defining
measurable criteria before deployment rather than discovering problems
after harm has occurred.

\textbf{Practical Testing Strategies.} Despite the inherent challenges,
several concrete testing approaches can surface responsibility issues
before deployment:

\begin{itemize}
\item
  \textbf{Slice-based evaluation} partitions test data into meaningful
  subgroups and reports metrics separately for each slice. A model may
  achieve 95\% accuracy overall but only 78\% accuracy on low-income
  applicants or users from rural areas.
\item
  \textbf{Invariance testing} checks whether predictions change when
  they should not. Replacing ``John'' with ``Jamal'' in a loan
  application should not change approval likelihood if the feature is
  not legitimate for the decision.
\item
  \textbf{Boundary testing} evaluates model behavior at the edges of
  input distributions (unusual ages, extreme values, rare categories)
  where training data may be sparse and predictions unreliable.
\item
  \textbf{Stress testing} examines performance under adverse conditions:
  corrupted inputs, distribution shift, adversarial examples, and edge
  cases designed to probe failure modes.
\item
  \textbf{Stakeholder red-teaming} engages domain experts and affected
  community members to identify scenarios that engineers may not
  anticipate but users will encounter.
\end{itemize}

These strategies complement rather than replace traditional software
testing. Each requires engineering judgment to select, configure, and
interpret. A legal team cannot specify which demographic slices matter
for a healthcare algorithm; a product manager cannot determine
appropriate invariance tests for a loan model. The technical depth
required to implement responsible testing points to a critical
organizational truth: only engineers possess the knowledge to translate
abstract fairness goals into measurable, testable properties.

\subsection{Why Engineers Must Lead on
Responsibility}\label{sec-responsible-engineering-engineers-must-lead-responsibility-9272}

\phantomsection\label{callout-definitionux2a-1.3}
\begin{fbx}{callout-definition}{Definition: }{Responsible AI Engineering}
\phantomsection\label{callout-definition*-1.3}
\textbf{Responsible AI Engineering} is the practice of designing,
developing, and deploying AI systems with the explicit intent to ensure
\emph{fairness}, \emph{reliability}, \emph{safety}, and \emph{societal
benefit}, integrating these values as core \emph{engineering
constraints} rather than afterthoughts.

\end{fbx}

Responsibility in ML systems cannot be delegated exclusively to ethics
boards or legal departments. These groups provide essential oversight
but lack the technical access required to identify problems early in the
development process. By the time a system reaches legal review,
architectural decisions have already constrained the space of possible
fairness interventions. Amazon's recruiting tool reached review only
after the model had learned proxy signals; at that point, remediation
required starting over, not adjusting parameters. Engineers who
understand both technical implementation and responsibility requirements
can build appropriate safeguards from inception.

Engineers occupy a critical position in the ML development lifecycle
because technical decisions define the solution space for all subsequent
interventions. Model architecture selection determines which fairness
constraints can be applied during training. Optimization objective
specification defines what patterns the system learns to recognize. Data
pipeline design establishes what demographic information can be tracked
for disaggregated evaluation. These foundational choices enable or
foreclose responsible outcomes more decisively than any later
remediation efforts.

The timing of responsibility interventions determines their
effectiveness. An ethics review conducted before deployment can identify
problems but faces limited remediation options. If the model has already
been trained without fairness constraints, if the architecture cannot
support interpretability requirements, if the data pipeline lacks
demographic attributes for monitoring, the ethics review can only
recommend rejection or acceptance of the existing system. Engineering
involvement from project inception enables proactive design rather than
reactive assessment.

This engineering-centered approach does not diminish the importance of
diverse perspectives in identifying potential harms. Product managers,
user researchers, affected communities, and policy experts contribute
essential knowledge about how systems fail socially despite technical
success. Engineers translate these concerns into measurable requirements
and testable properties that can be verified throughout the development
lifecycle. Effective responsibility requires engineers who both listen
to stakeholder concerns and possess the technical capability to
implement appropriate safeguards. Figure~\ref{fig-governance-layers}
illustrates how engineering practices nest within broader
organizational, industry, and regulatory governance structures.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/responsible_engr/images/png/human_centered_ai.png}}

}

\caption{\label{fig-governance-layers}\textbf{Responsible AI Governance
Layers}. Responsible engineering operates within nested governance
structures. At the center, engineering teams implement technical
practices (audit trails, bias testing, explainable UIs). These exist
within organizational safety culture and management strategies. Industry
certification and external reviews provide independent oversight.
Government regulation establishes the outermost boundary. Engineers must
understand where their work fits in this ecosystem: technical excellence
at the center enables compliance with requirements flowing from outer
layers.}

\end{figure}%

Understanding where engineering fits within this governance ecosystem
leads naturally to the question of scope: what exactly falls under an
engineer's responsibility? The answer extends beyond the metrics we have
optimized throughout this book.

\phantomsection\label{callout-perspectiveux2a-1.4}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Full Cost of the Iron Law}
\phantomsection\label{callout-perspective*-1.4}
The \textbf{Iron Law of ML Systems} established in
\textbf{?@sec-ai-training-iron-law-training-performance-a53f} holds that
system performance depends on the interaction between data, compute, and
system overhead. We have spent previous chapters optimizing each term:
compressing models (\textbf{?@sec-model-compression}), accelerating
hardware (\textbf{?@sec-ai-acceleration}), and automating operations
(\textbf{?@sec-machine-learning-operations-mlops}). But every
optimization has costs beyond those captured in benchmarks.

A model quantized for edge deployment consumes less energy, but also
produces outputs that may differ across demographic groups. A
recommendation system optimized for engagement maximizes a business
metric, but may amplify harmful content. Responsible engineering extends
our accounting to include these broader impacts: the carbon cost of
computation, the fairness cost of optimization choices, and the societal
cost of deployment at scale. The Iron Law governs \emph{how fast} our
systems run; responsible engineering governs \emph{how well} they serve.

\end{fbx}

Beyond ethical imperatives, responsible engineering delivers measurable
business value through three mechanisms:

\textbf{Risk Mitigation.} ML system failures create legal and financial
exposure that responsible engineering practices reduce. Amazon's
recruiting tool cancellation represented years of development investment
lost to inadequate fairness consideration. COMPAS-related litigation has
cost jurisdictions millions in legal fees and settlements. Organizations
implementing systematic responsibility practices (disaggregated
evaluation, documentation, monitoring) reduce the probability of costly
failures and demonstrate due diligence if problems emerge.

\textbf{Regulatory Compliance.} The regulatory landscape for ML systems
is rapidly expanding. The EU AI Act classifies high-risk AI applications
and mandates specific technical requirements including risk assessment,
data governance, transparency, and human oversight. Organizations that
build responsibility into engineering practice can demonstrate
compliance through existing documentation and monitoring rather than
expensive retrofitting. The cost of proactive compliance is typically
10--20\% of reactive remediation.

\textbf{Competitive Differentiation.} Trust increasingly drives
enterprise purchasing decisions for ML-powered services. Organizations
that can demonstrate systematic responsibility practices through model
cards, audit trails, and published evaluation results win contracts that
competitors cannot. Apple's privacy positioning, Microsoft's responsible
AI principles, and Anthropic's safety research all represent strategic
investments in responsibility as differentiation.

The quantization techniques from \textbf{?@sec-model-compression} reduce
inference energy by 2-4x, directly supporting sustainable deployment.
The monitoring infrastructure from
\textbf{?@sec-machine-learning-operations-mlops} enables disaggregated
fairness evaluation across demographic groups. Responsible engineering
synthesizes these capabilities into systematic practice through
structured frameworks that translate principles into processes.

\phantomsection\label{quiz-question-sec-responsible-engineering-engineering-responsibility-gap-e69b}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.2}{}
\phantomsection\label{quiz-question-sec-responsible-engineering-engineering-responsibility-gap-e69b}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In the Amazon recruiting tool case, why did removing explicit gender
  labels fail to eliminate bias?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    The model was not trained for enough epochs.
  \item
    The model learned proxy signals (like college names) that correlated
    with gender.
  \item
    The engineers forgot to delete the gender column.
  \item
    The dataset was too small to be accurate.
  \end{enumerate}
\item
  Explain how a `feedback loop' in a recommendation system can lead to
  bias amplification.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-responsible-engineering-engineering-responsibility-gap-e69b]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{The Responsible Engineering
Checklist}\label{sec-responsible-engineering-responsible-engineering-checklist-844e}

Understanding why ML systems fail is necessary but not sufficient;
engineers need systematic processes that prevent failures before they
occur. The preceding analysis established both the mechanisms by which
ML systems cause harm and the engineering capabilities required to
prevent it. The frameworks that follow integrate responsibility concerns
into existing development workflows throughout the ML lifecycle.

\subsection{Pre-Deployment
Assessment}\label{sec-responsible-engineering-predeployment-assessment-2324}

Production deployment requires systematic evaluation of potential
impacts across multiple dimensions.
Table~\ref{tbl-pre-deployment-assessment} structures this evaluation
into five phases, distinguishing critical-path blockers from
high-priority items that can proceed with documented risk acceptance.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1466}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1293}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3534}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3534}}@{}}
\caption{\textbf{Pre-Deployment Assessment Framework}: Critical Path
items block deployment until addressed. High Priority items should be
completed before or shortly after launch. This framework ensures
systematic coverage of responsibility concerns throughout the ML
lifecycle.}\label{tbl-pre-deployment-assessment}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Questions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Documentation Required}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Questions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Documentation Required}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data} & Critical Path & Where did this data come from? Who is
represented? Who is missing? What historical biases might be encoded? &
Data provenance records, demographic composition analysis, collection
methodology documentation \\
\textbf{Training} & High & What are we optimizing for? What might we be
implicitly penalizing? How do architecture choices affect outcomes? &
Objective function specification, regularization choices, hyperparameter
selection rationale \\
\textbf{Evaluation} & Critical Path & Does performance hold across
different user groups? What edge cases exist? How were test sets
constructed? & Disaggregated metrics by demographic group, edge case
testing results, test set composition analysis \\
\textbf{Deployment} & Critical Path & Who will this system affect? What
happens when it fails? What recourse do affected users have? & Impact
assessment, stakeholder identification, rollback procedures, user
notification protocols \\
\textbf{Monitoring} & High & How will we detect problems? Who reviews
system behavior? What triggers intervention? & Monitoring dashboard
specifications, alert thresholds, review schedules, escalation
procedures \\
\end{longtable}

Critical Path items are deployment blockers where the system must not go
to production until these questions are answered. High Priority items
should be addressed but may proceed with documented risk acceptance and
a remediation timeline. This distinction enables teams to ship
responsibly without requiring perfection on every dimension before
initial deployment.

\textbf{Human-in-the-Loop Safeguards.} For high-stakes applications, the
deployment phase should specify where human oversight is required.
Human-in-the-loop (HITL) systems route uncertain, high-consequence, or
flagged decisions to human reviewers rather than acting autonomously.
The design questions are: Which decisions require human review? What
confidence thresholds trigger escalation? How are reviewers trained and
monitored? HITL is not a catch-all solution: human reviewers can
rubber-stamp automated decisions, introduce their own biases, or become
overwhelmed by alert volume. Effective HITL design requires calibrating
the human-machine boundary to the specific application risks and
reviewer capabilities.

This framework parallels aviation pre-flight checklists, where pilots
follow every item without exception to ensure systematic coverage of
critical concerns despite time pressure. Production ML deployments
require equivalent discipline and systematic
verification.\sidenote{\textbf{Checklist Discipline}: Systematic
verification ensuring consistent coverage of critical items, inspired by
aviation's dramatic accident reduction. Surgeon Atul Gawande's
``Checklist Manifesto'' (\citeproc{ref-gawande2009checklist}{Gawande
2009}) documents the WHO Surgical Safety Checklist study, which reduced
major complications by over one-third and mortality by 47\% across
diverse hospital settings. ML Model Cards and deployment checklists
similarly catch issues individual judgment misses, especially under
deadline pressure when shortcuts seem tempting. } Checklists ensure
teams ask the right questions; documentation standards ensure the
answers persist and travel with the model.

\subsection{Model Documentation
Standards}\label{sec-responsible-engineering-model-documentation-standards-bef6}

Model cards provide a standardized format for documenting ML models
(\citeproc{ref-mitchell2019model}{Mitchell et al.
2019}).\sidenote{\textbf{Model Cards}: Introduced by Margaret Mitchell,
Timnit Gebru, and colleagues in their 2018 paper presented at FAT* 2019
(Conference on Fairness, Accountability, and Transparency). The metaphor
draws from ``nutrition labels'' for food: just as consumers deserve to
know what ingredients and nutritional content their food contains, users
of ML models deserve to know the model's capabilities, limitations, and
intended uses. The companion concept ``Datasheets for Datasets'' (Gebru
et al., 2018) applies similar transparency principles to training data.
Together, these frameworks established documentation as a core
responsible AI practice. } Originally developed at Google, they capture
information essential for responsible deployment.

A complete model card includes architecture, training procedures,
hyperparameters, and implementation specifics that enable
reproducibility and auditing. The intended use section specifies primary
use cases, intended users, and applications where the model should not
be used, preventing scope creep where models designed for one purpose
are repurposed for higher-stakes applications. Factors capture
demographic groups, environmental conditions, and instrumentation
factors that might affect model performance, guiding evaluation strategy
and monitoring protocols.

The metrics section reports performance measures including disaggregated
results across relevant factors, as aggregate accuracy metrics alone
prove insufficient for responsible deployment. Evaluation data
documentation describes datasets used for evaluation, their composition,
and their limitations, providing essential context for interpreting
performance results. Training data documentation enables assessment of
potential encoded biases. Ethical considerations document known
limitations, potential harms, and mitigations implemented, making
implicit tradeoffs explicit. Caveats and recommendations provide
guidance for users on appropriate use, known failure modes, and
recommended safeguards.

How do these abstract categories translate to practical documentation?
Consider Table~\ref{tbl-model-card-example}: a MobileNetV2 model
prepared for edge deployment shows how each section addresses specific
deployment concerns.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7500}}@{}}
\caption{\textbf{Example Model Card: MobileNetV2 for Edge Deployment}:
This excerpt demonstrates how abstract model card categories translate
to practical documentation that guides responsible deployment
decisions.}\label{tbl-model-card-example}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Section}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Content}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Section}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Content}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Model Details} & MobileNetV2 architecture with 3.5M parameters,
trained on ImageNet using depthwise separable convolutions. INT8
quantized for edge deployment. \\
\textbf{Intended Use} & Real-time image classification on mobile devices
with less than 50ms latency requirement. Suitable for consumer
applications including photo organization and accessibility features. \\
\textbf{Factors} & Performance varies with image quality (blur,
lighting), object size in frame, and categories outside ImageNet
distribution. \\
\textbf{Metrics} & 71.8\% top-1 accuracy on ImageNet validation (full
precision: 72.0\%). Accuracy varies by category: 85\% on common objects,
45\% on fine-grained distinctions. \\
\textbf{Ethical} \textbf{Considerations} & Training data reflects
ImageNet biases in geographic and demographic representation. Not
validated for high-stakes applications (medical diagnosis, security
screening). Performance may degrade on images from underrepresented
regions. \\
\end{longtable}

Datasheets for datasets provide analogous documentation for training
data (\citeproc{ref-gebru2021datasheets}{Gebru et al. 2021}). These
documents capture data provenance, collection methodology, demographic
composition, and known limitations that affect downstream model
behavior. Documentation establishes what a model is designed to do;
testing verifies whether it actually performs equitably across the
populations it serves.

\subsection{Testing Across
Populations}\label{sec-responsible-engineering-testing-across-populations-9f20}

Aggregate performance metrics mask significant disparities across user
populations. As shown in Table~\ref{tbl-gender-shades-results}, systems
can appear highly accurate in aggregate while showing 40x error rate
disparities across demographic groups. Responsible testing requires
disaggregated evaluation that examines performance for relevant
subgroups.

\phantomsection\label{callout-perspectiveux2a-1.5}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Danger of Averages}
\phantomsection\label{callout-perspective*-1.5}
\textbf{Averages Hide Failures}: In systems engineering, we rarely
design for the ``average'' case; we design for the \textbf{tail cases}
and \textbf{boundary conditions}. A bridge that is ``safe on average''
but collapses under a heavy truck is a failure. Similarly, an ML system
that is ``accurate on average'' but fails for a specific ethnic or
gender group is an engineering failure. Dave Patterson often points out
that just as we use \textbf{tail latency} (p99) to measure system
reliability, we must use \textbf{disaggregated evaluation} to measure
system fairness. If you only look at the aggregate accuracy, you are
blinded to the systemic failures occurring in the margins. Responsible
engineering requires making these ``tails'' visible through granular,
population-specific measurement.

\end{fbx}

The specific ``tails'' that matter depend on the workload. A vision
model fails differently than a recommendation system, and the fairness
metrics must match the failure mode.

\phantomsection\label{callout-lighthouseux2a-1.6}
\begin{fbx}{callout-lighthouse}{Lighthouse Example: }{Fairness Concerns by Archetype}
\phantomsection\label{callout-lighthouse*-1.6}
The dominant fairness risks differ by workload archetype, requiring
different evaluation strategies:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1307}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2941}}@{}}
\caption{Fairness risks vary by archetype's data source and deployment
context.}\label{tbl-fairness-archetype}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Fairness Risk}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Evaluation Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Real-World Example}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Fairness Risk}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Evaluation Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Real-World Example}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} (Compute Beast) & Training data bias
(underrepresentation of minority groups in ImageNet) &
\textbf{Disaggregated accuracy} by demographic group & Gender Shades:
99\% accuracy on light-skinned males, 65\% on dark-skinned females
(\citeproc{ref-buolamwini2018}{Buolamwini and Gebru 2018b}) \\
\textbf{GPT-2} (Bandwidth Hog) & Corpus bias (overrepresentation of
majority viewpoints in web text) & \textbf{Toxicity rate} by demographic
prompt context; \textbf{stereotype score} & LLMs produce more toxic
completions for prompts mentioning minority groups \\
\textbf{DLRM} (Sparse Scatter) & Feedback loop amplification (popular
items get more data) & \textbf{Exposure fairness} across item
categories; \textbf{supplier diversity} & Filter bubbles: system
recommends same content to similar users, reducing discovery of niche
creators \\
\textbf{DS-CNN} (Tiny Constraint) & Deployment context mismatch (trained
on clean audio, deployed in noisy real-world environments) &
\textbf{False positive rate} by acoustic environment and speaker accent
& Voice assistants perform worse on accented speech; wake-word triggers
on TV audio in some languages \\
\end{longtable}

\textbf{Key insight}: Fairness evaluation must match the archetype's
failure mode. Vision models require demographic stratification of
accuracy; LLMs require toxicity and stereotype probing; recommendation
systems require exposure audits; TinyML requires acoustic environment
diversity testing.

\end{fbx}

Engineers should identify relevant subgroups based on application
context. For healthcare applications, demographic factors like race,
age, and gender are essential. For content moderation, language and
cultural context matter. For financial services, protected categories
under fair lending laws require specific attention.

Testing infrastructure should support stratified evaluation where
performance metrics are computed separately for each relevant subgroup,
enabling comparison of error rates and error types across populations.
Intersectional analysis considers combinations of attributes because
harms may concentrate at intersections not visible in single-factor
analysis. Confidence intervals provide uncertainty quantification for
subgroup metrics when small subgroup sizes may yield unreliable
estimates. Temporal monitoring tracks subgroup performance over time,
detecting drift that affects some populations before others.

Several open source tools support responsible testing workflows.
Fairlearn provides fairness metrics and mitigation algorithms that
integrate with scikit-learn pipelines
(\citeproc{ref-bird2020fairlearn}{Bird et al. 2020}). AI Fairness 360
from IBM offers over 70 fairness metrics and 10 bias mitigation
algorithms across the ML lifecycle
(\citeproc{ref-bellamy2019aif360}{Bellamy et al. 2019}). Google's
What-If Tool enables interactive exploration of model behavior across
different subgroups without writing code. These tools lower the barrier
to systematic fairness evaluation, though they complement rather than
replace careful thinking about what fairness means in specific
application contexts.

\subsubsection{Worked Example: Fairness Analysis in Loan
Approval}\label{sec-responsible-engineering-worked-example-fairness-analysis-loan-approval-2c72}

A concrete example illustrates how fairness metrics reveal disparities
invisible in aggregate performance measures.
Table~\ref{tbl-confusion-group-a} and Table~\ref{tbl-confusion-group-b}
present confusion matrices for a loan approval model evaluated on two
demographic groups.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}@{}}
\caption{\textbf{Confusion Matrix for Group A (Majority)}: Loan approval
outcomes for 10,000 applicants from the majority demographic group. The
90\% true positive rate (4,500 approved of 5,000 qualified) and 20\%
false positive rate establish the baseline for fairness
comparison.}\label{tbl-confusion-group-a}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approved (pred)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Rejected (pred)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approved (pred)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Rejected (pred)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Repaid (actual)} & 4,500 (TP) & 500 (FN) \\
\textbf{Defaulted} \textbf{(actual)} & 1,000 (FP) & 4,000 (TN) \\
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}@{}}
\caption{\textbf{Confusion Matrix for Group B (Minority)}: Loan approval
outcomes for 2,000 applicants from the minority demographic group. The
60\% true positive rate (600 approved of 1,000 qualified) reveals a 30
percentage point disparity compared to Group A, indicating the model
applies stricter criteria to minority
applicants.}\label{tbl-confusion-group-b}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approved (pred)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Rejected (pred)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approved (pred)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Rejected (pred)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Repaid (actual)} & 600 (TP) & 400 (FN) \\
\textbf{Defaulted} \textbf{(actual)} & 200 (FP) & 800 (TN) \\
\end{longtable}

Three standard fairness metrics computed from these confusion matrices
reveal significant disparities.\sidenote{\textbf{Fairness Metrics
Origins}: These metrics formalize concepts from civil rights law into
mathematical constraints. ``Demographic parity'' (also called
``statistical parity'') requires outcomes independent of group
membership, echoing the principle behind the 1964 Civil Rights Act.
``Equal opportunity'' and ``equalized odds'' were formalized by Hardt,
Price, and Srebro (\citeproc{ref-hardt2016equality}{Hardt, Price, and
Srebro 2016}), who demonstrated that different fairness definitions are
mathematically incompatible. This impossibility result, proven by
Chouldechova (\citeproc{ref-chouldechova2017fair}{Chouldechova 2017}),
shows that except in special cases, no classifier can simultaneously
satisfy calibration and equal error rates across groups. }

Demographic parity requires equal approval rates across groups. Group A
receives approval at a rate of (4,500 + 1,000) / 10,000 = 55\%, while
Group B receives approval at (600 + 200) / 2,000 = 40\%. The 15
percentage point disparity indicates unequal treatment in approval
decisions.

Equal opportunity requires equal true positive rates among qualified
applicants. Group A achieves a TPR of 4,500 / (4,500 + 500) = 90\%,
meaning 90\% of applicants who would repay receive approval. Group B
achieves only 600 / (600 + 400) = 60\% TPR. This 30 percentage point
disparity means qualified applicants from Group B face substantially
higher rejection rates than equally qualified applicants from Group A.

Equalized odds requires both equal true positive rates and equal false
positive rates. Group A shows an FPR of 1,000 / (1,000 + 4,000) = 20\%,
and Group B shows 200 / (200 + 800) = 20\%. While false positive rates
are equal, the true positive rate disparity means equalized odds is
violated.

The pattern revealed by these metrics has a clear interpretation: the
model rejects qualified applicants from Group B at a much higher rate
(40\% false negative rate versus 10\%) while maintaining similar false
positive rates. This suggests the model has learned stricter approval
criteria for Group B, potentially encoding historical discrimination in
lending patterns where minority applicants faced higher scrutiny despite
equivalent qualifications.

Production systems must automate these calculations.
Listing~\ref{lst-fairness-metrics-code} implements the fairness metrics
computation, demonstrating how to derive demographic parity, equal
opportunity, and equalized odds from confusion matrix values.

\begin{codelisting}

\caption{\label{lst-fairness-metrics-code}\textbf{Disaggregated Fairness
Metrics}: Computing demographic parity, equal opportunity, and equalized
odds from confusion matrices reveals disparities invisible in aggregate
accuracy. Production systems automate these calculations across all
protected attributes, triggering alerts when disparities exceed
predefined thresholds.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}


\KeywordTok{def}\NormalTok{ compute\_fairness\_metrics(confusion\_matrix: }\BuiltInTok{dict}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{dict}\NormalTok{:}
    \CommentTok{"""}
\CommentTok{    Compute fairness metrics from confusion matrix values.}

\CommentTok{    Args:}
\CommentTok{        confusion\_matrix: Dict with keys \textquotesingle{}TP\textquotesingle{}, \textquotesingle{}FP\textquotesingle{}, \textquotesingle{}TN\textquotesingle{}, \textquotesingle{}FN\textquotesingle{}}

\CommentTok{    Returns:}
\CommentTok{        Dict containing approval\_rate, tpr, fpr, fnr}
\CommentTok{    """}
\NormalTok{    tp }\OperatorTok{=}\NormalTok{ confusion\_matrix[}\StringTok{"TP"}\NormalTok{]}
\NormalTok{    fp }\OperatorTok{=}\NormalTok{ confusion\_matrix[}\StringTok{"FP"}\NormalTok{]}
\NormalTok{    tn }\OperatorTok{=}\NormalTok{ confusion\_matrix[}\StringTok{"TN"}\NormalTok{]}
\NormalTok{    fn }\OperatorTok{=}\NormalTok{ confusion\_matrix[}\StringTok{"FN"}\NormalTok{]}

\NormalTok{    total }\OperatorTok{=}\NormalTok{ tp }\OperatorTok{+}\NormalTok{ fp }\OperatorTok{+}\NormalTok{ tn }\OperatorTok{+}\NormalTok{ fn}
\NormalTok{    positives }\OperatorTok{=}\NormalTok{ tp }\OperatorTok{+}\NormalTok{ fn  }\CommentTok{\# Actual positive class (would repay)}
\NormalTok{    negatives }\OperatorTok{=}\NormalTok{ fp }\OperatorTok{+}\NormalTok{ tn  }\CommentTok{\# Actual negative class (would default)}

    \CommentTok{\# Demographic parity: P(approved)}
    \CommentTok{\# Measures whether approval rates are equal across groups}
\NormalTok{    approval\_rate }\OperatorTok{=}\NormalTok{ (tp }\OperatorTok{+}\NormalTok{ fp) }\OperatorTok{/}\NormalTok{ total}

    \CommentTok{\# True positive rate (equal opportunity): P(approved | qualified)}
    \CommentTok{\# Measures whether qualified applicants are treated equally}
\NormalTok{    tpr }\OperatorTok{=}\NormalTok{ tp }\OperatorTok{/}\NormalTok{ positives }\ControlFlowTok{if}\NormalTok{ positives }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \FloatTok{0.0}

    \CommentTok{\# False positive rate: P(approved | unqualified)}
    \CommentTok{\# Together with TPR, determines equalized odds}
\NormalTok{    fpr }\OperatorTok{=}\NormalTok{ fp }\OperatorTok{/}\NormalTok{ negatives }\ControlFlowTok{if}\NormalTok{ negatives }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \FloatTok{0.0}

    \CommentTok{\# False negative rate: P(rejected | qualified)}
    \CommentTok{\# Reveals how often qualified applicants are incorrectly rejected}
\NormalTok{    fnr }\OperatorTok{=}\NormalTok{ fn }\OperatorTok{/}\NormalTok{ positives }\ControlFlowTok{if}\NormalTok{ positives }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \FloatTok{0.0}

    \ControlFlowTok{return}\NormalTok{ \{}
        \StringTok{"approval\_rate"}\NormalTok{: approval\_rate,}
        \StringTok{"tpr"}\NormalTok{: tpr,}
        \StringTok{"fpr"}\NormalTok{: fpr,}
        \StringTok{"fnr"}\NormalTok{: fnr,}
\NormalTok{    \}}


\CommentTok{\# Confusion matrices from loan approval example}
\NormalTok{group\_a }\OperatorTok{=}\NormalTok{ \{}\StringTok{"TP"}\NormalTok{: }\DecValTok{4500}\NormalTok{, }\StringTok{"FP"}\NormalTok{: }\DecValTok{1000}\NormalTok{, }\StringTok{"TN"}\NormalTok{: }\DecValTok{4000}\NormalTok{, }\StringTok{"FN"}\NormalTok{: }\DecValTok{500}\NormalTok{\}  }\CommentTok{\# Majority}
\NormalTok{group\_b }\OperatorTok{=}\NormalTok{ \{}\StringTok{"TP"}\NormalTok{: }\DecValTok{600}\NormalTok{, }\StringTok{"FP"}\NormalTok{: }\DecValTok{200}\NormalTok{, }\StringTok{"TN"}\NormalTok{: }\DecValTok{800}\NormalTok{, }\StringTok{"FN"}\NormalTok{: }\DecValTok{400}\NormalTok{\}  }\CommentTok{\# Minority}

\NormalTok{metrics\_a }\OperatorTok{=}\NormalTok{ compute\_fairness\_metrics(group\_a)}
\NormalTok{metrics\_b }\OperatorTok{=}\NormalTok{ compute\_fairness\_metrics(group\_b)}

\CommentTok{\# Compute disparities between groups}
\NormalTok{tpr\_disparity }\OperatorTok{=}\NormalTok{ metrics\_a[}\StringTok{"tpr"}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ metrics\_b[}\StringTok{"tpr"}\NormalTok{]}
\NormalTok{approval\_disparity }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    metrics\_a[}\StringTok{"approval\_rate"}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ metrics\_b[}\StringTok{"approval\_rate"}\NormalTok{]}
\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Group A TPR: }\SpecialCharTok{\{}\NormalTok{metrics\_a[}\StringTok{\textquotesingle{}tpr\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.1\%\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# 90.0\%}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Group B TPR: }\SpecialCharTok{\{}\NormalTok{metrics\_b[}\StringTok{\textquotesingle{}tpr\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.1\%\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# 60.0\%}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"TPR Disparity: }\SpecialCharTok{\{}\NormalTok{tpr\_disparity}\SpecialCharTok{:.1\%\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# 30.0 percentage points}
\CommentTok{\# Disparity exceeds typical 5\% threshold for high{-}stakes decisions}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The 30 percentage point TPR disparity far exceeds common industry
thresholds of 5 percentage points for high-stakes applications,
indicating the model requires fairness intervention before deployment.

Table~\ref{tbl-fairness-metrics-summary} reveals the troubling pattern
in these computed metrics and disparities.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1795}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1795}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2821}}@{}}
\caption{\textbf{Fairness Metrics Summary}: Comparison of fairness
metrics across demographic groups reveals substantial disparities in how
the model treats qualified applicants from each
group.}\label{tbl-fairness-metrics-summary}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Group A}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Group B}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Disparity}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Group A}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Group B}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Disparity}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Approval Rate} & 55\% & 40\% & 15 percentage points \\
\textbf{True Positive Rate} & 90\% & 60\% & 30 percentage points \\
\textbf{False Positive Rate} & 20\% & 20\% & 0 percentage points \\
\end{longtable}

Figure~\ref{fig-fairness-threshold} visualizes why aggregate metrics
hide these disparities. When a single threshold is applied to
populations with different score distributions, the same decision
boundary produces vastly different outcomes for each group.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/responsible_engr/images/png/fairness_cartoon.png}}

}

\caption{\label{fig-fairness-threshold}\textbf{Threshold Effects on
Subgroup Outcomes}. A single classification threshold (vertical lines)
applied to two subgroups with different score distributions produces
disparate outcomes. Circles represent positive outcomes (loan
repayment), crosses represent negative outcomes (default). The 75\%
threshold approves most of Subgroup A but rejects most of Subgroup B,
even when qualified individuals exist in both groups. The 81.25\%
threshold shows how threshold adjustment changes the fairness-accuracy
tradeoff. This visualization explains why aggregate accuracy can mask
severe subgroup disparities.}

\end{figure}%

Several mitigation approaches exist, each with distinct tradeoffs.
Threshold adjustment lowers the approval threshold for Group B to
equalize TPR but may increase false positives for that group.
Reweighting increases the weight of Group B samples during training to
give the model stronger signal about this population but may reduce
overall accuracy. Adversarial debiasing trains with an adversary that
prevents the model from learning group membership but adds training
complexity.\sidenote{\textbf{Adversarial Debiasing}: The term
``adversarial'' derives from Latin \emph{adversarius} (opponent), itself
from \emph{advertere} meaning ``to turn toward'' or ``turn against.''
This etymology captures the technique's mechanism: Zhang et al.
(\citeproc{ref-zhang2018adversarial}{Zhang, Lemoine, and Mitchell 2018})
introduced an in-processing fairness method where a predictor and
adversary network are pitted against each other. The predictor maximizes
task accuracy while the adversary attempts to predict protected
attributes from outputs. Gradient reversal during backpropagation
encourages the predictor to learn representations that conceal group
membership. Supports both demographic parity and equalized odds
constraints. Trade-offs: adds 20-50\% training time, may reduce accuracy
by 1-3\%, and requires careful hyperparameter tuning. } The choice among
these approaches requires stakeholder input about which tradeoffs are
acceptable in the specific application context. But how should engineers
present these tradeoffs to stakeholders? The answer lies in making the
tradeoffs explicit and quantifiable.

\subsubsection{The Fairness-Accuracy Pareto
Frontier}\label{sec-responsible-engineering-fairnessaccuracy-pareto-frontier-0b29}

In systems engineering, we rarely find a ``free lunch.'' Just as
optimizing for Power often sacrifices Performance, optimizing for
Fairness often sacrifices aggregate Accuracy. To surpass the qualitative
discussions of ethics, you must treat this tradeoff as a \textbf{Pareto
Frontier}\sidenote{\textbf{Pareto Frontier}: Named after Vilfredo Pareto
(1848-1923), an Italian economist and engineer who studied income
distribution and economic efficiency in the late 1800s. Pareto observed
that 80\% of Italy's land was owned by 20\% of the population, leading
to the ``Pareto principle.'' In optimization, the Pareto frontier
represents all solutions where improving one objective necessarily
worsens another. For ML fairness, this means configurations where you
cannot increase fairness without sacrificing accuracy (or vice versa).
The frontier makes explicit that fairness is not a bug to fix but a
design constraint requiring stakeholder input on acceptable tradeoffs.
}, the set of optimal configurations where you cannot improve one metric
without degrading another.

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Price of Fairness}
\phantomsection\label{callout-perspective*-1.7}
\textbf{The Problem}: Your stakeholders demand that you eliminate a 20\%
True Positive Rate (TPR) disparity in a hiring model. What is the
``Price of Fairness'' in terms of hiring quality?

\textbf{The Physics}: You can equalize TPRs by adjusting the
classification threshold (\(\tau\)) for the disadvantaged group.

\begin{itemize}
\item
  \textbf{Original State}: Group A (TPR=90\%), Group B (TPR=70\%).
  Aggregate Accuracy = 85\%.
\item
  \textbf{Intervention}: Lower \(\tau_B\) until \(\text{TPR}_B = 90\%\).
\item
  \textbf{The Cost}: Lowering the threshold increases \textbf{False
  Positives} (hiring candidates who don't meet the bar).
\end{itemize}

\textbf{The Calculation}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  To close the 20\% TPR gap, you must accept a \textbf{5\% increase} in
  False Positives.
\item
  If the value of a successful hire is\\
  00k and the cost of a bad hire is \$50k:

  \begin{itemize}
  \item
    Utility Loss = (Utility of Correct Hires) - (Cost of Extra False
    Positives).
  \item
    In this scenario, closing the gap reduces the system's \textbf{Total
    Utility by 3\%}.
  \end{itemize}
\end{enumerate}

\textbf{The Systems Conclusion}: The ``Price of Fairness'' in this
system is a 3\% utility tax. This is not a ``bug''; it is a
\textbf{System Constraint}. Your job is not to find a ``fair'' model,
but to present the \textbf{Pareto Curve} to stakeholders so they can
choose the Utility/Fairness tradeoff that aligns with organizational
values.

\end{fbx}

Quantifying disparities through metrics is necessary but not sufficient
for responsible deployment. When a loan applicant receives a rejection,
stating that ``the model's true positive rate for your demographic group
is 60\% compared to 90\% for other groups'' provides no actionable
information. The applicant needs to know: Why was \emph{my} application
rejected? What could I change? These questions require explainability,
which is the ability to articulate which input features drove specific
predictions.

\subsection{Explainability
Requirements}\label{sec-responsible-engineering-explainability-requirements-0b67}

Explainability serves multiple responsibility purposes: enabling human
oversight of automated decisions, supporting debugging when problems
emerge, and satisfying regulatory requirements for decision
transparency.

The level of explainability required varies by application context and
regulatory environment. Table~\ref{tbl-explainability-requirements} maps
common deployment scenarios to their explainability needs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2688}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4301}}@{}}
\caption{\textbf{Explainability Requirements by Domain}: Different
applications require different levels of decision transparency. Credit
and medical applications face regulatory requirements for individual
explanations. Fraud detection may intentionally limit explainability to
prevent gaming. The engineering challenge is matching explainability
mechanisms to domain
requirements.}\label{tbl-explainability-requirements}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Application Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Explainability Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Requirements}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Application Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Explainability Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Requirements}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Credit decisions} & Individual explanation required & Specific
factors contributing to denial must be disclosed to applicant \\
\textbf{Medical diagnosis} & Clinical reasoning support & Explanation
must support physician decision-making, not replace it \\
\textbf{Content moderation} & Appeal-supporting & Sufficient detail for
users to understand and contest decisions \\
\textbf{Recommendation} & Transparency optional & ``Because you watched
X'' sufficient for most contexts \\
\textbf{Fraud detection} & Internal audit only & Detailed explanations
may enable adversarial gaming \\
\end{longtable}

Engineering teams should select explainability approaches based on these
domain requirements. Post-hoc explanation methods (LIME, SHAP) generate
feature importance scores for individual predictions without requiring
model architecture changes.\sidenote{\textbf{LIME and SHAP}: Two
dominant post-hoc explainability methods with different computational
trade-offs. LIME (Local Interpretable Model-agnostic Explanations)
(\citeproc{ref-ribeiro2016why}{M. T. Ribeiro, Singh, and Guestrin 2016})
fits a simple interpretable model around each prediction, offering
faster computation but potentially inconsistent explanations. SHAP
derives its name from SHapley Additive exPlanations, honoring Lloyd
Shapley, the mathematician who introduced Shapley values in his 1953
game theory work on fair allocation of cooperative gains. Lundberg and
Lee (\citeproc{ref-lundberg2017unified}{Lundberg and Lee 2017}) adapted
this framework to compute feature contributions, providing
mathematically consistent explanations but with exponential worst-case
complexity. Shapley received the 2012 Nobel Prize in Economics for this
foundational work. Systems implication: SHAP may add 10-100x inference
latency, making LIME preferable for real-time applications. } Inherently
interpretable models (linear models, decision trees, attention
mechanisms) provide explanations as part of their structure but may
sacrifice predictive performance. Concept-based explanations map model
behavior to human-understandable concepts rather than raw features. The
choice involves tradeoffs between explanation fidelity, computational
cost, and model flexibility. Figure~\ref{fig-interpretability-spectrum}
illustrates this spectrum of model interpretability.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/responsible_engr/images/png/interpretability_model_spectrum.png}}

}

\caption{\label{fig-interpretability-spectrum}\textbf{Model
Interpretability Spectrum}. Model architectures vary in their inherent
interpretability. Decision trees and linear regression are intrinsically
interpretable, meaning their decision logic can be directly inspected.
Neural networks and convolutional networks require post-hoc explanation
techniques (LIME, SHAP) to understand their behavior. When regulatory
requirements demand explainability, this spectrum constrains model
selection: high-stakes applications may require inherently interpretable
models even if they sacrifice some predictive performance.}

\end{figure}%

\subsection{The Regulatory
Landscape}\label{sec-responsible-engineering-regulatory-landscape-1ec1}

The explainability requirements in
Table~\ref{tbl-explainability-requirements} are not merely engineering
best practices; many are legal mandates that carry penalties for
non-compliance. Responsible engineering increasingly operates within
explicit regulatory frameworks that mandate specific technical
requirements. While regulations vary by jurisdiction, several patterns
are emerging globally that engineers must understand.

\textbf{The EU AI Act} establishes the most comprehensive framework to
date, classifying AI systems by risk level and mandating requirements
accordingly.\sidenote{\textbf{EU AI Act (Regulation 2024/1689)}: The
world's first comprehensive legal framework for AI, entered into force 1
August 2024 with phased compliance deadlines: prohibited AI practices by
February 2025, general-purpose AI model requirements by August 2025, and
high-risk AI systems by August 2026. The Act defines four risk tiers:
unacceptable (banned), high-risk (strict requirements), limited risk
(transparency obligations), and minimal risk (no requirements).
Penalties reach up to 35 million EUR or 7\% of global annual turnover
for prohibited practices, and 20 million EUR or 4\% for high-risk
non-compliance. The Act has extraterritorial reach: US organizations
must comply if AI outputs affect EU residents. Systems engineering
implications: high-risk AI requires CE marking, conformity assessment,
logging infrastructure for audit trails, and human oversight mechanisms
built into the architecture. } High-risk systems (including those used
in employment, credit, education, and critical infrastructure) must
implement risk management systems, data governance practices, technical
documentation, transparency measures, human oversight mechanisms, and
accuracy/robustness/security requirements. The engineering implications
are substantial: systems must be designed for auditability from
inception, with documentation practices that demonstrate compliance.

\textbf{GDPR's Article 22} grants EU citizens the right not to be
subject to decisions based solely on automated processing that produce
legal or similarly significant effects.\sidenote{\textbf{GDPR Article
22}: Establishes that individuals have the right not to be subject to
decisions ``based solely on automated processing'' that produce ``legal
effects'' or ``similarly significantly affects'' them. The European Data
Protection Board clarifies that human involvement must be substantive,
not mere rubber-stamping. Recital 71 requires providing ``specific
information'' and the ``right to obtain an explanation.'' Systems
engineering implications: high-stakes ML systems must implement
meaningful human-in-the-loop review (not just approval workflows),
maintain audit logs of automated decisions, and provide explainability
infrastructure that generates human-readable justifications for
individual predictions. } This creates requirements for human oversight
in automated decision systems and for providing ``meaningful information
about the logic involved.'' While legal interpretation varies,
engineering teams should assume that high-stakes automated decisions
require both human review mechanisms and explainability capabilities.

\textbf{US sectoral regulations} impose domain-specific requirements.
Fair lending laws (ECOA, Fair Housing Act) require creditors to provide
specific reasons for adverse credit decisions, the origin of the
``adverse action notice'' requirement that drives explainability needs
in financial ML. Healthcare regulations (HIPAA, FDA guidance) impose
data protection and validation requirements on medical AI systems.
Employment law prohibits discriminatory hiring practices regardless of
whether discrimination results from human or algorithmic
decision-making.

The engineering response to this regulatory landscape is proactive
design. Systems built with documentation, monitoring, explainability,
and human oversight from inception can demonstrate compliance
efficiently. Systems where these capabilities must be retrofitted face
expensive redesign or deployment constraints. The foundation established
here, that responsibility is an engineering requirement rather than a
legal afterthought, enables more sophisticated compliance strategies as
regulatory frameworks mature. Yet even well-designed systems can fail,
making incident response preparation essential.

\subsection{Incident Response
Preparation}\label{sec-responsible-engineering-incident-response-preparation-a145}

Planning for system failures before they occur is a core responsibility
engineering practice. Table~\ref{tbl-incident-response} structures this
preparation into five components, specifying both the requirements and
pre-deployment verification criteria for each.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3900}}@{}}
\caption{\textbf{Incident Response Framework}: Systematic preparation
for ML system failures requires five distinct components. Detection
identifies anomalies through specialized monitoring; assessment
evaluates scope using severity classifications; mitigation reduces harm
through tested rollback procedures; communication notifies stakeholders
through pre-approved channels; remediation implements permanent fixes
through root cause analysis. Each component requires both operational
requirements and pre-deployment
verification.}\label{tbl-incident-response}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Requirements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pre-Deployment Verification}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Requirements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pre-Deployment Verification}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Detection} & Monitoring systems that identify anomalies,
degraded performance, and fairness violations & Alert thresholds tested,
on-call rotation established, escalation paths documented \\
\textbf{Assessment} & Procedures for evaluating incident scope and
severity & Severity classification defined, impact assessment templates
prepared \\
\textbf{Mitigation} & Technical capabilities to reduce harm while
investigation proceeds & Rollback procedures tested, fallback systems
operational, kill switches functional \\
\textbf{Communication} & Protocols for stakeholder notification &
Contact lists current, message templates prepared, approval chains
defined \\
\textbf{Remediation} & Processes for permanent fixes and system
improvements & Root cause analysis procedures, change management
integration \\
\end{longtable}

ML systems create unique maintenance challenges
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). Models can
degrade silently, dependencies can shift unexpectedly, and feedback
loops can amplify small problems into large ones. Incident response
planning must account for these ML-specific failure modes, but effective
response requires the continuous monitoring infrastructure that detects
problems in the first place.

\subsection{Continuous Monitoring
Requirements}\label{sec-responsible-engineering-continuous-monitoring-requirements-c5ad}

The monitoring infrastructure from
\textbf{?@sec-machine-learning-operations-mlops} provides the foundation
for responsible system operation, extending traditional operational
metrics to include outcome quality measures.

Key monitoring dimensions include performance stability to track
prediction quality over time and detect gradual degradation that might
not trigger immediate alerts. Subgroup parity monitoring tracks
performance across demographic groups to detect emerging disparities
before they cause significant harm. Input distribution monitoring tracks
changes that might indicate population shift or adversarial
manipulation. Outcome monitoring validates that predictions translate to
intended results where actual outcomes can be tracked. User feedback
systems provide systematic collection and analysis of user complaints
and corrections that might indicate problems invisible to automated
monitoring.

Effective monitoring requires both data collection and review processes.
Dashboards that no one examines provide no protection. Engineering teams
should establish regular review cadences with clear ownership and
escalation procedures.

The frameworks established in this section address one dimension of
responsible engineering: ensuring systems work fairly and reliably
across user populations. But responsible engineering has a second
dimension that many teams overlook: resource consumption. A model that
requires 4x more compute than necessary does not just waste money. It
excludes organizations without massive compute budgets from
participating in ML deployment, consumes 4x more electricity, and
generates 4x more carbon emissions. When multiplied across millions of
inference requests, these inefficiencies accumulate into environmental
impact at planetary scale.

\phantomsection\label{quiz-question-sec-responsible-engineering-responsible-engineering-checklist-844e}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.3}{}
\phantomsection\label{quiz-question-sec-responsible-engineering-responsible-engineering-checklist-844e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary purpose of a `Model Card' in responsible
  engineering?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To act as a warranty for the model software.
  \item
    To provide a standardized format for documenting intended use,
    performance factors, and ethical considerations.
  \item
    To store the binary weights of the trained neural network.
  \item
    To list all the developers who worked on the project.
  \end{enumerate}
\item
  Define `Disaggregated Evaluation' and explain why aggregate accuracy
  metrics can be misleading.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-responsible-engineering-responsible-engineering-checklist-844e]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Environmental and Cost
Awareness}\label{sec-responsible-engineering-environmental-cost-awareness-0f3e}

The connection between efficiency and responsibility may not be
immediately obvious. We typically think of fairness and environmental
impact as separate concerns, but they share a common root: both require
engineers to account for costs that do not appear in accuracy metrics.
The optimization techniques from \textbf{?@sec-model-compression} and
\textbf{?@sec-ai-acceleration} are not just performance tools; they are
responsibility tools. They determine whether ML capabilities remain
accessible to organizations without massive compute budgets and whether
the environmental cost of deployment remains sustainable. Understanding
resource costs enables informed tradeoffs rather than defaulting to the
largest available model.

\subsection{The Efficiency-Responsibility
Connection}\label{sec-responsible-engineering-efficiencyresponsibility-connection-1c01}

The computational demands of modern ML systems have grown dramatically.
Training large language models requires thousands of GPU hours,
consuming energy measured in megawatt-hours
(\citeproc{ref-strubell2019energy}{Strubell, Ganesh, and McCallum
2019}). But much of this expense reflects inefficient practices:
training from scratch when fine-tuning would suffice, using larger
models than tasks require, and running hyperparameter searches that
explore redundant configurations. Computational cost is largely a
function of engineering practice, not inherent model
requirements.\sidenote{\textbf{Green AI Movement}: Schwartz et al.
(\citeproc{ref-schwartz2020green}{Schwartz et al. 2020}) contrast ``Red
AI'' (performance at any cost) with ``Green AI'' (efficiency as primary
metric). They propose reporting FLOPs alongside accuracy, documenting
that state-of-the-art accuracy gains from 2012--2018 (AlexNet to
AlphaZero) required a 300,000× compute increase, with NLP models
following a similar exponential trend. Responsible engineering embraces
Green AI: optimizing for performance-per-watt and carbon-aware training.
}

Resource efficiency and responsible engineering are fundamentally linked
through three mechanisms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Environmental Impact}: A model that requires 4× more compute
  than necessary generates 4× more carbon emissions. The efficiency
  techniques from \textbf{?@sec-model-compression} that enable edge
  deployment also reduce the environmental footprint of cloud inference.
\item
  \textbf{Accessibility}: Resource-efficient models can run on less
  expensive hardware, democratizing access to ML capabilities. A
  quantized model that runs on a smartphone enables users who cannot
  afford cloud API costs.
\item
  \textbf{Sustainability at Scale}: Systems serving millions of users
  multiply inefficiencies across every request. A 10ms latency reduction
  per query translates to thousands of GPU-hours saved annually.
\end{enumerate}

The techniques from earlier chapters directly serve responsibility
goals. Quantization (\textbf{?@sec-model-compression}) reduces compute
by 2--4× with minimal accuracy impact. Pruning removes 50--90\% of
parameters. Knowledge distillation typically achieves 5--20× compression
while retaining 90--95\% of the original accuracy. Hardware acceleration
(\textbf{?@sec-ai-acceleration}) achieves 10--100× better energy
efficiency than general-purpose processors.

Responsible engineers apply these techniques as design requirements, not
afterthoughts. The question shifts from ``What is the most accurate
model?'' to ``What is the most accurate model that meets our efficiency
constraints?''

\subsection{Efficiency Engineering in
Practice}\label{sec-responsible-engineering-efficiency-engineering-practice-d6c9}

Translating efficiency principles into practice requires measurable
targets. The goal is selecting the smallest model that meets task
requirements, then applying systematic optimization to reduce resource
consumption further.

Edge deployment scenarios make efficiency requirements concrete. When a
wearable device has a 500 mW power budget and must run inference
continuously for 24 hours on a small battery, abstract efficiency
discussions become engineering constraints with measurable consequences.
Table~\ref{tbl-edge-deployment-constraints} quantifies these constraints
across four deployment contexts, from smartphones with 3W budgets to IoT
sensors operating at 100mW.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2525}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1919}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2626}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2727}}@{}}
\caption{\textbf{Edge Deployment Constraints}: Power and latency
requirements across four deployment contexts. Smartphones allow 3W and
100ms latency for photo enhancement and voice assistants. IoT sensors
operate at 100mW with 1-second tolerance for anomaly detection. Embedded
cameras require 1W at 33ms (30 FPS) for real-time object detection.
Wearables budget 500mW with 500ms latency for health monitoring. These
concrete constraints transform abstract efficiency discussions into
engineering
requirements.}\label{tbl-edge-deployment-constraints}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Power Budget}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency Requirement}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Use Cases}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Power Budget}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency Requirement}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Use Cases}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Smartphone} & 3 W & 100 ms & Photo enhancement, voice
assistants \\
\textbf{IoT Sensor} & 100 mW & 1 second & Anomaly detection,
environmental monitoring \\
\textbf{Embedded Camera} & 1 W & 30 FPS (33 ms) & Real-time object
detection, surveillance \\
\textbf{Wearable Device} & 500 mW & 500 ms & Health monitoring, activity
recognition \\
\end{longtable}

Table~\ref{tbl-model-efficiency-comparison} compares how model
architectures fit different deployment constraints.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1864}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1441}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1864}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1186}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1949}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1356}}@{}}
\caption{\textbf{Model Efficiency Comparison}: Model selection must
account for deployment constraints. Larger models provide better
accuracy but require more power and time. The smallest model that meets
accuracy requirements minimizes both cost and environmental
impact.}\label{tbl-model-efficiency-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Inference Power}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fits Smartphone?}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fits IoT?}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Inference Power}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fits Smartphone?}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fits IoT?}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MobileNetV2} & 3.5 M & 1.2 W & 40 ms & Yes & No \\
\textbf{EfficientNet-B0} & 5.3 M & 1.8 W & 65 ms & Yes & No \\
\textbf{ResNet-50} & 25.6 M & 4.5 W & 180 ms & No & No \\
\textbf{TinyML Model} & 50 K & 50 mW & 200 ms & Yes & Yes \\
\end{longtable}

These concrete benchmarks provide actionable guidance for efficiency
optimization. The techniques that enable deployment on power-constrained
platforms (quantization, pruning, and efficient architectures) directly
reduce environmental impact per inference regardless of deployment
context. Power savings at inference time translate directly to financial
savings when aggregated across millions of requests.

\subsection{Total Cost of
Ownership}\label{sec-responsible-engineering-total-cost-ownership-35c1}

Power budgets translate directly to financial costs: a model that
consumes 2W instead of 4W cuts electricity expenses in half. Similarly,
latency requirements determine throughput, which determines how much
hardware a service requires. Understanding these translations enables
engineers to make efficiency decisions with full awareness of their
economic implications.

Financial cost analysis for ML systems must extend beyond initial
training to encompass the full lifecycle. For successful production
systems, inference costs typically exceed training costs by 10 to 1000
times depending on traffic volume. This dominance of inference costs
changes where optimization efforts should focus.

Consider a concrete example of a recommendation system serving 10
million users daily. Training costs appear substantial: data preparation
consumes 100 GPU-hours at approximately \$4 per hour (\$400),
hyperparameter search across multiple configurations requires 500
GPU-hours (\$2,000), and the final training run uses 200 GPU-hours
(\$800). Total training cost reaches approximately \$3,200.

Inference costs dominate. With 10 million users each receiving 20
recommendations per day, the system serves 200 million inferences daily.
Assuming 10 milliseconds per inference on GPU hardware, the system
requires approximately 23 GPUs running continuously. At \$2.50 per
GPU-hour, annual GPU costs reach \$504,300.

Over a three-year operational period, quarterly retraining produces
total training costs of approximately \$10,000, while inference costs
over the same period total \$1.5 million. The 150:1 ratio between
inference and training costs is typical for production systems and has
significant implications for engineering priorities.

Per-query optimization becomes essential when serving billions of
requests. Reducing inference latency by 10 milliseconds per query
translates to substantial reductions in required hardware across
billions of queries despite appearing negligible for individual
requests. Hardware selection between CPU, GPU, and TPU deployment
changes costs and carbon footprint by factors of 10 or more. Model
compression through quantization and pruning delivers immediate return
on investment for high-volume systems because inference cost reduction
compounds across every subsequent query.

Total cost of ownership encompasses additional dimensions beyond
computation. Operational costs include monitoring, maintenance,
retraining, and incident response. These costs scale with system
complexity and the rate of distribution shift in the application domain.
Opportunity costs reflect that resources consumed by ML systems cannot
be used for other purposes. Wasteful resource consumption in one project
constrains what other projects can attempt.

Engineers should evaluate whether the value an ML system delivers
justifies its resource consumption. A recommendation system that
increases engagement by 1\% might not justify millions of dollars in
computational costs, while a medical diagnosis system that saves lives
does. Explicit tradeoffs enable responsible resource
allocation.\sidenote{\textbf{ML Return on Investment}: Rigorous analysis
comparing ML deployment costs (infrastructure, maintenance, technical
debt) against business value delivered. Industry experience suggests
most ML projects never reach production; of those deployed, many fail to
justify costs. Responsible engineering requires honest assessment: a
simple heuristic sometimes outperforms complex ML at a fraction of the
cost. }

\phantomsection\label{callout-perspectiveux2a-1.8}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Carbon Cost of Compute}
\phantomsection\label{callout-perspective*-1.8}
\textbf{Quantifying Environmental Impact}: To make carbon a first-class
engineering metric, we must convert ``compute hours'' into ``kg CO2eq''.
The standard formula is:

\[ \text{Carbon} = \text{Energy (kWh)} \times \text{Carbon Intensity (kg/kWh)} \]

For the TCO examples below, we use these baseline assumptions: *
\textbf{Power}: 400W per GPU-hour (including PUE cooling overhead). *
\textbf{Intensity}: 0.4 kg CO2eq/kWh (global grid average). *
\textbf{Conversion Factor}:
\(0.4 \text{ kW} \times 0.4 \text{ kg/kWh} = \mathbf{0.16 \text{ kg CO2eq per GPU-hour}}\).

This conversion allows us to track ``Carbon Cost'' alongside ``Dollar
Cost'' in our ledgers.

\end{fbx}

\subsubsection{TCO Calculation
Methodology}\label{sec-responsible-engineering-tco-calculation-methodology-7cb0}

Engineers can estimate three-year total cost of ownership using a
structured approach that accounts for training, inference, and
operational costs. The following methodology applies to the
recommendation system example discussed above.

\textbf{Training Costs} include both initial development and ongoing
retraining. Table~\ref{tbl-tco-training} breaks down these costs,
showing how quarterly retraining cycles accumulate over a three-year
operational period.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2957}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2783}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2174}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1913}}@{}}
\caption{\textbf{Training Cost Calculation}: Training costs accumulate
through initial development (\$3,200 per cycle) and quarterly retraining
over a three-year operational period. Data preparation, hyperparameter
search, and final training each consume GPU hours at \$4/hour, totaling
\$38,400 across 12 training cycles. Despite appearing substantial,
training represents only 2\% of total cost of
ownership.}\label{tbl-tco-training}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calculation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Financial Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Carbon (kg CO2)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calculation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Financial Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Carbon (kg CO2)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Initial data preparation} & hours × rate & 100 GPU-hr × \$4 =
\$400 & 16 kg \\
\textbf{Hyperparameter search} & experiments × cost/experiment & 50 ×
\$40 = \$2,000 & 80 kg \\
\textbf{Final training} & hours × rate & 200 GPU-hr × \$4 = \$800 & 32
kg \\
\textbf{Subtotal per training cycle} & & \textbf{\$3,200} & \textbf{128
kg} \\
\textbf{Retraining frequency} & cycles/year × years & 4/year × 3 years =
12 & 12 \\
\textbf{Total training cost} & subtotal x cycles & \textbf{\$38,400} &
\textbf{1,536 kg} \\
\end{longtable}

\textbf{Inference Costs} typically dominate total cost of ownership for
production systems, as Table~\ref{tbl-tco-inference} details.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2718}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2233}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2718}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2136}}@{}}
\caption{\textbf{Inference Cost Calculation}: Inference costs scale with
query volume: 200 million daily queries at 10 ms each require 556
GPU-hours daily, totaling \$507K annually and \$1.52M over three years.
At 74\% of total cost, inference dominates for high-traffic systems and
justifies aggressive per-query optimization through quantization,
pruning, and efficient serving.}\label{tbl-tco-inference}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calculation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Financial Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Carbon (kg CO2)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calculation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Financial Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Carbon (kg CO2)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Daily queries} & users × queries/user & 10M × 20 = 200M &
\begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
\end{itemize}
\end{minipage} \\
\textbf{GPU-seconds/day} & queries × latency & 200M × 0.01 s = 2M sec &
\begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
\end{itemize}
\end{minipage} \\
\textbf{GPU-hours/day} & seconds ÷ 3600 & 556 GPU-hr & 89 kg \\
\textbf{Annual GPU cost} & hours × 365 × rate & 556 × 365 × \$2.50 =
\$507K & 32,470 kg \\
\textbf{3-year inference cost} & annual x 3 & \textbf{\$1.52M} &
\textbf{97,410 kg} \\
\end{longtable}

\textbf{Operational Costs} encompass infrastructure, personnel, and
incident response. Table~\ref{tbl-tco-operations} itemizes these ongoing
expenses, which often surprise teams focused primarily on compute costs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4615}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2821}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2436}}@{}}
\caption{\textbf{Operational Cost Calculation}: Operational costs
include monitoring infrastructure (\$50K/year), on-call engineering at
0.5 FTE (\$100K/year), and incident response reserves (\$20K/year). The
\$510K three-year total represents 24\% of TCO and often surprises teams
focused primarily on compute costs. These expenses persist regardless of
model performance and grow with system
complexity.}\label{tbl-tco-operations}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Annual Estimate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{3-Year Total}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Annual Estimate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{3-Year Total}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Monitoring infrastructure} & \$50K & \$150K \\
\textbf{On-call engineering (0.5 FTE)} & \$100K & \$300K \\
\textbf{Incident response (estimated)} & \$20K & \$60K \\
\textbf{Total operational} & & \textbf{\$510K} \\
\end{longtable}

The stark breakdown in Table~\ref{tbl-tco-summary} answers where the
money actually goes: inference at 74\%, operations at 24\%, and training
at just 2\%.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2297}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2432}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2297}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2703}}@{}}
\caption{\textbf{Total Cost of Ownership Summary}: Three-year TCO of
\$2.07M breaks down as: training \$38K (2\%), inference \$1.52M (74\%),
and operations \$510K (24\%). The 37:1 ratio between inference and
training costs is typical for production systems serving 10 million
daily users. A 20\% reduction in inference latency through quantization
would save \$304K and approximately 20 tons of CO2, easily justifying
the optimization engineering
investment.}\label{tbl-tco-summary}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{3-Year Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Percentage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Carbon Impact}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{3-Year Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Percentage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Carbon Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Training} & \$38K & 2\% & 1.5 tons \\
\textbf{Inference} & \$1.52M & 74\% & 97.4 tons \\
\textbf{Operations} & \$510K & 24\% &
\begin{minipage}[t]{\linewidth}\raggedleft
\begin{itemize}
\tightlist
\item
\end{itemize}
\end{minipage} \\
\textbf{Total TCO} & \textbf{\$2.07M} & 100\% &
\textbf{\textasciitilde99 tons} \\
\end{longtable}

\subsection{Environmental
Impact}\label{sec-responsible-engineering-environmental-impact-59fe}

The TCO analysis above captures costs that appear on invoices, but
computational resources carry costs that no invoice reflects.
Environmental impact follows from computational efficiency: the same
optimization techniques that reduce TCO also reduce carbon emissions.
The optimization techniques from \textbf{?@sec-data-efficiency} and
\textbf{?@sec-model-compression} reduce energy consumption per
inference, directly lowering carbon footprint. Data centers consume
approximately 1--2\% of global electricity (estimated at 1--1.3\% in
2022, rising toward 2\% by 2025), with ML workloads representing a
growing fraction (\citeproc{ref-henderson2020towards}{Henderson et al.
2020}). Engineers can reduce this impact by selecting cloud regions
powered by renewable energy (5× carbon reduction), applying model
efficiency techniques (2--4× reduction through quantization), and
scheduling intensive workloads during periods of abundant renewable
energy.

\phantomsection\label{callout-perspectiveux2a-1.9}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Carbon Cost of Scale}
\phantomsection\label{callout-perspective*-1.9}
\textbf{Problem}: You are training a foundation model at the scale of
\textbf{GPT-3}. Your training run consumes \textbf{1,300 Megawatt-hours
(MWh)} of electricity. What is the environmental impact?

\textbf{The Math}: 1. \textbf{Energy Consumption}: 1,300 MWh = 1,300,000
kWh. 2. \textbf{Carbon Intensity}: The average US grid emits \(\approx\)
\textbf{0.4 kg CO2 per kWh}. 3. \textbf{Total Emissions}:
\(1,300,000 \times 0.4 = \mathbf{520,000 \text{ kg CO2}}\) (520 metric
tons). 4. \textbf{Comparison}: A typical passenger car emits \(\approx\)
4.6 metric tons of CO2 per year.

\textbf{The Systems Conclusion}: Training a single state-of-the-art
model is equivalent to the annual carbon footprint of \textbf{113 cars}.
This scale of consumption transforms efficiency from a technical
preference into a moral requirement. Every 1\% improvement in the
\textbf{Efficiency (\(\eta\))} of your training pipeline removes the
equivalent of one car's annual emissions from the atmosphere.

\end{fbx}

The key insight is that efficiency optimization and environmental
responsibility align: the techniques that reduce inference costs also
reduce carbon emissions per prediction. More sophisticated carbon
accounting methodologies, such as lifecycle assessment, scope 1/2/3
emissions tracking, and carbon-aware scheduling, build upon this
foundation for organizations requiring detailed environmental impact
analysis.

We have now established the complete responsible engineering toolkit:
systematic assessment and documentation before deployment, fairness
testing across populations, explainability for stakeholder trust,
regulatory compliance, incident response preparation, continuous
monitoring, and efficiency optimization that reduces both costs and
environmental impact. Yet possessing these tools is not enough. Teams
that understand responsible engineering principles still fail, often in
predictable ways that stem from intuitions developed in traditional
software engineering.

\phantomsection\label{quiz-question-sec-responsible-engineering-environmental-cost-awareness-0f3e}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.4}{}
\phantomsection\label{quiz-question-sec-responsible-engineering-environmental-cost-awareness-0f3e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For a successful production ML system, which cost component typically
  dominates the Total Cost of Ownership (TCO)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Initial data labeling
  \item
    Hyperparameter search
  \item
    Inference costs
  \item
    Academic research grants
  \end{enumerate}
\item
  How do model optimization techniques like quantization support both
  financial and environmental responsibility?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-responsible-engineering-environmental-cost-awareness-0f3e]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Fallacies and
Pitfalls}\label{sec-responsible-engineering-fallacies-pitfalls-61b9}

The following fallacies and pitfalls capture the most common mistakes in
responsible engineering practice. These misconceptions waste resources,
delay problem detection, and lead to systems that work technically but
fail responsibly.

\textbf{Fallacy:} \emph{Responsibility can be addressed after the system
achieves technical objectives.}

Teams often assume fairness constraints or monitoring capabilities can
be retrofitted once the model demonstrates strong benchmark performance.
In production, architectural decisions made during initial development
constrain what interventions remain feasible. Amazon's recruiting tool
illustrates this trap: after discovering gender bias, attempted
remediation through feature removal failed because the model had learned
proxy signals (college names, activity descriptions, career gap
patterns) that reconstructed protected attributes. Complete remediation
required architectural changes the team determined were not feasible,
leading to project cancellation. Organizations that defer responsibility
considerations until late-stage reviews face choices between expensive
system redesign (often 6-12 months of rework), deployment with known
harms and documented risk acceptance, or project cancellation after sunk
investment. The right approach integrates fairness constraints,
disaggregated evaluation infrastructure, and monitoring capabilities
from system inception when these interventions cost weeks rather than
quarters.

\textbf{Pitfall:} \emph{Relying on aggregate metrics to assess
fairness.}

Engineers assume high overall accuracy indicates the system works well
for all users. In production, aggregate metrics conceal severe subgroup
disparities. The Gender Shades study demonstrated commercial facial
recognition systems reporting 95\% overall accuracy while achieving
99.2\% accuracy for light-skinned males versus 65.3\% accuracy for
dark-skinned females (a 40× error rate disparity from 0.8\% to 34.7\%).
Similarly, the loan approval example in
Section~\ref{sec-responsible-engineering-worked-example-fairness-analysis-loan-approval-2c72}
showed systems with strong aggregate performance exhibiting 30
percentage point TPR gaps between demographic groups, meaning qualified
minority applicants faced rejection rates 3× higher than equally
qualified majority applicants. These disparities can persist for months
before detection because traditional monitoring tracks only aggregate
metrics. Disaggregated evaluation across relevant populations must be
implemented before deployment with automated alerts when subgroup
performance disparities exceed predefined thresholds (commonly 1.25×
error rate ratio or 5 percentage point TPR difference for high-stakes
applications).

\textbf{Fallacy:} \emph{Removing sensitive attributes from training data
eliminates bias.}

This belief drives teams to remove gender, race, and protected
attributes from feature sets, expecting this intervention alone ensures
fairness. In reality, models reconstruct protected attributes through
proxy variables that correlate with sensitive characteristics. Amazon's
recruiting tool penalized resumes despite gender being excluded from
training data: it learned that ``women's'' in activity descriptions,
attendance at all-women's colleges, and certain career gap patterns
predicted gender with high accuracy, then used these proxies to apply
different standards. Healthcare risk prediction algorithms excluded race
but used healthcare cost history (which encoded racial disparities in
care access), systematically underestimating Black patients' needs
despite equivalent health conditions. Addressing bias requires
understanding the causal pathways through which historical
discrimination enters training data, then applying interventions like
adversarial debiasing (training with an adversary that prevents proxy
learning), fairness constraints during optimization, or threshold
adjustment per subgroup. Feature removal is necessary but insufficient;
responsible systems require architectural changes that prevent the model
from learning demographic proxies, not just removing direct demographic
labels.

\textbf{Pitfall:} \emph{Treating documentation as sufficient
accountability.}

Teams invest significant effort in model cards and datasheets, then
consider responsibility requirements satisfied once documentation is
complete. While model cards provide essential transparency,
documentation without enforcement mechanisms offers limited protection.
A model card specifying ``not validated for high-stakes decisions'' has
no effect when the system is later repurposed for loan approvals or
medical diagnosis without technical restrictions preventing such use.
Accountability requires operational integration: monitoring dashboards
tracking disaggregated metrics across populations, alert thresholds
triggering investigation when subgroup disparities emerge, incident
response procedures specifying who investigates fairness violations and
with what authority, and access controls preventing model deployment
beyond validated use cases. Documentation enables accountability but
does not implement it; responsible engineering requires the same
operational rigor applied to reliability through uptime monitoring,
latency alerts, and on-call rotations.

\textbf{Fallacy:} \emph{Responsible AI is primarily a legal compliance
issue that legal teams should address.}

This assumption treats responsibility as external oversight rather than
engineering practice. Legal teams provide essential guidance on
regulatory requirements, but they review systems late in development
when fundamental design choices have already been made. Model
architecture selection determines what fairness interventions are
feasible during training (differential privacy requires specific
architectures; explainability constraints affect model families). Data
pipeline design establishes whether demographic attributes can be
tracked for disaggregated evaluation or whether this data was discarded
before legal review occurs. Optimization objective specification defines
what patterns the system learns from historical data. These engineering
decisions made months before legal review constrain the solution space
more decisively than any subsequent compliance assessment. Systems
designed with responsibility as an engineering objective from inception
can be validated efficiently; systems where responsibility is added
during late-stage review face costly redesign or deployment with
documented risks. Responsibility integrated as an engineering
requirement alongside latency, throughput, and accuracy targets enables
proactive design rather than reactive remediation.

\section{Summary}\label{sec-responsible-engineering-summary-45cf}

Responsible engineering is ML systems engineering done completely, not a
separate discipline. The responsibility gap exists because a model can
achieve excellent accuracy while causing systematic harm, and
traditional metrics cannot detect this failure mode. Closing this gap
requires treating responsibility as an engineering requirement from
system inception, not an external constraint imposed during late-stage
review.

The key insight is that responsibility concerns become tractable when
translated into measurable properties. Fairness becomes disaggregated
metrics across demographic groups. Efficiency becomes latency, power
consumption, and carbon accounting. Documentation becomes model cards
with explicit intended use and known limitations. Silent failures become
monitoring alerts for distribution shift and subgroup performance
degradation. The checklist approach transforms abstract concerns into
concrete questions that engineers must answer before deployment with the
same confidence they answer questions about latency and throughput.

Efficiency and responsibility align in practice. The optimization
techniques from earlier chapters (quantization, pruning, knowledge
distillation) reduce both operational costs and environmental impact. A
20\% latency reduction through quantization might save \$300K over three
years while eliminating 20 tons of CO2 emissions. The most responsible
systems are often the most efficient because responsibility thinking
requires engineers to justify resource consumption against delivered
value.

\begin{tcolorbox}[enhanced jigsaw, colback=white, bottomrule=.15mm, rightrule=.15mm, colframe=quarto-callout-important-color-frame, opacityback=0, coltitle=black, toptitle=1mm, colbacktitle=quarto-callout-important-color!10!white, arc=.35mm, left=2mm, toprule=.15mm, bottomtitle=1mm, breakable, titlerule=0mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, leftrule=.75mm, opacitybacktitle=0.6]

\begin{itemize}
\item
  \textbf{Correctness is insufficient}: A model can achieve 95\%
  accuracy while systematically disadvantaging specific populations.
  Aggregate metrics obscure disparities that disaggregated evaluation
  reveals.
\item
  \textbf{Responsibility should be formulated as engineering
  requirements}: ``Fairness gap \textless5\% across groups'' is
  actionable; ``be fair'' is not. Responsibility metrics should be
  treated with the same rigor as performance targets---measured and
  enforced.
\item
  \textbf{Efficiency and responsibility align}: A 4× more efficient
  model uses 4× less energy, costs 4× less, and enables 4× more
  organizations to deploy. Inefficient systems impose unnecessary
  resource costs and limit accessibility.
\item
  \textbf{Pre-deployment checklists prevent late-stage failures}: The
  aviation industry has reduced accidents through systematic
  verification. ML deployments require equivalent discipline---abstract
  concerns become concrete questions.
\item
  \textbf{Silent bias requires proactive detection}: Systems causing
  harm continue operating without alerts. Outcome distributions across
  demographic groups must be monitored, not just aggregate accuracy.
\end{itemize}

\end{tcolorbox}

\textbf{?@sec-conclusion} synthesizes the principles established
throughout this volume, from neural network fundamentals through
optimization, deployment, and now responsible practice. We have
journeyed from the bit-level physics of quantization to the
societal-level physics of fairness. Now, one final task remains: to
assemble these pieces into a coherent philosophy of \textbf{Engineering
Excellence}.

\section{Self-Check Answers}\label{self-check-answers}

\phantomsection\label{quiz-answer-sec-responsible-engineering-introduction-3c46}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.1}{}
\phantomsection\label{quiz-answer-sec-responsible-engineering-introduction-3c46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Why is responsible engineering particularly critical for
  machine learning systems compared to traditional software?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems are more expensive to develop.
  \item
    ML systems fail silently through biased outputs that appear normal.
  \item
    Traditional software does not require any testing.
  \item
    ML systems always produce deterministic results.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. ML systems fail silently
  through biased outputs that appear normal. Unlike traditional software
  that crashes visibly, ML systems can produce discriminatory results
  for months without triggering conventional alerts, necessitating a
  proactive responsibility framework.

  \emph{Learning Objective}: Contrast failure modes of ML systems and
  traditional software.
\item
  \textbf{Why can responsibility not be delegated exclusively to ethics
  boards or legal departments in an ML project?}

  \emph{Answer}: Engineers occupy a critical position because technical
  decisions made during inception---such as model architecture, data
  pipeline design, and optimization objectives---define and constrain
  the space for all subsequent fairness interventions. Ethics boards
  often only see the system after these decisive foundational choices
  have been made.

  \emph{Learning Objective}: Explain the engineer's role in proactive
  responsibility design.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-responsible-engineering-introduction-3c46]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-responsible-engineering-engineering-responsibility-gap-e69b}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.2}{}
\phantomsection\label{quiz-answer-sec-responsible-engineering-engineering-responsibility-gap-e69b}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{In the Amazon recruiting tool case, why did removing explicit
  gender labels fail to eliminate bias?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    The model was not trained for enough epochs.
  \item
    The model learned proxy signals (like college names) that correlated
    with gender.
  \item
    The engineers forgot to delete the gender column.
  \item
    The dataset was too small to be accurate.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. The model learned proxy
  signals that correlated with gender. Even without direct labels, the
  model reconstructed protected attributes from other data features like
  school names and activity descriptions that encoded historical gender
  patterns.

  \emph{Learning Objective}: Understand how models learn protected
  attributes through proxy variables.
\item
  \textbf{Explain how a `feedback loop' in a recommendation system can
  lead to bias amplification.}

  \emph{Answer}: Feedback loops occur when a model's predictions
  influence the data it later observes as training input. For example,
  if a system recommends provocative content to increase watch time, and
  users engage with it, the system interprets this as success and
  recommends even more extreme content, reinforcing and amplifying the
  initial algorithmic bias over time.

  \emph{Learning Objective}: Analyze the mechanics of bias amplification
  in ML systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-responsible-engineering-engineering-responsibility-gap-e69b]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-responsible-engineering-responsible-engineering-checklist-844e}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.3}{}
\phantomsection\label{quiz-answer-sec-responsible-engineering-responsible-engineering-checklist-844e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary purpose of a `Model Card' in responsible
  engineering?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To act as a warranty for the model software.
  \item
    To provide a standardized format for documenting intended use,
    performance factors, and ethical considerations.
  \item
    To store the binary weights of the trained neural network.
  \item
    To list all the developers who worked on the project.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. To provide a standardized
  format for documenting intended use, performance factors, and ethical
  considerations. Model cards ensure that essential context and
  limitations are communicated to users and auditors, preventing
  inappropriate model reuse.

  \emph{Learning Objective}: Explain the role of standardized
  documentation in ML accountability.
\item
  \textbf{Define `Disaggregated Evaluation' and explain why aggregate
  accuracy metrics can be misleading.}

  \emph{Answer}: Disaggregated evaluation is the practice of breaking
  down performance metrics by demographic subgroups. Aggregate metrics
  can be misleading because a high overall accuracy (e.g., 95\%) can
  conceal severe failures in a minority subgroup (e.g., 65\% accuracy),
  a disparity that only becomes visible when evaluating groups
  separately.

  \emph{Learning Objective}: Apply disaggregated evaluation concepts to
  detect performance disparities.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-responsible-engineering-responsible-engineering-checklist-844e]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-responsible-engineering-environmental-cost-awareness-0f3e}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.4}{}
\phantomsection\label{quiz-answer-sec-responsible-engineering-environmental-cost-awareness-0f3e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{For a successful production ML system, which cost component
  typically dominates the Total Cost of Ownership (TCO)?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Initial data labeling
  \item
    Hyperparameter search
  \item
    Inference costs
  \item
    Academic research grants
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Inference costs. For
  high-volume production systems, inference costs can be 10x to 1000x
  higher than training costs, as they compound across every query served
  to users over the system's lifetime.

  \emph{Learning Objective}: Analyze the components of TCO in production
  ML systems.
\item
  \textbf{How do model optimization techniques like quantization support
  both financial and environmental responsibility?}

  \emph{Answer}: Optimization techniques reduce the computational
  resources required per inference. Quantization (e.g., FP32 to INT8)
  typically reduces memory and compute needs by 2-4x. This lowers the
  electricity consumption (reducing carbon footprint) and the hardware
  requirements (reducing operational expenses and TCO) simultaneously.

  \emph{Learning Objective}: Connect model optimization to environmental
  and economic outcomes.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-responsible-engineering-environmental-cost-awareness-0f3e]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-angwin2016machine}
Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022.
{``Machine Bias.''} In \emph{Ethics of Data and Analytics}, 254--64.
Auerbach Publications. \url{https://doi.org/10.1201/9781003278290-37}.

\bibitem[\citeproctext]{ref-bellamy2019aif360}
Bellamy, R. K. E., K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan,
P. Lohia, et al. 2019. {``AI Fairness 360: An Extensible Toolkit for
Detecting and Mitigating Algorithmic Bias.''} \emph{IBM Journal of
Research and Development} 63 (4/5): 4:1--15.
\url{https://doi.org/10.1147/jrd.2019.2942287}.

\bibitem[\citeproctext]{ref-bird2020fairlearn}
Bird, Sarah, Miro Dudı́k, Richard Edgar, Brandon Horn, Roman Lutz,
Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker.
2020. {``Fairlearn: A Toolkit for Assessing and Improving Fairness in
AI.''} In \emph{Microsoft Technical Report
MSR-TR-2020-32}.\href{\%0A\%20\%20\%20\%20https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/\%0A\%20\%20}{https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/
}.

\bibitem[\citeproctext]{ref-buolamwini2018gender}
Buolamwini, Joy, and Timnit Gebru. 2018a. {``Gender Shades:
Intersectional Accuracy Disparities in Commercial Gender
Classification.''} In \emph{Conference on Fairness, Accountability and
Transparency}, 77--91. PMLR.
\url{http://proceedings.mlr.press/v81/buolamwini18a.html}.

\bibitem[\citeproctext]{ref-buolamwini2018}
---------. 2018b. {``Gender Shades: Intersectional Accuracy Disparities
in Commercial Gender Classification.''} In \emph{Proceedings of the 1st
Conference on Fairness, Accountability and Transparency}, 81:77--91.
Proceedings of Machine Learning Research. PMLR.
\url{https://proceedings.mlr.press/v81/buolamwini18a.html}.

\bibitem[\citeproctext]{ref-chouldechova2017fair}
Chouldechova, Alexandra. 2017. {``Fair Prediction with Disparate Impact:
A Study of Bias in Recidivism Prediction Instruments.''} In \emph{Big
Data}, 5:153--63. 2. Mary Ann Liebert Inc.
\url{https://doi.org/10.1089/big.2016.0047}.

\bibitem[\citeproctext]{ref-dastin2018amazon}
Dastin, Jeffrey. 2022. {``Amazon Scraps Secret AI Recruiting Tool That
Showed Bias Against Women.''} In \emph{Ethics of Data and Analytics},
296--99. Auerbach Publications.
\url{https://doi.org/10.1201/9781003278290-44}.

\bibitem[\citeproctext]{ref-dwork2006calibrating}
Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.
{``Calibrating Noise to Sensitivity in Private Data Analysis.''} In
\emph{Theory of Cryptography Conference (TCC)}, 3876:265--84. Lecture
Notes in Computer Science. Springer Berlin Heidelberg.
\url{https://doi.org/10.1007/11681878_14}.

\bibitem[\citeproctext]{ref-twitter2021cropping}
Engineering, Twitter. 2021. {``Sharing Learnings about Our Image
Cropping Algorithm.''}
\textless https://blog.twitter.com/engineering/en\_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm\textgreater.

\bibitem[\citeproctext]{ref-gawande2009checklist}
Gawande, Atul. 2009. \emph{The Checklist Manifesto: How to Get Things
Right}. Metropolitan Books.

\bibitem[\citeproctext]{ref-gebru2021datasheets}
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman
Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.
{``Datasheets for Datasets.''} \emph{Communications of the ACM} 64 (12):
86--92. \url{https://doi.org/10.1145/3458723}.

\bibitem[\citeproctext]{ref-hardt2016equality}
Hardt, Moritz, Eric Price, and Nathan Srebro. 2016. {``Equality of
Opportunity in Supervised Learning.''} In \emph{Advances in Neural
Information Processing Systems}, 29:3315--23. Curran Associates, Inc.
\url{https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning}.

\bibitem[\citeproctext]{ref-henderson2020towards}
Henderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky,
and Joelle Pineau. 2020. {``Towards the Systematic Reporting of the
Energy and Carbon Footprints of Machine Learning.''} \emph{CoRR}
abs/2002.05651 (248): 1--43.
\url{https://doi.org/10.48550/arxiv.2002.05651}.

\bibitem[\citeproctext]{ref-lundberg2017unified}
Lundberg, Scott M., and Su-In Lee. 2017. {``A Unified Approach to
Interpreting Model Predictions.''} In \emph{Advances in Neural
Information Processing Systems}, 30:4765--74. Curran Associates, Inc.
\url{https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions}.

\bibitem[\citeproctext]{ref-mitchell2019model}
Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy
Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and
Timnit Gebru. 2019. {``Model Cards for Model Reporting.''} In
\emph{Proceedings of the Conference on Fairness, Accountability, and
Transparency}, 220--29. ACM.
\url{https://doi.org/10.1145/3287560.3287596}.

\bibitem[\citeproctext]{ref-obermeyer2019dissecting}
Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil
Mullainathan. 2019. {``Dissecting Racial Bias in an Algorithm Used to
Manage the Health of Populations.''} \emph{Science} 366 (6464): 447--53.
\url{https://doi.org/10.1126/science.aax2342}.

\bibitem[\citeproctext]{ref-raji2019actionable}
Raji, Inioluwa Deborah, and Joy Buolamwini. 2019. {``Actionable
Auditing: Investigating the Impact of Publicly Naming Biased Performance
Results of Commercial AI Products.''} In \emph{Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society}, 429--35. ACM.
\url{https://doi.org/10.1145/3306618.3314244}.

\bibitem[\citeproctext]{ref-ribeiro2020auditing}
Ribeiro, Manoel Horta, Raphael Ottoni, Robert West, Virgı́lio A. F.
Almeida, and Wagner Meira Jr. 2020. {``Auditing Radicalization Pathways
on YouTube.''} In \emph{Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency}, 131--41. ACM.
\url{https://doi.org/10.1145/3351095.3372879}.

\bibitem[\citeproctext]{ref-ribeiro2016why}
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. {``{`Why
Should i Trust You?'}: Explaining the Predictions of Any Classifier.''}
In \emph{Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining}, 1135--44. ACM.
\url{https://doi.org/10.1145/2939672.2939778}.

\bibitem[\citeproctext]{ref-schwartz2020green}
Schwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020.
{``Green AI.''} \emph{Communications of the ACM} 63 (12): 54--63.
\url{https://doi.org/10.1145/3381831}.

\bibitem[\citeproctext]{ref-sculley2015hidden}
Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,
Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and
Dan Dennison. 2021. {``Technical Debt in Machine Learning Systems.''} In
\emph{Technical Debt in Practice}, 28:177--92. The MIT Press.
\url{https://doi.org/10.7551/mitpress/12440.003.0011}.

\bibitem[\citeproctext]{ref-strubell2019energy}
Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. {``Energy and
Policy Considerations for Deep Learning in NLP.''} In \emph{Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics}, 3645--50. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/p19-1355}.

\bibitem[\citeproctext]{ref-zhang2018adversarial}
Zhang, Brian Hu, Blake Lemoine, and Margaret Mitchell. 2018.
{``Mitigating Unwanted Biases with Adversarial Learning.''} In
\emph{Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and
Society}, 335--40. ACM. \url{https://doi.org/10.1145/3278721.3278779}.

\end{CSLReferences}


\backmatter


\end{document}
