% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{AI Acceleration}\label{sec-ai-acceleration}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Create an intricate and colorful representation
of a System on Chip (SoC) design in a rectangular format. Showcase a
variety of specialized machine learning accelerators and chiplets, all
integrated into the processor. Provide a detailed view inside the chip,
highlighting the rapid movement of electrons. Each accelerator and
chiplet should be designed to interact with neural network neurons,
layers, and activations, emphasizing their processing speed. Depict the
neural networks as a network of interconnected nodes, with vibrant data
streams flowing between the accelerator pieces, showcasing the enhanced
computation speed.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/images/png/cover_ai_hardware.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does moving data cost more than computing on it, and how does
this inversion reshape everything about ML system design?}

The fundamental surprise of modern computing is that arithmetic is
nearly free while memory access is expensive. A GPU can perform
thousands of floating-point operations in the time required to fetch a
single value from main memory. This inversion---where the cost of
feeding data to processors dominates the cost of processing
it---explains why specialized accelerators achieve order-of-magnitude
speedups on neural networks: they are not primarily faster at
arithmetic, but better at hiding and reducing data movement through
massive parallelism, specialized memory hierarchies, and architectures
tuned to the regular access patterns of matrix operations. The same
inversion explains why some optimizations that reduce theoretical
computation fail to improve actual runtime: if the operation was already
memory-bound, computing less does nothing because the bottleneck was
never computation. Understanding this reality transforms hardware
selection from comparing peak FLOPS to analyzing whether workload
characteristics align with what the hardware actually accelerates.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, rightrule=.15mm, opacityback=0, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, titlerule=0mm, breakable, toprule=.15mm, bottomtitle=1mm, colback=white, toptitle=1mm, bottomrule=.15mm, coltitle=black]

\begin{itemize}
\item
  Calculate arithmetic intensity for neural network operations and use
  the roofline model to determine whether workloads are compute-bound or
  memory-bound on specific hardware
\item
  Explain why systolic arrays and tensor cores achieve 10-100× better
  efficiency than general-purpose processors for matrix operations
\item
  Predict performance bottlenecks by quantifying the memory wall:
  bandwidth limits, energy costs of data movement, and cache hierarchy
  trade-offs
\item
  Select appropriate dataflow strategies (weight-stationary,
  output-stationary, input-stationary) based on whether a workload
  prioritizes weight reuse, activation reuse, or accumulator locality
\item
  Identify compiler optimization opportunities including kernel fusion,
  tiling, and memory planning that translate high-level models into
  efficient hardware execution
\item
  Evaluate accelerator choices for specific deployment scenarios
  (training vs.~inference, cloud vs.~edge, latency vs.~throughput) using
  quantitative cost-performance analysis
\item
  Recognize common pitfalls such as ignoring memory bandwidth limits,
  expecting linear multi-GPU scaling, or optimizing for peak FLOPS
  rather than sustained throughput
\end{itemize}

\end{tcolorbox}

\section{AI Hardware Acceleration
Fundamentals}\label{sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28}

Before examining specific hardware architectures, we establish the
fundamental law that governs all acceleration efforts.

\phantomsection\label{callout-perspectiveux2a-1.1}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Iron Law of Acceleration}
\phantomsection\label{callout-perspective*-1.1}
Hardware acceleration does not speed up the entire system; it only
speeds up the parallelizable fraction (\(P\)). This is governed by
\textbf{Amdahl's Law for AI}:

\[ Speedup = \frac{1}{(1 - P) + \frac{P}{S}} \]

\begin{itemize}
\tightlist
\item
  \textbf{\(P\) (Parallel Fraction):} The matrix multiplications
  (typically 90-99\% of an ML workload).
\item
  \textbf{\(S\) (Speedup):} The raw speed advantage of the GPU/TPU over
  the CPU (typically 100x-1000x).
\item
  \textbf{\(1-P\) (Serial Fraction):} Data loading, Python overhead, and
  kernel launch latency.
\end{itemize}

\textbf{The Pitfall:} If data loading takes 10\% of the time
(\(P=0.9\)), even an \textbf{infinite speed} accelerator (\(S=\infty\))
can only achieve a \textbf{10x} total speedup. The ``boring'' serial
part dominates the ``exciting'' AI part.

\end{fbx}

To see Amdahl's Law in action, consider how the parallel fraction \(P\)
differs dramatically between workload archetypes on the same hardware.

\phantomsection\label{callout-lighthouseux2a-1.2}
\begin{fbx}{callout-lighthouse}{Lighthouse Example: }{Amdahl's Law on H100}
\phantomsection\label{callout-lighthouse*-1.2}
\textbf{ResNet-50 inference on NVIDIA H100:}

\begin{itemize}
\tightlist
\item
  H100 delivers \(S = 500\times\) speedup over CPU for matrix multiply
  (3,958 TOPS INT8 vs.~\textasciitilde8 TOPS on baseline CPU without AMX
  extensions)
\item
  Typical inference has \(P = 0.95\) (95\% parallelizable, 5\% serial:
  data loading, preprocessing, postprocessing)
\end{itemize}

\[Speedup = \frac{1}{(1-0.95) + \frac{0.95}{500}} = \frac{1}{0.05 + 0.0019} = \frac{1}{0.0519} \approx 19.3\times\]

Despite a 500× hardware advantage, total system speedup is only
\textbf{19×}. The 5\% serial fraction caps practical gains.

\textbf{Contrast with GPT-2 (autoregressive):}

\begin{itemize}
\tightlist
\item
  Same H100, but GPT-2 token generation has \(P = 0.80\) (20\% serial:
  KV-cache updates, sampling, Python overhead)
\end{itemize}

\[Speedup = \frac{1}{(1-0.80) + \frac{0.80}{500}} = \frac{1}{0.20 + 0.0016} \approx 5.0\times\]

The \textbf{Bandwidth Hog} archetype suffers more from serial
bottlenecks. Even infinite accelerator speed yields only
\(1/(1-P) = 5\times\) maximum speedup. This is why LLM inference
optimization focuses on reducing the serial fraction (batching,
speculative decoding) rather than raw hardware speed.

\end{fbx}

Hardware acceleration is the engineering discipline of maximizing the
\textbf{Throughput} and \textbf{Bandwidth} denominators in the
\textbf{Iron Law of ML Systems}
(\textbf{?@sec-ai-training-iron-law-training-performance-a53f}). While
architectural choices (\textbf{?@sec-dnn-architectures}) set the ``Ops
budget,'' the hardware determines the theoretical floor for execution
time. However, as the Iron Law reveals, increasing peak throughput only
improves performance if \textbf{Utilization} remains high. This chapter
examines the hardware-software co-design required to reach those
theoretical ceilings.

The Purpose section posed a straightforward question: why does moving
data cost more than computing on it? This chapter answers that question
by examining how specialized accelerators minimize data movement while
maximizing compute throughput. Every architectural innovation examined
here, from systolic arrays to tensor cores to memory hierarchies,
represents a strategy for addressing the fundamental asymmetry between
cheap arithmetic and expensive memory access.

\textbf{?@sec-model-compression} introduced techniques that reduce
computational requirements through algorithmic means: quantization
shrinks precision from FP32 to INT8, pruning removes unnecessary
weights, and knowledge distillation trains smaller networks. But
\emph{why} does reducing precision from 32 bits to 8 bits yield 2-4×
speedup? Why does structured pruning improve performance while
unstructured pruning often doesn't? The answers lie in hardware
architecture. Quantization works because accelerators pack 4× more INT8
operations into the same silicon. Structured pruning works because it
preserves memory access patterns that hardware can optimize. This
chapter explains the physical constraints that make algorithmic
optimizations effective and reveals why some theoretically promising
techniques fail to deliver real-world speedups.

\phantomsection\label{callout-definitionux2a-1.3}
\begin{fbx}{callout-definition}{Definition: }{Hardware Acceleration}
\phantomsection\label{callout-definition*-1.3}
\textbf{Hardware Acceleration} refers to the use of \emph{specialized
computing hardware} (GPUs, TPUs, NPUs, and custom ASICs) designed to
exploit the \emph{regular computational patterns} of machine learning
workloads. By optimizing for dense matrix operations, reduced-precision
arithmetic, and high memory bandwidth, accelerators achieve 10-1000x
improvements in \emph{throughput} and \emph{energy efficiency} compared
to general-purpose processors, transforming ML from computationally
prohibitive research into deployable systems.

\end{fbx}

Hardware alone, however, cannot achieve these gains. The algorithms must
be designed to leverage what the hardware offers, and the hardware must
be built to accelerate the operations algorithms actually use. This
symbiosis motivates a complementary principle.

\phantomsection\label{callout-definitionux2a-1.4}
\begin{fbx}{callout-definition}{Definition: }{Hardware-Software Co-design}
\phantomsection\label{callout-definition*-1.4}
\textbf{Hardware-Software Co-design} refers to the simultaneous and
coordinated design of \emph{machine learning algorithms} and
\emph{computational hardware} to optimize for performance, efficiency,
and scalability, recognizing that neither can achieve peak potential in
isolation.

\end{fbx}

Modern machine learning systems challenge the architectural assumptions
underlying general-purpose processors. The efficiency framework
established in \textbf{?@sec-introduction} identified three pillars of
ML system efficiency: algorithmic, compute, and data. While the software
optimization techniques examined in \textbf{?@sec-model-compression}
address the algorithmic pillar through precision reduction, structural
pruning, and execution refinements, hardware acceleration targets the
compute pillar through architectural redesign rather than algorithmic
modification. This performance gap has driven a shift toward
domain-specific hardware acceleration within computer architecture.

One question recurs throughout this chapter: \emph{Is this workload
limited by how fast we can compute, or how fast we can move data?} The
answer determines everything: which accelerator to choose, which
optimizations matter, and whether a 10× more powerful chip will actually
help. The \textbf{Roofline Model}
(Section~\ref{sec-ai-acceleration-roofline-model}) provides the
analytical framework for answering this question. It plots an
operation's \textbf{arithmetic intensity} (operations per byte of memory
traffic) against hardware capabilities, revealing whether performance is
capped by compute or bandwidth. A dense matrix multiplication with high
arithmetic intensity benefits from more TFLOPS; a LayerNorm with low
arithmetic intensity benefits from more memory bandwidth. Most real
neural networks contain both types of operations, requiring careful
analysis to optimize end-to-end performance. We will analyze this
distinction through the Lighthouse Examples established in
\textbf{?@sec-introduction}: ResNet-50's convolutions are compute-bound
while GPT-2's attention layers are memory-bound, explaining why these
architectures require fundamentally different optimization strategies.

The co-evolution of machine learning algorithms and specialized
computing architectures has enabled the transition from computationally
prohibitive research conducted on high-performance computing systems to
ubiquitous deployment across diverse computing environments, from
hyperscale data centers to resource-constrained edge devices. This
chapter examines the hardware-software co-design principles that make
this transition possible, progressing from computational primitives
through memory systems to compiler and runtime support.

Hardware acceleration for machine learning systems sits at the
intersection of computer systems engineering, computer architecture, and
applied machine learning. For practitioners developing production
systems, the choice of accelerator technology (GPUs, TPUs, or
neuromorphic processors) shapes system-level performance
characteristics, energy efficiency profiles, and implementation
complexity. Deployed systems in domains such as natural language
processing, computer vision, and autonomous systems demonstrate
100-1000x performance improvements relative to general-purpose
implementations.

The historical evolution of domain-specific computing architectures
shows how design patterns, from floating-point coprocessors to graphics
processing units, inform contemporary AI acceleration strategies.
Subsequent sections address the computational primitives that
characterize machine learning workloads, including matrix
multiplication, vector operations, and nonlinear activation functions,
and analyze the architectural mechanisms through which specialized
hardware optimizes these operations via innovations such as systolic
array architectures and tensor processing cores.

Memory hierarchy design plays a critical role in acceleration
effectiveness, given that data movement energy costs typically exceed
computational energy by more than 100x. Memory architecture design
principles, from on-chip SRAM buffer optimization to high-bandwidth
memory interfaces, are examined, along with approaches to minimizing
energy-intensive data movement patterns. We also address compiler
optimization and runtime system support, which determine the extent to
which theoretical hardware capabilities translate into measurable system
performance.

Multi-chip architectures, from chiplet-based integration to distributed
warehouse-scale systems, introduce tradeoffs between computational
parallelism and inter-chip communication overhead. This chapter provides
awareness of these scaling challenges while focusing on
single-accelerator systems. Through detailed analysis of contemporary
systems including NVIDIA GPU architectures, Google Tensor Processing
Units, and emerging neuromorphic computing platforms, we establish the
theoretical foundations and practical considerations necessary for
effective deployment of AI acceleration within single-machine contexts.

\phantomsection\label{quiz-question-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.1}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary reason for the shift from general-purpose
  processors to domain-specific hardware in machine learning systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To reduce the cost of hardware components
  \item
    To improve the parallel processing capabilities and efficiency
  \item
    To simplify the design of machine learning algorithms
  \item
    To increase the utilization of existing software optimizations
  \end{enumerate}
\item
  True or False: Hardware acceleration in machine learning systems only
  focuses on improving computational speed, not energy efficiency.
\item
  How do architectural selection decisions impact system-level
  performance in machine learning systems?
\item
  Which of the following architectural innovations is used to optimize
  matrix multiplication in machine learning workloads?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Floating-point coprocessors
  \item
    Sequential processing models
  \item
    Systolic array architectures
  \item
    High-bandwidth memory interfaces
  \end{enumerate}
\item
  In a production system, what trade-offs might you consider when
  choosing between single-chip and multi-chip architectures for AI
  acceleration?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Evolution of Hardware
Specialization}\label{sec-ai-acceleration-evolution-hardware-specialization-fdb7}

Computing architectures follow a recurring pattern: as workloads grow in
complexity, general-purpose processors become inefficient, prompting
specialized hardware development. Machine learning acceleration
represents the latest stage in this evolution, following a trajectory
observed in floating-point arithmetic, graphics processing, and digital
signal processing. Understanding this history serves a practical
purpose: the architectural innovations that addressed floating-point
bottlenecks in the 1980s, graphics throughput in the 1990s, and media
processing in the 2000s inform today's AI accelerator designs. Each era
confronted the same fundamental constraint introduced in the Purpose
section: data movement costs dominate computation costs, and
specialization succeeds by minimizing unnecessary data movement.

Modern ML accelerators (GPUs with tensor cores, Google's
TPUs\sidenote{\textbf{TPU Origins}: Google secretly developed the Tensor
Processing Unit (TPU) starting in 2013 when they realized CPUs couldn't
handle the computational demands of their neural networks. The TPUv1,
deployed in 2015, delivered 15-30\(\times\) better performance per watt
than contemporary GPUs for inference. This breakthrough significantly
changed how the industry approached AI hardware, proving that
domain-specific architectures could dramatically outperform
general-purpose processors for neural network workloads. }, Apple's
Neural Engine) emerged from these established architectural principles.
This section traces the evolution through four phases: specialized
computing origins, parallel graphics processing, domain-specific
architectures, and the emergence of ML-specific hardware. Each phase
reveals design principles that remain relevant for understanding and
optimizing contemporary AI systems.

Hardware specialization enhances performance by implementing frequently
executed patterns in dedicated circuits, but introduces tradeoffs in
flexibility, silicon area, and programming complexity. The principles
that shaped early floating-point and graphics accelerators now inform AI
hardware design.

\subsection{Specialized
Computing}\label{sec-ai-acceleration-specialized-computing-22ce}

Hardware specialization emerges when specific computational patterns
become the primary system bottleneck, preventing general-purpose
processors from scaling efficiently. Historically, this progression
follows three distinct phases: the \textbf{Precision Bottleneck} (scalar
floating-point), the \textbf{Throughput Bottleneck} (parallel graphics),
and the \textbf{Integration Bottleneck} (memory-compute locality).

The first phase, the \textbf{Precision Bottleneck}, occurred when
scientific and engineering applications required high-precision decimal
math that general-purpose CPUs performed poorly. In the late 1970s, CPUs
typically emulated floating-point operations in software, requiring
hundreds of cycles for a single multiplication. This scalar inefficiency
led to the first major instance of hardware specialization: the
mathematics coprocessor.

The Intel 8087 (1980)\sidenote{\textbf{Intel 8087 Impact}: The 8087
coprocessor transformed scientific computing by providing dedicated
hardware for the IEEE 754 floating-point standard. This success
established the economic model for hardware specialization: premium
pricing for dramatic performance jumps in specific domains (CAD,
simulation). } addressed this bottleneck by offloading
arithmetic-intensive tasks to a dedicated unit. By implementing
floating-point logic in hardware rather than software emulation, the
8087 achieved up to 100\(\times\) performance gains for scientific
workloads (\citeproc{ref-fisher_8087_1981}{Fisher 1981}). This
established a fundamental principle: when a specific data type or
operation consumes the majority of execution cycles, moving it to
specialized silicon provides 10-100x improvements.

As specialized functions like floating-point math proved their value,
they followed a recurring pattern of \textbf{integration}. The Intel
486DX (1989) moved the FPU directly onto the CPU die, eliminating the
off-chip communication latency and making high-precision math a standard
feature rather than an optional accelerator
(\citeproc{ref-patterson2021computer}{Patterson and Hennessy 2021}).
This cycle (specialization to solve a bottleneck, followed by
integration into the general-purpose stack) repeats across every era of
hardware evolution.

This progression from specialization to integration has shaped modern
computing. Each domain---graphics, signal processing, machine
learning---introduced specialized architectures that were later absorbed
into general-purpose platforms.

Figure~\ref{fig-timeline} traces this trajectory: each era produced
accelerators addressing the dominant computational bottleneck of the
period. The capabilities enabling today's real-time translation,
recommendations, and on-device inference build directly on principles
established in earlier specialization waves.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/678c020f6d57bedf4ceff80d3535d90f1730eb97.pdf}}

}

\caption{\label{fig-timeline}\textbf{Hardware Specialization
Trajectory}: Computing architectures progressively incorporate
specialized accelerators to address emerging performance bottlenecks and
workload demands, mirroring a historical pattern from floating-point
units to graphics processors and, ultimately, machine learning
accelerators. This evolution reflects a strategy for improving
computational efficiency by tailoring hardware to specific task
characteristics and advancing increasingly complex applications.}

\end{figure}%

\subsection{Parallel Computing and Graphics
Processing}\label{sec-ai-acceleration-parallel-computing-graphics-processing-4654}

The principles established through floating-point acceleration provided
a blueprint for addressing emerging computational challenges. As
computing applications diversified, new computational patterns emerged
that exceeded the capabilities of general-purpose processors. This
expansion of specialized computing manifested across multiple domains,
each contributing unique insights to hardware acceleration strategies.

Graphics processing emerged as a primary driver of hardware
specialization in the 1990s. Early graphics accelerators focused on
specific operations like bitmap transfers and polygon filling. The
introduction of fixed-function graphics accelerators with NVIDIA's
GeForce 256 in 1999 represented a significant advancement in specialized
computing. The GeForce 256 implemented hardware-accelerated transform
and lighting (T\&L), moving these computations from CPU to dedicated
silicon. While not yet programmable, these Graphics Processing Units
(GPUs) demonstrated how fixed-function parallel architectures could
efficiently handle data-parallel workloads, achieving 50-100\(\times\)
speedups in 3D rendering tasks like texture mapping and vertex
transformation. The transition to programmable shaders with the GeForce
3 (2001) and unified shader architectures with the GeForce 8 (2006)
eventually enabled GPU computing for general-purpose workloads. By 2004,
high-end GPUs could process over 100 million polygons per second
(\citeproc{ref-owens2008gpu}{Owens et al. 2008}).

Concurrently, Digital Signal Processing (DSP) processors established
parallel data path architectures with specialized multiply-accumulate
units and circular buffers optimized for filtering and transform
operations. Texas Instruments' TMS32010 (1983) demonstrated how
domain-specific instruction sets could dramatically improve performance
for signal processing applications
(\citeproc{ref-lyons2011understanding}{Lyons 2011}).

Network processing introduced additional patterns of specialization.
Network processors developed unique architectures to handle packet
processing at line rate, incorporating multiple processing cores,
specialized packet manipulation units, and sophisticated memory
management systems. Intel's IXP2800 network processor demonstrated how
multiple levels of hardware specialization could be combined to address
complex processing requirements.

These diverse domains share common characteristics: identification of
domain-specific computational patterns, development of specialized
processing elements and memory hierarchies, creation of domain-specific
programming models, and progressive evolution toward more flexible
architectures. This pattern of architectural co-evolution established
the foundation for contemporary AI hardware design. The GPU's success in
parallelizing 3D graphics pipelines enabled its adoption for training
deep neural networks, exemplified by AlexNet\sidenote{\textbf{AlexNet's
GPU Revolution}: AlexNet's breakthrough wasn't just algorithmic. It
demonstrated that GPUs could train deep networks substantially faster
than CPUs for the dominant linear-algebra workloads
(\citeproc{ref-krizhevsky2012alexnet}{Krizhevsky, Sutskever, and Hinton
2017}). The team split the 8-layer network across two GPUs, reducing
training time from weeks to days and helping catalyze the shift toward
GPU-centric deep learning development. The enduring scale takeaway is
that matching the workload to parallel hardware can yield
order-of-magnitude improvements in time-to-train. } in 2012, which
executed on consumer-grade NVIDIA GPUs. DSP innovations in low-power
signal processing facilitated real-time inference on edge devices,
including voice assistants and wearables. These domains informed ML
hardware designs and established that accelerators could be deployed
across both cloud and embedded contexts, principles that continue to
influence contemporary AI ecosystem development.

\subsection{Emergence of Domain-Specific
Architectures}\label{sec-ai-acceleration-emergence-domainspecific-architectures-e56e}

These diverse acceleration patterns converged in a broader architectural
shift. The emergence of domain-specific architectures
(DSA)\sidenote{\textbf{Domain-Specific Architectures (DSA)}: Computing
architectures optimized for specific application domains rather than
general-purpose computation. Unlike CPUs designed for flexibility, DSAs
sacrifice programmability for dramatic efficiency gains. Google's TPU
achieves 15-30\(\times\) better performance per watt than GPUs for
neural networks, while video codecs provide 100-1000\(\times\)
improvements over software decoding. The 2018 Turing Award recognized
this shift as the defining trend in modern computer architecture. }
marks a transition in computer system design, driven by two factors: the
breakdown of traditional scaling laws and the increasing computational
demands of specialized workloads. The slowdown of Moore's
Law\sidenote{\textbf{Moore's Law}: Intel co-founder Gordon Moore's 1965
observation that transistor density doubles every 18-24 months. This
exponential scaling drove computing progress for decades, enabling
everything from smartphones to supercomputers. However, power density
constraints and rising manufacturing complexity slowed the historical
pace of cost-effective scaling. As advanced-node development costs rose
from millions to billions of dollars, the industry shifted toward
parallelism and specialization. }, which previously ensured predictable
enhancements in transistor density every 18 to 24 months, and the end of
Dennard scaling\sidenote{\textbf{Dennard Scaling}: Robert Dennard's 1974
principle that as transistors shrink, their power density remains
constant, allowing higher frequencies without increased power
consumption. This enabled CPUs to reach 3+ GHz by 2005. However, quantum
effects and leakage current ended Dennard scaling around 2005, forcing
architects to prioritize efficiency over raw speed and leading to the
multi-core revolution. }, which permitted frequency increases without
corresponding power increases, created a performance and efficiency
bottleneck in general-purpose computing. As John Hennessy and David
Patterson noted in their 2017 Turing Lecture
(\citeproc{ref-HennessyPatterson2017Turing}{Hennessy and Patterson
2019}), these limitations signaled the onset of a new era in computer
architecture, one centered on domain-specific solutions that optimize
hardware for specialized workloads.

Historically, improvements in processor performance depended on
semiconductor process scaling and increasing clock speeds. However, as
power density limitations restricted further frequency scaling, and as
transistor miniaturization encountered increasing physical and economic
constraints, architects explored alternative approaches to sustain
computational growth. This resulted in a shift toward domain-specific
architectures, which dedicate silicon resources to optimize computation
for specific application domains, trading flexibility for efficiency.

Domain-specific architectures achieve superior performance and energy
efficiency through several key principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Customized data paths}: Design processing paths specifically
  optimized for target application patterns, enabling direct hardware
  execution of common operations. For example, matrix multiplication
  units in AI accelerators implement systolic arrays (grid-like networks
  of processing elements that rhythmically compute and pass data through
  neighboring units) tailored for neural network computations.
\item
  \textbf{Specialized memory hierarchies}: Optimize memory systems
  around domain-specific access patterns and data reuse characteristics.
  This includes custom cache configurations, prefetching logic, and
  memory controllers tuned for expected workloads.
\item
  \textbf{Reduced instruction overhead}: Implement domain-specific
  instruction sets that minimize decode and dispatch complexity by
  encoding common operation sequences into single instructions. This
  improves both performance and energy efficiency.
\item
  \textbf{Direct hardware implementation}: Create dedicated circuit
  blocks that natively execute frequently used operations without
  software intervention. This eliminates instruction processing overhead
  and maximizes throughput.
\end{enumerate}

These principles achieve compelling demonstration in modern smartphones.
Modern smartphones can decode 4K video at 60 frames per second while
consuming only a few watts of power, despite video processing requiring
billions of operations per second. This efficiency is achieved through
dedicated hardware video codecs that implement industry standards such
as H.264/AVC (introduced in 2003) and H.265/HEVC (finalized in 2013)
(\citeproc{ref-sullivan2012overview}{Sullivan et al. 2012}). These
specialized circuits provide 100--1000\(\times\) improvements in both
performance and power efficiency compared to software-based decoding on
general-purpose processors.

The trend toward specialization continues to accelerate, with new
architectures emerging for an expanding range of domains. Genomics
processing benefits from custom accelerators that optimize sequence
alignment and variant calling, reducing the time required for DNA
analysis (\citeproc{ref-Shang2018GenomicsAccel}{Shang, Wang, and Liu
2018}). Similarly, blockchain computation has produced
application-specific integrated circuits
(ASICs)\sidenote{\textbf{Application-Specific Integrated Circuits
(ASICs)}: Custom silicon chips designed for a single application,
offering maximum efficiency by eliminating unused features. For
well-defined workloads, ASICs can achieve orders-of-magnitude better
energy efficiency than general-purpose processors (often (10\^{}3) to
(10\^{}5\times) in specialized domains). However, their inflexibility
means they can become obsolete if algorithms or standards change (for
example, when cryptocurrency networks change consensus mechanisms). }
optimized for cryptographic hashing, substantially increasing the
efficiency of mining operations
(\citeproc{ref-Taylor2017ASICMining}{Bedford Taylor 2017}).

This shift represents an important engineering lesson: the era of
``free'' performance gains from general-purpose scaling is over. For
decades, software engineers could rely on Moore's Law to accelerate
existing code without architectural changes. The breakdown of Dennard
scaling forced a paradigm shift: we can no longer wait for faster CPUs
to solve computational bottlenecks. Instead, we must design the hardware
to fit the algorithm. This necessity of hardware-software co-design is
why modern AI engineering requires deep understanding of the underlying
silicon. Performance is now determined by how well the algorithm's
memory access patterns and parallelism map to the specialized physical
structures of Domain-Specific Architectures.

\subsection{Machine Learning Hardware
Specialization}\label{sec-ai-acceleration-machine-learning-hardware-specialization-09c5}

Machine learning constitutes a computational domain with unique
characteristics that have driven the development of specialized hardware
architectures. Unlike traditional computing workloads that exhibit
irregular memory access patterns and diverse instruction streams, neural
networks are characterized by predictable patterns: dense matrix
multiplications, regular data flow, and tolerance for reduced precision.
These characteristics enable specialized hardware optimizations that
would be ineffective for general-purpose computing but provide
substantial speedups for ML workloads.

\phantomsection\label{callout-definitionux2a-1.5}
\begin{fbx}{callout-definition}{Definition: }{ML Accelerator}
\phantomsection\label{callout-definition*-1.5}
\textbf{Machine Learning Accelerators} refer to specialized computing
hardware optimized for the \emph{computational patterns} of neural
networks, achieving superior \emph{performance per watt} through
\emph{parallel processing}, \emph{specialized memory hierarchies}, and
\emph{reduced-precision arithmetic}.

\end{fbx}

Machine learning computational requirements reveal limitations in
traditional processors. CPUs achieve only 5-10\% utilization on neural
network workloads, delivering approximately 100
GFLOPS\sidenote{\textbf{GFLOPS/TOPS Performance Metrics}: GFLOPS (10⁹
floating-point ops/second) measures floating-point throughput; TOPS
(10¹² ops/second) typically measures INT8 operations in AI accelerators.
The A100 delivers 312 TFLOPS for FP16/BF16 Tensor Core operations (156
TFLOPS for TF32, or 312 TFLOPS with sparsity enabled); Apple A17 NPU
achieves 35 INT8 TOPS. Real workload performance depends on memory
bandwidth, achieving 10-30\% of peak on typical ML models. } while
consuming hundreds of watts. This inefficiency results from
architectural mismatches: CPUs optimize for single-thread performance
and irregular memory access, while neural networks require massive
parallelism and predictable data streams. The memory
bandwidth\sidenote{Memory bandwidth (introduced in
\textbf{?@sec-introduction}) measures data transfer rate between memory
and processors. For accelerator design, the gap between CPU memory (tens
to hundreds of GB/s) and HBM (terabytes per second) explains why
specialized hardware achieves 10-100x speedups on bandwidth-bound neural
network operations. } constraint becomes particularly severe: a single
neural network layer may require accessing gigabytes of parameters,
overwhelming CPU cache hierarchies\sidenote{\textbf{Cache}: From French
``cacher'' (to hide), describing memory that stores frequently accessed
data out of sight from the programmer. The IBM System/360 Model 85
(1968) introduced the first commercial cache. Modern CPUs use
multi-level hierarchies (L1/L2/L3) with progressively larger capacity
but higher latency. Neural networks often exceed cache capacity, forcing
frequent DRAM accesses that reduce effective throughput. } designed for
kilobyte-scale working sets.

The energy economics of data movement influence accelerator design.
Accessing data from DRAM can consume on the order of (10\^{}2)× more
energy than a multiply-accumulate operation (exact values vary by
technology node and design), making minimizing data movement a primary
optimization target. This disparity helps explain the progression from
repurposed graphics processors to purpose-built neural network
accelerators. TPUs and other custom accelerators can sustain high
utilization on dense kernels by implementing systolic arrays and other
architectures that maximize data reuse while minimizing movement.

Training and inference present distinct computational profiles that
influence accelerator design. Training requires high-precision
arithmetic (FP32 or FP16) for gradient computation and weight updates,
bidirectional data flow for
backpropagation\sidenote{\textbf{Backpropagation}: Short for ``backward
propagation of errors,'' formalized by Rumelhart, Hinton, and Williams
in 1986, though the mathematical foundation (reverse-mode automatic
differentiation) traces to the 1960s. The name describes its mechanism:
errors flow backward from output to input, propagating gradients through
the network via the chain rule. This bidirectional data flow requires
storing all intermediate activations, increasing memory requirements
2-3x versus inference-only forward passes. }, and large memory capacity
for storing activations. Inference can exploit reduced precision (INT8
or INT4), requires only forward computation, and prioritizes latency
over throughput\sidenote{\textbf{Latency vs Throughput}: ``Latency''
from Latin ``latere'' (to lie hidden), originally described the delay
before something becomes apparent. ``Throughput'' emerged from
industrial manufacturing, measuring production rate. In computing,
latency measures single-request response time (milliseconds), while
throughput measures processing rate (requests/second). Training
optimizes throughput for batch processing; inference prioritizes latency
for real-time responses. }. These differences drive specialized
architectures: training accelerators maximize FLOPS and memory
bandwidth, while inference accelerators optimize for energy efficiency
and deterministic latency.

Deployment context shapes architectural choices. Datacenter accelerators
often operate within \emph{hundreds of watts} to maximize throughput for
training massive models. Edge devices must deliver real-time inference
within \emph{tight power budgets} (often milliwatts to a few watts),
driving architectures that minimize unnecessary data movement. Mobile
processors balance performance with battery life, while automotive
systems prioritize deterministic response times for safety-critical
applications. This diversity has produced a rich ecosystem of
specialized accelerators, each optimized for specific deployment
scenarios and computational requirements. The following examples
illustrate how different deployment contexts drive distinct
architectural priorities.

In data centers, training accelerators such as NVIDIA H100 and Google
TPUv4 can reduce model iteration time substantially through massive
parallelism and high-bandwidth memory systems. These systems prioritize
raw computational throughput, accepting high power consumption to
achieve petaflop-scale performance. The economics can support this
trade-off when faster iteration reduces both time-to-deploy and the
cumulative cost of long-running training jobs.

At the opposite extreme, edge deployment requires different optimization
strategies. Processing-in-memory architectures reduce data movement by
integrating compute more directly with memory. Dynamic voltage scaling
can reduce power substantially during low-intensity operations.
Neuromorphic designs process only changing inputs, which can yield large
power reductions for temporal workloads, sometimes approaching
orders-of-magnitude improvements. These techniques enable sophisticated
AI models to operate continuously on battery power, supporting
applications from smartphone photography to autonomous sensors that can
function for months to years without external power.

The success of application-specific accelerators demonstrates that no
single architecture can efficiently address all ML workloads. A massive
installed base of edge devices demands architectures optimized for
energy efficiency and real-time latency targets, while cloud-scale
training continues advancing the boundaries of computational throughput.
This diversity drives continued innovation in specialized architectures,
each optimized for its specific deployment context and computational
requirements.

This historical progression reveals a key pattern: each wave of hardware
specialization responded to a specific computational bottleneck.
Floating-point coprocessors addressed arithmetic precision limitations.
GPUs addressed graphics throughput limitations. But what bottleneck does
AI acceleration address? Understanding this question is essential
because it reveals \emph{why} modern accelerators are designed the way
they are---and why simply adding more transistors to general-purpose
processors cannot solve the fundamental challenge. Before examining this
integration bottleneck in detail, Table~\ref{tbl-hw-evolution}
summarizes the key milestones in hardware specialization. While these
accelerators initially emerged to optimize domain-specific workloads
such as floating-point operations, graphics rendering, and media
processing, they also introduced architectural strategies that persist
in contemporary systems. The specialization principles outlined in
earlier generations now underpin the design of modern AI accelerators.
Understanding this historical trajectory provides context for analyzing
how hardware specialization continues to enable scalable, efficient
execution of machine learning workloads across diverse deployment
environments.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2569}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3264}}@{}}
\caption{\textbf{Hardware Specialization Trends}: Successive computing
eras progressively integrate specialized hardware to accelerate
prevalent workloads, moving from general-purpose CPUs to domain-specific
architectures and ultimately to customizable AI accelerators. This
evolution reflects a fundamental principle: tailoring hardware to
computational patterns improves performance and energy efficiency,
driving innovation in machine learning
systems.}\label{tbl-hw-evolution}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Computational Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristics}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Computational Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristics}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1980s} & Floating-Point \& Signal Processing & FPU, DSP &
\begin{minipage}[t]{\linewidth}\raggedright
Single-purpose engines

Focused instruction sets

Coprocessor interfaces
\end{minipage} \\
\textbf{1990s} & 3D Graphics \& Multimedia & GPU, SIMD Units &
\begin{minipage}[t]{\linewidth}\raggedright
Many identical compute units

Regular data patterns

Wide memory interfaces
\end{minipage} \\
\textbf{2000s} & Real-time Media Coding & Media Codecs, Network
Processors & \begin{minipage}[t]{\linewidth}\raggedright
Fixed-function pipelines

High throughput processing

Power-performance optimization
\end{minipage} \\
\textbf{2010s} & Deep Learning Tensor Operations & TPU, GPU Tensor Cores
& \begin{minipage}[t]{\linewidth}\raggedright
Matrix multiplication units

Massive parallelism

Memory bandwidth optimization
\end{minipage} \\
\textbf{2020s} & Application-Specific Acceleration & ML Engines, Smart
NICs, Domain Accelerators & \begin{minipage}[t]{\linewidth}\raggedright
Workload-specific datapaths

Customized memory hierarchies

Application-optimized designs
\end{minipage} \\
\end{longtable}

What distinguishes AI acceleration from earlier specialization waves is
the scale of integration required. AI accelerators must work seamlessly
with frameworks like TensorFlow, PyTorch, and JAX. They require
sophisticated compiler support for graph-level transformations, kernel
fusion, and memory scheduling. And they must deploy across environments
from data centers to mobile devices, each with distinct performance and
efficiency requirements. This creates a system-level transformation that
requires tight hardware-software coupling, a theme that will recur
throughout this chapter.

But first, we must understand \emph{what} bottleneck AI accelerators are
designed to solve. Unlike floating-point coprocessors that addressed
arithmetic precision or GPUs that addressed graphics throughput, AI
accelerators target a fundamentally different constraint. The answer
determines every subsequent architectural decision.

\subsection{The Integration Bottleneck: Why AI Needs Specialized
Hardware}\label{sec-ai-acceleration-integration-bottleneck-ai-needs-specialized-hardware-0b41}

Machine learning constitutes a computational domain where the primary
performance limit has shifted from \textbf{arithmetic} to
\textbf{integration}. While early coprocessors solved the
\emph{Precision Bottleneck} (8087) and GPUs solved the \emph{Throughput
Bottleneck} (rasterization), modern AI workloads are constrained by the
\textbf{Integration Bottleneck}: the energy and latency cost of moving
massive amounts of data between memory and thousands of parallel compute
units.

Neural networks are characterized by three unique properties that drive
this shift:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Massive Parallelism}: Unlike general-purpose code with complex
  branching, neural networks execute billions of independent matrix
  multiplications and convolutions. This regular structure allows
  replacing complex CPU control logic with dense arrays of processing
  elements (systolic arrays).
\item
  \textbf{Predictable Data Flow}: Data movement in deep learning is
  mathematically determined by the network's layers. This predictability
  enables hardware to ``prefetch'' data into local
  scratchpads\sidenote{\textbf{Scratchpad}: From the physical scratch
  pads used for quick calculations before computers. In hardware,
  scratchpad memory is software-managed fast memory near compute units,
  contrasting with caches that are hardware-managed. Unlike caches that
  guess what data to keep, scratchpads give programmers explicit control
  over what data resides in fast memory, enabling the predictable data
  movement that ML workloads require. 3. \textbf{Tolerance for Reduced
  Precision}: Neural networks typically remain robust even when using
  8-bit or 4-bit integers instead of 64-bit floating-point numbers. This
  flexibility allows architects to fit 10\(\times\) more compute units
  in the same silicon area. }, bypassing the expensive random-access
  cache hierarchies of CPUs.
\end{enumerate}

The primary engineering challenge is no longer ``how fast can we
calculate?'' but ``how close can we keep the data to the calculation?''
In modern accelerators, accessing data from external memory (DRAM) can
consume \(100\times\) more energy than the actual arithmetic operation.
This disparity drives the ``Anatomy of an Accelerator''
(Figure~\ref{fig-accelerator-anatomy}), prioritizing high-bandwidth
memory (HBM)\sidenote{HBM (introduced in
\textbf{?@sec-dnn-architectures}) achieves 2-10x higher bandwidth than
GDDR memory through 3D die stacking with thousands of TSVs. From a
hardware architecture perspective, HBM's 2-3 TB/s bandwidth (vs.~500-700
GB/s for GDDR6X) transforms memory-bound ML workloads toward
compute-bound performance. The trade-off is higher manufacturing cost,
limiting HBM to data center accelerators where bandwidth justifies the
premium. } and large on-chip scratchpads to minimize data movement.

The evolution from the Intel 8087 to the Google TPU reveals a consistent
pattern: hardware evolves to fit the algorithm's dominant bottleneck.
Where the 8087 addressed floating-point operations that consumed 80\% of
scientific computing time, modern AI accelerators address matrix
operations that constitute over 95\% of neural network computation. This
concentration of demand explains why specialized AI silicon achieves
100-1000\(\times\) performance improvements over general-purpose
processors.

The fundamental constraints identified above (massive parallelism,
predictable data flow, and tolerance for reduced precision) shape
accelerator architecture. Before examining the computational primitives
that exploit these characteristics, we examine the architectural
organization that enables their efficient execution. Modern AI
accelerators achieve their dramatic performance improvements through a
carefully orchestrated hierarchy of specialized components operating in
concert.

The processing substrate consists of an array of processing elements,
each containing dedicated computational units optimized for specific
operations: tensor cores execute matrix multiplication, vector units
perform element-wise operations, and special function units compute
activation functions. These processing elements are organized in a grid
topology that enables massive parallelism, with dozens to hundreds of
units operating simultaneously on different portions of the computation,
exploiting the data-level parallelism inherent in neural network
workloads.

The memory hierarchy forms an equally critical architectural component.
High-bandwidth memory provides the aggregate throughput required to
sustain these numerous processing elements, while a multi-level cache
hierarchy from shared L2 caches down to per-element L1 caches and
scratchpads minimizes the energy cost of data movement. This
hierarchical organization embodies a design principle: in AI
accelerators, data movement typically consumes more energy than
computation itself. This necessitates architectural strategies that
prioritize data reuse by maintaining frequently accessed values
(including weights and partial results) in proximity to compute units.

The host interface establishes connectivity between the specialized
accelerator and the broader computing system, enabling coordination
between general-purpose CPUs that manage program control flow and the
accelerator that executes computationally intensive neural network
operations.

This architectural partitioning reflects specialization at the system
level: CPUs address control flow, conditional logic, and system
coordination, while accelerators focus on the regular, massively
parallel arithmetic operations that dominate neural network execution.
Figure~\ref{fig-accelerator-anatomy} shows how specialized compute
units, hierarchical memory subsystems, and host connectivity integrate
to form a system optimized for AI workloads.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/fb7412643d7981a0d54bff811debfe33105ccba5.pdf}}

}

\caption{\label{fig-accelerator-anatomy}\textbf{Anatomy of a Modern AI
Accelerator}: AI accelerators integrate specialized processing elements
containing tensor cores, vector units, and special function units,
supported by a hierarchical memory system from high-bandwidth memory
down to local caches. This architecture maximizes data reuse and
parallel execution while minimizing energy-intensive data movement,
forming the foundation for 100-1000× performance improvements over
general-purpose processors.}

\end{figure}%

\phantomsection\label{quiz-question-sec-ai-acceleration-evolution-hardware-specialization-fdb7}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.2}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-evolution-hardware-specialization-fdb7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the primary motivation for the
  development of specialized hardware accelerators in computing?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To reduce the cost of general-purpose processors
  \item
    To increase the flexibility of computing systems
  \item
    To handle increasingly complex computational workloads efficiently
  \item
    To simplify the programming models for developers
  \end{enumerate}
\item
  Explain how the evolution of specialized hardware has influenced the
  design of modern machine learning accelerators.
\item
  True or False: The integration of specialized functions into
  general-purpose processors is a common trend observed in the evolution
  of computing architectures.
\item
  What is a key trade-off introduced by the use of specialized hardware
  accelerators?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increased flexibility in programming
  \item
    Reduced programming complexity
  \item
    Higher energy consumption
  \item
    Reduced silicon area utilization
  \end{enumerate}
\item
  In a production system, how might the choice of hardware accelerators
  impact the deployment of machine learning models?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-evolution-hardware-specialization-fdb7]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{AI Compute
Primitives}\label{sec-ai-acceleration-ai-compute-primitives-2c99}

The accelerator architecture presented in
Figure~\ref{fig-accelerator-anatomy} raises an immediate question: why
these specific components? The tensor cores, vector units, and
hierarchical memory exist not by accident but because neural network
computations repeatedly invoke a small set of operations. Understanding
these computational patterns, which we call compute primitives, reveals
why specialized hardware achieves 100-1000x improvements over
general-purpose processors. The transition from CPUs achieving
approximately 100 GFLOPS to accelerators delivering 100,000+ GFLOPS
reflects architectural optimization for these specific patterns, which
appear repeatedly across all neural network architectures regardless of
application domain or model size.

These patterns manifest in a small number of core computational
operations. Regardless of the layer type, whether fully connected,
convolutional, or attention-based layers, the underlying operation
typically involves multiplying input values by learned weights and
accumulating the results. This repeated multiply-accumulate process
dominates neural network execution and defines the arithmetic foundation
of AI workloads. The regularity and frequency of these operations have
led to the development of AI compute primitives: hardware-level
abstractions optimized to execute these core computations with high
efficiency.

These recurring multiply-accumulate operations exhibit a key property:
they are highly structured and data-parallel, enabling architectural
specialization. Building on the parallelization principles established
in
Section~\ref{sec-ai-acceleration-parallel-computing-graphics-processing-4654},
these patterns emphasize predictable data reuse and fixed operation
sequences. AI compute primitives distill these patterns into reusable
architectural units that support high-throughput and energy-efficient
execution.

Listing~\ref{lst-dense_layer_def} demonstrates how a dense layer
decomposes at the framework level, encapsulating thousands of
multiply-accumulate operations in a single high-level call.

\begin{codelisting}

\caption{\label{lst-dense_layer_def}\textbf{Dense Layer Abstraction}:
High-level framework APIs encapsulate 131,072 multiply-accumulate
operations (256 inputs times 512 outputs) in a single function call,
hiding the computational complexity from developers while enabling
automatic hardware optimization.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Framework abstracts compute{-}intensive operations}
\NormalTok{dense }\OperatorTok{=}\NormalTok{ Dense(}\DecValTok{512}\NormalTok{)(input\_tensor)  }\CommentTok{\# 256×512 = 131K MACs per sample}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Listing~\ref{lst-dense_expansion} reveals how the framework expands this
high-level call into mathematical operations.

\begin{codelisting}

\caption{\label{lst-dense_expansion}\textbf{Matrix Operation Expansion}:
Each dense layer decomposes into matrix multiplication and element-wise
operations, exposing the dominant compute pattern that consumes over
95\% of neural network execution time.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Linear transformation: O(input\_dim × output\_dim × batch) operations}
\NormalTok{output }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    matmul(}\BuiltInTok{input}\NormalTok{, weights) }\OperatorTok{+}\NormalTok{ bias}
\NormalTok{)  }\CommentTok{\# Matrix multiply dominates cost}
\NormalTok{output }\OperatorTok{=}\NormalTok{ activation(output)  }\CommentTok{\# Element{-}wise: O(output\_dim × batch)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

At the processor level, Listing~\ref{lst-loop_level_dense} reveals how
nested loops multiply inputs and weights, sum the results, and apply a
nonlinear function, exposing the O(batch times input times output)
complexity that accelerators must handle efficiently.

\begin{codelisting}

\caption{\label{lst-loop_level_dense}\textbf{Processor-Level Execution}:
Nested loops reveal the O(batch times input times output)
multiply-accumulate operations that accelerators must execute, with 4
million MACs for typical batch=32, input=256, output=512
configurations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Total operations: batch\_size × output\_size × input\_size MACs}
\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):  }\CommentTok{\# Batch dimension: parallelizable}
    \ControlFlowTok{for}\NormalTok{ m }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(output\_size):  }\CommentTok{\# Output neurons: parallelizable}
        \BuiltInTok{sum} \OperatorTok{=}\NormalTok{ bias[m]  }\CommentTok{\# Initialize accumulator}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(input\_size):  }\CommentTok{\# Reduction dimension: sequential}
            \BuiltInTok{sum} \OperatorTok{+=} \BuiltInTok{input}\NormalTok{[n, k] }\OperatorTok{*}\NormalTok{ weights[k, m]  }\CommentTok{\# MAC operation}
\NormalTok{        output[n, m] }\OperatorTok{=}\NormalTok{ activation(}\BuiltInTok{sum}\NormalTok{)  }\CommentTok{\# Non{-}linear transformation}
\CommentTok{\# Example: 32 × 512 × 256 = 4.2M multiply{-}accumulate operations}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This transformation reveals four computational characteristics. First,
data-level parallelism enables simultaneous execution across independent
operations. Second, structured matrix operations define the
computational workloads. Third, predictable data movement patterns drive
memory optimization strategies. Fourth, frequent nonlinear
transformations motivate the development of specialized function units.

The design of AI compute primitives follows three architectural
criteria. First, the primitive must be used frequently enough to justify
dedicated hardware resources. Second, its specialized implementation
must offer substantial performance or energy efficiency gains relative
to general-purpose alternatives. Third, the primitive must remain stable
across generations of neural network architectures to ensure long-term
applicability. These considerations shape the inclusion of primitives
such as vector operations, matrix operations, and special function units
in modern ML accelerators. Together, they serve as the architectural
foundation for efficient and scalable neural network execution.

\subsection{Vector
Operations}\label{sec-ai-acceleration-vector-operations-19bf}

Vector operations provide the first level of hardware acceleration by
processing multiple data elements simultaneously. This parallelism
exists at multiple scales, from individual neurons to entire layers,
making vector processing necessary for efficient neural network
execution. Framework-level code translates to hardware instructions,
revealing how vector processing enables neural accelerators.

\subsubsection{High-Level Framework
Operations}\label{sec-ai-acceleration-highlevel-framework-operations-7b1d}

Machine learning frameworks hide hardware complexity through high-level
abstractions. These abstractions decompose into progressively
lower-level operations, revealing opportunities for hardware
acceleration. Listing~\ref{lst-linear_layer_highlevel} illustrates this
principle through a linear layer's execution flow, where a single
function call transforms 256 input features into 512 outputs.

\begin{codelisting}

\caption{\label{lst-linear_layer_highlevel}\textbf{Linear Layer}: Neural
networks transform input data into a higher-dimensional space using
linear mappings to enable complex feature extraction.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{layer }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{256}\NormalTok{, }\DecValTok{512}\NormalTok{)  }\CommentTok{\# 256 inputs to}
\CommentTok{\# 512 outputs}
\NormalTok{output }\OperatorTok{=}\NormalTok{ layer(input\_tensor)  }\CommentTok{\# Process a batch of inputs}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This abstraction represents a fully connected layer that transforms
input features through learned weights.
Listing~\ref{lst-linear_math_internal} exposes the mathematical
operations behind this high-level expression, revealing hardware
acceleration opportunities.

\begin{codelisting}

\caption{\label{lst-linear_math_internal}\textbf{Fully Connected Layer}:
Each output is computed as a weighted sum of all inputs plus a bias,
followed by an activation function transformation. Linear
transformations enable complex model architectures in neural networks.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Z }\OperatorTok{=}\NormalTok{ matmul(weights, }\BuiltInTok{input}\NormalTok{) }\OperatorTok{+}\NormalTok{ bias  }\CommentTok{\# Each output needs all inputs}
\NormalTok{output }\OperatorTok{=}\NormalTok{ activation(Z)  }\CommentTok{\# Transform each result}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

During processor execution, these mathematical operations decompose into
explicit computational steps. Listing~\ref{lst-loop_linear_layer}
demonstrates how nested loops implement the multiply-accumulate
operations.

\begin{codelisting}

\caption{\label{lst-loop_linear_layer}\textbf{Linear Layer Computation}:
Each output neuron is computed by summing weighted inputs from all
features, followed by an activation function application. Understanding
this process helps in grasping the fundamental building blocks of neural
networks.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):            }\CommentTok{\# Process 32 samples at once}
    \ControlFlowTok{for}\NormalTok{ out\_neuron }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):  }\CommentTok{\# Compute each output neuron}
        \BuiltInTok{sum} \OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{for}\NormalTok{ in\_feature }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{256}\NormalTok{): }\CommentTok{\# Each output needs}
                                      \CommentTok{\# all inputs}
            \BuiltInTok{sum} \OperatorTok{+=} \BuiltInTok{input}\NormalTok{[batch, in\_feature] }\OperatorTok{*}
\NormalTok{                         weights[out\_neuron, in\_feature]}
\NormalTok{        output[batch, out\_neuron] }\OperatorTok{=}\NormalTok{ activation(}\BuiltInTok{sum} \OperatorTok{+}
\NormalTok{                                    bias[out\_neuron])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{Sequential Scalar
Execution}\label{sec-ai-acceleration-sequential-scalar-execution-682e}

Traditional scalar processors execute these operations sequentially,
processing individual values one at a time. For the linear layer example
above with a batch of 32 samples, computing the outputs requires over 4
million multiply-accumulate operations. Each operation involves loading
an input value and a weight value, multiplying them, and accumulating
the result. This sequential approach becomes highly inefficient when
processing the massive number of identical operations required by neural
networks.

To address this inefficiency, modern processors use vector processing to
execute multiple operations simultaneously.

\subsubsection{Parallel Vector
Execution}\label{sec-ai-acceleration-parallel-vector-execution-27a7}

Vector processing units achieve this transformation by operating on
multiple data elements simultaneously.
Listing~\ref{lst-riscv_vector_mac} reveals these capabilities through
RISC-V\sidenote{\textbf{RISC-V for AI}: RISC-V, the open-source
instruction set architecture from UC Berkeley (2010), is becoming
important for AI accelerators because it's freely customizable.
Companies like SiFive and Google have created RISC-V chips with custom
AI extensions. Unlike proprietary architectures, RISC-V allows hardware
designers to add specialized ML instructions without licensing fees,
potentially democratizing AI hardware development beyond the current
duopoly of x86 and ARM. } assembly code, where a single instruction
processes eight data elements in parallel.

\begin{codelisting}

\caption{\label{lst-riscv_vector_mac}\textbf{Vectorized
Multiply-Accumulate Loop}: This loop showcases how RISC-V vector
instructions enable efficient batch processing by performing 8
multiply-add operations simultaneously, reducing computational latency
in neural network training. (\citeproc{ref-riscv_manual}{Waterman and
Asanovic 2019})}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vsetvli t0}\OperatorTok{,}\NormalTok{ a0}\OperatorTok{,}\NormalTok{ e32   \# }\OperatorTok{\textless{}}\DecValTok{1}\OperatorTok{\textgreater{}}
\NormalTok{loop\_batch}\OperatorTok{:}
\NormalTok{    loop\_neuron}\OperatorTok{:}
\NormalTok{        vxor}\OperatorTok{.}\NormalTok{vv v0}\OperatorTok{,}\NormalTok{ v0}\OperatorTok{,}\NormalTok{ v0    \# }\OperatorTok{\textless{}}\DecValTok{2}\OperatorTok{\textgreater{}}
\NormalTok{        loop\_feature}\OperatorTok{:}
\NormalTok{            vle32}\OperatorTok{.}\NormalTok{v v1}\OperatorTok{,} \OperatorTok{(}\NormalTok{in\_ptr}\OperatorTok{)}\NormalTok{    \# }\OperatorTok{\textless{}}\DecValTok{3}\OperatorTok{\textgreater{}}
\NormalTok{            vle32}\OperatorTok{.}\NormalTok{v v2}\OperatorTok{,} \OperatorTok{(}\NormalTok{wt\_ptr}\OperatorTok{)}\NormalTok{    \# }\OperatorTok{\textless{}}\DecValTok{3}\OperatorTok{\textgreater{}}
\NormalTok{            vfmacc}\OperatorTok{.}\NormalTok{vv v0}\OperatorTok{,}\NormalTok{ v1}\OperatorTok{,}\NormalTok{ v2    \# }\OperatorTok{\textless{}}\DecValTok{4}\OperatorTok{\textgreater{}}
\NormalTok{            add in\_ptr}\OperatorTok{,}\NormalTok{ in\_ptr}\OperatorTok{,} \DecValTok{32}\NormalTok{  \# }\OperatorTok{\textless{}}\DecValTok{5}\OperatorTok{\textgreater{}}
\NormalTok{            add wt\_ptr}\OperatorTok{,}\NormalTok{ wt\_ptr}\OperatorTok{,} \DecValTok{32}\NormalTok{  \# }\OperatorTok{\textless{}}\DecValTok{5}\OperatorTok{\textgreater{}}
\NormalTok{            bnez feature\_cnt}\OperatorTok{,}\NormalTok{ loop\_feature}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Vector Length Configuration}: Configures the vector units to
  process 32-bit elements, automatically determining how many operations
  happen in parallel based on hardware width (VLEN).
\item
  \textbf{Vector Initialization}: Clears the accumulator vector
  \texttt{v0} (containing e.g., 8 parallel sums) using an exclusive-OR
  operation, which is more efficient than a load immediate.
\item
  \textbf{Vector Loads}: Loads continuous 32-bit input and weight values
  from memory into vector registers \texttt{v1} and \texttt{v2} in a
  single instruction, maximizing memory bandwidth utilization.
\item
  \textbf{Fused Multiply-Accumulate}: Performs parallel multiply-add
  operations (\(v_0 = v_0 + v_1 \times v_2\)). This is the core
  computational primitive, doubling throughput compared to separate
  multiply and add instructions.
\item
  \textbf{Pointer Arithmetic}: Updates memory pointers by the vector
  byte length to prepare for the next data chunk.
\end{enumerate}

\begin{verbatim}
:::

This vector implementation processes eight data elements in parallel, reducing both computation time and energy consumption. Vector load instructions transfer eight values simultaneously, maximizing memory bandwidth utilization. The vector multiply-accumulate instruction processes eight pairs of values in parallel, dramatically reducing the total instruction count from over 4 million to approximately 500,000.

Key vector operations map directly to common deep learning patterns. @tbl-vector enumerates how operations such as reduction, gather, scatter, and masked operations appear frequently in pooling, embedding lookups, and attention mechanisms, clarifying the direct mapping between low-level vector hardware and high-level machine learning workloads.

+-----------------------------+-----------------------------------------------------+---------------------------------------------+
| **Vector Operation**        | **Description**                                     | **Neural Network Application**              |
+:============================+:====================================================+:============================================+
| **Reduction**               | Combines elements across a vector (e.g., sum, max)  | Pooling layers, attention score computation |
+-----------------------------+-----------------------------------------------------+---------------------------------------------+
| **Gather**                  | Loads multiple non-consecutive memory elements      | Embedding lookups, sparse operations        |
+-----------------------------+-----------------------------------------------------+---------------------------------------------+
| **Scatter**                 | Writes to multiple non-consecutive memory locations | Gradient updates for embeddings             |
+-----------------------------+-----------------------------------------------------+---------------------------------------------+
| **Masked operations**       | Selectively operates on vector elements             | Attention masks, padding handling           |
+-----------------------------+-----------------------------------------------------+---------------------------------------------+
| **Vector-scalar broadcast** | Applies scalar to all vector elements               | Bias addition, scaling operations           |
+-----------------------------+-----------------------------------------------------+---------------------------------------------+

: **Vector Operations**: Neural network layers frequently utilize core vector operations such as reduction, gather, and scatter to accelerate computation and efficiently process data in parallel; these operations clarify how low-level hardware optimizations map to high-level machine learning algorithms. These operations enable efficient implementation of common layers like pooling, embedding lookups, and attention mechanisms within deep learning models. {#tbl-vector}

Vector processing efficiency gains extend beyond instruction count reduction. Memory bandwidth utilization improves as vector loads transfer multiple values per operation. Energy efficiency increases because control logic is shared across multiple operations. These improvements compound across the deep layers of modern neural networks, where billions of operations execute for each forward pass.

To understand why these architectural choices persist, we trace their origins to the foundational systems that first demonstrated their value.

#### Vector Processing History {#sec-ai-acceleration-vector-processing-history-c631}

The principles underlying vector operations have long been central to high-performance computing. In the 1970s and 1980s, vector processors emerged as an architectural solution for scientific computing, weather modeling, and physics simulations, where large arrays of data required efficient parallel processing. Early systems such as the Cray-1[^fn-cray-vector], one of the first commercially successful supercomputers, introduced dedicated vector units to perform arithmetic operations on entire data vectors in a single instruction. These vector units dramatically improved computational throughput compared to traditional scalar execution [@jordan1982guide].

[^fn-cray-vector]: **Cray-1 Vector Legacy**: The Cray-1 (1975) cost $8.8 million (approximately $40-45 million in 2024 dollars) but could perform 160 million floating-point operations per second, 1000x faster than typical computers. Its 64-element vector registers and pipelined vector units established the architectural template that modern AI accelerators still follow: process many data elements simultaneously with specialized hardware pipelines.

These concepts have reemerged in machine learning, where neural networks exhibit structure suited to vectorized execution. The same operations, such as vector addition, multiplication, and reduction, that once accelerated numerical simulations now drive the execution of machine learning workloads. While the scale and specialization of modern AI accelerators differ from their historical predecessors, the underlying architectural principles remain the same.

Vector operations establish the foundation for neural network acceleration by enabling efficient parallel processing of independent data elements. While vector operations excel at element-wise transformations like activation functions, neural networks also require structured computations that combine multiple input features to produce output features, transformations that naturally express themselves as matrix operations. This need for coordinated computation across multiple dimensions simultaneously leads to the next architectural primitive: matrix operations.

### Matrix Operations {#sec-ai-acceleration-matrix-operations-508d}

Matrix operations form the computational workhorse of neural networks, transforming high-dimensional data through structured patterns of weights, activations, and gradients [@Goodfellow-et-al-2016]. While vector operations process elements independently, matrix operations orchestrate computations across multiple dimensions simultaneously. These operations reveal patterns that drive hardware acceleration strategies.

#### Matrix Operations in Neural Networks {#sec-ai-acceleration-matrix-operations-neural-networks-527a}

Neural network computations decompose into hierarchical matrix operations. @lst-linear_matrix_hierarchy captures this hierarchy through a linear layer that transforms input features into output neurons over a batch.

::: {#lst-linear_matrix_hierarchy lst-cap="**Matrix Operations**: Neural networks perform transformations using matrix multiplications and biases to achieve output predictions. Training requires careful management of input batches and activation functions to optimize model performance."}
```{.python}
layer = nn.Linear(256, 512)  # Layer transforms 256 inputs to
# 512 outputs {#sec-512-outputs}
output = layer(input_batch)  # Process a batch of 32 samples

# Framework Internal: Core operations {#sec-framework-internal-core-operations}
Z = matmul(weights, input)  # Matrix: transforms [256 x 32]
# input to [512 x 32] output {#sec-input-512-x-32-output}
Z = Z + bias  # Vector: adds bias to each
# output independently {#sec-output-independently}
output = relu(Z)  # Vector: applies activation to
# each element independently {#sec-element-independently}
\end{verbatim}

}

\end{codelisting}%

This computation demonstrates the scale of matrix operations in neural
networks. Each output neuron (512 total) must process all input features
(256 total) for every sample in the batch (32 samples). The weight
matrix alone contains \(256 \times 512 = 131,072\) parameters that
define these transformations, illustrating why efficient matrix
multiplication dominates performance considerations.

Neural networks employ matrix operations across diverse architectural
patterns beyond simple linear layers.

\subsubsection{Types of Matrix Computations in Neural
Networks}\label{sec-ai-acceleration-types-matrix-computations-neural-networks-b497}

Matrix operations appear consistently across modern neural
architectures. Convolution operations transform into matrix
multiplications through the im2col technique\sidenote{\textbf{Im2col
(Image-to-Column)}: A preprocessing technique that converts convolution
operations into matrix multiplications by unfolding image patches into
column vectors. A 3×3 convolution on a 224×224 image creates a matrix
with \textasciitilde50,000 columns, enabling efficient GEMM execution
but increasing memory usage 9× due to overlapping patches. This
transformation explains why convolutions are actually matrix operations
in modern ML accelerators. }, enabling efficient execution on
matrix-optimized hardware. Listing~\ref{lst-matrix_patterns} illustrates
these diverse applications.

\begin{codelisting}

\caption{\label{lst-matrix_patterns}\textbf{Linear Layers}: Layer
transformations combine input features to produce hidden
representations. Matrix operations in neural networks enable efficient
feature extraction and transformation, forming the backbone of many
machine learning architectures.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hidden }\OperatorTok{=}\NormalTok{ matmul(weights, inputs)}
\CommentTok{\# weights: [out\_dim x in\_dim], inputs: [in\_dim x batch] \{\#sec{-}weights{-}out\_dim{-}x{-}in\_dim{-}inputs{-}in\_dim{-}x{-}batch\}}
\CommentTok{\# Result combines all inputs for each output \{\#sec{-}result{-}combines{-}inputs{-}output\}}

\CommentTok{\# Attention Mechanisms {-} Multiple matrix operations}
\NormalTok{Q }\OperatorTok{=}\NormalTok{ matmul(Wq, inputs)}
\CommentTok{\# Project inputs to query space [query\_dim x batch]}
\NormalTok{K }\OperatorTok{=}\NormalTok{ matmul(Wk, inputs)}
\CommentTok{\# Project inputs to key space[key\_dim x batch]}
\NormalTok{attention }\OperatorTok{=}\NormalTok{ matmul(Q, K.T)}
\CommentTok{\# Compare all queries with all keys [query\_dim x key\_dim]}

\CommentTok{\# Convolutions {-} Matrix multiply after reshaping \{\#sec{-}convolutions{-}matrix{-}multiply{-}reshaping\}}
\NormalTok{patches }\OperatorTok{=}\NormalTok{ im2col(}\BuiltInTok{input}\NormalTok{)}
\CommentTok{\# Convert [H x W x C] image to matrix of patches \{\#sec{-}convert{-}h{-}x{-}w{-}x{-}c{-}image{-}matrix{-}patches\}}
\NormalTok{output }\OperatorTok{=}\NormalTok{ matmul(kernel, patches)}
\CommentTok{\# Apply kernels to all patches simultaneously \{\#sec{-}apply{-}kernels{-}patches{-}simultaneously\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This pervasive pattern of matrix multiplication has direct implications
for hardware design. The need for efficient matrix operations drives the
development of specialized hardware architectures that can handle these
computations at scale. Modern AI accelerators implement matrix
operations, focusing on their architectural features and performance
optimizations.

\subsubsection{Matrix Operations Hardware
Acceleration}\label{sec-ai-acceleration-matrix-operations-hardware-acceleration-514a}

The computational demands of matrix operations have driven specialized
hardware optimizations. Listing~\ref{lst-matrix_unit} demonstrates how
modern processors implement dedicated matrix units that process entire
16x16 blocks simultaneously, achieving 32x higher throughput than vector
processing alone.

\begin{codelisting}

\caption{\label{lst-matrix_unit}\textbf{Matrix Unit Operation}: Enables
efficient block-wise matrix multiplication and accumulation in
hardware-accelerated systems, demonstrating how specialized units
streamline computational tasks for AI/ML operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mload mr1}\OperatorTok{,} \OperatorTok{(}\NormalTok{weight\_ptr}\OperatorTok{)}\NormalTok{     \# Load e}\OperatorTok{.}\NormalTok{g}\OperatorTok{.,} \DecValTok{16}\ErrorTok{x16}\NormalTok{ block of}
                            \PreprocessorTok{\# }\ErrorTok{weight matrix}
\NormalTok{mload mr2}\OperatorTok{,} \OperatorTok{(}\NormalTok{input\_ptr}\OperatorTok{)}\NormalTok{      \# Load corresponding input block}
\NormalTok{matmul}\OperatorTok{.}\NormalTok{mm mr3}\OperatorTok{,}\NormalTok{ mr1}\OperatorTok{,}\NormalTok{ mr2     \# Multiply and accumulate entire}
                            \PreprocessorTok{\# }\ErrorTok{blocks at once}
\NormalTok{mstore }\OperatorTok{(}\NormalTok{output\_ptr}\OperatorTok{),}\NormalTok{ mr3    \# Store computed output block}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This matrix processing unit can handle \(16\times16\) blocks of the
linear layer computation described earlier, processing 256
multiply-accumulate operations simultaneously compared to the 8
operations possible with vector processing. These matrix operations
complement vectorized computation by enabling structured many-to-many
transformations. The interplay between matrix and vector operations
shapes the efficiency of neural network execution.

Matrix operations provide computational capabilities for neural networks
through coordinated parallel processing across multiple dimensions.
While they enable transformations such as attention mechanisms and
convolutions, their performance depends on efficient data handling.
Conversely, vector operations are optimized for one-to-one
transformations like activation functions and layer normalization. The
distinction between these operations highlights the importance of
dataflow patterns in neural accelerator design, examined next
(\citeproc{ref-Hwu2011GPU}{Hwu 2011}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1412}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1529}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2941}}@{}}
\caption{\textbf{Operation Characteristics}: Matrix operations excel at
many-to-many transformations common in neural network layers, while
vector operations efficiently handle one-to-one transformations like
activation functions and normalization. Understanding these distinctions
guides the selection of appropriate computational primitives for
different machine learning tasks and impacts system
performance.}\label{tbl-matrix}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Characteristic}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Characteristic}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Matrix Operations} & Many-to-many transforms & Layer
transformations, attention, convolutions & Each output depends on
multiple inputs \\
\textbf{Vector Operations} & One-to-one transforms & Activation
functions, layer normalization, element-wise gradients & Each output
depends only on corresponding input \\
\end{longtable}

\subsubsection{Historical Foundations of Matrix
Computation}\label{sec-ai-acceleration-historical-foundations-matrix-computation-402e}

Matrix operations have long served as a cornerstone of computational
mathematics, with applications extending from numerical simulations to
graphics processing (\citeproc{ref-Golub1996Matrix}{Golub and Loan
1996}). The structured nature of matrix multiplications and
transformations made them natural targets for acceleration in early
computing architectures. In the 1980s and 1990s, specialized digital
signal processors (DSPs) and graphics processing units (GPUs) optimized
for matrix computations played a critical role in accelerating workloads
such as image processing, scientific computing, and 3D rendering
(\citeproc{ref-owens2008gpu}{Owens et al. 2008}).

The widespread adoption of machine learning has reinforced the
importance of efficient matrix computation. Neural networks,
fundamentally built on matrix multiplications and tensor operations,
have driven the development of dedicated hardware architectures that
extend beyond traditional vector processing. Modern tensor processing
units (TPUs) and AI accelerators implement matrix multiplication at
scale, reflecting the same architectural principles that once
underpinned early scientific computing and graphics workloads.

Table~\ref{tbl-matrix} contrasts matrix and vector operations, revealing
how different computational patterns map to neural network primitives.
While matrix operations provide the computational backbone for neural
networks, they represent only part of the acceleration challenge. Neural
networks also depend critically on non-linear transformations that
cannot be efficiently expressed through linear algebra alone.

\subsection{Special Function
Units}\label{sec-ai-acceleration-special-function-units-ed00}

While vector and matrix operations efficiently handle the linear
transformations in neural networks, non-linear functions present unique
computational challenges that require dedicated hardware solutions.
Special Function Units (SFUs) provide hardware acceleration for these
computations, completing the set of fundamental processing primitives
needed for efficient neural network execution.

\subsubsection{Non-Linear
Functions}\label{sec-ai-acceleration-nonlinear-functions-fdce}

Non-linear functions enable neural networks to model complex
relationships (\citeproc{ref-Goodfellow-et-al-2016}{Goodfellow,
Courville, and Bengio 2013}). Listing~\ref{lst-nonlinear_layer} presents
a typical neural network layer sequence that combines linear
transformations with non-linear activations.

\begin{codelisting}

\caption{\label{lst-nonlinear_layer}\textbf{Non-Linear Transformations}:
Neural networks process input data through a sequence of linear
transformations followed by non-linear activations to capture complex
patterns. This layer sequence enhances model expressiveness and learning
capabilities.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{layer }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{    nn.Linear(}\DecValTok{256}\NormalTok{, }\DecValTok{512}\NormalTok{), nn.ReLU(), nn.BatchNorm1d(}\DecValTok{512}\NormalTok{)}
\NormalTok{)}
\NormalTok{output }\OperatorTok{=}\NormalTok{ layer(input\_tensor)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This sequence introduces multiple non-linear transformations that extend
beyond simple matrix operations. Listing~\ref{lst-nonlinear_math} breaks
down these operations into their mathematical components, exposing the
computational complexity that hardware must address.

\begin{codelisting}

\caption{\label{lst-nonlinear_math}\textbf{Non-linear Transformations}:
Neural networks apply linear and non-linear operations to transform
input data into meaningful features for learning. Machine learning
models use these transformations to capture complex patterns in data
efficiently.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Z }\OperatorTok{=}\NormalTok{ matmul(weights, }\BuiltInTok{input}\NormalTok{) }\OperatorTok{+}\NormalTok{ bias  }\CommentTok{\# Linear transformation}
\NormalTok{H }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{, Z)  }\CommentTok{\# ReLU activation}
\NormalTok{mean }\OperatorTok{=}\NormalTok{ reduce\_mean(H, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)  }\CommentTok{\# BatchNorm statistics}
\NormalTok{var }\OperatorTok{=}\NormalTok{ reduce\_mean((H }\OperatorTok{{-}}\NormalTok{ mean) }\OperatorTok{**} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Variance computation}
\NormalTok{output }\OperatorTok{=}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ (H }\OperatorTok{{-}}\NormalTok{ mean) }\OperatorTok{/}\NormalTok{ sqrt(var }\OperatorTok{+}\NormalTok{ eps) }\OperatorTok{+}\NormalTok{ beta}
\CommentTok{\# Normalization \{\#sec{-}normalization\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{Hardware Implementation of Non-Linear
Functions}\label{sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d}

The computational complexity of these operations becomes apparent when
examining their implementation on traditional processors. These
seemingly simple mathematical operations translate into complex
sequences of instructions. Consider the computation of batch
normalization: calculating the square root requires multiple iterations
of numerical approximation, while exponential functions in operations
like softmax need series expansion or lookup tables
(\citeproc{ref-Ioffe2015}{Ioffe and Szegedy 2015}). Even a simple ReLU
activation introduces branching logic that can disrupt instruction
pipelining. Listing~\ref{lst-traditional_overhead} demonstrates these
inefficiencies.

\begin{codelisting}

\caption{\label{lst-traditional_overhead}\textbf{ReLU and BatchNorm
Operations}: Neural networks process input data through conditional
operations that can disrupt instruction pipelining and multiple passes
required for normalization, highlighting efficiency challenges in
traditional implementations. (\citeproc{ref-ieee_spectrum_relu}{Cass
2020})}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):}
    \ControlFlowTok{for}\NormalTok{ feature }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):}
       \CommentTok{\# ReLU: Requires branch prediction and potential}
       \CommentTok{\# pipeline stalls}
\NormalTok{       z }\OperatorTok{=}\NormalTok{ matmul\_output[batch, feature]}
\NormalTok{       h }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\FloatTok{0.0}\NormalTok{, z)    }\CommentTok{\# Conditional operation}

       \CommentTok{\# BatchNorm: Multiple passes over data}
\NormalTok{       mean\_sum[feature] }\OperatorTok{+=}\NormalTok{ h    }\CommentTok{\# First pass for mean}
\NormalTok{       var\_sum[feature] }\OperatorTok{+=}\NormalTok{ h }\OperatorTok{*}\NormalTok{ h }\CommentTok{\# Additional pass for variance}

\NormalTok{       temp[batch, feature] }\OperatorTok{=}\NormalTok{ h  }\CommentTok{\# Extra memory storage needed}

\CommentTok{\# Normalization requires complex arithmetic \{\#sec{-}normalization{-}requires{-}complex{-}arithmetic\}}
\ControlFlowTok{for}\NormalTok{ feature }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):}
\NormalTok{    mean }\OperatorTok{=}\NormalTok{ mean\_sum[feature] }\OperatorTok{/}\NormalTok{ batch\_size}
\NormalTok{    var }\OperatorTok{=}\NormalTok{ (var\_sum[feature] }\OperatorTok{/}\NormalTok{ batch\_size) }\OperatorTok{{-}}\NormalTok{ mean }\OperatorTok{*}\NormalTok{ mean}

    \CommentTok{\# Square root computation: Multiple iterations}
\NormalTok{    scale }\OperatorTok{=}\NormalTok{ gamma[feature] }\OperatorTok{/}\NormalTok{ sqrt(var }\OperatorTok{+}\NormalTok{ eps)  }\CommentTok{\# Iterative}
                                              \CommentTok{\# approximation}
\NormalTok{    shift }\OperatorTok{=}\NormalTok{ beta[feature] }\OperatorTok{{-}}\NormalTok{ mean }\OperatorTok{*}\NormalTok{ scale}

    \CommentTok{\# Additional pass over data for final computation}
    \ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):}
\NormalTok{        output[batch, feature] }\OperatorTok{=}\NormalTok{ temp[batch, feature] }\OperatorTok{*}
\NormalTok{                                 scale }\OperatorTok{+}\NormalTok{ shift}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

These operations introduce several key inefficiencies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multiple passes over data, increasing memory bandwidth requirements
\item
  Complex arithmetic requiring many instruction cycles
\item
  Conditional operations that can cause pipeline stalls
\item
  Additional memory storage for intermediate results
\item
  Poor utilization of vector processing units
\end{enumerate}

More specifically, each operation introduces distinct challenges. Batch
normalization requires multiple passes through data: one for mean
computation, another for variance, and a final pass for output
transformation. Each pass loads and stores data through the memory
hierarchy. Operations that appear simple in mathematical notation often
expand into many instructions. The square root computation typically
requires 10-20 iterations of numerical methods like Newton-Raphson
approximation for suitable precision
(\citeproc{ref-Goldberg1991}{Goldberg 1991}). Conditional operations
like ReLU's max function require branch instructions that can stall the
processor's pipeline. The implementation needs temporary storage for
intermediate values, increasing memory usage and bandwidth consumption.
While vector units excel at regular computations, functions like
exponentials and square roots often require scalar operations that
cannot fully utilize vector processing capabilities.

\subsubsection{Hardware
Acceleration}\label{sec-ai-acceleration-hardware-acceleration-08e3}

SFUs address these inefficiencies through dedicated hardware
implementation. Modern ML accelerators include specialized circuits that
transform these complex operations into single-cycle or fixed-latency
computations. Listing~\ref{lst-sfu_vector_ops} demonstrates this
efficiency: loading a vector of values allows the accelerator to apply
ReLU, sigmoid, and square root operations directly in 1-8 cycles,
eliminating multiple passes and complex instruction sequences.

\begin{codelisting}

\caption{\label{lst-sfu_vector_ops}\textbf{Hardware Acceleration}:
Single-cycle non-linear operations enable efficient vector processing in
ML accelerators, demonstrating how specialized hardware reduces
computational latency.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vld}\OperatorTok{.}\NormalTok{v v1}\OperatorTok{,} \OperatorTok{(}\NormalTok{input\_ptr}\OperatorTok{)}\NormalTok{    \# Load vector of values}
\NormalTok{vrelu}\OperatorTok{.}\NormalTok{v v2}\OperatorTok{,}\NormalTok{ v1           \# Single}\OperatorTok{{-}}\NormalTok{cycle ReLU on entire vector}
\NormalTok{vsigm}\OperatorTok{.}\NormalTok{v v3}\OperatorTok{,}\NormalTok{ v1           \# Fixed}\OperatorTok{{-}}\NormalTok{latency sigmoid computation}
\NormalTok{vtanh}\OperatorTok{.}\NormalTok{v v4}\OperatorTok{,}\NormalTok{ v1           \# Direct hardware tanh implementation}
\NormalTok{vrsqrt}\OperatorTok{.}\NormalTok{v v5}\OperatorTok{,}\NormalTok{ v1          \# Fast reciprocal square root}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each SFU implements a specific function through specialized circuitry.
For instance, a ReLU unit performs the comparison and selection in
dedicated logic, eliminating branching overhead. Square root operations
use hardware implementations of algorithms like Newton-Raphson with
fixed iteration counts, providing predictable latency bounds.
Exponential and logarithmic functions often combine small lookup tables
with hardware interpolation circuits
(\citeproc{ref-Lauterbach2019}{Costa et al. 2019}). Table~\ref{tbl-sfu}
summarizes the various hardware implementations and their typical
latencies, spanning from single-cycle activations to logarithmic-time
reductions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2110}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2018}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3670}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2018}}@{}}
\caption{\textbf{Special Function Units}: Dedicated hardware
implementations of common mathematical functions (like relu, sigmoid,
and reciprocal square root) accelerate machine learning computations by
eliminating software overhead and enabling parallel processing of vector
data. Typical latencies of 1-2 cycles per function demonstrate the
performance gains achieved through specialized circuitry instead of
general-purpose arithmetic.}\label{tbl-sfu}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Function Unit}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Latency}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Function Unit}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Latency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Activation Unit} & ReLU, sigmoid, tanh & Piece-wise
approximation circuits & 1-2 cycles \\
\textbf{Statistics Unit} & Mean, variance & Parallel reduction trees &
log(N) cycles \\
\textbf{Exponential Unit} & exp, log & Table lookup + hardware
interpolation & 2-4 cycles \\
\textbf{Root/Power Unit} & sqrt, rsqrt & Fixed-iteration Newton-Raphson
& 4-8 cycles \\
\end{longtable}

\subsubsection{SFUs
History}\label{sec-ai-acceleration-sfus-history-a1b6}

The need for efficient non-linear function evaluation has shaped
computer architecture for decades. Early processors incorporated
hardware support for complex mathematical functions, such as logarithms
and trigonometric operations, to accelerate workloads in scientific
computing and signal processing (\citeproc{ref-Smith1997}{Smith 1997}).
In the 1970s and 1980s, floating-point co-processors were introduced to
handle complex mathematical operations separately from the main CPU
(\citeproc{ref-palmer_8087_1981}{Palmer 1980}). In the 1990s,
instruction set extensions such as Intel's SSE and ARM's NEON provided
dedicated hardware for vectorized mathematical transformations,
improving efficiency for multimedia and signal processing applications.

Machine learning workloads have reintroduced a strong demand for
specialized functional units, as activation functions, normalization
layers, and exponential transformations are fundamental to neural
network computations. Rather than relying on iterative software
approximations, modern AI accelerators implement fast, fixed-latency
SFUs for these operations, mirroring historical trends in scientific
computing.

With vector operations, matrix operations, and special function units
now established as the fundamental computational primitives, the next
question is how these components are organized into complete execution
units. Understanding this organization reveals both the theoretical
capabilities and the practical performance characteristics that
developers encounter when targeting contemporary AI accelerators.

\subsection{Compute Units and Execution
Models}\label{sec-ai-acceleration-compute-units-execution-models-f406}

The vector operations, matrix operations, and special function units
examined previously represent the fundamental computational primitives
in AI accelerators. Modern AI processors package these primitives into
distinct execution units, such as SIMD units, tensor cores, and
processing elements, which define how computations are structured and
exposed to users. Understanding this organization reveals both the
theoretical capabilities and practical performance characteristics that
developers can leverage in contemporary AI accelerators.

\subsubsection{Mapping Primitives to Execution
Units}\label{sec-ai-acceleration-mapping-primitives-execution-units-ccb6}

The progression from computational primitives to execution units follows
a structured hierarchy that reflects the increasing complexity and
specialization of AI accelerators:

\begin{itemize}
\tightlist
\item
  Vector operations → SIMD/SIMT units that enable parallel processing of
  independent data elements
\item
  Matrix operations → Tensor cores and systolic arrays that provide
  structured matrix multiplication
\item
  Special functions → Dedicated hardware units integrated within
  processing elements
\end{itemize}

Each execution unit combines these computational primitives with
specialized memory and control mechanisms, optimizing both performance
and energy efficiency. This structured packaging allows hardware vendors
to expose standardized programming interfaces while implementing diverse
underlying architectures tailored to specific workload requirements. The
choice of execution unit significantly influences overall system
efficiency, affecting data locality, compute density, and workload
adaptability. Subsequent sections examine how these execution units
operate within AI accelerators to maximize performance across different
machine learning tasks.

\subsubsection{Evolution from SIMD to SIMT
Architectures}\label{sec-ai-acceleration-evolution-simd-simt-architectures-e1fd}

Single Instruction Multiple Data (SIMD)\sidenote{\textbf{SIMD (Single
Instruction, Multiple Data)}: From Michael Flynn's 1966 taxonomy
classifying computer architectures by instruction and data streams. SIMD
describes machines where one instruction operates on multiple data
elements simultaneously. SIMT (Single Instruction, Multiple Thread),
coined by NVIDIA, extends this to many lightweight threads sharing
instruction fetch. While CPUs use wide SIMD units (e.g., AVX-512), GPUs
coordinate tens of thousands of threads concurrently through SIMT,
enabling the massive parallelism neural networks require. } execution
applies identical operations to multiple data elements in parallel,
minimizing instruction overhead while maximizing data throughput. This
execution model is widely used to accelerate workloads with regular,
independent data parallelism, such as neural network computations. The
ARM Scalable Vector Extension (SVE) provides a representative example of
how modern architectures implement SIMD operations efficiently.
Listing~\ref{lst-arm_sve_vector} demonstrates this approach.

\begin{codelisting}

\caption{\label{lst-arm_sve_vector}\textbf{Vector Operation}: Vector
multiplication and addition operations enable efficient parallel
processing in machine learning models. (\citeproc{ref-ARM2020}{Ltd.
2020})}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ptrue p0}\OperatorTok{.}\NormalTok{s              \# Create predicate }\ControlFlowTok{for}\NormalTok{ vector length}
\NormalTok{ld1w z0}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ p0}\OperatorTok{/}\NormalTok{z}\OperatorTok{,} \OperatorTok{[}\NormalTok{x0}\OperatorTok{]}\NormalTok{   \# Load vector of inputs}
\NormalTok{fmul z1}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z0}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z0}\OperatorTok{.}\NormalTok{s   \# Multiply elements}
\NormalTok{fadd z2}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z1}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z0}\OperatorTok{.}\NormalTok{s   \# Add elements}
\NormalTok{st1w z2}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ p0}\OperatorTok{,} \OperatorTok{[}\NormalTok{x1}\OperatorTok{]}\NormalTok{     \# Store results}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Processor architectures continue to expand SIMD capabilities to
accommodate increasing computational demands. Intel's Advanced Matrix
Extensions (AMX) (\citeproc{ref-intel2021amx}{I. Corporation 2021}) and
ARM's SVE2 architecture (\citeproc{ref-stephens2017arm}{Stephens et al.
2017}) provide flexible SIMD execution, enabling software to scale
across different hardware implementations.

To address these limitations, SIMT extends SIMD principles by enabling
parallel execution across multiple independent threads, each maintaining
its own program counter and architectural state
(\citeproc{ref-lindholm2008nvidia}{Lindholm et al. 2008}). This model
maps naturally to matrix computations, where each thread processes
different portions of a workload while still benefiting from shared
instruction execution. In NVIDIA's GPU architectures, each Streaming
Multiprocessor (SM)\sidenote{\textbf{Streaming Multiprocessor (SM)}: A
GPU compute unit containing many lightweight execution lanes, shared
memory, and schedulers. SMs execute threads in a SIMT fashion, where
groups of threads follow the same instruction stream while operating on
different data, enabling massive parallelism on regular numerical
workloads. } coordinates thousands of threads executing in parallel,
allowing for efficient scaling of neural network computations. Threads
are organized into warps\sidenote{\textbf{Warp}: From weaving
terminology, where the warp consists of parallel threads held taut on a
loom while the weft weaves through them. NVIDIA adopted this metaphor
for its fundamental execution unit of 32 threads that execute in
lock-step, emphasizing how these parallel threads are ``woven'' together
in coordinated execution. All threads in a warp share instruction fetch;
if they diverge in control flow, the warp serializes execution paths,
reducing efficiency. }, which are the fundamental execution units that
enable SIMT efficiency. Listing~\ref{lst-cuda_simt} shows this parallel
processing model in action.

\begin{codelisting}

\caption{\label{lst-cuda_simt}\textbf{SIMT Execution}: Each thread
processes a unique output element in parallel, demonstrating how SIMT
enables efficient matrix multiplication on GPUs.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_\_global\_\_ }\DataTypeTok{void}\NormalTok{ matrix\_multiply}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{*}\NormalTok{ C}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ A}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}
\NormalTok{                                B}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ N}\OperatorTok{)} \OperatorTok{\{}
    \CommentTok{// Each thread processes one output element}
    \DataTypeTok{int}\NormalTok{ row }\OperatorTok{=}\NormalTok{ blockIdx}\OperatorTok{.}\NormalTok{y }\OperatorTok{*}\NormalTok{ blockDim}\OperatorTok{.}\NormalTok{y }\OperatorTok{+}\NormalTok{ threadIdx}\OperatorTok{.}\NormalTok{y}\OperatorTok{;}
    \DataTypeTok{int}\NormalTok{ col }\OperatorTok{=}\NormalTok{ blockIdx}\OperatorTok{.}\NormalTok{x }\OperatorTok{*}\NormalTok{ blockDim}\OperatorTok{.}\NormalTok{x }\OperatorTok{+}\NormalTok{ threadIdx}\OperatorTok{.}\NormalTok{x}\OperatorTok{;}

    \DataTypeTok{float}\NormalTok{ sum }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ k }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ k }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{;}\NormalTok{ k}\OperatorTok{++)} \OperatorTok{\{}
        \CommentTok{// Threads in a warp execute in parallel}
\NormalTok{        sum }\OperatorTok{+=}\NormalTok{ A}\OperatorTok{[}\NormalTok{row }\OperatorTok{*}\NormalTok{ N }\OperatorTok{+}\NormalTok{ k}\OperatorTok{]} \OperatorTok{*}\NormalTok{ B}\OperatorTok{[}\NormalTok{k }\OperatorTok{*}\NormalTok{ N }\OperatorTok{+}\NormalTok{ col}\OperatorTok{];}
    \OperatorTok{\}}
\NormalTok{    C}\OperatorTok{[}\NormalTok{row }\OperatorTok{*}\NormalTok{ N }\OperatorTok{+}\NormalTok{ col}\OperatorTok{]} \OperatorTok{=}\NormalTok{ sum}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

SIMT execution allows neural network computations to scale efficiently
across thousands of threads while maintaining flexibility for divergent
execution paths. Similar execution models appear in AMD's RDNA and
Intel's Xe architectures, reinforcing SIMT as a fundamental mechanism
for AI acceleration.

\subsubsection{Tensor
Cores}\label{sec-ai-acceleration-tensor-cores-771f}

While SIMD and SIMT units provide efficient execution of vector
operations, neural networks rely heavily on matrix computations that
require specialized execution units for structured multi-dimensional
processing. The energy economics of matrix operations drive this
specialization: traditional scalar processing can require multiple
off-chip memory accesses per operation, while tensor cores amortize data
movement across entire matrix blocks. Tensor processing units extend
SIMD and SIMT principles by enabling efficient matrix operations through
dedicated hardware blocks that execute matrix multiplications and
accumulations on matrix tiles. In many cases, this shifts the dominant
cost from off-chip data movement toward on-chip reuse and arithmetic,
depending on the kernel mix and memory behavior.

Tensor cores\sidenote{\textbf{Tensor Core}: The term ``tensor'' derives
from Latin ``tendere'' (to stretch), originally describing mathematical
objects that transform under coordinate changes. In ML, tensors
generalize matrices to arbitrary dimensions. NVIDIA introduced tensor
cores in the V100 (2017) to accelerate the small matrix operations
(4x4x4 tiles) common in neural networks. On supported kernels and
reduced-precision modes, tensor cores deliver large speedups over
conventional GPU execution, with modern accelerators reaching hundreds
of TFLOPS for reduced-precision dense kernels. } provide an example of
this approach. Listing~\ref{lst-tensor_core_op} exposes matrix
computation capabilities through specialized instructions that use
dedicated hardware blocks.

\begin{codelisting}

\caption{\label{lst-tensor_core_op}\textbf{Tensor Core Operation}:
Matrix multiplications are performed in parallel across entire matrix
blocks, optimizing computational efficiency for neural network
training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{Tensor Core Operation }\OperatorTok{(}\NormalTok{example GPU}\OperatorTok{):}
\NormalTok{mma}\OperatorTok{.}\NormalTok{sync}\OperatorTok{.}\NormalTok{aligned}\OperatorTok{.}\NormalTok{m16n16k16}\OperatorTok{.}\NormalTok{f16}\OperatorTok{.}\NormalTok{f16}
  \OperatorTok{\{}\NormalTok{d0}\OperatorTok{,}\NormalTok{d1}\OperatorTok{,}\NormalTok{d2}\OperatorTok{,}\NormalTok{d3}\OperatorTok{\},}     \CommentTok{// Destination registers}
  \OperatorTok{\{}\NormalTok{a0}\OperatorTok{,}\NormalTok{a1}\OperatorTok{,}\NormalTok{a2}\OperatorTok{,}\NormalTok{a3}\OperatorTok{\},}     \CommentTok{// Source matrix A}
  \OperatorTok{\{}\NormalTok{b0}\OperatorTok{,}\NormalTok{b1}\OperatorTok{,}\NormalTok{b2}\OperatorTok{,}\NormalTok{b3}\OperatorTok{\},}     \CommentTok{// Source matrix B}
  \OperatorTok{\{}\NormalTok{c0}\OperatorTok{,}\NormalTok{c1}\OperatorTok{,}\NormalTok{c2}\OperatorTok{,}\NormalTok{c3}\OperatorTok{\}}      \CommentTok{// Accumulator}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

A single tensor core instruction processes an entire matrix block while
maintaining intermediate results in local registers, significantly
improving computational efficiency compared to implementations based on
scalar or vector operations. This structured approach enables hardware
to achieve high throughput while reducing the burden of explicit loop
unrolling and data management at the software level.

Tensor processing unit architectures differ based on design priorities.
Some GPU families incorporate tensor cores optimized for general-purpose
deep learning acceleration. TPU-style designs utilize large-scale matrix
units arranged in systolic arrays to maximize sustained training
throughput on dense tensor kernels. Mobile
NPUs\sidenote{\textbf{On-Device Neural Engine Strategy}: Many mobile
SoCs include dedicated neural engines (NPUs) to enable on-device ML
within tight battery and thermal envelopes. These blocks can accelerate
common inference kernels efficiently, supporting interactive features
such as vision and speech processing without requiring cloud
connectivity. } integrate smaller matrix processors optimized for
low-power inference workloads, while some server CPUs introduce matrix
instruction extensions (AMX-class tiles) designed for datacenter
inference and mixed workloads.

The increasing specialization of AI hardware has driven significant
performance improvements in deep learning workloads.
Figure~\ref{fig-ai-performance} charts the trajectory of AI accelerator
performance in NVIDIA GPUs, highlighting the transition from
general-purpose floating-point execution units to highly optimized
tensor processing cores.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/images/png/int8_tops_nvidia.png}}

}

\caption{\label{fig-ai-performance}\textbf{GPU Performance Scaling}:
NVIDIA GPUs experienced a \(10\times\) increase in integer 8-bit TOPS
(tera operations per second) over a decade, driven by architectural
innovations transitioning from floating-point to tensor core
acceleration. This trend reflects the growing specialization of hardware
for deep learning workloads and the increasing demand for efficient
inference capabilities.}

\end{figure}%

\subsubsection{Processing
Elements}\label{sec-ai-acceleration-processing-elements-daa1}

The highest level of execution unit organization integrates multiple
tensor cores with local memory into processing elements (PEs). A
processing element serves as a fundamental building block in many AI
accelerators, combining different computational units to efficiently
execute neural network operations. Each PE typically includes vector
units for element-wise operations, tensor cores for matrix computation,
special function units for non-linear transformations, and dedicated
memory resources to optimize data locality and minimize data movement
overhead.

Processing elements balance computational density with memory access
efficiency. Their design varies across different architectures to
support diverse workloads and scalability requirements. Graphcore's
Intelligence Processing Unit (IPU) distributes computation across 1,472
tiles, each containing independent processing elements optimized for
fine-grained parallelism (\citeproc{ref-Graphcore2020}{Graphcore 2020}).
Cerebras extends this approach in the CS-2 system, integrating 850,000
processing elements across a wafer-scale device to accelerate sparse
computations. Tesla's D1 processor arranges processing elements with
substantial local memory, optimizing throughput and latency for
real-time autonomous vehicle workloads
(\citeproc{ref-Tesla2021}{Quinnell 2024}).

Processing elements provide the structural foundation for large-scale AI
acceleration. Their efficiency depends not only on computational
capability but also on interconnect strategies and memory hierarchy
design. Architectural choices impact performance across different AI
workloads.

Beyond the basic organization of processing elements, modern
accelerators employ increasingly sophisticated techniques to extract
performance from neural network workloads. Tensor processing units have
enabled substantial efficiency gains in AI workloads by using
hardware-accelerated matrix computation. Their role continues to evolve
as architectures incorporate support for advanced execution techniques,
including structured sparsity and workload-specific optimizations. One
such technique is N:M structured sparsity, which enables hardware to
exploit model sparsity without sacrificing memory access efficiency.

\subsubsection{N:M Structured Sparsity
Mechanics}\label{sec-ai-acceleration-nm-sparsity}

While unstructured pruning reduces model size, it rarely translates to
hardware speedup because memory access becomes irregular. Hardware
accelerators solve this with \textbf{N:M Structured
Sparsity}\sidenote{\textbf{Sparsity}: From Latin ``sparsus''
(scattered), describing how zeros are distributed throughout a matrix
rather than densely packed. In ML, sparse matrices have most elements as
zero. The N:M notation indicates that exactly N elements must be
non-zero in every M-element block, combining the computational benefits
of zeros with the predictable access patterns hardware requires. }, a
pattern-based approach that enforces regularity.

\textbf{The 2:4 Pattern}: NVIDIA's Sparse Tensor Cores utilize a 2:4
pattern, requiring that in every contiguous block of 4 values, at least
2 must be zero. This constraint allows the hardware to compress the
matrix by 50\% in memory and metadata.

\textbf{Hardware Execution}: 1. \textbf{Compression}: The hardware
stores only the 2 non-zero values and 2 bits of metadata (indices) for
every 4-element block. 2. \textbf{Compute}: During matrix
multiplication, the Sparse Tensor Core reads the metadata to select the
corresponding activations and performs math only on the non-zero
weights. 3. \textbf{Result}: This effectively doubles the FLOPs/byte
ratio, providing a theoretical \(2\times\) speedup over dense matrix
multiplication with minimal accuracy loss, provided the model is
fine-tuned to respect the 2:4 constraint.

Beyond structured sparsity optimizations, different hardware
architectures implement matrix operations through distinct computational
structures. Systolic arrays represent one such approach that has proven
particularly effective for AI workloads.

\subsubsection{Systolic
Arrays}\label{sec-ai-acceleration-systolic-arrays-6fa8}

While tensor cores package matrix operations into structured
computational units, systolic arrays provide an alternative approach
optimized for continuous data flow and operand reuse. The fundamental
motivation for systolic architectures stems from the energy efficiency
constraints discussed earlier: minimizing the impact of memory access
penalties through architectural design.

\phantomsection\label{callout-perspectiveux2a-1.6}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Energy Economics of AI Hardware}
\phantomsection\label{callout-perspective*-1.6}
\textbf{Why We Reuse Data}: In modern silicon, the energy cost of moving
data dominates everything else. A 32-bit floating-point
multiply-accumulate (MAC) operation consumes approximately \textbf{1 pJ}
(picojoule) of energy. In contrast, reading those same 32 bits from
off-chip DRAM consumes roughly \textbf{100-200 pJ}---a \textbf{100× to
200× energy penalty} for a single data fetch. Systolic arrays are a
direct architectural response to this economic reality. By passing data
directly from one processing element to another without returning to
memory, we ``amortize'' the energy cost of the initial fetch over
hundreds of computations. In a \(128 \times 128\) array, a single weight
value can be reused 128 times, effectively reducing the per-operation
energy overhead by two orders of magnitude.

\end{fbx}

A systolic array arranges processing elements in a grid pattern, where
data flows rhythmically between neighboring units in a synchronized
manner, enabling each operand to participate in multiple computations as
it propagates through the array. This structured movement minimizes
external memory accesses by maximizing local data reuse---a single
weight value can contribute to dozens of operations as it moves through
the processing elements, fundamentally transforming the energy profile
from memory-bound to compute-efficient execution.

The concept of systolic arrays was first introduced by Kung and
Leiserson\sidenote{\textbf{Systolic Array}: Named from the Greek
``systole'' (contraction), referring to the rhythmic pumping of the
heart. H.T. Kung and Charles Leiserson chose this term at CMU in 1979
because data pulses through the processing element grid in waves, like
blood through cardiac chambers. Each element contracts (computes) and
passes data to neighbors in a coordinated rhythm. Google's TPUs revived
systolic designs for neural networks by pairing them with software
stacks optimized for dense linear algebra, achieving high throughput
when workloads map well to this rhythmic dataflow pattern. }, who
formalized their use in parallel computing architectures for efficient
matrix operations (\citeproc{ref-Kung1982}{Kung 1982}). Unlike
general-purpose execution units, systolic arrays exploit spatial and
temporal locality by reusing operands as they propagate through the
grid. Google's TPU exemplifies this architectural approach. In the
TPUv4, a \(128\times128\) systolic array of multiply-accumulate units
processes matrix operations by streaming data through the array in a
pipelined manner. Figure~\ref{fig-systolic-array} captures this dataflow
architecture.

The systolic array architecture achieves computational efficiency
through synchronized data movement across a structured grid of
processing elements. Systolic arrays organize computation around four
fundamental components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Control Unit}: Coordinates timing and data distribution across
  the array, maintaining synchronized operation throughout the
  computational grid
\item
  Data Streams: Input matrices propagate through coordinated pathways
  where matrix A elements traverse horizontally while matrix B elements
  flow vertically through the processing grid
\item
  \textbf{Processing Element Grid}: Individual processing elements
  execute multiply-accumulate operations on streaming data, generating
  partial results that accumulate toward the final computation
\item
  \textbf{Output Collection}: Results aggregate at designated output
  boundaries where accumulated partial sums form complete matrix
  elements
\end{enumerate}

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Matching Architecture to Workload}
\phantomsection\label{callout-perspective*-1.7}
\textbf{The Architects' Dilemma}: Systolic arrays must choose which data
to keep stationary (in registers) to minimize movement. This choice
hard-codes the hardware's preference for certain model types.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1404}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1287}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1696}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5497}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stationary Item}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimized For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Workload}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Weight-Stationary} & Weights (\(W\)) & High Reuse of Weights &
\textbf{CNNs (Conv2D)}: Filters are small and reused across the entire
image. \\
\textbf{Output-Stationary} & Partial Sums (\(C\)) & High Reuse of
Accumulators & \textbf{Large Batch MatMul}: Accumulating results for
many inputs against a large weight matrix. \\
\textbf{Row-Stationary} & Input Rows (\(A\)) & Data Reuse &
\textbf{General MatMul}: Balancing input and weight reuse. \\
\end{longtable}

There is no ``perfect'' accelerator. A chip optimized for
Weight-Stationary flow (like early TPUs) excels at CNNs but might
struggle with the massive, changing weights of LLMs, which push
architectures toward different reuse patterns.

\end{fbx}

The synchronized data flow ensures that matrix element A{[}i,k{]}
encounters corresponding B{[}k,j{]} elements at precise temporal
intervals, executing the multiply-accumulate operations required for
matrix multiplication C{[}i,j{]} = Σ A{[}i,k{]} × B{[}k,j{]}. This
systematic reuse of operands across multiple processing elements
substantially reduces memory bandwidth requirements by eliminating
redundant data fetches from external memory subsystems.

Consider the multiplication of 2×2 matrices A and B within a systolic
array. During the first computational cycle, element A{[}0,0{]}=2
propagates horizontally while B{[}0,0{]}=1 moves vertically, converging
at processing element PE(0,0) to execute the multiplication 2×1=2. In
the subsequent cycle, the same A{[}0,0{]}=2 advances to PE(0,1) where it
encounters B{[}0,1{]}=3, computing 2×3=6. Concurrently, A{[}0,1{]}=4
enters PE(0,0) to engage with the next B matrix element. This
coordinated data movement enables systematic operand reuse across
multiple computational operations, eliminating redundant memory accesses
and exemplifying the fundamental efficiency principle underlying
systolic array architectures.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0aaeb4807d4b322cac341e9f590da85033f8c8a3.pdf}}

}

\caption{\label{fig-systolic-array}\textbf{Systolic Array Dataflow}:
Processing elements within the array execute matrix operations by
streaming data in a pipelined manner, maximizing operand reuse and
minimizing memory access compared to traditional memory-compute
architectures. This spatial and temporal locality enables efficient
parallel computation, as exemplified by the multiply-accumulate units in
Google's tpuv4.}

\end{figure}%

Each processing element in the array performs a multiply-accumulate
operation in every cycle:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Receives an input activation from above
\item
  Receives a weight value from the left
\item
  Multiplies these values and adds to its running sum
\item
  Passes the input activation downward and the weight value rightward to
  neighboring elements
\end{enumerate}

This structured computation model minimizes data movement between global
memory and processing elements, improving both efficiency and
scalability. As systolic arrays operate in a streaming fashion, they are
particularly effective for high-throughput workloads such as deep
learning training and inference.

One common systolic array implementation shows the general structure,
yet systolic architectures vary significantly across different
accelerator designs. Training-focused architectures like Google's TPU
employ large arrays optimized for high computational throughput, while
inference-oriented designs found in edge devices prioritize energy
efficiency with smaller configurations.

The fundamental principle remains consistent: data flows systematically
through processing elements, with inputs moving horizontally and
vertically to compute partial sums in a synchronized fashion. However,
as detailed in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
practical effectiveness is ultimately constrained by memory bandwidth
bottlenecks.

A 128×128 systolic array capable of 16,384 operations per cycle requires
continuous data feed to maintain utilization. Each cycle demands fresh
input activations and weight parameters that must traverse from off-chip
memory through on-chip buffers to the array edges. The TPU's 1,200 GB/s
on-chip bandwidth enables high utilization, but even this substantial
bandwidth becomes limiting when processing large transformer models
where memory requirements exceed on-chip capacity.

Recall from \textbf{?@sec-model-compression} that quantization reduces
model memory footprint by converting FP32 weights to INT8
representations. This optimization directly addresses the memory
bandwidth constraints identified here. Converting 32-bit floating-point
weights to 8-bit integers reduces memory traffic by 4×, transforming
bandwidth-bound operations into compute-bound workloads where systolic
arrays can achieve higher utilization. Similarly, structured pruning
removes entire rows or columns of weight matrices, reducing both the
data volume that must traverse memory hierarchies and the computation
required. These algorithmic optimizations prove valuable precisely
because they target the memory bottleneck that limits accelerator
performance in practice.

\subsubsection{Numerics in AI
Acceleration}\label{sec-ai-acceleration-numerics-ai-acceleration-f7be}

Building on the quantization and mixed-precision techniques established
in \textbf{?@sec-model-compression}, this section examines how AI
accelerators implement hardware support for reduced-precision formats.
The efficiency of AI accelerators is not determined by computational
power alone but also by how effectively the hardware supports different
numerical representations. The choice of numerical format shapes the
balance between accuracy, throughput, and energy consumption,
influencing how different execution units, such as SIMD and SIMT units,
tensor cores, and systolic arrays, are designed and deployed.

\paragraph{Precision
Trade-offs}\label{sec-ai-acceleration-precision-tradeoffs-8fa8}

Numerical precision represents a key design parameter in modern AI
accelerators. While higher precision formats provide mathematical
stability and accuracy, they come with substantial costs in terms of
power consumption, memory bandwidth, and computational throughput.
Hardware architects must balance these factors when designing
accelerator datapaths.

The evolution of AI hardware reflects this co-design between software
optimization and hardware capability. Early GPU architectures supported
only FP32 for deep learning workloads, but as the precision trade-offs
from \textbf{?@sec-model-compression} demonstrated that reduced
precision could maintain model accuracy, hardware vendors responded by
adding native support for FP16, BF16, and integer formats. This hardware
evolution enables software optimizations to translate directly into
performance gains, as reduced-precision operations execute on dedicated
circuits optimized for those specific formats.

The transition from high-precision to lower-precision formats is deeply
integrated into hardware execution models. As detailed in
Section~\ref{sec-ai-acceleration-evolution-simd-simt-architectures-e1fd},
SIMD and SIMT units provide flexible support for multiple precisions.
Tensor cores (Section~\ref{sec-ai-acceleration-tensor-cores-771f})
accelerate computation using reduced-precision arithmetic, while
systolic arrays (Section~\ref{sec-ai-acceleration-systolic-arrays-6fa8})
optimize performance by minimizing memory bandwidth constraints through
low-precision formats that maximize operand reuse.

Despite the advantages of reduced precision, deep learning models cannot
always rely solely on low-bit representations. To address this
challenge, modern AI accelerators implement mixed-precision computing,
where different numerical formats are used at different stages of
execution. These precision choices affect model fairness and
reliability. For example, matrix multiplications may be performed in
FP16 or BF16, while accumulations are maintained in FP32 to prevent
precision loss. Similarly, inference engines leverage INT8 arithmetic
while preserving key activations in higher precision when necessary.

\paragraph{Mixed-Precision
Computing}\label{sec-ai-acceleration-mixedprecision-computing-656f}

Modern AI accelerators increasingly support mixed-precision execution,
allowing different numerical formats to be used at various stages of
computation. Training workloads often leverage FP16 or BF16 for matrix
multiplications, while maintaining FP32 accumulations to preserve
precision. The software implementation of mixed-precision training,
including loss scaling techniques and framework support, is covered in
\textbf{?@sec-ai-training-mixedprecision-training-9218}. Inference
workloads, by contrast, optimize for INT8 or even INT4, achieving high
efficiency while retaining acceptable accuracy.

This shift toward precision diversity is evident in the evolution of AI
hardware. Early architectures such as NVIDIA Volta provided limited
support for lower precision beyond FP16, whereas later architectures,
including Turing and Ampere, expanded the range of supported formats.
Table~\ref{tbl-nvidia-numerics} traces this progression: Ampere GPUs
introduced TF32 as a hybrid between FP32 and FP16, alongside broader
support for BF16, INT8, and INT4.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1727}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3727}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3364}}@{}}
\caption{\textbf{Precision Support Evolution}: GPU architectures
progressively expanded support for lower-precision data types, enabling
performance gains and efficiency improvements in AI workloads. Early
architectures primarily utilized FP32, while later generations
incorporated FP16, BF16, INT8, and INT4 to accelerate both training and
inference tasks.}\label{tbl-nvidia-numerics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported Tensor Core Precisions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported CUDA Core Precisions}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported Tensor Core Precisions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported CUDA Core Precisions}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Volta} & 2017 & FP16 & FP64, FP32, FP16 \\
\textbf{Turing} & 2018 & FP16, INT8 & FP64, FP32, FP16, INT8 \\
\textbf{Ampere} & 2020 & FP64, TF32, bfloat16, FP16, INT8, INT4 & FP64,
FP32, FP16, bfloat16, INT8 \\
\end{longtable}

Newer architectures incorporate a growing diversity of numerical
formats, reflecting the need for greater flexibility across different AI
workloads. This trend suggests that future AI accelerators will continue
expanding support for adaptive precision, optimizing both computational
efficiency and model accuracy.

The precision format used in hardware design has several implications.
By adopting lower-precision formats, the data transferred between
execution units and memory is reduced, leading to decreased memory
bandwidth requirements and storage. Tensor cores and systolic arrays can
process more lower-precision elements in parallel, thereby increasing
the effective throughput in terms of FLOPs. Energy efficiency is also
improved, as integer-based computations (e.g., INT8) require lower power
compared to floating-point arithmetic---a clear advantage for inference
workloads.

As AI models continue to scale in size, accelerator architectures are
evolving to support more efficient numerical formats. Future designs are
expected to incorporate adaptive precision techniques, dynamically
adjusting computation precision based on workload characteristics.
Understanding how these execution units and precision formats integrate
into complete accelerator architectures reveals the full picture of AI
hardware design.

\subsubsection{Architectural
Integration}\label{sec-ai-acceleration-architectural-integration-01b6}

The organization of computational primitives into execution units
determines the efficiency of AI accelerators. While SIMD, tensor cores,
and systolic arrays serve as fundamental building blocks, their
integration into full-chip architectures varies significantly across
different AI processors. The choice of execution units, their numerical
precision support, and their connectivity impact how effectively
hardware can scale for deep learning workloads.

Modern AI processors exhibit a range of design trade-offs based on their
intended applications. Some architectures, such as NVIDIA's A100,
integrate large numbers of tensor cores optimized for FP16-based
training, while Google's TPUv4 prioritizes high-throughput BF16 matrix
multiplications. Inference-focused processors, such as Intel's Sapphire
Rapids, incorporate INT8-optimized tensor cores to maximize efficiency.
The Apple M1, designed for mobile workloads, employs smaller processing
elements optimized for low-power FP16 execution.
Table~\ref{tbl-execution-units} compares these architectural
configurations, revealing how design choices reflect the growing
flexibility in numerical precision and execution unit organization.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1810}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1466}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2155}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2241}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2069}}@{}}
\caption{\textbf{AI Processor Configurations}: Modern AI processors
prioritize different execution unit characteristics to optimize
performance for specific workloads; NVIDIA A100 leverages wide SIMD and
tensor cores for training, Google TPUv4 emphasizes high-throughput BF16
matrix multiplication, and Intel Sapphire Rapids focuses on
INT8-optimized inference, while apple M1 prioritizes low-power FP16
execution on smaller processing elements. These variations in SIMD
width, tensor core size, and processing element count reflect the
growing diversity in AI hardware architectures and their targeted
applications.}\label{tbl-execution-units}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Processor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{SIMD Width}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Tensor Core Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Processing Elements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Workloads}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Processor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{SIMD Width}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Tensor Core Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Processing Elements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Workloads}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NVIDIA A100} & 1024-bit & \(4\times4\times4\) FP16 & 108 SMs &
Training, HPC \\
\textbf{Google TPUv4} & 128-wide & \(128\times128\) BF16 & 2 cores/chip
& Training \\
\textbf{Intel Sapphire} & 512-bit AVX & \(32\times32\) INT8/BF16 & 56
cores & Inference \\
\textbf{Apple M1} & 128-bit NEON & \(16\times16\) FP16 & 8 NPU cores &
Mobile inference \\
\end{longtable}

These configurations optimize for different deep learning workloads
across architectures. Training accelerators prioritize high-throughput
floating-point tensor operations, whereas inference processors focus on
low-precision integer execution for efficiency. Meanwhile, mobile
accelerators balance precision and power efficiency to meet real-time
constraints.

\subsection{Cost-Performance
Analysis}\label{sec-ai-acceleration-costperformance-analysis-e925}

While architectural specifications define computational potential,
practical deployment decisions require understanding cost-performance
trade-offs across different accelerator options. However, raw
computational metrics alone provide an incomplete picture. The
fundamental constraint in modern AI acceleration is not compute capacity
but data movement efficiency.

The energy differential established earlier (where memory access costs
dominate computation) drives the entire specialized hardware revolution.
This disparity helps explain why many accelerators achieve only a
fraction of peak compute on memory-bound workloads, while architectures
that maximize data reuse (e.g., systolic arrays on dense matrix kernels)
can sustain substantially higher utilization under favorable conditions.

Consider an organization choosing between ``more of an older
accelerator'' versus ``fewer of a newer accelerator.'' Peak FLOPS can be
misleading for transformer-style workloads with low arithmetic
intensity, where training is often memory-bandwidth bound rather than
compute-bound. In such cases, bandwidth per dollar and achievable
utilization can matter more than headline compute, so a newer
accelerator with substantially higher bandwidth can deliver materially
better \emph{sustained} performance even if peak FLOPS improves by a
smaller factor.

These dynamics help explain the rapid adoption of newer accelerators
despite higher unit prices. For memory-bound workloads, improvements in
effective bandwidth (and the software stack's ability to use it) can
dominate real-world performance. Cloud deployment further complicates
the analysis, as rental pricing, utilization, and operational overheads
can change the break-even point between purchasing and renting hardware.

Table~\ref{tbl-accelerator-economics} provides representative
cost-performance data for common accelerators. Note that these figures
are approximate and vary by vendor, region, and purchase volume; the key
insight is the trend rather than the absolute numbers.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1709}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1966}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2051}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1966}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2051}}@{}}
\caption{\textbf{Accelerator Cost-Performance Comparison}: Hardware
costs must be evaluated against computational capabilities to determine
optimal deployment strategies. While newer accelerators offer better
price-performance ratios, total cost of ownership includes power
consumption, cooling requirements, and infrastructure costs that
significantly impact operational economics. Prices are approximate list
prices and vary by region and volume; TPU pricing estimated from cloud
rates.}\label{tbl-accelerator-economics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{List Price (USD)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FLOPS (FP16)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Price/Performance}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{List Price (USD)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FLOPS (FP16)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Price/Performance}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NVIDIA V100} & \textasciitilde\$9,000 & 125 TFLOPS & 900 GB/s &
\$72/TFLOP \\
\textbf{NVIDIA A100} & \textasciitilde\$15,000 & 312 TFLOPS & 1,935 GB/s
(PCIe) & \$48/TFLOP \\
\textbf{NVIDIA H100} & \textasciitilde\$25,000-30,000 & 756 TFLOPS
(TF32) & 3,350 GB/s & \textasciitilde\$33/TFLOP \\
\textbf{Google TPUv4} & \textasciitilde\$8,000* & 275 TFLOPS (BF16) &
1,200 GB/s & \textasciitilde\$29/TFLOP \\
\textbf{Intel Gaudi 2} & \textasciitilde\$12,000 & 200 TFLOPS (INT8) &
800 GB/s & \$60/TFLOP \\
\end{longtable}

The table reveals several important patterns. First, price-performance
improves with each generation, but the gains are not uniform across
workload types. Second, memory bandwidth often improves faster than the
price-performance ratio suggests, making newer accelerators
disproportionately valuable for memory-bound workloads. Third, the
``best'' accelerator depends heavily on workload characteristics: a
transformer training workload that is memory-bandwidth bound may benefit
more from H100's 3,350 GB/s bandwidth than from raw FLOPS improvements.

Framework selection significantly impacts these economic decisions.
Detailed hardware-framework optimization strategies are covered in
\textbf{?@sec-ai-frameworks}, while performance evaluation methodologies
are discussed in \textbf{?@sec-benchmarking-ai}.

The preceding sections revealed impressive computational machinery:
vector units achieving 8× parallelism through SIMD execution, matrix
operations processing 256 elements simultaneously, and tensor cores
executing 16×16×16 fused multiply-accumulate blocks in single cycles. An
NVIDIA A100's tensor cores can execute 312 trillion operations per
second, and an H100 pushes this further to nearly 2 petaFLOPS in FP8
precision. At these rates, the pure arithmetic for a ResNet-50 forward
pass could complete in microseconds.

Yet real ResNet-50 inference takes milliseconds, not microseconds. The
gap between theoretical capability and practical performance reveals the
chapter's central tension, first posed in the Purpose section:
computational capability has outpaced our ability to feed data to
processors. Moving data from memory costs 100-1000× more energy than
arithmetic, and memory bandwidth grows at roughly 20\% annually while
compute throughput doubles every two years. This disparity determines
whether those 312 TFLOPS translate to 30 TFLOPS of sustained performance
(10\% utilization) or 250 TFLOPS (80\% utilization). The memory systems
examined next explain why this gap exists and what architectural
innovations address it.

\phantomsection\label{quiz-question-sec-ai-acceleration-ai-compute-primitives-2c99}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.3}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-ai-compute-primitives-2c99}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary role of AI compute primitives in neural network
  execution?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To optimize the execution of core computational patterns in neural
    networks
  \item
    To provide a high-level programming interface for machine learning
    frameworks
  \item
    To replace general-purpose CPUs in all computing tasks
  \item
    To ensure compatibility across different neural network
    architectures
  \end{enumerate}
\item
  Explain how vector operations enhance the efficiency of neural network
  computations in AI accelerators.
\item
  The hardware component that performs non-linear transformations like
  ReLU and sigmoid in a single cycle is known as the \_\_\_\_.
\item
  Order the following computational steps for executing a dense layer in
  a neural network: (1) Apply activation function, (2) Multiply inputs
  by weights, (3) Add bias.
\item
  Which of the following is NOT a characteristic of AI compute
  primitives?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They are frequently used in neural network computations.
  \item
    They offer significant energy efficiency gains.
  \item
    They are designed to replace all general-purpose computing tasks.
  \item
    They remain stable across different neural network architectures.
  \end{enumerate}
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-ai-compute-primitives-2c99]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{AI Memory
Systems}\label{sec-ai-acceleration-ai-memory-systems-0057}

The execution units examined in previous sections (SIMD units, tensor
cores, and systolic arrays) provide impressive computational throughput:
modern accelerators achieve 100 to 1000 TFLOPS for neural network
operations. Yet these theoretical capabilities remain unrealized in
practice when memory subsystems cannot supply data at sufficient rates.
This fundamental constraint, termed the AI memory wall, represents the
dominant bottleneck in real-world accelerator performance.

Unlike conventional workloads, ML models require frequent access to
large volumes of parameters, activations, and intermediate results,
leading to substantial memory bandwidth demands. This challenge
intersects with the data management strategies covered in
\textbf{?@sec-data-engineering-ml}. Modern AI hardware addresses these
challenges through advanced memory hierarchies, efficient data movement
techniques, and compression strategies that promote efficient execution
and improved AI acceleration.

Four perspectives inform memory system design. First, we quantify the
growing disparity between computational throughput and memory bandwidth,
revealing why the AI memory wall represents the dominant performance
constraint in modern accelerators. Second, we explore how memory
hierarchies balance competing demands for speed, capacity, and energy
efficiency through carefully structured tiers from on-chip SRAM to
off-chip DRAM. Third, we analyze communication patterns between host
systems and accelerators, exposing transfer bottlenecks that limit
end-to-end performance. Finally, we examine how different neural network
architectures---multilayer perceptrons, convolutional networks, and
transformers---create distinct memory pressure patterns that inform
hardware design decisions and optimization strategies.

\subsection{Understanding the AI Memory
Wall}\label{sec-ai-acceleration-understanding-ai-memory-wall-3ea9}

The AI memory wall represents the fundamental bottleneck constraining
modern accelerator performance: the growing disparity between
computational throughput and memory bandwidth that prevents accelerators
from achieving their theoretical capabilities. While compute units can
execute millions of operations per second through specialized primitives
like vector operations and matrix multiplications, they depend
critically on memory systems to supply the continuous stream of weights,
activations, and intermediate results these operations require.

\phantomsection\label{callout-definitionux2a-1.8}
\begin{fbx}{callout-definition}{Definition: }{AI Memory Wall}
\phantomsection\label{callout-definition*-1.8}
\textbf{The AI Memory Wall} refers to the widening gap between
\emph{computational throughput} (growing exponentially) and \emph{memory
bandwidth} (growing linearly), creating a bottleneck where accelerators
spend more time \emph{waiting for data} than computing. This physical
constraint forces modern AI architectures to prioritize \emph{data
reuse}, \emph{large on-chip caches}, and \emph{model compression} to
maintain utilization.

\end{fbx}

\subsubsection{Quantifying the Compute-Memory Performance
Gap}\label{sec-ai-acceleration-quantifying-computememory-performance-gap-1526}

The severity of this constraint becomes apparent when examining scaling
trends. Over the past two decades, peak computational capabilities have
grown substantially faster than DRAM bandwidth
(\citeproc{ref-gholami2024ai}{Gholami et al. 2024}). This divergence
creates a widening gap where accelerators possess massive computational
power but cannot access data quickly enough to utilize it.
Representative high-end accelerators can deliver on the order of
(10\^{}3) TFLOPS of peak tensor throughput while providing only a few
TB/s of memory bandwidth (\citeproc{ref-nvidia2022h100}{Choquette
2023}). This implies a ridge point on the order of (10\^{}2) operations
per byte to fully utilize compute, which can exceed the arithmetic
intensity of many practical neural network workloads.

The memory wall manifests through three critical constraints. First, the
energy disparity: accessing DRAM can consume orders of magnitude more
energy than a multiply-accumulate operation
(\citeproc{ref-Horowitz2014}{Horowitz 2014}), which often shifts
bottlenecks from raw compute to power and data movement. Second, the
bandwidth limitation: even TB/s memory systems may not feed large
parallel compute arrays continuously on memory-bound workloads, leaving
compute underutilized. Third, the latency hierarchy: off-chip memory
access can require hundreds of cycles, creating pipeline stalls that
cascade through parallel execution units.

Figure~\ref{fig-compute-memory-imbalance} illustrates this ``AI Memory
Wall,'' which continues to widen, making memory bandwidth rather than
compute the primary constraint in AI acceleration.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-compute-memory-imbalance-1.pdf}}

}

\caption{\label{fig-compute-memory-imbalance}\textbf{AI Memory Wall}:
The growing disparity between compute performance and memory bandwidth
emphasizes the increasing challenge in sustaining peak computational
efficiency due to memory constraints. Over the past 20 years, while
computational capabilities have advanced rapidly, memory bandwidth has
not kept pace, leading to potential bottlenecks in data-intensive
applications.}

\end{figure}%

Beyond performance limitations, memory access imposes a significant
energy cost. Fetching data from off-chip DRAM consumes far more energy
than performing arithmetic operations
(\citeproc{ref-Horowitz2014}{Horowitz 2014}). This inefficiency is
particularly evident in machine learning models, where large parameter
sizes, frequent memory accesses, and non-uniform data movement patterns
exacerbate memory bottlenecks. The energy differential drives
architectural decisions---Google's TPU achieves 30-83\(\times\) better
energy efficiency than contemporary GPUs by minimizing data movement
through systolic arrays and large on-chip memory. These design choices
demonstrate that energy constraints, not computational limits, often
determine practical deployment feasibility.

\subsubsection{Memory Access Patterns in ML
Workloads}\label{sec-ai-acceleration-memory-access-patterns-ml-workloads-a960}

Machine learning workloads place substantial demands on memory systems
due to the large volume of data involved in computation. Unlike
traditional compute-bound applications, where performance is often
dictated by the speed of arithmetic operations, ML workloads are
characterized by high data movement requirements. The efficiency of an
accelerator is not solely determined by its computational throughput but
also by its ability to continuously supply data to processing units
without introducing stalls or delays.

A neural network processes multiple types of data throughout its
execution, each with distinct memory access patterns:

\begin{itemize}
\tightlist
\item
  \textbf{Model parameters (weights and biases)}: Machine learning
  models, particularly those used in large-scale applications such as
  natural language processing and computer vision, often contain
  millions to billions of parameters. Storing and accessing these
  weights efficiently is necessary for maintaining throughput.
\item
  \textbf{Intermediate activations}: During both training and inference,
  each layer produces intermediate results that must be temporarily
  stored and retrieved for subsequent operations. These activations can
  contribute significantly to memory overhead, particularly in deep
  architectures.
\item
  \textbf{Gradients (during training)}: Backpropagation requires storing
  and accessing gradients for every parameter, further increasing the
  volume of data movement between compute units and memory.
\end{itemize}

As models increase in size and complexity, improvements in memory
capacity and bandwidth become essential. Although specialized compute
units accelerate operations like matrix multiplications, their overall
performance depends on the continuous, efficient delivery of data to the
processing elements. In large-scale applications, such as natural
language processing and computer vision, models often incorporate
millions to billions of parameters (\citeproc{ref-Brown2020}{Brown et
al. 2020}). Consequently, achieving high performance necessitates
minimizing delays and stalls caused by inefficient data movement between
memory and compute units (\citeproc{ref-Narayanan2021}{Narayanan et al.
2021}; \citeproc{ref-Huang2019}{Xingyu 2019}).

One way to quantify this challenge is by comparing the data transfer
time with the time required for computations. Specifically, we define
the memory transfer time as \[
T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}},
\] where \(M_{\text{total}}\) is the total data volume and
\(B_{\text{mem}}\) is the available memory bandwidth. In contrast, the
compute time is given by \[
T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}},
\] with the number of floating-point operations (FLOPs) divided by the
peak hardware throughput, \(P_{\text{peak}}\). When
\(T_{\text{mem}} > T_{\text{compute}}\), the system becomes
memory-bound, meaning that the processing elements spend more time
waiting for data than performing computations. This imbalance
demonstrates the need for memory-optimized architectures and efficient
data movement strategies to sustain high performance.

Figure~\ref{fig-memory-wall} quantifies this disparity for specific
models and hardware generations, showing how model parameter counts have
outpaced memory bandwidth improvements. The gap between these
curves---from AlexNet to trillion-parameter models---represents the
engineering challenge that drives accelerator memory system design.

\subsubsection{Irregular Memory
Access}\label{sec-ai-acceleration-irregular-memory-access-c6ec}

Unlike traditional computing workloads, where memory access follows
well-structured and predictable patterns, machine learning models often
exhibit irregular memory access behaviors that make efficient data
retrieval a challenge. These irregularities arise due to the nature of
ML computations, where memory access patterns are influenced by factors
such as batch size, layer type, and sparsity. As a result, standard
caching mechanisms and memory hierarchies often struggle to optimize
performance, leading to increased memory latency and inefficient
bandwidth utilization.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-memory-wall-1.pdf}}

}

\caption{\label{fig-memory-wall}\textbf{AI Memory Wall}: The figure
emphasizes the growing disparity between model sizes and hardware memory
bandwidths, illustrating the challenge in sustaining performance as
models become more complex.}

\end{figure}%

To better understand how ML workloads differ from traditional computing
workloads, it is useful to compare their respective memory access
patterns. Traditional workloads, such as scientific computing,
general-purpose CPU applications, and database processing, typically
exhibit well-defined memory access characteristics that benefit from
standard caching and prefetching techniques. ML workloads, on the other
hand, introduce highly dynamic access patterns
(Table~\ref{tbl-traditional-vs-ml-mem}) that challenge conventional
memory optimization strategies.

One key source of irregularity in ML workloads stems from batch size and
execution order. The way input data is processed in batches directly
affects memory reuse, creating a complex optimization challenge. Small
batch sizes decrease the likelihood of reusing cached activations and
weights, resulting in frequent memory fetches from slower, off-chip
memory. Larger batch sizes can improve reuse and amortize memory access
costs, but simultaneously place higher demands on available memory
bandwidth, potentially creating congestion at different memory hierarchy
levels. This delicate balance requires careful consideration of model
architecture and available hardware resources.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2023}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4277}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3642}}@{}}
\caption{\textbf{Memory Access Characteristics}: Traditional workloads
exhibit predictable, sequential memory access, benefiting from standard
caching, while machine learning workloads introduce irregular and
dynamic patterns due to sparsity and data dependencies that challenge
conventional memory optimization techniques. Understanding these
differences helps designers build memory systems that efficiently
support modern AI
applications.}\label{tbl-traditional-vs-ml-mem}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Computing Workloads}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Workloads}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Computing Workloads}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Workloads}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory Access Pattern} & Regular and predictable (e.g.,
sequential reads, structured patterns) & Irregular and dynamic (e.g.,
sparsity, attention mechanisms) \\
\textbf{Cache Locality} & High temporal and spatial locality & Often low
locality, especially in large models \\
\textbf{Data Reuse} & Structured loops with frequent data reuse & Sparse
and dynamic reuse depending on layer type \\
\textbf{Data Dependencies} & Well-defined dependencies allow efficient
prefetching & Variable dependencies based on network structure \\
\textbf{Workload Example} & Scientific computing (e.g., matrix
factorizations, physics simulations) & Neural networks (e.g., CNNs,
Transformers, sparse models) \\
\textbf{Memory Bottleneck} & DRAM latency, cache misses & Off-chip
bandwidth constraints, memory fragmentation \\
\textbf{Impact on Energy Consumption} & Moderate, driven by FLOP-heavy
execution & High, dominated by data movement costs \\
\end{longtable}

Different neural network layers interact with memory in distinct ways
beyond batch size considerations. Convolutional layers benefit from
spatial locality, as neighboring pixels in an image are processed
together, enabling efficient caching of small weight kernels.
Conversely, fully connected layers require frequent access to large
weight matrices, often leading to more randomized memory access patterns
that poorly align with standard caching policies. Transformers introduce
additional complexity, as attention mechanisms demand accessing large
key-value pairs stored across varied memory locations. The dynamic
nature of sequence length and attention span renders traditional
prefetching strategies ineffective, resulting in unpredictable memory
latencies.

Another significant factor contributing to irregular memory access is
sparsity\sidenote{\textbf{Sparsity in Neural Networks}: The property
that most weights or activations in a neural network are zero or
near-zero, enabling computational and memory optimizations. Natural
sparsity occurs when ReLU activations zero out 50-90\% of values, while
artificial sparsity results from pruning techniques that remove 90-99\%
of weights with minimal accuracy loss. Sparse networks can be
10-100\(\times\) smaller and faster, but require specialized hardware
support (like NVIDIA's 2:4 sparsity in A100) or software optimization to
realize benefits, as standard dense hardware performs zero
multiplications inefficiently. } in neural networks. Many modern ML
models employ techniques such as weight pruning, activation sparsity,
and structured sparsity to reduce computational overhead. However, these
optimizations often lead to non-uniform memory access, as sparse
representations necessitate fetching scattered elements rather than
sequential blocks, making hardware caching less effective. Models that
incorporate dynamic computation paths, such as Mixture of Experts and
Adaptive Computation Time, introduce highly non-deterministic memory
access patterns, where the active neurons or model components can vary
with each inference step. This variability challenges efficient
prefetching and caching strategies.

These irregularities have significant consequences. ML workloads often
experience reduced cache efficiency, as activations and weights may not
be accessed in predictable sequences. This leads to increased reliance
on off-chip memory traffic, which slows down execution and consumes more
energy. Irregular access patterns contribute to memory fragmentation,
where the way data is allocated and retrieved results in inefficient
utilization of available memory resources. The combined effect is that
ML accelerators frequently encounter memory bottlenecks that limit their
ability to fully utilize available compute power.

The irregular access patterns and memory wall constraints examined above
create formidable challenges, but they also reveal optimization
opportunities. Although individual memory accesses may appear
unpredictable, ML workloads exhibit structured reuse patterns at a
higher level: the same weights are applied across batch elements, the
same kernels slide across spatial dimensions, and the same attention
patterns recur across sequence positions. Hardware designers exploit
these regularities through carefully structured memory hierarchies that
maintain frequently accessed data close to compute units, even when the
specific access sequence varies.

This insight motivates the hierarchical memory architectures found in
all modern AI accelerators: rather than treating memory as a monolithic
resource, these systems organize storage into distinct tiers, each
optimized for different access patterns and reuse characteristics.

\subsection{Memory
Hierarchy}\label{sec-ai-acceleration-memory-hierarchy-1839}

Modern AI accelerators implement sophisticated memory hierarchies that
balance speed, capacity, and energy efficiency by exploiting these
structured reuse patterns. Unlike general-purpose computing, where
memory access patterns are often unpredictable, ML workloads exhibit
structured reuse patterns that can be optimized through careful
organization of data across multiple memory levels.

At the highest level, large-capacity but slow storage devices provide
long-term model storage. At the lowest level, high-speed registers and
caches ensure that compute units can access operands with minimal
latency. Between these extremes, intermediate memory levels, such as
scratchpad memory, high-bandwidth memory, and off-chip DRAM, offer
trade-offs between performance and capacity.

Table~\ref{tbl-memory-hierarchy} summarizes the multiple memory levels
employed by modern AI accelerators, each with distinct latency,
bandwidth, and capacity properties that directly influence how neural
network data should be allocated.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2349}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1325}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0964}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0904}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.4277}}@{}}
\caption{\textbf{Memory Hierarchy Trade-Offs}: AI accelerators use a
multi-level memory hierarchy to balance performance and capacity,
optimizing data access for computationally intensive machine learning
tasks. Each level provides distinct latency, bandwidth, and capacity
characteristics that dictate how neural network components (weights,
activations, and intermediate results) should be strategically allocated
to minimize bottlenecks and maximize
throughput.}\label{tbl-memory-hierarchy}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approx. Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Capacity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Use in Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approx. Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Capacity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Use in Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Registers} & \textasciitilde1 cycle & Highest & Few values &
Storing operands for immediate computation \\
\textbf{L1/L2 Cache (SRAM)} & \textasciitilde1-10 ns & High & KBs-MBs &
Caching frequently accessed activations and small weight blocks \\
\textbf{Scratchpad Memory} & \textasciitilde5-20 ns & High & MBs &
Software-managed storage for intermediate computations \\
\textbf{High-Bandwidth Memory (HBM)} & \textasciitilde100 ns & Very High
& GBs & Storing large model parameters and activations for high-speed
access \\
\textbf{Off-Chip DRAM (DDR, GDDR, LPDDR)} & \textasciitilde50-150 ns &
Moderate & GBs-TBs & Storing entire model weights that do not fit
on-chip \\
\textbf{Flash Storage (SSD/NVMe)} & \textasciitilde100 µs - 1 ms & Low &
TBs & Storing pre-trained models and checkpoints for later loading \\
\end{longtable}

\phantomsection\label{callout-perspectiveux2a-1.9}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Speed of Light Limit}
\phantomsection\label{callout-perspective*-1.9}
\textbf{Problem}: Why do we need on-chip SRAM? Why can't we just fetch
everything from HBM?

\textbf{The Physics}: 1. \textbf{Distance}: On a large 700mm² chip,
signals travel \textasciitilde20mm. 2. \textbf{Speed}: Signals in
silicon travel at \(\approx 0.5c\) (half speed of light). 3.
\textbf{Latency}: 20mm takes \(\approx 130 \text{ ps}\). 4.
\textbf{Clock Cycle}: At 2 GHz, a cycle is \(500 \text{ ps}\). 5.
\textbf{DRAM}: Off-chip HBM is centimeters away + protocol overhead =
\textbf{100+ cycles}.

\textbf{The Systems Conclusion}: You cannot fetch data from DRAM in a
single cycle. It is physically impossible. You \emph{must} have local
registers and SRAM (L1) to feed compute units at 2 GHz. The ``Memory
Wall'' is partially a \textbf{Distance Wall}.

\end{fbx}

\subsubsection{On-Chip
Memory}\label{sec-ai-acceleration-onchip-memory-72d1}

Each level of the memory hierarchy serves a distinct role in AI
acceleration, with different trade-offs in speed, capacity, and
accessibility. Registers, located within compute cores, provide the
fastest access but can only store a few operands at a time. These are
best utilized for immediate computations, where the operands needed for
an operation can be loaded and consumed within a few cycles. However,
because register storage is so limited, frequent memory accesses are
required to fetch new operands and store intermediate results.

To reduce the need for constant data movement between registers and
external memory, small but fast caches serve as an intermediary buffer.
These caches store recently accessed activations, weights, and
intermediate values, ensuring that frequently used data remains
available with minimal delay. However, the size of caches is limited,
making them insufficient for storing full feature maps or large weight
tensors in machine learning models. As a result, only the most
frequently used portions of a model's parameters or activations can
reside here at any given time.

For larger working datasets, many AI accelerators include scratchpad
memory, which offers more storage than caches but with a crucial
difference: it allows explicit software control over what data is stored
and when it is evicted. Unlike caches, which rely on hardware-based
eviction policies, scratchpad memory enables machine learning workloads
to retain key values such as activations and filter weights for multiple
layers of computation. This capability is useful in models like
convolutional neural networks, where the same input feature maps and
filter weights are reused across multiple operations. By keeping this
data in scratchpad memory rather than reloading it from external memory,
accelerators can significantly reduce unnecessary memory transfers and
improve overall efficiency (\citeproc{ref-Chen2016}{Chen, Emer, and Sze
2017}).

\subsubsection{Off-Chip
Memory}\label{sec-ai-acceleration-offchip-memory-ecdb}

Beyond on-chip memory, high-bandwidth memory provides rapid access to
larger model parameters and activations that do not fit within caches or
scratchpad buffers. HBM achieves its high performance by stacking
multiple memory dies and using wide memory interfaces, allowing it to
transfer large amounts of data with minimal latency compared to
traditional DRAM. Because of its high bandwidth and lower latency, HBM
is often used to store entire layers of machine learning models that
must be accessed quickly during execution. However, its cost and power
consumption limit its use primarily to high-performance AI accelerators,
making it less common in power-constrained environments such as edge
devices.

When a machine learning model exceeds the capacity of on-chip memory and
HBM, it must rely on off-chip DRAM, such as DDR, GDDR, or LPDDR. While
DRAM offers significantly greater storage capacity, its access latency
is higher, meaning that frequent retrievals from DRAM can introduce
execution bottlenecks. To make effective use of DRAM, models must be
structured so that only the necessary portions of weights and
activations are retrieved at any given time, minimizing the impact of
long memory fetch times.

At the highest level of the hierarchy, flash storage and solid-state
drives (SSDs) store large pre-trained models, datasets, and checkpointed
weights. These storage devices offer large capacities but are too slow
for real-time execution, requiring models to be loaded into faster
memory tiers before computation begins. For instance, in training
scenarios, checkpointed models stored in SSDs must be loaded into DRAM
or HBM before resuming computation, as direct execution from SSDs would
be too slow to maintain efficient accelerator utilization
(\citeproc{ref-Narayanan2021}{Narayanan et al. 2021}).

The memory hierarchy thus balances competing objectives of speed,
capacity, and energy efficiency. However, moving data through multiple
memory levels introduces bottlenecks that limit accelerator performance.
Data transfers between memory levels incur latency costs, particularly
for off-chip accesses. Limited bandwidth restricts data flow between
memory tiers. Memory capacity constraints force constant data movement
as models exceed local storage. These constraints make memory bandwidth
the fundamental determinant of real-world accelerator performance, a
topic we examine next.

\subsection{Memory Bandwidth and Architectural
Trade-offs}\label{sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c}

Building on the memory wall analysis established in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
this section quantifies how specific bandwidth characteristics impact
system performance across different deployment scenarios.

Modern accelerators exhibit distinct bandwidth-capacity trade-offs.
Representative datacenter accelerators can provide memory bandwidth on
the order of a few TB/s, often paired with tens of GB of high-bandwidth
memory, optimizing for flexibility across diverse workloads. Other
designs prioritize energy efficiency and data reuse by combining high
internal bandwidth with smaller on-chip memory. These choices affect
which workloads are easiest to run efficiently: memory-intensive models
benefit from high effective bandwidth and sufficient capacity, while
compute-intensive kernels can perform well when architectures maximize
on-chip reuse.

Different neural network operations achieve varying bandwidth
utilization: transformer attention mechanisms often achieve lower
fractions of peak bandwidth due to irregular access patterns,
convolutional layers can achieve higher fractions through predictable
spatial access patterns, and fully connected layers can approach peak
bandwidth when batch sizes are large enough.

As established earlier, on-chip memory access typically consumes energy
in the single-digit-to-tens of picojoules per access, while external
DRAM can be on the order of hundreds of picojoules per access, an
orders-of-magnitude energy penalty. AI accelerators minimize DRAM access
through three key strategies: weight stationarity (keeping model
parameters in on-chip memory), input stationarity (buffering input
activations locally), and output stationarity (accumulating partial sums
on-chip).

Memory bandwidth scaling follows different trajectories across
accelerator designs:

\begin{itemize}
\tightlist
\item
  \textbf{GPU scaling}: Bandwidth increases with memory channels,
  reaching on the order of 1 TB/s and, in high-end systems, a few TB/s,
  enabling larger model support
\item
  \textbf{TPU scaling}: Bandwidth and utilization are strongly
  influenced by systolic array dataflow and on-chip reuse, often trading
  flexibility for efficiency on dense tensor kernels
\item
  \textbf{Mobile accelerator scaling}: Mobile SoCs can deliver on the
  order of hundreds of GB/s of unified memory bandwidth within a
  few-watt power envelope, requiring careful workload scheduling and
  thermal management
\end{itemize}

HBM provides far higher bandwidth than commodity DDR memory, but at
substantially higher cost and packaging complexity. High-bandwidth
accelerators therefore trade higher memory-system cost for higher
sustained performance on bandwidth-bound workloads. Edge accelerators
often sacrifice bandwidth to meet tight cost and power targets while
maintaining sufficient performance for inference workloads.

These bandwidth characteristics directly influence deployment decisions:
cloud training prioritizes raw bandwidth for maximum model capacity,
edge inference optimizes bandwidth efficiency for energy constraints,
and mobile deployment balances bandwidth with cost limitations. Beyond
the accelerator's internal memory system, however, data must also flow
between the host CPU and the accelerator, introducing another potential
bottleneck.

\subsection{Host-Accelerator
Communication}\label{sec-ai-acceleration-hostaccelerator-communication-bb7a}

Machine learning accelerators, such as GPUs and TPUs, achieve high
computational throughput through parallel execution. However, their
efficiency is often constrained by data movement between the host (CPU)
and accelerator memory. Compared to many traditional workloads that keep
most data within a single memory domain, AI workloads can require
frequent transfers between CPU memory and accelerator memory,
introducing latency, consuming bandwidth, and affecting overall
performance.

Host-accelerator data movement follows a structured sequence. Before
computation begins, data is copied from CPU memory to the accelerator's
memory. The CPU then issues execution instructions, and the accelerator
processes the data in parallel. Once computation completes, the results
are stored in accelerator memory and transferred back to the CPU.
Figure~\ref{fig-host-accelerator-data-movement} details these sequential
steps, each introducing potential inefficiencies that must be managed to
optimize performance.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/355106e848ba557d2f07fd7340ae8109bb745368.pdf}}

}

\caption{\label{fig-host-accelerator-data-movement}\textbf{Host-Accelerator
Data Transfer}: AI workloads require frequent data movement between CPU
memory and accelerators; this figure details the sequential steps of
copying input data, executing computation, and transferring results,
each introducing potential performance bottlenecks. Understanding this
data transfer sequence helps optimize AI system performance and minimize
latency.}

\end{figure}%

The key challenges in host-accelerator data movement include latency,
bandwidth constraints, and synchronization overheads. Optimizing data
transfers through efficient memory management and interconnect
technologies is essential for maximizing accelerator utilization.

\subsubsection{Data Transfer
Patterns}\label{sec-ai-acceleration-data-transfer-patterns-689a}

The efficiency of ML accelerators depends not only on their
computational power but also on the continuous supply of data. Even
high-performance GPUs and TPUs remain underutilized if data transfers
are inefficient. Host and accelerator memory exist as separate domains,
requiring explicit transfers over interconnects such as PCIe, NVLink, or
proprietary links. Ineffective data movement causes execution stalls,
making transfer optimization essential.

\subsubsection{Node-Level Interconnect
Topology}\label{sec-ai-acceleration-node-topology}

To optimize data movement, we must understand the physical topology of
the compute node. A typical AI server is not a flat mesh of connected
devices but a hierarchy of bandwidths that tapers as we move away from
the chip.

\textbf{1. Device-Device Interconnect (NVLink / Infinity Fabric)}:
Modern multi-GPU nodes use specialized high-speed bridges like NVLink to
connect accelerators directly, bypassing the host CPU. *
\textbf{Bandwidth}: 600--900 GB/s (per GPU). * \textbf{Use Case}:
Gradient synchronization (AllReduce)\sidenote{\textbf{AllReduce}: From
MPI (Message Passing Interface) terminology, where ``reduce'' operations
combine values across processes using a function like sum or max, and
``all'' means every process receives the result. MPI standardized these
collective operations in 1994. In distributed training, AllReduce
aggregates gradients from all GPUs (the reduce), then distributes the
averaged result back to all GPUs (the all). Ring AllReduce algorithms
achieve optimal bandwidth utilization by having each process send and
receive simultaneously. } during distributed training. This bandwidth is
critical for scaling; without it, multi-GPU training often scales
poorly.

\textbf{2. Host-Device Interconnect (PCIe)}: The link between the CPU
and the Accelerator. * \textbf{Bandwidth}: 32--64 GB/s (PCIe Gen4/Gen5).
* \textbf{Constraint}: This is the ``Data Loading Bottleneck.'' All
training data must pass through this thin pipe. Even with 8 GPUs
providing 5 TB/s of aggregate compute bandwidth, the system is fed by a
single \textasciitilde64 GB/s PCIe switch.

\textbf{3. Node-Network Interconnect (NIC)}: The link to the outside
world (other nodes). * \textbf{Bandwidth}: 25--50 GB/s (200--400 Gbps
Ethernet/InfiniBand). * \textbf{Impact}: Limits scaling across multiple
nodes.

\textbf{The Bandwidth Taper}:
\[ \text{HBM (2000 GB/s)} \gg \text{NVLink (900 GB/s)} \gg \text{PCIe (64 GB/s)} \gg \text{Network (50 GB/s)} \]

System efficiency depends on keeping data as high up this hierarchy as
possible. Once data drops to PCIe or Network speeds, it encounters a
30-100\(\times\) slowdown.

This structured sequence begins with step (1), where data is copied from
CPU memory to accelerator memory, as GPUs cannot directly access host
memory at high speeds. A direct memory access
(DMA)\sidenote{\textbf{Direct Memory Access (DMA)}: Hardware mechanism
that enables devices to transfer data to and from memory without CPU
intervention. DMA engines free the CPU to perform other tasks while data
moves between system memory and accelerators. In representative systems,
host-accelerator links can provide bandwidth on the order of tens of
GB/s (PCIe-class), while some proprietary intra-node interconnects can
provide bandwidth on the order of hundreds of GB/s. This asynchronous
capability is crucial for AI workloads where data movement can overlap
with computation, improving overall system utilization. } engine
typically handles this transfer without consuming CPU cycles. In step
(2), the CPU issues execution commands via APIs like CUDA, ROCm, or
OpenCL. Step (3) involves parallel execution on the accelerator, where
stalls can occur if data is not available when needed. Finally, in step
(4), computed results are copied back to CPU memory for further
processing.

Latency and bandwidth limitations significantly impact AI workloads.
PCIe-class host interconnects are typically much slower than an
accelerator's on-package high-bandwidth memory, so large transfers can
become bottlenecks, particularly in deep learning tasks. Additionally,
synchronization overheads arise when computation must wait for data
transfers to complete. Efficient scheduling and overlapping transfers
with execution are necessary to mitigate these inefficiencies.

\subsubsection{Data Transfer
Mechanisms}\label{sec-ai-acceleration-data-transfer-mechanisms-4109}

The movement of data between the host (CPU) and the accelerator (GPU,
TPU, or other AI hardware) depends on the interconnect technology that
links the two processing units. The choice of interconnect determines
the bandwidth available for transfers, the latency of communication, and
the overall efficiency of host-accelerator execution. The most commonly
used transfer mechanisms include PCIe (Peripheral Component Interconnect
Express), NVLink, Direct Memory Access, and Unified Memory
Architectures. Each of these plays an important role in optimizing the
four-step data movement process described earlier.

\paragraph{PCIe
Interface}\label{sec-ai-acceleration-pcie-interface-c5b4}

Most accelerators communicate with the CPU via PCIe, the
industry-standard interconnect for data movement. PCIe provides
bandwidth on the order of tens of GB/s, which is still significantly
lower than HBM bandwidth within accelerators, making host transfers a
potential bottleneck for large AI workloads.

PCIe also introduces latency overheads due to its packet-based
communication and memory-mapped I/O model. Frequent small transfers are
inefficient, so batching data movement reduces overhead. Computation
commands, issued over PCIe, further contribute to latency, requiring
careful optimization of execution scheduling.

\paragraph{NVLink
Interface}\label{sec-ai-acceleration-nvlink-interface-312b}

To address the bandwidth limitations of PCIe, NVIDIA developed NVLink, a
proprietary high-speed interconnect that provides significantly higher
bandwidth between GPUs and, in some configurations, between the CPU and
GPU. Unlike PCIe, which operates as a shared bus, NVLink enables direct
point-to-point communication between connected devices, reducing
contention and improving efficiency for AI workloads.

For host-accelerator transfers, NVLink can be used in step (1) to
transfer input data from main memory to GPU memory at speeds far
exceeding PCIe in supported configurations. This can significantly
reduce the data movement bottleneck, allowing accelerators to access
input data with lower latency. In multi-GPU configurations, NVLink also
accelerates peer-to-peer transfers, allowing accelerators to exchange
data without routing through main memory, thereby optimizing step (3) of
the computation process.

Although NVLink offers substantial performance benefits, it is not
universally available. Unlike PCIe, which is an industry standard across
all accelerators, NVLink is specific to NVIDIA hardware, limiting its
applicability to systems designed with NVLink-enabled GPUs.

\paragraph{DMA for Data
Transfers}\label{sec-ai-acceleration-dma-data-transfers-a1a7}

In conventional memory transfers, the CPU issues load/store
instructions, consuming processing cycles. DMA offloads this task,
enabling asynchronous data movement without CPU intervention.

During data transfers, the CPU initiates a DMA request, allowing data to
be copied to accelerator memory in the background. Similarly, result
transfers back to main memory occur without blocking execution, enabling
AI workloads to overlap computation with data movement for improved
accelerator utilization.

\paragraph{Unified
Memory}\label{sec-ai-acceleration-unified-memory-b18f}

While PCIe, NVLink, and DMA optimize explicit memory transfers, some AI
workloads require a more flexible memory model that eliminates the need
for manual data copying. Unified Memory provides an abstraction that
allows both the host and accelerator to access a single, shared memory
space, automatically handling data movement when needed.

With Unified Memory, data does not need to be explicitly copied between
CPU and GPU memory before execution. Instead, when a computation
requires a memory region that is currently located in host memory, the
system automatically migrates it to the accelerator, handling step (1)
transparently. Similarly, when computed results are accessed by the CPU,
step (4) occurs automatically, eliminating the need for manual memory
management.

Although Unified Memory simplifies programming, it introduces
performance trade-offs. Since memory migrations occur on demand, they
can lead to unpredictable latencies, particularly if large datasets need
to be transferred frequently. Additionally, since Unified Memory is
implemented through page migration techniques, small memory accesses can
trigger excessive data movement, further reducing efficiency.

For AI workloads that require fine-grained memory control, explicit data
transfers using PCIe, NVLink, and DMA often provide better performance.
However, for applications where ease of development is more important
than absolute speed, Unified Memory offers a convenient alternative.

\subsubsection{Data Transfer
Overheads}\label{sec-ai-acceleration-data-transfer-overheads-fbc9}

Host-accelerator data movement introduces overheads that impact AI
workload execution. Unlike on-chip memory accesses, which occur at
nanosecond latencies, host-accelerator transfers traverse system
interconnects, adding latency, bandwidth constraints, and
synchronization delays.

Interconnect latency affects transfer speed, with PCIe, the standard
host-accelerator link, incurring significant overhead due to
packet-based transactions and memory-mapped I/O. This makes frequent
small transfers inefficient. Faster alternatives like NVLink reduce
latency and improve bandwidth but are limited to specific hardware
ecosystems.

Synchronization delays further contribute to inefficiencies. Synchronous
transfers block execution until data movement completes, ensuring data
consistency but introducing idle time. Asynchronous transfers allow
computation and data movement to overlap, reducing stalls but requiring
careful coordination to avoid execution mismatches.

These factors (interconnect latency, bandwidth limitations, and
synchronization overheads) determine AI workload efficiency.
Understanding these transfer mechanics is essential for improving
performance.

The transfer mechanisms examined thus far apply uniformly across all
neural network types, but different architectures impose dramatically
different memory demands. A convolutional layer processing images
exhibits regular spatial locality, while a transformer's attention
mechanism requires accessing distant tokens across long sequences. These
architectural differences create distinct memory pressure patterns that
directly influence accelerator design and optimization strategies.

\subsection{Model Memory
Pressure}\label{sec-ai-acceleration-model-memory-pressure-f95e}

Machine learning models impose varying memory access patterns that
significantly influence accelerator performance. The way data is
transferred between the host and accelerator, how frequently memory is
accessed, and the efficiency of caching mechanisms all determine overall
execution efficiency. While multilayer perceptrons (MLPs), convolutional
neural networks (CNNs), and transformer networks each require large
parameter sets, their distinct memory demands necessitate tailored
optimization strategies for accelerators. Understanding these
differences provides insight into why different hardware architectures
exhibit varying levels of efficiency across workloads.

To ground this analysis, we return to the Lighthouse Examples introduced
in \textbf{?@sec-introduction}: \textbf{ResNet-50} represents CNN
workloads with high spatial reuse, \textbf{GPT-2/Llama} exemplifies
transformer memory pressure, \textbf{MobileNet} demonstrates
efficiency-optimized architectures with depthwise convolutions, and
\textbf{DLRM} illustrates sparse embedding lookups that stress memory
systems differently than dense operations. These examples will recur
throughout the remainder of this chapter as we analyze how memory
characteristics translate to hardware utilization.

\subsubsection{Multilayer
Perceptrons}\label{sec-ai-acceleration-multilayer-perceptrons-0bbc}

MLPs, also referred to as fully connected networks, are among the
simplest neural architectures. Each layer consists of a dense matrix
multiplication, requiring every neuron to interact with all neurons in
the preceding layer. This results in high memory bandwidth demands,
particularly for weights, as every input activation contributes to a
large set of computations.

From a memory perspective, MLPs rely on large, dense weight matrices
that frequently exceed on-chip memory capacity, necessitating off-chip
memory accesses. Since accelerators cannot directly access host memory
at high speed, data transfers must be explicitly managed via
interconnects such as PCIe or NVLink. These transfers introduce latency
and consume bandwidth, affecting execution efficiency.

Despite their bandwidth-heavy nature, MLPs exhibit regular and
predictable memory access patterns, making them amenable to
optimizations such as prefetching and streaming memory accesses.
Dedicated AI accelerators mitigate transfer overhead by staging weight
matrices in fast SRAM caches and overlapping data movement with
computation through direct memory access engines, reducing execution
stalls. These optimizations allow accelerators to sustain high
throughput even when handling large parameter sets
(\citeproc{ref-Chen2016}{Chen, Emer, and Sze 2017}).

\subsubsection{Convolutional Neural
Networks}\label{sec-ai-acceleration-convolutional-neural-networks-3085}

Convolutional Neural Networks (CNNs) are widely used in image processing
and computer vision tasks. Unlike MLPs, which require dense matrix
multiplications, CNNs process input feature maps using small filter
kernels that slide across the image. This localized computation
structure results in high spatial data reuse, where the same input
pixels contribute to multiple convolutions.

CNN accelerators benefit from on-chip memory optimizations, as
convolution filters exhibit extensive reuse, allowing weights to be
stored in fast local SRAM instead of frequently accessing off-chip
memory. However, activation maps require careful management due to their
size. Since accessing main memory over interconnects like PCIe
introduces latency and bandwidth bottlenecks, CNN accelerators employ
tiling techniques to divide feature maps into smaller regions that fit
within on-chip buffers. This minimizes costly external memory transfers,
improving overall efficiency (\citeproc{ref-Chen2016}{Chen, Emer, and
Sze 2017}).

While CNN workloads are more memory-efficient than MLPs, managing
intermediate activations remains a challenge. Accelerators use
hierarchical caching strategies and DMA engines to optimize memory
movement, ensuring that computations are not stalled by inefficient
host-accelerator data transfers. These memory optimizations help CNN
accelerators maintain high throughput by reducing reliance on off-chip
memory bandwidth (\citeproc{ref-Chen2016}{Chen, Emer, and Sze 2017}).

\subsubsection{Transformer
Networks}\label{sec-ai-acceleration-transformer-networks-638c}

The transformer architectures introduced in
\textbf{?@sec-dnn-architectures} have become the dominant architecture
for natural language processing and are increasingly used in other
domains such as vision and speech recognition. Unlike CNNs, which rely
on local computations, transformers perform global
attention\sidenote{\textbf{Attention}: Borrowed from cognitive
psychology, where attention describes the brain's selective focus on
relevant stimuli while filtering distractions. Bahdanau, Cho, and Bengio
introduced ``attention'' to neural networks in 2014, using the term
because the mechanism lets models ``attend to'' relevant parts of input
sequences. The analogy is apt: just as humans selectively focus on
important information, attention mechanisms learn to weight different
input positions based on relevance to the current computation. This
leads to irregular and bandwidth-intensive memory access patterns, as
large key-value matrices must be fetched and updated frequently. }
mechanisms, where each token in an input sequence can interact with all
other tokens.

These models are particularly challenging for accelerators due to their
massive parameter sizes, which often exceed on-chip memory capacity. As
a result, frequent memory transfers between host and accelerator
introduce substantial latency overheads, particularly when relying on
interconnects such as PCIe. Unified Memory architectures can mitigate
some of these issues by dynamically handling data movement, but they
introduce additional latency due to unpredictable on-demand memory
migrations. Because transformers are memory-bound rather than
compute-bound, accelerators optimized for them rely on high-bandwidth
memory, tensor tiling, and memory partitioning to sustain performance
(\citeproc{ref-Brown2020}{Brown et al. 2020}).

Additionally, attention caching mechanisms and specialized tensor
layouts reduce redundant memory fetches, improving execution efficiency.
Given the bandwidth limitations of traditional interconnects,
NVLink-enabled architectures offer significant advantages for
large-scale transformer training, as they provide higher throughput and
lower latency compared to PCIe. DMA-based asynchronous memory transfers
enable overlapping computation with data movement, reducing execution
stalls (\citeproc{ref-Narayanan2021}{Narayanan et al. 2021}).

\subsection{ML Accelerators
Implications}\label{sec-ai-acceleration-ml-accelerators-implications-c962}

The diverse memory requirements of MLPs, CNNs, and Transformers
highlight the need to tailor memory architectures to specific workloads.
Table~\ref{tbl-model-mem-compare} reveals how memory access patterns
vary dramatically across model types.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1406}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1406}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1797}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2578}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2578}}@{}}
\caption{\textbf{ML Model Memory Access}: Different machine learning
models exhibit distinct memory access patterns and bottlenecks due to
variations in weight size, activation reuse, and data sparsity; these
characteristics significantly impact hardware accelerator design and
performance optimization. Transformers demand high bandwidth and
capacity due to their massive, sparsely accessed weights, while cnns
benefit from spatial locality and high activation reuse, reducing memory
pressure.}\label{tbl-model-mem-compare}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Weight Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Activation Reuse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Access Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Weight Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Activation Reuse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Access Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MLP (Dense)} & Large, dense & Low & Regular, sequential
(streamed) & Bandwidth (off-chip) \\
\textbf{CNN} & Small, reused & High & Spatial locality & Feature map
movement \\
\textbf{Transformer} & Massive, sparse & Low & Irregular, high-bandwidth
& Memory capacity + Interconnect \\
\end{longtable}

Each model type presents unique challenges that directly impact
accelerator design. MLPs benefit from fast streaming access to dense
weight matrices, making memory bandwidth a critical factor in
performance, especially when transferring large weights from host memory
to accelerator memory. CNNs, with their high activation reuse and
structured memory access patterns, can leverage on-chip caching and
tiling strategies to minimize off-chip memory transfers. Transformers,
however, impose significant demands on both bandwidth and capacity, as
attention mechanisms require frequent access to large key-value
matrices, leading to high interconnect traffic and increased memory
pressure.

To address these challenges, modern AI accelerators incorporate
multi-tier memory hierarchies that balance speed, capacity, and energy
efficiency. On-chip SRAM caches and scratchpad memories store frequently
accessed data, while high-bandwidth external memory provides scalability
for large models. Efficient interconnects, such as NVLink, help
alleviate host-accelerator transfer bottlenecks, particularly in
transformer workloads where memory movement constraints can dominate
execution time.

As ML workloads continue to grow in complexity, memory efficiency
becomes as critical as raw compute power. The analysis reveals how
memory systems dominate accelerator performance: DRAM access has 100x or
higher energy cost than on-chip arithmetic, carefully structured memory
hierarchies can improve effective bandwidth substantially, and different
neural network architectures create distinct memory pressure patterns.
These constraints---bandwidth limitations, energy costs, and
communication overheads---determine whether theoretical computational
capabilities translate into real-world performance. But how do we know
if a specific workload is limited by compute or memory on a given
accelerator? The memory wall analysis establishes \emph{why} memory
matters, but practitioners need a quantitative framework to predict
\emph{which} operations will bottleneck on a specific hardware
configuration. The Roofline Model provides this analytical lens,
transforming hardware selection from intuition into engineering.

\phantomsection\label{quiz-question-sec-ai-acceleration-ai-memory-systems-0057}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.4}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-ai-memory-systems-0057}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary constraint that defines the AI memory wall?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    The limited number of compute units available in accelerators.
  \item
    The cost of high-bandwidth memory compared to traditional DRAM.
  \item
    The energy consumption of arithmetic operations compared to memory
    access.
  \item
    The disparity between computational throughput and memory bandwidth.
  \end{enumerate}
\item
  Explain how memory hierarchies in AI accelerators balance speed,
  capacity, and energy efficiency.
\item
  The energy penalty for accessing \_\_\_\_ is significantly higher than
  for computation, influencing AI accelerator design.
\item
  Which neural network architecture is most likely to be constrained by
  memory capacity and interconnect bandwidth?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Transformer Networks
  \item
    Convolutional Neural Networks (CNNs)
  \item
    Multilayer Perceptrons (MLPs)
  \item
    Recurrent Neural Networks (RNNs)
  \end{enumerate}
\item
  In a system design scenario, how might you address the memory
  bottlenecks imposed by transformer networks?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-ai-memory-systems-0057]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Measuring Hardware
Efficiency}\label{sec-ai-acceleration-measuring-efficiency}

Hardware acceleration is valuable only if it translates into measurable
performance gains. While theoretical peak FLOPS figures are often
marketed, achievable performance depends on the complex interaction
between compute capability and memory bandwidth. To evaluate hardware
efficiency realistically, we must use rigorous models that capture these
constraints.

This section introduces the Roofline Model, the fundamental tool for
visualizing and analyzing the performance limits of a system. By
plotting arithmetic intensity against performance, we can determine
whether a workload is compute-bound or memory-bound, guiding
optimization efforts effectively.

\subsection{The Roofline
Model}\label{sec-ai-acceleration-roofline-model}

The roofline model\sidenote{\textbf{Roofline Model}: Named for its
visual appearance when plotted: a flat horizontal line (the compute
ceiling) meets a sloped line (the memory ceiling) forming a shape
resembling a building roofline. Samuel Williams, Andrew Waterman, and
David Patterson introduced this visualization at UC Berkeley in 2009.
The metaphor captures how workloads hit different ``ceilings'' depending
on their arithmetic intensity, making performance bounds intuitive at a
glance. } provides the standard framework for understanding whether
workloads are compute-bound or memory-bound, directly connecting the
memory wall discussion to practical performance analysis. This model
enables quantitative reasoning about accelerator utilization and guides
optimization decisions.

\textbf{Model Definition}: Performance is bounded by two ceilings:
\[\text{Attainable Performance} = \min(\text{Peak Compute}, \text{Peak Bandwidth} \times \text{Arithmetic Intensity})\]

\phantomsection\label{callout-definitionux2a-1.10}
\begin{fbx}{callout-definition}{Definition: }{Arithmetic Intensity}
\phantomsection\label{callout-definition*-1.10}
\textbf{Arithmetic Intensity} refers to the ratio of
\emph{floating-point operations (FLOPs)} performed by an algorithm to
the number of \emph{bytes of data} transferred from memory, measured in
FLOPs/byte. This metric determines whether a workload is bottlenecked by
the processor's speed (compute-bound) or the memory system's bandwidth
(memory-bound).

\end{fbx}

\textbf{Arithmetic Intensity (AI)} measures operations per byte of
memory traffic:
\[\text{AI} = \frac{\text{FLOPs}}{\text{Bytes Transferred}}\]

The roofline visualization shows performance (TFLOPS) on the vertical
axis and arithmetic intensity (FLOPS/byte) on the horizontal axis. At
low arithmetic intensity, performance increases linearly with intensity
(memory-bound region). Above a threshold called the ridge point,
performance saturates at peak compute (compute-bound region).

\textbf{Hardware Ridge Points}: The ridge point determines the
arithmetic intensity threshold where the transition from memory-bound to
compute-bound occurs. Table~\ref{tbl-ridge-points} quantifies how
different accelerators exhibit distinct characteristics based on their
compute-to-bandwidth ratios:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1957}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2899}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3116}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1884}}@{}}
\caption{\textbf{Hardware Ridge Points (Representative,
order-of-magnitude)}: Different accelerators have different ridge points
determined by their compute-to-bandwidth ratios. Higher ridge points
require more operations per byte to achieve peak
utilization.}\label{tbl-ridge-points}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FP16}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Ridge Point}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FP16}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Ridge Point}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{GPU (2017-era)} & (\sim 10\^{}2) TFLOPS & (\sim 10\^{}3) GB/s &
(\sim 10\^{}2) FLOP/byte \\
\textbf{GPU (2020-era)} & (\sim 10\^{}2) TFLOPS & (\sim 10\^{}3) GB/s to
(\sim 10\^{}0) TB/s & (\sim 10\^{}2) FLOP/byte \\
\textbf{GPU (2023-era)} & (\sim 10\^{}3) TFLOPS & a few TB/s &
(\sim 10\^{}2) FLOP/byte \\
\textbf{TPU-class (2023-era)} & (\sim 10\^{}2) to (\sim 10\^{}3) TFLOPS
& (\sim 1) TB/s & (\sim 10\^{}2) FLOP/byte \\
\end{longtable}

\phantomsection\label{callout-perspectiveux2a-1.11}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: Calculating the Ridge Point}
\phantomsection\label{callout-perspective*-1.11}
\textbf{Problem}: You are using an \textbf{NVIDIA H100} GPU. You want to
know if your attention layer is compute-bound or bandwidth-bound.

\textbf{The Math}: 1. \textbf{Peak Compute}: 990 TFLOPS (FP16). 2.
\textbf{Peak Bandwidth}: 3.4 TB/s. 3. \textbf{Ridge Point}:
\(990 \text{ TFLOPS} / 3.4 \text{ TB/s} \approx \mathbf{291 \text{ Ops/Byte}}\).

\textbf{The Systems Conclusion}: To achieve 100\% utilization of an
H100, your code must perform \textbf{291 mathematical operations for
every 1 byte} it pulls from memory. - A standard \textbf{ReLU} performs
1 operation for every 8 bytes (0.125 Ops/Byte). It is \textbf{2,300x
below the roofline}. - A large \textbf{Dense MatMul} (batch=128) might
reach 300 Ops/Byte. It is \textbf{Compute-Bound}.

\textbf{The Engineering Moral}: Most of the time, your ``Ferrari'' (the
H100) is stuck in traffic because you aren't feeding it enough ``Fuel''
(Data Bandwidth). This is why \textbf{Kernel Fusion} is the most
important optimization in \textbf{?@sec-model-compression}.

\end{fbx}

Table~\ref{tbl-roofline-operations} maps common neural network
operations to the Roofline model:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2400}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2700}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2100}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2600}}@{}}
\caption{\textbf{Operations on the Roofline}: Neural network layers span
a wide range of arithmetic intensities. By mapping these operations to
our \textbf{Lighthouse Examples}, we can see why ResNet-50 is
compute-bound (high AI) while MobileNet and DLRM are memory-bound (low
AI).}\label{tbl-roofline-operations}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arithmetic Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Classification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Lighthouse Example}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arithmetic Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Classification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Lighthouse Example}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Conv2D (Dense)} & 50-200 FLOP/byte & Compute-bound &
\textbf{ResNet-50} \\
\textbf{Dense MatMul} & 64-256 FLOP/byte & Compute-bound & \textbf{GPT-2
(Projections)} \\
\textbf{Depthwise Conv} & 10-20 FLOP/byte & Memory-bound &
\textbf{MobileNet} \\
\textbf{Attention Softmax} & 2-5 FLOP/byte & Memory-bound &
\textbf{GPT-2 (Generation)} \\
\textbf{LayerNorm} & 5-10 FLOP/byte & Memory-bound & \textbf{GPT-2 /
Llama} \\
\textbf{Embedding lookup} & \textless1 FLOP/byte & Memory-bound &
\textbf{DLRM} \\
\end{longtable}

\phantomsection\label{callout-notebook-1.1}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.1: }{Worked Example: Transformer Layer Analysis}
\phantomsection\label{callout-notebook-1.1}
For a transformer with hidden\_dim=768, batch=32, seq=512:

\emph{Attention QKV Projection}:

\begin{itemize}
\tightlist
\item
  FLOPs: \(3 \times 32 \times 512 \times 768 \times 768 = 29\) billion
  FLOPs
\item
  Bytes: (input + weights + output)
  \(= (32 \times 512 \times 768 + 3 \times 768 \times 768 + 32 \times 512 \times 768 \times 3) \times 2 = 127\)
  MB
\item
  AI \(= 29B / 127M = 228\) FLOP/byte, which is \textbf{compute-bound on
  A100} (above 156 threshold)
\end{itemize}

\emph{Softmax}:

\begin{itemize}
\tightlist
\item
  FLOPs: \(32 \times 12 \times 512 \times 512 \times 3 \approx 300M\)
  FLOPs (exp, sum, div)
\item
  Bytes: \(32 \times 12 \times 512 \times 512 \times 2 \times 2 = 402\)
  MB
\item
  AI \(= 300M / 402M = 0.75\) FLOP/byte, which is \textbf{memory-bound}
\end{itemize}

This analysis explains why FlashAttention focuses on reducing memory
traffic in attention rather than reducing FLOPs.

\end{fbx}

\textbf{Optimization Implications}:

For \textbf{memory-bound operations}: reduce data movement through
operator fusion, use reduced precision (FP16, INT8), and increase
arithmetic intensity through algorithmic changes like FlashAttention.

For \textbf{compute-bound operations}: maximize hardware utilization
through batching and parallelism, use Tensor Cores and specialized
compute units, and optimize compute efficiency through tiling and
scheduling.

\textbf{Calculating Memory Bandwidth Bounds}: The roofline model's
memory-bound region is determined by the peak memory bandwidth. For an
operation to achieve performance \(P\) (TFLOPS) in the memory-bound
regime, the required bandwidth is:
\[\text{Required Bandwidth} = \frac{P}{\text{AI}} \text{ bytes/sec}\]

When Required Bandwidth exceeds Peak Bandwidth, performance is capped
at: \[P_{\text{attainable}} = \text{Peak Bandwidth} \times \text{AI}\]

\phantomsection\label{callout-notebook-1.2}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.2: }{Worked Example: Convolutional Layer Analysis}
\phantomsection\label{callout-notebook-1.2}
Consider a Conv2D layer with input shape (batch=32, channels=128,
height=56, width=56), output channels=256, kernel size 3×3 on an A100
GPU:

\emph{Computational Requirements}:

\begin{itemize}
\tightlist
\item
  Output size: \(32 \times 256 \times 56 \times 56 = 25.7M\) elements
\item
  FLOPs per output: \(128 \times 3 \times 3 \times 2 = 2,304\)
  (multiply-add)
\item
  Total FLOPs: \(25.7M \times 2,304 = 59.2\) billion FLOPs
\end{itemize}

\emph{Memory Traffic Analysis}:

\begin{itemize}
\tightlist
\item
  Input: \(32 \times 128 \times 56 \times 56 \times 2 = 25.7\) MB (FP16)
\item
  Weights: \(256 \times 128 \times 3 \times 3 \times 2 = 1.2\) MB (FP16)
\item
  Output: \(32 \times 256 \times 56 \times 56 \times 2 = 51.4\) MB
  (FP16)
\item
  Total: \(78.3\) MB
\end{itemize}

\emph{Arithmetic Intensity}:
\[\text{AI} = \frac{59.2 \text{ GFLOPs}}{78.3 \text{ MB}} = 756 \text{ FLOP/byte}\]

This is \textbf{well above} A100's ridge point of 156 FLOP/byte, making
this operation \textbf{compute-bound}. The layer will achieve near-peak
performance of \textasciitilde312 TFLOPS (FP16 with Tensor Cores).

\end{fbx}

The convolutional layer's high arithmetic intensity arises from its
weight reuse pattern: the same 3×3 kernel is applied across all spatial
locations, amortizing the cost of loading weights across millions of
output computations. This is the architectural pattern that makes CNNs
so efficient on modern accelerators.

However, not all layers in a neural network exhibit this favorable
profile. The fully connected (dense) layers that typically appear at the
end of classification networks, or as the projection layers in
transformers, have fundamentally different arithmetic intensity
characteristics. Understanding this contrast is essential for predicting
where bottlenecks will occur in end-to-end model execution.

\phantomsection\label{callout-notebook-1.3}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.3: }{Worked Example: Dense Layer Analysis}
\phantomsection\label{callout-notebook-1.3}
Consider a fully connected layer: input (batch=32, features=2048) →
output (batch=32, features=2048) on the same A100:

\emph{Computational Requirements}:

\begin{itemize}
\tightlist
\item
  Matrix multiply: \((32 \times 2048) \times (2048 \times 2048)\)
\item
  Total FLOPs: \(2 \times 32 \times 2048 \times 2048 = 274\) million
  FLOPs
\end{itemize}

\emph{Memory Traffic Analysis}:

\begin{itemize}
\tightlist
\item
  Input: \(32 \times 2048 \times 2 = 131\) KB (FP16)
\item
  Weights: \(2048 \times 2048 \times 2 = 8.4\) MB (FP16)
\item
  Output: \(32 \times 2048 \times 2 = 131\) KB (FP16)
\item
  Total: \(8.7\) MB
\end{itemize}

\emph{Arithmetic Intensity}:
\[\text{AI} = \frac{274 \text{ MFLOPs}}{8.7 \text{ MB}} = 31.5 \text{ FLOP/byte}\]

This is \textbf{below} A100's ridge point of 156 FLOP/byte, making this
operation \textbf{memory-bound}. Attainable performance:
\[P_{\text{attainable}} = 2000 \text{ GB/s} \times 31.5 \text{ FLOP/byte} = 63 \text{ TFLOPS}\]

This is only 20\% of peak compute capability, demonstrating the memory
wall effect for small batch sizes.

\end{fbx}

The dense layer's lower arithmetic intensity stems from limited weight
reuse: each weight element is used only once per batch element, whereas
convolutional weights are reused across spatial dimensions. This
difference explains why transformer inference (dominated by dense
projections) is typically memory-bound while CNN inference can be
compute-bound.

The situation becomes even more extreme for element-wise operations like
normalization layers. These operations perform very little computation
relative to the data they touch---each element is loaded, transformed by
a simple formula, and written back. There is essentially no opportunity
for data reuse.

\phantomsection\label{callout-notebook-1.4}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.4: }{Worked Example: LayerNorm Analysis}
\phantomsection\label{callout-notebook-1.4}
LayerNorm with input shape (batch=32, seq=512, hidden=768):

\emph{Computational Requirements}:

\begin{itemize}
\tightlist
\item
  Elements: \(32 \times 512 \times 768 = 12.6M\)
\item
  Operations per element: mean (1 ADD), variance (2 ADD, 1 MUL),
  normalize (1 ADD, 1 MUL, 1 DIV) ≈ 6 FLOPs
\item
  Total FLOPs: \(12.6M \times 6 = 75.5M\) FLOPs
\end{itemize}

\emph{Memory Traffic}:

\begin{itemize}
\tightlist
\item
  Input: \(12.6M \times 2 = 25.2\) MB
\item
  Parameters (scale, bias): \(768 \times 2 \times 2 = 3\) KB
  (negligible)
\item
  Output: \(12.6M \times 2 = 25.2\) MB
\item
  Total: \(50.4\) MB
\end{itemize}

\emph{Arithmetic Intensity}:
\[\text{AI} = \frac{75.5 \text{ MFLOPs}}{50.4 \text{ MB}} = 1.5 \text{ FLOP/byte}\]

This is \textbf{severely memory-bound} (156× below ridge point).
Performance is limited to:
\[P_{\text{attainable}} = 2000 \text{ GB/s} \times 1.5 \text{ FLOP/byte} = 3 \text{ TFLOPS}\]

This represents less than 1\% of A100's compute capacity, explaining why
normalization layers contribute negligible compute time but significant
latency.

\end{fbx}

\textbf{Optimization Strategy Selection by Arithmetic Intensity}:

The roofline analysis directly informs optimization priorities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{High AI (\textgreater200 FLOP/byte)}: Compute-bound operations
  like large convolutions

  \begin{itemize}
  \tightlist
  \item
    Priority: Maximize compute utilization
  \item
    Techniques: Use Tensor Cores, optimize thread block dimensions,
    maximize occupancy
  \item
    Impact: Can approach 90-95\% of peak TFLOPS
  \end{itemize}
\item
  \textbf{Medium AI (20-200 FLOP/byte)}: Borderline operations like
  medium-sized dense layers

  \begin{itemize}
  \tightlist
  \item
    Priority: Balance compute and memory optimization
  \item
    Techniques: Increase batch size to improve AI, use register tiling,
    fuse with adjacent operations
  \item
    Impact: Can move from memory-bound to compute-bound regime
  \end{itemize}
\item
  \textbf{Low AI (\textless20 FLOP/byte)}: Memory-bound operations like
  small dense layers, element-wise operations

  \begin{itemize}
  \tightlist
  \item
    Priority: Reduce memory traffic
  \item
    Techniques: Aggressive operator fusion, reduce precision (FP16 →
    INT8), algorithmic changes
  \item
    Impact: 2-4× speedup possible through fusion alone
  \end{itemize}
\item
  \textbf{Very Low AI (\textless2 FLOP/byte)}: Severely memory-bound
  operations like normalization, activation functions

  \begin{itemize}
  \tightlist
  \item
    Priority: Eliminate memory round-trips
  \item
    Techniques: Mandatory fusion with adjacent operations, in-place
    computation where possible
  \item
    Impact: Can achieve 10× speedup through fusion (e.g., LayerNorm +
    GELU → single fused kernel)
  \end{itemize}
\end{enumerate}

\phantomsection\label{callout-notebook-1.5}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.5: }{Batch Size Impact on Arithmetic Intensity}
\phantomsection\label{callout-notebook-1.5}
Increasing batch size improves AI for matrix operations by amortizing
weight loading:

For dense layer \((B \times M) \times (M \times N)\):
\[\text{AI} = \frac{2BMN}{2BM + 2MN + 2BN} \approx \frac{2BMN}{2MN} = B \text{ as } B \to \infty\]

Example: Dense layer with M=N=2048 - Batch=1: AI = 2 FLOP/byte
(memory-bound) - Batch=32: AI = 32 FLOP/byte (memory-bound) - Batch=256:
AI = 186 FLOP/byte (compute-bound on A100)

This explains the 10-100× throughput improvement from batching in
production inference systems, as the MLPerf scenarios in
\textbf{?@sec-benchmarking-ai} demonstrate.

\end{fbx}

The batch size analysis reveals why inference serving systems are
designed around batching: it fundamentally changes the arithmetic
intensity regime of memory-bound workloads. However, batching introduces
latency trade-offs---requests must wait in a queue until a batch forms.
This tension between throughput (favoring large batches) and latency
(favoring small batches) is a central challenge in ML serving systems,
explored in depth in \textbf{?@sec-model-serving-systems}.

For workloads where batching is impractical---such as interactive LLM
generation where users expect streaming responses---the arithmetic
intensity remains fundamentally low. Understanding this ceiling is
essential for setting realistic performance expectations.

\phantomsection\label{callout-notebook-1.6}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.6: }{Engineering Calculation: The Throughput Ceiling}
\phantomsection\label{callout-notebook-1.6}
\textbf{The Problem:} Predict the maximum possible utilization of an
NVIDIA A100 when running GPT-2 inference (batch size 1).

\textbf{1. The Hardware Constraints (The Denominators)} * \textbf{Peak
Compute:} 312 TFLOPS (FP16 Tensor Core). * \textbf{Peak Bandwidth:} 2.0
TB/s (HBM2e). * \textbf{Ridge Point (Compute/BW):}
\(312 / 2.0 = \mathbf{156 \text{ FLOPs/Byte}}\) (for FP16 Tensor Core).
* \emph{Meaning:} To saturate this chip at FP16 precision, you must
perform 156 operations for every byte loaded. The ridge point varies by
precision: FP32 operations (19.5 TFLOPS peak) have a ridge point of only
\textasciitilde10 FLOP/byte.

\textbf{2. The Workload Characteristics (The Numerator)} *
\textbf{Model:} GPT-2 XL (1.5B parameters). * \textbf{Operation:}
Autoregressive generation (1 token at a time). * \textbf{Data Movement:}
Must load all weights (3.0 GB @ FP16) for every token. *
\textbf{Compute:} Vector-Matrix multiplication.
\(2 \times \text{Params} \approx 3.0 \text{ GFLOPs}\). *
\textbf{Arithmetic Intensity:}
\[ \frac{3.0 \text{ GFLOPs}}{3.0 \text{ GB}} = \mathbf{1.0 \text{ FLOP/Byte}} \]

\textbf{3. The Prediction (Iron Law)} Since Actual Intensity (\(1.0\))
\(\ll\) Ridge Point (\(156\)), the system is \textbf{Bandwidth Bound}. *
\textbf{Maximum Throughput:}
\(1.0 \text{ FLOP/Byte} \times 2.0 \text{ TB/s} = \mathbf{2.0 \text{ TFLOPS}}\).
* \textbf{Utilization Ceiling:}
\[ \frac{2.0 \text{ TFLOPS (Actual)}}{312 \text{ TFLOPS (Peak)}} \approx \mathbf{0.6\%} \]

\textbf{The Systems Conclusion:} Without batching or caching, a \$15,000
GPU runs at \textbf{less than 1\% efficiency} on LLM inference. This
``Utilization Gap'' drives the need for Key-Value Caching
(\textbf{?@sec-optimization-kv-cache}) and Quantization
(\textbf{?@sec-model-compression}).

\end{fbx}

As this derivation demonstrates, the Roofline model provides the
diagnostic framework for identifying whether operations are
compute-bound or memory-bound. Knowing that a workload is memory-bound
at 0.6\% utilization is only the first step; the next challenge is
translating this diagnosis into efficient execution plans that exploit
accelerator architectures. This is the domain of hardware mapping: the
art of assigning computations to processing elements and scheduling data
movement to maximize the utilization that the Roofline analysis reveals
as possible.

\section{Hardware Mapping Fundamentals for Neural
Networks}\label{sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9}

The Roofline analysis taught us to diagnose whether specific operations
are compute-bound or memory-bound on given hardware. We saw that
ResNet-50's convolutions achieve high arithmetic intensity (50-200
FLOP/byte) and operate in the compute-bound regime, while GPT-2's
attention layers achieve only 2-5 FLOP/byte and are severely
memory-bound. But diagnosis is only half the challenge. Once we know
that LayerNorm achieves just 1-2 FLOP/byte on an A100, the question
becomes: how do we execute it efficiently despite this limitation? This
is the domain of hardware mapping, the art of translating abstract
computational graphs into concrete execution plans that exploit
accelerator architectures while respecting their constraints.

The memory system challenges examined in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9}
established \emph{why} memory access dominates modern AI systems: DRAM
access consumes 100-200x more energy than a multiply-accumulate
operation. The Roofline model established \emph{how to measure} whether
a workload is compute-bound or memory-bound. This section addresses the
critical follow-up: \emph{how to map} computations to maximize data
reuse and minimize the energy-intensive transfers that the Roofline
analysis revealed as the primary bottleneck.

Efficient execution of machine learning models on specialized AI
acceleration hardware requires a structured approach to computation,
ensuring that available resources are fully utilized while minimizing
performance bottlenecks. These mapping considerations become
particularly critical in distributed training scenarios, as explored in
\textbf{?@sec-ai-training}. Unlike general-purpose processors, which
rely on dynamic task scheduling, AI accelerators operate under a
structured execution model that maximizes throughput by carefully
assigning computations to processing elements. This process, known as
mapping, dictates how computations are distributed across hardware
resources, influencing execution speed, memory access patterns, and
overall efficiency.

\phantomsection\label{callout-definitionux2a-1.12}
\begin{fbx}{callout-definition}{Definition: }{Mapping in AI Acceleration}
\phantomsection\label{callout-definition*-1.12}
\textbf{Mapping in AI Acceleration} refers to the process of assigning
ML \emph{computations} to \emph{hardware units} through \emph{spatial
allocation}, \emph{temporal scheduling}, and \emph{memory-aware
placement} to maximize execution efficiency and resource utilization.

\end{fbx}

Mapping machine learning models onto AI accelerators presents several
challenges due to hardware constraints and the diversity of model
architectures. Given the hierarchical memory system of modern
accelerators, mapping strategies must carefully manage when and where
data is accessed to minimize latency and power overhead while ensuring
that compute units remain actively engaged. Poor mapping decisions can
lead to underutilized compute resources, excessive data movement, and
increased execution time, ultimately reducing overall efficiency.

Mapping encompasses three aspects that form the foundation of effective
AI accelerator design.

\begin{itemize}
\tightlist
\item
  \textbf{Computation Placement}: Systematically assigns operations
  (e.g., matrix multiplications, convolutions) to processing elements to
  maximize parallelism and reduce idle time.
\item
  \textbf{Memory Allocation}: Carefully determines where model
  parameters, activations, and intermediate results reside within the
  memory hierarchy to optimize access efficiency.
\item
  \textbf{Dataflow and Execution Scheduling}: Structures the movement of
  data between compute units to reduce bandwidth bottlenecks and ensure
  smooth, continuous execution.
\end{itemize}

Effective mapping strategies minimize off-chip memory accesses, maximize
compute utilization, and efficiently manage data movement across
different levels of the memory hierarchy.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{The Role of the Compiler}, rightrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, breakable, toprule=.15mm, bottomtitle=1mm, colback=white, toptitle=1mm, bottomrule=.15mm, coltitle=black]

Developers rarely perform this complex mapping manually. Instead, a
specialized \textbf{compiler} (like NVIDIA's NVCC or Google's XLA) takes
the high-level model from the framework and automatically explores the
mapping search space to find an optimal execution plan for the target
hardware. The compiler is the crucial software layer that translates the
model's computational graph into an efficient hardware-specific
dataflow, balancing the three interrelated aspects of computation
placement, memory allocation, and execution scheduling described above.
This compiler support is examined in detail in
Section~\ref{sec-ai-acceleration-compiler-support-172e}.

\end{tcolorbox}

Key mapping choices influence execution efficiency and lay the
groundwork for optimization strategies that refine these decisions.

\subsection{Computation
Placement}\label{sec-ai-acceleration-computation-placement-23d2}

Modern AI accelerators are designed to execute machine learning models
with massive parallelism, using thousands to millions of processing
elements to perform computations simultaneously. However, simply having
many compute units is not enough. How computations are assigned to these
units determines overall efficiency.

Without careful placement, some processing elements may sit idle while
others are overloaded, leading to wasted resources, increased memory
traffic, and reduced performance. Computation placement is the process
of strategically mapping operations onto available hardware resources to
sustain high throughput, minimize stalls, and optimize execution
efficiency.

\subsubsection{Computation Placement
Definition}\label{sec-ai-acceleration-computation-placement-definition-e130}

AI accelerators contain thousands to millions of processing elements,
making computation placement a large-scale problem. Modern GPUs, such as
the NVIDIA H100, feature over 16,000 streaming processors and more than
500 specialized tensor cores, each designed to accelerate matrix
operations (\citeproc{ref-nvidia2022h100}{Choquette 2023}). TPUs utilize
systolic arrays composed of thousands of interconnected
multiply-accumulate (MAC) units, while wafer-scale processors like
Cerebras' CS-2 push parallelism even further, integrating over 850,000
cores on a single chip (\citeproc{ref-Cerebras2021}{Systems 2021}). In
these architectures, even minor inefficiencies in computation placement
can lead to significant performance losses, as idle cores or excessive
memory movement compound across the system.

Computation placement ensures that all processing elements contribute
effectively to execution. This means that workloads must be distributed
in a way that avoids imbalanced execution, where some processing
elements sit idle while others remain overloaded. Similarly, placement
must minimize unnecessary data movement, as excessive memory transfers
introduce latency and power overheads that degrade system performance.

Neural network computations vary significantly based on the model
architecture, influencing how placement strategies are applied. For
example, in a CNN, placement focuses on dividing image regions across
processing elements to maximize parallelism. A \(256\times256\) image
processed through thousands of GPU cores might be broken into small
tiles, each mapped to a different processing unit to execute
convolutional operations simultaneously. In contrast, a
transformer-based model requires placement strategies that accommodate
self-attention mechanisms, where each token in a sequence interacts with
all others, leading to irregular and memory-intensive computation
patterns. Meanwhile, Graph Neural Networks (GNNs) introduce additional
complexity, as computations depend on sparse and dynamic graph
structures that require adaptive workload distribution
(\citeproc{ref-Zheng2020}{Zheng et al. 2020}).

Because computation placement directly impacts resource utilization,
execution speed, and power efficiency, it is one of the most important
factors in AI acceleration. A well-placed computation can reduce latency
by 10-100x, while a poorly placed one can render thousands of processing
units underutilized. Efficient computation placement and the
consequences of suboptimal mapping strategies are explored next.

\subsubsection{Computation Placement
Importance}\label{sec-ai-acceleration-computation-placement-importance-e7e9}

While computation placement is a hardware-driven process, its importance
is fundamentally shaped by the structure of neural network workloads.
Different types of machine learning models exhibit distinct computation
patterns, which directly influence how efficiently they can be mapped
onto accelerators. Without careful placement, workloads can become
unbalanced, memory access patterns can become inefficient, and the
overall performance of the system can degrade significantly.

For models with structured computation patterns, such as CNNs,
computation placement is relatively straightforward. CNNs process images
using filters that are applied to small, localized regions, meaning
their computations can be evenly distributed across processing elements.
Because these operations are highly parallelizable, CNNs benefit from
spatial partitioning, where the input is divided into tiles that are
processed independently. This structured execution makes CNNs
well-suited for accelerators that favor regular dataflows, minimizing
the complexity of placement decisions.

However, for models with irregular computation patterns, such as
transformers and GNNs, computation placement becomes significantly more
challenging. Transformers, which rely on self-attention mechanisms,
require each token in a sequence to interact with all others, resulting
in non-uniform computation demands. Unlike CNNs, where each processing
element performs a similar amount of work, transformers introduce
workload imbalance, where certain operations, including the computation
of attention scores, require far more computation than others. Without
careful placement, this imbalance can lead to stalls, where some
processing elements remain idle while others struggle to keep up.

The challenge is even greater in graph neural networks (GNNs), where
computation depends on sparse and dynamically changing graph structures.
Unlike CNNs, which operate on dense and regularly structured data, GNNs
must process nodes and edges with highly variable degrees of
connectivity. Some regions of a graph may require significantly more
computation than others, making workload balancing across processing
elements difficult (\citeproc{ref-Zheng2020}{Zheng et al. 2020}). If
computations are not placed strategically, some compute units will sit
idle while others remain overloaded, leading to underutilization and
inefficiencies in execution.

Poor computation placement adversely affects AI execution by creating
workload imbalance, inducing excessive data movement, and causing
execution stalls and bottlenecks. An uneven distribution of computations
can lead to idle processing elements, preventing full hardware
utilization and diminishing throughput. Inefficient execution assignment
increases memory traffic by necessitating frequent data transfers
between memory hierarchies, introducing latency and raising power
consumption. Finally, such misallocation can cause operations to wait on
data dependencies, resulting in pipeline inefficiencies that ultimately
lower overall system performance.

Computation placement ensures that models execute efficiently given
their unique computational structure. A well-placed workload reduces
execution time, memory overhead, and power consumption, while a poorly
placed one can lead to stalled execution pipelines and inefficient
resource utilization. Key considerations must be addressed to ensure
that computation placement is both efficient and adaptable to different
model architectures.

\subsubsection{Effective Computation
Placement}\label{sec-ai-acceleration-effective-computation-placement-099d}

Computation placement is a balancing act between hardware constraints
and workload characteristics. To achieve high efficiency, placement
strategies must account for parallelism, memory access, and workload
variability while ensuring that processing elements remain fully
utilized. Poor placement leads to imbalanced execution, increased data
movement, and performance degradation, making it essential to consider
key factors when designing placement strategies.

Computation placement faces several critical challenges that impact
execution efficiency. Table~\ref{tbl-placement-challenges} outlines how
effective mapping strategies must address these challenges by balancing
workload distribution, minimizing data movement, and optimizing
communication across processing elements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1486}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4699}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3775}}@{}}
\caption{\textbf{Computation Placement Challenges}: Effective neural
network deployment requires strategic allocation of computations to
processing elements, balancing workload distribution, data movement
costs, and hardware constraints to maximize execution efficiency and
avoid performance bottlenecks. Understanding these challenges guides the
design of mapping strategies that optimize resource utilization and
minimize communication
overhead.}\label{tbl-placement-challenges}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Placement}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Placement}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Workload Imbalance} & Some processing elements finish early
while others remain overloaded, leading to idle compute resources. &
Distribute operations evenly to prevent stalls and ensure full
utilization of PEs. \\
\textbf{Irregular Computation Patterns} & Models like transformers and
GNNs introduce non-uniform computation demands, making static placement
difficult. & Use adaptive placement strategies that adjust execution
based on workload characteristics. \\
\textbf{Excessive Data Movement} & Frequent memory transfers introduce
latency and increase power consumption. & Keep frequently used data
close to the compute units and minimize off-chip memory accesses. \\
\textbf{Limited Interconnect Bandwidth} & Poorly placed operations can
create congestion, slowing data movement between PEs. & Optimize spatial
and temporal placement to reduce communication overhead. \\
\textbf{Model-Specific Execution Needs} & CNNs, transformers, and GNNs
require different execution patterns, making a single placement strategy
ineffective. & Tailor placement strategies to match the computational
structure of each model type. \\
\end{longtable}

Each of these challenges highlights a core trade-off in computation
placement: maximizing parallelism while minimizing memory overhead. For
CNNs, placement strategies prioritize structured tiling to maintain
efficient data reuse. For transformers, placement must ensure balanced
execution across attention layers. For GNNs, placement must dynamically
adjust to sparse computation patterns.

Beyond model-specific needs, effective computation placement must also
be scalable. As models grow in size and complexity, placement strategies
must adapt dynamically rather than relying on static execution patterns.
Future AI accelerators increasingly integrate runtime-aware scheduling
mechanisms, where placement is optimized based on real-time workload
behavior rather than predetermined execution plans.

Effective computation placement requires balancing hardware capabilities
with model characteristics. Computation placement interacts with memory
allocation and data movement, ensuring that AI accelerators operate at
peak efficiency.

\subsection{Memory
Allocation}\label{sec-ai-acceleration-memory-allocation-e095}

Efficient memory allocation is essential for high-performance AI
acceleration. As AI models grow in complexity, accelerators must manage
vast amounts of data movement: loading model parameters, storing
intermediate activations, and handling gradient computations. The way
this data is allocated across the memory hierarchy directly affects
execution efficiency, power consumption, and overall system throughput.

\subsubsection{Memory Allocation
Definition}\label{sec-ai-acceleration-memory-allocation-definition-e740}

While computation placement determines where operations are executed,
memory allocation defines where data is stored and how it is accessed
throughout execution. All AI accelerators rely on hierarchical memory
systems, from on-chip caches and scratchpads to HBM and DRAM. Poor
memory allocation can lead to excessive off-chip memory accesses,
increasing bandwidth contention and slowing down execution. Since AI
accelerators operate at teraflop and petaflop scales, inefficient memory
access patterns can result in substantial performance bottlenecks.

The primary goal of memory allocation is to minimize latency and reduce
power consumption by keeping frequently accessed data as close as
possible to the processing elements. Different hardware architectures
implement memory hierarchies tailored for AI workloads. GPUs rely on a
mix of global memory, shared memory, and registers, requiring careful
tiling strategies to optimize locality
(\citeproc{ref-nvidia2020ampere}{N. Corporation 2020}). TPUs use on-chip
SRAM scratchpads, where activations and weights must be efficiently
preloaded to sustain systolic array execution
(\citeproc{ref-jouppi_tpu_2017}{Norman P. Jouppi et al. 2017b}).
Wafer-scale processors, with their hundreds of thousands of cores,
demand sophisticated memory partitioning strategies to avoid excessive
interconnect traffic (\citeproc{ref-Cerebras2021}{Systems 2021}). In all
cases, the effectiveness of memory allocation determines the overall
throughput, power efficiency, and scalability of AI execution.

Memory allocation directly impacts AI acceleration efficiency through
data storage and access patterns. Unlike general-purpose computing,
where memory management is abstracted by caches and dynamic allocation,
AI accelerators require explicit data placement strategies to sustain
high throughput and avoid unnecessary stalls. This is particularly
evident in systolic arrays (Figure~\ref{fig-systolic-array}), where the
rhythmic data flow between processing elements depends on precisely
timed memory access patterns. In TPU's systolic arrays, for instance,
weights must be preloaded into on-chip scratchpads and streamed through
the array in perfect synchronization with input activations to maintain
the pipelined computation flow. When memory is not allocated
efficiently, AI workloads suffer from latency overhead, excessive power
consumption, and bottlenecks that limit computational performance.

\subsubsection{Memory Challenges for Different
Workloads}\label{sec-ai-acceleration-memory-challenges-different-workloads-e87c}

Neural network architectures have varying memory demands, which
influence the importance of proper allocation. CNNs rely on structured
and localized data access patterns, meaning that inefficient memory
allocation can lead to redundant data loads and cache inefficiencies
(\citeproc{ref-chen2016eyeriss}{Chen et al. 2016}). In contrast,
transformer models require frequent access to large model parameters and
intermediate activations, making them highly sensitive to memory
bandwidth constraints. GNNs introduce even greater challenges, as their
irregular and sparse data structures result in unpredictable memory
access patterns that can lead to inefficient use of memory resources.
Poor memory allocation has three major consequences for AI execution:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Increased Memory Latency}: When frequently accessed data is
  not stored in the right location, accelerators must retrieve it from
  higher-latency memory, slowing down execution.
\item
  \textbf{Higher Power Consumption}: Off-chip memory accesses consume
  significantly more energy than on-chip storage, leading to
  inefficiencies at scale.
\item
  \textbf{Reduced Computational Throughput}: If data is not available
  when needed, processing elements remain idle, reducing the overall
  performance of the system.
\end{enumerate}

As AI models continue to grow in size and complexity, scalable and
efficient memory allocation becomes more important. Memory limitations
can dictate how large of a model can be deployed on a given accelerator,
affecting feasibility and performance.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1703}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3886}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4367}}@{}}
\caption{\textbf{Memory Allocation Challenges}: Efficient memory
management in AI accelerators balances data access speed with hardware
constraints, mitigating performance bottlenecks caused by latency,
bandwidth limitations, and irregular data patterns. Addressing these
challenges is critical for deploying complex models, such as
transformers and graphs, which have variable and demanding memory
requirements.}\label{tbl-memory-allocation}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Allocation}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Allocation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{High Memory Latency} & Slow data access delays execution and
reduces throughput. & Prioritize placing frequently accessed data in
faster memory locations. \\
\textbf{Limited On-Chip Storage} & Small local memory constrains the
amount of data available near compute units. & Allocate storage
efficiently to maximize data availability without exceeding hardware
limits. \\
\textbf{High Off-Chip Bandwidth Demand} & Frequent access to external
memory increases delays and power consumption. & Reduce unnecessary
memory transfers by carefully managing when and how data is moved. \\
\textbf{Irregular Memory Access Patterns} & Some models require
accessing data unpredictably, leading to inefficient memory usage. &
Organize memory layout to align with access patterns and minimize
unnecessary data movement. \\
\textbf{Model-Specific Memory Needs} & Different models require
different allocation strategies to optimize performance. & Tailor
allocation decisions based on the structure and execution
characteristics of the workload. \\
\end{longtable}

Table~\ref{tbl-memory-allocation} summarizes the fundamental memory
management challenges that AI accelerators must address. Each of these
challenges requires careful memory management to balance execution
efficiency with hardware constraints. While structured models may
benefit from well-defined memory layouts that facilitate predictable
access, others, like transformer-based and graph-based models, require
more adaptive allocation strategies to handle variable and complex
memory demands. Beyond workload-specific considerations, memory
allocation must also be scalable. As model sizes continue to grow,
accelerators must dynamically manage memory resources rather than
relying on static allocation schemes. Ensuring that frequently used data
is accessible when needed without overwhelming memory capacity is
essential for maintaining high efficiency.

\subsection{Combinatorial
Complexity}\label{sec-ai-acceleration-combinatorial-complexity-ea33}

The efficient execution of machine learning models on AI accelerators
requires careful consideration of placement and allocation. Placement
involves spatial assignment of computations and data, while allocation
covers temporal distribution of resources. These decisions are
interdependent, and each introduces trade-offs that impact performance,
energy efficiency, and scalability.
Table~\ref{tbl-combinatorial-complexity} enumerates the fundamental
trade-offs between computation placement and resource allocation that
shape overall performance. Placement decisions influence parallelism,
memory access patterns, and communication overhead, while allocation
strategies determine how resources are distributed over time to balance
execution efficiency. The interplay between these factors requires a
careful balance to avoid bottlenecks such as excessive synchronization,
memory congestion, or underutilized compute resources. Optimizing these
trade-offs is essential for ensuring that AI accelerators operate at
peak efficiency.

Each of these dimensions requires balancing trade-offs between placement
and allocation. For instance, spatially distributing computations across
multiple processing elements can increase throughput; however, if data
allocation is not optimized, memory bandwidth limitations may introduce
bottlenecks. Likewise, allocating resources for fine-grained
computations may enhance flexibility but, without appropriate placement
strategies, may lead to excessive synchronization overhead.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1594}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4263}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4104}}@{}}
\caption{\textbf{Placement-Allocation Trade-Offs}: AI accelerator
performance depends on strategically mapping computations to hardware
and allocating resources over time, balancing parallelism, memory
access, and execution efficiency to avoid bottlenecks. Careful
consideration of these interdependent factors is essential for
maximizing throughput and minimizing energy consumption in machine
learning systems.}\label{tbl-combinatorial-complexity}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dimension}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Placement Considerations}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Allocation Considerations}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dimension}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Placement Considerations}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Allocation Considerations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Computational Granularity} & Fine-grained placement enables
greater parallelism but increases synchronization overhead. &
Coarse-grained allocation reduces synchronization overhead but may limit
flexibility. \\
\textbf{Spatial vs.~Temporal Mapping} & Spatial placement enhances
parallel execution but can lead to resource contention and memory
congestion. & Temporal allocation balances resource sharing but may
reduce overall throughput. \\
\textbf{Memory and Data Locality} & Placing data closer to compute units
minimizes latency but may reduce overall memory availability. &
Allocating data across multiple memory levels increases capacity but
introduces higher access costs. \\
\textbf{Communication and Synchronization} & Co-locating compute units
reduces communication latency but may introduce contention. & Allocating
synchronization mechanisms mitigates stalls but can introduce additional
overhead. \\
\textbf{Dataflow and Execution Ordering} & Static placement simplifies
execution but limits adaptability to workload variations. & Dynamic
allocation improves adaptability but adds scheduling complexity. \\
\end{longtable}

Because AI accelerator architectures impose constraints on both where
computations execute and how resources are assigned over time, selecting
an effective mapping strategy necessitates a coordinated approach to
placement and allocation. Understanding how these trade-offs influence
execution efficiency is essential for optimizing performance on AI
accelerators.

\subsubsection{Exploring the Configuration
Space}\label{sec-ai-acceleration-exploring-configuration-space-f010}

The efficiency of AI accelerators is determined not only by their
computational capabilities but also by how neural network computations
are mapped to hardware resources. Mapping defines how computations are
assigned to processing elements, how data is placed and moved through
the memory hierarchy, and how execution is scheduled. The choices made
in this process significantly impact performance, influencing compute
utilization, memory bandwidth efficiency, and energy consumption.

Mapping machine learning models to hardware presents a large and complex
design space. Unlike traditional computational workloads, model
execution involves multiple interacting factors (computation, data
movement, parallelism, and scheduling), each introducing constraints and
tradeoffs. The hierarchical memory structure of accelerators, as
discussed in the Memory Systems section, further complicates this
process by imposing limits on bandwidth, latency, and data reuse. As a
result, effective mapping strategies must carefully balance competing
objectives to maximize efficiency.

At the heart of this design space lie three interconnected aspects: data
placement, computation scheduling, and data movement timing. Data
placement refers to the allocation of data across various memory
hierarchies, such as on-chip buffers, caches, and off-chip DRAM, and its
effective management is critical because it influences both latency and
energy consumption. Inefficient placement often results in frequent,
costly memory accesses, whereas strategic placement ensures that data
used regularly remains in fast-access storage. Computation scheduling
governs the order in which operations execute, impacting compute
efficiency and memory access patterns; for instance, some execution
orders may optimize parallelism while introducing synchronization
overheads, and others may improve data locality at the expense of
throughput. Meanwhile, timing in data movement is equally essential, as
transferring data between memory levels incurs significant latency and
energy costs. Efficient mapping strategies thus focus on minimizing
unnecessary transfers by reusing data and overlapping communication with
computation to enhance overall performance.

These factors define a vast combinatorial design space, where small
variations in mapping decisions can lead to large differences in
performance and energy efficiency. A poor mapping strategy can result in
underutilized compute resources, excessive data movement, or imbalanced
workloads, creating bottlenecks that degrade overall efficiency.
Conversely, a well-designed mapping maximizes both throughput and
resource utilization, making efficient use of available hardware.

Because of the interconnected nature of mapping decisions, there is no
single optimal solution. Different workloads and hardware architectures
demand different approaches. Different mapping choices shape the
execution of machine learning workloads.

Mapping machine learning computations onto specialized hardware requires
balancing multiple constraints such as compute efficiency, memory
bandwidth, and execution scheduling. The challenge arises from the vast
number of possible ways to assign computations to processing elements,
order execution, and manage data movement. Each decision contributes to
a high-dimensional search space, where even minor variations in mapping
choices can significantly impact performance.

Unlike traditional workloads with predictable execution patterns,
machine learning models introduce diverse computational structures that
require flexible mappings adapted to data reuse, parallelization
opportunities, and memory constraints. The search space grows
combinatorially, making exhaustive search infeasible. Three sources of
variation contribute to this complexity:

\subsubsection{Ordering Computation and
Execution}\label{sec-ai-acceleration-ordering-computation-execution-7251}

Machine learning workloads are often structured as nested loops that
iterate over various dimensions of computation. For instance, a matrix
multiplication kernel may loop over batch size (\(N\)), input features
(\(C\)), and output features (\(K\)). The order in which these loops
execute has a profound effect on data locality, reuse patterns, and
computational efficiency.

The number of ways to arrange \(d\) loops follows a factorial growth
pattern: \[
\mathcal{O} = d!
\] which scales rapidly. A typical convolutional layer may involve up to
seven loop dimensions, leading to: \[
7! = 5,040 \text{ possible execution orders.}
\]

When considering multiple memory levels, the search space expands as: \[
(d!)^l
\] where \(l\) is the number of memory hierarchy levels. This rapid
expansion shows why execution order optimization matters: poor loop
ordering can lead to excessive memory traffic, while an optimized order
improves cache utilization (\citeproc{ref-sze2017efficient}{Sze et al.
2017a}).

\subsubsection{Parallelization Across Processing
Elements}\label{sec-ai-acceleration-parallelization-across-processing-elements-90d6}

Modern AI accelerators use thousands of processing elements to maximize
parallelism, but determining which computations should be parallelized
is non-trivial. Excessive parallelization can introduce synchronization
overheads and increased bandwidth demands, while insufficient
parallelization leads to underutilized hardware.

The number of ways to distribute computations among parallel units
follows the binomial coefficient: \[
\mathcal{P} = \frac{d!}{(d-k)!}
\] where \(d\) is the number of loops, and \(k\) is the number selected
for parallel execution. For a six-loop computation where three loops are
chosen for parallel execution, the number of valid configurations is: \[
\frac{6!}{(6-3)!} = 120.
\]

Even for a single layer, there can be hundreds of valid parallelization
strategies, each affecting data synchronization, memory contention, and
overall compute efficiency. Expanding this across multiple layers and
model architectures further magnifies the complexity.

\subsubsection{Memory Placement and Data
Movement}\label{sec-ai-acceleration-memory-placement-data-movement-fd52}

The hierarchical memory structure of AI accelerators introduces
additional constraints, as data must be efficiently placed across
registers, caches, shared memory, and off-chip DRAM. Data placement
impacts latency, bandwidth consumption, and energy efficiency. Frequent
access to slow memory creates bottlenecks, while optimized placement
reduces costly memory transfers.

The number of ways to allocate data across memory levels follows an
exponential growth function: \[
\mathcal{M} = n^{d \times l}
\] where:

\begin{itemize}
\tightlist
\item
  \(n\) = number of placement choices per level,
\item
  \(d\) = number of computational dimensions,
\item
  \(l\) = number of memory hierarchy levels.
\end{itemize}

For a model with:

\begin{itemize}
\tightlist
\item
  \(d = 5\) computational dimensions,
\item
  \(l = 3\) memory levels,
\item
  \(n = 4\) possible placement choices per level,
\end{itemize}

\noindent the number of possible memory allocations is: \[
4^{5 \times 3} = 4^{15} = 1,073,741,824.
\]

This highlights how even a single layer may have over a billion possible
memory configurations, making manual optimization impractical.

\subsubsection{Mapping Search
Space}\label{sec-ai-acceleration-mapping-search-space-e9b6}

By combining the complexity from computation ordering, parallelization,
and memory placement, the total mapping search space can be approximated
as: \[
\mathcal{S} = \left( n^d \times d! \times \frac{d!}{(d-k)!} \right)^l
\] where:

\begin{itemize}
\tightlist
\item
  \(n^d\) represents memory placement choices,
\item
  \(d!\) accounts for computation ordering choices,
\item
  \(\frac{d!}{(d-k)!}\) captures parallelization possibilities,
\item
  \(l\) is the number of memory hierarchy levels.
\end{itemize}

This equation illustrates the exponential growth of the search space,
making brute-force search infeasible for all but the simplest cases.

The combinatorial explosion revealed by this analysis, with potentially
billions of valid configurations for a single neural network layer,
poses a fundamental question: how do practitioners routinely achieve
near-optimal performance despite this vast search space? Exhaustive
enumeration is clearly impossible, yet production systems consistently
achieve 60-80\% of theoretical peak performance. The answer lies in a
small set of principled dataflow patterns that capture the essential
trade-offs, constraining the search to well-understood strategies that
have proven effective across diverse workloads. These patterns reduce
the seemingly intractable configuration space to a manageable set of
strategic choices.

\phantomsection\label{quiz-question-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.5}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the primary goal of mapping in
  AI acceleration?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Optimizing execution efficiency by aligning computations with
    hardware resources.
  \item
    Minimizing the energy consumption of the accelerator.
  \item
    Maximizing the number of processing elements used at any time.
  \item
    Ensuring all computations are executed in parallel.
  \end{enumerate}
\item
  True or False: Effective computation placement on AI accelerators
  always requires manual intervention by developers.
\item
  Why is data locality critical in the mapping of neural networks onto
  AI accelerators?
\item
  Order the following steps in the mapping process for neural networks
  on AI accelerators: (1) Data placement, (2) Computation scheduling,
  (3) Data movement timing.
\item
  In a production system, how might poor computation placement affect
  the performance of AI accelerators?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Dataflow Optimization
Strategies}\label{sec-ai-acceleration-dataflow-optimization-strategies-ce52}

While the full configuration space is intractable, a small set of
principled dataflow patterns capture the essential trade-offs.

This section introduces these patterns through three questions that
structure all dataflow decisions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Which data stays local?} Weight-stationary, output-stationary,
  and input-stationary strategies each make different choices about what
  to cache near compute units, trading off different memory access
  patterns.
\item
  \textbf{How is data organized?} Tensor layouts (NHWC vs.~NCHW)
  determine whether memory accesses align with hardware preferences,
  with performance impacts of 2-5x.
\item
  \textbf{How are operations combined?} Kernel fusion and tiling
  restructure computation to minimize memory traffic, often achieving
  2-10x speedups through reduced data movement alone.
\end{enumerate}

By mastering these patterns, you will be able to reason about 90\% of
dataflow optimization decisions without exhaustive search. We examine
each question in turn, then see how they combine for specific neural
network architectures including ResNet-50, GPT-2, and MLPs.

Mapping strategies establish \emph{where} computations execute and
\emph{where} data resides within an accelerator's architecture, but they
do not specify \emph{how} data flows through processing elements during
execution. A systolic array might process a matrix multiplication with
weights stored in local memory, but the order in which weights, inputs,
and outputs move through the array fundamentally determines memory
bandwidth consumption and energy efficiency. These dataflow patterns,
termed optimization strategies, represent the critical implementation
dimension that translates abstract mapping decisions into concrete
execution plans.

The choice among weight-stationary, input-stationary, and
output-stationary approaches directly impacts whether an accelerator
operates in the compute-bound or memory-bound region. Understanding
these trade-offs is essential because compilers
(Section~\ref{sec-ai-acceleration-compiler-support-172e}) and runtime
systems (Section~\ref{sec-ai-acceleration-runtime-support-f94f}) must
select appropriate dataflow patterns based on computational
characteristics and memory hierarchy capabilities analyzed in
Section~\ref{sec-ai-acceleration-memory-hierarchy-1839}.

To overcome the combinatorial challenge, AI accelerators rely on
structured mapping strategies that systematically balance computational
efficiency, data locality, and parallel execution. Rather than
evaluating every possible configuration, these approaches use a
combination of heuristic, analytical, and machine learning-based
techniques to find high-performance mappings efficiently.

\subsection{Building Blocks of Mapping
Strategies}\label{sec-ai-acceleration-building-blocks-mapping-strategies-4932}

To navigate the complexity of mapping decisions, practitioners rely on a
set of foundational techniques that optimize execution across data
movement, memory access, and computation efficiency.

Key techniques include data movement strategies, which determine where
data is staged during computation in order to reduce redundant
transfers, such as in weight stationary, output stationary, and input
stationary approaches. Memory-aware tensor layouts also play an
important role by influencing memory access patterns and cache
efficiency through the organization of data in formats such as row-major
or channel-major.

Other strategies involve kernel fusion, a method that minimizes
redundant memory writes by combining multiple operations into a single
computational step. Tiling is employed as a technique that partitions
large computations into smaller, memory-friendly blocks to improve cache
efficiency and reduce memory bandwidth requirements. Finally, balancing
computation and communication is essential for managing the trade-offs
between parallel execution and memory access to achieve high throughput.

Each of these building blocks forms the basis for both heuristic and
model-driven optimization techniques.

\subsubsection{Data Movement
Patterns}\label{sec-ai-acceleration-data-movement-patterns-3b06}

While computational mapping determines where and when operations occur,
its success depends heavily on how efficiently data is accessed and
transferred across the memory hierarchy. As discussed in
Section~\ref{sec-ai-acceleration-irregular-memory-access-c6ec}, machine
learning workloads exhibit irregular access patterns that challenge
standard caching mechanisms. This irregularity makes data movement
strategy critical to overall system performance.

Even when computational units are mapped efficiently, poor data movement
strategies can severely degrade performance, leading to frequent memory
stalls and underutilized hardware resources. If data cannot be supplied
to processing elements at the required rate, computational units remain
idle, increasing latency, memory traffic, and energy consumption
(\citeproc{ref-chen2016eyeriss}{Chen et al. 2016}).

Listing~\ref{lst-matmul_data_movement} illustrates how data movement
inefficiencies affect the backbone computation of many machine learning
models through a typical matrix multiplication operation.

\begin{codelisting}

\caption{\label{lst-matmul_data_movement}\textbf{Matrix Multiplication}:
Data movement bottlenecks can lead to underutilized hardware resources,
illustrating the importance of efficient data flow in optimizing machine
learning model performance. Via This operation}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Matrix multiplication where: \{\#sec{-}ai{-}acceleration{-}matrix{-}multiplication{-}6a93\}}
\CommentTok{\#\# weights: [512 x 256] {-} model parameters \{\#sec{-}ai{-}acceleration{-}weights{-}512{-}x{-}256{-}model{-}parameters{-}de74\}}
\CommentTok{\#\# input:   [256 x 32]  {-} batch of activations \{\#sec{-}ai{-}acceleration{-}input{-}256{-}x{-}32{-}batch{-}activations{-}0fe1\}}
\CommentTok{\#\# Z:       [512 x 32]  {-} output activations \{\#sec{-}ai{-}acceleration{-}z{-}512{-}x{-}32{-}output{-}activations{-}9bfa\}}

\CommentTok{\#\# Computing each output element Z[i,j]: \{\#sec{-}ai{-}acceleration{-}computing{-}output{-}element{-}zij{-}085f\}}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{256}\NormalTok{):}
\NormalTok{            Z[i, j] }\OperatorTok{+=}\NormalTok{ weights[i, k] }\OperatorTok{*} \BuiltInTok{input}\NormalTok{[k, j]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This computation reveals several critical dataflow challenges. The first
challenge is the number of memory accesses required. For each output
\(Z[i, j]\), the computation must fetch an entire row of weights from
the weight matrix and a full column of activations from the input
matrix. Since the weight matrix contains 512 rows and the input matrix
contains 32 columns, this results in repeated memory accesses that place
a significant burden on memory bandwidth.

The second challenge comes from weight reuse. The same weights are
applied to multiple inputs, meaning that an ideal mapping strategy
should maximize weight locality to avoid redundant memory fetches.
Without proper reuse, the accelerator would waste bandwidth loading the
same weights multiple times (\citeproc{ref-chen2018tvm}{0001 et al.
2018a}).

The third challenge involves the accumulation of intermediate results.
Since each element in \(Z[i,j]\) requires contributions from 256
different weight-input pairs, partial sums must be stored and retrieved
before the final value is computed. If these intermediate values are
stored inefficiently, the system will require frequent memory accesses,
further increasing bandwidth demands.

One way to mitigate these challenges is to use SIMD and SIMT execution
models, which allow multiple values to be fetched in parallel. However,
even with these optimizations, data movement remains a bottleneck. The
issue is not just how quickly data is retrieved but how often it must be
moved and where it is placed within the memory hierarchy
(\citeproc{ref-han2016eie}{Han et al. 2016}).

Given that data movement is 100-1000x more expensive than computation,
the single most important goal of an accelerator is to minimize memory
access. Dataflow strategies achieve this by maximizing data reuse. The
question is: which data is most valuable to keep local? To address this,
accelerators implement dataflow strategies that determine which data
remains fixed in memory and which data is streamed dynamically. These
strategies represent different answers to the fundamental question of
data locality: weight-stationary keeps model parameters local,
input-stationary maintains activation data, and output-stationary
preserves intermediate results. Each approach trades off different
memory access patterns to maximize data reuse and minimize the
energy-intensive transfers that constitute the primary bottleneck in AI
acceleration.

\paragraph{Weight
Stationary}\label{sec-ai-acceleration-weight-stationary-156a}

The Weight Stationary strategy keeps weights fixed in local memory,
while input activations and partial sums are streamed through the
system. Weight stationary approaches prove particularly beneficial in
CNNs and matrix multiplications, where the same set of weights is
applied across multiple inputs. By ensuring weights remain stationary,
this method reduces redundant memory fetches, which helps alleviate
bandwidth bottlenecks and improves energy efficiency.

A key advantage of weight stationary is that it maximizes weight reuse,
reducing the frequency of memory accesses to external storage. Since
weight parameters are often shared across multiple computations, keeping
them in local memory eliminates unnecessary data movement, lowering the
overall energy cost of computation. This makes it particularly effective
for architectures where weights represent the dominant memory overhead,
such as systolic arrays and custom accelerators designed for machine
learning.

Listing~\ref{lst-weight_stationary} demonstrates how Weight Stationary
execution keeps weights fixed in local memory while streaming inputs and
accumulating partial sums.

\begin{codelisting}

\caption{\label{lst-weight_stationary}\textbf{Weight Stationary Matrix
Multiplication}: Weight stationary matrix multiplication keeps weights
fixed in local memory while input activations stream through,
demonstrating how it maximizes weight reuse to reduce energy costs.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Weight Stationary Matrix Multiplication \{\#sec{-}ai{-}acceleration{-}weight{-}stationary{-}matrix{-}multiplication{-}8ff3\}}
\CommentTok{\#\# {-} Weights remain fixed in local memory \{\#sec{-}ai{-}acceleration{-}weights{-}remain{-}fixed{-}local{-}memory{-}e4d7\}}
\CommentTok{\#\# {-} Input activations stream through \{\#sec{-}ai{-}acceleration{-}input{-}activations{-}stream{-}8ec6\}}
\CommentTok{\#\# {-} Partial sums accumulate for final output \{\#sec{-}ai{-}acceleration{-}partial{-}sums{-}accumulate{-}final{-}output{-}f96d\}}

\ControlFlowTok{for}\NormalTok{ weight\_block }\KeywordTok{in}\NormalTok{ weights:  }\CommentTok{\# Load and keep weights stationary}
\NormalTok{    load\_to\_local(weight\_block)  }\CommentTok{\# Fixed in local storage}
    \ControlFlowTok{for}\NormalTok{ input\_block }\KeywordTok{in}\NormalTok{ inputs:  }\CommentTok{\# Stream inputs dynamically}
        \ControlFlowTok{for}\NormalTok{ output\_block }\KeywordTok{in}\NormalTok{ outputs:  }\CommentTok{\# Compute results}
\NormalTok{            output\_block }\OperatorTok{+=}\NormalTok{ compute(weight\_block, input\_block)}
            \CommentTok{\# Reuse weights across inputs}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

In weight stationary execution, weights are loaded once into local
memory and remain fixed throughout the computation, while inputs are
streamed dynamically, thereby reducing redundant memory accesses. At the
same time, partial sums are accumulated in an efficient manner that
minimizes unnecessary data movement, ensuring that the system maintains
high throughput and energy efficiency.

By keeping weights fixed in local storage, memory bandwidth requirements
are significantly reduced, as weights do not need to be reloaded for
each new computation. Instead, the system efficiently reuses the stored
weights across multiple input activations, allowing for high throughput
execution. This makes weight stationary dataflow highly effective for
workloads with heavy weight reuse patterns, such as CNNs and matrix
multiplications.

However, while this strategy reduces weight-related memory traffic, it
introduces trade-offs in input and output movement. Since inputs must be
streamed dynamically while weights remain fixed, the efficiency of this
approach depends on how well input activations can be delivered to the
computational units without causing stalls. Additionally, partial sums,
which represent intermediate results, must be carefully accumulated to
avoid excessive memory traffic. The total performance gain depends on
the size of available on-chip memory, as storing larger weight matrices
locally can become a constraint in models with millions or billions of
parameters.

The weight stationary strategy is well-suited for workloads where
weights exhibit high reuse and memory bandwidth is a limiting factor. It
is commonly employed in CNNs, systolic arrays, and matrix multiplication
kernels, where structured weight reuse leads to significant performance
improvements. However, for models where input or output reuse is more
critical, alternative dataflow strategies, such as output stationary or
input stationary, may provide better trade-offs.

\paragraph{Output
Stationary}\label{sec-ai-acceleration-output-stationary-54e5}

The Output Stationary strategy keeps partial sums fixed in local memory,
while weights and input activations stream through the system. This
approach is particularly effective for fully connected layers, systolic
arrays, and other operations where an output element accumulates
contributions from multiple weight-input pairs. By keeping partial sums
stationary, this method reduces redundant memory writes, minimizing
bandwidth consumption and improving energy efficiency
(\citeproc{ref-chen2016eyeriss}{Chen et al. 2016}).

A key advantage of the output stationary approach is that it optimizes
accumulation efficiency, ensuring that each output element is computed
as efficiently as possible before being written to memory. Unlike Weight
Stationary, which prioritizes weight reuse, Output Stationary execution
is designed to minimize memory bandwidth overhead caused by frequent
writes of intermediate results. This makes it well-suited for workloads
where accumulation dominates the computational pattern, such as fully
connected layers and matrix multiplications in transformer-based models.

Listing~\ref{lst-output_stationary} demonstrates how accumulating
partial sums locally minimizes memory writes and enhances efficiency
during matrix multiplication.

\begin{codelisting}

\caption{\label{lst-output_stationary}\textbf{Output Stationary
Execution}: Accumulates partial sums locally to reduce memory writes and
enhance efficiency during matrix multiplication, making it ideal for
transformer-based models.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# {-} Partial sums remain in local memory \{\#sec{-}ai{-}acceleration{-}partial{-}sums{-}remain{-}local{-}memory{-}bd14\}}
\CommentTok{\#\# {-} Weights and input activations stream through dynamically \{\#sec{-}ai{-}acceleration{-}weights{-}input{-}activations{-}stream{-}dynamically{-}6686\}}
\CommentTok{\#\# {-} Final outputs are written only once \{\#sec{-}ai{-}acceleration{-}final{-}outputs{-}written{-}9187\}}

\ControlFlowTok{for}\NormalTok{ output\_block }\KeywordTok{in}\NormalTok{ outputs:  }\CommentTok{\# Keep partial sums stationary}
\NormalTok{    accumulator }\OperatorTok{=} \DecValTok{0}  \CommentTok{\# Initialize accumulation buffer}
    \ControlFlowTok{for}\NormalTok{ weight\_block, input\_block }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(weights, inputs):}
\NormalTok{        accumulator }\OperatorTok{+=}\NormalTok{ compute(weight\_block, input\_block)}
        \CommentTok{\# Accumulate partial sums}
\NormalTok{    store\_output(accumulator)  }\CommentTok{\# Single write to memory}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This implementation follows the core principles of output stationary
execution:

\begin{itemize}
\tightlist
\item
  Partial sums are kept in local memory throughout the computation.
\item
  Weights and inputs are streamed dynamically, ensuring that
  intermediate results remain locally accessible.
\item
  Final outputs are written back to memory only once, reducing
  unnecessary memory traffic.
\end{itemize}

By accumulating partial sums locally, this approach eliminates excessive
memory writes, improving overall system efficiency. In architectures
such as systolic arrays, where computation progresses through a grid of
processing elements, keeping partial sums stationary aligns naturally
with structured accumulation workflows, reducing synchronization
overhead.

However, while Output Stationary reduces memory write traffic, it
introduces trade-offs in weight and input movement. Since weights and
activations must be streamed dynamically, the efficiency of this
approach depends on how well data can be fed into the system without
causing stalls. Additionally, parallel implementations must carefully
synchronize updates to partial sums, especially in architectures where
multiple processing elements contribute to the same output.

The Output Stationary strategy is most effective for workloads where
accumulation is the dominant operation and minimizing intermediate
memory writes is critical. It is commonly employed in fully connected
layers, attention mechanisms, and systolic arrays, where structured
accumulation leads to significant performance improvements. However, for
models where input reuse is more critical, alternative dataflow
strategies, such as Input Stationary, may provide better trade-offs.

\paragraph{Input
Stationary}\label{sec-ai-acceleration-input-stationary-6c7b}

The Input Stationary strategy keeps input activations fixed in local
memory, while weights and partial sums stream through the system. This
approach is particularly effective for batch processing, transformer
models, and sequence-based architectures, where input activations are
reused across multiple computations. By ensuring that activations remain
in local memory, this method reduces redundant input fetches, improving
data locality and minimizing memory traffic.

A key advantage of the Input Stationary approach is that it maximizes
input reuse, reducing the frequency of memory accesses for activations.
Since many models, especially those in NLP and recommendation systems,
process the same input data across multiple computations, keeping inputs
stationary eliminates unnecessary memory transfers, thereby lowering
energy consumption. This strategy is particularly useful when dealing
with large batch sizes, where a single batch of input activations
contributes to multiple weight transformations.

Listing~\ref{lst-input_stationary} illustrates this approach, maximizing
reuse by keeping input activations stationary in local memory while
dynamically streaming weights.

\begin{codelisting}

\caption{\label{lst-input_stationary}\textbf{Input Stationary}: This
approach keeps input activations stationary while dynamically streaming
weights to maximize memory reuse and reduce energy consumption.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# {-} Input activations remain in local memory \{\#sec{-}ai{-}acceleration{-}input{-}activations{-}remain{-}local{-}memory{-}1ce5\}}
\CommentTok{\#\# {-} Weights stream through dynamically \{\#sec{-}ai{-}acceleration{-}weights{-}stream{-}dynamically{-}6907\}}
\CommentTok{\#\# {-} Partial sums accumulate and are written out \{\#sec{-}ai{-}acceleration{-}partial{-}sums{-}accumulate{-}written{-}0fcf\}}

\ControlFlowTok{for}\NormalTok{ input\_block }\KeywordTok{in}\NormalTok{ inputs:  }\CommentTok{\# Keep input activations stationary}
\NormalTok{    load\_to\_local(input\_block)  }\CommentTok{\# Fixed in local storage}
    \ControlFlowTok{for}\NormalTok{ weight\_block }\KeywordTok{in}\NormalTok{ weights:  }\CommentTok{\# Stream weights dynamically}
        \ControlFlowTok{for}\NormalTok{ output\_block }\KeywordTok{in}\NormalTok{ outputs:  }\CommentTok{\# Compute results}
\NormalTok{            output\_block }\OperatorTok{+=}\NormalTok{ compute(weight\_block, input\_block)}
            \CommentTok{\# Reuse inputs across weights}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This implementation follows the core principles of input stationary
execution:

\begin{itemize}
\tightlist
\item
  Input activations are loaded into local memory and remain fixed during
  computation.
\item
  \textbf{Weights are streamed dynamically}, ensuring efficient
  application across multiple inputs.
\item
  \textbf{Partial sums are accumulated and written out}, optimizing
  memory bandwidth usage.
\end{itemize}

By keeping input activations stationary, this strategy minimizes
redundant memory accesses to input data, significantly reducing external
memory bandwidth requirements. This is particularly beneficial in
transformer architectures, where each token in an input sequence is used
across multiple attention heads and layers. Additionally, in batch
processing scenarios, keeping input activations in local memory improves
data locality, making it well-suited for fully connected layers and
matrix multiplications.

However, while Input Stationary reduces memory traffic for activations,
it introduces trade-offs in weight and output movement. Since weights
must be streamed dynamically while inputs remain fixed, the efficiency
of this approach depends on how well weights can be delivered to the
computational units without causing stalls. Additionally, partial sums
must be accumulated efficiently before being written back to memory,
which may require additional buffering mechanisms.

The Input Stationary strategy is most effective for workloads where
input activations exhibit high reuse, and memory bandwidth for inputs is
a critical constraint. It is commonly employed in transformers,
recurrent networks, and batch processing workloads, where structured
input reuse leads to significant performance improvements. However, for
models where output accumulation is more critical, alternative dataflow
strategies, such as Output Stationary, may provide better trade-offs.

\subsubsection{Memory-Efficient Tensor
Layouts}\label{sec-ai-acceleration-memoryefficient-tensor-layouts-e250}

Efficient execution of machine learning workloads depends not only on
how data moves (dataflow strategies) but also on how data is stored and
accessed in memory. Tensor layouts, which refers to the arrangement of
multidimensional data in memory, can significantly impact memory access
efficiency, cache performance, and computational throughput. Poorly
chosen layouts can lead to excessive memory stalls, inefficient cache
usage, and increased data movement costs.

In AI accelerators, tensor layout optimization is particularly important
because data is frequently accessed in patterns dictated by the
underlying hardware architecture. Choosing the right layout ensures that
memory accesses align with hardware-friendly access patterns, minimizing
overhead from costly memory transactions
(\citeproc{ref-nvidia2021cudnn}{N. Corporation 2021}).

While developers can sometimes manually specify tensor layouts, the
choice is often determined automatically by machine learning frameworks
(e.g., TensorFlow, PyTorch, JAX), compilers, or AI accelerator runtimes.
Low-level optimization tools such as cuDNN (for NVIDIA GPUs), XLA (for
TPUs), and MLIR (for custom accelerators) may rearrange tensor layouts
dynamically to optimize performance (\citeproc{ref-xla2020}{He 2023a}).
In high-level frameworks, layout transformations are typically applied
transparently, but developers working with custom kernels or low-level
libraries (e.g., CUDA, Metal, or OpenCL) may have direct control over
tensor format selection.

For example, in PyTorch, users can manually modify layouts using
tensor.permute() or tensor.contiguous() to ensure efficient memory
access (\citeproc{ref-paszke2019pytorch}{Paszke et al. 2019}). In
TensorFlow, layout optimizations are often applied internally by the XLA
compiler, choosing between NHWC (row-major) and NCHW (channel-major)
based on the target hardware (\citeproc{ref-tensorflow2022}{Brain
2022}). Hardware-aware machine learning libraries, such as cuDNN for
GPUs or OneDNN for CPUs, enforce specific memory layouts to maximize
cache locality and SIMD efficiency. Ultimately, while developers may
have some control over tensor layout selection, most layout decisions
are driven by the compiler and runtime system, ensuring that tensors are
stored in memory in a way that best suits the underlying hardware.

\paragraph{Row-Major
Layout}\label{sec-ai-acceleration-rowmajor-layout-741f}

Row-major layout refers to the way multi-dimensional tensors are stored
in memory, where elements are arranged row by row, ensuring that all
values in a given row are placed contiguously before moving to the next
row. This storage format is widely used in general-purpose CPUs and some
machine learning frameworks because it aligns naturally with sequential
memory access patterns, making it more cache-efficient for certain types
of operations (\citeproc{ref-oneDNN2021}{Intel 2021}).

To understand how row-major layout works, consider a single RGB image
represented as a tensor of shape (Height, Width, Channels). If the image
has a size of \(3\times 3\) pixels with 3 channels (RGB), the
corresponding tensor is structured as (3, 3, 3). The values are stored
in memory as follows: \begin{gather*}
I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0), I(0,1,1), \\
I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \ldots
\end{gather*}

Each row is stored contiguously, meaning all pixel values in the first
row are placed sequentially in memory before moving on to the second
row. This ordering is advantageous because CPUs and cache hierarchies
are optimized for sequential memory access. When data is accessed in a
row-wise fashion, such as when applying element-wise operations like
activation functions or basic arithmetic transformations, memory fetches
are efficient, and cache utilization is maximized
(\citeproc{ref-sodani2017knl}{Sodani 2015}).

The efficiency of row-major storage becomes particularly evident in
CPU-based machine learning workloads, where operations such as batch
normalization, matrix multiplications, and element-wise arithmetic
frequently process rows of data sequentially. Since modern CPUs employ
cache prefetching mechanisms, a row-major layout allows the next
required data values to be preloaded into cache ahead of execution,
reducing memory latency and improving overall computational throughput.

However, row-major layout can introduce inefficiencies when performing
operations that require accessing data across channels rather than
across rows. Consider a convolutional layer that applies a filter across
multiple channels of an input image. Since channel values are
interleaved in row-major storage, the convolution operation must jump
across memory locations to fetch all the necessary channel values for a
given pixel. These strided memory accesses can be costly on hardware
architectures that rely on vectorized execution and coalesced memory
access, such as GPUs and TPUs.

Despite these limitations, row-major layout remains a dominant storage
format in CPU-based machine learning frameworks. TensorFlow, for
instance, defaults to the NHWC (row-major) format on CPUs, ensuring that
cache locality is optimized for sequential processing. However, when
targeting GPUs, frameworks often rearrange data dynamically to take
advantage of more efficient memory layouts, such as channel-major
storage, which aligns better with parallelized computation.

\paragraph{Channel-Major
Layout}\label{sec-ai-acceleration-channelmajor-layout-d6a9}

In contrast to row-major layout, channel-major layout arranges data in
memory such that all values for a given channel are stored together
before moving to the next channel. This format is particularly
beneficial for GPUs, TPUs, and other AI accelerators, where vectorized
operations and memory coalescing significantly impact computational
efficiency.

To understand how channel-major layout works, consider the same RGB
image tensor of size (Height, Width, Channels) = (3, 3, 3). Instead of
storing pixel values row by row, the data is structured channel-first in
memory as follows: \begin{gather*}
I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0), I(1,1,0), I(2,1,0), \ldots, \\
I(0,0,1), I(1,0,1), I(2,0,1), \ldots, I(0,0,2), I(1,0,2), I(2,0,2), \ldots
\end{gather*}

In this format, all red channel values for the entire image are stored
first, followed by all green values, and then all blue values. This
ordering allows hardware accelerators to efficiently load and process
data across channels in parallel, which is essential for convolution
operations and SIMD (Single Instruction, Multiple Data) execution models
(\citeproc{ref-chetlur2014cudnn}{Chetlur et al. 2014}).

The advantage of channel-major layout becomes clear when performing
convolutions in machine learning models. Convolutional layers process
images by applying a shared set of filters across all channels. When the
data is stored in a channel-major format, a convolution kernel can load
an entire channel efficiently, reducing the number of scattered memory
fetches. This reduces memory latency, improves throughput, and enhances
data locality for matrix multiplications, which are fundamental to
machine learning workloads.

Because GPUs and TPUs rely on memory coalescing\sidenote{\textbf{Memory
Coalescing}: From Latin ``coalescere'' (to grow together), describing
how separate things merge into one. In GPU architecture, coalescing
combines multiple memory requests from threads in a warp into a single
efficient transaction when threads access consecutive addresses.
Uncoalesced access (scattered addresses) reduces bandwidth by 10-20x,
making tensor layouts and data organization crucial for GPU performance.
}, a technique in which consecutive threads fetch contiguous memory
addresses, channel-major layout aligns naturally with the way these
processors execute parallel computations. For example, in NVIDIA GPUs,
each thread in a warp (a group of threads executed simultaneously)
processes different elements of the same channel, ensuring that memory
accesses are efficient and reducing the likelihood of strided memory
accesses, which can degrade performance.

Despite its advantages in machine learning accelerators, channel-major
layout can introduce inefficiencies when running on general-purpose
CPUs. Since CPUs optimize for sequential memory access, storing all
values for a single channel before moving to the next disrupts cache
locality for row-wise operations. This is why many machine learning
frameworks (e.g., TensorFlow, PyTorch) default to row-major (NHWC) on
CPUs and channel-major (NCHW) on GPUs---optimizing for the strengths of
each hardware type.

Modern AI frameworks and compilers often transform tensor layouts
dynamically depending on the execution environment. For instance,
TensorFlow and PyTorch automatically switch between
NHWC\sidenote{\textbf{NHWC vs NCHW}: Tensor layout formats where letters
indicate dimension order: N(batch), H(height), W(width), C(channels).
NHWC stores data row-by-row with channels interleaved (CPU-friendly),
while NCHW groups all values for each channel together (GPU-friendly). A
224×224 RGB image in NHWC stores as {[}R1,G1,B1,R2,G2,B2,\ldots{]} while
NCHW stores as {[}R1,R2,\ldots,G1,G2,\ldots,B1,B2,\ldots{]}. This
seemingly minor difference can impact performance by 2-5\(\times\)
depending on hardware. } and NCHW based on whether a model is running on
a CPU, GPU, or TPU, ensuring that the memory layout aligns with the most
efficient execution path.

\paragraph{Comparing Row-Major and Channel-Major
Layouts}\label{sec-ai-acceleration-comparing-rowmajor-channelmajor-layouts-e410}

Both row-major (NHWC) and channel-major (NCHW) layouts serve distinct
purposes in machine learning workloads, with their efficiency largely
determined by the hardware architecture, memory access patterns, and
computational requirements. The choice of layout directly influences
cache utilization, memory bandwidth efficiency, and processing
throughput. Table~\ref{tbl-major} contrasts the performance trade-offs
and hardware compatibility between these two approaches.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2041}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3878}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4014}}@{}}
\caption{\textbf{Data Layout Strategies}: Row-major (NHWC) and
channel-major (NCHW) layouts optimize memory access patterns for
different hardware architectures; NHWC suits cpus and element-wise
operations, while NCHW accelerates GPU and TPU-based convolution
operations. Choosing the appropriate layout significantly impacts
performance by maximizing cache utilization and memory bandwidth
efficiency.}\label{tbl-major}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Row-Major (NHWC)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Channel-Major (NCHW)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Row-Major (NHWC)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Channel-Major (NCHW)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory Storage Order} & Pixels are stored row-by-row, channel
interleaved & All values for a given channel are stored together
first \\
\textbf{Best for} & CPUs, element-wise operations & GPUs, TPUs,
convolution operations \\
\textbf{Cache Efficiency} & High cache locality for sequential row
access & Optimized for memory coalescing across channels \\
\textbf{Convolution Performance} & Requires strided memory accesses
(inefficient on GPUs) & Efficient for GPU convolution kernels \\
\textbf{Memory Fetching} & Good for operations that process rows
sequentially & Optimized for SIMD execution across channels \\
\textbf{Default in Frameworks} & Default on CPUs (e.g., TensorFlow NHWC)
& Default on GPUs (e.g., cuDNN prefers NCHW) \\
\end{longtable}

The decision to use row-major (NHWC) or channel-major (NCHW) layouts is
not always made manually by developers. Instead, machine learning
frameworks and AI compilers often determine the optimal layout
dynamically based on the target hardware and operation type. CPUs tend
to favor NHWC due to cache-friendly sequential memory access, while GPUs
perform better with NCHW, which reduces memory fetch overhead for
machine learning computations.

In practice, modern AI compilers such as TensorFlow's XLA and PyTorch's
TorchScript perform automatic layout transformations, converting tensors
between NHWC and NCHW as needed to optimize performance across different
processing units. This ensures that machine learning models achieve the
highest possible throughput without requiring developers to manually
specify tensor layouts.

\subsubsection{Kernel
Fusion}\label{sec-ai-acceleration-kernel-fusion-7faf}

One of the most impactful optimization techniques in AI acceleration
involves reducing the overhead of intermediate data movement between
operations. We examine how kernel fusion\sidenote{\textbf{Kernel}: From
Old English ``cyrnel'' (seed, grain), the innermost essential part. In
operating systems, the kernel is the core that manages hardware
resources. GPU computing borrowed this term for the fundamental unit of
parallel execution launched on the device. In ML, a ``kernel'' is a
compiled function that executes on an accelerator, with ``kernel
fusion'' combining multiple such functions into one to eliminate
intermediate memory traffic. } transforms multiple separate computations
into unified operations, dramatically improving memory efficiency and
execution performance. We first analyze the memory bottlenecks created
by intermediate writes, then explore how fusion techniques eliminate
these inefficiencies.

\paragraph{Intermediate Memory
Write}\label{sec-ai-acceleration-intermediate-memory-write-f140}

Optimizing memory access is a fundamental challenge in AI acceleration.
While AI models rely on high-throughput computation, their performance
is often constrained by memory bandwidth and intermediate memory writes
rather than pure arithmetic operations. Every time an operation produces
an intermediate result that must be written to memory and later read
back, execution stalls occur due to data movement overhead.

Building on software optimization techniques from
\textbf{?@sec-model-compression} and memory bandwidth constraints
established in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
kernel fusion represents the critical bridge between software
optimization and hardware acceleration. Many AI workloads introduce
unnecessary intermediate memory writes, leading to increased memory
bandwidth consumption and reduced execution efficiency \emph{Source: N.
Corporation (\citeproc{ref-nvidia2017gpu}{2017})}.

Listing~\ref{lst-naive_execution} reveals how each operation becomes a
separate kernel in a naïve execution model, forcing intermediate results
to be written to memory and then read back for the next operation.

\begin{codelisting}

\caption{\label{lst-naive_execution}\textbf{Naïve Execution}: Each step
writes intermediate results to memory before processing the next,
leading to increased bandwidth usage and reduced efficiency.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\CommentTok{\#\# Input tensor \{\#sec{-}ai{-}acceleration{-}input{-}tensor{-}67da\}}
\NormalTok{X }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1024}\NormalTok{, }\DecValTok{1024}\NormalTok{).cuda()}

\CommentTok{\#\# Step{-}by{-}step execution (naïve approach) \{\#sec{-}ai{-}acceleration{-}stepbystep{-}execution{-}naïve{-}approach{-}2bf8\}}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ torch.relu(X)  }\CommentTok{\# Intermediate tensor stored}
\CommentTok{\# in memory \{\#sec{-}memory\}}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ torch.batch\_norm(X1)  }\CommentTok{\# Another intermediate tensor stored}
\NormalTok{Y }\OperatorTok{=} \FloatTok{2.0} \OperatorTok{*}\NormalTok{ X2 }\OperatorTok{+} \FloatTok{1.0}  \CommentTok{\# Final result}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each operation produces an intermediate tensor that must be written to
memory and retrieved for the next operation. On large tensors, this
overhead of moving data can outweigh the computational cost of the
operations (\citeproc{ref-shazeer2018mesh}{Shazeer et al. 2018}).
Table~\ref{tbl-memory-footprint} illustrates the memory overhead in a
naïve execution model. While only the final result \(Y\) is needed,
storing multiple intermediate tensors creates unnecessary memory traffic
and inefficient memory usage. This data movement bottleneck
significantly impacts performance, making memory optimization crucial
for AI accelerators.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2639}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6389}}@{}}
\caption{\textbf{Intermediate Tensor Storage}: Naïve execution models
require substantial memory to store intermediate tensors generated by
each operation; for a 1024x1024 tensor, this table shows that storing
these intermediate results (even if only the final output is needed)
quadruples the total memory footprint from 4 MB to 16 MB. Minimizing
this intermediate data storage is essential for improving memory
efficiency and accelerating AI
computations.}\label{tbl-memory-footprint}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tensor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Size (MB) for 1024 \(\times\) 1024 Tensor}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tensor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Size (MB) for 1024 \(\times\) 1024 Tensor}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{X} & 4 MB \\
\textbf{X'} & 4 MB \\
\textbf{X'\,'} & 4 MB \\
\textbf{Y} & 4 MB \\
\textbf{Total Memory} & 16 MB \\
\end{longtable}

Even though only the final result \(Y\) is needed, three additional
intermediate tensors consume extra memory without contributing to final
output storage. This excessive memory usage limits scalability and
wastes memory bandwidth, particularly in AI accelerators where
minimizing data movement is critical.

\paragraph{Kernel Fusion for Memory
Efficiency}\label{sec-ai-acceleration-kernel-fusion-memory-efficiency-f227}

Kernel fusion is a key optimization technique that aims to minimize
intermediate memory writes, reducing the memory footprint and bandwidth
consumption of machine learning workloads
(\citeproc{ref-jia2018beyond}{Zhihao Jia, Zaharia, and Aiken 2018}).

Kernel fusion involves merging multiple computation steps into a single,
optimized operation, eliminating the need for storing and reloading
intermediate tensors. Instead of executing each layer or element-wise
operation separately, in which each step writes its output to memory
before the next step begins, fusion enables direct data propagation
between operations, keeping computations within high-speed registers or
local memory.

A common machine learning sequence might involve applying a nonlinear
activation function (e.g., ReLU), followed by batch normalization, and
then scaling the values for input to the next layer. In a naïve
implementation, each of these steps generates an intermediate tensor,
which is written to memory, read back, and then modified again: \[
X' = \text{ReLU}(X)
X'' = \text{BatchNorm}(X')
Y = \alpha \cdot X'' + \beta
\]

With kernel fusion, these operations are combined into a single
computation step, allowing the entire transformation to occur without
generating unnecessary intermediate tensors: \[
Y = \alpha \cdot \text{BatchNorm}\big(\text{ReLU}(X)\big) + \beta
\]

Table~\ref{tbl-fusion-benefits} highlights the impact of operation
fusion on memory efficiency. By keeping intermediate results in
registers or local memory rather than writing them to main memory,
fusion significantly reduces memory traffic. This optimization is
especially beneficial on highly parallel architectures like GPUs and
TPUs, where minimizing memory accesses translates directly into improved
execution throughput. Compared to the naïve execution model, fused
execution eliminates the need for storing intermediate tensors,
dramatically lowering the total memory footprint and improving overall
efficiency.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2529}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3908}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3448}}@{}}
\caption{\textbf{Operation Fusion Benefits}: Fused execution reduces
memory usage by eliminating the need to store intermediate tensors,
directly improving efficiency on memory-bound hardware like gpus and
tpus. This table quantifies the memory savings, showing a reduction from
16 MB in naïve execution to 4 MB with fused
operations.}\label{tbl-fusion-benefits}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Execution Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Intermediate Tensors Stored}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Total Memory Usage (MB)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Execution Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Intermediate Tensors Stored}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Total Memory Usage (MB)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Naïve Execution} & X', X'\,' & 16 MB \\
\textbf{Fused Execution} & None & 4 MB \\
\end{longtable}

\paragraph{Performance Benefits and
Constraints}\label{sec-ai-acceleration-performance-benefits-constraints-1b74}

Kernel fusion brings several key advantages that enhance memory
efficiency and computation throughput. By reducing memory accesses,
fused kernels ensure that intermediate values stay within registers
instead of being repeatedly written to and read from memory. This
significantly lowers memory traffic, which is one of the primary
bottlenecks in machine learning workloads. GPUs and TPUs, in particular,
benefit from kernel fusion because high-bandwidth memory is a scarce
resource, and reducing memory transactions leads to better utilization
of compute units (\citeproc{ref-nvidia2020ampere}{N. Corporation 2020}).

However, not all operations can be fused. Element-wise operations, such
as ReLU, batch normalization, and simple arithmetic transformations, are
ideal candidates for fusion since their computations depend only on
single elements from the input tensor. In contrast, operations with
complex data dependencies, such as matrix multiplications and
convolutions, involve global data movement, making direct fusion
impractical. These operations require values from multiple input
elements to compute a single output, which prevents them from being
executed as a single fused kernel.

Another major consideration is register pressure. Fusing multiple
operations means all temporary values must be kept in registers rather
than memory. While this eliminates redundant memory writes, it also
increases register demand. If a fused kernel exceeds the available
registers per thread, the system must spill excess values into shared
memory, introducing additional latency and potentially negating the
benefits of fusion. On GPUs, where thread occupancy (the number of
threads that can run in parallel) is limited by available registers,
excessive fusion can reduce parallelism, leading to diminishing returns.

Different AI accelerators and compilers handle fusion in distinct ways.
NVIDIA GPUs, for example, favor warp-level parallelism, where
element-wise fusion is straightforward. TPUs, on the other hand,
prioritize systolic array execution, which is optimized for
matrix-matrix operations rather than element-wise fusion
(\citeproc{ref-nvidia2020ampere}{N. Corporation 2020}). AI compilers
such as XLA (TensorFlow), TorchScript (PyTorch), TensorRT (NVIDIA), and
MLIR automatically detect fusion opportunities and apply heuristics to
balance memory savings and execution efficiency
(\citeproc{ref-xla2021}{He 2023b}).

Despite its advantages, fusion is not always beneficial. Some AI
frameworks allow developers to disable fusion selectively, especially
when debugging performance issues or making frequent model
modifications. The decision to fuse operations must consider trade-offs
between memory efficiency, register usage, and hardware execution
constraints to ensure that fusion leads to tangible performance
improvements.

\phantomsection\label{callout-checkpointux2a-1.13}
\begin{fbx}{callout-checkpoint}{Checkpoint: }{Synthesis: Data Movement and Kernel Fusion}
\phantomsection\label{callout-checkpoint*-1.13}
At this point, you should be able to answer the first two questions from
the roadmap:

\textbf{Which data stays local?} The weight-stationary,
output-stationary, and input-stationary patterns each make a principled
choice about which data to cache near compute units. Weight-stationary
(used in Google's TPU) maximizes weight reuse for CNN workloads.
Output-stationary (used in NVIDIA's tensor cores) reduces partial sum
memory traffic for fully connected layers. Input-stationary minimizes
input reloads for models with shared inputs across multiple filters.

\textbf{How are operations combined?} Kernel fusion eliminates
intermediate memory writes by merging consecutive operations (Conv2D +
BatchNorm + ReLU becomes a single kernel). This optimization is most
effective for element-wise operations that share data dependencies and
can achieve 2-10x speedups by avoiding round-trips to DRAM.

The remaining question, \emph{how is data organized}, brings us to
tiling: the technique of partitioning computations into memory-friendly
blocks. Tiling complements the stationary strategies by ensuring that
whichever data we choose to keep local actually fits in fast memory.

\end{fbx}

\subsubsection{Memory-Efficient Tiling
Strategies}\label{sec-ai-acceleration-memoryefficient-tiling-strategies-9fce}

While modern AI accelerators offer high computational throughput, their
performance is often limited by memory bandwidth rather than raw
processing power. If data cannot be supplied to processing units fast
enough, execution stalls occur, leading to wasted cycles and inefficient
hardware utilization.

Tiling\sidenote{\textbf{Tiling}: Borrowed from floor or mosaic tiling,
where a large surface is covered by repeating smaller pieces. In
computing, Monica Lam popularized the term in her 1991 PhD thesis on
cache optimization. Just as physical tiles tessellate to cover a floor,
computational tiles partition large matrices into blocks that fit in
fast memory. The technique is also called ``blocking'' or ``loop
blocking'' in compiler literature. By doing so, tiling increases data
reuse, minimizes memory fetches, and improves overall computational
efficiency. } is a technique used to mitigate this issue by
restructuring computations into smaller, memory-friendly subproblems.
Instead of processing entire matrices or tensors at once, which leads to
excessive memory traffic, tiling partitions computations into smaller
blocks (tiles) that fit within fast local memory (e.g., caches, shared
memory, or registers) (\citeproc{ref-lam1991cache}{Lam, Rothberg, and
Wolf 1991}).

Matrix multiplication, widely used in AI models, demonstrates
inefficient memory access when implemented naively.
Listing~\ref{lst-naive_matmul} shows how, without tiling, repeated
memory accesses for the same data lead to unnecessary bandwidth
consumption.

\begin{codelisting}

\caption{\label{lst-naive_matmul}\textbf{Naïve Matrix Multiplication}:
Direct implementation without tiling requires O(N\^{}3) memory accesses
for N×N matrices, repeatedly fetching the same elements from slow DRAM
memory and limiting performance to a fraction of theoretical peak
throughput.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
\NormalTok{            C[i, j] }\OperatorTok{+=}\NormalTok{ A[i, k] }\OperatorTok{*}\NormalTok{ B[k, j]  }\CommentTok{\# Repeatedly fetching}
            \CommentTok{\# A[i, k] and B[k, j]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each iteration requires loading elements from matrices \(A\) and \(B\)
multiple times from memory, causing excessive data movement. As the size
of the matrices increases, the memory bottleneck worsens, limiting
performance.

Tiling addresses this problem by ensuring that smaller portions of
matrices are loaded into fast memory, reused efficiently, and only
written back to main memory when necessary. This technique is especially
crucial in AI accelerators, where memory accesses dominate execution
time. Figure~\ref{fig-tiling-diagram} visualizes how breaking large
matrices into smaller tiles enables computation to proceed efficiently
by maximizing data reuse in fast memory. The following sections examine
the fundamental principles of tiling, its different strategies, and the
key trade-offs involved in selecting an effective tiling approach.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a7a6143de1f22f1b3e3a65635f6df5a1c2c80df9.pdf}}

}

\caption{\label{fig-tiling-diagram}\textbf{Matrix Tiling}: Partitioning
large matrices into smaller tiles optimizes data reuse and reduces
memory access overhead during computation. This technique improves
performance on AI accelerators by enabling efficient loading and
processing of data in fast memory, minimizing transfers from slower main
memory.}

\end{figure}%

\paragraph{Tiling
Fundamentals}\label{sec-ai-acceleration-tiling-fundamentals-e9e6}

Tiling is based on a simple but powerful principle: instead of operating
on an entire data structure at once, computations are divided into
smaller tiles that fit within the available fast memory. By structuring
execution around these tiles, data reuse is maximized, reducing
redundant memory accesses and improving overall efficiency.

Consider matrix multiplication, a key operation in machine learning
workloads. The operation computes \(C = A \times B\) where each element
\(C[i,j] = \sum_{k} A[i,k] \times B[k,j]\). The naive implementation
shown earlier in Listing~\ref{lst-naive_matmul} demonstrates the core
problem: every iteration of the innermost loop fetches elements from
matrices \(A\) and \(B\) from memory, performs a multiplication, and
updates matrix \(C\). Because matrices are large, the processor
repeatedly reloads the same values from memory, even though they were
just used in previous computations.

This data movement overhead is expensive: fetching from DRAM is
100-1000x slower than accessing on-chip cache or registers. The solution
is tiling.

\paragraph{Performance Benefits of
Tiling}\label{sec-ai-acceleration-performance-benefits-tiling-e7bd}

Instead of computing one element at a time and constantly moving data in
and out of slow memory, tiling processes submatrices (tiles) at a time,
keeping frequently used values in fast memory. The idea is to divide the
matrices into smaller blocks that fit within the processor's cache or
shared memory, ensuring that once a block is loaded, it is reused
multiple times before moving to the next one.

Listing~\ref{lst-tiled_matmul} demonstrates how processing blocks of
data improves memory locality by ensuring frequently used values remain
in fast memory.

\begin{codelisting}

\caption{\label{lst-tiled_matmul}\textbf{Tiled Matrix Multiplication}:
This approach divides matrices into smaller blocks to optimize memory
usage by reusing data within processor cache, thereby improving
computational efficiency.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TILE\_SIZE }\OperatorTok{=} \DecValTok{32}  \CommentTok{\# Choose a tile size based on}
\CommentTok{\# hardware constraints \{\#sec{-}hardware{-}constraints\}}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
            \CommentTok{\# Compute the submatrix}
            \CommentTok{\# C[i:i+TILE\_SIZE, j:j+TILE\_SIZE]}
            \ControlFlowTok{for}\NormalTok{ ii }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, i }\OperatorTok{+}\NormalTok{ TILE\_SIZE):}
                \ControlFlowTok{for}\NormalTok{ jj }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(j, j }\OperatorTok{+}\NormalTok{ TILE\_SIZE):}
                    \ControlFlowTok{for}\NormalTok{ kk }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k, k }\OperatorTok{+}\NormalTok{ TILE\_SIZE):}
\NormalTok{                        C[ii, jj] }\OperatorTok{+=}\NormalTok{ A[ii, kk] }\OperatorTok{*}\NormalTok{ B[kk, jj]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This restructuring significantly improves performance for three main
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Better Memory Reuse}: Instead of fetching elements from \(A\)
  and \(B\) repeatedly from slow memory, this approach loads a small
  tile of data into fast memory, performs multiple computations using
  it, and only then moves on to the next tile. This minimizes redundant
  memory accesses.
\item
  \textbf{Reduced Memory Bandwidth Usage}: Since each tile is used
  multiple times before being evicted, memory traffic is reduced.
  Instead of repeatedly accessing DRAM, most required data is available
  in L1/L2 cache or shared memory, leading to faster execution.
\item
  \textbf{Increased Compute Efficiency}: Processors spend less time
  waiting for data and more time performing useful computations. In
  architectures like GPUs and TPUs, where thousands of parallel
  processing units operate simultaneously, tiling ensures that data is
  read and processed in a structured manner, avoiding unnecessary
  stalls.
\end{enumerate}

This technique is particularly effective in AI accelerators, where
machine learning workloads consist of large matrix multiplications and
tensor transformations. Without tiling, these workloads quickly become
memory-bound, meaning performance is constrained by how fast data can be
retrieved rather than by the raw computational power of the processor.

\paragraph{Tiling
Methods}\label{sec-ai-acceleration-tiling-methods-6257}

While the general principle of tiling remains the same, which involves
partitioning large computations into smaller subproblems to improve
memory reuse, there are different ways to apply tiling based on the
structure of the computation and hardware constraints. The two primary
tiling strategies are spatial tiling and temporal tiling. These
strategies optimize different aspects of computation and memory access,
and in practice, they are often combined to achieve the best
performance.

\subparagraph{Spatial
Tiling}\label{sec-ai-acceleration-spatial-tiling-247e}

Spatial tiling focuses on partitioning data structures into smaller
blocks that fit within fast memory. The tiled matrix multiplication in
Listing~\ref{lst-tiled_matmul} demonstrates this approach: each tile of
\(A\) and \(B\) is loaded into cache or shared memory before processing,
ensuring that the same data does not need to be fetched repeatedly from
slower memory. The tile is fully used before moving to the next block,
minimizing redundant memory accesses.

Spatial tiling is particularly beneficial for large tensors that exceed
fast memory capacity. By breaking computations into smaller tiles, data
movement between memory levels is minimized, keeping operations
localized within cache hierarchies.

\subparagraph{Temporal
Tiling}\label{sec-ai-acceleration-temporal-tiling-563b}

While spatial tiling optimizes how data is partitioned, temporal tiling
focuses on reorganizing the computation itself to improve data reuse
over time. Many machine learning workloads involve operations where the
same data is accessed repeatedly across multiple iterations. Without
temporal tiling, this often results in redundant memory fetches, leading
to inefficiencies. Temporal tiling, also known as loop blocking,
restructures the computation to ensure that frequently used data stays
in fast memory for as long as possible before moving on to the next
computation.

A classic example where temporal tiling is beneficial is convolutional
operations, where the same set of weights is applied to multiple input
regions. Without loop blocking, these weights might be loaded from
memory multiple times for each computation. With temporal tiling, the
computation is reordered so that the weights remain in fast memory
across multiple inputs, reducing unnecessary memory fetches and
improving overall efficiency.

Listing~\ref{lst-loop_blocking} illustrates how loop blocking
restructures computation to keep weights in fast memory across multiple
inputs, reducing redundant fetches.

\begin{codelisting}

\caption{\label{lst-loop_blocking}\textbf{Temporal Tiling}: Reduces
redundant memory accesses by caching weights in fast memory across
multiple matrix multiplications.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
            \CommentTok{\# Load tile into fast memory before computation}
\NormalTok{            A\_tile }\OperatorTok{=}\NormalTok{ A[i:i}\OperatorTok{+}\NormalTok{TILE\_SIZE, k:k}\OperatorTok{+}\NormalTok{TILE\_SIZE]}
\NormalTok{            B\_tile }\OperatorTok{=}\NormalTok{ B[k:k}\OperatorTok{+}\NormalTok{TILE\_SIZE, j:j}\OperatorTok{+}\NormalTok{TILE\_SIZE]}

            \ControlFlowTok{for}\NormalTok{ ii }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(TILE\_SIZE):}
                \ControlFlowTok{for}\NormalTok{ jj }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(TILE\_SIZE):}
                    \ControlFlowTok{for}\NormalTok{ kk }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(TILE\_SIZE):}
\NormalTok{                        C[i}\OperatorTok{+}\NormalTok{ii, j}\OperatorTok{+}\NormalTok{jj] }\OperatorTok{+=}\NormalTok{ A\_tile[ii, kk] }\OperatorTok{*}
\NormalTok{                                         B\_tile[kk, jj]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Temporal tiling improves performance by ensuring that the data loaded
into fast memory is used multiple times before being evicted. In this
implementation, small tiles of matrices \(A\) and \(B\) are explicitly
loaded into temporary storage before performing computations, reducing
memory fetch overhead. This restructuring allows the computation to
process an entire tile before moving to the next, thereby reducing the
number of times data must be loaded from slower memory.

This technique is particularly useful in workloads where certain values
are used repeatedly, such as convolutions, recurrent neural networks
(RNNs), and self-attention mechanisms in transformers. By applying loop
blocking, AI accelerators can significantly reduce memory stalls and
improve execution throughput.

\paragraph{Tiling Challenges and
Trade-offs}\label{sec-ai-acceleration-tiling-challenges-tradeoffs-e9c9}

While tiling significantly improves performance by optimizing memory
reuse and reducing redundant memory accesses, it introduces several
challenges and trade-offs. Selecting the right tile size is important,
as it directly affects computational efficiency and memory bandwidth
usage. If the tile size is too small, the benefits of tiling diminish,
as memory fetches still dominate execution time. On the other hand, if
the tile size is too large, it may exceed the available fast memory,
causing cache thrashing and performance degradation.

Load balancing is another key concern. In architectures such as GPUs and
TPUs, computations are executed in parallel across thousands of
processing units. If tiles are not evenly distributed, some units may
remain idle while others are overloaded, leading to suboptimal
utilization of computational resources. Effective tile scheduling
ensures that parallel execution remains balanced and efficient.

Data movement overhead is also an important consideration. Although
tiling reduces the number of slow memory accesses, transferring tiles
between different levels of memory still incurs a cost. This is
especially relevant in hierarchical memory systems, where accessing data
from cache is much faster than accessing it from DRAM. Efficient memory
prefetching and scheduling strategies are required to minimize latency
and ensure that data is available when needed.

Beyond spatial and temporal tiling, hybrid approaches combine elements
of both strategies to achieve optimal performance. Hybrid tiling adapts
to workload-specific constraints by dynamically adjusting tile sizes or
reordering computations based on real-time execution conditions. For
example, some AI accelerators use spatial tiling for matrix
multiplications while employing temporal tiling for weight reuse in
convolutional layers.

Other methods exist for optimizing memory usage and computational
efficiency beyond tiling. Techniques such as register blocking, double
buffering, and hierarchical tiling extend the basic tiling principles to
further optimize execution. AI compilers and runtime systems, such as
TensorFlow XLA, TVM, and MLIR, automatically select tiling strategies
based on hardware constraints, enabling fine-tuned performance
optimization without manual intervention.

Table~\ref{tbl-tiling-strategies} provides a comparative overview of
spatial, temporal, and hybrid tiling approaches, highlighting their
respective benefits and trade-offs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0969}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3605}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2829}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2519}}@{}}
\caption{\textbf{Tiling Strategies}: Spatial, temporal, and hybrid
tiling optimize memory access patterns for improved performance; spatial
tiling maximizes data reuse within fast memory, temporal tiling exploits
loop structure for reduced accesses, and hybrid tiling combines both
approaches to balance computational efficiency and memory bandwidth.
These techniques are essential for AI compilers and runtime systems to
automatically optimize model execution on diverse
hardware.}\label{tbl-tiling-strategies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Spatial Tiling (Data Tiling)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Temporal Tiling (Loop Blocking)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hybrid Tiling}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Spatial Tiling (Data Tiling)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Temporal Tiling (Loop Blocking)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hybrid Tiling}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Primary Goal} & Reduce memory accesses by keeping data in fast
memory longer & Increase data reuse across loop iterations & Adapt
dynamically to workload constraints \\
\textbf{Optimization Focus} & Partitioning data structures into smaller,
memory-friendly blocks & Reordering computations to maximize reuse
before eviction & Balancing spatial and temporal reuse strategies \\
\textbf{Memory Usage} & Improves cache locality and reduces DRAM access
& Keeps frequently used data in fast memory for multiple iterations &
Minimizes data movement while ensuring high reuse \\
\textbf{Common Use Cases} & Matrix multiplications, CNNs, self-attention
in transformers & Convolutions, recurrent neural networks (RNNs),
iterative computations & AI accelerators with hierarchical memory, mixed
workloads \\
\textbf{Performance Gains} & Reduced memory bandwidth requirements,
better cache utilization & Lower memory fetch latency, improved data
locality & Maximized efficiency across multiple hardware types \\
\textbf{Challenges} & Requires careful tile size selection, inefficient
for workloads with minimal spatial reuse & Can increase register
pressure, requires loop restructuring & Complexity in tuning tile size
and execution order dynamically \\
\textbf{Best When} & Data is large and needs to be partitioned for
efficient processing & The same data is accessed multiple times across
iterations & Both data partitioning and iteration-based reuse are
important \\
\end{longtable}

As machine learning models continue to grow in size and complexity,
tiling remains a critical tool for improving hardware efficiency,
ensuring that AI accelerators operate at their full potential. While
manual tiling strategies can provide substantial benefits, modern
compilers and hardware-aware optimization techniques further enhance
performance by automatically selecting the most effective tiling
strategies for a given workload.

\subsection{Applying Mapping Strategies to Neural
Networks}\label{sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110}

While these foundational mapping techniques apply broadly, their
effectiveness varies based on the computational structure, data access
patterns, and parallelization opportunities of different neural network
architectures. Each architecture imposes distinct constraints on data
movement, memory hierarchy, and computation scheduling, requiring
tailored mapping strategies to optimize performance.

A structured approach to mapping is essential to address the
combinatorial explosion of choices that arise when assigning
computations to AI accelerators. Rather than treating each model as a
separate optimization problem, we recognize that the same fundamental
principles apply across different architectures---only their priority
shifts based on workload characteristics. The goal is to systematically
select and apply mapping strategies that maximize efficiency for
different types of machine learning models.

These principles apply to three representative AI workloads, each
characterized by distinct computational demands. CNNs benefit from
spatial data reuse, making weight-stationary execution and the
application of tiling techniques especially effective. In contrast,
Transformers are inherently memory-bound and rely on strategies such as
efficient KV-cache management, fused attention mechanisms, and highly
parallel execution to mitigate memory traffic. MLPs, which involve
substantial matrix multiplication operations, demand the use of
structured tiling, optimized weight layouts, and memory-aware execution
to enhance overall performance.

Despite their differences, each of these models follows a common set of
mapping principles, with variations in how optimizations are
prioritized. The following table provides a structured mapping between
different optimization strategies and their suitability for CNNs,
Transformers, and MLPs. This table serves as a roadmap for selecting
appropriate mapping strategies for different machine learning workloads.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1273}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0982}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0873}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.6036}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{CNNs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Transformers}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLPs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Dataflow Strategy} & Weight Stationary & Activation Stationary &
Weight Stationary & CNNs reuse filters across spatial locations;
Transformers reuse activations (KV-cache); MLPs reuse weights across
batches. \\
\textbf{Memory-Aware Tensor Layouts} & NCHW (Channel-Major) & NHWC
(Row-Major) & NHWC & CNNs favor channel-major for convolution
efficiency; Transformers and MLPs prioritize row-major for fast memory
access. \\
\textbf{Kernel Fusion} & Convolution + Activation & Fused Attention &
GEMM Fusion & CNNs optimize convolution+activation fusion; Transformers
fuse attention mechanisms; MLPs benefit from fused matrix
multiplications. \\
\textbf{Tiling for Memory Efficiency} & Spatial Tiling & Temporal Tiling
& Blocked Tiling & CNNs tile along spatial dimensions; Transformers use
loop blocking to improve sequence memory efficiency; MLPs use blocked
tiling for large matrix multiplications. \\
\end{longtable}

This table highlights that each machine learning model benefits from a
different combination of optimization techniques, reinforcing the
importance of tailoring execution strategies to the computational and
memory characteristics of the workload.

We explore how these optimizations apply to each network type,
explaining how CNNs, Transformers, and MLPs use specific mapping
strategies to improve execution efficiency and hardware utilization.

\subsubsection{Convolutional Neural
Networks}\label{sec-ai-acceleration-convolutional-neural-networks-1e47}

CNNs are characterized by their structured spatial computations, where
small filters (or kernels) are repeatedly applied across an input
feature map. This structured weight reuse makes weight stationary
execution the most effective strategy for CNNs. Keeping filter weights
in fast memory while streaming activations ensures that weights do not
need to be repeatedly fetched from slower external memory, significantly
reducing memory bandwidth demands. Since each weight is applied to
multiple spatial locations, weight stationary execution maximizes
arithmetic intensity and minimizes redundant memory transfers.

Memory-aware tensor layouts also play a critical role in CNN execution.
Convolution operations benefit from a channel-major memory format, often
represented as NCHW (batch, channels, height, width). This layout aligns
with the access patterns of convolutions, enabling efficient memory
coalescing on accelerators such as GPUs and TPUs. By storing data in a
format that optimizes cache locality, accelerators can fetch contiguous
memory blocks efficiently, reducing latency and improving throughput.

Kernel fusion is another important optimization for CNNs. In a typical
machine learning pipeline, convolution operations are often followed by
activation functions such as ReLU and batch normalization. Instead of
treating these operations as separate computational steps, fusing them
into a single kernel reduces intermediate memory writes and improves
execution efficiency. This optimization minimizes memory bandwidth
pressure by keeping intermediate values in registers rather than writing
them to memory and fetching them back in subsequent steps.

Given the size of input images and feature maps, tiling is necessary to
ensure that computations fit within fast memory hierarchies. Spatial
tiling, where input feature maps are processed in smaller subregions,
allows for efficient utilization of on-chip memory while avoiding
excessive off-chip memory transfers. This technique ensures that input
activations, weights, and intermediate outputs remain within high-speed
caches or shared memory as long as possible, reducing memory stalls and
improving overall performance.

Together, these optimizations ensure that CNNs make efficient use of
available compute resources by maximizing weight reuse, optimizing
memory access patterns, reducing redundant memory writes, and
structuring computation to fit within fast memory constraints.

\subsubsection{Transformer
Architectures}\label{sec-ai-acceleration-transformer-architectures-8f25}

Unlike CNNs, which rely on structured spatial computations, Transformers
process variable-length sequences and rely heavily on attention
mechanisms. The primary computational bottleneck in Transformers is
memory bandwidth, as attention mechanisms require frequent access to
stored key-value pairs across multiple query vectors. Given this access
pattern, activation stationary execution is the most effective strategy.
By keeping key-value activations in fast memory and streaming query
vectors dynamically, activation reuse is maximized while minimizing
redundant memory fetches. This approach is critical in reducing
bandwidth overhead, especially in long-sequence tasks such as natural
language processing.

Beyond dataflow pattern selection, the physical arrangement of data in
memory equally determines execution efficiency. Memory layout
optimization is particularly important for Transformers. Unlike CNNs,
which benefit from channel-major layouts, Transformers require efficient
access to sequences of activations, making a row-major format (NHWC) the
preferred choice. This layout ensures that activations are accessed
contiguously in memory, reducing cache misses and improving memory
coalescing for matrix multiplications.

Kernel fusion plays a key role in optimizing Transformer execution. In
self-attention, multiple computational steps, such as query-key dot
products, softmax normalization, and weighted summation, can be fused
into a single operation. Fused attention kernels eliminate intermediate
memory writes by computing attention scores and performing weighted
summations within a single execution step. This optimization
significantly reduces memory traffic, particularly for large batch sizes
and long sequences.

Due to the nature of sequence processing, tiling must be adapted to
improve memory efficiency. Instead of spatial tiling, which is effective
for CNNs, Transformers benefit from temporal tiling, where computations
are structured to process sequence blocks efficiently. This method
ensures that activations are loaded into fast memory in manageable
chunks, reducing excessive memory transfers. Temporal tiling is
particularly beneficial for long-sequence models, where the memory
footprint of key-value activations grows significantly. By tiling
sequences into smaller segments, memory locality is improved, enabling
efficient cache utilization and reducing bandwidth pressure.

These optimizations collectively address the primary bottlenecks in
Transformer models by prioritizing activation reuse, structuring memory
layouts for efficient batched computations, fusing attention operations
to reduce intermediate memory writes, and employing tiling techniques
suited to sequence-based processing.

\subsubsection{Multi-Layer
Perceptrons}\label{sec-ai-acceleration-multilayer-perceptrons-eb18}

MLPs primarily consist of fully connected layers, where large matrices
of weights and activations are multiplied to produce output
representations. Given this structure, weight stationary execution is
the most effective strategy for MLPs. Similar to CNNs, MLPs benefit from
keeping weights in local memory while streaming activations dynamically,
as this ensures that weight matrices, which are typically reused across
multiple activations in a batch, do not need to be frequently reloaded.

The preferred memory layout for MLPs aligns with that of Transformers,
as matrix multiplications are more efficient when using a row-major
(NHWC) format. Since activation matrices are processed in batches, this
layout ensures that input activations are accessed efficiently without
introducing memory fragmentation. By aligning tensor storage with
compute-friendly memory access patterns, cache utilization is improved,
reducing memory stalls.

Kernel fusion in MLPs is primarily applied to General Matrix
Multiplication (GEMM)\sidenote{\textbf{GEMM (General Matrix
Multiplication)}: The operation C = αAB + βC that underlies most neural
network computations. The name reflects its role as the ``general'' case
encompassing vector-matrix and matrix-vector products as special cases.
GEMM accounts for 90-95\% of computation time in training deep networks
and is the primary target of AI hardware optimization. Optimized
libraries like cuBLAS (NVIDIA) and oneDNN (Intel) achieve 80-95\% of
theoretical peak through register blocking, vectorization, and
hierarchical tiling. Modern AI accelerators are essentially specialized
GEMM engines. } operations. Since dense layers are often followed by
activation functions and bias additions, fusing these operations into a
single computation step reduces memory traffic. GEMM fusion ensures that
activations, weights, and biases are processed within a single optimized
kernel, avoiding unnecessary memory writes and reloads.

To further improve memory efficiency, MLPs rely on blocked tiling
strategies, where large matrix multiplications are divided into smaller
sub-blocks that fit within the accelerator's shared memory. This method
ensures that frequently accessed portions of matrices remain in fast
memory throughout computation, reducing external memory accesses. By
structuring computations in a way that balances memory utilization with
efficient parallel execution, blocked tiling minimizes bandwidth
limitations and maximizes throughput.

These optimizations ensure that MLPs achieve high computational
efficiency by structuring execution around weight reuse, optimizing
memory layouts for dense matrix operations, reducing redundant memory
writes through kernel fusion, and employing blocked tiling strategies to
maximize on-chip memory utilization.

\subsection{Hybrid Mapping
Strategies}\label{sec-ai-acceleration-hybrid-mapping-strategies-3e8c}

While general mapping strategies provide a structured framework for
optimizing machine learning models, real-world architectures often
involve diverse computational requirements that cannot be effectively
addressed with a single, fixed approach. Hybrid mapping strategies allow
AI accelerators to dynamically apply different optimizations to specific
layers or components within a model, ensuring that each computation is
executed with maximum efficiency.

Machine learning models typically consist of multiple layer types, each
exhibiting distinct memory access patterns, data reuse characteristics,
and parallelization opportunities. By tailoring mapping strategies to
these specific properties, hybrid approaches achieve higher
computational efficiency, improved memory bandwidth utilization, and
reduced data movement overhead compared to a uniform mapping approach
(\citeproc{ref-sze2020efficient}{Sze et al. 2017b}).

\subsubsection{Layer-Specific
Mapping}\label{sec-ai-acceleration-layerspecific-mapping-1102}

Hybrid mapping strategies are particularly beneficial in models that
combine spatially localized computations, such as convolutions, with
fully connected operations, such as dense layers or attention
mechanisms. These operations possess distinct characteristics that
require different mapping strategies for optimal performance.

In convolutional neural networks, hybrid strategies are frequently
employed to optimize performance. Specifically, weight stationary
execution is applied to convolutional layers, ensuring that filters
remain in local memory while activations are streamed dynamically. For
fully connected layers, output stationary execution is utilized to
minimize redundant memory writes during matrix multiplications.
Additionally, kernel fusion is integrated to combine activation
functions, batch normalization, and element wise operations into a
single computational step, thereby reducing intermediate memory traffic.
Collectively, these approaches enhance computational efficiency and
memory utilization, contributing to the overall performance of the
network.

Transformers employ several strategies to enhance performance by
optimizing memory usage and computational efficiency. Specifically, they
use activation stationary mapping in self-attention layers to maximize
the reuse of stored key-value pairs, thereby reducing memory fetches. In
feedforward layers, weight stationary mapping is applied to ensure that
large weight matrices are efficiently reused across computations.
Additionally, these models incorporate fused attention kernels that
integrate softmax and weighted summation into a single computation step,
significantly enhancing execution speed
(\citeproc{ref-dao2022flashattention}{Dao et al. 2022}).

For multilayer perceptrons, hybrid mapping strategies are employed to
optimize performance through a combination of techniques that enhance
both memory efficiency and computational throughput. Specifically,
weight stationary execution is utilized to maximize the reuse of weights
across activations, ensuring that these frequently accessed parameters
remain readily available and reduce redundant memory accesses. In
addition, blocked tiling strategies are implemented for large matrix
multiplications, which significantly improve cache locality by
partitioning the computation into manageable sub-blocks that fit within
fast memory. Complementing these approaches, general matrix
multiplication fusion is applied, effectively reducing memory stalls by
merging consecutive matrix multiplication operations with subsequent
functional transformations. Collectively, these optimizations illustrate
how tailored mapping strategies can systematically balance memory
constraints with computational demands in multilayer perceptron
architectures.

Hybrid mapping strategies are widely employed in vision transformers,
which seamlessly integrate convolutional and self-attention operations.
In these models, the patch embedding layer performs a convolution-like
operation that benefits from weight stationary mapping
(\citeproc{ref-Dosovitskiy2020ViT}{Dosovitskiy et al. 2020}). The
self-attention layers, on the other hand, require activation stationary
execution to efficiently reuse the key-value cache across multiple
queries. Additionally, the MLP component leverages general matrix
multiplication fusion and blocked tiling to execute dense matrix
multiplications efficiently. This layer-specific optimization framework
effectively balances memory locality with computational efficiency,
rendering vision transformers particularly well-suited for AI
accelerators.

\subsection{Hardware Implementations of Hybrid
Strategies}\label{sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8}

Several modern AI accelerators incorporate hybrid mapping strategies to
optimize execution by tailoring layer-specific techniques to the unique
computational requirements of diverse neural network architectures. For
example, Google TPUs employ weight stationary mapping for convolutional
layers and activation stationary mapping for attention layers within
transformer models, ensuring that the most critical data remains in fast
memory. Likewise, NVIDIA GPUs leverage fused kernels alongside hybrid
memory layouts, which enable the application of different mapping
strategies within the same model to maximize performance. In addition,
Graphcore IPUs dynamically select execution strategies on a per-layer
basis to optimize memory access, thereby enhancing overall computational
efficiency.

These implementations illustrate how hybrid mapping strategies bridge
the gap between different types of machine learning computations,
ensuring that each layer executes with maximum efficiency. However,
hardware support is essential for these techniques to be practical.
Accelerators must provide architectural features such as programmable
memory hierarchies, efficient interconnects, and specialized execution
pipelines to fully exploit hybrid mapping.

Hybrid mapping provides a flexible and efficient approach to deep
learning execution, enabling AI accelerators to adapt to the diverse
computational requirements of modern architectures. By selecting the
optimal mapping technique for each layer, hybrid strategies help reduce
memory bandwidth constraints, improve data locality, and maximize
parallelism.

While hybrid mapping strategies offer an effective way to optimize
computations at a layer-specific level, they remain static design-time
optimizations. In real-world AI workloads, execution conditions can
change dynamically due to varying input sizes, memory contention, or
hardware resource availability. Machine learning compilers and runtime
systems extend these mapping techniques by introducing dynamic
scheduling, memory optimizations, and automatic tuning mechanisms. These
systems ensure that hybrid strategies are not just predefined execution
choices, but rather adaptive mechanisms that allow deep learning
workloads to operate efficiently across different accelerators and
deployment environments.

The mapping strategies and dataflow optimizations examined in preceding
sections represent the ``what'' of efficient execution: which data to
keep local, how to tile computations, and which parallelization
strategies to employ. However, determining optimal configurations for
specific hardware and workloads requires systematic automation. This is
where machine learning compilers become essential: they transform
abstract mapping principles into concrete execution plans tailored to
target accelerators, bridging the gap between high-level model
definitions and low-level hardware instructions.

\phantomsection\label{quiz-question-sec-ai-acceleration-dataflow-optimization-strategies-ce52}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.6}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-dataflow-optimization-strategies-ce52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following dataflow strategies keeps weights fixed in
  local memory while streaming input activations through the system?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Weight Stationary
  \item
    Input Stationary
  \item
    Output Stationary
  \item
    Activation Stationary
  \end{enumerate}
\item
  True or False: In an output stationary dataflow strategy, input
  activations are kept fixed in local memory.
\item
  What are the trade-offs of using an input stationary strategy in a
  transformer model?
\item
  In a system design scenario, which dataflow strategy would be most
  effective for a CNN with high weight reuse?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Activation Stationary
  \item
    Output Stationary
  \item
    Input Stationary
  \item
    Weight Stationary
  \end{enumerate}
\item
  How might you decide between using a weight stationary or output
  stationary strategy in a new AI model?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-dataflow-optimization-strategies-ce52]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Compiler
Support}\label{sec-ai-acceleration-compiler-support-172e}

Machine learning compilers automate the translation of dataflow
strategies into executable code, addressing a fundamental challenge: the
mapping decisions analyzed above must be instantiated differently for
each hardware target. To see why this matters, consider what happens
when you compile ResNet-50 for GPU inference:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Graph optimization} fuses the 49 Conv2D-BatchNorm-ReLU
  sequences into 49 single kernels, eliminating 98 intermediate memory
  writes that would otherwise consume bandwidth
\item
  \textbf{Kernel selection} chooses Tensor Core implementations for the
  3x3 convolutions, exploiting the high arithmetic intensity (50-200
  FLOP/byte) we calculated in the Roofline analysis
\item
  \textbf{Memory planning} determines that intermediate activations
  require approximately 2.1 GB at batch size 32, fitting comfortably in
  the A100's 40 GB HBM
\item
  \textbf{Computation scheduling} overlaps memory transfers for layer
  N+1 with computation of layer N, hiding a substantial portion of
  transfer latency
\end{enumerate}

The result: inference time drops from approximately 47 ms (naive
execution) to approximately 8 ms (optimized), roughly a 5-6x improvement
from compilation alone, before any algorithmic changes to the model.
This concrete example illustrates how the dataflow strategies from the
previous section, including kernel fusion
(Section~\ref{sec-ai-acceleration-kernel-fusion-7faf}) and tiling
(Section~\ref{sec-ai-acceleration-memoryefficient-tiling-strategies-9fce}),
translate into real performance through systematic compiler
optimization.

This process exemplifies the hardware-software co-design principle
established in
Section~\ref{sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28},
where machine learning compilers bridge high-level model representations
with low-level hardware execution. The compiler optimizes models by
restructuring computations, selecting efficient execution kernels, and
maximizing hardware utilization (\citeproc{ref-chen_tvmlang_2018}{0001
et al. 2018b}). Unlike traditional compilers designed for
general-purpose computing, ML workloads require specialized approaches
for tensor computations and parallel execution.

\subsection{Compiler Design Differences for ML
Workloads}\label{sec-ai-acceleration-compiler-design-differences-ml-workloads-0698}

Machine learning workloads introduce challenges that traditional
compilers were not designed to handle. Unlike conventional software
execution, which primarily involves sequential or multi-threaded program
flow, machine learning models are expressed as computation graphs that
describe large-scale tensor operations. These graphs require specialized
optimizations that traditional compilers cannot efficiently apply
(\citeproc{ref-cui_mlcompilers_2019}{Cui, Li, and Xie 2019}).

Table~\ref{tbl-ml-vs-traditional-compilers} outlines the fundamental
differences between traditional compilers and those designed for machine
learning workloads. While traditional compilers optimize linear program
execution through techniques like instruction scheduling and register
allocation, ML compilers focus on optimizing computation graphs for
efficient tensor operations. This distinction is critical, as ML
compilers must incorporate domain-specific transformations such as
kernel fusion, memory-aware scheduling, and hardware-accelerated
execution plans to achieve high performance on specialized accelerators
like GPUs and TPUs.

This comparison highlights why machine learning models require a
different compilation approach. Instead of optimizing instruction-level
execution, machine learning compilers must transform entire computation
graphs, apply tensor-aware memory optimizations, and schedule operations
across thousands of parallel processing elements. These requirements
make traditional compiler techniques insufficient for modern deep
learning workloads.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1899}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3924}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4114}}@{}}
\caption{\textbf{Compiler Optimization Priorities}: Traditional and
machine learning compilers diverge in their optimization targets;
traditional compilers prioritize efficient execution of sequential code,
while ML compilers focus on optimizing tensor operations within
computation graphs for specialized hardware. This table clarifies how ML
compilers incorporate domain-specific transformations---like kernel
fusion and memory-aware scheduling---to achieve high performance on
accelerators, unlike the instruction scheduling and register allocation
techniques used in conventional software
compilation.}\label{tbl-ml-vs-traditional-compilers}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Compiler}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Compiler}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Compiler}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Compiler}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input Representation} & Linear program code (C, Python) &
Computational graph (ML models) \\
\textbf{Execution Model} & Sequential or multi-threaded execution &
Massively parallel tensor-based execution \\
\textbf{Optimization Priorities} & Instruction scheduling, loop
unrolling, register allocation & Graph transformations, kernel fusion,
memory-aware execution \\
\textbf{Memory Management} & Stack and heap memory allocation & Tensor
layout transformations, tiling, memory-aware scheduling \\
\textbf{Target Hardware} & CPUs (general-purpose execution) & GPUs,
TPUs, and custom accelerators \\
\textbf{Compilation Output} & CPU-specific machine code &
Hardware-specific execution plan (kernels, memory scheduling) \\
\end{longtable}

\phantomsection\label{callout-perspectiveux2a-1.14}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Hidden Optimization Layer}
\phantomsection\label{callout-perspective*-1.14}
Most practitioners never interact directly with ML compilers, yet
compiler quality often determines whether your model achieves 20\% or
80\% of hardware peak performance. When you call
\texttt{model.compile()} in Keras, \texttt{torch.compile()} in PyTorch,
or deploy through TensorRT, you're invoking sophisticated optimization
pipelines that:

\begin{itemize}
\tightlist
\item
  \textbf{Fuse operations} you never explicitly combined (Conv2D +
  BatchNorm + ReLU → single kernel)
\item
  \textbf{Reorder computations} to improve memory locality (tiling large
  matrix multiplies)
\item
  \textbf{Select kernels} from libraries containing hundreds of
  hand-tuned implementations
\item
  \textbf{Transform tensor layouts} between what your code expects and
  what hardware prefers
\end{itemize}

This matters practically: the same model definition can run 2-5× faster
simply by switching compilation backends (e.g., PyTorch eager mode
vs.~torch.compile with different backends). When performance doesn't
meet expectations, compiler configuration and backend selection are
often the first optimization levers---requiring no changes to model
architecture or training procedure.

\end{fbx}

\subsection{ML Compilation
Pipeline}\label{sec-ai-acceleration-ml-compilation-pipeline-7676}

Machine learning models, as defined in modern frameworks, are initially
represented in a high-level computation graph that describes operations
on tensors. However, these representations are not directly executable
on hardware accelerators such as GPUs, TPUs, and custom AI chips. To
achieve efficient execution, models must go through a compilation
process that transforms them into optimized execution plans suited for
the target hardware (\citeproc{ref-tensorflow_xla_2020}{Brain 2020}).

The machine learning compilation workflow consists of several key
stages, each responsible for applying specific optimizations that ensure
minimal memory overhead, maximum parallel execution, and optimal compute
utilization. These stages include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Graph Optimization}: The computation graph is restructured to
  eliminate inefficiencies.
\item
  \textbf{Kernel Selection}: Each operation is mapped to an optimized
  hardware-specific implementation.
\item
  \textbf{Memory Planning}: Tensor layouts and memory access patterns
  are optimized to reduce bandwidth consumption.
\item
  \textbf{Computation Scheduling}: Workloads are distributed across
  parallel processing elements to maximize hardware utilization.
\item
  \textbf{Code Generation}: The optimized execution plan is translated
  into machine-specific instructions for execution.
\end{enumerate}

At each stage, the compiler applies theoretical optimizations discussed
earlier such as kernel fusion, tiling, data movement strategies, and
computation placement, ensuring that these optimizations are
systematically incorporated into the final execution plan.

By understanding this workflow, we can see how machine learning
acceleration is realized not just through hardware improvements but also
through compiler-driven software optimizations.

\subsection{Graph
Optimization}\label{sec-ai-acceleration-graph-optimization-f888}

AI accelerators provide specialized hardware to speed up computation,
but raw model representations are not inherently optimized for execution
on these accelerators. Machine learning frameworks define models using
high-level computation graphs, where nodes represent operations (such as
convolutions, matrix multiplications, and activations), and edges define
data dependencies. However, if executed as defined, these graphs often
contain redundant operations, inefficient memory access patterns, and
suboptimal execution sequences that can prevent the hardware from
operating at peak efficiency.

For example, in a Transformer model, the self-attention mechanism
involves repeated accesses to the same key-value pairs across multiple
attention heads. If compiled naïvely, the model may reload the same data
multiple times, leading to excessive memory traffic
(\citeproc{ref-shoeybi_megatron_2020}{Shoeybi et al. 2019a}). Similarly,
in a CNN, applying batch normalization and activation functions as
separate operations after each convolution leads to unnecessary
intermediate memory writes, increasing memory bandwidth usage. These
inefficiencies are addressed during graph optimization, where the
compiler restructures the computation graph to eliminate unnecessary
operations and improve memory locality
(\citeproc{ref-chen_tvmlang_2018}{0001 et al. 2018b}).

The graph optimization phase of compilation is responsible for
transforming this high-level computation graph into an optimized
execution plan before it is mapped to hardware. Rather than requiring
manual optimization, the compiler systematically applies transformations
that improve data movement, reduce redundant computations, and
restructure operations for efficient parallel execution
(\citeproc{ref-nvidia_tensorRT_2021}{NVIDIA 2021}).

At this stage, the compiler is still working at a hardware-agnostic
level, focusing on high-level restructuring that improves efficiency
before more hardware-specific optimizations are applied later.

\subsubsection{Computation Graph
Optimization}\label{sec-ai-acceleration-computation-graph-optimization-a028}

Graph optimization transforms the computation graph through a series of
structured techniques designed to enhance execution efficiency. One key
technique is kernel fusion, which merges consecutive operations to
eliminate unnecessary memory writes and reduce the number of kernel
launches. This approach is particularly effective in convolutional
neural networks, where fusing convolution, batch normalization, and
activation functions notably accelerates processing. Another important
technique is computation reordering, which adjusts the execution order
of operations to improve data locality and maximize parallel execution.
For instance, in Transformer models, such reordering enables the reuse
of cached key-value pairs rather than reloading them repeatedly from
memory, thereby reducing latency.

Additionally, redundant computation elimination plays an important role.
By identifying and removing duplicate or unnecessary operations, this
method is especially beneficial in models with residual connections
where common subexpressions might otherwise be redundantly computed.
Memory-aware dataflow adjustments enhance overall performance by
refining tensor layouts and optimizing memory movement. For example,
tiling matrix multiplications to meet the structural requirements of
systolic arrays in TPUs ensures that hardware resources are utilized
optimally. This combined approach not only reduces unnecessary
processing but also aligns data storage and movement with the
accelerator's strengths, leading to efficient execution across diverse
AI workloads. Together, these techniques prepare the model for
acceleration by minimizing overhead and ensuring an optimal balance
between computational and memory resources.

\subsubsection{Implementation in AI
Compilers}\label{sec-ai-acceleration-implementation-ai-compilers-1df9}

Modern AI compilers perform graph optimization through the use of
automated pattern recognition and structured rewrite rules,
systematically transforming computation graphs to maximize efficiency
without manual intervention. For example, Google's XLA (Accelerated
Linear Algebra) in TensorFlow applies graph-level transformations such
as fusion and layout optimizations that streamline execution on TPUs and
GPUs. Similarly, TVM (Tensor Virtual Machine) not only refines tensor
layouts and adjusts computational structures but also tunes execution
strategies across diverse hardware backends, which is particularly
beneficial for deploying models on embedded TinyML devices with strict
memory constraints.

NVIDIA's TensorRT, another specialized deep learning compiler, focuses
on minimizing kernel launch overhead by fusing operations and optimizing
execution scheduling on GPUs, thereby improving utilization and reducing
inference latency in large-scale convolutional neural network
applications. Additionally, MLIR (Multi-Level Intermediate
Representation) facilitates flexible graph optimization across various
AI accelerators by enabling multi-stage transformations that improve
execution order and memory access patterns, thus easing the transition
of models from CPU-based implementations to accelerator-optimized
versions. These compilers preserve the mathematical integrity of the
models while rewriting the computation graph to ensure that the
subsequent hardware-specific optimizations can be effectively applied.

\subsubsection{Graph Optimization
Importance}\label{sec-ai-acceleration-graph-optimization-importance-9ccb}

Graph optimization enables AI accelerators to operate at peak
efficiency. Without this phase, even the most optimized hardware would
be underutilized, as models would be executed in a way that introduces
unnecessary memory stalls, redundant computations, and inefficient data
movement. By systematically restructuring computation graphs, the
compiler arranges operations for efficient execution that mitigates
bottlenecks before mapping to hardware, minimizes memory movement to
keep tensors in high-speed memory, and optimizes parallel execution to
reduce unnecessary serialization while enhancing hardware utilization.
For instance, without proper graph optimization, a large Transformer
model running on an edge device may experience excessive memory stalls
due to suboptimal data access patterns; however, through effective graph
restructuring, the model can operate with significantly reduced memory
bandwidth consumption and latency, thus enabling real-time inference on
devices with constrained resources.

With the computation graph now fully optimized, the next step in
compilation is kernel selection, where the compiler determines which
hardware-specific implementation should be used for each operation. This
ensures that the structured execution plan is translated into optimized
low-level instructions for the target accelerator.

\subsection{Kernel
Selection}\label{sec-ai-acceleration-kernel-selection-df01}

At this stage, the compiler translates the abstract operations in the
computation graph into optimized low-level functions, ensuring that
execution is performed as efficiently as possible given the constraints
of the target accelerator. A kernel is a specialized implementation of a
computational operation designed to run efficiently on a particular
hardware architecture. Most accelerators, including GPUs, TPUs, and
custom AI chips, provide multiple kernel implementations for the same
operation, each optimized for different execution scenarios. Choosing
the right kernel for each operation is essential for maximizing
computational throughput, minimizing memory stalls, and ensuring that
the accelerator's specialized processing elements are fully utilized
(\citeproc{ref-nvidia_tensorRT_2021}{NVIDIA 2021}).

Kernel selection builds upon the graph optimization phase, ensuring that
the structured execution plan is mapped to the most efficient
implementation available. While graph optimization eliminates
inefficiencies at the model level, kernel selection ensures that each
individual operation is executed using the most efficient
hardware-specific routine. The effectiveness of this process directly
impacts the model's overall performance, as poor kernel choices can
nullify the benefits of prior optimizations by introducing unnecessary
computation overhead or memory bottlenecks
(\citeproc{ref-chen_tvmlang_2018}{0001 et al. 2018b}).

In a Transformer model, the matrix multiplications that dominate
self-attention computations can be executed using different strategies
depending on the available hardware. On a CPU, a general-purpose matrix
multiplication routine is typically employed, exploiting vectorized
execution to improve efficiency. In contrast, on a GPU, the compiler may
select an implementation that leverages tensor cores to accelerate
matrix multiplications using mixed-precision arithmetic. When the model
is deployed on a TPU, the operation can be mapped onto a systolic array,
ensuring that data flows through the accelerator in a manner that
maximizes reuse and minimizes off-chip memory accesses. Additionally,
for inference workloads, an integer arithmetic kernel may be preferable,
as it facilitates computations in INT8 instead of floating-point
precision, thereby reducing power consumption without significantly
compromising accuracy.

In many cases, compilers do not generate custom kernels from scratch but
instead select from vendor-optimized kernel libraries that provide
highly tuned implementations for different architectures. For instance,
cuDNN and cuBLAS offer optimized kernels for deep learning on NVIDIA
GPUs, while oneDNN provides optimized execution for Intel architectures.
Similarly, ACL (Arm Compute Library) is optimized for Arm-based devices,
and Eigen and BLIS provide efficient CPU-based implementations of deep
learning operations. These libraries allow the compiler to choose
pre-optimized, high-performance kernels rather than having to reinvent
execution strategies for each hardware platform.

\subsubsection{Implementation in AI
Compilers}\label{sec-ai-acceleration-implementation-ai-compilers-c917}

AI compilers use heuristics\sidenote{\textbf{Heuristic}: From Greek
``heuriskein'' (to discover), sharing roots with ``eureka.'' A heuristic
is a practical rule-of-thumb that finds good solutions quickly without
exhaustively searching all possibilities. In AI compilers, heuristics
encode expert knowledge about hardware behavior, enabling fast kernel
selection decisions that would be intractable to compute optimally given
the exponential search space of possible configurations. }, profiling,
and cost models to determine the best kernel for each operation. These
strategies ensure that each computation is executed in a way that
maximizes throughput and minimizes memory bottlenecks.

In rule-based selection, the compiler applies predefined heuristics
based on the known capabilities of the hardware. For instance, XLA, the
compiler used in TensorFlow, automatically selects tensor core-optimized
kernels for NVIDIA GPUs when mixed-precision execution is enabled. These
predefined rules allow the compiler to make fast, reliable decisions
about which kernel to use without requiring extensive analysis.

Profile-guided selection takes a more dynamic approach, benchmarking
different kernel options and choosing the one that performs best for a
given workload. TVM, an open-source AI compiler, uses AutoTVM to
empirically evaluate kernel performance, tuning execution strategies
based on real-world execution times. By testing different kernels before
deployment, profile-guided selection helps ensure that operations are
assigned to the most efficient implementation under actual execution
conditions.

Another approach, cost model-based selection, relies on performance
predictions to estimate execution time and memory consumption for
various kernels before choosing the most efficient one. MLIR, a compiler
infrastructure designed for machine learning workloads, applies this
technique to determine the most effective tiling and memory access
strategies (\citeproc{ref-mlir_framework_2021}{Lattner et al. 2020}). By
modeling how different kernels interact with the accelerator's compute
units and memory hierarchy, the compiler can select the kernel that
minimizes execution cost while maximizing performance.

Many AI compilers also incorporate precision-aware kernel selection,
where the selected kernel is optimized for specific numerical formats
such as FP32, FP16, BF16, or INT8. Training workloads often prioritize
higher precision (FP32, BF16) to maintain model accuracy, whereas
inference workloads favor lower precision (FP16, INT8) to increase speed
and reduce power consumption. For example, an NVIDIA GPU running
inference with TensorRT can dynamically select FP16 or INT8 kernels
based on a model's accuracy constraints. This trade-off between
precision and performance is a key aspect of kernel selection,
especially when deploying models in resource-constrained environments.

Some compilers go beyond static kernel selection and implement adaptive
kernel tuning, where execution strategies are adjusted at runtime based
on the system's workload and available resources. AutoTVM in TVM
measures kernel performance across different workloads and dynamically
refines execution strategies. TensorRT applies real-time optimizations
based on batch size, memory constraints, and GPU load, adjusting kernel
selection dynamically. Google's TPU compiler takes a similar approach,
optimizing kernel selection based on cloud resource availability and
execution environment constraints.

\subsubsection{Kernel Selection
Importance}\label{sec-ai-acceleration-kernel-selection-importance-3c3f}

The efficiency of AI acceleration depends not only on how computations
are structured but also on how they are executed. Even the best-designed
computation graph will fail to achieve peak performance if the selected
kernels do not fully utilize the hardware's capabilities.

Proper kernel selection allows models to execute using the most
efficient algorithms available for the given hardware, ensuring that
memory is accessed in a way that avoids unnecessary stalls and that
specialized acceleration features, such as tensor cores or systolic
arrays, are leveraged wherever possible. Selecting an inappropriate
kernel can lead to underutilized compute resources, excessive memory
transfers, and increased power consumption, all of which limit the
performance of AI accelerators.

For instance, if a Transformer model running on a GPU is assigned a
non-tensor-core kernel for its matrix multiplications, it may execute at
only a fraction of the possible performance. Conversely, if a model
designed for FP32 execution is forced to run on an INT8-optimized
kernel, it may experience significant numerical instability, degrading
accuracy. These choices illustrate why kernel selection is as much about
maintaining numerical correctness as it is about optimizing performance.

With kernel selection complete, the next stage in compilation involves
execution scheduling and memory management, where the compiler
determines how kernels are launched and how data is transferred between
different levels of the memory hierarchy. These final steps in the
compilation pipeline ensure that computations run with maximum
parallelism while minimizing the overhead of data movement. As kernel
selection determines what to execute, execution scheduling and memory
management dictate when and how those kernels are executed, ensuring
that AI accelerators operate at peak efficiency.

\subsection{Memory
Planning}\label{sec-ai-acceleration-memory-planning-fb9f}

The memory planning phase ensures that data is allocated and accessed in
a way that minimizes memory bandwidth consumption, reduces latency, and
maximizes cache efficiency (\citeproc{ref-zhang2020optimizing}{Zhang,
Li, and Ouyang 2020}). Even with the most optimized execution plan, a
model can still suffer from severe performance degradation if memory is
not managed efficiently.

Machine learning workloads are often memory-intensive. They require
frequent movement of large tensors between different levels of the
memory hierarchy. The compiler must determine how tensors are stored,
how they are accessed, and how intermediate results are handled to
ensure that memory does not become a bottleneck.

The memory planning phase focuses on optimizing tensor layouts, memory
access patterns, and buffer reuse to prevent unnecessary stalls and
memory contention during execution. In this phase, tensors are arranged
in a memory-efficient format that aligns with hardware access patterns,
thereby minimizing the need for format conversions. Additionally, memory
accesses are structured to reduce cache misses and stalls, which in turn
lowers overall bandwidth consumption. Buffer reuse is also a critical
aspect, as it reduces redundant memory allocations by intelligently
managing intermediate results. Together, these strategies ensure that
data is efficiently placed and accessed, thereby enhancing both
computational performance and energy efficiency in AI workloads.

\subsubsection{Implementation in AI
Compilers}\label{sec-ai-acceleration-implementation-ai-compilers-2ae0}

Memory planning is a complex problem because AI models must balance
memory availability, reuse, and access efficiency while operating across
multiple levels of the memory hierarchy. AI compilers use several key
strategies to manage memory effectively and prevent unnecessary data
movement.

The first step in memory planning is tensor layout optimization, where
the compiler determines how tensors should be arranged in memory to
maximize locality and prevent unnecessary data format conversions.
Different hardware accelerators have different preferred storage
layouts---for instance, NVIDIA GPUs often use row-major storage (NHWC
format), while TPUs favor channel-major layouts (NCHW format) to
optimize memory coalescing (\citeproc{ref-abadi2016tensorflow}{Abadi et
al. 2016}). The compiler automatically transforms tensor layouts based
on the expected access patterns of the target hardware, ensuring that
memory accesses are aligned for maximum efficiency.

Beyond layout optimization, memory planning also includes buffer
allocation and reuse, where the compiler minimizes memory footprint by
reusing intermediate storage whenever possible. Deep learning workloads
generate many temporary tensors, such as activations and gradients,
which can quickly overwhelm on-chip memory if not carefully managed.
Instead of allocating new memory for each tensor, the compiler analyzes
the computation graph to identify opportunities for buffer reuse,
ensuring that intermediate values are stored and overwritten efficiently
(\citeproc{ref-moreau2018relay}{Jones 2018}).

Another critical aspect of memory planning is minimizing data movement
between different levels of the memory hierarchy. AI accelerators
typically have a mix of high-speed on-chip memory (such as caches or
shared SRAM) and larger, but slower, external DRAM. If tensor data is
repeatedly moved between these memory levels, the model may become
memory-bound, reducing computational efficiency. To prevent this,
compilers use tiling strategies that break large computations into
smaller, memory-friendly chunks, allowing execution to fit within fast,
local memory and reducing the need for costly off-chip memory accesses.

\subsubsection{Memory Planning
Importance}\label{sec-ai-acceleration-memory-planning-importance-e987}

Without proper memory planning, even optimized computation graphs and
kernel selection will fail to deliver high performance. Excessive memory
transfers, inefficient memory layouts, and redundant memory allocations
can all lead to bottlenecks that prevent AI accelerators from reaching
their peak throughput.

For instance, a CNN running on a GPU may achieve high computational
efficiency in theory, but if its convolutional feature maps are stored
in an incompatible format, for example, if it uses a row-major layout
that necessitates conversion to a channel-friendly format such as NCHW
or a variant like NHCW, constant tensor format conversions can introduce
significant overhead. Similarly, a Transformer model deployed on an edge
device may struggle to meet real-time inference requirements if memory
is not carefully planned, leading to frequent off-chip memory accesses
that increase latency and power consumption.

Through careful management of tensor placement, optimizing memory access
patterns, and reducing unnecessary data movement, memory planning can
help ensure efficient operation of AI accelerators, leading to tangible
performance improvements in real-world applications. With memory
allocation determined, the compiler must next decide when and where each
computation executes.

\subsection{Computation
Scheduling}\label{sec-ai-acceleration-computation-scheduling-7ccd}

With graph optimization completed, kernels selected, and memory planning
finalized, computation scheduling determines the execution order and
resource assignment for each operation. This phase determines when and
where each computation should be executed, ensuring that workloads are
efficiently distributed across available processing elements while
avoiding unnecessary stalls and resource contention
(\citeproc{ref-Rajbhandari2020}{Rajbhandari et al. 2020};
\citeproc{ref-Zheng2020}{Zheng et al. 2020}).

AI accelerators achieve high performance through massive parallelism,
but without an effective scheduling strategy, computational units may
sit idle, memory bandwidth may be underutilized, and execution
efficiency may degrade. Computation scheduling is responsible for
ensuring that all processing elements remain active, execution
dependencies are managed correctly, and workloads are distributed
optimally (\citeproc{ref-Jia2019}{Ziheng Jia et al. 2019}).

In the scheduling phase, parallel execution, synchronization, and
resource allocation are managed systematically. Task partitioning
decomposes extensive computations into smaller, manageable tasks that
can be distributed efficiently among multiple compute cores. Execution
order optimization then determines the most effective sequence for
launching these operations, maximizing hardware performance while
reducing execution stalls. Additionally, resource allocation and
synchronization are orchestrated to ensure that compute cores, memory
bandwidth, and shared caches are utilized effectively, avoiding
contention. Through these coordinated strategies, computation scheduling
achieves optimal hardware utilization, minimizes memory access delays,
and supports a streamlined and efficient execution process.

\subsubsection{Implementation in AI
Compilers}\label{sec-ai-acceleration-implementation-ai-compilers-ff25}

Computation scheduling is highly dependent on the underlying hardware
architecture, as different AI accelerators have unique execution models
that must be considered when determining how workloads are scheduled. AI
compilers implement several key strategies to optimize scheduling for
efficient execution.

One of the most fundamental aspects of scheduling is task partitioning,
where the compiler divides large computational graphs into smaller,
manageable units that can be executed in parallel. On GPUs, this
typically means mapping matrix multiplications and convolutions to
thousands of CUDA cores, while on TPUs, tasks are partitioned to fit
within systolic arrays that operate on structured data flows
(\citeproc{ref-norrie2021design}{Norrie et al. 2021}). In CPUs,
partitioning is often focused on breaking computations into vectorized
chunks that align with SIMD execution. The goal is to map workloads to
available processing units efficiently, ensuring that each core remains
active throughout execution.

Scheduling involves optimizing execution order to minimize dependencies
and maximize throughput beyond task partitioning. Many AI models include
operations that can be computed independently (e.g., different batches
in a batch processing pipeline) alongside operations that have strict
dependencies (e.g., recurrent layers in an RNN). AI compilers analyze
these dependencies and attempt to rearrange execution where possible,
reducing idle time and improving parallel efficiency. For example, in
Transformer models, scheduling may prioritize preloading attention
matrices into memory while earlier layers are still executing, ensuring
that data is ready when needed (\citeproc{ref-Shoeybi2019}{Shoeybi et
al. 2019b}).

Another crucial aspect of computation scheduling is resource allocation
and synchronization, where the compiler determines how compute cores
share memory and coordinate execution. Modern AI accelerators often
support overlapping computation and data transfers, meaning that while
one task executes, the next task can begin fetching its required data.
Compilers take advantage of this by scheduling tasks in a way that hides
memory latency, ensuring that execution remains compute-bound rather
than memory-bound (\citeproc{ref-Chen2018}{0001 et al. 2018c}). TensorRT
and XLA, for example, employ streaming execution strategies where
multiple kernels are launched in parallel, and synchronization is
carefully managed to prevent execution stalls
(\citeproc{ref-GoogleXLA}{Google 2025}).

\subsubsection{Computation Scheduling
Importance}\label{sec-ai-acceleration-computation-scheduling-importance-04a1}

Without effective scheduling, even the most optimized model can suffer
from underutilized compute resources, memory bottlenecks, and execution
inefficiencies. Poor scheduling decisions can lead to idle processing
elements, forcing expensive compute cores to wait for data or
synchronization events before continuing execution.

For instance, a CNN running on a GPU may have highly optimized kernels
and efficient memory layouts, but if its execution is not scheduled
correctly, compute units may remain idle between kernel launches,
reducing throughput. Similarly, a Transformer model deployed on a TPU
may perform matrix multiplications efficiently but could experience
performance degradation if attention layers are not scheduled to overlap
efficiently with memory transfers. With scheduling complete, the final
compilation stage translates this optimized execution plan into
hardware-specific instructions.

\subsubsection{Code
Generation}\label{sec-ai-acceleration-code-generation-85c8}

Unlike the previous phases, which required AI-specific optimizations,
code generation follows many of the same principles as traditional
compilers. This process includes instruction selection, register
allocation, and final optimization passes, ensuring that execution makes
full use of hardware-specific features such as vectorized execution,
memory prefetching, and instruction reordering.

For CPUs and GPUs, AI compilers typically generate machine code or
optimized assembly instructions, while for TPUs,
FPGAs\sidenote{\textbf{FPGA (Field-Programmable Gate Array)}:
``Field-programmable'' means configurable after leaving the factory, in
the customer's ``field'' of deployment, contrasting with
factory-programmed ASICs. Xilinx (now AMD) introduced the first
commercial FPGA in 1985. The ``gate array'' refers to the matrix of
logic blocks that can be wired together through programmable
interconnects. FPGAs achieve 2-10x better performance per watt than GPUs
for specific workloads but require hardware description languages
(Verilog/VHDL), limiting adoption compared to GPU programming. }, and
other accelerators, the output may be optimized bytecode or execution
graphs that are interpreted by the hardware's runtime system.

At this point, the compilation pipeline is complete: the original
high-level model representation has been transformed into an optimized,
executable format tailored for efficient execution on the target
hardware. The combination of graph transformations, kernel selection,
memory-aware execution, and parallel scheduling ensures that AI
accelerators run workloads with maximum efficiency, minimal memory
overhead, and optimal computational throughput.

\subsection{Compilation-Runtime
Support}\label{sec-ai-acceleration-compilationruntime-support-0206}

The compiler plays a fundamental role in AI acceleration, transforming
high-level machine learning models into optimized execution plans
tailored to the constraints of specialized hardware. Throughout this
section, we have seen how graph optimization restructures computation,
kernel selection maps operations to hardware-efficient implementations,
memory planning optimizes data placement, and computation scheduling
ensures efficient parallel execution. Each of these phases is crucial in
enabling AI models to fully leverage modern accelerators, ensuring high
throughput, minimal memory overhead, and efficient execution pipelines.

However, the compiler optimizations examined throughout this section
share a critical limitation: they all occur \emph{before} execution
begins. Graph restructuring that fuses Conv2D-BatchNorm-ReLU into single
kernels, kernel selection that chooses Tensor Core implementations,
memory planning that schedules activations across HBM, and computation
scheduling that overlaps transfers with computation all produce a
single, optimized execution plan based on assumptions about batch sizes,
dedicated hardware availability, and clean memory state.

Production AI systems inhabit a dynamic world that rarely matches these
static assumptions. Batch sizes vary from 1 (latency-sensitive single
requests) to 128 (throughput-oriented batch serving) within the same
deployment. GPU memory fragments during long-running inference servers,
forcing suboptimal tensor layouts. Multiple workloads compete for
accelerator resources in multi-tenant cloud environments. Thermal
throttling reduces sustained performance below the peaks observed in
short benchmarks. The runtime system bridges static optimization and
dynamic reality, continuously adapting execution to actual conditions
rather than assumed conditions.

\phantomsection\label{quiz-question-sec-ai-acceleration-compiler-support-172e}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.7}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-compiler-support-172e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following is a primary focus of machine learning
  compilers compared to traditional compilers?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Graph transformations and kernel fusion
  \item
    Instruction scheduling and register allocation
  \item
    Loop unrolling and memory allocation
  \item
    Sequential program optimization
  \end{enumerate}
\item
  Explain why kernel fusion is important in machine learning compilers.
\item
  Order the following stages in the ML compilation pipeline: (1) Graph
  Optimization, (2) Memory Planning, (3) Kernel Selection, (4)
  Computation Scheduling.
\item
  In a production system, what trade-offs might you consider when
  selecting kernels for ML model execution?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Precision versus performance
  \item
    All of the above
  \item
    Execution speed versus memory usage
  \item
    Power consumption versus accuracy
  \end{enumerate}
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-compiler-support-172e]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Runtime
Support}\label{sec-ai-acceleration-runtime-support-f94f}

The gap between compile-time optimization and production reality
motivates AI runtimes, which extend static compilation with real-time
adaptation (\citeproc{ref-nvidia_tensorRT_2021}{NVIDIA 2021}). AI
workloads operate in varied execution environments, where factors such
as fluctuating batch sizes, shared hardware resources, memory
contention, and latency constraints necessitate real-time adaptation.
Precompiled execution plans, optimized for a fixed set of assumptions,
may become suboptimal when actual runtime conditions change. AI runtimes
bridge this gap by providing a dynamic layer of execution management
that extends compile-time optimizations with real-time decision-making.
Unlike traditional compiled programs that execute a fixed sequence of
instructions, AI workloads require adaptive control over memory
allocation, kernel execution, and resource scheduling. AI runtimes
continuously monitor execution conditions and make on-the-fly
adjustments to ensure that machine learning models fully utilize
available hardware while maintaining efficiency and performance targets.

At a high level, AI runtimes manage three critical aspects of execution:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Kernel Execution Management}: AI runtimes dynamically select
  and dispatch computation kernels based on the current system state,
  ensuring that workloads are executed with minimal latency.
\item
  \textbf{Memory Adaptation and Allocation}: Since AI workloads
  frequently process large tensors with varying memory footprints,
  runtimes adjust memory allocation dynamically to prevent bottlenecks
  and excessive data movement (\citeproc{ref-deepmind_gpipe_2019}{Huang
  et al. 2019}).
\item
  \textbf{Execution Scaling}: AI runtimes handle workload distribution
  across multiple accelerators, supporting large-scale execution in
  multi-chip, multi-node, or cloud environments
  (\citeproc{ref-mirhoseini_device_placement_2017}{Mirhoseini et al.
  2017}).
\end{enumerate}

By dynamically handling these execution aspects, AI runtimes complement
compiler-based optimizations, ensuring that models continue to perform
efficiently under varying runtime conditions. Understanding how AI
runtimes differ from traditional software runtimes clarifies why machine
learning workloads require fundamentally different execution strategies.

\subsection{Runtime Architecture Differences for ML
Systems}\label{sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e}

Traditional software runtimes are designed for managing general-purpose
program execution, primarily handling sequential and multi-threaded
workloads on CPUs. These runtimes allocate memory, schedule tasks, and
optimize execution at the level of individual function calls and
instructions. In contrast, AI runtimes are specialized for machine
learning workloads, which require massively parallel computation,
large-scale tensor operations, and dynamic memory management.

Table~\ref{tbl-runtime-comparison} highlights the fundamental
differences between traditional and AI runtimes. One of the key
distinctions lies in execution flow. Traditional software runtimes
operate on a predictable, structured execution model where function
calls and CPU threads follow a predefined control path. AI runtimes,
however, execute computational graphs, requiring complex scheduling
decisions that account for dependencies between tensor operations,
parallel kernel execution, and efficient memory access.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2308}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3154}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4462}}@{}}
\caption{\textbf{Runtime Execution Models}: Traditional and AI runtimes
diverge in their execution approaches; traditional runtimes prioritize
sequential or multi-threaded instruction processing, while AI runtimes
use massively parallel tensor operations for accelerated computation on
machine learning workloads. This distinction necessitates specialized AI
runtime architectures designed for efficient parallelization and memory
management of large-scale tensor
data.}\label{tbl-runtime-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Runtime}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{AI Runtime}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Runtime}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{AI Runtime}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Execution Model} & Sequential or multi-threaded execution &
Massively parallel tensor execution \\
\textbf{Task Scheduling} & CPU thread management & Kernel dispatch
across accelerators \\
\textbf{Memory Management} & Static allocation (stack/heap) & Dynamic
tensor allocation, buffer reuse \\
\textbf{Optimization Priorities} & Low-latency instruction execution &
Minimizing memory stalls, maximizing parallel execution \\
\textbf{Adaptability} & Mostly static execution plan & Adapts to batch
size and hardware availability \\
\textbf{Target Hardware} & CPUs (general-purpose execution) & GPUs,
TPUs, and custom accelerators \\
\end{longtable}

Memory management is another major differentiator. Traditional software
runtimes handle small, frequent memory allocations, optimizing for cache
efficiency and low-latency access. AI runtimes, in contrast, must
dynamically allocate, reuse, and optimize large tensors, ensuring that
memory access patterns align with accelerator-friendly execution. Poor
memory management in AI workloads can lead to performance bottlenecks,
particularly due to excessive off-chip memory transfers and inefficient
cache usage.

AI runtimes are inherently designed for adaptability. While traditional
runtimes often follow a mostly static execution plan, AI workloads
typically operate in highly variable execution environments, such as
cloud-based accelerators or multi-tenant hardware. As a result, AI
runtimes must continuously adjust batch sizes, reallocate compute
resources, and manage real-time scheduling decisions to maintain high
throughput and minimize execution delays.

These distinctions demonstrate why AI runtimes require fundamentally
different execution strategies compared to traditional software
runtimes. Rather than simply managing CPU processes, AI runtimes must
oversee large-scale tensor execution, multi-device coordination, and
real-time workload adaptation. The practical implications of this
difference become apparent when models move from development to
production.

\phantomsection\label{callout-perspectiveux2a-1.15}
\begin{fbx}{callout-perspective}{Systems Perspective: }{When Production Differs from Development}
\phantomsection\label{callout-perspective*-1.15}
Runtime behavior often surprises engineers who optimized their models in
development environments. Common production surprises include:

\textbf{Variable batch sizes}: Training uses fixed batch sizes, but
production inference may receive single requests (batch=1) or bursts
(batch=64+). Runtimes must handle both efficiently---single requests
need latency optimization, bursts need throughput optimization.

\textbf{Memory fragmentation}: Long-running inference servers gradually
fragment GPU memory. Runtimes implement defragmentation strategies, but
understanding when to restart services or pre-allocate memory pools
requires awareness of these dynamics.

\textbf{Multi-tenant interference}: Cloud accelerators are often shared.
Your model might run 20\% slower when a neighbor's workload competes for
memory bandwidth. Production systems need monitoring to detect and
respond to this interference.

\textbf{Thermal throttling}: Sustained workloads may trigger thermal
throttling that wasn't observed in short benchmarks. The A100 SXM
operates at 400W TDP while the A100 PCIe operates at 300W TDP; these
represent different form factors with different cooling requirements,
not boost versus sustained states. Production performance depends on
which variant is deployed and whether thermal limits are approached.

Understanding runtime adaptation mechanisms helps engineers design
systems that degrade gracefully rather than fail unexpectedly.

\end{fbx}

\subsection{Dynamic Kernel
Execution}\label{sec-ai-acceleration-dynamic-kernel-execution-33fc}

Dynamic kernel execution is the process of mapping machine learning
models to hardware and optimizing runtime execution. While static
compilation provides a solid foundation, efficient execution of machine
learning workloads requires real-time adaptation to fluctuating
conditions such as available memory, data sizes, and computational
loads. The runtime functions as an intermediary that continuously
adjusts execution strategies to match both the constraints of the
underlying hardware and the characteristics of the workload.

When mapping a machine learning model to hardware, individual
computational operations, including matrix multiplications,
convolutions, and activation functions, must be assigned to the most
appropriate processing units. This mapping is not fixed; it must be
modified during runtime in response to changes in input data, memory
availability, and overall system load. Dynamic kernel execution allows
the runtime to make real-time decisions regarding kernel selection,
execution order, and memory management, ensuring that workloads remain
efficient despite these changing conditions.

For example, consider an AI accelerator executing a deep neural network
(DNN) for image classification. If an incoming batch of high-resolution
images requires significantly more memory than expected, a statically
planned execution may cause cache thrashing or excessive off-chip memory
accesses. Instead, a dynamic runtime can adjust tiling strategies on the
fly, breaking down tensor operations into smaller tiles that fit within
the high-speed on-chip memory. This prevents memory stalls and ensures
optimal utilization of caches.

Similarly, when running a transformer-based NLP model, the sequence
length of input text may vary between inference requests. A static
execution plan optimized for a fixed sequence length may lead to
underutilization of compute resources when processing shorter sequences
or excessive memory pressure with longer sequences. Dynamic kernel
execution can mitigate this by selecting different kernel
implementations based on the actual sequence length, dynamically
adjusting memory allocations and execution strategies to maintain
efficiency.

Overlapping computation with memory movement is an important strategy to
mitigate performance bottlenecks. AI workloads often encounter delays
due to memory-bound issues, where data movement between memory
hierarchies limits computation speed. To combat this, AI runtimes
implement techniques like asynchronous execution and double buffering,
ensuring that computations proceed without waiting for memory transfers
to complete. In a large-scale model, for instance, image data can be
prefetched while computations are performed on the previous batch, thus
maintaining a steady flow of data and avoiding pipeline stalls.

Another practical example is the execution of convolutional layers in a
CNN on a GPU. If multiple convolution kernels need to be scheduled, a
static scheduling approach may lead to inefficient resource utilization
due to variation in layer sizes and compute requirements. By dynamically
scheduling kernel execution, AI runtimes can prioritize smaller kernels
when compute units are partially occupied, improving hardware
utilization. For instance, in NVIDIA's TensorRT runtime, fusion of small
kernels into larger execution units is done dynamically to avoid launch
overhead, optimizing latency-sensitive inference tasks.

Dynamic kernel execution ensures that machine learning models execute
efficiently. By dynamically adjusting execution strategies in response
to real-time system conditions, AI runtimes optimize both training and
inference performance across various hardware platforms. Beyond
execution strategy adaptation, runtimes must also select which specific
kernel implementations to invoke for each operation.

\subsection{Runtime Kernel
Selection}\label{sec-ai-acceleration-runtime-kernel-selection-1ffe}

While compilers perform an initial selection of kernels based on static
analysis, AI runtimes often need to override these decisions during
execution. Real-time factors, such as available memory, hardware
utilization, and workload priorities, may differ significantly from the
assumptions made during compilation. By dynamically selecting and
switching kernels at runtime, AI runtimes can adapt to these changing
conditions, ensuring that models continue to perform efficiently.

For instance, consider transformer-based language models, where a
significant portion of execution time is spent on matrix
multiplications. The AI runtime must determine the most efficient way to
execute these operations based on the current system state. If the model
is running on a GPU with specialized Tensor Cores, the runtime may
switch from a standard FP32 kernel to an FP16 kernel to take advantage
of hardware acceleration (\citeproc{ref-shoeybi_megatron_2020}{Shoeybi
et al. 2019a}). Conversely, if the lower precision of FP16 causes
unacceptable numerical instability, the runtime can opt for
mixed-precision execution, selectively using FP32 where higher precision
is necessary.

Memory constraints also influence kernel selection. When memory
bandwidth is limited, the runtime may adjust its execution strategy,
reordering operations or changing the tiling strategy to fit
computations into the available cache rather than relying on slower main
memory. For example, a large matrix multiplication may be broken into
smaller chunks, ensuring that the computation fits into the on-chip
memory of the GPU, reducing overall latency.

Additionally, batch size can influence kernel selection. For workloads
that handle a mix of small and large batches, the AI runtime may choose
a latency-optimized kernel for small batches and a throughput-optimized
kernel for large-scale batch processing. This adjustment ensures that
the model continues to operate efficiently across different execution
scenarios, without the need for manual tuning. Once the appropriate
kernel is selected, the runtime must schedule its execution to maximize
hardware utilization.

\subsection{Kernel Scheduling and
Utilization}\label{sec-ai-acceleration-kernel-scheduling-utilization-99d6}

Effective kernel scheduling ensures that selected kernels execute in a
way that maximizes parallelism and resource utilization. Unlike
traditional task schedulers, which are designed to manage CPU threads,
AI runtimes must coordinate a much larger number of tasks across
parallel execution units such as GPU cores, tensor processing units, or
custom AI accelerators (\citeproc{ref-google_tpu_2017}{Norman P. Jouppi
et al. 2017a}). Effective scheduling ensures that these computational
resources are kept fully engaged, preventing bottlenecks and maximizing
throughput.

For example, in image recognition models that use convolutional layers,
operations can be distributed across multiple processing units, enabling
different filters to run concurrently. This parallelization ensures that
the available hardware is fully utilized, speeding up execution.
Similarly, batch normalization and activation functions must be
scheduled efficiently to avoid unnecessary delays. If these operations
are not interleaved with other computations, they may block the pipeline
and reduce overall throughput.

Efficient kernel scheduling can also be influenced by real-time memory
management . AI runtimes ensure that intermediate data, such as feature
maps in deep neural networks, are preloaded into cache before they are
needed. This proactive management helps prevent delays caused by waiting
for data to be loaded from slower memory tiers, ensuring continuous
execution.

These techniques enable AI runtimes to ensure optimal resource
utilization and efficient parallel computation, which are essential for
the high-performance execution of machine learning models, particularly
in environments that require scaling across multiple hardware
accelerators. The compiler and runtime systems examined thus far
optimize execution within single accelerators, but the largest AI
workloads exceed what any single chip can deliver. Single-chip
optimizations achieve impressive results: ResNet-50 inference
accelerates from 47 ms to 8 ms through compiler optimization alone; the
dataflow strategies we examined can push GPU utilization from 20\% to
80\% of peak throughput. Yet for the largest AI workloads, even
perfectly optimized single-chip execution proves fundamentally
insufficient.

Consider the scale of training GPT-3, which required approximately
\(3.14 \times 10^{23}\) floating-point operations
(\citeproc{ref-Brown2020}{Brown et al. 2020}). Even at the H100's peak
FP16 throughput of nearly 2 petaFLOPS, completing this computation on a
single accelerator would require over five years of continuous operation
at theoretical peak, and considerably longer at realistic utilization
rates of 40-60\%. Real-time inference serving for global applications
like ChatGPT or Google Search demands throughput beyond any single
accelerator's capacity, requiring distributed inference across hundreds
of chips. These computational requirements necessitate scaling beyond
single-chip systems, introducing fundamentally different engineering
challenges from those we have examined.

\phantomsection\label{quiz-question-sec-ai-acceleration-runtime-support-f94f}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.8}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-runtime-support-f94f}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes a key function of AI runtimes in
  machine learning systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Static memory allocation
  \item
    Sequential task execution
  \item
    Dynamic kernel execution management
  \item
    Fixed execution plans
  \end{enumerate}
\item
  How do AI runtimes differ from traditional software runtimes in terms
  of memory management?
\item
  Order the following tasks in AI runtime management: (1) Memory
  adaptation, (2) Kernel execution management, (3) Execution scaling.
\item
  In a production system, what might be a consequence of poor dynamic
  kernel execution management?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Improved parallel execution
  \item
    Increased latency and resource underutilization
  \item
    Reduced memory requirements
  \item
    Enhanced sequential processing
  \end{enumerate}
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-runtime-support-f94f]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Scaling Beyond Single
Accelerators}\label{sec-ai-acceleration-scaling-beyond-single}

This section provides essential awareness of multi-chip scaling while
maintaining our Volume I focus on single-machine systems. The techniques
we have covered, dataflow optimization, kernel fusion, memory hierarchy
exploitation, and compiler optimization, remain the foundation for
efficient execution even in distributed settings. Each individual
accelerator in a multi-chip system must still be optimized using these
principles. However, multi-chip architectures introduce additional
concerns around communication overhead, memory coherence, and fault
tolerance that transform the optimization landscape. The detailed
implementation of distributed training systems, including gradient
synchronization protocols, parameter server architectures, and
cluster-scale orchestration, is covered in advanced treatments of
machine learning infrastructure.

When single-accelerator capacity proves insufficient, AI systems must
scale across multiple chips. Understanding these scaling approaches is
essential for practitioners who will encounter multi-chip systems in
production environments, even when working primarily with
single-accelerator deployments.

\subsection{Multi-Chip Scaling
Approaches}\label{sec-ai-acceleration-multichip-scaling-approaches}

Modern AI systems employ several strategies to scale beyond individual
accelerators, each with distinct trade-offs.

One approach partitions large designs into smaller, modular dies
interconnected within a single package (chiplet-based architectures).
This approach bypasses manufacturing limits of monolithic chips while
maintaining relatively low communication latency within the package.

When even greater compute capacity is required, systems connect multiple
discrete accelerators, each with dedicated memory and compute resources.
This enables workloads to be split using data parallelism (each
accelerator processes different batches) or model parallelism (different
accelerators handle different network layers). High-bandwidth intra-node
interconnects can enable efficient gradient synchronization across the
system, though realized performance depends on topology and collective
communication efficiency.

At data center scale, purpose-built interconnect fabrics enable hundreds
of accelerators to work together. Cluster topology and collective
communication algorithms become central determinants of scaling
efficiency, and near-linear scaling is achievable on some workloads when
communication overhead is controlled.

The most aggressive scaling approach treats an entire silicon wafer as a
unified compute fabric. Wafer-scale integration platforms (e.g.,
Cerebras WSE-class systems) integrate extremely large numbers of
transistors and cores on a single device, reducing or eliminating
inter-chip communication overhead. This approach introduces its own
challenges in thermal dissipation, fault tolerance, and manufacturing
yield, representing the frontier of single-system compute density.

\subsection{Why Scaling Introduces New
Constraints}\label{sec-ai-acceleration-why-scaling-constraints}

The transition from single-chip to multi-chip architectures introduces
qualitatively different constraints that fundamentally transform system
optimization.

Communication overhead emerges as the primary limit on scaling
efficiency. Amdahl's Law\sidenote{Recall from
\textbf{?@sec-ml-system-architecture} that Amdahl's Law limits speedup
based on the serial fraction of computation. For distributed training,
this serial fraction includes gradient synchronization, which at 5\%
overhead caps maximum speedup at 20x regardless of GPU count. This
explains why scaling efficiency degrades at large cluster sizes and
motivates algorithmic innovations like gradient compression and
communication-computation overlap. } quantifies how communication during
gradient synchronization creates sequential bottlenecks. For
hundred-billion-parameter-scale models, AllReduce operations can require
exchanging hundreds of gigabytes of gradients per training step.

This communication overhead explains why scaling to very large
accelerator counts can show diminishing returns without algorithmic
innovations like gradient compression, overlap, or alternative
parallelization strategies.

Memory coherence presents another challenge at scale. Ensuring all
processors see consistent views of shared memory adds 10-50 ns latency
per access in traditional coherence protocols. For AI accelerators with
thousands of cores, this overhead becomes prohibitive, forcing explicit
memory management where programmers control data placement and
synchronization manually.

As systems grow larger, fault tolerance requirements increase
correspondingly. Large-scale systems must handle component failures
gracefully since the probability of at least one failure increases with
system size. TPU Pods implement sophisticated consensus algorithms to
maintain training consistency when optical links fail, while wafer-scale
systems incorporate redundant cores to tolerate localized silicon
defects.

Perhaps most significantly, the energy costs of data movement come to
dominate system design. Moving data across a TPU Pod's optical
interconnect can consume orders of magnitude more energy than on-chip
communication within individual TPUs. This energy differential
transforms distributed training into a careful balance between
computation parallelism and communication efficiency, a concern that
shapes both hardware architecture and algorithm design.

Data center scaling and edge deployment represent opposite ends of a
deployment spectrum, yet they share the same fundamental principles.
Data center scaling asks ``how do we coordinate many powerful chips?''
while edge scaling asks ``how do we fit useful AI into a few constrained
watts?'' Both questions share a common answer: match workload
characteristics to hardware capabilities while minimizing data movement.
The principles of compute specialization, memory hierarchy optimization,
and workload mapping apply at both scales; only the constraints differ.
Data centers optimize for aggregate throughput within power budgets
measured in megawatts; edge devices optimize for responsiveness within
power budgets measured in milliwatts.

\section{Heterogeneous SoC AI
Acceleration}\label{sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb}

At the edge end of the deployment spectrum, the hardware acceleration
principles established in this chapter---specialized compute units,
memory hierarchy optimization, and workload mapping strategies---must
operate under dramatically different constraints. A smartphone's SoC
operates within a 3-7 watt sustained power budget (with brief peaks to
10-15W), autonomous vehicles require deterministic sub-100ms latency for
perception-to-action loops, and IoT sensors must function for months to
years on battery power. These constraints necessitate heterogeneous
System-on-Chip (SoC) architectures that integrate CPU cores, GPU
shaders, digital signal processors (DSPs), and dedicated neural
processing units (NPUs) within a single chip. Orchestrating these
diverse processors to achieve optimal performance under strict power,
thermal, and latency requirements demands fundamentally different
approaches than data center deployments.

\subsection{Mobile SoC Architecture
Evolution}\label{sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8}

Qualcomm's Snapdragon AI Engine exemplifies heterogeneous computing for
mobile AI, coordinating CPU cores, GPU shaders, a DSP, and a dedicated
NPU\sidenote{NPUs (introduced in \textbf{?@sec-ml-system-architecture})
excel at low-power inference on common operator patterns but have less
programmability than GPUs. From an architecture perspective, NPUs
achieve efficiency by hardcoding common dataflows (systolic arrays for
matrix multiplication) rather than supporting general computation,
trading flexibility for 10-100x better energy efficiency on supported
operations. } across a shared memory hierarchy. Modern mobile SoCs use
workload distribution so that computer vision kernels can execute on the
GPU's parallel shaders, audio processing can leverage DSP arithmetic
units, and transformer attention mechanisms can utilize NPU-optimized
matrix engines. This coordination requires careful scheduling to meet
real-time constraints while managing thermal throttling and battery
life.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Representative snapshot (mobile SoC peak throughput, as of 2024)}, rightrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, breakable, toprule=.15mm, bottomtitle=1mm, colback=white, toptitle=1mm, bottomrule=.15mm, coltitle=black]

Public vendor materials often report peak NPU throughput on the order of
10s of TOPS for high-end mobile SoCs, typically measured on selected
operations and precisions. These numbers are not directly comparable
across vendors, and sustained throughput can be materially lower under
thermal limits.

\end{tcolorbox}

While Qualcomm's approach emphasizes diverse processor specialization,
vertically integrated strategies highlight how tight hardware-software
co-design can enable sophisticated heterogeneous execution. Unified
memory architectures can reduce explicit data copying overhead, and
different compute blocks can be scheduled for different operator types
(for example, matrix-heavy layers on an NPU, convolutional operators on
a GPU, and control flow on the CPU). This coordination supports
interactive on-device experiences, though realized latency depends on
the full pipeline and device thermal conditions.

Beyond vertically integrated solutions, IP licensing models allow SoC
designers to customize processor combinations based on target
applications, mixing CPU, GPU, DSP, and NPU blocks. This modular
flexibility allows automotive SoCs to emphasize deterministic real-time
processing while smartphone SoCs optimize for interactive performance
and battery efficiency.

\subsection{Strategies for Dynamic Workload
Distribution}\label{sec-ai-acceleration-strategies-dynamic-workload-distribution-a421}

With multiple specialized processors available on heterogeneous SoCs,
the critical challenge becomes intelligently distributing neural network
operations across these resources to maximize performance while
respecting power and latency constraints.

Modern neural networks require intelligent partitioning across
heterogeneous processors based on operation characteristics and current
system state. Convolutional layers with regular data access patterns
typically execute efficiently on GPU shader cores, while fully connected
layers with irregular sparsity patterns may perform better on
general-purpose CPU cores with large caches. Attention mechanisms in
transformers benefit from NPU matrix engines when sequences are long,
but may execute more efficiently on CPU when sequence lengths are small
due to the NPU setup overhead.

Beyond static operation-to-processor mapping, heterogeneous SoCs
implement dynamic processor selection based on multiple constraints:

\begin{itemize}
\tightlist
\item
  \textbf{Power Budget}: During battery operation, the system may route
  computations to lower-power DSP cores rather than high-performance GPU
  cores
\item
  \textbf{Thermal State}: When approaching thermal limits, workloads
  shift from power-hungry NPU to more efficient CPU execution
\item
  \textbf{Latency Requirements}: Safety-critical automotive applications
  prioritize deterministic CPU execution over potentially faster but
  variable NPU processing
\item
  \textbf{Concurrent Workload Interference}: Multiple AI applications
  may require load balancing across available processors to maintain
  Quality of Service
\end{itemize}

Compounding the processor selection challenge, shared memory
architectures require sophisticated arbitration when multiple processors
access LPDDR simultaneously. The Snapdragon 8 Gen 3's memory controller
implements priority-based scheduling where camera processing receives
higher priority than background AI tasks, ensuring real-time video
processing while background neural networks adapt their execution
patterns to available memory bandwidth. This arbitration becomes
critical during memory-intensive operations like large language model
inference, where parameter streaming from DRAM must be carefully
coordinated across processors.

\subsection{Power and Thermal
Management}\label{sec-ai-acceleration-power-thermal-management-6c00}

Mobile AI workloads must maintain high performance while operating
within strict power budgets and thermal envelopes. These constraints
require sophisticated coordination across heterogeneous processors.

Heterogeneous SoCs implement coordinated DVFS across multiple processors
to optimize the power-performance envelope. When one processor increases
frequency to meet latency demands, the system may reduce voltage on
other processors to maintain total power budget. This coordination
becomes complex in AI workloads where computational phases may shift
rapidly between processors. The system must predict upcoming workload
transitions to preemptively adjust operating points while avoiding
voltage/frequency oscillations that degrade efficiency.

When DVFS alone cannot maintain the power envelope, mobile SoCs
implement thermal throttling through intelligent task migration rather
than simple frequency reduction. When the NPU approaches thermal limits
during intensive neural network processing, the runtime system can
migrate layers to the GPU or CPU while maintaining computational
throughput. This approach preserves performance during thermal events,
though it requires sophisticated workload characterization to predict
execution time and power consumption across different processors.

Beyond real-time power and thermal management, mobile AI systems must
also adapt their computational strategies based on battery state and
charging status. During low battery conditions, the system may switch
from high-accuracy models to efficient approximations, migrate workloads
from power-hungry NPU to energy-efficient DSP, or reduce inference
frequency while maintaining application responsiveness. Conversely,
during charging, the system can enable higher-performance models and
increase processing frequency to deliver enhanced user experiences.

\subsection{Automotive Heterogeneous AI
Systems}\label{sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda}

Automotive applications introduce unique heterogeneous computing
challenges that combine mobile-style power efficiency with hard
real-time latency requirements and functional safety requirements. This
combination demands fundamentally different architectural approaches.

Automotive SoCs aim to provide deterministic inference latency for
safety-critical functions while supporting advanced driver assistance
systems (ADAS). The Snapdragon Ride platform coordinates multiple AI
accelerators across safety domains. Redundant processing elements
support functional safety objectives while high-performance accelerators
handle perception, planning, and control algorithms. This architecture
requires temporal isolation between safety-critical and convenience
functions, implemented through hardware partitioning and time-triggered
scheduling.

These safety requirements become even more complex when considering that
modern vehicles integrate multiple AI-enabled SoCs for different
domains. Vision processing SoCs handle camera-based perception, radar
processing SoCs manage RF sensor data, while central compute platforms
coordinate high-level decision making. These distributed systems must
maintain temporal coherence across sensor modalities with
microsecond-precision timing, requiring specialized inter-SoC
communication protocols and distributed synchronization mechanisms.

Extending beyond the vehicle's internal sensors, vehicle-to-everything
(V2X) communication adds another layer of heterogeneous processing where
AI algorithms must coordinate local sensor processing with information
received from other vehicles and infrastructure. This requires ultra-low
latency processing chains where 5G modems, AI accelerators, and control
systems operate within millisecond deadlines while maintaining
functional safety requirements.

\subsection{Software Stack
Challenges}\label{sec-ai-acceleration-software-stack-challenges-255c}

The architectural sophistication of heterogeneous SoCs creates
substantial software development challenges that span programming
models, memory management, and runtime optimization.

Programming heterogeneous SoCs requires frameworks that abstract
processor differences while exposing performance-critical optimization
opportunities. OpenCL and Vulkan provide cross-processor execution, but
achieving optimal performance requires processor-specific optimizations
that complicate portable development. Modern ML frameworks like
TensorFlow Lite and PyTorch Mobile implement automatic processor
selection, but developers still need to understand heterogeneous
execution patterns to achieve optimal results.

Complicating the programming challenge further, heterogeneous SoCs with
shared memory architectures require sophisticated memory management that
considers processor-specific caching behaviors, memory access patterns,
and coherency requirements. CPU caches may interfere with GPU memory
access patterns, while NPU direct memory access (DMA) operations must be
synchronized with CPU cache operations to maintain data consistency.

To address the complexity of manual optimization across these
dimensions, advanced heterogeneous SoCs implement machine learning-based
runtime optimization that learns from execution patterns to improve
processor selection, thermal management, and power optimization. These
systems collect telemetry on workload characteristics, processor
utilization, and power consumption to build models that predict optimal
execution strategies for new workloads.

This heterogeneous approach to AI acceleration represents the future of
computing, where no single processor architecture can optimally handle
the diverse computational patterns in modern AI applications.
Understanding these coordination challenges is essential for developing
efficient mobile AI systems that deliver high performance while meeting
the strict power, thermal, and real-time constraints of edge deployment
scenarios.

The complexity of hardware acceleration, spanning data center
architectures to heterogeneous mobile SoCs, creates opportunities for
misconception and suboptimal design decisions. The following section
distills common errors that waste expensive accelerator resources and
lead to deployments achieving only a fraction of theoretical
performance.

\phantomsection\label{quiz-question-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.9}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is a primary reason for using heterogeneous SoC architectures in
  mobile AI systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To increase computational throughput without power constraints.
  \item
    To maximize data center workload efficiency.
  \item
    To simplify the design process by using a single type of processor.
  \item
    To coordinate multiple specialized processors within strict power
    and thermal limits.
  \end{enumerate}
\item
  Explain how dynamic workload distribution strategies in heterogeneous
  SoCs help manage power and thermal constraints.
\item
  Order the following steps in managing workload distribution on a
  heterogeneous SoC: (1) Assess system power budget, (2) Evaluate
  processor thermal state, (3) Allocate tasks based on constraints, (4)
  Monitor performance and adjust.
\item
  Which of the following best describes a challenge in programming
  heterogeneous SoCs?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Managing memory coherency across diverse processors.
  \item
    Ensuring all processors use the same programming model.
  \item
    Achieving optimal performance without considering processor-specific
    optimizations.
  \item
    Implementing a single execution strategy for all tasks.
  \end{enumerate}
\item
  In a production system, what trade-offs might you consider when
  implementing AI acceleration on a heterogeneous SoC for an autonomous
  vehicle?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Fallacies and
Pitfalls}\label{sec-ai-acceleration-fallacies-pitfalls-dc1f}

Hardware acceleration involves counterintuitive performance
characteristics where impressive specifications mask fundamental
bottlenecks. The fallacies and pitfalls below capture hardware selection
and optimization errors that waste expensive accelerator resources and
lead to deployments that achieve only 10-30\% of theoretical
performance.

\textbf{Fallacy:} \emph{More specialized hardware always provides better
performance than general-purpose alternatives.}

Engineers assume specialized accelerators automatically outperform
general-purpose processors for all AI workloads. In reality, specialized
hardware achieves peak performance only when workloads match
architectural assumptions. As demonstrated in
Section~\ref{sec-ai-acceleration-roofline-model}, operations must exceed
the accelerator's ridge point to be compute-bound; an A100 GPU has a
ridge point of 156 FLOP/byte, meaning operations with arithmetic
intensity below this threshold are memory-bound regardless of the
accelerator's 312 TFLOPS peak compute. A transformer attention softmax
with AI = 2-5 FLOP/byte achieves only 4-10 TFLOPS (3\% utilization) on
an A100, while achieving 80-90\% of a CPU's lower peak because CPUs have
ridge points of 10-20 FLOP/byte. Models with irregular memory access,
small batch sizes, or dynamic computation graphs may perform better on
flexible processors. Effective hardware selection requires matching
workload arithmetic intensity to architectural ridge points, not
assuming specialization always wins.

\textbf{Pitfall:} \emph{Ignoring memory bandwidth limitations when
selecting acceleration strategies.}

Practitioners focus on peak TFLOPS without analyzing whether their
workloads can achieve compute-bound performance. As quantified in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
accessing DRAM consumes 100-200 pJ per access versus 1-10 pJ for on-chip
memory, creating orders-of-magnitude energy penalties. An accelerator
advertising 300 TFLOPS with 2 TB/s bandwidth has a ridge point of 150
FLOP/byte; LayerNorm operations with AI = 1.5 FLOP/byte achieve only 3
TFLOPS (1\% utilization). Organizations deploy expensive high-compute
accelerators for memory-bound workloads, achieving 10-20\% utilization
when lower-cost, bandwidth-optimized alternatives would perform
identically. Teams must calculate workload arithmetic intensity and
compare against hardware ridge points before purchasing accelerators.

\textbf{Fallacy:} \emph{Hardware acceleration benefits scale linearly
with additional accelerators.}

Teams expect 8 GPUs to train 8× faster than 1 GPU. Multi-accelerator
scaling introduces communication overhead that violates linear scaling
assumptions. As noted in
Section~\ref{sec-ai-acceleration-scaling-beyond-single}, AllReduce
operations for gradient synchronization can require exchanging hundreds
of gigabytes per training step for large models. With NVLink at 600 GB/s
bidirectional, synchronizing 1 GB of gradients requires 1.67 ms; for a
50 ms training step, this represents 3.3\% overhead with perfect
overlap. Without overlap, 8-GPU setups achieve 7.5× speedup (94\%
efficiency) at best, and typical workloads see 6-7× (75-87\% efficiency)
due to load imbalance and synchronization barriers. Small models with
insufficient parallel work achieve even worse scaling, sometimes seeing
3-4× speedup on 8 GPUs (37-50\% efficiency).

\textbf{Fallacy:} \emph{Peak FLOPS specifications determine real-world
accelerator performance.}

Vendors advertise peak TFLOPS under optimal conditions (large matrix
multiplies, perfect memory access patterns, Tensor Core utilization).
Real workloads rarely achieve these peaks. An A100 advertises 312 TFLOPS
for FP16 with Tensor Cores, but typical transformer training achieves
120-180 TFLOPS (38-58\% utilization) due to memory-bound operations
(attention, LayerNorm), kernel launch overhead, and imperfect batching.
Recommendation models with sparse embeddings achieve 30-80 TFLOPS
(10-25\% utilization) despite the same peak specification. Organizations
budget projects assuming peak performance, then experience 2-3× longer
training times than predicted. Effective capacity planning requires
measuring sustained throughput on representative workloads, not relying
on peak specifications.

\textbf{Pitfall:} \emph{Deploying small-batch inference workloads on
high-compute accelerators.}

Teams deploy high-throughput training accelerators (A100, H100) for
latency-sensitive inference with batch size 1-4. Small batches severely
reduce arithmetic intensity: a dense layer with M=N=2048 achieves AI = 2
FLOP/byte at batch=1 versus AI = 186 FLOP/byte at batch=256. At batch=1,
an A100 achieves 4 TFLOPS (1.3\% utilization) due to memory bottlenecks,
while a lower-cost T4 achieves 3.5 TFLOPS. For FP16 Tensor Core
operations, the T4's peak is 65 TFLOPS with a ridge point of
approximately 203 FLOP/byte (65 TFLOPS / 320 GB/s); for FP32 operations,
the peak drops to 8.1 TFLOPS with a ridge point of only 25 FLOP/byte.
Either way, small-batch inference remains memory-bound, but the T4's
lower cost makes it more economical for these workloads. The cost
difference is substantial: A100 instances cost 3-4× more than T4
instances for identical latency. Inference deployments should match
batch size to accelerator characteristics, using high-compute
accelerators only for batched serving where arithmetic intensity exceeds
ridge points.

\textbf{Pitfall:} \emph{Vendor-specific optimizations without
considering long-term portability.}

Organizations optimize exclusively for specific vendors to maximize
performance without considering system flexibility. As discussed in
Section~\ref{sec-ai-acceleration-compiler-support-172e}, deep
integration with vendor-specific libraries (CUDA, TensorRT, XLA) and
custom kernels creates lock-in. A codebase with 50+ hand-written CUDA
kernels requires 6-12 engineer-months to port to a different accelerator
vendor, delaying hardware upgrades and preventing multi-vendor
deployments. While vendor-specific optimizations provide 20-40\%
performance gains, they should be isolated behind hardware abstraction
layers. Maintaining portable code paths enables vendor competition,
hardware flexibility, and faster adoption of emerging accelerators while
still capturing most performance benefits through framework-level
optimizations.

\textbf{Hardware Selection Checklist}

Before procuring accelerators, work through this decision framework:

\textbf{Step 1: Characterize Your Workload}

\begin{itemize}
\tightlist
\item
  Calculate arithmetic intensity for dominant operations (use roofline
  analysis from Section~\ref{sec-ai-acceleration-roofline-model})
\item
  Determine memory footprint: Do model weights + activations fit in
  accelerator memory?
\item
  Identify batch size requirements: Single-request latency or batched
  throughput?
\end{itemize}

\textbf{Step 2: Match Workload to Hardware}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4324}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2883}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2703}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{If Your Workload Is\ldots{}}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Prioritize\ldots{}}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deprioritize\ldots{}}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
****Memory-bound** (AI \textless{} 50 FLOP/byte)** & Memory bandwidth
(GB/s) & Peak TFLOPS \\
****Compute-bound** (AI \textgreater{} 200 FLOP/byte)** & Peak TFLOPS,
Tensor Cores & Memory capacity \\
****Latency-sensitive** (batch=1-4)** & Lower-cost inference cards &
High-compute training cards \\
****Throughput-oriented** (large batches)** & High-compute datacenter
GPUs & Edge accelerators \\
****Memory-constrained** (\textgreater40GB model)** & Memory capacity
(80GB+ cards) & Entry-level accelerators \\
\end{longtable}

\textbf{Step 3: Validate Before Committing}

\begin{itemize}
\tightlist
\item
  Run representative workloads on target hardware (cloud instances
  enable low-cost trials)
\item
  Measure \emph{sustained} throughput, not peak benchmarks
\item
  Profile to confirm whether bottleneck matches expectations (compute
  vs.~memory)
\item
  Calculate cost-per-inference or cost-per-training-step, not just
  hardware cost
\end{itemize}

\textbf{Step 4: Plan for Evolution}

\begin{itemize}
\tightlist
\item
  Will model sizes increase? Budget for memory headroom.
\item
  Will batch sizes change in production? Test across expected range.
\item
  Can you switch vendors if pricing or performance shifts? Maintain
  portable code paths.
\end{itemize}

This checklist synthesizes the principles developed throughout this
chapter, translating theoretical understanding into practical
engineering decisions.

Beyond performance optimization, hardware decisions carry broader
implications. As AI systems scale to planetary deployment, the
environmental impact of these choices demands consideration alongside
traditional metrics.

\phantomsection\label{quiz-question-sec-ai-acceleration-fallacies-pitfalls-dc1f}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.10}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-fallacies-pitfalls-dc1f}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following scenarios would most likely benefit from using
  general-purpose processors over specialized hardware accelerators?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    A workload with irregular memory access patterns and dynamic
    computation graphs.
  \item
    A workload with dense, regular computations and large batch sizes.
  \item
    A workload that requires high computational throughput with minimal
    memory access.
  \item
    A workload optimized for a specific vendor's proprietary libraries.
  \end{enumerate}
\item
  True or False: Adding more accelerators to a system will always result
  in linear performance improvements.
\item
  Explain why memory bandwidth limitations can undermine the performance
  benefits of AI accelerators.
\item
  The belief that hardware acceleration benefits scale linearly with
  additional accelerators is a common \_\_\_\_. This misconception
  overlooks the communication and synchronization overheads that limit
  scaling efficiency.
\item
  In a production system, what trade-offs should be considered when
  optimizing for vendor-specific hardware?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-fallacies-pitfalls-dc1f]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Hardware Responsibility: The Sustainability
Dimension}\label{sec-ai-acceleration-sustainability}

To surpass the traditional focus on raw performance, we must evaluate
hardware through the lens of \textbf{Silicon Sustainability}. In the era
of planetary-scale AI, ``Performance per Watt'' is not just a mobile
constraint---it is a global environmental mandate.

\phantomsection\label{callout-perspectiveux2a-1.16}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Carbon ROI of Specialized Silicon}
\phantomsection\label{callout-perspective*-1.16}
\textbf{The Problem}: Should you run your inference fleet on generic
\textbf{CPUs} or invest in specialized \textbf{NPUs} (Neural Processing
Units)?

\textbf{The Physics}: Specialized hardware achieves higher
\textbf{Arithmetic Intensity} while using fewer transistors for control
logic. * \textbf{CPU Inference}: 100 Watts for 1 TFLOP (Efficiency =
0.01 TFLOPS/W). * \textbf{NPU Inference}: 5 Watts for 10 TFLOPS
(Efficiency = 2.0 TFLOPS/W). * \textbf{The Gap}: The NPU is \textbf{200x
more energy-efficient} per operation.

\textbf{The Calculation}: 1. \textbf{Workload}: 1 Billion inferences per
day. 2. \textbf{CPU Energy}: \textasciitilde2,400 kWh/day. 3.
\textbf{NPU Energy}: \textasciitilde12 kWh/day. 4. \textbf{Carbon
Savings}: At 0.4 kg CO2/kWh, switching to NPUs saves
\textbf{\textasciitilde350 metric tons of CO2 per year}.

\textbf{The Systems Conclusion}: Custom silicon is the ultimate
``Green'' technology for ML. Investing in specialized accelerators like
the \textbf{Lighthouse NPU} isn't just about speed; it is the single
most effective way to reduce the carbon footprint of intelligence.

\end{fbx}

\section{Summary}\label{sec-ai-acceleration-summary-a5f8}

The preceding sections established a decision framework for hardware
selection and a sustainability perspective that grounds these choices in
broader responsibility. Hardware acceleration has emerged as the enabler
that transforms machine learning from academic curiosity to practical
reality, reshaping how we design both computational systems and the
algorithms that run on them. The evolution from general-purpose
processors to specialized AI accelerators represents more than just
incremental improvement---it reflects a paradigm shift toward
domain-specific computing where hardware and software are co-designed to
optimize specific computational patterns. The journey from CPUs through
GPUs to specialized TPUs, NPUs, and wafer-scale systems demonstrates how
understanding workload characteristics drives architectural innovation,
creating opportunities for orders-of-magnitude performance improvements
through targeted specialization.

The technical challenges of AI acceleration span multiple layers of the
computing stack, from low-level memory hierarchy optimization to
high-level compiler transformations and runtime orchestration. Memory
bandwidth limitations create fundamental bottlenecks that require
sophisticated techniques like data tiling, kernel fusion, and
hierarchy-aware scheduling to overcome. Mapping neural network
computations to hardware involves complex trade-offs between different
dataflow patterns, memory allocation strategies, and execution
scheduling approaches that must balance computational efficiency with
resource utilization.

Building on these foundational concepts, the emergence of multi-chip and
distributed acceleration systems introduces additional complexities
around communication overhead, memory coherence, and workload
partitioning that require careful system-level optimization.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, rightrule=.15mm, opacityback=0, colframe=quarto-callout-important-color-frame, colbacktitle=quarto-callout-important-color!10!white, titlerule=0mm, breakable, toprule=.15mm, bottomtitle=1mm, colback=white, toptitle=1mm, bottomrule=.15mm, coltitle=black]

\begin{itemize}
\item
  \textbf{The Roofline model identifies performance bottlenecks}:
  Plotting arithmetic intensity against throughput reveals whether
  workloads are memory-bound (attention, embeddings) requiring bandwidth
  optimization, or compute-bound (convolutions, GEMMs) requiring FLOPS
  optimization.
\item
  \textbf{Memory bandwidth constrains performance}: GPU compute capacity
  has grown orders of magnitude faster than memory bandwidth over the
  past two decades. Most inference workloads are memory-bound, making
  data movement optimization the primary concern.
\item
  \textbf{Hardware-software co-design can achieve 10--100× performance
  improvements}: Matching algorithm patterns to architectural
  capabilities (systolic arrays for dense GEMM, sparse accelerators for
  pruned models) typically outperforms raw hardware upgrades.
\item
  \textbf{Tensor Cores require specific conditions}: FP16 inputs,
  appropriate tensor dimensions, and sufficient batch size are necessary
  for peak utilization. Batch size directly affects arithmetic intensity
  and determines whether workloads reach the compute-bound regime.
\item
  \textbf{Arithmetic intensity determines optimization strategy}:
  Operations with low arithmetic intensity (1--2 FLOP/byte, like
  LayerNorm) are memory-bound; operations with high intensity (50--200
  FLOP/byte, like convolutions) are compute-bound. The ridge point
  (e.g., 156 FLOP/byte for A100) marks the transition.
\end{itemize}

\end{tcolorbox}

The principles of hardware acceleration established here provide the
foundation for understanding how benchmarking methodologies evaluate
accelerator performance and how deployment strategies must account for
hardware constraints and capabilities.

\phantomsection\label{callout-chapter-connectionux2a-1.17}
\begin{fbxSimple}{callout-chapter-connection}{Related Topics: }{From Hardware to Data: The Final Optimization Frontier}
\phantomsection\label{callout-chapter-connection*-1.17}
We have compressed the model (reducing parameters and precision) and
accelerated the hardware (maximizing throughput per watt). These
optimizations follow a historical pattern: first we made algorithms more
efficient, then we made silicon faster. But a third frontier has
emerged---and it may be the most impactful of all.

Consider the economics: GPU compute scales exponentially (roughly 10×
every 3-4 years), but high-quality training data grows far slower
(perhaps 2× per decade). The internet has been scraped. Domain experts
cannot label faster. We are increasingly \textbf{compute-rich and
data-poor}.

This asymmetry inverts the optimization priority. When compute was
scarce and data abundant, the winning strategy was hardware
acceleration. Now that compute is abundant and quality data is scarce,
the winning strategy is \textbf{data efficiency}---extracting maximum
learning from every sample.

\textbf{?@sec-data-efficiency} addresses this newest optimization
frontier:

\begin{itemize}
\tightlist
\item
  \textbf{Static data pruning}: Removing redundant samples before
  training begins---often cutting datasets by 30-50\% with minimal
  accuracy impact
\item
  \textbf{Dynamic selection}: Curriculum learning and active learning to
  prioritize high-value samples during training
\item
  \textbf{Synthetic generation}: Creating targeted training examples to
  fill gaps the original dataset cannot cover
\item
  \textbf{Systems engineering}: Data loaders, distributed selection, and
  I/O optimization for efficient data pipelines
\end{itemize}

The hardware understanding from this chapter directly enables data
efficiency analysis. Memory bandwidth determines whether data loading or
computation dominates training cost. Arithmetic intensity reveals
whether a workload is compute-bound (benefit from better algorithms) or
memory-bound (benefit from smarter data pipelines).

We have optimized the math and the silicon. Now we optimize the input:
\textbf{Data Efficiency} (\textbf{?@sec-data-efficiency}) completes the
optimization trilogy.

\end{fbxSimple}

\phantomsection\label{quiz-question-sec-ai-acceleration-summary-a5f8}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.11}{}
\phantomsection\label{quiz-question-sec-ai-acceleration-summary-a5f8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the primary advantage of
  hardware-software co-design in AI accelerators?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Reduced hardware costs
  \item
    Simplified software development
  \item
    Improved computational efficiency
  \item
    Increased general-purpose applicability
  \end{enumerate}
\item
  Explain how memory hierarchy management can become a bottleneck in AI
  acceleration and how it can be mitigated.
\item
  Order the following steps in optimizing a multi-chip AI acceleration
  system: (1) Workload partitioning, (2) Communication overhead
  reduction, (3) Memory coherence management.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-acceleration-summary-a5f8]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Self-Check Answers}\label{self-check-answers}

\phantomsection\label{quiz-answer-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.1}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary reason for the shift from general-purpose
  processors to domain-specific hardware in machine learning systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To reduce the cost of hardware components
  \item
    To improve the parallel processing capabilities and efficiency
  \item
    To simplify the design of machine learning algorithms
  \item
    To increase the utilization of existing software optimizations
  \end{enumerate}

  \emph{Answer}: The correct answer is B. To improve the parallel
  processing capabilities and efficiency. This shift is driven by the
  need to address the architectural misalignments of general-purpose
  processors with the parallel, data-intensive nature of ML workloads.

  \emph{Learning Objective}: Understand the motivations behind using
  domain-specific hardware in ML systems.
\item
  \textbf{True or False: Hardware acceleration in machine learning
  systems only focuses on improving computational speed, not energy
  efficiency.}

  \emph{Answer}: False. Hardware acceleration also aims to improve
  energy efficiency, as data movement energy costs typically exceed
  computational energy by more than two orders of magnitude.

  \emph{Learning Objective}: Recognize the dual goals of hardware
  acceleration: improving computational speed and energy efficiency.
\item
  \textbf{How do architectural selection decisions impact system-level
  performance in machine learning systems?}

  \emph{Answer}: Architectural selection decisions impact system-level
  performance by determining the efficiency of computations, energy
  usage, and implementation complexity. For example, choosing between
  GPUs, TPUs, or neuromorphic processors affects how well a system can
  handle specific ML workloads. This is important because it influences
  the overall effectiveness and cost-efficiency of deploying ML systems.

  \emph{Learning Objective}: Analyze the impact of different hardware
  architectures on ML system performance.
\item
  \textbf{Which of the following architectural innovations is used to
  optimize matrix multiplication in machine learning workloads?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Floating-point coprocessors
  \item
    Sequential processing models
  \item
    Systolic array architectures
  \item
    High-bandwidth memory interfaces
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Systolic array architectures.
  These are used to optimize matrix multiplication by efficiently
  managing data flow and computation.

  \emph{Learning Objective}: Identify architectural innovations that
  optimize key ML operations.
\item
  \textbf{In a production system, what trade-offs might you consider
  when choosing between single-chip and multi-chip architectures for AI
  acceleration?}

  \emph{Answer}: When choosing between single-chip and multi-chip
  architectures, trade-offs include balancing computational parallelism
  with inter-chip communication overhead. Single-chip solutions may
  offer lower latency and simpler integration, while multi-chip
  architectures can provide greater computational capacity but may
  introduce complexity and communication delays. This is important
  because it affects the scalability and performance of AI systems in
  different deployment contexts.

  \emph{Learning Objective}: Evaluate trade-offs in architectural
  choices for AI acceleration in production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-evolution-hardware-specialization-fdb7}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.2}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-evolution-hardware-specialization-fdb7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the primary motivation
  for the development of specialized hardware accelerators in
  computing?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To reduce the cost of general-purpose processors
  \item
    To increase the flexibility of computing systems
  \item
    To handle increasingly complex computational workloads efficiently
  \item
    To simplify the programming models for developers
  \end{enumerate}

  \emph{Answer}: The correct answer is C. To handle increasingly complex
  computational workloads efficiently. Specialized hardware accelerators
  are developed to optimize performance and energy efficiency for
  specific tasks, addressing the limitations of general-purpose
  processors.

  \emph{Learning Objective}: Understand the motivations behind the shift
  from general-purpose processors to specialized hardware accelerators.
\item
  \textbf{Explain how the evolution of specialized hardware has
  influenced the design of modern machine learning accelerators.}

  \emph{Answer}: The evolution of specialized hardware, such as FPUs and
  GPUs, has informed the design of modern ML accelerators by
  demonstrating the benefits of optimizing hardware for specific
  computational patterns. This approach has led to significant
  performance and efficiency gains in executing neural network
  workloads, which are characterized by predictable data flows and
  parallelism. For example, tensor cores in GPUs are specifically
  designed for matrix operations, a common pattern in ML.

  \emph{Learning Objective}: Analyze the influence of historical
  hardware specialization on the design of contemporary ML accelerators.
\item
  \textbf{True or False: The integration of specialized functions into
  general-purpose processors is a common trend observed in the evolution
  of computing architectures.}

  \emph{Answer}: True. This is true because successful specialized
  functions, like floating-point units, are often integrated into
  general-purpose processors to enhance their capabilities and
  efficiency over time.

  \emph{Learning Objective}: Recognize the trend of integrating
  specialized functions into general-purpose processors in computing
  history.
\item
  \textbf{What is a key trade-off introduced by the use of specialized
  hardware accelerators?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increased flexibility in programming
  \item
    Reduced programming complexity
  \item
    Higher energy consumption
  \item
    Reduced silicon area utilization
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Reduced silicon area
  utilization. Specialized hardware accelerators optimize performance
  for specific tasks, which can lead to trade-offs in flexibility and
  silicon area utilization, as they are not as versatile as
  general-purpose processors.

  \emph{Learning Objective}: Identify trade-offs associated with the use
  of specialized hardware accelerators in computing systems.
\item
  \textbf{In a production system, how might the choice of hardware
  accelerators impact the deployment of machine learning models?}

  \emph{Answer}: The choice of hardware accelerators can significantly
  impact the deployment of ML models by affecting performance, energy
  efficiency, and scalability. For example, using TPUs can accelerate
  training and inference tasks, reducing time-to-market and operational
  costs. However, it may also require adjustments in software frameworks
  and programming models to fully leverage the hardware's capabilities.
  This choice must balance performance gains with integration and
  development costs.

  \emph{Learning Objective}: Evaluate the impact of hardware accelerator
  choices on the deployment and operation of machine learning models in
  production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-evolution-hardware-specialization-fdb7]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-ai-compute-primitives-2c99}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.3}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-ai-compute-primitives-2c99}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary role of AI compute primitives in neural
  network execution?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To optimize the execution of core computational patterns in neural
    networks
  \item
    To provide a high-level programming interface for machine learning
    frameworks
  \item
    To replace general-purpose CPUs in all computing tasks
  \item
    To ensure compatibility across different neural network
    architectures
  \end{enumerate}

  \emph{Answer}: The correct answer is A. To optimize the execution of
  core computational patterns in neural networks. AI compute primitives
  are designed to efficiently handle the multiply-accumulate operations
  that dominate neural network workloads. Options B, C, and D do not
  accurately describe the role of compute primitives.

  \emph{Learning Objective}: Understand the function and importance of
  AI compute primitives in optimizing neural network computations.
\item
  \textbf{Explain how vector operations enhance the efficiency of neural
  network computations in AI accelerators.}

  \emph{Answer}: Vector operations enhance efficiency by processing
  multiple data elements simultaneously, reducing computation time and
  energy consumption. For example, vector processing units can perform
  multiple multiply-add operations in parallel, maximizing memory
  bandwidth utilization and improving throughput. This is important
  because it enables high-performance execution of neural network
  layers, which rely on data-parallel computations.

  \emph{Learning Objective}: Analyze the role of vector operations in
  improving computational efficiency in AI systems.
\item
  \textbf{The hardware component that performs non-linear
  transformations like ReLU and sigmoid in a single cycle is known as
  the \_\_\_\_. }

  \emph{Answer}: Special Function Unit. This unit is designed to
  efficiently handle non-linear functions, reducing computational
  latency and improving performance in neural networks.

  \emph{Learning Objective}: Recall the specific hardware components
  used for non-linear operations in AI accelerators.
\item
  \textbf{Order the following computational steps for executing a dense
  layer in a neural network: (1) Apply activation function, (2) Multiply
  inputs by weights, (3) Add bias.}

  \emph{Answer}: The correct order is: (2) Multiply inputs by weights,
  (3) Add bias, (1) Apply activation function. This sequence reflects
  the typical computation in a dense layer, where inputs are first
  transformed by weights, then adjusted by biases, and finally passed
  through an activation function.

  \emph{Learning Objective}: Understand the sequence of operations in
  neural network layer computations.
\item
  \textbf{Which of the following is NOT a characteristic of AI compute
  primitives?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They are frequently used in neural network computations.
  \item
    They offer significant energy efficiency gains.
  \item
    They are designed to replace all general-purpose computing tasks.
  \item
    They remain stable across different neural network architectures.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. They are designed to replace
  all general-purpose computing tasks. AI compute primitives are
  specifically optimized for neural network tasks and do not replace all
  general-purpose computing tasks. Options A, B, and D accurately
  describe characteristics of AI compute primitives.

  \emph{Learning Objective}: Identify the characteristics and
  limitations of AI compute primitives in machine learning systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-ai-compute-primitives-2c99]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-ai-memory-systems-0057}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.4}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-ai-memory-systems-0057}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary constraint that defines the AI memory
  wall?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    The limited number of compute units available in accelerators.
  \item
    The cost of high-bandwidth memory compared to traditional DRAM.
  \item
    The energy consumption of arithmetic operations compared to memory
    access.
  \item
    The disparity between computational throughput and memory bandwidth.
  \end{enumerate}

  \emph{Answer}: The correct answer is D. The disparity between
  computational throughput and memory bandwidth. This is correct because
  the AI memory wall is the fundamental bottleneck due to the growing
  gap between the two, limiting accelerator performance.

  \emph{Learning Objective}: Understand the concept of the AI memory
  wall and its implications on accelerator performance.
\item
  \textbf{Explain how memory hierarchies in AI accelerators balance
  speed, capacity, and energy efficiency.}

  \emph{Answer}: Memory hierarchies balance these factors by using
  multiple levels of memory, each optimized for different trade-offs.
  Registers and caches provide fast access for frequently used data,
  while larger but slower memories like DRAM offer greater capacity for
  less frequently accessed data. This structure minimizes latency and
  energy consumption while maximizing data availability for compute
  units.

  \emph{Learning Objective}: Analyze how memory hierarchies are
  structured to optimize AI accelerator performance.
\item
  \textbf{The energy penalty for accessing \_\_\_\_ is significantly
  higher than for computation, influencing AI accelerator design.}

  \emph{Answer}: DRAM. The energy penalty for accessing DRAM is
  significantly higher than for computation, influencing AI accelerator
  design to minimize off-chip memory access.

  \emph{Learning Objective}: Recall the energy implications of different
  memory access types in AI systems.
\item
  \textbf{Which neural network architecture is most likely to be
  constrained by memory capacity and interconnect bandwidth?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Transformer Networks
  \item
    Convolutional Neural Networks (CNNs)
  \item
    Multilayer Perceptrons (MLPs)
  \item
    Recurrent Neural Networks (RNNs)
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Transformer Networks. This is
  because transformers have massive parameter sizes and irregular access
  patterns, which create significant demands on both memory capacity and
  interconnect bandwidth.

  \emph{Learning Objective}: Identify how different neural network
  architectures impose distinct memory constraints.
\item
  \textbf{In a system design scenario, how might you address the memory
  bottlenecks imposed by transformer networks?}

  \emph{Answer}: To address memory bottlenecks in transformer networks,
  one could use high-bandwidth memory to reduce latency, employ
  high-speed interconnects for faster data transfer, and optimize data
  movement with DMA engines. Additionally, leveraging attention caching
  and tensor tiling can minimize redundant memory accesses, improving
  overall efficiency.

  \emph{Learning Objective}: Evaluate strategies to mitigate memory
  bottlenecks in specific neural network architectures.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-ai-memory-systems-0057]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.5}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the primary goal of
  mapping in AI acceleration?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Optimizing execution efficiency by aligning computations with
    hardware resources.
  \item
    Minimizing the energy consumption of the accelerator.
  \item
    Maximizing the number of processing elements used at any time.
  \item
    Ensuring all computations are executed in parallel.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Optimizing execution
  efficiency by aligning computations with hardware resources. This is
  correct because mapping aims to maximize resource utilization and
  minimize memory access costs by strategically placing computations.

  \emph{Learning Objective}: Understand the primary objectives of
  mapping in AI acceleration.
\item
  \textbf{True or False: Effective computation placement on AI
  accelerators always requires manual intervention by developers.}

  \emph{Answer}: False. This is false because specialized compilers are
  typically used to automate the mapping process, exploring the search
  space to find optimal execution plans.

  \emph{Learning Objective}: Recognize the role of compilers in
  automating computation placement on AI accelerators.
\item
  \textbf{Why is data locality critical in the mapping of neural
  networks onto AI accelerators?}

  \emph{Answer}: Data locality is critical because it minimizes latency
  and power consumption by keeping frequently accessed data close to
  processing elements. For example, in specialized matrix processing
  architectures, data must be preloaded into on-chip scratchpads to
  maintain efficient execution. This is important because poor data
  locality can lead to excessive memory access, increasing latency and
  energy use.

  \emph{Learning Objective}: Explain the importance of data locality in
  neural network mapping.
\item
  \textbf{Order the following steps in the mapping process for neural
  networks on AI accelerators: (1) Data placement, (2) Computation
  scheduling, (3) Data movement timing.}

  \emph{Answer}: The correct order is: (1) Data placement, (3) Data
  movement timing, (2) Computation scheduling. Data placement determines
  where data is stored, data movement timing manages the transfer
  between memory levels, and computation scheduling organizes the
  execution order.

  \emph{Learning Objective}: Understand the sequential steps involved in
  the mapping process for neural networks.
\item
  \textbf{In a production system, how might poor computation placement
  affect the performance of AI accelerators?}

  \emph{Answer}: Poor computation placement can lead to underutilized
  processing elements, increased data movement, and execution stalls.
  For example, if computations are not evenly distributed, some elements
  may remain idle while others are overloaded. This is important because
  it can significantly degrade system throughput and efficiency.

  \emph{Learning Objective}: Analyze the impact of computation placement
  on AI accelerator performance in practical scenarios.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-dataflow-optimization-strategies-ce52}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.6}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-dataflow-optimization-strategies-ce52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following dataflow strategies keeps weights fixed
  in local memory while streaming input activations through the system?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Weight Stationary
  \item
    Input Stationary
  \item
    Output Stationary
  \item
    Activation Stationary
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Weight Stationary. This
  strategy keeps weights in local memory to maximize reuse, reducing
  redundant memory fetches and improving energy efficiency.

  \emph{Learning Objective}: Understand the basic concept of weight
  stationary dataflow strategy.
\item
  \textbf{True or False: In an output stationary dataflow strategy,
  input activations are kept fixed in local memory.}

  \emph{Answer}: False. In output stationary dataflow, partial sums are
  kept fixed in local memory, while weights and input activations stream
  through the system.

  \emph{Learning Objective}: Distinguish between different dataflow
  strategies and their memory usage.
\item
  \textbf{What are the trade-offs of using an input stationary strategy
  in a transformer model?}

  \emph{Answer}: Input stationary strategies keep input activations
  fixed, reducing redundant fetches and improving data locality.
  However, it requires efficient streaming of weights and partial sums,
  which can be challenging if memory bandwidth is limited. This strategy
  is beneficial in transformer models where input reuse is high.

  \emph{Learning Objective}: Analyze the trade-offs of input stationary
  strategies in specific AI models.
\item
  \textbf{In a system design scenario, which dataflow strategy would be
  most effective for a CNN with high weight reuse?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Activation Stationary
  \item
    Output Stationary
  \item
    Input Stationary
  \item
    Weight Stationary
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Weight Stationary. CNNs
  benefit from weight stationary strategies due to their structured
  weight reuse across spatial locations, reducing memory bandwidth
  demands.

  \emph{Learning Objective}: Apply dataflow strategy concepts to
  real-world AI systems.
\item
  \textbf{How might you decide between using a weight stationary or
  output stationary strategy in a new AI model?}

  \emph{Answer}: The decision depends on the model's computational
  pattern and memory constraints. Weight stationary is ideal for models
  with high weight reuse, like CNNs, while output stationary suits
  models where accumulation dominates, like fully connected layers.
  Consider memory bandwidth, reuse patterns, and hardware capabilities.

  \emph{Learning Objective}: Evaluate dataflow strategies based on model
  characteristics and system constraints.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-dataflow-optimization-strategies-ce52]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-compiler-support-172e}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.7}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-compiler-support-172e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following is a primary focus of machine learning
  compilers compared to traditional compilers?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Graph transformations and kernel fusion
  \item
    Instruction scheduling and register allocation
  \item
    Loop unrolling and memory allocation
  \item
    Sequential program optimization
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Graph transformations and
  kernel fusion. This is correct because ML compilers optimize
  computation graphs for efficient tensor operations, unlike traditional
  compilers that focus on linear code execution.

  \emph{Learning Objective}: Understand the primary optimization focus
  of ML compilers compared to traditional compilers.
\item
  \textbf{Explain why kernel fusion is important in machine learning
  compilers.}

  \emph{Answer}: Kernel fusion is important because it merges
  consecutive operations to reduce memory writes and kernel launches,
  enhancing execution efficiency. For example, in CNNs, fusing
  convolution, batch normalization, and activation functions accelerates
  processing. This is important because it minimizes redundant data
  movement and optimizes parallel execution.

  \emph{Learning Objective}: Explain the role and benefits of kernel
  fusion in optimizing ML models.
\item
  \textbf{Order the following stages in the ML compilation pipeline: (1)
  Graph Optimization, (2) Memory Planning, (3) Kernel Selection, (4)
  Computation Scheduling.}

  \emph{Answer}: The correct order is: (1) Graph Optimization, (3)
  Kernel Selection, (2) Memory Planning, (4) Computation Scheduling.
  Graph optimization restructures the computation graph, kernel
  selection maps operations to efficient implementations, memory
  planning optimizes data placement, and computation scheduling
  determines execution timing.

  \emph{Learning Objective}: Understand the sequence of stages in the ML
  compilation pipeline and their roles.
\item
  \textbf{In a production system, what trade-offs might you consider
  when selecting kernels for ML model execution?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Precision versus performance
  \item
    All of the above
  \item
    Execution speed versus memory usage
  \item
    Power consumption versus accuracy
  \end{enumerate}

  \emph{Answer}: The correct answer is B. All of the above. Kernel
  selection involves trade-offs between precision, power consumption,
  execution speed, and memory usage, impacting overall model performance
  and resource efficiency.

  \emph{Learning Objective}: Identify trade-offs involved in kernel
  selection for ML model execution.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-compiler-support-172e]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-runtime-support-f94f}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.8}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-runtime-support-f94f}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes a key function of AI
  runtimes in machine learning systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Static memory allocation
  \item
    Sequential task execution
  \item
    Dynamic kernel execution management
  \item
    Fixed execution plans
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Dynamic kernel execution
  management. AI runtimes dynamically manage kernel execution to adapt
  to real-time system conditions, unlike static memory allocation or
  fixed execution plans.

  \emph{Learning Objective}: Understand the dynamic execution management
  role of AI runtimes.
\item
  \textbf{How do AI runtimes differ from traditional software runtimes
  in terms of memory management?}

  \emph{Answer}: AI runtimes dynamically allocate and manage large
  tensors, optimizing memory access for parallel execution, unlike
  traditional runtimes that use static allocation for small, frequent
  memory operations. This is important because it prevents bottlenecks
  and excessive data movement in AI workloads.

  \emph{Learning Objective}: Explain the differences in memory
  management between AI and traditional runtimes.
\item
  \textbf{Order the following tasks in AI runtime management: (1) Memory
  adaptation, (2) Kernel execution management, (3) Execution scaling.}

  \emph{Answer}: The correct order is: (2) Kernel execution management,
  (1) Memory adaptation, (3) Execution scaling. AI runtimes first manage
  kernel execution based on system state, then adapt memory allocation,
  and finally scale execution across accelerators.

  \emph{Learning Objective}: Understand the sequence of tasks managed by
  AI runtimes.
\item
  \textbf{In a production system, what might be a consequence of poor
  dynamic kernel execution management?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Improved parallel execution
  \item
    Increased latency and resource underutilization
  \item
    Reduced memory requirements
  \item
    Enhanced sequential processing
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Increased latency and resource
  underutilization. Poor dynamic kernel execution management can lead to
  inefficient resource use and higher latency due to suboptimal
  adaptation to runtime conditions.

  \emph{Learning Objective}: Analyze the impact of dynamic kernel
  execution management on system performance.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-runtime-support-f94f]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.9}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is a primary reason for using heterogeneous SoC
  architectures in mobile AI systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To increase computational throughput without power constraints.
  \item
    To maximize data center workload efficiency.
  \item
    To simplify the design process by using a single type of processor.
  \item
    To coordinate multiple specialized processors within strict power
    and thermal limits.
  \end{enumerate}

  \emph{Answer}: The correct answer is D. To coordinate multiple
  specialized processors within strict power and thermal limits. This is
  correct because mobile AI systems operate under stringent constraints
  that require efficient coordination of diverse processors. Options A,
  B, and C do not address the specific challenges of mobile
  environments.

  \emph{Learning Objective}: Understand the motivation behind using
  heterogeneous SoC architectures in constrained environments.
\item
  \textbf{Explain how dynamic workload distribution strategies in
  heterogeneous SoCs help manage power and thermal constraints.}

  \emph{Answer}: Dynamic workload distribution strategies allocate tasks
  to processors based on current power and thermal conditions. For
  example, during high power demand, tasks may shift from power-hungry
  NPUs to more efficient CPUs. This is important because it ensures
  system performance while maintaining operational constraints.

  \emph{Learning Objective}: Analyze how dynamic workload distribution
  in heterogeneous SoCs addresses power and thermal challenges.
\item
  \textbf{Order the following steps in managing workload distribution on
  a heterogeneous SoC: (1) Assess system power budget, (2) Evaluate
  processor thermal state, (3) Allocate tasks based on constraints, (4)
  Monitor performance and adjust.}

  \emph{Answer}: The correct order is: (1) Assess system power budget,
  (2) Evaluate processor thermal state, (3) Allocate tasks based on
  constraints, (4) Monitor performance and adjust. This sequence ensures
  that tasks are allocated efficiently while continuously adapting to
  changing system conditions.

  \emph{Learning Objective}: Understand the process of dynamic workload
  management in heterogeneous SoCs.
\item
  \textbf{Which of the following best describes a challenge in
  programming heterogeneous SoCs?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Managing memory coherency across diverse processors.
  \item
    Ensuring all processors use the same programming model.
  \item
    Achieving optimal performance without considering processor-specific
    optimizations.
  \item
    Implementing a single execution strategy for all tasks.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Managing memory coherency
  across diverse processors. This is a challenge because each processor
  may have different caching and memory access patterns, requiring
  careful synchronization. Options B, C, and D do not accurately capture
  the complexity of programming heterogeneous systems.

  \emph{Learning Objective}: Identify challenges in software development
  for heterogeneous SoCs.
\item
  \textbf{In a production system, what trade-offs might you consider
  when implementing AI acceleration on a heterogeneous SoC for an
  autonomous vehicle?}

  \emph{Answer}: Trade-offs include balancing real-time processing needs
  with power efficiency. For example, safety-critical tasks may require
  deterministic CPU execution, while less critical tasks can run on
  NPUs. This is important because it affects both performance and energy
  consumption, crucial for vehicle operation.

  \emph{Learning Objective}: Evaluate trade-offs in deploying AI
  acceleration on heterogeneous SoCs in automotive applications.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-fallacies-pitfalls-dc1f}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.10}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-fallacies-pitfalls-dc1f}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following scenarios would most likely benefit
  from using general-purpose processors over specialized hardware
  accelerators?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    A workload with irregular memory access patterns and dynamic
    computation graphs.
  \item
    A workload with dense, regular computations and large batch sizes.
  \item
    A workload that requires high computational throughput with minimal
    memory access.
  \item
    A workload optimized for a specific vendor's proprietary libraries.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. A workload with irregular
  memory access patterns and dynamic computation graphs. General-purpose
  processors are better suited for workloads that do not align with the
  architectural assumptions of specialized hardware, such as irregular
  memory access patterns and dynamic computation graphs. Specialized
  hardware is optimized for dense, regular computations.

  \emph{Learning Objective}: Understand the conditions under which
  general-purpose processors may outperform specialized hardware.
\item
  \textbf{True or False: Adding more accelerators to a system will
  always result in linear performance improvements.}

  \emph{Answer}: False. This is false because multi-accelerator setups
  introduce communication overhead, synchronization costs, and load
  balancing challenges that can limit scaling efficiency. Performance
  gains are often non-linear due to these factors.

  \emph{Learning Objective}: Challenge the misconception that
  performance scales linearly with additional hardware.
\item
  \textbf{Explain why memory bandwidth limitations can undermine the
  performance benefits of AI accelerators.}

  \emph{Answer}: Memory bandwidth limitations can bottleneck AI
  accelerators by preventing them from achieving their theoretical
  computational throughput. If the memory cannot supply data at the rate
  needed by the accelerator, the hardware remains underutilized. This is
  important because it highlights the need to balance computational
  power with memory access capabilities to achieve optimal performance.

  \emph{Learning Objective}: Analyze how memory bandwidth constraints
  affect the real-world performance of AI accelerators.
\item
  \textbf{The belief that hardware acceleration benefits scale linearly
  with additional accelerators is a common \_\_\_\_. This misconception
  overlooks the communication and synchronization overheads that limit
  scaling efficiency.}

  \emph{Answer}: fallacy. This misconception overlooks the communication
  and synchronization overheads that limit scaling efficiency.

  \emph{Learning Objective}: Identify and understand common
  misconceptions in hardware acceleration scaling.
\item
  \textbf{In a production system, what trade-offs should be considered
  when optimizing for vendor-specific hardware?}

  \emph{Answer}: Optimizing for vendor-specific hardware can provide
  significant performance benefits but may lead to vendor lock-in,
  complicating future upgrades or migrations. This is important because
  maintaining flexibility and portability can be crucial for long-term
  system evolution and adaptation to new technologies.

  \emph{Learning Objective}: Evaluate the trade-offs between performance
  optimization and system flexibility in hardware selection.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-fallacies-pitfalls-dc1f]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-acceleration-summary-a5f8}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.11}{}
\phantomsection\label{quiz-answer-sec-ai-acceleration-summary-a5f8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the primary advantage of
  hardware-software co-design in AI accelerators?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Reduced hardware costs
  \item
    Simplified software development
  \item
    Improved computational efficiency
  \item
    Increased general-purpose applicability
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Improved computational
  efficiency. This is correct because hardware-software co-design aligns
  algorithm characteristics with architectural capabilities, leading to
  significant performance improvements. Other options do not directly
  address the efficiency gains from co-design.

  \emph{Learning Objective}: Understand the benefits of
  hardware-software co-design in AI accelerators.
\item
  \textbf{Explain how memory hierarchy management can become a
  bottleneck in AI acceleration and how it can be mitigated.}

  \emph{Answer}: Memory hierarchy management becomes a bottleneck due to
  limited bandwidth and latency issues. Techniques like data tiling,
  kernel fusion, and hierarchy-aware scheduling can mitigate these by
  optimizing data movement and reducing memory access latency. This is
  important because efficient memory management is critical for
  maximizing the performance of AI accelerators.

  \emph{Learning Objective}: Analyze the challenges and solutions
  related to memory hierarchy management in AI acceleration.
\item
  \textbf{Order the following steps in optimizing a multi-chip AI
  acceleration system: (1) Workload partitioning, (2) Communication
  overhead reduction, (3) Memory coherence management.}

  \emph{Answer}: The correct order is: (1) Workload partitioning, (3)
  Memory coherence management, (2) Communication overhead reduction.
  Workload partitioning is the initial step to distribute tasks across
  chips. Memory coherence management ensures data consistency across
  distributed memory. Finally, reducing communication overhead optimizes
  data transfer between chips.

  \emph{Learning Objective}: Understand the process of optimizing
  multi-chip AI acceleration systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-acceleration-summary-a5f8]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-chen_tvmlang_2018}
0001, Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q.
Yan, Haichen Shen, Meghan Cowan, et al. 2018b. {``TVM: An Automated
End-to-End Optimizing Compiler for Deep Learning.''} In \emph{13th
USENIX Symposium on Operating Systems Design and Implementation (OSDI
18)}, 578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-Chen2018}
---------, et al. 2018c. {``TVM: An Automated End-to-End Optimizing
Compiler for Deep Learning.''} In \emph{OSDI}, 578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-chen2018tvm}
---------, et al. 2018a. {``TVM: An Automated End-to-End Optimizing
Compiler for Deep Learning.''} In \emph{OSDI}, 578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-abadi2016tensorflow}
Abadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, et al. 2016. {``TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems.''} \emph{arXiv
Preprint arXiv:1603.04467}, March.
\url{http://arxiv.org/abs/1603.04467v2}.

\bibitem[\citeproctext]{ref-Taylor2017ASICMining}
Bedford Taylor, Michael. 2017. {``The Evolution of Bitcoin Hardware.''}
\emph{Computer} 50 (9): 58--66.
\url{https://doi.org/10.1109/mc.2017.3571056}.

\bibitem[\citeproctext]{ref-tensorflow_xla_2020}
Brain, Google. 2020. {``XLA: Optimizing Compiler for Machine
Learning.''} \emph{TensorFlow Blog}.
\url{https://www.tensorflow.org/xla}.

\bibitem[\citeproctext]{ref-tensorflow2022}
---------. 2022. \emph{TensorFlow Documentation}.
\url{https://www.tensorflow.org/}.

\bibitem[\citeproctext]{ref-Brown2020}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} \emph{NeurIPS}, May.
\url{http://arxiv.org/abs/2005.14165v4}.

\bibitem[\citeproctext]{ref-ieee_spectrum_relu}
Cass, Stephen. 2020. {``The History of the ReLU.''} \emph{IEEE
Spectrum}. \url{https://spectrum.ieee.org/the-history-of-the-relu}.

\bibitem[\citeproctext]{ref-Chen2016}
Chen, Yu-Hsin, Joel Emer, and Vivienne Sze. 2017. {``Using Dataflow to
Optimize Energy Efficiency of Deep Neural Network Accelerators.''}
\emph{IEEE Micro} 37 (3): 12--21.
\url{https://doi.org/10.1109/mm.2017.54}.

\bibitem[\citeproctext]{ref-chen2016eyeriss}
Chen, Yu-Hsin, Tushar Krishna, Joel S. Emer, and Vivienne Sze. 2016.
{``Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for
Convolutional Neural Networks.''} \emph{IEEE Journal of Solid-State
Circuits} 51 (1): 186--98.
\url{https://doi.org/10.1109/JSSC.2015.2488709}.

\bibitem[\citeproctext]{ref-chetlur2014cudnn}
Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,
John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. {``cuDNN:
Efficient Primitives for Deep Learning.''} \emph{arXiv Preprint
arXiv:1410.0759}, October. \url{http://arxiv.org/abs/1410.0759v3}.

\bibitem[\citeproctext]{ref-nvidia2022h100}
Choquette, Jack. 2023. {``NVIDIA Hopper H100 GPU: Scaling
Performance.''} \emph{IEEE Micro} 43 (3): 9--17.
\url{https://doi.org/10.1109/mm.2023.3256796}.

\bibitem[\citeproctext]{ref-intel2021amx}
Corporation, Intel. 2021. {``Intel Advanced Matrix Extensions (Intel
AMX).''} In \emph{Intel Architecture Instruction Set Extensions
Programming Reference}. Intel
Corporation.\href{\%0A\%20\%20\%20\%20https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html\%0A\%20\%20}{https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html
}.

\bibitem[\citeproctext]{ref-nvidia2017gpu}
Corporation, NVIDIA. 2017. {``NVIDIA Tesla V100 GPU Architecture.''}
NVIDIA Whitepaper.
\url{https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}.

\bibitem[\citeproctext]{ref-nvidia2020ampere}
---------. 2020. {``NVIDIA A100 Tensor Core GPU Architecture.''} NVIDIA
Whitepaper.\href{\%0A\%20\%20\%20\%20https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\%0A\%20\%20}{https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf
}.

\bibitem[\citeproctext]{ref-nvidia2021cudnn}
---------. 2021. \emph{NVIDIA cuDNN Developer Guide}. NVIDIA
Corporation.
\url{https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html}.

\bibitem[\citeproctext]{ref-Lauterbach2019}
Costa, Tiago, Chen Shi, Kevin Tien, and Kenneth L. Shepard. 2019. {``A
CMOS 2D Transmit Beamformer with Integrated PZT Ultrasound Transducers
for Neuromodulation.''} In \emph{2019 IEEE Custom Integrated Circuits
Conference (CICC)}, 1--4. IEEE.
\url{https://doi.org/10.1109/cicc.2019.8780236}.

\bibitem[\citeproctext]{ref-cui_mlcompilers_2019}
Cui, Hongyi, Jiajun Li, and Peng et al. Xie. 2019. {``A Survey on
Machine Learning Compilers: Taxonomy, Challenges, and Future
Directions.''} \emph{ACM Computing Surveys} 52 (4): 1--39.

\bibitem[\citeproctext]{ref-dao2022flashattention}
Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
2022. {``FlashAttention: Fast and Memory-Efficient Exact Attention with
IO-Awareness.''} In \emph{Advances in Neural Information Processing
Systems 35 (NeurIPS 2022)}, 16344--59. Curran Associates,
Inc.\href{\%0A\%20\%20\%20\%20https://proceedings.neurips.cc/paper/_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html\%0A\%20\%20}{https://proceedings.neurips.cc/paper\textbackslash\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html
}.

\bibitem[\citeproctext]{ref-Dosovitskiy2020ViT}
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.
2020. {``An Image Is Worth 16x16 Words: Transformers for Image
Recognition at Scale.''} \emph{International Conference on Learning
Representations (ICLR)}, October.
\url{http://arxiv.org/abs/2010.11929v2}.

\bibitem[\citeproctext]{ref-fisher_8087_1981}
Fisher, Lawrence D. 1981. {``The 8087 Numeric Data Processor.''}
\emph{IEEE Computer} 14 (7): 19--29.
\url{https://doi.org/10.1109/MC.1981.1653991}.

\bibitem[\citeproctext]{ref-gholami2024ai}
Gholami, Amir, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W.
Mahoney, and Kurt Keutzer. 2024. {``AI and Memory Wall.''} \emph{IEEE
Micro} 44 (3): 33--39. \url{https://doi.org/10.1109/mm.2024.3373763}.

\bibitem[\citeproctext]{ref-Goldberg1991}
Goldberg, David. 1991. {``What Every Computer Scientist Should Know
about Floating-Point Arithmetic.''} \emph{ACM Computing Surveys} 23 (1):
5--48. \url{https://doi.org/10.1145/103162.103163}.

\bibitem[\citeproctext]{ref-Golub1996Matrix}
Golub, Gene H., and Charles F. Van Loan. 1996. \emph{Matrix
Computations}. Johns Hopkins University Press.

\bibitem[\citeproctext]{ref-Goodfellow-et-al-2016}
Goodfellow, Ian J., Aaron Courville, and Yoshua Bengio. 2013. {``Scaling
up Spike-and-Slab Models for Unsupervised Feature Learning.''}
\emph{IEEE Transactions on Pattern Analysis and Machine Intelligence} 35
(8): 1902--14. \url{https://doi.org/10.1109/tpami.2012.273}.

\bibitem[\citeproctext]{ref-GoogleXLA}
Google. 2025. {``XLA: Optimizing Compiler for Machine Learning.''}
\url{https://tensorflow.org/xla}.

\bibitem[\citeproctext]{ref-Graphcore2020}
Graphcore. 2020. {``The Colossus MK2 IPU Processor.''} \emph{Graphcore
Technical Paper}.

\bibitem[\citeproctext]{ref-han2016eie}
Han, Song, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A.
Horowitz, and William J. Dally. 2016. {``EIE: Efficient Inference Engine
on Compressed Deep Neural Network.''} In \emph{2016 ACM/IEEE 43rd Annual
International Symposium on Computer Architecture (ISCA)}, 243--54. IEEE.
\url{https://doi.org/10.1109/isca.2016.30}.

\bibitem[\citeproctext]{ref-xla2020}
He, Xuzhen. 2023a. {``Accelerated Linear Algebra Compiler for
Computationally Efficient Numerical Models: Success and Potential Area
of Improvement.''} \emph{PLOS ONE} 18 (2): e0282265.
\url{https://doi.org/10.1371/journal.pone.0282265}.

\bibitem[\citeproctext]{ref-xla2021}
---------. 2023b. {``Accelerated Linear Algebra Compiler for
Computationally Efficient Numerical Models: Success and Potential Area
of Improvement.''} \emph{PLOS ONE} 18 (2): e0282265.
\url{https://doi.org/10.1371/journal.pone.0282265}.

\bibitem[\citeproctext]{ref-HennessyPatterson2017Turing}
Hennessy, John L., and David A. Patterson. 2019. {``A New Golden Age for
Computer Architecture.''} \emph{Communications of the ACM} 62 (2):
48--60. \url{https://doi.org/10.1145/3282307}.

\bibitem[\citeproctext]{ref-Horowitz2014}
Horowitz, Mark. 2014. {``1.1 Computing's Energy Problem (and What We Can
Do about It).''} In \emph{2014 IEEE International Solid-State Circuits
Conference Digest of Technical Papers (ISSCC)}. IEEE.
\url{https://doi.org/10.1109/isscc.2014.6757323}.

\bibitem[\citeproctext]{ref-deepmind_gpipe_2019}
Huang, Yanping et al. 2019. {``GPipe: Efficient Training of Giant Neural
Networks Using Pipeline Parallelism.''} In \emph{Advances in Neural
Information Processing Systems (NeurIPS)}.

\bibitem[\citeproctext]{ref-Hwu2011GPU}
Hwu, Wen-mei W. 2011. {``Introduction.''} In \emph{GPU Computing Gems
Emerald Edition}, xix--xx. Elsevier.
\url{https://doi.org/10.1016/b978-0-12-384988-5.00064-4}.

\bibitem[\citeproctext]{ref-oneDNN2021}
Intel, Corporation. 2021. \emph{oneDNN: Intel's Deep Learning Neural
Network Library}. \url{https://github.com/oneapi-src/oneDNN}.

\bibitem[\citeproctext]{ref-Ioffe2015}
Ioffe, Sergey, and Christian Szegedy. 2015. {``Batch Normalization:
Accelerating Deep Network Training by Reducing Internal Covariate
Shift.''} \emph{International Conference on Machine Learning (ICML)},
February, 448--56. \url{http://arxiv.org/abs/1502.03167v3}.

\bibitem[\citeproctext]{ref-jia2018beyond}
Jia, Zhihao, Matei Zaharia, and Alex Aiken. 2018. {``Beyond Data and
Model Parallelism for Deep Neural Networks.''} \emph{arXiv Preprint
arXiv:1807.05358}, July. \url{http://arxiv.org/abs/1807.05358v1}.

\bibitem[\citeproctext]{ref-Jia2019}
Jia, Ziheng, Nathan Tillman, Luis Vega, Po-An Ouyang, Matei Zaharia, and
Joseph E. Gonzalez. 2019. {``Optimizing DNN Computation with Relaxed
Graph Substitutions.''} \emph{Conference on Machine Learning and Systems
(MLSys)}.

\bibitem[\citeproctext]{ref-moreau2018relay}
Jones, Gareth A. 2018. {``Joining Dessins Together.''} \emph{arXiv
Preprint arXiv:1810.03960}, October.
\url{http://arxiv.org/abs/1810.03960v1}.

\bibitem[\citeproctext]{ref-google_tpu_2017}
Jouppi, Norman P et al. 2017a. {``In-Datacenter Performance Analysis of
a Tensor Processing Unit.''} \emph{Proceedings of the 44th Annual
International Symposium on Computer Architecture (ISCA)}.

\bibitem[\citeproctext]{ref-jouppi_tpu_2017}
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav
Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017b. {``In-Datacenter
Performance Analysis of a Tensor Processing Unit.''} In
\emph{Proceedings of the 44th Annual International Symposium on Computer
Architecture}, 1--12. ACM.
\url{https://doi.org/10.1145/3079856.3080246}.

\bibitem[\citeproctext]{ref-krizhevsky2012alexnet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-Kung1982}
Kung, Hsiang-Tsung. 1982. {``Why Systolic Architectures?''}
\emph{Computer} 15 (1): 37--46.
\url{https://doi.org/10.1109/mc.1982.1653825}.

\bibitem[\citeproctext]{ref-lam1991cache}
Lam, Monica D., Edward E. Rothberg, and Michael E. Wolf. 1991. {``The
Cache Performance and Optimizations of Blocked Algorithms.''} In
\emph{Proceedings of the Fourth International Conference on
Architectural Support for Programming Languages and Operating Systems -
ASPLOS-IV}, 63--74. ACM Press.
\url{https://doi.org/10.1145/106972.106981}.

\bibitem[\citeproctext]{ref-mlir_framework_2021}
Lattner, Chris, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis,
Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and
Oleksandr Zinenko. 2020. {``MLIR: A Compiler Infrastructure for the End
of Moore's Law.''} \emph{arXiv Preprint arXiv:2002.11054}, February.
\url{http://arxiv.org/abs/2002.11054v2}.

\bibitem[\citeproctext]{ref-lindholm2008nvidia}
Lindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008.
{``NVIDIA Tesla: A Unified Graphics and Computing Architecture.''}
\emph{IEEE Micro} 28 (2): 39--55.
\url{https://doi.org/10.1109/mm.2008.31}.

\bibitem[\citeproctext]{ref-ARM2020}
Ltd., Arm. 2020. \emph{Arm Cortex-M55 Processor Technical Reference
Manual}. Arm Holdings.
\url{https://developer.arm.com/documentation/101051/latest/}.

\bibitem[\citeproctext]{ref-lyons2011understanding}
Lyons, Richard G. 2011. \emph{Understanding Digital Signal Processing}.
3rd ed. Prentice Hall.

\bibitem[\citeproctext]{ref-mirhoseini_device_placement_2017}
Mirhoseini, Azalia et al. 2017. {``Device Placement Optimization with
Reinforcement Learning.''} \emph{International Conference on Machine
Learning (ICML)}.

\bibitem[\citeproctext]{ref-Narayanan2021}
Narayanan, Deepak, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,
Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, et al. 2021.
{``Efficient Large-Scale Language Model Training on GPU Clusters Using
Megatron-LM.''} \emph{NeurIPS}, April.
\url{http://arxiv.org/abs/2104.04473v5}.

\bibitem[\citeproctext]{ref-norrie2021design}
Norrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li,
James Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021.
{``The Design Process for Google's Training Chips: TPUv2 and TPUv3.''}
\emph{IEEE Micro} 41 (2): 56--63.
\url{https://doi.org/10.1109/mm.2021.3058217}.

\bibitem[\citeproctext]{ref-nvidia_tensorRT_2021}
NVIDIA. 2021. {``TensorRT: High-Performance Deep Learning Inference
Library.''} \emph{NVIDIA Developer Blog}.
\url{https://developer.nvidia.com/tensorrt}.

\bibitem[\citeproctext]{ref-owens2008gpu}
Owens, J. D., M. Houston, D. Luebke, S. Green, J. E. Stone, and J. C.
Phillips. 2008. {``GPU Computing.''} \emph{Proceedings of the IEEE} 96
(5): 879--99. \url{https://doi.org/10.1109/jproc.2008.917757}.

\bibitem[\citeproctext]{ref-palmer_8087_1981}
Palmer, John F. 1980. {``The INTEL\textregistered{} 8087 Numeric Data
Processor.''} In \emph{Proceedings of the May 19-22, 1980, National
Computer Conference on - AFIPS '80}, 887. ACM Press.
\url{https://doi.org/10.1145/1500518.1500674}.

\bibitem[\citeproctext]{ref-paszke2019pytorch}
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
Gregory Chanan, Trevor Killeen, et al. 2019. {``PyTorch: An Imperative
Style, High-Performance Deep Learning Library.''} In \emph{Advances in
Neural Information Processing Systems}, 32:8024--35.
\url{https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}.

\bibitem[\citeproctext]{ref-patterson2021computer}
Patterson, David A, and John L Hennessy. 2021. {``Computer Architecture:
A Quantitative Approach.''} In \emph{Proceedings of the ACM/IEEE
International Symposium on Computer Architecture}. Morgan Kaufmann.

\bibitem[\citeproctext]{ref-Tesla2021}
Quinnell, Eric. 2024. {``Tesla Transport Protocol over Ethernet (TTPoE):
A New Lossy, Exa-Scale Fabric for the Dojo AI Supercomputer.''} In
\emph{2024 IEEE Hot Chips 36 Symposium (HCS)}, 1--23. IEEE.
\url{https://doi.org/10.1109/hcs61935.2024.10664947}.

\bibitem[\citeproctext]{ref-Rajbhandari2020}
Rajbhandari, Samyam, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020.
{``ZeRO: Memory Optimization Towards Training Trillion Parameter
Models.''} In \emph{Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis (SC)}.
\url{https://doi.org/10.5555/3433701.3433721}.

\bibitem[\citeproctext]{ref-Shang2018GenomicsAccel}
Shang, J., G. Wang, and Y. Liu. 2018. {``Accelerating Genomic Data
Analysis with Domain-Specific Architectures.''} \emph{IEEE Transactions
on Computers} 67 (7): 965--78.
\url{https://doi.org/10.1109/TC.2018.2799212}.

\bibitem[\citeproctext]{ref-shazeer2018mesh}
Shazeer, Noam, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani,
Penporn Koanantakool, Peter Hawkins, et al. 2018. {``Mesh-TensorFlow:
Deep Learning for Supercomputers.''} \emph{arXiv Preprint
arXiv:1811.02084}, November. \url{http://arxiv.org/abs/1811.02084v1}.

\bibitem[\citeproctext]{ref-shoeybi_megatron_2020}
Shoeybi, Mohammad, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
Casper, and Bryan Catanzaro. 2019a. {``Megatron-LM: Training
Multi-Billion Parameter Language Models Using Model Parallelism.''}
\emph{arXiv Preprint arXiv:1909.08053}, September.
\url{http://arxiv.org/abs/1909.08053v4}.

\bibitem[\citeproctext]{ref-Shoeybi2019}
---------. 2019b. {``Megatron-LM: Training Multi-Billion Parameter
Language Models Using Model Parallelism.''} \emph{arXiv Preprint
arXiv:1909.08053}, September. \url{http://arxiv.org/abs/1909.08053v4}.

\bibitem[\citeproctext]{ref-Smith1997}
Smith, Steven W. 1997. \emph{The Scientist and Engineer's Guide to
Digital Signal Processing}. California Technical Publishing.
\url{https://www.dspguide.com/}.

\bibitem[\citeproctext]{ref-sodani2017knl}
Sodani, Avinash. 2015. {``Knights Landing (KNL): 2nd Generation
Intel\textregistered{} Xeon Phi Processor.''} In \emph{2015 IEEE Hot
Chips 27 Symposium (HCS)}, 1--24. IEEE.
\url{https://doi.org/10.1109/hotchips.2015.7477467}.

\bibitem[\citeproctext]{ref-stephens2017arm}
Stephens, Nigel, Stuart Biles, Matthias Boettcher, Jacob Eapen, Mbou
Eyole, Giacomo Gabrielli, Matt Horsnell, et al. 2017. {``The ARM
Scalable Vector Extension.''} \emph{IEEE Micro} 37 (2): 26--39.
\url{https://doi.org/10.1109/mm.2017.35}.

\bibitem[\citeproctext]{ref-sullivan2012overview}
Sullivan, Gary J., Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand.
2012. {``Overview of the High Efficiency Video Coding (HEVC)
Standard.''} \emph{IEEE Transactions on Circuits and Systems for Video
Technology} 22 (12): 1649--68.
\url{https://doi.org/10.1109/tcsvt.2012.2221191}.

\bibitem[\citeproctext]{ref-Cerebras2021}
Systems, Cerebras. 2021. {``Wafer-Scale Deep Learning Acceleration with
the Cerebras CS-2.''} \emph{Cerebras Technical Paper}.

\bibitem[\citeproctext]{ref-sze2017efficient}
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. 2017a.
{``Efficient Processing of Deep Neural Networks: A Tutorial and
Survey.''} \emph{Proceedings of the IEEE} 105 (12): 2295--2329.
\url{https://doi.org/10.1109/jproc.2017.2761740}.

\bibitem[\citeproctext]{ref-sze2020efficient}
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017b.
{``Efficient Processing of Deep Neural Networks: A Tutorial and
Survey.''} \emph{Proceedings of the IEEE} 105 (12): 2295--2329.
\url{https://doi.org/10.1109/jproc.2017.2761740}.

\bibitem[\citeproctext]{ref-riscv_manual}
Waterman, Andrew, and Krste Asanovic. 2019. \emph{The RISC-v Instruction
Set Manual, Volume i: User-Level ISA}. RISC-V Foundation.
\url{https://riscv.org/technical/specifications/}.

\bibitem[\citeproctext]{ref-Huang2019}
Xingyu, Huang et al. 2019. {``Addressing the Memory Bottleneck in AI
Accelerators.''} \emph{IEEE Micro}.

\bibitem[\citeproctext]{ref-zhang2020optimizing}
Zhang, Y., J. Li, and H. Ouyang. 2020. {``Optimizing Memory Access for
Deep Learning Workloads.''} \emph{IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems} 39 (11): 2345--58.

\bibitem[\citeproctext]{ref-Zheng2020}
Zheng, Lianmin, Ziheng Jia, Yida Gao, Jiacheng Lin, Song Han, Xuehai
Geng, Eric Zhao, and Tianqi Wu. 2020. {``Ansor: Generating
High-Performance Tensor Programs for Deep Learning.''} \emph{USENIX
Symposium on Operating Systems Design and Implementation (OSDI)},
863--79.

\end{CSLReferences}


\backmatter


\end{document}
